[{"instruction": "Summarizing the code.", "input": "def sources_add(source_uri, ruby=None, runas=None, gem_bin=None):\n    '''\n    \n    '''\n    return _gem(['sources', '--add', source_uri],\n                ruby,\n                gem_bin=gem_bin,\n                runas=runas)", "output": "Add a gem source.\n\n    :param source_uri: string\n        The source URI to add.\n    :param gem_bin: string : None\n        Full path to ``gem`` binary to use.\n    :param ruby: string : None\n        If RVM or rbenv are installed, the ruby version and gemset to use.\n        Ignored if ``gem_bin`` is specified.\n    :param runas: string : None\n        The user to run gem as.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gem.sources_add http://rubygems.org/", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_actor_handle_id(actor_handle_id, num_forks):\n    \"\"\"\n    \"\"\"\n    assert isinstance(actor_handle_id, ActorHandleID)\n    handle_id_hash = hashlib.sha1()\n    handle_id_hash.update(actor_handle_id.binary())\n    handle_id_hash.update(str(num_forks).encode(\"ascii\"))\n    handle_id = handle_id_hash.digest()\n    return ActorHandleID(handle_id)", "output": "Deterministically compute an actor handle ID.\n\n    A new actor handle ID is generated when it is forked from another actor\n    handle. The new handle ID is computed as hash(old_handle_id || num_forks).\n\n    Args:\n        actor_handle_id (common.ObjectID): The original actor handle ID.\n        num_forks: The number of times the original actor handle has been\n                   forked so far.\n\n    Returns:\n        An ID for the new actor handle.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_http_status(status_code, message, **kwargs):\n    \"\"\"\n    \"\"\"\n    error_class = exception_class_for_http_status(status_code)\n    error = error_class(message, **kwargs)\n\n    if error.code is None:\n        error.code = status_code\n\n    return error", "output": "Create a :class:`GoogleAPICallError` from an HTTP status code.\n\n    Args:\n        status_code (int): The HTTP status code.\n        message (str): The exception message.\n        kwargs: Additional arguments passed to the :class:`GoogleAPICallError`\n            constructor.\n\n    Returns:\n        GoogleAPICallError: An instance of the appropriate subclass of\n            :class:`GoogleAPICallError`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate(self, sam_template):\n        \"\"\" \n        \"\"\"\n\n        if \"Resources\" not in sam_template or not isinstance(sam_template[\"Resources\"], dict) \\\n                or not sam_template[\"Resources\"]:\n            raise InvalidDocumentException(\n                [InvalidTemplateException(\"'Resources' section is required\")])\n\n        SamTemplateValidator.validate(sam_template)", "output": "Validates the template and parameter values and raises exceptions if there's an issue\n\n        :param dict sam_template: SAM template", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\"\"\"\n        self.curr_idx = 0\n        random.shuffle(self.idx)\n        for buck in self.data:\n            np.random.shuffle(buck)", "output": "Resets the iterator to the beginning of the data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def route_filter_get(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    expand = kwargs.get('expand')\n\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n\n    try:\n        route_filter = netconn.route_filters.get(\n            route_filter_name=name,\n            resource_group_name=resource_group,\n            expand=expand\n        )\n        result = route_filter.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get details about a specific route filter.\n\n    :param name: The name of the route table to query.\n\n    :param resource_group: The resource group name assigned to the\n        route filter.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_filter_get test-filter testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def url_path(self):\n        ''' \n\n        '''\n        if self.failed:\n            return None\n        else:\n            # TODO should fix invalid URL characters\n            return '/' + os.path.splitext(os.path.basename(self._runner.path))[0]", "output": "The last path component for the basename of the configured filename.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def calculate(self, order, transaction):\n        \"\"\"\n        \n        \"\"\"\n        cost_per_share = transaction.price * self.cost_per_dollar\n        return abs(transaction.amount) * cost_per_share", "output": "Pay commission based on dollar value of shares.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_impl_ver():\n    # type: () -> str\n    \"\"\"\"\"\"\n    impl_ver = get_config_var(\"py_version_nodot\")\n    if not impl_ver or get_abbr_impl() == 'pp':\n        impl_ver = ''.join(map(str, get_impl_version_info()))\n    return impl_ver", "output": "Return implementation version.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_meta_delete(image_id=None,     # pylint: disable=C0103\n                      name=None,\n                      keys=None,\n                      profile=None, **kwargs):\n    '''\n    \n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.image_meta_delete(\n        image_id,\n        name,\n        keys\n    )", "output": "Delete a key=value pair from the metadata for an image\n    (nova image-meta set)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' nova.image_meta_delete 6f52b2ff-0b31-4d84-8fd1-af45b84824f6 keys=cheese\n        salt '*' nova.image_meta_delete name=myimage keys=salad,beans", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def modify(db=None, sql=None):\n    '''\n    \n    '''\n    cur = _connect(db)\n\n    if not cur:\n        return False\n\n    cur.execute(sql)\n    return True", "output": "Issue an SQL query to sqlite3 (with no return data), usually used\n    to modify the database in some way (insert, delete, create, etc)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sqlite3.modify /root/test.db 'CREATE TABLE test(id INT, testdata TEXT);'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def porttree_matches(name):\n    '''\n    \n    '''\n    matches = []\n    for category in _porttree().dbapi.categories:\n        if _porttree().dbapi.cp_list(category + \"/\" + name):\n            matches.append(category + \"/\" + name)\n    return matches", "output": "Returns a list containing the matches for a given package name from the\n    portage tree. Note that the specific version of the package will not be\n    provided for packages that have several versions in the portage tree, but\n    rather the name of the package (i.e. \"dev-python/paramiko\").", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _push_next(self):\n        \"\"\"\"\"\"\n        r = next(self._iter, None)\n        if r is None:\n            return\n        self._key_queue.put((self._sent_idx, r))\n        self._sent_idx += 1", "output": "Assign next batch workload to workers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setRootElement(self, root):\n        \"\"\" \"\"\"\n        if root is None: root__o = None\n        else: root__o = root._o\n        ret = libxml2mod.xmlDocSetRootElement(self._o, root__o)\n        if ret is None:return None\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Set the root element of the document (doc->children is a\n           list containing possibly comments, PIs, etc ...).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_kex(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.transport.server_mode:\n            self.transport._expect_packet(MSG_KEXGSS_GROUPREQ)\n            return\n        # request a bit range: we accept (min_bits) to (max_bits), but prefer\n        # (preferred_bits).  according to the spec, we shouldn't pull the\n        # minimum up above 1024.\n        self.gss_host = self.transport.gss_host\n        m = Message()\n        m.add_byte(c_MSG_KEXGSS_GROUPREQ)\n        m.add_int(self.min_bits)\n        m.add_int(self.preferred_bits)\n        m.add_int(self.max_bits)\n        self.transport._send_message(m)\n        self.transport._expect_packet(MSG_KEXGSS_GROUP)", "output": "Start the GSS-API / SSPI Authenticated Diffie-Hellman Group Exchange", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_imsize(self, im_name):\n        \"\"\"\n        \n        \"\"\"\n        img = cv2.imread(im_name)\n        return (img.shape[0], img.shape[1])", "output": "get image size info\n        Returns:\n        ----------\n        tuple of (height, width)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_repos(config_path=_DEFAULT_CONFIG_PATH, with_packages=False):\n    '''\n    \n    '''\n    _validate_config(config_path)\n\n    ret = dict()\n    cmd = ['repo', 'list', '-config={}'.format(config_path), '-raw=true']\n\n    cmd_ret = _cmd_run(cmd)\n    repos = [line.strip() for line in cmd_ret.splitlines()]\n\n    log.debug('Found repositories: %s', len(repos))\n\n    for name in repos:\n        ret[name] = get_repo(name=name, config_path=config_path,\n                             with_packages=with_packages)\n    return ret", "output": "List all of the local package repositories.\n\n    :param str config_path: The path to the configuration file for the aptly instance.\n    :param bool with_packages: Return a list of packages in the repo.\n\n    :return: A dictionary of the repositories.\n    :rtype: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.list_repos", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def notification_channel_descriptor_path(cls, project, channel_descriptor):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/notificationChannelDescriptors/{channel_descriptor}\",\n            project=project,\n            channel_descriptor=channel_descriptor,\n        )", "output": "Return a fully-qualified notification_channel_descriptor string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_combobox(self, text, choices, option, default=NoDefault,\r\n                        tip=None, restart=False):\r\n        \"\"\"\"\"\"\r\n        label = QLabel(text)\r\n        combobox = QComboBox()\r\n        if tip is not None:\r\n            combobox.setToolTip(tip)\r\n        for name, key in choices:\r\n            if not (name is None and key is None):\r\n                combobox.addItem(name, to_qvariant(key))\r\n        # Insert separators\r\n        count = 0\r\n        for index, item in enumerate(choices):\r\n            name, key = item\r\n            if name is None and key is None:\r\n                combobox.insertSeparator(index + count)\r\n                count += 1\r\n        self.comboboxes[combobox] = (option, default)\r\n        layout = QHBoxLayout()\r\n        layout.addWidget(label)\r\n        layout.addWidget(combobox)\r\n        layout.addStretch(1)\r\n        layout.setContentsMargins(0, 0, 0, 0)\r\n        widget = QWidget(self)\r\n        widget.label = label\r\n        widget.combobox = combobox\r\n        widget.setLayout(layout)\r\n        combobox.restart_required = restart\r\n        combobox.label_text = text\r\n        return widget", "output": "choices: couples (name, key)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def feature_types(self, feature_types):\n        \"\"\"\n        \"\"\"\n        if feature_types is not None:\n            if self._feature_names is None:\n                msg = 'Unable to set feature types before setting names'\n                raise ValueError(msg)\n\n            if isinstance(feature_types, STRING_TYPES):\n                # single string will be applied to all columns\n                feature_types = [feature_types] * self.num_col()\n\n            try:\n                if not isinstance(feature_types, str):\n                    feature_types = [n for n in iter(feature_types)]\n                else:\n                    feature_types = [feature_types]\n            except TypeError:\n                feature_types = [feature_types]\n\n            if len(feature_types) != self.num_col():\n                msg = 'feature_types must have the same length as data'\n                raise ValueError(msg)\n\n            valid = ('int', 'float', 'i', 'q')\n            if not all(isinstance(f, STRING_TYPES) and f in valid\n                       for f in feature_types):\n                raise ValueError('All feature_names must be {int, float, i, q}')\n        self._feature_types = feature_types", "output": "Set feature types (column types).\n\n        This is for displaying the results and unrelated\n        to the learning process.\n\n        Parameters\n        ----------\n        feature_types : list or None\n            Labels for features. None will reset existing feature names", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sum(self):\n        \"\"\"\n        \n        \"\"\"\n        return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)", "output": "Add up the elements in this RDD.\n\n        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n        6.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_output(self, cmd):\n        \"\"\"\n        \n        \"\"\"\n        p = self.Popen(cmd, stdout=subprocess.PIPE)\n        output, _ = p.communicate()\n        if p.returncode != 0:\n            raise RemoteCalledProcessError(p.returncode, cmd, self.host, output=output)\n        return output", "output": "Execute a shell command remotely and return the output.\n\n        Simplified version of Popen when you only want the output as a string and detect any errors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safe_accept(target, tgt_type='glob'):\n    '''\n    \n    '''\n    salt_key = salt.key.Key(__opts__)\n    ssh_client = salt.client.ssh.client.SSHClient()\n\n    ret = ssh_client.cmd(target, 'key.finger', tgt_type=tgt_type)\n\n    failures = {}\n    for minion, finger in six.iteritems(ret):\n        if not FINGERPRINT_REGEX.match(finger):\n            failures[minion] = finger\n        else:\n            fingerprints = salt_key.finger(minion)\n            accepted = fingerprints.get('minions', {})\n            pending = fingerprints.get('minions_pre', {})\n            if minion in accepted:\n                del ret[minion]\n                continue\n            elif minion not in pending:\n                failures[minion] = (\"Minion key {0} not found by salt-key\"\n                                    .format(minion))\n            elif pending[minion] != finger:\n                failures[minion] = (\"Minion key {0} does not match the key in \"\n                                    \"salt-key: {1}\"\n                                    .format(finger, pending[minion]))\n            else:\n                subprocess.call([\"salt-key\", \"-qya\", minion])\n\n        if minion in failures:\n            del ret[minion]\n\n    if failures:\n        print('safe_accept failed on the following minions:')\n        for minion, message in six.iteritems(failures):\n            print(minion)\n            print('-' * len(minion))\n            print(message)\n            print('')\n\n    __jid_event__.fire_event({'message': 'Accepted {0:d} keys'.format(len(ret))}, 'progress')\n    return ret, failures", "output": ".. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Accept a minion's public key after checking the fingerprint over salt-ssh\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.safe_accept my_minion\n        salt-run manage.safe_accept minion1,minion2 tgt_type=list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_timestamp_pb(cls, stamp):\n        \"\"\"\n        \"\"\"\n        microseconds = int(stamp.seconds * 1e6)\n        bare = from_microseconds(microseconds)\n        return cls(\n            bare.year,\n            bare.month,\n            bare.day,\n            bare.hour,\n            bare.minute,\n            bare.second,\n            nanosecond=stamp.nanos,\n            tzinfo=pytz.UTC,\n        )", "output": "Parse RFC 3339-compliant timestamp, preserving nanoseconds.\n\n        Args:\n            stamp (:class:`~google.protobuf.timestamp_pb2.Timestamp`): timestamp message\n\n        Returns:\n            :class:`DatetimeWithNanoseconds`:\n                an instance matching the timestamp message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_updated_request(self):\n        \"\"\" \n        \"\"\"\n        r_kwargs = {\n            \"table_name\": self.message.table_name,\n            \"filter\": self.message.filter,\n        }\n\n        if self.message.rows_limit != 0:\n            r_kwargs[\"rows_limit\"] = max(\n                1, self.message.rows_limit - self.rows_read_so_far\n            )\n\n        # if neither RowSet.row_keys nor RowSet.row_ranges currently exist,\n        # add row_range that starts with last_scanned_key as start_key_open\n        # to request only rows that have not been returned yet\n        if not self.message.HasField(\"rows\"):\n            row_range = data_v2_pb2.RowRange(start_key_open=self.last_scanned_key)\n            r_kwargs[\"rows\"] = data_v2_pb2.RowSet(row_ranges=[row_range])\n        else:\n            row_keys = self._filter_rows_keys()\n            row_ranges = self._filter_row_ranges()\n            r_kwargs[\"rows\"] = data_v2_pb2.RowSet(\n                row_keys=row_keys, row_ranges=row_ranges\n            )\n        return data_messages_v2_pb2.ReadRowsRequest(**r_kwargs)", "output": "Updates the given message request as per last scanned key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_active_contract_at_offset(self, root_symbol, dt, offset):\n        \"\"\"\n        \n        \"\"\"\n        oc = self.asset_finder.get_ordered_contracts(root_symbol)\n        session = self.trading_calendar.minute_to_session_label(dt)\n        front = oc.contract_before_auto_close(session.value)\n        back = oc.contract_at_offset(front, 1, dt.value)\n        if back is None:\n            return front\n        primary = self._active_contract(oc, front, back, session)\n        return oc.contract_at_offset(primary, offset, session.value)", "output": "For the given root symbol, find the contract that is considered active\n        on a specific date at a specific offset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_datetime_str(response):\n    '''\n    \n    '''\n    if response:\n        return dict([(k, '{0}'.format(v)) if isinstance(v, datetime.date) else (k, v) for k, v in six.iteritems(response)])\n    return None", "output": "modify any key-value pair where value is a datetime object to a string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eval_or_save_inference_results(self, results, dataset, output=None):\n        \"\"\"\n        \n        \"\"\"\n        continuous_id_to_COCO_id = {v: k for k, v in COCODetection.COCO_id_to_category_id.items()}\n        for res in results:\n            # convert to COCO's incontinuous category id\n            res['category_id'] = continuous_id_to_COCO_id[res['category_id']]\n            # COCO expects results in xywh format\n            box = res['bbox']\n            box[2] -= box[0]\n            box[3] -= box[1]\n            res['bbox'] = [round(float(x), 3) for x in box]\n\n        assert output is not None, \"COCO evaluation requires an output file!\"\n        with open(output, 'w') as f:\n            json.dump(results, f)\n        if len(results):\n            # sometimes may crash if the results are empty?\n            return COCODetection(cfg.DATA.BASEDIR, dataset).print_coco_metrics(output)\n        else:\n            return {}", "output": "Args:\n            results (list[dict]): the inference results as dicts.\n                Each dict corresponds to one __instance__. It contains the following keys:\n\n                image_id (str): the id that matches `load_inference_roidbs`.\n                category_id (int): the category prediction, in range [1, #category]\n                bbox (list[float]): x1, y1, x2, y2\n                score (float):\n                segmentation: the segmentation mask in COCO's rle format.\n\n            dataset (str): the name of the dataset to evaluate.\n            output (str): the output file to optionally save the results to.\n\n        Returns:\n            dict: the evaluation results.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_cstr_to_pystr(data, length):\n    \"\"\"\n    \"\"\"\n    if PY3:\n        res = []\n        for i in range(length.value):\n            try:\n                res.append(str(data[i].decode('ascii')))\n            except UnicodeDecodeError:\n                res.append(str(data[i].decode('utf-8')))\n    else:\n        res = []\n        for i in range(length.value):\n            try:\n                res.append(str(data[i].decode('ascii')))\n            except UnicodeDecodeError:\n                # pylint: disable=undefined-variable\n                res.append(unicode(data[i].decode('utf-8')))\n    return res", "output": "Revert C pointer to Python str\n\n    Parameters\n    ----------\n    data : ctypes pointer\n        pointer to data\n    length : ctypes pointer\n        pointer to length of data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def push_doc(self, document):\n        ''' \n\n        '''\n        msg = self._protocol.create('PUSH-DOC', document)\n        reply = self._send_message_wait_for_reply(msg)\n        if reply is None:\n            raise RuntimeError(\"Connection to server was lost\")\n        elif reply.header['msgtype'] == 'ERROR':\n            raise RuntimeError(\"Failed to push document: \" + reply.content['text'])\n        else:\n            return reply", "output": "Push a document to the server, overwriting any existing server-side doc.\n\n        Args:\n            document : (Document)\n                A Document to push to the server\n\n        Returns:\n            The server reply", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_valid_token(self, auth_token):\n        '''\n        \n        '''\n        # Make sure that auth token is hex. If it's None, or something other\n        # than hex, this will raise a ValueError.\n        try:\n            int(auth_token, 16)\n        except (TypeError, ValueError):\n            return False\n\n        # First check if the given token is in our session table; if so it's a\n        # salt-api token and we need to get the Salt token from there.\n        orig_session, _ = cherrypy.session.cache.get(auth_token, ({}, None))\n        # If it's not in the session table, assume it's a regular Salt token.\n        salt_token = orig_session.get('token', auth_token)\n\n        # The eauth system does not currently support perms for the event\n        # stream, so we're just checking if the token exists not if the token\n        # allows access.\n        if salt_token and self.resolver.get_token(salt_token):\n            return True\n\n        return False", "output": "Check if this is a valid salt-api token or valid Salt token\n\n        salt-api tokens are regular session tokens that tie back to a real Salt\n        token. Salt tokens are tokens generated by Salt's eauth system.\n\n        :return bool: True if valid, False if not valid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_artifacts(app):\n    \"\"\"\"\"\"\n    dest_path = app.builder.outdir + '/error'\n    source_path = app.builder.srcdir + '/build_version_doc/artifacts'\n    _run_cmd('cd ' + app.builder.srcdir)\n    _run_cmd('rm -rf ' + dest_path)\n    _run_cmd('mkdir -p ' + dest_path)\n    _run_cmd('cp ' + source_path + '/404.html ' + dest_path)\n    _run_cmd('cp ' + source_path + '/api.html ' + dest_path)\n    dest_path = app.builder.outdir + '/_static'\n    _run_cmd('rm -rf ' + dest_path)\n    _run_cmd('mkdir -p ' + dest_path)\n    _run_cmd('cp ' + app.builder.srcdir + '/_static/mxnet.css ' + dest_path)", "output": "Copies artifacts needed for website presentation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(cls, train_ds, valid_ds, test_ds=None, path:PathOrStr='.', no_check:bool=False, bs=64, val_bs:int=None,\n               num_workers:int=0, device:torch.device=None, collate_fn:Callable=data_collate,\n               dl_tfms:Optional[Collection[Callable]]=None, bptt:int=70, backwards:bool=False, **dl_kwargs) -> DataBunch:\n        \"\"\n        datasets = cls._init_ds(train_ds, valid_ds, test_ds)\n        val_bs = ifnone(val_bs, bs)\n        datasets = [LanguageModelPreLoader(ds, shuffle=(i==0), bs=(bs if i==0 else val_bs), bptt=bptt, backwards=backwards)\n                    for i,ds in enumerate(datasets)]\n        val_bs = bs\n        dls = [DataLoader(d, b, shuffle=False, **dl_kwargs) for d,b in zip(datasets, (bs,val_bs,val_bs,val_bs)) if d is not None]\n        return cls(*dls, path=path, device=device, dl_tfms=dl_tfms, collate_fn=collate_fn, no_check=no_check)", "output": "Create a `TextDataBunch` in `path` from the `datasets` for language modelling. Passes `**dl_kwargs` on to `DataLoader()`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode(tokens):\n  \"\"\"\n  \"\"\"\n  token_is_alnum = [t[0] in _ALPHANUMERIC_CHAR_SET for t in tokens]\n  ret = []\n  for i, token in enumerate(tokens):\n    if i > 0 and token_is_alnum[i - 1] and token_is_alnum[i]:\n      ret.append(u\" \")\n    ret.append(token)\n  return \"\".join(ret)", "output": "Decode a list of tokens to a unicode string.\n\n  Args:\n    tokens: a list of Unicode strings\n  Returns:\n    a unicode string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def res_block(nf, dense:bool=False, norm_type:Optional[NormType]=NormType.Batch, bottle:bool=False, **conv_kwargs):\n    \"\"\n    norm2 = norm_type\n    if not dense and (norm_type==NormType.Batch): norm2 = NormType.BatchZero\n    nf_inner = nf//2 if bottle else nf\n    return SequentialEx(conv_layer(nf, nf_inner, norm_type=norm_type, **conv_kwargs),\n                      conv_layer(nf_inner, nf, norm_type=norm2, **conv_kwargs),\n                      MergeLayer(dense))", "output": "Resnet block of `nf` features. `conv_kwargs` are passed to `conv_layer`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_coerce_result(self, result):\n        \"\"\"  \"\"\"\n        if isinstance(result, np.ndarray):\n            if result.dtype.kind in ['i', 'f']:\n                result = result.astype('M8[ns]')\n\n        elif isinstance(result, (np.integer, np.float, np.datetime64)):\n            result = self._box_func(result)\n        return result", "output": "reverse of try_coerce_args", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def arg_dict(self):\n        \"\"\"\n        \"\"\"\n        if self._arg_dict is None:\n            self._arg_dict = Executor._get_dict(\n                self._symbol.list_arguments(), self.arg_arrays)\n        return self._arg_dict", "output": "Get dictionary representation of argument arrrays.\n\n        Returns\n        -------\n        arg_dict : dict of str to NDArray\n            The dictionary that maps the names of arguments to NDArrays.\n\n        Raises\n        ------\n        ValueError : if there are duplicated names in the arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer_and_cast(value: Any):\n    \"\"\"\n    \n    \"\"\"\n    # pylint: disable=too-many-return-statements\n    if isinstance(value, (int, float, bool)):\n        # Already one of our desired types, so leave as is.\n        return value\n    elif isinstance(value, list):\n        # Recursively call on each list element.\n        return [infer_and_cast(item) for item in value]\n    elif isinstance(value, dict):\n        # Recursively call on each dict value.\n        return {key: infer_and_cast(item) for key, item in value.items()}\n    elif isinstance(value, str):\n        # If it looks like a bool, make it a bool.\n        if value.lower() == \"true\":\n            return True\n        elif value.lower() == \"false\":\n            return False\n        else:\n            # See if it could be an int.\n            try:\n                return int(value)\n            except ValueError:\n                pass\n            # See if it could be a float.\n            try:\n                return float(value)\n            except ValueError:\n                # Just return it as a string.\n                return value\n    else:\n        raise ValueError(f\"cannot infer type of {value}\")", "output": "In some cases we'll be feeding params dicts to functions we don't own;\n    for example, PyTorch optimizers. In that case we can't use ``pop_int``\n    or similar to force casts (which means you can't specify ``int`` parameters\n    using environment variables). This function takes something that looks JSON-like\n    and recursively casts things that look like (bool, int, float) to (bool, int, float).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_entities(doc):\n    \"\"\"\n    \"\"\"\n    with doc.retokenize() as retokenizer:\n        for ent in doc.ents:\n            attrs = {\"tag\": ent.root.tag, \"dep\": ent.root.dep, \"ent_type\": ent.label}\n            retokenizer.merge(ent, attrs=attrs)\n    return doc", "output": "Merge entities into a single token.\n\n    doc (Doc): The Doc object.\n    RETURNS (Doc): The Doc object with merged entities.\n\n    DOCS: https://spacy.io/api/pipeline-functions#merge_entities", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _copy_new_parent(self, parent):\n        \"\"\"\"\"\"\n        if self.parent == \"undefined\":\n            param = copy.copy(self)\n            param.parent = parent.uid\n            return param\n        else:\n            raise ValueError(\"Cannot copy from non-dummy parent %s.\" % parent)", "output": "Copy the current param to a new parent, must be a dummy param.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_variable(variable, gradient=None):\n    \n    '''\n    name = variable.name.replace(':', '_')\n    mean = tf.reduce_mean(variable)\n    tf.summary.scalar(name='%s/mean'   % name, tensor=mean)\n    tf.summary.scalar(name='%s/sttdev' % name, tensor=tf.sqrt(tf.reduce_mean(tf.square(variable - mean))))\n    tf.summary.scalar(name='%s/max'    % name, tensor=tf.reduce_max(variable))\n    tf.summary.scalar(name='%s/min'    % name, tensor=tf.reduce_min(variable))\n    tf.summary.histogram(name=name, values=variable)\n    if gradient is not None:\n        if isinstance(gradient, tf.IndexedSlices):\n            grad_values = gradient.values\n        else:\n            grad_values = gradient\n        if grad_values is not None:\n            tf.summary.histogram(name='%s/gradients' % name, values=grad_values)", "output": "r'''\n    We introduce a function for logging a tensor variable's current state.\n    It logs scalar values for the mean, standard deviation, minimum and maximum.\n    Furthermore it logs a histogram of its state and (if given) of an optimization gradient.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rdd(self):\n        \"\"\"\n        \"\"\"\n        if self._lazy_rdd is None:\n            jrdd = self._jdf.javaToPython()\n            self._lazy_rdd = RDD(jrdd, self.sql_ctx._sc, BatchedSerializer(PickleSerializer()))\n        return self._lazy_rdd", "output": "Returns the content as an :class:`pyspark.RDD` of :class:`Row`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def postprocess(self, tempname, filename):\n        \"\"\"\n        \"\"\"\n\n        if os.name == 'posix':\n            # Make the resource executable\n            mode = ((os.stat(tempname).st_mode) | 0o555) & 0o7777\n            os.chmod(tempname, mode)", "output": "Perform any platform-specific postprocessing of `tempname`\n\n        This is where Mac header rewrites should be done; other platforms don't\n        have anything special they should do.\n\n        Resource providers should call this method ONLY after successfully\n        extracting a compressed resource.  They must NOT call it on resources\n        that are already in the filesystem.\n\n        `tempname` is the current (temporary) name of the file, and `filename`\n        is the name it will be renamed to by the caller after this routine\n        returns.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_tree_from_tag(self, ref):\n        '''\n        \n        '''\n        try:\n            return git.TagReference(\n                self.repo,\n                'refs/tags/{0}'.format(ref)).commit.tree\n        except ValueError:\n            return None", "output": "Return a git.Tree object matching a tag ref fetched into refs/tags/", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _credentials(self):\n        \"\"\"\n        \n        \"\"\"\n\n        if self.aws_account_id and self.aws_arn_role_name:\n            return 'aws_iam_role=arn:aws:iam::{id}:role/{role}'.format(\n                id=self.aws_account_id,\n                role=self.aws_arn_role_name\n            )\n        elif self.aws_access_key_id and self.aws_secret_access_key:\n            return 'aws_access_key_id={key};aws_secret_access_key={secret}{opt}'.format(\n                key=self.aws_access_key_id,\n                secret=self.aws_secret_access_key,\n                opt=';token={}'.format(self.aws_session_token) if self.aws_session_token else ''\n            )\n        else:\n            raise NotImplementedError(\"Missing Credentials. \"\n                                      \"Ensure one of the pairs of auth args below are set \"\n                                      \"in a configuration file, environment variables or by \"\n                                      \"being overridden in the task: \"\n                                      \"'aws_access_key_id' AND 'aws_secret_access_key' OR \"\n                                      \"'aws_account_id' AND 'aws_arn_role_name'\")", "output": "Return a credential string for the provided task. If no valid\n        credentials are set, raise a NotImplementedError.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_load(jid):\n    '''\n    \n    '''\n    log.debug('sqlite3 returner <get_load> called jid: %s', jid)\n    conn = _get_conn(ret=None)\n    cur = conn.cursor()\n    sql = '''SELECT load FROM jids WHERE jid = :jid'''\n    cur.execute(sql,\n                {'jid': jid})\n    data = cur.fetchone()\n    if data:\n        return salt.utils.json.loads(data[0].encode())\n    _close_conn(conn)\n    return {}", "output": "Return the load from a specified jid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unwrap(self, dt):\n        \"\"\"\n        \n        \"\"\"\n        expires = self._expires\n        if expires is AlwaysExpired or expires < dt:\n            raise Expired(self._expires)\n        return self._value", "output": "Get the cached value.\n\n        Returns\n        -------\n        value : object\n            The cached value.\n\n        Raises\n        ------\n        Expired\n            Raised when `dt` is greater than self.expires.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def head(self, n=None):\n        \"\"\"\n        \"\"\"\n        if n is None:\n            rs = self.head(1)\n            return rs[0] if rs else None\n        return self.take(n)", "output": "Returns the first ``n`` rows.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n\n        :param n: int, default 1. Number of rows to return.\n        :return: If n is greater than 1, return a list of :class:`Row`.\n            If n is 1, return a single Row.\n\n        >>> df.head()\n        Row(age=2, name=u'Alice')\n        >>> df.head(1)\n        [Row(age=2, name=u'Alice')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_response_norm(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs,\n                                                       {'bias': 'knorm',\n                                                        'size' : 'nsize'})\n    return 'LRN', new_attrs, inputs", "output": "Local Response Normalization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gem(command, ruby=None, runas=None, gem_bin=None):\n    '''\n    \n    '''\n    cmdline = [gem_bin or 'gem'] + command\n\n    # If a custom gem is given, use that and don't check for rvm/rbenv. User\n    # knows best!\n    if gem_bin is None:\n        if __salt__['rvm.is_installed'](runas=runas):\n            return __salt__['rvm.do'](ruby, cmdline, runas=runas)\n\n        if not salt.utils.platform.is_windows() \\\n                and __salt__['rbenv.is_installed'](runas=runas):\n            if ruby is None:\n                return __salt__['rbenv.do'](cmdline, runas=runas)\n            else:\n                return __salt__['rbenv.do_with_ruby'](ruby,\n                                                      cmdline,\n                                                      runas=runas)\n\n    ret = __salt__['cmd.run_all'](cmdline, runas=runas, python_shell=False)\n\n    if ret['retcode'] == 0:\n        return ret['stdout']\n    else:\n        raise CommandExecutionError(ret['stderr'])", "output": "Run the actual gem command. If rvm or rbenv is installed, run the command\n    using the corresponding module. rbenv is not available on windows, so don't\n    try.\n\n    :param command: string\n    Command to run\n    :param ruby: string : None\n    If RVM or rbenv are installed, the ruby version and gemset to use.\n    Ignored if ``gem_bin`` is specified.\n    :param runas: string : None\n    The user to run gem as.\n    :param gem_bin: string : None\n    Full path to the ``gem`` binary\n\n    :return:\n    Returns the full standard out including success codes or False if it fails", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes function must be called '\n            'with -f or --function.'\n        )\n\n    ret = {}\n    vm_properties = [\n        \"name\",\n        \"guest.ipAddress\",\n        \"config.guestFullName\",\n        \"config.hardware.numCPU\",\n        \"config.hardware.memoryMB\",\n        \"summary.runtime.powerState\"\n    ]\n\n    vm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\n    for vm in vm_list:\n        cpu = vm[\"config.hardware.numCPU\"] if \"config.hardware.numCPU\" in vm else \"N/A\"\n        ram = \"{0} MB\".format(vm[\"config.hardware.memoryMB\"]) if \"config.hardware.memoryMB\" in vm else \"N/A\"\n        vm_info = {\n            'id': vm[\"name\"],\n            'image': \"{0} (Detected)\".format(vm[\"config.guestFullName\"]) if \"config.guestFullName\" in vm else \"N/A\",\n            'size': \"cpu: {0}\\nram: {1}\".format(cpu, ram),\n            'size_dict': {\n                'cpu': cpu,\n                'memory': ram,\n            },\n            'state': six.text_type(vm[\"summary.runtime.powerState\"]) if \"summary.runtime.powerState\" in vm else \"N/A\",\n            'private_ips': [vm[\"guest.ipAddress\"]] if \"guest.ipAddress\" in vm else [],\n            'public_ips': []\n        }\n        ret[vm_info['id']] = vm_info\n\n    return ret", "output": "Return a list of all VMs and templates that are on the specified provider, with basic fields\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_nodes my-vmware-config\n\n    To return a list of all VMs and templates present on ALL configured providers, with basic\n    fields:\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -Q", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_keys(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        log.error(\n            'The list_keys function must be called with -f or --function.'\n        )\n        return False\n\n    if not kwargs:\n        kwargs = {}\n\n    ret = {}\n    rcode, data = query(command='my/keys', method='GET')\n    for pair in data:\n        ret[pair['name']] = pair['key']\n    return {'keys': ret}", "output": "List the keys available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_cond(self, apply_fn, grad, var, *args, **kwargs):\n    \"\"\"\"\"\"\n    grad_acc = self.get_slot(var, \"grad_acc\")\n\n    def apply_adam(grad_acc, apply_fn, grad, var, *args, **kwargs):\n      total_grad = (grad_acc + grad) / tf.cast(self._n_t, grad.dtype)\n      adam_op = apply_fn(total_grad, var, *args, **kwargs)\n      with tf.control_dependencies([adam_op]):\n        grad_acc_to_zero_op = grad_acc.assign(tf.zeros_like(grad_acc),\n                                              use_locking=self._use_locking)\n      return tf.group(adam_op, grad_acc_to_zero_op)\n\n    def accumulate_gradient(grad_acc, grad):\n      assign_op = tf.assign_add(grad_acc, grad, use_locking=self._use_locking)\n      return tf.group(assign_op)  # Strip return value\n\n    return tf.cond(\n        tf.equal(self._get_iter_variable(), 0),\n        lambda: apply_adam(grad_acc, apply_fn, grad, var, *args, **kwargs),\n        lambda: accumulate_gradient(grad_acc, grad))", "output": "Apply conditionally if counter is zero.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def disconnect_session(session_id):\n    '''\n    \n    '''\n    try:\n        win32ts.WTSDisconnectSession(win32ts.WTS_CURRENT_SERVER_HANDLE, session_id, True)\n    except PyWinError as error:\n        _LOG.error('Error calling WTSDisconnectSession: %s', error)\n        return False\n    return True", "output": "Disconnect a session.\n\n    .. versionadded:: 2016.11.0\n\n    :param session_id: The numeric Id of the session.\n    :return: A boolean representing whether the disconnect succeeded.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rdp.disconnect_session session_id\n\n        salt '*' rdp.disconnect_session 99", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_params(step_num, model, trainer, ckpt_dir):\n    \"\"\"\"\"\"\n    param_path = os.path.join(ckpt_dir, '%07d.params'%step_num)\n    trainer_path = os.path.join(ckpt_dir, '%07d.states'%step_num)\n    logging.info('[step %d] Saving checkpoints to %s, %s.',\n                 step_num, param_path, trainer_path)\n    model.save_parameters(param_path)\n    trainer.save_states(trainer_path)", "output": "Save the model parameter, marked by step_num.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def coerce_to_dtypes(result, dtypes):\n    \"\"\"\n    \n    \"\"\"\n    if len(result) != len(dtypes):\n        raise AssertionError(\"_coerce_to_dtypes requires equal len arrays\")\n\n    def conv(r, dtype):\n        try:\n            if isna(r):\n                pass\n            elif dtype == _NS_DTYPE:\n                r = tslibs.Timestamp(r)\n            elif dtype == _TD_DTYPE:\n                r = tslibs.Timedelta(r)\n            elif dtype == np.bool_:\n                # messy. non 0/1 integers do not get converted.\n                if is_integer(r) and r not in [0, 1]:\n                    return int(r)\n                r = bool(r)\n            elif dtype.kind == 'f':\n                r = float(r)\n            elif dtype.kind == 'i':\n                r = int(r)\n        except Exception:\n            pass\n\n        return r\n\n    return [conv(r, dtype) for r, dtype in zip(result, dtypes)]", "output": "given a dtypes and a result set, coerce the result elements to the\n    dtypes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _box_col_values(self, values, items):\n        \"\"\"\n        \n        \"\"\"\n        klass = self._constructor_sliced\n        return klass(values, index=self.index, name=items, fastpath=True)", "output": "Provide boxed values for a column.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def close(self) -> None:\n        \"\"\"\n        \"\"\"\n        self.stream.close()\n        # Block until the serving loop is done, but ignore any exceptions\n        # (start_serving is already responsible for logging them).\n        assert self._serving_future is not None\n        try:\n            await self._serving_future\n        except Exception:\n            pass", "output": "Closes the connection.\n\n        Returns a `.Future` that resolves after the serving loop has exited.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _router_request(router, method, data=None):\n    '''\n    \n    '''\n    if router not in ROUTERS:\n        return False\n\n    req_data = salt.utils.json.dumps([dict(\n        action=router,\n        method=method,\n        data=data,\n        type='rpc',\n        tid=1)])\n\n    config = __salt__['config.option']('zenoss')\n    log.debug('Making request to router %s with method %s', router, method)\n    url = '{0}/zport/dmd/{1}_router'.format(config.get('hostname'), ROUTERS[router])\n    response = _session().post(url, data=req_data)\n\n    # The API returns a 200 response code even whe auth is bad.\n    # With bad auth, the login page is displayed. Here I search for\n    # an element on the login form to determine if auth failed.\n    if re.search('name=\"__ac_name\"', response.content):\n        log.error('Request failed. Bad username/password.')\n        raise Exception('Request failed. Bad username/password.')\n\n    return salt.utils.json.loads(response.content).get('result', None)", "output": "Make a request to the Zenoss API router", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_tall():\n  \"\"\"\"\"\"\n  hparams = transformer_base()\n  hparams.batch_size = 2048\n  hparams.hidden_size = 768\n  hparams.filter_size = 3072\n  hparams.num_hidden_layers = 12\n  hparams.num_heads = 12\n  hparams.label_smoothing = 0.0\n  hparams.max_length = 1024\n  hparams.eval_drop_long_sequences = True\n  hparams.multiproblem_mixing_schedule = \"pretrain\"\n  hparams.multiproblem_vocab_size = 65536\n  hparams.clip_grad_norm = 1.0\n  return hparams", "output": "Hparams for transformer on LM for pretraining/finetuning/mixing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute(self):\n        \"\"\"\n        \n        \"\"\"\n        r = self\n        for func, args, kwargs in self._todo:\n            r = func(self)(*args, **kwargs)\n        return r", "output": "Execute the style functions built up in `self._todo`.\n\n        Relies on the conventions that all style functions go through\n        .apply or .applymap. The append styles to apply as tuples of\n\n        (application method, *args, **kwargs)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_activate_this(self):\n        \"\"\"\"\"\"\n        if self.is_venv:\n            activate_this = os.path.join(self.scripts_dir, \"activate_this.py\")\n            if not os.path.isfile(activate_this):\n                raise OSError(\"No such file: {0!s}\".format(activate_this))\n            with open(activate_this, \"r\") as f:\n                code = compile(f.read(), activate_this, \"exec\")\n                exec(code, dict(__file__=activate_this))", "output": "Runs the environment's inline activation script", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sink_pubsub(client, to_delete):\n    \"\"\"\"\"\"\n    topic = _sink_pubsub_setup(client)\n    to_delete.append(topic)\n    SINK_NAME = \"robots-pubsub-%d\" % (_millis(),)\n    FILTER = \"logName:apache-access AND textPayload:robot\"\n    UPDATED_FILTER = \"textPayload:robot\"\n\n    # [START sink_pubsub_create]\n    DESTINATION = \"pubsub.googleapis.com/%s\" % (topic.full_name,)\n    sink = client.sink(SINK_NAME, filter_=FILTER, destination=DESTINATION)\n    assert not sink.exists()  # API call\n    sink.create()  # API call\n    assert sink.exists()  # API call\n    # [END sink_pubsub_create]\n    to_delete.insert(0, sink)  # delete sink before topic\n\n    # [START client_list_sinks]\n    for sink in client.list_sinks():  # API call(s)\n        do_something_with(sink)\n    # [END client_list_sinks]\n\n    # [START sink_reload]\n    existing_sink = client.sink(SINK_NAME)\n    existing_sink.reload()\n    # [END sink_reload]\n    assert existing_sink.filter_ == FILTER\n    assert existing_sink.destination == DESTINATION\n\n    # [START sink_update]\n    existing_sink.filter_ = UPDATED_FILTER\n    existing_sink.update()\n    # [END sink_update]\n    existing_sink.reload()\n    assert existing_sink.filter_ == UPDATED_FILTER\n\n    # [START sink_delete]\n    sink.delete()\n    # [END sink_delete]\n    to_delete.pop(0)", "output": "Sink log entries to pubsub.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deeplift_grad(module, grad_input, grad_output):\n    \"\"\"\n    \"\"\"\n    # first, get the module type\n    module_type = module.__class__.__name__\n    # first, check the module is supported\n    if module_type in op_handler:\n        if op_handler[module_type].__name__ not in ['passthrough', 'linear_1d']:\n            return op_handler[module_type](module, grad_input, grad_output)\n    else:\n        print('Warning: unrecognized nn.Module: {}'.format(module_type))\n        return grad_input", "output": "The backward hook which computes the deeplift\n    gradient for an nn.Module", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def SLICE_0(self, instr):\n        ''\n        value = self.ast_stack.pop()\n\n        kw = dict(lineno=instr.lineno, col_offset=0)\n        slice = _ast.Slice(lower=None, step=None, upper=None, **kw)\n        subscr = _ast.Subscript(value=value, slice=slice, ctx=_ast.Load(), **kw)\n\n        self.ast_stack.append(subscr)", "output": "obj[:]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _learner_distributed(learn:Learner, cuda_id:int, cache_dir:PathOrStr='tmp'):\n    \"\"\n    learn.callbacks.append(DistributedTrainer(learn, cuda_id))\n    learn.callbacks.append(DistributedRecorder(learn, cuda_id, cache_dir))\n    return learn", "output": "Put `learn` on distributed training with `cuda_id`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert(model, input_features, output_features):\n    \"\"\"\n    \"\"\"\n\n    if not(_HAS_SKLEARN):\n        raise RuntimeError('scikit-learn not found. scikit-learn conversion API is disabled.')\n\n    # Test the scikit-learn model\n    _sklearn_util.check_expected_type(model, Normalizer)\n    _sklearn_util.check_fitted(model, lambda m: hasattr(m, 'norm'))\n\n    # Set the interface params.\n    spec = _Model_pb2.Model()\n    spec.specificationVersion = SPECIFICATION_VERSION\n    spec = _set_transform_interface_params(spec, input_features, output_features)\n\n    # Set the one hot encoder parameters\n    _normalizer_spec = spec.normalizer\n    if model.norm == 'l1':\n        _normalizer_spec.normType = _proto__normalizer.L1\n    elif model.norm == 'l2':\n        _normalizer_spec.normType = _proto__normalizer.L2\n    elif model.norm == 'max':\n        _normalizer_spec.normType = _proto__normalizer.LMax\n    return _MLModel(spec)", "output": "Convert a normalizer model to the protobuf spec.\n\n    Parameters\n    ----------\n    model: Normalizer\n        A Normalizer.\n\n    input_features: str\n        Name of the input column.\n\n    output_features: str\n        Name of the output column.\n\n    Returns\n    -------\n    model_spec: An object of type Model_pb.\n        Protobuf representation of the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_makefile_filename():\n    \"\"\"\"\"\"\n    if _PYTHON_BUILD:\n        return os.path.join(_PROJECT_BASE, \"Makefile\")\n    if hasattr(sys, 'abiflags'):\n        config_dir_name = 'config-%s%s' % (_PY_VERSION_SHORT, sys.abiflags)\n    else:\n        config_dir_name = 'config'\n    return os.path.join(get_path('stdlib'), config_dir_name, 'Makefile')", "output": "Return the path of the Makefile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train(cls, data, lambda_=1.0):\n        \"\"\"\n        \n        \"\"\"\n        first = data.first()\n        if not isinstance(first, LabeledPoint):\n            raise ValueError(\"`data` should be an RDD of LabeledPoint\")\n        labels, pi, theta = callMLlibFunc(\"trainNaiveBayesModel\", data, lambda_)\n        return NaiveBayesModel(labels.toArray(), pi.toArray(), numpy.array(theta))", "output": "Train a Naive Bayes model given an RDD of (label, features)\n        vectors.\n\n        This is the Multinomial NB (U{http://tinyurl.com/lsdw6p}) which\n        can handle all kinds of discrete data.  For example, by\n        converting documents into TF-IDF vectors, it can be used for\n        document classification. By making every vector a 0-1 vector,\n        it can also be used as Bernoulli NB (U{http://tinyurl.com/p7c96j6}).\n        The input feature values must be nonnegative.\n\n        :param data:\n          RDD of LabeledPoint.\n        :param lambda_:\n          The smoothing parameter.\n          (default: 1.0)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _patch_for_tf1_13(tf):\n  \"\"\"\"\"\"\n  if not hasattr(tf.io.gfile, \"GFile\"):\n    tf.io.gfile.GFile = tf.gfile.GFile\n  if not hasattr(tf, \"nest\"):\n    tf.nest = tf.contrib.framework.nest\n  if not hasattr(tf.compat, \"v2\"):\n    tf.compat.v2 = types.ModuleType(\"tf.compat.v2\")\n    tf.compat.v2.data = types.ModuleType(\"tf.compat.v2.data\")\n    from tensorflow.python.data.ops import dataset_ops\n    tf.compat.v2.data.Dataset = dataset_ops.DatasetV2\n  if not hasattr(tf.compat.v2.data.Dataset, \"output_shapes\"):\n    from tensorflow.python.data.ops import dataset_ops\n    if hasattr(dataset_ops, \"get_legacy_output_shapes\"):\n      tf.compat.v2.data.Dataset.output_shapes = property(\n          dataset_ops.get_legacy_output_shapes)\n      tf.compat.v2.data.Dataset.output_types = property(\n          dataset_ops.get_legacy_output_types)", "output": "Monkey patch tf 1.13 so tfds can use it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(argv=None):\n  \"\"\"\n  \n  \"\"\"\n  assert len(argv) >= 3\n  _name_of_script = argv[0]\n  model_filepath = argv[1]\n  adv_x_filepaths = argv[2:]\n\n  sess = tf.Session()\n  with sess.as_default():\n    model = serial.load(model_filepath)\n\n  factory = model.dataset_factory\n  factory.kwargs['train_start'] = FLAGS.train_start\n  factory.kwargs['train_end'] = FLAGS.train_end\n  factory.kwargs['test_start'] = FLAGS.test_start\n  factory.kwargs['test_end'] = FLAGS.test_end\n  dataset = factory()\n\n  adv_x_list = [np.load(filepath) for filepath in adv_x_filepaths]\n  x, y = dataset.get_set(FLAGS.which_set)\n  for adv_x in adv_x_list:\n    assert adv_x.shape == x.shape, (adv_x.shape, x.shape)\n    # Make sure these were made for the right dataset with right scaling\n    # arguments, etc.\n    assert adv_x.min() >= 0. - dataset.kwargs['center'] * dataset.max_val\n    assert adv_x.max() <= dataset.max_val\n    data_range = dataset.max_val * (1. + dataset.kwargs['center'])\n\n    if adv_x.max() - adv_x.min() <= .8 * data_range:\n      warnings.warn(\"Something is weird. Your adversarial examples use \"\n                    \"less than 80% of the data range.\"\n                    \"This might mean you generated them for a model with \"\n                    \"inputs in [0, 1] and are now using them for a model \"\n                    \"with inputs in [0, 255] or something like that. \"\n                    \"Or it could be OK if you're evaluating on a very small \"\n                    \"batch.\")\n\n  report_path = FLAGS.report_path\n  if report_path is None:\n    suffix = \"_bundled_examples_report.joblib\"\n    assert model_filepath.endswith('.joblib')\n    report_path = model_filepath[:-len('.joblib')] + suffix\n\n  goal = MaxConfidence()\n  bundle_examples_with_goal(sess, model, adv_x_list, y, goal,\n                            report_path, batch_size=FLAGS.batch_size)", "output": "Make a confidence report and save it to disk.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_element_by_css_selector(self, css_selector):\n        \"\"\"\n        \n        \"\"\"\n        return self.find_element(by=By.CSS_SELECTOR, value=css_selector)", "output": "Finds an element by css selector.\n\n        :Args:\n         - css_selector - CSS selector string, ex: 'a.nav#home'\n\n        :Returns:\n         - WebElement - the element if it was found\n\n        :Raises:\n         - NoSuchElementException - if the element wasn't found\n\n        :Usage:\n            ::\n\n                element = driver.find_element_by_css_selector('#foo')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_base_single_gpu():\n  \"\"\"\"\"\"\n  hparams = transformer_base()\n  hparams.batch_size = 1024\n  hparams.learning_rate_schedule = \"constant*linear_warmup*rsqrt_decay\"\n  hparams.learning_rate_constant = 0.1\n  hparams.learning_rate_warmup_steps = 16000\n  return hparams", "output": "HParams for transformer base model for single GPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _need_update(self, project_name, updatetime=None, md5sum=None):\n        ''''''\n        if project_name not in self.projects:\n            return True\n        elif md5sum and md5sum != self.projects[project_name]['info'].get('md5sum'):\n            return True\n        elif updatetime and updatetime > self.projects[project_name]['info'].get('updatetime', 0):\n            return True\n        elif time.time() - self.projects[project_name]['load_time'] > self.RELOAD_PROJECT_INTERVAL:\n            return True\n        return False", "output": "Check if project_name need update", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max(self, axis=None, skipna=True):\n        \"\"\"\n        \n        \"\"\"\n        nv.validate_minmax_axis(axis)\n        return nanops.nanmax(self._values, skipna=skipna)", "output": "Return the maximum value of the Index.\n\n        Parameters\n        ----------\n        axis : int, optional\n            For compatibility with NumPy. Only 0 or None are allowed.\n        skipna : bool, default True\n\n        Returns\n        -------\n        scalar\n            Maximum value.\n\n        See Also\n        --------\n        Index.min : Return the minimum value in an Index.\n        Series.max : Return the maximum value in a Series.\n        DataFrame.max : Return the maximum values in a DataFrame.\n\n        Examples\n        --------\n        >>> idx = pd.Index([3, 2, 1])\n        >>> idx.max()\n        3\n\n        >>> idx = pd.Index(['c', 'b', 'a'])\n        >>> idx.max()\n        'c'\n\n        For a MultiIndex, the maximum is determined lexicographically.\n\n        >>> idx = pd.MultiIndex.from_product([('a', 'b'), (2, 1)])\n        >>> idx.max()\n        ('b', 2)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_image(self, key):\r\n        \"\"\"\"\"\"\r\n        data = self.model.get_data()\r\n        data[key].show()", "output": "Show image (item is a PIL image)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def token(self, adata, load):\n        '''\n        \n        '''\n        try:\n            token = self.loadauth.get_tok(load['token'])\n        except Exception as exc:\n            log.error('Exception occurred when generating auth token: %s', exc)\n            yield {}\n        if not token:\n            log.warning('Authentication failure of type \"token\" occurred.')\n            yield {}\n        for sub_auth in adata:\n            for sub_adata in adata:\n                if token['eauth'] not in adata:\n                    continue\n            if not ((token['name'] in adata[token['eauth']]) |\n                    ('*' in adata[token['eauth']])):\n                continue\n            yield {'sub_auth': sub_auth, 'token': token}\n        yield {}", "output": "Determine if token auth is valid and yield the adata", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fix_missing(df, col, name, na_dict):\n    \"\"\" \n    \"\"\"\n    if is_numeric_dtype(col):\n        if pd.isnull(col).sum() or (name in na_dict):\n            df[name+'_na'] = pd.isnull(col)\n            filler = na_dict[name] if name in na_dict else col.median()\n            df[name] = col.fillna(filler)\n            na_dict[name] = filler\n    return na_dict", "output": "Fill missing data in a column of df with the median, and add a {name}_na column\n    which specifies if the data was missing.\n    Parameters:\n    -----------\n    df: The data frame that will be changed.\n    col: The column of data to fix by filling in missing data.\n    name: The name of the new filled column in df.\n    na_dict: A dictionary of values to create na's of and the value to insert. If\n        name is not a key of na_dict the median will fill any missing data. Also\n        if name is not a key of na_dict and there is no missing data in col, then\n        no {name}_na column is not created.\n    Examples:\n    ---------\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col1'], 'col1', {})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1     2    2    True\n    2     3    2   False\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col2'], 'col2', {})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]})\n    >>> df\n       col1 col2\n    0     1    5\n    1   nan    2\n    2     3    2\n    >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500})\n    >>> df\n       col1 col2 col1_na\n    0     1    5   False\n    1   500    2    True\n    2     3    2   False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_project(self, project):\n        ''''''\n        if project['name'] not in self.projects:\n            self.projects[project['name']] = Project(self, project)\n        else:\n            self.projects[project['name']].update(project)\n\n        project = self.projects[project['name']]\n\n        if project._send_on_get_info:\n            # update project runtime info from processor by sending a _on_get_info\n            # request, result is in status_page.track.save\n            project._send_on_get_info = False\n            self.on_select_task({\n                'taskid': '_on_get_info',\n                'project': project.name,\n                'url': 'data:,_on_get_info',\n                'status': self.taskdb.SUCCESS,\n                'fetch': {\n                    'save': self.get_info_attributes,\n                },\n                'process': {\n                    'callback': '_on_get_info',\n                },\n            })\n\n        # load task queue when project is running and delete task_queue when project is stoped\n        if project.active:\n            if not project.task_loaded:\n                self._load_tasks(project)\n                project.task_loaded = True\n        else:\n            if project.task_loaded:\n                project.task_queue = TaskQueue()\n                project.task_loaded = False\n\n            if project not in self._cnt['all']:\n                self._update_project_cnt(project.name)", "output": "update one project", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __ipv4_netmask(value):\n    ''''''\n    valid, errmsg = False, 'dotted quad or integer CIDR (0->32)'\n    valid, value, _ = __int(value)\n    if not (valid and 0 <= value <= 32):\n        valid = salt.utils.validate.net.netmask(value)\n    return (valid, value, errmsg)", "output": "validate an IPv4 dotted quad or integer CIDR netmask", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def top(self, N, mask=NotSpecified, groupby=NotSpecified):\n        \"\"\"\n        \n        \"\"\"\n        if N == 1:\n            # Special case: if N == 1, we can avoid doing a full sort on every\n            # group, which is a big win.\n            return self._maximum(mask=mask, groupby=groupby)\n        return self.rank(ascending=False, mask=mask, groupby=groupby) <= N", "output": "Construct a Filter matching the top N asset values of self each day.\n\n        If ``groupby`` is supplied, returns a Filter matching the top N asset\n        values for each group.\n\n        Parameters\n        ----------\n        N : int\n            Number of assets passing the returned filter each day.\n        mask : zipline.pipeline.Filter, optional\n            A Filter representing assets to consider when computing ranks.\n            If mask is supplied, top values are computed ignoring any\n            asset/date pairs for which `mask` produces a value of False.\n        groupby : zipline.pipeline.Classifier, optional\n            A classifier defining partitions over which to perform ranking.\n\n        Returns\n        -------\n        filter : zipline.pipeline.filters.Filter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlSaveFileEnc(self, filename, encoding):\n        \"\"\" \"\"\"\n        ret = libxml2mod.htmlSaveFileEnc(filename, self._o, encoding)\n        return ret", "output": "Dump an HTML document to a file using a given encoding and\n           formatting returns/spaces are added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_proplist(data):\n    '''\n    \n    '''\n    out = {}\n    for line in data.split(\"\\n\"):\n        line = re.split(r\"\\s+\", line, 1)\n        if len(line) == 2:\n            out[line[0]] = line[1]\n\n    return out", "output": "Parse properties list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _destructively_move(self, dest_doc):\n        ''' \n\n        '''\n\n        if dest_doc is self:\n            raise RuntimeError(\"Attempted to overwrite a document with itself\")\n\n        dest_doc.clear()\n        # we have to remove ALL roots before adding any\n        # to the new doc or else models referenced from multiple\n        # roots could be in both docs at once, which isn't allowed.\n        roots = []\n        self._push_all_models_freeze()\n        try:\n            while self.roots:\n                r = next(iter(self.roots))\n                self.remove_root(r)\n                roots.append(r)\n        finally:\n            self._pop_all_models_freeze()\n        for r in roots:\n            if r.document is not None:\n                raise RuntimeError(\"Somehow we didn't detach %r\" % (r))\n        if len(self._all_models) != 0:\n            raise RuntimeError(\"_all_models still had stuff in it: %r\" % (self._all_models))\n        for r in roots:\n            dest_doc.add_root(r)\n\n        dest_doc.title = self.title", "output": "Move all data in this doc to the dest_doc, leaving this doc empty.\n\n        Args:\n            dest_doc (Document) :\n                The Bokeh document to populate with data from this one\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def system_properties_absent(name, server=None):\n    '''\n    \n    '''\n    ret = {'name': '', 'result': None, 'comment': None, 'changes': {}}\n\n    try:\n        data = __salt__['glassfish.get_system_properties'](server=server)\n    except requests.ConnectionError as error:\n        if __opts__['test']:\n            ret['changes'] = {'Name': name}\n            ret['result'] = None\n            return ret\n        else:\n            ret['error'] = \"Can't connect to the server\"\n            return ret\n\n    if name in data:\n        if not __opts__['test']:\n            try:\n                __salt__['glassfish.delete_system_properties'](name, server=server)\n                ret['result'] = True\n                ret['comment'] = 'System properties deleted'\n            except CommandExecutionError as error:\n                ret['comment'] = error\n                ret['result'] = False\n        else:\n            ret['result'] = None\n            ret['comment'] = 'System properties would have been deleted'\n        ret['changes'] = {'Name': name}\n    else:\n        ret['result'] = True\n        ret['comment'] = 'System properties are already absent'\n    return ret", "output": "Ensures that the system property doesn't exists\n\n    name\n        Name of the system property", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_to_generate(self, data_dir, tmp_dir):\n    \"\"\"\"\"\"\n    self.get_or_create_vocab(data_dir, tmp_dir)\n    self.train_text_filepaths(tmp_dir)\n    self.dev_text_filepaths(tmp_dir)", "output": "Make sure that the data is prepared and the vocab is generated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _download_swagger(self, location):\n        \"\"\"\n        \n        \"\"\"\n\n        if not location:\n            return\n\n        bucket, key, version = self._parse_s3_location(location)\n        if bucket and key:\n            LOG.debug(\"Downloading Swagger document from Bucket=%s, Key=%s, Version=%s\", bucket, key, version)\n            swagger_str = self._download_from_s3(bucket, key, version)\n            return yaml_parse(swagger_str)\n\n        if not isinstance(location, string_types):\n            # This is not a string and not a S3 Location dictionary. Probably something invalid\n            LOG.debug(\"Unable to download Swagger file. Invalid location: %s\", location)\n            return\n\n        # ``location`` is a string and not a S3 path. It is probably a local path. Let's resolve relative path if any\n        filepath = location\n        if self.working_dir:\n            # Resolve relative paths, if any, with respect to working directory\n            filepath = os.path.join(self.working_dir, location)\n\n        if not os.path.exists(filepath):\n            LOG.debug(\"Unable to download Swagger file. File not found at location %s\", filepath)\n            return\n\n        LOG.debug(\"Reading Swagger document from local file at %s\", filepath)\n        with open(filepath, \"r\") as fp:\n            return yaml_parse(fp.read())", "output": "Download the file from given local or remote location and return it\n\n        Parameters\n        ----------\n        location : str or dict\n            Local path or S3 path to Swagger file to download. Consult the ``__init__.py`` documentation for specifics\n            on structure of this property.\n\n        Returns\n        -------\n        dict or None\n            Downloaded and parsed Swagger document. None, if unable to download", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pack(o, default=encode,\n         encoding='utf-8', unicode_errors='strict', use_single_float=False,\n         autoreset=1, use_bin_type=1):\n    \"\"\"\n    \n    \"\"\"\n\n    return Packer(default=default, encoding=encoding,\n                  unicode_errors=unicode_errors,\n                  use_single_float=use_single_float,\n                  autoreset=autoreset,\n                  use_bin_type=use_bin_type).pack(o)", "output": "Pack an object and return the packed bytes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_head(nf:int, nc:int, lin_ftrs:Optional[Collection[int]]=None, ps:Floats=0.5,\n                concat_pool:bool=True, bn_final:bool=False):\n    \"\"\n    lin_ftrs = [nf, 512, nc] if lin_ftrs is None else [nf] + lin_ftrs + [nc]\n    ps = listify(ps)\n    if len(ps) == 1: ps = [ps[0]/2] * (len(lin_ftrs)-2) + ps\n    actns = [nn.ReLU(inplace=True)] * (len(lin_ftrs)-2) + [None]\n    pool = AdaptiveConcatPool2d() if concat_pool else nn.AdaptiveAvgPool2d(1)\n    layers = [pool, Flatten()]\n    for ni,no,p,actn in zip(lin_ftrs[:-1], lin_ftrs[1:], ps, actns):\n        layers += bn_drop_lin(ni, no, True, p, actn)\n    if bn_final: layers.append(nn.BatchNorm1d(lin_ftrs[-1], momentum=0.01))\n    return nn.Sequential(*layers)", "output": "Model head that takes `nf` features, runs through `lin_ftrs`, and about `nc` classes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def commandline_to_list(self, cmdline_str, trigger_string):\n        '''\n        \n        '''\n        cmdline = salt.utils.args.shlex_split(cmdline_str[len(trigger_string):])\n        # Remove slack url parsing\n        #  Translate target=<http://host.domain.net|host.domain.net>\n        #  to target=host.domain.net\n        cmdlist = []\n        for cmditem in cmdline:\n            pattern = r'(?P<begin>.*)(<.*\\|)(?P<url>.*)(>)(?P<remainder>.*)'\n            mtch = re.match(pattern, cmditem)\n            if mtch:\n                origtext = mtch.group('begin') + mtch.group('url') + mtch.group('remainder')\n                cmdlist.append(origtext)\n            else:\n                cmdlist.append(cmditem)\n        return cmdlist", "output": "cmdline_str is the string of the command line\n        trigger_string is the trigger string, to be removed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_metric(self, metric: float) -> None:\n        \"\"\"\n        \n        \"\"\"\n        new_best = ((self._best_so_far is None) or\n                    (self._should_decrease and metric < self._best_so_far) or\n                    (not self._should_decrease and metric > self._best_so_far))\n\n        if new_best:\n            self.best_epoch = self._epoch_number\n            self._is_best_so_far = True\n            self._best_so_far = metric\n            self._epochs_with_no_improvement = 0\n        else:\n            self._is_best_so_far = False\n            self._epochs_with_no_improvement += 1\n        self._epoch_number += 1", "output": "Record a new value of the metric and update the various things that depend on it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(cls, f):\n    \"\"\"\"\"\"\n    header = WETHeader.read(f)\n    if header is None:\n      # EOF\n      return None\n    content = f.read(header.length)\n\n    # Consume empty separators\n    f.readline()\n    f.readline()\n\n    return cls(header.url, content)", "output": "Read WETRecord from file. Records end with 2 blank lines.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _time_from_iso8601_time_naive(value):\n    \"\"\"\n    \"\"\"\n    if len(value) == 8:  # HH:MM:SS\n        fmt = _TIMEONLY_NO_FRACTION\n    elif len(value) == 15:  # HH:MM:SS.micros\n        fmt = _TIMEONLY_W_MICROS\n    else:\n        raise ValueError(\"Unknown time format: {}\".format(value))\n    return datetime.datetime.strptime(value, fmt).time()", "output": "Convert a zoneless ISO8601 time string to naive datetime time\n\n    :type value: str\n    :param value: The time string to convert\n\n    :rtype: :class:`datetime.time`\n    :returns: A datetime time object created from the string\n    :raises ValueError: if the value does not match a known format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lookup_host(name, rdtype, timeout=None, server=None):\n    '''\n    \n    '''\n    cmd = 'host -t {0} '.format(rdtype)\n\n    if timeout:\n        cmd += '-W {0} '.format(int(timeout))\n    cmd += name\n    if server is not None:\n        cmd += ' {0}'.format(server)\n\n    cmd = __salt__['cmd.run_all'](cmd, python_shell=False, output_loglevel='quiet')\n\n    if 'invalid type' in cmd['stderr']:\n        raise ValueError('Invalid DNS type {}'.format(rdtype))\n    elif cmd['retcode'] != 0:\n        log.warning('host returned (%s): %s', cmd['retcode'], cmd['stderr'])\n        return False\n    elif 'has no' in cmd['stdout']:\n        return []\n\n    res = []\n    _stdout = cmd['stdout'] if server is None else cmd['stdout'].split('\\n\\n')[-1]\n    for line in _stdout.splitlines():\n        if rdtype != 'CNAME' and 'is an alias' in line:\n            continue\n        line = line.split(' ', 3)[-1]\n        for prefix in ('record', 'address', 'handled by', 'alias for'):\n            if line.startswith(prefix):\n                line = line[len(prefix) + 1:]\n                break\n        res.append(_data_clean(line))\n\n    return res", "output": "Use host to lookup addresses\n    :param name: Name of record to search\n    :param server: Server to query\n    :param rdtype: DNS record type\n    :param timeout: server response wait\n    :return: [] of records or False if error", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_elements_by_link_text(self, text):\n        \"\"\"\n        \n        \"\"\"\n        return self.find_elements(by=By.LINK_TEXT, value=text)", "output": "Finds elements by link text.\n\n        :Args:\n         - link_text: The text of the elements to be found.\n\n        :Returns:\n         - list of webelement - a list with elements if any was found.  an\n           empty list if not\n\n        :Usage:\n            ::\n\n                elements = driver.find_elements_by_link_text('Sign In')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def docCopyNodeList(self, doc):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        ret = libxml2mod.xmlDocCopyNodeList(doc__o, self._o)\n        if ret is None:raise treeError('xmlDocCopyNodeList() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Do a recursive copy of the node list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_bash_complete(cli, prog_name):\n    \"\"\"\n    \"\"\"\n    comp_words = os.environ['COMP_WORDS']\n    try:\n        cwords = shlex.split(comp_words)\n        quoted = False\n    except ValueError:  # No closing quotation\n        cwords = split_args(comp_words)\n        quoted = True\n    cword = int(os.environ['COMP_CWORD'])\n    args = cwords[1:cword]\n    try:\n        incomplete = cwords[cword]\n    except IndexError:\n        incomplete = ''\n    choices = get_choices(cli, prog_name, args, incomplete)\n\n    if quoted:\n        echo('\\t'.join(opt for opt, _ in choices), nl=False)\n    else:\n        echo('\\t'.join(re.sub(r\"\"\"([\\s\\\\\"'()])\"\"\", r'\\\\\\1', opt) for opt, _ in choices), nl=False)\n\n    return True", "output": "Do the completion for bash\n\n    Parameters\n    ----------\n    cli : click.Command\n        The main click Command of the program\n    prog_name : str\n        The program name on the command line\n\n    Returns\n    -------\n    bool\n        True if the completion was successful, False otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fastrcnn_outputs(feature, num_classes, class_agnostic_regression=False):\n    \"\"\"\n    \n    \"\"\"\n    classification = FullyConnected(\n        'class', feature, num_classes,\n        kernel_initializer=tf.random_normal_initializer(stddev=0.01))\n    num_classes_for_box = 1 if class_agnostic_regression else num_classes\n    box_regression = FullyConnected(\n        'box', feature, num_classes_for_box * 4,\n        kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n    box_regression = tf.reshape(box_regression, (-1, num_classes_for_box, 4), name='output_box')\n    return classification, box_regression", "output": "Args:\n        feature (any shape):\n        num_classes(int): num_category + 1\n        class_agnostic_regression (bool): if True, regression to N x 1 x 4\n\n    Returns:\n        cls_logits: N x num_class classification logits\n        reg_logits: N x num_classx4 or Nx2x4 if class agnostic", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_recv(self, callback):\n        '''\n        \n        '''\n        if callback is None:\n            self._on_recv = callback\n        else:\n            def wrap_recv(header, body):\n                callback(body)\n            self._on_recv = wrap_recv", "output": "Register a callback for received messages (that we didn't initiate)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_targets_bottom(x, model_hparams, vocab_size):\n  \"\"\"\"\"\"\n  pixel_embedding_size = 64\n  inputs = x\n  with tf.variable_scope(\"image_modality\"):\n    if not tf.executing_eagerly():\n      tf.summary.image(\n          \"targets_bottom\",\n          common_layers.tpu_safe_image_summary(inputs),\n          max_outputs=1)\n    inputs_shape = common_layers.shape_list(inputs)\n    if len(inputs_shape) != 4:\n      raise ValueError(\"Assuming images given as int tensors in the format \"\n                       \"[batch, height, width, channels] (256 values).\")\n    # We embed each of 256=vocab_size possible pixel values.\n    embedding_var = tf.get_variable(\n        \"pixel_embedding\",\n        [vocab_size, pixel_embedding_size])\n    hot_inputs = tf.one_hot(tf.to_int32(inputs), vocab_size)\n    hot_inputs = tf.reshape(hot_inputs, [-1, vocab_size])\n    embedded = tf.matmul(hot_inputs, embedding_var)\n    # Let's now merge all channels that were embedded into a single vector.\n    merged_size = pixel_embedding_size * inputs_shape[3]\n    embedded = tf.reshape(embedded, inputs_shape[:3] + [merged_size])\n    merged = tf.layers.dense(\n        embedded,\n        model_hparams.hidden_size,\n        name=\"merge_pixel_embedded_channels\")\n    return merged", "output": "Bottom transformation for target images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(finished=True, profile_process='worker'):\n    \"\"\"\n    \"\"\"\n    fin = 1 if finished is True else 0\n    profile_process2int = {'worker': 0, 'server': 1}\n    check_call(_LIB.MXDumpProcessProfile(fin,\n                                         profile_process2int[profile_process],\n                                         profiler_kvstore_handle))", "output": "Dump profile and stop profiler. Use this to save profile\n    in advance in case your program cannot exit normally.\n\n    Parameters\n    ----------\n    finished : boolean\n        Indicates whether to stop statistic output (dumping) after this dump.\n        Default is True\n    profile_process : string\n        whether to profile kvstore `server` or `worker`.\n        server can only be profiled when kvstore is of type dist.\n        if this is not passed, defaults to `worker`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_work(self):\n    \"\"\"\"\"\"\n    if os.path.exists(LOCAL_EVAL_ROOT_DIR):\n      sudo_remove_dirtree(LOCAL_EVAL_ROOT_DIR)\n    self.run_attacks()\n    self.run_defenses()", "output": "Run attacks and defenses", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lowdata_fmt():\n    '''\n    \n    '''\n\n    if cherrypy.request.method.upper() != 'POST':\n        return\n\n    data = cherrypy.request.unserialized_data\n\n    # if the data was sent as urlencoded, we need to make it a list.\n    # this is a very forgiving implementation as different clients set different\n    # headers for form encoded data (including charset or something similar)\n    if data and isinstance(data, collections.Mapping):\n        # Make the 'arg' param a list if not already\n        if 'arg' in data and not isinstance(data['arg'], list):\n            data['arg'] = [data['arg']]\n\n        # Finally, make a Low State and put it in request\n        cherrypy.request.lowstate = [data]\n    else:\n        cherrypy.serving.request.lowstate = data", "output": "Validate and format lowdata from incoming unserialized request data\n\n    This tool requires that the hypermedia_in tool has already been run.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keep_mask(nkeep, X_train, y_train, X_test, y_test, attr_test, model_generator, metric, trained_model, random_state):\n    \"\"\" \n    \"\"\"\n\n    X_train, X_test = to_array(X_train, X_test)\n\n    # how many features to mask\n    assert X_train.shape[1] == X_test.shape[1]\n\n    # keep nkeep top features for each test explanation\n    X_test_tmp = X_test.copy()\n    yp_masked_test = np.zeros(y_test.shape)\n    tie_breaking_noise = const_rand(X_train.shape[1], random_state) * 1e-6\n    mean_vals = X_train.mean(0)\n    for i in range(len(y_test)):\n        if nkeep[i] < X_test.shape[1]:\n            ordering = np.argsort(-attr_test[i,:] + tie_breaking_noise)\n            X_test_tmp[i,ordering[nkeep[i]:]] = mean_vals[ordering[nkeep[i]:]]\n\n    yp_masked_test = trained_model.predict(X_test_tmp)\n\n    return metric(y_test, yp_masked_test)", "output": "The model is revaluated for each test sample with the non-important features set to their mean.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def intrinsics_multi_constructor(loader, tag_prefix, node):\n    \"\"\"\n    \n    \"\"\"\n\n    # Get the actual tag name excluding the first exclamation\n    tag = node.tag[1:]\n\n    # Some intrinsic functions doesn't support prefix \"Fn::\"\n    prefix = \"Fn::\"\n    if tag in [\"Ref\", \"Condition\"]:\n        prefix = \"\"\n\n    cfntag = prefix + tag\n\n    if tag == \"GetAtt\" and isinstance(node.value, six.string_types):\n        # ShortHand notation for !GetAtt accepts Resource.Attribute format\n        # while the standard notation is to use an array\n        # [Resource, Attribute]. Convert shorthand to standard format\n        value = node.value.split(\".\", 1)\n\n    elif isinstance(node, ScalarNode):\n        # Value of this node is scalar\n        value = loader.construct_scalar(node)\n\n    elif isinstance(node, SequenceNode):\n        # Value of this node is an array (Ex: [1,2])\n        value = loader.construct_sequence(node)\n\n    else:\n        # Value of this node is an mapping (ex: {foo: bar})\n        value = loader.construct_mapping(node)\n\n    return {cfntag: value}", "output": "YAML constructor to parse CloudFormation intrinsics.\n    This will return a dictionary with key being the instrinsic name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def os_script(os_, vm_=None, opts=None, minion=''):\n    '''\n    \n    '''\n    if minion:\n        minion = salt_config_to_yaml(minion)\n\n    if os.path.isabs(os_):\n        # The user provided an absolute path to the deploy script, let's use it\n        return __render_script(os_, vm_, opts, minion)\n\n    if os.path.isabs('{0}.sh'.format(os_)):\n        # The user provided an absolute path to the deploy script, although no\n        # extension was provided. Let's use it anyway.\n        return __render_script('{0}.sh'.format(os_), vm_, opts, minion)\n\n    for search_path in opts['deploy_scripts_search_path']:\n        if os.path.isfile(os.path.join(search_path, os_)):\n            return __render_script(\n                os.path.join(search_path, os_), vm_, opts, minion\n            )\n\n        if os.path.isfile(os.path.join(search_path, '{0}.sh'.format(os_))):\n            return __render_script(\n                os.path.join(search_path, '{0}.sh'.format(os_)),\n                vm_, opts, minion\n            )\n    # No deploy script was found, return an empty string\n    return ''", "output": "Return the script as a string for the specific os", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_from_sliced_object(data):\n    \"\"\"\"\"\"\n    if data.base is not None and isinstance(data, np.ndarray) and isinstance(data.base, np.ndarray):\n        if not data.flags.c_contiguous:\n            warnings.warn(\"Usage of np.ndarray subset (sliced data) is not recommended \"\n                          \"due to it will double the peak memory cost in LightGBM.\")\n            return np.copy(data)\n    return data", "output": "Fix the memory of multi-dimensional sliced object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def browse_history(self, backward):\r\n        \"\"\"\"\"\"\r\n        if self.is_cursor_before('eol') and self.hist_wholeline:\r\n            self.hist_wholeline = False\r\n        tocursor = self.get_current_line_to_cursor()\r\n        text, self.histidx = self.find_in_history(tocursor, self.histidx,\r\n                                                  backward)\r\n        if text is not None:\r\n            if self.hist_wholeline:\r\n                self.clear_line()\r\n                self.insert_text(text)\r\n            else:\r\n                cursor_position = self.get_position('cursor')\r\n                # Removing text from cursor to the end of the line\r\n                self.remove_text('cursor', 'eol')\r\n                # Inserting history text\r\n                self.insert_text(text)\r\n                self.set_cursor_position(cursor_position)", "output": "Browse history", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_time(self):\n        '''\n        \n        '''\n        time1970 = self.__mod_time1970  # time of last resort\n        try:\n            # pylint: disable=no-member\n            date_string, item_type = \\\n                win32api.RegQueryValueEx(self.__reg_uninstall_handle, 'InstallDate')\n        except pywintypes.error as exc:  # pylint: disable=no-member\n            if exc.winerror == winerror.ERROR_FILE_NOT_FOUND:\n                return time1970  # i.e. use time of last resort\n            else:\n                raise\n\n        if item_type == win32con.REG_SZ:\n            try:\n                date_object = datetime.datetime.strptime(date_string, \"%Y%m%d\")\n                time1970 = time.mktime(date_object.timetuple())\n            except ValueError:  # date format is not correct\n                pass\n\n        return time1970", "output": "Return the install time, or provide an estimate of install time.\n\n        Installers or even self upgrading software must/should update the date\n        held within InstallDate field when they change versions. Some installers\n        do not set ``InstallDate`` at all so we use the last modified time on the\n        registry key.\n\n        Returns:\n            int: Seconds since 1970 UTC.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mask_missing(arr, values_to_mask):\n    \"\"\"\n    \n    \"\"\"\n    dtype, values_to_mask = infer_dtype_from_array(values_to_mask)\n\n    try:\n        values_to_mask = np.array(values_to_mask, dtype=dtype)\n\n    except Exception:\n        values_to_mask = np.array(values_to_mask, dtype=object)\n\n    na_mask = isna(values_to_mask)\n    nonna = values_to_mask[~na_mask]\n\n    mask = None\n    for x in nonna:\n        if mask is None:\n\n            # numpy elementwise comparison warning\n            if is_numeric_v_string_like(arr, x):\n                mask = False\n            else:\n                mask = arr == x\n\n            # if x is a string and arr is not, then we get False and we must\n            # expand the mask to size arr.shape\n            if is_scalar(mask):\n                mask = np.zeros(arr.shape, dtype=bool)\n        else:\n\n            # numpy elementwise comparison warning\n            if is_numeric_v_string_like(arr, x):\n                mask |= False\n            else:\n                mask |= arr == x\n\n    if na_mask.any():\n        if mask is None:\n            mask = isna(arr)\n        else:\n            mask |= isna(arr)\n\n    # GH 21977\n    if mask is None:\n        mask = np.zeros(arr.shape, dtype=bool)\n\n    return mask", "output": "Return a masking array of same size/shape as arr\n    with entries equaling any member of values_to_mask set to True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_column(self, column_name):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(column_name, str):\n            raise TypeError(\"Invalid column_nametype: must be str\")\n        with cython_context():\n            return SArray(data=[], _proxy=self.__proxy__.select_column(column_name))", "output": "Get a reference to the :class:`~turicreate.SArray` that corresponds with\n        the given column_name. Throws an exception if the column_name is\n        something other than a string or if the column name is not found.\n\n        Parameters\n        ----------\n        column_name: str\n            The column name.\n\n        Returns\n        -------\n        out : SArray\n            The SArray that is referred by ``column_name``.\n\n        See Also\n        --------\n        select_columns\n\n        Examples\n        --------\n        >>> sf = turicreate.SFrame({'user_id': [1,2,3],\n        ...                       'user_name': ['alice', 'bob', 'charlie']})\n        >>> # This line is equivalent to `sa = sf['user_name']`\n        >>> sa = sf.select_column('user_name')\n        >>> sa\n        dtype: str\n        Rows: 3\n        ['alice', 'bob', 'charlie']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xmoe_2d():\n  \"\"\"\"\"\"\n  hparams = xmoe_top_2()\n  hparams.decoder_layers = [\"att\", \"hmoe\"] * 4\n  hparams.mesh_shape = \"b0:2;b1:4\"\n  hparams.outer_batch_size = 4\n  hparams.layout = \"outer_batch:b0;inner_batch:b1,expert_x:b1,expert_y:b0\"\n  hparams.moe_num_experts = [4, 4]\n  return hparams", "output": "Two-dimensional hierarchical mixture of 16 experts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toogle_breakpoint(self, line_number=None, condition=None,\n                          edit_condition=False):\n        \"\"\"\"\"\"\n        if not self.editor.is_python_like():\n            return\n        if line_number is None:\n            block = self.editor.textCursor().block()\n        else:\n            block = self.editor.document().findBlockByNumber(line_number-1)\n        data = block.userData()\n        if not data:\n            data = BlockUserData(self.editor)\n            data.breakpoint = True\n        elif not edit_condition:\n            data.breakpoint = not data.breakpoint\n            data.breakpoint_condition = None\n        if condition is not None:\n            data.breakpoint_condition = condition\n        if edit_condition:\n            condition = data.breakpoint_condition\n            condition, valid = QInputDialog.getText(self.editor,\n                                                    _('Breakpoint'),\n                                                    _(\"Condition:\"),\n                                                    QLineEdit.Normal,\n                                                    condition)\n            if not valid:\n                return\n            data.breakpoint = True\n            data.breakpoint_condition = str(condition) if condition else None\n        if data.breakpoint:\n            text = to_text_string(block.text()).strip()\n            if len(text) == 0 or text.startswith(('#', '\"', \"'\")):\n                data.breakpoint = False\n        block.setUserData(data)\n        self.editor.sig_flags_changed.emit()\n        self.editor.sig_breakpoints_changed.emit()", "output": "Add/remove breakpoint.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_image_transformer_base_cifar():\n  \"\"\"\"\"\"\n  hparams = mtf_image_transformer_base()\n  hparams.mesh_shape = \"batch:8\"\n  hparams.layout = \"batch:batch\"\n  hparams.learning_rate_decay_steps = 13600  # one epoch\n  hparams.batch_size = 32\n  hparams.num_heads = 4\n  hparams.num_decoder_layers = 12\n  hparams.block_length = 256\n  hparams.hidden_size = 512\n  hparams.d_ff = 2048\n  hparams.learning_rate = 0.5\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.layer_prepostprocess_dropout = 0.3\n  hparams.unconditional = True\n  return hparams", "output": "Data parallel CIFAR parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setting(self, setting_name, default=None):  # type: (str) -> Any\n        \"\"\"\n        \n        \"\"\"\n        keys = setting_name.split(\".\")\n\n        config = self._content\n        for key in keys:\n            if key not in config:\n                return default\n\n            config = config[key]\n\n        return config", "output": "Retrieve a setting value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_mount_cache(real_name,\n                      device,\n                      mkmnt,\n                      fstype,\n                      mount_opts):\n    '''\n    \n    '''\n    cache = salt.utils.mount.read_cache(__opts__)\n\n    if not cache:\n        cache = {}\n        cache['mounts'] = {}\n    else:\n        if 'mounts' not in cache:\n            cache['mounts'] = {}\n\n    cache['mounts'][real_name] = {'device': device,\n                                  'fstype': fstype,\n                                  'mkmnt': mkmnt,\n                                  'opts': mount_opts}\n\n    cache_write = salt.utils.mount.write_cache(cache, __opts__)\n    if cache_write:\n        return True\n    else:\n        raise CommandExecutionError('Unable to write mount cache.')", "output": ".. versionadded:: 2018.3.0\n\n    Provide information if the path is mounted\n\n    :param real_name:     The real name of the mount point where the device is mounted.\n    :param device:        The device that is being mounted.\n    :param mkmnt:         Whether or not the mount point should be created.\n    :param fstype:        The file system that is used.\n    :param mount_opts:    Additional options used when mounting the device.\n    :return:              Boolean if message was sent successfully.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mount.write_mount_cache /mnt/share /dev/sda1 False ext4 defaults,nosuid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_list(self, lookup='all'):\n        '''\n        \n        '''\n        data = {}\n\n        lookups = self.lookup_providers(lookup)\n        if not lookups:\n            return data\n\n        for alias, driver in lookups:\n            fun = '{0}.avail_images'.format(driver)\n            if fun not in self.clouds:\n                # The capability to gather images is not supported by this\n                # cloud module\n                log.debug(\n                    'The \\'%s\\' cloud driver defined under \\'%s\\' provider '\n                    'alias is unable to get the images information',\n                    driver, alias\n                )\n                continue\n\n            if alias not in data:\n                data[alias] = {}\n\n            try:\n                with salt.utils.context.func_globals_inject(\n                    self.clouds[fun],\n                    __active_provider_name__=':'.join([alias, driver])\n                ):\n                    data[alias][driver] = self.clouds[fun]()\n            except Exception as err:\n                log.error(\n                    'Failed to get the output of \\'%s()\\': %s',\n                    fun, err, exc_info_on_loglevel=logging.DEBUG\n                )\n        return data", "output": "Return a mapping of all image data for available providers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_api_integration_response(restApiId, resourcePath, httpMethod, statusCode,\n                                    region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        resource = describe_api_resource(restApiId, resourcePath, region=region,\n                                         key=key, keyid=keyid, profile=profile).get('resource')\n        if resource:\n            conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n            conn.delete_integration_response(restApiId=restApiId, resourceId=resource['id'],\n                                             httpMethod=httpMethod, statusCode=statusCode)\n            return {'deleted': True}\n        return {'deleted': False, 'error': 'no such resource'}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Deletes an integration response for a given method in a given API\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.delete_api_integration_response restApiId resourcePath httpMethod statusCode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def provides(self):\n        \"\"\"\n        \n        \"\"\"\n        plist = self.metadata.provides\n        s = '%s (%s)' % (self.name, self.version)\n        if s not in plist:\n            plist.append(s)\n        return plist", "output": "A set of distribution names and versions provided by this distribution.\n        :return: A set of \"name (version)\" strings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_config(lines, **kwargs):\n    '''\n    \n    '''\n    if not isinstance(lines, list):\n        lines = [lines]\n    for i, _ in enumerate(lines):\n        lines[i] = 'no ' + lines[i]\n    result = None\n    try:\n        result = config(lines, **kwargs)\n    except CommandExecutionError as e:\n        # Some commands will generate error code 400 if they do not exist\n        # and we try to remove them.  These can be ignored.\n        if ast.literal_eval(e.message)['code'] != '400':\n            raise\n    return result", "output": "Delete one or more config lines to the switch running config.\n\n    lines\n        Configuration lines to remove.\n\n    no_save_config\n        If True, don't save configuration commands to startup configuration.\n        If False, save configuration to startup configuration.\n        Default: False\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd delete_config 'snmp-server community TESTSTRINGHERE group network-operator'\n\n    .. note::\n        For more than one config deleted per command, lines should be a list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_roles_exists(name, roles, database, user=None, password=None, host=None,\n                      port=None, authdb=None):\n    '''\n    \n    '''\n    try:\n        roles = _to_dict(roles)\n    except Exception:\n        return 'Roles provided in wrong format'\n\n    users = user_list(user, password, host, port, database, authdb)\n\n    if isinstance(users, six.string_types):\n        return 'Failed to connect to mongo database'\n\n    for user in users:\n        if name == dict(user).get('user'):\n            for role in roles:\n                # if the role was provided in the shortened form, we convert it to a long form\n                if not isinstance(role, dict):\n                    role = {'role': role, 'db': database}\n                if role not in dict(user).get('roles', []):\n                    return False\n            return True\n\n    return False", "output": "Checks if a user of a MongoDB database has specified roles\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.user_roles_exists johndoe '[\"readWrite\"]' dbname admin adminpwd localhost 27017\n\n    .. code-block:: bash\n\n        salt '*' mongodb.user_roles_exists johndoe '[{\"role\": \"readWrite\", \"db\": \"dbname\" }, {\"role\": \"read\", \"db\": \"otherdb\"}]' dbname admin adminpwd localhost 27017", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalize_example(self, example, hparams):\n    \"\"\"\"\"\"\n\n    length = self.max_length(hparams)\n    def _to_constant_shape(tensor):\n      tensor = tensor[:length]\n      tensor = tf.pad(tensor, [(0, length - tf.shape(tensor)[0])])\n      return tf.reshape(tensor, [length])\n\n    if self.has_inputs:\n      example['inputs'] = _to_constant_shape(example['inputs'])\n      example['targets'] = _to_constant_shape(example['targets'])\n    elif 'inputs' in example:\n      if self.packed_length:\n        raise ValueError('cannot concatenate packed examples on the fly.')\n      inputs = example.pop('inputs')[:-1]  # Remove EOS token.\n      targets = tf.concat([inputs, example['targets']], 0)\n      example['targets'] = _to_constant_shape(targets)\n    else:\n      example['targets'] = _to_constant_shape(example['targets'])\n    if self.packed_length:\n      if self.has_inputs:\n        if 'inputs_segmentation' in example:\n          example['inputs_segmentation'] = _to_constant_shape(\n              example['inputs_segmentation'])\n          example['inputs_position'] = _to_constant_shape(\n              example['inputs_position'])\n        else:\n          example['inputs_segmentation'] = tf.to_int64(\n              tf.not_equal(example['inputs'], 0))\n          example['inputs_position'] = (\n              example['inputs_segmentation'] * tf.range(length, dtype=tf.int64))\n      if 'targets_segmentation' in example:\n        example['targets_segmentation'] = _to_constant_shape(\n            example['targets_segmentation'])\n        example['targets_position'] = _to_constant_shape(\n            example['targets_position'])\n      else:\n        example['targets_segmentation'] = tf.to_int64(\n            tf.not_equal(example['targets'], 0))\n        example['targets_position'] = (\n            example['targets_segmentation'] * tf.range(length, dtype=tf.int64))\n    return example", "output": "Assumes that example contains both inputs and targets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def move(zone, zonepath):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    ## verify zone\n    res = __salt__['cmd.run_all']('zoneadm {zone} move {path}'.format(\n        zone='-u {0}'.format(zone) if _is_uuid(zone) else '-z {0}'.format(zone),\n        path=zonepath,\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    ret['message'] = ret['message'].replace('zoneadm: ', '')\n    if ret['message'] == '':\n        del ret['message']\n\n    return ret", "output": "Move zone to new zonepath.\n\n    zone : string\n        name or uuid of the zone\n    zonepath : string\n        new zonepath\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zoneadm.move meave /sweetwater/meave", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_string(cls, table_id, default_project=None):\n        \"\"\"\n        \"\"\"\n        from google.cloud.bigquery.dataset import DatasetReference\n\n        (\n            output_project_id,\n            output_dataset_id,\n            output_table_id,\n        ) = _helpers._parse_3_part_id(\n            table_id, default_project=default_project, property_name=\"table_id\"\n        )\n\n        return cls(\n            DatasetReference(output_project_id, output_dataset_id), output_table_id\n        )", "output": "Construct a table reference from table ID string.\n\n        Args:\n            table_id (str):\n                A table ID in standard SQL format. If ``default_project``\n                is not specified, this must included a project ID, dataset\n                ID, and table ID, each separated by ``.``.\n            default_project (str):\n                Optional. The project ID to use when ``table_id`` does not\n                include a project ID.\n\n        Returns:\n            TableReference: Table reference parsed from ``table_id``.\n\n        Examples:\n            >>> TableReference.from_string('my-project.mydataset.mytable')\n            TableRef...(DatasetRef...('my-project', 'mydataset'), 'mytable')\n\n        Raises:\n            ValueError:\n                If ``table_id`` is not a fully-qualified table ID in\n                standard SQL format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_subword_units(token, gram):\n    \"\"\"\n    \"\"\"\n    if token == '</s>':  # special token for padding purpose.\n        return [token]\n    t = '#' + token + '#'\n    return [t[i:i + gram] for i in range(0, len(t) - gram + 1)]", "output": "Return subword-units presentation, given a word/token.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_data_with_word2vec(word2vec_list):\n    \"\"\"\n    \"\"\"\n    # Load and preprocess data\n    sentences, labels = load_data_and_labels()\n    sentences_padded = pad_sentences(sentences)\n    # vocabulary, vocabulary_inv = build_vocab(sentences_padded)\n    return build_input_data_with_word2vec(sentences_padded, labels, word2vec_list)", "output": "Loads and preprocessed data for the MR dataset.\n    Returns input vectors, labels, vocabulary, and inverse vocabulary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _options_protobuf(self, retry_id):\n        \"\"\"\n        \"\"\"\n        if retry_id is not None:\n            if self._read_only:\n                raise ValueError(_CANT_RETRY_READ_ONLY)\n\n            return types.TransactionOptions(\n                read_write=types.TransactionOptions.ReadWrite(\n                    retry_transaction=retry_id\n                )\n            )\n        elif self._read_only:\n            return types.TransactionOptions(\n                read_only=types.TransactionOptions.ReadOnly()\n            )\n        else:\n            return None", "output": "Convert the current object to protobuf.\n\n        The ``retry_id`` value is used when retrying a transaction that\n        failed (e.g. due to contention). It is intended to be the \"first\"\n        transaction that failed (i.e. if multiple retries are needed).\n\n        Args:\n            retry_id (Union[bytes, NoneType]): Transaction ID of a transaction\n                to be retried.\n\n        Returns:\n            Optional[google.cloud.firestore_v1beta1.types.TransactionOptions]:\n            The protobuf ``TransactionOptions`` if ``read_only==True`` or if\n            there is a transaction ID to be retried, else :data:`None`.\n\n        Raises:\n            ValueError: If ``retry_id`` is not :data:`None` but the\n                transaction is read-only.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_installed_packages(site_packages, site_packages_64):\n        \"\"\"\n        \n        \"\"\"\n        import pkg_resources\n\n        package_to_keep = []\n        if os.path.isdir(site_packages):\n            package_to_keep += os.listdir(site_packages)\n        if os.path.isdir(site_packages_64):\n            package_to_keep += os.listdir(site_packages_64)\n\n        package_to_keep = [x.lower() for x in package_to_keep]\n\n        installed_packages = {package.project_name.lower(): package.version for package in\n                              pkg_resources.WorkingSet()\n                              if package.project_name.lower() in package_to_keep\n                              or package.location.lower() in [site_packages.lower(), site_packages_64.lower()]}\n\n        return installed_packages", "output": "Returns a dict of installed packages that Zappa cares about.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def quit(self):\n        ''''''\n        self._running = False\n        self._quit = True\n        self.ioloop.add_callback(self.ioloop.stop)\n        if hasattr(self, 'xmlrpc_server'):\n            self.xmlrpc_ioloop.add_callback(self.xmlrpc_server.stop)\n            self.xmlrpc_ioloop.add_callback(self.xmlrpc_ioloop.stop)", "output": "Quit fetcher", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_field(self, field_name, data):\n        \"\"\"\n        \"\"\"\n        if self.handle is None:\n            raise Exception(\"Cannot set %s before construct dataset\" % field_name)\n        if data is None:\n            # set to None\n            _safe_call(_LIB.LGBM_DatasetSetField(\n                self.handle,\n                c_str(field_name),\n                None,\n                ctypes.c_int(0),\n                ctypes.c_int(FIELD_TYPE_MAPPER[field_name])))\n            return self\n        dtype = np.float32\n        if field_name == 'group':\n            dtype = np.int32\n        elif field_name == 'init_score':\n            dtype = np.float64\n        data = list_to_1d_numpy(data, dtype, name=field_name)\n        if data.dtype == np.float32 or data.dtype == np.float64:\n            ptr_data, type_data, _ = c_float_array(data)\n        elif data.dtype == np.int32:\n            ptr_data, type_data, _ = c_int_array(data)\n        else:\n            raise TypeError(\"Excepted np.float32/64 or np.int32, meet type({})\".format(data.dtype))\n        if type_data != FIELD_TYPE_MAPPER[field_name]:\n            raise TypeError(\"Input type error for set_field\")\n        _safe_call(_LIB.LGBM_DatasetSetField(\n            self.handle,\n            c_str(field_name),\n            ptr_data,\n            ctypes.c_int(len(data)),\n            ctypes.c_int(type_data)))\n        return self", "output": "Set property into the Dataset.\n\n        Parameters\n        ----------\n        field_name : string\n            The field name of the information.\n        data : list, numpy 1-D array, pandas Series or None\n            The array of data to be set.\n\n        Returns\n        -------\n        self : Dataset\n            Dataset with set property.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self, filename):\n        \"\"\"\"\"\"\n        client = boto3.client(\"s3\")\n        bucket, path = self.bucket_and_path(filename)\n        r = client.list_objects(Bucket=bucket, Prefix=path, Delimiter=\"/\")\n        if r.get(\"Contents\") or r.get(\"CommonPrefixes\"):\n            return True\n        return False", "output": "Determines whether a path exists or not.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fingerprint_path(cls, user, fingerprint):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"users/{user}/sshPublicKeys/{fingerprint}\",\n            user=user,\n            fingerprint=fingerprint,\n        )", "output": "Return a fully-qualified fingerprint string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_search(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_openstack_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.search_users(**kwargs)", "output": "List users\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.user_list\n        salt '*' keystoneng.user_list domain_id=b62e76fbeeff4e8fb77073f591cf211e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def close(self, code: int=1000, message: bytes=b'') -> None:\n        \"\"\"\"\"\"\n        if isinstance(message, str):\n            message = message.encode('utf-8')\n        try:\n            await self._send_frame(\n                PACK_CLOSE_CODE(code) + message, opcode=WSMsgType.CLOSE)\n        finally:\n            self._closing = True", "output": "Close the websocket, sending the specified code and message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _simulate_installation_of(to_install, package_set):\n    # type: (List[InstallRequirement], PackageSet) -> Set[str]\n    \"\"\"\n    \"\"\"\n\n    # Keep track of packages that were installed\n    installed = set()\n\n    # Modify it as installing requirement_set would (assuming no errors)\n    for inst_req in to_install:\n        dist = make_abstract_dist(inst_req).dist()\n        name = canonicalize_name(dist.key)\n        package_set[name] = PackageDetails(dist.version, dist.requires())\n\n        installed.add(name)\n\n    return installed", "output": "Computes the version of packages after installing to_install.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_user_yes_no(question, default_value):\n    \"\"\"\n    \"\"\"\n    # Please see http://click.pocoo.org/4/api/#click.prompt\n    return click.prompt(\n        question,\n        default=default_value,\n        type=click.BOOL\n    )", "output": "Prompt the user to reply with 'yes' or 'no' (or equivalent values).\n\n    Note:\n      Possible choices are 'true', '1', 'yes', 'y' or 'false', '0', 'no', 'n'\n\n    :param str question: Question to the user\n    :param default_value: Value that will be returned if no input happens", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def selected(self):\r\n        \"\"\"\"\"\"\r\n        self.selected_text = self.currentText()\r\n        self.valid.emit(True, True)\r\n        self.open_dir.emit(self.selected_text)", "output": "Action to be executed when a valid item has been selected", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_todo_results(self, filename, todo_results):\r\n        \"\"\"\"\"\"\r\n        index = self.has_filename(filename)\r\n        if index is None:\r\n            return\r\n        self.data[index].set_todo_results(todo_results)", "output": "Synchronize todo results between editorstacks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_scanner (type, scanner):\n    \"\"\" \n    \"\"\"\n    if __debug__:\n        from .scanner import Scanner\n        assert isinstance(type, basestring)\n        assert issubclass(scanner, Scanner)\n    validate (type)\n    __types [type]['scanner'] = scanner", "output": "Sets a scanner class that will be used for this 'type'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, value):\n    \"\"\"\"\"\"\n    if self._disposed:\n      raise ValueError(\n          'Cannot add value: this _WatchStore instance is already disposed')\n    self._data.append(value)\n    if hasattr(value, 'nbytes'):\n      self._in_mem_bytes += value.nbytes\n      self._ensure_bytes_limits()", "output": "Add a tensor the watch store.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock(self):\n        \"\"\"\"\"\"\n        try:\n            self._do_lock()\n            return\n        except LockError:\n            time.sleep(self.TIMEOUT)\n\n        self._do_lock()", "output": "Acquire lock for dvc repo.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validateDtd(self, ctxt, dtd):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        if dtd is None: dtd__o = None\n        else: dtd__o = dtd._o\n        ret = libxml2mod.xmlValidateDtd(ctxt__o, self._o, dtd__o)\n        return ret", "output": "Try to validate the document against the dtd instance\n          Basically it does check all the definitions in the DtD.\n          Note the the internal subset (if present) is de-coupled\n          (i.e. not used), which could give problems if ID or IDREF\n           is present.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(ret):\n    '''\n    \n    '''\n\n    _options = _get_options(ret)\n\n    channel = _options.get('channel')\n    username = _options.get('username')\n    as_user = _options.get('as_user')\n    api_key = _options.get('api_key')\n    changes = _options.get('changes')\n    only_show_failed = _options.get('only_show_failed')\n    yaml_format = _options.get('yaml_format')\n\n    if not channel:\n        log.error('slack.channel not defined in salt config')\n        return\n\n    if not username:\n        log.error('slack.username not defined in salt config')\n        return\n\n    if not as_user:\n        log.error('slack.as_user not defined in salt config')\n        return\n\n    if not api_key:\n        log.error('slack.api_key not defined in salt config')\n        return\n\n    if only_show_failed and changes:\n        log.error('cannot define both slack.changes and slack.only_show_failed in salt config')\n        return\n\n    returns = ret.get('return')\n    if changes is True:\n        returns = {(key, value) for key, value in returns.items() if value['result'] is not True or value['changes']}\n\n    if only_show_failed is True:\n        returns = {(key, value) for key, value in returns.items() if value['result'] is not True}\n\n    if yaml_format is True:\n        returns = salt.utils.yaml.safe_dump(returns)\n    else:\n        returns = pprint.pformat(returns)\n\n    message = ('id: {0}\\r\\n'\n               'function: {1}\\r\\n'\n               'function args: {2}\\r\\n'\n               'jid: {3}\\r\\n'\n               'return: {4}\\r\\n').format(\n                    ret.get('id'),\n                    ret.get('fun'),\n                    ret.get('fun_args'),\n                    ret.get('jid'),\n                    returns)\n\n    slack = _post_message(channel,\n                          message,\n                          username,\n                          as_user,\n                          api_key)\n    return slack", "output": "Send an slack message with the data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _minutes_to_exclude(self):\n        \"\"\"\n        \n        \"\"\"\n        market_opens = self._market_opens.values.astype('datetime64[m]')\n        market_closes = self._market_closes.values.astype('datetime64[m]')\n        minutes_per_day = (market_closes - market_opens).astype(np.int64)\n        early_indices = np.where(\n            minutes_per_day != self._minutes_per_day - 1)[0]\n        early_opens = self._market_opens[early_indices]\n        early_closes = self._market_closes[early_indices]\n        minutes = [(market_open, early_close)\n                   for market_open, early_close\n                   in zip(early_opens, early_closes)]\n        return minutes", "output": "Calculate the minutes which should be excluded when a window\n        occurs on days which had an early close, i.e. days where the close\n        based on the regular period of minutes per day and the market close\n        do not match.\n\n        Returns\n        -------\n        List of DatetimeIndex representing the minutes to exclude because\n        of early closes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_html_header(response):\n    # type: (Response) -> None\n    \"\"\"\n    \"\"\"\n    content_type = response.headers.get(\"Content-Type\", \"\")\n    if not content_type.lower().startswith(\"text/html\"):\n        raise _NotHTML(content_type, response.request.method)", "output": "Check the Content-Type header to ensure the response contains HTML.\n\n    Raises `_NotHTML` if the content type is not text/html.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_sync_stats(self, cnt):\n        '''\n        \n        '''\n        stats = salt.utils.odict.OrderedDict()\n        if cnt.get('retcode') == salt.defaults.exitcodes.EX_OK:\n            for line in cnt.get('stdout', '').split(os.linesep):\n                line = line.split(': ')\n                if len(line) == 2:\n                    stats[line[0].lower().replace(' ', '_')] = line[1]\n            cnt['transfer'] = stats\n            del cnt['stdout']\n\n        # Remove empty\n        empty_sections = []\n        for section in cnt:\n            if not cnt[section] and section != 'retcode':\n                empty_sections.append(section)\n        for section in empty_sections:\n            del cnt[section]\n\n        return cnt", "output": "Format stats of the sync output.\n\n        :param cnt:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump_text(self, filename):\n        \"\"\"\n        \"\"\"\n        _safe_call(_LIB.LGBM_DatasetDumpText(\n            self.construct().handle,\n            c_str(filename)))\n        return self", "output": "Save Dataset to a text file.\n\n        This format cannot be loaded back in by LightGBM, but is useful for debugging purposes.\n\n        Parameters\n        ----------\n        filename : string\n            Name of the output file.\n\n        Returns\n        -------\n        self : Dataset\n            Returns self.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize(self, init=initializer.Uniform(), ctx=None, verbose=False,\n                   force_reinit=False):\n        \"\"\"\n        \"\"\"\n        self.collect_params().initialize(init, ctx, verbose, force_reinit)", "output": "Initializes :py:class:`Parameter` s of this :py:class:`Block` and its children.\n        Equivalent to ``block.collect_params().initialize(...)``\n\n        Parameters\n        ----------\n        init : Initializer\n            Global default Initializer to be used when :py:meth:`Parameter.init` is ``None``.\n            Otherwise, :py:meth:`Parameter.init` takes precedence.\n        ctx : Context or list of Context\n            Keeps a copy of Parameters on one or many context(s).\n        verbose : bool, default False\n            Whether to verbosely print out details on initialization.\n        force_reinit : bool, default False\n            Whether to force re-initialization if parameter is already initialized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def return_pub(self, ret):\n        '''\n        \n        '''\n        channel = salt.transport.client.ReqChannel.factory(self.opts, usage='salt_call')\n        load = {'cmd': '_return', 'id': self.opts['id']}\n        for key, value in six.iteritems(ret):\n            load[key] = value\n        try:\n            channel.send(load)\n        finally:\n            channel.close()", "output": "Return the data up to the master", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_last_time_step(self, **replace_time_step_kwargs):\n    \"\"\"\"\"\"\n\n    # Pre-conditions: self._time_steps shouldn't be empty.\n    assert self._time_steps\n    self._time_steps[-1] = self._time_steps[-1].replace(\n        **replace_time_step_kwargs)", "output": "Replace the last time-steps with the given kwargs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_strings_in_list(array_of_strigs, replace_with_strings):\n    \"\"\n    potentially_nested_list = [replace_with_strings.get(s) or s for s in array_of_strigs]\n    return list(flatten(potentially_nested_list))", "output": "A value in replace_with_strings can be either single string or list of strings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_package_finder(self, options, index_urls, session):\n        \"\"\"\n        \n        \"\"\"\n        return PackageFinder(\n            find_links=options.find_links,\n            index_urls=index_urls,\n            allow_all_prereleases=options.pre,\n            trusted_hosts=options.trusted_hosts,\n            session=session,\n        )", "output": "Create a package finder appropriate to this list command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_ssh_key():\n    \"\"\"\n    \"\"\"\n    path = os.environ.get(\"TUNE_CLUSTER_SSH_KEY\",\n                          os.path.expanduser(\"~/ray_bootstrap_key.pem\"))\n    if os.path.exists(path):\n        return path\n    return None", "output": "Returns ssh key to connecting to cluster workers.\n\n    If the env var TUNE_CLUSTER_SSH_KEY is provided, then this key\n    will be used for syncing across different nodes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def logical_or(lhs, rhs):\n    \"\"\"\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_logical_or,\n        lambda x, y: 1 if x or y else 0,\n        _internal._logical_or_scalar,\n        None)", "output": "Returns the result of element-wise **logical or** comparison\n    operation with broadcasting.\n\n    For each element in input arrays, return 1(true) if lhs elements or rhs elements\n    are true, otherwise return 0(false).\n\n    Equivalent to ``lhs or rhs`` and ``mx.nd.broadcast_logical_or(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First input of the function.\n    rhs : scalar or mxnet.ndarray.array\n         Second input of the function. If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        Output array of boolean values.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> mx.nd.logical_or(x, 1).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.logical_or(x, y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.logical_or(z, y).asnumpy()\n    array([[ 0.,  1.],\n           [ 1.,  1.]], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_sata_controllers(sata_controllers):\n    '''\n    \n    '''\n    sata_ctrls = []\n    keys = range(-15000, -15050, -1)\n    if sata_controllers:\n        devs = [sata['adapter'] for sata in sata_controllers]\n        log.trace('Creating SATA controllers %s', devs)\n        for sata, key in zip(sata_controllers, keys):\n            sata_ctrls.append(_apply_sata_controller_config(\n                sata['adapter'], 'add', key, sata['bus_number']))\n    return sata_ctrls", "output": "Returns a list of vim.vm.device.VirtualDeviceSpec objects representing\n    SATA controllers\n\n    sata_controllers\n        SATA properties", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def object_present(container, name, path, profile):\n    '''\n    \n    '''\n    existing_object = __salt__['libcloud_storage.get_container_object'](container, name, profile)\n    if existing_object is not None:\n        return state_result(True, \"Object already present\", name, {})\n    else:\n        result = __salt__['libcloud_storage.upload_object'](path, container, name, profile)\n        return state_result(result, \"Uploaded object\", name, {})", "output": "Ensures a object is presnt.\n\n    :param container: Container name\n    :type  container: ``str``\n\n    :param name: Object name in cloud\n    :type  name: ``str``\n\n    :param path: Local path to file\n    :type  path: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log(_self, _level, _message, *args, **kwargs):\n        \"\"\"\n        logger = _self.opt(\n            exception=_self._exception,\n            record=_self._record,\n            lazy=_self._lazy,\n            ansi=_self._ansi,\n            raw=_self._raw,\n            depth=_self._depth + 1,\n        )\n        logger._make_log_function(_level)(logger, _message, *args, **kwargs)", "output": "r\"\"\"Log ``_message.format(*args, **kwargs)`` with severity ``_level``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _range2cols(areas):\n    \"\"\"\n    \n    \"\"\"\n    cols = []\n\n    for rng in areas.split(\",\"):\n        if \":\" in rng:\n            rng = rng.split(\":\")\n            cols.extend(lrange(_excel2num(rng[0]), _excel2num(rng[1]) + 1))\n        else:\n            cols.append(_excel2num(rng))\n\n    return cols", "output": "Convert comma separated list of column names and ranges to indices.\n\n    Parameters\n    ----------\n    areas : str\n        A string containing a sequence of column ranges (or areas).\n\n    Returns\n    -------\n    cols : list\n        A list of 0-based column indices.\n\n    Examples\n    --------\n    >>> _range2cols('A:E')\n    [0, 1, 2, 3, 4]\n    >>> _range2cols('A,C,Z:AB')\n    [0, 2, 25, 26, 27]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def first_timestamp(self, event_key=None):\n    \"\"\"\n    \"\"\"\n    if event_key is None:\n      timestamps = [self._trackers[key].first_timestamp\n                    for key in self._trackers]\n      return min(timestamp for timestamp in timestamps if timestamp >= 0)\n    else:\n      return self._trackers[event_key].first_timestamp", "output": "Obtain the first timestamp.\n\n    Args:\n      event_key: the type key of the sought events (e.g., constants.NAN_KEY).\n      If None, includes all event type keys.\n\n    Returns:\n      First (earliest) timestamp of all the events of the given type (or all\n        event types if event_key is None).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_batch_coordinate(x, axis=0):\n  \"\"\"\"\"\"\n  # Compute the batch coordinate before flattening all batches\n  batch_coordinate = tf.expand_dims(\n      common_attention.coordinate_tensor(tf.shape(x)[:-1], axis=axis), axis=-1)\n  return batch_coordinate", "output": "Return a flat int32 tensor of shape [1, batch_size*length, 1].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def routes_list(route_table, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        routes = __utils__['azurearm.paged_object_to_list'](\n            netconn.routes.list(\n                resource_group_name=resource_group,\n                route_table_name=route_table\n            )\n        )\n\n        for route in routes:\n            result[route['name']] = route\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    List all routes within a route table.\n\n    :param route_table: The route table to query.\n\n    :param resource_group: The resource group name assigned to the\n        route table.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.routes_list test-rt-table testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autoencoder_range(rhp):\n  \"\"\"\"\"\"\n  rhp.set_float(\"dropout\", 0.01, 0.3)\n  rhp.set_float(\"gan_loss_factor\", 0.01, 0.1)\n  rhp.set_float(\"bottleneck_l2_factor\", 0.001, 0.1, scale=rhp.LOG_SCALE)\n  rhp.set_discrete(\"bottleneck_warmup_steps\", [200, 2000])\n  rhp.set_float(\"gumbel_temperature\", 0, 1)\n  rhp.set_float(\"gumbel_noise_factor\", 0, 0.5)", "output": "Tuning grid of the main autoencoder params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_train(self, df:DataFrame):\n        \"\"\n        self.categories = {}\n        for n in self.cat_names:\n            df.loc[:,n] = df.loc[:,n].astype('category').cat.as_ordered()\n            self.categories[n] = df[n].cat.categories", "output": "Transform `self.cat_names` columns in categorical.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _zoom(scale:uniform=1.0, row_pct:uniform=0.5, col_pct:uniform=0.5):\n    \"\"\n    s = 1-1/scale\n    col_c = s * (2*col_pct - 1)\n    row_c = s * (2*row_pct - 1)\n    return _get_zoom_mat(1/scale, 1/scale, col_c, row_c)", "output": "Zoom image by `scale`. `row_pct`,`col_pct` select focal point of zoom.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_pax_generic_header(cls, pax_headers, type, encoding):\n        \"\"\"\n        \"\"\"\n        # Check if one of the fields contains surrogate characters and thereby\n        # forces hdrcharset=BINARY, see _proc_pax() for more information.\n        binary = False\n        for keyword, value in pax_headers.items():\n            try:\n                value.encode(\"utf8\", \"strict\")\n            except UnicodeEncodeError:\n                binary = True\n                break\n\n        records = b\"\"\n        if binary:\n            # Put the hdrcharset field at the beginning of the header.\n            records += b\"21 hdrcharset=BINARY\\n\"\n\n        for keyword, value in pax_headers.items():\n            keyword = keyword.encode(\"utf8\")\n            if binary:\n                # Try to restore the original byte representation of `value'.\n                # Needless to say, that the encoding must match the string.\n                value = value.encode(encoding, \"surrogateescape\")\n            else:\n                value = value.encode(\"utf8\")\n\n            l = len(keyword) + len(value) + 3   # ' ' + '=' + '\\n'\n            n = p = 0\n            while True:\n                n = l + len(str(p))\n                if n == p:\n                    break\n                p = n\n            records += bytes(str(p), \"ascii\") + b\" \" + keyword + b\"=\" + value + b\"\\n\"\n\n        # We use a hardcoded \"././@PaxHeader\" name like star does\n        # instead of the one that POSIX recommends.\n        info = {}\n        info[\"name\"] = \"././@PaxHeader\"\n        info[\"type\"] = type\n        info[\"size\"] = len(records)\n        info[\"magic\"] = POSIX_MAGIC\n\n        # Create pax header + record blocks.\n        return cls._create_header(info, USTAR_FORMAT, \"ascii\", \"replace\") + \\\n                cls._create_payload(records)", "output": "Return a POSIX.1-2008 extended or global header sequence\n           that contains a list of keyword, value pairs. The values\n           must be strings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validatePushElement(self, doc, elem, qname):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidatePushElement(self._o, doc__o, elem__o, qname)\n        return ret", "output": "Push a new element start on the validation stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace(self, to_replace, value, inplace=False, filter=None,\n                regex=False, convert=True):\n        \"\"\"\n        \"\"\"\n\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        original_to_replace = to_replace\n\n        # try to replace, if we raise an error, convert to ObjectBlock and\n        # retry\n        try:\n            values, to_replace = self._try_coerce_args(self.values,\n                                                       to_replace)\n            mask = missing.mask_missing(values, to_replace)\n            if filter is not None:\n                filtered_out = ~self.mgr_locs.isin(filter)\n                mask[filtered_out.nonzero()[0]] = False\n\n            blocks = self.putmask(mask, value, inplace=inplace)\n            if convert:\n                blocks = [b.convert(by_item=True, numeric=False,\n                                    copy=not inplace) for b in blocks]\n            return blocks\n        except (TypeError, ValueError):\n            # GH 22083, TypeError or ValueError occurred within error handling\n            # causes infinite loop. Cast and retry only if not objectblock.\n            if is_object_dtype(self):\n                raise\n\n            # try again with a compatible block\n            block = self.astype(object)\n            return block.replace(to_replace=original_to_replace,\n                                 value=value,\n                                 inplace=inplace,\n                                 filter=filter,\n                                 regex=regex,\n                                 convert=convert)", "output": "replace the to_replace value with value, possible to create new\n        blocks here this is just a call to putmask. regex is not used here.\n        It is used in ObjectBlocks.  It is here for API compatibility.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_upgrades(refresh=True, **kwargs):\n    '''\n    \n    '''\n    pkgs = {}\n    for pkg in sorted(list_pkgs(refresh=refresh).keys()):\n        # NOTE: we already optionally refreshed in de list_pkg call\n        pkg_upgrade = latest_version(pkg, refresh=False)\n        if pkg_upgrade:\n            pkgs[pkg] = pkg_upgrade\n    return pkgs", "output": "List all available package upgrades.\n\n    .. versionadded:: 2018.3.0\n\n    refresh\n        Whether or not to refresh the package database before installing.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.list_upgrades", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_state_from_encoder(self, encoder_outputs, encoder_valid_length=None):\n        \"\"\"\n        \"\"\"\n        mem_value = encoder_outputs\n        decoder_states = [mem_value]\n        mem_length = mem_value.shape[1]\n        if encoder_valid_length is not None:\n            dtype = encoder_valid_length.dtype\n            ctx = encoder_valid_length.context\n            mem_masks = mx.nd.broadcast_lesser(\n                mx.nd.arange(mem_length, ctx=ctx, dtype=dtype).reshape((1, -1)),\n                encoder_valid_length.reshape((-1, 1)))\n            decoder_states.append(mem_masks)\n        self._encoder_valid_length = encoder_valid_length\n        return decoder_states", "output": "Initialize the state from the encoder outputs.\n\n        Parameters\n        ----------\n        encoder_outputs : list\n        encoder_valid_length : NDArray or None\n\n        Returns\n        -------\n        decoder_states : list\n            The decoder states, includes:\n\n            - mem_value : NDArray\n            - mem_masks : NDArray, optional", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_to_type(self,**kwargs):\r\n        \"\"\"\"\"\"\r\n        indexes = self.selectedIndexes()\r\n        if not indexes: return\r\n        for index in indexes:\r\n            self.model().parse_data_type(index, **kwargs)", "output": "Parse to a given type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_markdown_requisite(state, stateid, makelink=True):\n    '''\n    \n    '''\n    fmt_id = '{0}: {1}'.format(state, stateid)\n    if makelink:\n        return ' * [{0}](#{1})\\n'.format(fmt_id, _format_markdown_link(fmt_id))\n    else:\n        return ' * `{0}`\\n'.format(fmt_id)", "output": "format requisite as a link users can click", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gemset_present(name, ruby='default', user=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    ret = _check_rvm(ret, user)\n    if ret['result'] is False:\n        return ret\n\n    if '@' in name:\n        ruby, name = name.split('@')\n        ret = _check_ruby(ret, ruby)\n        if not ret['result']:\n            ret['result'] = False\n            ret['comment'] = 'Requested ruby implementation was not found.'\n            return ret\n\n    if name in __salt__['rvm.gemset_list'](ruby, runas=user):\n        ret['result'] = True\n        ret['comment'] = 'Gemset already exists.'\n    else:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'Set to install gemset {0}'.format(name)\n            return ret\n        if __salt__['rvm.gemset_create'](ruby, name, runas=user):\n            ret['result'] = True\n            ret['comment'] = 'Gemset successfully created.'\n            ret['changes'][name] = 'created'\n        else:\n            ret['result'] = False\n            ret['comment'] = 'Gemset could not be created.'\n\n    return ret", "output": "Verify that the gemset is present.\n\n    name\n        The name of the gemset.\n\n    ruby: default\n        The ruby version this gemset belongs to.\n\n    user: None\n        The user to run rvm as.\n\n        .. versionadded:: 0.17.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _configure_project(config):\n    \"\"\"\n    \"\"\"\n    project_id = config[\"provider\"].get(\"project_id\")\n    assert config[\"provider\"][\"project_id\"] is not None, (\n        \"'project_id' must be set in the 'provider' section of the autoscaler\"\n        \" config. Notice that the project id must be globally unique.\")\n    project = _get_project(project_id)\n\n    if project is None:\n        #  Project not found, try creating it\n        _create_project(project_id)\n        project = _get_project(project_id)\n\n    assert project is not None, \"Failed to create project\"\n    assert project[\"lifecycleState\"] == \"ACTIVE\", (\n        \"Project status needs to be ACTIVE, got {}\".format(\n            project[\"lifecycleState\"]))\n\n    config[\"provider\"][\"project_id\"] = project[\"projectId\"]\n\n    return config", "output": "Setup a Google Cloud Platform Project.\n\n    Google Compute Platform organizes all the resources, such as storage\n    buckets, users, and instances under projects. This is different from\n    aws ec2 where everything is global.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insert(self, loc, item):\n        \"\"\"\n        \n        \"\"\"\n        _self = np.asarray(self)\n        item = self._coerce_scalar_to_index(item)._ndarray_values\n        idx = np.concatenate((_self[:loc], item, _self[loc:]))\n        return self._shallow_copy_with_infer(idx)", "output": "Make new Index inserting new item at location.\n\n        Follows Python list.append semantics for negative values.\n\n        Parameters\n        ----------\n        loc : int\n        item : object\n\n        Returns\n        -------\n        new_index : Index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _heapreplace_max(heap, item):\n    \"\"\"\"\"\"\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup_max(heap, 0)\n    return returnitem", "output": "Maxheap version of a heappop followed by a heappush.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def query_all(self, query, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        # Make the initial query to Salesforce\n        response = self.query(query, **kwargs)\n\n        # get fields\n        fields = get_soql_fields(query)\n\n        # put fields and first page of results into a temp list to be written to TempFile\n        tmp_list = [fields]\n        tmp_list.extend(parse_results(fields, response))\n\n        tmp_dir = luigi.configuration.get_config().get('salesforce', 'local-tmp-dir', None)\n        tmp_file = tempfile.TemporaryFile(mode='a+b', dir=tmp_dir)\n\n        writer = csv.writer(tmp_file)\n        writer.writerows(tmp_list)\n\n        # The number of results might have exceeded the Salesforce batch limit\n        # so check whether there are more results and retrieve them if so.\n\n        length = len(response['records'])\n        while not response['done']:\n            response = self.query_more(response['nextRecordsUrl'], identifier_is_url=True, **kwargs)\n\n            writer.writerows(parse_results(fields, response))\n            length += len(response['records'])\n            if not length % 10000:\n                logger.info('Requested {0} lines...'.format(length))\n\n        logger.info('Requested a total of {0} lines.'.format(length))\n\n        tmp_file.seek(0)\n        return tmp_file", "output": "Returns the full set of results for the `query`. This is a\n        convenience wrapper around `query(...)` and `query_more(...)`.\n        The returned dict is the decoded JSON payload from the final call to\n        Salesforce, but with the `totalSize` field representing the full\n        number of results retrieved and the `records` list representing the\n        full list of records retrieved.\n\n        :param query: the SOQL query to send to Salesforce, e.g.\n                   `SELECT Id FROM Lead WHERE Email = \"waldo@somewhere.com\"`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _implementation():\n    \"\"\"\n    \"\"\"\n    implementation = platform.python_implementation()\n\n    if implementation == 'CPython':\n        implementation_version = platform.python_version()\n    elif implementation == 'PyPy':\n        implementation_version = '%s.%s.%s' % (sys.pypy_version_info.major,\n                                               sys.pypy_version_info.minor,\n                                               sys.pypy_version_info.micro)\n        if sys.pypy_version_info.releaselevel != 'final':\n            implementation_version = ''.join([\n                implementation_version, sys.pypy_version_info.releaselevel\n            ])\n    elif implementation == 'Jython':\n        implementation_version = platform.python_version()  # Complete Guess\n    elif implementation == 'IronPython':\n        implementation_version = platform.python_version()  # Complete Guess\n    else:\n        implementation_version = 'Unknown'\n\n    return {'name': implementation, 'version': implementation_version}", "output": "Return a dict with the Python implementation and version.\n\n    Provide both the name and the version of the Python implementation\n    currently running. For example, on CPython 2.7.5 it will return\n    {'name': 'CPython', 'version': '2.7.5'}.\n\n    This function works best on CPython and PyPy: in particular, it probably\n    doesn't work for Jython or IronPython. Future investigation should be done\n    to work out the correct shape of the code for those platforms.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def updateall(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        self._update_updateall(False, *args, **kwargs)\n        return self", "output": "Update this dictionary with the items from <mapping>, replacing\n        existing key:value items with shared keys before adding new key:value\n        items.\n\n        Example:\n          omd = omdict([(1,1), (2,2)])\n          omd.updateall([(2,'two'), (1,'one'), (2,222), (1,111)])\n          omd.allitems() == [(1, 'one'), (2, 'two'), (2, 222), (1, 111)]\n\n        Returns: <self>.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def default_security_rule_get(name, security_group, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n\n    default_rules = default_security_rules_list(\n        security_group=security_group,\n        resource_group=resource_group,\n        **kwargs\n    )\n\n    if isinstance(default_rules, dict) and 'error' in default_rules:\n        return default_rules\n\n    try:\n        for default_rule in default_rules:\n            if default_rule['name'] == name:\n                result = default_rule\n        if not result:\n            result = {\n                'error': 'Unable to find {0} in {1}!'.format(name, security_group)\n            }\n    except KeyError as exc:\n        log.error('Unable to find %s in %s!', name, security_group)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get details about a default security rule within a security group.\n\n    :param name: The name of the security rule to query.\n\n    :param security_group: The network security group containing the\n        security rule.\n\n    :param resource_group: The resource group name assigned to the\n        network security group.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.default_security_rule_get DenyAllOutBound testnsg testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_requirements(model, section_name):\n    \"\"\"\n    \"\"\"\n    if not model:\n        return {}\n    return {identify_requirment(r): r for r in (\n        requirementslib.Requirement.from_pipfile(name, package._data)\n        for name, package in model.get(section_name, {}).items()\n    )}", "output": "Produce a mapping of identifier: requirement from the section.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newText(content):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlNewText(content)\n    if ret is None:raise treeError('xmlNewText() failed')\n    return xmlNode(_obj=ret)", "output": "Creation of a new text node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_to_defaults(self, save=True, verbose=False, section=None):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        for sec, options in self.defaults:\r\n            if section == None or section == sec:\r\n                for option in options:\r\n                    value = options[ option ]\r\n                    self._set(sec, option, value, verbose)\r\n        if save:\r\n            self._save()", "output": "Reset config to Default values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_blkdev(name):\n    '''\n    \n    '''\n    name = os.path.expanduser(name)\n\n    stat_structure = None\n    try:\n        stat_structure = os.stat(name)\n    except OSError as exc:\n        if exc.errno == errno.ENOENT:\n            # If the block device does not exist in the first place\n            return False\n        else:\n            raise\n    return stat.S_ISBLK(stat_structure.st_mode)", "output": "Check if a file exists and is a block device.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' file.is_blkdev /dev/blk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_source(cls, source_file_hash, source):\n        \"\"\"\n        \"\"\"\n        embedding_name = cls.__name__.lower()\n        if source not in source_file_hash:\n            raise KeyError('Cannot find pre-trained source {} for token embedding {}. '\n                           'Valid pre-trained file names for embedding {}: {}'.format(\n                               source, embedding_name, embedding_name,\n                               ', '.join(source_file_hash.keys())))", "output": "Checks if a pre-trained token embedding source name is valid.\n\n\n        Parameters\n        ----------\n        source : str\n            The pre-trained token embedding source.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shiftLeft(col, numBits):\n    \"\"\"\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.shiftLeft(_to_java_column(col), numBits))", "output": "Shift the given value numBits left.\n\n    >>> spark.createDataFrame([(21,)], ['a']).select(shiftLeft('a', 1).alias('r')).collect()\n    [Row(r=42)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_router(self, router, name=None, admin_state_up=None, **kwargs):\n        '''\n        \n        '''\n        router_id = self._find_router_id(router)\n        body = {}\n\n        if 'ext_network' in kwargs:\n            if kwargs.get('ext_network') is None:\n                body['external_gateway_info'] = None\n            else:\n                net_id = self._find_network_id(kwargs.get('ext_network'))\n                body['external_gateway_info'] = {'network_id': net_id}\n        if name is not None:\n            body['name'] = name\n        if admin_state_up is not None:\n            body['admin_state_up'] = admin_state_up\n        return self.network_conn.update_router(\n            router=router_id, body={'router': body})", "output": "Updates a router", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sink_get(self, project, sink_name):\n        \"\"\"\n        \"\"\"\n        target = \"/projects/%s/sinks/%s\" % (project, sink_name)\n        return self.api_request(method=\"GET\", path=target)", "output": "API call:  retrieve a sink resource.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks/get\n\n        :type project: str\n        :param project: ID of the project containing the sink.\n\n        :type sink_name: str\n        :param sink_name: the name of the sink\n\n        :rtype: dict\n        :returns: The JSON sink object returned from the API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def options(name, conf_file=None):\n    '''\n    \n    '''\n    config = _read_config(conf_file)\n    section_name = 'program:{0}'.format(name)\n    if section_name not in config.sections():\n        raise CommandExecutionError('Process \\'{0}\\' not found'.format(name))\n    ret = {}\n    for key, val in config.items(section_name):\n        val = salt.utils.stringutils.to_num(val.split(';')[0].strip())\n        # pylint: disable=maybe-no-member\n        if isinstance(val, string_types):\n            if val.lower() == 'true':\n                val = True\n            elif val.lower() == 'false':\n                val = False\n        # pylint: enable=maybe-no-member\n        ret[key] = val\n    return ret", "output": ".. versionadded:: 2014.1.0\n\n    Read the config file and return the config options for a given process\n\n    name\n        Name of the configured process\n    conf_file\n        path to supervisord config file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' supervisord.options foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_data_columns(self, data_columns, min_itemsize):\n        \"\"\"\n        \"\"\"\n\n        if not len(self.non_index_axes):\n            return []\n\n        axis, axis_labels = self.non_index_axes[0]\n        info = self.info.get(axis, dict())\n        if info.get('type') == 'MultiIndex' and data_columns:\n            raise ValueError(\"cannot use a multi-index on axis [{0}] with \"\n                             \"data_columns {1}\".format(axis, data_columns))\n\n        # evaluate the passed data_columns, True == use all columns\n        # take only valide axis labels\n        if data_columns is True:\n            data_columns = list(axis_labels)\n        elif data_columns is None:\n            data_columns = []\n\n        # if min_itemsize is a dict, add the keys (exclude 'values')\n        if isinstance(min_itemsize, dict):\n\n            existing_data_columns = set(data_columns)\n            data_columns.extend([\n                k for k in min_itemsize.keys()\n                if k != 'values' and k not in existing_data_columns\n            ])\n\n        # return valid columns in the order of our axis\n        return [c for c in data_columns if c in axis_labels]", "output": "take the input data_columns and min_itemize and create a data\n        columns spec", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_all_callbacks(self):\n        \"\"\" \"\"\"\n        for cb_id in list(self._next_tick_callback_removers.keys()):\n            self.remove_next_tick_callback(cb_id)\n        for cb_id in list(self._timeout_callback_removers.keys()):\n            self.remove_timeout_callback(cb_id)\n        for cb_id in list(self._periodic_callback_removers.keys()):\n            self.remove_periodic_callback(cb_id)", "output": "Removes all registered callbacks.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(module, details=False):\n    '''\n    \n    '''\n    ret = {\n        'old': None,\n        'new': None,\n    }\n\n    info = show(module)\n    if 'error' in info:\n        return {\n            'error': info['error']\n        }\n\n    version = info.get('installed version', None)\n    if version is None:\n        return ret\n\n    ret['old'] = version\n\n    if 'cpan build dirs' not in info:\n        return {\n            'error': 'No CPAN data available to use for uninstalling'\n        }\n\n    mod_pathfile = module.replace('::', '/') + '.pm'\n    ins_path = info['installed file'].replace(mod_pathfile, '')\n\n    files = []\n    for build_dir in info['cpan build dirs']:\n        contents = os.listdir(build_dir)\n        if 'MANIFEST' not in contents:\n            continue\n        mfile = os.path.join(build_dir, 'MANIFEST')\n        with salt.utils.files.fopen(mfile, 'r') as fh_:\n            for line in fh_.readlines():\n                line = salt.utils.stringutils.to_unicode(line)\n                if line.startswith('lib/'):\n                    files.append(line.replace('lib/', ins_path).strip())\n\n    rm_details = {}\n    for file_ in files:\n        if file_ in rm_details:\n            continue\n        log.trace('Removing %s', file_)\n        if __salt__['file.remove'](file_):\n            rm_details[file_] = 'removed'\n        else:\n            rm_details[file_] = 'unable to remove'\n\n    if details:\n        ret['details'] = rm_details\n\n    return ret", "output": "Attempt to remove a Perl module that was installed from CPAN. Because the\n    ``cpan`` command doesn't actually support \"uninstall\"-like functionality,\n    this function will attempt to do what it can, with what it has from CPAN.\n\n    Until this function is declared stable, USE AT YOUR OWN RISK!\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cpan.remove Old::Package", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compare(self, statement_a, statement_b):\n        \"\"\"\n        \n        \"\"\"\n        # Make both strings lowercase\n        document_a = self.nlp(statement_a.text.lower())\n        document_b = self.nlp(statement_b.text.lower())\n\n        statement_a_lemmas = set([\n            token.lemma_ for token in document_a if not token.is_stop\n        ])\n        statement_b_lemmas = set([\n            token.lemma_ for token in document_b if not token.is_stop\n        ])\n\n        # Calculate Jaccard similarity\n        numerator = len(statement_a_lemmas.intersection(statement_b_lemmas))\n        denominator = float(len(statement_a_lemmas.union(statement_b_lemmas)))\n        ratio = numerator / denominator\n\n        return ratio", "output": "Return the calculated similarity of two\n        statements based on the Jaccard index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snapshot_path(cls, project, snapshot):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/snapshots/{snapshot}\",\n            project=project,\n            snapshot=snapshot,\n        )", "output": "Return a fully-qualified snapshot string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_base_10l_8h_big_uncond_dr03_dan_64():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_base_10l_8h_big_cond_dr03_dan()\n  hparams.unconditional = True\n  hparams.max_length = 14000\n  hparams.batch_size = 1\n  hparams.img_len = 64\n  hparams.layer_prepostprocess_dropout = 0.1\n  return hparams", "output": "big 1d model for unconditional generation on imagenet.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_object(self, object_type):\n        \"\"\"\"\"\"\n        node = self\n        while node is not None:\n            if isinstance(node.obj, object_type):\n                return node.obj\n            node = node.parent", "output": "Finds the closest object of a given type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_pillar_cfg(pillar_key,\n                    pillarenv=None,\n                    saltenv=None):\n    '''\n    \n    '''\n    pillar_cfg = __salt__['pillar.get'](pillar_key,\n                                        pillarenv=pillarenv,\n                                        saltenv=saltenv)\n    return pillar_cfg", "output": "Retrieve the pillar data from the right environment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flush(self, error=False, prompt=False):\r\n        \"\"\"\"\"\"\r\n        # Fix for Issue 2452 \r\n        if PY3:\r\n            try:\r\n                text = \"\".join(self.__buffer)\r\n            except TypeError:\r\n                text = b\"\".join(self.__buffer)\r\n                try:\r\n                    text = text.decode( locale.getdefaultlocale()[1] )\r\n                except:\r\n                    pass\r\n        else:\r\n            text = \"\".join(self.__buffer)\r\n\r\n        self.__buffer = []\r\n        self.insert_text(text, at_end=True, error=error, prompt=prompt)\r\n        QCoreApplication.processEvents()\r\n        self.repaint()\r\n        # Clear input buffer:\r\n        self.new_input_line = True", "output": "Flush buffer, write text to console", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_properties(path=\"\", method=\"GET\", forced_params=None):\n    '''\n    \n    '''\n    if api is None:\n        _import_api()\n\n    sub = api\n    path_levels = [level for level in path.split('/') if level != '']\n    search_path = ''\n    props = []\n    parameters = set([] if forced_params is None else forced_params)\n    # Browse all path elements but last\n    for elem in path_levels[:-1]:\n        search_path += '/' + elem\n        # Lookup for a dictionary with path = \"requested path\" in list\" and return its children\n        sub = (item for item in sub if item[\"path\"] == search_path).next()['children']\n    # Get leaf element in path\n    search_path += '/' + path_levels[-1]\n    sub = next((item for item in sub if item[\"path\"] == search_path))\n    try:\n        # get list of properties for requested method\n        props = sub['info'][method]['parameters']['properties'].keys()\n    except KeyError as exc:\n        log.error('method not found: \"%s\"', exc)\n    for prop in props:\n        numerical = re.match(r'(\\w+)\\[n\\]', prop)\n        # generate (arbitrarily) 10 properties for duplicatable properties identified by:\n        # \"prop[n]\"\n        if numerical:\n            for i in range(10):\n                parameters.add(numerical.group(1) + six.text_type(i))\n        else:\n            parameters.add(prop)\n    return parameters", "output": "Return the parameter list from api for defined path and HTTP method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self, path, recursive=True):\n        \"\"\"\n        \n        \"\"\"\n        if recursive:\n            cmd = [\"rm\", \"-r\", path]\n        else:\n            cmd = [\"rm\", path]\n\n        self.remote_context.check_output(cmd)", "output": "Remove file or directory at location `path`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_entity(entity):\n    '''\n    \n    '''\n\n    #Validate entity:\n    if entity['type'] == 'cluster':\n        schema = ESXClusterEntitySchema.serialize()\n    elif entity['type'] == 'vcenter':\n        schema = VCenterEntitySchema.serialize()\n    else:\n        raise ArgumentValueError('Unsupported entity type \\'{0}\\''\n                                 ''.format(entity['type']))\n    try:\n        jsonschema.validate(entity, schema)\n    except jsonschema.exceptions.ValidationError as exc:\n        raise InvalidEntityError(exc)", "output": "Validates the entity dict representation\n\n    entity\n        Dictionary representation of an entity.\n        See ``_get_entity`` docstrings for format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_policies(self, resource_properties):\n        \"\"\"\n        \n        \"\"\"\n\n        policies = None\n\n        if self._contains_policies(resource_properties):\n            policies = resource_properties[self.POLICIES_PROPERTY_NAME]\n\n        if not policies:\n            # Policies is None or empty\n            return []\n\n        if not isinstance(policies, list):\n            # Just a single entry. Make it into a list of convenience\n            policies = [policies]\n\n        result = []\n        for policy in policies:\n            policy_type = self._get_type(policy)\n            entry = PolicyEntry(data=policy, type=policy_type)\n            result.append(entry)\n\n        return result", "output": "Returns a list of policies from the resource properties. This method knows how to interpret and handle\n        polymorphic nature of the policies property.\n\n        Policies can be one of the following:\n\n            * Managed policy name: string\n            * List of managed policy names: list of strings\n            * IAM Policy document: dict containing Statement key\n            * List of IAM Policy documents: list of IAM Policy Document\n            * Policy Template: dict with only one key where key is in list of supported policy template names\n            * List of Policy Templates: list of Policy Template\n\n\n        :param dict resource_properties: Dictionary of resource properties containing the policies property.\n            It is assumed that this is already a dictionary and contains policies key.\n        :return list of PolicyEntry: List of policies, where each item is an instance of named tuple `PolicyEntry`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _replace_coerce(self, to_replace, value, inplace=True, regex=False,\n                        convert=False, mask=None):\n        \"\"\"\n        \n        \"\"\"\n\n        if mask.any():\n            if not regex:\n                self = self.coerce_to_target_dtype(value)\n                return self.putmask(mask, value, inplace=inplace)\n            else:\n                return self._replace_single(to_replace, value, inplace=inplace,\n                                            regex=regex,\n                                            convert=convert,\n                                            mask=mask)\n        return self", "output": "Replace value corresponding to the given boolean array with another\n        value.\n\n        Parameters\n        ----------\n        to_replace : object or pattern\n            Scalar to replace or regular expression to match.\n        value : object\n            Replacement object.\n        inplace : bool, default False\n            Perform inplace modification.\n        regex : bool, default False\n            If true, perform regular expression substitution.\n        convert : bool, default True\n            If true, try to coerce any object types to better types.\n        mask : array-like of bool, optional\n            True indicate corresponding element is ignored.\n\n        Returns\n        -------\n        A new block if there is anything to replace or the original block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_multi(self, keys):\n        \"\"\"\n        \"\"\"\n        if not keys:\n            return\n\n        # We allow partial keys to attempt a delete, the backend will fail.\n        current = self.current_batch\n        in_batch = current is not None\n\n        if not in_batch:\n            current = self.batch()\n            current.begin()\n\n        for key in keys:\n            current.delete(key)\n\n        if not in_batch:\n            current.commit()", "output": "Delete keys from the Cloud Datastore.\n\n        :type keys: list of :class:`google.cloud.datastore.key.Key`\n        :param keys: The keys to be deleted from the Datastore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wait(lfunc, log_lvl=None, log_msg=None, tries=10):\n    '''\n    \n    '''\n    i = 0\n    while i < tries:\n        time.sleep(1)\n\n        if lfunc():\n            return True\n        else:\n            i += 1\n    if log_lvl is not None:\n        log.log(LOG[log_lvl], log_msg)\n    return False", "output": "Wait for lfunc to be True\n    :return: True if lfunc succeeded within tries, False if it didn't", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __extract_model_summary_value(model, value):\n    \"\"\"\n    \n    \"\"\"\n    field_value = None\n    if isinstance(value, _precomputed_field):\n        field_value = value.field\n    else:\n        field_value = model._get(value)\n    if isinstance(field_value, float):\n        try:\n            field_value = round(field_value, 4)\n        except:\n            pass\n    return field_value", "output": "Extract a model summary field value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def change_env(name, val):\n    \"\"\"\n    \n    \"\"\"\n    oldval = os.environ.get(name, None)\n    os.environ[name] = val\n    yield\n    if oldval is None:\n        del os.environ[name]\n    else:\n        os.environ[name] = oldval", "output": "Args:\n        name(str), val(str):\n\n    Returns:\n        a context where the environment variable ``name`` being set to\n        ``val``. It will be set back after the context exits.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_parameter_group(name, Filters=None, MaxRecords=None, Marker=None,\n                             region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    res = __salt__['boto_rds.parameter_group_exists'](name, tags=None,\n                                                      region=region, key=key,\n                                                      keyid=keyid,\n                                                      profile=profile)\n    if not res.get('exists'):\n        return {'exists': bool(res)}\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if not conn:\n            return {'results': bool(conn)}\n\n        kwargs = {}\n        for key in ('Marker', 'Filters'):\n            if locals()[key] is not None:\n                kwargs[key] = str(locals()[key])  # future lint: disable=blacklisted-function\n\n        if locals()['MaxRecords'] is not None:\n            kwargs['MaxRecords'] = int(locals()['MaxRecords'])\n\n        info = conn.describe_db_parameter_groups(DBParameterGroupName=name,\n                                                 **kwargs)\n\n        if not info:\n            return {'results': bool(info), 'message':\n                    'Failed to get RDS description for group {0}.'.format(name)}\n\n        return {'results': bool(info), 'message':\n                'Got RDS descrition for group {0}.'.format(name)}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Returns a list of `DBParameterGroup` descriptions.\n    CLI example to description of parameter group::\n\n        salt myminion boto_rds.describe_parameter_group parametergroupname\\\n            region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def file_renamed(self, editor, new_filename):\r\n        \"\"\"\"\"\"\r\n        if editor is None:\r\n            # This is needed when we can't find an editor to attach\r\n            # the outline explorer to.\r\n            # Fix issue 8813\r\n            return\r\n        editor_id = editor.get_id()\r\n        if editor_id in list(self.editor_ids.values()):\r\n            root_item = self.editor_items[editor_id]\r\n            root_item.set_path(new_filename, fullpath=self.show_fullpath)\r\n            self.__sort_toplevel_items()", "output": "File was renamed, updating outline explorer tree", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply(model_path):\n    \"\"\" \"\"\"\n    pred_config = PredictConfig(\n        session_init=get_model_loader(model_path),\n        model=Model(),\n        input_names=['input_img'],\n        output_names=['prediction_img'])\n\n    pred = OfflinePredictor(pred_config)\n    img = cv2.imread('lena.png')\n    prediction = pred([img])[0]\n    cv2.imwrite('applied_default.jpg', prediction[0])", "output": "Run inference from a training model checkpoint.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collect_local(self, path):\n        \"\"\"  \"\"\"\n        for f in os.listdir(path):\n            lpath = os.path.join(path, f)\n            if not os.path.isfile(lpath):\n                continue\n            Artifact(self, lpath)", "output": "Collect artifacts from a local directory possibly previously\n        collected from s3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def temp_copy_extracted_submission(self):\n    \"\"\"\n    \"\"\"\n    tmp_copy_dir = os.path.join(self.submission_dir, 'tmp_copy')\n    shell_call(['cp', '-R', os.path.join(self.extracted_submission_dir),\n                tmp_copy_dir])\n    return tmp_copy_dir", "output": "Creates a temporary copy of extracted submission.\n\n    When executed, submission is allowed to modify it's own directory. So\n    to ensure that submission does not pass any data between runs, new\n    copy of the submission is made before each run. After a run temporary copy\n    of submission is deleted.\n\n    Returns:\n      directory where temporary copy is located", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_image(self, img, is_train):\n        \"\"\"\"\"\"\n        img_arr = mx.image.imread(img)\n        img_arr = transform(img_arr, 256, 256, is_train, self.boxes[img])\n        return img_arr", "output": "Load and transform an image.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sharded_filenames(filename_prefix, num_shards):\n  \"\"\"\"\"\"\n  shard_suffix = \"%05d-of-%05d\"\n  return [\n      \"%s-%s\" % (filename_prefix, shard_suffix % (i, num_shards))\n      for i in range(num_shards)\n  ]", "output": "Sharded filenames given prefix and number of shards.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clean_vdev_config(config):\n    '''\n    \n    '''\n    cln_config = OrderedDict()\n    for label, sub_config in config.items():\n        if label not in ['state', 'read', 'write', 'cksum']:\n            sub_config = _clean_vdev_config(sub_config)\n\n            if sub_config and isinstance(cln_config, list):\n                cln_config.append(OrderedDict([(label, sub_config)]))\n            elif sub_config and isinstance(cln_config, OrderedDict):\n                cln_config[label] = sub_config\n            elif isinstance(cln_config, list):\n                cln_config.append(label)\n            elif isinstance(cln_config, OrderedDict):\n                new_config = []\n                for old_label, old_config in cln_config.items():\n                    new_config.append(OrderedDict([(old_label, old_config)]))\n                new_config.append(label)\n                cln_config = new_config\n            else:\n                cln_config = [label]\n\n    return cln_config", "output": "Return a simple vdev tree from zpool.status' config section", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_logging(Name,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.start_logging(Name=Name)\n        return {'started': True}\n    except ClientError as e:\n        return {'started': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Start logging for a trail\n\n    Returns {started: true} if the trail was started and returns\n    {started: False} if the trail was not started.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cloudtrail.start_logging my_trail", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_signed_query_params_v2(credentials, expiration, string_to_sign):\n    \"\"\"\n    \"\"\"\n    ensure_signed_credentials(credentials)\n    signature_bytes = credentials.sign_bytes(string_to_sign)\n    signature = base64.b64encode(signature_bytes)\n    service_account_name = credentials.signer_email\n    return {\n        \"GoogleAccessId\": service_account_name,\n        \"Expires\": str(expiration),\n        \"Signature\": signature,\n    }", "output": "Gets query parameters for creating a signed URL.\n\n    :type credentials: :class:`google.auth.credentials.Signing`\n    :param credentials: The credentials used to create a private key\n                        for signing text.\n\n    :type expiration: int or long\n    :param expiration: When the signed URL should expire.\n\n    :type string_to_sign: str\n    :param string_to_sign: The string to be signed by the credentials.\n\n    :raises: :exc:`AttributeError` if credentials is not an instance\n            of :class:`google.auth.credentials.Signing`.\n\n    :rtype: dict\n    :returns: Query parameters matching the signing credentials with a\n              signed payload.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_ATR(DataFrame, N=14):\n    \"\"\"\n    \n\n    \"\"\"\n    C = DataFrame['close']\n    H = DataFrame['high']\n    L = DataFrame['low']\n    TR = MAX(MAX((H - L), ABS(REF(C, 1) - H)), ABS(REF(C, 1) - L))\n    atr = MA(TR, N)\n    return pd.DataFrame({'TR': TR, 'ATR': atr})", "output": "\u8f93\u51faTR:(\u6700\u9ad8\u4ef7-\u6700\u4f4e\u4ef7)\u548c\u6628\u6536-\u6700\u9ad8\u4ef7\u7684\u7edd\u5bf9\u503c\u7684\u8f83\u5927\u503c\u548c\u6628\u6536-\u6700\u4f4e\u4ef7\u7684\u7edd\u5bf9\u503c\u7684\u8f83\u5927\u503c\n    \u8f93\u51fa\u771f\u5b9e\u6ce2\u5e45:TR\u7684N\u65e5\u7b80\u5355\u79fb\u52a8\u5e73\u5747\n    \u7b97\u6cd5\uff1a\u4eca\u65e5\u632f\u5e45\u3001\u4eca\u65e5\u6700\u9ad8\u4e0e\u6628\u6536\u5dee\u4ef7\u3001\u4eca\u65e5\u6700\u4f4e\u4e0e\u6628\u6536\u5dee\u4ef7\u4e2d\u7684\u6700\u5927\u503c\uff0c\u4e3a\u771f\u5b9e\u6ce2\u5e45\uff0c\u6c42\u771f\u5b9e\u6ce2\u5e45\u7684N\u65e5\u79fb\u52a8\u5e73\u5747\n\n    \u53c2\u6570\uff1aN\u3000\u5929\u6570\uff0c\u4e00\u822c\u53d614", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extend(self, elem_seq):\n    \"\"\"\n    \"\"\"\n    message_class = self._message_descriptor._concrete_class\n    listener = self._message_listener\n    values = self._values\n    for message in elem_seq:\n      new_element = message_class()\n      new_element._SetListener(listener)\n      new_element.MergeFrom(message)\n      values.append(new_element)\n    listener.Modified()", "output": "Extends by appending the given sequence of elements of the same type\n    as this one, copying each individual message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def value(val, transform=None):\n    ''' \n\n    '''\n    if transform:\n        return dict(value=val, transform=transform)\n    return dict(value=val)", "output": "Convenience function to explicitly return a \"value\" specification for\n    a Bokeh :class:`~bokeh.core.properties.DataSpec` property.\n\n    Args:\n        val (any) : a fixed value to specify for a ``DataSpec`` property.\n\n        transform (Transform, optional) : a transform to apply (default: None)\n\n    Returns:\n        dict : ``{ \"value\": name }``\n\n    .. note::\n        String values for property specifications are by default interpreted\n        as field names. This function is especially useful when you want to\n        specify a fixed value with text properties.\n\n    Example:\n\n        .. code-block:: python\n\n            # The following will take text values to render from a data source\n            # column \"text_column\", but use a fixed value \"12pt\" for font size\n            p.text(\"x\", \"y\", text=\"text_column\",\n                   text_font_size=value(\"12pt\"), source=source)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mod_run_check_cmd(cmd, filename, **check_cmd_opts):\n    '''\n    \n    '''\n\n    log.debug('running our check_cmd')\n    _cmd = '{0} {1}'.format(cmd, filename)\n    cret = __salt__['cmd.run_all'](_cmd, **check_cmd_opts)\n    if cret['retcode'] != 0:\n        ret = {'comment': 'check_cmd execution failed',\n               'skip_watch': True,\n               'result': False}\n\n        if cret.get('stdout'):\n            ret['comment'] += '\\n' + cret['stdout']\n        if cret.get('stderr'):\n            ret['comment'] += '\\n' + cret['stderr']\n\n        return ret\n\n    # No reason to stop, return True\n    return True", "output": "Execute the check_cmd logic.\n\n    Return a result dict if ``check_cmd`` succeeds (check_cmd == 0)\n    otherwise return True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debugDumpAttr(self, output, depth):\n        \"\"\" \"\"\"\n        libxml2mod.xmlDebugDumpAttr(output, self._o, depth)", "output": "Dumps debug information for the attribute", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reboot(name, call=None):\n    '''\n    \n    '''\n    datacenter_id = get_datacenter_id()\n    conn = get_conn()\n    node = get_node(conn, name)\n\n    conn.reboot_server(datacenter_id=datacenter_id, server_id=node['id'])\n\n    return True", "output": "reboot a machine by name\n    :param name: name given to the machine\n    :param call: call value in this case is 'action'\n    :return: true if successful\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a reboot vm_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, key):\n        \"\"\"\n        \"\"\"\n        if key in self._vertices:\n            raise ValueError('vertex exists')\n        self._vertices.add(key)\n        self._forwards[key] = set()\n        self._backwards[key] = set()", "output": "Add a new vertex to the graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setData(self, index, value, role):\r\n        \"\"\"\"\"\"\r\n        row = index.row()\r\n        name, state = self.row(row)\r\n\r\n        if role == Qt.CheckStateRole:\r\n            self.set_row(row, [name, not state])\r\n            self._parent.setCurrentIndex(index)\r\n            self._parent.setFocus()\r\n            self.dataChanged.emit(index, index)\r\n            return True\r\n        elif role == Qt.EditRole:\r\n            self.set_row(row, [from_qvariant(value, to_text_string), state])\r\n            self.dataChanged.emit(index, index)\r\n            return True\r\n        return True", "output": "Override Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _post_plot_logic_common(self, ax, data):\n        \"\"\"\"\"\"\n\n        def get_label(i):\n            try:\n                return pprint_thing(data.index[i])\n            except Exception:\n                return ''\n\n        if self.orientation == 'vertical' or self.orientation is None:\n            if self._need_to_set_index:\n                xticklabels = [get_label(x) for x in ax.get_xticks()]\n                ax.set_xticklabels(xticklabels)\n            self._apply_axis_properties(ax.xaxis, rot=self.rot,\n                                        fontsize=self.fontsize)\n            self._apply_axis_properties(ax.yaxis, fontsize=self.fontsize)\n\n            if hasattr(ax, 'right_ax'):\n                self._apply_axis_properties(ax.right_ax.yaxis,\n                                            fontsize=self.fontsize)\n\n        elif self.orientation == 'horizontal':\n            if self._need_to_set_index:\n                yticklabels = [get_label(y) for y in ax.get_yticks()]\n                ax.set_yticklabels(yticklabels)\n            self._apply_axis_properties(ax.yaxis, rot=self.rot,\n                                        fontsize=self.fontsize)\n            self._apply_axis_properties(ax.xaxis, fontsize=self.fontsize)\n\n            if hasattr(ax, 'right_ax'):\n                self._apply_axis_properties(ax.right_ax.yaxis,\n                                            fontsize=self.fontsize)\n        else:  # pragma no cover\n            raise ValueError", "output": "Common post process for each axes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_params_pb(params, param_types):\n        \"\"\"\n        \"\"\"\n        if params is not None:\n            if param_types is None:\n                raise ValueError(\"Specify 'param_types' when passing 'params'.\")\n            return Struct(\n                fields={key: _make_value_pb(value) for key, value in params.items()}\n            )\n        else:\n            if param_types is not None:\n                raise ValueError(\"Specify 'params' when passing 'param_types'.\")\n\n        return None", "output": "Helper for :meth:`execute_update`.\n\n        :type params: dict, {str -> column value}\n        :param params: values for parameter replacement.  Keys must match\n                       the names used in ``dml``.\n\n        :type param_types: dict[str -> Union[dict, .types.Type]]\n        :param param_types:\n            (Optional) maps explicit types for one or more param values;\n            required if parameters are passed.\n\n        :rtype: Union[None, :class:`Struct`]\n        :returns: a struct message for the passed params, or None\n        :raises ValueError:\n            If ``param_types`` is None but ``params`` is not None.\n        :raises ValueError:\n            If ``params`` is None but ``param_types`` is not None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_preview(self):\r\n        \"\"\"\"\"\"\r\n        from qtpy.QtPrintSupport import QPrintPreviewDialog\r\n\r\n        editor = self.get_current_editor()\r\n        printer = Printer(mode=QPrinter.HighResolution,\r\n                          header_font=self.get_plugin_font('printer_header'))\r\n        preview = QPrintPreviewDialog(printer, self)\r\n        preview.setWindowFlags(Qt.Window)\r\n        preview.paintRequested.connect(lambda printer: editor.print_(printer))\r\n        self.redirect_stdio.emit(False)\r\n        preview.exec_()\r\n        self.redirect_stdio.emit(True)", "output": "Print preview for current file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_model(num_layers: int = 6,\n               input_size: int = 512,  # Attention size\n               hidden_size: int = 2048,  # FF layer size\n               heads: int = 8,\n               dropout: float = 0.1,\n               return_all_layers: bool = False) -> TransformerEncoder:\n    \"\"\"\"\"\"\n    attn = MultiHeadedAttention(heads, input_size, dropout)\n    ff = PositionwiseFeedForward(input_size, hidden_size, dropout)\n    model = TransformerEncoder(EncoderLayer(input_size, attn, ff, dropout),\n                               num_layers,\n                               return_all_layers=return_all_layers)\n\n    # Initialize parameters with Glorot / fan_avg.\n    for p in model.parameters():\n        if p.dim() > 1:\n            torch.nn.init.xavier_uniform_(p)\n    return model", "output": "Helper: Construct a model from hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_python_worker(self, func, *args, **kwargs):\n        \"\"\"\"\"\"\n        worker = PythonWorker(func, args, kwargs)\n        self._create_worker(worker)\n        return worker", "output": "Create a new python worker instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def can_allow_multiple_input_shapes(spec):\n    \"\"\"\n    \n    \"\"\"\n\n    # First, check that the model actually has a neural network in it\n    try:\n        layers = _get_nn_layers(spec)\n    except:\n        raise Exception('Unable to verify that this model contains a neural network.')\n\n    try:\n        shaper = NeuralNetworkShaper(spec, False)\n    except:\n        raise Exception('Unable to compute shapes for this neural network.')\n\n    inputs = _get_input_names(spec)\n\n    for name in inputs:\n\n        shape_dict = shaper.shape(name)\n        shape = NeuralNetworkMultiArrayShapeRange(shape_dict)\n\n        if (shape.isFlexible()):\n            return True\n\n    return False", "output": "Examines a model specification and determines if it can compute results for more than one output shape.\n\n    :param spec: MLModel\n        The protobuf specification of the model.\n\n    :return: Bool\n        Returns True if the model can allow multiple input shapes, False otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def db_remove(name, user=None, password=None, host=None, port=None):\n    '''\n    \n    '''\n    if not db_exists(name, user, password, host, port):\n        log.info('DB \\'%s\\' does not exist', name)\n        return False\n    client = _client(user=user, password=password, host=host, port=port)\n    return client.delete_database(name)", "output": "Remove a database\n\n    name\n        Database name to remove\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.db_remove <name>\n        salt '*' influxdb08.db_remove <name> <user> <password> <host> <port>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cache_jobs(opts, jid, ret):\n    '''\n    \n    '''\n    serial = salt.payload.Serial(opts=opts)\n\n    fn_ = os.path.join(\n        opts['cachedir'],\n        'minion_jobs',\n        jid,\n        'return.p')\n    jdir = os.path.dirname(fn_)\n    if not os.path.isdir(jdir):\n        os.makedirs(jdir)\n    with salt.utils.files.fopen(fn_, 'w+b') as fp_:\n        fp_.write(serial.dumps(ret))", "output": "Write job information to cache", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        config = cls()\n        config._properties = copy.deepcopy(resource)\n        return config", "output": "Factory: construct a job configuration given its API representation\n\n        :type resource: dict\n        :param resource:\n            An extract job configuration in the same representation as is\n            returned from the API.\n\n        :rtype: :class:`google.cloud.bigquery.job._JobConfig`\n        :returns: Configuration parsed from ``resource``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dirWavFeatureExtractionNoAveraging(dirName, mt_win, mt_step, st_win, st_step):\n    \"\"\"\n    \n    \"\"\"\n\n    all_mt_feats = numpy.array([])\n    signal_idx = numpy.array([])\n    process_times = []\n\n    types = ('*.wav', '*.aif',  '*.aiff', '*.ogg')\n    wav_file_list = []\n    for files in types:\n        wav_file_list.extend(glob.glob(os.path.join(dirName, files)))\n\n    wav_file_list = sorted(wav_file_list)\n\n    for i, wavFile in enumerate(wav_file_list):\n        [fs, x] = audioBasicIO.readAudioFile(wavFile)\n        if isinstance(x, int):\n            continue        \n        \n        x = audioBasicIO.stereo2mono(x)\n        [mt_term_feats, _, _] = mtFeatureExtraction(x, fs, round(mt_win * fs),\n                                                    round(mt_step * fs),\n                                                    round(fs * st_win),\n                                                    round(fs * st_step))\n\n        mt_term_feats = numpy.transpose(mt_term_feats)\n        if len(all_mt_feats) == 0:                # append feature vector\n            all_mt_feats = mt_term_feats\n            signal_idx = numpy.zeros((mt_term_feats.shape[0], ))\n        else:\n            all_mt_feats = numpy.vstack((all_mt_feats, mt_term_feats))\n            signal_idx = numpy.append(signal_idx, i * numpy.ones((mt_term_feats.shape[0], )))\n\n    return (all_mt_feats, signal_idx, wav_file_list)", "output": "This function extracts the mid-term features of the WAVE\n    files of a particular folder without averaging each file.\n\n    ARGUMENTS:\n        - dirName:          the path of the WAVE directory\n        - mt_win, mt_step:    mid-term window and step (in seconds)\n        - st_win, st_step:    short-term window and step (in seconds)\n    RETURNS:\n        - X:                A feature matrix\n        - Y:                A matrix of file labels\n        - filenames:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detach_disk(name=None, kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The detach_Disk action must be called with -a or --action.'\n        )\n\n    if not name:\n        log.error(\n            'Must specify an instance name.'\n        )\n        return False\n    if not kwargs or 'disk_name' not in kwargs:\n        log.error(\n            'Must specify a disk_name to detach.'\n        )\n        return False\n\n    node_name = name\n    disk_name = kwargs['disk_name']\n\n    conn = get_conn()\n    node = conn.ex_get_node(node_name)\n    disk = conn.ex_get_volume(disk_name)\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'detach disk',\n        'salt/cloud/disk/detaching',\n        args={\n            'name': node_name,\n            'disk_name': disk_name,\n        },\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    result = conn.detach_volume(disk, node)\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'detached disk',\n        'salt/cloud/disk/detached',\n        args={\n            'name': node_name,\n            'disk_name': disk_name,\n        },\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n    return result", "output": "Detach a disk from an instance.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a detach_disk myinstance disk_name=mydisk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_groupby_func(name, args, kwargs, allowed=None):\n    \"\"\"\n    \n    \"\"\"\n    if allowed is None:\n        allowed = []\n\n    kwargs = set(kwargs) - set(allowed)\n\n    if len(args) + len(kwargs) > 0:\n        raise UnsupportedFunctionCall((\n            \"numpy operations are not valid \"\n            \"with groupby. Use .groupby(...).\"\n            \"{func}() instead\".format(func=name)))", "output": "'args' and 'kwargs' should be empty, except for allowed\n    kwargs because all of\n    their necessary parameters are explicitly listed in\n    the function signature", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_gzh_info_by_history(text):\n        \"\"\"\n        \"\"\"\n\n        page = etree.HTML(text)\n        profile_area = get_first_of_element(page, '//div[@class=\"profile_info_area\"]')\n\n        profile_img = get_first_of_element(profile_area, 'div[1]/span/img/@src')\n        profile_name = get_first_of_element(profile_area, 'div[1]/div/strong/text()')\n        profile_wechat_id = get_first_of_element(profile_area, 'div[1]/div/p/text()')\n        profile_desc = get_first_of_element(profile_area, 'ul/li[1]/div/text()')\n        profile_principal = get_first_of_element(profile_area, 'ul/li[2]/div/text()')\n\n        return {\n            'wechat_name': profile_name.strip(),\n            'wechat_id': profile_wechat_id.replace('\u5fae\u4fe1\u53f7: ', '').strip('\\n'),\n            'introduction': profile_desc,\n            'authentication': profile_principal,\n            'headimage': profile_img\n        }", "output": "\u4ece \u5386\u53f2\u6d88\u606f\u9875\u7684\u6587\u672c \u63d0\u53d6\u516c\u4f17\u53f7\u4fe1\u606f\n\n        Parameters\n        ----------\n        text : str or unicode\n            \u5386\u53f2\u6d88\u606f\u9875\u7684\u6587\u672c\n\n        Returns\n        -------\n        dict\n            {\n                'wechat_name': '',  # \u540d\u79f0\n                'wechat_id': '',  # \u5fae\u4fe1id\n                'introduction': '',  # \u63cf\u8ff0\n                'authentication': '',  # \u8ba4\u8bc1\n                'headimage': ''  # \u5934\u50cf\n            }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate(self):\n        \"\"\"\n        \n        \"\"\"\n        if not 0.0 <= self._min_percentile < self._max_percentile <= 100.0:\n            raise BadPercentileBounds(\n                min_percentile=self._min_percentile,\n                max_percentile=self._max_percentile,\n                upper_bound=100.0\n            )\n        return super(PercentileFilter, self)._validate()", "output": "Ensure that our percentile bounds are well-formed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_na_arraylike(arr):\n    \"\"\"\n    \n    \"\"\"\n    if is_extension_array_dtype(arr):\n        return arr[notna(arr)]\n    else:\n        return arr[notna(lib.values_from_object(arr))]", "output": "Return array-like containing only true/non-NaN values, possibly empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def human_and_00(X, y, model_generator, method_name):\n    \"\"\" \n    \"\"\"\n    return _human_and(X, model_generator, method_name, False, False)", "output": "AND (false/false)\n\n    This tests how well a feature attribution method agrees with human intuition\n    for an AND operation combined with linear effects. This metric deals\n    specifically with the question of credit allocation for the following function\n    when all three inputs are true:\n    if fever: +2 points\n    if cough: +2 points\n    if fever and cough: +6 points\n\n    transform = \"identity\"\n    sort_order = 0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_chkpt_vars(model_path):\n    \"\"\" \n    \"\"\"\n    model_path = get_checkpoint_path(model_path)\n    reader = tfv1.train.NewCheckpointReader(model_path)\n    var_names = reader.get_variable_to_shape_map().keys()\n    result = {}\n    for n in var_names:\n        result[n] = reader.get_tensor(n)\n    return result", "output": "Load all variables from a checkpoint to a dict.\n\n    Args:\n        model_path(str): path to a checkpoint.\n\n    Returns:\n        dict: a name:value dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_backward_end(self, **kwargs):\n        \"\"\n        if self.clip: nn.utils.clip_grad_norm_(self.learn.model.parameters(), self.clip)", "output": "Clip the gradient before the optimizer step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exponential_sleep_generator(initial, maximum, multiplier=_DEFAULT_DELAY_MULTIPLIER):\n    \"\"\"\n    \"\"\"\n    delay = initial\n    while True:\n        # Introduce jitter by yielding a delay that is uniformly distributed\n        # to average out to the delay time.\n        yield min(random.uniform(0.0, delay * 2.0), maximum)\n        delay = delay * multiplier", "output": "Generates sleep intervals based on the exponential back-off algorithm.\n\n    This implements the `Truncated Exponential Back-off`_ algorithm.\n\n    .. _Truncated Exponential Back-off:\n        https://cloud.google.com/storage/docs/exponential-backoff\n\n    Args:\n        initial (float): The minimum about of time to delay. This must\n            be greater than 0.\n        maximum (float): The maximum about of time to delay.\n        multiplier (float): The multiplier applied to the delay.\n\n    Yields:\n        float: successive sleep intervals.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_env(user, name, value=None):\n    '''\n    \n    '''\n    lst = list_tab(user)\n    for env in lst['env']:\n        if name == env['name']:\n            if value != env['value']:\n                rm_env(user, name)\n                jret = set_env(user, name, value)\n                if jret == 'new':\n                    return 'updated'\n                else:\n                    return jret\n            return 'present'\n    env = {'name': name, 'value': value}\n    lst['env'].append(env)\n    comdat = _write_cron_lines(user, _render_tab(lst))\n    if comdat['retcode']:\n        # Failed to commit, return the error\n        return comdat['stderr']\n    return 'new'", "output": "Set up an environment variable in the crontab.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.set_env root MAILTO user@example.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match(to_match, values, na_sentinel=-1):\n    \"\"\"\n    \n    \"\"\"\n    values = com.asarray_tuplesafe(values)\n    htable, _, values, dtype, ndtype = _get_hashtable_algo(values)\n    to_match, _, _ = _ensure_data(to_match, dtype)\n    table = htable(min(len(to_match), 1000000))\n    table.map_locations(values)\n    result = table.lookup(to_match)\n\n    if na_sentinel != -1:\n\n        # replace but return a numpy array\n        # use a Series because it handles dtype conversions properly\n        from pandas import Series\n        result = Series(result.ravel()).replace(-1, na_sentinel)\n        result = result.values.reshape(result.shape)\n\n    return result", "output": "Compute locations of to_match into values\n\n    Parameters\n    ----------\n    to_match : array-like\n        values to find positions of\n    values : array-like\n        Unique set of values\n    na_sentinel : int, default -1\n        Value to mark \"not found\"\n\n    Examples\n    --------\n\n    Returns\n    -------\n    match : ndarray of integers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pause(jid, state_id=None, duration=None):\n    '''\n    \n    '''\n    minion = salt.minion.MasterMinion(__opts__)\n    minion.functions['state.pause'](jid, state_id, duration)", "output": "Set up a state id pause, this instructs a running state to pause at a given\n    state id. This needs to pass in the jid of the running state and can\n    optionally pass in a duration in seconds.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_fwrule(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The delete_fwrule function must be called with -f or --function.'\n        )\n\n    if not kwargs or 'name' not in kwargs:\n        log.error(\n            'A name must be specified when deleting a firewall rule.'\n        )\n        return False\n\n    name = kwargs['name']\n    conn = get_conn()\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'delete firewall',\n        'salt/cloud/firewall/deleting',\n        args={\n            'name': name,\n        },\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n\n    try:\n        result = conn.ex_destroy_firewall(\n            conn.ex_get_firewall(name)\n        )\n    except ResourceNotFoundError as exc:\n        log.error(\n            'Rule %s was not found. Exception was: %s',\n            name, exc, exc_info_on_loglevel=logging.DEBUG\n        )\n        return False\n\n    __utils__['cloud.fire_event'](\n        'event',\n        'deleted firewall',\n        'salt/cloud/firewall/deleted',\n        args={\n            'name': name,\n        },\n        sock_dir=__opts__['sock_dir'],\n        transport=__opts__['transport']\n    )\n    return result", "output": "Permanently delete a firewall rule.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f delete_fwrule gce name=allow-http", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_header(info, format, encoding, errors):\n        \"\"\"\n        \"\"\"\n        parts = [\n            stn(info.get(\"name\", \"\"), 100, encoding, errors),\n            itn(info.get(\"mode\", 0) & 0o7777, 8, format),\n            itn(info.get(\"uid\", 0), 8, format),\n            itn(info.get(\"gid\", 0), 8, format),\n            itn(info.get(\"size\", 0), 12, format),\n            itn(info.get(\"mtime\", 0), 12, format),\n            b\"        \", # checksum field\n            info.get(\"type\", REGTYPE),\n            stn(info.get(\"linkname\", \"\"), 100, encoding, errors),\n            info.get(\"magic\", POSIX_MAGIC),\n            stn(info.get(\"uname\", \"\"), 32, encoding, errors),\n            stn(info.get(\"gname\", \"\"), 32, encoding, errors),\n            itn(info.get(\"devmajor\", 0), 8, format),\n            itn(info.get(\"devminor\", 0), 8, format),\n            stn(info.get(\"prefix\", \"\"), 155, encoding, errors)\n        ]\n\n        buf = struct.pack(\"%ds\" % BLOCKSIZE, b\"\".join(parts))\n        chksum = calc_chksums(buf[-BLOCKSIZE:])[0]\n        buf = buf[:-364] + (\"%06o\\0\" % chksum).encode(\"ascii\") + buf[-357:]\n        return buf", "output": "Return a header block. info is a dictionary with file\n           information, format must be one of the *_FORMAT constants.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pillar(tgt, delimiter=DEFAULT_TARGET_DELIM):\n    '''\n    \n    '''\n    matchers = salt.loader.matchers(__opts__)\n    try:\n        return matchers['pillar_match.match'](tgt, delimiter=delimiter, opts=__opts__)\n    except Exception as exc:\n        log.exception(exc)\n        return False", "output": "Return True if the minion matches the given pillar target. The\n    ``delimiter`` argument can be used to specify a different delimiter.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' match.pillar 'cheese:foo'\n        salt '*' match.pillar 'clone_url|https://github.com/saltstack/salt.git' delimiter='|'\n\n    delimiter\n        Specify an alternate delimiter to use when traversing a nested dict\n\n        .. versionadded:: 2014.7.0\n\n    delim\n        Specify an alternate delimiter to use when traversing a nested dict\n\n        .. versionadded:: 0.16.4\n        .. deprecated:: 2015.8.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sampling_shap_1000(model, data):\n    \"\"\" \n    \"\"\"\n    return lambda X: SamplingExplainer(model.predict, data).shap_values(X, nsamples=1000)", "output": "IME 1000\n    color = red_blue_circle(0.5)\n    linestyle = dashed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local(self, fun, tgt, **kwargs):\n        '''\n        \n        '''\n        self.client_cache['local'].cmd_async(tgt, fun, **kwargs)", "output": "Wrap LocalClient for running :ref:`execution modules <all-salt.modules>`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_progress(opts, progress, progress_iter, out):\n    '''\n    \n    '''\n    # Look up the outputter\n    try:\n        progress_outputter = salt.loader.outputters(opts)[out]\n    except KeyError:  # Outputter is not loaded\n        log.warning('Progress outputter not available.')\n        return False\n    progress_outputter(progress, progress_iter)", "output": "Update the progress iterator for the given outputter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collect_params(self, select=None):\n        \"\"\"\n        \"\"\"\n        # We need to check here because blocks inside containers are not supported.\n        self._check_container_with_block()\n        ret = ParameterDict(self._params.prefix)\n        if not select:\n            ret.update(self.params)\n        else:\n            pattern = re.compile(select)\n            ret.update({name:value for name, value in self.params.items() if pattern.match(name)})\n        for cld in self._children.values():\n            ret.update(cld.collect_params(select=select))\n        return ret", "output": "Returns a :py:class:`ParameterDict` containing this :py:class:`Block` and all of its\n        children's Parameters(default), also can returns the select :py:class:`ParameterDict`\n        which match some given regular expressions.\n\n        For example, collect the specified parameters in ['conv1_weight', 'conv1_bias', 'fc_weight',\n        'fc_bias']::\n\n            model.collect_params('conv1_weight|conv1_bias|fc_weight|fc_bias')\n\n        or collect all parameters whose names end with 'weight' or 'bias', this can be done\n        using regular expressions::\n\n            model.collect_params('.*weight|.*bias')\n\n        Parameters\n        ----------\n        select : str\n            regular expressions\n\n        Returns\n        -------\n        The selected :py:class:`ParameterDict`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_func(f, name=None, sinceversion=None, doc=None):\n    \"\"\"\n    \n    \"\"\"\n    # See\n    # http://stackoverflow.com/questions/6527633/how-can-i-make-a-deepcopy-of-a-function-in-python\n    fn = types.FunctionType(f.__code__, f.__globals__, name or f.__name__, f.__defaults__,\n                            f.__closure__)\n    # in case f was given attrs (note this dict is a shallow copy):\n    fn.__dict__.update(f.__dict__)\n    if doc is not None:\n        fn.__doc__ = doc\n    if sinceversion is not None:\n        fn = since(sinceversion)(fn)\n    return fn", "output": "Returns a function with same code, globals, defaults, closure, and\n    name (or provide a new name).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_missing_schema_attributes(self):\n        '''\n        \n        '''\n        for attr in [attr for attr in dir(self) if not attr.startswith('__')]:\n            attr_val = getattr(self, attr)\n            if isinstance(getattr(self, attr), SchemaItem) and \\\n               attr not in self._attributes:\n\n                self._attributes.append(attr)", "output": "Adds any missed schema attributes to the _attributes list\n\n        The attributes can be class attributes and they won't be\n        included in the _attributes list automatically", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_board(args):\n    \"\"\"\n    \n    \"\"\"\n    init_config(args)\n\n    # backend service, should import after django settings initialized\n    from backend.collector import CollectorService\n\n    service = CollectorService(\n        args.logdir,\n        args.reload_interval,\n        standalone=False,\n        log_level=args.log_level)\n    service.run()\n\n    # frontend service\n    logger.info(\"Try to start automlboard on port %s\\n\" % args.port)\n    command = [\n        os.path.join(root_path, \"manage.py\"), \"runserver\",\n        \"0.0.0.0:%s\" % args.port, \"--noreload\"\n    ]\n    execute_from_command_line(command)", "output": "Run main entry for AutoMLBoard.\n\n    Args:\n        args: args parsed from command line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def SetAlpha(self, alpha):\n        '''\n        \n        '''\n        self._AlphaChannel = alpha\n        if self._AlphaChannel is not None:\n            self.QT_QMainWindow.setWindowOpacity(self._AlphaChannel)", "output": "Change the window's transparency\n        :param alpha: From 0 to 1 with 0 being completely transparent\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_reserved(self):\n        \"\"\"\n\n        \"\"\"\n        reserved_networks = [IPv6Network('::/8'), IPv6Network('100::/8'),\n                             IPv6Network('200::/7'), IPv6Network('400::/6'),\n                             IPv6Network('800::/5'), IPv6Network('1000::/4'),\n                             IPv6Network('4000::/3'), IPv6Network('6000::/3'),\n                             IPv6Network('8000::/3'), IPv6Network('A000::/3'),\n                             IPv6Network('C000::/3'), IPv6Network('E000::/4'),\n                             IPv6Network('F000::/5'), IPv6Network('F800::/6'),\n                             IPv6Network('FE00::/9')]\n\n        return any(self in x for x in reserved_networks)", "output": "Test if the address is otherwise IETF reserved.\n\n        Returns:\n            A boolean, True if the address is within one of the\n            reserved IPv6 Network ranges.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer_type(self, in_type):\n        \"\"\"\n        \"\"\"\n        return in_type, [in_type[0]]*len(self.list_outputs()), \\\n            [in_type[0]]*len(self.list_auxiliary_states())", "output": "infer_type interface. override to create new operators\n\n        Parameters\n        ----------\n        in_type : list of np.dtype\n            list of argument types in the same order as\n            declared in list_arguments.\n\n        Returns\n        -------\n        in_type : list\n            list of argument types. Can be modified from in_type.\n        out_type : list\n            list of output types calculated from in_type,\n            in the same order as declared in list_outputs.\n        aux_type : Optional, list\n            list of aux types calculated from in_type,\n            in the same order as declared in list_auxiliary_states.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def role_add(user, role):\n    '''\n    \n    '''\n    ret = {}\n\n    ## validate roles\n    roles = role.split(',')\n    known_roles = role_list().keys()\n    valid_roles = [r for r in roles if r in known_roles]\n    log.debug(\n        'rbac.role_add - roles=%s, known_roles=%s, valid_roles=%s',\n        roles,\n        known_roles,\n        valid_roles,\n    )\n\n    ## update user roles\n    if valid_roles:\n        res = __salt__['cmd.run_all']('usermod -R \"{roles}\" {login}'.format(\n            login=user,\n            roles=','.join(set(role_get(user) + valid_roles)),\n        ))\n        if res['retcode'] > 0:\n            ret['Error'] = {\n                'retcode': res['retcode'],\n                'message': res['stderr'] if 'stderr' in res else res['stdout']\n            }\n            return ret\n\n    ## update return value\n    active_roles = role_get(user)\n    for r in roles:\n        if r not in valid_roles:\n            ret[r] = 'Unknown'\n        elif r in active_roles:\n            ret[r] = 'Added'\n        else:\n            ret[r] = 'Failed'\n\n    return ret", "output": "Add role to user\n\n    user : string\n        username\n    role : string\n        role name\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbac.role_add martine netcfg\n        salt '*' rbac.role_add martine netcfg,zfssnap", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_perturbations(i, j, X, increase, theta, clip_min, clip_max):\n  \"\"\"\n  \n  \"\"\"\n  warnings.warn(\n      \"This function is dead code and will be removed on or after 2019-07-18\")\n\n  # perturb our input sample\n  if increase:\n    X[0, i] = np.minimum(clip_max, X[0, i] + theta)\n    X[0, j] = np.minimum(clip_max, X[0, j] + theta)\n  else:\n    X[0, i] = np.maximum(clip_min, X[0, i] - theta)\n    X[0, j] = np.maximum(clip_min, X[0, j] - theta)\n\n  return X", "output": "TensorFlow implementation for apply perturbations to input features based\n  on salency maps\n  :param i: index of first selected feature\n  :param j: index of second selected feature\n  :param X: a matrix containing our input features for our sample\n  :param increase: boolean; true if we are increasing pixels, false otherwise\n  :param theta: delta for each feature adjustment\n  :param clip_min: mininum value for a feature in our sample\n  :param clip_max: maximum value for a feature in our sample\n  : return: a perturbed input feature matrix for a target class", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_labels(cls, labels, inputCol, outputCol=None, handleInvalid=None):\n        \"\"\"\n        \n        \"\"\"\n        sc = SparkContext._active_spark_context\n        java_class = sc._gateway.jvm.java.lang.String\n        jlabels = StringIndexerModel._new_java_array(labels, java_class)\n        model = StringIndexerModel._create_from_java_class(\n            \"org.apache.spark.ml.feature.StringIndexerModel\", jlabels)\n        model.setInputCol(inputCol)\n        if outputCol is not None:\n            model.setOutputCol(outputCol)\n        if handleInvalid is not None:\n            model.setHandleInvalid(handleInvalid)\n        return model", "output": "Construct the model directly from an array of label strings,\n        requires an active SparkContext.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accept_changes(self):\r\n        \"\"\"\"\"\"\r\n        for (i, j), value in list(self.model.changes.items()):\r\n            self.data[i, j] = value\r\n        if self.old_data_shape is not None:\r\n            self.data.shape = self.old_data_shape", "output": "Accept changes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_positive_impute(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" \n    \"\"\"\n    return __run_measure(measures.remove_impute, X, y, model_generator, method_name, 1, num_fcounts, __mean_pred)", "output": "Remove Positive (impute)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 7", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pgettext(\n        self, context: str, message: str, plural_message: str = None, count: int = None\n    ) -> str:\n        \"\"\"\n        \"\"\"\n        if plural_message is not None:\n            assert count is not None\n            msgs_with_ctxt = (\n                \"%s%s%s\" % (context, CONTEXT_SEPARATOR, message),\n                \"%s%s%s\" % (context, CONTEXT_SEPARATOR, plural_message),\n                count,\n            )\n            result = self.ngettext(*msgs_with_ctxt)\n            if CONTEXT_SEPARATOR in result:\n                # Translation not found\n                result = self.ngettext(message, plural_message, count)\n            return result\n        else:\n            msg_with_ctxt = \"%s%s%s\" % (context, CONTEXT_SEPARATOR, message)\n            result = self.gettext(msg_with_ctxt)\n            if CONTEXT_SEPARATOR in result:\n                # Translation not found\n                result = message\n            return result", "output": "Allows to set context for translation, accepts plural forms.\n\n        Usage example::\n\n            pgettext(\"law\", \"right\")\n            pgettext(\"good\", \"right\")\n\n        Plural message example::\n\n            pgettext(\"organization\", \"club\", \"clubs\", len(clubs))\n            pgettext(\"stick\", \"club\", \"clubs\", len(clubs))\n\n        To generate POT file with context, add following options to step 1\n        of `load_gettext_translations` sequence::\n\n            xgettext [basic options] --keyword=pgettext:1c,2 --keyword=pgettext:1c,2,3\n\n        .. versionadded:: 4.2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stream(self):\n        '''\n        \n        '''\n        if not hasattr(self, '_stream'):\n            self._stream = zmq.eventloop.zmqstream.ZMQStream(self._socket, io_loop=self.io_loop)\n        return self._stream", "output": "Return the current zmqstream, creating one if necessary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def concatenate(arrays, axis=0, always_copy=True):\n    \"\"\"\n    \"\"\"\n    assert isinstance(arrays, list)\n    assert len(arrays) > 0\n    assert isinstance(arrays[0], NDArray)\n\n    if not always_copy and len(arrays) == 1:\n        return arrays[0]\n\n    shape_axis = arrays[0].shape[axis]\n    shape_rest1 = arrays[0].shape[0:axis]\n    shape_rest2 = arrays[0].shape[axis+1:]\n    dtype = arrays[0].dtype\n    for arr in arrays[1:]:\n        shape_axis += arr.shape[axis]\n        assert shape_rest1 == arr.shape[0:axis]\n        assert shape_rest2 == arr.shape[axis+1:]\n        assert dtype == arr.dtype\n    ret_shape = shape_rest1 + (shape_axis,) + shape_rest2\n    ret = empty(ret_shape, ctx=arrays[0].context, dtype=dtype)\n\n    idx = 0\n    begin = [0 for _ in ret_shape]\n    end = list(ret_shape)\n    for arr in arrays:\n        if axis == 0:\n            ret[idx:idx+arr.shape[0]] = arr\n        else:\n            begin[axis] = idx\n            end[axis] = idx+arr.shape[axis]\n            # pylint: disable=no-member,protected-access\n            _internal._crop_assign(ret, arr, out=ret,\n                                   begin=tuple(begin),\n                                   end=tuple(end))\n            # pylint: enable=no-member,protected-access\n        idx += arr.shape[axis]\n\n    return ret", "output": "DEPRECATED, use ``concat`` instead\n\n    Parameters\n    ----------\n    arrays : list of `NDArray`\n        Arrays to be concatenate. They must have identical shape except\n        the first dimension. They also must have the same data type.\n    axis : int\n        The axis along which to concatenate.\n    always_copy : bool\n        Default `True`. When not `True`, if the arrays only contain one\n        `NDArray`, that element will be returned directly, avoid copying.\n\n    Returns\n    -------\n    NDArray\n        An `NDArray` that lives on the same context as `arrays[0].context`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_file_ignored(opts, fname):\n    '''\n    \n    '''\n    if opts['file_ignore_regex']:\n        for regex in opts['file_ignore_regex']:\n            if re.search(regex, fname):\n                log.debug(\n                    'File matching file_ignore_regex. Skipping: %s',\n                    fname\n                )\n                return True\n\n    if opts['file_ignore_glob']:\n        for glob in opts['file_ignore_glob']:\n            if fnmatch.fnmatch(fname, glob):\n                log.debug(\n                    'File matching file_ignore_glob. Skipping: %s',\n                    fname\n                )\n                return True\n    return False", "output": "If file_ignore_regex or file_ignore_glob were given in config,\n    compare the given file path against all of them and return True\n    on the first match.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self):\n        ''' \n\n        '''\n        return HSL(self.h, self.s, self.l, self.a)", "output": "Return a copy of this color value.\n\n        Returns:\n            HSL", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def regen_keys():\n    '''\n    \n    '''\n    for fn_ in os.listdir(__opts__['pki_dir']):\n        path = os.path.join(__opts__['pki_dir'], fn_)\n        try:\n            os.remove(path)\n        except os.error:\n            pass\n    # TODO: move this into a channel function? Or auth?\n    # create a channel again, this will force the key regen\n    channel = salt.transport.client.ReqChannel.factory(__opts__)\n    channel.close()", "output": "Used to regenerate the minion keys.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.regen_keys", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _id_to_subword(self, subword_id):\n    \"\"\"\"\"\"\n    if subword_id < 0 or subword_id >= (self.vocab_size - 1):\n      raise ValueError(\"Received id %d which is invalid. Ids must be within \"\n                       \"[0, %d).\" % (subword_id + 1, self.vocab_size))\n\n    if 0 <= subword_id < len(self._subwords):\n      # Subword\n      return self._subwords[subword_id]\n    else:\n      # Byte\n      offset = len(self._subwords)\n      subword_id -= offset\n      bytestr = bytes(bytearray([subword_id]))\n      return bytestr", "output": "Converts a subword integer ID to a subword string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_range(cls, data, name=None, dtype=None, **kwargs):\n        \"\"\"  \"\"\"\n        if not isinstance(data, range):\n            raise TypeError(\n                '{0}(...) must be called with object coercible to a '\n                'range, {1} was passed'.format(cls.__name__, repr(data)))\n\n        start, stop, step = data.start, data.stop, data.step\n        return RangeIndex(start, stop, step, dtype=dtype, name=name, **kwargs)", "output": "Create RangeIndex from a range object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_namespaced_cron_job_status(self, name, namespace, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_cron_job_status_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.read_namespaced_cron_job_status_with_http_info(name, namespace, **kwargs)\n            return data", "output": "read status of the specified CronJob\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_cron_job_status(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CronJob (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V2alpha1CronJob\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tables(db=None):\n    '''\n    \n    '''\n    cur = _connect(db)\n\n    if not cur:\n        return False\n\n    cur.execute(\n        \"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\"\n    )\n    rows = cur.fetchall()\n    return rows", "output": "Show all tables in the database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sqlite3.tables /root/test.db", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpackb(packed, **kwargs):\n    '''\n    \n    '''\n    msgpack_module = kwargs.pop('_msgpack_module', msgpack)\n    return msgpack_module.unpackb(packed, **kwargs)", "output": ".. versionadded:: 2018.3.4\n\n    Wraps msgpack.unpack.\n\n    By default, this function uses the msgpack module and falls back to\n    msgpack_pure, if the msgpack is not available. You can pass an alternate\n    msgpack module using the _msgpack_module argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_parser(parser):\n    \"\"\"\n    \"\"\"\n    from pandas.core.computation.expr import _parsers\n\n    if parser not in _parsers:\n        raise KeyError('Invalid parser {parser!r} passed, valid parsers are'\n                       ' {valid}'.format(parser=parser, valid=_parsers.keys()))", "output": "Make sure a valid parser is passed.\n\n    Parameters\n    ----------\n    parser : str\n\n    Raises\n    ------\n    KeyError\n      * If an invalid parser is passed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_cookie(cookie: str) -> Dict[str, str]:\n    \"\"\"\n    \"\"\"\n    cookiedict = {}\n    for chunk in cookie.split(str(\";\")):\n        if str(\"=\") in chunk:\n            key, val = chunk.split(str(\"=\"), 1)\n        else:\n            # Assume an empty name per\n            # https://bugzilla.mozilla.org/show_bug.cgi?id=169091\n            key, val = str(\"\"), chunk\n        key, val = key.strip(), val.strip()\n        if key or val:\n            # unquote using Python's algorithm.\n            cookiedict[key] = _unquote_cookie(val)\n    return cookiedict", "output": "Parse a ``Cookie`` HTTP header into a dict of name/value pairs.\n\n    This function attempts to mimic browser cookie parsing behavior;\n    it specifically does not follow any of the cookie-related RFCs\n    (because browsers don't either).\n\n    The algorithm used is identical to that used by Django version 1.9.10.\n\n    .. versionadded:: 4.4.2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option'\n        )\n\n    items = query(action='template')\n    ret = {}\n    for item in items:\n        ret[item.attrib['name']] = item.attrib\n\n    return ret", "output": "Return a list of the images that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def release(self) -> None:\n        \"\"\"\"\"\"\n        if self._value >= self._initial_value:\n            raise ValueError(\"Semaphore released too many times\")\n        super(BoundedSemaphore, self).release()", "output": "Increment the counter and wake one waiter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iterate_with_exp_backoff(base_iter,\n                             max_num_tries=6,\n                             max_backoff=300.0,\n                             start_backoff=4.0,\n                             backoff_multiplier=2.0,\n                             frac_random_backoff=0.25):\n  \"\"\"\n  \"\"\"\n  try_number = 0\n  if hasattr(base_iter, '__iter__'):\n    base_iter = iter(base_iter)\n  while True:\n    try:\n      yield next(base_iter)\n      try_number = 0\n    except StopIteration:\n      break\n    except TooManyRequests as e:\n      logging.warning('TooManyRequests error: %s', tb.format_exc())\n      if try_number >= max_num_tries:\n        logging.error('Number of tries exceeded, too many requests: %s', e)\n        raise\n      # compute sleep time for truncated exponential backoff\n      sleep_time = start_backoff * math.pow(backoff_multiplier, try_number)\n      sleep_time *= (1.0 + frac_random_backoff * random.random())\n      sleep_time = min(sleep_time, max_backoff)\n      logging.warning('Too many requests error, '\n                      'retrying with exponential backoff %.3f', sleep_time)\n      time.sleep(sleep_time)\n      try_number += 1", "output": "Iterate with exponential backoff on failures.\n\n  Useful to wrap results of datastore Query.fetch to avoid 429 error.\n\n  Args:\n    base_iter: basic iterator of generator object\n    max_num_tries: maximum number of tries for each request\n    max_backoff: maximum backoff, in seconds\n    start_backoff: initial value of backoff\n    backoff_multiplier: backoff multiplier\n    frac_random_backoff: fraction of the value of random part of the backoff\n\n  Yields:\n    values of yielded by base iterator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_list_to_colors(names):\n    '''\n    \n    '''\n    # STEP A: compute strings distance between all combnations of strings\n    Dnames = np.zeros( (len(names), len(names)) )\n    for i in range(len(names)):\n        for j in range(len(names)):\n            Dnames[i,j] = 1 - 2.0 * levenshtein(names[i], names[j]) / float(len(names[i]+names[j]))\n\n    # STEP B: pca dimanesionality reduction to a single-dimension (from the distance space)\n    pca = sklearn.decomposition.PCA(n_components = 1)\n    pca.fit(Dnames)    \n    \n    # STEP C: mapping of 1-dimensional values to colors in a jet-colormap\n    textToColor = pca.transform(Dnames)\n    textToColor = 255 * (textToColor - textToColor.min()) / (textToColor.max() - textToColor.min())\n    textmaps = generateColorMap();\n    colors = [textmaps[int(c)] for c in textToColor]\n    return colors", "output": "Generates a list of colors based on a list of names (strings). Similar strings correspond to similar colors.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(key, profile=None):  # pylint: disable=W0613\n    '''\n    \n    '''\n    data = _get_values(profile)\n\n    # Decrypt SDB data if specified in the profile\n    if profile and profile.get('gpg', False):\n        return salt.utils.data.traverse_dict_and_list(_decrypt(data), key, None)\n\n    return salt.utils.data.traverse_dict_and_list(data, key, None)", "output": "Get a value from the dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_pythonw(filename):\r\n    \"\"\"\"\"\"\r\n    pattern = r'.*python(\\d\\.?\\d*)?w(.exe)?$'\r\n    if re.match(pattern, filename, flags=re.I) is None:\r\n        return False\r\n    else:\r\n        return True", "output": "Check that the python interpreter has 'pythonw'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GetErrorHandler(self):\n        \"\"\"\"\"\"\n        f,arg = libxml2mod.xmlTextReaderGetErrorHandler(self._o)\n        if f is None:\n            return None,None\n        else:\n            # assert f is _xmlTextReaderErrorFunc\n            return arg", "output": "Return (f,arg) as previously registered with setErrorHandler\n           or (None,None).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fprop(self, x):\n    \"\"\"\n    \n    \"\"\"\n\n    # Feed forward through the network layers\n    for layer_name in self.layer_names:\n      if layer_name == 'input':\n        prev_layer_act = x\n        continue\n      else:\n        self.layer_acts[layer_name] = self.layers[layer_name](\n            prev_layer_act)\n        prev_layer_act = self.layer_acts[layer_name]\n\n    # Adding softmax values to list of activations.\n    self.layer_acts['probs'] = tf.nn.softmax(\n        logits=self.layer_acts['logits'])\n    return self.layer_acts", "output": "Forward propagation throught the network\n    :return: dictionary with layer names mapping to activation values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _max_fitting_element(self, upper_limit):\n        \"\"\"\"\"\"\n        no_steps = (upper_limit - self._start) // abs(self._step)\n        return self._start + abs(self._step) * no_steps", "output": "Returns the largest element smaller than or equal to the limit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def allreduce_grads(self):\n        \"\"\"\n        \"\"\"\n        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n        assert not (self._kvstore and self._update_on_kvstore), \\\n                'allreduce_grads() when parameters are updated on kvstore ' \\\n                'is not supported. Try setting `update_on_kvstore` ' \\\n                'to False when creating trainer.'\n\n        self._allreduce_grads()", "output": "For each parameter, reduce the gradients from different contexts.\n\n        Should be called after `autograd.backward()`, outside of `record()` scope,\n        and before `trainer.update()`.\n\n        For normal parameter updates, `step()` should be used, which internally calls\n        `allreduce_grads()` and then `update()`. However, if you need to get the reduced\n        gradients to perform certain transformation, such as in gradient clipping, then\n        you may want to manually call `allreduce_grads()` and `update()` separately.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clear_old_remotes():\n    '''\n    \n    '''\n    bp_ = os.path.join(__opts__['cachedir'], 'svnfs')\n    try:\n        cachedir_ls = os.listdir(bp_)\n    except OSError:\n        cachedir_ls = []\n    repos = init()\n    # Remove actively-used remotes from list\n    for repo in repos:\n        try:\n            cachedir_ls.remove(repo['hash'])\n        except ValueError:\n            pass\n    to_remove = []\n    for item in cachedir_ls:\n        if item in ('hash', 'refs'):\n            continue\n        path = os.path.join(bp_, item)\n        if os.path.isdir(path):\n            to_remove.append(path)\n    failed = []\n    if to_remove:\n        for rdir in to_remove:\n            try:\n                shutil.rmtree(rdir)\n            except OSError as exc:\n                log.error(\n                    'Unable to remove old svnfs remote cachedir %s: %s',\n                    rdir, exc\n                )\n                failed.append(rdir)\n            else:\n                log.debug('svnfs removed old cachedir %s', rdir)\n    for fdir in failed:\n        to_remove.remove(fdir)\n    return bool(to_remove), repos", "output": "Remove cache directories for remotes no longer configured", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_path_and_array(cls, path, folder, y, classes=None, val_idxs=None, test_name=None,\n            num_workers=8, tfms=(None,None), bs=64):\n        \"\"\" \n        \"\"\"\n        assert not (tfms[0] is None or tfms[1] is None), \"please provide transformations for your train and validation sets\"\n        assert not (os.path.isabs(folder)), \"folder needs to be a relative path\"\n        fnames = np.core.defchararray.add(f'{folder}/', sorted(os.listdir(f'{path}{folder}')))\n        return cls.from_names_and_array(path, fnames, y, classes, val_idxs, test_name,\n                num_workers=num_workers, tfms=tfms, bs=bs)", "output": "Read in images given a sub-folder and their labels given a numpy array\n\n        Arguments:\n            path: a root path of the data (used for storing trained models, precomputed values, etc)\n            folder: a name of the folder in which training images are contained.\n            y: numpy array which contains target labels ordered by filenames.\n            bs: batch size\n            tfms: transformations (for data augmentations). e.g. output of `tfms_from_model`\n            val_idxs: index of images to be used for validation. e.g. output of `get_cv_idxs`.\n                If None, default arguments to get_cv_idxs are used.\n            test_name: a name of the folder which contains test images.\n            num_workers: number of workers\n\n        Returns:\n            ImageClassifierData", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _brief_print_list(lst, limit=7):\n    \"\"\"\"\"\"\n    lst = list(lst)\n    if len(lst) > limit:\n        return _brief_print_list(lst[:limit//2], limit) + ', ..., ' + \\\n            _brief_print_list(lst[-limit//2:], limit)\n    return ', '.join([\"'%s'\"%str(i) for i in lst])", "output": "Print at most `limit` elements of list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def conv(col, fromBase, toBase):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.conv(_to_java_column(col), fromBase, toBase))", "output": "Convert a number in a string column from one base to another.\n\n    >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n    >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n    [Row(hex=u'15')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_minute_message(self, dt, algo, metrics_tracker):\n        \"\"\"\n        \n        \"\"\"\n        rvars = algo.recorded_vars\n\n        minute_message = metrics_tracker.handle_minute_close(\n            dt,\n            self.data_portal,\n        )\n\n        minute_message['minute_perf']['recorded_vars'] = rvars\n        return minute_message", "output": "Get a perf message for the given datetime.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, other, join='left', overwrite=True, filter_func=None,\n               errors='ignore'):\n        \"\"\"\n        \n        \"\"\"\n\n        if not isinstance(other, self._constructor):\n            other = self._constructor(other)\n\n        axis_name = self._info_axis_name\n        axis_values = self._info_axis\n        other = other.reindex(**{axis_name: axis_values})\n\n        for frame in axis_values:\n            self[frame].update(other[frame], join=join, overwrite=overwrite,\n                               filter_func=filter_func, errors=errors)", "output": "Modify Panel in place using non-NA values from other Panel.\n\n        May also use object coercible to Panel. Will align on items.\n\n        Parameters\n        ----------\n        other : Panel, or object coercible to Panel\n            The object from which the caller will be udpated.\n        join : {'left', 'right', 'outer', 'inner'}, default 'left'\n            How individual DataFrames are joined.\n        overwrite : bool, default True\n            If True then overwrite values for common keys in the calling Panel.\n        filter_func : callable(1d-array) -> 1d-array<bool>, default None\n            Can choose to replace values other than NA. Return True for values\n            that should be updated.\n        errors : {'raise', 'ignore'}, default 'ignore'\n            If 'raise', will raise an error if a DataFrame and other both.\n\n            .. versionchanged :: 0.24.0\n               Changed from `raise_conflict=False|True`\n               to `errors='ignore'|'raise'`.\n\n        See Also\n        --------\n        DataFrame.update : Similar method for DataFrames.\n        dict.update : Similar method for dictionaries.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlNodeDumpFile(self, out, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        libxml2mod.htmlNodeDumpFile(out, self._o, cur__o)", "output": "Dump an HTML node, recursive behaviour,children are printed\n           too, and formatting returns are added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def region_option(f):\n    \"\"\"\n    \n    \"\"\"\n    def callback(ctx, param, value):\n        state = ctx.ensure_object(Context)\n        state.region = value\n        return value\n\n    return click.option('--region',\n                        expose_value=False,\n                        help='Set the AWS Region of the service (e.g. us-east-1).',\n                        callback=callback)(f)", "output": "Configures --region option for CLI\n\n    :param f: Callback Function to be passed to Click", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def conflicting_deps(tree):\n    \"\"\"\n\n    \"\"\"\n    conflicting = defaultdict(list)\n    for p, rs in tree.items():\n        for req in rs:\n            if req.is_conflicting():\n                conflicting[p].append(req)\n    return conflicting", "output": "Returns dependencies which are not present or conflict with the\n    requirements of other packages.\n\n    e.g. will warn if pkg1 requires pkg2==2.0 and pkg2==1.0 is installed\n\n    :param tree: the requirements tree (dict)\n    :returns: dict of DistPackage -> list of unsatisfied/unknown ReqPackage\n    :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_approximate_times(times: List[int]) -> List[int]:\n    \"\"\"\n    \n    \"\"\"\n    approximate_times = []\n    for time in times:\n        hour = int(time/HOUR_TO_TWENTY_FOUR) % 24\n        minute = time % HOUR_TO_TWENTY_FOUR\n        approximate_time = datetime.now()\n        approximate_time = approximate_time.replace(hour=hour, minute=minute)\n\n        start_time_range = approximate_time - timedelta(minutes=30)\n        end_time_range = approximate_time + timedelta(minutes=30)\n        approximate_times.extend([start_time_range.hour * HOUR_TO_TWENTY_FOUR + start_time_range.minute,\n                                  end_time_range.hour * HOUR_TO_TWENTY_FOUR + end_time_range.minute])\n\n    return approximate_times", "output": "Given a list of times that follow a word such as ``about``,\n    we return a list of times that could appear in the query as a result\n    of this. For example if ``about 7pm`` appears in the utterance, then\n    we also want to add ``1830`` and ``1930``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minute_to_session(column, close_locs, data, out):\n    \"\"\"\n    \n    \"\"\"\n    if column == 'open':\n        _minute_to_session_open(close_locs, data, out)\n    elif column == 'high':\n        _minute_to_session_high(close_locs, data, out)\n    elif column == 'low':\n        _minute_to_session_low(close_locs, data, out)\n    elif column == 'close':\n        _minute_to_session_close(close_locs, data, out)\n    elif column == 'volume':\n        _minute_to_session_volume(close_locs, data, out)\n    return out", "output": "Resample an array with minute data into an array with session data.\n\n    This function assumes that the minute data is the exact length of all\n    minutes in the sessions in the output.\n\n    Parameters\n    ----------\n    column : str\n        The `open`, `high`, `low`, `close`, or `volume` column.\n    close_locs : array[intp]\n        The locations in `data` which are the market close minutes.\n    data : array[float64|uint32]\n        The minute data to be sampled into session data.\n        The first value should align with the market open of the first session,\n        containing values for all minutes for all sessions. With the last value\n        being the market close of the last session.\n    out : array[float64|uint32]\n        The output array into which to write the sampled sessions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def simulate(self, action):\n    \"\"\"\n    \"\"\"\n    with tf.name_scope(\"environment/simulate\"):\n      if action.dtype in (tf.float16, tf.float32, tf.float64):\n        action = tf.check_numerics(action, \"action\")\n      def step(action):\n        step_response = self._batch_env.step(action)\n        # Current env doesn't return `info`, but EnvProblem does.\n        # TODO(afrozm): The proper way to do this is to make T2TGymEnv return\n        # an empty info return value.\n        if len(step_response) == 3:\n          (observ, reward, done) = step_response\n        else:\n          (observ, reward, done, _) = step_response\n        return (observ, reward.astype(np.float32), done)\n      observ, reward, done = tf.py_func(\n          step, [action],\n          [self.observ_dtype, tf.float32, tf.bool], name=\"step\")\n      reward = tf.check_numerics(reward, \"reward\")\n      reward.set_shape((len(self),))\n      done.set_shape((len(self),))\n      with tf.control_dependencies([self._observ.assign(observ)]):\n        return tf.identity(reward), tf.identity(done)", "output": "Step the batch of environments.\n\n    The results of the step can be accessed from the variables defined below.\n\n    Args:\n      action: Tensor holding the batch of actions to apply.\n\n    Returns:\n      Operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_infer_dtype_type(element):\n    \"\"\"\n    \"\"\"\n    tipo = None\n    if hasattr(element, 'dtype'):\n        tipo = element.dtype\n    elif is_list_like(element):\n        element = np.asarray(element)\n        tipo = element.dtype\n    return tipo", "output": "Try to infer an object's dtype, for use in arithmetic ops\n\n    Uses `element.dtype` if that's available.\n    Objects implementing the iterator protocol are cast to a NumPy array,\n    and from there the array's type is used.\n\n    Parameters\n    ----------\n    element : object\n        Possibly has a `.dtype` attribute, and possibly the iterator\n        protocol.\n\n    Returns\n    -------\n    tipo : type\n\n    Examples\n    --------\n    >>> from collections import namedtuple\n    >>> Foo = namedtuple(\"Foo\", \"dtype\")\n    >>> maybe_infer_dtype_type(Foo(np.dtype(\"i8\")))\n    numpy.int64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_docstring(format_dict):\n    \"\"\"\n    \n    \"\"\"\n    def add_docstring_context(func):\n        def wrapper(*args, **kwargs):\n            return func(*args, **kwargs)\n        wrapper.__doc__ = func.__doc__.format(**format_dict)\n        return wrapper\n    return add_docstring_context", "output": "Format a doc-string on the fly.\n    @arg format_dict: A dictionary to format the doc-strings\n    Example:\n\n        @add_docstring({'context': __doc_string_context})\n        def predict(x):\n            '''\n            {context}\n            >> model.predict(data)\n            '''\n            return x", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_from_npy2d(self, mat, missing):\n        \"\"\"\n        \n        \"\"\"\n        if len(mat.shape) != 2:\n            raise ValueError('Input numpy.ndarray must be 2 dimensional')\n        data = np.array(mat.reshape(mat.size), dtype=np.float32)\n        self.handle = ctypes.c_void_p()\n        _check_call(_LIB.XGDMatrixCreateFromMat(data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n                                                mat.shape[0], mat.shape[1],\n                                                ctypes.c_float(missing),\n                                                ctypes.byref(self.handle)))", "output": "Initialize data from a 2-D numpy matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _peek_table(self):  # type: () -> Tuple[bool, str]\n        \"\"\"\n        \n        \"\"\"\n        # we always want to restore after exiting this scope\n        with self._state(save_marker=True, restore=True):\n            if self._current != \"[\":\n                raise self.parse_error(\n                    InternalParserError,\n                    \"_peek_table() entered on non-bracket character\",\n                )\n\n            # AoT\n            self.inc()\n            is_aot = False\n            if self._current == \"[\":\n                self.inc()\n                is_aot = True\n\n            self.mark()\n\n            while self._current != \"]\" and self.inc():\n                table_name = self.extract()\n\n            return is_aot, table_name", "output": "Peeks ahead non-intrusively by cloning then restoring the\n        initial state of the parser.\n\n        Returns the name of the table about to be parsed,\n        as well as whether it is part of an AoT.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install(module):\n    '''\n    \n    '''\n    ret = {\n        'old': None,\n        'new': None,\n    }\n\n    old_info = show(module)\n\n    cmd = 'cpan -i {0}'.format(module)\n    out = __salt__['cmd.run'](cmd)\n\n    if \"don't know what it is\" in out:\n        ret['error'] = 'CPAN cannot identify this package'\n        return ret\n\n    new_info = show(module)\n    ret['old'] = old_info.get('installed version', None)\n    ret['new'] = new_info['installed version']\n\n    return ret", "output": "Install a Perl module from CPAN\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cpan.install Template::Alloy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_visitor(self, node):\n        \"\"\"\n        \"\"\"\n        method = 'visit_' + node.__class__.__name__\n        return getattr(self, method, None)", "output": "Return the visitor function for this node or `None` if no visitor\n        exists for this node.  In that case the generic visit function is\n        used instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _walk(r):\n    '''\n    \n    '''\n    if not r.dir:\n        return [r.key.split('/', 3)[3]]\n\n    keys = []\n    for c in client.read(r.key).children:\n        keys.extend(_walk(c))\n    return keys", "output": "Recursively walk dirs. Return flattened list of keys.\n    r: etcd.EtcdResult", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock(self):\n        '''\n        \n        '''\n        success = []\n        failed = []\n        try:\n            result = self._lock(lock_type='update')\n        except GitLockError as exc:\n            failed.append(exc.strerror)\n        else:\n            if result is not None:\n                success.append(result)\n        return success, failed", "output": "Place an lock file and report on the success/failure. This is an\n        interface to be used by the fileserver runner, so it is hard-coded to\n        perform an update lock. We aren't using the gen_lock()\n        contextmanager here because the lock is meant to stay and not be\n        automatically removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def requote_uri(uri):\n    \"\"\"\n    \"\"\"\n    safe_with_percent = \"!#$%&'()*+,/:;=?@[]~\"\n    safe_without_percent = \"!#$&'()*+,/:;=?@[]~\"\n    try:\n        # Unquote only the unreserved characters\n        # Then quote only illegal characters (do not quote reserved,\n        # unreserved, or '%')\n        return quote(unquote_unreserved(uri), safe=safe_with_percent)\n    except InvalidURL:\n        # We couldn't unquote the given URI, so let's try quoting it, but\n        # there may be unquoted '%'s in the URI. We need to make sure they're\n        # properly quoted so they do not cause issues elsewhere.\n        return quote(uri, safe=safe_without_percent)", "output": "Re-quote the given URI.\n\n    This function passes the given URI through an unquote/quote cycle to\n    ensure that it is fully and consistently quoted.\n\n    :rtype: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_model(modelname, add_sentencizer=False):\n    \"\"\"  \"\"\"\n    loading_start = time.time()\n    nlp = spacy.load(modelname)\n    if add_sentencizer:\n        nlp.add_pipe(nlp.create_pipe('sentencizer'))\n    loading_end = time.time()\n    loading_time = loading_end - loading_start\n    if add_sentencizer:\n        return nlp, loading_time, modelname + '_sentencizer'\n    return nlp, loading_time, modelname", "output": "Load a specific spaCy model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def function_table(self, function_id=None):\n        \"\"\"\n        \"\"\"\n        self._check_connected()\n        function_table_keys = self.redis_client.keys(\n            ray.gcs_utils.FUNCTION_PREFIX + \"*\")\n        results = {}\n        for key in function_table_keys:\n            info = self.redis_client.hgetall(key)\n            function_info_parsed = {\n                \"DriverID\": binary_to_hex(info[b\"driver_id\"]),\n                \"Module\": decode(info[b\"module\"]),\n                \"Name\": decode(info[b\"name\"])\n            }\n            results[binary_to_hex(info[b\"function_id\"])] = function_info_parsed\n        return results", "output": "Fetch and parse the function table.\n\n        Returns:\n            A dictionary that maps function IDs to information about the\n                function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pearson_correlation_coefficient(predictions, labels, weights_fn=None):\n  \"\"\"\n  \"\"\"\n  del weights_fn\n  _, pearson = tf.contrib.metrics.streaming_pearson_correlation(predictions,\n                                                                labels)\n  return pearson, tf.constant(1.0)", "output": "Calculate pearson correlation coefficient.\n\n  Args:\n    predictions: The raw predictions.\n    labels: The actual labels.\n    weights_fn: Weighting function.\n\n  Returns:\n    The pearson correlation coefficient.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _upload_file(self, file_name, full_path, quiet, request, resources):\n        \"\"\" \n        \"\"\"\n\n        if not quiet:\n            print('Starting upload for file ' + file_name)\n\n        content_length = os.path.getsize(full_path)\n        token = self.dataset_upload_file(full_path, quiet)\n        if token is None:\n            if not quiet:\n                print('Upload unsuccessful: ' + file_name)\n            return True\n        if not quiet:\n            print('Upload successful: ' + file_name + ' (' +\n                  File.get_size(content_length) + ')')\n        upload_file = DatasetUploadFile()\n        upload_file.token = token\n        if resources:\n            for item in resources:\n                if file_name == item.get('path'):\n                    upload_file.description = item.get('description')\n                    if 'schema' in item:\n                        fields = self.get_or_default(item['schema'], 'fields',\n                                                     [])\n                        processed = []\n                        count = 0\n                        for field in fields:\n                            processed.append(self.process_column(field))\n                            processed[count].order = count\n                            count += 1\n                        upload_file.columns = processed\n        request.files.append(upload_file)\n        return False", "output": "Helper function to upload a single file\n            Parameters\n            ==========\n            file_name: name of the file to upload\n            full_path: path to the file to upload\n            request: the prepared request\n            resources: optional file metadata\n            quiet: suppress verbose output\n            :return: True - upload unsuccessful; False - upload successful", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_model_from_link(name, **overrides):\n    \"\"\"\"\"\"\n    path = get_data_path() / name / \"__init__.py\"\n    try:\n        cls = import_file(name, path)\n    except AttributeError:\n        raise IOError(Errors.E051.format(name=name))\n    return cls.load(**overrides)", "output": "Load a model from a shortcut link, or directory in spaCy data path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def host_dns(proxy=None):\n    '''\n    \n    '''\n    if not __opts__.get('napalm_host_dns_grain', False):\n        return\n    device_host = host(proxy=proxy)\n    if device_host:\n        device_host_value = device_host['host']\n        host_dns_ret = {\n            'host_dns': {\n                'A': [],\n                'AAAA': []\n            }\n        }\n        dns_a = salt.utils.dns.lookup(device_host_value, 'A')\n        if dns_a:\n            host_dns_ret['host_dns']['A'] = dns_a\n        dns_aaaa = salt.utils.dns.lookup(device_host_value, 'AAAA')\n        if dns_aaaa:\n            host_dns_ret['host_dns']['AAAA'] = dns_aaaa\n        return host_dns_ret", "output": "Return the DNS information of the host.\n    This grain is a dictionary having two keys:\n\n    - ``A``\n    - ``AAAA``\n\n    .. note::\n        This grain is disabled by default, as the proxy startup may be slower\n        when the lookup fails.\n        The user can enable it using the ``napalm_host_dns_grain`` option (in\n        the pillar or proxy configuration file):\n\n        .. code-block:: yaml\n\n            napalm_host_dns_grain: true\n\n    .. versionadded:: 2017.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'device*' grains.get host_dns\n\n    Output:\n\n    .. code-block:: yaml\n\n        device1:\n            A:\n                - 172.31.9.153\n            AAAA:\n                - fd52:188c:c068::1\n        device2:\n            A:\n                - 172.31.46.249\n            AAAA:\n                - fdca:3b17:31ab::17\n        device3:\n            A:\n                - 172.31.8.167\n            AAAA:\n                - fd0f:9fd6:5fab::1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_data_futuremin_resample(min_data, type_='5min'):\n    \"\"\"\n    \"\"\"\n\n    min_data.tradeime = pd.to_datetime(min_data.tradetime)\n\n    CONVERSION = {\n        'code': 'first',\n        'open': 'first',\n        'high': 'max',\n        'low': 'min',\n        'close': 'last',\n        'trade': 'sum',\n        'tradetime': 'last',\n        'date': 'last'\n    }\n    resx = min_data.resample(\n        type_,\n        closed='right',\n        loffset=type_\n    ).apply(CONVERSION)\n    return resx.dropna().reset_index().set_index(['datetime', 'code'])", "output": "\u671f\u8d27\u5206\u949f\u7ebf\u91c7\u6837\u6210\u5927\u5468\u671f\n\n\n    \u5206\u949f\u7ebf\u91c7\u6837\u6210\u5b50\u7ea7\u522b\u7684\u5206\u949f\u7ebf\n\n    future:\n\n    vol ==> trade\n    amount X", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def edit_distance_matrix(train_x, train_y=None):\n    \"\"\"\n    \"\"\"\n    if train_y is None:\n        ret = np.zeros((train_x.shape[0], train_x.shape[0]))\n        for x_index, x in enumerate(train_x):\n            for y_index, y in enumerate(train_x):\n                if x_index == y_index:\n                    ret[x_index][y_index] = 0\n                elif x_index < y_index:\n                    ret[x_index][y_index] = edit_distance(x, y)\n                else:\n                    ret[x_index][y_index] = ret[y_index][x_index]\n        return ret\n    ret = np.zeros((train_x.shape[0], train_y.shape[0]))\n    for x_index, x in enumerate(train_x):\n        for y_index, y in enumerate(train_y):\n            ret[x_index][y_index] = edit_distance(x, y)\n    return ret", "output": "Calculate the edit distance.\n    Args:\n        train_x: A list of neural architectures.\n        train_y: A list of neural architectures.\n    Returns:\n        An edit-distance matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def increase_i(self):\n        \"\"\"\"\"\"\n        self.i += 1\n        if self.i > self.bracket_id:\n            self.no_more_trial = True", "output": "i means the ith round. Increase i by 1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prefix_shared_name_attributes(meta_graph, absolute_import_scope):\n  \"\"\"\"\"\"\n  shared_name_attr = \"shared_name\"\n  for node in meta_graph.graph_def.node:\n    shared_name_value = node.attr.get(shared_name_attr, None)\n    if shared_name_value and shared_name_value.HasField(\"s\"):\n      if shared_name_value.s:\n        node.attr[shared_name_attr].s = tf.compat.as_bytes(\n            prepend_name_scope(\n                shared_name_value.s, import_scope=absolute_import_scope))", "output": "In-place prefixes shared_name attributes of nodes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_bitransformer_base():\n  \"\"\"\"\"\"\n  hparams = mtf_transformer2_base()\n  hparams.max_length = 256\n  hparams.shared_embedding = True\n  # HYPERPARAMETERS FOR THE LAYER STACKS\n  hparams.add_hparam(\"encoder_layers\", [\"self_att\", \"drd\"] * 6)\n  hparams.add_hparam(\"decoder_layers\", [\"self_att\", \"enc_att\", \"drd\"] * 6)\n  hparams.add_hparam(\"encoder_num_layers\", 6)\n  hparams.add_hparam(\"decoder_num_layers\", 6)\n  # number of heads in multihead attention\n  hparams.add_hparam(\"encoder_num_heads\", 8)\n  hparams.add_hparam(\"decoder_num_heads\", 8)\n  hparams.add_hparam(\"local_attention_radius\", 128)\n\n  # default of 0 for standard transformer behavior\n  # 1 means a single set of keys and values that are read by all query heads\n  hparams.add_hparam(\"encoder_num_memory_heads\", 0)\n  hparams.add_hparam(\"decoder_num_memory_heads\", 0)\n  # share attention keys and values\n  hparams.add_hparam(\"encoder_shared_kv\", False)\n  hparams.add_hparam(\"decoder_shared_kv\", False)\n\n  # Parameters for computing the maximum decode length in beam search.\n  # Maximum decode length is:\n  #    min(max_length,\n  #        decode_length_multiplier * input_length + decode_length_constant)\n  hparams.add_hparam(\"decode_length_multiplier\", 1.5)\n  hparams.add_hparam(\"decode_length_constant\", 10.0)\n  # used during decoding\n  hparams.add_hparam(\"alpha\", 0.6)\n  hparams.sampling_temp = 0.0\n  return hparams", "output": "Machine translation base configuration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_folder(self, basedir):\r\n        \"\"\"\"\"\"\r\n        title = _('')\r\n        subtitle = _('Folder name:')\r\n        self.create_new_folder(basedir, title, subtitle, is_package=False)", "output": "New folder", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detect_current_filename():\n    ''' \n    '''\n    import inspect\n\n    filename = None\n    frame = inspect.currentframe()\n    try:\n        while frame.f_back and frame.f_globals.get('name') != '__main__':\n            frame = frame.f_back\n\n        filename = frame.f_globals.get('__file__')\n    finally:\n        del frame\n\n    return filename", "output": "Attempt to return the filename of the currently running Python process\n\n    Returns None if the filename cannot be detected.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rewards_to_go(rewards, mask, gamma=0.99):\n  \n  \"\"\"\n  B, T = rewards.shape  # pylint: disable=invalid-name,unused-variable\n\n  masked_rewards = rewards * mask  # (B, T)\n\n  # We use the following recurrence relation, derived from the equation above:\n  #\n  # r2g[t+1] = (r2g[t] - r[t]) / gamma\n  #\n  # This means we'll need to calculate r2g[0] first and then r2g[1] and so on ..\n  #\n  # **However** this leads to overflows for long sequences: r2g[t] - r[t] > 0\n  # and gamma < 1.0, so the division keeps increasing.\n  #\n  # So we just run the recurrence in reverse, i.e.\n  #\n  # r2g[t] = r[t] + (gamma*r2g[t+1])\n  #\n  # This is much better, but might have lost updates since the (small) rewards\n  # at earlier time-steps may get added to a (very?) large sum.\n\n  # Compute r2g_{T-1} at the start and then compute backwards in time.\n  r2gs = [masked_rewards[:, -1]]\n\n  # Go from T-2 down to 0.\n  for t in reversed(range(T - 1)):\n    r2gs.append(masked_rewards[:, t] + (gamma * r2gs[-1]))\n\n  # The list should have length T.\n  assert T == len(r2gs)\n\n  # First we stack them in the correct way to make it (B, T), but these are\n  # still from newest (T-1) to oldest (0), so then we flip it on time axis.\n  return np.flip(np.stack(r2gs, axis=1), axis=1)", "output": "r\"\"\"Computes rewards to go.\n\n  Reward to go is defined as follows, the discounted reward that we have to\n  yet collect, going forward from this point, i.e.:\n\n  r2g_t = \\sum_{l=0}^{\\infty} (\\gamma^{l} * reward_{t+l})\n\n  Args:\n    rewards: np.ndarray of shape (B, T) of rewards.\n    mask: np.ndarray of shape (B, T) of mask for the rewards.\n    gamma: float, discount factor.\n\n  Returns:\n    rewards to go, np.ndarray of shape (B, T).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close(self) -> None:\n        \"\"\"\n\n        \"\"\"\n        if self._closed:\n            return\n        self._closed = True\n        if self._instance_cache is not None:\n            cached_val = self._instance_cache.pop(self.io_loop, None)\n            # If there's an object other than self in the instance\n            # cache for our IOLoop, something has gotten mixed up. A\n            # value of None appears to be possible when this is called\n            # from a destructor (HTTPClient.__del__) as the weakref\n            # gets cleared before the destructor runs.\n            if cached_val is not None and cached_val is not self:\n                raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")", "output": "Destroys this HTTP client, freeing any file descriptors used.\n\n        This method is **not needed in normal use** due to the way\n        that `AsyncHTTPClient` objects are transparently reused.\n        ``close()`` is generally only necessary when either the\n        `.IOLoop` is also being closed, or the ``force_instance=True``\n        argument was used when creating the `AsyncHTTPClient`.\n\n        No other methods may be called on the `AsyncHTTPClient` after\n        ``close()``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _value_with_fmt(self, val):\n        \"\"\"\n        \"\"\"\n        fmt = None\n\n        if is_integer(val):\n            val = int(val)\n        elif is_float(val):\n            val = float(val)\n        elif is_bool(val):\n            val = bool(val)\n        elif isinstance(val, datetime):\n            fmt = self.datetime_format\n        elif isinstance(val, date):\n            fmt = self.date_format\n        elif isinstance(val, timedelta):\n            val = val.total_seconds() / float(86400)\n            fmt = '0'\n        else:\n            val = compat.to_str(val)\n\n        return val, fmt", "output": "Convert numpy types to Python types for the Excel writers.\n\n        Parameters\n        ----------\n        val : object\n            Value to be written into cells\n\n        Returns\n        -------\n        Tuple with the first element being the converted value and the second\n            being an optional format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_instances(instance_id=None, name=None, tags=None, region=None,\n                   key=None, keyid=None, profile=None, return_objs=False,\n                   in_states=None, filters=None):\n\n    '''\n    \n\n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        filter_parameters = {'filters': {}}\n\n        if instance_id:\n            filter_parameters['instance_ids'] = [instance_id]\n\n        if name:\n            filter_parameters['filters']['tag:Name'] = name\n\n        if tags:\n            for tag_name, tag_value in six.iteritems(tags):\n                filter_parameters['filters']['tag:{0}'.format(tag_name)] = tag_value\n\n        if filters:\n            filter_parameters['filters'].update(filters)\n\n        reservations = conn.get_all_reservations(**filter_parameters)\n        instances = [i for r in reservations for i in r.instances]\n        log.debug('The filters criteria %s matched the following '\n                  'instances:%s', filter_parameters, instances)\n\n        if in_states:\n            instances = [i for i in instances if i.state in in_states]\n            log.debug(\n                'Limiting instance matches to those in the requested states: %s',\n                instances\n            )\n        if instances:\n            if return_objs:\n                return instances\n            return [instance.id for instance in instances]\n        else:\n            return []\n    except boto.exception.BotoServerError as exc:\n        log.error(exc)\n        return []", "output": "Given instance properties, find and return matching instance ids\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion boto_ec2.find_instances # Lists all instances\n        salt myminion boto_ec2.find_instances name=myinstance\n        salt myminion boto_ec2.find_instances tags='{\"mytag\": \"value\"}'\n        salt myminion boto_ec2.find_instances filters='{\"vpc-id\": \"vpc-12345678\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dihedral(x, dih):\n    \"\"\"  \"\"\"\n    x = np.rot90(x, dih%4)\n    return x if dih<4 else np.fliplr(x)", "output": "Perform any of 8 permutations of 90-degrees rotations or flips for image x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _data_dep_init(self, inputs):\n    \"\"\"\"\"\"\n\n    with tf.variable_scope(\"data_dep_init\"):\n      # Generate data dependent init values\n      activation = self.layer.activation\n      self.layer.activation = None\n      x_init = self.layer.call(inputs)\n      m_init, v_init = tf.moments(x_init, self.norm_axes)\n      scale_init = 1. / tf.sqrt(v_init + 1e-10)\n\n    # Assign data dependent init values\n    self.layer.g = self.layer.g * scale_init\n    self.layer.bias = (-m_init * scale_init)\n    self.layer.activation = activation\n    self.initialized = True", "output": "Data dependent initialization for eager execution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_shadow_vars(avg_grads):\n        \"\"\"\n        \n        \"\"\"\n        ps_var_grads = []\n        for grad, var in avg_grads:\n            assert var.name.startswith('tower'), var.name\n            my_name = '/'.join(var.name.split('/')[1:])\n            my_name = get_op_tensor_name(my_name)[0]\n            new_v = tf.get_variable(my_name, dtype=var.dtype.base_dtype,\n                                    initializer=var.initial_value,\n                                    trainable=True)\n            # (g, v) to be applied, where v is global (ps vars)\n            ps_var_grads.append((grad, new_v))\n        return ps_var_grads", "output": "Create shadow variables on PS, and replace variables in avg_grads\n        by these shadow variables.\n\n        Args:\n            avg_grads: list of (grad, var) tuples", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _interleaved_dtype(\n        blocks: List[Block]\n) -> Optional[Union[np.dtype, ExtensionDtype]]:\n    \"\"\"\n    \"\"\"\n    if not len(blocks):\n        return None\n\n    return find_common_type([b.dtype for b in blocks])", "output": "Find the common dtype for `blocks`.\n\n    Parameters\n    ----------\n    blocks : List[Block]\n\n    Returns\n    -------\n    dtype : Optional[Union[np.dtype, ExtensionDtype]]\n        None is returned when `blocks` is empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flags(self, index):\r\n        \"\"\"\"\"\"\r\n        if not index.isValid():\r\n            return Qt.ItemIsEnabled\r\n        column = index.column()\r\n        if column in [0]:\r\n            return Qt.ItemFlags(Qt.ItemIsEnabled | Qt.ItemIsSelectable |\r\n                                Qt.ItemIsUserCheckable | Qt.ItemIsEditable)\r\n        else:\r\n            return Qt.ItemFlags(Qt.ItemIsEnabled)", "output": "Override Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_encoding_from_headers(headers):\n    \"\"\"\n    \"\"\"\n    if headers and \"Content-Type\" in headers:\n        content_type, params = cgi.parse_header(headers[\"Content-Type\"])\n        if \"charset\" in params:\n            return params['charset']\n    return None", "output": "Determine if we have any encoding information in our headers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        Statement = self.get_model('statement')\n\n        if 'tags' in kwargs:\n            kwargs['tags'] = list(set(kwargs['tags']))\n\n        if 'search_text' not in kwargs:\n            kwargs['search_text'] = self.tagger.get_bigram_pair_string(kwargs['text'])\n\n        if 'search_in_response_to' not in kwargs:\n            if kwargs.get('in_response_to'):\n                kwargs['search_in_response_to'] = self.tagger.get_bigram_pair_string(kwargs['in_response_to'])\n\n        inserted = self.statements.insert_one(kwargs)\n\n        kwargs['id'] = inserted.inserted_id\n\n        return Statement(**kwargs)", "output": "Creates a new statement matching the keyword arguments specified.\n        Returns the created statement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def present(subset=None, show_ip=False, show_ipv4=None):\n    '''\n    \n    '''\n    show_ip = _show_ip_migration(show_ip, show_ipv4)\n    return list_state(subset=subset, show_ip=show_ip)", "output": ".. versionchanged:: 2019.2.0\n        The 'show_ipv4' argument has been renamed to 'show_ip' as it now\n        includes IPv6 addresses for IPv6-connected minions.\n\n    Print a list of all minions that are up according to Salt's presence\n    detection (no commands will be sent to minions)\n\n    subset : None\n        Pass in a CIDR range to filter minions by IP address.\n\n    show_ip : False\n        Also show the IP address each minion is connecting from.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.present", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _proc_gnusparse_00(self, next, pax_headers, buf):\n        \"\"\"\n        \"\"\"\n        offsets = []\n        for match in re.finditer(br\"\\d+ GNU.sparse.offset=(\\d+)\\n\", buf):\n            offsets.append(int(match.group(1)))\n        numbytes = []\n        for match in re.finditer(br\"\\d+ GNU.sparse.numbytes=(\\d+)\\n\", buf):\n            numbytes.append(int(match.group(1)))\n        next.sparse = list(zip(offsets, numbytes))", "output": "Process a GNU tar extended sparse header, version 0.0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_tuple(data, encoding=None, errors='strict', keep=False,\n                 preserve_dict_class=False):\n    '''\n    \n    '''\n    return tuple(\n        encode_list(data, encoding, errors, keep, preserve_dict_class, True))", "output": "Encode all string values to Unicode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kill(self, sig):\n        \"\"\"\n        \"\"\"\n\n        # Same as os.kill, but the pid is given for you.\n        if self.isalive():\n            os.kill(self.pid, sig)", "output": "Send the given signal to the child application.\n\n        In keeping with UNIX tradition it has a misleading name. It does not\n        necessarily kill the child unless you send the right signal. See the\n        :mod:`signal` module for constants representing signal numbers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def task_path(cls, project, location, queue, task):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/queues/{queue}/tasks/{task}\",\n            project=project,\n            location=location,\n            queue=queue,\n            task=task,\n        )", "output": "Return a fully-qualified task string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _quantize(x, params, randomize=True):\n  \"\"\"\"\"\"\n  if not params.quantize:\n    return x\n\n  if not randomize:\n    return tf.bitcast(\n        tf.cast(x / params.quantization_scale, tf.int16), tf.float16)\n\n  abs_x = tf.abs(x)\n  sign_x = tf.sign(x)\n  y = abs_x / params.quantization_scale\n  y = tf.floor(y + tf.random_uniform(common_layers.shape_list(x)))\n  y = tf.minimum(y, tf.int16.max) * sign_x\n  q = tf.bitcast(tf.cast(y, tf.int16), tf.float16)\n  return q", "output": "Quantize x according to params, optionally randomizing the rounding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bind(\n        self,\n        port: int,\n        address: str = None,\n        family: socket.AddressFamily = socket.AF_UNSPEC,\n        backlog: int = 128,\n        reuse_port: bool = False,\n    ) -> None:\n        \"\"\"\n        \"\"\"\n        sockets = bind_sockets(\n            port, address=address, family=family, backlog=backlog, reuse_port=reuse_port\n        )\n        if self._started:\n            self.add_sockets(sockets)\n        else:\n            self._pending_sockets.extend(sockets)", "output": "Binds this server to the given port on the given address.\n\n        To start the server, call `start`. If you want to run this server\n        in a single process, you can call `listen` as a shortcut to the\n        sequence of `bind` and `start` calls.\n\n        Address may be either an IP address or hostname.  If it's a hostname,\n        the server will listen on all IP addresses associated with the\n        name.  Address may be an empty string or None to listen on all\n        available interfaces.  Family may be set to either `socket.AF_INET`\n        or `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise\n        both will be used if available.\n\n        The ``backlog`` argument has the same meaning as for\n        `socket.listen <socket.socket.listen>`. The ``reuse_port`` argument\n        has the same meaning as for `.bind_sockets`.\n\n        This method may be called multiple times prior to `start` to listen\n        on multiple ports or interfaces.\n\n        .. versionchanged:: 4.4\n           Added the ``reuse_port`` argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextPrecedingSibling(self, ctxt):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        ret = libxml2mod.xmlXPathNextPrecedingSibling(ctxt__o, self._o)\n        if ret is None:raise xpathError('xmlXPathNextPrecedingSibling() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"preceding-sibling\" direction\n          The preceding-sibling axis contains the preceding siblings\n          of the context node in reverse document order; the first\n          preceding sibling is first on the axis; the sibling\n           preceding that node is the second on the axis and so on.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_group_dict(user=None, include_default=True):\n    '''\n    \n    '''\n    if HAS_GRP is False or HAS_PWD is False:\n        return {}\n    group_dict = {}\n    group_names = get_group_list(user, include_default=include_default)\n    for group in group_names:\n        group_dict.update({group: grp.getgrnam(group).gr_gid})\n    return group_dict", "output": "Returns a dict of all of the system groups as keys, and group ids\n    as values, of which the user is a member.\n    E.g.: {'staff': 501, 'sudo': 27}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_function(name,\n                   region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    for funcs in __utils__['boto3.paged_call'](conn.list_functions):\n        for func in funcs['Functions']:\n            if func['FunctionName'] == name:\n                return func\n    return None", "output": "Given function name, find and return matching Lambda information.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_internal_url(url):\n    \"\"\"\n    \n    \"\"\"\n    if not url:\n        raise ValueError('Invalid url: %s' % url)\n\n    from .. import _sys_util\n    from . import _file_util\n\n    # Convert Windows paths to Unix-style slashes\n    url = _convert_slashes(url)\n\n    # Try to split the url into (protocol, path).\n    protocol = _file_util.get_protocol(url)\n    is_local = False\n    if protocol in ['http', 'https']:\n        pass\n    elif protocol == 'hdfs':\n        if not _sys_util.get_hadoop_class_path():\n            raise ValueError(\"HDFS URL is not supported because Hadoop not found. Please make hadoop available from PATH or set the environment variable HADOOP_HOME and try again.\")\n    elif protocol == 's3':\n        return _try_inject_s3_credentials(url)\n    elif protocol == '':\n        is_local = True\n    elif (protocol == 'local' or protocol == 'remote'):\n        # local and remote are legacy protocol for separate server process\n        is_local = True\n        # This code assumes local and remote are same machine\n        url = _re.sub(protocol+'://','',url,count=1)\n    else:\n        raise ValueError('Invalid url protocol %s. Supported url protocols are: local, s3://, https:// and hdfs://' % protocol)\n\n    if is_local:\n        url = _os.path.abspath(_os.path.expanduser(url))\n    return url", "output": "Process user input url string with proper normalization\n    For all urls:\n      Expands ~ to $HOME\n    For S3 urls:\n      Returns the s3 URL with credentials filled in using turicreate.aws.get_aws_credential().\n      For example: \"s3://mybucket/foo\" -> \"s3://$AWS_ACCESS_KEY_ID:$AWS_SECRET_ACCESS_KEY:mybucket/foo\".\n    For hdfs urls:\n      Error if hadoop classpath is not set\n    For local file urls:\n      convert slashes for windows sanity\n\n    Parameters\n    ----------\n    string\n        A URL (as described above).\n\n    Raises\n    ------\n    ValueError\n        If a bad url is provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_src(tree_base, source, saltenv='base'):\n    '''\n    \n    '''\n    parsed = _urlparse(source)\n    sbase = os.path.basename(source)\n    dest = os.path.join(tree_base, sbase)\n    if parsed.scheme:\n        __salt__['cp.get_url'](source, dest, saltenv=saltenv)\n    else:\n        shutil.copy(source, dest)", "output": "Get the named sources and place them into the tree_base", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_item_data(self, item):\r\n        \"\"\"\"\"\"\r\n        filename, line_number_str = get_item_user_text(item).split(self.SEP)\r\n        return filename, int(line_number_str)", "output": "Get tree item user data: (filename, line_number)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tree(path='/', profile=None, **kwargs):\n    '''\n    \n    '''\n    client = __utils__['etcd_util.get_conn'](__opts__, profile, **kwargs)\n    return client.tree(path)", "output": ".. versionadded:: 2014.7.0\n\n    Recurse through etcd and return all values.  Returns None on failure.\n\n    CLI Example:\n\n\n    .. code-block:: bash\n\n        salt myminion etcd.tree\n        salt myminion etcd.tree profile=my_etcd_config\n        salt myminion etcd.tree host=127.0.0.1 port=2379\n        salt myminion etcd.tree /path/to/keys profile=my_etcd_config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_info(name, **client_args):\n    '''\n    \n    '''\n    matching_users = (user for user in list_users(**client_args)\n                      if user.get('user') == name)\n\n    try:\n        return next(matching_users)\n    except StopIteration:\n        pass", "output": "Get information about given user.\n\n    name\n        Name of the user for which to get information.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb.user_info <name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clip_boxes(boxes, shape):\n    \"\"\"\n    \n    \"\"\"\n    orig_shape = boxes.shape\n    boxes = boxes.reshape([-1, 4])\n    h, w = shape\n    boxes[:, [0, 1]] = np.maximum(boxes[:, [0, 1]], 0)\n    boxes[:, 2] = np.minimum(boxes[:, 2], w)\n    boxes[:, 3] = np.minimum(boxes[:, 3], h)\n    return boxes.reshape(orig_shape)", "output": "Args:\n        boxes: (...)x4, float\n        shape: h, w", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_error_message(exception_message, task_exception=False):\n    \"\"\"\n    \"\"\"\n    lines = exception_message.split(\"\\n\")\n    if task_exception:\n        # For errors that occur inside of tasks, remove lines 1 and 2 which are\n        # always the same, they just contain information about the worker code.\n        lines = lines[0:1] + lines[3:]\n        pass\n    return \"\\n\".join(lines)", "output": "Improve the formatting of an exception thrown by a remote function.\n\n    This method takes a traceback from an exception and makes it nicer by\n    removing a few uninformative lines and adding some space to indent the\n    remaining lines nicely.\n\n    Args:\n        exception_message (str): A message generated by traceback.format_exc().\n\n    Returns:\n        A string of the formatted exception message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_element_by_class_name(self, name):\n        \"\"\"\n        \n        \"\"\"\n        return self.find_element(by=By.CLASS_NAME, value=name)", "output": "Finds an element by class name.\n\n        :Args:\n         - name: The class name of the element to find.\n\n        :Returns:\n         - WebElement - the element if it was found\n\n        :Raises:\n         - NoSuchElementException - if the element wasn't found\n\n        :Usage:\n            ::\n\n                element = driver.find_element_by_class_name('foo')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cert_base_path(cacert_path=None):\n    '''\n    \n    '''\n    if not cacert_path:\n        cacert_path = __context__.get(\n            'ca.contextual_cert_base_path',\n            __salt__['config.option']('ca.contextual_cert_base_path'))\n    if not cacert_path:\n        cacert_path = __context__.get(\n            'ca.cert_base_path',\n            __salt__['config.option']('ca.cert_base_path'))\n    return cacert_path", "output": "Return the base path for certs from CLI or from options\n\n    cacert_path\n        absolute path to ca certificates root directory\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' tls.cert_base_path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(provider, names, opts=None, **kwargs):\n    '''\n    \n    '''\n    client = _get_client()\n    if isinstance(opts, dict):\n        client.opts.update(opts)\n    info = client.create(provider, names, **kwargs)\n    return info", "output": "Create an instance using Salt Cloud\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.create my-ec2-config myinstance image=ami-1624987f size='t1.micro' ssh_username=ec2-user securitygroup=default delvol_on_destroy=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean(file_, imports):\n    \"\"\"\"\"\"\n    modules_not_imported = compare_modules(file_, imports)\n    re_remove = re.compile(\"|\".join(modules_not_imported))\n    to_write = []\n\n    try:\n        f = open_func(file_, \"r+\")\n    except OSError:\n        logging.error(\"Failed on file: {}\".format(file_))\n        raise\n    else:\n        for i in f.readlines():\n            if re_remove.match(i) is None:\n                to_write.append(i)\n        f.seek(0)\n        f.truncate()\n\n        for i in to_write:\n            f.write(i)\n    finally:\n        f.close()\n\n    logging.info(\"Successfully cleaned up requirements in \" + file_)", "output": "Remove modules that aren't imported in project from file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized and self.optimizer_initialized\n\n        self._params_dirty = True\n        if self._update_on_kvstore:\n            _update_params_on_kvstore(self._exec_group.param_arrays,\n                                      self._exec_group.grad_arrays,\n                                      self._kvstore, self._exec_group.param_names)\n        else:\n            _update_params(self._exec_group.param_arrays,\n                           self._exec_group.grad_arrays,\n                           updater=self._updater,\n                           num_device=len(self._context),\n                           kvstore=self._kvstore,\n                           param_names=self._exec_group.param_names)", "output": "Updates parameters according to the installed optimizer and the gradients computed\n        in the previous forward-backward batch.\n\n        When KVStore is used to update parameters for multi-device or multi-machine training,\n        a copy of the parameters are stored in KVStore. Note that for `row_sparse` parameters,\n        this function does update the copy of parameters in KVStore, but doesn't broadcast the\n        updated parameters to all devices / machines. Please call `prepare` to broadcast\n        `row_sparse` parameters with the next batch of data.\n\n        See Also\n        ----------\n        :meth:`BaseModule.update`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_userdict(self, f):\n        '''\n        \n        '''\n        self.check_initialized()\n        if isinstance(f, string_types):\n            f_name = f\n            f = open(f, 'rb')\n        else:\n            f_name = resolve_filename(f)\n        for lineno, ln in enumerate(f, 1):\n            line = ln.strip()\n            if not isinstance(line, text_type):\n                try:\n                    line = line.decode('utf-8').lstrip('\\ufeff')\n                except UnicodeDecodeError:\n                    raise ValueError('dictionary file %s must be utf-8' % f_name)\n            if not line:\n                continue\n            # match won't be None because there's at least one character\n            word, freq, tag = re_userdict.match(line).groups()\n            if freq is not None:\n                freq = freq.strip()\n            if tag is not None:\n                tag = tag.strip()\n            self.add_word(word, freq, tag)", "output": "Load personalized dict to improve detect rate.\n\n        Parameter:\n            - f : A plain text file contains words and their ocurrences.\n                  Can be a file-like object, or the path of the dictionary file,\n                  whose encoding must be utf-8.\n\n        Structure of dict file:\n        word1 freq1 word_type1\n        word2 freq2 word_type2\n        ...\n        Word type may be ignored", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _key_paren_left(self, text):\n        \"\"\"  \"\"\"\n        self.current_prompt_pos = self.parentWidget()._prompt_pos\n        if self.get_current_line_to_cursor():\n            last_obj = self.get_last_obj()\n            if last_obj and not last_obj.isdigit():\n                self.show_object_info(last_obj)\n        self.insert_text(text)", "output": "Action for '('", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_attacks_data(self):\n    \"\"\"\n    \"\"\"\n    if self.attacks_data_initialized:\n      return\n    # init data from datastore\n    self.submissions.init_from_datastore()\n    self.dataset_batches.init_from_datastore()\n    self.adv_batches.init_from_datastore()\n    # copy dataset locally\n    if not os.path.exists(LOCAL_DATASET_DIR):\n      os.makedirs(LOCAL_DATASET_DIR)\n    eval_lib.download_dataset(self.storage_client, self.dataset_batches,\n                              LOCAL_DATASET_DIR,\n                              os.path.join(LOCAL_DATASET_COPY,\n                                           self.dataset_name, 'images'))\n    # download dataset metadata\n    self.read_dataset_metadata()\n    # mark as initialized\n    self.attacks_data_initialized = True", "output": "Initializes data necessary to execute attacks.\n\n    This method could be called multiple times, only first call does\n    initialization, subsequent calls are noop.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def destroy(self, blocking=False):\n        \"\"\"\n        \n        \"\"\"\n        if self._jbroadcast is None:\n            raise Exception(\"Broadcast can only be destroyed in driver\")\n        self._jbroadcast.destroy(blocking)\n        os.unlink(self._path)", "output": "Destroy all data and metadata related to this broadcast variable.\n        Use this with caution; once a broadcast variable has been destroyed,\n        it cannot be used again.\n\n        .. versionchanged:: 3.0.0\n           Added optional argument `blocking` to specify whether to block until all\n           blocks are deleted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare(self,\n            method=None, url=None, headers=None, files=None, data=None,\n            params=None, auth=None, cookies=None, hooks=None, json=None):\n        \"\"\"\"\"\"\n\n        self.prepare_method(method)\n        self.prepare_url(url, params)\n        self.prepare_headers(headers)\n        self.prepare_cookies(cookies)\n        self.prepare_body(data, files, json)\n        self.prepare_auth(auth, url)\n\n        # Note that prepare_auth must be last to enable authentication schemes\n        # such as OAuth to work on a fully prepared request.\n\n        # This MUST go after prepare_auth. Authenticators could add a hook\n        self.prepare_hooks(hooks)", "output": "Prepares the entire request with the given parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def allocate_ids(self, incomplete_key, num_ids):\n        \"\"\"\n        \"\"\"\n        if not incomplete_key.is_partial:\n            raise ValueError((\"Key is not partial.\", incomplete_key))\n\n        incomplete_key_pb = incomplete_key.to_protobuf()\n        incomplete_key_pbs = [incomplete_key_pb] * num_ids\n\n        response_pb = self._datastore_api.allocate_ids(\n            incomplete_key.project, incomplete_key_pbs\n        )\n        allocated_ids = [\n            allocated_key_pb.path[-1].id for allocated_key_pb in response_pb.keys\n        ]\n        return [\n            incomplete_key.completed_key(allocated_id) for allocated_id in allocated_ids\n        ]", "output": "Allocate a list of IDs from a partial key.\n\n        :type incomplete_key: :class:`google.cloud.datastore.key.Key`\n        :param incomplete_key: Partial key to use as base for allocated IDs.\n\n        :type num_ids: int\n        :param num_ids: The number of IDs to allocate.\n\n        :rtype: list of :class:`google.cloud.datastore.key.Key`\n        :returns: The (complete) keys allocated with ``incomplete_key`` as\n                  root.\n        :raises: :class:`ValueError` if ``incomplete_key`` is not a\n                 partial key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call_plac(f):\n    \"\"\n    name = inspect.currentframe().f_back.f_globals['__name__']\n    if name == '__main__':\n        import plac\n        res = plac.call(f)\n        if callable(res): res()\n    else: return f", "output": "Decorator to create a simple CLI from `func` using `plac`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_write_pb(table, columns, values):\n    \"\"\"\n    \"\"\"\n    return Mutation.Write(\n        table=table, columns=columns, values=_make_list_value_pbs(values)\n    )", "output": "Helper for :meth:`Batch.insert` et aliae.\n\n    :type table: str\n    :param table: Name of the table to be modified.\n\n    :type columns: list of str\n    :param columns: Name of the table columns to be modified.\n\n    :type values: list of lists\n    :param values: Values to be modified.\n\n    :rtype: :class:`google.cloud.spanner_v1.proto.mutation_pb2.Mutation.Write`\n    :returns: Write protobuf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vm_disk_snapshot_create(name, kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The vm_disk_snapshot_create action must be called with -a or --action.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    disk_id = kwargs.get('disk_id', None)\n    description = kwargs.get('description', None)\n\n    if disk_id is None or description is None:\n        raise SaltCloudSystemExit(\n            'The vm_disk_snapshot_create function requires a \\'disk_id\\' and a \\'description\\' '\n            'to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    vm_id = int(get_vm_id(kwargs={'name': name}))\n    response = server.one.vm.disksnapshotcreate(auth,\n                                                vm_id,\n                                                int(disk_id),\n                                                description)\n\n    data = {\n        'action': 'vm.disksnapshotcreate',\n        'created': response[0],\n        'snapshot_id': response[1],\n        'error_code': response[2],\n    }\n\n    return data", "output": "Takes a new snapshot of the disk image.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the VM of which to take the snapshot.\n\n    disk_id\n        The ID of the disk to save.\n\n    description\n        The description for the snapshot.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a vm_disk_snapshot_create my-vm disk_id=0 description=\"My Snapshot Description\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_type(self, policy):\n        \"\"\"\n        \n        \"\"\"\n\n        # Must handle intrinsic functions. Policy could be a primitive type or an intrinsic function\n\n        # Managed policies are either string or an intrinsic function that resolves to a string\n        if isinstance(policy, string_types) or is_instrinsic(policy):\n            return PolicyTypes.MANAGED_POLICY\n\n        # Policy statement is a dictionary with the key \"Statement\" in it\n        if isinstance(policy, dict) and \"Statement\" in policy:\n            return PolicyTypes.POLICY_STATEMENT\n\n        # This could be a policy template then.\n        if self._is_policy_template(policy):\n            return PolicyTypes.POLICY_TEMPLATE\n\n        # Nothing matches. Don't take opinions on how to handle it. Instead just set the appropriate type.\n        return PolicyTypes.UNKNOWN", "output": "Returns the type of the given policy\n\n        :param string or dict policy: Policy data\n        :return PolicyTypes: Type of the given policy. None, if type could not be inferred", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_shortcuts(self, menu):\r\n        \"\"\"\"\"\"\r\n        for element in getattr(self, menu + '_menu_actions'):\r\n            if element and isinstance(element, QAction):\r\n                if element._shown_shortcut is not None:\r\n                    element.setShortcut(element._shown_shortcut)", "output": "Show action shortcuts in menu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_stock_info(code, ip=None, port=None):\n    ''\n    ip, port = get_mainmarket_ip(ip, port)\n    api = TdxHq_API()\n    market_code = _select_market_code(code)\n    with api.connect(ip, port):\n        return api.to_df(api.get_finance_info(market_code, code))", "output": "\u80a1\u7968\u57fa\u672c\u4fe1\u606f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_librispeech_tpu_v2():\n  \"\"\"\"\"\"\n  hparams = transformer_librispeech_v2()\n  update_hparams_for_tpu(hparams)\n\n  hparams.batch_size = 16\n  librispeech.set_librispeech_length_hparams(hparams)\n  return hparams", "output": "HParams for training ASR model on Librispeech on TPU v2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, inputs, begin_state): # pylint: disable=arguments-differ\n        \"\"\"\n        \"\"\"\n        encoded = self.embedding(inputs)\n        length = inputs.shape[0]\n        batch_size = inputs.shape[1]\n        encoded, state = self.encoder.unroll(length, encoded, begin_state,\n                                             layout='TNC', merge_outputs=True)\n        encoded = encoded.reshape((-1, self._projection_size))\n        out = self.decoder(encoded)\n        out = out.reshape((length, batch_size, -1))\n        return out, state", "output": "Implement forward computation.\n\n        Parameters\n        -----------\n        inputs : NDArray\n            input tensor with shape `(sequence_length, batch_size)`\n            when `layout` is \"TNC\".\n        begin_state : list\n            initial recurrent state tensor with length equals to num_layers*2.\n            For each layer the two initial states have shape `(batch_size, num_hidden)`\n            and `(batch_size, num_projection)`\n\n        Returns\n        --------\n        out : NDArray\n            output tensor with shape `(sequence_length, batch_size, vocab_size)`\n              when `layout` is \"TNC\".\n        out_states : list\n            output recurrent state tensor with length equals to num_layers*2.\n            For each layer the two initial states have shape `(batch_size, num_hidden)`\n            and `(batch_size, num_projection)`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_field_mapping(self, fields, index=None, doc_type=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if fields in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'fields'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(index, \"_mapping\", doc_type, \"field\", fields),\n            params=params,\n        )", "output": "Retrieve mapping definition of a specific field.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-field-mapping.html>`_\n\n        :arg fields: A comma-separated list of fields\n        :arg index: A comma-separated list of index names\n        :arg doc_type: A comma-separated list of document types\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg include_defaults: Whether the default mapping values should be\n            returned as well\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg include_type_name: Specify whether requests and responses should include a\n            type name (default: depends on Elasticsearch version).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_detach(b:Tensors, cpu:bool=True):\n    \"\"\n    if is_listy(b): return [to_detach(o, cpu) for o in b]\n    if not isinstance(b,Tensor): return b\n    b = b.detach()\n    return b.cpu() if cpu else b", "output": "Recursively detach lists of tensors in `b `; put them on the CPU if `cpu=True`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def CopyToProto(self, proto):\n    \"\"\"\n    \"\"\"\n    if (self.file is not None and\n        self._serialized_start is not None and\n        self._serialized_end is not None):\n      proto.ParseFromString(self.file.serialized_pb[\n          self._serialized_start:self._serialized_end])\n    else:\n      raise Error('Descriptor does not contain serialization.')", "output": "Copies this to the matching proto in descriptor_pb2.\n\n    Args:\n      proto: An empty proto instance from descriptor_pb2.\n\n    Raises:\n      Error: If self couldnt be serialized, due to to few constructor arguments.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_key(cls, key):\n        \"\"\n        return cls.get().get(key, cls.DEFAULT_CONFIG.get(key,None))", "output": "Get the path to `key` in the config file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_tags(cwd,\n              user=None,\n              password=None,\n              ignore_retcode=False,\n              output_encoding=None):\n    '''\n    \n    '''\n    cwd = _expand_path(cwd, user)\n    command = ['git', 'for-each-ref', '--format', '%(refname:short)',\n               'refs/tags/']\n    return _git_run(command,\n                    cwd=cwd,\n                    user=user,\n                    password=password,\n                    ignore_retcode=ignore_retcode,\n                    output_encoding=output_encoding)['stdout'].splitlines()", "output": ".. versionadded:: 2015.8.0\n\n    Return a list of tags\n\n    cwd\n        The path to the git checkout\n\n    user\n        User under which to run the git command. By default, the command is run\n        by the user under which the minion is running.\n\n    password\n        Windows only. Required when specifying ``user``. This parameter will be\n        ignored on non-Windows platforms.\n\n      .. versionadded:: 2016.3.4\n\n    ignore_retcode : False\n        If ``True``, do not log an error to the minion log if the git command\n        returns a nonzero exit status.\n\n        .. versionadded:: 2015.8.0\n\n    output_encoding\n        Use this option to specify which encoding to use to decode the output\n        from any git commands which are run. This should not be needed in most\n        cases.\n\n        .. note::\n            This should only be needed if the files in the repository were\n            created with filenames using an encoding other than UTF-8 to handle\n            Unicode characters.\n\n        .. versionadded:: 2018.3.1\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt myminion git.list_tags /path/to/repo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sysapi_changed_nilrt():\n    '''\n    \n    '''\n    nisysapi_path = '/usr/local/natinst/share/nisysapi.ini'\n    if os.path.exists(nisysapi_path) and _file_changed_nilrt(nisysapi_path):\n        return True\n\n    restartcheck_state_dir = '/var/lib/salt/restartcheck_state'\n    nisysapi_conf_d_path = \"/usr/lib/{0}/nisysapi/conf.d/experts/\".format(\n        'arm-linux-gnueabi' if 'arm' in __grains__.get('cpuarch') else 'x86_64-linux-gnu'\n    )\n\n    if os.path.exists(nisysapi_conf_d_path):\n        rs_count_file = '{0}/sysapi.conf.d.count'.format(restartcheck_state_dir)\n        if not os.path.exists(rs_count_file):\n            return True\n\n        with salt.utils.files.fopen(rs_count_file, 'r') as fcount:\n            current_nb_files = len(os.listdir(nisysapi_conf_d_path))\n            rs_stored_nb_files = int(fcount.read())\n            if current_nb_files != rs_stored_nb_files:\n                return True\n\n        for fexpert in os.listdir(nisysapi_conf_d_path):\n            if _file_changed_nilrt('{0}/{1}'.format(nisysapi_conf_d_path, fexpert)):\n                return True\n\n    return False", "output": "Besides the normal Linux kernel driver interfaces, NILinuxRT-supported hardware features an\n    extensible, plugin-based device enumeration and configuration interface named \"System API\".\n    When an installed package is extending the API it is very hard to know all repercurssions and\n    actions to be taken, so reboot making sure all drivers are reloaded, hardware reinitialized,\n    daemons restarted, etc.\n\n    Returns:\n             - True/False depending if nisysapi .ini files got modified/touched\n             - False if no nisysapi .ini files exist", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_selection(self):\r\n        \"\"\"\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.clearSelection()\r\n        self.setTextCursor(cursor)", "output": "Clear current selection", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    \n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss", "output": "Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(name, root=None):\n    '''\n    \n    '''\n    if root is not None:\n        getspnam = functools.partial(_getspnam, root=root)\n    else:\n        getspnam = functools.partial(spwd.getspnam)\n\n    try:\n        data = getspnam(name)\n        ret = {\n            'name': data.sp_namp if hasattr(data, 'sp_namp') else data.sp_nam,\n            'passwd': data.sp_pwdp if hasattr(data, 'sp_pwdp') else data.sp_pwd,\n            'lstchg': data.sp_lstchg,\n            'min': data.sp_min,\n            'max': data.sp_max,\n            'warn': data.sp_warn,\n            'inact': data.sp_inact,\n            'expire': data.sp_expire}\n    except KeyError:\n        return {\n            'name': '',\n            'passwd': '',\n            'lstchg': '',\n            'min': '',\n            'max': '',\n            'warn': '',\n            'inact': '',\n            'expire': ''}\n    return ret", "output": "Return information for the specified user\n\n    name\n        User to get the information for\n\n    root\n        Directory to chroot into\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.info root", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self, declarations_str):\n        \"\"\"\n        \"\"\"\n        for decl in declarations_str.split(';'):\n            if not decl.strip():\n                continue\n            prop, sep, val = decl.partition(':')\n            prop = prop.strip().lower()\n            # TODO: don't lowercase case sensitive parts of values (strings)\n            val = val.strip().lower()\n            if sep:\n                yield prop, val\n            else:\n                warnings.warn('Ill-formatted attribute: expected a colon '\n                              'in {decl!r}'.format(decl=decl), CSSWarning)", "output": "Generates (prop, value) pairs from declarations\n\n        In a future version may generate parsed tokens from tinycss/tinycss2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_norm(self,\n                   input_layer=None,\n                   decay=0.999,\n                   scale=False,\n                   epsilon=0.001):\n        \"\"\"\"\"\"\n        if input_layer is None:\n            input_layer = self.top_layer\n        else:\n            self.top_size = None\n        name = \"batchnorm\" + str(self.counts[\"batchnorm\"])\n        self.counts[\"batchnorm\"] += 1\n\n        with tf.variable_scope(name) as scope:\n            if self.use_tf_layers:\n                bn = tf.contrib.layers.batch_norm(\n                    input_layer,\n                    decay=decay,\n                    scale=scale,\n                    epsilon=epsilon,\n                    is_training=self.phase_train,\n                    fused=True,\n                    data_format=self.data_format,\n                    scope=scope)\n            else:\n                bn = self._batch_norm_without_layers(input_layer, decay, scale,\n                                                     epsilon)\n        self.top_layer = bn\n        self.top_size = bn.shape[\n            3] if self.data_format == \"NHWC\" else bn.shape[1]\n        self.top_size = int(self.top_size)\n        return bn", "output": "Adds a Batch Normalization layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compute_and_save_video_metrics(\n    output_dirs, problem_name, video_length, frame_shape):\n  \"\"\"\"\"\"\n  statistics, all_results = compute_video_metrics_from_png_files(\n      output_dirs, problem_name, video_length, frame_shape)\n  for results, output_dir in zip(all_results, output_dirs):\n    save_results(results, output_dir, problem_name)\n\n  parent_dir = os.path.join(output_dirs[0], os.pardir)\n  final_dir = os.path.join(parent_dir, \"decode\")\n  tf.gfile.MakeDirs(parent_dir)\n\n  save_results(statistics, final_dir, problem_name)", "output": "Compute and saves the video metrics.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reorder_for_extension_array_stack(arr, n_rows, n_columns):\n    \"\"\"\n    \n    \"\"\"\n    # final take to get the order correct.\n    # idx is an indexer like\n    # [c0r0, c1r0, c2r0, ...,\n    #  c0r1, c1r1, c2r1, ...]\n    idx = np.arange(n_rows * n_columns).reshape(n_columns, n_rows).T.ravel()\n    return arr.take(idx)", "output": "Re-orders the values when stacking multiple extension-arrays.\n\n    The indirect stacking method used for EAs requires a followup\n    take to get the order correct.\n\n    Parameters\n    ----------\n    arr : ExtensionArray\n    n_rows, n_columns : int\n        The number of rows and columns in the original DataFrame.\n\n    Returns\n    -------\n    taken : ExtensionArray\n        The original `arr` with elements re-ordered appropriately\n\n    Examples\n    --------\n    >>> arr = np.array(['a', 'b', 'c', 'd', 'e', 'f'])\n    >>> _reorder_for_extension_array_stack(arr, 2, 3)\n    array(['a', 'c', 'e', 'b', 'd', 'f'], dtype='<U1')\n\n    >>> _reorder_for_extension_array_stack(arr, 3, 2)\n    array(['a', 'd', 'b', 'e', 'c', 'f'], dtype='<U1')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_origin(self, origin):\n        ''' \n\n        '''\n        from ..util import check_whitelist\n        parsed_origin = urlparse(origin)\n        origin_host = parsed_origin.netloc.lower()\n\n        allowed_hosts = self.application.websocket_origins\n        if settings.allowed_ws_origin():\n            allowed_hosts = set(settings.allowed_ws_origin())\n\n        allowed = check_whitelist(origin_host, allowed_hosts)\n        if allowed:\n            return True\n        else:\n            log.error(\"Refusing websocket connection from Origin '%s'; \\\n                      use --allow-websocket-origin=%s or set BOKEH_ALLOW_WS_ORIGIN=%s to permit this; currently we allow origins %r\",\n                      origin, origin_host, origin_host, allowed_hosts)\n            return False", "output": "Implement a check_origin policy for Tornado to call.\n\n        The supplied origin will be compared to the Bokeh server whitelist. If the\n        origin is not allow, an error will be logged and ``False`` will be returned.\n\n        Args:\n            origin (str) :\n                The URL of the connection origin\n\n        Returns:\n            bool, True if the connection is allowed, False otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_subnet_explicit_route_table(subnet_id, vpc_id, conn=None, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    if not conn:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    if conn:\n        vpc_route_tables = conn.get_all_route_tables(filters={'vpc_id': vpc_id})\n        for vpc_route_table in vpc_route_tables:\n            for rt_association in vpc_route_table.associations:\n                if rt_association.subnet_id == subnet_id and not rt_association.main:\n                    return rt_association.id\n    return None", "output": "helper function to find subnet explicit route table associations\n\n    .. versionadded:: 2016.11.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def utime(self, tarinfo, targetpath):\n        \"\"\"\n        \"\"\"\n        if not hasattr(os, 'utime'):\n            return\n        try:\n            os.utime(targetpath, (tarinfo.mtime, tarinfo.mtime))\n        except EnvironmentError as e:\n            raise ExtractError(\"could not change modification time\")", "output": "Set modification time of targetpath according to tarinfo.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def positional_encoding(inputs,\n                        num_units=None,\n                        zero_pad=True,\n                        scale=True,\n                        scope=\"positional_encoding\",\n                        reuse=None):\n    '''\n    \n    '''\n    Shape = tf.shape(inputs)\n    N = Shape[0]\n    T = Shape[1]\n    num_units = Shape[2]\n    with tf.variable_scope(scope, reuse=reuse):\n        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n\n        # First part of the PE function: sin and cos argument\n        #  Second part, apply the cosine to even columns and sin to odds.\n        X = tf.expand_dims(tf.cast(tf.range(T), tf.float32), axis=1)\n        Y = tf.expand_dims(\n            tf.cast(10000 ** -(2 * tf.range(num_units) / num_units), tf.float32), axis=0)\n        h1 = tf.cast((tf.range(num_units) + 1) % 2, tf.float32)\n        h2 = tf.cast((tf.range(num_units) % 2), tf.float32)\n        position_enc = tf.multiply(X, Y)\n        position_enc = tf.sin(position_enc) * tf.multiply(tf.ones_like(X), h1) + \\\n            tf.cos(position_enc) * tf.multiply(tf.ones_like(X), h2)\n\n        # Convert to a tensor\n        lookup_table = position_enc\n\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n                                      lookup_table[1:, :]), 0)\n        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n\n        if scale:\n            outputs = outputs * tf.sqrt(tf.cast(num_units, tf.float32))\n\n        return outputs", "output": "Return positinal embedding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_api_method(restApiId, resourcePath, httpMethod, authorizationType,\n                      apiKeyRequired=False, requestParameters=None, requestModels=None,\n                      region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        resource = describe_api_resource(restApiId, resourcePath, region=region,\n                                         key=key, keyid=keyid, profile=profile).get('resource')\n        if resource:\n            requestParameters = dict() if requestParameters is None else requestParameters\n            requestModels = dict() if requestModels is None else requestModels\n\n            conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n            method = conn.put_method(restApiId=restApiId, resourceId=resource['id'], httpMethod=httpMethod,\n                                     authorizationType=str(authorizationType), apiKeyRequired=apiKeyRequired,  # future lint: disable=blacklisted-function\n                                     requestParameters=requestParameters, requestModels=requestModels)\n            return {'created': True, 'method': method}\n        return {'created': False, 'error': 'Failed to create method'}\n\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Creates API method for a resource in the given API\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.create_api_method restApiId resourcePath, httpMethod, authorizationType, \\\\\n            apiKeyRequired=False, requestParameters='{\"name\", \"value\"}', requestModels='{\"content-type\", \"value\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_query_params(self, headers_only=False, page_size=None):\n        \"\"\"\n        \"\"\"\n        params = {\"name\": self._project_path, \"filter_\": self.filter}\n\n        params[\"interval\"] = types.TimeInterval()\n        params[\"interval\"].end_time.FromDatetime(self._end_time)\n        if self._start_time:\n            params[\"interval\"].start_time.FromDatetime(self._start_time)\n\n        if (\n            self._per_series_aligner\n            or self._alignment_period_seconds\n            or self._cross_series_reducer\n            or self._group_by_fields\n        ):\n            params[\"aggregation\"] = types.Aggregation(\n                per_series_aligner=self._per_series_aligner,\n                cross_series_reducer=self._cross_series_reducer,\n                group_by_fields=self._group_by_fields,\n                alignment_period={\"seconds\": self._alignment_period_seconds},\n            )\n\n        if headers_only:\n            params[\"view\"] = enums.ListTimeSeriesRequest.TimeSeriesView.HEADERS\n        else:\n            params[\"view\"] = enums.ListTimeSeriesRequest.TimeSeriesView.FULL\n\n        if page_size is not None:\n            params[\"page_size\"] = page_size\n\n        return params", "output": "Return key-value pairs for the list_time_series API call.\n\n        :type headers_only: bool\n        :param headers_only:\n             Whether to omit the point data from the\n             :class:`~google.cloud.monitoring_v3.types.TimeSeries` objects.\n\n        :type page_size: int\n        :param page_size:\n            (Optional) The maximum number of points in each page of results\n            from this request. Non-positive values are ignored. Defaults\n            to a sensible value set by the API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ctc_beam_search_decoder_batch(probs_seq,\n                                  seq_lengths,\n                                  alphabet,\n                                  beam_size,\n                                  num_processes,\n                                  cutoff_prob=1.0,\n                                  cutoff_top_n=40,\n                                  scorer=None):\n    \"\"\"\n    \"\"\"\n    batch_beam_results = swigwrapper.ctc_beam_search_decoder_batch(\n        probs_seq, seq_lengths, alphabet.config_file(), beam_size, num_processes,\n        cutoff_prob, cutoff_top_n, scorer)\n    batch_beam_results = [\n        [(res.probability, alphabet.decode(res.tokens)) for res in beam_results]\n        for beam_results in batch_beam_results\n    ]\n    return batch_beam_results", "output": "Wrapper for the batched CTC beam search decoder.\n\n    :param probs_seq: 3-D list with each element as an instance of 2-D list\n                      of probabilities used by ctc_beam_search_decoder().\n    :type probs_seq: 3-D list\n    :param alphabet: alphabet list.\n    :alphabet: Alphabet\n    :param beam_size: Width for beam search.\n    :type beam_size: int\n    :param num_processes: Number of parallel processes.\n    :type num_processes: int\n    :param cutoff_prob: Cutoff probability in alphabet pruning,\n                        default 1.0, no pruning.\n    :type cutoff_prob: float\n    :param cutoff_top_n: Cutoff number in pruning, only top cutoff_top_n\n                         characters with highest probs in alphabet will be\n                         used in beam search, default 40.\n    :type cutoff_top_n: int\n    :param num_processes: Number of parallel processes.\n    :type num_processes: int\n    :param scorer: External scorer for partially decoded sentence, e.g. word\n                   count or language model.\n    :type scorer: Scorer\n    :return: List of tuples of log probability and sentence as decoding\n             results, in descending order of the probability.\n    :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def img2img_transformer2d_q3():\n  \"\"\"\"\"\"\n  hparams = img2img_transformer2d_q1()\n  hparams.batch_size = 2\n  hparams.query_shape = (8, 16)\n  hparams.memory_flange = (8, 32)\n  return hparams", "output": "Current best hparams for local 2d.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_groups(self) -> Tuple[Optional[str], Optional[int]]:\n        \"\"\"\n        \"\"\"\n        pattern = self.regex.pattern\n        if pattern.startswith(\"^\"):\n            pattern = pattern[1:]\n        if pattern.endswith(\"$\"):\n            pattern = pattern[:-1]\n\n        if self.regex.groups != pattern.count(\"(\"):\n            # The pattern is too complicated for our simplistic matching,\n            # so we can't support reversing it.\n            return None, None\n\n        pieces = []\n        for fragment in pattern.split(\"(\"):\n            if \")\" in fragment:\n                paren_loc = fragment.index(\")\")\n                if paren_loc >= 0:\n                    pieces.append(\"%s\" + fragment[paren_loc + 1 :])\n            else:\n                try:\n                    unescaped_fragment = re_unescape(fragment)\n                except ValueError:\n                    # If we can't unescape part of it, we can't\n                    # reverse this url.\n                    return (None, None)\n                pieces.append(unescaped_fragment)\n\n        return \"\".join(pieces), self.regex.groups", "output": "Returns a tuple (reverse string, group count) for a url.\n\n        For example: Given the url pattern /([0-9]{4})/([a-z-]+)/, this method\n        would return ('/%s/%s/', 2).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def get_authenticated_user(\n        self, http_client: httpclient.AsyncHTTPClient = None\n    ) -> Dict[str, Any]:\n        \"\"\"\n        \"\"\"\n        handler = cast(RequestHandler, self)\n        # Verify the OpenID response via direct request to the OP\n        args = dict(\n            (k, v[-1]) for k, v in handler.request.arguments.items()\n        )  # type: Dict[str, Union[str, bytes]]\n        args[\"openid.mode\"] = u\"check_authentication\"\n        url = self._OPENID_ENDPOINT  # type: ignore\n        if http_client is None:\n            http_client = self.get_auth_http_client()\n        resp = await http_client.fetch(\n            url, method=\"POST\", body=urllib.parse.urlencode(args)\n        )\n        return self._on_authentication_verified(resp)", "output": "Fetches the authenticated user data upon redirect.\n\n        This method should be called by the handler that receives the\n        redirect from the `authenticate_redirect()` method (which is\n        often the same as the one that calls it; in that case you would\n        call `get_authenticated_user` if the ``openid.mode`` parameter\n        is present and `authenticate_redirect` if it is not).\n\n        The result of this method will generally be used to set a cookie.\n\n        .. versionchanged:: 6.0\n\n            The ``callback`` argument was removed. Use the returned\n            awaitable object instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, file_path):\n    \"\"\"\n    \"\"\"\n\n    with tf.io.gfile.GFile(file_path) as f:\n      raw_data = csv.DictReader(f)\n      for row in raw_data:\n        survive_val = row.pop(\"survived\")\n        yield {\n            \"survived\": convert_to_label(survive_val, _SURVIVED_DICT),\n            \"features\": {\n                name: FEATURE_DICT[name][1](value)\n                for name, value in row.items()\n            }\n        }", "output": "Generate features and target given the directory path.\n\n    Args:\n      file_path: path where the csv file is stored\n\n    Yields:\n      The features and the target", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_poolmanager(self, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs):\n        \"\"\"\n        \"\"\"\n        # save these values for pickling\n        self._pool_connections = connections\n        self._pool_maxsize = maxsize\n        self._pool_block = block\n\n        self.poolmanager = PoolManager(num_pools=connections, maxsize=maxsize,\n                                       block=block, strict=True, **pool_kwargs)", "output": "Initializes a urllib3 PoolManager.\n\n        This method should not be called from user code, and is only\n        exposed for use when subclassing the\n        :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.\n\n        :param connections: The number of urllib3 connection pools to cache.\n        :param maxsize: The maximum number of connections to save in the pool.\n        :param block: Block when no free connections are available.\n        :param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def modelpath4file(filename, ext:str='.tgz'):\n    \"\"\n    local_path = URLs.LOCAL_PATH/'models'/filename\n    if local_path.exists() or local_path.with_suffix(ext).exists(): return local_path\n    else: return Config.model_path()/filename", "output": "Return model path to `filename`, checking locally first then in the config file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_target_delegate(\n        self, target: Any, request: httputil.HTTPServerRequest, **target_params: Any\n    ) -> Optional[httputil.HTTPMessageDelegate]:\n        \"\"\"\n        \"\"\"\n        if isinstance(target, Router):\n            return target.find_handler(request, **target_params)\n\n        elif isinstance(target, httputil.HTTPServerConnectionDelegate):\n            assert request.connection is not None\n            return target.start_request(request.server_connection, request.connection)\n\n        elif callable(target):\n            assert request.connection is not None\n            return _CallableAdapter(\n                partial(target, **target_params), request.connection\n            )\n\n        return None", "output": "Returns an instance of `~.httputil.HTTPMessageDelegate` for a\n        Rule's target. This method is called by `~.find_handler` and can be\n        extended to provide additional target types.\n\n        :arg target: a Rule's target.\n        :arg httputil.HTTPServerRequest request: current request.\n        :arg target_params: additional parameters that can be useful\n            for `~.httputil.HTTPMessageDelegate` creation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PluginTagToContent(self, plugin_name):\n    \"\"\"\n    \"\"\"\n    if plugin_name not in self._plugin_to_tag_to_content:\n      raise KeyError('Plugin %r could not be found.' % plugin_name)\n    return self._plugin_to_tag_to_content[plugin_name]", "output": "Returns a dict mapping tags to content specific to that plugin.\n\n    Args:\n      plugin_name: The name of the plugin for which to fetch plugin-specific\n        content.\n\n    Raises:\n      KeyError: if the plugin name is not found.\n\n    Returns:\n      A dict mapping tags to plugin-specific content (which are always strings).\n      Those strings are often serialized protos.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_perms(self):\n        \"\"\"\"\"\"\n        logging.info('Cleaning faulty perms')\n        sesh = self.get_session\n        pvms = (\n            sesh.query(ab_models.PermissionView)\n            .filter(or_(\n                ab_models.PermissionView.permission == None,  # NOQA\n                ab_models.PermissionView.view_menu == None,  # NOQA\n            ))\n        )\n        deleted_count = pvms.delete()\n        sesh.commit()\n        if deleted_count:\n            logging.info('Deleted {} faulty permissions'.format(deleted_count))", "output": "FAB leaves faulty permissions that need to be cleaned up", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, dtrain, iteration, fobj=None):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(dtrain, DMatrix):\n            raise TypeError('invalid training matrix: {}'.format(type(dtrain).__name__))\n        self._validate_features(dtrain)\n\n        if fobj is None:\n            _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, iteration, dtrain.handle))\n        else:\n            pred = self.predict(dtrain)\n            grad, hess = fobj(pred, dtrain)\n            self.boost(dtrain, grad, hess)", "output": "Update for one iteration, with objective function calculated internally.\n\n        Parameters\n        ----------\n        dtrain : DMatrix\n            Training data.\n        iteration : int\n            Current iteration number.\n        fobj : function\n            Customized objective function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def consolidate_predictions(outputs: List[List[str]], sent_tokens: List[Token]) -> Dict[str, List[str]]:\n    \"\"\"\n    \n    \"\"\"\n    pred_dict: Dict[str, List[str]] = {}\n    merged_outputs = [join_mwp(output) for output in outputs]\n    predicate_texts = [get_predicate_text(sent_tokens, tags)\n                       for tags in merged_outputs]\n\n    for pred1_text, tags1 in zip(predicate_texts, merged_outputs):\n        # A flag indicating whether to add tags1 to predictions\n        add_to_prediction = True\n\n        #  Check if this predicate overlaps another predicate\n        for pred2_text, tags2 in pred_dict.items():\n            if predicates_overlap(tags1, tags2):\n                # tags1 overlaps tags2\n                pred_dict[pred2_text] = merge_overlapping_predictions(tags1, tags2)\n                add_to_prediction = False\n\n        # This predicate doesn't overlap - add as a new predicate\n        if add_to_prediction:\n            pred_dict[pred1_text] = tags1\n\n    return pred_dict", "output": "Identify that certain predicates are part of a multiword predicate\n    (e.g., \"decided to run\") in which case, we don't need to return\n    the embedded predicate (\"run\").", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unzip_from_uri(uri, layer_zip_path, unzip_output_dir, progressbar_label):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        get_request = requests.get(uri, stream=True, verify=os.environ.get('AWS_CA_BUNDLE', True))\n\n        with open(layer_zip_path, 'wb') as local_layer_file:\n            file_length = int(get_request.headers['Content-length'])\n\n            with progressbar(file_length, progressbar_label) as p_bar:\n                # Set the chunk size to None. Since we are streaming the request, None will allow the data to be\n                # read as it arrives in whatever size the chunks are received.\n                for data in get_request.iter_content(chunk_size=None):\n                    local_layer_file.write(data)\n                    p_bar.update(len(data))\n\n        # Forcefully set the permissions to 700 on files and directories. This is to ensure the owner\n        # of the files is the only one that can read, write, or execute the files.\n        unzip(layer_zip_path, unzip_output_dir, permission=0o700)\n\n    finally:\n        # Remove the downloaded zip file\n        path_to_layer = Path(layer_zip_path)\n        if path_to_layer.exists():\n            path_to_layer.unlink()", "output": "Download the LayerVersion Zip to the Layer Pkg Cache\n\n    Parameters\n    ----------\n    uri str\n        Uri to download from\n    layer_zip_path str\n        Path to where the content from the uri should be downloaded to\n    unzip_output_dir str\n        Path to unzip the zip to\n    progressbar_label str\n        Label to use in the Progressbar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datasets_download_file(self, owner_slug, dataset_slug, file_name, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_download_file_with_http_info(owner_slug, dataset_slug, file_name, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_download_file_with_http_info(owner_slug, dataset_slug, file_name, **kwargs)  # noqa: E501\n            return data", "output": "Download dataset file  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_download_file(owner_slug, dataset_slug, file_name, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :param str file_name: File name (required)\n        :param str dataset_version_number: Dataset version number\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_(name, mods=None, **kwargs):\n    '''\n    \n    '''\n    if mods:\n        return sls(name, mods, **kwargs)\n    return highstate(name, **kwargs)", "output": ".. versionadded:: 2019.2.0\n\n    Apply states! This function will call highstate or state.sls based on the\n    arguments passed in, ``apply`` is intended to be the main gateway for\n    all state executions.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'docker' docker.apply web01\n        salt 'docker' docker.apply web01 test\n        salt 'docker' docker.apply web01 test,pkgs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mongo_to_object(self, statement_data):\n        \"\"\"\n        \n        \"\"\"\n        Statement = self.get_model('statement')\n\n        statement_data['id'] = statement_data['_id']\n\n        return Statement(**statement_data)", "output": "Return Statement object when given data\n        returned from Mongo DB.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tap_and_hold(self, xcoord, ycoord):\n        \"\"\"\n        \n        \"\"\"\n        self._actions.append(lambda: self._driver.execute(\n            Command.TOUCH_DOWN, {\n                'x': int(xcoord),\n                'y': int(ycoord)}))\n        return self", "output": "Touch down at given coordinates.\n\n        :Args:\n         - xcoord: X Coordinate to touch down.\n         - ycoord: Y Coordinate to touch down.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_clusters(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_clusters function must be called with -f or --function.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    cluster_pool = server.one.clusterpool.info(auth)[1]\n\n    clusters = {}\n    for cluster in _get_xml(cluster_pool):\n        clusters[cluster.find('NAME').text] = _xml_to_dict(cluster)\n\n    return clusters", "output": "Returns a list of clusters in OpenNebula.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_clusters opennebula", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_settings_source(opts, iface_type, enabled, iface):\n    '''\n    \n    '''\n    adapters = salt.utils.odict.OrderedDict()\n    adapters[iface] = salt.utils.odict.OrderedDict()\n\n    adapters[iface]['type'] = iface_type\n\n    adapters[iface]['data'] = salt.utils.odict.OrderedDict()\n    iface_data = adapters[iface]['data']\n    iface_data['sources'] = [opts['source']]\n\n    return adapters", "output": "Filters given options and outputs valid settings for a\n    network interface.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _send(self, message):\n        \"\"\"\n        \n        \"\"\"\n        message['command'] = 'zappa.asynchronous.route_lambda_task'\n        payload = json.dumps(message).encode('utf-8')\n        if len(payload) > LAMBDA_ASYNC_PAYLOAD_LIMIT: # pragma: no cover\n            raise AsyncException(\"Payload too large for async Lambda call\")\n        self.response = self.client.invoke(\n                                    FunctionName=self.lambda_function_name,\n                                    InvocationType='Event', #makes the call async\n                                    Payload=payload\n                                )\n        self.sent = (self.response.get('StatusCode', 0) == 202)", "output": "Given a message, directly invoke the lamdba function for this task.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setClockShowDate(kvalue, **kwargs):\n    '''\n    \n\n    '''\n    if kvalue is not True and kvalue is not False:\n        return False\n    _gsession = _GSettings(user=kwargs.get('user'),\n                           schema='org.gnome.desktop.interface',\n                           key='clock-show-date')\n    return _gsession._set(kvalue)", "output": "Set whether the date is visible in the clock\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gnome.setClockShowDate <True|False> user=<username>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_encoder(self, name:str):\n        \"\"\n        encoder = get_model(self.model)[0]\n        if hasattr(encoder, 'module'): encoder = encoder.module\n        torch.save(encoder.state_dict(), self.path/self.model_dir/f'{name}.pth')", "output": "Save the encoder to `name` inside the model directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def removed_tree(self, dirname):\r\n        \"\"\"\"\"\"\r\n        dirname = osp.abspath(to_text_string(dirname))\r\n        for fname in self.get_filenames():\r\n            if osp.abspath(fname).startswith(dirname):\r\n                self.close_file_from_name(fname)", "output": "Directory was removed in project explorer widget", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_instances(cls,\n                       instances: Iterable['adi.Instance'],\n                       min_count: Dict[str, int] = None,\n                       max_vocab_size: Union[int, Dict[str, int]] = None,\n                       non_padded_namespaces: Iterable[str] = DEFAULT_NON_PADDED_NAMESPACES,\n                       pretrained_files: Optional[Dict[str, str]] = None,\n                       only_include_pretrained_words: bool = False,\n                       tokens_to_add: Dict[str, List[str]] = None,\n                       min_pretrained_embeddings: Dict[str, int] = None) -> 'Vocabulary':\n        \"\"\"\n        \n        \"\"\"\n        logger.info(\"Fitting token dictionary from dataset.\")\n        namespace_token_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))\n        for instance in Tqdm.tqdm(instances):\n            instance.count_vocab_items(namespace_token_counts)\n\n        return cls(counter=namespace_token_counts,\n                   min_count=min_count,\n                   max_vocab_size=max_vocab_size,\n                   non_padded_namespaces=non_padded_namespaces,\n                   pretrained_files=pretrained_files,\n                   only_include_pretrained_words=only_include_pretrained_words,\n                   tokens_to_add=tokens_to_add,\n                   min_pretrained_embeddings=min_pretrained_embeddings)", "output": "Constructs a vocabulary given a collection of `Instances` and some parameters.\n        We count all of the vocabulary items in the instances, then pass those counts\n        and the other parameters, to :func:`__init__`.  See that method for a description\n        of what the other parameters do.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_absolute_impute__r2(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" \n    \"\"\"\n    return __run_measure(measures.remove_impute, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.r2_score)", "output": "Remove Absolute (impute)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"1 - R^2\"\n    transform = \"one_minus\"\n    sort_order = 9", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def paths(self):\n        '''\n        \n        '''\n        paths = self._cfg.get('paths')\n        if not paths:\n            raise ValueError('Paths Object has no values, You need to define them in your swagger file')\n        for path in paths:\n            if not path.startswith('/'):\n                raise ValueError('Path object {0} should start with /. Please fix it'.format(path))\n        return six.iteritems(paths)", "output": "returns an iterator for the relative resource paths specified in the swagger file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_watch(self, id, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n        return self.transport.perform_request(\n            \"DELETE\", _make_path(\"_watcher\", \"watch\", id), params=params\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-delete-watch.html>`_\n\n        :arg id: Watch ID", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_initialize(self, data):\n        '''\n        \n        '''\n        self.tuner.update_search_space(data)\n        send(CommandType.Initialized, '')\n        return True", "output": "data is search space", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_lines_wo_child(parent_regex, child_regex, source='running'):\n    '''\n      \n    '''\n    config_txt = __salt__['net.config'](source=source)['out'][source]\n    return __salt__['ciscoconfparse.find_lines_wo_child'](config=config_txt,\n                                                          parent_regex=parent_regex,\n                                                          child_regex=child_regex)", "output": ".. versionadded:: 2019.2.0\n\n    Return the configuration lines that match the regular expressions from the\n    ``parent_regex`` argument, having the child lines *not* matching\n    ``child_regex``.\n    The configuration is read from the network device interrogated.\n\n    .. note::\n        This function is only available only when the underlying library\n        `ciscoconfparse <http://www.pennington.net/py/ciscoconfparse/index.html>`_\n        is installed. See\n        :py:func:`ciscoconfparse module <salt.modules.ciscoconfparse_mod>` for\n        more details.\n\n    parent_regex\n        The regular expression to match the parent configuration lines against.\n\n    child_regex\n        The regular expression to match the child configuration lines against.\n\n    source: ``running``\n        The configuration type to retrieve from the network device. Default:\n        ``running``. Available options: ``running``, ``startup``, ``candidate``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.config_lines_wo_child '^interface' 'ip address'\n        salt '*' napalm.config_lines_wo_child '^interface' 'shutdown' source=candidate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pyeapi_nxos_api_args(**prev_kwargs):\n    '''\n    \n    '''\n    kwargs = {}\n    napalm_opts = salt.utils.napalm.get_device_opts(__opts__, salt_obj=__salt__)\n    optional_args = napalm_opts['OPTIONAL_ARGS']\n    kwargs['host'] = napalm_opts['HOSTNAME']\n    kwargs['username'] = napalm_opts['USERNAME']\n    kwargs['password'] = napalm_opts['PASSWORD']\n    kwargs['timeout'] = napalm_opts['TIMEOUT']\n    kwargs['transport'] = optional_args.get('transport')\n    kwargs['port'] = optional_args.get('port')\n    kwargs['verify'] = optional_args.get('verify')\n    prev_kwargs.update(kwargs)\n    return prev_kwargs", "output": ".. versionadded:: 2019.2.0\n\n    Return the key-value arguments used for the authentication arguments for the\n    :mod:`pyeapi execution module <salt.module.arista_pyeapi>`.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.pyeapi_nxos_api_args", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_tokens(cls, path:PathOrStr, trn_tok:Collection[Collection[str]], trn_lbls:Collection[Union[int,float]],\n                 val_tok:Collection[Collection[str]], val_lbls:Collection[Union[int,float]], vocab:Vocab=None,\n                 tst_tok:Collection[Collection[str]]=None, classes:Collection[Any]=None, max_vocab:int=60000, min_freq:int=3,\n                 **kwargs) -> DataBunch:\n        \"\"\n        processor = NumericalizeProcessor(vocab=vocab, max_vocab=max_vocab, min_freq=min_freq)\n        src = ItemLists(path, TextList(trn_tok, path=path, processor=processor),\n                        TextList(val_tok, path=path, processor=processor))\n        src = src.label_for_lm() if cls==TextLMDataBunch else src.label_from_lists(trn_lbls, val_lbls, classes=classes)\n        if tst_tok is not None: src.add_test(TextList(tst_tok, path=path))\n        return src.databunch(**kwargs)", "output": "Create a `TextDataBunch` from tokens and labels. `kwargs` are passed to the dataloader creation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"  \"\"\"\n        self.interval_start = timeit.default_timer()\n        self.progbar = Progbar(target=self.interval)\n        self.metrics = []\n        self.infos = []\n        self.info_names = None\n        self.episode_rewards = []", "output": "Reset statistics", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_tensor(f, format=None):  # type: (Union[IO[bytes], Text], Optional[Any]) -> TensorProto\n    '''\n    \n    '''\n    s = _load_bytes(f)\n    return load_tensor_from_string(s, format=format)", "output": "Loads a serialized TensorProto into memory\n\n    @params\n    f can be a file-like object (has \"read\" function) or a string containing a file name\n    format is for future use\n\n    @return\n    Loaded in-memory TensorProto", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(from_file=None, **kwargs):\n    '''\n    \n    '''\n    ret = {}\n    # prepare vmcfg\n    vmcfg = {}\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    for k, v in six.iteritems(kwargs):\n        vmcfg[k] = v\n\n    if from_file:\n        return _create_update_from_file('create', path=from_file)\n    else:\n        return _create_update_from_cfg('create', vmcfg=vmcfg)", "output": "Create a new vm\n\n    from_file : string\n        json file to create the vm from -- if present, all other options will be ignored\n    kwargs : string|int|...\n        options to set for the vm\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.create from_file=/tmp/new_vm.json\n        salt '*' vmadm.create image_uuid='...' alias='...' nics='[{ \"nic_tag\": \"admin\", \"ip\": \"198.51.100.123\", ...}, {...}]' [...]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bind(self, environment):\n        \"\"\"\"\"\"\n        rv = object.__new__(self.__class__)\n        rv.__dict__.update(self.__dict__)\n        rv.environment = environment\n        return rv", "output": "Create a copy of this extension bound to another environment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build(self):\n        \"\"\"\n        \n        \"\"\"\n        keys = self._param_grid.keys()\n        grid_values = self._param_grid.values()\n\n        def to_key_value_pairs(keys, values):\n            return [(key, key.typeConverter(value)) for key, value in zip(keys, values)]\n\n        return [dict(to_key_value_pairs(keys, prod)) for prod in itertools.product(*grid_values)]", "output": "Builds and returns all combinations of parameters specified\n        by the param grid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_cached(self):\n        \"\"\"\n        \"\"\"\n        for remote_function in self._functions_to_export:\n            self._do_export(remote_function)\n        self._functions_to_export = None\n        for info in self._actors_to_export:\n            (key, actor_class_info) = info\n            self._publish_actor_class_to_key(key, actor_class_info)", "output": "Export cached remote functions\n\n        Note: this should be called only once when worker is connected.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def location(self):\n        \"\"\"\"\"\"\n        if self._w3c:\n            old_loc = self._execute(Command.GET_ELEMENT_RECT)['value']\n        else:\n            old_loc = self._execute(Command.GET_ELEMENT_LOCATION)['value']\n        new_loc = {\"x\": round(old_loc['x']),\n                   \"y\": round(old_loc['y'])}\n        return new_loc", "output": "The location of the element in the renderable canvas.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_from_tuple(tup_or_range):\n    \"\"\"\n    \"\"\"\n    if isinstance(tup_or_range, tuple):\n        return from_tuple(tup_or_range)\n    elif isinstance(tup_or_range, range):\n        return tup_or_range\n\n    raise ValueError(\n        'maybe_from_tuple expects a tuple or range, got %r: %r' % (\n            type(tup_or_range).__name__,\n            tup_or_range,\n        ),\n    )", "output": "Convert a tuple into a range but pass ranges through silently.\n\n    This is useful to ensure that input is a range so that attributes may\n    be accessed with `.start`, `.stop` or so that containment checks are\n    constant time.\n\n    Parameters\n    ----------\n    tup_or_range : tuple or range\n        A tuple to pass to from_tuple or a range to return.\n\n    Returns\n    -------\n    range : range\n        The input to convert to a range.\n\n    Raises\n    ------\n    ValueError\n        Raised when the input is not a tuple or a range. ValueError is also\n        raised if the input is a tuple whose length is not 2 or 3.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_objects(Bucket, Delete, MFA=None, RequestPayer=None,\n                   region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    if isinstance(Delete, six.string_types):\n        Delete = salt.utils.json.loads(Delete)\n    if not isinstance(Delete, dict):\n        raise SaltInvocationError(\"Malformed Delete request.\")\n    if 'Objects' not in Delete:\n        raise SaltInvocationError(\"Malformed Delete request.\")\n\n    failed = []\n    objs = Delete['Objects']\n    for i in range(0, len(objs), 1000):\n        chunk = objs[i:i+1000]\n        subset = {'Objects': chunk, 'Quiet': True}\n        try:\n            args = {'Bucket': Bucket}\n            args.update({'MFA': MFA}) if MFA else None\n            args.update({'RequestPayer': RequestPayer}) if RequestPayer else None\n            args.update({'Delete': subset})\n            conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n            ret = conn.delete_objects(**args)\n            failed += ret.get('Errors', [])\n        except ClientError as e:\n            return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}\n\n    if failed:\n        return {'deleted': False, 'failed': failed}\n    else:\n        return {'deleted': True}", "output": "Delete objects in a given S3 bucket.\n\n    Returns {deleted: true} if all objects were deleted\n    and {deleted: false, failed: [key, ...]} otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_objects mybucket '{Objects: [Key: myobject]}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump_data(logdir):\n  \"\"\"\"\"\"\n  # Create a tfevents file in the logdir so it is detected as a run.\n  write_empty_event_file(logdir)\n\n  plugin_logdir = plugin_asset_util.PluginDirectory(\n      logdir, profile_plugin.ProfilePlugin.plugin_name)\n  _maybe_create_directory(plugin_logdir)\n\n  for run in profile_demo_data.RUNS:\n    run_dir = os.path.join(plugin_logdir, run)\n    _maybe_create_directory(run_dir)\n    if run in profile_demo_data.TRACES:\n      with open(os.path.join(run_dir, 'trace'), 'w') as f:\n        proto = trace_events_pb2.Trace()\n        text_format.Merge(profile_demo_data.TRACES[run], proto)\n        f.write(proto.SerializeToString())\n\n    if run not in profile_demo_data.TRACE_ONLY:\n      shutil.copyfile('tensorboard/plugins/profile/profile_demo.op_profile.json',\n                      os.path.join(run_dir, 'op_profile.json'))\n      shutil.copyfile(\n          'tensorboard/plugins/profile/profile_demo.memory_viewer.json',\n          os.path.join(run_dir, 'memory_viewer.json'))\n      shutil.copyfile(\n          'tensorboard/plugins/profile/profile_demo.pod_viewer.json',\n          os.path.join(run_dir, 'pod_viewer.json'))\n      shutil.copyfile(\n          'tensorboard/plugins/profile/profile_demo.google_chart_demo.json',\n          os.path.join(run_dir, 'google_chart_demo.json'))\n\n  # Unsupported tool data should not be displayed.\n  run_dir = os.path.join(plugin_logdir, 'empty')\n  _maybe_create_directory(run_dir)\n  with open(os.path.join(run_dir, 'unsupported'), 'w') as f:\n    f.write('unsupported data')", "output": "Dumps plugin data to the log directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute(opts, data, func, args, kwargs):\n    '''\n    \n    '''\n    cmd = ['sudo',\n           '-u', opts.get('sudo_user'),\n           'salt-call',\n           '--out', 'json',\n           '--metadata',\n           '-c', opts.get('config_dir'),\n           '--',\n           data.get('fun')]\n    if data['fun'] in ('state.sls', 'state.highstate', 'state.apply'):\n        kwargs['concurrent'] = True\n    for arg in args:\n        cmd.append(_cmd_quote(six.text_type(arg)))\n    for key in kwargs:\n        cmd.append(_cmd_quote('{0}={1}'.format(key, kwargs[key])))\n\n    cmd_ret = __salt__['cmd.run_all'](cmd, use_vt=True, python_shell=False)\n\n    if cmd_ret['retcode'] == 0:\n        cmd_meta = salt.utils.json.loads(cmd_ret['stdout'])['local']\n        ret = cmd_meta['return']\n        __context__['retcode'] = cmd_meta.get('retcode', 0)\n    else:\n        ret = cmd_ret['stderr']\n        __context__['retcode'] = cmd_ret['retcode']\n\n    return ret", "output": "Allow for the calling of execution modules via sudo.\n\n    This module is invoked by the minion if the ``sudo_user`` minion config is\n    present.\n\n    Example minion config:\n\n    .. code-block:: yaml\n\n        sudo_user: saltdev\n\n    Once this setting is made, any execution module call done by the minion will be\n    run under ``sudo -u <sudo_user> salt-call``.  For example, with the above\n    minion config,\n\n    .. code-block:: bash\n\n        salt sudo_minion cmd.run 'cat /etc/sudoers'\n\n    is equivalent to\n\n    .. code-block:: bash\n\n        sudo -u saltdev salt-call cmd.run 'cat /etc/sudoers'\n\n    being run on ``sudo_minion``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clean_create_kwargs(**kwargs):\n    '''\n    \n    '''\n    VALID_OPTS = {\n        'name': six.string_types,\n        'image': six.string_types,\n        'flavor': six.string_types,\n        'auto_ip': bool,\n        'ips': list,\n        'ip_pool': six.string_types,\n        'root_volume': six.string_types,\n        'boot_volume': six.string_types,\n        'terminate_volume': bool,\n        'volumes': list,\n        'meta': dict,\n        'files': dict,\n        'reservation_id': six.string_types,\n        'security_groups': list,\n        'key_name': six.string_types,\n        'availability_zone': six.string_types,\n        'block_device_mapping': dict,\n        'block_device_mapping_v2': dict,\n        'nics': list,\n        'scheduler_hints': dict,\n        'config_drive': bool,\n        'disk_config': six.string_types,  # AUTO or MANUAL\n        'admin_pass': six.string_types,\n        'wait': bool,\n        'timeout': int,\n        'reuse_ips': bool,\n        'network': dict,\n        'boot_from_volume': bool,\n        'volume_size': int,\n        'nat_destination': six.string_types,\n        'group': six.string_types,\n        'userdata': six.string_types,\n    }\n    extra = kwargs.pop('extra', {})\n    for key, value in six.iteritems(kwargs.copy()):\n        if key in VALID_OPTS:\n            if isinstance(value, VALID_OPTS[key]):\n                continue\n            log.error('Error %s: %s is not of type %s', key, value, VALID_OPTS[key])\n        kwargs.pop(key)\n    return __utils__['dictupdate.update'](kwargs, extra)", "output": "Sanatize kwargs to be sent to create_server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new_package(self, basedir):\r\n        \"\"\"\"\"\"\r\n        title = _('')\r\n        subtitle = _('Package name:')\r\n        self.create_new_folder(basedir, title, subtitle, is_package=True)", "output": "New package", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_input_schema(self):\n        \"\"\"\"\"\"\n        assert avro, 'avro module required'\n\n        input_target = flatten(self.input())[0]\n        input_fs = input_target.fs if hasattr(input_target, 'fs') else GCSClient()\n        input_uri = self.source_uris()[0]\n        if '*' in input_uri:\n            file_uris = list(input_fs.list_wildcard(input_uri))\n            if file_uris:\n                input_uri = file_uris[0]\n            else:\n                raise RuntimeError('No match for ' + input_uri)\n\n        schema = []\n        exception_reading_schema = []\n\n        def read_schema(fp):\n            # fp contains the file part downloaded thus far. We rely on that the DataFileReader\n            # initializes itself fine as soon as the file header with schema is downloaded, without\n            # requiring the remainder of the file...\n            try:\n                reader = avro.datafile.DataFileReader(fp, avro.io.DatumReader())\n                schema[:] = [reader.datum_reader.writers_schema]\n            except Exception as e:\n                # Save but assume benign unless schema reading ultimately fails. The benign\n                # exception in case of insufficiently big downloaded file part seems to be:\n                # TypeError('ord() expected a character, but string of length 0 found',).\n                exception_reading_schema[:] = [e]\n                return False\n            return True\n\n        input_fs.download(input_uri, 64 * 1024, read_schema).close()\n        if not schema:\n            raise exception_reading_schema[0]\n        return schema[0]", "output": "Arbitrarily picks an object in input and reads the Avro schema from it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_ignore_interrupts(iwtd, owtd, ewtd, timeout=None):\n\n    ''' '''\n\n    # if select() is interrupted by a signal (errno==EINTR) then\n    # we loop back and enter the select() again.\n    if timeout is not None:\n        end_time = time.time() + timeout\n    while True:\n        try:\n            return select.select(iwtd, owtd, ewtd, timeout)\n        except InterruptedError:\n            err = sys.exc_info()[1]\n            if err.args[0] == errno.EINTR:\n                # if we loop back we have to subtract the\n                # amount of time we already waited.\n                if timeout is not None:\n                    timeout = end_time - time.time()\n                    if timeout < 0:\n                        return([], [], [])\n            else:\n                # something else caused the select.error, so\n                # this actually is an exception.\n                raise", "output": "This is a wrapper around select.select() that ignores signals. If\n    select.select raises a select.error exception and errno is an EINTR\n    error then it is ignored. Mainly this is used to ignore sigwinch\n    (terminal resize).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def undeclared_query_parameters(self):\n        \"\"\"\n        \"\"\"\n        parameters = []\n        undeclared = self._job_statistics().get(\"undeclaredQueryParameters\", ())\n\n        for parameter in undeclared:\n            p_type = parameter[\"parameterType\"]\n\n            if \"arrayType\" in p_type:\n                klass = ArrayQueryParameter\n            elif \"structTypes\" in p_type:\n                klass = StructQueryParameter\n            else:\n                klass = ScalarQueryParameter\n\n            parameters.append(klass.from_api_repr(parameter))\n\n        return parameters", "output": "Return undeclared query parameters from job statistics, if present.\n\n        See:\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#statistics.query.undeclaredQueryParameters\n\n        :rtype:\n            list of\n            :class:`~google.cloud.bigquery.ArrayQueryParameter`,\n            :class:`~google.cloud.bigquery.ScalarQueryParameter`, or\n            :class:`~google.cloud.bigquery.StructQueryParameter`\n        :returns: undeclared parameters, or an empty list if the query has\n                  not yet completed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parseURIRaw(str, raw):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlParseURIRaw(str, raw)\n    if ret is None:raise uriError('xmlParseURIRaw() failed')\n    return URI(_obj=ret)", "output": "Parse an URI but allows to keep intact the original\n       fragments.  URI-reference = URI / relative-ref", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_policy_version(policyName, policyVersionId,\n            region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_policy_version(policyName=policyName,\n                                   policyVersionId=policyVersionId)\n        return {'deleted': True}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a policy name and version, delete it.\n\n    Returns {deleted: true} if the policy version was deleted and returns\n    {deleted: false} if the policy version was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.delete_policy_version mypolicy version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def numeric_columns(self, include_bool=True):\n        \"\"\"\n        \"\"\"\n        columns = []\n        for col, dtype in zip(self.columns, self.dtypes):\n            if is_numeric_dtype(dtype) and (\n                include_bool or (not include_bool and dtype != np.bool_)\n            ):\n                columns.append(col)\n        return columns", "output": "Returns the numeric columns of the Manager.\n\n        Returns:\n            List of index names.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def state(self, metric=None, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if index and not metric:\n            metric = '_all'\n        return self.transport.perform_request('GET', _make_path('_cluster',\n            'state', metric, index), params=params)", "output": "Get a comprehensive state information of the whole cluster.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-state.html>`_\n\n        :arg metric: Limit the information returned to the specified metrics\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string to perform the operation on all indices\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'open', valid\n            choices are: 'open', 'closed', 'none', 'all'\n        :arg flat_settings: Return settings in flat format (default: false)\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Specify timeout for connection to master", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_temp_file_location():\n    '''\n    \n\n    '''\n    from .._connect import main as _glconnect\n    unity = _glconnect.get_unity()\n    cache_dir = _convert_slashes(unity.get_current_cache_file_location())\n    if not _os.path.exists(cache_dir):\n        _os.makedirs(cache_dir)\n    return cache_dir", "output": "Returns user specified temporary file location.\n    The temporary location is specified through:\n\n    >>> turicreate.config.set_runtime_config('TURI_CACHE_FILE_LOCATIONS', ...)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def output(data, **kwargs):\n    '''\n    \n    '''\n\n    rows = _find_durations(data)\n\n    kwargs['opts'] = __opts__\n    kwargs['rows_key'] = 'rows'\n    kwargs['labels_key'] = 'labels'\n\n    to_show = {'labels': ['name', 'mod.fun', 'duration (ms)'],\n               'rows':   rows}\n\n    return table_out.output(to_show, **kwargs)", "output": "Display the profiling data in a table format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_flights():\n    \"\"\"\"\"\"\n    tbl_name = 'flights'\n    data = get_example_data('flight_data.csv.gz', make_bytes=True)\n    pdf = pd.read_csv(data, encoding='latin-1')\n\n    # Loading airports info to join and get lat/long\n    airports_bytes = get_example_data('airports.csv.gz', make_bytes=True)\n    airports = pd.read_csv(airports_bytes, encoding='latin-1')\n    airports = airports.set_index('IATA_CODE')\n\n    pdf['ds'] = pdf.YEAR.map(str) + '-0' + pdf.MONTH.map(str) + '-0' + pdf.DAY.map(str)\n    pdf.ds = pd.to_datetime(pdf.ds)\n    del pdf['YEAR']\n    del pdf['MONTH']\n    del pdf['DAY']\n\n    pdf = pdf.join(airports, on='ORIGIN_AIRPORT', rsuffix='_ORIG')\n    pdf = pdf.join(airports, on='DESTINATION_AIRPORT', rsuffix='_DEST')\n    pdf.to_sql(\n        tbl_name,\n        db.engine,\n        if_exists='replace',\n        chunksize=500,\n        dtype={\n            'ds': DateTime,\n        },\n        index=False)\n    tbl = db.session.query(TBL).filter_by(table_name=tbl_name).first()\n    if not tbl:\n        tbl = TBL(table_name=tbl_name)\n    tbl.description = 'Random set of flights in the US'\n    tbl.database = utils.get_or_create_main_db()\n    db.session.merge(tbl)\n    db.session.commit()\n    tbl.fetch_metadata()\n    print('Done loading table!')", "output": "Loading random time series data from a zip file in the repo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stat(self, filename):\n        \"\"\"\"\"\"\n        # NOTE: Size of the file is given by .st_size as returned from\n        # os.stat(), but we convert to .length\n        try:\n            len = os.stat(compat.as_bytes(filename)).st_size\n        except OSError:\n            raise errors.NotFoundError(None, None, \"Could not find file\")\n        return StatData(len)", "output": "Returns file statistics for a given path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_params(sql, params):\n    \"\"\"\"\"\"\n    args = [sql]\n    if params is not None:\n        if hasattr(params, 'keys'):  # test if params is a mapping\n            args += [params]\n        else:\n            args += [list(params)]\n    return args", "output": "Convert SQL and params args to DBAPI2.0 compliant format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _squish(scale:uniform=1.0, row_pct:uniform=0.5, col_pct:uniform=0.5):\n    \"\"\n    if scale <= 1:\n        col_c = (1-scale) * (2*col_pct - 1)\n        return _get_zoom_mat(scale, 1, col_c, 0.)\n    else:\n        row_c = (1-1/scale) * (2*row_pct - 1)\n        return _get_zoom_mat(1, 1/scale, 0., row_c)", "output": "Squish image by `scale`. `row_pct`,`col_pct` select focal point of zoom.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_mutating_webhook_configuration(self, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_mutating_webhook_configuration_with_http_info(body, **kwargs)\n        else:\n            (data) = self.create_mutating_webhook_configuration_with_http_info(body, **kwargs)\n            return data", "output": "create a MutatingWebhookConfiguration\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_mutating_webhook_configuration(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1beta1MutatingWebhookConfiguration body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1beta1MutatingWebhookConfiguration\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clone(name_a, name_b, **kwargs):\n    '''\n    \n\n    '''\n    ## Configure command\n    # NOTE: initialize the defaults\n    flags = []\n    target = []\n\n    # NOTE: push filesystem properties\n    filesystem_properties = kwargs.get('properties', {})\n\n    # NOTE: set extra config from kwargs\n    if kwargs.get('create_parent', False):\n        flags.append('-p')\n\n    # NOTE: update target\n    target.append(name_a)\n    target.append(name_b)\n\n    ## Clone filesystem/volume\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zfs_command'](\n            command='clone',\n            flags=flags,\n            filesystem_properties=filesystem_properties,\n            target=target,\n        ),\n        python_shell=False,\n    )\n\n    return __utils__['zfs.parse_command_result'](res, 'cloned')", "output": "Creates a clone of the given snapshot.\n\n    name_a : string\n        name of snapshot\n    name_b : string\n        name of filesystem or volume\n    create_parent : boolean\n        creates all the non-existing parent datasets. any property specified on the\n        command line using the -o option is ignored.\n    properties : dict\n        additional zfs properties (-o)\n\n    .. note::\n\n        ZFS properties can be specified at the time of creation of the filesystem by\n        passing an additional argument called \"properties\" and specifying the properties\n        with their respective values in the form of a python dictionary::\n\n            properties=\"{'property1': 'value1', 'property2': 'value2'}\"\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zfs.clone myzpool/mydataset@yesterday myzpool/mydataset_yesterday", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode(s):\n    \"\"\"\n    \"\"\"\n    strs = []\n    i = 0\n    while i < len(s):\n        index = s.find(\":\", i)\n        size = int(s[i:index])\n        strs.append(s[index+1: index+1+size])\n        i = index+1+size\n    return strs", "output": "Decodes a single string to a list of strings.\n    :type s: str\n    :rtype: List[str]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _preprocess(self, data):\n        \"\"\"\n        \n        \"\"\"\n        transformed_data = _copy(data)\n        for name, step in self._transformers[:-1]:\n            transformed_data = step.fit_transform(transformed_data)\n            if type(transformed_data) != _tc.SFrame:\n                raise RuntimeError(\"The transform function in step '%s' did not\"\n                    \" return an SFrame (got %s instead).\" % (name,\n                                            type(transformed_data).__name__))\n        return transformed_data", "output": "Internal function to perform fit_transform() on all but last step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _usage_specific(raw):\n    '''\n    \n    '''\n    get_key = lambda val: dict([tuple(val.split(\":\")), ])\n    raw = raw.split(\"\\n\")\n    section, size, used = raw[0].split(\" \")\n    section = section.replace(\",\", \"_\").replace(\":\", \"\").lower()\n\n    data = {}\n    data[section] = {}\n\n    for val in [size, used]:\n        data[section].update(get_key(val.replace(\",\", \"\")))\n\n    for devices in raw[1:]:\n        data[section].update(get_key(re.sub(r\"\\s+\", \":\", devices.strip())))\n\n    return data", "output": "Parse usage/specific.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_EXPMA(DataFrame, P1=5, P2=10, P3=20, P4=60):\n    \"\"\" \"\"\"\n    CLOSE = DataFrame.close\n    MA1 = EMA(CLOSE, P1)\n    MA2 = EMA(CLOSE, P2)\n    MA3 = EMA(CLOSE, P3)\n    MA4 = EMA(CLOSE, P4)\n    return pd.DataFrame({\n        'MA1': MA1, 'MA2': MA2, 'MA3': MA3, 'MA4': MA4\n    })", "output": "\u6307\u6570\u5e73\u5747\u7ebf EXPMA", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_error(model, path, shapes, output = 'softmax_output', verbose = True):\n    \"\"\"\n    \n    \"\"\"\n    coreml_model = _coremltools.models.MLModel(path)\n    input_data = {}\n    input_data_copy = {}\n    for ip in shapes:\n        input_data[ip] = _np.random.rand(*shapes[ip]).astype('f')\n        input_data_copy[ip] = _np.copy(input_data[ip])\n\n    dataIter = _mxnet.io.NDArrayIter(input_data_copy)\n    mx_out = model.predict(dataIter).flatten()\n\n    e_out_dict = coreml_model.predict(_mxnet_remove_batch(input_data))\n    e_out = e_out_dict[output].flatten()\n    error = _np.linalg.norm(e_out - mx_out)\n\n    if verbose:\n        print(\"First few predictions from CoreML : %s\" % e_out[0:10])\n        print(\"First few predictions from MXNet  : %s\" % e_out[0:10])\n        print(\"L2 Error on random data %s\" % error)\n    return error", "output": "Check the difference between predictions from MXNet and CoreML.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log(arg1, arg2=None):\n    \"\"\"\n    \"\"\"\n    sc = SparkContext._active_spark_context\n    if arg2 is None:\n        jc = sc._jvm.functions.log(_to_java_column(arg1))\n    else:\n        jc = sc._jvm.functions.log(arg1, _to_java_column(arg2))\n    return Column(jc)", "output": "Returns the first argument-based logarithm of the second argument.\n\n    If there is only one argument, then this takes the natural logarithm of the argument.\n\n    >>> df.select(log(10.0, df.age).alias('ten')).rdd.map(lambda l: str(l.ten)[:7]).collect()\n    ['0.30102', '0.69897']\n\n    >>> df.select(log(df.age).alias('e')).rdd.map(lambda l: str(l.e)[:7]).collect()\n    ['0.69314', '1.60943']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_topic(Name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        ret = conn.create_topic(Name=Name)\n        log.info('SNS topic %s created with ARN %s', Name, ret['TopicArn'])\n        return ret['TopicArn']\n    except botocore.exceptions.ClientError as e:\n        log.error('Failed to create SNS topic %s: %s', Name, e)\n        return None\n    except KeyError:\n        log.error('Failed to create SNS topic %s', Name)\n        return None", "output": "Create an SNS topic.\n\n    CLI example::\n\n        salt myminion boto3_sns.create_topic mytopic region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(self, func):\n        \"\"\"\n        \n        \"\"\"\n        if func.__code__.co_argcount == 1:\n            oldfunc = func\n            func = lambda t, rdd: oldfunc(rdd)\n        assert func.__code__.co_argcount == 2, \"func should take one or two arguments\"\n        return TransformedDStream(self, func)", "output": "Return a new DStream in which each RDD is generated by applying a function\n        on each RDD of this DStream.\n\n        `func` can have one argument of `rdd`, or have two arguments of\n        (`time`, `rdd`)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_data_calc_marketvalue(data, xdxr):\n    ''\n    mv = xdxr.query('category!=6').loc[:,\n                                       ['shares_after',\n                                        'liquidity_after']].dropna()\n    res = pd.concat([data, mv], axis=1)\n\n    res = res.assign(\n        shares=res.shares_after.fillna(method='ffill'),\n        lshares=res.liquidity_after.fillna(method='ffill')\n    )\n    return res.assign(mv=res.close*res.shares*10000, liquidity_mv=res.close*res.lshares*10000).drop(['shares_after', 'liquidity_after'], axis=1)\\\n            .loc[(slice(data.index.remove_unused_levels().levels[0][0],data.index.remove_unused_levels().levels[0][-1]),slice(None)),:]", "output": "\u4f7f\u7528\u6570\u636e\u5e93\u6570\u636e\u8ba1\u7b97\u590d\u6743", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clear_policy(self, lambda_name):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            policy_response = self.lambda_client.get_policy(\n                FunctionName=lambda_name\n            )\n            if policy_response['ResponseMetadata']['HTTPStatusCode'] == 200:\n                statement = json.loads(policy_response['Policy'])['Statement']\n                for s in statement:\n                    delete_response = self.lambda_client.remove_permission(\n                        FunctionName=lambda_name,\n                        StatementId=s['Sid']\n                    )\n                    if delete_response['ResponseMetadata']['HTTPStatusCode'] != 204:\n                        logger.error('Failed to delete an obsolete policy statement: {}'.format(policy_response))\n            else:\n                logger.debug('Failed to load Lambda function policy: {}'.format(policy_response))\n        except ClientError as e:\n            if e.args[0].find('ResourceNotFoundException') > -1:\n                logger.debug('No policy found, must be first run.')\n            else:\n                logger.error('Unexpected client error {}'.format(e.args[0]))", "output": "Remove obsolete policy statements to prevent policy from bloating over the limit after repeated updates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, name: str, value: str) -> None:\n        \"\"\"\"\"\"\n        norm_name = _normalized_headers[name]\n        self._last_key = norm_name\n        if norm_name in self:\n            self._dict[norm_name] = (\n                native_str(self[norm_name]) + \",\" + native_str(value)\n            )\n            self._as_list[norm_name].append(value)\n        else:\n            self[norm_name] = value", "output": "Adds a new value for the given key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_payload(payload):\n        \"\"\"\n        \"\"\"\n        blocks, remainder = divmod(len(payload), BLOCKSIZE)\n        if remainder > 0:\n            payload += (BLOCKSIZE - remainder) * NUL\n        return payload", "output": "Return the string payload filled with zero bytes\n           up to the next 512 byte border.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _render_cmd(cmd, cwd, template, saltenv='base', pillarenv=None, pillar_override=None):\n    '''\n    \n    '''\n    if not template:\n        return (cmd, cwd)\n\n    # render the path as a template using path_template_engine as the engine\n    if template not in salt.utils.templates.TEMPLATE_REGISTRY:\n        raise CommandExecutionError(\n            'Attempted to render file paths with unavailable engine '\n            '{0}'.format(template)\n        )\n\n    kwargs = {}\n    kwargs['salt'] = __salt__\n    if pillarenv is not None or pillar_override is not None:\n        pillarenv = pillarenv or __opts__['pillarenv']\n        kwargs['pillar'] = _gather_pillar(pillarenv, pillar_override)\n    else:\n        kwargs['pillar'] = __pillar__\n    kwargs['grains'] = __grains__\n    kwargs['opts'] = __opts__\n    kwargs['saltenv'] = saltenv\n\n    def _render(contents):\n        # write out path to temp file\n        tmp_path_fn = salt.utils.files.mkstemp()\n        with salt.utils.files.fopen(tmp_path_fn, 'w+') as fp_:\n            fp_.write(salt.utils.stringutils.to_str(contents))\n        data = salt.utils.templates.TEMPLATE_REGISTRY[template](\n            tmp_path_fn,\n            to_str=True,\n            **kwargs\n        )\n        salt.utils.files.safe_rm(tmp_path_fn)\n        if not data['result']:\n            # Failed to render the template\n            raise CommandExecutionError(\n                'Failed to execute cmd with error: {0}'.format(\n                    data['data']\n                )\n            )\n        else:\n            return data['data']\n\n    cmd = _render(cmd)\n    cwd = _render(cwd)\n    return (cmd, cwd)", "output": "If template is a valid template engine, process the cmd and cwd through\n    that engine.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __write(self, s):\n        \"\"\"\n        \"\"\"\n        self.buf += s\n        while len(self.buf) > self.bufsize:\n            self.fileobj.write(self.buf[:self.bufsize])\n            self.buf = self.buf[self.bufsize:]", "output": "Write string s to the stream if a whole new block\n           is ready to be written.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tar_and_copy_usr_dir(usr_dir, train_dir):\n  \"\"\"\"\"\"\n  tf.logging.info(\"Tarring and pushing t2t_usr_dir.\")\n  usr_dir = os.path.abspath(os.path.expanduser(usr_dir))\n  # Copy usr dir to a temp location\n  top_dir = os.path.join(tempfile.gettempdir(), \"t2t_usr_container\")\n  tmp_usr_dir = os.path.join(top_dir, usr_dir_lib.INTERNAL_USR_DIR_PACKAGE)\n  shutil.rmtree(top_dir, ignore_errors=True)\n  shutil.copytree(usr_dir, tmp_usr_dir)\n  # Insert setup.py if one does not exist\n  top_setup_fname = os.path.join(top_dir, \"setup.py\")\n  setup_file_str = get_setup_file(\n      name=\"DummyUsrDirPackage\",\n      packages=get_requirements(usr_dir)\n  )\n  with tf.gfile.Open(top_setup_fname, \"w\") as f:\n    f.write(setup_file_str)\n  usr_tar = _tar_and_copy(top_dir, train_dir)\n  return usr_tar", "output": "Package, tar, and copy usr_dir to GCS train_dir.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def result(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.subplots:\n            if self.layout is not None and not is_list_like(self.ax):\n                return self.axes.reshape(*self.layout)\n            else:\n                return self.axes\n        else:\n            sec_true = isinstance(self.secondary_y, bool) and self.secondary_y\n            all_sec = (is_list_like(self.secondary_y) and\n                       len(self.secondary_y) == self.nseries)\n            if (sec_true or all_sec):\n                # if all data is plotted on secondary, return right axes\n                return self._get_ax_layer(self.axes[0], primary=False)\n            else:\n                return self.axes[0]", "output": "Return result axes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def listen(self, grpc_port):\n    \"\"\"\n    \"\"\"\n    if self._grpc_port:\n      raise ValueError(\n          'This InteractiveDebuggerPlugin instance is already listening at '\n          'gRPC port %d' % self._grpc_port)\n    self._grpc_port = grpc_port\n\n    sys.stderr.write('Creating InteractiveDebuggerPlugin at port %d\\n' %\n                     self._grpc_port)\n    sys.stderr.flush()\n    self._debugger_data_server = (\n        interactive_debugger_server_lib.InteractiveDebuggerDataServer(\n            self._grpc_port))\n\n    self._server_thread = threading.Thread(\n        target=self._debugger_data_server.run_server)\n    self._server_thread.start()\n\n    signal.signal(signal.SIGINT, self.signal_handler)", "output": "Start listening on the given gRPC port.\n\n    This method of an instance of InteractiveDebuggerPlugin can be invoked at\n    most once. This method is not thread safe.\n\n    Args:\n      grpc_port: port number to listen at.\n\n    Raises:\n      ValueError: If this instance is already listening at a gRPC port.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_endtime(jid):\n    '''\n    \n    '''\n    jid_dir = salt.utils.jid.jid_dir(jid, _job_dir(), __opts__['hash_type'])\n    etpath = os.path.join(jid_dir, ENDTIME)\n    if not os.path.exists(etpath):\n        return False\n    with salt.utils.files.fopen(etpath, 'r') as etfile:\n        endtime = salt.utils.stringutils.to_unicode(etfile.read()).strip('\\n')\n    return endtime", "output": "Retrieve the stored endtime for a given job\n\n    Returns False if no endtime is present", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find(self):\r\n        \"\"\"\"\"\"\r\n        editorstack = self.get_current_editorstack()\r\n        editorstack.find_widget.show()\r\n        editorstack.find_widget.search_text.setFocus()", "output": "Find slot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def calculate_dates(self, dt):\n        \"\"\"\n        \n        \"\"\"\n        period_end = self.cal.open_and_close_for_session(\n            self.cal.minute_to_session_label(dt),\n        )[1]\n\n        # Align the market close time here with the execution time used by the\n        # simulation clock. This ensures that scheduled functions trigger at\n        # the correct times.\n        self._period_end = self.cal.execution_time_from_close(period_end)\n\n        self._period_start = self._period_end - self.offset\n        self._period_close = self._period_end", "output": "Given a dt, find that day's close and period start (close - offset).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_date_columns(data_frame, parse_dates):\n    \"\"\"\n    \n    \"\"\"\n    parse_dates = _process_parse_dates_argument(parse_dates)\n\n    # we want to coerce datetime64_tz dtypes for now to UTC\n    # we could in theory do a 'nice' conversion from a FixedOffset tz\n    # GH11216\n    for col_name, df_col in data_frame.iteritems():\n        if is_datetime64tz_dtype(df_col) or col_name in parse_dates:\n            try:\n                fmt = parse_dates[col_name]\n            except TypeError:\n                fmt = None\n            data_frame[col_name] = _handle_date_column(df_col, format=fmt)\n\n    return data_frame", "output": "Force non-datetime columns to be read as such.\n    Supports both string formatted and integer timestamp columns.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_pkg_string(pkg):\n    '''\n    \n    '''\n    pkg_name, separator, pkg_ver = pkg.partition('-')\n    return (pkg_name.strip(), separator, pkg_ver.strip())", "output": "Parse pkg string and return a tuple of package name, separator, and\n    package version.\n\n    Cabal support install package with following format:\n\n    * foo-1.0\n    * foo < 1.2\n    * foo > 1.3\n\n    For the sake of simplicity only the first form is supported,\n    support for other forms can be added later.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_templates(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_templates function must be called with -f or --function.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    template_pool = server.one.templatepool.info(auth, -2, -1, -1)[1]\n\n    templates = {}\n    for template in _get_xml(template_pool):\n        templates[template.find('NAME').text] = _xml_to_dict(template)\n\n    return templates", "output": "Lists all templates available to the user and the user's groups.\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_templates opennebula", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_mode(device):\n    '''\n    \n    '''\n    ret = {}\n    cmd = 'quotaon -p {0}'.format(device)\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n    for line in out.splitlines():\n        comps = line.strip().split()\n        if comps[3] not in ret:\n            if comps[0].startswith('quotaon'):\n                if comps[1].startswith('Mountpoint'):\n                    ret[comps[4]] = 'disabled'\n                    continue\n                elif comps[1].startswith('Cannot'):\n                    ret[device] = 'Not found'\n                    return ret\n                continue\n            ret[comps[3]] = {\n                'device': comps[4].replace('(', '').replace(')', ''),\n            }\n        ret[comps[3]][comps[0]] = comps[6]\n    return ret", "output": "Report whether the quota system for this device is on or off\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' quota.get_mode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mine_flush(self, load, skip_verify=False):\n        '''\n        \n        '''\n        if not skip_verify and 'id' not in load:\n            return False\n        if self.opts.get('minion_data_cache', False) or self.opts.get('enforce_mine_cache', False):\n            return self.cache.flush('minions/{0}'.format(load['id']), 'mine')\n        return True", "output": "Allow the minion to delete all of its own mine contents", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def buy_avg_holding_price(self):\n        \"\"\"\n        \n        \"\"\"\n        return 0 if self.buy_quantity == 0 else self._buy_holding_cost / self.buy_quantity / self.contract_multiplier", "output": "[float] \u4e70\u65b9\u5411\u6301\u4ed3\u5747\u4ef7", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self):\n        \"\"\"\n        \"\"\"\n        update_mask_pb = field_mask_pb2.FieldMask()\n        if self.display_name is not None:\n            update_mask_pb.paths.append(\"display_name\")\n        if self.type_ is not None:\n            update_mask_pb.paths.append(\"type\")\n        if self.labels is not None:\n            update_mask_pb.paths.append(\"labels\")\n        instance_pb = instance_pb2.Instance(\n            name=self.name,\n            display_name=self.display_name,\n            type=self.type_,\n            labels=self.labels,\n        )\n\n        return self._client.instance_admin_client.partial_update_instance(\n            instance=instance_pb, update_mask=update_mask_pb\n        )", "output": "Updates an instance within a project.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_update_instance]\n            :end-before: [END bigtable_update_instance]\n\n        .. note::\n\n            Updates any or all of the following values:\n            ``display_name``\n            ``type``\n            ``labels``\n            To change a value before\n            updating, assign that values via\n\n            .. code:: python\n\n                instance.display_name = 'New display name'\n\n            before calling :meth:`update`.\n\n        :rtype: :class:`~google.api_core.operation.Operation`\n        :returns: The long-running operation corresponding to the update\n                    operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_string(self, buf=None, columns=None, col_space=None, header=True,\n                  index=True, na_rep='NaN', formatters=None, float_format=None,\n                  sparsify=None, index_names=True, justify=None,\n                  max_rows=None, max_cols=None, show_dimensions=False,\n                  decimal='.', line_width=None):\n        \"\"\"\n        \n        \"\"\"\n\n        formatter = fmt.DataFrameFormatter(self, buf=buf, columns=columns,\n                                           col_space=col_space, na_rep=na_rep,\n                                           formatters=formatters,\n                                           float_format=float_format,\n                                           sparsify=sparsify, justify=justify,\n                                           index_names=index_names,\n                                           header=header, index=index,\n                                           max_rows=max_rows,\n                                           max_cols=max_cols,\n                                           show_dimensions=show_dimensions,\n                                           decimal=decimal,\n                                           line_width=line_width)\n        formatter.to_string()\n\n        if buf is None:\n            result = formatter.buf.getvalue()\n            return result", "output": "Render a DataFrame to a console-friendly tabular output.\n        %(shared_params)s\n        line_width : int, optional\n            Width to wrap a line in characters.\n        %(returns)s\n        See Also\n        --------\n        to_html : Convert DataFrame to HTML.\n\n        Examples\n        --------\n        >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]}\n        >>> df = pd.DataFrame(d)\n        >>> print(df.to_string())\n           col1  col2\n        0     1     4\n        1     2     5\n        2     3     6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ack(self, items):\n        \"\"\"\n        \"\"\"\n        # If we got timing information, add it to the histogram.\n        for item in items:\n            time_to_ack = item.time_to_ack\n            if time_to_ack is not None:\n                self._manager.ack_histogram.add(time_to_ack)\n\n        ack_ids = [item.ack_id for item in items]\n        request = types.StreamingPullRequest(ack_ids=ack_ids)\n        self._manager.send(request)\n\n        # Remove the message from lease management.\n        self.drop(items)", "output": "Acknowledge the given messages.\n\n        Args:\n            items(Sequence[AckRequest]): The items to acknowledge.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_compare_custom_predict_fn(self, predict_fn):\n    \"\"\"\n    \"\"\"\n    # If estimator is set, remove it before setting predict_fn\n    self.delete('compare_estimator_and_spec')\n\n    self.store('compare_custom_predict_fn', predict_fn)\n    self.set_compare_inference_address('custom_predict_fn')\n    # If no model name has been set, give a default\n    if not self.has_compare_model_name():\n      self.set_compare_model_name('2')\n    return self", "output": "Sets a second custom function for inference.\n\n    If you wish to compare the results of two models in WIT, use this method\n    to setup the details of the second model.\n\n    Instead of using TF Serving to host a model for WIT to query, WIT can\n    directly use a custom function as the model to query. In this case, the\n    provided function should accept example protos and return:\n      - For classification: A 2D list of numbers. The first dimension is for\n        each example being predicted. The second dimension are the probabilities\n        for each class ID in the prediction.\n      - For regression: A 1D list of numbers, with a regression score for each\n        example being predicted.\n\n    Args:\n      predict_fn: The custom python function which will be used for model\n      inference.\n\n    Returns:\n      self, in order to enabled method chaining.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def glow_hparams():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.clip_grad_norm = None\n  hparams.weight_decay = 0.0\n  hparams.learning_rate_constant = 3e-4\n  hparams.batch_size = 32\n  # can be prev_level, prev_step or normal.\n  # see: glow_ops.merge_level_and_latent_dist\n  hparams.add_hparam(\"level_scale\", \"prev_level\")\n  hparams.add_hparam(\"n_levels\", 3)\n  hparams.add_hparam(\"n_bits_x\", 8)\n  hparams.add_hparam(\"depth\", 32)\n  # Activation - Relu or Gatu\n  hparams.add_hparam(\"activation\", \"relu\")\n  # Coupling layer, additive or affine.\n  hparams.add_hparam(\"coupling\", \"affine\")\n  hparams.add_hparam(\"coupling_width\", 512)\n  hparams.add_hparam(\"coupling_dropout\", 0.0)\n  hparams.add_hparam(\"top_prior\", \"single_conv\")\n  # init_batch_size denotes the number of examples used for data-dependent\n  # initialization. A higher init_batch_size is required for training\n  # stability especially when hparams.batch_size is low.\n  hparams.add_hparam(\"init_batch_size\", 256)\n  hparams.add_hparam(\"temperature\", 1.0)\n\n  return hparams", "output": "Glow Hparams.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _arg2opt(arg):\n    '''\n    \n    '''\n    res = [o for o, a in option_toggles.items() if a == arg]\n    res += [o for o, a in option_flags.items() if a == arg]\n    return res[0] if res else None", "output": "Turn a pass argument into the correct option", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup(app):\n    '''  '''\n    app.connect('html-page-context', html_page_context)\n    app.connect('build-finished',    build_finished)\n    app.sitemap_links = set()", "output": "Required Sphinx extension setup function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build(self, input_shape=None):\n    \"\"\"\"\"\"\n    input_shape = tf.TensorShape(input_shape).as_list()\n    self.input_spec = layers().InputSpec(shape=input_shape)\n\n    if not self.layer.built:\n      self.layer.build(input_shape)\n      self.layer.built = False\n\n      if not hasattr(self.layer, \"kernel\"):\n        raise ValueError(\"`WeightNorm` must wrap a layer that\"\n                         \" contains a `kernel` for weights\")\n\n      # The kernel's filter or unit dimension is -1\n      self.layer_depth = int(self.layer.kernel.shape[-1])\n      self.norm_axes = list(range(self.layer.kernel.shape.ndims - 1))\n\n      self.layer.v = self.layer.kernel\n      self.layer.g = self.layer.add_variable(\n          name=\"g\",\n          shape=(self.layer_depth,),\n          initializer=tf.ones_initializer,\n          dtype=self.layer.kernel.dtype,\n          trainable=True)\n\n      # with ops.control_dependencies([self.layer.g.assign(\n      #     self._init_norm(self.layer.v))]):\n      #   self._compute_weights()\n      self._compute_weights()\n\n      self.layer.built = True\n\n    super(WeightNorm, self).build()\n    self.built = True", "output": "Build `Layer`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_names(infos):\n    \"\"\"\n    \"\"\"\n    names = []\n    for x in infos:\n        for y in x:\n            names.append(y[\"name\"][0])\n    return names", "output": "Get names of all parameters.\n\n    Parameters\n    ----------\n    infos : list\n        Content of the config header file.\n\n    Returns\n    -------\n    names : list\n        Names of all parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_julian_date(self):\n        \"\"\"\n        \n        \"\"\"\n\n        # http://mysite.verizon.net/aesir_research/date/jdalg2.htm\n        year = np.asarray(self.year)\n        month = np.asarray(self.month)\n        day = np.asarray(self.day)\n        testarr = month < 3\n        year[testarr] -= 1\n        month[testarr] += 12\n        return (day +\n                np.fix((153 * month - 457) / 5) +\n                365 * year +\n                np.floor(year / 4) -\n                np.floor(year / 100) +\n                np.floor(year / 400) +\n                1721118.5 +\n                (self.hour +\n                 self.minute / 60.0 +\n                 self.second / 3600.0 +\n                 self.microsecond / 3600.0 / 1e+6 +\n                 self.nanosecond / 3600.0 / 1e+9\n                 ) / 24.0)", "output": "Convert Datetime Array to float64 ndarray of Julian Dates.\n        0 Julian date is noon January 1, 4713 BC.\n        http://en.wikipedia.org/wiki/Julian_day", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _slice(self, slicer):\n        \"\"\"\n        \n        \"\"\"\n\n        # only allow 1 dimensional slicing, but can\n        # in a 2-d case be passd (slice(None),....)\n        if isinstance(slicer, tuple) and len(slicer) == 2:\n            if not com.is_null_slice(slicer[0]):\n                raise AssertionError(\"invalid slicing for a 1-ndim \"\n                                     \"categorical\")\n            slicer = slicer[1]\n\n        codes = self._codes[slicer]\n        return self._constructor(values=codes, dtype=self.dtype, fastpath=True)", "output": "Return a slice of myself.\n\n        For internal compatibility with numpy arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        ''''''\n        logger.info(\"fetcher starting...\")\n\n        def queue_loop():\n            if not self.outqueue or not self.inqueue:\n                return\n            while not self._quit:\n                try:\n                    if self.outqueue.full():\n                        break\n                    if self.http_client.free_size() <= 0:\n                        break\n                    task = self.inqueue.get_nowait()\n                    # FIXME: decode unicode_obj should used after data selete from\n                    # database, it's used here for performance\n                    task = utils.decode_unicode_obj(task)\n                    self.fetch(task)\n                except queue.Empty:\n                    break\n                except KeyboardInterrupt:\n                    break\n                except Exception as e:\n                    logger.exception(e)\n                    break\n\n        tornado.ioloop.PeriodicCallback(queue_loop, 100, io_loop=self.ioloop).start()\n        tornado.ioloop.PeriodicCallback(self.clear_robot_txt_cache, 10000, io_loop=self.ioloop).start()\n        self._running = True\n\n        try:\n            self.ioloop.start()\n        except KeyboardInterrupt:\n            pass\n\n        logger.info(\"fetcher exiting...\")", "output": "Run loop", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_insecure_stub(stub_class, host, port=None):\n    \"\"\"\n    \"\"\"\n    if port is None:\n        target = host\n    else:\n        # NOTE: This assumes port != http_client.HTTPS_PORT:\n        target = \"%s:%d\" % (host, port)\n    channel = grpc.insecure_channel(target)\n    return stub_class(channel)", "output": "Makes an insecure stub for an RPC service.\n\n    Uses / depends on gRPC.\n\n    :type stub_class: type\n    :param stub_class: A gRPC stub type for a given service.\n\n    :type host: str\n    :param host: The host for the service. May also include the port\n                 if ``port`` is unspecified.\n\n    :type port: int\n    :param port: (Optional) The port for the service.\n\n    :rtype: object, instance of ``stub_class``\n    :returns: The stub object used to make gRPC requests to a given API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def store_job(self):\n        '''\n        \n        '''\n        try:\n            class_name = self.__class__.__name__.lower()\n        except AttributeError:\n            log.warning(\n                'Unable to determine class name',\n                exc_info_on_loglevel=logging.DEBUG\n            )\n            return True\n\n        try:\n            return self.opts['{0}_returns'.format(class_name)]\n        except KeyError:\n            # No such option, assume this isn't one we care about gating and\n            # just return True.\n            return True", "output": "Helper that allows us to turn off storing jobs for different classes\n        that may incorporate this mixin.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self, stream, *args, **kwargs):\n        \"\"\"\n\n        \"\"\"\n        self._parse(stream, False, None, *args, **kwargs)\n        return self.tree.getDocument()", "output": "Parse a HTML document into a well-formed tree\n\n        :arg stream: a file-like object or string containing the HTML to be parsed\n\n            The optional encoding parameter must be a string that indicates\n            the encoding.  If specified, that encoding will be used,\n            regardless of any BOM or later declaration (such as in a meta\n            element).\n\n        :arg scripting: treat noscript elements as if JavaScript was turned on\n\n        :returns: parsed tree\n\n        Example:\n\n        >>> from html5lib.html5parser import HTMLParser\n        >>> parser = HTMLParser()\n        >>> parser.parse('<html><body><p>This is a doc</p></body></html>')\n        <Element u'{http://www.w3.org/1999/xhtml}html' at 0x7feac4909db0>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config_id(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudException(\n            'The get_config_id function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    linode_id = kwargs.get('linode_id', None)\n    if name is None and linode_id is None:\n        raise SaltCloudSystemExit(\n            'The get_config_id function requires either a \\'name\\' or a \\'linode_id\\' '\n            'to be provided.'\n        )\n    if linode_id is None:\n        linode_id = get_linode_id_from_name(name)\n\n    response = _query('linode', 'config.list', args={'LinodeID': linode_id})['DATA']\n    config_id = {'config_id': response[0]['ConfigID']}\n\n    return config_id", "output": "Returns a config_id for a given linode.\n\n    .. versionadded:: 2015.8.0\n\n    name\n        The name of the Linode for which to get the config_id. Can be used instead\n        of ``linode_id``.h\n\n    linode_id\n        The ID of the Linode for which to get the config_id. Can be used instead\n        of ``name``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_config_id my-linode-config name=my-linode\n        salt-cloud -f get_config_id my-linode-config linode_id=1234567", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def pong(self, message: bytes=b'') -> None:\n        \"\"\"\"\"\"\n        if isinstance(message, str):\n            message = message.encode('utf-8')\n        await self._send_frame(message, WSMsgType.PONG)", "output": "Send pong message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_secret(path, key=None):\n    '''\n    \n    '''\n    log.debug('Reading Vault secret for %s at %s', __grains__['id'], path)\n    try:\n        url = 'v1/{0}'.format(path)\n        response = __utils__['vault.make_request']('GET', url)\n        if response.status_code != 200:\n            response.raise_for_status()\n        data = response.json()['data']\n\n        if key is not None:\n            return data[key]\n        return data\n    except Exception as err:\n        log.error('Failed to read secret! %s: %s', type(err).__name__, err)\n        return None", "output": "Return the value of key at path in vault, or entire secret\n\n    Jinja Example:\n\n    .. code-block:: jinja\n\n        my-secret: {{ salt['vault'].read_secret('secret/my/secret', 'some-key') }}\n\n    .. code-block:: jinja\n\n        {% set supersecret = salt['vault'].read_secret('secret/my/secret') %}\n        secrets:\n            first: {{ supersecret.first }}\n            second: {{ supersecret.second }}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def block(self):\n        \"\"\"\"\"\"\n        if self._uses_subprocess:\n            # consume stdout and stderr\n            if self.blocking:\n                try:\n                    stdout, stderr = self.subprocess.communicate()\n                    self.__out = stdout\n                    self.__err = stderr\n                except ValueError:\n                    pass  # Don't read from finished subprocesses.\n            else:\n                self.subprocess.stdin.close()\n                self.std_out.close()\n                self.std_err.close()\n                self.subprocess.wait()\n        else:\n            self.subprocess.sendeof()\n            try:\n                self.subprocess.wait()\n            finally:\n                if self.subprocess.proc.stdout:\n                    self.subprocess.proc.stdout.close()", "output": "Blocks until process is complete.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tojson(val, indent=None):\n    '''\n    \n    '''\n    options = {'ensure_ascii': True}\n    if indent is not None:\n        options['indent'] = indent\n    return (\n        salt.utils.json.dumps(\n            val, **options\n        ).replace('<', '\\\\u003c')\n         .replace('>', '\\\\u003e')\n         .replace('&', '\\\\u0026')\n         .replace(\"'\", '\\\\u0027')\n    )", "output": "Implementation of tojson filter (only present in Jinja 2.9 and later). If\n    Jinja 2.9 or later is installed, then the upstream version of this filter\n    will be used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apiinfo_version(**kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'apiinfo.version'\n            params = {}\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']\n        else:\n            raise KeyError\n    except KeyError:\n        return False", "output": "Retrieve the version of the Zabbix API.\n\n    .. versionadded:: 2016.3.0\n\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: On success string with Zabbix API version, False on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.apiinfo_version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _move_after(xpath, target):\n    '''\n    \n\n    '''\n    query = {'type': 'config',\n             'action': 'move',\n             'xpath': xpath,\n             'where': 'after',\n             'dst': target}\n\n    response = __proxy__['panos.call'](query)\n\n    return _validate_response(response)", "output": "Moves an xpath to the after of its section.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_disk_timeout(timeout, power='ac', scheme=None):\n    '''\n    \n    '''\n    return _set_powercfg_value(\n        scheme=scheme,\n        sub_group='SUB_DISK',\n        setting_guid='DISKIDLE',\n        power=power,\n        value=timeout)", "output": "Set the disk timeout in minutes for the given power scheme\n\n    Args:\n        timeout (int):\n            The amount of time in minutes before the disk will timeout\n\n        power (str):\n            Set the value for AC or DC power. Default is ``ac``. Valid options\n            are:\n\n                - ``ac`` (AC Power)\n                - ``dc`` (Battery)\n\n        scheme (str):\n            The scheme to use, leave as ``None`` to use the current. Default is\n            ``None``. This can be the GUID or the Alias for the Scheme. Known\n            Aliases are:\n\n                - ``SCHEME_BALANCED`` - Balanced\n                - ``SCHEME_MAX`` - Power saver\n                - ``SCHEME_MIN`` - High performance\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        # Sets the disk timeout to 30 minutes on battery\n        salt '*' powercfg.set_disk_timeout 30 power=dc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deploy_ray_func(func, partition, kwargs):\n    \"\"\"\n    \"\"\"\n    try:\n        result = func(partition, **kwargs)\n    # Sometimes Arrow forces us to make a copy of an object before we operate\n    # on it. We don't want the error to propagate to the user, and we want to\n    # avoid copying unless we absolutely have to.\n    except Exception:\n        result = func(partition.to_pandas(), **kwargs)\n        if isinstance(result, pandas.Series):\n            result = pandas.DataFrame(result).T\n        if isinstance(result, pandas.DataFrame):\n            return pyarrow.Table.from_pandas(result)\n    return result", "output": "Deploy a function to a partition in Ray.\n\n    Args:\n        func: The function to apply.\n        partition: The partition to apply the function to.\n        kwargs: A dictionary of keyword arguments for the function.\n\n    Returns:\n        The result of the function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_archive(base_name, format, root_dir=None, base_dir=None, verbose=0,\n                 dry_run=0, owner=None, group=None, logger=None):\n    \"\"\"\n    \"\"\"\n    save_cwd = os.getcwd()\n    if root_dir is not None:\n        if logger is not None:\n            logger.debug(\"changing into '%s'\", root_dir)\n        base_name = os.path.abspath(base_name)\n        if not dry_run:\n            os.chdir(root_dir)\n\n    if base_dir is None:\n        base_dir = os.curdir\n\n    kwargs = {'dry_run': dry_run, 'logger': logger}\n\n    try:\n        format_info = _ARCHIVE_FORMATS[format]\n    except KeyError:\n        raise ValueError(\"unknown archive format '%s'\" % format)\n\n    func = format_info[0]\n    for arg, val in format_info[1]:\n        kwargs[arg] = val\n\n    if format != 'zip':\n        kwargs['owner'] = owner\n        kwargs['group'] = group\n\n    try:\n        filename = func(base_name, base_dir, **kwargs)\n    finally:\n        if root_dir is not None:\n            if logger is not None:\n                logger.debug(\"changing back to '%s'\", save_cwd)\n            os.chdir(save_cwd)\n\n    return filename", "output": "Create an archive file (eg. zip or tar).\n\n    'base_name' is the name of the file to create, minus any format-specific\n    extension; 'format' is the archive format: one of \"zip\", \"tar\", \"bztar\"\n    or \"gztar\".\n\n    'root_dir' is a directory that will be the root directory of the\n    archive; ie. we typically chdir into 'root_dir' before creating the\n    archive.  'base_dir' is the directory where we start archiving from;\n    ie. 'base_dir' will be the common prefix of all files and\n    directories in the archive.  'root_dir' and 'base_dir' both default\n    to the current directory.  Returns the name of the archive file.\n\n    'owner' and 'group' are used when creating a tar archive. By default,\n    uses the current owner and group.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_file_auth_config():\n    '''\n    \n    '''\n\n    config = {\n        'filetype': 'text',\n        'hashtype': 'plaintext',\n        'field_separator': ':',\n        'username_field': 1,\n        'password_field': 2,\n    }\n\n    for opt in __opts__['external_auth'][__virtualname__]:\n        if opt.startswith('^'):\n            config[opt[1:]] = __opts__['external_auth'][__virtualname__][opt]\n\n    if 'filename' not in config:\n        log.error('salt.auth.file: An authentication file must be specified '\n                  'via external_auth:file:^filename')\n        return False\n\n    if not os.path.exists(config['filename']):\n        log.error('salt.auth.file: The configured external_auth:file:^filename (%s)'\n                  'does not exist on the filesystem', config['filename'])\n        return False\n\n    config['username_field'] = int(config['username_field'])\n    config['password_field'] = int(config['password_field'])\n\n    return config", "output": "Setup defaults and check configuration variables for auth backends", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def most_specific_convertible_shape(self, other):\n        \"\"\"\n        \"\"\"\n\n        other = as_shape(other)\n        if self._dims is None or other.dims is None or self.ndims != other.ndims:\n            return unknown_shape()\n\n        dims = [(Dimension(None))] * self.ndims\n        for i, (d1, d2) in enumerate(zip(self._dims, other.dims)):\n            if d1 is not None and d2 is not None and d1 == d2:\n                dims[i] = d1\n        return TensorShape(dims)", "output": "Returns the most specific TensorShape convertible with `self` and `other`.\n\n        * TensorShape([None, 1]) is the most specific TensorShape convertible with\n          both TensorShape([2, 1]) and TensorShape([5, 1]). Note that\n          TensorShape(None) is also convertible with above mentioned TensorShapes.\n\n        * TensorShape([1, 2, 3]) is the most specific TensorShape convertible with\n          both TensorShape([1, 2, 3]) and TensorShape([1, 2, 3]). There are more\n          less specific TensorShapes convertible with above mentioned TensorShapes,\n          e.g. TensorShape([1, 2, None]), TensorShape(None).\n\n        Args:\n          other: Another `TensorShape`.\n\n        Returns:\n          A `TensorShape` which is the most specific convertible shape of `self`\n          and `other`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schema_to_json(self, schema_list, destination):\n        \"\"\"\n        \"\"\"\n        json_schema_list = [f.to_api_repr() for f in schema_list]\n\n        if isinstance(destination, io.IOBase):\n            return self._schema_to_json_file_object(json_schema_list, destination)\n\n        with open(destination, mode=\"w\") as file_obj:\n            return self._schema_to_json_file_object(json_schema_list, file_obj)", "output": "Takes a list of schema field objects.\n\n        Serializes the list of schema field objects as json to a file.\n\n        Destination is a file path or a file object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_callback(connection_id, data_buffer, data_length_pointer):\n    \"\"\"\n    \n    \"\"\"\n    wrapped_socket = None\n    try:\n        wrapped_socket = _connection_refs.get(connection_id)\n        if wrapped_socket is None:\n            return SecurityConst.errSSLInternal\n        base_socket = wrapped_socket.socket\n\n        requested_length = data_length_pointer[0]\n\n        timeout = wrapped_socket.gettimeout()\n        error = None\n        read_count = 0\n\n        try:\n            while read_count < requested_length:\n                if timeout is None or timeout >= 0:\n                    if not util.wait_for_read(base_socket, timeout):\n                        raise socket.error(errno.EAGAIN, 'timed out')\n\n                remaining = requested_length - read_count\n                buffer = (ctypes.c_char * remaining).from_address(\n                    data_buffer + read_count\n                )\n                chunk_size = base_socket.recv_into(buffer, remaining)\n                read_count += chunk_size\n                if not chunk_size:\n                    if not read_count:\n                        return SecurityConst.errSSLClosedGraceful\n                    break\n        except (socket.error) as e:\n            error = e.errno\n\n            if error is not None and error != errno.EAGAIN:\n                data_length_pointer[0] = read_count\n                if error == errno.ECONNRESET or error == errno.EPIPE:\n                    return SecurityConst.errSSLClosedAbort\n                raise\n\n        data_length_pointer[0] = read_count\n\n        if read_count != requested_length:\n            return SecurityConst.errSSLWouldBlock\n\n        return 0\n    except Exception as e:\n        if wrapped_socket is not None:\n            wrapped_socket._exception = e\n        return SecurityConst.errSSLInternal", "output": "SecureTransport read callback. This is called by ST to request that data\n    be returned from the socket.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_brew_commands(brew_path_prefix):\n    \"\"\"\"\"\"\n    brew_cmd_path = brew_path_prefix + BREW_CMD_PATH\n\n    return [name[:-3] for name in os.listdir(brew_cmd_path)\n            if name.endswith(('.rb', '.sh'))]", "output": "To get brew default commands on local environment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_stderr(self, s):\n        \"\"\"\n        \n        \"\"\"\n\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_EXTENDED_DATA)\n        m.add_int(self.remote_chanid)\n        m.add_int(1)\n        return self._send(s, m)", "output": "Send data to the channel on the \"stderr\" stream.  This is normally\n        only used by servers to send output from shell commands -- clients\n        won't use this.  Returns the number of bytes sent, or 0 if the channel\n        stream is closed.  Applications are responsible for checking that all\n        data has been sent: if only some of the data was transmitted, the\n        application needs to attempt delivery of the remaining data.\n\n        :param str s: data to send.\n        :return: number of bytes actually sent, as an `int`.\n\n        :raises socket.timeout:\n            if no data could be sent before the timeout set by `settimeout`.\n\n        .. versionadded:: 1.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mean(l, ignore_nan=False, empty=0):\n    \"\"\"\n    \n    \"\"\"\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == 'raise':\n            raise ValueError('Empty mean')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n", "output": "nanmean compatible with generators.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recovery(self, using=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        return self._get_connection(using).indices.recovery(index=self._name, **kwargs)", "output": "The indices recovery API provides insight into on-going shard\n        recoveries for the index.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.recovery`` unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_java(self):\n        \"\"\"\n        \n        \"\"\"\n\n        estimator, epms, evaluator = super(TrainValidationSplit, self)._to_java_impl()\n\n        _java_obj = JavaParams._new_java_obj(\"org.apache.spark.ml.tuning.TrainValidationSplit\",\n                                             self.uid)\n        _java_obj.setEstimatorParamMaps(epms)\n        _java_obj.setEvaluator(evaluator)\n        _java_obj.setEstimator(estimator)\n        _java_obj.setTrainRatio(self.getTrainRatio())\n        _java_obj.setSeed(self.getSeed())\n        _java_obj.setParallelism(self.getParallelism())\n        _java_obj.setCollectSubModels(self.getCollectSubModels())\n        return _java_obj", "output": "Transfer this instance to a Java TrainValidationSplit. Used for ML persistence.\n        :return: Java object equivalent to this instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_position_label(i, words, tags, heads, labels, ents):\n    \"\"\"\n    \"\"\"\n    if len(words) < 20:\n        return \"short-doc\"\n    elif i == 0:\n        return \"first-word\"\n    elif i < 10:\n        return \"early-word\"\n    elif i < 20:\n        return \"mid-word\"\n    elif i == len(words) - 1:\n        return \"last-word\"\n    else:\n        return \"late-word\"", "output": "Return labels indicating the position of the word in the document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_base_stochastic():\n  \"\"\"\"\"\"\n  hparams = rlmb_base()\n  hparams.initial_epoch_train_steps_multiplier = 5\n  hparams.generative_model = \"next_frame_basic_stochastic\"\n  hparams.generative_model_params = \"next_frame_basic_stochastic\"\n  return hparams", "output": "Base setting with a stochastic next-frame model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_histograms(self, model: Model, histogram_parameters: Set[str]) -> None:\n        \"\"\"\n        \n        \"\"\"\n        for name, param in model.named_parameters():\n            if name in histogram_parameters:\n                self.add_train_histogram(\"parameter_histogram/\" + name, param)", "output": "Send histograms of parameters to tensorboard.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ctl_cmd(cmd, name, conf_file, bin_env):\n    '''\n    \n    '''\n    ret = [_get_supervisorctl_bin(bin_env)]\n    if conf_file is not None:\n        ret += ['-c', conf_file]\n    ret.append(cmd)\n    if name:\n        ret.append(name)\n    return ret", "output": "Return the command list to use", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ctypes2numpy_shared(cptr, shape):\n    \"\"\"\n    \"\"\"\n    if not isinstance(cptr, ctypes.POINTER(mx_float)):\n        raise RuntimeError('expected float pointer')\n    size = 1\n    for s in shape:\n        size *= s\n    dbuffer = (mx_float * size).from_address(ctypes.addressof(cptr.contents))\n    return _np.frombuffer(dbuffer, dtype=_np.float32).reshape(shape)", "output": "Convert a ctypes pointer to a numpy array.\n\n    The resulting NumPy array shares the memory with the pointer.\n\n    Parameters\n    ----------\n    cptr : ctypes.POINTER(mx_float)\n        pointer to the memory region\n\n    shape : tuple\n        Shape of target `NDArray`.\n\n    Returns\n    -------\n    out : numpy_array\n        A numpy array : numpy array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def role_delete(role_id=None, name=None, profile=None,\n                **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n\n    if name:\n        for role in kstone.roles.list():\n            if role.name == name:\n                role_id = role.id\n                break\n    if not role_id:\n        return {'Error': 'Unable to resolve role id'}\n\n    role = kstone.roles.get(role_id)\n    kstone.roles.delete(role)\n\n    ret = 'Role ID {0} deleted'.format(role_id)\n    if name:\n        ret += ' ({0})'.format(name)\n    return ret", "output": "Delete a role (keystone role-delete)\n\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.role_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.role_delete role_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.role_delete name=admin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_if_callable(maybe_callable, obj, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n\n    if callable(maybe_callable):\n        return maybe_callable(obj, **kwargs)\n\n    return maybe_callable", "output": "Evaluate possibly callable input using obj and kwargs if it is callable,\n    otherwise return as it is.\n\n    Parameters\n    ----------\n    maybe_callable : possibly a callable\n    obj : NDFrame\n    **kwargs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybridize(self, active=True, **kwargs):\n        \"\"\"\n        \"\"\"\n        for cld in self._children.values():\n            cld.hybridize(active, **kwargs)", "output": "Activates or deactivates :py:class:`HybridBlock` s recursively. Has no effect on\n        non-hybrid children.\n\n        Parameters\n        ----------\n        active : bool, default True\n            Whether to turn hybrid on or off.\n        static_alloc : bool, default False\n            Statically allocate memory to improve speed. Memory usage may increase.\n        static_shape : bool, default False\n            Optimize for invariant input shapes between iterations. Must also\n            set static_alloc to True. Change of input shapes is still allowed\n            but slower.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read(obj):\n    \"\"\"\n    \"\"\"\n    if _is_url(obj):\n        with urlopen(obj) as url:\n            text = url.read()\n    elif hasattr(obj, 'read'):\n        text = obj.read()\n    elif isinstance(obj, (str, bytes)):\n        text = obj\n        try:\n            if os.path.isfile(text):\n                with open(text, 'rb') as f:\n                    return f.read()\n        except (TypeError, ValueError):\n            pass\n    else:\n        raise TypeError(\"Cannot read object of type %r\" % type(obj).__name__)\n    return text", "output": "Try to read from a url, file or string.\n\n    Parameters\n    ----------\n    obj : str, unicode, or file-like\n\n    Returns\n    -------\n    raw_text : str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_lambda_package(self, package_name, path):\n        \"\"\"\n        \n        \"\"\"\n        lambda_package = lambda_packages[package_name][self.runtime]\n\n        # Trash the local version to help with package space saving\n        shutil.rmtree(os.path.join(path, package_name), ignore_errors=True)\n\n        tar = tarfile.open(lambda_package['path'], mode=\"r:gz\")\n        for member in tar.getmembers():\n            tar.extract(member, path)", "output": "Extracts the lambda package into a given path. Assumes the package exists in lambda packages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_dict(cp, dictionary):\n    '''\n    \n    '''\n    for section, keys in dictionary.items():\n        section = six.text_type(section)\n\n        if _is_defaultsect(section):\n            if six.PY2:\n                section = configparser.DEFAULTSECT\n        else:\n            cp.add_section(section)\n\n        for key, value in keys.items():\n            key = cp.optionxform(six.text_type(key))\n            if value is not None:\n                value = six.text_type(value)\n            cp.set(section, key, value)", "output": "Cribbed from python3's ConfigParser.read_dict function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_before_transform_template(self, template_dict):\n        \"\"\"\n        \n        \"\"\"\n        template = SamTemplate(template_dict)\n\n        for logicalId, api in template.iterate(SamResourceType.Api.value):\n            if api.properties.get('DefinitionBody') or api.properties.get('DefinitionUri'):\n                continue\n\n            api.properties['DefinitionBody'] = SwaggerEditor.gen_skeleton()\n            api.properties['__MANAGE_SWAGGER'] = True", "output": "Hook method that gets called before the SAM template is processed.\n        The template has passed the validation and is guaranteed to contain a non-empty \"Resources\" section.\n\n        :param dict template_dict: Dictionary of the SAM template\n        :return: Nothing", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_users(verbose=True, hashes=False):\n    '''\n    \n    '''\n    users = {} if verbose else []\n\n    if verbose:\n        # parse detailed user data\n        res = __salt__['cmd.run_all'](\n            'pdbedit --list --verbose {hashes}'.format(hashes=\"--smbpasswd-style\" if hashes else \"\"),\n        )\n\n        if res['retcode'] > 0:\n            log.error(res['stderr'] if 'stderr' in res else res['stdout'])\n            return users\n\n        user_data = {}\n        for user in res['stdout'].splitlines():\n            if user.startswith('-'):\n                if 'unix username' in user_data:\n                    users[user_data['unix username']] = user_data\n                user_data = {}\n            elif ':' in user:\n                label = user[:user.index(':')].strip().lower()\n                data = user[(user.index(':')+1):].strip()\n                user_data[label] = data\n\n        if user_data:\n            users[user_data['unix username']] = user_data\n    else:\n        # list users\n        res = __salt__['cmd.run_all']('pdbedit --list')\n\n        if res['retcode'] > 0:\n            return {'Error': res['stderr'] if 'stderr' in res else res['stdout']}\n\n        for user in res['stdout'].splitlines():\n            if ':' not in user:\n                continue\n            user_data = user.split(':')\n            if len(user_data) >= 3:\n                users.append(user_data[0])\n\n    return users", "output": "List user accounts\n\n    verbose : boolean\n        return all information\n    hashes : boolean\n        include NT HASH and LM HASH in verbose output\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pdbedit.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toListString(value):\n        \"\"\"\n        \n        \"\"\"\n        if TypeConverters._can_convert_to_list(value):\n            value = TypeConverters.toList(value)\n            if all(map(lambda v: TypeConverters._can_convert_to_string(v), value)):\n                return [TypeConverters.toString(v) for v in value]\n        raise TypeError(\"Could not convert %s to list of strings\" % value)", "output": "Convert a value to list of strings, if possible.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_join_info():\n    '''\n    \n    '''\n    info = win32net.NetGetJoinInformation()\n    status = {win32netcon.NetSetupUnknown: 'Unknown',\n              win32netcon.NetSetupUnjoined: 'Unjoined',\n              win32netcon.NetSetupWorkgroupName: 'Workgroup',\n              win32netcon.NetSetupDomainName: 'Domain'}\n    return {'Domain': info[0],\n            'DomainType': status[info[1]]}", "output": "Gets information about the domain/workgroup. This will tell you if the\n    system is joined to a domain or a workgroup\n\n    .. version-added:: 2018.3.4\n\n    Returns:\n        dict: A dictionary containing the domain/workgroup and it's status", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_key_pairs(profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    keys = conn.list_key_pairs(**libcloud_kwargs)\n\n    ret = []\n    for key in keys:\n        ret.append(_simple_key_pair(key))\n    return ret", "output": "List all the available key pair objects.\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's list_key_pairs method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_compute.list_key_pairs profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resume_follow(self, index, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if index in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'index'.\")\n        return self.transport.perform_request(\n            \"POST\", _make_path(index, \"_ccr\", \"resume_follow\"), params=params, body=body\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-post-resume-follow.html>`_\n\n        :arg index: The name of the follow index to resume following.\n        :arg body: The name of the leader index and other optional ccr related\n            parameters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh(self):\n        '''\n        \n        '''\n        # https://msdn.microsoft.com/en-us/library/windows/desktop/aa386526(v=vs.85).aspx\n        search_string = 'Type=\\'Software\\' or ' \\\n                        'Type=\\'Driver\\''\n\n        # Create searcher object\n        searcher = self._session.CreateUpdateSearcher()\n        self._session.ClientApplicationID = 'Salt: Load Updates'\n\n        # Load all updates into the updates collection\n        try:\n            results = searcher.Search(search_string)\n            if results.Updates.Count == 0:\n                log.debug('No Updates found for:\\n\\t\\t%s', search_string)\n                return 'No Updates found: {0}'.format(search_string)\n        except pywintypes.com_error as error:\n            # Something happened, raise an error\n            hr, msg, exc, arg = error.args  # pylint: disable=W0633\n            try:\n                failure_code = self.fail_codes[exc[5]]\n            except KeyError:\n                failure_code = 'Unknown Failure: {0}'.format(error)\n\n            log.error('Search Failed: %s\\n\\t\\t%s', failure_code, search_string)\n            raise CommandExecutionError(failure_code)\n\n        self._updates = results.Updates", "output": "Refresh the contents of the ``_updates`` collection. This gets all\n        updates in the Windows Update system and loads them into the collection.\n        This is the part that is slow.\n\n        Code Example:\n\n        .. code-block:: python\n\n            import salt.utils.win_update\n            wua = salt.utils.win_update.WindowsUpdateAgent()\n            wua.refresh()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_cluster_role(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_cluster_role_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.replace_cluster_role_with_http_info(name, body, **kwargs)\n            return data", "output": "replace the specified ClusterRole\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_cluster_role(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the ClusterRole (required)\n        :param V1ClusterRole body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1ClusterRole\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_locale(loc):\n    '''\n    \n    '''\n    def split(st, char):\n        '''\n        Split a string `st` once by `char`; always return a two-element list\n        even if the second element is empty.\n        '''\n        split_st = st.split(char, 1)\n        if len(split_st) == 1:\n            split_st.append('')\n        return split_st\n\n    comps = {}\n    work_st, comps['charmap'] = split(loc, ' ')\n    work_st, comps['modifier'] = split(work_st, '@')\n    work_st, comps['codeset'] = split(work_st, '.')\n    comps['language'], comps['territory'] = split(work_st, '_')\n    return comps", "output": "Split a locale specifier.  The general format is\n\n    language[_territory][.codeset][@modifier] [charmap]\n\n    For example:\n\n    ca_ES.UTF-8@valencia UTF-8", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wiki_articles(shard_id, wikis_dir=None):\n  \"\"\"\"\"\"\n  if not wikis_dir:\n    wikis_dir = WIKI_CONTENT_DIR\n  with tf.Graph().as_default():\n    dataset = tf.data.TFRecordDataset(\n        cc_utils.readahead(\n            os.path.join(wikis_dir, WIKI_CONTENT_FILE % shard_id)),\n        buffer_size=16 * 1000 * 1000)\n\n    def _parse_example(ex_ser):\n      \"\"\"Parse serialized Example containing Wikipedia article content.\"\"\"\n      features = {\n          \"url\": tf.VarLenFeature(tf.string),\n          \"title\": tf.VarLenFeature(tf.string),\n          \"section_titles\": tf.VarLenFeature(tf.string),\n          \"section_texts\": tf.VarLenFeature(tf.string),\n      }\n      ex = tf.parse_single_example(ex_ser, features)\n      for k in ex.keys():\n        ex[k] = ex[k].values\n      ex[\"url\"] = ex[\"url\"][0]\n      ex[\"title\"] = ex[\"title\"][0]\n      return ex\n\n    dataset = dataset.map(_parse_example, num_parallel_calls=32)\n    dataset = dataset.prefetch(100)\n    record_it = dataset.make_one_shot_iterator().get_next()\n\n    with tf.Session() as sess:\n      while True:\n        try:\n          ex = sess.run(record_it)\n        except tf.errors.OutOfRangeError:\n          break\n\n        sections = [\n            WikipediaSection(title=text_encoder.to_unicode(title),\n                             text=text_encoder.to_unicode(text))\n            for title, text in zip(ex[\"section_titles\"], ex[\"section_texts\"])\n        ]\n        yield WikipediaArticle(\n            url=text_encoder.to_unicode(ex[\"url\"]),\n            title=text_encoder.to_unicode(ex[\"title\"]),\n            sections=sections)", "output": "Generates WikipediaArticles from GCS that are part of shard shard_id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_image_list(self, dataset, fns_idxs):\n        \"\"\n        items = dataset.x.items\n        if self._duplicates:\n            chunked_idxs = chunks(fns_idxs, 2)\n            chunked_idxs = [chunk for chunk in chunked_idxs if Path(items[chunk[0]]).is_file() and Path(items[chunk[1]]).is_file()]\n            return  [(dataset.x[i]._repr_jpeg_(), items[i], self._labels[dataset.y[i].data]) for chunk in chunked_idxs for i in chunk]\n        else:\n            return [(dataset.x[i]._repr_jpeg_(), items[i], self._labels[dataset.y[i].data]) for i in fns_idxs if\n                    Path(items[i]).is_file()]", "output": "Create a list of images, filenames and labels but first removing files that are not supposed to be displayed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nalu(x, depth, epsilon=1e-30, name=None, reuse=None):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name, default_name=\"nalu\", values=[x], reuse=reuse):\n    x_shape = shape_list(x)\n    x_flat = tf.reshape(x, [-1, x_shape[-1]])\n    gw = tf.get_variable(\"w\", [x_shape[-1], depth])\n    g = tf.nn.sigmoid(tf.matmul(x_flat, gw))\n    g = tf.reshape(g, x_shape[:-1] + [depth])\n    a = nac(x, depth, name=\"nac_lin\")\n    log_x = tf.log(tf.abs(x) + epsilon)\n    m = nac(log_x, depth, name=\"nac_log\")\n    return g * a + (1 - g) * tf.exp(m)", "output": "NALU as in https://arxiv.org/abs/1808.00508.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_squeeze(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = attrs.get(\"axis\", None)\n    if not axis:\n        raise AttributeError(\"Squeeze: Missing axis attribute: ONNX currently requires axis to \"\n                             \"be specified for squeeze operator\")\n    axis = convert_string_to_list(axis)\n\n    node = onnx.helper.make_node(\n        \"Squeeze\",\n        input_nodes,\n        [name],\n        axes=axis,\n        name=name,\n    )\n    return [node]", "output": "Map MXNet's squeeze operator attributes to onnx's squeeze operator\n    and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_reporter(redis_address,\n                   stdout_file=None,\n                   stderr_file=None,\n                   redis_password=None):\n    \"\"\"\n    \"\"\"\n    reporter_filepath = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"reporter.py\")\n    command = [\n        sys.executable, \"-u\", reporter_filepath,\n        \"--redis-address={}\".format(redis_address)\n    ]\n    if redis_password:\n        command += [\"--redis-password\", redis_password]\n\n    try:\n        import psutil  # noqa: F401\n    except ImportError:\n        logger.warning(\"Failed to start the reporter. The reporter requires \"\n                       \"'pip install psutil'.\")\n        return None\n\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_REPORTER,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info", "output": "Start a reporter process.\n\n    Args:\n        redis_address (str): The address of the Redis instance.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        redis_password (str): The password of the redis server.\n\n    Returns:\n        ProcessInfo for the process that was started.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_extmods():\n    '''\n    \n    '''\n    ret = {}\n    ext_dir = os.path.join(__opts__['cachedir'], 'extmods')\n    mod_types = os.listdir(ext_dir)\n    for mod_type in mod_types:\n        ret[mod_type] = set()\n        for _, _, files in salt.utils.path.os_walk(os.path.join(ext_dir, mod_type)):\n            for fh_ in files:\n                ret[mod_type].add(fh_.split('.')[0])\n        ret[mod_type] = list(ret[mod_type])\n    return ret", "output": ".. versionadded:: 2017.7.0\n\n    List Salt modules which have been synced externally\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.list_extmods", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preprocess_frame(frame):\n  \"\"\"\n  \"\"\"\n  # Normalize from [0.0, 1.0] -> [-0.5, 0.5]\n  frame = common_layers.convert_rgb_to_real(frame)\n  frame = frame - 0.5\n  frame, _ = glow_ops.uniform_binning_correction(frame)\n  return frame", "output": "Preprocess frame.\n\n  1. Converts [0, 255] to [-0.5, 0.5]\n  2. Adds uniform noise.\n\n  Args:\n    frame: 3-D Tensor representing pixels.\n  Returns:\n    frame: 3-D Tensor with values in between [-0.5, 0.5]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_loadbalancers(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-loadbalancers option'\n        )\n\n    ret = {}\n    conn = get_conn()\n    datacenter = get_datacenter(conn)\n\n    for item in conn.list_loadbalancers(datacenter['id'])['items']:\n        lb = {'id': item['id']}\n        lb.update(item['properties'])\n        ret[lb['name']] = lb\n\n    return ret", "output": "Return a list of the loadbalancers that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_boolean(self, b):\n        \"\"\"\n        \n        \"\"\"\n        if b:\n            self.packet.write(one_byte)\n        else:\n            self.packet.write(zero_byte)\n        return self", "output": "Add a boolean value to the stream.\n\n        :param bool b: boolean value to add", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def conv_block(inputs, filters, dilation_rates_and_kernel_sizes, **kwargs):\n  \"\"\"\"\"\"\n  return conv_block_internal(conv, inputs, filters,\n                             dilation_rates_and_kernel_sizes, **kwargs)", "output": "A block of standard 2d convolutions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _error_result_to_exception(error_result):\n    \"\"\"\n    \"\"\"\n    reason = error_result.get(\"reason\")\n    status_code = _ERROR_REASON_TO_EXCEPTION.get(\n        reason, http_client.INTERNAL_SERVER_ERROR\n    )\n    return exceptions.from_http_status(\n        status_code, error_result.get(\"message\", \"\"), errors=[error_result]\n    )", "output": "Maps BigQuery error reasons to an exception.\n\n    The reasons and their matching HTTP status codes are documented on\n    the `troubleshooting errors`_ page.\n\n    .. _troubleshooting errors: https://cloud.google.com/bigquery\\\n        /troubleshooting-errors\n\n    :type error_result: Mapping[str, str]\n    :param error_result: The error result from BigQuery.\n\n    :rtype google.cloud.exceptions.GoogleCloudError:\n    :returns: The mapped exception.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _connect(uri):\n    '''\n    \n    '''\n    # cx_Oracle.Connection() does not support 'as sysdba' syntax\n    uri_l = uri.rsplit(' as ', 1)\n    if len(uri_l) == 2:\n        credentials, mode = uri_l\n        mode = MODE[mode]\n    else:\n        credentials = uri_l[0]\n        mode = 0\n    # force UTF-8 client encoding\n    os.environ['NLS_LANG'] = '.AL32UTF8'\n    if '@' in uri:\n        serv_name = False\n        userpass, hostportsid = credentials.split('@')\n        user, password = userpass.split('/')\n        hostport, sid = hostportsid.split('/')\n        if 'servicename' in sid:\n            serv_name = True\n            sid = sid.split('servicename')[0].strip()\n        hostport_l = hostport.split(':')\n        if len(hostport_l) == 2:\n            host, port = hostport_l\n        else:\n            host = hostport_l[0]\n            port = 1521\n        log.debug('connect: %s', (user, password, host, port, sid, mode))\n        if serv_name:\n            conn = cx_Oracle.connect(user, password, cx_Oracle.makedsn(host, port, service_name=sid), mode)\n        else:\n            conn = cx_Oracle.connect(user, password, cx_Oracle.makedsn(host, port, sid), mode)\n    else:\n        sid = uri.rsplit(' as ', 1)[0]\n        orahome = _parse_oratab(sid)\n        if orahome:\n            os.environ['ORACLE_HOME'] = orahome\n        else:\n            raise CommandExecutionError('No uri defined and SID {0} not found in oratab'.format(sid))\n        os.environ['ORACLE_SID'] = sid\n        log.debug('connect: %s', (sid, mode))\n        conn = cx_Oracle.connect(mode=MODE['sysdba'])\n    conn.outputtypehandler = _unicode_output\n    return conn", "output": "uri = user/password@host[:port]/sid[servicename as {sysdba|sysoper}]\n     or\n    uri = sid[ as {sysdba|sysoper}]\n     (this syntax only makes sense on non-Windows minions, ORAHOME is taken from oratab)\n\n    Return cx_Oracle.Connection instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_shells():\n    '''\n    \n    '''\n    start = time.time()\n    if 'sh.last_shells' in __context__:\n        if start - __context__['sh.last_shells'] > 5:\n            __context__['sh.last_shells'] = start\n        else:\n            __context__['sh.shells'] = __salt__['cmd.shells']()\n    else:\n        __context__['sh.last_shells'] = start\n        __context__['sh.shells'] = __salt__['cmd.shells']()\n    return __context__['sh.shells']", "output": "Return the valid shells on this system", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _merge_simple(adjustment_lists, front_idx, back_idx):\n    \"\"\"\n    \n    \"\"\"\n    if len(adjustment_lists) == 1:\n        return list(adjustment_lists[0])\n    else:\n        return adjustment_lists[front_idx] + adjustment_lists[back_idx]", "output": "Merge lists of new and existing adjustments for a given index by appending\n    or prepending new adjustments to existing adjustments.\n\n    Notes\n    -----\n    This method is meant to be used with ``toolz.merge_with`` to merge\n    adjustment mappings. In case of a collision ``adjustment_lists`` contains\n    two lists, existing adjustments at index 0 and new adjustments at index 1.\n    When there are no collisions, ``adjustment_lists`` contains a single list.\n\n    Parameters\n    ----------\n    adjustment_lists : list[list[Adjustment]]\n        List(s) of new and/or existing adjustments for a given index.\n    front_idx : int\n        Index of list in ``adjustment_lists`` that should be used as baseline\n        in case of a collision.\n    back_idx : int\n        Index of list in ``adjustment_lists`` that should extend baseline list\n        in case of a collision.\n\n    Returns\n    -------\n    adjustments : list[Adjustment]\n        List of merged adjustments for a given index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tmpconfig(request):\n    \"\"\"\n    \n    \"\"\"\n    SUBFOLDER = tempfile.mkdtemp()\n    CONF = UserConfig('spyder-test',\n                      defaults=DEFAULTS,\n                      version=CONF_VERSION,\n                      subfolder=SUBFOLDER,\n                      raw_mode=True,\n                      )\n\n    def fin():\n        \"\"\"\n        Fixture finalizer to delete the temporary CONF element.\n        \"\"\"\n        shutil.rmtree(SUBFOLDER)\n\n    request.addfinalizer(fin)\n    return CONF", "output": "Fixtures that returns a temporary CONF element.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_list(list_aliases=False, remote_addr=None,\n               cert=None, key=None, verify_cert=True):\n    ''' \n    '''\n    client = pylxd_client_get(remote_addr, cert, key, verify_cert)\n\n    images = client.images.all()\n    if list_aliases:\n        return {i.fingerprint: [a['name'] for a in i.aliases] for i in images}\n\n    return map(_pylxd_model_to_dict, images)", "output": "Lists all images from the LXD.\n\n        list_aliases :\n\n            Return a dict with the fingerprint as key and\n            a list of aliases as value instead.\n\n        remote_addr :\n            An URL to a remote Server, you also have to give cert and key if\n            you provide remote_addr and its a TCP Address!\n\n            Examples:\n                https://myserver.lan:8443\n                /var/lib/mysocket.sock\n\n        cert :\n            PEM Formatted SSL Certificate.\n\n            Examples:\n                ~/.config/lxc/client.crt\n\n        key :\n            PEM Formatted SSL Key.\n\n            Examples:\n                ~/.config/lxc/client.key\n\n        verify_cert : True\n            Wherever to verify the cert, this is by default True\n            but in the most cases you want to set it off as LXD\n            normaly uses self-signed certificates.\n\n        CLI Examples:\n\n        .. code-block:: bash\n\n            $ salt '*' lxd.image_list true --out=json\n            $ salt '*' lxd.image_list --out=json", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_headers(event):\n    \"\"\"\n    \n    \"\"\"\n    headers = event.get('headers') or {}\n    multi_headers = (event.get('multiValueHeaders') or {}).copy()\n    for h in set(headers.keys()):\n        if h not in multi_headers:\n            multi_headers[h] = [headers[h]]\n    for h in multi_headers.keys():\n        multi_headers[h] = ', '.join(multi_headers[h])\n    return multi_headers", "output": "Merge the values of headers and multiValueHeaders into a single dict.\n    Opens up support for multivalue headers via API Gateway and ALB.\n    See: https://github.com/Miserlou/Zappa/pull/1756", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, mode, metric):\n    \"\"\"\"\"\"\n    if mode not in self._values:\n      logging.info(\"Metric %s not found for mode %s\", metric, mode)\n      return []\n    return list(self._values[mode][metric])", "output": "Get the history for the given metric and mode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_device(name, safety_on=True):\n    '''\n    \n\n    '''\n\n    config = _get_vistara_configuration()\n    if not config:\n        return False\n\n    access_token = _get_oath2_access_token(config['client_key'], config['client_secret'])\n\n    if not access_token:\n        return 'Vistara access token not available'\n\n    query_string = 'dnsName:{0}'.format(name)\n\n    devices = _search_devices(query_string, config['client_id'], access_token)\n\n    if not devices:\n        return \"No devices found\"\n\n    device_count = len(devices)\n\n    if safety_on and device_count != 1:\n        return \"Expected to delete 1 device and found {0}. \"\\\n            \"Set safety_on=False to override.\".format(device_count)\n\n    delete_responses = []\n    for device in devices:\n        device_id = device['id']\n        log.debug(device_id)\n        delete_response = _delete_resource(device_id, config['client_id'], access_token)\n        if not delete_response:\n            return False\n        delete_responses.append(delete_response)\n\n    return delete_responses", "output": "Deletes a device from Vistara based on DNS name or partial name. By default,\n    delete_device will only perform the delete if a single host is returned. Set\n    safety_on=False to delete all matches (up to default API search page size)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run vistara.delete_device 'hostname-101.mycompany.com'\n        salt-run vistara.delete_device 'hostname-101'\n        salt-run vistara.delete_device 'hostname-1' safety_on=False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aux_dict(self):\n        \"\"\"\n        \"\"\"\n        if self._aux_dict is None:\n            self._aux_dict = Executor._get_dict(\n                self._symbol.list_auxiliary_states(), self.aux_arrays)\n        return self._aux_dict", "output": "Get dictionary representation of auxiliary states arrays.\n\n        Returns\n        -------\n        aux_dict : dict of str to NDArray\n            The dictionary that maps name of auxiliary states to NDArrays.\n\n        Raises\n        ------\n        ValueError : if there are duplicated names in the auxiliary states.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def openStream(self, source):\n        \"\"\"\n\n        \"\"\"\n        # Already a file object\n        if hasattr(source, 'read'):\n            stream = source\n        else:\n            stream = BytesIO(source)\n\n        try:\n            stream.seek(stream.tell())\n        except:  # pylint:disable=bare-except\n            stream = BufferedStream(stream)\n\n        return stream", "output": "Produces a file object from source.\n\n        source can be either a file object, local filename or a string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _td_array_cmp(cls, op):\n    \"\"\"\n    \n    \"\"\"\n    opname = '__{name}__'.format(name=op.__name__)\n    nat_result = opname == '__ne__'\n\n    def wrapper(self, other):\n        if isinstance(other, (ABCDataFrame, ABCSeries, ABCIndexClass)):\n            return NotImplemented\n\n        if _is_convertible_to_td(other) or other is NaT:\n            try:\n                other = Timedelta(other)\n            except ValueError:\n                # failed to parse as timedelta\n                return ops.invalid_comparison(self, other, op)\n\n            result = op(self.view('i8'), other.value)\n            if isna(other):\n                result.fill(nat_result)\n\n        elif not is_list_like(other):\n            return ops.invalid_comparison(self, other, op)\n\n        elif len(other) != len(self):\n            raise ValueError(\"Lengths must match\")\n\n        else:\n            try:\n                other = type(self)._from_sequence(other)._data\n            except (ValueError, TypeError):\n                return ops.invalid_comparison(self, other, op)\n\n            result = op(self.view('i8'), other.view('i8'))\n            result = com.values_from_object(result)\n\n            o_mask = np.array(isna(other))\n            if o_mask.any():\n                result[o_mask] = nat_result\n\n        if self._hasnans:\n            result[self._isnan] = nat_result\n\n        return result\n\n    return compat.set_function_name(wrapper, opname, cls)", "output": "Wrap comparison operations to convert timedelta-like to timedelta64", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lmx_h4k_f16k():\n  \"\"\"\"\"\"\n  hparams = lmx_base()\n  hparams.hidden_size = 4096\n  hparams.filter_size = 16384\n  hparams.batch_size = 1024\n  hparams.weight_dtype = \"bfloat16\"\n  return hparams", "output": "HParams for training languagemodel_lm1b32k_packed.  1470M Params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deprecated(since_or_msg, old=None, new=None, extra=None):\n    \"\"\"  \"\"\"\n\n    if isinstance(since_or_msg, tuple):\n        if old is None or new is None:\n            raise ValueError(\"deprecated entity and a replacement are required\")\n\n        if len(since_or_msg) != 3 or not all(isinstance(x, int) and x >=0 for x in since_or_msg):\n            raise ValueError(\"invalid version tuple: %r\" % (since_or_msg,))\n\n        since = \"%d.%d.%d\" % since_or_msg\n        message = \"%(old)s was deprecated in Bokeh %(since)s and will be removed, use %(new)s instead.\"\n        message = message % dict(old=old, since=since, new=new)\n        if extra is not None:\n            message += \" \" + extra.strip()\n    elif isinstance(since_or_msg, six.string_types):\n        if not (old is None and new is None and extra is None):\n            raise ValueError(\"deprecated(message) signature doesn't allow extra arguments\")\n\n        message = since_or_msg\n    else:\n        raise ValueError(\"expected a version tuple or string message\")\n\n    warn(message)", "output": "Issue a nicely formatted deprecation warning.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_nested_list_like(obj):\n    \"\"\"\n    \n    \"\"\"\n    return (is_list_like(obj) and hasattr(obj, '__len__') and\n            len(obj) > 0 and all(is_list_like(item) for item in obj))", "output": "Check if the object is list-like, and that all of its elements\n    are also list-like.\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    obj : The object to check\n\n    Returns\n    -------\n    is_list_like : bool\n        Whether `obj` has list-like properties.\n\n    Examples\n    --------\n    >>> is_nested_list_like([[1, 2, 3]])\n    True\n    >>> is_nested_list_like([{1, 2, 3}, {1, 2, 3}])\n    True\n    >>> is_nested_list_like([\"foo\"])\n    False\n    >>> is_nested_list_like([])\n    False\n    >>> is_nested_list_like([[1, 2, 3], 1])\n    False\n\n    Notes\n    -----\n    This won't reliably detect whether a consumable iterator (e. g.\n    a generator) is a nested-list-like without consuming the iterator.\n    To avoid consuming it, we always return False if the outer container\n    doesn't define `__len__`.\n\n    See Also\n    --------\n    is_list_like", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_value(self, index, value):\r\n        \"\"\"\"\"\"\r\n        self._data[ self.keys[index.row()] ] = value\r\n        self.showndata[ self.keys[index.row()] ] = value\r\n        self.sizes[index.row()] = get_size(value)\r\n        self.types[index.row()] = get_human_readable_type(value)\r\n        self.sig_setting_data.emit()", "output": "Set value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def capture_model_internals(self) -> Iterator[dict]:\n        \"\"\"\n        \n        \"\"\"\n        results = {}\n        hooks = []\n\n        # First we'll register hooks to add the outputs of each module to the results dict.\n        def add_output(idx: int):\n            def _add_output(mod, _, outputs):\n                results[idx] = {\"name\": str(mod), \"output\": sanitize(outputs)}\n            return _add_output\n\n        for idx, module in enumerate(self._model.modules()):\n            if module != self._model:\n                hook = module.register_forward_hook(add_output(idx))\n                hooks.append(hook)\n\n        # If you capture the return value of the context manager, you get the results dict.\n        yield results\n\n        # And then when you exit the context we remove all the hooks.\n        for hook in hooks:\n            hook.remove()", "output": "Context manager that captures the internal-module outputs of\n        this predictor's model. The idea is that you could use it as follows:\n\n        .. code-block:: python\n\n            with predictor.capture_model_internals() as internals:\n                outputs = predictor.predict_json(inputs)\n\n            return {**outputs, \"model_internals\": internals}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _depend_on_lambda_permissions(self, bucket, permission):\n        \"\"\"\n        \n        \"\"\"\n\n        depends_on = bucket.get(\"DependsOn\", [])\n\n        # DependsOn can be either a list of strings or a scalar string\n        if isinstance(depends_on, string_types):\n            depends_on = [depends_on]\n\n        depends_on_set = set(depends_on)\n        depends_on_set.add(permission.logical_id)\n        bucket[\"DependsOn\"] = list(depends_on_set)\n\n        return bucket", "output": "Make the S3 bucket depends on Lambda Permissions resource because when S3 adds a Notification Configuration,\n        it will check whether it has permissions to access Lambda. This will fail if the Lambda::Permissions is not\n        already applied for this bucket to invoke the Lambda.\n\n        :param dict bucket: Dictionary representing the bucket in SAM template. This is a raw dictionary and not a\n            \"resource\" object\n        :param model.lambda_.lambda_permission permission: Lambda Permission resource that needs to be created before\n            the bucket.\n        :return: Modified Bucket dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_extremum_session_metrics(session_group, aggregation_metric,\n                                  extremum_fn):\n  \"\"\"\n  \"\"\"\n  measurements = _measurements(session_group, aggregation_metric)\n  ext_session = extremum_fn(\n      measurements,\n      key=operator.attrgetter('metric_value.value')).session_index\n  del session_group.metric_values[:]\n  session_group.metric_values.MergeFrom(\n      session_group.sessions[ext_session].metric_values)", "output": "Sets the metrics for session_group to those of its \"extremum session\".\n\n  The extremum session is the session in session_group with the extremum value\n  of the metric given by 'aggregation_metric'. The extremum is taken over the\n  subset of sessions in the group whose 'aggregation_metric' was measured\n  at the largest training step among the sessions in the group.\n\n  Args:\n    session_group: A SessionGroup protobuffer.\n    aggregation_metric: A MetricName protobuffer.\n    extremum_fn: callable. Must be either 'min' or 'max'. Determines the type of\n      extremum to compute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init(opts):\n    '''\n    \n    '''\n    if CONFIG_BASE_URL in opts['proxy']:\n        CONFIG[CONFIG_BASE_URL] = opts['proxy'][CONFIG_BASE_URL]\n    else:\n        log.error('missing proxy property %s', CONFIG_BASE_URL)\n    log.debug('CONFIG: %s', CONFIG)", "output": "Perform any needed setup.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(_):\n  \"\"\"\"\"\"\n  # Images for inception classifier are normalized to be in [-1, 1] interval,\n  # eps is a difference between pixels so it should be in [0, 2] interval.\n  # Renormalizing epsilon from [0, 255] to [0, 2].\n  eps = 2.0 * FLAGS.max_epsilon / 255.0\n  batch_shape = [FLAGS.batch_size, FLAGS.image_height, FLAGS.image_width, 3]\n  nb_classes = 1001\n\n  tf.logging.set_verbosity(tf.logging.INFO)\n\n  with tf.Graph().as_default():\n    # Prepare graph\n    x_input = tf.placeholder(tf.float32, shape=batch_shape)\n\n    model = InceptionModel(nb_classes)\n\n    fgsm = FastGradientMethod(model)\n    x_adv = fgsm.generate(x_input, eps=eps, clip_min=-1., clip_max=1.)\n\n    # Run computation\n    saver = tf.train.Saver(slim.get_model_variables())\n    session_creator = tf.train.ChiefSessionCreator(\n        scaffold=tf.train.Scaffold(saver=saver),\n        checkpoint_filename_with_path=FLAGS.checkpoint_path,\n        master=FLAGS.master)\n\n    with tf.train.MonitoredSession(session_creator=session_creator) as sess:\n      for filenames, images in load_images(FLAGS.input_dir, batch_shape):\n        adv_images = sess.run(x_adv, feed_dict={x_input: images})\n        save_images(adv_images, filenames, FLAGS.output_dir)", "output": "Run the sample attack", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newGlobalNs(self, href, prefix):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlNewGlobalNs(self._o, href, prefix)\n        if ret is None:raise treeError('xmlNewGlobalNs() failed')\n        __tmp = xmlNs(_obj=ret)\n        return __tmp", "output": "Creation of a Namespace, the old way using PI and without\n           scoping DEPRECATED !!!", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config(self):\n        \"\"\"\n        \"\"\"\n        config = super(SequentialMemory, self).get_config()\n        config['limit'] = self.limit\n        return config", "output": "Return configurations of SequentialMemory\n\n        # Returns\n            Dict of config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_query_columns(engine, query):\n    \"\"\" \n    \"\"\"\n    con = engine.connect()\n    result = con.execute(query).fetchone()\n    values = list(result)\n    cols_names = result.keys()\n    cols = OrderedDict()\n    for i in range(len(cols_names)):\n        cols[cols_names[i]] = type(values[i]).__name__\n    return cols", "output": "Extract columns names and python typos from query\n\n    Args:\n        engine: SQLAlchemy connection engine\n        query: SQL query\n\n    Returns:\n        dict with columns names and python types", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_pillar(self):\n        '''\n        \n        '''\n        log.debug('Pillar cache getting external pillar with ext: %s', self.ext)\n        fresh_pillar = Pillar(self.opts,\n                              self.grains,\n                              self.minion_id,\n                              self.saltenv,\n                              ext=self.ext,\n                              functions=self.functions,\n                              pillarenv=self.pillarenv)\n        return fresh_pillar.compile_pillar()", "output": "In the event of a cache miss, we need to incur the overhead of caching\n        a new pillar.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_clusters(self):\n        \"\"\"\n        \"\"\"\n        resp = self.instance_admin_client.list_clusters(\n            self.instance_admin_client.instance_path(self.project, \"-\")\n        )\n        clusters = []\n        instances = {}\n        for cluster in resp.clusters:\n            match_cluster_name = _CLUSTER_NAME_RE.match(cluster.name)\n            instance_id = match_cluster_name.group(\"instance\")\n            if instance_id not in instances:\n                instances[instance_id] = self.instance(instance_id)\n            clusters.append(Cluster.from_pb(cluster, instances[instance_id]))\n        return clusters, resp.failed_locations", "output": "List the clusters in the project.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_list_clusters_in_project]\n            :end-before: [END bigtable_list_clusters_in_project]\n\n        :rtype: tuple\n        :returns:\n            (clusters, failed_locations), where 'clusters' is list of\n            :class:`google.cloud.bigtable.instance.Cluster`, and\n            'failed_locations' is a list of strings representing\n            locations which could not be resolved.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_directory(path):\n    \"\"\"\"\"\"\n    dirname = os.path.dirname(path)\n    py31compat.makedirs(dirname, exist_ok=True)", "output": "Ensure that the parent directory of `path` exists", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_std_icons():\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    app = qapplication()\r\n    dialog = ShowStdIcons(None)\r\n    dialog.show()\r\n    sys.exit(app.exec_())", "output": "Show all standard Icons", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_event_source_mapping(UUID,\n                                FunctionName=None, Enabled=None, BatchSize=None,\n                                region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        args = {}\n        if FunctionName is not None:\n            args['FunctionName'] = FunctionName\n        if Enabled is not None:\n            args['Enabled'] = Enabled\n        if BatchSize is not None:\n            args['BatchSize'] = BatchSize\n        r = conn.update_event_source_mapping(UUID=UUID, **args)\n        if r:\n            keys = ('UUID', 'BatchSize', 'EventSourceArn',\n                    'FunctionArn', 'LastModified', 'LastProcessingResult',\n                    'State', 'StateTransitionReason')\n            return {'updated': True, 'event_source_mapping': dict([(k, r.get(k)) for k in keys])}\n        else:\n            log.warning('Mapping was not updated')\n            return {'updated': False}\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Update the event source mapping identified by the UUID.\n\n    Returns {updated: true} if the alias was updated and returns\n    {updated: False} if the alias was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_lamba.update_event_source_mapping uuid FunctionName=new_function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def modify_write(self, write_pb, **unused_kwargs):\n        \"\"\"\n        \"\"\"\n        current_doc = types.Precondition(update_time=self._last_update_time)\n        write_pb.current_document.CopyFrom(current_doc)", "output": "Modify a ``Write`` protobuf based on the state of this write option.\n\n        The ``last_update_time`` is added to ``write_pb`` as an \"update time\"\n        precondition. When set, the target document must exist and have been\n        last updated at that time.\n\n        Args:\n            write_pb (google.cloud.firestore_v1beta1.types.Write): A\n                ``Write`` protobuf instance to be modified with a precondition\n                determined by the state of this option.\n            unused_kwargs (Dict[str, Any]): Keyword arguments accepted by\n                other subclasses that are unused here.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def updated(bank, key, cachedir):\n    '''\n    \n    '''\n    key_file = os.path.join(cachedir, os.path.normpath(bank), '{0}.p'.format(key))\n    if not os.path.isfile(key_file):\n        log.warning('Cache file \"%s\" does not exist', key_file)\n        return None\n    try:\n        return int(os.path.getmtime(key_file))\n    except IOError as exc:\n        raise SaltCacheError(\n            'There was an error reading the mtime for \"{0}\": {1}'.format(\n                key_file, exc\n            )\n        )", "output": "Return the epoch of the mtime for this cache file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _encode_sequence(self, inputs, token_types, valid_length=None):\n        \"\"\"\n        \"\"\"\n        # embedding\n        word_embedding = self.word_embed(inputs)\n        type_embedding = self.token_type_embed(token_types)\n        embedding = word_embedding + type_embedding\n        # encoding\n        outputs, additional_outputs = self.encoder(embedding, None, valid_length)\n        return outputs, additional_outputs", "output": "Generate the representation given the input sequences.\n\n        This is used for pre-training or fine-tuning a BERT model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def require_initialized(exception):\n    \"\"\"\n    \n    \"\"\"\n    def decorator(method):\n        @wraps(method)\n        def wrapped_method(self, *args, **kwargs):\n            if not self.initialized:\n                raise exception\n            return method(self, *args, **kwargs)\n        return wrapped_method\n    return decorator", "output": "Decorator for API methods that should only be called after\n    TradingAlgorithm.initialize.  `exception` will be raised if the method is\n    called before initialize has completed.\n\n    Examples\n    --------\n    @require_initialized(SomeException(\"Don't do that!\"))\n    def method(self):\n        # Do stuff that should only be allowed after initialize.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer_game_name_from_filenames(data_dir, snake_case=True):\n  \"\"\"\"\"\"\n  names = os.listdir(data_dir)\n  game_names = [re.findall(pattern=r\"^Gym(.*)NoFrameskip\", string=name)\n                for name in names]\n  assert game_names, \"No data files found in {}\".format(data_dir)\n  game_names = sum(game_names, [])\n  game_name = game_names[0]\n  assert all(game_name == other for other in game_names), \\\n      \"There are multiple different game names in {}\".format(data_dir)\n  if snake_case:\n    game_name = camelcase_to_snakecase(game_name)\n  return game_name", "output": "Infer name from filenames.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_sizes(conn=None, call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_sizes function must be called with '\n            '-f or --function, or with the --list-sizes option'\n        )\n\n    if not conn:\n        conn = get_conn()   # pylint: disable=E0602\n\n    sizes = conn.list_sizes()\n    ret = {}\n    for size in sizes:\n        if isinstance(size.name, six.string_types) and not six.PY3:\n            size_name = size.name.encode('ascii', 'salt-cloud-force-ascii')\n        else:\n            size_name = str(size.name)  # future lint: disable=blacklisted-function\n\n        ret[size_name] = {}\n        for attr in dir(size):\n            if attr.startswith('_') or attr in ('driver', 'get_uuid'):\n                continue\n\n            try:\n                attr_value = getattr(size, attr)\n            except Exception:\n                pass\n\n            if isinstance(attr_value, six.string_types) and not six.PY3:\n                attr_value = attr_value.encode(\n                    'ascii', 'salt-cloud-force-ascii'\n                )\n            ret[size_name][attr] = attr_value\n    return ret", "output": "Return a dict of all available VM images on the cloud provider with\n    relevant data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validator(code_or_name, validator_type):\n    ''' \n\n    '''\n    if validator_type == \"error\":\n        from .errors import codes\n        from .errors import EXT\n    elif validator_type == \"warning\":\n        from .warnings import codes\n        from .warnings import EXT\n    else:\n        pass # TODO (bev) ValueError?\n\n    def decorator(func):\n        def wrapper(*args, **kw):\n            extra = func(*args, **kw)\n            if extra is None: return []\n            if isinstance(code_or_name, string_types):\n                code = EXT\n                name = codes[code][0] + \":\" + code_or_name\n            else:\n                code = code_or_name\n                name = codes[code][0]\n            text = codes[code][1]\n            return [(code, name, text, extra)]\n        wrapper.validator_type = validator_type\n        return wrapper\n\n    return decorator", "output": "Internal shared implementation to handle both error and warning\n    validation checks.\n\n    Args:\n        code code_or_name (int or str) : a defined error code or custom message\n        validator_type (str) : either \"error\" or \"warning\"\n\n    Returns:\n        validation decorator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enterEvent(self, event):\n        \"\"\" \n        \"\"\"\n        super(CallTipWidget, self).enterEvent(event)\n        if self.as_tooltip:\n            self.hide()\n\n        if (self._hide_timer.isActive() and\n          self.app.topLevelAt(QCursor.pos()) == self):\n            self._hide_timer.stop()", "output": "Reimplemented to cancel the hide timer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_crop_target(target_px:Union[int,TensorImageSize], mult:int=None)->Tuple[int,int]:\n    \"\"\n    target_r,target_c = tis2hw(target_px)\n    return _round_multiple(target_r,mult),_round_multiple(target_c,mult)", "output": "Calc crop shape of `target_px` to nearest multiple of `mult`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_vs(lb, name, ip, port, protocol, profile, pool_name):\n    '''\n    \n    '''\n    if __opts__['load_balancers'].get(lb, None):\n        (username, password) = list(__opts__['load_balancers'][lb].values())\n    else:\n        raise Exception('Unable to find `{0}` load balancer'.format(lb))\n\n    F5 = F5Mgmt(lb, username, password)\n    F5.create_vs(name, ip, port, protocol, profile, pool_name)\n    return True", "output": "Create a virtual server\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-run f5.create_vs lbalancer vs_name 10.0.0.1 80 tcp http poolname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_storage_policy(profile_manager, policy, policy_spec):\n    '''\n    \n    '''\n    try:\n        profile_manager.Update(policy.profileId, policy_spec)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)", "output": "Updates a storage policy.\n\n    profile_manager\n        Reference to the profile manager.\n\n    policy\n        Reference to the policy to be updated.\n\n    policy_spec\n        Policy update spec.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def context(self, context):\n        \"\"\"\n        \"\"\"\n        initial_context = self.execute('GET_CONTEXT').pop('value')\n        self.set_context(context)\n        try:\n            yield\n        finally:\n            self.set_context(initial_context)", "output": "Sets the context that Selenium commands are running in using\n        a `with` statement. The state of the context on the server is\n        saved before entering the block, and restored upon exiting it.\n\n        :param context: Context, may be one of the class properties\n            `CONTEXT_CHROME` or `CONTEXT_CONTENT`.\n\n        Usage example::\n\n            with selenium.context(selenium.CONTEXT_CHROME):\n                # chrome scope\n                ... do stuff ...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_symbol_function(handle, name, func_name):\n    \"\"\"\"\"\"\n    code, doc_str = _generate_symbol_function_code(handle, name, func_name)\n\n    local = {}\n    exec(code, None, local)  # pylint: disable=exec-used\n    symbol_function = local[func_name]\n    symbol_function.__name__ = func_name\n    symbol_function.__doc__ = doc_str\n    symbol_function.__module__ = 'mxnet.symbol'\n    return symbol_function", "output": "Create a symbol function by handle and function name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name):\n    '''\n    \n    '''\n    if _sd_version() >= 219:\n        ret = _machinectl('start {0}'.format(name))\n    else:\n        cmd = 'systemctl start systemd-nspawn@{0}'.format(name)\n        ret = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    if ret['retcode'] != 0:\n        __context__['retcode'] = salt.defaults.exitcodes.EX_UNAVAILABLE\n        return False\n    return True", "output": "Start the named container\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion nspawn.start <name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flush(set=None, family='ipv4'):\n    '''\n    \n    '''\n\n    settype = _find_set_type(set)\n    if not settype:\n        return 'Error: Set {0} does not exist'.format(set)\n\n    ipset_family = _IPSET_FAMILIES[family]\n    if set:\n        cmd = '{0} flush {1}'.format(_ipset_cmd(), set)\n    else:\n        cmd = '{0} flush'.format(_ipset_cmd())\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n\n    return not out", "output": "Flush entries in the specified set,\n    Flush all sets if set is not specified.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ipset.flush\n\n        salt '*' ipset.flush set\n\n        IPv6:\n        salt '*' ipset.flush\n\n        salt '*' ipset.flush set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_default_format(self, vmin, vmax):\n        \"\"\n\n        if self.plot_obj.date_axis_info is None:\n            self.plot_obj.date_axis_info = self.finder(vmin, vmax, self.freq)\n        info = self.plot_obj.date_axis_info\n\n        if self.isminor:\n            format = np.compress(info['min'] & np.logical_not(info['maj']),\n                                 info)\n        else:\n            format = np.compress(info['maj'], info)\n        self.formatdict = {x: f for (x, _, _, f) in format}\n        return self.formatdict", "output": "Returns the default ticks spacing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_cloudformation(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        resources = []\n\n        function = kwargs.get('function')\n\n        if not function:\n            raise TypeError(\"Missing required keyword argument: function\")\n\n        if self.Method is not None:\n            # Convert to lower case so that user can specify either GET or get\n            self.Method = self.Method.lower()\n\n        resources.extend(self._get_permissions(kwargs))\n\n        explicit_api = kwargs['explicit_api']\n        if explicit_api.get(\"__MANAGE_SWAGGER\"):\n            self._add_swagger_integration(explicit_api, function)\n\n        return resources", "output": "If the Api event source has a RestApi property, then simply return the Lambda Permission resource allowing\n        API Gateway to call the function. If no RestApi is provided, then additionally inject the path, method, and the\n        x-amazon-apigateway-integration into the Swagger body for a provided implicit API.\n\n        :param dict kwargs: a dict containing the implicit RestApi to be modified, should no explicit RestApi \\\n                be provided.\n        :returns: a list of vanilla CloudFormation Resources, to which this Api event expands\n        :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_host_port(host_port):\n    \"\"\"\n    \n    \"\"\"\n    host, port = None, None  # default\n\n    _s_ = host_port[:]\n    if _s_[0] == \"[\":\n        if \"]\" in host_port:\n            host, _s_ = _s_.lstrip(\"[\").rsplit(\"]\", 1)\n            host = ipaddress.IPv6Address(host).compressed\n            if _s_[0] == \":\":\n                port = int(_s_.lstrip(\":\"))\n            else:\n                if len(_s_) > 1:\n                    raise ValueError('found ambiguous \"{}\" port in \"{}\"'.format(_s_, host_port))\n    else:\n        if _s_.count(\":\") == 1:\n            host, _hostport_separator_, port = _s_.partition(\":\")\n            try:\n                port = int(port)\n            except ValueError as _e_:\n                log.error('host_port \"%s\" port value \"%s\" is not an integer.', host_port, port)\n                raise _e_\n        else:\n            host = _s_\n    try:\n        if not isinstance(host, ipaddress._BaseAddress):\n            host_ip = ipaddress.ip_address(host).compressed\n            host = host_ip\n    except ValueError:\n        log.debug('\"%s\" Not an IP address? Assuming it is a hostname.', host)\n        if host != sanitize_host(host):\n            log.error('bad hostname: \"%s\"', host)\n            raise ValueError('bad hostname: \"{}\"'.format(host))\n\n    return host, port", "output": "Takes a string argument specifying host or host:port.\n\n    Returns a (hostname, port) or (ip_address, port) tuple. If no port is given,\n    the second (port) element of the returned tuple will be None.\n\n    host:port argument, for example, is accepted in the forms of:\n      - hostname\n      - hostname:1234\n      - hostname.domain.tld\n      - hostname.domain.tld:5678\n      - [1234::5]:5678\n      - 1234::5\n      - 10.11.12.13:4567\n      - 10.11.12.13", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n    \"\"\"\n\n    \"\"\"\n    session_groups = self._build_session_groups()\n    session_groups = self._filter(session_groups)\n    self._sort(session_groups)\n    return self._create_response(session_groups)", "output": "Handles the request specified on construction.\n\n    Returns:\n      A ListSessionGroupsResponse object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_current_options(self, options):\n        \"\"\"\n        \n        \"\"\"\n\n        opts = self._get_current_options()\n        opts.update(options)\n        response = self.__proxy__.set_current_options(opts)\n        return response", "output": "Set current options for a model.\n\n        Parameters\n        ----------\n        options : dict\n            A dictionary of the desired option settings. The key should be the name\n            of the option and each value is the desired value of the option.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(version):\n    \"\"\"\n    \"\"\"\n    match = _REGEX.match(version)\n    if match is None:\n        raise ValueError('%s is not valid SemVer string' % version)\n\n    version_parts = match.groupdict()\n\n    version_parts['major'] = int(version_parts['major'])\n    version_parts['minor'] = int(version_parts['minor'])\n    version_parts['patch'] = int(version_parts['patch'])\n\n    return version_parts", "output": "Parse version to major, minor, patch, pre-release, build parts.\n\n    :param version: version string\n    :return: dictionary with the keys 'build', 'major', 'minor', 'patch',\n             and 'prerelease'. The prerelease or build keys can be None\n             if not provided\n    :rtype: dict\n\n    >>> import semver\n    >>> ver = semver.parse('3.4.5-pre.2+build.4')\n    >>> ver['major']\n    3\n    >>> ver['minor']\n    4\n    >>> ver['patch']\n    5\n    >>> ver['prerelease']\n    'pre.2'\n    >>> ver['build']\n    'build.4'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_dlpack(dlpack):\n    \"\"\"\n    \"\"\"\n    handle = NDArrayHandle()\n    dlpack = ctypes.py_object(dlpack)\n    assert ctypes.pythonapi.PyCapsule_IsValid(dlpack, _c_str_dltensor), ValueError(\n        'Invalid DLPack Tensor. DLTensor capsules can be consumed only once.')\n    dlpack_handle = ctypes.c_void_p(ctypes.pythonapi.PyCapsule_GetPointer(dlpack, _c_str_dltensor))\n    check_call(_LIB.MXNDArrayFromDLPack(dlpack_handle, ctypes.byref(handle)))\n    # Rename PyCapsule (DLPack)\n    ctypes.pythonapi.PyCapsule_SetName(dlpack, _c_str_used_dltensor)\n    # delete the deleter of the old dlpack\n    ctypes.pythonapi.PyCapsule_SetDestructor(dlpack, None)\n    return NDArray(handle=handle)", "output": "Returns a NDArray backed by a dlpack tensor.\n\n    Parameters\n    ----------\n    dlpack: PyCapsule (the pointer of DLManagedTensor)\n        input data\n\n    Returns\n    -------\n    NDArray\n        a NDArray backed by a dlpack tensor\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.to_dlpack_for_read(x)\n    >>> type(y)\n    <class 'PyCapsule'>\n    >>> z = mx.nd.from_dlpack(y)\n    >>> type(z)\n    <class 'mxnet.ndarray.ndarray.NDArray'>\n    >>> z\n    [[ 1.  1.  1.]\n     [ 1.  1.  1.]]\n    <NDArray 2x3 @cpu(0)>\n\n    >>> w = mx.nd.to_dlpack_for_write(x)\n    >>> type(w)\n    <class 'PyCapsule'>\n    >>> u = mx.nd.from_dlpack(w)\n    >>> u += 1\n    >>> x\n    [[2. 2. 2.]\n     [2. 2. 2.]]\n    <NDArray 2x3 @cpu(0)>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(self, fn):\n        \"\"\n        return open_image(fn, convert_mode=self.convert_mode, after_open=self.after_open)", "output": "Open image in `fn`, subclass and overwrite for custom behavior.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_impl_version_info():\n    # type: () -> Tuple[int, ...]\n    \"\"\"\"\"\"\n    if get_abbr_impl() == 'pp':\n        # as per https://github.com/pypa/pip/issues/2882\n        # attrs exist only on pypy\n        return (sys.version_info[0],\n                sys.pypy_version_info.major,  # type: ignore\n                sys.pypy_version_info.minor)  # type: ignore\n    else:\n        return sys.version_info[0], sys.version_info[1]", "output": "Return sys.version_info-like tuple for use in decrementing the minor\n    version.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_requirement(self):\n        \"\"\"\"\"\"\n        if isinstance(self.parsed_version, packaging.version.Version):\n            spec = \"%s==%s\" % (self.project_name, self.parsed_version)\n        else:\n            spec = \"%s===%s\" % (self.project_name, self.parsed_version)\n\n        return Requirement.parse(spec)", "output": "Return a ``Requirement`` that matches this distribution exactly", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_interfaces_mac(ip):  # pylint: disable=invalid-name\n    '''\n    \n    '''\n    all_interfaces = _get_mine('net.interfaces')\n    all_ipaddrs = _get_mine('net.ipaddrs')\n\n    for device, device_ipaddrs in six.iteritems(all_ipaddrs):\n        if not device_ipaddrs.get('result', False):\n            continue\n        for interface, interface_ipaddrs in six.iteritems(device_ipaddrs.get('out', {})):\n            ip_addresses = interface_ipaddrs.get('ipv4', {}).keys()\n            ip_addresses.extend(interface_ipaddrs.get('ipv6', {}).keys())\n            for ipaddr in ip_addresses:\n                if ip != ipaddr:\n                    continue\n                interface_mac = all_interfaces.get(device, {}).get('out', {}).get(interface, {}).get('mac_address', '')\n                return device, interface, interface_mac\n\n    return ('', '', '')", "output": "Helper to get the interfaces hardware address using the IP Address.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_alias(self, index=None, name=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_alias\", name), params=params\n        )", "output": "Retrieve a specified alias.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-aliases.html>`_\n\n        :arg index: A comma-separated list of index names to filter aliases\n        :arg name: A comma-separated list of alias names to return\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default 'all', valid choices\n            are: 'open', 'closed', 'none', 'all'\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register(self, subject, avro_schema):\n        \"\"\"\n        \n        \"\"\"\n\n        schemas_to_id = self.subject_to_schema_ids[subject]\n        schema_id = schemas_to_id.get(avro_schema, None)\n        if schema_id is not None:\n            return schema_id\n        # send it up\n        url = '/'.join([self.url, 'subjects', subject, 'versions'])\n        # body is { schema : json_string }\n\n        body = {'schema': json.dumps(avro_schema.to_json())}\n        result, code = self._send_request(url, method='POST', body=body)\n        if (code == 401 or code == 403):\n            raise ClientError(\"Unauthorized access. Error code:\" + str(code))\n        elif code == 409:\n            raise ClientError(\"Incompatible Avro schema:\" + str(code))\n        elif code == 422:\n            raise ClientError(\"Invalid Avro schema:\" + str(code))\n        elif not (code >= 200 and code <= 299):\n            raise ClientError(\"Unable to register schema. Error code:\" + str(code))\n        # result is a dict\n        schema_id = result['id']\n        # cache it\n        self._cache_schema(avro_schema, schema_id, subject)\n        return schema_id", "output": "POST /subjects/(string: subject)/versions\n        Register a schema with the registry under the given subject\n        and receive a schema id.\n\n        avro_schema must be a parsed schema from the python avro library\n\n        Multiple instances of the same schema will result in cache misses.\n\n        :param str subject: subject name\n        :param schema avro_schema: Avro schema to be registered\n        :returns: schema_id\n        :rtype: int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_app_path(url):\n    ''' \n\n    '''\n    app_path = urlparse(url).path.rstrip(\"/\")\n    if not app_path.startswith(\"/\"):\n        app_path = \"/\" + app_path\n    return app_path", "output": "Extract the app path from a Bokeh server URL\n\n    Args:\n        url (str) :\n\n    Returns:\n        str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def intersection(self, other, sort=False):\n        \"\"\"\n        \n        \"\"\"\n        self._validate_sort_keyword(sort)\n        self._assert_can_do_setop(other)\n        other, result_names = self._convert_can_do_setop(other)\n\n        if self.equals(other):\n            return self\n\n        self_tuples = self._ndarray_values\n        other_tuples = other._ndarray_values\n        uniq_tuples = set(self_tuples) & set(other_tuples)\n\n        if sort is None:\n            uniq_tuples = sorted(uniq_tuples)\n\n        if len(uniq_tuples) == 0:\n            return MultiIndex(levels=self.levels,\n                              codes=[[]] * self.nlevels,\n                              names=result_names, verify_integrity=False)\n        else:\n            return MultiIndex.from_arrays(lzip(*uniq_tuples), sortorder=0,\n                                          names=result_names)", "output": "Form the intersection of two MultiIndex objects.\n\n        Parameters\n        ----------\n        other : MultiIndex or array / Index of tuples\n        sort : False or None, default False\n            Sort the resulting MultiIndex if possible\n\n            .. versionadded:: 0.24.0\n\n            .. versionchanged:: 0.24.1\n\n               Changed the default from ``True`` to ``False``, to match\n               behaviour from before 0.24.0\n\n        Returns\n        -------\n        Index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def activated(self, item):\r\n        \"\"\"\"\"\"\r\n        editor_item = self.editor_items.get(\r\n            self.editor_ids.get(self.current_editor))\r\n        line = 0\r\n        if item == editor_item:\r\n            line = 1\r\n        elif isinstance(item, TreeItem):\r\n            line = item.line\r\n\r\n        self.freeze = True\r\n        root_item = self.get_root_item(item)\r\n        if line:\r\n            self.parent().edit_goto.emit(root_item.path, line, item.text(0))\r\n        else:\r\n            self.parent().edit.emit(root_item.path)\r\n        self.freeze = False\r\n\r\n        parent = self.current_editor.parent()\r\n        for editor_id, i_item in list(self.editor_items.items()):\r\n            if i_item is root_item:\r\n                for editor, _id in list(self.editor_ids.items()):\r\n                    if _id == editor_id and editor.parent() is parent:\r\n                        self.current_editor = editor\r\n                        break\r\n                break", "output": "Double-click event", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _delete(self, state=None):\n        \"\"\"\n        \"\"\"\n        mutation_val = data_v2_pb2.Mutation.DeleteFromRow()\n        mutation_pb = data_v2_pb2.Mutation(delete_from_row=mutation_val)\n        self._get_mutations(state).append(mutation_pb)", "output": "Helper for :meth:`delete`\n\n        Adds a delete mutation (for the entire row) to the accumulated\n        mutations.\n\n        ``state`` is unused by :class:`DirectRow` but is used by\n        subclasses.\n\n        :type state: bool\n        :param state: (Optional) The state that is passed along to\n                      :meth:`_get_mutations`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dns_servers(interface='Local Area Connection'):\n    '''\n    \n    '''\n    # remove any escape characters\n    interface = interface.split('\\\\')\n    interface = ''.join(interface)\n\n    with salt.utils.winapi.Com():\n        c = wmi.WMI()\n        for iface in c.Win32_NetworkAdapter(NetEnabled=True):\n            if interface == iface.NetConnectionID:\n                iface_config = c.Win32_NetworkAdapterConfiguration(Index=iface.Index).pop()\n                try:\n                    return list(iface_config.DNSServerSearchOrder)\n                except TypeError:\n                    return []\n    log.debug('Interface \"%s\" not found', interface)\n    return False", "output": "Return a list of the configured DNS servers of the specified interface\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_dns_client.get_dns_servers 'Local Area Connection'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def legend(self):\n        ''' \n\n        '''\n        panels = self.above + self.below + self.left + self.right + self.center\n        legends = [obj for obj in panels if isinstance(obj, Legend)]\n        return _legend_attr_splat(legends)", "output": "Splattable list of :class:`~bokeh.models.annotations.Legend` objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_to_shape(inputs, shape, scope):\n  \"\"\"\"\"\"\n  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n    x = inputs\n    x = tfl.flatten(x)\n    x = tfl.dense(x, shape[2], activation=None, name=\"dec_dense\")\n    x = tf.expand_dims(x, axis=1)\n    return x", "output": "Encode the given tensor to given image shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _external_items(self):\n        \"\"\"  \"\"\"\n        assert not self.data\n        if any(self.pdata):\n            self._spill()\n        # disable partitioning and spilling when merge combiners from disk\n        self.pdata = []\n\n        try:\n            for i in range(self.partitions):\n                for v in self._merged_items(i):\n                    yield v\n                self.data.clear()\n\n                # remove the merged partition\n                for j in range(self.spills):\n                    path = self._get_spill_dir(j)\n                    os.remove(os.path.join(path, str(i)))\n        finally:\n            self._cleanup()", "output": "Return all partitioned items as iterator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summary(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.hasSummary:\n            return BisectingKMeansSummary(super(BisectingKMeansModel, self).summary)\n        else:\n            raise RuntimeError(\"No training summary available for this %s\" %\n                               self.__class__.__name__)", "output": "Gets summary (e.g. cluster assignments, cluster sizes) of the model trained on the\n        training set. An exception is thrown if no summary exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lossfn(real_input, fake_input, compress, hparams, lsgan, name):\n  \"\"\"\"\"\"\n  eps = 1e-12\n  with tf.variable_scope(name):\n    d1 = discriminator(real_input, compress, hparams, \"discriminator\")\n    d2 = discriminator(fake_input, compress, hparams, \"discriminator\",\n                       reuse=True)\n    if lsgan:\n      dloss = tf.reduce_mean(\n          tf.squared_difference(d1, 0.9)) + tf.reduce_mean(tf.square(d2))\n      gloss = tf.reduce_mean(tf.squared_difference(d2, 0.9))\n      loss = (dloss + gloss)/2\n    else:  # cross_entropy\n      dloss = -tf.reduce_mean(\n          tf.log(d1 + eps)) - tf.reduce_mean(tf.log1p(eps - d2))\n      gloss = -tf.reduce_mean(tf.log(d2 + eps))\n      loss = (dloss + gloss)/2\n    return loss", "output": "Loss function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def paged_call(function, *args, **kwargs):\n    \"\"\"\n    \"\"\"\n    marker_flag = kwargs.pop('marker_flag', 'NextMarker')\n    marker_arg = kwargs.pop('marker_arg', 'Marker')\n    while True:\n        ret = function(*args, **kwargs)\n        marker = ret.get(marker_flag)\n        yield ret\n        if not marker:\n            break\n        kwargs[marker_arg] = marker", "output": "Retrieve full set of values from a boto3 API call that may truncate\n    its results, yielding each page as it is obtained.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ctypes2numpy(cptr, length, dtype):\n    \"\"\"\n    \"\"\"\n    if not isinstance(cptr, ctypes.POINTER(ctypes.c_float)):\n        raise RuntimeError('expected float pointer')\n    res = np.zeros(length, dtype=dtype)\n    if not ctypes.memmove(res.ctypes.data, cptr, length * res.strides[0]):\n        raise RuntimeError('memmove failed')\n    return res", "output": "Convert a ctypes pointer array to a numpy array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpointerNewContext(self, doc, origin):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if origin is None: origin__o = None\n        else: origin__o = origin._o\n        ret = libxml2mod.xmlXPtrNewContext(doc__o, self._o, origin__o)\n        if ret is None:raise treeError('xmlXPtrNewContext() failed')\n        __tmp = xpathContext(_obj=ret)\n        return __tmp", "output": "Create a new XPointer context", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_acl(self, load):\n        '''\n        \n        '''\n        if 'eauth' not in load:\n            return None\n        mod = self.opts['eauth_acl_module']\n        if not mod:\n            mod = load['eauth']\n        fstr = '{0}.acl'.format(mod)\n        if fstr not in self.auth:\n            return None\n        fcall = salt.utils.args.format_call(\n            self.auth[fstr],\n            load,\n            expected_extra_kws=AUTH_INTERNAL_KEYWORDS)\n        try:\n            return self.auth[fstr](*fcall['args'], **fcall['kwargs'])\n        except Exception as e:\n            log.debug('Authentication module threw %s', e)\n            return None", "output": "Returns ACL for a specific user.\n        Returns None if eauth doesn't provide any for the user. I. e. None means: use acl declared\n        in master config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_day(self, day):\n        \"\"\"\n        \"\"\"\n\n        def _select_day(day):\n            return self.data.loc[day, slice(None)]\n\n        try:\n            return self.new(_select_day(day), self.type, self.if_fq)\n        except:\n            raise ValueError('QA CANNOT GET THIS Day {} '.format(day))", "output": "\u9009\u53d6\u65e5\u671f(\u4e00\u822c\u7528\u4e8e\u5206\u949f\u7ebf)\n\n        Arguments:\n            day {[type]} -- [description]\n\n        Raises:\n            ValueError -- [description]\n\n        Returns:\n            [type] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_directory(self):\r\n        \"\"\"\"\"\"\r\n        basedir = to_text_string(self.wd_edit.text())\r\n        if not osp.isdir(basedir):\r\n            basedir = getcwd_or_home()\r\n        directory = getexistingdirectory(self, _(\"\"), basedir)\r\n        if directory:\r\n            self.wd_edit.setText(directory)\r\n            self.dir = directory", "output": "Select directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fbresnet152(num_classes=1000, pretrained='imagenet'):\n    \"\"\"\n    \"\"\"\n    model = FBResNet(Bottleneck, [3, 8, 36, 3], num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['fbresnet152'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            \"num_classes should be {}, but is {}\".format(settings['num_classes'], num_classes)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    return model", "output": "Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zcard(key, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.zcard(key)", "output": "Get the length of a sorted set in Redis\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.zcard foo_sorted", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def boot_time(time_format=None):\n    '''\n    \n    '''\n    try:\n        b_time = int(psutil.boot_time())\n    except AttributeError:\n        # get_boot_time() has been removed in newer psutil versions, and has\n        # been replaced by boot_time() which provides the same information.\n        b_time = int(psutil.boot_time())\n    if time_format:\n        # Load epoch timestamp as a datetime.datetime object\n        b_time = datetime.datetime.fromtimestamp(b_time)\n        try:\n            return b_time.strftime(time_format)\n        except TypeError as exc:\n            raise SaltInvocationError('Invalid format string: {0}'.format(exc))\n    return b_time", "output": "Return the boot time in number of seconds since the epoch began.\n\n    CLI Example:\n\n    time_format\n        Optionally specify a `strftime`_ format string. Use\n        ``time_format='%c'`` to get a nicely-formatted locale specific date and\n        time (i.e. ``Fri May  2 19:08:32 2014``).\n\n        .. _strftime: https://docs.python.org/2/library/datetime.html#strftime-strptime-behavior\n\n        .. versionadded:: 2014.1.4\n\n    .. code-block:: bash\n\n        salt '*' ps.boot_time", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def matthewscc(self, use_global=False):\n        \"\"\"\n        \n        \"\"\"\n        if use_global:\n            if not self.global_total_examples:\n                return 0.\n\n            true_pos = float(self.global_true_positives)\n            false_pos = float(self.global_false_positives)\n            false_neg = float(self.global_false_negatives)\n            true_neg = float(self.global_true_negatives)\n        else:\n            if not self.total_examples:\n                return 0.\n\n            true_pos = float(self.true_positives)\n            false_pos = float(self.false_positives)\n            false_neg = float(self.false_negatives)\n            true_neg = float(self.true_negatives)\n\n        terms = [(true_pos + false_pos),\n                 (true_pos + false_neg),\n                 (true_neg + false_pos),\n                 (true_neg + false_neg)]\n        denom = 1.\n        for t in filter(lambda t: t != 0., terms):\n            denom *= t\n        return ((true_pos * true_neg) - (false_pos * false_neg)) / math.sqrt(denom)", "output": "Calculate the Matthew's Correlation Coefficent", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adduser(name, username):\n    '''\n    \n    '''\n    # Note: pw exits with code 65 if group is unknown\n    retcode = __salt__['cmd.retcode']('pw groupmod {0} -m {1}'.format(\n        name, username), python_shell=False)\n\n    return not retcode", "output": "Add a user in the group.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n         salt '*' group.adduser foo bar\n\n    Verifies if a valid username 'bar' as a member of an existing group 'foo',\n    if not then adds it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_plot_keywords(self, kwds, y):\n        \"\"\"\"\"\"\n        # y is required for KdePlot\n        kwds['bottom'] = self.bottom\n        kwds['bins'] = self.bins\n        return kwds", "output": "merge BoxPlot/KdePlot properties to passed kwds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def promote(name):\n    '''\n    \n\n    '''\n    ## Promote clone\n    res = __salt__['cmd.run_all'](\n        __utils__['zfs.zfs_command'](\n            command='promote',\n            target=name,\n        ),\n        python_shell=False,\n    )\n\n    return __utils__['zfs.parse_command_result'](res, 'promoted')", "output": "Promotes a clone file system to no longer be dependent on its \"origin\"\n    snapshot.\n\n    .. note::\n\n        This makes it possible to destroy the file system that the\n        clone was created from. The clone parent-child dependency relationship\n        is reversed, so that the origin file system becomes a clone of the\n        specified file system.\n\n        The snapshot that was cloned, and any snapshots previous to this\n        snapshot, are now owned by the promoted clone. The space they use moves\n        from the origin file system to the promoted clone, so enough space must\n        be available to accommodate these snapshots. No new space is consumed\n        by this operation, but the space accounting is adjusted. The promoted\n        clone must not have any conflicting snapshot names of its own. The\n        rename subcommand can be used to rename any conflicting snapshots.\n\n    name : string\n        name of clone-filesystem\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zfs.promote myzpool/myclone", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keep_resample(nkeep, X_train, y_train, X_test, y_test, attr_test, model_generator, metric, trained_model, random_state):\n    \"\"\" \n    \"\"\" # why broken? overwriting?\n\n    X_train, X_test = to_array(X_train, X_test)\n\n    # how many features to mask\n    assert X_train.shape[1] == X_test.shape[1]\n\n    # how many samples to take\n    nsamples = 100\n\n    # keep nkeep top features for each test explanation\n    N,M = X_test.shape\n    X_test_tmp = np.tile(X_test, [1, nsamples]).reshape(nsamples * N, M)\n    tie_breaking_noise = const_rand(M) * 1e-6\n    inds = sklearn.utils.resample(np.arange(N), n_samples=nsamples, random_state=random_state)\n    for i in range(N):\n        if nkeep[i] < M:\n            ordering = np.argsort(-attr_test[i,:] + tie_breaking_noise)\n            X_test_tmp[i*nsamples:(i+1)*nsamples, ordering[nkeep[i]:]] = X_train[inds, :][:, ordering[nkeep[i]:]]\n\n    yp_masked_test = trained_model.predict(X_test_tmp)\n    yp_masked_test = np.reshape(yp_masked_test, (N, nsamples)).mean(1) # take the mean output over all samples\n\n    return metric(y_test, yp_masked_test)", "output": "The model is revaluated for each test sample with the non-important features set to resample background values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlParseChunk(self, chunk, size, terminate):\n        \"\"\" \"\"\"\n        ret = libxml2mod.htmlParseChunk(self._o, chunk, size, terminate)\n        return ret", "output": "Parse a Chunk of memory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def master2model(model_params:Sequence[Tensor], master_params:Sequence[Tensor], flat_master:bool=False)->None:\n    \"\"\n    if flat_master:\n        for model_group,master_group in zip(model_params,master_params):\n            if len(model_group) != 0:\n                for model, master in zip(model_group, _unflatten_dense_tensors(master_group[0].data, model_group)):\n                    model.data.copy_(master)\n    else:\n        for model_group,master_group in zip(model_params,master_params):\n            for model, master in zip(model_group, master_group): model.data.copy_(master.data)", "output": "Copy `master_params` to `model_params`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _setup_memory(self):\n        \"\"\"\n        \"\"\"\n        if self.memory:\n            if isinstance(self.memory, str):\n                if self.memory == \"auto\":\n                    # Create a temporary folder to store the transformers of the pipeline\n                    self._cachedir = mkdtemp()\n                else:\n                    if not os.path.isdir(self.memory):\n                        try:\n                            os.makedirs(self.memory)\n                        except:\n                            raise ValueError(\n                                'Could not create directory for memory caching: {}'.format(self.memory)\n                            )\n                    self._cachedir = self.memory\n\n                self._memory = Memory(cachedir=self._cachedir, verbose=0)\n            elif isinstance(self.memory, Memory):\n                self._memory = self.memory\n            else:\n                raise ValueError(\n                    'Could not recognize Memory object for pipeline caching. '\n                    'Please provide an instance of sklearn.external.joblib.Memory,'\n                    ' a path to a directory on your system, or \\\"auto\\\".'\n                )", "output": "Setup Memory object for memory caching.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def system(session):\n    \"\"\"\"\"\"\n\n    # Sanity check: Only run system tests if the environment variable is set.\n    if not os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', ''):\n        session.skip('Credentials must be set via environment variable.')\n\n    # Use pre-release gRPC for system tests.\n    session.install('--pre', 'grpcio')\n\n    # Install all test dependencies, then install this package into the\n    # virtualenv's dist-packages.\n    session.install('mock', 'pytest')\n    for local_dep in LOCAL_DEPS:\n        session.install('-e', local_dep)\n    systest_deps = [\n        '../bigquery/',\n        '../pubsub/',\n        '../storage/',\n        '../test_utils/',\n    ]\n    for systest_dep in systest_deps:\n        session.install('-e', systest_dep)\n    session.install('-e', '.')\n\n    # Run py.test against the system tests.\n    session.run(\n        'py.test',\n        '-vvv',\n        '-s',\n        'tests/system',\n        *session.posargs)", "output": "Run the system test suite.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getIdleActivation(**kwargs):\n    '''\n    \n\n    '''\n    _gsession = _GSettings(user=kwargs.get('user'),\n                           schema='org.gnome.desktop.screensaver',\n                           key='idle-activation-enabled')\n    return _gsession._get()", "output": "Get whether the idle activation is enabled\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gnome.getIdleActivation user=<username>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imshow(self, key):\r\n        \"\"\"\"\"\"\r\n        data = self.model.get_data()\r\n        import spyder.pyplot as plt\r\n        plt.figure()\r\n        plt.imshow(data[key])\r\n        plt.show()", "output": "Show item's image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flatten_check(out:Tensor, targ:Tensor) -> Tensor:\n    \"\"\n    out,targ = out.contiguous().view(-1),targ.contiguous().view(-1)\n    assert len(out) == len(targ), f\"Expected output and target to have the same number of elements but got {len(out)} and {len(targ)}.\"\n    return out,targ", "output": "Check that `out` and `targ` have the same number of elements and flatten them.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grains():\n    '''\n    \n    '''\n    if not DETAILS.get('grains_cache', {}):\n        DETAILS['grains_cache'] = GRAINS_CACHE\n        try:\n            compute_rack = get_config_resolver_class('computeRackUnit', False)\n            DETAILS['grains_cache'] = compute_rack['outConfigs']['computeRackUnit']\n        except salt.exceptions.CommandExecutionError:\n            pass\n        except Exception as err:\n            log.error(err)\n    return DETAILS['grains_cache']", "output": "Get the grains from the proxied device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_parameter(name, parameter, value, path=None):\n    '''\n    \n    '''\n    if not exists(name, path=path):\n        return None\n\n    cmd = 'lxc-cgroup'\n    if path:\n        cmd += ' -P {0}'.format(pipes.quote(path))\n    cmd += ' -n {0} {1} {2}'.format(name, parameter, value)\n    ret = __salt__['cmd.run_all'](cmd, python_shell=False)\n    if ret['retcode'] != 0:\n        return False\n    else:\n        return True", "output": "Set the value of a cgroup parameter for a container.\n\n    path\n        path to the container parent directory\n        default: /var/lib/lxc (system)\n\n        .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' lxc.set_parameter name parameter value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nodeListGetString(self, list, inLine):\n        \"\"\" \"\"\"\n        if list is None: list__o = None\n        else: list__o = list._o\n        ret = libxml2mod.xmlNodeListGetString(self._o, list__o, inLine)\n        return ret", "output": "Build the string equivalent to the text contained in the\n           Node list made of TEXTs and ENTITY_REFs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_cidr_ips(cidr):\n    '''\n    \n    '''\n    ips = netaddr.IPNetwork(cidr)\n    return [six.text_type(ip) for ip in list(ips)]", "output": "Get a list of IP addresses from a CIDR.\n\n    CLI example::\n\n        salt myminion netaddress.list_cidr_ips 192.168.0.0/20", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_logging(cli_options):\r\n    \"\"\"\"\"\"\r\n    if cli_options.debug_info or get_debug_level() > 0:\r\n        levels = {2: logging.INFO, 3: logging.DEBUG}\r\n        log_level = levels[get_debug_level()]\r\n        log_format = '%(asctime)s [%(levelname)s] [%(name)s] -> %(message)s'\r\n\r\n        if cli_options.debug_output == 'file':\r\n            log_file = 'spyder-debug.log'\r\n        else:\r\n            log_file = None\r\n\r\n        logging.basicConfig(level=log_level,\r\n                            format=log_format,\r\n                            filename=log_file,\r\n                            filemode='w+')", "output": "Setup logging with cli options defined by the user.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_parquet(path, engine='auto', columns=None, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n\n    impl = get_engine(engine)\n    return impl.read(path, columns=columns, **kwargs)", "output": "Load a parquet object from the file path, returning a DataFrame.\n\n    .. versionadded 0.21.0\n\n    Parameters\n    ----------\n    path : string\n        File path\n    engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto'\n        Parquet library to use. If 'auto', then the option\n        ``io.parquet.engine`` is used. The default ``io.parquet.engine``\n        behavior is to try 'pyarrow', falling back to 'fastparquet' if\n        'pyarrow' is unavailable.\n    columns : list, default=None\n        If not None, only these columns will be read from the file.\n\n        .. versionadded 0.21.1\n    **kwargs\n        Any additional kwargs are passed to the engine.\n\n    Returns\n    -------\n    DataFrame", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def access_keys(opts):\n    '''\n    \n    '''\n    # TODO: Need a way to get all available users for systems not supported by pwd module.\n    #       For now users pattern matching will not work for publisher_acl.\n    keys = {}\n    publisher_acl = opts['publisher_acl']\n    acl_users = set(publisher_acl.keys())\n    if opts.get('user'):\n        acl_users.add(opts['user'])\n    acl_users.add(salt.utils.user.get_user())\n    for user in acl_users:\n        log.info('Preparing the %s key for local communication', user)\n        key = mk_key(opts, user)\n        if key is not None:\n            keys[user] = key\n\n    # Check other users matching ACL patterns\n    if opts['client_acl_verify'] and HAS_PWD:\n        log.profile('Beginning pwd.getpwall() call in masterapi access_keys function')\n        for user in pwd.getpwall():\n            user = user.pw_name\n            if user not in keys and salt.utils.stringutils.check_whitelist_blacklist(user, whitelist=acl_users):\n                keys[user] = mk_key(opts, user)\n        log.profile('End pwd.getpwall() call in masterapi access_keys function')\n\n    return keys", "output": "A key needs to be placed in the filesystem with permissions 0400 so\n    clients are required to run as root.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cartesian(self, other):\n        \"\"\"\n        \n        \"\"\"\n        # Due to batching, we can't use the Java cartesian method.\n        deserializer = CartesianDeserializer(self._jrdd_deserializer,\n                                             other._jrdd_deserializer)\n        return RDD(self._jrdd.cartesian(other._jrdd), self.ctx, deserializer)", "output": "Return the Cartesian product of this RDD and another one, that is, the\n        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n        C{b} is in C{other}.\n\n        >>> rdd = sc.parallelize([1, 2])\n        >>> sorted(rdd.cartesian(rdd).collect())\n        [(1, 1), (1, 2), (2, 1), (2, 2)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_blob(self, blob_name, client=None, generation=None):\n        \"\"\"\n\n        \"\"\"\n        client = self._require_client(client)\n        blob = Blob(blob_name, bucket=self, generation=generation)\n\n        # We intentionally pass `_target_object=None` since a DELETE\n        # request has no response value (whether in a standard request or\n        # in a batch request).\n        client._connection.api_request(\n            method=\"DELETE\",\n            path=blob.path,\n            query_params=blob._query_params,\n            _target_object=None,\n        )", "output": "Deletes a blob from the current bucket.\n\n        If the blob isn't found (backend 404), raises a\n        :class:`google.cloud.exceptions.NotFound`.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n          :start-after: [START delete_blob]\n          :end-before: [END delete_blob]\n\n        If :attr:`user_project` is set, bills the API request to that project.\n\n        :type blob_name: str\n        :param blob_name: A blob name to delete.\n\n        :type client: :class:`~google.cloud.storage.client.Client` or\n                      ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current bucket.\n\n        :type generation: long\n        :param generation: Optional. If present, permanently deletes a specific\n                           revision of this object.\n\n        :raises: :class:`google.cloud.exceptions.NotFound` (to suppress\n                 the exception, call ``delete_blobs``, passing a no-op\n                 ``on_error`` callback, e.g.:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START delete_blobs]\n            :end-before: [END delete_blobs]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preprocess_batch(images_batch, preproc_func=None):\n  \"\"\"\n  \n  \"\"\"\n  if preproc_func is None:\n    return images_batch\n\n  with tf.variable_scope('preprocess'):\n    images_list = tf.split(images_batch, int(images_batch.shape[0]))\n    result_list = []\n    for img in images_list:\n      reshaped_img = tf.reshape(img, img.shape[1:])\n      processed_img = preproc_func(reshaped_img)\n      result_list.append(tf.expand_dims(processed_img, axis=0))\n    result_images = tf.concat(result_list, axis=0)\n  return result_images", "output": "Creates a preprocessing graph for a batch given a function that processes\n  a single image.\n\n  :param images_batch: A tensor for an image batch.\n  :param preproc_func: (optional function) A function that takes in a\n      tensor and returns a preprocessed input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_entry_to_path(path):\n    # type: (Dict[S, Union[S, bool, Tuple[S], List[S]]]) -> S\n    \"\"\"\"\"\"\n\n    if not isinstance(path, Mapping):\n        raise TypeError(\"expecting a mapping, received {0!r}\".format(path))\n\n    if not any(key in path for key in [\"file\", \"path\"]):\n        raise ValueError(\"missing path-like entry in supplied mapping {0!r}\".format(path))\n\n    if \"file\" in path:\n        path = vistir.path.url_to_path(path[\"file\"])\n\n    elif \"path\" in path:\n        path = path[\"path\"]\n    return path", "output": "Convert a pipfile entry to a string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute_watch(self, id=None, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"PUT\",\n            _make_path(\"_watcher\", \"watch\", id, \"_execute\"),\n            params=params,\n            body=body,\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-execute-watch.html>`_\n\n        :arg id: Watch ID\n        :arg body: Execution control\n        :arg debug: indicates whether the watch should execute in debug mode", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dispatch(self, inp):\n    \"\"\"\n    \"\"\"\n    inp = tf.gather(inp, self._batch_index)\n    return tf.split(inp, self._part_sizes_tensor, 0, num=self._num_experts)", "output": "Create one input Tensor for each expert.\n\n    The `Tensor` for a expert `i` contains the slices of `inp` corresponding\n    to the batch elements `b` where `gates[b, i] > 0`.\n\n    Args:\n      inp: a `Tensor` of shape \"[batch_size, <extra_input_dims>]`\n    Returns:\n      a list of `num_experts` `Tensor`s with shapes\n        `[expert_batch_size_i, <extra_input_dims>]`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unsqueeze(attrs, inputs, cls):\n    \"\"\"\"\"\"\n    # MXNet can only add one axis at a time.\n    mxnet_op = inputs[0]\n    for axis in attrs[\"axes\"]:\n        mxnet_op = symbol.expand_dims(mxnet_op, axis=axis)\n\n    return mxnet_op, attrs, inputs", "output": "Inserts a new axis of size 1 into the array shape", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subvolume_set_default(subvolid, path):\n    '''\n    \n\n    '''\n    cmd = ['btrfs', 'subvolume', 'set-default', subvolid, path]\n\n    res = __salt__['cmd.run_all'](cmd)\n    salt.utils.fsutils._verify_run(res)\n    return True", "output": "Set the subvolume as default\n\n    subvolid\n        ID of the new default subvolume\n\n    path\n        Mount point for the filesystem\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' btrfs.subvolume_set_default 257 /var/volumes/tmp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exclusion_path(cls, project, exclusion):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/exclusions/{exclusion}\",\n            project=project,\n            exclusion=exclusion,\n        )", "output": "Return a fully-qualified exclusion string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_comment(issue_key,\n                comment,\n                visibility=None,\n                is_internal=False,\n                server=None,\n                username=None,\n                password=None):\n    '''\n    \n    '''\n    jira_ = _get_jira(server=server,\n                      username=username,\n                      password=password)\n    comm = jira_.add_comment(issue_key,\n                             comment,\n                             visibility=visibility,\n                             is_internal=is_internal)\n    return True", "output": "Add a comment to an existing ticket. Return ``True`` when it successfully\n    added the comment.\n\n    issue_key\n        The issue ID to add the comment to.\n\n    comment\n        The body of the comment to be added.\n\n    visibility: ``None``\n        A dictionary having two keys:\n\n        - ``type``: is ``role`` (or ``group`` if the JIRA server has configured\n          comment visibility for groups).\n        - ``value``: the name of the role (or group) to which viewing of this\n          comment will be restricted.\n\n    is_internal: ``False``\n        Whether a comment has to be marked as ``Internal`` in Jira Service Desk.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' jira.add_comment NE-123 'This is a comment'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def trunc_normal_(x:Tensor, mean:float=0., std:float=1.) -> Tensor:\n    \"\"\n    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n    return x.normal_().fmod_(2).mul_(std).add_(mean)", "output": "Truncated normal initialization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_topic_attributes(TopicArn, AttributeName, AttributeValue, region=None, key=None, keyid=None,\n                         profile=None):\n    '''\n    \n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        conn.set_topic_attributes(TopicArn=TopicArn, AttributeName=AttributeName,\n                                  AttributeValue=AttributeValue)\n        log.debug('Set attribute %s=%s on SNS topic %s',\n                  AttributeName, AttributeValue, TopicArn)\n        return True\n    except botocore.exceptions.ClientError as e:\n        log.error('Failed to set attribute %s=%s for SNS topic %s: %s',\n                  AttributeName, AttributeValue, TopicArn, e)\n        return False", "output": "Set an attribute of a topic to a new value.\n\n    CLI example::\n\n        salt myminion boto3_sns.set_topic_attributes someTopic DisplayName myDisplayNameValue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_default(path, dest, name):\n    '''\n    \n    '''\n    subvol_id = __salt__['btrfs.subvolume_show'](path)[name]['subvolume id']\n    return __salt__['btrfs.subvolume_set_default'](subvol_id, dest)", "output": "Set the subvolume as the current default.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def time_to_str(timestamp: int) -> str:\n    \"\"\"\n    \n    \"\"\"\n    datetimestamp = datetime.datetime.fromtimestamp(timestamp)\n    return '{:04d}-{:02d}-{:02d}-{:02d}-{:02d}-{:02d}'.format(\n            datetimestamp.year, datetimestamp.month, datetimestamp.day,\n            datetimestamp.hour, datetimestamp.minute, datetimestamp.second\n    )", "output": "Convert seconds past Epoch to human readable string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_from_csc(self, csc):\n        \"\"\"\n        \n        \"\"\"\n        if len(csc.indices) != len(csc.data):\n            raise ValueError('length mismatch: {} vs {}'.format(len(csc.indices), len(csc.data)))\n        self.handle = ctypes.c_void_p()\n        _check_call(_LIB.XGDMatrixCreateFromCSC(c_array(ctypes.c_ulong, csc.indptr),\n                                                c_array(ctypes.c_uint, csc.indices),\n                                                c_array(ctypes.c_float, csc.data),\n                                                len(csc.indptr), len(csc.data),\n                                                ctypes.byref(self.handle)))", "output": "Initialize data from a CSC matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def brightness(x, severity=1):\n  \"\"\"\n  \"\"\"\n  c = [.1, .2, .3, .4, .5][severity - 1]\n\n  x = np.array(x) / 255.\n  x = tfds.core.lazy_imports.skimage.color.rgb2hsv(x)\n  x[:, :, 2] = np.clip(x[:, :, 2] + c, 0, 1)\n  x = tfds.core.lazy_imports.skimage.color.hsv2rgb(x)\n  x_clip = np.clip(x, 0, 1) * 255\n  return around_and_astype(x_clip)", "output": "Change brightness of images.\n\n  Args:\n    x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255].\n    severity: integer, severity of corruption.\n\n  Returns:\n    numpy array, image with uint8 pixels in [0,255]. Changed brightness.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _astype(self, dtype, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        dtype = pandas_dtype(dtype)\n\n        # if we are passed a datetime64[ns, tz]\n        if is_datetime64tz_dtype(dtype):\n            values = self.values\n            if getattr(values, 'tz', None) is None:\n                values = DatetimeIndex(values).tz_localize('UTC')\n            values = values.tz_convert(dtype.tz)\n            return self.make_block(values)\n\n        # delegate\n        return super()._astype(dtype=dtype, **kwargs)", "output": "these automatically copy, so copy=True has no effect\n        raise on an except if raise == True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_jids_filter(count, filter_find_job=True):\n    '''\n    \n    '''\n    keys = []\n    ret = []\n    for jid, job, _, _ in _walk_through(_job_dir()):\n        job = salt.utils.jid.format_jid_instance_ext(jid, job)\n        if filter_find_job and job['Function'] == 'saltutil.find_job':\n            continue\n        i = bisect.bisect(keys, jid)\n        if len(keys) == count and i == 0:\n            continue\n        keys.insert(i, jid)\n        ret.insert(i, job)\n        if len(keys) > count:\n            del keys[0]\n            del ret[0]\n    return ret", "output": "Return a list of all jobs information filtered by the given criteria.\n    :param int count: show not more than the count of most recent jobs\n    :param bool filter_find_jobs: filter out 'saltutil.find_job' jobs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def global_avgpooling(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._add_extra_attributes(attrs, {'global_pool': True,\n                                                                'kernel': (1, 1),\n                                                                'pool_type': 'avg'})\n    return 'Pooling', new_attrs, inputs", "output": "Performs avg pooling on the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decrypt(self, data):\n        '''\n        \n        '''\n        aes_key, hmac_key = self.keys\n        sig = data[-self.SIG_SIZE:]\n        data = data[:-self.SIG_SIZE]\n        if six.PY3 and not isinstance(data, bytes):\n            data = salt.utils.stringutils.to_bytes(data)\n        mac_bytes = hmac.new(hmac_key, data, hashlib.sha256).digest()\n        if len(mac_bytes) != len(sig):\n            log.debug('Failed to authenticate message')\n            raise AuthenticationError('message authentication failed')\n        result = 0\n\n        if six.PY2:\n            for zipped_x, zipped_y in zip(mac_bytes, sig):\n                result |= ord(zipped_x) ^ ord(zipped_y)\n        else:\n            for zipped_x, zipped_y in zip(mac_bytes, sig):\n                result |= zipped_x ^ zipped_y\n        if result != 0:\n            log.debug('Failed to authenticate message')\n            raise AuthenticationError('message authentication failed')\n        iv_bytes = data[:self.AES_BLOCK_SIZE]\n        data = data[self.AES_BLOCK_SIZE:]\n        if HAS_M2:\n            cypher = EVP.Cipher(alg='aes_192_cbc', key=aes_key, iv=iv_bytes, op=0, padding=False)\n            encr = cypher.update(data)\n            data = encr + cypher.final()\n        else:\n            cypher = AES.new(aes_key, AES.MODE_CBC, iv_bytes)\n            data = cypher.decrypt(data)\n        if six.PY2:\n            return data[:-ord(data[-1])]\n        else:\n            return data[:-data[-1]]", "output": "verify HMAC-SHA256 signature and decrypt data with AES-CBC", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(name, profile=\"splunk\"):\n    '''\n    \n    '''\n    client = _get_splunk(profile)\n    search = None\n    # uglyness of splunk lib\n    try:\n        search = client.saved_searches[name]\n    except KeyError:\n        pass\n    return search", "output": "Get a splunk search\n\n    CLI Example:\n\n        splunk_search.get 'my search name'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loads(s, **kwargs):\n    '''\n    \n    '''\n    json_module = kwargs.pop('_json_module', json)\n    try:\n        return json_module.loads(s, **kwargs)\n    except TypeError as exc:\n        # json.loads cannot load bytestrings in Python < 3.6\n        if six.PY3 and isinstance(s, bytes):\n            return json_module.loads(salt.utils.stringutils.to_unicode(s), **kwargs)\n        else:\n            raise exc", "output": ".. versionadded:: 2018.3.0\n\n    Wraps json.loads and prevents a traceback in the event that a bytestring is\n    passed to the function. (Python < 3.6 cannot load bytestrings)\n\n    You can pass an alternate json module (loaded via import_json() above)\n    using the _json_module argument)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_abstract_dist(req):\n    # type: (InstallRequirement) -> DistAbstraction\n    \"\"\"\n    \"\"\"\n    if req.editable:\n        return IsSDist(req)\n    elif req.link and req.link.is_wheel:\n        return IsWheel(req)\n    else:\n        return IsSDist(req)", "output": "Factory to make an abstract dist object.\n\n    Preconditions: Either an editable req with a source_dir, or satisfied_by or\n    a wheel link, or a non-editable req with a source_dir.\n\n    :return: A concrete DistAbstraction.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def item_selected(self, item=None):\r\n        \"\"\"\"\"\"\r\n        if item is None:\r\n            item = self.currentItem()\r\n\r\n        # stack history is in inverse order\r\n        try:\r\n            index = self.stack_history[-(self.currentRow()+1)]\r\n        except IndexError:\r\n            pass\r\n        else:\r\n            self.editor.set_stack_index(index)\r\n            self.editor.current_changed(index)\r\n        self.hide()", "output": "Change to the selected document and hide this widget.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_header(line: str) -> Tuple[str, Dict[str, str]]:\n    \n    \"\"\"\n    parts = _parseparam(\";\" + line)\n    key = next(parts)\n    # decode_params treats first argument special, but we already stripped key\n    params = [(\"Dummy\", \"value\")]\n    for p in parts:\n        i = p.find(\"=\")\n        if i >= 0:\n            name = p[:i].strip().lower()\n            value = p[i + 1 :].strip()\n            params.append((name, native_str(value)))\n    decoded_params = email.utils.decode_params(params)\n    decoded_params.pop(0)  # get rid of the dummy again\n    pdict = {}\n    for name, decoded_value in decoded_params:\n        value = email.utils.collapse_rfc2231_value(decoded_value)\n        if len(value) >= 2 and value[0] == '\"' and value[-1] == '\"':\n            value = value[1:-1]\n        pdict[name] = value\n    return key, pdict", "output": "r\"\"\"Parse a Content-type like header.\n\n    Return the main content-type and a dictionary of options.\n\n    >>> d = \"form-data; foo=\\\"b\\\\\\\\a\\\\\\\"r\\\"; file*=utf-8''T%C3%A4st\"\n    >>> ct, d = _parse_header(d)\n    >>> ct\n    'form-data'\n    >>> d['file'] == r'T\\u00e4st'.encode('ascii').decode('unicode_escape')\n    True\n    >>> d['foo']\n    'b\\\\a\"r'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_systemd(version):\n    '''\n    \n    '''\n    try:\n        version = int(version)\n    except ValueError:\n        raise CommandExecutionError('Invalid version \\'{0}\\''.format(version))\n\n    try:\n        installed = _sd_version()\n        log.debug('nspawn: detected systemd %s', installed)\n    except (IndexError, ValueError):\n        raise CommandExecutionError('nspawn: Unable to get systemd version')\n\n    if installed < version:\n        raise CommandExecutionError(\n            'This function requires systemd >= {0} '\n            '(Detected version: {1}).'.format(version, installed)\n        )", "output": "Raises an exception if the systemd version is not greater than the\n    passed version.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(key, val, delimiter=DEFAULT_TARGET_DELIM):\n    '''\n    \n    '''\n    grains = get(key, [], delimiter)\n    if not isinstance(grains, list):\n        return 'The key {0} is not a valid list'.format(key)\n    if val not in grains:\n        return 'The val {0} was not in the list {1}'.format(val, key)\n    grains.remove(val)\n\n    while delimiter in key:\n        key, rest = key.rsplit(delimiter, 1)\n        _grain = get(key, None, delimiter)\n        if isinstance(_grain, dict):\n            _grain.update({rest: grains})\n        grains = _grain\n\n    return setval(key, grains)", "output": ".. versionadded:: 0.17.0\n\n    Remove a value from a list in the grains config file\n\n    key\n        The grain key to remove.\n\n    val\n        The value to remove.\n\n    delimiter\n        The key can be a nested dict key. Use this parameter to\n        specify the delimiter you use, instead of the default ``:``.\n        You can now append values to a list in nested dictionary grains. If the\n        list doesn't exist at this level, it will be created.\n\n        .. versionadded:: 2015.8.2\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' grains.remove key val", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def receive_trial_result(self, parameter_id, parameters, value):\n        \"\"\" \n        \"\"\"\n        reward = extract_scalar_reward(value)\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError(\"Received parameter_id not in total_data.\")\n\n        (_, father_id, model_id) = self.total_data[parameter_id]\n\n        graph = self.bo.searcher.load_model_by_id(model_id)\n\n        # to use the value and graph\n        self.add_model(reward, model_id)\n        self.update(father_id, graph, reward, model_id)", "output": "Record an observation of the objective function.\n    \n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have \"default\" key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop_data_frame_transform(self, transform_id, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if transform_id in SKIP_IN_PATH:\n            raise ValueError(\n                \"Empty value passed for a required argument 'transform_id'.\"\n            )\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_stop\"),\n            params=params,\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/stop-data-frame-transform.html>`_\n\n        :arg transform_id: The id of the transform to stop\n        :arg timeout: Controls the time to wait until the transform has stopped.\n            Default to 30 seconds\n        :arg wait_for_completion: Whether to wait for the transform to fully\n            stop before returning or not. Default to false", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_node(node_id=str, force=bool):\n    '''\n    \n    '''\n    client = docker.APIClient(base_url='unix://var/run/docker.sock')\n    try:\n        if force == 'True':\n            service = client.remove_node(node_id, force=True)\n            return service\n        else:\n            service = client.remove_node(node_id, force=False)\n            return service\n    except TypeError:\n        salt_return = {}\n        salt_return.update({'Error': 'Is the node_id and/or force=True/False missing?'})\n        return salt_return", "output": "Remove a node from a swarm and the target needs to be a swarm manager\n\n    node_id\n        The node id from the return of swarm.node_ls\n\n    force\n        Forcefully remove the node/minion from the service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' swarm.remove_node node_id=z4gjbe9rwmqahc2a91snvolm5 force=false", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sepconv_relu_sepconv(inputs,\n                         filter_size,\n                         output_size,\n                         first_kernel_size=(1, 1),\n                         second_kernel_size=(1, 1),\n                         padding=\"LEFT\",\n                         nonpadding_mask=None,\n                         dropout=0.0,\n                         name=None):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name, \"sepconv_relu_sepconv\", [inputs]):\n    inputs = maybe_zero_out_padding(inputs, first_kernel_size, nonpadding_mask)\n    if inputs.get_shape().ndims == 3:\n      is_3d = True\n      inputs = tf.expand_dims(inputs, 2)\n    else:\n      is_3d = False\n    h = separable_conv(\n        inputs,\n        filter_size,\n        first_kernel_size,\n        activation=tf.nn.relu,\n        padding=padding,\n        name=\"conv1\")\n    if dropout != 0.0:\n      h = tf.nn.dropout(h, 1.0 - dropout)\n    h = maybe_zero_out_padding(h, second_kernel_size, nonpadding_mask)\n    ret = separable_conv(\n        h, output_size, second_kernel_size, padding=padding, name=\"conv2\")\n    if is_3d:\n      ret = tf.squeeze(ret, 2)\n    return ret", "output": "Hidden layer with RELU activation followed by linear projection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_token_encoder(vocab_dir, vocab_name, filename):\n  \"\"\"\"\"\"\n  vocab_path = os.path.join(vocab_dir, vocab_name)\n  if not tf.gfile.Exists(vocab_path):\n    _build_vocab(filename, vocab_path, 10000)\n  return text_encoder.TokenTextEncoder(vocab_path)", "output": "Reads from file and returns a `TokenTextEncoder` for the vocabulary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def matchOnlyAtCol(n):\n    \"\"\"\n    \"\"\"\n    def verifyCol(strg,locn,toks):\n        if col(locn,strg) != n:\n            raise ParseException(strg,locn,\"matched token not at column %d\" % n)\n    return verifyCol", "output": "Helper method for defining parse actions that require matching at\n    a specific column in the input text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gather(params, indices, dtype=tf.float32):\n  \"\"\"\"\"\"\n  if not is_xla_compiled():\n    return tf.gather(params, indices)\n  vocab_size = params.get_shape().as_list()[0]\n  indices_flat = tf.reshape(indices, [-1])\n  out = tf.matmul(tf.one_hot(indices_flat, vocab_size, dtype=dtype), params)\n  out = reshape_like(out, tf.expand_dims(indices, -1))\n  return out", "output": "Version of tf.gather that works faster on tpu.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_domain_match(domain: str, hostname: str) -> bool:\n        \"\"\"\"\"\"\n        if hostname == domain:\n            return True\n\n        if not hostname.endswith(domain):\n            return False\n\n        non_matching = hostname[:-len(domain)]\n\n        if not non_matching.endswith(\".\"):\n            return False\n\n        return not is_ip_address(hostname)", "output": "Implements domain matching adhering to RFC 6265.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path_to(self, p):\n        \"\"\"\"\"\"\n        if os.path.isabs(p):\n            return p\n\n        return os.sep.join([self._original_dir, p])", "output": "Returns the absolute path to a given relative path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_format_timedelta64(values, nat_rep='NaT', box=False):\n    \"\"\"\n    \n    \"\"\"\n\n    values_int = values.astype(np.int64)\n\n    consider_values = values_int != iNaT\n\n    one_day_nanos = (86400 * 1e9)\n    even_days = np.logical_and(consider_values,\n                               values_int % one_day_nanos != 0).sum() == 0\n    all_sub_day = np.logical_and(\n        consider_values, np.abs(values_int) >= one_day_nanos).sum() == 0\n\n    if even_days:\n        format = None\n    elif all_sub_day:\n        format = 'sub_day'\n    else:\n        format = 'long'\n\n    def _formatter(x):\n        if x is None or (is_scalar(x) and isna(x)):\n            return nat_rep\n\n        if not isinstance(x, Timedelta):\n            x = Timedelta(x)\n        result = x._repr_base(format=format)\n        if box:\n            result = \"'{res}'\".format(res=result)\n        return result\n\n    return _formatter", "output": "Return a formatter function for a range of timedeltas.\n    These will all have the same format argument\n\n    If box, then show the return in quotes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reboot(name, call=None):\n    '''\n    \n    '''\n    node = get_node(name)\n    ret = take_action(name=name, call=call, method='POST',\n                      command='my/machines/{0}'.format(node['id']),\n                      location=node['location'], data={'action': 'reboot'})\n    return ret[0] in VALID_RESPONSE_CODES", "output": "reboot a machine by name\n    :param name: name given to the machine\n    :param call: call value in this case is 'action'\n    :return: true if successful\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a reboot vm_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_names(cls, name):\n        \"\"\"\n        \"\"\"\n        for i in range(1, len(name)):\n            for candidate in itertools.combinations_with_replacement(\n                    cls.LEADING_CHARS, i - 1):\n                new_name = '~' + ''.join(candidate) + name[i:]\n                if new_name != name:\n                    yield new_name\n\n        # If we make it this far, we will have to make a longer name\n        for i in range(len(cls.LEADING_CHARS)):\n            for candidate in itertools.combinations_with_replacement(\n                    cls.LEADING_CHARS, i):\n                new_name = '~' + ''.join(candidate) + name\n                if new_name != name:\n                    yield new_name", "output": "Generates a series of temporary names.\n\n        The algorithm replaces the leading characters in the name\n        with ones that are valid filesystem characters, but are not\n        valid package names (for both Python and pip definitions of\n        package).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def classes_and_not_datetimelike(*klasses):\n    \"\"\"\n    \n    \"\"\"\n    return lambda tipo: (issubclass(tipo, klasses) and\n                         not issubclass(tipo, (np.datetime64, np.timedelta64)))", "output": "evaluate if the tipo is a subclass of the klasses\n    and not a datetimelike", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute_update(self, dml, params=None, param_types=None, query_mode=None):\n        \"\"\"\n        \"\"\"\n        params_pb = self._make_params_pb(params, param_types)\n        database = self._session._database\n        metadata = _metadata_with_prefix(database.name)\n        transaction = self._make_txn_selector()\n        api = database.spanner_api\n\n        response = api.execute_sql(\n            self._session.name,\n            dml,\n            transaction=transaction,\n            params=params_pb,\n            param_types=param_types,\n            query_mode=query_mode,\n            seqno=self._execute_sql_count,\n            metadata=metadata,\n        )\n\n        self._execute_sql_count += 1\n        return response.stats.row_count_exact", "output": "Perform an ``ExecuteSql`` API request with DML.\n\n        :type dml: str\n        :param dml: SQL DML statement\n\n        :type params: dict, {str -> column value}\n        :param params: values for parameter replacement.  Keys must match\n                       the names used in ``dml``.\n\n        :type param_types: dict[str -> Union[dict, .types.Type]]\n        :param param_types:\n            (Optional) maps explicit types for one or more param values;\n            required if parameters are passed.\n\n        :type query_mode:\n            :class:`google.cloud.spanner_v1.proto.ExecuteSqlRequest.QueryMode`\n        :param query_mode: Mode governing return of results / query plan. See\n            https://cloud.google.com/spanner/reference/rpc/google.spanner.v1#google.spanner.v1.ExecuteSqlRequest.QueryMode1\n\n        :rtype: int\n        :returns: Count of rows affected by the DML statement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync_pillar(saltenv=None, refresh=True, extmod_whitelist=None, extmod_blacklist=None):\n    '''\n    \n    '''\n    if __opts__['file_client'] != 'local':\n        raise CommandExecutionError(\n            'Pillar modules can only be synced to masterless minions'\n        )\n    ret = _sync('pillar', saltenv, extmod_whitelist, extmod_blacklist)\n    if refresh:\n        refresh_modules()\n        refresh_pillar()\n    return ret", "output": ".. versionadded:: 2015.8.11,2016.3.2\n\n    Sync pillar modules from the ``salt://_pillar`` directory on the Salt\n    fileserver. This function is environment-aware, pass the desired\n    environment to grab the contents of the ``_pillar`` directory from that\n    environment. The default environment, if none is specified,  is ``base``.\n\n    refresh : True\n        Also refresh the execution modules available to the minion, and refresh\n        pillar data.\n\n    extmod_whitelist : None\n        comma-seperated list of modules to sync\n\n    extmod_blacklist : None\n        comma-seperated list of modules to blacklist based on type\n\n    .. note::\n        This function will raise an error if executed on a traditional (i.e.\n        not masterless) minion\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.sync_pillar\n        salt '*' saltutil.sync_pillar saltenv=dev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock(self):  # type: () -> Installer\n        \"\"\"\n        \n        \"\"\"\n        self.update()\n        self.execute_operations(False)\n        self._lock = True\n\n        return self", "output": "Prepare the installer for locking only.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_direct_url_to_url(direct_url):\n    # type: (AnyStr) -> AnyStr\n    \"\"\"\n    \n    \"\"\"\n    direct_match = DIRECT_URL_RE.match(direct_url)  # type: Optional[Match]\n    if direct_match is None:\n        url_match = URL_RE.match(direct_url)\n        if url_match or is_valid_url(direct_url):\n            return direct_url\n    match_dict = (\n        {}\n    )  # type: Dict[STRING_TYPE, Union[Tuple[STRING_TYPE, ...], STRING_TYPE]]\n    if direct_match is not None:\n        match_dict = direct_match.groupdict()  # type: ignore\n    if not match_dict:\n        raise ValueError(\n            \"Failed converting value to normal URL, is it a direct URL? {0!r}\".format(\n                direct_url\n            )\n        )\n    url_segments = [match_dict.get(s) for s in (\"scheme\", \"host\", \"path\", \"pathsep\")]\n    url = \"\"  # type: STRING_TYPE\n    url = \"\".join([s for s in url_segments if s is not None])  # type: ignore\n    new_url = build_vcs_uri(\n        None,\n        url,\n        ref=match_dict.get(\"ref\"),\n        name=match_dict.get(\"name\"),\n        extras=match_dict.get(\"extras\"),\n        subdirectory=match_dict.get(\"subdirectory\"),\n    )\n    return new_url", "output": "Given a direct url as defined by *PEP 508*, convert to a :class:`~pip_shims.shims.Link`\n    compatible URL by moving the name and extras into an **egg_fragment**.\n\n    :param str direct_url: A pep-508 compliant direct url.\n    :return: A reformatted URL for use with Link objects and :class:`~pip_shims.shims.InstallRequirement` objects.\n    :rtype: AnyStr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_element_by_xpath(self, xpath):\n        \"\"\"\n        \n        \"\"\"\n        return self.find_element(by=By.XPATH, value=xpath)", "output": "Finds an element by xpath.\n\n        :Args:\n         - xpath - The xpath locator of the element to find.\n\n        :Returns:\n         - WebElement - the element if it was found\n\n        :Raises:\n         - NoSuchElementException - if the element wasn't found\n\n        :Usage:\n            ::\n\n                element = driver.find_element_by_xpath('//div/td[1]')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dumps(obj, protocol=None):\n    \"\"\"\n    \"\"\"\n    file = StringIO()\n    try:\n        cp = CloudPickler(file, protocol=protocol)\n        cp.dump(obj)\n        return file.getvalue()\n    finally:\n        file.close()", "output": "Serialize obj as a string of bytes allocated in memory\n\n    protocol defaults to cloudpickle.DEFAULT_PROTOCOL which is an alias to\n    pickle.HIGHEST_PROTOCOL. This setting favors maximum communication speed\n    between processes running the same Python version.\n\n    Set protocol=pickle.DEFAULT_PROTOCOL instead if you need to ensure\n    compatibility with older versions of Python.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Write(packer_type, buf, head, n):\n    \"\"\"  \"\"\"\n    packer_type.pack_into(buf, head, n)", "output": "Write encodes `n` at buf[head] using `packer_type`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setsebool(boolean, value, persist=False):\n    '''\n    \n    '''\n    if persist:\n        cmd = 'setsebool -P {0} {1}'.format(boolean, value)\n    else:\n        cmd = 'setsebool {0} {1}'.format(boolean, value)\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)", "output": "Set the value for a boolean\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.setsebool virt_use_usb off", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_issues_url(page, after):\n    \"\"\"\"\"\"\n    template = '{base_url}/{owner}/{repo}/issues?state=closed&per_page=100&page={page}&since={after}'\n    return template.format(page=page, after=after.isoformat(), **API_PARAMS)", "output": "Returns github API URL for querying tags.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def patch_csi_driver(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_csi_driver_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.patch_csi_driver_with_http_info(name, body, **kwargs)\n            return data", "output": "partially update the specified CSIDriver\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_csi_driver(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the CSIDriver (required)\n        :param object body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).\n        :param bool force: Force is going to \\\"force\\\" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.\n        :return: V1beta1CSIDriver\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _map_unity_proxy_to_object(value):\n    \"\"\"\n    \n    \"\"\"\n    vtype = type(value)\n    if vtype in _proxy_map:\n        return _proxy_map[vtype](value)\n    elif vtype == list:\n        return [_map_unity_proxy_to_object(v) for v in value]\n    elif vtype == dict:\n        return {k:_map_unity_proxy_to_object(v) for k,v in value.items()}\n    else:\n        return value", "output": "Map returning value, if it is unity SFrame, SArray, map it", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_compressor_options(\n        self,\n        side: str,\n        agreed_parameters: Dict[str, Any],\n        compression_options: Dict[str, Any] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"\n        \"\"\"\n        options = dict(\n            persistent=(side + \"_no_context_takeover\") not in agreed_parameters\n        )  # type: Dict[str, Any]\n        wbits_header = agreed_parameters.get(side + \"_max_window_bits\", None)\n        if wbits_header is None:\n            options[\"max_wbits\"] = zlib.MAX_WBITS\n        else:\n            options[\"max_wbits\"] = int(wbits_header)\n        options[\"compression_options\"] = compression_options\n        return options", "output": "Converts a websocket agreed_parameters set to keyword arguments\n        for our compressor objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_storer(self, key):\n        \"\"\"  \"\"\"\n        group = self.get_node(key)\n        if group is None:\n            raise KeyError('No object named {key} in the file'.format(key=key))\n\n        s = self._create_storer(group)\n        s.infer_axes()\n        return s", "output": "return the storer object for a key, raise if not in the file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _select_most_recent_symbols_chunk(self, sid_group):\n        \"\"\"\n        \"\"\"\n        cols = self.equity_symbol_mappings.c\n\n        # These are the columns we actually want.\n        data_cols = (cols.sid,) + tuple(cols[name] for name in symbol_columns)\n\n        # Also select the max of end_date so that all non-grouped fields take\n        # on the value associated with the max end_date. The SQLite docs say\n        # this:\n        #\n        # When the min() or max() aggregate functions are used in an aggregate\n        # query, all bare columns in the result set take values from the input\n        # row which also contains the minimum or maximum. Only the built-in\n        # min() and max() functions work this way.\n        #\n        # See https://www.sqlite.org/lang_select.html#resultset, for more info.\n        to_select = data_cols + (sa.func.max(cols.end_date),)\n\n        return sa.select(\n            to_select,\n        ).where(\n            cols.sid.in_(map(int, sid_group))\n        ).group_by(\n            cols.sid,\n        )", "output": "Retrieve the most recent symbol for a set of sids.\n\n        Parameters\n        ----------\n        sid_group : iterable[int]\n            The sids to lookup. The length of this sequence must be less than\n            or equal to SQLITE_MAX_VARIABLE_NUMBER because the sids will be\n            passed in as sql bind params.\n\n        Returns\n        -------\n        sel : Selectable\n            The sqlalchemy selectable that will query for the most recent\n            symbol for each sid.\n\n        Notes\n        -----\n        This is implemented as an inner select of the columns of interest\n        ordered by the end date of the (sid, symbol) mapping. We then group\n        that inner select on the sid with no aggregations to select the last\n        row per group which gives us the most recently active symbol for all\n        of the sids.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _contents_changed(self):\n        \"\"\"\"\"\"\n        desc_chars = (len(self.input_description.toPlainText()) -\n                      self.initial_chars)\n        if desc_chars < DESC_MIN_CHARS:\n            self.desc_chars_label.setText(\n                u\"{} {}\".format(DESC_MIN_CHARS - desc_chars,\n                                _(\"more characters to go...\")))\n        else:\n            self.desc_chars_label.setText(_(\"Description complete; thanks!\"))\n\n        title_chars = len(self.title.text())\n        if title_chars < TITLE_MIN_CHARS:\n            self.title_chars_label.setText(\n                u\"{} {}\".format(TITLE_MIN_CHARS - title_chars,\n                                _(\"more characters to go...\")))\n        else:\n            self.title_chars_label.setText(_(\"Title complete; thanks!\"))\n\n        submission_enabled = (desc_chars >= DESC_MIN_CHARS and\n                              title_chars >= TITLE_MIN_CHARS)\n        self.submit_btn.setEnabled(submission_enabled)", "output": "Activate submit_btn.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _refine_mode(mode):\n    '''\n    \n    '''\n    mode = six.text_type(mode).lower()\n    if any([mode.startswith('e'),\n            mode == '1',\n            mode == 'on']):\n        return 'Enforcing'\n    if any([mode.startswith('p'),\n            mode == '0',\n            mode == 'off']):\n        return 'Permissive'\n    if any([mode.startswith('d')]):\n        return 'Disabled'\n    return 'unknown'", "output": "Return a mode value that is predictable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_nearest_indexer(self, target, limit, tolerance):\n        \"\"\"\n        \n        \"\"\"\n        left_indexer = self.get_indexer(target, 'pad', limit=limit)\n        right_indexer = self.get_indexer(target, 'backfill', limit=limit)\n\n        target = np.asarray(target)\n        left_distances = abs(self.values[left_indexer] - target)\n        right_distances = abs(self.values[right_indexer] - target)\n\n        op = operator.lt if self.is_monotonic_increasing else operator.le\n        indexer = np.where(op(left_distances, right_distances) |\n                           (right_indexer == -1), left_indexer, right_indexer)\n        if tolerance is not None:\n            indexer = self._filter_indexer_tolerance(target, indexer,\n                                                     tolerance)\n        return indexer", "output": "Get the indexer for the nearest index labels; requires an index with\n        values that can be subtracted from each other (e.g., not strings or\n        tuples).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_cyclic(head):\n    \"\"\"\n    \n    \"\"\"\n    if not head:\n        return False\n    runner = head\n    walker = head\n    while runner.next and runner.next.next:\n        runner = runner.next.next\n        walker = walker.next\n        if runner == walker:\n            return True\n    return False", "output": ":type head: Node\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_target_group(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    if not __salt__['boto_elbv2.target_group_exists'](name, region, key, keyid, profile):\n        ret['result'] = True\n        ret['comment'] = 'Target Group {0} does not exists'.format(name)\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'Target Group {0} will be deleted'.format(name)\n        return ret\n\n    state = __salt__['boto_elbv2.delete_target_group'](name,\n                                                       region=region,\n                                                       key=key,\n                                                       keyid=keyid,\n                                                       profile=profile)\n\n    if state:\n        ret['result'] = True\n        ret['changes']['target_group'] = name\n        ret['comment'] = 'Target Group {0} deleted'.format(name)\n    else:\n        ret['result'] = False\n        ret['comment'] = 'Target Group {0} deletion failed'.format(name)\n    return ret", "output": "Delete target group.\n\n    name\n        (string) - The Amazon Resource Name (ARN) of the resource.\n\n    returns\n        (bool) - True on success, False on failure.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        check-target:\n          boto_elb2.delete_targets_group:\n            - name: myALB\n            - protocol: https\n            - port: 443\n            - vpc_id: myVPC", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_summary(self, session, frame):\n    ''''''\n    summary = session.run(self.summary_op, feed_dict={\n        self.frame_placeholder: frame\n    })\n    path = '{}/{}'.format(self.PLUGIN_LOGDIR, SUMMARY_FILENAME)\n    write_file(summary, path)", "output": "Writes the frame to disk as a tensor summary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_simulated_env(\n    output_dir, grayscale, resize_width_factor, resize_height_factor,\n    frame_stack_size, generative_model, generative_model_params,\n    random_starts=True, which_epoch_data=\"last\", **other_hparams\n):\n  \"\"\"\"\"\"\"\n  # We need these, to initialize T2TGymEnv, but these values (hopefully) have\n  # no effect on player.\n  a_bit_risky_defaults = {\n      \"game\": \"pong\",  # assumes that T2TGymEnv has always reward_range (-1,1)\n      \"real_batch_size\": 1,\n      \"rl_env_max_episode_steps\": -1,\n      \"max_num_noops\": 0\n  }\n\n  for key in a_bit_risky_defaults:\n    if key not in other_hparams:\n      other_hparams[key] = a_bit_risky_defaults[key]\n\n  hparams = hparam.HParams(\n      grayscale=grayscale,\n      resize_width_factor=resize_width_factor,\n      resize_height_factor=resize_height_factor,\n      frame_stack_size=frame_stack_size,\n      generative_model=generative_model,\n      generative_model_params=generative_model_params,\n      **other_hparams\n  )\n  return load_data_and_make_simulated_env(\n      output_dir, wm_dir=None, hparams=hparams,\n      which_epoch_data=which_epoch_data,\n      random_starts=random_starts)", "output": "Create SimulatedEnv with minimal subset of hparams.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def term_all_jobs():\n    '''\n    \n    '''\n    ret = []\n    for data in running():\n        ret.append(signal_job(data['jid'], signal.SIGTERM))\n    return ret", "output": "Sends a termination signal (SIGTERM 15) to all currently running jobs\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.term_all_jobs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe(DomainName,\n             region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        domain = conn.describe_elasticsearch_domain_config(DomainName=DomainName)\n        if domain and 'DomainConfig' in domain:\n            domain = domain['DomainConfig']\n            keys = ('ElasticsearchClusterConfig', 'EBSOptions', 'AccessPolicies',\n                    'SnapshotOptions', 'AdvancedOptions')\n            return {'domain': dict([(k, domain.get(k, {}).get('Options')) for k in keys if k in domain])}\n        else:\n            return {'domain': None}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Given a domain name describe its properties.\n\n    Returns a dictionary of interesting properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_elasticsearch_domain.describe mydomain", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_color_scheme(self, color_scheme, reset=True):\r\n        \"\"\"\"\"\"\r\n        # Needed to handle not initialized kernel_client\r\n        # See issue 6996\r\n        try:\r\n            self.shellwidget.set_color_scheme(color_scheme, reset)\r\n        except AttributeError:\r\n            pass", "output": "Set IPython color scheme.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_load(jid, load, minions=None):\n    '''\n    \n    '''\n    options = _get_options()\n\n    index = options['master_job_cache_index']\n    doc_type = options['master_job_cache_doc_type']\n\n    if options['index_date']:\n        index = '{0}-{1}'.format(index,\n            datetime.date.today().strftime('%Y.%m.%d'))\n\n    _ensure_index(index)\n\n    # addressing multiple types (bool, string, dict, ...) issue in master_job_cache index for return key (#20826)\n    if not load.get('return', None) is None:\n        # if load.return is not a dict, moving the result to load.return.<job_fun_escaped>.return\n        if not isinstance(load['return'], dict):\n            job_fun_escaped = load['fun'].replace('.', '_')\n            load['return'] = {job_fun_escaped: {'return': load['return']}}\n        # rename load.return to load.data in order to have the same key in all indices (master_job_cache, job)\n        load['data'] = load.pop('return')\n\n    data = {\n        'jid': jid,\n        'load': load,\n    }\n\n    ret = __salt__['elasticsearch.document_create'](index=index,\n                                                    doc_type=doc_type,\n                                                    id=jid,\n                                                    body=salt.utils.json.dumps(data))", "output": "Save the load to the specified jid id\n\n    .. versionadded:: 2015.8.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, input):\n        \"\"\" \n        \"\"\"\n        sl,bs = input.size()\n        if bs!=self.bs:\n            self.bs=bs\n            self.reset()\n        with set_grad_enabled(self.training):\n            emb = self.encoder_with_dropout(input, dropout=self.dropoute if self.training else 0)\n            emb = self.dropouti(emb)\n            raw_output = emb\n            new_hidden,raw_outputs,outputs = [],[],[]\n            for l, (rnn,drop) in enumerate(zip(self.rnns, self.dropouths)):\n                current_input = raw_output\n                with warnings.catch_warnings():\n                    warnings.simplefilter(\"ignore\")\n                    raw_output, new_h = rnn(raw_output, self.hidden[l])\n                new_hidden.append(new_h)\n                raw_outputs.append(raw_output)\n                if l != self.n_layers - 1: raw_output = drop(raw_output)\n                outputs.append(raw_output)\n\n            self.hidden = repackage_var(new_hidden)\n        return raw_outputs, outputs", "output": "Invoked during the forward propagation of the RNN_Encoder module.\n        Args:\n            input (Tensor): input of shape (sentence length x batch_size)\n\n        Returns:\n            raw_outputs (tuple(list (Tensor), list(Tensor)): list of tensors evaluated from each RNN layer without using\n            dropouth, list of tensors evaluated from each RNN layer using dropouth,", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cleanup_expired_assets(self, dt, position_assets):\n        \"\"\"\n        \n        \"\"\"\n        algo = self.algo\n\n        def past_auto_close_date(asset):\n            acd = asset.auto_close_date\n            return acd is not None and acd <= dt\n\n        # Remove positions in any sids that have reached their auto_close date.\n        assets_to_clear = \\\n            [asset for asset in position_assets if past_auto_close_date(asset)]\n        metrics_tracker = algo.metrics_tracker\n        data_portal = self.data_portal\n        for asset in assets_to_clear:\n            metrics_tracker.process_close_position(asset, dt, data_portal)\n\n        # Remove open orders for any sids that have reached their auto close\n        # date. These orders get processed immediately because otherwise they\n        # would not be processed until the first bar of the next day.\n        blotter = algo.blotter\n        assets_to_cancel = [\n            asset for asset in blotter.open_orders\n            if past_auto_close_date(asset)\n        ]\n        for asset in assets_to_cancel:\n            blotter.cancel_all_orders_for_asset(asset)\n\n        # Make a copy here so that we are not modifying the list that is being\n        # iterated over.\n        for order in copy(blotter.new_orders):\n            if order.status == ORDER_STATUS.CANCELLED:\n                metrics_tracker.process_order(order)\n                blotter.new_orders.remove(order)", "output": "Clear out any assets that have expired before starting a new sim day.\n\n        Performs two functions:\n\n        1. Finds all assets for which we have open orders and clears any\n           orders whose assets are on or after their auto_close_date.\n\n        2. Finds all assets for which we have positions and generates\n           close_position events for any assets that have reached their\n           auto_close_date.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable(name, **kwargs):\n    '''\n    \n    '''\n    if _service_is_upstart(name):\n        return _upstart_enable(name)\n    executable = _get_service_exec()\n    cmd = '{0} -f {1} defaults'.format(executable, name)\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)", "output": "Enable the named service to start at boot\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enable <service name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _all_minions(self, expr=None):\n        '''\n        \n        '''\n        mlist = []\n        for fn_ in salt.utils.data.sorted_ignorecase(os.listdir(os.path.join(self.opts['pki_dir'], self.acc))):\n            if not fn_.startswith('.') and os.path.isfile(os.path.join(self.opts['pki_dir'], self.acc, fn_)):\n                mlist.append(fn_)\n        return {'minions': mlist, 'missing': []}", "output": "Return a list of all minions that have auth'd", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_batch(self, job_id, data, file_type):\n        \"\"\"\n        \n        \"\"\"\n        if not job_id or not self.has_active_session():\n            raise Exception(\"Can not create a batch without a valid job_id and an active session.\")\n\n        headers = self._get_create_batch_content_headers(file_type)\n        headers['Content-Length'] = str(len(data))\n\n        response = requests.post(self._get_create_batch_url(job_id),\n                                 headers=headers,\n                                 data=data)\n        response.raise_for_status()\n\n        root = ET.fromstring(response.text)\n        batch_id = root.find('%sid' % self.API_NS).text\n        return batch_id", "output": "Creates a batch with either a string of data or a file containing data.\n\n        If a file is provided, this will pull the contents of the file_target into memory when running.\n        That shouldn't be a problem for any files that meet the Salesforce single batch upload\n        size limit (10MB) and is done to ensure compressed files can be uploaded properly.\n\n        :param job_id: job_id as returned by 'create_operation_job(...)'\n        :param data:\n\n        :return: Returns batch_id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vqa_attention_base_range(rhp):\n  \"\"\"\"\"\"\n  # After starting from base, set intervals for some parameters.\n  rhp.set_float(\"learning_rate\", 0.1, 1.0, scale=rhp.LOG_SCALE)\n  rhp.set_float(\"clip_grad_norm\", 0.1, 10, scale=rhp.LOG_SCALE)\n  rhp.set_discrete(\"batch_size\", [128, 256, 512, 1024])\n  rhp.set_float(\"weight_decay\", 0.0, 1e-4)\n  rhp.set_categorical(\"rnn_type\", [\"lstm\", \"lstm_layernorm\"])", "output": "Small range of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eval(self, data, name, feval=None):\n        \"\"\"\n        \"\"\"\n        if not isinstance(data, Dataset):\n            raise TypeError(\"Can only eval for Dataset instance\")\n        data_idx = -1\n        if data is self.train_set:\n            data_idx = 0\n        else:\n            for i in range_(len(self.valid_sets)):\n                if data is self.valid_sets[i]:\n                    data_idx = i + 1\n                    break\n        # need to push new valid data\n        if data_idx == -1:\n            self.add_valid(data, name)\n            data_idx = self.__num_dataset - 1\n\n        return self.__inner_eval(name, data_idx, feval)", "output": "Evaluate for data.\n\n        Parameters\n        ----------\n        data : Dataset\n            Data for the evaluating.\n        name : string\n            Name of the data.\n        feval : callable or None, optional (default=None)\n            Customized evaluation function.\n            Should accept two parameters: preds, train_data,\n            and return (eval_name, eval_result, is_higher_better) or list of such tuples.\n            For multi-class task, the preds is group by class_id first, then group by row_id.\n            If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\n\n        Returns\n        -------\n        result : list\n            List with evaluation results.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(self, periods=1, axis=0):\r\n        \"\"\"\r\n        \"\"\"\r\n        axis = self._get_axis_number(axis)\r\n        return self.__constructor__(\r\n            query_compiler=self._query_compiler.diff(periods=periods, axis=axis)\r\n        )", "output": "Finds the difference between elements on the axis requested\r\n\r\n        Args:\r\n            periods: Periods to shift for forming difference\r\n            axis: Take difference over rows or columns\r\n\r\n        Returns:\r\n            DataFrame with the diff applied", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_time_indices(s):\n  \"\"\"\n  \"\"\"\n  if not s.startswith('['):\n    s = '[' + s + ']'\n  parsed = command_parser._parse_slices(s)\n  if len(parsed) != 1:\n    raise ValueError(\n        'Invalid number of slicing objects in time indices (%d)' % len(parsed))\n  else:\n    return parsed[0]", "output": "Parse a string as time indices.\n\n  Args:\n    s: A valid slicing string for time indices. E.g., '-1', '[:]', ':', '2:10'\n\n  Returns:\n    A slice object.\n\n  Raises:\n    ValueError: If `s` does not represent valid time indices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deserialize(stream_or_string, **options):\n    '''\n    \n    '''\n\n    try:\n        if not isinstance(stream_or_string, (bytes, six.string_types)):\n            return salt.utils.json.load(\n                stream_or_string, _json_module=_json, **options)\n\n        if isinstance(stream_or_string, bytes):\n            stream_or_string = stream_or_string.decode('utf-8')\n\n        return salt.utils.json.loads(stream_or_string, _json_module=_json)\n    except Exception as error:\n        raise DeserializationError(error)", "output": "Deserialize any string or stream like object into a Python data structure.\n\n    :param stream_or_string: stream or string to deserialize.\n    :param options: options given to lower json/simplejson module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MessageToDict(message,\n                  including_default_value_fields=False,\n                  preserving_proto_field_name=False):\n  \"\"\"\n  \"\"\"\n  printer = _Printer(including_default_value_fields,\n                     preserving_proto_field_name)\n  # pylint: disable=protected-access\n  return printer._MessageToJsonObject(message)", "output": "Converts protobuf message to a JSON dictionary.\n\n  Args:\n    message: The protocol buffers message instance to serialize.\n    including_default_value_fields: If True, singular primitive fields,\n        repeated fields, and map fields will always be serialized.  If\n        False, only serialize non-empty fields.  Singular message fields\n        and oneof fields are not affected by this option.\n    preserving_proto_field_name: If True, use the original proto field\n        names as defined in the .proto file. If False, convert the field\n        names to lowerCamelCase.\n\n  Returns:\n    A dict representation of the JSON formatted protocol buffer message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_file(filename):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    from qtpy.QtCore import QUrl\r\n    from qtpy.QtGui import QDesktopServices\r\n\r\n    # We need to use setUrl instead of setPath because this is the only\r\n    # cross-platform way to open external files. setPath fails completely on\r\n    # Mac and doesn't open non-ascii files on Linux.\r\n    # Fixes Issue 740\r\n    url = QUrl()\r\n    url.setUrl(filename)\r\n    return QDesktopServices.openUrl(url)", "output": "Generalized os.startfile for all platforms supported by Qt\r\n\r\n    This function is simply wrapping QDesktopServices.openUrl\r\n\r\n    Returns True if successfull, otherwise returns False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restart(job_label, runas=None):\n    '''\n    \n    '''\n    stop(job_label, runas=runas)\n    return start(job_label, runas=runas)", "output": "Restart the named service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.restart <service label>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_delta(dist_post_update, current_iteration,\n                 clip_max, clip_min, d, theta, constraint):\n  \"\"\"\n  \n  \"\"\"\n  if current_iteration == 1:\n    delta = 0.1 * (clip_max - clip_min)\n  else:\n    if constraint == 'l2':\n      delta = np.sqrt(d) * theta * dist_post_update\n    elif constraint == 'linf':\n      delta = d * theta * dist_post_update\n\n  return delta", "output": "Choose the delta at the scale of distance\n   between x and perturbed sample.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_moe_2k():\n  \"\"\"\n  \"\"\"\n  hparams = transformer_moe_8k()\n  hparams.batch_size = 2048\n\n  hparams.default_ff = \"sep\"\n\n  # hparams.layer_types contains the network architecture:\n  encoder_archi = \"a/a/a/a/a\"\n  decoder_archi = \"a-sepm/a-sepm/a-moe/a-sepm/a-sepm\"\n  hparams.layer_types = \"{}#{}\".format(encoder_archi, decoder_archi)\n\n  return hparams", "output": "Base transformers model with moe.\n\n  Will have the following architecture:\n  * No encoder.\n    * Layer 0: a - sep  (self-attention - unmasked separable convolutions)\n    * Layer 1: a - sep\n    * Layer 2: a - sep\n    * Layer 3: a - sep\n    * Layer 4: a - sep\n  * Decoder architecture:\n    * Layer 0: a - a - sepm  (self-attention - enco/deco-attention - masked sep)\n    * Layer 1: a - a - sepm\n    * Layer 2: a - a - moe  (mixture of expert layers in the middle)\n    * Layer 3: a - a - sepm\n    * Layer 4: a - a - sepm\n\n  Returns:\n    hparams", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_python_version(output):\n    \"\"\"\n    \"\"\"\n    version_line = output.split(\"\\n\", 1)[0]\n    version_pattern = re.compile(\n        r\"\"\"\n        ^                   # Beginning of line.\n        Python              # Literally \"Python\".\n        \\s                  # Space.\n        (?P<major>\\d+)      # Major = one or more digits.\n        \\.                  # Dot.\n        (?P<minor>\\d+)      # Minor = one or more digits.\n        (?:                 # Unnamed group for dot-micro.\n            \\.              # Dot.\n            (?P<micro>\\d+)  # Micro = one or more digit.\n        )?                  # Micro is optional because pypa/pipenv#1893.\n        .*                  # Trailing garbage.\n        $                   # End of line.\n    \"\"\",\n        re.VERBOSE,\n    )\n\n    match = version_pattern.match(version_line)\n    if not match:\n        return None\n    return match.groupdict(default=\"0\")", "output": "Parse a Python version output returned by `python --version`.\n\n    Return a dict with three keys: major, minor, and micro. Each value is a\n    string containing a version part.\n\n    Note: The micro part would be `'0'` if it's missing from the input string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def leaveWhitespace( self ):\n        \"\"\"\"\"\"\n        self.skipWhitespace = False\n        self.exprs = [ e.copy() for e in self.exprs ]\n        for e in self.exprs:\n            e.leaveWhitespace()\n        return self", "output": "Extends ``leaveWhitespace`` defined in base class, and also invokes ``leaveWhitespace`` on\n           all contained expressions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def samefile(self, other_path):\n        \"\"\"\n        \"\"\"\n        if hasattr(os.path, \"samestat\"):\n            st = self.stat()\n            try:\n                other_st = other_path.stat()\n            except AttributeError:\n                other_st = os.stat(other_path)\n            return os.path.samestat(st, other_st)\n        else:\n            filename1 = six.text_type(self)\n            filename2 = six.text_type(other_path)\n            st1 = _win32_get_unique_path_id(filename1)\n            st2 = _win32_get_unique_path_id(filename2)\n            return st1 == st2", "output": "Return whether other_path is the same or not as this file\n        (as returned by os.path.samefile()).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def newTextChild(self, ns, name, content):\n        \"\"\" \"\"\"\n        if ns is None: ns__o = None\n        else: ns__o = ns._o\n        ret = libxml2mod.xmlNewTextChild(self._o, ns__o, name, content)\n        if ret is None:raise treeError('xmlNewTextChild() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Creation of a new child element, added at the end of\n          @parent children list. @ns and @content parameters are\n          optional (None). If @ns is None, the newly created element\n          inherits the namespace of @parent. If @content is non None,\n          a child TEXT node will be created containing the string\n          @content. NOTE: Use xmlNewChild() if @content will contain\n          entities that need to be preserved. Use this function,\n          xmlNewTextChild(), if you need to ensure that reserved XML\n          chars that might appear in @content, such as the ampersand,\n          greater-than or less-than signs, are automatically replaced\n           by their XML escaped entity representations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_layer_opt(self, lrs, wds):\n\n        \"\"\"\n        \"\"\"\n        return LayerOptimizer(self.opt_fn, self.get_layer_groups(), lrs, wds)", "output": "Method returns an instance of the LayerOptimizer class, which\n        allows for setting differential learning rates for different\n        parts of the model.\n\n        An example of how a model maybe differentiated into different parts\n        for application of differential learning rates and weight decays is\n        seen in ../.../courses/dl1/fastai/conv_learner.py, using the dict\n        'model_meta'. Currently, this seems supported only for convolutional\n        networks such as VGG-19, ResNet-XX etc.\n\n        Args:\n            lrs (float or list(float)): learning rate(s) for the model\n\n            wds (float or list(float)): weight decay parameter(s).\n\n        Returns:\n            An instance of a LayerOptimizer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _needs_reindex_multi(self, axes, method, level):\n        \"\"\"\"\"\"\n        return ((com.count_not_none(*axes.values()) == self._AXIS_LEN) and\n                method is None and level is None and not self._is_mixed_type)", "output": "Check if we do need a multi reindex.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _heappop_max(heap):\n    \"\"\"\"\"\"\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup_max(heap, 0)\n        return returnitem\n    return lastelt", "output": "Maxheap version of a heappop.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_distributed_session_creator(server):\n    \"\"\"\n    \n    \"\"\"\n\n    server_def = server.server_def\n    is_chief = (server_def.job_name == 'worker') and (server_def.task_index == 0)\n\n    init_op = tf.global_variables_initializer()\n    local_init_op = tf.local_variables_initializer()\n    ready_op = tf.report_uninitialized_variables()\n    ready_for_local_init_op = tf.report_uninitialized_variables(tf.global_variables())\n    sm = tf.train.SessionManager(\n        local_init_op=local_init_op,\n        ready_op=ready_op,\n        ready_for_local_init_op=ready_for_local_init_op,\n        graph=tf.get_default_graph())\n\n    # to debug wrong variable collection\n    # from pprint import pprint\n    # print(\"GLOBAL:\")\n    # pprint([(k.name, k.device) for k in tf.global_variables()])\n    # print(\"LOCAL:\")\n    # pprint([(k.name, k.device) for k in tf.local_variables()])\n\n    class _Creator(tf.train.SessionCreator):\n        def create_session(self):\n            if is_chief:\n                return sm.prepare_session(master=server.target, init_op=init_op)\n            else:\n                tf.logging.set_verbosity(tf.logging.INFO)   # print message about uninitialized vars\n                ret = sm.wait_for_session(master=server.target)\n                tf.logging.set_verbosity(tf.logging.WARN)\n                return ret\n\n    return _Creator()", "output": "Args:\n       server (tf.train.Server):\n\n    Returns:\n        tf.train.SessionCreator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def match_class_glob(_class, saltclass_path):\n    '''\n    \n    '''\n    straight, sub_init, sub_straight = get_class_paths(_class, saltclass_path)\n    classes = []\n    matches = []\n    matches.extend(glob.glob(straight))\n    matches.extend(glob.glob(sub_straight))\n    matches.extend(glob.glob(sub_init))\n    if not matches:\n        log.warning('%s: Class globbing did not yield any results', _class)\n    for match in matches:\n        classes.append(get_class_from_file(match, saltclass_path))\n    return classes", "output": "Takes a class name possibly including `*` or `?` wildcards (or any other wildcards supportet by `glob.glob`) and\n    returns a list of expanded class names without wildcards.\n\n    .. code-block:: python\n\n       classes = match_class_glob('services.*', '/srv/saltclass')\n       print(classes)\n       # services.mariadb\n       # services.nginx...\n\n\n    :param str _class: dotted class name, globbing allowed.\n    :param str saltclass_path: path to the saltclass root directory.\n\n    :return: The list of expanded class matches.\n    :rtype: list(str)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uniformVectorRDD(sc, numRows, numCols, numPartitions=None, seed=None):\n        \"\"\"\n        \n        \"\"\"\n        return callMLlibFunc(\"uniformVectorRDD\", sc._jsc, numRows, numCols, numPartitions, seed)", "output": "Generates an RDD comprised of vectors containing i.i.d. samples drawn\n        from the uniform distribution U(0.0, 1.0).\n\n        :param sc: SparkContext used to create the RDD.\n        :param numRows: Number of Vectors in the RDD.\n        :param numCols: Number of elements in each Vector.\n        :param numPartitions: Number of partitions in the RDD.\n        :param seed: Seed for the RNG that generates the seed for the generator in each partition.\n        :return: RDD of Vector with vectors containing i.i.d samples ~ `U(0.0, 1.0)`.\n\n        >>> import numpy as np\n        >>> mat = np.matrix(RandomRDDs.uniformVectorRDD(sc, 10, 10).collect())\n        >>> mat.shape\n        (10, 10)\n        >>> mat.max() <= 1.0 and mat.min() >= 0.0\n        True\n        >>> RandomRDDs.uniformVectorRDD(sc, 10, 10, 4).getNumPartitions()\n        4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_bookmarks_without_file(filename):\n    \"\"\"\"\"\"\n    bookmarks = _load_all_bookmarks()\n    return {k: v for k, v in bookmarks.items() if v[0] != filename}", "output": "Load all bookmarks but those from a specific file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_temp_diagrams(config, results, temp_dir):\n    \"\"\"\"\"\"\n    display_name = {\n        'time': 'Compilation time (s)',\n        'memory': 'Compiler memory usage (MB)',\n    }\n\n    files = config['files']\n    img_files = []\n\n    if any('slt' in result for result in results) and 'bmp' in files.values()[0]:\n        config['modes']['slt'] = 'Using BOOST_METAPARSE_STRING with string literal templates'\n        for f in files.values():\n            f['slt'] = f['bmp'].replace('bmp', 'slt')\n\n    for measured in ['time', 'memory']:\n        mpts = sorted(int(k) for k in files.keys())\n        img_files.append(os.path.join(temp_dir, '_{0}.png'.format(measured)))\n        plot(\n            {\n                m: [(x, results[files[str(x)][m]][measured]) for x in mpts]\n                for m in config['modes'].keys()\n            },\n            config['modes'],\n            display_name[measured],\n            (config['x_axis_label'], display_name[measured]),\n            img_files[-1]\n        )\n    return img_files", "output": "Plot temporary diagrams", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aggregate_tile(cells, ti, tj, aggregate, params, metadata, layout, summary):\n  \"\"\"\n    \n  \"\"\"\n  tile = []\n  keys = cells.keys()\n  for i,key in enumerate(keys):\n    print(\"cell\", i+1, \"/\", len(keys), end='\\r')\n    cell_json = aggregate(cells[key], params, metadata, layout, summary)\n    tile.append({\"aggregate\":cell_json, \"i\":int(key[0]), \"j\":int(key[1])})\n  return tile", "output": "Call the user defined aggregation function on each cell and combine into a single json object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_principal(name):\n    '''\n    \n    '''\n    ret = {}\n\n    cmd = __execute_kadmin('delprinc -force {0}'.format(name))\n\n    if cmd['retcode'] != 0 or cmd['stderr']:\n        ret['comment'] = cmd['stderr'].splitlines()[-1]\n        ret['result'] = False\n\n        return ret\n\n    return True", "output": "Delete Principal\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'kdc.example.com' kerberos.delete_principal host/example.com@EXAMPLE.COM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_standardize(text):\n    \"\"\"\n    \n    \"\"\"\n    text = text.replace('\u2014', '-')\n    text = text.replace('\u2013', '-')\n    text = text.replace('\u2015', '-')\n    text = text.replace('\u2026', '...')\n    text = text.replace('\u00b4', \"'\")\n    text = re.sub(r'''(-+|~+|!+|\"+|;+|\\?+|\\++|,+|\\)+|\\(+|\\\\+|\\/+|\\*+|\\[+|\\]+|}+|{+|\\|+|_+)''', r' \\1 ', text)\n    text = re.sub(r'\\s*\\n\\s*', ' \\n ', text)\n    text = re.sub(r'[^\\S\\n]+', ' ', text)\n    return text.strip()", "output": "Apply text standardization following original implementation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mediatype_update(mediatypeid, name=False, mediatype=False, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'mediatype.update'\n            params = {\"mediatypeid\": mediatypeid}\n            if name:\n                params['description'] = name\n            if mediatype:\n                params['type'] = mediatype\n            params = _params_extend(params, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['mediatypeids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Update existing mediatype\n\n    .. note::\n        This function accepts all standard mediatype properties: keyword\n        argument names differ depending on your zabbix version, see here__.\n\n        .. __: https://www.zabbix.com/documentation/3.0/manual/api/reference/mediatype/object\n\n    :param mediatypeid: ID of the mediatype to update\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: IDs of the updated mediatypes, False on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.usergroup_update 8 name=\"Email update\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlNodeDumpFormatOutput(self, doc, cur, encoding, format):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        libxml2mod.htmlNodeDumpFormatOutput(self._o, doc__o, cur__o, encoding, format)", "output": "Dump an HTML node, recursive behaviour,children are printed\n           too.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode(self, ids, strip_extraneous=False):\n    \"\"\"\n    \"\"\"\n    del strip_extraneous\n    _, tmp_file_path = tempfile.mkstemp(\"_decode.png\")\n    if self._height is None or self._width is None:\n      size = int(math.sqrt(len(ids) / self._channels))\n      length = size * size * self._channels\n    else:\n      size = None\n      length = self._height * self._width * self._channels\n    if len(ids) != length:\n      raise ValueError(\"Length of ids (%d) must be height (%d) x width (%d) x \"\n                       \"channels (%d); %d != %d.\\n Ids: %s\"\n                       % (len(ids), self._height, self._width, self._channels,\n                          len(ids), length, \" \".join([str(i) for i in ids])))\n    with tf.Graph().as_default():\n      raw = tf.constant(ids, dtype=tf.uint8)\n      if size is None:\n        img = tf.reshape(raw, [self._height, self._width, self._channels])\n      else:\n        img = tf.reshape(raw, [size, size, self._channels])\n      png = tf.image.encode_png(img)\n      op = tf.write_file(tmp_file_path, png)\n      with tf.Session() as sess:\n        sess.run(op)\n    return tmp_file_path", "output": "Transform a sequence of int ids into an image file.\n\n    Args:\n      ids: list of integers to be converted.\n      strip_extraneous: unused\n\n    Returns:\n      Path to the temporary file where the image was saved.\n\n    Raises:\n      ValueError: if the ids are not of the appropriate size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subvolume_create(name, dest=None, qgroupids=None):\n    '''\n    \n\n    '''\n    if qgroupids and type(qgroupids) is not list:\n        raise CommandExecutionError('Qgroupids parameter must be a list')\n\n    if dest:\n        name = os.path.join(dest, name)\n\n    # If the subvolume is there, we are done\n    if subvolume_exists(name):\n        return False\n\n    cmd = ['btrfs', 'subvolume', 'create']\n    if type(qgroupids) is list:\n        cmd.append('-i')\n        cmd.extend(qgroupids)\n    cmd.append(name)\n\n    res = __salt__['cmd.run_all'](cmd)\n    salt.utils.fsutils._verify_run(res)\n    return True", "output": "Create subvolume `name` in `dest`.\n\n    Return True if the subvolume is created, False is the subvolume is\n    already there.\n\n    name\n         Name of the new subvolume\n\n    dest\n         If not given, the subvolume will be created in the current\n         directory, if given will be in /dest/name\n\n    qgroupids\n         Add the newly created subcolume to a qgroup. This parameter\n         is a list\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' btrfs.subvolume_create var\n        salt '*' btrfs.subvolume_create var dest=/mnt\n        salt '*' btrfs.subvolume_create var qgroupids='[200]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize_stats_dict(individual):\n    '''\n    \n    '''\n    individual.statistics['generation'] = 0\n    individual.statistics['mutation_count'] = 0\n    individual.statistics['crossover_count'] = 0\n    individual.statistics['predecessor'] = 'ROOT',", "output": "Initializes the stats dict for individual\n    The statistics initialized are:\n        'generation': generation in which the individual was evaluated. Initialized as: 0\n        'mutation_count': number of mutation operations applied to the individual and its predecessor cumulatively. Initialized as: 0\n        'crossover_count': number of crossover operations applied to the individual and its predecessor cumulatively. Initialized as: 0\n        'predecessor': string representation of the individual. Initialized as: ('ROOT',)\n\n    Parameters\n    ----------\n    individual: deap individual\n\n    Returns\n    -------\n    object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self):\n        \"\"\"\n        \"\"\"\n        if self.num is None:\n            if self.num_inst == 0:\n                return (self.name, float('nan'))\n            else:\n                return (self.name, self.sum_metric / self.num_inst)\n        else:\n            names = ['%s'%(self.name[i]) for i in range(self.num)]\n            values = [x / y if y != 0 else float('nan') \\\n                for x, y in zip(self.sum_metric, self.num_inst)]\n            return (names, values)", "output": "Get the current evaluation result.\n        Override the default behavior\n\n        Returns\n        -------\n        name : str\n           Name of the metric.\n        value : float\n           Value of the evaluation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_item_cache(self, item):\n        \"\"\"\"\"\"\n        cache = self._item_cache\n        res = cache.get(item)\n        if res is None:\n            values = self._data.get(item)\n            res = self._box_item_values(item, values)\n            cache[item] = res\n            res._set_as_cached(item, self)\n\n            # for a chain\n            res._is_copy = self._is_copy\n        return res", "output": "Return the cached item, item represents a label indexer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def array_to_base64_png(array):\n  \"\"\"\n  \"\"\"\n  # TODO(cais): Deal with 3D case.\n  # TODO(cais): If there are None values in here, replace them with all NaNs.\n  array = np.array(array, dtype=np.float32)\n  if len(array.shape) != 2:\n    raise ValueError(\n        \"Expected rank-2 array; received rank-%d array.\" % len(array.shape))\n  if not np.size(array):\n    raise ValueError(\n        \"Cannot encode an empty array (size: %s) as image.\" % (array.shape,))\n\n  is_infinity = np.isinf(array)\n  is_positive = array > 0.0\n  is_positive_infinity = np.logical_and(is_infinity, is_positive)\n  is_negative_infinity = np.logical_and(is_infinity,\n                                        np.logical_not(is_positive))\n  is_nan = np.isnan(array)\n  finite_indices = np.where(np.logical_and(np.logical_not(is_infinity),\n                                           np.logical_not(is_nan)))\n  if np.size(finite_indices):\n    # Finite subset is not empty.\n    minval = np.min(array[finite_indices])\n    maxval = np.max(array[finite_indices])\n    scaled = np.array((array - minval) / (maxval - minval) * 255,\n                      dtype=np.uint8)\n    rgb = np.repeat(np.expand_dims(scaled, -1), IMAGE_COLOR_CHANNELS, axis=-1)\n  else:\n    rgb = np.zeros(array.shape + (IMAGE_COLOR_CHANNELS,), dtype=np.uint8)\n\n  # Color-code pixels that correspond to infinities and nans.\n  rgb[is_positive_infinity] = POSITIVE_INFINITY_RGB\n  rgb[is_negative_infinity] = NEGATIVE_INFINITY_RGB\n  rgb[is_nan] = NAN_RGB\n\n  image_encoded = base64.b64encode(encoder.encode_png(rgb))\n  return image_encoded", "output": "Convert an array into base64-enoded PNG image.\n\n  Args:\n    array: A 2D np.ndarray or nested list of items.\n\n  Returns:\n    A base64-encoded string the image. The image is grayscale if the array is\n    2D. The image is RGB color if the image is 3D with lsat dimension equal to\n    3.\n\n  Raises:\n    ValueError: If the input `array` is not rank-2, or if the rank-2 `array` is\n      empty.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_remote(self, config, name):\n        \"\"\"\n        \n        \"\"\"\n        from dvc.remote import Remote\n\n        remote = config.get(name)\n\n        if not remote:\n            return None\n\n        settings = self.repo.config.get_remote_settings(remote)\n        return Remote(self.repo, settings)", "output": "The config file is stored in a way that allows you to have a\n        cache for each remote.\n\n        This is needed when specifying external outputs\n        (as they require you to have an external cache location).\n\n        Imagine a config file like the following:\n\n                ['remote \"dvc-storage\"']\n                url = ssh://localhost/tmp\n                ask_password = true\n\n                [cache]\n                ssh = dvc-storage\n\n        This method resolves the name under the cache section into the\n        correct Remote instance.\n\n        Args:\n            config (dict): The cache section on the config file\n            name (str): Name of the section we are interested in to retrieve\n\n        Returns:\n            remote (dvc.Remote): Remote instance that the section is referring.\n                None when there's no remote with that name.\n\n        Example:\n            >>> _get_remote(config={'ssh': 'dvc-storage'}, name='ssh')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_value(self, index, col, value, takeable=False):\n        \"\"\"\n        \n        \"\"\"\n        warnings.warn(\"set_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._set_value(index, col, value, takeable=takeable)", "output": "Put single value at passed column and index.\n\n        .. deprecated:: 0.21.0\n            Use .at[] or .iat[] accessors instead.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        value : scalar\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        DataFrame\n            If label pair is contained, will be reference to calling DataFrame,\n            otherwise a new object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dispatch(self, receiver):\n        ''' \n\n        '''\n        super(ColumnsPatchedEvent, self).dispatch(receiver)\n        if hasattr(receiver, '_columns_patched'):\n            receiver._columns_patched(self)", "output": "Dispatch handling of this event to a receiver.\n\n        This method will invoke ``receiver._columns_patched`` if it exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_one_var_from_string(name, param_type, checks):\n    \"\"\"\n    \"\"\"\n    ret = \"\"\n    univar_mapper = {\"int\": \"GetInt\", \"double\": \"GetDouble\", \"bool\": \"GetBool\", \"std::string\": \"GetString\"}\n    if \"vector\" not in param_type:\n        ret += \"  %s(params, \\\"%s\\\", &%s);\\n\" % (univar_mapper[param_type], name, name)\n        if len(checks) > 0:\n            for check in checks:\n                ret += \"  CHECK(%s %s);\\n\" % (name, check)\n        ret += \"\\n\"\n    else:\n        ret += \"  if (GetString(params, \\\"%s\\\", &tmp_str)) {\\n\" % (name)\n        type2 = param_type.split(\"<\")[1][:-1]\n        if type2 == \"std::string\":\n            ret += \"    %s = Common::Split(tmp_str.c_str(), ',');\\n\" % (name)\n        else:\n            ret += \"    %s = Common::StringToArray<%s>(tmp_str, ',');\\n\" % (name, type2)\n        ret += \"  }\\n\\n\"\n    return ret", "output": "Construct code for auto config file for one param value.\n\n    Parameters\n    ----------\n    name : string\n        Name of the parameter.\n    param_type : string\n        Type of the parameter.\n    checks : list\n        Constraints of the parameter.\n\n    Returns\n    -------\n    ret : string\n        Lines of auto config file with getting and checks of one parameter value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalize(self, form):\n        \"\"\"\n        \n        \"\"\"\n        import unicodedata\n        f = lambda x: unicodedata.normalize(form, x)\n        result = _na_map(f, self._parent)\n        return self._wrap_result(result)", "output": "Return the Unicode normal form for the strings in the Series/Index.\n        For more information on the forms, see the\n        :func:`unicodedata.normalize`.\n\n        Parameters\n        ----------\n        form : {'NFC', 'NFKC', 'NFD', 'NFKD'}\n            Unicode form\n\n        Returns\n        -------\n        normalized : Series/Index of objects", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def leaveEvent(self, event):\n        \"\"\"\"\"\"\n        super(ToolTipWidget, self).leaveEvent(event)\n        self.hide()", "output": "Override Qt method to hide the tooltip on leave.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def complete_transaction(cleanup_only=False, recursive=False, max_attempts=3):\n    '''\n    \n    '''\n\n    return _complete_transaction(cleanup_only, recursive, max_attempts, 1, [])", "output": ".. versionadded:: Fluorine\n\n    Execute ``yum-complete-transaction``, which is provided by the ``yum-utils`` package.\n\n    cleanup_only\n        Specify if the ``--cleanup-only`` option should be supplied.\n\n    recursive\n        Specify if ``yum-complete-transaction`` should be called recursively\n        (it only completes one transaction at a time).\n\n    max_attempts\n        If ``recursive`` is ``True``, the maximum times ``yum-complete-transaction`` should be called.\n\n    .. note::\n\n        Recursive calls will stop once ``No unfinished transactions left.`` is in the returned output.\n\n    .. note::\n\n        ``yum-utils`` will already be installed on the minion if the package\n        was installed from the Fedora / EPEL repositories.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.complete_transaction\n        salt '*' pkg.complete_transaction cleanup_only=True\n        salt '*' pkg.complete_transaction recursive=True max_attempts=5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def discard(self, min_freq, unknown_token):\n        \"\"\"\n        \"\"\"\n        freq = 0\n        ret = Counter({})\n        for token, count in self.items():\n            if count < min_freq:\n                freq += count\n            else:\n                ret[token] = count\n        ret[unknown_token] = ret.get(unknown_token, 0) + freq\n        return ret", "output": "Discards tokens with frequency below min_frequency and represents them\n        as `unknown_token`.\n\n        Parameters\n        ----------\n        min_freq: int\n            Tokens whose frequency is under min_freq is counted as `unknown_token` in\n            the Counter returned.\n        unknown_token: str\n            The representation for any unknown token.\n\n        Returns\n        -------\n        The Counter instance.\n\n        Examples\n        --------\n        >>> a = gluonnlp.data.Counter({'a': 10, 'b': 1, 'c': 1})\n        >>> a.discard(3, '<unk>')\n        Counter({'a': 10, '<unk>': 2})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _concat_categorical(to_concat, axis=0):\n    \"\"\"\n    \"\"\"\n\n    # we could have object blocks and categoricals here\n    # if we only have a single categoricals then combine everything\n    # else its a non-compat categorical\n    categoricals = [x for x in to_concat if is_categorical_dtype(x.dtype)]\n\n    # validate the categories\n    if len(categoricals) != len(to_concat):\n        pass\n    else:\n        # when all categories are identical\n        first = to_concat[0]\n        if all(first.is_dtype_equal(other) for other in to_concat[1:]):\n            return union_categoricals(categoricals)\n\n    # extract the categoricals & coerce to object if needed\n    to_concat = [x.get_values() if is_categorical_dtype(x.dtype)\n                 else np.asarray(x).ravel() if not is_datetime64tz_dtype(x)\n                 else np.asarray(x.astype(object)) for x in to_concat]\n    result = _concat_compat(to_concat)\n    if axis == 1:\n        result = result.reshape(1, len(result))\n    return result", "output": "Concatenate an object/categorical array of arrays, each of which is a\n    single dtype\n\n    Parameters\n    ----------\n    to_concat : array of arrays\n    axis : int\n        Axis to provide concatenation in the current implementation this is\n        always 0, e.g. we only have 1D categoricals\n\n    Returns\n    -------\n    Categorical\n        A single array, preserving the combined dtypes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_option_group(name, engine_name, major_engine_version,\n                        option_group_description, tags=None, region=None,\n                        key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    res = __salt__['boto_rds.option_group_exists'](name, tags, region, key, keyid,\n                                                   profile)\n    if res.get('exists'):\n        return {'exists': bool(res)}\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if not conn:\n            return {'results': bool(conn)}\n\n        taglist = _tag_doc(tags)\n        rds = conn.create_option_group(OptionGroupName=name,\n                                       EngineName=engine_name,\n                                       MajorEngineVersion=major_engine_version,\n                                       OptionGroupDescription=option_group_description,\n                                       Tags=taglist)\n\n        return {'exists': bool(rds)}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Create an RDS option group\n\n    CLI example to create an RDS option group::\n\n        salt myminion boto_rds.create_option_group my-opt-group mysql 5.6 \\\n                \"group description\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def import_usr_dir(usr_dir):\n  \"\"\"\"\"\"\n  if not usr_dir:\n    return\n  if usr_dir == INTERNAL_USR_DIR_PACKAGE:\n    # The package has been installed with pip under this name for Cloud ML\n    # Engine so just import it.\n    importlib.import_module(INTERNAL_USR_DIR_PACKAGE)\n    return\n\n  dir_path = os.path.abspath(os.path.expanduser(usr_dir).rstrip(\"/\"))\n  containing_dir, module_name = os.path.split(dir_path)\n  tf.logging.info(\"Importing user module %s from path %s\", module_name,\n                  containing_dir)\n  sys.path.insert(0, containing_dir)\n  importlib.import_module(module_name)\n  sys.path.pop(0)", "output": "Import module at usr_dir, if provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add(self, cell):\n        \"\"\"\n        \"\"\"\n        self._cells.append(cell)\n        if self._override_cell_params:\n            assert cell._own_params, \\\n                \"Either specify params for SequentialRNNCell \" \\\n                \"or child cells, not both.\"\n            cell.params._params.update(self.params._params)\n        self.params._params.update(cell.params._params)", "output": "Append a cell into the stack.\n\n        Parameters\n        ----------\n        cell : BaseRNNCell\n            The cell to be appended. During unroll, previous cell's output (or raw inputs if\n            no previous cell) is used as the input to this cell.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prop(self, name):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlGetProp(self._o, name)\n        return ret", "output": "Search and get the value of an attribute associated to a\n          node This does the entity substitution. This function looks\n          in DTD attribute declaration for #FIXED or default\n          declaration values unless DTD use has been turned off.\n          NOTE: this function acts independently of namespaces\n          associated to the attribute. Use xmlGetNsProp() or\n           xmlGetNoNsProp() for namespace aware processing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(s):\n        \"\"\"\n        \n        \"\"\"\n        start = s.find('[')\n        if start == -1:\n            raise ValueError(\"Array should start with '['.\")\n        end = s.find(']')\n        if end == -1:\n            raise ValueError(\"Array should end with ']'.\")\n        s = s[start + 1: end]\n\n        try:\n            values = [float(val) for val in s.split(',') if val]\n        except ValueError:\n            raise ValueError(\"Unable to parse values from %s\" % s)\n        return DenseVector(values)", "output": "Parse string representation back into the DenseVector.\n\n        >>> DenseVector.parse(' [ 0.0,1.0,2.0,  3.0]')\n        DenseVector([0.0, 1.0, 2.0, 3.0])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_measure(measure):\r\n        \"\"\"\"\"\"\r\n        # Convert to a positive value.\r\n        measure = abs(measure)\r\n\r\n        # For number of calls\r\n        if isinstance(measure, int):\r\n            return to_text_string(measure)\r\n\r\n        # For time measurements\r\n        if 1.e-9 < measure <= 1.e-6:\r\n            measure = u\"{0:.2f} ns\".format(measure / 1.e-9)\r\n        elif 1.e-6 < measure <= 1.e-3:\r\n            measure = u\"{0:.2f} us\".format(measure / 1.e-6)\r\n        elif 1.e-3 < measure <= 1:\r\n            measure = u\"{0:.2f} ms\".format(measure / 1.e-3)\r\n        elif 1 < measure <= 60:\r\n            measure = u\"{0:.2f} sec\".format(measure)\r\n        elif 60 < measure <= 3600:\r\n            m, s = divmod(measure, 3600)\r\n            if s > 60:\r\n                m, s = divmod(measure, 60)\r\n                s = to_text_string(s).split(\".\")[-1]\r\n            measure = u\"{0:.0f}.{1:.2s} min\".format(m, s)\r\n        else:\r\n            h, m = divmod(measure, 3600)\r\n            if m > 60:\r\n                m /= 60\r\n            measure = u\"{0:.0f}h:{1:.0f}min\".format(h, m)\r\n        return measure", "output": "Get format and units for data coming from profiler task.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_diagrams(results, configs, compiler, out_dir):\n    \"\"\"\"\"\"\n    compiler_fn = make_filename(compiler)\n    total = psutil.virtual_memory().total  # pylint:disable=I0011,E1101\n    memory = int(math.ceil(byte_to_gb(total)))\n\n    images_dir = os.path.join(out_dir, 'images')\n\n    for config in configs:\n        out_prefix = '{0}_{1}'.format(config['name'], compiler_fn)\n\n        plot_diagram(\n            config,\n            results,\n            images_dir,\n            os.path.join(images_dir, '{0}.png'.format(out_prefix))\n        )\n\n        with open(\n            os.path.join(out_dir, '{0}.qbk'.format(out_prefix)),\n            'wb'\n        ) as out_f:\n            qbk_content = \"\"\"{0}\nMeasured on a {2} host with {3} GB memory. Compiler used: {4}.\n\n[$images/metaparse/{1}.png [width 100%]]\n\"\"\".format(config['desc'], out_prefix, platform.platform(), memory, compiler)\n            out_f.write(qbk_content)", "output": "Plot all diagrams specified by the configs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def skipgram_batch(centers, contexts, num_tokens, dtype, index_dtype):\n    \"\"\"\"\"\"\n    contexts = mx.nd.array(contexts[2], dtype=index_dtype)\n    indptr = mx.nd.arange(len(centers) + 1)\n    centers = mx.nd.array(centers, dtype=index_dtype)\n    centers_csr = mx.nd.sparse.csr_matrix(\n        (mx.nd.ones(centers.shape), centers, indptr), dtype=dtype,\n        shape=(len(centers), num_tokens))\n    return centers_csr, contexts, centers", "output": "Create a batch for SG training objective.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def annotation_spec_path(cls, project, location, dataset, annotation_spec):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/datasets/{dataset}/annotationSpecs/{annotation_spec}\",\n            project=project,\n            location=location,\n            dataset=dataset,\n            annotation_spec=annotation_spec,\n        )", "output": "Return a fully-qualified annotation_spec string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalize_layout(layout, min_percentile=1, max_percentile=99, relative_margin=0.1):\n    \"\"\"\"\"\"\n\n    # compute percentiles\n    mins = np.percentile(layout, min_percentile, axis=(0))\n    maxs = np.percentile(layout, max_percentile, axis=(0))\n\n    # add margins\n    mins -= relative_margin * (maxs - mins)\n    maxs += relative_margin * (maxs - mins)\n\n    # `clip` broadcasts, `[None]`s added only for readability\n    clipped = np.clip(layout, mins, maxs)\n\n    # embed within [0,1] along both axes\n    clipped -= clipped.min(axis=0)\n    clipped /= clipped.max(axis=0)\n\n    return clipped", "output": "Removes outliers and scales layout to between [0,1].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def reinvoke(self, *, call_hooks=False, restart=True):\n        \"\"\"\n        \"\"\"\n        cmd = self.command\n        view = self.view\n        if cmd is None:\n            raise ValueError('This context is not valid.')\n\n        # some state to revert to when we're done\n        index, previous = view.index, view.previous\n        invoked_with = self.invoked_with\n        invoked_subcommand = self.invoked_subcommand\n        subcommand_passed = self.subcommand_passed\n\n        if restart:\n            to_call = cmd.root_parent or cmd\n            view.index = len(self.prefix)\n            view.previous = 0\n            view.get_word() # advance to get the root command\n        else:\n            to_call = cmd\n\n        try:\n            await to_call.reinvoke(self, call_hooks=call_hooks)\n        finally:\n            self.command = cmd\n            view.index = index\n            view.previous = previous\n            self.invoked_with = invoked_with\n            self.invoked_subcommand = invoked_subcommand\n            self.subcommand_passed = subcommand_passed", "output": "|coro|\n\n        Calls the command again.\n\n        This is similar to :meth:`~.Context.invoke` except that it bypasses\n        checks, cooldowns, and error handlers.\n\n        .. note::\n\n            If you want to bypass :exc:`.UserInputError` derived exceptions,\n            it is recommended to use the regular :meth:`~.Context.invoke`\n            as it will work more naturally. After all, this will end up\n            using the old arguments the user has used and will thus just\n            fail again.\n\n        Parameters\n        ------------\n        call_hooks: :class:`bool`\n            Whether to call the before and after invoke hooks.\n        restart: :class:`bool`\n            Whether to start the call chain from the very beginning\n            or where we left off (i.e. the command that caused the error).\n            The default is to start where we left off.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_code(self, code):\n        \"\"\"\n        \n        \"\"\"\n\n        def _select_code(code):\n            return self.data.loc[(slice(None), code), :]\n\n        try:\n            return self.new(_select_code(code), self.type, self.if_fq)\n        except:\n            raise ValueError('QA CANNOT FIND THIS CODE {}'.format(code))", "output": "\u9009\u62e9\u80a1\u7968\n\n        @2018/06/03 pandas \u7684\u7d22\u5f15\u95ee\u9898\u5bfc\u81f4\n        https://github.com/pandas-dev/pandas/issues/21299\n\n        \u56e0\u6b64\u5148\u7528set_index\u53bb\u91cd\u505a\u4e00\u6b21index\n        \u5f71\u54cd\u7684\u6709selects,select_time,select_month,get_bar\n\n        @2018/06/04\n        \u5f53\u9009\u62e9\u7684\u65f6\u95f4\u8d8a\u754c/\u80a1\u7968\u4e0d\u5b58\u5728,raise ValueError\n\n        @2018/06/04 pandas\u7d22\u5f15\u95ee\u9898\u5df2\u7ecf\u89e3\u51b3\n        \u5168\u90e8\u6062\u590d", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_host(name, data, **api_opts):\n    '''\n    \n    '''\n    o = get_host(name=name, **api_opts)\n    return update_object(objref=o['_ref'], data=data, **api_opts)", "output": "Update host record. This is a helper call to update_object.\n\n    Find a hosts ``_ref`` then call update_object with the record data.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call infoblox.update_host name=fqdn data={}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sources(zone, permanent=True):\n    '''\n    \n    '''\n    cmd = '--zone={0} --list-sources'.format(zone)\n\n    if permanent:\n        cmd += ' --permanent'\n\n    return __firewall_cmd(cmd).split()", "output": "List sources bound to a zone\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.get_sources zone", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(**kwargs):\n    '''\n    \n    '''\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    id_ = kwargs.pop('id', 0)\n    if kwargs:\n        salt.utils.args.invalid_kwargs(kwargs)\n\n    conn = __proxy__['junos.conn']()\n    ret = {}\n    ret['out'] = True\n    try:\n        ret['message'] = conn.cu.diff(rb_id=id_)\n    except Exception as exception:\n        ret['message'] = 'Could not get diff with error \"{0}\"'.format(\n            exception)\n        ret['out'] = False\n\n    return ret", "output": "Returns the difference between the candidate and the current configuration\n\n    id : 0\n        The rollback ID value (0-49)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'device_name' junos.diff 3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_panel_ids(dashboard):\n    ''''''\n    panel_id = 1\n    for row in dashboard.get('rows', []):\n        for panel in row.get('panels', []):\n            panel['id'] = panel_id\n            panel_id += 1", "output": "Assign panels auto-incrementing IDs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def container_stop(name, timeout=30, force=True, remote_addr=None,\n                   cert=None, key=None, verify_cert=True):\n    '''\n    \n    '''\n    container = container_get(\n        name, remote_addr, cert, key, verify_cert, _raw=True\n    )\n    container.stop(timeout, force, wait=True)\n    return _pylxd_model_to_dict(container)", "output": "Stop a container\n\n    name :\n        Name of the container to stop\n\n    remote_addr :\n        An URL to a remote Server, you also have to give cert and key if\n        you provide remote_addr and its a TCP Address!\n\n        Examples:\n            https://myserver.lan:8443\n            /var/lib/mysocket.sock\n\n    cert :\n        PEM Formatted SSL Certificate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Wherever to verify the cert, this is by default True\n        but in the most cases you want to set it off as LXD\n        normaly uses self-signed certificates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self):\n        \"\"\"\n        \"\"\"\n        api = self._client.instance_admin_api\n        metadata = _metadata_with_prefix(self.name)\n\n        api.delete_instance(self.name, metadata=metadata)", "output": "Mark an instance and all of its databases for permanent deletion.\n\n        See\n        https://cloud.google.com/spanner/reference/rpc/google.spanner.admin.instance.v1#google.spanner.admin.instance.v1.InstanceAdmin.DeleteInstance\n\n        Immediately upon completion of the request:\n\n        * Billing will cease for all of the instance's reserved resources.\n\n        Soon afterward:\n\n        * The instance and all databases within the instance will be deleteed.\n          All data in the databases will be permanently deleted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def independentlinear60(display=False):\n    \"\"\" \n    \"\"\"\n\n    # set a constant seed\n    old_seed = np.random.seed()\n    np.random.seed(0)\n\n    # generate dataset with known correlation\n    N = 1000\n    M = 60\n\n    # set one coefficent from each group of 3 to 1\n    beta = np.zeros(M)\n    beta[0:30:3] = 1\n    f = lambda X: np.matmul(X, beta)\n\n    # Make sure the sample correlation is a perfect match\n    X_start = np.random.randn(N, M)\n    X = X_start - X_start.mean(0)\n    y = f(X) + np.random.randn(N) * 1e-2\n\n    # restore the previous numpy random seed\n    np.random.seed(old_seed)\n\n    return pd.DataFrame(X), y", "output": "A simulated dataset with tight correlations among distinct groups of features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_and_install_ruby(ret, ruby, default=False, user=None, opts=None, env=None):\n    '''\n    \n    '''\n    ret = _check_ruby(ret, ruby, user=user)\n    if not ret['result']:\n        if __salt__['rvm.install_ruby'](ruby, runas=user, opts=opts, env=env):\n            ret['result'] = True\n            ret['changes'][ruby] = 'Installed'\n            ret['comment'] = 'Successfully installed ruby.'\n            ret['default'] = False\n        else:\n            ret['result'] = False\n            ret['comment'] = 'Could not install ruby.'\n            return ret\n\n    if default:\n        __salt__['rvm.set_default'](ruby, runas=user)\n\n    return ret", "output": "Verify that ruby is installed, install if unavailable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AllFieldsFromDescriptor(self, message_descriptor):\n    \"\"\"\"\"\"\n    self.Clear()\n    for field in message_descriptor.fields:\n      self.paths.append(field.name)", "output": "Gets all direct fields of Message Descriptor to FieldMask.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_unique_prompt(self):\n        '''\n        '''\n\n        self.sendline(\"unset PROMPT_COMMAND\")\n        self.sendline(self.PROMPT_SET_SH) # sh-style\n        i = self.expect ([TIMEOUT, self.PROMPT], timeout=10)\n        if i == 0: # csh-style\n            self.sendline(self.PROMPT_SET_CSH)\n            i = self.expect([TIMEOUT, self.PROMPT], timeout=10)\n            if i == 0:\n                return False\n        return True", "output": "This sets the remote prompt to something more unique than ``#`` or ``$``.\n        This makes it easier for the :meth:`prompt` method to match the shell prompt\n        unambiguously. This method is called automatically by the :meth:`login`\n        method, but you may want to call it manually if you somehow reset the\n        shell prompt. For example, if you 'su' to a different user then you\n        will need to manually reset the prompt. This sends shell commands to\n        the remote host to set the prompt, so this assumes the remote host is\n        ready to receive commands.\n\n        Alternatively, you may use your own prompt pattern. In this case you\n        should call :meth:`login` with ``auto_prompt_reset=False``; then set the\n        :attr:`PROMPT` attribute to a regular expression. After that, the\n        :meth:`prompt` method will try to match your prompt pattern.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_field(self, *, name, value, inline=True):\n        \"\"\"\n        \"\"\"\n\n        field = {\n            'inline': inline,\n            'name': str(name),\n            'value': str(value)\n        }\n\n        try:\n            self._fields.append(field)\n        except AttributeError:\n            self._fields = [field]\n\n        return self", "output": "Adds a field to the embed object.\n\n        This function returns the class instance to allow for fluent-style\n        chaining.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The name of the field.\n        value: :class:`str`\n            The value of the field.\n        inline: :class:`bool`\n            Whether the field should be displayed inline.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def block(bdaddr):\n    '''\n    \n    '''\n    if not salt.utils.validate.net.mac(bdaddr):\n        raise CommandExecutionError(\n            'Invalid BD address passed to bluetooth.block'\n        )\n\n    cmd = 'hciconfig {0} block'.format(bdaddr)\n    __salt__['cmd.run'](cmd).splitlines()", "output": "Block a specific bluetooth device by BD Address\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluetooth.block DE:AD:BE:EF:CA:FE", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _save_impl(self, pickler):\n        \"\"\"\n        \n        \"\"\"\n        pickler.dump( (self.__proxy__.state, self._exclude, self._features) )", "output": "Save the model as a directory, which can be loaded with the\n        :py:func:`~turicreate.load_model` method.\n\n        Parameters\n        ----------\n        pickler : GLPickler\n            An opened GLPickle archive (Do not close the archive).\n\n        See Also\n        --------\n        turicreate.load_model\n\n        Examples\n        --------\n        >>> model.save('my_model_file')\n        >>> loaded_model = turicreate.load_model('my_model_file')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bbox_overlaps(boxes, query_boxes):\n    \"\"\"\n    \n    \"\"\"\n    n_ = boxes.shape[0]\n    k_ = query_boxes.shape[0]\n    overlaps = np.zeros((n_, k_), dtype=np.float)\n    for k in range(k_):\n        query_box_area = (query_boxes[k, 2] - query_boxes[k, 0] + 1) * (query_boxes[k, 3] - query_boxes[k, 1] + 1)\n        for n in range(n_):\n            iw = min(boxes[n, 2], query_boxes[k, 2]) - max(boxes[n, 0], query_boxes[k, 0]) + 1\n            if iw > 0:\n                ih = min(boxes[n, 3], query_boxes[k, 3]) - max(boxes[n, 1], query_boxes[k, 1]) + 1\n                if ih > 0:\n                    box_area = (boxes[n, 2] - boxes[n, 0] + 1) * (boxes[n, 3] - boxes[n, 1] + 1)\n                    all_area = float(box_area + query_box_area - iw * ih)\n                    overlaps[n, k] = iw * ih / all_area\n    return overlaps", "output": "determine overlaps between boxes and query_boxes\n    :param boxes: n * 4 bounding boxes\n    :param query_boxes: k * 4 bounding boxes\n    :return: overlaps: n * k overlaps", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_stock_list(engine, client=DATABASE):\n    \"\"\"\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_stock_list(client=client)", "output": "save stock_list\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _interactive_input_tensor_to_features_dict(feature_map, hparams):\n  \"\"\"\n  \"\"\"\n  inputs = tf.convert_to_tensor(feature_map[\"inputs\"])\n  input_is_image = False if len(inputs.get_shape()) < 3 else True\n\n  x = inputs\n  if input_is_image:\n    x = tf.image.resize_images(x, [299, 299])\n    x = tf.reshape(x, [1, 299, 299, -1])\n    x = tf.to_int32(x)\n  else:\n    # Remove the batch dimension.\n    num_samples = x[0]\n    length = x[2]\n    x = tf.slice(x, [3], tf.to_int32([length]))\n    x = tf.reshape(x, [1, -1, 1, 1])\n    # Transform into a batch of size num_samples to get that many random\n    # decodes.\n    x = tf.tile(x, tf.to_int32([num_samples, 1, 1, 1]))\n\n  p_hparams = hparams.problem_hparams\n  input_space_id = tf.constant(p_hparams.input_space_id)\n  target_space_id = tf.constant(p_hparams.target_space_id)\n\n  features = {}\n  features[\"input_space_id\"] = input_space_id\n  features[\"target_space_id\"] = target_space_id\n  features[\"decode_length\"] = (\n      IMAGE_DECODE_LENGTH if input_is_image else inputs[1])\n  features[\"inputs\"] = x\n  return features", "output": "Convert the interactive input format (see above) to a dictionary.\n\n  Args:\n    feature_map: dict with inputs.\n    hparams: model hyperparameters\n\n  Returns:\n    a features dictionary, as expected by the decoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _call_yum(args, **kwargs):\n    '''\n    \n    '''\n    params = {'output_loglevel': 'trace',\n              'python_shell': False,\n              'env': salt.utils.environment.get_module_environment(globals())}\n    params.update(kwargs)\n    cmd = []\n    if salt.utils.systemd.has_scope(__context__) and __salt__['config.get']('systemd.scope', True):\n        cmd.extend(['systemd-run', '--scope'])\n    cmd.append(_yum())\n    cmd.extend(args)\n\n    return __salt__['cmd.run_all'](cmd, **params)", "output": "Call yum/dnf.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_log_path(config_file_name):\n    ''''''\n    stdout_full_path = os.path.join(NNICTL_HOME_DIR, config_file_name, 'stdout')\n    stderr_full_path = os.path.join(NNICTL_HOME_DIR, config_file_name, 'stderr')\n    return stdout_full_path, stderr_full_path", "output": "generate stdout and stderr log path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_group(cls, group, lines, dist=None):\n        \"\"\"\"\"\"\n        if not MODULE(group):\n            raise ValueError(\"Invalid group name\", group)\n        this = {}\n        for line in yield_lines(lines):\n            ep = cls.parse(line, dist)\n            if ep.name in this:\n                raise ValueError(\"Duplicate entry point\", group, ep.name)\n            this[ep.name] = ep\n        return this", "output": "Parse an entry point group", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _login_dockerhub():\n    \"\"\"\n    \n    \"\"\"\n    dockerhub_credentials = _get_dockerhub_credentials()\n\n    logging.info('Logging in to DockerHub')\n    # We use password-stdin instead of --password to avoid leaking passwords in case of an error.\n    # This method will produce the following output:\n    # > WARNING! Your password will be stored unencrypted in /home/jenkins_slave/.docker/config.json.\n    # > Configure a credential helper to remove this warning. See\n    # > https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n    # Since we consider the restricted slaves a secure environment, that's fine. Also, using this will require\n    # third party applications which would need a review first as well.\n    p = subprocess.run(['docker', 'login', '--username', dockerhub_credentials['username'], '--password-stdin'],\n                       stdout=subprocess.PIPE, input=str.encode(dockerhub_credentials['password']))\n    logging.info(p.stdout)\n    logging.info('Successfully logged in to DockerHub')", "output": "Login to the Docker Hub account\n    :return: None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_n_args(self, args, example, n):\n        \"\"\"\n        \"\"\"\n        if len(args) != n:\n            msg = (\n                'Got unexpected number of arguments, expected {}. '\n                '(example: \"{} config {}\")'\n            ).format(n, get_prog(), example)\n            raise PipError(msg)\n\n        if n == 1:\n            return args[0]\n        else:\n            return args", "output": "Helper to make sure the command got the right number of arguments", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pipe(self, name):\n        \"\"\"\n        \"\"\"\n        for pipe_name, component in self.pipeline:\n            if pipe_name == name:\n                return component\n        raise KeyError(Errors.E001.format(name=name, opts=self.pipe_names))", "output": "Get a pipeline component for a given component name.\n\n        name (unicode): Name of pipeline component to get.\n        RETURNS (callable): The pipeline component.\n\n        DOCS: https://spacy.io/api/language#get_pipe", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main():\r\n    \"\"\"\"\"\"\r\n    from spyder.utils.qthelpers import qapplication\r\n    app = qapplication()\r\n    if os.name == 'nt':\r\n        dialog = WinUserEnvDialog()\r\n    else:\r\n        dialog = EnvDialog()\r\n    dialog.show()\r\n    app.exec_()", "output": "Run Windows environment variable editor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readFile(filename, encoding, options):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlReadFile(filename, encoding, options)\n    if ret is None:raise treeError('xmlReadFile() failed')\n    return xmlDoc(_obj=ret)", "output": "parse an XML file from the filesystem or the network.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_new(self, rev):\n        # type: (str) -> RevOptions\n        \"\"\"\n        \n        \"\"\"\n        return self.vcs.make_rev_options(rev, extra_args=self.extra_args)", "output": "Make a copy of the current instance, but with a new rev.\n\n        Args:\n          rev: the name of the revision for the new object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dirname(self, index):\r\n        \"\"\"\"\"\"\r\n        fname = self.get_filename(index)\r\n        if fname:\r\n            if osp.isdir(fname):\r\n                return fname\r\n            else:\r\n                return osp.dirname(fname)", "output": "Return dirname associated with *index*", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit(self, data):\n        \"\"\"\n        \n        \"\"\"\n\n        _raise_error_if_not_sframe(data, \"data\")\n\n        fitted_state = {}\n        feature_columns = _internal_utils.get_column_names(data, self._exclude, self._features)\n\n        if not feature_columns:\n            raise RuntimeError(\"No valid feature columns specified in transformation.\")\n\n        fitted_state['features'] = feature_columns\n        fitted_state['fitted'] = True\n\n        self.__proxy__.update(fitted_state)\n\n        return self", "output": "Fits the transformer using the given data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def last_archive(self):\n        '''\n        \n        '''\n        archives = {}\n        for archive in self.archives():\n            archives[int(archive.split('.')[0].split('-')[-1])] = archive\n\n        return archives and archives[max(archives)] or None", "output": "Get the last available archive\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cnn_model(logits=False, input_ph=None, img_rows=28, img_cols=28,\n              channels=1, nb_filters=64, nb_classes=10):\n  \"\"\"\n  \n  \"\"\"\n  model = Sequential()\n\n  # Define the layers successively (convolution layers are version dependent)\n  if tf.keras.backend.image_data_format() == 'channels_first':\n    input_shape = (channels, img_rows, img_cols)\n  else:\n    assert tf.keras.backend.image_data_format() == 'channels_last'\n    input_shape = (img_rows, img_cols, channels)\n\n  layers = [conv_2d(nb_filters, (8, 8), (2, 2), \"same\",\n                    input_shape=input_shape),\n            Activation('relu'),\n            conv_2d((nb_filters * 2), (6, 6), (2, 2), \"valid\"),\n            Activation('relu'),\n            conv_2d((nb_filters * 2), (5, 5), (1, 1), \"valid\"),\n            Activation('relu'),\n            Flatten(),\n            Dense(nb_classes)]\n\n  for layer in layers:\n    model.add(layer)\n\n  if logits:\n    logits_tensor = model(input_ph)\n  model.add(Activation('softmax'))\n\n  if logits:\n    return model, logits_tensor\n  else:\n    return model", "output": "Defines a CNN model using Keras sequential model\n  :param logits: If set to False, returns a Keras model, otherwise will also\n                  return logits tensor\n  :param input_ph: The TensorFlow tensor for the input\n                  (needed if returning logits)\n                  (\"ph\" stands for placeholder but it need not actually be a\n                  placeholder)\n  :param img_rows: number of row in the image\n  :param img_cols: number of columns in the image\n  :param channels: number of color channels (e.g., 1 for MNIST)\n  :param nb_filters: number of convolutional filters per layer\n  :param nb_classes: the number of output classes\n  :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(name):\n    '''\n    \n    '''\n    info = __salt__['user.info'](name=name)\n\n    ret = {'name': name,\n           'passwd': '',\n           'lstchg': '',\n           'min': '',\n           'max': '',\n           'warn': '',\n           'inact': '',\n           'expire': ''}\n\n    if info:\n        ret = {'name': info['name'],\n               'passwd': 'Unavailable',\n               'lstchg': info['password_changed'],\n               'min': '',\n               'max': '',\n               'warn': '',\n               'inact': '',\n               'expire': info['expiration_date']}\n\n    return ret", "output": "Return information for the specified user\n    This is just returns dummy data so that salt states can work.\n\n    :param str name: The name of the user account to show.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.info root", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_image_summary(name, val):\n    \"\"\"\n    \n    \"\"\"\n    assert isinstance(name, six.string_types), type(name)\n    n, h, w, c = val.shape\n    val = val.astype('uint8')\n    s = tf.Summary()\n    imparams = [cv2.IMWRITE_PNG_COMPRESSION, 9]\n    for k in range(n):\n        arr = val[k]\n        # CV2 will only write correctly in BGR chanel order\n        if c == 3:\n            arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)\n        elif c == 4:\n            arr = cv2.cvtColor(arr, cv2.COLOR_RGBA2BGRA)\n        tag = name if n == 1 else '{}/{}'.format(name, k)\n        retval, img_str = cv2.imencode('.png', arr, imparams)\n        if not retval:\n            # Encoding has failed.\n            continue\n        img_str = img_str.tostring()\n\n        img = tf.Summary.Image()\n        img.height = h\n        img.width = w\n        # 1 - grayscale 3 - RGB 4 - RGBA\n        img.colorspace = c\n        img.encoded_image_string = img_str\n        s.value.add(tag=tag, image=img)\n    return s", "output": "Args:\n        name(str):\n        val(np.ndarray): 4D tensor of NHWC. assume RGB if C==3.\n            Can be either float or uint8. Range has to be [0,255].\n\n    Returns:\n        tf.Summary:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mimedata2url(source, extlist=None):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    pathlist = []\r\n    if source.hasUrls():\r\n        for url in source.urls():\r\n            path = _process_mime_path(to_text_string(url.toString()), extlist)\r\n            if path is not None:\r\n                pathlist.append(path)\r\n    elif source.hasText():\r\n        for rawpath in to_text_string(source.text()).splitlines():\r\n            path = _process_mime_path(rawpath, extlist)\r\n            if path is not None:\r\n                pathlist.append(path)\r\n    if pathlist:\r\n        return pathlist", "output": "Extract url list from MIME data\r\n    extlist: for example ('.py', '.pyw')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_no_duplicates(self, name, domain=None, path=None):\n        \"\"\"\n        \"\"\"\n        toReturn = None\n        for cookie in iter(self):\n            if cookie.name == name:\n                if domain is None or cookie.domain == domain:\n                    if path is None or cookie.path == path:\n                        if toReturn is not None:  # if there are multiple cookies that meet passed in criteria\n                            raise CookieConflictError('There are multiple cookies with name, %r' % (name))\n                        toReturn = cookie.value  # we will eventually return this as long as no cookie conflict\n\n        if toReturn:\n            return toReturn\n        raise KeyError('name=%r, domain=%r, path=%r' % (name, domain, path))", "output": "Both ``__get_item__`` and ``get`` call this function: it's never\n        used elsewhere in Requests.\n\n        :param name: a string containing name of cookie\n        :param domain: (optional) string containing domain of cookie\n        :param path: (optional) string containing path of cookie\n        :raises KeyError: if cookie is not found\n        :raises CookieConflictError: if there are multiple cookies\n            that match name and optionally domain and path\n        :return: cookie.value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ipv4_to_bits(ipaddr):\n    '''\n    \n    '''\n    return ''.join([bin(int(x))[2:].rjust(8, '0') for x in ipaddr.split('.')])", "output": "Accepts an IPv4 dotted quad and returns a string representing its binary\n    counterpart", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        '''\n        \n        '''\n        if self.opts['chunked']:\n            ret = self.run_chunked()\n        else:\n            ret = self.run_oldstyle()\n\n        salt.output.display_output(\n                ret,\n                self.opts.get('output', 'nested'),\n                self.opts)", "output": "Make the salt client call", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sls_exists(mods, test=None, queue=False, **kwargs):\n    '''\n    \n    '''\n    return isinstance(\n        show_sls(mods, test=test, queue=queue, **kwargs),\n        dict\n    )", "output": "Tests for the existance the of a specific SLS or list of SLS files on the\n    master. Similar to :py:func:`state.show_sls <salt.modules.state.show_sls>`,\n    rather than returning state details, returns True or False. The default\n    environment is ``base``, use ``saltenv`` to specify a different environment.\n\n    .. versionadded:: 2019.2.0\n\n    saltenv\n        Specify a salt fileserver environment from which to look for the SLS files\n        specified in the ``mods`` argument\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.sls_exists core,edit.vim saltenv=dev", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_summary(self, summary):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(summary, six.binary_type):\n            summary = tf.Summary.FromString(summary)\n        assert isinstance(summary, tf.Summary), type(summary)\n\n        # TODO other types\n        for val in summary.value:\n            if val.WhichOneof('value') == 'simple_value':\n                val.tag = re.sub('tower[0-9]+/', '', val.tag)   # TODO move to subclasses\n\n                # TODO This hack is still needed, seem to disappear only when\n                # compiled from source.\n                suffix = '-summary'  # tensorflow#6150, tensorboard#59\n                if val.tag.endswith(suffix):\n                    val.tag = val.tag[:-len(suffix)]\n\n                self._dispatch(lambda m: m.process_scalar(val.tag, val.simple_value))\n\n        self._dispatch(lambda m: m.process_summary(summary))", "output": "Put a `tf.Summary`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def service_get(service_name=None, service_rootid=None, **kwargs):\n    '''\n    \n    '''\n\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'service.get'\n            if not service_name:\n                return {'result': False, 'comment': 'service_name param is required'}\n            params = {'output': ['name']}\n            if service_rootid:\n                params['parentids'] = service_rootid\n            params['filter'] = {'name': service_name}\n            params = _params_extend(params, _ignore_name=True, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result'] if ret['result'] else False\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: Fluorine\n\n    Get service according to name and parent service ID.\n\n    .. note::\n        https://www.zabbix.com/documentation/3.4/manual/api/reference/service/get\n\n    :param service_name: Name of the service\n    :param service_rootid: ID of service parent\n\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: Service details, False if no service found or on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.service_get 'My service' 11", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def market_sell(self, security, amount, ttype=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        self._switch_left_menus([\"\u5e02\u4ef7\u59d4\u6258\", \"\u5356\u51fa\"])\n\n        return self.market_trade(security, amount, ttype)", "output": "\u5e02\u4ef7\u5356\u51fa\n        :param security: \u516d\u4f4d\u8bc1\u5238\u4ee3\u7801\n        :param amount: \u4ea4\u6613\u6570\u91cf\n        :param ttype: \u5e02\u4ef7\u59d4\u6258\u7c7b\u578b\uff0c\u9ed8\u8ba4\u5ba2\u6237\u7aef\u9ed8\u8ba4\u9009\u62e9\uff0c\n                     \u6df1\u5e02\u53ef\u9009 ['\u5bf9\u624b\u65b9\u6700\u4f18\u4ef7\u683c', '\u672c\u65b9\u6700\u4f18\u4ef7\u683c', '\u5373\u65f6\u6210\u4ea4\u5269\u4f59\u64a4\u9500', '\u6700\u4f18\u4e94\u6863\u5373\u65f6\u6210\u4ea4\u5269\u4f59 '\u5168\u989d\u6210\u4ea4\u6216\u64a4\u9500']\n                     \u6caa\u5e02\u53ef\u9009 ['\u6700\u4f18\u4e94\u6863\u6210\u4ea4\u5269\u4f59\u64a4\u9500', '\u6700\u4f18\u4e94\u6863\u6210\u4ea4\u5269\u4f59\u8f6c\u9650\u4ef7']\n\n        :return: {'entrust_no': '\u59d4\u6258\u5355\u53f7'}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_singlekey(self):\r\n        \"\"\"\"\"\"\r\n        if len(self._qsequences) == 0:\r\n            return True\r\n        else:\r\n            keystr = self._qsequences[0]\r\n            valid_single_keys = (EDITOR_SINGLE_KEYS if\r\n                                 self.context == 'editor' else SINGLE_KEYS)\r\n            if any((m in keystr for m in ('Ctrl', 'Alt', 'Shift', 'Meta'))):\r\n                return True\r\n            else:\r\n                # This means that the the first subsequence is composed of\r\n                # a single key with no modifier.\r\n                valid_single_keys = (EDITOR_SINGLE_KEYS if\r\n                                     self.context == 'editor' else SINGLE_KEYS)\r\n                if any((k == keystr for k in valid_single_keys)):\r\n                    return True\r\n                else:\r\n                    return False", "output": "Check if the first sub-sequence of the new key sequence is valid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _shares_exec_prefix(basedir):\n    ''' \n\n    '''\n    import sys\n    prefix = sys.exec_prefix\n    return (prefix is not None and basedir.startswith(prefix))", "output": "Whether a give base directory is on the system exex prefix", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(name, **params):\n    '''\n\n    '''\n    if check_exists(name):\n        msg = 'Trying to create check that already exists : {0}'.format(name)\n        log.error(msg)\n        raise CommandExecutionError(msg)\n    application_url = _get_application_url()\n    log.debug('[uptime] trying PUT request')\n    params.update(url=name)\n    req = requests.put('{0}/api/checks'.format(application_url), data=params)\n    if not req.ok:\n        raise CommandExecutionError(\n            'request to uptime failed : {0}'.format(req.reason)\n        )\n    log.debug('[uptime] PUT request successful')\n    return req.json()['_id']", "output": "Create a check on a given URL.\n\n    Additional parameters can be used and are passed to API (for\n    example interval, maxTime, etc). See the documentation\n    https://github.com/fzaninotto/uptime for a full list of the\n    parameters.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' uptime.create http://example.org", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tenant_get(tenant_id=None, name=None, profile=None,\n               **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    ret = {}\n\n    if name:\n        for tenant in getattr(kstone, _TENANTS, None).list():\n            if tenant.name == name:\n                tenant_id = tenant.id\n                break\n    if not tenant_id:\n        return {'Error': 'Unable to resolve tenant id'}\n    tenant = getattr(kstone, _TENANTS, None).get(tenant_id)\n    ret[tenant.name] = dict((value, getattr(tenant, value)) for value in dir(tenant)\n                            if not value.startswith('_') and\n                            isinstance(getattr(tenant, value), (six.string_types, dict, bool)))\n    return ret", "output": "Return a specific tenants (keystone tenant-get)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.tenant_get c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.tenant_get tenant_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.tenant_get name=nova", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self, series, exponent=None):\n        '''\n        \n        '''\n        try:\n            return self.calculateHurst(series, exponent)\n        except Exception as e:\n            print(\"   Error: %s\" % e)", "output": ":type series: List\n        :type exponent: int\n        :rtype: float", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancel_requests(self):\n        \"\"\"\"\"\"\n        self._timer.stop()\n        self._job = None\n        self._args = None\n        self._kwargs = None", "output": "Cancels pending requests.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def NamedTemporaryFile(\n    mode=\"w+b\",\n    buffering=-1,\n    encoding=None,\n    newline=None,\n    suffix=None,\n    prefix=None,\n    dir=None,\n    delete=True,\n    wrapper_class_override=None,\n):\n    \"\"\"\n    \"\"\"\n    prefix, suffix, dir, output_type = _sanitize_params(prefix, suffix, dir)\n    flags = _bin_openflags\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if not wrapper_class_override:\n        wrapper_class_override = _TemporaryFileWrapper\n    if os.name == \"nt\" and delete:\n        flags |= os.O_TEMPORARY\n    if sys.version_info < (3, 5):\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n    else:\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags, output_type)\n    try:\n        file = io.open(fd, mode, buffering=buffering, newline=newline, encoding=encoding)\n        if wrapper_class_override is not None:\n            return type(str(\"_TempFileWrapper\"), (wrapper_class_override, object), {})(\n                file, name, delete\n            )\n        else:\n            return _TemporaryFileWrapper(file, name, delete)\n\n    except BaseException:\n        os.unlink(name)\n        os.close(fd)\n        raise", "output": "Create and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to io.open (default \"w+b\").\n    'buffering' -- the buffer size argument to io.open (default -1).\n    'encoding' -- the encoding argument to io.open (default None)\n    'newline' -- the newline argument to io.open (default None)\n    'delete' -- whether the file is deleted on close (default True).\n    The file is created as mkstemp() would do it.\n    Returns an object with a file-like interface; the name of the file\n    is accessible as its 'name' attribute.  The file will be automatically\n    deleted when it is closed unless the 'delete' argument is set to False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_create_cfn_template_request(self, app_id, semver, key, logical_id):\n        \"\"\"\n        \n        \"\"\"\n        create_cfn_template = (lambda app_id, semver: self._sar_client.create_cloud_formation_template(\n            ApplicationId=self._sanitize_sar_str_param(app_id),\n            SemanticVersion=self._sanitize_sar_str_param(semver)\n        ))\n        response = self._sar_service_call(create_cfn_template, logical_id, app_id, semver)\n        self._applications[key] = response[self.TEMPLATE_URL_KEY]\n        if response['Status'] != \"ACTIVE\":\n            self._in_progress_templates.append((response[self.APPLICATION_ID_KEY], response['TemplateId']))", "output": "Method that handles the create_cloud_formation_template API call to the serverless application repo\n\n        :param string app_id: ApplicationId\n        :param string semver: SemanticVersion\n        :param string key: The dictionary key consisting of (ApplicationId, SemanticVersion)\n        :param string logical_id: the logical_id of this application resource", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check(peers):\n\n    ''''''\n\n    if not isinstance(peers, list):\n        return False\n\n    for peer in peers:\n        if not isinstance(peer, six.string_types):\n            return False\n\n    if not HAS_NETADDR:  # if does not have this lib installed, will simply try to load what user specified\n        # if the addresses are not correctly specified, will trow error when loading the actual config\n        return True\n\n    ip_only_peers = []\n    for peer in peers:\n        try:\n            ip_only_peers.append(six.text_type(IPAddress(peer)))  # append the str value\n        except AddrFormatError:\n            # if not a valid IP Address\n            # will try to see if it is a nameserver and resolve it\n            if not HAS_DNSRESOLVER:\n                continue  # without the dns resolver cannot populate the list of NTP entities based on their nameserver\n                # so we'll move on\n            dns_reply = []\n            try:\n                # try to see if it is a valid NS\n                dns_reply = dns.resolver.query(peer)\n            except dns.resolver.NoAnswer:\n                # no a valid DNS entry either\n                return False\n            for dns_ip in dns_reply:\n                ip_only_peers.append(six.text_type(dns_ip))\n\n    peers = ip_only_peers\n\n    return True", "output": "Checks whether the input is a valid list of peers and transforms domain names into IP Addresses", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_params(cls, params: Iterable[Tuple[str, Params]] = ()) -> Optional['RegularizerApplicator']:\n        \"\"\"\n        \n        \"\"\"\n        if not params:\n            return None\n\n        instantiated_regularizers = []\n        for parameter_regex, regularizer_params in params:\n            if isinstance(regularizer_params, str):\n                regularizer = Regularizer.by_name(regularizer_params)()\n            else:\n                regularizer_type = Regularizer.by_name(regularizer_params.pop(\"type\"))\n                regularizer = regularizer_type(**regularizer_params)  # type: ignore\n            instantiated_regularizers.append((parameter_regex, regularizer))\n        return RegularizerApplicator(instantiated_regularizers)", "output": "Converts a List of pairs (regex, params) into an RegularizerApplicator.\n        This list should look like\n\n        [[\"regex1\", {\"type\": \"l2\", \"alpha\": 0.01}], [\"regex2\", \"l1\"]]\n\n        where each parameter receives the penalty corresponding to the first regex\n        that matches its name (which may be no regex and hence no penalty).\n        The values can either be strings, in which case they correspond to the names\n        of regularizers, or dictionaries, in which case they must contain the \"type\"\n        key, corresponding to the name of a regularizer. In addition, they may contain\n        auxiliary named parameters which will be fed to the regularizer itself.\n        To determine valid auxiliary parameters, please refer to the torch.nn.init documentation.\n\n        Parameters\n        ----------\n        params : ``Params``, required.\n            A Params object containing a \"regularizers\" key.\n\n        Returns\n        -------\n        A RegularizerApplicator containing the specified Regularizers,\n        or ``None`` if no Regularizers are specified.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_line_at(self, coordinates):\r\n        \"\"\"\"\"\"\r\n        cursor = self.cursorForPosition(coordinates)\r\n        cursor.select(QTextCursor.BlockUnderCursor)\r\n        return to_text_string(cursor.selectedText()).replace(u'\\u2029', '')", "output": "Return line at *coordinates* (QPoint)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def push(self, item, priority=None):\n        \"\"\"\n        \"\"\"\n        priority = item if priority is None else priority\n        node = PriorityQueueNode(item, priority)\n        for index, current in enumerate(self.priority_queue_list):\n            if current.priority < node.priority:\n                self.priority_queue_list.insert(index, node)\n                return\n        # when traversed complete queue\n        self.priority_queue_list.append(node)", "output": "Push the item in the priority queue.\n        if priority is not given, priority is set to the value of item.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _xmlTextReaderErrorFunc(xxx_todo_changeme,msg,severity,locator):\n    \"\"\"\"\"\"\n    (f,arg) = xxx_todo_changeme\n    return f(arg,msg,severity,xmlTextReaderLocator(locator))", "output": "Intermediate callback to wrap the locator", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_plain_text(self, checked):\r\n        \"\"\"\"\"\"\r\n        if checked:\r\n            self.docstring = checked\r\n            self.switch_to_plain_text()\r\n            self.force_refresh()\r\n        self.set_option('rich_mode', not checked)", "output": "Toggle plain text docstring", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_shape_str(tensors):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(tensors, (list, tuple)):\n        for v in tensors:\n            assert isinstance(v, (tf.Tensor, tf.Variable)), \"Not a tensor: {}\".format(type(v))\n        shape_str = \",\".join(\n            map(lambda x: str(x.get_shape().as_list()), tensors))\n    else:\n        assert isinstance(tensors, (tf.Tensor, tf.Variable)), \"Not a tensor: {}\".format(type(tensors))\n        shape_str = str(tensors.get_shape().as_list())\n    return shape_str", "output": "Internally used by layer registry, to print shapes of inputs/outputs of layers.\n\n    Args:\n        tensors (list or tf.Tensor): a tensor or a list of tensors\n    Returns:\n        str: a string to describe the shape", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_overwrites_to_context(context, overwrite_context):\n    \"\"\"\"\"\"\n    for variable, overwrite in overwrite_context.items():\n        if variable not in context:\n            # Do not include variables which are not used in the template\n            continue\n\n        context_value = context[variable]\n\n        if isinstance(context_value, list):\n            # We are dealing with a choice variable\n            if overwrite in context_value:\n                # This overwrite is actually valid for the given context\n                # Let's set it as default (by definition first item in list)\n                # see ``cookiecutter.prompt.prompt_choice_for_config``\n                context_value.remove(overwrite)\n                context_value.insert(0, overwrite)\n        else:\n            # Simply overwrite the value for this variable\n            context[variable] = overwrite", "output": "Modify the given context in place based on the overwrite_context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sconnect(host=None, port=None, password=None):\n    '''\n    \n    '''\n    if host is None:\n        host = __salt__['config.option']('redis_sentinel.host', 'localhost')\n    if port is None:\n        port = __salt__['config.option']('redis_sentinel.port', 26379)\n    if password is None:\n        password = __salt__['config.option']('redis_sentinel.password')\n\n    return redis.StrictRedis(host, port, password=password, decode_responses=True)", "output": "Returns an instance of the redis client", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_features(self, data):\n        \"\"\"\n        \n        \"\"\"\n        if self.feature_names is None:\n            self.feature_names = data.feature_names\n            self.feature_types = data.feature_types\n        else:\n            # Booster can't accept data with different feature names\n            if self.feature_names != data.feature_names:\n                dat_missing = set(self.feature_names) - set(data.feature_names)\n                my_missing = set(data.feature_names) - set(self.feature_names)\n\n                msg = 'feature_names mismatch: {0} {1}'\n\n                if dat_missing:\n                    msg += ('\\nexpected ' + ', '.join(str(s) for s in dat_missing) +\n                            ' in input data')\n\n                if my_missing:\n                    msg += ('\\ntraining data did not have the following fields: ' +\n                            ', '.join(str(s) for s in my_missing))\n\n                raise ValueError(msg.format(self.feature_names,\n                                            data.feature_names))", "output": "Validate Booster and data's feature_names are identical.\n        Set feature_names and feature_types from DMatrix", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_remote_connection_headers(cls, parsed_url, keep_alive=False):\n        \"\"\"\n        \n        \"\"\"\n\n        system = platform.system().lower()\n        if system == \"darwin\":\n            system = \"mac\"\n\n        headers = {\n            'Accept': 'application/json',\n            'Content-Type': 'application/json;charset=UTF-8',\n            'User-Agent': 'selenium/{} (python {})'.format(__version__, system)\n        }\n\n        if parsed_url.username:\n            base64string = base64.b64encode('{0.username}:{0.password}'.format(parsed_url).encode())\n            headers.update({\n                'Authorization': 'Basic {}'.format(base64string.decode())\n            })\n\n        if keep_alive:\n            headers.update({\n                'Connection': 'keep-alive'\n            })\n\n        return headers", "output": "Get headers for remote request.\n\n        :Args:\n         - parsed_url - The parsed url\n         - keep_alive (Boolean) - Is this a keep-alive connection (default: False)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_lambda_error_response(lambda_response):\n        \"\"\"\n        \n        \"\"\"\n        is_lambda_user_error_response = False\n        try:\n            lambda_response_dict = json.loads(lambda_response)\n\n            # This is a best effort attempt to determine if the output (lambda_response) from the container was an\n            # Error/Exception that was raised/returned/thrown from the container. To ensure minimal false positives in\n            # this checking, we check for all three keys that can occur in Lambda raised/thrown/returned an\n            # Error/Exception. This still risks false positives when the data returned matches exactly a dictionary with\n            # the keys 'errorMessage', 'errorType' and 'stackTrace'.\n            if isinstance(lambda_response_dict, dict) and \\\n                    len(lambda_response_dict) == 3 and \\\n                    'errorMessage' in lambda_response_dict and \\\n                    'errorType' in lambda_response_dict and \\\n                    'stackTrace' in lambda_response_dict:\n                is_lambda_user_error_response = True\n        except ValueError:\n            # If you can't serialize the output into a dict, then do nothing\n            pass\n        return is_lambda_user_error_response", "output": "Check to see if the output from the container is in the form of an Error/Exception from the Lambda invoke\n\n        Parameters\n        ----------\n        lambda_response str\n            The response the container returned\n\n        Returns\n        -------\n        bool\n            True if the output matches the Error/Exception Dictionary otherwise False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unicode_dict(_dict):\n    \"\"\"\n    \n    \"\"\"\n    r = {}\n    for k, v in iteritems(_dict):\n        r[unicode_obj(k)] = unicode_obj(v)\n    return r", "output": "Make sure keys and values of dict is unicode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_head(self, path: str, handler: _WebHandler,\n                 **kwargs: Any) -> AbstractRoute:\n        \"\"\"\n        \n        \"\"\"\n        return self.add_route(hdrs.METH_HEAD, path, handler, **kwargs)", "output": "Shortcut for add_route with method HEAD", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def activation_atlas(\n    model,\n    layer,\n    grid_size=10,\n    icon_size=96,\n    number_activations=NUMBER_OF_AVAILABLE_SAMPLES,\n    icon_batch_size=32,\n    verbose=False,\n):\n    \"\"\"\"\"\"\n\n    activations = layer.activations[:number_activations, ...]\n    layout, = aligned_umap(activations, verbose=verbose)\n    directions, coordinates, _ = bin_laid_out_activations(\n        layout, activations, grid_size\n    )\n    icons = []\n    for directions_batch in chunked(directions, icon_batch_size):\n        icon_batch, losses = render_icons(\n            directions_batch, model, layer=layer.name, size=icon_size, num_attempts=1\n        )\n        icons += icon_batch\n    canvas = make_canvas(icons, coordinates, grid_size)\n\n    return canvas", "output": "Renders an Activation Atlas of the given model's layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_key(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        log.error(\n            'The list_keys function must be called with -f or --function.'\n        )\n        return False\n\n    if not kwargs:\n        kwargs = {}\n\n    if 'keyname' not in kwargs:\n        log.error('A keyname is required.')\n        return False\n\n    rcode, data = query(\n        command='my/keys/{0}'.format(kwargs['keyname']),\n        method='GET',\n    )\n    return {'keys': {data['name']: data['key']}}", "output": "List the keys available", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_absolute_tool_path(command):\n    \"\"\"\n        \n    \"\"\"\n    assert isinstance(command, basestring)\n    if os.path.dirname(command):\n        return os.path.dirname(command)\n    else:\n        programs = path.programs_path()\n        m = path.glob(programs, [command, command + '.exe' ])\n        if not len(m):\n            if __debug_configuration:\n                print \"Could not find:\", command, \"in\", programs\n            return None\n        return os.path.dirname(m[0])", "output": "Given an invocation command,\n        return the absolute path to the command. This works even if commnad\n        has not path element and is present in PATH.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def auth_gssapi_with_mic(self, username, gss_host, gss_deleg_creds):\n        \"\"\"\n        \n        \"\"\"\n        if (not self.active) or (not self.initial_kex_done):\n            # we should never try to authenticate unless we're on a secure link\n            raise SSHException(\"No existing session\")\n        my_event = threading.Event()\n        self.auth_handler = AuthHandler(self)\n        self.auth_handler.auth_gssapi_with_mic(\n            username, gss_host, gss_deleg_creds, my_event\n        )\n        return self.auth_handler.wait_for_response(my_event)", "output": "Authenticate to the Server using GSS-API / SSPI.\n\n        :param str username: The username to authenticate as\n        :param str gss_host: The target host\n        :param bool gss_deleg_creds: Delegate credentials or not\n        :return: list of auth types permissible for the next stage of\n                 authentication (normally empty)\n        :raises: `.BadAuthenticationType` -- if gssapi-with-mic isn't\n            allowed by the server (and no event was passed in)\n        :raises:\n            `.AuthenticationException` -- if the authentication failed (and no\n            event was passed in)\n        :raises: `.SSHException` -- if there was a network error", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_config():\n    '''\n    \n    '''\n    cmd = 'Get-DscConfiguration | Select-Object * -ExcludeProperty Cim*'\n\n    try:\n        raw_config = _pshell(cmd, ignore_retcode=True)\n    except CommandExecutionError as exc:\n        if 'Current configuration does not exist' in exc.info['stderr']:\n            raise CommandExecutionError('Not Configured')\n        raise\n\n    config = dict()\n    if raw_config:\n        # Get DSC Configuration Name\n        if 'ConfigurationName' in raw_config[0]:\n            config[raw_config[0]['ConfigurationName']] = {}\n        # Add all DSC Configurations by ResourceId\n        for item in raw_config:\n            config[item['ConfigurationName']][item['ResourceId']] = {}\n            for key in item:\n                if key not in ['ConfigurationName', 'ResourceId']:\n                    config[item['ConfigurationName']][item['ResourceId']][key] = item[key]\n\n    return config", "output": "Get the current DSC Configuration\n\n    Returns:\n        dict: A dictionary representing the DSC Configuration on the machine\n\n    Raises:\n        CommandExecutionError: On failure\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dsc.get_config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status_raw(name=None, user=None, conf_file=None, bin_env=None):\n    '''\n    \n    '''\n    ret = __salt__['cmd.run_all'](\n        _ctl_cmd('status', name, conf_file, bin_env),\n        runas=user,\n        python_shell=False,\n    )\n    return _get_return(ret)", "output": "Display the raw output of status\n\n    user\n        user to run supervisorctl as\n    conf_file\n        path to supervisord config file\n    bin_env\n        path to supervisorctl bin or path to virtualenv with supervisor\n        installed\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' supervisord.status_raw", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def namePush(ctxt, value):\n    \"\"\" \"\"\"\n    if ctxt is None: ctxt__o = None\n    else: ctxt__o = ctxt._o\n    ret = libxml2mod.namePush(ctxt__o, value)\n    return ret", "output": "Pushes a new element name on top of the name stack", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aggregate_single_gradient(grad_and_vars, use_mean, check_inf_nan):\n    \"\"\"\n  \"\"\"\n    grads = [g for g, _ in grad_and_vars]\n    grad = tf.add_n(grads)\n\n    if use_mean and len(grads) > 1:\n        grad = tf.multiply(grad, 1.0 / len(grads))\n\n    v = grad_and_vars[0][1]\n    if check_inf_nan:\n        has_nan_or_inf = tf.logical_not(tf.reduce_all(tf.is_finite(grads)))\n        return (grad, v), has_nan_or_inf\n    else:\n        return (grad, v), None", "output": "Calculate the average gradient for a shared variable across all towers.\n\n  Note that this function provides a synchronization point across all towers.\n\n  Args:\n    grad_and_vars: A list or tuple of (gradient, variable) tuples. Each\n      (gradient, variable) pair within the outer list represents the gradient\n      of the variable calculated for a single tower, and the number of pairs\n      equals the number of towers.\n    use_mean: if True, mean is taken, else sum of gradients is taken.\n    check_inf_nan: check grads for nans and infs.\n\n  Returns:\n    The tuple ([(average_gradient, variable),], has_nan_or_inf) where the\n      gradient has been averaged across all towers. The variable is chosen from\n      the first tower. The has_nan_or_inf indicates the grads has nan or inf.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(self):\n        \"\"\n        clear_output()\n        self.write_csv()\n        if self.empty() and self._skipped>0:\n            return display(f'No images to show :). {self._skipped} pairs were '\n                    f'skipped since at least one of the images was deleted by the user.')\n        elif self.empty():\n            return display('No images to show :)')\n        if self.batch_contains_deleted():\n            self.next_batch(None)\n            self._skipped += 1\n        else:\n            display(self.make_horizontal_box(self.get_widgets(self._duplicates)))\n            display(self.make_button_widget('Next Batch', handler=self.next_batch, style=\"primary\"))", "output": "Re-render Jupyter cell for batch of images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_dict_inputs(inputs, tensor_info_map):\n  \"\"\"\n  \"\"\"\n  dict_inputs = _prepare_dict_inputs(inputs, tensor_info_map)\n  return tensor_info.convert_dict_to_compatible_tensor(dict_inputs,\n                                                       tensor_info_map)", "output": "Converts from inputs into dict of input tensors.\n\n  This handles:\n    - putting inputs into a dict, per _prepare_dict_inputs(),\n    - converting all input values into tensors compatible with the\n      expected input tensor (dtype, shape).\n    - check sparse/non-sparse tensor types.\n\n  Args:\n    inputs: inputs fed to Module.__call__().\n    tensor_info_map: A map from string to `tensor_info.ParsedTensorInfo`\n      describing the signature inputs.\n\n  Returns:\n    A dict of tensors to feed to the signature instantiation.\n\n  Raises:\n    TypeError: If it fails to convert the input values into a dict of tensors\n      to feed to the signature instantiation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(vm, key='uuid'):\n    '''\n    \n    '''\n    ret = {}\n    if key not in ['uuid', 'alias', 'hostname']:\n        ret['Error'] = 'Key must be either uuid, alias or hostname'\n        return ret\n    vm = lookup('{0}={1}'.format(key, vm), one=True)\n    if 'Error' in vm:\n        return vm\n    # vmadm delete <uuid>\n    cmd = 'vmadm delete {0}'.format(vm)\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n        return ret\n    return True", "output": "Delete a vm\n\n    vm : string\n        vm to be deleted\n    key : string [uuid|alias|hostname]\n        value type of 'vm' parameter\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.delete 186da9ab-7392-4f55-91a5-b8f1fe770543\n        salt '*' vmadm.delete nacl key=alias", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_frame_ae():\n  \"\"\"\"\"\"\n  hparams = next_frame_basic_deterministic()\n  hparams.bottom[\"inputs\"] = modalities.video_bitwise_bottom\n  hparams.top[\"inputs\"] = modalities.video_top\n  hparams.hidden_size = 256\n  hparams.batch_size = 8\n  hparams.num_hidden_layers = 4\n  hparams.num_compress_steps = 4\n  hparams.dropout = 0.4\n  return hparams", "output": "Conv autoencoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call(method, *args, **kwargs):\n    '''\n    \n    '''\n    kwargs = clean_kwargs(**kwargs)\n    return getattr(pyeapi_device['connection'], method)(*args, **kwargs)", "output": "Calls an arbitrary pyeapi method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_authed(self, ptype, message):\n        \"\"\"\n        \n        \"\"\"\n        if (\n            not self.server_mode\n            or ptype <= HIGHEST_USERAUTH_MESSAGE_ID\n            or self.is_authenticated()\n        ):\n            return None\n        # WELP. We must be dealing with someone trying to do non-auth things\n        # without being authed. Tell them off, based on message class.\n        reply = Message()\n        # Global requests have no details, just failure.\n        if ptype == MSG_GLOBAL_REQUEST:\n            reply.add_byte(cMSG_REQUEST_FAILURE)\n        # Channel opens let us reject w/ a specific type + message.\n        elif ptype == MSG_CHANNEL_OPEN:\n            kind = message.get_text()  # noqa\n            chanid = message.get_int()\n            reply.add_byte(cMSG_CHANNEL_OPEN_FAILURE)\n            reply.add_int(chanid)\n            reply.add_int(OPEN_FAILED_ADMINISTRATIVELY_PROHIBITED)\n            reply.add_string(\"\")\n            reply.add_string(\"en\")\n        # NOTE: Post-open channel messages do not need checking; the above will\n        # reject attemps to open channels, meaning that even if a malicious\n        # user tries to send a MSG_CHANNEL_REQUEST, it will simply fall under\n        # the logic that handles unknown channel IDs (as the channel list will\n        # be empty.)\n        return reply", "output": "Checks message type against current auth state.\n\n        If server mode, and auth has not succeeded, and the message is of a\n        post-auth type (channel open or global request) an appropriate error\n        response Message is crafted and returned to caller for sending.\n\n        Otherwise (client mode, authed, or pre-auth message) returns None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalize_path(filename):\n    \"\"\"\"\"\"\n    return os.path.normcase(os.path.realpath(os.path.normpath(_cygwin_patch(filename))))", "output": "Normalize a file/dir name for comparison purposes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tsql_query(query, **kwargs):\n    '''\n    \n    '''\n    try:\n        cur = _get_connection(**kwargs).cursor()\n        cur.execute(query)\n        # Making sure the result is JSON serializable\n        return loads(_MssqlEncoder().encode({'resultset': cur.fetchall()}))['resultset']\n    except Exception as err:\n        # Trying to look like the output of cur.fetchall()\n        return (('Could not run the query', ), (six.text_type(err), ))", "output": "Run a SQL query and return query result as list of tuples, or a list of dictionaries if as_dict was passed, or an empty list if no data is available.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minion mssql.tsql_query 'SELECT @@version as version' as_dict=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetchmany(self, size=None):\n        \"\"\"\n        \"\"\"\n        if size is None:\n            size = self.arraysize\n\n        self._try_fetch(size=size)\n        rows = []\n\n        for row in self._query_data:\n            rows.append(row)\n            if len(rows) >= size:\n                break\n\n        return rows", "output": "Fetch multiple results from the last ``execute*()`` call.\n\n        .. note::\n            The size parameter is not used for the request/response size.\n            Set the ``arraysize`` attribute before calling ``execute()`` to\n            set the batch size.\n\n        :type size: int\n        :param size:\n            (Optional) Maximum number of rows to return. Defaults to the\n            ``arraysize`` property value.\n\n        :rtype: List[tuple]\n        :returns: A list of rows.\n        :raises: :class:`~google.cloud.bigquery.dbapi.InterfaceError`\n            if called before ``execute()``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batchnorm_2d(nf:int, norm_type:NormType=NormType.Batch):\n    \"\"\n    bn = nn.BatchNorm2d(nf)\n    with torch.no_grad():\n        bn.bias.fill_(1e-3)\n        bn.weight.fill_(0. if norm_type==NormType.BatchZero else 1.)\n    return bn", "output": "A batchnorm2d layer with `nf` features initialized depending on `norm_type`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_commit(self, transaction):\n        \"\"\"\n        \"\"\"\n        try:\n            transaction._commit()\n            return True\n        except exceptions.GoogleAPICallError as exc:\n            if transaction._read_only:\n                raise\n\n            if isinstance(exc, exceptions.Aborted):\n                # If a read-write transaction returns ABORTED, retry.\n                return False\n            else:\n                raise", "output": "Try to commit the transaction.\n\n        If the transaction is read-write and the ``Commit`` fails with the\n        ``ABORTED`` status code, it will be retried. Any other failure will\n        not be caught.\n\n        Args:\n            transaction (~.firestore_v1beta1.transaction.Transaction): The\n                transaction to be ``Commit``-ed.\n\n        Returns:\n            bool: Indicating if the commit succeeded.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernel_id(self):\r\n        \"\"\"\"\"\"\r\n        if self.connection_file is not None:\r\n            json_file = osp.basename(self.connection_file)\r\n            return json_file.split('.json')[0]", "output": "Get kernel id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, data: Union[bytes, memoryview]) -> \"Future[None]\":\n        \"\"\"\n\n        \"\"\"\n        self._check_closed()\n        if data:\n            if (\n                self.max_write_buffer_size is not None\n                and len(self._write_buffer) + len(data) > self.max_write_buffer_size\n            ):\n                raise StreamBufferFullError(\"Reached maximum write buffer size\")\n            self._write_buffer.append(data)\n            self._total_write_index += len(data)\n        future = Future()  # type: Future[None]\n        future.add_done_callback(lambda f: f.exception())\n        self._write_futures.append((self._total_write_index, future))\n        if not self._connecting:\n            self._handle_write()\n            if self._write_buffer:\n                self._add_io_state(self.io_loop.WRITE)\n            self._maybe_add_error_listener()\n        return future", "output": "Asynchronously write the given data to this stream.\n\n        This method returns a `.Future` that resolves (with a result\n        of ``None``) when the write has been completed.\n\n        The ``data`` argument may be of type `bytes` or `memoryview`.\n\n        .. versionchanged:: 4.0\n            Now returns a `.Future` if no callback is given.\n\n        .. versionchanged:: 4.5\n            Added support for `memoryview` arguments.\n\n        .. versionchanged:: 6.0\n\n           The ``callback`` argument was removed. Use the returned\n           `.Future` instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dict_of(validate_key, validate_item):\n    \"\"\"\n    \"\"\"\n    def validate(value, should_raise=True):\n        validate_type = is_type(dict)\n        if not validate_type(value, should_raise=should_raise):\n            return False\n\n        for key, item in value.items():\n            try:\n                validate_key(key)\n            except TypeError as e:\n                if should_raise:\n                    samtranslator.model.exceptions.prepend(e, \"dict contained an invalid key\")\n                    raise\n                return False\n\n            try:\n                validate_item(item)\n            except TypeError as e:\n                if should_raise:\n                    samtranslator.model.exceptions.prepend(e, \"dict contained an invalid value\")\n                    raise\n                return False\n        return True\n    return validate", "output": "Returns a validator function that succeeds only if the input is a dict, and each key and value in the dict passes\n    as input to the provided validators validate_key and validate_item, respectively.\n\n    :param callable validate_key: the validator function for keys in the dict\n    :param callable validate_item: the validator function for values in the list\n    :returns: a function which returns True its input is an dict of valid items, and raises TypeError otherwise\n    :rtype: callable", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_env(hparams,\n              batch_size,\n              max_num_noops,\n              rl_env_max_episode_steps=-1,\n              env_name=None):\n  \"\"\"\"\"\"\n  if not env_name:\n    env_name = full_game_name(hparams.game)\n\n  maxskip_envs = should_apply_max_and_skip_env(hparams)\n\n  env = T2TGymEnv(\n      base_env_name=env_name,\n      batch_size=batch_size,\n      grayscale=hparams.grayscale,\n      should_derive_observation_space=hparams\n      .rl_should_derive_observation_space,\n      resize_width_factor=hparams.resize_width_factor,\n      resize_height_factor=hparams.resize_height_factor,\n      rl_env_max_episode_steps=rl_env_max_episode_steps,\n      max_num_noops=max_num_noops,\n      maxskip_envs=maxskip_envs,\n      sticky_actions=hparams.sticky_actions\n  )\n  return env", "output": "Setup.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource):\n        \"\"\"\n        \"\"\"\n        etag = resource.get(\"etag\")\n\n        if etag is not None:\n            resource = resource.copy()\n            resource[\"etag\"] = base64.b64decode(etag.encode(\"ascii\"))\n\n        return super(Policy, cls).from_api_repr(resource)", "output": "Factory: create a policy from a JSON resource.\n\n        Overrides the base class version to store :attr:`etag` as bytes.\n\n        Args:\n            resource (dict): JSON policy resource returned by the\n            ``getIamPolicy`` REST API.\n\n        Returns:\n            :class:`Policy`: the parsed policy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extractfile(self, member):\n        \"\"\"\n        \"\"\"\n        self._check(\"r\")\n\n        if isinstance(member, str):\n            tarinfo = self.getmember(member)\n        else:\n            tarinfo = member\n\n        if tarinfo.isreg():\n            return self.fileobject(self, tarinfo)\n\n        elif tarinfo.type not in SUPPORTED_TYPES:\n            # If a member's type is unknown, it is treated as a\n            # regular file.\n            return self.fileobject(self, tarinfo)\n\n        elif tarinfo.islnk() or tarinfo.issym():\n            if isinstance(self.fileobj, _Stream):\n                # A small but ugly workaround for the case that someone tries\n                # to extract a (sym)link as a file-object from a non-seekable\n                # stream of tar blocks.\n                raise StreamError(\"cannot extract (sym)link as file object\")\n            else:\n                # A (sym)link's file object is its target's file object.\n                return self.extractfile(self._find_link_target(tarinfo))\n        else:\n            # If there's no data associated with the member (directory, chrdev,\n            # blkdev, etc.), return None instead of a file object.\n            return None", "output": "Extract a member from the archive as a file object. `member' may be\n           a filename or a TarInfo object. If `member' is a regular file, a\n           file-like object is returned. If `member' is a link, a file-like\n           object is constructed from the link's target. If `member' is none of\n           the above, None is returned.\n           The file-like object is read-only and provides the following\n           methods: read(), readline(), readlines(), seek() and tell()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download(progress=True):\n    ''' \n\n    '''\n    data_dir = external_data_dir(create=True)\n    print(\"Using data directory: %s\" % data_dir)\n\n    s3 = 'https://bokeh-sampledata.s3.amazonaws.com'\n    files = [\n        (s3, 'CGM.csv'),\n        (s3, 'US_Counties.zip'),\n        (s3, 'us_cities.json'),\n        (s3, 'unemployment09.csv'),\n        (s3, 'AAPL.csv'),\n        (s3, 'FB.csv'),\n        (s3, 'GOOG.csv'),\n        (s3, 'IBM.csv'),\n        (s3, 'MSFT.csv'),\n        (s3, 'WPP2012_SA_DB03_POPULATION_QUINQUENNIAL.zip'),\n        (s3, 'gapminder_fertility.csv'),\n        (s3, 'gapminder_population.csv'),\n        (s3, 'gapminder_life_expectancy.csv'),\n        (s3, 'gapminder_regions.csv'),\n        (s3, 'world_cities.zip'),\n        (s3, 'airports.json'),\n        (s3, 'movies.db.zip'),\n        (s3, 'airports.csv'),\n        (s3, 'routes.csv'),\n        (s3, 'haarcascade_frontalface_default.xml'),\n    ]\n\n    for base_url, filename in files:\n        _download_file(base_url, filename, data_dir, progress=progress)", "output": "Download larger data sets for various Bokeh examples.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_more(self, rows=False, columns=False):\r\n        \"\"\"\"\"\"\r\n        if  self.axis == 1 and self.total_rows > self.rows_loaded:\r\n            reminder = self.total_rows - self.rows_loaded\r\n            items_to_fetch = min(reminder, ROWS_TO_LOAD)\r\n            self.beginInsertRows(QModelIndex(), self.rows_loaded,\r\n                                 self.rows_loaded + items_to_fetch - 1)\r\n            self.rows_loaded += items_to_fetch\r\n            self.endInsertRows()\r\n        if self.axis == 0 and self.total_cols > self.cols_loaded:\r\n            reminder = self.total_cols - self.cols_loaded\r\n            items_to_fetch = min(reminder, COLS_TO_LOAD)\r\n            self.beginInsertColumns(QModelIndex(), self.cols_loaded,\r\n                                    self.cols_loaded + items_to_fetch - 1)\r\n            self.cols_loaded += items_to_fetch\r\n            self.endInsertColumns()", "output": "Get more columns or rows (based on axis).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_trivial_worker(self, state):\n        \"\"\"\n        \n        \"\"\"\n        if self.assistant:\n            return False\n        return all(not task.resources for task in self.get_tasks(state, PENDING))", "output": "If it's not an assistant having only tasks that are without\n        requirements.\n\n        We have to pass the state parameter for optimization reasons.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_figure_tofile(fig, fmt, fname):\n    \"\"\"\"\"\"\n    root, ext = osp.splitext(fname)\n    if ext == '.png' and fmt == 'image/svg+xml':\n        qimg = svg_to_image(fig)\n        qimg.save(fname)\n    else:\n        if fmt == 'image/svg+xml' and is_unicode(fig):\n            fig = fig.encode('utf-8')\n\n        with open(fname, 'wb') as f:\n            f.write(fig)", "output": "Save fig to fname in the format specified by fmt.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_exception_type(self, exc):\n        \"\"\"\n        \"\"\"\n        old_length = len(self._valid_exception)\n        self._valid_exception = tuple(x for x in self._valid_exception if x is not exc)\n        return len(self._valid_exception) != old_length", "output": "Removes an exception type from being handled during the reconnect logic.\n\n        Parameters\n        ------------\n        exc: Type[:class:`BaseException`]\n            The exception class to handle.\n\n        Returns\n        ---------\n        :class:`bool`\n            Whether it was successfully removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _fmadm_action_fmri(action, fmri):\n    '''\n    \n    '''\n    ret = {}\n    fmadm = _check_fmadm()\n    cmd = '{cmd} {action} {fmri}'.format(\n        cmd=fmadm,\n        action=action,\n        fmri=fmri\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    result = {}\n    if retcode != 0:\n        result['Error'] = res['stderr']\n    else:\n        result = True\n\n    return result", "output": "Internal function for fmadm.repqired, fmadm.replaced, fmadm.flush", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate_classifier_with_probabilities(model, data,\n                                           probabilities='probabilities',\n                                           verbose = False):\n    \"\"\"\n    \n    \"\"\"\n\n    model = _get_model(model)\n    if verbose:\n        print(\"\")\n        print(\"Other Framework\\t\\tPredicted\")\n\n    max_probability_error, num_key_mismatch = 0, 0\n\n    for _,row in data.iterrows():\n        predicted_values = model.predict(dict(row))[_to_unicode(probabilities)]\n        other_values = row[probabilities]\n\n        if set(predicted_values.keys()) != set(other_values.keys()):\n            if verbose:\n                print(\"Different classes: \", str(predicted_values.keys()), str(other_values.keys()))\n            num_key_mismatch += 1\n            continue\n\n        for cur_class, cur_predicted_class_values in predicted_values.items():\n            delta = cur_predicted_class_values - other_values[cur_class]\n            if verbose:\n                print(delta, cur_predicted_class_values, other_values[cur_class])\n\n            max_probability_error = max(abs(delta), max_probability_error)\n\n        if verbose:\n            print(\"\")\n\n    ret = {\n        \"num_samples\": len(data),\n        \"max_probability_error\": max_probability_error,\n        \"num_key_mismatch\": num_key_mismatch\n    }\n\n    if verbose:\n        print(\"results: %s\" % ret)\n\n    return ret", "output": "Evaluate a classifier specification for testing.\n\n    Parameters\n    ----------\n    filename: [str | Model]\n        File from where to load the model from (OR) a loaded\n        version of the MLModel.\n\n    data: [str | Dataframe]\n        Test data on which to evaluate the models (dataframe,\n        or path to a csv file).\n\n    probabilities: str\n       Column to interpret as the probabilities column\n\n    verbose: bool\n       Verbosity levels of the predictions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_arch_string():\n    \n    '''\n    rc, stdout, stderr = exec_command('uname -sm')\n    if rc > 0:\n        raise AssertionError('Error checking OS')\n\n    stdout = stdout.lower().strip()\n    if not 'linux' in stdout:\n        raise AssertionError('Unsupported OS')\n\n    if 'armv7l' in stdout:\n        return 'arm'\n\n    if 'x86_64' in stdout:\n        nv_rc, nv_stdout, nv_stderr = exec_command('nvidia-smi')\n        nv_stdout = nv_stdout.lower().strip()\n        if 'NVIDIA-SMI' in nv_stdout:\n            return 'gpu'\n        else:\n            return 'cpu'\n\n    raise AssertionError('Unsupported arch:', stdout)", "output": "r'''\n    Check local or remote system arch, to produce TaskCluster proper link.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_mlperf_tpu():\n  \"\"\"\"\"\"\n  hparams = transformer_base_v3()\n  hparams.mlperf_mode = True\n  hparams.symbol_modality_num_shards = 1\n  hparams.max_length = 256  # ignored when using \"_packed\" problems\n  hparams.batch_size = 2048  # per-chip batch size matches the reference model\n  hparams.hidden_size = 1024\n  hparams.filter_size = 4096\n  hparams.num_heads = 16\n  hparams.attention_dropout_broadcast_dims = \"0,1\"  # batch, heads\n  hparams.relu_dropout_broadcast_dims = \"1\"  # length\n  hparams.layer_prepostprocess_dropout_broadcast_dims = \"1\"  # length\n  return hparams", "output": "HParams for Transformer model on TPU for MLPerf on TPU 2x2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def timerEvent(self, event):\n        \"\"\" \n        \"\"\"\n        if event.timerId() == self._hide_timer.timerId():\n            self._hide_timer.stop()\n            self.hide()", "output": "Reimplemented to hide the widget when the hide timer fires.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _show_message(self, text):\r\n        \"\"\"\"\"\"\r\n        self.splash.showMessage(text, Qt.AlignBottom | Qt.AlignCenter |\r\n                                Qt.AlignAbsolute, QColor(Qt.white))", "output": "Show message on splash screen.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(identifier, value=None):\n  \"\"\"\"\"\"\n  if value is None:\n    value = identifier\n  if identifier is None:\n    return None\n  elif isinstance(identifier, dict):\n    try:\n      return deserialize(identifier)\n    except ValueError:\n      return value\n  elif isinstance(identifier, six.string_types):\n    config = {'class_name': str(identifier), 'config': {}}\n    try:\n      return deserialize(config)\n    except ValueError:\n      return value\n  elif callable(identifier):\n    return identifier\n  return value", "output": "Getter for loading from strings; returns value if can't load.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_datacenter(datacenter_name, service_instance=None):\n    '''\n    \n    '''\n    salt.utils.vmware.create_datacenter(service_instance, datacenter_name)\n    return {'create_datacenter': True}", "output": "Creates a datacenter.\n\n    Supported proxies: esxdatacenter\n\n    datacenter_name\n        The datacenter name\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.create_datacenter dc1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pad_for_tpu(shapes_dict, hparams, max_length):\n  \"\"\"\"\"\"\n  padded_shapes = {}\n\n  def get_filler(specified_max_length):\n    if not specified_max_length:\n      return max_length\n    return min(specified_max_length, max_length)\n\n  inputs_none_filler = get_filler(hparams.max_input_seq_length)\n  targets_none_filler = get_filler(hparams.max_target_seq_length)\n\n  def pad_one_shape(shape, none_filler):\n    return [\n        (dim if dim is not None else none_filler) for dim in shape.as_list()\n    ]\n\n  for key, shape in six.iteritems(shapes_dict):\n    if key == \"inputs\":\n      padded_shapes[key] = pad_one_shape(shape, inputs_none_filler)\n    elif key == \"targets\":\n      padded_shapes[key] = pad_one_shape(shape, targets_none_filler)\n    else:\n      padded_shapes[key] = pad_one_shape(shape, max_length)\n  return padded_shapes", "output": "Pads unknown features' dimensions for TPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _createFromRDD(self, rdd, schema, samplingRatio):\n        \"\"\"\n        \n        \"\"\"\n        if schema is None or isinstance(schema, (list, tuple)):\n            struct = self._inferSchema(rdd, samplingRatio, names=schema)\n            converter = _create_converter(struct)\n            rdd = rdd.map(converter)\n            if isinstance(schema, (list, tuple)):\n                for i, name in enumerate(schema):\n                    struct.fields[i].name = name\n                    struct.names[i] = name\n            schema = struct\n\n        elif not isinstance(schema, StructType):\n            raise TypeError(\"schema should be StructType or list or None, but got: %s\" % schema)\n\n        # convert python objects to sql data\n        rdd = rdd.map(schema.toInternal)\n        return rdd, schema", "output": "Create an RDD for DataFrame from an existing RDD, returns the RDD and schema.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _maybe_warn_for_unseparable_batches(self, output_key: str):\n        \"\"\"\n        \n        \"\"\"\n        if  output_key not in self._warn_for_unseparable_batches:\n            logger.warning(f\"Encountered the {output_key} key in the model's return dictionary which \"\n                           \"couldn't be split by the batch size. Key will be ignored.\")\n            # We only want to warn once for this key,\n            # so we set this to false so we don't warn again.\n            self._warn_for_unseparable_batches.add(output_key)", "output": "This method warns once if a user implements a model which returns a dictionary with\n        values which we are unable to split back up into elements of the batch. This is controlled\n        by a class attribute ``_warn_for_unseperable_batches`` because it would be extremely verbose\n        otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_argscope_for_function(func, log_shape=True):\n    \"\"\"\n\n    \"\"\"\n\n    assert callable(func), \"func should be a callable\"\n\n    @wraps(func)\n    def wrapped_func(*args, **kwargs):\n        actual_args = copy.copy(get_arg_scope()[func.__name__])\n        actual_args.update(kwargs)\n        out_tensor = func(*args, **actual_args)\n        in_tensor = args[0]\n\n        ctx = get_current_tower_context()\n        name = func.__name__ if 'name' not in kwargs else kwargs['name']\n        if log_shape:\n            if ('tower' not in ctx.ns_name.lower()) or ctx.is_main_training_tower:\n                # we assume the first parameter is the most interesting\n                if isinstance(out_tensor, tuple):\n                    out_tensor_descr = out_tensor[0]\n                else:\n                    out_tensor_descr = out_tensor\n                logger.info('%20s: %20s -> %20s' %\n                            (name, in_tensor.shape.as_list(),\n                             out_tensor_descr.shape.as_list()))\n\n        return out_tensor\n    # argscope requires this property\n    wrapped_func.symbolic_function = None\n    return wrapped_func", "output": "Decorator for function to support argscope\n\n    Example:\n\n        .. code-block:: python\n\n            from mylib import myfunc\n            myfunc = enable_argscope_for_function(myfunc)\n\n    Args:\n        func: A function mapping one or multiple tensors to one or multiple\n            tensors.\n        log_shape (bool): Specify whether the first input resp. output tensor\n            shape should be printed once.\n\n    Remarks:\n        If the function ``func`` returns multiple input or output tensors,\n        only the first input/output tensor shape is displayed during logging.\n\n    Returns:\n        The decorated function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def license_name(self, license_name):\n        \"\"\"\n        \"\"\"\n        allowed_values = [\"CC0-1.0\", \"CC-BY-SA-4.0\", \"GPL-2.0\", \"ODbL-1.0\", \"CC-BY-NC-SA-4.0\", \"unknown\", \"DbCL-1.0\", \"CC-BY-SA-3.0\", \"copyright-authors\", \"other\", \"reddit-api\", \"world-bank\"]  # noqa: E501\n        if license_name not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `license_name` ({0}), must be one of {1}\"  # noqa: E501\n                .format(license_name, allowed_values)\n            )\n\n        self._license_name = license_name", "output": "Sets the license_name of this DatasetNewRequest.\n\n        The license that should be associated with the dataset  # noqa: E501\n\n        :param license_name: The license_name of this DatasetNewRequest.  # noqa: E501\n        :type: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def go_to_error(self, text):\r\n        \"\"\"\"\"\"\r\n        match = get_error_match(to_text_string(text))\r\n        if match:\r\n            fname, lnb = match.groups()\r\n            if (\"<ipython-input-\" in fname and\r\n                    self.run_cell_filename is not None):\r\n                fname = self.run_cell_filename\r\n            self.edit_goto.emit(osp.abspath(fname), int(lnb), '')", "output": "Go to error if relevant", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_stock_min(code, start, end, format='numpy', frequence='1min', collections=DATABASE.stock_min):\n    ''\n    if frequence in ['1min', '1m']:\n        frequence = '1min'\n    elif frequence in ['5min', '5m']:\n        frequence = '5min'\n    elif frequence in ['15min', '15m']:\n        frequence = '15min'\n    elif frequence in ['30min', '30m']:\n        frequence = '30min'\n    elif frequence in ['60min', '60m']:\n        frequence = '60min'\n    else:\n        print(\"QA Error QA_fetch_stock_min parameter frequence=%s is none of 1min 1m 5min 5m 15min 15m 30min 30m 60min 60m\" % frequence)\n\n    __data = []\n    # code checking\n    code = QA_util_code_tolist(code)\n\n    cursor = collections.find({\n        'code': {'$in': code}, \"time_stamp\": {\n            \"$gte\": QA_util_time_stamp(start),\n            \"$lte\": QA_util_time_stamp(end)\n        }, 'type': frequence\n    }, {\"_id\": 0}, batch_size=10000)\n\n    res = pd.DataFrame([item for item in cursor])\n    try:\n        res = res.assign(volume=res.vol, datetime=pd.to_datetime(\n            res.datetime)).query('volume>1').drop_duplicates(['datetime', 'code']).set_index('datetime', drop=False)\n        # return res\n    except:\n        res = None\n    if format in ['P', 'p', 'pandas', 'pd']:\n        return res\n    elif format in ['json', 'dict']:\n        return QA_util_to_json_from_pandas(res)\n    # \u591a\u79cd\u6570\u636e\u683c\u5f0f\n    elif format in ['n', 'N', 'numpy']:\n        return numpy.asarray(res)\n    elif format in ['list', 'l', 'L']:\n        return numpy.asarray(res).tolist()\n    else:\n        print(\"QA Error QA_fetch_stock_min format parameter %s is none of  \\\"P, p, pandas, pd , json, dict , n, N, numpy, list, l, L, !\\\" \" % format)\n        return None", "output": "\u83b7\u53d6\u80a1\u7968\u5206\u949f\u7ebf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_action(self, q_values):\n        \"\"\"\n        \"\"\"\n        assert q_values.ndim == 1\n        nb_actions = q_values.shape[0]\n\n        if np.random.uniform() < self.eps:\n            action = np.random.randint(0, nb_actions)\n        else:\n            action = np.argmax(q_values)\n        return action", "output": "Return the selected action\n\n        # Arguments\n            q_values (np.ndarray): List of the estimations of Q for each action\n\n        # Returns\n            Selection action", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def libname_from_dir(dirname):\n    \"\"\"\"\"\"\n    parts = []\n    for part in dirname.split('-'):\n        if part[0].isdigit():\n            break\n        parts.append(part)\n    return '-'.join(parts)", "output": "Reconstruct the library name without it's version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_collapsed(block, val):\n        \"\"\"\n        \n        \"\"\"\n        if block is None:\n            return\n        state = block.userState()\n        if state == -1:\n            state = 0\n        state &= 0x77FFFFFF\n        state |= int(val) << 27\n        block.setUserState(state)", "output": "Sets the fold trigger state (collapsed or expanded).\n\n        :param block: The block to modify\n        :param val: The new trigger state (True=collapsed, False=expanded)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def highlight_matches(self):\r\n        \"\"\"\"\"\"\r\n        if self.is_code_editor and self.highlight_button.isChecked():\r\n            text = self.search_text.currentText()\r\n            words = self.words_button.isChecked()\r\n            regexp = self.re_button.isChecked()\r\n            self.editor.highlight_found_results(text, words=words,\r\n                                                regexp=regexp)", "output": "Highlight found results", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_to_url(self, text):\r\n        \"\"\"\"\"\"\r\n        if text.startswith('/'):\r\n            text = text[1:]\r\n        return QUrl(self.home_url.toString()+text+'.html')", "output": "Convert text address into QUrl object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(self, len):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlParserInputBufferRead(self._o, len)\n        return ret", "output": "Refresh the content of the input buffer, the old data are\n          considered consumed This routine handle the I18N\n           transcoding to internal UTF-8", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_wheel(\n            self, wheel_directory, config_settings=None,\n            metadata_directory=None):\n        \"\"\"\n        \"\"\"\n        if metadata_directory is not None:\n            metadata_directory = abspath(metadata_directory)\n        return self._call_hook('build_wheel', {\n            'wheel_directory': abspath(wheel_directory),\n            'config_settings': config_settings,\n            'metadata_directory': metadata_directory,\n        })", "output": "Build a wheel from this project.\n\n        Returns the name of the newly created file.\n\n        In general, this will call the 'build_wheel' hook in the backend.\n        However, if that was previously called by\n        'prepare_metadata_for_build_wheel', and the same metadata_directory is\n        used, the previously built wheel will be copied to wheel_directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vgdisplay(vgname='', quiet=False):\n    '''\n    \n    '''\n    ret = {}\n    cmd = ['vgdisplay', '-c']\n    if vgname:\n        cmd.append(vgname)\n    cmd_ret = __salt__['cmd.run_all'](cmd, python_shell=False,\n                                      ignore_retcode=quiet)\n\n    if cmd_ret['retcode'] != 0:\n        return {}\n\n    out = cmd_ret['stdout'].splitlines()\n    for line in out:\n        comps = line.strip().split(':')\n        ret[comps[0]] = {\n            'Volume Group Name': comps[0],\n            'Volume Group Access': comps[1],\n            'Volume Group Status': comps[2],\n            'Internal Volume Group Number': comps[3],\n            'Maximum Logical Volumes': comps[4],\n            'Current Logical Volumes': comps[5],\n            'Open Logical Volumes': comps[6],\n            'Maximum Logical Volume Size': comps[7],\n            'Maximum Physical Volumes': comps[8],\n            'Current Physical Volumes': comps[9],\n            'Actual Physical Volumes': comps[10],\n            'Volume Group Size (kB)': comps[11],\n            'Physical Extent Size (kB)': comps[12],\n            'Total Physical Extents': comps[13],\n            'Allocated Physical Extents': comps[14],\n            'Free Physical Extents': comps[15],\n            'UUID': comps[16],\n            }\n    return ret", "output": "Return information about the volume group(s)\n\n    vgname\n        volume group name\n\n    quiet\n        if the volume group is not present, do not show any error\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' lvm.vgdisplay\n        salt '*' lvm.vgdisplay nova-volumes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_process_mapping():\n    \"\"\"\n    \"\"\"\n    stat_name = detect_proc()\n    self_tty = _get_stat(os.getpid(), stat_name)[0]\n    processes = {}\n    for pid in os.listdir('/proc'):\n        if not pid.isdigit():\n            continue\n        try:\n            tty, ppid = _get_stat(pid, stat_name)\n            if tty != self_tty:\n                continue\n            args = _get_cmdline(pid)\n            processes[pid] = Process(args=args, pid=pid, ppid=ppid)\n        except IOError:\n            # Process has disappeared - just ignore it.\n            continue\n    return processes", "output": "Try to look up the process tree via the /proc interface.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _wrap_deprecated_function(func, message):\n    \"\"\" \"\"\"\n    def _(col):\n        warnings.warn(message, DeprecationWarning)\n        return func(col)\n    return functools.wraps(func)(_)", "output": "Wrap the deprecated function to print out deprecation warnings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_pipe(self, name):\n        \"\"\"\n        \"\"\"\n        if name not in self.pipe_names:\n            raise ValueError(Errors.E001.format(name=name, opts=self.pipe_names))\n        return self.pipeline.pop(self.pipe_names.index(name))", "output": "Remove a component from the pipeline.\n\n        name (unicode): Name of the component to remove.\n        RETURNS (tuple): A `(name, component)` tuple of the removed component.\n\n        DOCS: https://spacy.io/api/language#remove_pipe", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pad(x, p=3):\n    \"\"\"\n    \"\"\"\n    return tf.pad(x, [[0, 0], [0, 0], [p, p], [p, p]])", "output": "Pad tensor in H, W\n\n    Remarks:\n        TensorFlow uses \"ceil(input_spatial_shape[i] / strides[i])\" rather than explicit padding\n        like Caffe, pyTorch does. Hence, we need to pad here beforehand.\n\n    Args:\n        x (tf.tensor): incoming tensor\n        p (int, optional): padding for H, W\n\n    Returns:\n        tf.tensor: padded tensor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_train_end(self, **kwargs:Any)->None:\n        \"\"\n        self.learn.load('tmp', purge=False)\n        if hasattr(self.learn.model, 'reset'): self.learn.model.reset()\n        for cb in self.callbacks:\n            if hasattr(cb, 'reset'): cb.reset()\n        print('LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.')", "output": "Cleanup learn model weights disturbed during LRFinder exploration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fib_list(n):\n    \"\"\"\n    \"\"\"\n\n    # precondition\n    assert n >= 0, 'n must be a positive integer'\n\n    list_results = [0, 1]\n    for i in range(2, n+1):\n        list_results.append(list_results[i-1] + list_results[i-2])\n    return list_results[n]", "output": "[summary]\n    This algorithm computes the n-th fibbonacci number\n    very quick. approximate O(n)\n    The algorithm use dynamic programming.\n    \n    Arguments:\n        n {[int]} -- [description]\n    \n    Returns:\n        [int] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_task_done(self):\n        ''''''\n        cnt = 0\n        try:\n            while True:\n                task = self.status_queue.get_nowait()\n                # check _on_get_info result here\n                if task.get('taskid') == '_on_get_info' and 'project' in task and 'track' in task:\n                    if task['project'] not in self.projects:\n                        continue\n                    project = self.projects[task['project']]\n                    project.on_get_info(task['track'].get('save') or {})\n                    logger.info(\n                        '%s on_get_info %r', task['project'], task['track'].get('save', {})\n                    )\n                    continue\n                elif not self.task_verify(task):\n                    continue\n                self.on_task_status(task)\n                cnt += 1\n        except Queue.Empty:\n            pass\n        return cnt", "output": "Check status queue", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_elements_by_id(self, id_):\n        \"\"\"\n        \n        \"\"\"\n        return self.find_elements(by=By.ID, value=id_)", "output": "Finds multiple elements by id.\n\n        :Args:\n         - id\\\\_ - The id of the elements to be found.\n\n        :Returns:\n         - list of WebElement - a list with elements if any was found.  An\n           empty list if not\n\n        :Usage:\n            ::\n\n                elements = driver.find_elements_by_id('foo')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_active(self):\n    \"\"\"\n    \"\"\"\n    if self._db_connection_provider:\n      # The plugin is active if one relevant tag can be found in the database.\n      db = self._db_connection_provider()\n      cursor = db.execute(\n          '''\n          SELECT 1\n          FROM Tags\n          WHERE Tags.plugin_name = ?\n          LIMIT 1\n          ''',\n          (metadata.PLUGIN_NAME,))\n      return bool(list(cursor))\n\n    if not self._multiplexer:\n      return False\n\n    all_runs = self._multiplexer.PluginRunToTagToContent(metadata.PLUGIN_NAME)\n\n    # The plugin is active if any of the runs has a tag relevant to the plugin.\n    return any(six.itervalues(all_runs))", "output": "Determines whether this plugin is active.\n\n    This plugin is active only if PR curve summary data is read by TensorBoard.\n\n    Returns:\n      Whether this plugin is active.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self, qname):\n        '''\n        \n        '''\n        try:\n            if self.exists(qname):\n                log.error('Queues \"%s\" already exists. Nothing done.', qname)\n                return True\n\n            self.conn.create(qname)\n\n            return True\n        except pyrax.exceptions as err_msg:\n            log.error('RackSpace API got some problems during creation: %s',\n                      err_msg)\n        return False", "output": "Create RackSpace Queue.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_parallelism(self, num_instances):\n        \"\"\"\n        \"\"\"\n        assert (num_instances > 0)\n        self.env._set_parallelism(self.src_operator_id, num_instances)\n        return self", "output": "Sets the number of instances for the source operator of the stream.\n\n        Attributes:\n             num_instances (int): The level of parallelism for the source\n             operator of the stream.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_to_default(self):\r\n        \"\"\"\"\"\"\r\n        reset = QMessageBox.warning(self, _(\"Shortcuts reset\"),\r\n                                    _(\"Do you want to reset \"\r\n                                      \"to default values?\"),\r\n                                    QMessageBox.Yes | QMessageBox.No)\r\n        if reset == QMessageBox.No:\r\n            return\r\n        reset_shortcuts()\r\n        self.main.apply_shortcuts()\r\n        self.table.load_shortcuts()\r\n        self.load_from_conf()\r\n        self.set_modified(False)", "output": "Reset to default values of the shortcuts making a confirmation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable(iface):\n    '''\n    \n    '''\n    if is_enabled(iface):\n        return True\n    cmd = ['netsh', 'interface', 'set', 'interface',\n           'name={0}'.format(iface),\n           'admin=ENABLED']\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return is_enabled(iface)", "output": "Enable an interface\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt -G 'os_family:Windows' ip.enable 'Local Area Connection #2'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_bias_batch(batch_coordinates_q,\n                         batch_coordinates_k=None,\n                         condition_fn=None):\n  \"\"\"\n  \"\"\"\n  if batch_coordinates_k is None:\n    batch_coordinates_k = batch_coordinates_q\n\n  # Convert to float first because of b/25387198.\n  def to_float(bc):\n    bc = tf.squeeze(bc, 1)\n    bc = tf.to_float(bc)\n    return bc\n\n  # Broadcast to create [length_q, length_k] mask.\n  bc_v = tf.expand_dims(to_float(batch_coordinates_q), 1)\n  bc_h = tf.expand_dims(to_float(batch_coordinates_k), 0)\n  bias_batch = bc_h - bc_v\n  bias_batch = condition_fn(bias_batch)\n  bias_batch *= -1e9\n  return bias_batch", "output": "Generate a mask to prevent the batch to attend to each others.\n\n  Args:\n    batch_coordinates_q: Int-like Tensor of shape [length_q, 1] containing the\n      coordinates of the batches\n    batch_coordinates_k: Int-like Tensor of shape [length_k, 1] containing the\n      coordinates of the batches. If None, do self-attention.\n    condition_fn: Callable defining the attention mask.\n\n  Returns:\n    Float-like Tensor of shape [length_q, length_k] containing either 0 or\n    -infinity (-1e9).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_query_results(self, response_pb):\n        \"\"\"\n        \"\"\"\n        self._skipped_results = response_pb.batch.skipped_results\n\n        if response_pb.batch.more_results == _NO_MORE_RESULTS:\n            self.next_page_token = None\n        else:\n            self.next_page_token = base64.urlsafe_b64encode(\n                response_pb.batch.end_cursor\n            )\n        self._end_cursor = None\n\n        if response_pb.batch.more_results == _NOT_FINISHED:\n            self._more_results = True\n        elif response_pb.batch.more_results in _FINISHED:\n            self._more_results = False\n        else:\n            raise ValueError(\"Unexpected value returned for `more_results`.\")\n\n        return [result.entity for result in response_pb.batch.entity_results]", "output": "Process the response from a datastore query.\n\n        :type response_pb: :class:`.datastore_pb2.RunQueryResponse`\n        :param response_pb: The protobuf response from a ``runQuery`` request.\n\n        :rtype: iterable\n        :returns: The next page of entity results.\n        :raises ValueError: If ``more_results`` is an unexpected value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_algebra_inverse_sample(vlist, ops, solve_ops, min_depth,\n                                    max_depth):\n  \"\"\"\n  \"\"\"\n  side = random.randrange(2)\n  left_depth = random.randrange(min_depth if side else 0, max_depth + 1)\n  right_depth = random.randrange(min_depth if not side else 0, max_depth + 1)\n\n  var_index = random.randrange(len(vlist))\n  var = vlist[var_index]\n  consts = vlist[:var_index] + vlist[var_index + 1:]\n\n  left = random_expr_with_required_var(left_depth, var\n                                       if side else None, consts, ops)\n  right = random_expr_with_required_var(right_depth, var\n                                        if not side else None, consts, ops)\n\n  left_str = str(left)\n  right_str = str(right)\n  target = str(algebra_inverse_solve(left, right, var, solve_ops))\n  sample = \"%s:%s=%s\" % (var, left_str, right_str)\n  return sample, target", "output": "Randomly generate an algebra inverse dataset sample.\n\n  Given an input equation and variable, produce the expression equal to the\n  variable.\n\n  Args:\n    vlist: Variable list. List of chars that can be used in the expression.\n    ops: List of ExprOp instances. The allowed operators for the expression.\n    solve_ops: See `solve_ops` documentation in `algebra_inverse_solve`.\n    min_depth: Expression trees will not have a smaller depth than this. 0 means\n        there is just a variable. 1 means there is one operation.\n    max_depth: Expression trees will not have a larger depth than this. To make\n        all trees have the same depth, set this equal to `min_depth`.\n\n  Returns:\n    sample: String representation of the input. Will be of the form\n        'solve_var:left_side=right_side'.\n    target: String representation of the solution.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __criteria(self, obj, matches=None, mt=None, lt=None, eq=None):\n        '''\n        \n        '''\n        # Fail matcher if \"less than\"\n        for field, value in (mt or {}).items():\n            if getattr(obj, field) <= value:\n                return False\n\n        # Fail matcher if \"more than\"\n        for field, value in (lt or {}).items():\n            if getattr(obj, field) >= value:\n                return False\n\n        # Fail matcher if \"not equal\"\n        for field, value in (eq or {}).items():\n            if getattr(obj, field) != value:\n                return False\n\n        # Fail matcher if \"doesn't match\"\n        for field, value in (matches or {}).items():\n            if not re.search(value, str(getattr(obj, field))):\n                return False\n\n        return True", "output": "Returns True if object is aligned to the criteria.\n\n        :param obj:\n        :param matches:\n        :param mt:\n        :param lt:\n        :param eq:\n        :return: Boolean", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_line(headers, fields):\n  \"\"\"\n  \"\"\"\n  assert len(fields) == len(headers), (fields, headers)\n  fields = [\"%2.4f\" % field if isinstance(field, float) else str(field)\n            for field in fields]\n  return '  '.join(' ' * max(0, len(header) - len(field)) + field\n                   for (header, field) in zip(headers, fields))", "output": "Format a line of a table.\n\n  Arguments:\n    headers: A list of strings that are used as the table headers.\n    fields: A list of the same length as `headers` where `fields[i]` is\n      the entry for `headers[i]` in this row. Elements can be of\n      arbitrary types. Pass `headers` to print the header row.\n\n  Returns:\n    A pretty string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ParseLines(self, input_lines):\n    \"\"\"\n    \"\"\"\n    current_macro = None\n    for line in input_lines:\n      if line.startswith('PDDM-'):\n        directive = line.split(' ', 1)[0]\n        if directive == 'PDDM-DEFINE':\n          name, args = self._ParseDefineLine(line)\n          if self._macros.get(name):\n            raise PDDMError('Attempt to redefine macro: \"%s\"' % line)\n          current_macro = self.MacroDefinition(name, args)\n          self._macros[name] = current_macro\n          continue\n        if directive == 'PDDM-DEFINE-END':\n          if not current_macro:\n            raise PDDMError('Got DEFINE-END directive without an active macro:'\n                            ' \"%s\"' % line)\n          current_macro = None\n          continue\n        raise PDDMError('Hit a line with an unknown directive: \"%s\"' % line)\n\n      if current_macro:\n        current_macro.AppendLine(line)\n        continue\n\n      # Allow blank lines between macro definitions.\n      if line.strip() == '':\n        continue\n\n      raise PDDMError('Hit a line that wasn\\'t a directive and no open macro'\n                      ' definition: \"%s\"' % line)", "output": "Parses list of lines.\n\n    Args:\n      input_lines: A list of strings of input to parse (no newlines on the\n                   strings).\n\n    Raises:\n      PDDMError if there are any issues.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def key_value_to_tree(data):\n    '''\n    \n    '''\n    tree = {}\n    for flatkey, value in six.iteritems(data):\n        t = tree\n        keys = flatkey.split(__opts__['pepa_delimiter'])\n        for i, key in enumerate(keys, 1):\n            if i == len(keys):\n                t[key] = value\n            else:\n                t = t.setdefault(key, {})\n    return tree", "output": "Convert key/value to tree", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_pb(self):\n        \"\"\"\n        \"\"\"\n        kwargs = {}\n\n        if self.start_open is not None:\n            kwargs[\"start_open\"] = _make_list_value_pb(self.start_open)\n\n        if self.start_closed is not None:\n            kwargs[\"start_closed\"] = _make_list_value_pb(self.start_closed)\n\n        if self.end_open is not None:\n            kwargs[\"end_open\"] = _make_list_value_pb(self.end_open)\n\n        if self.end_closed is not None:\n            kwargs[\"end_closed\"] = _make_list_value_pb(self.end_closed)\n\n        return KeyRangePB(**kwargs)", "output": "Construct a KeyRange protobuf.\n\n        :rtype: :class:`~google.cloud.spanner_v1.proto.keys_pb2.KeyRange`\n        :returns: protobuf corresponding to this instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_data(query_func, load_data=False, save_data=False):\n    \"\"\"\"\"\"\n    if hasattr(query_func, '__name__'):\n        func_name = query_func.__name__\n    elif hasattr(query_func, 'func'):\n        func_name = query_func.func.__name__\n\n    pickle_file = '{}.pickle'.format(func_name)\n\n    if load_data:\n        data = load_object(pickle_file)\n    else:\n        data = query_func()\n        if save_data:\n            save_object(pickle_file, data)\n    return data", "output": "Gets data from query_func, optionally saving that data to a file; or loads data from a file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_role(item):\n    \"\"\"\n    \"\"\"\n\n    def predicate(ctx):\n        if not isinstance(ctx.channel, discord.abc.GuildChannel):\n            raise NoPrivateMessage()\n\n        if isinstance(item, int):\n            role = discord.utils.get(ctx.author.roles, id=item)\n        else:\n            role = discord.utils.get(ctx.author.roles, name=item)\n        if role is None:\n            raise MissingRole(item)\n        return True\n\n    return check(predicate)", "output": "A :func:`.check` that is added that checks if the member invoking the\n    command has the role specified via the name or ID specified.\n\n    If a string is specified, you must give the exact name of the role, including\n    caps and spelling.\n\n    If an integer is specified, you must give the exact snowflake ID of the role.\n\n    If the message is invoked in a private message context then the check will\n    return ``False``.\n\n    This check raises one of two special exceptions, :exc:`.MissingRole` if the user\n    is missing a role, or :exc:`.NoPrivateMessage` if it is used in a private message.\n    Both inherit from :exc:`.CheckFailure`.\n\n    .. versionchanged:: 1.1.0\n\n        Raise :exc:`.MissingRole` or :exc:`.NoPrivateMessage`\n        instead of generic :exc:`.CheckFailure`\n\n    Parameters\n    -----------\n    item: Union[:class:`int`, :class:`str`]\n        The name or ID of the role to check.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serialize_array(array, force_list=False, buffers=None):\n    ''' \n\n    '''\n    if isinstance(array, np.ma.MaskedArray):\n        array = array.filled(np.nan)  # Set masked values to nan\n    if (array_encoding_disabled(array) or force_list):\n        return transform_array_to_list(array)\n    if not array.flags['C_CONTIGUOUS']:\n        array = np.ascontiguousarray(array)\n    if buffers is None:\n        return encode_base64_dict(array)\n    else:\n        return encode_binary_dict(array, buffers)", "output": "Transforms a NumPy array into serialized form.\n\n    Args:\n        array (np.ndarray) : the NumPy array to transform\n        force_list (bool, optional) : whether to only output to standard lists\n            This function can encode some dtypes using a binary encoding, but\n            setting this argument to True will override that and cause only\n            standard Python lists to be emitted. (default: False)\n\n        buffers (set, optional) :\n            If binary buffers are desired, the buffers parameter may be\n            provided, and any columns that may be sent as binary buffers\n            will be added to the set. If None, then only base64 encoding\n            will be used (default: None)\n\n            If force_list is True, then this value will be ignored, and\n            no buffers will be generated.\n\n            **This is an \"out\" parameter**. The values it contains will be\n            modified in-place.\n\n    Returns:\n        list or dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def img2img_transformer_tiny():\n  \"\"\"\"\"\"\n  hparams = img2img_transformer2d_base()\n  hparams.num_hidden_layers = 2\n  hparams.hidden_size = 128\n  hparams.batch_size = 4\n  hparams.max_length = 128\n  hparams.attention_key_channels = hparams.attention_value_channels = 0\n  hparams.filter_size = 128\n  hparams.num_heads = 1\n  hparams.pos = \"timing\"\n  return hparams", "output": "Tiny params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_from_npy2d(self, mat, missing, nthread):\n        \"\"\"\n        \n        \"\"\"\n        if len(mat.shape) != 2:\n            raise ValueError('Input numpy.ndarray must be 2 dimensional')\n        # flatten the array by rows and ensure it is float32.\n        # we try to avoid data copies if possible (reshape returns a view when possible\n        # and we explicitly tell np.array to try and avoid copying)\n        data = np.array(mat.reshape(mat.size), copy=False, dtype=np.float32)\n        handle = ctypes.c_void_p()\n        missing = missing if missing is not None else np.nan\n        if nthread is None:\n            _check_call(_LIB.XGDMatrixCreateFromMat(\n                data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n                c_bst_ulong(mat.shape[0]),\n                c_bst_ulong(mat.shape[1]),\n                ctypes.c_float(missing),\n                ctypes.byref(handle)))\n        else:\n            _check_call(_LIB.XGDMatrixCreateFromMat_omp(\n                data.ctypes.data_as(ctypes.POINTER(ctypes.c_float)),\n                c_bst_ulong(mat.shape[0]),\n                c_bst_ulong(mat.shape[1]),\n                ctypes.c_float(missing),\n                ctypes.byref(handle),\n                nthread))\n        self.handle = handle", "output": "Initialize data from a 2-D numpy matrix.\n\n        If ``mat`` does not have ``order='C'`` (aka row-major) or is not contiguous,\n        a temporary copy will be made.\n\n        If ``mat`` does not have ``dtype=numpy.float32``, a temporary copy will be made.\n\n        So there could be as many as two temporary data copies; be mindful of input layout\n        and type if memory use is a concern.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_file(path,\n             dest,\n             saltenv='base',\n             makedirs=False,\n             template=None,\n             gzip=None):\n    '''\n    \n    '''\n    if gzip is not None:\n        log.warning('The gzip argument to cp.get_file in salt-ssh is '\n                    'unsupported')\n\n    if template is not None:\n        (path, dest) = _render_filenames(path, dest, saltenv, template)\n\n    src = __context__['fileclient'].cache_file(\n        path,\n        saltenv,\n        cachedir=os.path.join('salt-ssh', __salt__.kwargs['id_']))\n    single = salt.client.ssh.Single(\n            __opts__,\n            '',\n            **__salt__.kwargs)\n    ret = single.shell.send(src, dest, makedirs)\n    return not ret[2]", "output": "Send a file from the master to the location in specified\n\n    .. note::\n\n        gzip compression is not supported in the salt-ssh version of\n        cp.get_file. The argument is only accepted for interface compatibility.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _downgrade_v4(op):\n    \"\"\"\n    \n    \"\"\"\n    op.drop_index('ix_equities_fuzzy_symbol')\n    op.drop_index('ix_equities_company_symbol')\n\n    op.execute(\"UPDATE equities SET exchange = exchange_full\")\n\n    with op.batch_alter_table('equities') as batch_op:\n        batch_op.drop_column('exchange_full')\n\n    op.create_index('ix_equities_fuzzy_symbol',\n                    table_name='equities',\n                    columns=['fuzzy_symbol'])\n    op.create_index('ix_equities_company_symbol',\n                    table_name='equities',\n                    columns=['company_symbol'])", "output": "Downgrades assets db by copying the `exchange_full` column to `exchange`,\n    then dropping the `exchange_full` column.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _syscall(self, command, input=None, env=None, *params):\n        '''\n        \n        '''\n        return Popen([command] + list(params), stdout=PIPE, stdin=PIPE, stderr=STDOUT,\n                     env=env or os.environ).communicate(input=input)", "output": "Call an external system command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_update(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_openstack_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    if 'new_name' in kwargs:\n        kwargs['name'] = kwargs.pop('new_name')\n    return cloud.update_project(**kwargs)", "output": "Update a project\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.project_update name=project1 new_name=newproject\n        salt '*' keystoneng.project_update name=project2 enabled=False description='new description'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shadow_calc(data):\n    \"\"\"\n    \"\"\"\n\n    up_shadow = abs(data.high - (max(data.open, data.close)))\n    down_shadow = abs(data.low - (min(data.open, data.close)))\n    entity = abs(data.open - data.close)\n    towards = True if data.open < data.close else False\n    print('=' * 15)\n    print('up_shadow : {}'.format(up_shadow))\n    print('down_shadow : {}'.format(down_shadow))\n    print('entity: {}'.format(entity))\n    print('towards : {}'.format(towards))\n    return up_shadow, down_shadow, entity, data.date, data.code", "output": "\u8ba1\u7b97\u4e0a\u4e0b\u5f71\u7ebf\n\n    Arguments:\n        data {DataStruct.slice} -- \u8f93\u5165\u7684\u662f\u4e00\u4e2a\u884c\u60c5\u5207\u7247\n\n    Returns:\n        up_shadow {float} -- \u4e0a\u5f71\u7ebf\n        down_shdow {float} -- \u4e0b\u5f71\u7ebf\n        entity {float} -- \u5b9e\u4f53\u90e8\u5206\n        date {str} -- \u65f6\u95f4\n        code {str} -- \u4ee3\u7801", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_lib_path():\n    \"\"\"\"\"\"\n    curr_path = os.path.dirname(os.path.abspath(os.path.expanduser(__file__)))\n    amalgamation_lib_path = os.path.join(curr_path, '../../lib/libmxnet_predict.so')\n    if os.path.exists(amalgamation_lib_path) and os.path.isfile(amalgamation_lib_path):\n        lib_path = [amalgamation_lib_path]\n        return lib_path\n    else:\n        logging.info('Cannot find libmxnet_predict.so. Will search for MXNet library using libinfo.py then.')\n        try:\n            from mxnet.libinfo import find_lib_path\n            lib_path = find_lib_path()\n            return lib_path\n        except ImportError:\n            libinfo_path = os.path.join(curr_path, '../../python/mxnet/libinfo.py')\n            if os.path.exists(libinfo_path) and os.path.isfile(libinfo_path):\n                libinfo = {'__file__': libinfo_path}\n                exec(compile(open(libinfo_path, \"rb\").read(), libinfo_path, 'exec'), libinfo, libinfo)\n                lib_path = libinfo['find_lib_path']()\n                return lib_path\n            else:\n                raise RuntimeError('Cannot find libinfo.py at %s.' % libinfo_path)", "output": "Find mxnet library.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_network(self,\n                      images,\n                      phase_train=True,\n                      nclass=1001,\n                      image_depth=3,\n                      data_type=tf.float32,\n                      data_format=\"NCHW\",\n                      use_tf_layers=True,\n                      fp16_vars=False):\n        \"\"\"\"\"\"\n        if data_format == \"NCHW\":\n            images = tf.transpose(images, [0, 3, 1, 2])\n        var_type = tf.float32\n        if data_type == tf.float16 and fp16_vars:\n            var_type = tf.float16\n        network = convnet_builder.ConvNetBuilder(\n            images, image_depth, phase_train, use_tf_layers, data_format,\n            data_type, var_type)\n        with tf.variable_scope(\n                \"cg\", custom_getter=network.get_custom_getter()):\n            self.add_inference(network)\n            # Add the final fully-connected class layer\n            logits = (network.affine(nclass, activation=\"linear\")\n                      if not self.skip_final_affine_layer() else\n                      network.top_layer)\n            aux_logits = None\n            if network.aux_top_layer is not None:\n                with network.switch_to_aux_top_layer():\n                    aux_logits = network.affine(\n                        nclass, activation=\"linear\", stddev=0.001)\n        if data_type == tf.float16:\n            # TODO(reedwm): Determine if we should do this cast here.\n            logits = tf.cast(logits, tf.float32)\n            if aux_logits is not None:\n                aux_logits = tf.cast(aux_logits, tf.float32)\n        return logits, aux_logits", "output": "Returns logits and aux_logits from images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cast_inplace(terms, acceptable_dtypes, dtype):\n    \"\"\"\n    \"\"\"\n    dt = np.dtype(dtype)\n    for term in terms:\n        if term.type in acceptable_dtypes:\n            continue\n\n        try:\n            new_value = term.value.astype(dt)\n        except AttributeError:\n            new_value = dt.type(term.value)\n        term.update(new_value)", "output": "Cast an expression inplace.\n\n    Parameters\n    ----------\n    terms : Op\n        The expression that should cast.\n    acceptable_dtypes : list of acceptable numpy.dtype\n        Will not cast if term's dtype in this list.\n\n        .. versionadded:: 0.19.0\n\n    dtype : str or numpy.dtype\n        The dtype to cast to.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _consolidate(self, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        if inplace:\n            self._consolidate_inplace()\n        else:\n            f = lambda: self._data.consolidate()\n            cons_data = self._protect_consolidate(f)\n            return self._constructor(cons_data).__finalize__(self)", "output": "Compute NDFrame with \"consolidated\" internals (data of each dtype\n        grouped together in a single ndarray).\n\n        Parameters\n        ----------\n        inplace : boolean, default False\n            If False return new object, otherwise modify existing object\n\n        Returns\n        -------\n        consolidated : same type as caller", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _combine_document_events(new_event, old_events):\n    ''' \n\n    '''\n    for event in reversed(old_events):\n        if event.combine(new_event):\n            return\n\n    # no combination was possible\n    old_events.append(new_event)", "output": "Attempt to combine a new event with a list of previous events.\n\n    The ``old_event`` will be scanned in reverse, and ``.combine(new_event)``\n    will be called on each. If a combination can be made, the function\n    will return immediately. Otherwise, ``new_event`` will be appended to\n    ``old_events``.\n\n    Args:\n        new_event (DocumentChangedEvent) :\n            The new event to attempt to combine\n\n        old_events (list[DocumentChangedEvent])\n            A list of previous events to attempt to combine new_event with\n\n            **This is an \"out\" parameter**. The values it contains will be\n            modified in-place.\n\n    Returns:\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_entity(self, entity):\n        \"\"\"\n        \"\"\"\n        self._ensure_loaded()\n        self.entities[str(entity)] = entity", "output": "Add an entity to the ACL.\n\n        :type entity: :class:`_ACLEntity`\n        :param entity: The entity to add to this ACL.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sink_delete(self, project, sink_name):\n        \"\"\"\n        \"\"\"\n        target = \"/projects/%s/sinks/%s\" % (project, sink_name)\n        self.api_request(method=\"DELETE\", path=target)", "output": "API call:  delete a sink resource.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks/delete\n\n        :type project: str\n        :param project: ID of the project containing the sink.\n\n        :type sink_name: str\n        :param sink_name: the name of the sink", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def modified(self):\n        \"\"\"\n        \"\"\"\n        modified_time = self._properties.get(\"lastModifiedTime\")\n        if modified_time is not None:\n            # modified_time will be in milliseconds.\n            return google.cloud._helpers._datetime_from_microseconds(\n                1000.0 * float(modified_time)\n            )", "output": "Union[datetime.datetime, None]: Datetime at which the dataset was\n        last modified (:data:`None` until set from the server).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_metric(self, reset: bool) -> Union[float, Tuple[float, ...], Dict[str, float], Dict[str, List[float]]]:\n        \"\"\"\n        \n        \"\"\"\n        raise NotImplementedError", "output": "Compute and return the metric. Optionally also call :func:`self.reset`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def virtual_network_get(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        vnet = netconn.virtual_networks.get(\n            virtual_network_name=name,\n            resource_group_name=resource_group\n        )\n        result = vnet.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get details about a specific virtual network.\n\n    :param name: The name of the virtual network to query.\n\n    :param resource_group: The resource group name assigned to the\n        virtual network.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.virtual_network_get testnet testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def tenant_delete(tenant_id=None, name=None, profile=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    if name:\n        for tenant in getattr(kstone, _TENANTS, None).list():\n            if tenant.name == name:\n                tenant_id = tenant.id\n                break\n    if not tenant_id:\n        return {'Error': 'Unable to resolve tenant id'}\n    getattr(kstone, _TENANTS, None).delete(tenant_id)\n    ret = 'Tenant ID {0} deleted'.format(tenant_id)\n    if name:\n\n        ret += ' ({0})'.format(name)\n    return ret", "output": "Delete a tenant (keystone tenant-delete)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.tenant_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.tenant_delete tenant_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.tenant_delete name=demo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _getspnam(name, root=None):\n    '''\n    \n    '''\n    root = '/' if not root else root\n    passwd = os.path.join(root, 'etc/shadow')\n    with salt.utils.files.fopen(passwd) as fp_:\n        for line in fp_:\n            line = salt.utils.stringutils.to_unicode(line)\n            comps = line.strip().split(':')\n            if comps[0] == name:\n                # Generate a getspnam compatible output\n                for i in range(2, 9):\n                    comps[i] = int(comps[i]) if comps[i] else -1\n                return spwd.struct_spwd(comps)\n    raise KeyError", "output": "Alternative implementation for getspnam, that use only /etc/shadow", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_params_on_kvstore(param_arrays, grad_arrays, kvstore, param_names):\n    \"\"\"\"\"\"\n    for index, pair in enumerate(zip(param_arrays, grad_arrays)):\n        arg_list, grad_list = pair\n        if grad_list[0] is None:\n            continue\n        name = param_names[index]\n        # push gradient, priority is negative index\n        kvstore.push(name, grad_list, priority=-index)\n        # pull back the weights\n        kvstore.pull(name, arg_list, priority=-index)", "output": "Perform update of param_arrays from grad_arrays on kvstore.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_last_line(self):\n        \"\"\"\"\"\"\n        editor = self._editor\n        text_cursor = editor.textCursor()\n        text_cursor.movePosition(text_cursor.End, text_cursor.MoveAnchor)\n        text_cursor.select(text_cursor.LineUnderCursor)\n        text_cursor.removeSelectedText()\n        text_cursor.deletePreviousChar()\n        editor.setTextCursor(text_cursor)", "output": "Removes the last line of the document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_thing_type(thingTypeName, thingTypeDescription,\n    searchableAttributesList, region=None, key=None,\n    keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        thingTypeProperties = dict(\n            thingTypeDescription=thingTypeDescription,\n            searchableAttributes=searchableAttributesList\n        )\n        thingtype = conn.create_thing_type(\n            thingTypeName=thingTypeName,\n            thingTypeProperties=thingTypeProperties\n        )\n\n        if thingtype:\n            log.info('The newly created thing type ARN is %s', thingtype['thingTypeArn'])\n\n            return {'created': True, 'thingTypeArn': thingtype['thingTypeArn']}\n        else:\n            log.warning('thing type was not created')\n            return {'created': False}\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a valid config, create a thing type.\n\n    Returns {created: true} if the thing type was created and returns\n    {created: False} if the thing type was not created.\n\n    .. versionadded:: 2016.11.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.create_thing_type mythingtype \\\\\n              thingtype_description_string '[\"searchable_attr_1\", \"searchable_attr_2\"]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def submit(args):\n    gpus = args.gpus.strip().split(',')\n    \"\"\"\"\"\"\n    def mthread_submit(nworker, nserver, envs):\n        \"\"\"\n        customized submit script, that submit nslave jobs, each must contain args as parameter\n        note this can be a lambda function containing additional parameters in input\n\n        Parameters\n        ----------\n        nworker: number of slave process to start up\n        nserver: number of server nodes to start up\n        envs: enviroment variables to be added to the starting programs\n        \"\"\"\n        procs = {}\n        for i, gpu in enumerate(gpus):\n            for j in range(args.num_threads):\n                procs[i] = Thread(target=exec_cmd, args=(args.command + ['--gpus=%s'%gpu], 'worker', i*args.num_threads+j, envs))\n                procs[i].setDaemon(True)\n                procs[i].start()\n        for i in range(len(gpus)*args.num_threads, len(gpus)*args.num_threads + nserver):\n            procs[i] = Thread(target=exec_cmd, args=(args.command, 'server', i, envs))\n            procs[i].setDaemon(True)\n            procs[i].start()\n\n    # call submit, with nslave, the commands to run each job and submit function\n    tracker.submit(args.num_threads*len(gpus), args.num_servers, fun_submit=mthread_submit,\n                   pscmd=(' '.join(args.command)))", "output": "Submit function of local jobs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _traverse_results(value, fields, row, path):\n    \"\"\"\n    \n    \"\"\"\n    for f, v in value.iteritems():  # for each item in obj\n        field_name = '{path}.{name}'.format(path=path, name=f) if path else f\n\n        if not isinstance(v, (dict, list, tuple)):  # if not data structure\n            if field_name in fields:\n                row[fields.index(field_name)] = ensure_utf(v)\n\n        elif isinstance(v, dict) and f != 'attributes':  # it is a dict\n            _traverse_results(v, fields, row, field_name)", "output": "Helper method for parse_results().\n\n    Traverses through ordered dict and recursively calls itself when encountering a dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_location(opts=None, provider=None):\n    '''\n    \n    '''\n    if opts is None:\n        opts = {}\n    ret = opts.get('location')\n    if ret is None and provider is not None:\n        ret = provider.get('location')\n    if ret is None:\n        ret = get_region_from_metadata()\n    if ret is None:\n        ret = DEFAULT_LOCATION\n    return ret", "output": "Return the region to use, in this order:\n        opts['location']\n        provider['location']\n        get_region_from_metadata()\n        DEFAULT_LOCATION", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_vcs(pipfile_entry):\n    # type: (PipfileType) -> bool\n    \"\"\"\"\"\"\n    if isinstance(pipfile_entry, Mapping):\n        return any(key for key in pipfile_entry.keys() if key in VCS_LIST)\n\n    elif isinstance(pipfile_entry, six.string_types):\n        if not is_valid_url(pipfile_entry) and pipfile_entry.startswith(\"git+\"):\n            pipfile_entry = add_ssh_scheme_to_git_uri(pipfile_entry)\n\n        parsed_entry = urlsplit(pipfile_entry)\n        return parsed_entry.scheme in VCS_SCHEMES\n    return False", "output": "Determine if dictionary entry from Pipfile is for a vcs dependency.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_window_position(self, windowHandle='current'):\n        \"\"\"\n        \n        \"\"\"\n        if self.w3c:\n            if windowHandle != 'current':\n                warnings.warn(\"Only 'current' window is supported for W3C compatibile browsers.\")\n            position = self.get_window_rect()\n        else:\n            position = self.execute(Command.GET_WINDOW_POSITION,\n                                    {'windowHandle': windowHandle})['value']\n\n        return {k: position[k] for k in ('x', 'y')}", "output": "Gets the x,y position of the current window.\n\n        :Usage:\n            ::\n\n                driver.get_window_position()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_clone(call=None, kwargs=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The image_clone function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    image_id = kwargs.get('image_id', None)\n    image_name = kwargs.get('image_name', None)\n\n    if name is None:\n        raise SaltCloudSystemExit(\n            'The image_clone function requires a \\'name\\' to be provided.'\n        )\n\n    if image_id:\n        if image_name:\n            log.warning(\n                'Both the \\'image_id\\' and \\'image_name\\' arguments were provided. '\n                '\\'image_id\\' will take precedence.'\n            )\n    elif image_name:\n        image_id = get_image_id(kwargs={'name': image_name})\n    else:\n        raise SaltCloudSystemExit(\n            'The image_clone function requires either an \\'image_id\\' or an '\n            '\\'image_name\\' to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    response = server.one.image.clone(auth, int(image_id), name)\n\n    data = {\n        'action': 'image.clone',\n        'cloned': response[0],\n        'cloned_image_id': response[1],\n        'cloned_image_name': name,\n        'error_code': response[2],\n    }\n\n    return data", "output": "Clones an existing image.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the new image.\n\n    image_id\n        The ID of the image to be cloned. Can be used instead of ``image_name``.\n\n    image_name\n        The name of the image to be cloned. Can be used instead of ``image_id``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f image_clone opennebula name=my-new-image image_id=10\n        salt-cloud -f image_clone opennebula name=my-new-image image_name=my-image-to-clone", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_value(self, value):\n        \"\"\"\"\"\"\n        self.value = value\n        if self.isVisible():\n            self.label_value.setText(value)", "output": "Set formatted text value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_remove_readonly(func, path, exc):\n    \"\"\"\"\"\"\n    # Check for read-only attribute\n    default_warning_message = (\n        \"Unable to remove file due to permissions restriction: {!r}\"\n    )\n    # split the initial exception out into its type, exception, and traceback\n    exc_type, exc_exception, exc_tb = exc\n    if is_readonly_path(path):\n        # Apply write permission and call original function\n        set_write_bit(path)\n        try:\n            func(path)\n        except (OSError, IOError) as e:\n            if e.errno in [errno.EACCES, errno.EPERM]:\n                warnings.warn(default_warning_message.format(path), ResourceWarning)\n                return\n\n    if exc_exception.errno in [errno.EACCES, errno.EPERM]:\n        warnings.warn(default_warning_message.format(path), ResourceWarning)\n        return\n\n    raise exc", "output": "Error handler for shutil.rmtree.\n\n    Windows source repo folders are read-only by default, so this error handler\n    attempts to set them as writeable and then proceed with deletion.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_predictions_to_image_summaries(hook_args):\n  \"\"\"\n  \"\"\"\n  decode_hparams = hook_args.decode_hparams\n  if not decode_hparams.display_decoded_images:\n    return []\n  predictions = hook_args.predictions[0]\n\n  # Display ten random inputs and outputs so that tensorboard does not hang.\n  all_summaries = []\n  rand_predictions = np.random.choice(predictions, size=10)\n  for ind, prediction in enumerate(rand_predictions):\n    output_summary = image_to_tf_summary_value(\n        prediction[\"outputs\"], tag=\"%d_output\" % ind)\n    input_summary = image_to_tf_summary_value(\n        prediction[\"inputs\"], tag=\"%d_input\" % ind)\n    all_summaries.append(input_summary)\n    all_summaries.append(output_summary)\n  return all_summaries", "output": "Optionally converts images from hooks_args to image summaries.\n\n  Args:\n    hook_args: DecodeHookArgs namedtuple\n  Returns:\n    summaries: list of tf.Summary values if hook_args.decode_hpara", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def universal_transformer_base():\n  \"\"\"\"\"\"\n  hparams = transformer.transformer_base()\n  # To have a similar capacity to the transformer_base with 6 layers,\n  # we need to increase the size of the UT's layer\n  # since, in fact, UT has a single layer repeating multiple times.\n  hparams.hidden_size = 1024\n  hparams.filter_size = 4096\n  hparams.num_heads = 16\n  hparams.layer_prepostprocess_dropout = 0.3\n  hparams = update_hparams_for_universal_transformer(hparams)\n  return hparams", "output": "Base parameters for Universal Transformer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_depthtospace(node, **kwargs):\n    \"\"\"\n    \"\"\"\n    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    blksize = int(attrs.get(\"block_size\", 0))\n\n    node = onnx.helper.make_node(\n        \"DepthToSpace\",\n        input_nodes,\n        [name],\n        blocksize=blksize,\n        name=name,\n    )\n    return [node]", "output": "Map MXNet's depth_to_space operator attributes to onnx's\n    DepthToSpace operator and return the created node.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_cursor(self):\n        '''\n        \n        '''\n        _options = self._get_options()\n        conn = psycopg2.connect(host=_options['host'],\n                                user=_options['user'],\n                                password=_options['pass'],\n                                dbname=_options['db'],\n                                port=_options['port'])\n        cursor = conn.cursor()\n        try:\n            yield cursor\n            log.debug('Connected to POSTGRES DB')\n        except psycopg2.DatabaseError as err:\n            log.exception('Error in ext_pillar POSTGRES: %s', err.args)\n        finally:\n            conn.close()", "output": "Yield a POSTGRES cursor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def withColumnRenamed(self, existing, new):\n        \"\"\"\n        \"\"\"\n        return DataFrame(self._jdf.withColumnRenamed(existing, new), self.sql_ctx)", "output": "Returns a new :class:`DataFrame` by renaming an existing column.\n        This is a no-op if schema doesn't contain the given column name.\n\n        :param existing: string, name of the existing column to rename.\n        :param new: string, new name of the column.\n\n        >>> df.withColumnRenamed('age', 'age2').collect()\n        [Row(age2=2, name=u'Alice'), Row(age2=5, name=u'Bob')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_job_status(self, job_id):\n        \"\"\"\n        \"\"\"\n        response = self._client.describe_jobs(jobs=[job_id])\n\n        # Error checking\n        status_code = response['ResponseMetadata']['HTTPStatusCode']\n        if status_code != 200:\n            msg = 'Job status request received status code {0}:\\n{1}'\n            raise Exception(msg.format(status_code, response))\n\n        return response['jobs'][0]['status']", "output": "Retrieve task statuses from ECS API\n\n        :param job_id (str): AWS Batch job uuid\n\n        Returns one of {SUBMITTED|PENDING|RUNNABLE|STARTING|RUNNING|SUCCEEDED|FAILED}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def findinfiles_callback(self):\r\n        \"\"\"\"\"\"\r\n        widget = QApplication.focusWidget()\r\n        if not self.ismaximized:\r\n            self.dockwidget.setVisible(True)\r\n            self.dockwidget.raise_()\r\n        text = ''\r\n        try:\r\n            if widget.has_selected_text():\r\n                text = widget.get_selected_text()\r\n        except AttributeError:\r\n            # This is not a text widget deriving from TextEditBaseWidget\r\n            pass\r\n        self.findinfiles.set_search_text(text)\r\n        if text:\r\n            self.findinfiles.find()", "output": "Find in files callback", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keyevent2tuple(event):\r\n    \"\"\"\"\"\"\r\n    return (event.type(), event.key(), event.modifiers(), event.text(),\r\n            event.isAutoRepeat(), event.count())", "output": "Convert QKeyEvent instance into a tuple", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def construct_scalar(self, node):\n        '''\n        \n        '''\n        if node.tag == 'tag:yaml.org,2002:int':\n            if node.value == '0':\n                pass\n            elif node.value.startswith('0') and not node.value.startswith(('0b', '0x')):\n                node.value = node.value.lstrip('0')\n                # If value was all zeros, node.value would have been reduced to\n                # an empty string. Change it to '0'.\n                if node.value == '':\n                    node.value = '0'\n        return super(SaltYamlSafeLoader, self).construct_scalar(node)", "output": "Verify integers and pass them in correctly is they are declared\n        as octal", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_netD():\n    \"\"\"\"\"\"\n    # build the discriminator\n    netD = nn.Sequential()\n    with netD.name_scope():\n        # input is (nc) x 64 x 64\n        netD.add(nn.Conv2D(ndf, 4, 2, 1, use_bias=False))\n        netD.add(nn.LeakyReLU(0.2))\n        # state size. (ndf) x 32 x 32\n        netD.add(nn.Conv2D(ndf * 2, 4, 2, 1, use_bias=False))\n        netD.add(nn.BatchNorm())\n        netD.add(nn.LeakyReLU(0.2))\n        # state size. (ndf*2) x 16 x 16\n        netD.add(nn.Conv2D(ndf * 4, 4, 2, 1, use_bias=False))\n        netD.add(nn.BatchNorm())\n        netD.add(nn.LeakyReLU(0.2))\n        # state size. (ndf*4) x 8 x 8\n        netD.add(nn.Conv2D(ndf * 8, 4, 2, 1, use_bias=False))\n        netD.add(nn.BatchNorm())\n        netD.add(nn.LeakyReLU(0.2))\n        # state size. (ndf*8) x 4 x 4\n        netD.add(nn.Conv2D(2, 4, 1, 0, use_bias=False))\n        # state size. 2 x 1 x 1\n\n    return netD", "output": "Get the netD", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lambda_handler(event, context):\n    '''\n    '''\n    #print(\"Received event: \" + json.dumps(event, indent=2))\n\n    operations = {\n        'DELETE': lambda dynamo, x: dynamo.delete_item(**x),\n        'GET': lambda dynamo, x: dynamo.scan(**x),\n        'POST': lambda dynamo, x: dynamo.put_item(**x),\n        'PUT': lambda dynamo, x: dynamo.update_item(**x),\n    }\n\n    operation = event['httpMethod']\n    if operation in operations:\n        payload = event['queryStringParameters'] if operation == 'GET' else json.loads(event['body'])\n        return respond(None, operations[operation](dynamo, payload))\n    else:\n        return respond(ValueError('Unsupported method \"{}\"'.format(operation)))", "output": "Demonstrates a simple HTTP endpoint using API Gateway. You have full\n    access to the request and response payload, including headers and\n    status code.\n\n    To scan a DynamoDB table, make a GET request with the TableName as a\n    query string parameter. To put, update, or delete an item, make a POST,\n    PUT, or DELETE request respectively, passing in the payload to the\n    DynamoDB API as a JSON body.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def countApproxDistinct(self, relativeSD=0.05):\n        \"\"\"\n        \n        \"\"\"\n        if relativeSD < 0.000017:\n            raise ValueError(\"relativeSD should be greater than 0.000017\")\n        # the hash space in Java is 2^32\n        hashRDD = self.map(lambda x: portable_hash(x) & 0xFFFFFFFF)\n        return hashRDD._to_java_object_rdd().countApproxDistinct(relativeSD)", "output": ".. note:: Experimental\n\n        Return approximate number of distinct elements in the RDD.\n\n        The algorithm used is based on streamlib's implementation of\n        `\"HyperLogLog in Practice: Algorithmic Engineering of a State\n        of The Art Cardinality Estimation Algorithm\", available here\n        <https://doi.org/10.1145/2452376.2452456>`_.\n\n        :param relativeSD: Relative accuracy. Smaller values create\n                           counters that require more space.\n                           It must be greater than 0.000017.\n\n        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n        >>> 900 < n < 1100\n        True\n        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n        >>> 16 < n < 24\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_exists(self, dataset):\n        \"\"\"\n        \"\"\"\n\n        try:\n            response = self.client.datasets().get(projectId=dataset.project_id,\n                                                  datasetId=dataset.dataset_id).execute()\n            if dataset.location is not None:\n                fetched_location = response.get('location')\n                if dataset.location != fetched_location:\n                    raise Exception('''Dataset already exists with regional location {}. Can't use {}.'''.format(\n                        fetched_location if fetched_location is not None else 'unspecified',\n                        dataset.location))\n\n        except http.HttpError as ex:\n            if ex.resp.status == 404:\n                return False\n            raise\n\n        return True", "output": "Returns whether the given dataset exists.\n        If regional location is specified for the dataset, that is also checked\n        to be compatible with the remote dataset, otherwise an exception is thrown.\n\n           :param dataset:\n           :type dataset: BQDataset", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_policy_version(policyName, policyVersionId,\n             region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        policy = conn.get_policy_version(policyName=policyName,\n                                         policyVersionId=policyVersionId)\n        if policy:\n            keys = ('policyName', 'policyArn', 'policyDocument',\n                    'policyVersionId', 'isDefaultVersion')\n            return {'policy': dict([(k, policy.get(k)) for k in keys])}\n        else:\n            return {'policy': None}\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if e.response.get('Error', {}).get('Code') == 'ResourceNotFoundException':\n            return {'policy': None}\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Given a policy name and version describe its properties.\n\n    Returns a dictionary of interesting properties.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.describe_policy_version mypolicy version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contrast(x, severity=1):\n  \"\"\"\n  \"\"\"\n  c = [0.4, .3, .2, .1, .05][severity - 1]\n\n  x = np.array(x) / 255.\n  means = np.mean(x, axis=(0, 1), keepdims=True)\n  x_clip = np.clip((x - means) * c + means, 0, 1) * 255\n  return around_and_astype(x_clip)", "output": "Change contrast of images.\n\n  Args:\n    x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255].\n    severity: integer, severity of corruption.\n\n  Returns:\n    numpy array, image with uint8 pixels in [0,255]. Changed contrast.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metric_get(self, project, metric_name):\n        \"\"\"\n        \"\"\"\n        path = \"projects/%s/metrics/%s\" % (project, metric_name)\n        metric_pb = self._gapic_api.get_log_metric(path)\n        # NOTE: LogMetric message type does not have an ``Any`` field\n        #       so `MessageToDict`` can safely be used.\n        return MessageToDict(metric_pb)", "output": "API call:  retrieve a metric resource.\n\n        :type project: str\n        :param project: ID of the project containing the metric.\n\n        :type metric_name: str\n        :param metric_name: the name of the metric\n\n        :rtype: dict\n        :returns: The metric object returned from the API (converted from a\n                  protobuf to a dictionary).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _construct_return_type(self, result, axes=None):\n        \"\"\"\n        \n        \"\"\"\n        ndim = getattr(result, 'ndim', None)\n\n        # need to assume they are the same\n        if ndim is None:\n            if isinstance(result, dict):\n                ndim = getattr(list(result.values())[0], 'ndim', 0)\n\n                # have a dict, so top-level is +1 dim\n                if ndim != 0:\n                    ndim += 1\n\n        # scalar\n        if ndim == 0:\n            return Series(result)\n\n        # same as self\n        elif self.ndim == ndim:\n            # return the construction dictionary for these axes\n            if axes is None:\n                return self._constructor(result)\n            return self._constructor(result, **self._construct_axes_dict())\n\n        # sliced\n        elif self.ndim == ndim + 1:\n            if axes is None:\n                return self._constructor_sliced(result)\n            return self._constructor_sliced(\n                result, **self._extract_axes_for_slice(self, axes))\n\n        raise ValueError('invalid _construct_return_type [self->{self}] '\n                         '[result->{result}]'.format(self=self, result=result))", "output": "Return the type for the ndim of the result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_session(target='', timeout_sec=10):\n  '''\n  '''\n  graph = tf.Graph()\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  config.operation_timeout_in_ms = int(timeout_sec*1000)\n  return tf.InteractiveSession(target=target, graph=graph, config=config)", "output": "Create an intractive TensorFlow session.\n\n  Helper function that creates TF session that uses growing GPU memory\n  allocation and opration timeout. 'allow_growth' flag prevents TF\n  from allocating the whole GPU memory an once, which is useful\n  when having multiple python sessions sharing the same GPU.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_train_examples(self, data_dir):\n        \"\"\"\"\"\"\n        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, \"train.tsv\")))\n        return self._create_examples(\n            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")", "output": "See base class.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_opener(self, name):\n        \"\"\"\n\n        \"\"\"\n        if name not in self.registry:\n            raise NoOpenerError(\"No opener for %s\" % name)\n        index = self.registry[name]\n        return self.openers[index]", "output": "Retrieve an opener for the given protocol\n\n        :param name: name of the opener to open\n        :type name: string\n        :raises NoOpenerError: if no opener has been registered of that name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dependencies(ireq, sources=None, parent=None):\n    # type: (Union[InstallRequirement, InstallationCandidate], Optional[List[Dict[S, Union[S, bool]]]], Optional[AbstractDependency]) -> Set[S, ...]\n    \"\"\"\n    \"\"\"\n    if not isinstance(ireq, pip_shims.shims.InstallRequirement):\n        name = getattr(\n            ireq, \"project_name\",\n            getattr(ireq, \"project\", ireq.name),\n        )\n        version = getattr(ireq, \"version\", None)\n        if not version:\n            ireq = pip_shims.shims.InstallRequirement.from_line(\"{0}\".format(name))\n        else:\n            ireq = pip_shims.shims.InstallRequirement.from_line(\"{0}=={1}\".format(name, version))\n    pip_options = get_pip_options(sources=sources)\n    getters = [\n        get_dependencies_from_cache,\n        get_dependencies_from_wheel_cache,\n        get_dependencies_from_json,\n        functools.partial(get_dependencies_from_index, pip_options=pip_options)\n    ]\n    for getter in getters:\n        deps = getter(ireq)\n        if deps is not None:\n            return deps\n    raise RuntimeError('failed to get dependencies for {}'.format(ireq))", "output": "Get all dependencies for a given install requirement.\n\n    :param ireq: A single InstallRequirement\n    :type ireq: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :param sources: Pipfile-formatted sources, defaults to None\n    :type sources: list[dict], optional\n    :param parent: The parent of this list of dependencies, defaults to None\n    :type parent: :class:`~pip._internal.req.req_install.InstallRequirement`\n    :return: A set of dependency lines for generating new InstallRequirements.\n    :rtype: set(str)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_scalar_add(net, node, model, builder):\n    \"\"\"\n    \"\"\"\n    import numpy as _np\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    param = _get_attr(node)\n    mode = 'ADD'\n    alpha = _np.array([float(param['scalar'])])\n    builder.add_scale(name = name, input_name = input_name,\n            output_name = output_name, W = _np.array([1.0]), b = alpha, has_bias=True)", "output": "Convert a scalar add layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    net: network\n        A mxnet network object.\n\n    node: layer\n        Node to convert.\n\n    model: model\n        An model for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_name_func(cls, path:PathOrStr, fnames:FilePathList, label_func:Callable, valid_pct:float=0.2, **kwargs):\n        \"\"\n        src = ImageList(fnames, path=path).split_by_rand_pct(valid_pct)\n        return cls.create_from_ll(src.label_from_func(label_func), **kwargs)", "output": "Create from list of `fnames` in `path` with `label_func`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format_raw_script(raw_script):\n    \"\"\"\n\n    \"\"\"\n    if six.PY2:\n        script = ' '.join(arg.decode('utf-8') for arg in raw_script)\n    else:\n        script = ' '.join(raw_script)\n\n    return script.strip()", "output": "Creates single script from a list of script parts.\n\n    :type raw_script: [basestring]\n    :rtype: basestring", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': True, 'comment': '', 'changes': {}}\n    if not __salt__['boto_cfn.exists'](name, region, key, keyid, profile):\n        ret['comment'] = 'Stack {0} does not exist.'.format(name)\n        ret['changes'] = {}\n        return ret\n    if __opts__['test']:\n        ret['comment'] = 'Stack {0} is set to be deleted.'.format(name)\n        ret['result'] = None\n        return ret\n    deleted = __salt__['boto_cfn.delete'](name, region, key, keyid, profile)\n    if isinstance(deleted, six.string_types):\n        code, message = _get_error(deleted)\n        ret['comment'] = 'Stack {0} could not be deleted.\\n{1}\\n{2}'.format(name, code, message)\n        ret['result'] = False\n        ret['changes'] = {}\n        return ret\n    if deleted:\n        ret['comment'] = 'Stack {0} was deleted.'.format(name)\n        ret['changes']['deleted'] = name\n        return ret", "output": "Ensure cloud formation stack is absent.\n\n    name (string) \u2013 The name of the stack to delete.\n\n    region (string) - Region to connect to.\n\n    key (string) - Secret key to be used.\n\n    keyid (string) - Access key to be used.\n\n    profile (dict) - A dict with region, key and keyid, or a pillar key (string) that contains a dict with region, key\n    and keyid.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_value(self, index, col, takeable=False):\n        \"\"\"\n        \n        \"\"\"\n\n        warnings.warn(\"get_value is deprecated and will be removed \"\n                      \"in a future release. Please use \"\n                      \".at[] or .iat[] accessors instead\", FutureWarning,\n                      stacklevel=2)\n        return self._get_value(index, col, takeable=takeable)", "output": "Quickly retrieve single value at passed column and index.\n\n        .. deprecated:: 0.21.0\n            Use .at[] or .iat[] accessors instead.\n\n        Parameters\n        ----------\n        index : row label\n        col : column label\n        takeable : interpret the index/col as indexers, default False\n\n        Returns\n        -------\n        scalar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def create_dm(self):\n        \"\"\"\n        \"\"\"\n        found = self.dm_channel\n        if found is not None:\n            return found\n\n        state = self._state\n        data = await state.http.start_private_message(self.id)\n        return state.add_dm_channel(data)", "output": "Creates a :class:`DMChannel` with this user.\n\n        This should be rarely called, as this is done transparently for most\n        people.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def post_attention(self, token, x):\n    \"\"\"\n    \"\"\"\n    with tf.variable_scope(self.name + \"/post_attention\", reuse=tf.AUTO_REUSE):\n      depth = common_layers.shape_list(x)[-1]\n      actual_batch_size = common_layers.shape_list(x)[0]\n      memory_output = tf.gather(token[\"retrieved_mem\"],\n                                tf.range(actual_batch_size))\n      output = tf.add(tf.layers.dense(x, depth, use_bias=False),\n                      tf.layers.dense(memory_output, depth))\n      with tf.control_dependencies([output]):\n        with tf.control_dependencies([\n            self.write(token[\"x\"], token[\"access_logits\"])]):\n          return tf.identity(output)", "output": "Called after self-attention. The memory can be updated here.\n\n    Args:\n      token: Data returned by pre_attention, which can be used to carry over\n        state related to the current memory operation.\n      x: a Tensor of data after self-attention and feed-forward\n    Returns:\n      a (possibly modified) version of the input x", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def proc_fvals(self, fvals):\n    \"\"\"\n    \n    \"\"\"\n    inputs = self.inputs\n    outputs = self.outputs\n\n    # Move data to the next sub-graph for the next step\n    cur = 0\n    for i in range(len(inputs)-1):\n      if not self.active_gpus[i]:\n        self.next_vals[i+1] = None\n        continue\n      self.next_vals[i+1] = OrderedDict()\n      for k in outputs[i]:\n        self.next_vals[i+1][k] = fvals[cur]\n        cur += 1\n      if i == 0:\n        self.next_vals[0] = None\n\n    # Return the output of the last sub-graph\n    last_fvals = OrderedDict()\n    if self.active_gpus[-1]:\n      assert cur+len(outputs[-1]) == len(fvals)\n      for k in outputs[-1]:\n        last_fvals[k] = fvals[cur]\n        cur += 1\n    return last_fvals", "output": "Postprocess the outputs of the Session.run(). Move the outputs of\n    sub-graphs to next ones and return the output of the last sub-graph.\n\n    :param fvals: A list of fetched values returned by Session.run()\n    :return: A dictionary of fetched values returned by the last sub-graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh_metrics(self):\n        \"\"\"\"\"\"\n        metrics = self.get_metrics()\n        dbmetrics = (\n            db.session.query(DruidMetric)\n            .filter(DruidMetric.datasource_id == self.datasource_id)\n            .filter(DruidMetric.metric_name.in_(metrics.keys()))\n        )\n        dbmetrics = {metric.metric_name: metric for metric in dbmetrics}\n        for metric in metrics.values():\n            dbmetric = dbmetrics.get(metric.metric_name)\n            if dbmetric:\n                for attr in ['json', 'metric_type']:\n                    setattr(dbmetric, attr, getattr(metric, attr))\n            else:\n                with db.session.no_autoflush:\n                    metric.datasource_id = self.datasource_id\n                    db.session.add(metric)", "output": "Refresh metrics based on the column metadata", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_dict(self):\n        \"\"\"\n        \n        \"\"\"\n        request_context_dict = {}\n        if self.request_context:\n            request_context_dict = self.request_context.to_dict()\n\n        json_dict = {\"httpMethod\": self.http_method,\n                     \"body\": self.body if self.body else None,\n                     \"resource\": self.resource,\n                     \"requestContext\": request_context_dict,\n                     \"queryStringParameters\": dict(self.query_string_params) if self.query_string_params else None,\n                     \"headers\": dict(self.headers) if self.headers else None,\n                     \"pathParameters\": dict(self.path_parameters) if self.path_parameters else None,\n                     \"stageVariables\": dict(self.stage_variables) if self.stage_variables else None,\n                     \"path\": self.path,\n                     \"isBase64Encoded\": self.is_base_64_encoded\n                     }\n\n        return json_dict", "output": "Constructs an dictionary representation of the ApiGatewayLambdaEvent Object to be used in serializing to JSON\n\n        :return: dict representing the object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(self, local_path, path, atomic=True):\n        \"\"\"\n        \n        \"\"\"\n        self._connect()\n\n        if self.sftp:\n            self._sftp_put(local_path, path, atomic)\n        else:\n            self._ftp_put(local_path, path, atomic)\n\n        self._close()", "output": "Put file from local filesystem to (s)FTP.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_env_per_bucket():\n    '''\n    \n    '''\n\n    buckets = _get_buckets()\n    if isinstance(buckets, dict):\n        return True\n    elif isinstance(buckets, list):\n        return False\n    else:\n        raise ValueError('Incorrect s3.buckets type given in config')", "output": "Return the configuration mode, either buckets per environment or a list of\n    buckets that have environment dirs in their root", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def checkpoint(self):\n        \"\"\"\n        \"\"\"\n        if not self._metadata_checkpoint_dir:\n            return\n        metadata_checkpoint_dir = self._metadata_checkpoint_dir\n        if not os.path.exists(metadata_checkpoint_dir):\n            os.makedirs(metadata_checkpoint_dir)\n        runner_state = {\n            \"checkpoints\": list(\n                self.trial_executor.get_checkpoints().values()),\n            \"runner_data\": self.__getstate__(),\n            \"timestamp\": time.time()\n        }\n        tmp_file_name = os.path.join(metadata_checkpoint_dir,\n                                     \".tmp_checkpoint\")\n        with open(tmp_file_name, \"w\") as f:\n            json.dump(runner_state, f, indent=2, cls=_TuneFunctionEncoder)\n\n        os.rename(\n            tmp_file_name,\n            os.path.join(metadata_checkpoint_dir,\n                         TrialRunner.CKPT_FILE_TMPL.format(self._session_str)))\n        return metadata_checkpoint_dir", "output": "Saves execution state to `self._metadata_checkpoint_dir`.\n\n        Overwrites the current session checkpoint, which starts when self\n        is instantiated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(path):\n  \"\"\"\"\"\"\n  proto = _parse_saved_model(path)\n  _merge_assets_key_collection(proto, path)\n  handler = SavedModelHandler()\n  handler._proto = proto  # pylint: disable=protected-access\n  return handler", "output": "Creates a SavedModelHandler from a SavedModel in `path`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sanitize_win_path(winpath):\n    '''\n    \n    '''\n    intab = '<>:|?*'\n    if isinstance(winpath, six.text_type):\n        winpath = winpath.translate(dict((ord(c), '_') for c in intab))\n    elif isinstance(winpath, six.string_types):\n        outtab = '_' * len(intab)\n        trantab = ''.maketrans(intab, outtab) if six.PY3 else string.maketrans(intab, outtab)  # pylint: disable=no-member\n        winpath = winpath.translate(trantab)\n    return winpath", "output": "Remove illegal path characters for windows", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_index(self, **kwargs):\n        \"\"\"\n        \"\"\"\n        drop = kwargs.get(\"drop\", False)\n        new_index = pandas.RangeIndex(len(self.index))\n        if not drop:\n            if isinstance(self.index, pandas.MultiIndex):\n                # TODO (devin-petersohn) ensure partitioning is properly aligned\n                new_column_names = pandas.Index(self.index.names)\n                new_columns = new_column_names.append(self.columns)\n                index_data = pandas.DataFrame(list(zip(*self.index))).T\n                result = self.data.from_pandas(index_data).concat(1, self.data)\n                return self.__constructor__(result, new_index, new_columns)\n            else:\n                new_column_name = (\n                    self.index.name\n                    if self.index.name is not None\n                    else \"index\"\n                    if \"index\" not in self.columns\n                    else \"level_0\"\n                )\n                new_columns = self.columns.insert(0, new_column_name)\n                result = self.insert(0, new_column_name, self.index)\n                return self.__constructor__(result.data, new_index, new_columns)\n        else:\n            # The copies here are to ensure that we do not give references to\n            # this object for the purposes of updates.\n            return self.__constructor__(\n                self.data.copy(), new_index, self.columns.copy(), self._dtype_cache\n            )", "output": "Removes all levels from index and sets a default level_0 index.\n\n        Returns:\n            A new QueryCompiler with updated data and reset index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predict(self, dataset, missing_value_action='auto'):\n        \"\"\"\n        \n        \"\"\"\n\n        return super(LinearRegression, self).predict(dataset, missing_value_action=missing_value_action)", "output": "Return target value predictions for ``dataset``, using the trained\n        linear regression model. This method can be used to get fitted values\n        for the model by inputting the training dataset.\n\n        Parameters\n        ----------\n        dataset : SFrame | pandas.Dataframe\n            Dataset of new observations. Must include columns with the same\n            names as the features used for model training, but does not require\n            a target column. Additional columns are ignored.\n\n        missing_value_action : str, optional\n            Action to perform when missing values are encountered. This can be\n            one of:\n\n            - 'auto': Default to 'impute'\n            - 'impute': Proceed with evaluation by filling in the missing\n              values with the mean of the training data. Missing\n              values are also imputed if an entire column of data is\n              missing during evaluation.\n            - 'error': Do not proceed with prediction and terminate with\n              an error message.\n\n\n        Returns\n        -------\n        out : SArray\n            Predicted target value for each example (i.e. row) in the dataset.\n\n        See Also\n        ----------\n        create, evaluate\n\n        Examples\n        ----------\n        >>> data =  turicreate.SFrame('https://static.turi.com/datasets/regression/houses.csv')\n\n        >>> model = turicreate.linear_regression.create(data,\n                                             target='price',\n                                             features=['bath', 'bedroom', 'size'])\n        >>> results = model.predict(data)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recentProgress(self):\n        \"\"\"\n        \"\"\"\n        return [json.loads(p.json()) for p in self._jsq.recentProgress()]", "output": "Returns an array of the most recent [[StreamingQueryProgress]] updates for this query.\n        The number of progress updates retained for each stream is configured by Spark session\n        configuration `spark.sql.streaming.numRecentProgressUpdates`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def growByteBuffer(self):\n        \"\"\"\"\"\"\n        if len(self.Bytes) == Builder.MAX_BUFFER_SIZE:\n            msg = \"flatbuffers: cannot grow buffer beyond 2 gigabytes\"\n            raise BuilderSizeError(msg)\n\n        newSize = min(len(self.Bytes) * 2, Builder.MAX_BUFFER_SIZE)\n        if newSize == 0:\n            newSize = 1\n        bytes2 = bytearray(newSize)\n        bytes2[newSize-len(self.Bytes):] = self.Bytes\n        self.Bytes = bytes2", "output": "Doubles the size of the byteslice, and copies the old data towards\n           the end of the new buffer (since we build the buffer backwards).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ll_pre_transform(self, train_tfm:List[Callable], valid_tfm:List[Callable]):\n    \"\"\n    self.train.x.after_open = compose(train_tfm)\n    self.valid.x.after_open = compose(valid_tfm)\n    return self", "output": "Call `train_tfm` and `valid_tfm` after opening image, before converting from `PIL.Image`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def visit_list(self, node, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        rv = self.visit(node, *args, **kwargs)\n        if not isinstance(rv, list):\n            rv = [rv]\n        return rv", "output": "As transformers may return lists in some places this method\n        can be used to enforce a list as return value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_all(self, layers, force=False):\n        \"\"\"\n        \n        \"\"\"\n        layer_dirs = []\n        for layer in layers:\n            layer_dirs.append(self.download(layer, force))\n\n        return layer_dirs", "output": "Download a list of layers to the cache\n\n        Parameters\n        ----------\n        layers list(samcli.commands.local.lib.provider.Layer)\n            List of Layers representing the layer to be downloaded\n        force bool\n            True to download the layer even if it exists already on the system\n\n        Returns\n        -------\n        List(Path)\n            List of Paths to where the layer was cached", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subscription_get(subscription_id=None, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n\n    if not subscription_id:\n        subscription_id = kwargs.get('subscription_id')\n    elif not kwargs.get('subscription_id'):\n        kwargs['subscription_id'] = subscription_id\n\n    subconn = __utils__['azurearm.get_client']('subscription', **kwargs)\n    try:\n        subscription = subconn.subscriptions.get(\n            subscription_id=kwargs.get('subscription_id')\n        )\n\n        result = subscription.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get details about a subscription.\n\n    :param subscription_id: The ID of the subscription to query.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.subscription_get XXXXXXXX", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_buffers(self, conn, locked=True):\n        ''' \n\n        '''\n        if conn is None:\n            raise ValueError(\"Cannot write_buffers to connection None\")\n        sent = 0\n        for header, payload in self._buffers:\n            yield conn.write_message(header, locked=locked)\n            yield conn.write_message(payload, binary=True, locked=locked)\n            sent += (len(header) + len(payload))\n        raise gen.Return(sent)", "output": "Write any buffer headers and payloads to the given connection.\n\n        Args:\n            conn (object) :\n                May be any object with a ``write_message`` method. Typically,\n                a Tornado ``WSHandler`` or ``WebSocketClientConnection``\n\n            locked (bool) :\n\n        Returns:\n            int : number of bytes sent", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def docCopyNode(self, doc, extended):\n        \"\"\" \"\"\"\n        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        ret = libxml2mod.xmlDocCopyNode(self._o, doc__o, extended)\n        if ret is None:raise treeError('xmlDocCopyNode() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Do a copy of the node to a given document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def count(self, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_cat', 'count',\n            index), params=params)", "output": "Count provides quick access to the document count of the entire cluster,\n        or individual indices.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-count.html>`_\n\n        :arg index: A comma-separated list of index names to limit the returned\n            information\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg v: Verbose mode. Display column headers, default False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ignore_unicode_prefix(f):\n    \"\"\"\n    \n    \"\"\"\n    if sys.version >= '3':\n        # the representation of unicode string in Python 3 does not have prefix 'u',\n        # so remove the prefix 'u' for doc tests\n        literal_re = re.compile(r\"(\\W|^)[uU](['])\", re.UNICODE)\n        f.__doc__ = literal_re.sub(r'\\1\\2', f.__doc__)\n    return f", "output": "Ignore the 'u' prefix of string in doc tests, to make it works\n    in both python 2 and 3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_task(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        task_id = kwargs['task_id']\n        status = kwargs['status']\n        runnable = kwargs['runnable']\n        task = self._scheduled_tasks.get(task_id)\n        if task:\n            self._add_task_history.append((task, status, runnable))\n            kwargs['owners'] = task._owner_list()\n\n        if task_id in self._batch_running_tasks:\n            for batch_task in self._batch_running_tasks.pop(task_id):\n                self._add_task_history.append((batch_task, status, True))\n\n        if task and kwargs.get('params'):\n            kwargs['param_visibilities'] = task._get_param_visibilities()\n\n        self._scheduler.add_task(*args, **kwargs)\n\n        logger.info('Informed scheduler that task   %s   has status   %s', task_id, status)", "output": "Call ``self._scheduler.add_task``, but store the values too so we can\n        implement :py:func:`luigi.execution_summary.summary`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def locate(substr, str, pos=1):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.locate(substr, _to_java_column(str), pos))", "output": "Locate the position of the first occurrence of substr in a string column, after position pos.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n        could not be found in str.\n\n    :param substr: a string\n    :param str: a Column of :class:`pyspark.sql.types.StringType`\n    :param pos: start position (zero based)\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n    [Row(s=2)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def swaplevel(self, i=-2, j=-1):\n        \"\"\"\n        \n        \"\"\"\n        new_levels = list(self.levels)\n        new_codes = list(self.codes)\n        new_names = list(self.names)\n\n        i = self._get_level_number(i)\n        j = self._get_level_number(j)\n\n        new_levels[i], new_levels[j] = new_levels[j], new_levels[i]\n        new_codes[i], new_codes[j] = new_codes[j], new_codes[i]\n        new_names[i], new_names[j] = new_names[j], new_names[i]\n\n        return MultiIndex(levels=new_levels, codes=new_codes,\n                          names=new_names, verify_integrity=False)", "output": "Swap level i with level j.\n\n        Calling this method does not change the ordering of the values.\n\n        Parameters\n        ----------\n        i : int, str, default -2\n            First level of index to be swapped. Can pass level name as string.\n            Type of parameters can be mixed.\n        j : int, str, default -1\n            Second level of index to be swapped. Can pass level name as string.\n            Type of parameters can be mixed.\n\n        Returns\n        -------\n        MultiIndex\n            A new MultiIndex.\n\n        .. versionchanged:: 0.18.1\n\n           The indexes ``i`` and ``j`` are now optional, and default to\n           the two innermost levels of the index.\n\n        See Also\n        --------\n        Series.swaplevel : Swap levels i and j in a MultiIndex.\n        Dataframe.swaplevel : Swap levels i and j in a MultiIndex on a\n            particular axis.\n\n        Examples\n        --------\n        >>> mi = pd.MultiIndex(levels=[['a', 'b'], ['bb', 'aa']],\n        ...                    codes=[[0, 0, 1, 1], [0, 1, 0, 1]])\n        >>> mi\n        MultiIndex(levels=[['a', 'b'], ['bb', 'aa']],\n                   codes=[[0, 0, 1, 1], [0, 1, 0, 1]])\n        >>> mi.swaplevel(0, 1)\n        MultiIndex(levels=[['bb', 'aa'], ['a', 'b']],\n                   codes=[[0, 1, 0, 1], [0, 0, 1, 1]])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_bn_weight(layer):\n    '''\n    '''\n    n_filters = layer.num_features\n    new_weights = [\n        add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n    ]\n    layer.set_weights(new_weights)", "output": "initilize batch norm layer weight.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute_code(self, lines, current_client=True, clear_variables=False):\r\n        \"\"\"\"\"\"\r\n        sw = self.get_current_shellwidget()\r\n        if sw is not None:\r\n            if sw._reading:\r\n                pass\r\n            else:\r\n                if not current_client:\r\n                    # Clear console and reset namespace for\r\n                    # dedicated clients\r\n                    # See issue 5748\r\n                    try:\r\n                        sw.sig_prompt_ready.disconnect()\r\n                    except TypeError:\r\n                        pass\r\n                    sw.reset_namespace(warning=False)\r\n                elif current_client and clear_variables:\r\n                    sw.reset_namespace(warning=False)\r\n                # Needed to handle an error when kernel_client is none\r\n                # See issue 6308\r\n                try:\r\n                    sw.execute(to_text_string(lines))\r\n                except AttributeError:\r\n                    pass\r\n            self.activateWindow()\r\n            self.get_current_client().get_control().setFocus()", "output": "Execute code instructions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lastProgress(self):\n        \"\"\"\n        \n        \"\"\"\n        lastProgress = self._jsq.lastProgress()\n        if lastProgress:\n            return json.loads(lastProgress.json())\n        else:\n            return None", "output": "Returns the most recent :class:`StreamingQueryProgress` update of this streaming query or\n        None if there were no progress updates\n        :return: a map", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def subtract(lhs, rhs):\n    \"\"\"\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_sub,\n        operator.sub,\n        _internal._minus_scalar,\n        _internal._rminus_scalar)", "output": "Returns element-wise difference of the input arrays with broadcasting.\n\n    Equivalent to ``lhs - rhs``, ``mx.nd.broadcast_sub(lhs, rhs)`` and\n    ``mx.nd.broadcast_minus(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First array to be subtracted.\n    rhs : scalar or mxnet.ndarray.array\n         Second array to be subtracted.\n        If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        The element-wise difference of the input arrays.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> (x-2).asnumpy()\n    array([[-1., -1., -1.],\n           [-1., -1., -1.]], dtype=float32)\n    >>> (x-y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> mx.nd.subtract(x,y).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 0.,  0.,  0.]], dtype=float32)\n    >>> (z-y).asnumpy()\n    array([[ 0.,  1.],\n           [-1.,  0.]], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _has_nulltype(dt):\n    \"\"\"  \"\"\"\n    if isinstance(dt, StructType):\n        return any(_has_nulltype(f.dataType) for f in dt.fields)\n    elif isinstance(dt, ArrayType):\n        return _has_nulltype((dt.elementType))\n    elif isinstance(dt, MapType):\n        return _has_nulltype(dt.keyType) or _has_nulltype(dt.valueType)\n    else:\n        return isinstance(dt, NullType)", "output": "Return whether there is NullType in `dt` or not", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def begin(self):\n    \"\"\"\n\n    \"\"\"\n    variables_to_restore = tf.contrib.framework.get_variables_to_restore(\n        include=self._include, exclude=self._exclude)\n    # remove new_model_scope from variable name prefix\n    assignment_map = {variable.name[len(self._new_model_scope):]: variable\n                      for variable in variables_to_restore\n                      if variable.name.startswith(self._new_model_scope)}\n    # remove :0 from variable name suffix\n    assignment_map = {name.split(\":\")[0]: variable\n                      for name, variable in six.iteritems(assignment_map)\n                      if name.startswith(self._old_model_scope)}\n    self._assignment_map = assignment_map\n\n    tf.logging.info(\"restoring %d variables from checkpoint %s\"%(\n        len(assignment_map), self._checkpoint_path))\n    tf.train.init_from_checkpoint(self._checkpoint_path, self._assignment_map)", "output": "Load variables from checkpoint.\n\n    New model variables have the following name foramt:\n    new_model_scope/old_model_scope/xxx/xxx:0 To find the map of\n    name to variable, need to strip the new_model_scope and then\n    match the old_model_scope and remove the suffix :0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pem_finger(path=None, key=None, sum_type='sha256'):\n    '''\n    \n    '''\n    if not key:\n        if not os.path.isfile(path):\n            return ''\n\n        with salt.utils.files.fopen(path, 'rb') as fp_:\n            key = b''.join([x for x in fp_.readlines() if x.strip()][1:-1])\n\n    pre = getattr(hashlib, sum_type)(key).hexdigest()\n    finger = ''\n    for ind, _ in enumerate(pre):\n        if ind % 2:\n            # Is odd\n            finger += '{0}:'.format(pre[ind])\n        else:\n            finger += pre[ind]\n    return finger.rstrip(':')", "output": "Pass in either a raw pem string, or the path on disk to the location of a\n    pem file, and the type of cryptographic hash to use. The default is SHA256.\n    The fingerprint of the pem will be returned.\n\n    If neither a key nor a path are passed in, a blank string will be returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toPlainText(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        # Fix what appears to be a PyQt4 bug when getting file\r\n        # contents under Windows and PY3. This bug leads to\r\n        # corruptions when saving files with certain combinations\r\n        # of unicode chars on them (like the one attached on\r\n        # Issue 1546)\r\n        if os.name == 'nt' and PY3:\r\n            text = self.get_text('sof', 'eof')\r\n            return text.replace('\\u2028', '\\n').replace('\\u2029', '\\n')\\\r\n                       .replace('\\u0085', '\\n')\r\n        else:\r\n            return super(TextEditBaseWidget, self).toPlainText()", "output": "Reimplement Qt method\r\n        Fix PyQt4 bug on Windows and Python 3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exception(self, *exceptions):\n        \"\"\"\n        \"\"\"\n\n        def response(handler):\n            for exception in exceptions:\n                if isinstance(exception, (tuple, list)):\n                    for e in exception:\n                        self.error_handler.add(e, handler)\n                else:\n                    self.error_handler.add(exception, handler)\n            return handler\n\n        return response", "output": "Decorate a function to be registered as a handler for exceptions\n\n        :param exceptions: exceptions\n        :return: decorated function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_reachable_host(entity_name):\n    '''\n    \n    '''\n    try:\n        assert type(socket.getaddrinfo(entity_name, 0, 0, 0, 0)) == list\n        ret = True\n    except socket.gaierror:\n        ret = False\n\n    return ret", "output": "Returns a bool telling if the entity name is a reachable host (IPv4/IPv6/FQDN/etc).\n    :param hostname:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _partition_index_names(provisioned_index_names, index_names):\n    ''''''\n    existing_index_names = set()\n    new_index_names = set()\n    for name in index_names:\n        if name in provisioned_index_names:\n            existing_index_names.add(name)\n        else:\n            new_index_names.add(name)\n    index_names_to_be_deleted = provisioned_index_names - existing_index_names\n    return existing_index_names, new_index_names, index_names_to_be_deleted", "output": "Returns 3 disjoint sets of indexes: existing, to be created, and to be deleted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(key, default=''):\n    '''\n    \n    '''\n    if not isinstance(key, six.string_types):\n        log.debug('%s: \\'key\\' argument is not a string type: \\'%s\\'', __name__, key)\n        return False\n    return os.environ.get(key, default)", "output": "Get a single salt process environment variable.\n\n    key\n        String used as the key for environment lookup.\n\n    default\n        If the key is not found in the environment, return this value.\n        Default: ''\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' environ.get foo\n        salt '*' environ.get baz default=False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply(\n        self,\n        func,\n        num_splits=None,\n        other_axis_partition=None,\n        maintain_partitioning=True,\n        **kwargs\n    ):\n        \"\"\"\n        \"\"\"\n        if num_splits is None:\n            num_splits = len(self.list_of_blocks)\n\n        if other_axis_partition is not None:\n            return self._wrap_partitions(\n                self.deploy_func_between_two_axis_partitions(\n                    self.axis,\n                    func,\n                    num_splits,\n                    len(self.list_of_blocks),\n                    kwargs,\n                    *tuple(self.list_of_blocks + other_axis_partition.list_of_blocks)\n                )\n            )\n        args = [self.axis, func, num_splits, kwargs, maintain_partitioning]\n        args.extend(self.list_of_blocks)\n        return self._wrap_partitions(self.deploy_axis_func(*args))", "output": "Applies func to the object in the plasma store.\n\n        See notes in Parent class about this method.\n\n        Args:\n            func: The function to apply.\n            num_splits: The number of times to split the result object.\n            other_axis_partition: Another `PandasOnRayFrameAxisPartition` object to apply to\n                func with this one.\n            maintain_partitioning: Whether or not to keep the partitioning in the same\n                orientation as it was previously. This is important because we may be\n                operating on an individual AxisPartition and not touching the rest.\n                In this case, we have to return the partitioning to its previous\n                orientation (the lengths will remain the same). This is ignored between\n                two axis partitions.\n\n        Returns:\n            A list of `RayRemotePartition` objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_logits_over_interval(*args, **kwargs):\n  \"\"\"\"\"\"\n  warnings.warn(\"`get_logits_over_interval` has moved to \"\n                \"`cleverhans.plot.pyplot_image`. \"\n                \"cleverhans.utils.get_logits_over_interval may be removed on \"\n                \"or after 2019-04-24.\")\n  # pylint:disable=line-too-long\n  from cleverhans.plot.pyplot_image import get_logits_over_interval as new_get_logits_over_interval\n  return new_get_logits_over_interval(*args, **kwargs)", "output": "Deprecation wrapper", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keep_negative_impute(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" \n    \"\"\"\n    return __run_measure(measures.keep_impute, X, y, model_generator, method_name, -1, num_fcounts, __mean_pred)", "output": "Keep Negative (impute)\n    xlabel = \"Max fraction of features kept\"\n    ylabel = \"Negative mean model output\"\n    transform = \"negate\"\n    sort_order = 17", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_unordered(self, inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        inplace = validate_bool_kwarg(inplace, 'inplace')\n        return self.set_ordered(False, inplace=inplace)", "output": "Set the Categorical to be unordered.\n\n        Parameters\n        ----------\n        inplace : bool, default False\n           Whether or not to set the ordered attribute in-place or return\n           a copy of this categorical with ordered set to False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_opts(self, schema=None, **options):\n        \"\"\"\n        \n        \"\"\"\n        if schema is not None:\n            self.schema(schema)\n        for k, v in options.items():\n            if v is not None:\n                self.option(k, v)", "output": "Set named options (filter out those the value is None)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sigmoid_range(x, low, high):\n    \"\"\n    return torch.sigmoid(x) * (high - low) + low", "output": "Sigmoid function with range `(low, high)`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_file(path):\n    '''\n    \n    '''\n    try:\n        with salt.utils.files.fopen(path, 'rb') as contents:\n            return salt.utils.yaml.safe_load(contents)\n    except (OSError, IOError):\n        return {}", "output": "Reads and returns the contents of a text file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _df_dtypes(self, table_name, convert_dates):\n        \"\"\"\n        \"\"\"\n        out = self._raw_table_dtypes[table_name]\n        if convert_dates:\n            out = out.copy()\n            for date_column in self._datetime_int_cols[table_name]:\n                out[date_column] = datetime64ns_dtype\n\n        return out", "output": "Get dtypes to use when unpacking sqlite tables as dataframes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def states(opts, functions, utils, serializers, whitelist=None, proxy=None, context=None):\n    '''\n    \n    '''\n    if context is None:\n        context = {}\n\n    ret = LazyLoader(\n        _module_dirs(opts, 'states'),\n        opts,\n        tag='states',\n        pack={'__salt__': functions, '__proxy__': proxy or {}},\n        whitelist=whitelist,\n    )\n    ret.pack['__states__'] = ret\n    ret.pack['__utils__'] = utils\n    ret.pack['__serializers__'] = serializers\n    ret.pack['__context__'] = context\n    return ret", "output": "Returns the state modules\n\n    :param dict opts: The Salt options dictionary\n    :param dict functions: A dictionary of minion modules, with module names as\n                            keys and funcs as values.\n\n    .. code-block:: python\n\n        import salt.config\n        import salt.loader\n\n        __opts__ = salt.config.minion_config('/etc/salt/minion')\n        statemods = salt.loader.states(__opts__, None, None)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_pcre_minions(self, expr, greedy):  # pylint: disable=unused-argument\n        '''\n        \n        '''\n        reg = re.compile(expr)\n        return {'minions': [m for m in self._pki_minions() if reg.match(m)],\n                'missing': []}", "output": "Return the minions found by looking via regular expressions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_position(self, subject):\r\n        \"\"\"\"\"\"\r\n        cursor = self.textCursor()\r\n        if subject == 'cursor':\r\n            pass\r\n        elif subject == 'sol':\r\n            cursor.movePosition(QTextCursor.StartOfBlock)\r\n        elif subject == 'eol':\r\n            cursor.movePosition(QTextCursor.EndOfBlock)\r\n        elif subject == 'eof':\r\n            cursor.movePosition(QTextCursor.End)\r\n        elif subject == 'sof':\r\n            cursor.movePosition(QTextCursor.Start)\r\n        else:\r\n            # Assuming that input argument was already a position\r\n            return subject\r\n        return cursor.position()", "output": "Get offset in character for the given subject from the start of\r\n           text edit area", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(self, mode):\n        \"\"\"\n        \n        \"\"\"\n        if mode == 'w':\n            return self.format.pipe_writer(AtomicFtpFile(self._fs, self.path))\n\n        elif mode == 'r':\n            temp_dir = os.path.join(tempfile.gettempdir(), 'luigi-contrib-ftp')\n            self.__tmp_path = temp_dir + '/' + self.path.lstrip('/') + '-luigi-tmp-%09d' % random.randrange(0, 1e10)\n            # download file to local\n            self._fs.get(self.path, self.__tmp_path)\n\n            return self.format.pipe_reader(\n                FileWrapper(io.BufferedReader(io.FileIO(self.__tmp_path, 'r')))\n            )\n        else:\n            raise Exception(\"mode must be 'r' or 'w' (got: %s)\" % mode)", "output": "Open the FileSystem target.\n\n        This method returns a file-like object which can either be read from or written to depending\n        on the specified mode.\n\n        :param mode: the mode `r` opens the FileSystemTarget in read-only mode, whereas `w` will\n                     open the FileSystemTarget in write mode. Subclasses can implement\n                     additional options.\n        :type mode: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gen_form_data(self) -> multipart.MultipartWriter:\n        \"\"\"\"\"\"\n        for dispparams, headers, value in self._fields:\n            try:\n                if hdrs.CONTENT_TYPE in headers:\n                    part = payload.get_payload(\n                        value, content_type=headers[hdrs.CONTENT_TYPE],\n                        headers=headers, encoding=self._charset)\n                else:\n                    part = payload.get_payload(\n                        value, headers=headers, encoding=self._charset)\n            except Exception as exc:\n                raise TypeError(\n                    'Can not serialize value type: %r\\n '\n                    'headers: %r\\n value: %r' % (\n                        type(value), headers, value)) from exc\n\n            if dispparams:\n                part.set_content_disposition(\n                    'form-data', quote_fields=self._quote_fields, **dispparams\n                )\n                # FIXME cgi.FieldStorage doesn't likes body parts with\n                # Content-Length which were sent via chunked transfer encoding\n                assert part.headers is not None\n                part.headers.popall(hdrs.CONTENT_LENGTH, None)\n\n            self._writer.append_payload(part)\n\n        return self._writer", "output": "Encode a list of fields using the multipart/form-data MIME format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def feature (name, values, attributes = []):\n    \"\"\" \n    \"\"\"\n    __validate_feature_attributes (name, attributes)\n\n    feature = Feature(name, [], attributes)\n    __all_features[name] = feature\n    # Temporary measure while we have not fully moved from 'gristed strings'\n    __all_features[\"<\" + name + \">\"] = feature\n\n    name = add_grist(name)\n\n    if 'subfeature' in attributes:\n        __all_subfeatures.append(name)\n    else:\n        __all_top_features.append(feature)\n\n    extend (name, values)\n\n    # FIXME: why his is needed.\n    if 'free' in attributes:\n        __free_features.append (name)\n\n    return feature", "output": "Declares a new feature with the given name, values, and attributes.\n        name: the feature name\n        values: a sequence of the allowable values - may be extended later with feature.extend\n        attributes: a sequence of the feature's attributes (e.g. implicit, free, propagated, ...)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancel_order(self, order_param):\n        \"\"\"\n        \"\"\"\n        order_id = order_param\n        if isinstance(order_param, zipline.protocol.Order):\n            order_id = order_param.id\n\n        self.blotter.cancel(order_id)", "output": "Cancel an open order.\n\n        Parameters\n        ----------\n        order_param : str or Order\n            The order_id or order object to cancel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_random_line(self):\n        \"\"\"\n        \n        \"\"\"\n        # Similar to original tf repo: This outer loop should rarely go for more than one iteration for large\n        # corpora. However, just to be careful, we try to make sure that\n        # the random document is not the same as the document we're processing.\n        for _ in range(10):\n            if self.on_memory:\n                rand_doc_idx = random.randint(0, len(self.all_docs)-1)\n                rand_doc = self.all_docs[rand_doc_idx]\n                line = rand_doc[random.randrange(len(rand_doc))]\n            else:\n                rand_index = random.randint(1, self.corpus_lines if self.corpus_lines < 1000 else 1000)\n                #pick random line\n                for _ in range(rand_index):\n                    line = self.get_next_line()\n            #check if our picked random line is really from another doc like we want it to be\n            if self.current_random_doc != self.current_doc:\n                break\n        return line", "output": "Get random line from another document for nextSentence task.\n        :return: str, content of one line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile(self, func):\n        \"\"\"  \"\"\"\n        pr = cProfile.Profile()\n        pr.runcall(func)\n        st = pstats.Stats(pr)\n        st.stream = None  # make it picklable\n        st.strip_dirs()\n\n        # Adds a new profile to the existing accumulated value\n        self._accumulator.add(st)", "output": "Runs and profiles the method to_profile passed in. A profile object is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compile_func(ast_node, filename, globals, **defaults):\n    '''\n    \n    '''\n\n    function_name = ast_node.name\n    module = _ast.Module(body=[ast_node])\n\n    ctx = {'%s_default' % key : arg for key, arg in defaults.items()}\n\n    code = compile(module, filename, 'exec')\n\n    eval(code, globals, ctx)\n\n    function = ctx[function_name]\n\n    return function", "output": "Compile a function from an ast.FunctionDef instance.\n    \n    :param ast_node: ast.FunctionDef instance\n    :param filename: path where function source can be found. \n    :param globals: will be used as func_globals\n    \n    :return: A python function object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dispatch(self, receiver):\n        ''' \n\n        '''\n        super(ColumnDataChangedEvent, self).dispatch(receiver)\n        if hasattr(receiver, '_column_data_changed'):\n            receiver._column_data_changed(self)", "output": "Dispatch handling of this event to a receiver.\n\n        This method will invoke ``receiver._column_data_changed`` if it exists.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_b12l_4h_b128_uncond_dr03_tpu():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_bas8l_8h_big_uncond_dr03_imgnet()\n  update_hparams_for_tpu(hparams)\n  hparams.batch_size = 2\n  hparams.num_heads = 4   # heads are expensive on tpu\n  hparams.num_decoder_layers = 12\n  hparams.block_length = 128\n  hparams.hidden_size = 256\n  hparams.filter_size = 2048\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.optimizer = \"Adafactor\"\n  hparams.learning_rate_schedule = \"rsqrt_decay\"\n  hparams.learning_rate_warmup_steps = 10000\n  return hparams", "output": "TPU config for cifar 10.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_pretty_deprecated(deprecated):\n    \"\"\"\n    \n\n    \"\"\"\n\n    if deprecated != \"\":\n        deprecated = \"\\n\".join(map(lambda n: n[4:], deprecated.split(\"\\n\")))\n        return \"**Deprecated:**\\n%s\\n\" % deprecated", "output": "Makes the deprecated description pretty and returns a formatted string if `deprecated`\n    is not an empty string. Otherwise, returns None.\n\n    Expected input:\n\n        ...\n\n    Expected output:\n    **Deprecated:**\n\n    ...", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_filter(self, property_name, operator, value):\n        \"\"\"\n        \"\"\"\n        if self.OPERATORS.get(operator) is None:\n            error_message = 'Invalid expression: \"%s\"' % (operator,)\n            choices_message = \"Please use one of: =, <, <=, >, >=.\"\n            raise ValueError(error_message, choices_message)\n\n        if property_name == \"__key__\" and not isinstance(value, Key):\n            raise ValueError('Invalid key: \"%s\"' % value)\n\n        self._filters.append((property_name, operator, value))", "output": "Filter the query based on a property name, operator and a value.\n\n        Expressions take the form of::\n\n          .add_filter('<property>', '<operator>', <value>)\n\n        where property is a property stored on the entity in the datastore\n        and operator is one of ``OPERATORS``\n        (ie, ``=``, ``<``, ``<=``, ``>``, ``>=``)::\n\n          >>> from google.cloud import datastore\n          >>> client = datastore.Client()\n          >>> query = client.query(kind='Person')\n          >>> query.add_filter('name', '=', 'James')\n          >>> query.add_filter('age', '>', 50)\n\n        :type property_name: str\n        :param property_name: A property name.\n\n        :type operator: str\n        :param operator: One of ``=``, ``<``, ``<=``, ``>``, ``>=``.\n\n        :type value: :class:`int`, :class:`str`, :class:`bool`,\n                     :class:`float`, :class:`NoneType`,\n                     :class:`datetime.datetime`,\n                     :class:`google.cloud.datastore.key.Key`\n        :param value: The value to filter on.\n\n        :raises: :class:`ValueError` if ``operation`` is not one of the\n                 specified values, or if a filter names ``'__key__'`` but\n                 passes an invalid value (a key is required).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_bufsize_linux(iface):\n    '''\n    \n    '''\n    ret = {'result': False}\n\n    cmd = '/sbin/ethtool -g {0}'.format(iface)\n    out = __salt__['cmd.run'](cmd)\n    pat = re.compile(r'^(.+):\\s+(\\d+)$')\n    suffix = 'max-'\n    for line in out.splitlines():\n        res = pat.match(line)\n        if res:\n            ret[res.group(1).lower().replace(' ', '-') + suffix] = int(res.group(2))\n            ret['result'] = True\n        elif line.endswith('maximums:'):\n            suffix = '-max'\n        elif line.endswith('settings:'):\n            suffix = ''\n    if not ret['result']:\n        parts = out.split()\n        # remove shell cmd prefix from msg\n        if parts[0].endswith('sh:'):\n            out = ' '.join(parts[1:])\n        ret['comment'] = out\n    return ret", "output": "Return network interface buffer information using ethtool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_trial_result(self, trial_runner, trial, result):\n        \"\"\"\"\"\"\n\n        bracket, _ = self._trial_info[trial]\n        bracket.update_trial_stats(trial, result)\n\n        if bracket.continue_trial(trial):\n            return TrialScheduler.CONTINUE\n\n        action = self._process_bracket(trial_runner, bracket, trial)\n        return action", "output": "If bracket is finished, all trials will be stopped.\n\n        If a given trial finishes and bracket iteration is not done,\n        the trial will be paused and resources will be given up.\n\n        This scheduler will not start trials but will stop trials.\n        The current running trial will not be handled,\n        as the trialrunner will be given control to handle it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def host_info(host=None):\n    '''\n    \n    '''\n    data = query(host, quiet=True)\n    for id_ in data:\n        if 'vm_info' in data[id_]:\n            data[id_].pop('vm_info')\n    __jid_event__.fire_event({'data': data, 'outputter': 'nested'}, 'progress')\n    return data", "output": "Return information about the host connected to this master", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def binaryRecords(self, path, recordLength):\n        \"\"\"\n        \n        \"\"\"\n        return RDD(self._jsc.binaryRecords(path, recordLength), self, NoOpSerializer())", "output": ".. note:: Experimental\n\n        Load data from a flat binary file, assuming each record is a set of numbers\n        with the specified numerical format (see ByteBuffer), and the number of\n        bytes per record is constant.\n\n        :param path: Directory to the input data files\n        :param recordLength: The length at which to split the records", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_hosts_via_proxy(hostnames=None, datacenter=None,\n                         cluster=None, service_instance=None):\n    '''\n    \n    '''\n    if cluster:\n        if not datacenter:\n            raise salt.exceptions.ArgumentValueError(\n                'Datacenter is required when cluster is specified')\n    get_all_hosts = False\n    if not hostnames:\n        get_all_hosts = True\n    hosts = salt.utils.vmware.get_hosts(service_instance,\n                                        datacenter_name=datacenter,\n                                        host_names=hostnames,\n                                        cluster_name=cluster,\n                                        get_all_hosts=get_all_hosts)\n    return [salt.utils.vmware.get_managed_object_name(h) for h in hosts]", "output": "Returns a list of hosts for the the specified VMware environment. The list\n    of hosts can be filtered by datacenter name and/or cluster name\n\n    hostnames\n        Hostnames to filter on.\n\n    datacenter_name\n        Name of datacenter. Only hosts in this datacenter will be retrieved.\n        Default is None.\n\n    cluster_name\n        Name of cluster. Only hosts in this cluster will be retrieved. If a\n        datacenter is not specified the first cluster with this name will be\n        considerred. Default is None.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_hosts_via_proxy\n\n        salt '*' vsphere.list_hosts_via_proxy hostnames=[esxi1.example.com]\n\n        salt '*' vsphere.list_hosts_via_proxy datacenter=dc1 cluster=cluster1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(self):\n        \"\"\"\n        \n        \"\"\"\n        output = '{type}\\nFile path: {path}\\n'.format(\n            type=type(self), path=pprint_thing(self._path))\n        if self.is_open:\n            lkeys = sorted(list(self.keys()))\n            if len(lkeys):\n                keys = []\n                values = []\n\n                for k in lkeys:\n                    try:\n                        s = self.get_storer(k)\n                        if s is not None:\n                            keys.append(pprint_thing(s.pathname or k))\n                            values.append(\n                                pprint_thing(s or 'invalid_HDFStore node'))\n                    except Exception as detail:\n                        keys.append(k)\n                        values.append(\n                            \"[invalid_HDFStore node: {detail}]\".format(\n                                detail=pprint_thing(detail)))\n\n                output += adjoin(12, keys, values)\n            else:\n                output += 'Empty'\n        else:\n            output += \"File is CLOSED\"\n\n        return output", "output": "Print detailed information on the store.\n\n        .. versionadded:: 0.21.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mod_watch(name, sfun=None, **kwargs):\n    '''\n    \n    '''\n    if sfun == 'present':\n        # Force image to be updated\n        kwargs['force'] = True\n        return present(name, **kwargs)\n\n    return {'name': name,\n            'changes': {},\n            'result': False,\n            'comment': 'watch requisite is not implemented for '\n                       '{0}'.format(sfun)}", "output": "The docker_image  watcher, called to invoke the watch command.\n\n    .. note::\n        This state exists to support special handling of the ``watch``\n        :ref:`requisite <requisites>`. It should not be called directly.\n\n        Parameters for this function should be set by the state being triggered.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_netG():\n    \"\"\"\"\"\"\n    # build the generator\n    netG = nn.Sequential()\n    with netG.name_scope():\n        # input is Z, going into a convolution\n        netG.add(nn.Conv2DTranspose(ngf * 8, 4, 1, 0, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf*8) x 4 x 4\n        netG.add(nn.Conv2DTranspose(ngf * 4, 4, 2, 1, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf*4) x 8 x 8\n        netG.add(nn.Conv2DTranspose(ngf * 2, 4, 2, 1, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf*2) x 16 x 16\n        netG.add(nn.Conv2DTranspose(ngf, 4, 2, 1, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf) x 32 x 32\n        netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, use_bias=False))\n        netG.add(nn.Activation('tanh'))\n        # state size. (nc) x 64 x 64\n\n    return netG", "output": "Get net G", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_trade_commission(self, trade):\n        \"\"\"\n        \n        \"\"\"\n        order_id = trade.order_id\n        if self.env.data_proxy.instruments(trade.order_book_id).type == 'PublicFund':\n            return self._get_public_fund_commission(trade.order_book_id, trade.side, trade.last_price * trade.last_quantity)\n        commission = self.commission_map[order_id]\n        cost_commission = trade.last_price * trade.last_quantity * self.commission_rate * self.commission_multiplier\n        if cost_commission > commission:\n            if commission == self.min_commission:\n                self.commission_map[order_id] = 0\n                return cost_commission\n            else:\n                self.commission_map[order_id] = 0\n                return cost_commission - commission\n        else:\n            if commission == self.min_commission:\n                self.commission_map[order_id] -= cost_commission\n                return commission\n            else:\n                self.commission_map[order_id] -= cost_commission\n                return 0", "output": "\u8ba1\u7b97\u624b\u7eed\u8d39\u8fd9\u4e2a\u903b\u8f91\u6bd4\u8f83\u590d\u6742\uff0c\u6309\u7167\u5982\u4e0b\u7b97\u6cd5\u6765\u8ba1\u7b97\uff1a\n        1.  \u5b9a\u4e49\u4e00\u4e2a\u5269\u4f59\u624b\u7eed\u8d39\u7684\u6982\u5ff5\uff0c\u6839\u636eorder_id\u5b58\u50a8\u5728commission_map\u4e2d\uff0c\u9ed8\u8ba4\u4e3amin_commission\n        2.  \u5f53trade\u6765\u65f6\u8ba1\u7b97\u8be5trade\u4ea7\u751f\u7684\u624b\u7eed\u8d39cost_money\n        3.  \u5982\u679ccost_money > commission\n            3.1 \u5982\u679ccommission \u7b49\u4e8e min_commission\uff0c\u8bf4\u660e\u8fd9\u662f\u7b2c\u4e00\u7b14trade\uff0c\u6b64\u65f6\uff0c\u76f4\u63a5commission\u7f6e0\uff0c\u8fd4\u56decost_money\u5373\u53ef\n            3.2 \u5982\u679ccommission \u4e0d\u7b49\u4e8e min_commission, \u5219\u8bf4\u660e\u8fd9\u4e0d\u662f\u7b2c\u4e00\u7b14trade,\u6b64\u65f6\uff0c\u76f4\u63a5cost_money - commission\u5373\u53ef\n        4.  \u5982\u679ccost_money <= commission\n            4.1 \u5982\u679ccommission \u7b49\u4e8e min_commission, \u8bf4\u660e\u662f\u7b2c\u4e00\u7b14trade, \u6b64\u65f6\uff0c\u8fd4\u56demin_commission(\u63d0\u524d\u628a\u6700\u5c0f\u624b\u7eed\u8d39\u6536\u4e86)\n            4.2 \u5982\u679ccommission \u4e0d\u7b49\u4e8e min_commission\uff0c \u8bf4\u660e\u4e0d\u662f\u7b2c\u4e00\u7b14trade, \u4e4b\u524d\u7684trade\u4e2dmin_commission\u5df2\u7ecf\u6536\u8fc7\u4e86\uff0c\u6240\u4ee5\u8fd4\u56de0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_parent(self, location):\n        \"\"\"\"\"\"\n        assert isinstance(location, basestring)\n        found = b2.util.path.glob_in_parents(\n            location, self.JAMROOT + self.JAMFILE)\n\n        if not found:\n            print \"error: Could not find parent for project at '%s'\" % location\n            print \"error: Did not find Jamfile.jam or Jamroot.jam in any parent directory.\"\n            sys.exit(1)\n\n        return self.load(os.path.dirname(found[0]))", "output": "Loads parent of Jamfile at 'location'.\n        Issues an error if nothing is found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_action(self, nb_actions, probs):\n        \"\"\"\n\n        \"\"\"\n        action = np.random.choice(range(nb_actions), p=probs)\n        return action", "output": "Return the selected action\n\n        # Arguments\n            probs (np.ndarray) : Probabilty for each action\n\n        # Returns\n            action", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unit_net_value(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._units == 0:\n            return np.nan\n        return self.total_value / self._units", "output": "[float] \u5b9e\u65f6\u51c0\u503c", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_eip_address_info(addresses=None, allocation_ids=None, region=None, key=None,\n                         keyid=None, profile=None):\n    '''\n    \n    '''\n    if type(addresses) == (type('string')):\n        addresses = [addresses]\n    if type(allocation_ids) == (type('string')):\n        allocation_ids = [allocation_ids]\n\n    ret = _get_all_eip_addresses(addresses=addresses, allocation_ids=allocation_ids,\n                       region=region, key=key, keyid=keyid, profile=profile)\n\n    interesting = ['allocation_id', 'association_id', 'domain', 'instance_id',\n                   'network_interface_id', 'network_interface_owner_id', 'public_ip',\n                   'private_ip_address']\n\n    return [dict([(x, getattr(address, x)) for x in interesting]) for address in ret]", "output": "Get 'interesting' info about some, or all EIPs associated with the current account.\n\n    addresses\n        (list) - Optional list of addresses.  If provided, only the addresses\n        associated with those in the list will be returned.\n    allocation_ids\n        (list) - Optional list of allocation IDs.  If provided, only the\n        addresses associated with the given allocation IDs will be returned.\n\n    returns\n        (list of dicts) - A list of dicts, each containing the info for one of the requested EIPs.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call boto_ec2.get_eip_address_info addresses=52.4.2.15\n\n    .. versionadded:: 2016.3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _search_keys(text, keyserver, user=None):\n    '''\n    \n    '''\n    gpg = _create_gpg(user)\n    if keyserver:\n        _keys = gpg.search_keys(text, keyserver)\n    else:\n        _keys = gpg.search_keys(text)\n    return _keys", "output": "Helper function for searching keys from keyserver", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def action(\n        fun=None,\n        cloudmap=None,\n        names=None,\n        provider=None,\n        instance=None,\n        **kwargs):\n    '''\n    \n    '''\n    client = _get_client()\n    try:\n        info = client.action(fun, cloudmap, names, provider, instance, kwargs)\n    except SaltCloudConfigError as err:\n        log.error(err)\n        return None\n\n    return info", "output": "Execute a single action on the given provider/instance\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minionname cloud.action start instance=myinstance\n        salt minionname cloud.action stop instance=myinstance\n        salt minionname cloud.action show_image provider=my-ec2-config image=ami-1624987f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_params(self, filename, ctx=None, allow_missing=False,\n                    ignore_extra=False):\n        \"\"\"\n        \"\"\"\n        warnings.warn(\"load_params is deprecated. Please use load_parameters.\")\n        self.load_parameters(filename, ctx, allow_missing, ignore_extra)", "output": "[Deprecated] Please use load_parameters.\n\n        Load parameters from file.\n\n        filename : str\n            Path to parameter file.\n        ctx : Context or list of Context, default cpu()\n            Context(s) to initialize loaded parameters on.\n        allow_missing : bool, default False\n            Whether to silently skip loading parameters not represents in the file.\n        ignore_extra : bool, default False\n            Whether to silently ignore parameters from the file that are not\n            present in this Block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize_view(self):\r\n        \"\"\"\"\"\"\r\n        self.clear()\r\n        self.item_depth = 0   # To be use for collapsing/expanding one level\r\n        self.item_list = []  # To be use for collapsing/expanding one level\r\n        self.items_to_be_shown = {}\r\n        self.current_view_depth = 0", "output": "Clean the tree and view parameters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_trainer(self, trainer):\n        \"\"\"  \"\"\"\n        # trainer cannot be replaced for sparse params\n        if self._stype != 'default' and self._trainer and trainer and self._trainer is not trainer:\n            raise RuntimeError(\n                \"Failed to set the trainer for Parameter '%s' because it was already set. \" \\\n                \"More than one trainers for a %s Parameter is not supported.\" \\\n                %(self.name, self._stype))\n        self._trainer = trainer", "output": "Set the trainer this parameter is associated with.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def feed(self, x):\n        \"\"\"\n        \n        \"\"\"\n        self._n += 1\n        delta = x - self._mean\n        self._mean += delta * (1.0 / self._n)\n        delta2 = x - self._mean\n        self._M2 += delta * delta2", "output": "Args:\n            x (float or np.ndarray): must have the same shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def configure_network(ip, netmask, gateway):\n    '''\n    \n    '''\n    current = network()\n\n    # Check to see if the network is already configured\n    if (ip in current['Network Settings']['IP_ADDRESS']['VALUE'] and\n            netmask in current['Network Settings']['SUBNET_MASK']['VALUE'] and\n            gateway in current['Network Settings']['GATEWAY_IP_ADDRESS']['VALUE']):\n        return True\n\n    _xml = \"\"\"<RIBCL VERSION=\"2.0\">\n                <LOGIN USER_LOGIN=\"adminname\" PASSWORD=\"password\">\n                  <RIB_INFO MODE=\"write\">\n                    <MOD_NETWORK_SETTINGS>\n                      <IP_ADDRESS value=\"{0}\"/>\n                      <SUBNET_MASK value=\"{1}\"/>\n                      <GATEWAY_IP_ADDRESS value=\"{2}\"/>\n                    </MOD_NETWORK_SETTINGS>\n                  </RIB_INFO>\n                </LOGIN>\n              </RIBCL> \"\"\".format(ip, netmask, gateway)\n\n    return __execute_cmd('Configure_Network', _xml)", "output": "Configure Network Interface\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ilo.configure_network [IP ADDRESS] [NETMASK] [GATEWAY]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_ssh_credentials(vm_):\n    '''\n    \n    '''\n    ssh_user = config.get_cloud_config_value(\n        'ssh_username', vm_, __opts__, default=os.getenv('USER'))\n    ssh_key = config.get_cloud_config_value(\n        'ssh_keyfile', vm_, __opts__,\n        default=os.path.expanduser('~/.ssh/google_compute_engine'))\n    return ssh_user, ssh_key", "output": "Get configured SSH credentials.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _addMethod(self, effect, verb, resource, conditions):\n        \"\"\"\"\"\"\n        if verb != \"*\" and not hasattr(HttpVerb, verb):\n            raise NameError(\"Invalid HTTP verb \" + verb + \". Allowed verbs in HttpVerb class\")\n        resourcePattern = re.compile(self.pathRegex)\n        if not resourcePattern.match(resource):\n            raise NameError(\"Invalid resource path: \" + resource + \". Path should match \" + self.pathRegex)\n\n        if resource[:1] == \"/\":\n            resource = resource[1:]\n\n        resourceArn = (\"arn:aws:execute-api:\" +\n            self.region + \":\" +\n            self.awsAccountId + \":\" +\n            self.restApiId + \"/\" +\n            self.stage + \"/\" +\n            verb + \"/\" +\n            resource)\n\n        if effect.lower() == \"allow\":\n            self.allowMethods.append({\n                'resourceArn' : resourceArn,\n                'conditions' : conditions\n            })\n        elif effect.lower() == \"deny\":\n            self.denyMethods.append({\n                'resourceArn' : resourceArn,\n                'conditions' : conditions\n            })", "output": "Adds a method to the internal lists of allowed or denied methods. Each object in\n        the internal list contains a resource ARN and a condition statement. The condition\n        statement can be null.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm_special(user, cmd, special=None, identifier=None):\n    '''\n    \n    '''\n    lst = list_tab(user)\n    ret = 'absent'\n    rm_ = None\n    for ind in range(len(lst['special'])):\n        if rm_ is not None:\n            break\n        if _cron_matched(lst['special'][ind], cmd, identifier=identifier):\n            if special is None:\n                # No special param was specified\n                rm_ = ind\n            else:\n                if lst['special'][ind]['spec'] == special:\n                    rm_ = ind\n    if rm_ is not None:\n        lst['special'].pop(rm_)\n        ret = 'removed'\n    comdat = _write_cron_lines(user, _render_tab(lst))\n    if comdat['retcode']:\n        # Failed to commit, return the error\n        return comdat['stderr']\n    return ret", "output": "Remove a special cron job for a specified user.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cron.rm_special root /usr/bin/foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flatMapValues(self, f):\n        \"\"\"\n        \n        \"\"\"\n        flat_map_fn = lambda kv: ((kv[0], x) for x in f(kv[1]))\n        return self.flatMap(flat_map_fn, preservesPartitioning=True)", "output": "Pass each value in the key-value pair RDD through a flatMap function\n        without changing the keys; this also retains the original RDD's\n        partitioning.\n\n        >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n        >>> def f(x): return x\n        >>> x.flatMapValues(f).collect()\n        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_attention1d_masked_decoder(x, kv_dim, heads_dim,\n                                     feedforward_dim, hparams):\n  \"\"\"\"\"\"\n  print(x)\n  _, length_dim, model_dim = x.shape.dims\n  for layer in range(hparams.num_decoder_layers):\n    layer_name = \"decoder_layer_%d\" % layer\n    with tf.variable_scope(layer_name):\n      # Self attention layer\n      length_per_split = mtf.tensor_dim_to_size_per_split(\n          hparams.layout, hparams.mesh_shape, length_dim)\n      x += layer_prepostprocess_dropout(\n          mtf.layers.masked_local_attention_1d(\n              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_att\"),\n              kv_dim,\n              heads_dim,\n              window_size=hparams.block_length,\n              length_per_split=length_per_split,\n              name=\"self_att\"), hparams)\n      # ffn layer\n      x += layer_prepostprocess_dropout(\n          mtf.layers.dense_relu_dense(\n              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_ffn\"),\n              feedforward_dim,\n              hparams.dropout,\n              dropout_broadcast_dims=[length_dim]), hparams)\n\n  output = mtf.layers.layer_norm(x, model_dim, name=\"final_layer_norm\")\n  return output", "output": "Image Transformer decoder with local1D masked layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self):\n        \"\"\"\n        \"\"\"\n        new_client = self._client.copy()\n        return self.__class__(\n            self.instance_id,\n            new_client,\n            self.configuration_name,\n            node_count=self.node_count,\n            display_name=self.display_name,\n        )", "output": "Make a copy of this instance.\n\n        Copies the local data stored as simple types and copies the client\n        attached to this instance.\n\n        :rtype: :class:`~google.cloud.spanner_v1.instance.Instance`\n        :returns: A copy of the current instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def custom_filterset_factory(model, filterset_base_class=FilterSet, **meta):\n    \"\"\" \n    \"\"\"\n    meta.update({\"model\": model})\n    meta_class = type(str(\"Meta\"), (object,), meta)\n    filterset = type(\n        str(\"%sFilterSet\" % model._meta.object_name),\n        (filterset_base_class, GrapheneFilterSetMixin),\n        {\"Meta\": meta_class},\n    )\n    return filterset", "output": "Create a filterset for the given model using the provided meta data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_search(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_openstack_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.search_projects(**kwargs)", "output": "Search projects\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.project_search\n        salt '*' keystoneng.project_search name=project1\n        salt '*' keystoneng.project_search domain_id=b62e76fbeeff4e8fb77073f591cf211e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def strip_esc_sequence(txt):\n    '''\n    \n    '''\n    if isinstance(txt, six.string_types):\n        try:\n            return txt.replace('\\033', '?')\n        except UnicodeDecodeError:\n            return txt.replace(str('\\033'), str('?'))  # future lint: disable=blacklisted-function\n    else:\n        return txt", "output": "Replace ESC (ASCII 27/Oct 33) to prevent unsafe strings\n    from writing their own terminal manipulation commands", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_module_path(modname):\r\n    \"\"\"\"\"\"\r\n    return osp.abspath(osp.dirname(sys.modules[modname].__file__))", "output": "Return module *modname* base path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _conv_general_permutations(self, dimension_numbers):\n    \"\"\"\"\"\"\n    lhs_spec, rhs_spec, out_spec = dimension_numbers\n    lhs_char, rhs_char, out_char = ('N', 'C'), ('O', 'I'), ('N', 'C')\n    charpairs = (lhs_char, rhs_char, out_char)\n    for i, (a, b) in enumerate(charpairs):\n      if not (dimension_numbers[i].count(a) == 1 and\n              dimension_numbers[i].count(b) == 1):\n        msg = ('convolution dimension_numbers[{}] must contain the characters '\n               '\"{}\" and \"{}\" exatly once, got {}.')\n        raise TypeError(msg.format(i, a, b, dimension_numbers[i]))\n      if len(dimension_numbers[i]) != len(set(dimension_numbers[i])):\n        msg = ('convolution dimension_numbers[{}] cannot have duplicate '\n               'characters, got {}.')\n        raise TypeError(msg.format(i, dimension_numbers[i]))\n    if not (set(lhs_spec) - set(lhs_char) == set(rhs_spec) - set(rhs_char) ==\n            set(out_spec) - set(out_char)):\n      msg = ('convolution dimension_numbers elements must each have the same '\n             'set of spatial characters, got {}.')\n      raise TypeError(msg.format(dimension_numbers))\n\n    def getperm(spec, charpair):\n      spatial = (i for i, c in enumerate(spec) if c not in charpair)\n      if spec is not rhs_spec:\n        spatial = sorted(spatial, key=lambda i: rhs_spec.index(spec[i]))\n      return (spec.index(charpair[0]), spec.index(charpair[1])) + tuple(spatial)\n\n    lhs_perm, rhs_perm, out_perm = map(getperm, dimension_numbers, charpairs)\n    return lhs_perm, rhs_perm, out_perm", "output": "Utility for convolution dimension permutations relative to Conv HLO.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_path_from_index(self, index):\n        \"\"\"\n        \n        \"\"\"\n        assert self.image_set_index is not None, \"Dataset not initialized\"\n        name = self.image_set_index[index]\n        image_file = os.path.join(self.image_dir, 'images', name)\n        assert os.path.isfile(image_file), 'Path does not exist: {}'.format(image_file)\n        return image_file", "output": "given image index, find out full path\n\n        Parameters:\n        ----------\n        index: int\n            index of a specific image\n        Returns:\n        ----------\n        full path of this image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inspect_current_object(self):\r\n        \"\"\"\"\"\"\r\n        editor = self.get_current_editor()\r\n        editor.sig_display_signature.connect(self.display_signature_help)\r\n        line, col = editor.get_cursor_line_column()\r\n        editor.request_hover(line, col)", "output": "Inspect current object in the Help plugin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rows_from_json(values, schema):\n    \"\"\"\"\"\"\n    from google.cloud.bigquery import Row\n\n    field_to_index = _field_to_index_mapping(schema)\n    return [Row(_row_tuple_from_json(r, schema), field_to_index) for r in values]", "output": "Convert JSON row data to rows with appropriate types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_vertical_box(cls, children, layout=Layout(), duplicates=False):\n        \"\"\n        if not duplicates: return widgets.VBox(children, layout=layout)\n        else: return widgets.VBox([children[0], children[2]], layout=layout)", "output": "Make a vertical box with `children` and `layout`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_kexgss_continue(self, m):\n        \"\"\"\n        \n        \"\"\"\n        if not self.transport.server_mode:\n            srv_token = m.get_string()\n            m = Message()\n            m.add_byte(c_MSG_KEXGSS_CONTINUE)\n            m.add_string(\n                self.kexgss.ssh_init_sec_context(\n                    target=self.gss_host, recv_token=srv_token\n                )\n            )\n            self.transport.send_message(m)\n            self.transport._expect_packet(\n                MSG_KEXGSS_CONTINUE, MSG_KEXGSS_COMPLETE, MSG_KEXGSS_ERROR\n            )\n        else:\n            pass", "output": "Parse the SSH2_MSG_KEXGSS_CONTINUE message.\n\n        :param `.Message` m: The content of the SSH2_MSG_KEXGSS_CONTINUE\n            message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clone_from(self, other):\r\n        \"\"\"\"\"\"\r\n        for other_finfo in other.data:\r\n            self.clone_editor_from(other_finfo, set_current=True)\r\n        self.set_stack_index(other.get_stack_index())", "output": "Clone EditorStack from other instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_predicted_class(self, x, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    return tf.argmax(self.get_logits(x, **kwargs), axis=1)", "output": ":param x: A symbolic representation (Tensor) of the network input\n    :return: A symbolic representation (Tensor) of the predicted label", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vqa_v2_preprocess_image(\n    image,\n    height,\n    width,\n    mode,\n    resize_side=512,\n    distort=True,\n    image_model_fn=\"resnet_v1_152\",\n):\n  \"\"\"\"\"\"\n\n  image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n  assert resize_side > 0\n  if resize_side:\n    image = _aspect_preserving_resize(image, resize_side)\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    image = tf.random_crop(image, [height, width, 3])\n  else:\n    # Central crop, assuming resize_height > height, resize_width > width.\n    image = tf.image.resize_image_with_crop_or_pad(image, height, width)\n\n  image = tf.clip_by_value(image, 0.0, 1.0)\n\n  if mode == tf.estimator.ModeKeys.TRAIN and distort:\n    image = _flip(image)\n    num_distort_cases = 4\n    # pylint: disable=unnecessary-lambda\n    image = _apply_with_random_selector(\n        image, lambda x, ordering: _distort_color(x, ordering),\n        num_cases=num_distort_cases)\n\n  if image_model_fn.startswith(\"resnet_v1\"):\n    # resnet_v1 uses vgg preprocessing\n    image = image * 255.\n    image = _mean_image_subtraction(image, [_R_MEAN, _G_MEAN, _B_MEAN])\n  elif image_model_fn.startswith(\"resnet_v2\"):\n    # resnet v2 uses inception preprocessing\n    image = tf.subtract(image, 0.5)\n    image = tf.multiply(image, 2.0)\n\n  return image", "output": "vqa v2 preprocess image.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cache_key(self, query_obj, **extra):\n        \"\"\"\n        \n        \"\"\"\n        cache_dict = copy.copy(query_obj)\n        cache_dict.update(extra)\n\n        for k in ['from_dttm', 'to_dttm']:\n            del cache_dict[k]\n\n        cache_dict['time_range'] = self.form_data.get('time_range')\n        cache_dict['datasource'] = self.datasource.uid\n        json_data = self.json_dumps(cache_dict, sort_keys=True)\n        return hashlib.md5(json_data.encode('utf-8')).hexdigest()", "output": "The cache key is made out of the key/values in `query_obj`, plus any\n        other key/values in `extra`.\n\n        We remove datetime bounds that are hard values, and replace them with\n        the use-provided inputs to bounds, which may be time-relative (as in\n        \"5 days ago\" or \"now\").\n\n        The `extra` arguments are currently used by time shift queries, since\n        different time shifts wil differ only in the `from_dttm` and `to_dttm`\n        values which are stripped.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gettime_s(text):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    pattern = r'([+-]?\\d+\\.?\\d*) ?([munsecinh]+)'\r\n    matches = re.findall(pattern, text)\r\n    if len(matches) == 0:\r\n        return None\r\n    time = 0.\r\n    for res in matches:\r\n        tmp = float(res[0])\r\n        if res[1] == 'ns':\r\n            tmp *= 1e-9\r\n        elif res[1] == 'us':\r\n            tmp *= 1e-6\r\n        elif res[1] == 'ms':\r\n            tmp *= 1e-3\r\n        elif res[1] == 'min':\r\n            tmp *= 60\r\n        elif res[1] == 'h':\r\n            tmp *= 3600\r\n        time += tmp\r\n    return time", "output": "Parse text and return a time in seconds.\r\n\r\n    The text is of the format 0h : 0.min:0.0s:0 ms:0us:0 ns.\r\n    Spaces are not taken into account and any of the specifiers can be ignored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def advance(self, size: int) -> None:\n        \"\"\"\n        \n        \"\"\"\n        assert 0 < size <= self._size\n        self._size -= size\n        pos = self._first_pos\n\n        buffers = self._buffers\n        while buffers and size > 0:\n            is_large, b = buffers[0]\n            b_remain = len(b) - size - pos\n            if b_remain <= 0:\n                buffers.popleft()\n                size -= len(b) - pos\n                pos = 0\n            elif is_large:\n                pos += size\n                size = 0\n            else:\n                # Amortized O(1) shrink for Python 2\n                pos += size\n                if len(b) <= 2 * pos:\n                    del typing.cast(bytearray, b)[:pos]\n                    pos = 0\n                size = 0\n\n        assert size == 0\n        self._first_pos = pos", "output": "Advance the current buffer position by ``size`` bytes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def beta_schedule(schedule, global_step, final_beta, decay_start, decay_end):\n  \"\"\"\"\"\"\n  if decay_start > decay_end:\n    raise ValueError(\"decay_end is smaller than decay_end.\")\n\n  # Since some of the TF schedules do not support incrementing a value,\n  # in all of the schedules, we anneal the beta from final_beta to zero\n  # and then reverse it at the bottom.\n  if schedule == \"constant\":\n    decayed_value = 0.0\n  elif schedule == \"linear\":\n    decayed_value = tf.train.polynomial_decay(\n        learning_rate=final_beta,\n        global_step=global_step - decay_start,\n        decay_steps=decay_end - decay_start,\n        end_learning_rate=0.0)\n  elif schedule == \"noisy_linear_cosine_decay\":\n    decayed_value = tf.train.noisy_linear_cosine_decay(\n        learning_rate=final_beta,\n        global_step=global_step - decay_start,\n        decay_steps=decay_end - decay_start)\n  # TODO(mechcoder): Add log_annealing schedule.\n  else:\n    raise ValueError(\"Unknown beta schedule.\")\n\n  increased_value = final_beta - decayed_value\n  increased_value = tf.maximum(0.0, increased_value)\n\n  beta = tf.case(\n      pred_fn_pairs={\n          tf.less(global_step, decay_start): lambda: 0.0,\n          tf.greater(global_step, decay_end): lambda: final_beta},\n      default=lambda: increased_value)\n  return beta", "output": "Get KL multiplier (beta) based on the schedule.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _active_mounts_openbsd(ret):\n    '''\n    \n    '''\n    for line in __salt__['cmd.run_stdout']('mount -v').split('\\n'):\n        comps = re.sub(r\"\\s+\", \" \", line).split()\n        parens = re.findall(r'\\((.*?)\\)', line, re.DOTALL)\n        if len(parens) > 1:\n            nod = __salt__['cmd.run_stdout']('ls -l {0}'.format(comps[0]))\n            nod = ' '.join(nod.split()).split(\" \")\n            ret[comps[3]] = {'device': comps[0],\n                         'fstype': comps[5],\n                         'opts': _resolve_user_group_names(parens[1].split(\", \")),\n                         'major': six.text_type(nod[4].strip(\",\")),\n                         'minor': six.text_type(nod[5]),\n                         'device_uuid': parens[0]}\n        else:\n            ret[comps[2]] = {'device': comps[0],\n                            'fstype': comps[4],\n                            'opts': _resolve_user_group_names(parens[0].split(\", \"))}\n    return ret", "output": "List active mounts on OpenBSD systems", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _expand_variable_match(positional_vars, named_vars, match):\n    \"\"\"\n    \"\"\"\n    positional = match.group(\"positional\")\n    name = match.group(\"name\")\n    if name is not None:\n        try:\n            return six.text_type(named_vars[name])\n        except KeyError:\n            raise ValueError(\n                \"Named variable '{}' not specified and needed by template \"\n                \"`{}` at position {}\".format(name, match.string, match.start())\n            )\n    elif positional is not None:\n        try:\n            return six.text_type(positional_vars.pop(0))\n        except IndexError:\n            raise ValueError(\n                \"Positional variable not specified and needed by template \"\n                \"`{}` at position {}\".format(match.string, match.start())\n            )\n    else:\n        raise ValueError(\"Unknown template expression {}\".format(match.group(0)))", "output": "Expand a matched variable with its value.\n\n    Args:\n        positional_vars (list): A list of positonal variables. This list will\n            be modified.\n        named_vars (dict): A dictionary of named variables.\n        match (re.Match): A regular expression match.\n\n    Returns:\n        str: The expanded variable to replace the match.\n\n    Raises:\n        ValueError: If a positional or named variable is required by the\n            template but not specified or if an unexpected template expression\n            is encountered.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_processor_name():\n    \"\"\"\n    \"\"\"\n    mxlen = 256\n    length = ctypes.c_ulong()\n    buf = ctypes.create_string_buffer(mxlen)\n    _LIB.RabitGetProcessorName(buf, ctypes.byref(length), mxlen)\n    return buf.value", "output": "Get the processor name.\n\n    Returns\n    -------\n    name : str\n        the name of processor(host)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def time_machine(host, mode):\n    \"\"\"\"\"\"\n    now = datetime.datetime.now()\n    to = str(now.year) + str(now.day) + str(now.month)\n    if now.month > 6:\n    \tfro = str(now.year) + str(now.day) + str(now.month - 6)\n    else:\n    \tfro = str(now.year - 1) + str(now.day) + str(now.month + 6)\n    url = \"http://web.archive.org/cdx/search?url=%s&matchType=%s&collapse=urlkey&fl=original&filter=mimetype:text/html&filter=statuscode:200&output=json&from=%s&to=%s\" % (host, mode, fro, to)\n    response = get(url).text\n    parsed = json.loads(response)[1:]\n    urls = []\n    for item in parsed:\n        urls.append(item[0])\n    return urls", "output": "Query archive.org.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, using=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        return self._get_connection(using).indices.get(index=self._name, **kwargs)", "output": "The get index API allows to retrieve information about the index.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.get`` unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_for_task(upid, timeout=300):\n    '''\n    \n    '''\n    start_time = time.time()\n    info = _lookup_proxmox_task(upid)\n    if not info:\n        log.error('wait_for_task: No task information '\n                  'retrieved based on given criteria.')\n        raise SaltCloudExecutionFailure\n\n    while True:\n        if 'status' in info and info['status'] == 'OK':\n            log.debug('Task has been finished!')\n            return True\n        time.sleep(3)  # Little more patience, we're not in a hurry\n        if time.time() - start_time > timeout:\n            log.debug('Timeout reached while waiting for task to be finished')\n            return False\n        info = _lookup_proxmox_task(upid)", "output": "Wait until a the task has been finished successfully", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nearest_workday(dt):\n    \"\"\"\n    \n    \"\"\"\n    if dt.weekday() == 5:\n        return dt - timedelta(1)\n    elif dt.weekday() == 6:\n        return dt + timedelta(1)\n    return dt", "output": "If holiday falls on Saturday, use day before (Friday) instead;\n    if holiday falls on Sunday, use day thereafter (Monday) instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_cythonize(extensions, *args, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if len(sys.argv) > 1 and 'clean' in sys.argv:\n        # Avoid running cythonize on `python setup.py clean`\n        # See https://github.com/cython/cython/issues/1495\n        return extensions\n    if not cython:\n        # Avoid trying to look up numpy when installing from sdist\n        # https://github.com/pandas-dev/pandas/issues/25193\n        # TODO: See if this can be removed after pyproject.toml added.\n        return extensions\n\n    numpy_incl = pkg_resources.resource_filename('numpy', 'core/include')\n    # TODO: Is this really necessary here?\n    for ext in extensions:\n        if (hasattr(ext, 'include_dirs') and\n                numpy_incl not in ext.include_dirs):\n            ext.include_dirs.append(numpy_incl)\n\n    build_ext.render_templates(_pxifiles)\n    return cythonize(extensions, *args, **kwargs)", "output": "Render tempita templates before calling cythonize", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exec_command(self, command):\n        \"\"\"\n        \n        \"\"\"\n        m = Message()\n        m.add_byte(cMSG_CHANNEL_REQUEST)\n        m.add_int(self.remote_chanid)\n        m.add_string(\"exec\")\n        m.add_boolean(True)\n        m.add_string(command)\n        self._event_pending()\n        self.transport._send_user_message(m)\n        self._wait_for_event()", "output": "Execute a command on the server.  If the server allows it, the channel\n        will then be directly connected to the stdin, stdout, and stderr of\n        the command being executed.\n\n        When the command finishes executing, the channel will be closed and\n        can't be reused.  You must open a new channel if you wish to execute\n        another command.\n\n        :param str command: a shell command to execute.\n\n        :raises:\n            `.SSHException` -- if the request was rejected or the channel was\n            closed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_lm_tpu_0():\n  \"\"\"\"\"\"\n  hparams = transformer_clean_big()\n  update_hparams_for_tpu(hparams)\n  hparams.num_heads = 4  # Heads are expensive on TPUs.\n  hparams.batch_size = 4096\n  hparams.shared_embedding_and_softmax_weights = False\n  hparams.layer_prepostprocess_dropout = 0.1\n  return hparams", "output": "HParams for training languagemodel_lm1b8k on tpu.  92M Params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def range(self, start, end=None, step=1, numPartitions=None):\n        \"\"\"\n        \n        \"\"\"\n        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n\n        if end is None:\n            jdf = self._jsparkSession.range(0, int(start), int(step), int(numPartitions))\n        else:\n            jdf = self._jsparkSession.range(int(start), int(end), int(step), int(numPartitions))\n\n        return DataFrame(jdf, self._wrapped)", "output": "Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n        ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n        step value ``step``.\n\n        :param start: the start value\n        :param end: the end value (exclusive)\n        :param step: the incremental step (default: 1)\n        :param numPartitions: the number of partitions of the DataFrame\n        :return: :class:`DataFrame`\n\n        >>> spark.range(1, 7, 2).collect()\n        [Row(id=1), Row(id=3), Row(id=5)]\n\n        If only one argument is specified, it will be used as the end value.\n\n        >>> spark.range(3).collect()\n        [Row(id=0), Row(id=1), Row(id=2)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def saveFormatFileTo(self, buf, encoding, format):\n        \"\"\" \"\"\"\n        if buf is None: buf__o = None\n        else: buf__o = buf._o\n        ret = libxml2mod.xmlSaveFormatFileTo(buf__o, self._o, encoding, format)\n        return ret", "output": "Dump an XML document to an I/O buffer. Warning ! This call\n          xmlOutputBufferClose() on buf which is not available after\n           this call.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_without_progress(url):\n    \"\"\"\n    \n    \"\"\"\n    resp = requests.get(url)\n    resp.raise_for_status()\n    return BytesIO(resp.content)", "output": "Download data from a URL, returning a BytesIO containing the loaded data.\n\n    Parameters\n    ----------\n    url : str\n        A URL that can be understood by ``requests.get``.\n\n    Returns\n    -------\n    data : BytesIO\n        A BytesIO containing the downloaded data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def factorize(\n            self,\n            na_sentinel: int = -1,\n    ) -> Tuple[np.ndarray, ABCExtensionArray]:\n        \"\"\"\n        \n        \"\"\"\n        # Impelmentor note: There are two ways to override the behavior of\n        # pandas.factorize\n        # 1. _values_for_factorize and _from_factorize.\n        #    Specify the values passed to pandas' internal factorization\n        #    routines, and how to convert from those values back to the\n        #    original ExtensionArray.\n        # 2. ExtensionArray.factorize.\n        #    Complete control over factorization.\n        from pandas.core.algorithms import _factorize_array\n\n        arr, na_value = self._values_for_factorize()\n\n        labels, uniques = _factorize_array(arr, na_sentinel=na_sentinel,\n                                           na_value=na_value)\n\n        uniques = self._from_factorized(uniques, self)\n        return labels, uniques", "output": "Encode the extension array as an enumerated type.\n\n        Parameters\n        ----------\n        na_sentinel : int, default -1\n            Value to use in the `labels` array to indicate missing values.\n\n        Returns\n        -------\n        labels : ndarray\n            An integer NumPy array that's an indexer into the original\n            ExtensionArray.\n        uniques : ExtensionArray\n            An ExtensionArray containing the unique values of `self`.\n\n            .. note::\n\n               uniques will *not* contain an entry for the NA value of\n               the ExtensionArray if there are any missing values present\n               in `self`.\n\n        See Also\n        --------\n        pandas.factorize : Top-level factorize method that dispatches here.\n\n        Notes\n        -----\n        :meth:`pandas.factorize` offers a `sort` keyword as well.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer_dtype_from_object(dtype):\n    \"\"\"\n    \n    \"\"\"\n\n    if isinstance(dtype, type) and issubclass(dtype, np.generic):\n        # Type object from a dtype\n        return dtype\n    elif isinstance(dtype, (np.dtype, PandasExtensionDtype, ExtensionDtype)):\n        # dtype object\n        try:\n            _validate_date_like_dtype(dtype)\n        except TypeError:\n            # Should still pass if we don't have a date-like\n            pass\n        return dtype.type\n\n    try:\n        dtype = pandas_dtype(dtype)\n    except TypeError:\n        pass\n\n    if is_extension_array_dtype(dtype):\n        return dtype.type\n    elif isinstance(dtype, str):\n\n        # TODO(jreback)\n        # should deprecate these\n        if dtype in ['datetimetz', 'datetime64tz']:\n            return DatetimeTZDtype.type\n        elif dtype in ['period']:\n            raise NotImplementedError\n\n        if dtype == 'datetime' or dtype == 'timedelta':\n            dtype += '64'\n        try:\n            return infer_dtype_from_object(getattr(np, dtype))\n        except (AttributeError, TypeError):\n            # Handles cases like _get_dtype(int) i.e.,\n            # Python objects that are valid dtypes\n            # (unlike user-defined types, in general)\n            #\n            # TypeError handles the float16 type code of 'e'\n            # further handle internal types\n            pass\n\n    return infer_dtype_from_object(np.dtype(dtype))", "output": "Get a numpy dtype.type-style object for a dtype object.\n\n    This methods also includes handling of the datetime64[ns] and\n    datetime64[ns, TZ] objects.\n\n    If no dtype can be found, we return ``object``.\n\n    Parameters\n    ----------\n    dtype : dtype, type\n        The dtype object whose numpy dtype.type-style\n        object we want to extract.\n\n    Returns\n    -------\n    dtype_object : The extracted numpy dtype.type-style object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_compare_estimator_and_feature_spec(self, estimator, feature_spec):\n    \"\"\"\n    \"\"\"\n    # If custom function is set, remove it before setting estimator\n    self.delete('compare_custom_predict_fn')\n\n    self.store('compare_estimator_and_spec', {\n      'estimator': estimator, 'feature_spec': feature_spec})\n    self.set_compare_inference_address('estimator')\n    # If no model name has been set, give a default\n    if not self.has_compare_model_name():\n      self.set_compare_model_name('2')\n    return self", "output": "Sets a second model for inference as a TF Estimator.\n\n    If you wish to compare the results of two models in WIT, use this method\n    to setup the details of the second model.\n\n    Instead of using TF Serving to host a model for WIT to query, WIT can\n    directly use a TF Estimator object as the model to query. In order to\n    accomplish this, a feature_spec must also be provided to parse the\n    example protos for input into the estimator.\n\n    Args:\n      estimator: The TF Estimator which will be used for model inference.\n      feature_spec: The feature_spec object which will be used for example\n      parsing.\n\n    Returns:\n      self, in order to enabled method chaining.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_epoch_end(self, epoch:int, **kwargs:Any)->None:\n        \"\"\n        if self.every==\"epoch\": self.learn.save(f'{self.name}_{epoch}')\n        else: #every=\"improvement\"\n            current = self.get_monitor_value()\n            if current is not None and self.operator(current, self.best):\n                print(f'Better model found at epoch {epoch} with {self.monitor} value: {current}.')\n                self.best = current\n                self.learn.save(f'{self.name}')", "output": "Compare the value monitored to its best score and maybe save the model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_abbr_impl():\n    # type: () -> str\n    \"\"\"\"\"\"\n    if hasattr(sys, 'pypy_version_info'):\n        pyimpl = 'pp'\n    elif sys.platform.startswith('java'):\n        pyimpl = 'jy'\n    elif sys.platform == 'cli':\n        pyimpl = 'ip'\n    else:\n        pyimpl = 'cp'\n    return pyimpl", "output": "Return abbreviated implementation name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process(self, statement, additional_response_selection_parameters=None):\n        \"\"\"\n        \n        \"\"\"\n        from mathparse import mathparse\n\n        input_text = statement.text\n\n        # Use the result cached by the process method if it exists\n        if input_text in self.cache:\n            cached_result = self.cache[input_text]\n            self.cache = {}\n            return cached_result\n\n        # Getting the mathematical terms within the input statement\n        expression = mathparse.extract_expression(input_text, language=self.language.ISO_639.upper())\n\n        response = Statement(text=expression)\n\n        try:\n            response.text += ' = ' + str(\n                mathparse.parse(expression, language=self.language.ISO_639.upper())\n            )\n\n            # The confidence is 1 if the expression could be evaluated\n            response.confidence = 1\n        except mathparse.PostfixTokenEvaluationException:\n            response.confidence = 0\n\n        return response", "output": "Takes a statement string.\n        Returns the equation from the statement with the mathematical terms solved.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def configure_boto_session_method_kwargs(self, service, kw):\n        \"\"\"\"\"\"\n        if service in self.endpoint_urls and not 'endpoint_url' in kw:\n            kw['endpoint_url'] = self.endpoint_urls[service]\n        return kw", "output": "Allow for custom endpoint urls for non-AWS (testing and bootleg cloud) deployments", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_results_from_args(args: argparse.Namespace):\n    \"\"\"\n    \n    \"\"\"\n    path = args.path\n    metrics_name = args.metrics_filename\n    keys = args.keys\n\n    results_dict = {}\n    for root, _, files in os.walk(path):\n\n        if metrics_name in files:\n            full_name = os.path.join(root, metrics_name)\n            metrics = json.load(open(full_name))\n            results_dict[full_name] = metrics\n\n\n    sorted_keys = sorted(list(results_dict.keys()))\n    print(f\"model_run, {', '.join(keys)}\")\n    for name in sorted_keys:\n        results = results_dict[name]\n        keys_to_print = [str(results.get(key, \"N/A\")) for key in keys]\n        print(f\"{name}, {', '.join(keys_to_print)}\")", "output": "Prints results from an ``argparse.Namespace`` object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inverse_removing(self, words_to_remove):\n        \"\"\"\n        \"\"\"\n        mask = np.ones(self.as_np.shape[0], dtype='bool')\n        mask[self.__get_idxs(words_to_remove)] = False\n        if not self.bow:\n            return ''.join([self.as_list[i] if mask[i]\n                            else 'UNKWORDZ' for i in range(mask.shape[0])])\n        return ''.join([self.as_list[v] for v in mask.nonzero()[0]])", "output": "Returns a string after removing the appropriate words.\n\n        If self.bow is false, replaces word with UNKWORDZ instead of removing\n        it.\n\n        Args:\n            words_to_remove: list of ids (ints) to remove\n\n        Returns:\n            original raw string with appropriate words removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readinto(self, buff):\n        \"\"\"\n        \n        \"\"\"\n        data = self.read(len(buff))\n        buff[: len(data)] = data\n        return len(data)", "output": "Read up to ``len(buff)`` bytes into ``bytearray`` *buff* and return the\n        number of bytes read.\n\n        :returns:\n            The number of bytes read.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def search(format, string, pos=0, endpos=None, extra_types=None, evaluate_result=True,\n        case_sensitive=False):\n    '''\n    '''\n    p = Parser(format, extra_types=extra_types, case_sensitive=case_sensitive)\n    return p.search(string, pos, endpos, evaluate_result=evaluate_result)", "output": "Search \"string\" for the first occurrence of \"format\".\n\n    The format may occur anywhere within the string. If\n    instead you wish for the format to exactly match the string\n    use parse().\n\n    Optionally start the search at \"pos\" character index and limit the search\n    to a maximum index of endpos - equivalent to search(string[:endpos]).\n\n    If ``evaluate_result`` is True the return value will be an Result instance with two attributes:\n\n     .fixed - tuple of fixed-position values from the string\n     .named - dict of named values from the string\n\n    If ``evaluate_result`` is False the return value will be a Match instance with one method:\n\n     .evaluate_result() - This will return a Result instance like you would get\n                          with ``evaluate_result`` set to True\n\n    The default behaviour is to match strings case insensitively. You may match with\n    case by specifying case_sensitive=True.\n\n    If the format is invalid a ValueError will be raised.\n\n    See the module documentation for the use of \"extra_types\".\n\n    In the case there is no match parse() will return None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_generic_c_patterns(keywords, builtins,\r\n                            instance=None, define=None, comment=None):\r\n    \"\"\r\n    kw = r\"\\b\" + any(\"keyword\", keywords.split()) + r\"\\b\"\r\n    builtin = r\"\\b\" + any(\"builtin\", builtins.split()+C_TYPES.split()) + r\"\\b\"\r\n    if comment is None:\r\n        comment = any(\"comment\", [r\"//[^\\n]*\", r\"\\/\\*(.*?)\\*\\/\"])\r\n    comment_start = any(\"comment_start\", [r\"\\/\\*\"])\r\n    comment_end = any(\"comment_end\", [r\"\\*\\/\"])\r\n    if instance is None:\r\n        instance = any(\"instance\", [r\"\\bthis\\b\"])\r\n    number = any(\"number\",\r\n                 [r\"\\b[+-]?[0-9]+[lL]?\\b\",\r\n                  r\"\\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\\b\",\r\n                  r\"\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?\\b\"])\r\n    sqstring = r\"(\\b[rRuU])?'[^'\\\\\\n]*(\\\\.[^'\\\\\\n]*)*'?\"\r\n    dqstring = r'(\\b[rRuU])?\"[^\"\\\\\\n]*(\\\\.[^\"\\\\\\n]*)*\"?'\r\n    string = any(\"string\", [sqstring, dqstring])\r\n    if define is None:\r\n        define = any(\"define\", [r\"#[^\\n]*\"])\r\n    return \"|\".join([instance, kw, comment, string, number,\r\n                     comment_start, comment_end, builtin,\r\n                     define, any(\"SYNC\", [r\"\\n\"])])", "output": "Strongly inspired from idlelib.ColorDelegator.make_pat", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autoencoder_discrete_tiny():\n  \"\"\"\"\"\"\n  hparams = autoencoder_ordered_discrete()\n  hparams.num_hidden_layers = 2\n  hparams.bottleneck_bits = 24\n  hparams.batch_size = 2\n  hparams.gan_loss_factor = 0.\n  hparams.bottleneck_l2_factor = 0.001\n  hparams.add_hparam(\"video_modality_loss_cutoff\", 0.02)\n  hparams.num_residual_layers = 1\n  hparams.hidden_size = 32\n  hparams.max_hidden_size = 64\n  return hparams", "output": "Discrete autoencoder model for compressing pong frames for testing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(ctx, key):\n    ''''''\n    file = ctx.obj['FILE']\n    stored_value = get_key(file, key)\n    if stored_value:\n        click.echo('%s=%s' % (key, stored_value))\n    else:\n        exit(1)", "output": "Retrieve the value for the given key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_operatorsetid(\n        domain,  # type: Text\n        version,  # type: int\n):  # type: (...) -> OperatorSetIdProto\n    \"\"\"\n    \"\"\"\n    operatorsetid = OperatorSetIdProto()\n    operatorsetid.domain = domain\n    operatorsetid.version = version\n    return operatorsetid", "output": "Construct an OperatorSetIdProto.\n\n    Arguments:\n        domain (string): The domain of the operator set id\n        version (integer): Version of operator set id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_df(cls, df:DataFrame, path:PathOrStr, cols:IntsOrStrs=0, folder:PathOrStr=None, suffix:str='', **kwargs)->'ItemList':\n        \"\"\n        suffix = suffix or ''\n        res = super().from_df(df, path=path, cols=cols, **kwargs)\n        pref = f'{res.path}{os.path.sep}'\n        if folder is not None: pref += f'{folder}{os.path.sep}'\n        res.items = np.char.add(np.char.add(pref, res.items.astype(str)), suffix)\n        return res", "output": "Get the filenames in `cols` of `df` with `folder` in front of them, `suffix` at the end.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n        \"\"\"\n        \n        \"\"\"\n        threshold = TypeConverters.toFloat(threshold)\n        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)", "output": "Join two datasets to approximately find all pairs of rows whose distance are smaller than\n        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n        transformed data when necessary.\n\n        :param datasetA: One of the datasets to join.\n        :param datasetB: Another dataset to join.\n        :param threshold: The threshold for the distance of row pairs.\n        :param distCol: Output column for storing the distance between each pair of rows. Use\n                        \"distCol\" as default value if it's not specified.\n        :return: A joined dataset containing pairs of rows. The original rows are in columns\n                 \"datasetA\" and \"datasetB\", and a column \"distCol\" is added to show the distance\n                 between each pair.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _insert_new_layers(self, new_layers, start_node_id, end_node_id):\n        \"\"\"\"\"\"\n        new_node_id = self._add_node(deepcopy(self.node_list[end_node_id]))\n        temp_output_id = new_node_id\n        for layer in new_layers[:-1]:\n            temp_output_id = self.add_layer(layer, temp_output_id)\n\n        self._add_edge(new_layers[-1], temp_output_id, end_node_id)\n        new_layers[-1].input = self.node_list[temp_output_id]\n        new_layers[-1].output = self.node_list[end_node_id]\n        self._redirect_edge(start_node_id, end_node_id, new_node_id)", "output": "Insert the new_layers after the node with start_node_id.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_subnet_group(name, description, subnet_ids, tags=None,\n                        region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    res = __salt__['boto_rds.subnet_group_exists'](name, tags, region, key,\n                                                 keyid, profile)\n    if res.get('exists'):\n        return {'exists': bool(res)}\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if not conn:\n            return {'results': bool(conn)}\n\n        taglist = _tag_doc(tags)\n        rds = conn.create_db_subnet_group(DBSubnetGroupName=name,\n                                          DBSubnetGroupDescription=description,\n                                          SubnetIds=subnet_ids, Tags=taglist)\n\n        return {'created': bool(rds)}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Create an RDS subnet group\n\n    CLI example to create an RDS subnet group::\n\n        salt myminion boto_rds.create_subnet_group my-subnet-group \\\n            \"group description\" '[subnet-12345678, subnet-87654321]' \\\n            region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def model_argmax(sess, x, predictions, samples, feed=None):\n  \"\"\"\n  \n  \"\"\"\n  feed_dict = {x: samples}\n  if feed is not None:\n    feed_dict.update(feed)\n  probabilities = sess.run(predictions, feed_dict)\n\n  if samples.shape[0] == 1:\n    return np.argmax(probabilities)\n  else:\n    return np.argmax(probabilities, axis=1)", "output": "Helper function that computes the current class prediction\n  :param sess: TF session\n  :param x: the input placeholder\n  :param predictions: the model's symbolic output\n  :param samples: numpy array with input samples (dims must match x)\n  :param feed: An optional dictionary that is appended to the feeding\n           dictionary before the session runs. Can be used to feed\n           the learning phase of a Keras model for instance.\n  :return: the argmax output of predictions, i.e. the current predicted class", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_update(userid, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'user.update'\n            params = {\"userid\": userid, }\n            params = _params_extend(params, _ignore_name=True, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['userids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: 2016.3.0\n\n    Update existing users\n\n    .. note::\n        This function accepts all standard user properties: keyword argument\n        names differ depending on your zabbix version, see here__.\n\n        .. __: https://www.zabbix.com/documentation/2.0/manual/appendix/api/user/definitions#user\n\n    :param userid: id of the user to update\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: Id of the updated user on success.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.user_update 16 visible_name='James Brown'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_iface_info(iface):\n    '''\n    \n    '''\n    iface_info = interfaces()\n\n    if iface in iface_info.keys():\n        return iface_info, False\n    else:\n        error_msg = ('Interface \"{0}\" not in available interfaces: \"{1}\"'\n                     ''.format(iface, '\", \"'.join(iface_info.keys())))\n        log.error(error_msg)\n        return None, error_msg", "output": "If `iface` is available, return interface info and no error, otherwise\n    return no info and log and return an error", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_file_system(filesystemid,\n                       keyid=None,\n                       key=None,\n                       profile=None,\n                       region=None,\n                       **kwargs):\n    '''\n    \n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    client.delete_file_system(FileSystemId=filesystemid)", "output": "Deletes a file system, permanently severing access to its contents.\n    Upon return, the file system no longer exists and you can't access\n    any contents of the deleted file system. You can't delete a file system\n    that is in use. That is, if the file system has any mount targets,\n    you must first delete them.\n\n    filesystemid\n        (string) - ID of the file system to delete.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.delete_file_system filesystemid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map(self, mapper):\n        \"\"\"\n        \n        \"\"\"\n        # this is used in apply.\n        # We get hit since we're an \"is_extension_type\" but regular extension\n        # types are not hit. This may be worth adding to the interface.\n        if isinstance(mapper, ABCSeries):\n            mapper = mapper.to_dict()\n\n        if isinstance(mapper, abc.Mapping):\n            fill_value = mapper.get(self.fill_value, self.fill_value)\n            sp_values = [mapper.get(x, None) for x in self.sp_values]\n        else:\n            fill_value = mapper(self.fill_value)\n            sp_values = [mapper(x) for x in self.sp_values]\n\n        return type(self)(sp_values, sparse_index=self.sp_index,\n                          fill_value=fill_value)", "output": "Map categories using input correspondence (dict, Series, or function).\n\n        Parameters\n        ----------\n        mapper : dict, Series, callable\n            The correspondence from old values to new.\n\n        Returns\n        -------\n        SparseArray\n            The output array will have the same density as the input.\n            The output fill value will be the result of applying the\n            mapping to ``self.fill_value``\n\n        Examples\n        --------\n        >>> arr = pd.SparseArray([0, 1, 2])\n        >>> arr.apply(lambda x: x + 10)\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n\n        >>> arr.apply({0: 10, 1: 11, 2: 12})\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)\n\n        >>> arr.apply(pd.Series([10, 11, 12], index=[0, 1, 2]))\n        [10, 11, 12]\n        Fill: 10\n        IntIndex\n        Indices: array([1, 2], dtype=int32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _aws_model_ref_from_swagger_ref(self, r):\n        '''\n        \n        '''\n        model_name = r.split('/')[-1]\n        return 'https://apigateway.amazonaws.com/restapis/{0}/models/{1}'.format(self.restApiId, model_name)", "output": "Helper function to reference models created on aws apigw", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_nodes(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The list_nodes function must be called with -f or --function.'\n        )\n\n    ret = {}\n    for vm_name, vm_details in six.iteritems(get_resources_vms(includeConfig=True)):\n        log.debug('VM_Name: %s', vm_name)\n        log.debug('vm_details: %s', vm_details)\n\n        # Limit resultset on what Salt-cloud demands:\n        ret[vm_name] = {}\n        ret[vm_name]['id'] = six.text_type(vm_details['vmid'])\n        ret[vm_name]['image'] = six.text_type(vm_details['vmid'])\n        ret[vm_name]['size'] = six.text_type(vm_details['disk'])\n        ret[vm_name]['state'] = six.text_type(vm_details['status'])\n\n        # Figure out which is which to put it in the right column\n        private_ips = []\n        public_ips = []\n\n        if 'ip_address' in vm_details['config'] and vm_details['config']['ip_address'] != '-':\n            ips = vm_details['config']['ip_address'].split(' ')\n            for ip_ in ips:\n                if IP(ip_).iptype() == 'PRIVATE':\n                    private_ips.append(six.text_type(ip_))\n                else:\n                    public_ips.append(six.text_type(ip_))\n\n        ret[vm_name]['private_ips'] = private_ips\n        ret[vm_name]['public_ips'] = public_ips\n\n    return ret", "output": "Return a list of the VMs that are managed by the provider\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -Q my-proxmox-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def word2id(self, xs):\n        \"\"\"\n        \"\"\"\n        if isinstance(xs, list):\n            return [self._word2id.get(x, self.UNK) for x in xs]\n        return self._word2id.get(xs, self.UNK)", "output": "Map word(s) to its id(s)\n\n        Parameters\n        ----------\n        xs : str or list\n            word or a list of words\n\n        Returns\n        -------\n        int or list\n            id or a list of ids", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mine(tgt=None, tgt_type='glob', **kwargs):\n    '''\n    \n    '''\n    pillar_util = salt.utils.master.MasterPillarUtil(tgt, tgt_type,\n                                                     use_cached_grains=False,\n                                                     grains_fallback=False,\n                                                     use_cached_pillar=False,\n                                                     pillar_fallback=False,\n                                                     opts=__opts__)\n    cached_mine = pillar_util.get_cached_mine_data()\n    return cached_mine", "output": ".. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Return cached mine data of the targeted minions\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run cache.mine", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_image(root, recursive, exts):\n    \"\"\"\n    \"\"\"\n\n    i = 0\n    if recursive:\n        cat = {}\n        for path, dirs, files in os.walk(root, followlinks=True):\n            dirs.sort()\n            files.sort()\n            for fname in files:\n                fpath = os.path.join(path, fname)\n                suffix = os.path.splitext(fname)[1].lower()\n                if os.path.isfile(fpath) and (suffix in exts):\n                    if path not in cat:\n                        cat[path] = len(cat)\n                    yield (i, os.path.relpath(fpath, root), cat[path])\n                    i += 1\n        for k, v in sorted(cat.items(), key=lambda x: x[1]):\n            print(os.path.relpath(k, root), v)\n    else:\n        for fname in sorted(os.listdir(root)):\n            fpath = os.path.join(root, fname)\n            suffix = os.path.splitext(fname)[1].lower()\n            if os.path.isfile(fpath) and (suffix in exts):\n                yield (i, os.path.relpath(fpath, root), 0)\n                i += 1", "output": "Traverses the root of directory that contains images and\n    generates image list iterator.\n    Parameters\n    ----------\n    root: string\n    recursive: bool\n    exts: string\n    Returns\n    -------\n    image iterator that contains all the image under the specified path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_cache(self):\n        \"\"\"\"\"\"\n        if os.path.exists(self._cache_file):\n            self._cache = read_cache_file(self._cache_file)\n        else:\n            self._cache = {}", "output": "Reads the cached contents into memory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(name=None, pkgs=None, **kwargs):\n    '''\n    \n    '''\n    targets = salt.utils.args.split_input(pkgs) if pkgs else [name]\n    if not targets:\n        return {}\n\n    if pkgs:\n        log.debug('Removing these fileset(s)/rpm package(s) %s: %s', name, targets)\n\n    errors = []\n\n    # Get a list of the currently installed pkgs.\n    old = list_pkgs()\n\n    # Remove the fileset or rpm package(s)\n    for target in targets:\n        try:\n            named, versionpkg, rpmpkg = _check_pkg(target)\n        except CommandExecutionError as exc:\n            if exc.info:\n                errors.append(exc.info['errors'])\n            continue\n\n        if rpmpkg:\n            cmd = ['/usr/bin/rpm', '-e', named]\n            out = __salt__['cmd.run_all'](cmd, output_loglevel='trace')\n        else:\n            cmd = ['/usr/sbin/installp', '-u', named]\n            out = __salt__['cmd.run_all'](cmd, output_loglevel='trace')\n\n    # Get a list of the packages after the uninstall\n    __context__.pop('pkg.list_pkgs', None)\n    new = list_pkgs()\n    ret = salt.utils.data.compare_dicts(old, new)\n\n    if errors:\n        raise CommandExecutionError(\n            'Problems encountered removing filesets(s)/package(s)',\n            info={\n                'changes': ret,\n                'errors': errors\n            }\n        )\n\n    return ret", "output": "Remove specified fileset(s)/rpm package(s).\n\n    name\n        The name of the fileset or rpm package to be deleted.\n\n\n    Multiple Package Options:\n\n    pkgs\n        A list of filesets and/or rpm packages to delete.\n        Must be passed as a python list. The ``name`` parameter will be\n        ignored if this option is passed.\n\n\n    Returns a list containing the removed packages.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.remove <fileset/rpm package name>\n        salt '*' pkg.remove tcsh\n        salt '*' pkg.remove xlC.rte\n        salt '*' pkg.remove Firefox.base.adt\n        salt '*' pkg.remove pkgs='[\"foo\", \"bar\"]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_stat(x, message=None):\n    \"\"\" \n    \"\"\"\n    if message is None:\n        message = x.op.name\n    lst = [tf.shape(x), tf.reduce_mean(x)]\n    if x.dtype.is_floating:\n        lst.append(rms(x))\n    return tf.Print(x, lst + [x], summarize=20,\n                    message=message, name='print_' + x.op.name)", "output": "A simple print Op that might be easier to use than :meth:`tf.Print`.\n        Use it like: ``x = print_stat(x, message='This is x')``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_ca_bundle(opts=None):\n    '''\n    \n    '''\n    if hasattr(get_ca_bundle, '__return_value__'):\n        return get_ca_bundle.__return_value__\n\n    if opts is None:\n        opts = {}\n\n    opts_bundle = opts.get('ca_bundle', None)\n    if opts_bundle is not None and os.path.exists(opts_bundle):\n        return opts_bundle\n\n    file_roots = opts.get('file_roots', {'base': [salt.syspaths.SRV_ROOT_DIR]})\n\n    # Please do not change the order without good reason\n\n    # Check Salt first\n    for salt_root in file_roots.get('base', []):\n        for path in ('cacert.pem', 'ca-bundle.crt'):\n            cert_path = os.path.join(salt_root, path)\n            if os.path.exists(cert_path):\n                return cert_path\n\n    locations = (\n        # Debian has paths that often exist on other distros\n        '/etc/ssl/certs/ca-certificates.crt',\n        # RedHat is also very common\n        '/etc/pki/tls/certs/ca-bundle.crt',\n        '/etc/pki/tls/certs/ca-bundle.trust.crt',\n        # RedHat's link for Debian compatibility\n        '/etc/ssl/certs/ca-bundle.crt',\n        # SUSE has an unusual path\n        '/var/lib/ca-certificates/ca-bundle.pem',\n        # OpenBSD has an unusual path\n        '/etc/ssl/cert.pem',\n    )\n    for path in locations:\n        if os.path.exists(path):\n            return path\n\n    if salt.utils.platform.is_windows() and HAS_CERTIFI:\n        return certifi.where()\n\n    return None", "output": "Return the location of the ca bundle file. See the following article:\n\n        http://tinyurl.com/k7rx42a", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_requires_python(candidates):\n    \"\"\"\"\"\"\n    all_candidates = []\n    sys_version = \".\".join(map(str, sys.version_info[:3]))\n    from packaging.version import parse as parse_version\n\n    py_version = parse_version(os.environ.get(\"PIP_PYTHON_VERSION\", sys_version))\n    for c in candidates:\n        from_location = attrgetter(\"location.requires_python\")\n        requires_python = getattr(c, \"requires_python\", from_location(c))\n        if requires_python:\n            # Old specifications had people setting this to single digits\n            # which is effectively the same as '>=digit,<digit+1'\n            if requires_python.isdigit():\n                requires_python = \">={0},<{1}\".format(\n                    requires_python, int(requires_python) + 1\n                )\n            try:\n                specifierset = SpecifierSet(requires_python)\n            except InvalidSpecifier:\n                continue\n            else:\n                if not specifierset.contains(py_version):\n                    continue\n        all_candidates.append(c)\n    return all_candidates", "output": "Get a cleaned list of all the candidates with valid specifiers in the `requires_python` attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _talk2modjk(name, lbn, target, action, profile='default', tgt_type='glob'):\n    '''\n    \n    '''\n\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': ''}\n\n    action_map = {\n        'worker_stop': 'STP',\n        'worker_disable': 'DIS',\n        'worker_activate': 'ACT',\n    }\n\n    # Check what needs to be done\n    status = _worker_status(\n        target, name, action_map[action], profile, tgt_type\n    )\n    if not status['result']:\n        ret['result'] = False\n        ret['comment'] = ('no servers answered the published command '\n                          'modjk.worker_status')\n        return ret\n    if status['errors']:\n        ret['result'] = False\n        ret['comment'] = ('the following balancers could not find the '\n                          'worker {0}: {1}'.format(name, status['errors']))\n        return ret\n    if not status['wrong_state']:\n        ret['comment'] = ('the worker is in the desired activation state on '\n                          'all the balancers')\n        return ret\n    else:\n        ret['comment'] = ('the action {0} will be sent to the balancers '\n                          '{1}'.format(action, status['wrong_state']))\n        ret['changes'] = {action: status['wrong_state']}\n\n    if __opts__['test']:\n        ret['result'] = None\n        return ret\n\n    # Send the action command to target\n    response = _send_command(action, name, lbn, target, profile, tgt_type)\n    ret['comment'] = response['msg']\n    ret['result'] = response['code']\n    return ret", "output": "Wrapper function for the stop/disable/activate functions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def HMC(sym, data_inputs, X, Y, X_test, Y_test, sample_num,\n        initializer=None, noise_precision=1 / 9.0, prior_precision=0.1,\n        learning_rate=1E-6, L=10, dev=mx.gpu()):\n    \"\"\"\"\"\"\n    label_key = list(set(data_inputs.keys()) - set(['data']))[0]\n    exe, exe_params, exe_grads, _ = get_executor(sym, dev, data_inputs, initializer)\n    exe.arg_dict['data'][:] = X\n    exe.arg_dict[label_key][:] = Y\n    sample_pool = []\n    accept_num = 0\n    start = time.time()\n    for i in range(sample_num):\n        sample_params, is_accept = step_HMC(exe, exe_params, exe_grads, label_key, noise_precision,\n                                            prior_precision, L, learning_rate)\n        accept_num += is_accept\n\n        if (i + 1) % 10 == 0:\n            sample_pool.append(sample_params)\n            if (i + 1) % 100000 == 0:\n                end = time.time()\n                print(\"Current Iter Num: %d\" % (i + 1), \"Time Spent: %f\" % (end - start), \"MSE:\",\n                      sample_test_regression(exe, X=X_test, Y=Y_test, sample_pool=sample_pool,\n                                             minibatch_size=Y.shape[0],\n                                             save_path='regression_HMC.txt'))\n                start = time.time()\n        exe.copy_params_from(sample_params)\n    print('accept ratio', accept_num / float(sample_num))\n    return sample_pool", "output": "Generate the implementation of HMC", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_constraints(self):\n        \"\"\"\n        \n        \"\"\"\n        constraints = self.get_constraints()\n        for constraint in constraints:\n            try:\n                constraint.check_if_exists(False)\n            except Exception:\n                from pipenv.exceptions import DependencyConflict\n                msg = (\n                    \"Cannot resolve conflicting version {0}{1} while {1}{2} is \"\n                    \"locked.\".format(\n                        self.name, self.updated_specifier, self.old_name, self.old_specifiers\n                    )\n                )\n                raise DependencyConflict(msg)\n        return True", "output": "Retrieves the full set of available constraints and iterate over them, validating\n        that they exist and that they are not causing unresolvable conflicts.\n\n        :return: True if the constraints are satisfied by the resolution provided\n        :raises: :exc:`pipenv.exceptions.DependencyConflict` if the constraints dont exist", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_graph(node, call_deps=False):\n    '''\n    \n    '''\n\n    gen = GraphGen(call_deps=call_deps)\n    gen.visit(node)\n\n    return gen.graph, gen.undefined", "output": "Create a dependency graph from an ast node.\n    \n    :param node: ast node.\n    :param call_deps: if true, then the graph will create a cyclic dependence for all\n                      function calls. (i.e for `a.b(c)` a depends on b and b depends on a)\n                      \n    :returns: a tuple of (graph, undefined)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_initial_request(self):\n        \"\"\"\n        \"\"\"\n        # Any ack IDs that are under lease management need to have their\n        # deadline extended immediately.\n        if self._leaser is not None:\n            # Explicitly copy the list, as it could be modified by another\n            # thread.\n            lease_ids = list(self._leaser.ack_ids)\n        else:\n            lease_ids = []\n\n        # Put the request together.\n        request = types.StreamingPullRequest(\n            modify_deadline_ack_ids=list(lease_ids),\n            modify_deadline_seconds=[self.ack_deadline] * len(lease_ids),\n            stream_ack_deadline_seconds=self.ack_histogram.percentile(99),\n            subscription=self._subscription,\n        )\n\n        # Return the initial request.\n        return request", "output": "Return the initial request for the RPC.\n\n        This defines the initial request that must always be sent to Pub/Sub\n        immediately upon opening the subscription.\n\n        Returns:\n            google.cloud.pubsub_v1.types.StreamingPullRequest: A request\n            suitable for being the first request on the stream (and not\n            suitable for any other purpose).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def saturating_sigmoid(x):\n  \"\"\"\"\"\"\n  with tf.name_scope(\"saturating_sigmoid\", values=[x]):\n    y = tf.sigmoid(x)\n    return tf.minimum(1.0, tf.maximum(0.0, 1.2 * y - 0.1))", "output": "Saturating sigmoid: 1.2 * sigmoid(x) - 0.1 cut to [0, 1].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_data(self):\r\n        \"\"\"\"\"\"\r\n        title = _( \"Save profiler result\")\r\n        filename, _selfilter = getsavefilename(\r\n                self, title, getcwd_or_home(),\r\n                _(\"Profiler result\")+\" (*.Result)\")\r\n        if filename:\r\n            self.datatree.save_data(filename)", "output": "Save data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def synchronize(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        answer = QMessageBox.question(self, _(\"Synchronize\"),\r\n            _(\"This will synchronize Spyder's path list with \"\r\n                    \"<b>PYTHONPATH</b> environment variable for current user, \"\r\n                    \"allowing you to run your Python modules outside Spyder \"\r\n                    \"without having to configure sys.path. \"\r\n                    \"<br>Do you want to clear contents of PYTHONPATH before \"\r\n                    \"adding Spyder's path list?\"),\r\n            QMessageBox.Yes | QMessageBox.No | QMessageBox.Cancel)\r\n        if answer == QMessageBox.Cancel:\r\n            return\r\n        elif answer == QMessageBox.Yes:\r\n            remove = True\r\n        else:\r\n            remove = False\r\n        from spyder.utils.environ import (get_user_env, set_user_env,\r\n                                          listdict2envdict)\r\n        env = get_user_env()\r\n        if remove:\r\n            ppath = self.active_pathlist+self.ro_pathlist\r\n        else:\r\n            ppath = env.get('PYTHONPATH', [])\r\n            if not isinstance(ppath, list):\r\n                ppath = [ppath]\r\n            ppath = [path for path in ppath\r\n                     if path not in (self.active_pathlist+self.ro_pathlist)]\r\n            ppath.extend(self.active_pathlist+self.ro_pathlist)\r\n        env['PYTHONPATH'] = ppath\r\n        set_user_env(listdict2envdict(env), parent=self)", "output": "Synchronize Spyder's path list with PYTHONPATH environment variable\r\n        Only apply to: current user, on Windows platforms", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uninstall(self, auto_confirm=False, verbose=False,\n                  use_user_site=False):\n        # type: (bool, bool, bool) -> Optional[UninstallPathSet]\n        \"\"\"\n        \n\n        \"\"\"\n        if not self.check_if_exists(use_user_site):\n            logger.warning(\"Skipping %s as it is not installed.\", self.name)\n            return None\n        dist = self.satisfied_by or self.conflicts_with\n\n        uninstalled_pathset = UninstallPathSet.from_dist(dist)\n        uninstalled_pathset.remove(auto_confirm, verbose)\n        return uninstalled_pathset", "output": "Uninstall the distribution currently satisfying this requirement.\n\n        Prompts before removing or modifying files unless\n        ``auto_confirm`` is True.\n\n        Refuses to delete or modify files outside of ``sys.prefix`` -\n        thus uninstallation within a virtual environment can only\n        modify that virtual environment, even if the virtualenv is\n        linked to global site-packages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_resources(self, folder, resources):\n        \"\"\" \n        \"\"\"\n        self.validate_files_exist(folder, resources)\n        self.validate_no_duplicate_paths(resources)", "output": "validate resources is a wrapper to validate the existence of files\n            and that there are no duplicates for a folder and set of resources.\n\n            Parameters\n            ==========\n            folder: the folder to validate\n            resources: one or more resources to validate within the folder", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_all_bookmarks():\n    \"\"\"\"\"\"\n    slots = CONF.get('editor', 'bookmarks', {})\n    for slot_num in list(slots.keys()):\n        if not osp.isfile(slots[slot_num][0]):\n            slots.pop(slot_num)\n    return slots", "output": "Load all bookmarks from config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_frame_l2():\n  \"\"\"\"\"\"\n  hparams = next_frame_basic_deterministic()\n  hparams.loss[\"targets\"] = modalities.video_l2_loss\n  hparams.top[\"targets\"] = modalities.video_l1_top\n  hparams.video_modality_loss_cutoff = 2.4\n  return hparams", "output": "Basic conv model with L2 modality.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close_window(self):\n        \"\"\"\"\"\"\n        if self.undocked_window is not None:\n            self.undocked_window.close()\n            self.undocked_window = None\n\n            # Oddly, these actions can appear disabled after the Dock\n            # action is pressed\n            self.undock_action.setDisabled(False)\n            self.close_plugin_action.setDisabled(False)", "output": "Close QMainWindow instance that contains this plugin.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_delta(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(other, (Tick, timedelta, np.timedelta64)):\n            new_values = self._add_timedeltalike_scalar(other)\n        elif is_timedelta64_dtype(other):\n            # ndarray[timedelta64] or TimedeltaArray/index\n            new_values = self._add_delta_tdi(other)\n\n        return new_values", "output": "Add a timedelta-like, Tick or TimedeltaIndex-like object\n        to self, yielding an int64 numpy array\n\n        Parameters\n        ----------\n        delta : {timedelta, np.timedelta64, Tick,\n                 TimedeltaIndex, ndarray[timedelta64]}\n\n        Returns\n        -------\n        result : ndarray[int64]\n\n        Notes\n        -----\n        The result's name is set outside of _add_delta by the calling\n        method (__add__ or __sub__), if necessary (i.e. for Indexes).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_lexer_for_filename(filename):\n    \"\"\"\n    \"\"\"\n    filename = filename or ''\n    root, ext = os.path.splitext(filename)\n    if ext in custom_extension_lexer_mapping:\n        lexer = get_lexer_by_name(custom_extension_lexer_mapping[ext])\n    else:\n        try:\n            lexer = get_lexer_for_filename(filename)\n        except Exception:\n            return TextLexer()\n    return lexer", "output": "Get a Pygments Lexer given a filename.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resources_from_resource_arguments(default_num_cpus, default_num_gpus,\n                                      default_resources, runtime_num_cpus,\n                                      runtime_num_gpus, runtime_resources):\n    \"\"\"\n    \"\"\"\n    if runtime_resources is not None:\n        resources = runtime_resources.copy()\n    elif default_resources is not None:\n        resources = default_resources.copy()\n    else:\n        resources = {}\n\n    if \"CPU\" in resources or \"GPU\" in resources:\n        raise ValueError(\"The resources dictionary must not \"\n                         \"contain the key 'CPU' or 'GPU'\")\n\n    assert default_num_cpus is not None\n    resources[\"CPU\"] = (default_num_cpus\n                        if runtime_num_cpus is None else runtime_num_cpus)\n\n    if runtime_num_gpus is not None:\n        resources[\"GPU\"] = runtime_num_gpus\n    elif default_num_gpus is not None:\n        resources[\"GPU\"] = default_num_gpus\n\n    return resources", "output": "Determine a task's resource requirements.\n\n    Args:\n        default_num_cpus: The default number of CPUs required by this function\n            or actor method.\n        default_num_gpus: The default number of GPUs required by this function\n            or actor method.\n        default_resources: The default custom resources required by this\n            function or actor method.\n        runtime_num_cpus: The number of CPUs requested when the task was\n            invoked.\n        runtime_num_gpus: The number of GPUs requested when the task was\n            invoked.\n        runtime_resources: The custom resources requested when the task was\n            invoked.\n\n    Returns:\n        A dictionary of the resource requirements for the task.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _on_action_toggle(self):\n        \"\"\"\"\"\"\n        block = FoldScope.find_parent_scope(self.editor.textCursor().block())\n        self.toggle_fold_trigger(block)", "output": "Toggle the current fold trigger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reboot(name, conn=None):\n    '''\n    \n    '''\n    if not conn:\n        conn = get_conn()   # pylint: disable=E0602\n\n    node = get_node(conn, name)\n    if node is None:\n        log.error('Unable to find the VM %s', name)\n    log.info('Rebooting VM: %s', name)\n    ret = conn.reboot_node(node)\n    if ret:\n        log.info('Rebooted VM: %s', name)\n        # Fire reboot action\n        __utils__['cloud.fire_event'](\n            'event',\n            '{0} has been rebooted'.format(name), 'salt-cloud'\n            'salt/cloud/{0}/rebooting'.format(name),\n            args={'name': name},\n            sock_dir=__opts__['sock_dir'],\n            transport=__opts__['transport']\n        )\n        return True\n\n    log.error('Failed to reboot VM: %s', name)\n    return False", "output": "Reboot a single VM", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nxos_api_show(commands, raw_text=True, **kwargs):\n    '''\n    \n    '''\n    nxos_api_kwargs = pyeapi_nxos_api_args(**kwargs)\n    return __salt__['nxos_api.show'](commands,\n                                     raw_text=raw_text,\n                                     **nxos_api_kwargs)", "output": ".. versionadded:: 2019.2.0\n\n    Execute one or more show (non-configuration) commands.\n\n    commands\n        The commands to be executed.\n\n    raw_text: ``True``\n        Whether to return raw text or structured data.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.nxos_api_show 'show version'\n        salt '*' napalm.nxos_api_show 'show bgp sessions' 'show processes' raw_text=False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_gpg(user=None, gnupghome=None):\n    '''\n    \n    '''\n    if not gnupghome:\n        gnupghome = _get_user_gnupghome(user)\n\n    if GPG_1_3_1:\n        gpg = gnupg.GPG(homedir=gnupghome)\n    else:\n        gpg = gnupg.GPG(gnupghome=gnupghome)\n\n    return gpg", "output": "Create the GPG object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _column_resized(self, col, old_width, new_width):\r\n        \"\"\"\"\"\"\r\n        self.dataTable.setColumnWidth(col, new_width)\r\n        self._update_layout()", "output": "Update the column width.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_timeout(self):\n        \"\"\" \n        \"\"\"\n        if (self.total is not None and\n                self.total is not self.DEFAULT_TIMEOUT and\n                self._read is not None and\n                self._read is not self.DEFAULT_TIMEOUT):\n            # In case the connect timeout has not yet been established.\n            if self._start_connect is None:\n                return self._read\n            return max(0, min(self.total - self.get_connect_duration(),\n                              self._read))\n        elif self.total is not None and self.total is not self.DEFAULT_TIMEOUT:\n            return max(0, self.total - self.get_connect_duration())\n        else:\n            return self._read", "output": "Get the value for the read timeout.\n\n        This assumes some time has elapsed in the connection timeout and\n        computes the read timeout appropriately.\n\n        If self.total is set, the read timeout is dependent on the amount of\n        time taken by the connect timeout. If the connection time has not been\n        established, a :exc:`~urllib3.exceptions.TimeoutStateError` will be\n        raised.\n\n        :return: Value to use for the read timeout.\n        :rtype: int, float, :attr:`Timeout.DEFAULT_TIMEOUT` or None\n        :raises urllib3.exceptions.TimeoutStateError: If :meth:`start_connect`\n            has not yet been called on this object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def version():\n    '''\n    \n    '''\n    cmd = 'bluetoothctl -v'\n    out = __salt__['cmd.run'](cmd).splitlines()\n    bluez_version = out[0]\n    pybluez_version = '<= 0.18 (Unknown, but installed)'\n    try:\n        pybluez_version = bluetooth.__version__\n    except Exception as exc:\n        pass\n    return {'Bluez': bluez_version, 'PyBluez': pybluez_version}", "output": "Return Bluez version from bluetoothd -v\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluetoothd.version", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def binds(val, **kwargs):  # pylint: disable=unused-argument\n    '''\n    \n    '''\n    if not isinstance(val, dict):\n        if not isinstance(val, list):\n            try:\n                val = helpers.split(val)\n            except AttributeError:\n                raise SaltInvocationError(\n                    '\\'{0}\\' is not a dictionary or list of bind '\n                    'definitions'.format(val)\n                )\n    return val", "output": "On the CLI, these are passed as multiple instances of a given CLI option.\n    In Salt, we accept these as a comma-delimited list but the API expects a\n    Python list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_chunked(self, method, url, body=None, headers=None):\n        \"\"\"\n        \n        \"\"\"\n        headers = HTTPHeaderDict(headers if headers is not None else {})\n        skip_accept_encoding = 'accept-encoding' in headers\n        skip_host = 'host' in headers\n        self.putrequest(\n            method,\n            url,\n            skip_accept_encoding=skip_accept_encoding,\n            skip_host=skip_host\n        )\n        for header, value in headers.items():\n            self.putheader(header, value)\n        if 'transfer-encoding' not in headers:\n            self.putheader('Transfer-Encoding', 'chunked')\n        self.endheaders()\n\n        if body is not None:\n            stringish_types = six.string_types + (bytes,)\n            if isinstance(body, stringish_types):\n                body = (body,)\n            for chunk in body:\n                if not chunk:\n                    continue\n                if not isinstance(chunk, bytes):\n                    chunk = chunk.encode('utf8')\n                len_str = hex(len(chunk))[2:]\n                self.send(len_str.encode('utf-8'))\n                self.send(b'\\r\\n')\n                self.send(chunk)\n                self.send(b'\\r\\n')\n\n        # After the if clause, to always have a closed body\n        self.send(b'0\\r\\n\\r\\n')", "output": "Alternative to the common request method, which sends the\n        body with chunked encoding and not as one block", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symlink_list(load):\n    '''\n    \n    '''\n    if 'env' in load:\n        # \"env\" is not supported; Use \"saltenv\".\n        load.pop('env')\n\n    ret = {}\n    if load['saltenv'] not in __opts__['file_roots'] and '__env__' not in __opts__['file_roots']:\n        return ret\n\n    if 'prefix' in load:\n        prefix = load['prefix'].strip('/')\n    else:\n        prefix = ''\n\n    symlinks = _file_lists(load, 'links')\n    return dict([(key, val)\n                 for key, val in six.iteritems(symlinks)\n                 if key.startswith(prefix)])", "output": "Return a dict of all symlinks based on a given path on the Master", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_script(self):\n        \"\"\"\n\n        \"\"\"\n        if settings.repeat:\n            repeat_fuck = '{} --repeat {}--force-command {}'.format(\n                get_alias(),\n                '--debug ' if settings.debug else '',\n                shell.quote(self.script))\n            return shell.or_(self.script, repeat_fuck)\n        else:\n            return self.script", "output": "Returns fixed commands script.\n\n        If `settings.repeat` is `True`, appends command with second attempt\n        of running fuck in case fixed command fails again.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def db_list(user=None, password=None, host=None, port=None):\n    '''\n    \n\n    '''\n    client = _client(user=user, password=password, host=host, port=port)\n    return client.get_list_database()", "output": "List all InfluxDB databases\n\n    user\n        The user to connect as\n\n    password\n        The password of the user\n\n    host\n        The host to connect to\n\n    port\n        The port to connect to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb08.db_list\n        salt '*' influxdb08.db_list <user> <password> <host> <port>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_show_source(self, checked):\r\n        \"\"\"\"\"\"\r\n        if checked:\r\n            self.switch_to_plain_text()\r\n        self.docstring = not checked\r\n        self.force_refresh()\r\n        self.set_option('rich_mode', not checked)", "output": "Toggle show source code", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sync_role_definitions(self):\n        \"\"\"\"\"\"\n        from superset import conf\n        logging.info('Syncing role definition')\n\n        self.create_custom_permissions()\n\n        # Creating default roles\n        self.set_role('Admin', self.is_admin_pvm)\n        self.set_role('Alpha', self.is_alpha_pvm)\n        self.set_role('Gamma', self.is_gamma_pvm)\n        self.set_role('granter', self.is_granter_pvm)\n        self.set_role('sql_lab', self.is_sql_lab_pvm)\n\n        if conf.get('PUBLIC_ROLE_LIKE_GAMMA', False):\n            self.set_role('Public', self.is_gamma_pvm)\n\n        self.create_missing_perms()\n\n        # commit role and view menu updates\n        self.get_session.commit()\n        self.clean_perms()", "output": "Inits the Superset application with security roles and such", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_jid(jid):\n    '''\n    \n    '''\n    serv = _get_serv(ret=None)\n\n    sql = \"select id, full_ret from returns where jid = '{0}'\".format(jid)\n\n    data = serv.query(sql)\n    ret = {}\n    if data:\n        points = data[0]['points']\n        for point in points:\n            ret[point[3]] = salt.utils.json.loads(point[2])\n\n    return ret", "output": "Return the information returned when the specified job id was executed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def weights_multi_problem_all(labels, taskid=-1):\n  \"\"\"\"\"\"\n  taskid = check_nonnegative(taskid)\n  weights = to_float(tf.not_equal(labels, 0))\n  past_taskid = tf.cumsum(to_float(tf.equal(labels, taskid)), axis=1)\n  # Additionally zero out the task id location\n  past_taskid *= to_float(tf.not_equal(labels, taskid))\n  non_taskid = to_float(labels)\n  example_mask = to_float(tf.not_equal(past_taskid * non_taskid, 0))\n  example_mask = tf.reduce_sum(example_mask, axis=1)\n  example_mask = to_float(\n      tf.greater(example_mask, tf.zeros_like(example_mask)))\n\n  return weights * tf.expand_dims(example_mask, axis=-1)", "output": "Assign weight 1.0 to only examples from the given task.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def do_truncate(env, s, length=255, killwords=False, end='...', leeway=None):\n    \"\"\"\n    \"\"\"\n    if leeway is None:\n        leeway = env.policies['truncate.leeway']\n    assert length >= len(end), 'expected length >= %s, got %s' % (len(end), length)\n    assert leeway >= 0, 'expected leeway >= 0, got %s' % leeway\n    if len(s) <= length + leeway:\n        return s\n    if killwords:\n        return s[:length - len(end)] + end\n    result = s[:length - len(end)].rsplit(' ', 1)[0]\n    return result + end", "output": "Return a truncated copy of the string. The length is specified\n    with the first parameter which defaults to ``255``. If the second\n    parameter is ``true`` the filter will cut the text at length. Otherwise\n    it will discard the last word. If the text was in fact\n    truncated it will append an ellipsis sign (``\"...\"``). If you want a\n    different ellipsis sign than ``\"...\"`` you can specify it using the\n    third parameter. Strings that only exceed the length by the tolerance\n    margin given in the fourth parameter will not be truncated.\n\n    .. sourcecode:: jinja\n\n        {{ \"foo bar baz qux\"|truncate(9) }}\n            -> \"foo...\"\n        {{ \"foo bar baz qux\"|truncate(9, True) }}\n            -> \"foo ba...\"\n        {{ \"foo bar baz qux\"|truncate(11) }}\n            -> \"foo bar baz qux\"\n        {{ \"foo bar baz qux\"|truncate(11, False, '...', 0) }}\n            -> \"foo bar...\"\n\n    The default leeway on newer Jinja2 versions is 5 and was 0 before but\n    can be reconfigured globally.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _GetFieldByName(message_descriptor, field_name):\n  \"\"\"\n  \"\"\"\n  try:\n    return message_descriptor.fields_by_name[field_name]\n  except KeyError:\n    raise ValueError('Protocol message %s has no \"%s\" field.' %\n                     (message_descriptor.name, field_name))", "output": "Returns a field descriptor by field name.\n\n  Args:\n    message_descriptor: A Descriptor describing all fields in message.\n    field_name: The name of the field to retrieve.\n  Returns:\n    The field descriptor associated with the field name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run(self, cmd):\n        '''\n        \n        '''\n\n        if isinstance(cmd, six.string_types):\n            cmd = salt.utils.args.shlex_split(cmd)\n\n        try:\n            log.debug(cmd)\n            p = subprocess.Popen(\n                cmd,\n                shell=True,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE)\n            return p.communicate()\n\n        except (OSError, IOError) as exc:\n            log.debug('Command Failed: %s', ' '.join(cmd))\n            log.debug('Error: %s', exc)\n            raise CommandExecutionError(exc)", "output": "Internal function for running commands. Used by the uninstall function.\n\n        Args:\n            cmd (str, list): The command to run\n\n        Returns:\n            str: The stdout of the command", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def language(self, language):\n        \"\"\"\n        \"\"\"\n        if language is None:\n            raise ValueError(\"Invalid value for `language`, must not be `None`\")  # noqa: E501\n        allowed_values = [\"python\", \"r\", \"rmarkdown\"]  # noqa: E501\n        if language not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `language` ({0}), must be one of {1}\"  # noqa: E501\n                .format(language, allowed_values)\n            )\n\n        self._language = language", "output": "Sets the language of this KernelPushRequest.\n\n        The language that the kernel is written in  # noqa: E501\n\n        :param language: The language of this KernelPushRequest.  # noqa: E501\n        :type: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_mlp():\n    \"\"\"\"\"\"\n    data = mx.symbol.Variable('data')\n    fc1 = mx.symbol.CaffeOp(data_0=data, num_weight=2, name='fc1',\n                            prototxt=\"layer{type:\\\"InnerProduct\\\" inner_product_param{num_output: 128} }\")\n    act1 = mx.symbol.CaffeOp(data_0=fc1, prototxt=\"layer{type:\\\"TanH\\\"}\")\n    fc2 = mx.symbol.CaffeOp(data_0=act1, num_weight=2, name='fc2',\n                            prototxt=\"layer{type:\\\"InnerProduct\\\" inner_product_param{num_output: 64} }\")\n    act2 = mx.symbol.CaffeOp(data_0=fc2, prototxt=\"layer{type:\\\"TanH\\\"}\")\n    fc3 = mx.symbol.CaffeOp(data_0=act2, num_weight=2, name='fc3',\n                            prototxt=\"layer{type:\\\"InnerProduct\\\" inner_product_param{num_output: 10}}\")\n    if use_caffe_loss:\n        label = mx.symbol.Variable('softmax_label')\n        mlp = mx.symbol.CaffeLoss(data=fc3, label=label, grad_scale=1, name='softmax',\n                                  prototxt=\"layer{type:\\\"SoftmaxWithLoss\\\"}\")\n    else:\n        mlp = mx.symbol.SoftmaxOutput(data=fc3, name='softmax')\n    return mlp", "output": "Get multi-layer perceptron", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_cached(self):\n        \"\"\"\n        \n        \"\"\"\n        from dvc.remote.local import RemoteLOCAL\n        from dvc.remote.s3 import RemoteS3\n\n        old = Stage.load(self.repo, self.path)\n        if old._changed_outs():\n            return False\n\n        # NOTE: need to save checksums for deps in order to compare them\n        # with what is written in the old stage.\n        for dep in self.deps:\n            dep.save()\n\n        old_d = old.dumpd()\n        new_d = self.dumpd()\n\n        # NOTE: need to remove checksums from old dict in order to compare\n        # it to the new one, since the new one doesn't have checksums yet.\n        old_d.pop(self.PARAM_MD5, None)\n        new_d.pop(self.PARAM_MD5, None)\n        outs = old_d.get(self.PARAM_OUTS, [])\n        for out in outs:\n            out.pop(RemoteLOCAL.PARAM_CHECKSUM, None)\n            out.pop(RemoteS3.PARAM_CHECKSUM, None)\n\n        if old_d != new_d:\n            return False\n\n        # NOTE: committing to prevent potential data duplication. For example\n        #\n        #    $ dvc config cache.type hardlink\n        #    $ echo foo > foo\n        #    $ dvc add foo\n        #    $ rm -f foo\n        #    $ echo foo > foo\n        #    $ dvc add foo # should replace foo with a link to cache\n        #\n        old.commit()\n\n        return True", "output": "Checks if this stage has been already ran and stored", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max_sliding_window(nums, k):\n    \"\"\"\n    \n    \"\"\"\n    if not nums:\n        return nums\n    queue = collections.deque()\n    res = []\n    for num in nums:\n        if len(queue) < k:\n            queue.append(num)\n        else:\n            res.append(max(queue))\n            queue.popleft()\n            queue.append(num)\n    res.append(max(queue))\n    return res", "output": ":type nums: List[int]\n    :type k: int\n    :rtype: List[int]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_names(names):\n    \"\"\"\n    \n    \"\"\"\n\n    if names is not None:\n        if len(names) != len(set(names)):\n            msg = (\"Duplicate names specified. This \"\n                   \"will raise an error in the future.\")\n            warnings.warn(msg, UserWarning, stacklevel=3)\n\n    return names", "output": "Check if the `names` parameter contains duplicates.\n\n    If duplicates are found, we issue a warning before returning.\n\n    Parameters\n    ----------\n    names : array-like or None\n        An array containing a list of the names used for the output DataFrame.\n\n    Returns\n    -------\n    names : array-like or None\n        The original `names` parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_data(timeseries_length, timeseries_params):\n  \"\"\"\n  \"\"\"\n  x = range(timeseries_length)\n\n  multi_timeseries = []\n  for p in timeseries_params:\n    # Trend\n    y1 = [p[\"m\"] * i + p[\"b\"] for i in x]\n    # Period\n    y2 = [p[\"A\"] * p[\"fn\"](i / p[\"freqcoeff\"]) for i in x]\n    # Noise\n    y3 = np.random.normal(0, p[\"rndA\"], timeseries_length).tolist()\n    # Sum of Trend, Period and Noise. Replace negative values with zero.\n    y = [max(a + b + c, 0) for a, b, c in zip(y1, y2, y3)]\n    multi_timeseries.append(y)\n\n  return multi_timeseries", "output": "Generates synthetic timeseries using input parameters.\n\n  Each generated timeseries has timeseries_length data points.\n  Parameters for each timeseries are specified by timeseries_params.\n\n  Args:\n    timeseries_length: Number of data points to generate for each timeseries.\n    timeseries_params: Parameters used to generate the timeseries. The following\n      parameters need to be specified for each timeseries:\n      m = Slope of the timeseries used to compute the timeseries trend.\n      b = y-intercept of the timeseries used to compute the timeseries trend.\n      A = Timeseries amplitude used to compute timeseries period.\n      freqcoeff = Frequency coefficient used to compute timeseries period.\n      rndA = Random amplitude used to inject noise into the timeseries.\n      fn = Base timeseries function (np.cos or np.sin).\n      Example params for two timeseries.\n      [{\"m\": 0.006, \"b\": 300.0, \"A\":50.0, \"freqcoeff\":1500.0, \"rndA\":15.0,\n      \"fn\": np.sin},\n      {\"m\": 0.000, \"b\": 500.0, \"A\":35.0, \"freqcoeff\":3500.0, \"rndA\":25.0,\n      \"fn\": np.cos}]\n\n  Returns:\n    Multi-timeseries (list of list).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def jump_search(arr,target):\n    \"\"\"\n\n    \"\"\"\n    n = len(arr)\n    block_size = int(math.sqrt(n))\n    block_prev = 0\n    block= block_size\n\n    # return -1 means that array doesn't contain taget value\n    # find block that contains target value\n    \n    if arr[n - 1] < target:\n        return -1  \n    while block <= n and arr[block - 1] < target:\n        block_prev = block\n        block += block_size\n\n    # find target value in block\n    \n    while arr[block_prev] < target :\n        block_prev += 1\n        if block_prev == min(block, n) :\n            return -1\n\n    # if there is target value in array, return it\n    \n    if arr[block_prev] == target :\n        return block_prev\n    else :\n        return -1", "output": "Jump Search\n        Worst-case Complexity: O(\u221an) (root(n))\n        All items in list must be sorted like binary search\n\n        Find block that contains target value and search it linearly in that block\n        It returns a first target value in array\n\n        reference: https://en.wikipedia.org/wiki/Jump_search", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toImage(self, array, origin=\"\"):\n        \"\"\"\n        \n        \"\"\"\n\n        if not isinstance(array, np.ndarray):\n            raise TypeError(\n                \"array argument should be numpy.ndarray; however, it got [%s].\" % type(array))\n\n        if array.ndim != 3:\n            raise ValueError(\"Invalid array shape\")\n\n        height, width, nChannels = array.shape\n        ocvTypes = ImageSchema.ocvTypes\n        if nChannels == 1:\n            mode = ocvTypes[\"CV_8UC1\"]\n        elif nChannels == 3:\n            mode = ocvTypes[\"CV_8UC3\"]\n        elif nChannels == 4:\n            mode = ocvTypes[\"CV_8UC4\"]\n        else:\n            raise ValueError(\"Invalid number of channels\")\n\n        # Running `bytearray(numpy.array([1]))` fails in specific Python versions\n        # with a specific Numpy version, for example in Python 3.6.0 and NumPy 1.13.3.\n        # Here, it avoids it by converting it to bytes.\n        if LooseVersion(np.__version__) >= LooseVersion('1.9'):\n            data = bytearray(array.astype(dtype=np.uint8).ravel().tobytes())\n        else:\n            # Numpy prior to 1.9 don't have `tobytes` method.\n            data = bytearray(array.astype(dtype=np.uint8).ravel())\n\n        # Creating new Row with _create_row(), because Row(name = value, ... )\n        # orders fields by name, which conflicts with expected schema order\n        # when the new DataFrame is created by UDF\n        return _create_row(self.imageFields,\n                           [origin, height, width, nChannels, mode, data])", "output": "Converts an array with metadata to a two-dimensional image.\n\n        :param `numpy.ndarray` array: The array to convert to image.\n        :param str origin: Path to the image, optional.\n        :return: a :class:`Row` that is a two dimensional image.\n\n        .. versionadded:: 2.3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pixel(self, func:PixelFunc, *args, **kwargs)->'ImagePoints':\n        \"\"\n        self = func(self, *args, **kwargs)\n        self.transformed=True\n        return self", "output": "Equivalent to `self = func_flow(self)`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_toml_outline_tables(parsed):\n    \"\"\"\"\"\"\n    def convert_tomlkit_table(section):\n        for key, value in section._body:\n            if not key:\n                continue\n            if hasattr(value, \"keys\") and not isinstance(value, tomlkit.items.InlineTable):\n                table = tomlkit.inline_table()\n                table.update(value.value)\n                section[key.key] = table\n\n    def convert_toml_table(section):\n        for package, value in section.items():\n            if hasattr(value, \"keys\") and not isinstance(value, toml.decoder.InlineTableDict):\n                table = toml.TomlDecoder().get_empty_inline_table()\n                table.update(value)\n                section[package] = table\n\n    is_tomlkit_parsed = isinstance(parsed, tomlkit.container.Container)\n    for section in (\"packages\", \"dev-packages\"):\n        table_data = parsed.get(section, {})\n        if not table_data:\n            continue\n        if is_tomlkit_parsed:\n            convert_tomlkit_table(table_data)\n        else:\n            convert_toml_table(table_data)\n\n    return parsed", "output": "Converts all outline tables to inline tables.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def human_or_01(X, y, model_generator, method_name):\n    \"\"\" \n    \"\"\"\n    return _human_or(X, model_generator, method_name, False, True)", "output": "OR (false/true)\n\n    This tests how well a feature attribution method agrees with human intuition\n    for an OR operation combined with linear effects. This metric deals\n    specifically with the question of credit allocation for the following function\n    when all three inputs are true:\n    if fever: +2 points\n    if cough: +2 points\n    if fever or cough: +6 points\n\n    transform = \"identity\"\n    sort_order = 1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _CheckForOutOfOrderStepAndMaybePurge(self, event):\n    \"\"\"\n    \"\"\"\n    if event.step < self.most_recent_step and event.HasField('summary'):\n      self._Purge(event, by_tags=True)", "output": "Check for out-of-order event.step and discard expired events for tags.\n\n    Check if the event is out of order relative to the global most recent step.\n    If it is, purge outdated summaries for tags that the event contains.\n\n    Args:\n      event: The event to use as reference. If the event is out-of-order, all\n        events with the same tags, but with a greater event.step will be purged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_reference(target_reference, project):\n    \"\"\" \n    \"\"\"\n    # Separate target name from properties override\n    assert isinstance(target_reference, basestring)\n    assert isinstance(project, ProjectTarget)\n    split = _re_separate_target_from_properties.match (target_reference)\n    if not split:\n        raise BaseException (\"Invalid reference: '%s'\" % target_reference)\n\n    id = split.group (1)\n\n    sproperties = []\n\n    if split.group (3):\n        sproperties = property.create_from_strings(feature.split(split.group(3)))\n        sproperties = feature.expand_composites(sproperties)\n\n    # Find the target\n    target = project.find (id)\n\n    return (target, property_set.create(sproperties))", "output": "Given a target_reference, made in context of 'project',\n    returns the AbstractTarget instance that is referred to, as well\n    as properties explicitly specified for this reference.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select(self, crit, axis=0):\n        \"\"\"\n        \n        \"\"\"\n        warnings.warn(\"'select' is deprecated and will be removed in a \"\n                      \"future release. You can use \"\n                      \".loc[labels.map(crit)] as a replacement\",\n                      FutureWarning, stacklevel=2)\n\n        axis = self._get_axis_number(axis)\n        axis_name = self._get_axis_name(axis)\n        axis_values = self._get_axis(axis)\n\n        if len(axis_values) > 0:\n            new_axis = axis_values[\n                np.asarray([bool(crit(label)) for label in axis_values])]\n        else:\n            new_axis = axis_values\n\n        return self.reindex(**{axis_name: new_axis})", "output": "Return data corresponding to axis labels matching criteria.\n\n        .. deprecated:: 0.21.0\n            Use df.loc[df.index.map(crit)] to select via labels\n\n        Parameters\n        ----------\n        crit : function\n            To be called on each index (label). Should return True or False\n        axis : int\n\n        Returns\n        -------\n        selection : same type as caller", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self):\n        ''' \n\n        '''\n        return RGB(self.r, self.g, self.b, self.a)", "output": "Return a copy of this color value.\n\n        Returns:\n            :class:`~bokeh.colors.rgb.RGB`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insert_order(self, order):\n        '''\n        \n        '''\n        #print(\"     *>> QAOrder!insert_order  {}\".format(order))\n        # QUEUED = 300  # queued \u7528\u4e8e\u8868\u793a\u5728order_queue\u4e2d \u5b9e\u9645\u8868\u8fbe\u7684\u610f\u601d\u662f\u8ba2\u5355\u5b58\u6d3b \u5f85\u6210\u4ea4\n        #order.status = ORDER_STATUS.QUEUED\n        # \ud83d\udee0 todo \u662f\u4e3a\u4e86\u901f\u5ea6\u5feb\u628aorder\u5bf9\u8c61\u8f6c\u6362\u6210 df \u5bf9\u8c61\u7684\u5417\uff1f\n        #self.queue_df = self.queue_df.append(order.to_df(), ignore_index=True)\n        #self.queue_df.set_index('order_id', drop=True, inplace=True)\n        if order is not None:\n            self.order_list[order.order_id] = order\n            return order\n        else:\n            print('QAERROR Wrong for get None type while insert order to Queue')", "output": ":param order: QA_Order\u7c7b\u578b\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_file(self):\n        ''''''\n        try:\n            with open(self.experiment_file, 'w') as file:\n                json.dump(self.experiments, file)\n        except IOError as error:\n            print('Error:', error)\n            return", "output": "save config to local file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_bytes(self, bytes_data, exclude=tuple(), disable=None, **kwargs):\n        \"\"\"\n        \"\"\"\n        if disable is not None:\n            deprecation_warning(Warnings.W014)\n            exclude = disable\n        deserializers = OrderedDict()\n        deserializers[\"meta.json\"] = lambda b: self.meta.update(srsly.json_loads(b))\n        deserializers[\"vocab\"] = lambda b: self.vocab.from_bytes(b) and _fix_pretrained_vectors_name(self)\n        deserializers[\"tokenizer\"] = lambda b: self.tokenizer.from_bytes(b, exclude=[\"vocab\"])\n        for name, proc in self.pipeline:\n            if name in exclude:\n                continue\n            if not hasattr(proc, \"from_bytes\"):\n                continue\n            deserializers[name] = lambda b, proc=proc: proc.from_bytes(b, exclude=[\"vocab\"])\n        exclude = util.get_serialization_exclude(deserializers, exclude, kwargs)\n        util.from_bytes(bytes_data, deserializers, exclude)\n        return self", "output": "Load state from a binary string.\n\n        bytes_data (bytes): The data to load from.\n        exclude (list): Names of components or serialization fields to exclude.\n        RETURNS (Language): The `Language` object.\n\n        DOCS: https://spacy.io/api/language#from_bytes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_path_iterator(file_path: str) -> Iterator[str]:\n        \"\"\"\n        \n        \"\"\"\n        logger.info(\"Reading CONLL sentences from dataset files at: %s\", file_path)\n        for root, _, files in list(os.walk(file_path)):\n            for data_file in files:\n                # These are a relic of the dataset pre-processing. Every\n                # file will be duplicated - one file called filename.gold_skel\n                # and one generated from the preprocessing called filename.gold_conll.\n                if not data_file.endswith(\"gold_conll\"):\n                    continue\n\n                yield os.path.join(root, data_file)", "output": "An iterator returning file_paths in a directory\n        containing CONLL-formatted files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_website(Bucket, ErrorDocument=None, IndexDocument=None,\n           RedirectAllRequestsTo=None, RoutingRules=None,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        WebsiteConfiguration = {}\n        for key in ('ErrorDocument', 'IndexDocument',\n                    'RedirectAllRequestsTo', 'RoutingRules'):\n            val = locals()[key]\n            if val is not None:\n                if isinstance(val, six.string_types):\n                    WebsiteConfiguration[key] = salt.utils.json.loads(val)\n                else:\n                    WebsiteConfiguration[key] = val\n        conn.put_bucket_website(Bucket=Bucket,\n                WebsiteConfiguration=WebsiteConfiguration)\n        return {'updated': True, 'name': Bucket}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a valid config, update the website configuration for a bucket.\n\n    Returns {updated: true} if website configuration was updated and returns\n    {updated: False} if website configuration was not updated.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.put_website my_bucket IndexDocument='{\"Suffix\":\"index.html\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resize(self, size:Union[int,TensorImageSize]) -> 'ImagePoints':\n        \"\"\n        if isinstance(size, int): size=(1, size, size)\n        self._flow.size = size[1:]\n        return self", "output": "Resize the image to `size`, size can be a single int.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot_lr(self):\n        ''''''\n        if not in_ipynb():\n            plt.switch_backend('agg')\n        if self.record_mom:\n            fig, axs = plt.subplots(1,2,figsize=(12,4))\n            for i in range(0,2): axs[i].set_xlabel('iterations')\n            axs[0].set_ylabel('learning rate')\n            axs[1].set_ylabel('momentum')\n            axs[0].plot(self.iterations,self.lrs)\n            axs[1].plot(self.iterations,self.momentums)   \n        else:\n            plt.xlabel(\"iterations\")\n            plt.ylabel(\"learning rate\")\n            plt.plot(self.iterations, self.lrs)\n        if not in_ipynb():\n            plt.savefig(os.path.join(self.save_path, 'lr_plot.png'))", "output": "Plots learning rate in jupyter notebook or console, depending on the enviroment of the learner.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_interface_info(interface):\n    '''\n    \n    '''\n    adapter_mode = _get_adapter_mode_info(interface.name)\n    if adapter_mode == 'disabled':\n        return _get_base_interface_info(interface)\n    elif adapter_mode == 'ethercat':\n        return _get_ethercat_interface_info(interface)\n    return _get_tcpip_interface_info(interface)", "output": "return details about given interface", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def qualified_name(self):\n        \"\"\"\n        \"\"\"\n\n        parent = self.full_parent_name\n        if parent:\n            return parent + ' ' + self.name\n        else:\n            return self.name", "output": "Retrieves the fully qualified command name.\n\n        This is the full parent name with the command name as well.\n        For example, in ``?one two three`` the qualified name would be\n        ``one two three``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prefix_from_ip_int(cls, ip_int):\n        \"\"\"\n        \"\"\"\n        trailing_zeroes = _count_righthand_zero_bits(ip_int,\n                                                     cls._max_prefixlen)\n        prefixlen = cls._max_prefixlen - trailing_zeroes\n        leading_ones = ip_int >> trailing_zeroes\n        all_ones = (1 << prefixlen) - 1\n        if leading_ones != all_ones:\n            byteslen = cls._max_prefixlen // 8\n            details = _compat_to_bytes(ip_int, byteslen, 'big')\n            msg = 'Netmask pattern %r mixes zeroes & ones'\n            raise ValueError(msg % details)\n        return prefixlen", "output": "Return prefix length from the bitwise netmask.\n\n        Args:\n            ip_int: An integer, the netmask in expanded bitwise format\n\n        Returns:\n            An integer, the prefix length.\n\n        Raises:\n            ValueError: If the input intermingles zeroes & ones", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def label_from_df(self, cols:IntsOrStrs=1, label_cls:Callable=None, **kwargs):\n        \"\"\n        labels = self.inner_df.iloc[:,df_names_to_idx(cols, self.inner_df)]\n        assert labels.isna().sum().sum() == 0, f\"You have NaN values in column(s) {cols} of your dataframe, please fix it.\"\n        if is_listy(cols) and len(cols) > 1 and (label_cls is None or label_cls == MultiCategoryList):\n            new_kwargs,label_cls = dict(one_hot=True, classes= cols),MultiCategoryList\n            kwargs = {**new_kwargs, **kwargs}\n        return self._label_from_list(_maybe_squeeze(labels), label_cls=label_cls, **kwargs)", "output": "Label `self.items` from the values in `cols` in `self.inner_df`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_esxi_proxy_details():\n    '''\n    \n    '''\n    det = __proxy__['esxi.get_details']()\n    host = det.get('host')\n    if det.get('vcenter'):\n        host = det['vcenter']\n    esxi_hosts = None\n    if det.get('esxi_host'):\n        esxi_hosts = [det['esxi_host']]\n    return host, det.get('username'), det.get('password'), \\\n            det.get('protocol'), det.get('port'), det.get('mechanism'), \\\n            det.get('principal'), det.get('domain'), esxi_hosts", "output": "Returns the running esxi's proxy details", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_indexer_for(self, target, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        if self.is_unique:\n            return self.get_indexer(target, **kwargs)\n        indexer, _ = self.get_indexer_non_unique(target, **kwargs)\n        return indexer", "output": "Guaranteed return of an indexer even when non-unique.\n\n        This dispatches to get_indexer or get_indexer_nonunique\n        as appropriate.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def competition_download_files(self,\n                                   competition,\n                                   path=None,\n                                   force=False,\n                                   quiet=True):\n        \"\"\" \n        \"\"\"\n        files = self.competition_list_files(competition)\n        if not files:\n            print('This competition does not have any available data files')\n        for file_name in files:\n            self.competition_download_file(competition, file_name.ref, path,\n                                           force, quiet)", "output": "a wrapper to competition_download_file to download all competition\n            files.\n\n            Parameters\n            =========\n            competition: the name of the competition\n            path: a path to download the file to\n            force: force the download if the file already exists (default False)\n            quiet: suppress verbose output (default is True)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_password(username,\n                 password,\n                 encrypted=False,\n                 role=None,\n                 crypt_salt=None,\n                 algorithm='sha256',\n                 **kwargs):\n    '''\n    \n    '''\n    password_line = get_user(username, **kwargs)\n    if encrypted is False:\n        if crypt_salt is None:\n            # NXOS does not like non alphanumeric characters.  Using the random module from pycrypto\n            # can lead to having non alphanumeric characters in the salt for the hashed password.\n            crypt_salt = secure_password(8, use_random=False)\n        hashed_pass = gen_hash(crypt_salt=crypt_salt, password=password, algorithm=algorithm)\n    else:\n        hashed_pass = password\n    password_line = 'username {0} password 5 {1}'.format(username, hashed_pass)\n    if role is not None:\n        password_line += ' role {0}'.format(role)\n    return config(password_line, **kwargs)", "output": "Set users password on switch.\n\n    username\n        Username to configure\n\n    password\n        Password to configure for username\n\n    encrypted\n        Whether or not to encrypt the password\n        Default: False\n\n    role\n        Configure role for the username\n        Default: None\n\n    crypt_salt\n        Configure crypt_salt setting\n        Default: None\n\n    alogrithm\n        Encryption algorithm\n        Default: sha256\n\n    no_save_config\n        If True, don't save configuration commands to startup configuration.\n        If False, save configuration to startup configuration.\n        Default: False\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd set_password admin TestPass\n        salt '*' nxos.cmd set_password admin \\\\\n            password='$5$2fWwO2vK$s7.Hr3YltMNHuhywQQ3nfOd.gAPHgs3SOBYYdGT3E.A' \\\\\n            encrypted=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_multilevel_rpn_anchor_input(im, boxes, is_crowd):\n    \"\"\"\n    \n    \"\"\"\n    boxes = boxes.copy()\n    anchors_per_level = get_all_anchors_fpn()\n    flatten_anchors_per_level = [k.reshape((-1, 4)) for k in anchors_per_level]\n    all_anchors_flatten = np.concatenate(flatten_anchors_per_level, axis=0)\n\n    inside_ind, inside_anchors = filter_boxes_inside_shape(all_anchors_flatten, im.shape[:2])\n    anchor_labels, anchor_gt_boxes = get_anchor_labels(inside_anchors, boxes[is_crowd == 0], boxes[is_crowd == 1])\n\n    # map back to all_anchors, then split to each level\n    num_all_anchors = all_anchors_flatten.shape[0]\n    all_labels = -np.ones((num_all_anchors, ), dtype='int32')\n    all_labels[inside_ind] = anchor_labels\n    all_boxes = np.zeros((num_all_anchors, 4), dtype='float32')\n    all_boxes[inside_ind] = anchor_gt_boxes\n\n    start = 0\n    multilevel_inputs = []\n    for level_anchor in anchors_per_level:\n        assert level_anchor.shape[2] == len(cfg.RPN.ANCHOR_RATIOS)\n        anchor_shape = level_anchor.shape[:3]   # fHxfWxNUM_ANCHOR_RATIOS\n        num_anchor_this_level = np.prod(anchor_shape)\n        end = start + num_anchor_this_level\n        multilevel_inputs.append(\n            (all_labels[start: end].reshape(anchor_shape),\n             all_boxes[start: end, :].reshape(anchor_shape + (4,))\n             ))\n        start = end\n    assert end == num_all_anchors, \"{} != {}\".format(end, num_all_anchors)\n    return multilevel_inputs", "output": "Args:\n        im: an image\n        boxes: nx4, floatbox, gt. shoudn't be changed\n        is_crowd: n,\n\n    Returns:\n        [(fm_labels, fm_boxes)]: Returns a tuple for each FPN level.\n        Each tuple contains the anchor labels and target boxes for each pixel in the featuremap.\n\n        fm_labels: fHxfWx NUM_ANCHOR_RATIOS\n        fm_boxes: fHxfWx NUM_ANCHOR_RATIOS x4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def patch_priority_class(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.patch_priority_class_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.patch_priority_class_with_http_info(name, body, **kwargs)\n            return data", "output": "partially update the specified PriorityClass\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.patch_priority_class(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the PriorityClass (required)\n        :param object body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).\n        :param bool force: Force is going to \\\"force\\\" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.\n        :return: V1PriorityClass\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(self, expr, context):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(expr, string_types):\n            if expr[0] in '\\'\"':\n                result = expr[1:-1]\n            else:\n                if expr not in context:\n                    raise SyntaxError('unknown variable: %s' % expr)\n                result = context[expr]\n        else:\n            assert isinstance(expr, dict)\n            op = expr['op']\n            if op not in self.operations:\n                raise NotImplementedError('op not implemented: %s' % op)\n            elhs = expr['lhs']\n            erhs = expr['rhs']\n            if _is_literal(expr['lhs']) and _is_literal(expr['rhs']):\n                raise SyntaxError('invalid comparison: %s %s %s' % (elhs, op, erhs))\n\n            lhs = self.evaluate(elhs, context)\n            rhs = self.evaluate(erhs, context)\n            result = self.operations[op](lhs, rhs)\n        return result", "output": "Evaluate a marker expression returned by the :func:`parse_requirement`\n        function in the specified context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def head(self, n=5):\r\n        \"\"\"\r\n        \"\"\"\r\n        if n >= len(self.index):\r\n            return self.copy()\r\n        return self.__constructor__(query_compiler=self._query_compiler.head(n))", "output": "Get the first n rows of the DataFrame.\r\n\r\n        Args:\r\n            n (int): The number of rows to return.\r\n\r\n        Returns:\r\n            A new DataFrame with the first n rows of the DataFrame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_datastore_full(kwargs=None, call=None, datastore=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The list_datastore_full function must be called with '\n            '-f or --function.'\n        )\n\n    if kwargs:\n        datastore = kwargs.get('datastore', None)\n\n    if not datastore:\n        raise SaltCloudSystemExit(\n            'The list_datastore_full function requires a datastore'\n        )\n\n    return {datastore: salt.utils.vmware.list_datastore_full(_get_si(), datastore)}", "output": "Returns a dictionary with basic information for the given datastore\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f list_datastore_full my-vmware-config datastore=datastore-name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_auth_line(key, enc, comment, options):\n    '''\n    \n    '''\n    line = ''\n    if options:\n        line += '{0} '.format(','.join(options))\n    line += '{0} {1} {2}\\n'.format(enc, key, comment)\n    return line", "output": "Properly format user input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __assert_empty(returned):\n        '''\n        \n        '''\n        result = \"Pass\"\n        try:\n            assert (not returned), \"{0} is not empty\".format(returned)\n        except AssertionError as err:\n            result = \"Fail: \" + six.text_type(err)\n        return result", "output": "Test if a returned value is empty", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_updates(rule):\n    '''\n    \n    '''\n    rules = shlex.split(rule)\n    rules.pop(0)\n    return {'url': rules[0]} if rules else True", "output": "Parse the updates line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set(self, value):\n        '''\n        \n\n        '''\n        user = self.USER\n        try:\n            uid = pwd.getpwnam(user).pw_uid\n        except KeyError:\n            log.info('User does not exist')\n            result = {}\n            result['retcode'] = 1\n            result['stdout'] = 'User {0} does not exist'.format(user)\n            return result\n\n        cmd = self.gsetting_command + ['set', self.SCHEMA, self.KEY, value]\n        environ = {}\n        environ['XDG_RUNTIME_DIR'] = '/run/user/{0}'.format(uid)\n        result = __salt__['cmd.run_all'](cmd, runas=user, env=environ, python_shell=False)\n        return result", "output": "set the value for user in gsettings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_log_content(config_file_name):\n    ''''''\n    stdout_full_path, stderr_full_path = get_log_path(config_file_name)\n    print_normal(' Stdout:')\n    print(check_output_command(stdout_full_path))\n    print('\\n\\n')\n    print_normal(' Stderr:')\n    print(check_output_command(stderr_full_path))", "output": "print log information", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def login(self, token, *, bot=True):\n        \"\"\"\n        \"\"\"\n\n        log.info('logging in using static token')\n        await self.http.static_login(token, bot=bot)\n        self._connection.is_bot = bot", "output": "|coro|\n\n        Logs in the client with the specified credentials.\n\n        This function can be used in two different ways.\n\n        .. warning::\n\n            Logging on with a user token is against the Discord\n            `Terms of Service <https://support.discordapp.com/hc/en-us/articles/115002192352>`_\n            and doing so might potentially get your account banned.\n            Use this at your own risk.\n\n        Parameters\n        -----------\n        token: :class:`str`\n            The authentication token. Do not prefix this token with\n            anything as the library will do it for you.\n        bot: :class:`bool`\n            Keyword argument that specifies if the account logging on is a bot\n            token or not.\n\n        Raises\n        ------\n        LoginFailure\n            The wrong credentials are passed.\n        HTTPException\n            An unknown HTTP related error occurred,\n            usually when it isn't 200 or the known incorrect credentials\n            passing status code.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_disks(disk_ids=None, scsi_addresses=None, service_instance=None):\n    '''\n    \n    '''\n    host_ref = _get_proxy_target(service_instance)\n    hostname = __proxy__['esxi.get_details']()['esxi_host']\n    log.trace('Retrieving disks if host \\'%s\\'', hostname)\n    log.trace('disk ids = %s', disk_ids)\n    log.trace('scsi_addresses = %s', scsi_addresses)\n    # Default to getting all disks if no filtering is done\n    get_all_disks = True if not (disk_ids or scsi_addresses) else False\n    ret_list = []\n    scsi_address_to_lun = salt.utils.vmware.get_scsi_address_to_lun_map(\n        host_ref, hostname=hostname)\n    canonical_name_to_scsi_address = {\n        lun.canonicalName: scsi_addr\n        for scsi_addr, lun in six.iteritems(scsi_address_to_lun)}\n    for d in salt.utils.vmware.get_disks(host_ref, disk_ids, scsi_addresses,\n                                         get_all_disks):\n        ret_list.append({'id': d.canonicalName,\n                         'scsi_address':\n                         canonical_name_to_scsi_address[d.canonicalName]})\n    return ret_list", "output": "Returns a list of dict representations of the disks in an ESXi host.\n    The list of disks can be filtered by disk canonical names or\n    scsi addresses.\n\n    disk_ids:\n        List of disk canonical names to be retrieved. Default is None.\n\n    scsi_addresses\n        List of scsi addresses of disks to be retrieved. Default is None\n\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_disks\n\n        salt '*' vsphere.list_disks disk_ids='[naa.00, naa.001]'\n\n        salt '*' vsphere.list_disks\n            scsi_addresses='[vmhba0:C0:T0:L0, vmhba1:C0:T0:L0]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_json_content(file_path):\n    \"\"\"\n    \"\"\"\n    try:\n        with open(file_path, 'r') as file:\n            return json.load(file)\n    except TypeError as err:\n        print('Error: ', err)\n        return None", "output": "Load json file content\n    \n    Parameters\n    ----------\n    file_path:\n        path to the file\n    \n    Raises\n    ------\n    TypeError\n        Error with the file path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def length_limits(max_length_limit, length_limit_step):\n    \"\"\"\"\"\"\n    string_len = len(str(max_length_limit))\n    return [\n        str(i).zfill(string_len) for i in\n        xrange(\n            length_limit_step,\n            max_length_limit + length_limit_step - 1,\n            length_limit_step\n        )\n    ]", "output": "Generates the length limits", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _close(self):\n        \"\"\"\n        \n        \"\"\"\n        # Some file-like objects might not support flush\n        try:\n            self._file.flush()\n        except AttributeError:\n            pass\n        if self._own_file:\n            self._file.close()", "output": "Close the file if it was created by the writer.\n\n        If a buffer or file-like object was passed in, for example a GzipFile,\n        then leave this file open for the caller to close. In either case,\n        attempt to flush the file contents to ensure they are written to disk\n        (if supported)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_saved_model(path):\n  \"\"\"\"\"\"\n  # Based on tensorflow/python/saved_model/loader.py implementation.\n  path_to_pb = _get_saved_model_proto_path(path)\n  file_content = tf_v1.gfile.Open(path_to_pb, \"rb\").read()\n  saved_model = saved_model_pb2.SavedModel()\n  try:\n    saved_model.ParseFromString(file_content)\n  except message.DecodeError as e:\n    raise IOError(\"Cannot parse file %s: %s.\" % (path_to_pb, str(e)))\n  return saved_model", "output": "Reads the savedmodel.pb file containing `SavedModel`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_dataset(args, dataset):\n    \"\"\"\n    \n    \"\"\"\n    path = os.path.join(vars(args)[dataset])\n    logger.info('reading data from {}'.format(path))\n    examples = [line.strip().split('\\t') for line in open(path)]\n    if args.max_num_examples > 0:\n        examples = examples[:args.max_num_examples]\n    # NOTE: assume data has been tokenized\n    dataset = gluon.data.SimpleDataset([(e[0], e[1], LABEL_TO_IDX[e[2]]) for e in examples])\n    dataset = dataset.transform(lambda s1, s2, label: (\n        ['NULL'] + s1.lower().split(),\n        ['NULL'] + s2.lower().split(), label),\n                                lazy=False)\n    logger.info('read {} examples'.format(len(dataset)))\n    return dataset", "output": "Read dataset from tokenized files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _assert_take_fillable(self, values, indices, allow_fill=True,\n                              fill_value=None, na_value=np.nan):\n        \"\"\"\n        \n        \"\"\"\n        indices = ensure_platform_int(indices)\n\n        # only fill if we are passing a non-None fill_value\n        if allow_fill and fill_value is not None:\n            if (indices < -1).any():\n                msg = ('When allow_fill=True and fill_value is not None, '\n                       'all indices must be >= -1')\n                raise ValueError(msg)\n            taken = algos.take(values,\n                               indices,\n                               allow_fill=allow_fill,\n                               fill_value=na_value)\n        else:\n            taken = values.take(indices)\n        return taken", "output": "Internal method to handle NA filling of take.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000,\n                  sequenceCol=\"sequence\"):\n        \"\"\"\n        \n        \"\"\"\n        kwargs = self._input_kwargs\n        return self._set(**kwargs)", "output": "setParams(self, minSupport=0.1, maxPatternLength=10, maxLocalProjDBSize=32000000, \\\n                  sequenceCol=\"sequence\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setModel(self, model, relayout=True):\r\n        \"\"\"\"\"\"\r\n        self._model = model\r\n        sel_model = self.dataTable.selectionModel()\r\n        sel_model.currentColumnChanged.connect(\r\n                self._resizeCurrentColumnToContents)\r\n\r\n        # Asociate the models (level, vertical index and horizontal header)\r\n        # with its corresponding view.\r\n        self._reset_model(self.table_level, DataFrameLevelModel(model,\r\n                                                                self.palette(),\r\n                                                                self.font()))\r\n        self._reset_model(self.table_header, DataFrameHeaderModel(\r\n                                                            model,\r\n                                                            0,\r\n                                                            self.palette()))\r\n        self._reset_model(self.table_index, DataFrameHeaderModel(\r\n                                                            model,\r\n                                                            1,\r\n                                                            self.palette()))\r\n\r\n        # Needs to be called after setting all table models\r\n        if relayout:\r\n            self._update_layout()", "output": "Set the model for the data, header/index and level views.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dividend_receivable(self):\n        \"\"\"\n        \n        \"\"\"\n        return sum(d['quantity'] * d['dividend_per_share'] for d in six.itervalues(self._dividend_receivable))", "output": "[float] \u6295\u8d44\u7ec4\u5408\u5728\u5206\u7ea2\u73b0\u91d1\u6536\u5230\u8d26\u9762\u4e4b\u524d\u7684\u5e94\u6536\u5206\u7ea2\u90e8\u5206\u3002\u5177\u4f53\u7ec6\u8282\u5728\u5206\u7ea2\u90e8\u5206", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def explainParam(self, param):\n        \"\"\"\n        \n        \"\"\"\n        param = self._resolveParam(param)\n        values = []\n        if self.isDefined(param):\n            if param in self._defaultParamMap:\n                values.append(\"default: %s\" % self._defaultParamMap[param])\n            if param in self._paramMap:\n                values.append(\"current: %s\" % self._paramMap[param])\n        else:\n            values.append(\"undefined\")\n        valueStr = \"(\" + \", \".join(values) + \")\"\n        return \"%s: %s %s\" % (param.name, param.doc, valueStr)", "output": "Explains a single param and returns its name, doc, and optional\n        default value and user-supplied value in a string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_junction(arg):\n    '''\n    \n    '''\n    return isinstance(arg, dict) and len(arg) == 1 and next(six.iterkeys(arg)) == 'junction'", "output": "Return True, if arg is a junction statement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_namespaced_deployment_scale(self, name, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_deployment_scale_with_http_info(name, namespace, body, **kwargs)\n        else:\n            (data) = self.replace_namespaced_deployment_scale_with_http_info(name, namespace, body, **kwargs)\n            return data", "output": "replace scale of the specified Deployment\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_deployment_scale(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Scale (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1Scale body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1Scale\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_det_jacobian(self, inputs):\n    \"\"\"\"\"\"\n    del inputs  # unused\n    # Number of events is number of all elements excluding the batch and\n    # channel dimensions.\n    num_events = tf.reduce_prod(tf.shape(inputs)[1:-1])\n    log_det_jacobian = num_events * tf.reduce_sum(self.log_scale)\n    return log_det_jacobian", "output": "Returns log det | dx / dy | = num_events * sum log | scale |.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_network_settings(**settings):\n    '''\n    \n    '''\n    if __grains__['lsb_distrib_id'] == 'nilrt':\n        raise salt.exceptions.CommandExecutionError('Not supported in this version.')\n    if 'require_reboot' not in settings:\n        settings['require_reboot'] = False\n\n    if 'apply_hostname' not in settings:\n        settings['apply_hostname'] = False\n\n    hostname_res = True\n    if settings['apply_hostname'] in _CONFIG_TRUE:\n        if 'hostname' in settings:\n            hostname_res = __salt__['network.mod_hostname'](settings['hostname'])\n        else:\n            log.warning(\n                'The network state sls is trying to apply hostname '\n                'changes but no hostname is defined.'\n            )\n            hostname_res = False\n\n    res = True\n    if settings['require_reboot'] in _CONFIG_TRUE:\n        log.warning(\n            'The network state sls is requiring a reboot of the system to '\n            'properly apply network configuration.'\n        )\n        res = True\n    else:\n        stop = __salt__['service.stop']('connman')\n        time.sleep(2)\n        res = stop and __salt__['service.start']('connman')\n\n    return hostname_res and res", "output": "Apply global network configuration.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.apply_network_settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_log_monitor(redis_address,\n                      logs_dir,\n                      stdout_file=None,\n                      stderr_file=None,\n                      redis_password=None):\n    \"\"\"\n    \"\"\"\n    log_monitor_filepath = os.path.join(\n        os.path.dirname(os.path.abspath(__file__)), \"log_monitor.py\")\n    command = [\n        sys.executable, \"-u\", log_monitor_filepath,\n        \"--redis-address={}\".format(redis_address),\n        \"--logs-dir={}\".format(logs_dir)\n    ]\n    if redis_password:\n        command += [\"--redis-password\", redis_password]\n    process_info = start_ray_process(\n        command,\n        ray_constants.PROCESS_TYPE_LOG_MONITOR,\n        stdout_file=stdout_file,\n        stderr_file=stderr_file)\n    return process_info", "output": "Start a log monitor process.\n\n    Args:\n        redis_address (str): The address of the Redis instance.\n        logs_dir (str): The directory of logging files.\n        stdout_file: A file handle opened for writing to redirect stdout to. If\n            no redirection should happen, then this should be None.\n        stderr_file: A file handle opened for writing to redirect stderr to. If\n            no redirection should happen, then this should be None.\n        redis_password (str): The password of the redis server.\n\n    Returns:\n        ProcessInfo for the process that was started.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_keys_split(self, decoded):\n        \"\"\"\n        \n        \"\"\"\n        bad_keys = set(decoded.keys()).difference(set(self._split_keys))\n        if bad_keys:\n            bad_keys = \", \".join(bad_keys)\n            raise ValueError(\"JSON data had unexpected key(s): {bad_keys}\"\n                             .format(bad_keys=pprint_thing(bad_keys)))", "output": "Checks that dict has only the appropriate keys for orient='split'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_command(self, name):\n        \"\"\"\n        \"\"\"\n        command = self.all_commands.pop(name, None)\n\n        # does not exist\n        if command is None:\n            return None\n\n        if name in command.aliases:\n            # we're removing an alias so we don't want to remove the rest\n            return command\n\n        # we're not removing the alias so let's delete the rest of them.\n        for alias in command.aliases:\n            self.all_commands.pop(alias, None)\n        return command", "output": "Remove a :class:`.Command` or subclasses from the internal list\n        of commands.\n\n        This could also be used as a way to remove aliases.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The name of the command to remove.\n\n        Returns\n        --------\n        :class:`.Command` or subclass\n            The command that was removed. If the name is not valid then\n            `None` is returned instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def fetch_emoji(self, emoji_id):\n        \"\"\"\n        \"\"\"\n        data = await self._state.http.get_custom_emoji(self.id, emoji_id)\n        return Emoji(guild=self, state=self._state, data=data)", "output": "|coro|\n\n        Retrieves a custom :class:`Emoji` from the guild.\n\n        .. note::\n\n            This method is an API call.\n            For general usage, consider iterating over :attr:`emojis` instead.\n\n        Parameters\n        -------------\n        emoji_id: :class:`int`\n            The emoji's ID.\n\n        Raises\n        ---------\n        NotFound\n            The emoji requested could not be found.\n        HTTPException\n            An error occurred fetching the emoji.\n\n        Returns\n        --------\n        :class:`Emoji`\n            The retrieved emoji.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def strip_extras(requirement):\n    \"\"\"\n    \"\"\"\n    line = requirement.as_line()\n    new = type(requirement).from_line(line)\n    new.extras = None\n    return new", "output": "Returns a new requirement object with extras removed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_list(self, key):\r\n        \"\"\"\"\"\"\r\n        data = self.model.get_data()\r\n        return isinstance(data[key], (tuple, list))", "output": "Return True if variable is a list or a tuple", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_domain_name_list(list_name):\n    '''\n    \n\n    '''\n    payload = {\"jsonrpc\": \"2.0\",\n               \"id\": \"ID0\",\n               \"method\": \"add_policy_domain_names_list\",\n               \"params\": [{\"list_name\": list_name}]}\n\n    response = __proxy__['bluecoat_sslv.call'](payload, True)\n\n    return _validate_change_result(response)", "output": "Add a list of policy domain names.\n\n    list_name(str): The name of the specific policy domain name list to add.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' bluecoat_sslv.add_domain_name_list MyDomainNameList", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_uniform(low, high, size:Optional[List[int]]=None)->FloatOrTensor:\n    \"\"\n    res = uniform(log(low), log(high), size)\n    return exp(res) if size is None else res.exp_()", "output": "Draw 1 or shape=`size` random floats from uniform dist: min=log(`low`), max=log(`high`).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_event_source_mapping(UUID=None, EventSourceArn=None, FunctionName=None,\n                                region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    ids = _get_ids(UUID, EventSourceArn=EventSourceArn,\n                   FunctionName=FunctionName)\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        for id in ids:\n            conn.delete_event_source_mapping(UUID=id)\n        return {'deleted': True}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given an event source mapping ID or an event source ARN and FunctionName,\n    delete the event source mapping\n\n    Returns {deleted: true} if the mapping was deleted and returns\n    {deleted: false} if the mapping was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_lambda.delete_event_source_mapping 260c423d-e8b5-4443-8d6a-5e91b9ecd0fa", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _diff(state_data, resource_object):\n    '''\n    '''\n    objects_differ = None\n\n    for k, v in state_data.items():\n        if k == 'escalation_rules':\n            v = _escalation_rules_to_string(v)\n            resource_value = _escalation_rules_to_string(resource_object[k])\n        else:\n            if k not in resource_object.keys():\n                objects_differ = True\n            else:\n                resource_value = resource_object[k]\n        if v != resource_value:\n            objects_differ = '{0} {1} {2}'.format(k, v, resource_value)\n            break\n\n    if objects_differ:\n        return state_data\n    else:\n        return {}", "output": "helper method to compare salt state info with the PagerDuty API json structure,\n    and determine if we need to update.\n\n    returns the dict to pass to the PD API to perform the update, or empty dict if no update.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_data_frame_transform_stats(self, transform_id=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\"_data_frame\", \"transforms\", transform_id, \"_stats\"),\n            params=params,\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/get-data-frame-transform-stats.html>`_\n\n        :arg transform_id: The id of the transform for which to get stats.\n            '_all' or '*' implies all transforms", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expected_bar_value(asset_id, date, colname):\n    \"\"\"\n    \n    \"\"\"\n    from_asset = asset_id * 100000\n    from_colname = OHLCV.index(colname) * 1000\n    from_date = (date - PSEUDO_EPOCH).days\n    return from_asset + from_colname + from_date", "output": "Check that the raw value for an asset/date/column triple is as\n    expected.\n\n    Used by tests to verify data written by a writer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def min_query(self, query_type='list_nodes_min'):\n        '''\n        \n        '''\n        mapper = salt.cloud.Map(self._opts_defaults())\n        mapper.opts['selected_query_option'] = 'list_nodes_min'\n        return mapper.map_providers_parallel(query_type)", "output": "Query select instance information", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def column_family_definition(keyspace, column_family):\n    '''\n    \n\n    '''\n    sys = _sys_mgr()\n\n    try:\n        return vars(sys.get_keyspace_column_families(keyspace)[column_family])\n    except Exception:\n        log.debug('Invalid Keyspace/CF combination')\n        return None", "output": "Return a dictionary of column family definitions for the given\n    keyspace/column_family\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cassandra.column_family_definition <keyspace> <column_family>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split(str, pattern, limit=-1):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.split(_to_java_column(str), pattern, limit))", "output": "Splits str around matches of the given pattern.\n\n    :param str: a string expression to split\n    :param pattern: a string representing a regular expression. The regex string should be\n        a Java regular expression.\n    :param limit: an integer which controls the number of times `pattern` is applied.\n\n        * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n                         resulting array's last entry will contain all input beyond the last\n                         matched pattern.\n        * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n                          array can be of any size.\n\n    .. versionchanged:: 3.0\n       `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n\n    >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n    >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n    [Row(s=[u'one', u'twoBthreeC'])]\n    >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n    [Row(s=[u'one', u'two', u'three', u''])]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def with_rank(self, rank):\n        \"\"\"\n        \"\"\"\n        try:\n            return self.merge_with(unknown_shape(ndims=rank))\n        except ValueError:\n            raise ValueError(\"Shape %s must have rank %d\" % (self, rank))", "output": "Returns a shape based on `self` with the given rank.\n\n        This method promotes a completely unknown shape to one with a\n        known rank.\n\n        Args:\n          rank: An integer.\n\n        Returns:\n          A shape that is at least as specific as `self` with the given rank.\n\n        Raises:\n          ValueError: If `self` does not represent a shape with the given `rank`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shake_shake_layer(x, output_filters, num_blocks, stride, hparams):\n  \"\"\"\"\"\"\n  for block_num in range(num_blocks):\n    curr_stride = stride if (block_num == 0) else 1\n    with tf.variable_scope(\"layer_{}\".format(block_num)):\n      x = shake_shake_block(x, output_filters, curr_stride, hparams)\n  return x", "output": "Builds many sub layers into one full layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summarize_neural_network_spec(mlmodel_spec):\n    \"\"\" \n    \"\"\"\n    inputs = [(blob.name, _get_feature_description_summary(blob)) for blob in mlmodel_spec.description.input]\n    outputs = [(blob.name, _get_feature_description_summary(blob)) for blob in mlmodel_spec.description.output]\n    nn = None\n\n    if mlmodel_spec.HasField('neuralNetwork'):\n        nn = mlmodel_spec.neuralNetwork\n    elif mlmodel_spec.HasField('neuralNetworkClassifier'):\n        nn = mlmodel_spec.neuralNetworkClassifier\n    elif mlmodel_spec.HasField('neuralNetworkRegressor'):\n        nn = mlmodel_spec.neuralNetworkRegressor\n\n    layers = [_summarize_network_layer_info(layer) for layer in nn.layers] if nn != None else None\n    return (inputs, outputs, layers)", "output": "Summarize network into the following structure.\n    Args:\n    mlmodel_spec : mlmodel spec\n    Returns:\n    inputs : list[(str, str)] - a list of two tuple (name, descriptor) for each input blob.\n    outputs : list[(str, str)] - a list of two tuple (name, descriptor) for each output blob\n    layers : list[(str, list[str], list[str], list[(str, str)])] - a list of layers represented by\n        layer name, input blobs, output blobs, a list of (parameter name, content)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_value(self, val:float)->None:\n        \"\"\n        self.n += 1\n        self.mov_avg = self.beta * self.mov_avg + (1 - self.beta) * val\n        self.smooth = self.mov_avg / (1 - self.beta ** self.n)", "output": "Add `val` to calculate updated smoothed value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def key_regen():\n    '''\n    \n    '''\n    client = salt.client.get_local_client(__opts__['conf_file'])\n    try:\n        client.cmd('*', 'saltutil.regen_keys')\n    except SaltClientError as client_error:\n        print(client_error)\n        return False\n\n    for root, _, files in salt.utils.path.os_walk(__opts__['pki_dir']):\n        for fn_ in files:\n            path = os.path.join(root, fn_)\n            try:\n                os.remove(path)\n            except os.error:\n                pass\n    msg = ('The minion and master keys have been deleted.  Restart the Salt\\n'\n           'Master within the next 60 seconds!!!\\n\\n'\n           'Wait for the minions to reconnect.  Once the minions reconnect\\n'\n           'the new keys will appear in pending and will need to be re-\\n'\n           'accepted by running:\\n'\n           '    salt-key -A\\n\\n'\n           'Be advised that minions not currently connected to the master\\n'\n           'will not be able to reconnect and may require manual\\n'\n           'regeneration via a local call to\\n'\n           '    salt-call saltutil.regen_keys')\n    return msg", "output": "This routine is used to regenerate all keys in an environment. This is\n    invasive! ALL KEYS IN THE SALT ENVIRONMENT WILL BE REGENERATED!!\n\n    The key_regen routine sends a command out to minions to revoke the master\n    key and remove all minion keys, it then removes all keys from the master\n    and prompts the user to restart the master. The minions will all reconnect\n    and keys will be placed in pending.\n\n    After the master is restarted and minion keys are in the pending directory\n    execute a salt-key -A command to accept the regenerated minion keys.\n\n    The master *must* be restarted within 60 seconds of running this command or\n    the minions will think there is something wrong with the keys and abort.\n\n    Only Execute this runner after upgrading minions and master to 0.15.1 or\n    higher!\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.key_regen", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge(dest, src, merge_lists=False, in_place=True):\n    '''\n    \n    '''\n    if in_place:\n        merged = dest\n    else:\n        merged = copy.deepcopy(dest)\n    return dictupdate.update(merged, src, merge_lists=merge_lists)", "output": "defaults.merge\n        Allows deep merging of dicts in formulas.\n\n    merge_lists : False\n        If True, it will also merge lists instead of replace their items.\n\n    in_place : True\n        If True, it will merge into dest dict,\n        if not it will make a new copy from that dict and return it.\n\n        CLI Example:\n        .. code-block:: bash\n\n        salt '*' default.merge a=b d=e\n\n    It is more typical to use this in a templating language in formulas,\n    instead of directly on the command-line.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def renames(old, new):\n    # type: (str, str) -> None\n    \"\"\"\"\"\"\n    # Implementation borrowed from os.renames().\n    head, tail = os.path.split(new)\n    if head and tail and not os.path.exists(head):\n        os.makedirs(head)\n\n    shutil.move(old, new)\n\n    head, tail = os.path.split(old)\n    if head and tail:\n        try:\n            os.removedirs(head)\n        except OSError:\n            pass", "output": "Like os.renames(), but handles renaming across devices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_verify_command(self, signature_filename, data_filename,\n                           keystore=None):\n        \"\"\"\n        \n        \"\"\"\n        cmd = [self.gpg, '--status-fd', '2', '--no-tty']\n        if keystore is None:\n            keystore = self.gpg_home\n        if keystore:\n            cmd.extend(['--homedir', keystore])\n        cmd.extend(['--verify', signature_filename, data_filename])\n        logger.debug('invoking: %s', ' '.join(cmd))\n        return cmd", "output": "Return a suitable command for verifying a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: The verifying command as a list suitable to be\n                 passed to :class:`subprocess.Popen`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all_attack_ids(self):\n    \"\"\"\"\"\"\n    return list(self.attacks.keys()) + list(self.targeted_attacks.keys())", "output": "Returns IDs of all attacks (targeted and non-targeted).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_SU_save_etf_list(engine, client=DATABASE):\n    \"\"\"\n    \"\"\"\n\n    engine = select_save_engine(engine)\n    engine.QA_SU_save_etf_list(client=client)", "output": "save etf_list\n\n    Arguments:\n        engine {[type]} -- [description]\n\n    Keyword Arguments:\n        client {[type]} -- [description] (default: {DATABASE})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _tokens_to_subtoken_ids(self, tokens):\n    \"\"\"\n    \"\"\"\n    ret = []\n    for token in tokens:\n      ret.extend(self._token_to_subtoken_ids(token))\n    return ret", "output": "Converts a list of tokens to a list of subtoken ids.\n\n    Args:\n      tokens: a list of strings.\n    Returns:\n      a list of integers in the range [0, vocab_size)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def collect(self, name, arr):\n        \"\"\"\"\"\"\n        name = py_str(name)\n        if self.include_layer is not None and not self.include_layer(name):\n            return\n        handle = ctypes.cast(arr, NDArrayHandle)\n        arr = NDArray(handle, writable=False)\n        min_range = ndarray.min(arr).asscalar()\n        max_range = ndarray.max(arr).asscalar()\n        if name in self.min_max_dict:\n            cur_min_max = self.min_max_dict[name]\n            self.min_max_dict[name] = (min(cur_min_max[0], min_range),\n                                       max(cur_min_max[1], max_range))\n        else:\n            self.min_max_dict[name] = (min_range, max_range)\n        if self.logger is not None:\n            self.logger.info(\"Collecting layer %s min_range=%f, max_range=%f\"\n                             % (name, min_range, max_range))", "output": "Callback function for collecting min and max values from an NDArray.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_current_step(self):\n        \"\"\"\n        \n        \"\"\"\n        step = self.sess.run(self.model.global_step)\n        return step", "output": "Gets current model step.\n        :return: current model step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def namedtuple_with_defaults(typename, field_names, default_values=()):\n    \"\"\"  \"\"\"\n    T = collections.namedtuple(typename, field_names)\n    T.__new__.__defaults__ = (None, ) * len(T._fields)\n    if isinstance(default_values, collections.Mapping):\n        prototype = T(**default_values)\n    else:\n        prototype = T(*default_values)\n    T.__new__.__defaults__ = tuple(prototype)\n    return T", "output": "create a namedtuple with default values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_for_update(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            version = pkg_resources.require(\"zappa\")[0].version\n            updateable = check_new_version_available(version)\n            if updateable:\n                click.echo(click.style(\"Important!\", fg=\"yellow\", bold=True) +\n                           \" A new version of \" + click.style(\"Zappa\", bold=True) + \" is available!\")\n                click.echo(\"Upgrade with: \" + click.style(\"pip install zappa --upgrade\", bold=True))\n                click.echo(\"Visit the project page on GitHub to see the latest changes: \" +\n                           click.style(\"https://github.com/Miserlou/Zappa\", bold=True))\n        except Exception as e: # pragma: no cover\n            print(e)\n            return", "output": "Print a warning if there's a new Zappa version available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decoded_output_boxes_class_agnostic(self):\n        \"\"\"  \"\"\"\n        assert self._bbox_class_agnostic\n        box_logits = tf.reshape(self.box_logits, [-1, 4])\n        decoded = decode_bbox_target(\n            box_logits / self.bbox_regression_weights,\n            self.proposals.boxes\n        )\n        return decoded", "output": "Returns: Nx4", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self, prefix, epoch=None):\n        \"\"\"\n        \"\"\"\n        if epoch is None:\n            epoch = self.num_epoch\n        assert epoch is not None\n        save_checkpoint(prefix, epoch, self.symbol, self.arg_params, self.aux_params)", "output": "Checkpoint the model checkpoint into file.\n        You can also use `pickle` to do the job if you only work on Python.\n        The advantage of `load` and `save` (as compared to `pickle`) is that\n        the resulting file can be loaded from other MXNet language bindings.\n        One can also directly `load`/`save` from/to cloud storage(S3, HDFS)\n\n        Parameters\n        ----------\n        prefix : str\n            Prefix of model name.\n\n        Notes\n        -----\n        - ``prefix-symbol.json`` will be saved for symbol.\n        - ``prefix-epoch.params`` will be saved for parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        session = self.Session()\n\n        tags = set(kwargs.pop('tags', []))\n\n        if 'search_text' not in kwargs:\n            kwargs['search_text'] = self.tagger.get_bigram_pair_string(kwargs['text'])\n\n        if 'search_in_response_to' not in kwargs:\n            in_response_to = kwargs.get('in_response_to')\n            if in_response_to:\n                kwargs['search_in_response_to'] = self.tagger.get_bigram_pair_string(in_response_to)\n\n        statement = Statement(**kwargs)\n\n        for tag_name in tags:\n            tag = session.query(Tag).filter_by(name=tag_name).first()\n\n            if not tag:\n                # Create the tag\n                tag = Tag(name=tag_name)\n\n            statement.tags.append(tag)\n\n        session.add(statement)\n\n        session.flush()\n\n        session.refresh(statement)\n\n        statement_object = self.model_to_object(statement)\n\n        self._session_finish(session)\n\n        return statement_object", "output": "Creates a new statement matching the keyword arguments specified.\n        Returns the created statement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all_anchors_fpn(strides=None, sizes=None):\n    \"\"\"\n    \n    \"\"\"\n    if strides is None:\n        strides = cfg.FPN.ANCHOR_STRIDES\n    if sizes is None:\n        sizes = cfg.RPN.ANCHOR_SIZES\n    assert len(strides) == len(sizes)\n    foas = []\n    for stride, size in zip(strides, sizes):\n        foa = get_all_anchors(stride=stride, sizes=(size,))\n        foas.append(foa)\n    return foas", "output": "Returns:\n        [anchors]: each anchors is a SxSx NUM_ANCHOR_RATIOS x4 array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lookup(self, key):\n        \"\"\"\n        \n        \"\"\"\n        values = self.filter(lambda kv: kv[0] == key).values()\n\n        if self.partitioner is not None:\n            return self.ctx.runJob(values, lambda x: x, [self.partitioner(key)])\n\n        return values.collect()", "output": "Return the list of values in the RDD for key `key`. This operation\n        is done efficiently if the RDD has a known partitioner by only\n        searching the partition that the key maps to.\n\n        >>> l = range(1000)\n        >>> rdd = sc.parallelize(zip(l, l), 10)\n        >>> rdd.lookup(42)  # slow\n        [42]\n        >>> sorted = rdd.sortByKey()\n        >>> sorted.lookup(42)  # fast\n        [42]\n        >>> sorted.lookup(1024)\n        []\n        >>> rdd2 = sc.parallelize([(('a', 'b'), 'c')]).groupByKey()\n        >>> list(rdd2.lookup(('a', 'b'))[0])\n        ['c']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_vhost(vhost, runas=None):\n    '''\n    \n    '''\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n    res = __salt__['cmd.run_all'](\n        [RABBITMQCTL, 'add_vhost', vhost],\n        reset_system_locale=False,\n        runas=runas,\n        python_shell=False)\n\n    msg = 'Added'\n    return _format_response(res, msg)", "output": "Adds a vhost via rabbitmqctl add_vhost.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq add_vhost '<vhost_name>'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decode(self, sequence, masked_positions):\n        \"\"\"\n        \"\"\"\n        batch_size = sequence.shape[0]\n        num_masked_positions = masked_positions.shape[1]\n        ctx = masked_positions.context\n        dtype = masked_positions.dtype\n        # batch_idx = [0,0,0,1,1,1,2,2,2...]\n        # masked_positions = [1,2,4,0,3,4,2,3,5...]\n        batch_idx = mx.nd.arange(0, batch_size, repeat=num_masked_positions, dtype=dtype, ctx=ctx)\n        batch_idx = batch_idx.reshape((1, -1))\n        masked_positions = masked_positions.reshape((1, -1))\n        position_idx = mx.nd.Concat(batch_idx, masked_positions, dim=0)\n        encoded = mx.nd.gather_nd(sequence, position_idx)\n        encoded = encoded.reshape((batch_size, num_masked_positions, sequence.shape[-1]))\n        decoded = self.decoder(encoded)\n        return decoded", "output": "Generate unnormalized prediction for the masked language model task.\n\n        This is only used for pre-training the BERT model.\n\n        Inputs:\n            - **sequence**: input tensor of sequence encodings.\n              Shape (batch_size, seq_length, units).\n            - **masked_positions**: input tensor of position of tokens for masked LM decoding.\n              Shape (batch_size, num_masked_positions). For each sample in the batch, the values\n              in this tensor must not be out of bound considering the length of the sequence.\n\n        Outputs:\n            - **masked_lm_outputs**: output tensor of token predictions for target masked_positions.\n                Shape (batch_size, num_masked_positions, vocab_size).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_module_names(path_dir, exclude=None):\n    if exclude is None: exclude = _default_exclude\n    \"\"\n    files = sorted(path_dir.glob('*'), key=lambda x: (x.is_dir(), x.name), reverse=True) # directories first\n    res = [f'{path_dir.name}']\n    for f in files:\n        if f.is_dir() and f.name in exclude: continue # exclude directories\n        if any([f.name.endswith(ex) for ex in exclude]): continue # exclude extensions\n\n        if f.suffix == '.py': res.append(f'{path_dir.name}.{f.stem}')\n        elif f.is_dir(): res += [f'{path_dir.name}.{name}' for name in get_module_names(f)]\n    return res", "output": "Search a given `path_dir` and return all the modules contained inside except those in `exclude`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_current_client_working_directory(self, directory):\r\n        \"\"\"\"\"\"\r\n        shellwidget = self.get_current_shellwidget()\r\n        if shellwidget is not None:\r\n            shellwidget.set_cwd(directory)", "output": "Set current client working directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def servermods():\n    '''\n    \n    '''\n    cmd = '{0} -l'.format(_detect_os())\n    ret = []\n    out = __salt__['cmd.run'](cmd).splitlines()\n    for line in out:\n        if not line:\n            continue\n        if '.c' in line:\n            ret.append(line.strip())\n    return ret", "output": "Return list of modules compiled into the server (``apachectl -l``)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' apache.servermods", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_config(xpath, element):\n    '''\n    \n\n    '''\n    query = {'type': 'config',\n             'action': 'set',\n             'xpath': xpath,\n             'element': element}\n\n    response = __proxy__['panos.call'](query)\n\n    return _validate_response(response)", "output": "Sends a set request to the device.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def human_sum_00(X, y, model_generator, method_name):\n    \"\"\" \n    \"\"\"\n    return _human_sum(X, model_generator, method_name, False, False)", "output": "SUM (false/false)\n\n    This tests how well a feature attribution method agrees with human intuition\n    for a SUM operation. This metric deals\n    specifically with the question of credit allocation for the following function\n    when all three inputs are true:\n    if fever: +2 points\n    if cough: +2 points\n\n    transform = \"identity\"\n    sort_order = 0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernels_status_cli(self, kernel, kernel_opt=None):\n        \"\"\" \n        \"\"\"\n        kernel = kernel or kernel_opt\n        response = self.kernels_status(kernel)\n        status = response['status']\n        message = response['failureMessage']\n        if message:\n            print('%s has status \"%s\"' % (kernel, status))\n            print('Failure message: \"%s\"' % message)\n        else:\n            print('%s has status \"%s\"' % (kernel, status))", "output": "client wrapper for kernel_status\n             Parameters\n            ==========\n            kernel_opt: additional option from the client, if kernel not defined", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_time_at(time_in=None, time_at=None, out_fmt='%Y-%m-%dT%H:%M:%S'):\n    '''\n    \n    '''\n    dt = get_timestamp_at(time_in=time_in, time_at=time_at)\n    return time.strftime(out_fmt, time.localtime(dt))", "output": "Return the time in human readable format for a future event that may occur\n    in ``time_in`` time, or at ``time_at``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_directive(self, directive):\n        \"\"\"\n        \n        \"\"\"\n        words = directive.split()\n        if len(words) == 1 and words[0] not in ('include', 'exclude',\n                                                'global-include',\n                                                'global-exclude',\n                                                'recursive-include',\n                                                'recursive-exclude',\n                                                'graft', 'prune'):\n            # no action given, let's use the default 'include'\n            words.insert(0, 'include')\n\n        action = words[0]\n        patterns = thedir = dir_pattern = None\n\n        if action in ('include', 'exclude',\n                      'global-include', 'global-exclude'):\n            if len(words) < 2:\n                raise DistlibException(\n                    '%r expects <pattern1> <pattern2> ...' % action)\n\n            patterns = [convert_path(word) for word in words[1:]]\n\n        elif action in ('recursive-include', 'recursive-exclude'):\n            if len(words) < 3:\n                raise DistlibException(\n                    '%r expects <dir> <pattern1> <pattern2> ...' % action)\n\n            thedir = convert_path(words[1])\n            patterns = [convert_path(word) for word in words[2:]]\n\n        elif action in ('graft', 'prune'):\n            if len(words) != 2:\n                raise DistlibException(\n                    '%r expects a single <dir_pattern>' % action)\n\n            dir_pattern = convert_path(words[1])\n\n        else:\n            raise DistlibException('unknown action %r' % action)\n\n        return action, patterns, thedir, dir_pattern", "output": "Validate a directive.\n        :param directive: The directive to validate.\n        :return: A tuple of action, patterns, thedir, dir_patterns", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_output_info_dict(self, signature=None):\n    \"\"\"\n    \"\"\"\n    return self._spec.get_output_info_dict(signature=signature, tags=self._tags)", "output": "Describes the outputs provided by a signature.\n\n    Args:\n      signature: A string with the signature to get ouputs information for.\n        If None, the default signature is used if defined.\n\n    Returns:\n      The result of ModuleSpec.get_output_info_dict() for the given signature,\n      and the graph variant selected by `tags` when this Module was initialized.\n\n    Raises:\n      KeyError: if there is no such signature.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, writer, sheet_name='Sheet1', startrow=0,\n              startcol=0, freeze_panes=None, engine=None):\n        \"\"\"\n        \n        \"\"\"\n        from pandas.io.excel import ExcelWriter\n        from pandas.io.common import _stringify_path\n\n        if isinstance(writer, ExcelWriter):\n            need_save = False\n        else:\n            writer = ExcelWriter(_stringify_path(writer), engine=engine)\n            need_save = True\n\n        formatted_cells = self.get_formatted_cells()\n        writer.write_cells(formatted_cells, sheet_name,\n                           startrow=startrow, startcol=startcol,\n                           freeze_panes=freeze_panes)\n        if need_save:\n            writer.save()", "output": "writer : string or ExcelWriter object\n            File path or existing ExcelWriter\n        sheet_name : string, default 'Sheet1'\n            Name of sheet which will contain DataFrame\n        startrow :\n            upper left cell row to dump data frame\n        startcol :\n            upper left cell column to dump data frame\n        freeze_panes : tuple of integer (length 2), default None\n            Specifies the one-based bottommost row and rightmost column that\n            is to be frozen\n        engine : string, default None\n            write engine to use if writer is a path - you can also set this\n            via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``,\n            and ``io.excel.xlsm.writer``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_initial_frame_chooser(\n    real_env, frame_stack_size, simulation_random_starts,\n    simulation_flip_first_random_for_beginning,\n    split=tf.estimator.ModeKeys.TRAIN,\n):\n  \"\"\"\n  \"\"\"\n  initial_frame_rollouts = real_env.current_epoch_rollouts(\n      split=split, minimal_rollout_frames=frame_stack_size,\n  )\n  def initial_frame_chooser(batch_size):\n    \"\"\"Frame chooser.\"\"\"\n\n    deterministic_initial_frames =\\\n        initial_frame_rollouts[0][:frame_stack_size]\n    if not simulation_random_starts:\n      # Deterministic starts: repeat first frames from the first rollout.\n      initial_frames = [deterministic_initial_frames] * batch_size\n    else:\n      # Random starts: choose random initial frames from random rollouts.\n      initial_frames = random_rollout_subsequences(\n          initial_frame_rollouts, batch_size, frame_stack_size\n      )\n      if simulation_flip_first_random_for_beginning:\n        # Flip first entry in the batch for deterministic initial frames.\n        initial_frames[0] = deterministic_initial_frames\n\n    return np.stack([\n        [frame.observation.decode() for frame in initial_frame_stack]  # pylint: disable=g-complex-comprehension\n        for initial_frame_stack in initial_frames\n    ])\n  return initial_frame_chooser", "output": "Make frame chooser.\n\n  Args:\n    real_env: T2TEnv to take initial frames from.\n    frame_stack_size (int): Number of consecutive frames to extract.\n    simulation_random_starts (bool): Whether to choose frames at random.\n    simulation_flip_first_random_for_beginning (bool): Whether to flip the first\n      frame stack in every batch for the frames at the beginning.\n    split (tf.estimator.ModeKeys or None): Data split to take the frames from,\n      None means use all frames.\n\n  Returns:\n    Function batch_size -> initial_frames.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def invalid_marker(text):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        evaluate_marker(text)\n    except SyntaxError as e:\n        e.filename = None\n        e.lineno = None\n        return e\n    return False", "output": "Validate text as a PEP 508 environment marker; return an exception\n    if invalid or False otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chown(path, user, group):\n    '''\n    \n    '''\n    path = os.path.expanduser(path)\n\n    uid = user_to_uid(user)\n    gid = group_to_gid(group)\n    err = ''\n    if uid == '':\n        if user:\n            err += 'User does not exist\\n'\n        else:\n            uid = -1\n    if gid == '':\n        if group:\n            err += 'Group does not exist\\n'\n        else:\n            gid = -1\n    if not os.path.exists(path):\n        try:\n            # Broken symlinks will return false, but still need to be chowned\n            return os.lchown(path, uid, gid)\n        except OSError:\n            pass\n        err += 'File not found'\n    if err:\n        return err\n    return os.chown(path, uid, gid)", "output": "Chown a file, pass the file the desired user and group\n\n    path\n        path to the file or directory\n\n    user\n        user owner\n\n    group\n        group owner\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.chown /etc/passwd root root", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nodeGetBase(self, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        ret = libxml2mod.xmlNodeGetBase(self._o, cur__o)\n        return ret", "output": "Searches for the BASE URL. The code should work on both XML\n          and HTML document even if base mechanisms are completely\n          different. It returns the base as defined in RFC 2396\n          sections 5.1.1. Base URI within Document Content and 5.1.2.\n          Base URI from the Encapsulating Entity However it does not\n           return the document base (5.1.3), use doc->URL in this case", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def calculate_dates(self, dt):\n        \"\"\"\n        \n        \"\"\"\n        period_start, period_close = self.cal.open_and_close_for_session(\n            self.cal.minute_to_session_label(dt),\n        )\n\n        # Align the market open and close times here with the execution times\n        # used by the simulation clock. This ensures that scheduled functions\n        # trigger at the correct times.\n        self._period_start = self.cal.execution_time_from_open(period_start)\n        self._period_close = self.cal.execution_time_from_close(period_close)\n\n        self._period_end = self._period_start + self.offset - self._one_minute", "output": "Given a date, find that day's open and period end (open + offset).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_assign_target(self, with_tuple=True, name_only=False,\n                            extra_end_rules=None, with_namespace=False):\n        \"\"\"\n        \"\"\"\n        if with_namespace and self.stream.look().type == 'dot':\n            token = self.stream.expect('name')\n            next(self.stream)  # dot\n            attr = self.stream.expect('name')\n            target = nodes.NSRef(token.value, attr.value, lineno=token.lineno)\n        elif name_only:\n            token = self.stream.expect('name')\n            target = nodes.Name(token.value, 'store', lineno=token.lineno)\n        else:\n            if with_tuple:\n                target = self.parse_tuple(simplified=True,\n                                          extra_end_rules=extra_end_rules)\n            else:\n                target = self.parse_primary()\n            target.set_ctx('store')\n        if not target.can_assign():\n            self.fail('can\\'t assign to %r' % target.__class__.\n                      __name__.lower(), target.lineno)\n        return target", "output": "Parse an assignment target.  As Jinja2 allows assignments to\n        tuples, this function can parse all allowed assignment targets.  Per\n        default assignments to tuples are parsed, that can be disable however\n        by setting `with_tuple` to `False`.  If only assignments to names are\n        wanted `name_only` can be set to `True`.  The `extra_end_rules`\n        parameter is forwarded to the tuple parsing function.  If\n        `with_namespace` is enabled, a namespace assignment may be parsed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_put_node_proxy_with_path(self, name, path, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.connect_put_node_proxy_with_path_with_http_info(name, path, **kwargs)\n        else:\n            (data) = self.connect_put_node_proxy_with_path_with_http_info(name, path, **kwargs)\n            return data", "output": "connect PUT requests to proxy of Node\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.connect_put_node_proxy_with_path(name, path, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the NodeProxyOptions (required)\n        :param str path: path to the resource (required)\n        :param str path2: Path is the URL path to use for the current proxy request to node.\n        :return: str\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_domain(self, pipeline):\n        \"\"\"\n        \"\"\"\n        domain = pipeline.domain(default=self._default_domain)\n        if domain is GENERIC:\n            raise ValueError(\n                \"Unable to determine domain for Pipeline.\\n\"\n                \"Pass domain=<desired domain> to your Pipeline to set a \"\n                \"domain.\"\n            )\n        return domain", "output": "Resolve a concrete domain for ``pipeline``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hard_reset(self):\n        \"\"\"\"\"\"\n        if self.seq is not None and self.shuffle:\n            random.shuffle(self.seq)\n        if self.imgrec is not None:\n            self.imgrec.reset()\n        self.cur = 0\n        self._allow_read = True\n        self._cache_data = None\n        self._cache_label = None\n        self._cache_idx = None", "output": "Resets the iterator and ignore roll over data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_params_from(self, arg_params, aux_params=None, allow_extra_params=False):\n        \"\"\"\n        \"\"\"\n        for name, array in arg_params.items():\n            if name in self.arg_dict:\n                dst = self.arg_dict[name]\n                array.astype(dst.dtype).copyto(dst)\n            elif not allow_extra_params:\n                raise ValueError('Find name \\\"%s\\\" that is not in the arguments' % name)\n\n        if aux_params is None:\n            return\n\n        for name, array in aux_params.items():\n            if name in self.aux_dict:\n                dst = self.aux_dict[name]\n                array.astype(dst.dtype).copyto(dst)\n            elif not allow_extra_params:\n                raise ValueError('Find name %s that is not in the auxiliary states' % name)", "output": "Copy parameters from arg_params, aux_params into executor's internal array.\n\n        Parameters\n        ----------\n        arg_params : dict of str to NDArray\n            Parameters, dict of name to NDArray of arguments.\n\n        aux_params : dict of str to NDArray, optional\n            Parameters, dict of name to NDArray of auxiliary states.\n\n        allow_extra_params : boolean, optional\n            Whether allow extra parameters that are not needed by symbol.\n            If this is True, no error will be thrown when arg_params or aux_params\n            contain extra parameters that is not needed by the executor.\n\n        Raises\n        ------\n        ValueError\n            If there is additional parameters in the dict but ``allow_extra_params=False``.\n\n        Examples\n        --------\n        >>> # set parameters with existing model checkpoint\n        >>> model_prefix = 'mx_mlp'\n        >>> sym, arg_params, aux_params = mx.model.load_checkpoint(model_prefix, 0)\n        >>> texec.copy_params_from(arg_params, aux_params)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dep_map(self):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return self.__dep_map\n        except AttributeError:\n            self.__dep_map = self._filter_extras(self._build_dep_map())\n        return self.__dep_map", "output": "A map of extra to its list of (direct) requirements\n        for this distribution, including the null extra.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_input_fn_from_generator(gen):\n  \"\"\"\"\"\"\n  first_ex = six.next(gen)\n  flattened = tf.contrib.framework.nest.flatten(first_ex)\n  types = [t.dtype for t in flattened]\n  shapes = [[None] * len(t.shape) for t in flattened]\n  first_ex_list = [first_ex]\n\n  def py_func():\n    if first_ex_list:\n      example = first_ex_list.pop()\n    else:\n      example = six.next(gen)\n    return tf.contrib.framework.nest.flatten(example)\n\n  def input_fn():\n    flat_example = tf.py_func(py_func, [], types)\n    _ = [t.set_shape(shape) for t, shape in zip(flat_example, shapes)]\n    example = tf.contrib.framework.nest.pack_sequence_as(first_ex, flat_example)\n    return example\n\n  return input_fn", "output": "Use py_func to yield elements from the given generator.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restart_with_reloader():\n    \"\"\"\n    \"\"\"\n    cwd = os.getcwd()\n    args = _get_args_for_reloading()\n    new_environ = os.environ.copy()\n    new_environ[\"SANIC_SERVER_RUNNING\"] = \"true\"\n    cmd = \" \".join(args)\n    worker_process = Process(\n        target=subprocess.call,\n        args=(cmd,),\n        kwargs={\"cwd\": cwd, \"shell\": True, \"env\": new_environ},\n    )\n    worker_process.start()\n    return worker_process", "output": "Create a new process and a subprocess in it with the same arguments as\n    this one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pad_line_to_ontonotes(line, domain) -> List[str]:\n    \"\"\"\n    \n    \"\"\"\n    word_ind, word = line[ : 2]\n    pos = 'XX'\n    oie_tags = line[2 : ]\n    line_num = 0\n    parse = \"-\"\n    lemma = \"-\"\n    return [domain, line_num, word_ind, word, pos, parse, lemma, '-',\\\n            '-', '-', '*'] + list(oie_tags) + ['-', ]", "output": "Pad line to conform to ontonotes representation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_tab_event_filter(self, value):\n        \"\"\"\n        \n        \"\"\"\n        dock_tabbar = None\n        tabbars = self.main.findChildren(QTabBar)\n        for tabbar in tabbars:\n            for tab in range(tabbar.count()):\n                title = tabbar.tabText(tab)\n                if title == self.title:\n                    dock_tabbar = tabbar\n                    break\n        if dock_tabbar is not None:\n            self.dock_tabbar = dock_tabbar\n            # Install filter only once per QTabBar\n            if getattr(self.dock_tabbar, 'filter', None) is None:\n                self.dock_tabbar.filter = TabFilter(self.dock_tabbar,\n                                                    self.main)\n                self.dock_tabbar.installEventFilter(self.dock_tabbar.filter)", "output": "Install an event filter to capture mouse events in the tabs of a\n        QTabBar holding tabified dockwidgets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diet_expert(x, hidden_size, params):\n  \"\"\"\n  \"\"\"\n\n  @fn_with_diet_vars(params)\n  def diet_expert_internal(x):\n    dim = x.get_shape().as_list()[-1]\n    h = tf.layers.dense(x, hidden_size, activation=tf.nn.relu, use_bias=False)\n    y = tf.layers.dense(h, dim, use_bias=False)\n    y *= tf.rsqrt(tf.to_float(dim * hidden_size))\n    return y\n\n  return diet_expert_internal(x)", "output": "A two-layer feed-forward network with relu activation on hidden layer.\n\n  Uses diet variables.\n  Recomputes hidden layer on backprop to save activation memory.\n\n  Args:\n    x: a Tensor with shape [batch, io_size]\n    hidden_size: an integer\n    params: a diet variable HParams object.\n\n  Returns:\n    a Tensor with shape [batch, io_size]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def can_set_locale(lc, lc_var=locale.LC_ALL):\n    \"\"\"\n    \n    \"\"\"\n\n    try:\n        with set_locale(lc, lc_var=lc_var):\n            pass\n    except (ValueError, locale.Error):\n        # horrible name for a Exception subclass\n        return False\n    else:\n        return True", "output": "Check to see if we can set a locale, and subsequently get the locale,\n    without raising an Exception.\n\n    Parameters\n    ----------\n    lc : str\n        The locale to attempt to set.\n    lc_var : int, default `locale.LC_ALL`\n        The category of the locale being set.\n\n    Returns\n    -------\n    is_valid : bool\n        Whether the passed locale can be set", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _roll_data(self):\n        \"\"\"\n        \n        \"\"\"\n\n        self.buffer.values[:, :self._window, :] = \\\n            self.buffer.values[:, -self._window:, :]\n        self.date_buf[:self._window] = self.date_buf[-self._window:]\n        self._pos = self._window", "output": "Roll window worth of data up to position zero.\n        Save the effort of having to expensively roll at each iteration", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send(self, msg, timeout=None, callback=None, raw=False):\n        '''\n        \n        '''\n        message_id = self._message_id()\n        header = {'mid': message_id}\n\n        future = tornado.concurrent.Future()\n        if callback is not None:\n            def handle_future(future):\n                response = future.result()\n                self.io_loop.add_callback(callback, response)\n            future.add_done_callback(handle_future)\n        # Add this future to the mapping\n        self.send_future_map[message_id] = future\n\n        if self.opts.get('detect_mode') is True:\n            timeout = 1\n\n        if timeout is not None:\n            send_timeout = self.io_loop.call_later(timeout, self.timeout_message, message_id)\n            self.send_timeout_map[message_id] = send_timeout\n\n        # if we don't have a send queue, we need to spawn the callback to do the sending\n        if not self.send_queue:\n            self.io_loop.spawn_callback(self._stream_send)\n        self.send_queue.append((message_id, salt.transport.frame.frame_msg(msg, header=header)))\n        return future", "output": "Send given message, and return a future", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def inherit_attributes(self, project_module, parent_module):\n        \"\"\"\"\"\"\n        assert isinstance(project_module, basestring)\n        assert isinstance(parent_module, basestring)\n\n        attributes = self.module2attributes[project_module]\n        pattributes = self.module2attributes[parent_module]\n\n        # Parent module might be locationless user-config.\n        # FIXME:\n        #if [ modules.binding $(parent-module) ]\n        #{\n        #    $(attributes).set parent : [ path.parent\n        #                                 [ path.make [ modules.binding $(parent-module) ] ] ] ;\n        #    }\n\n        attributes.set(\"project-root\", pattributes.get(\"project-root\"), exact=True)\n        attributes.set(\"default-build\", pattributes.get(\"default-build\"), exact=True)\n        attributes.set(\"requirements\", pattributes.get(\"requirements\"), exact=True)\n        attributes.set(\"usage-requirements\",\n                       pattributes.get(\"usage-requirements\"), exact=1)\n\n        parent_build_dir = pattributes.get(\"build-dir\")\n\n        if parent_build_dir:\n        # Have to compute relative path from parent dir to our dir\n        # Convert both paths to absolute, since we cannot\n        # find relative path from \"..\" to \".\"\n\n             location = attributes.get(\"location\")\n             parent_location = pattributes.get(\"location\")\n\n             our_dir = os.path.join(os.getcwd(), location)\n             parent_dir = os.path.join(os.getcwd(), parent_location)\n\n             build_dir = os.path.join(parent_build_dir,\n                                      os.path.relpath(our_dir, parent_dir))\n             attributes.set(\"build-dir\", build_dir, exact=True)", "output": "Make 'project-module' inherit attributes of project\n        root and parent module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_profiler(self):\n        \"\"\"\"\"\"\n        if self.main.editor.save():\n            self.switch_to_plugin()\n            self.analyze(self.main.editor.get_current_filename())", "output": "Run profiler", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_from_adversarial_batches_write_to_datastore(self, submissions,\n                                                       adv_batches):\n    \"\"\"\n    \"\"\"\n    # prepare classification batches\n    idx = 0\n    for s_id in iterkeys(submissions.defenses):\n      for adv_id in iterkeys(adv_batches.data):\n        class_batch_id = CLASSIFICATION_BATCH_ID_PATTERN.format(idx)\n        idx += 1\n        self.data[class_batch_id] = {\n            'adversarial_batch_id': adv_id,\n            'submission_id': s_id,\n            'result_path': os.path.join(\n                self._round_name,\n                CLASSIFICATION_BATCHES_SUBDIR,\n                s_id + '_' + adv_id + '.csv')\n        }\n    # save them to datastore\n    client = self._datastore_client\n    with client.no_transact_batch() as batch:\n      for key, value in iteritems(self.data):\n        entity = client.entity(client.key(KIND_CLASSIFICATION_BATCH, key))\n        entity.update(value)\n        batch.put(entity)", "output": "Populates data from adversarial batches and writes to datastore.\n\n    Args:\n      submissions: instance of CompetitionSubmissions\n      adv_batches: instance of AversarialBatches", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_program_action(parent, text, name, icon=None, nt_name=None):\r\n    \"\"\"\"\"\"\r\n    if is_text_string(icon):\r\n        icon = get_icon(icon)\r\n    if os.name == 'nt' and nt_name is not None:\r\n        name = nt_name\r\n    path = programs.find_program(name)\r\n    if path is not None:\r\n        return create_action(parent, text, icon=icon,\r\n                             triggered=lambda: programs.run_program(name))", "output": "Create action to run a program", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        '''\n        \n        '''\n        import salt.runner\n        self.parse_args()\n\n        # Setup file logging!\n        self.setup_logfile_logger()\n        verify_log(self.config)\n        profiling_enabled = self.options.profiling_enabled\n\n        runner = salt.runner.Runner(self.config)\n        if self.options.doc:\n            runner.print_docs()\n            self.exit(salt.defaults.exitcodes.EX_OK)\n\n        # Run this here so SystemExit isn't raised anywhere else when\n        # someone tries to use the runners via the python API\n        try:\n            if check_user(self.config['user']):\n                pr = salt.utils.profile.activate_profile(profiling_enabled)\n                try:\n                    ret = runner.run()\n                    # In older versions ret['data']['retcode'] was used\n                    # for signaling the return code. This has been\n                    # changed for the orchestrate runner, but external\n                    # runners might still use it. For this reason, we\n                    # also check ret['data']['retcode'] if\n                    # ret['retcode'] is not available.\n                    if isinstance(ret, dict) and 'retcode' in ret:\n                        self.exit(ret['retcode'])\n                    elif isinstance(ret, dict) and 'retcode' in ret.get('data', {}):\n                        self.exit(ret['data']['retcode'])\n                finally:\n                    salt.utils.profile.output_profile(\n                        pr,\n                        stats_path=self.options.profiling_path,\n                        stop=True)\n\n        except SaltClientError as exc:\n            raise SystemExit(six.text_type(exc))", "output": "Execute salt-run", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def posix_rename(self, oldpath, newpath):\n        \"\"\"\n        \n        \"\"\"\n        oldpath = self._adjust_cwd(oldpath)\n        newpath = self._adjust_cwd(newpath)\n        self._log(DEBUG, \"posix_rename({!r}, {!r})\".format(oldpath, newpath))\n        self._request(\n            CMD_EXTENDED, \"posix-rename@openssh.com\", oldpath, newpath\n        )", "output": "Rename a file or folder from ``oldpath`` to ``newpath``, following\n        posix conventions.\n\n        :param str oldpath: existing name of the file or folder\n        :param str newpath: new name for the file or folder, will be\n            overwritten if it already exists\n\n        :raises:\n            ``IOError`` -- if ``newpath`` is a folder, posix-rename is not\n            supported by the server or something else goes wrong\n\n        :versionadded: 2.2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assign_default_storage_policy_to_datastore(policy, datastore,\n                                               service_instance=None):\n    '''\n    \n    '''\n    log.trace('Assigning policy %s to datastore %s', policy, datastore)\n    profile_manager = salt.utils.pbm.get_profile_manager(service_instance)\n    # Find policy\n    policies = salt.utils.pbm.get_storage_policies(profile_manager, [policy])\n    if not policies:\n        raise VMwareObjectRetrievalError('Policy \\'{0}\\' was not found'\n                                         ''.format(policy))\n    policy_ref = policies[0]\n    # Find datastore\n    target_ref = _get_proxy_target(service_instance)\n    ds_refs = salt.utils.vmware.get_datastores(service_instance, target_ref,\n                                               datastore_names=[datastore])\n    if not ds_refs:\n        raise VMwareObjectRetrievalError('Datastore \\'{0}\\' was not '\n                                         'found'.format(datastore))\n    ds_ref = ds_refs[0]\n    salt.utils.pbm.assign_default_storage_policy_to_datastore(\n        profile_manager, policy_ref, ds_ref)\n    return True", "output": "Assigns a storage policy as the default policy to a datastore.\n\n    policy\n        Name of the policy to assign.\n\n    datastore\n        Name of the datastore to assign.\n        The datastore needs to be visible to the VMware entity the proxy\n        points to.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.assign_storage_policy_to_datastore\n            policy='policy name' datastore=ds1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_host(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The connect_host function must be called with '\n            '-f or --function.'\n        )\n\n    host_name = kwargs.get('host') if kwargs and 'host' in kwargs else None\n\n    if not host_name:\n        raise SaltCloudSystemExit(\n            'You must specify name of the host system.'\n        )\n\n    # Get the service instance\n    si = _get_si()\n\n    host_ref = salt.utils.vmware.get_mor_by_property(si, vim.HostSystem, host_name)\n    if not host_ref:\n        raise SaltCloudSystemExit(\n            'Specified host system does not exist.'\n        )\n\n    if host_ref.runtime.connectionState == 'connected':\n        return {host_name: 'host system already connected'}\n\n    try:\n        task = host_ref.ReconnectHost_Task()\n        salt.utils.vmware.wait_for_task(task, host_name, 'connect host', 5, 'info')\n    except Exception as exc:\n        log.error(\n            'Error while connecting host %s: %s',\n            host_name, exc,\n            # Show the traceback if the debug logging level is enabled\n            exc_info_on_loglevel=logging.DEBUG\n        )\n        return {host_name: 'failed to connect host'}\n\n    return {host_name: 'connected host'}", "output": "Connect the specified host system in this VMware environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f connect_host my-vmware-config host=\"myHostSystemName\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_environment(self, environ):\n        \"\"\"\"\"\"\n        if environ:\n            q_environ = self._process.processEnvironment()\n            for k, v in environ.items():\n                q_environ.insert(k, v)\n            self._process.setProcessEnvironment(q_environ)", "output": "Set the environment on the QProcess.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_comparison_method(cls, op):\n        \"\"\"\n        \n        \"\"\"\n        def wrapper(self, other):\n            if isinstance(other, ABCSeries):\n                # the arrays defer to Series for comparison ops but the indexes\n                #  don't, so we have to unwrap here.\n                other = other._values\n\n            result = op(self._data, maybe_unwrap_index(other))\n            return result\n\n        wrapper.__doc__ = op.__doc__\n        wrapper.__name__ = '__{}__'.format(op.__name__)\n        return wrapper", "output": "Create a comparison method that dispatches to ``cls.values``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name, quiet=False, path=None):\n    '''\n    \n    '''\n    data = _do_names(name, 'start', path=path)\n    if data and not quiet:\n        __jid_event__.fire_event(\n            {'data': data, 'outputter': 'lxc_start'}, 'progress')\n    return data", "output": "Start the named container.\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0\n\n    .. code-block:: bash\n\n        salt-run lxc.start name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bleu_wrapper(ref_filename, hyp_filename, case_sensitive=False):\n  \"\"\"\"\"\"\n  ref_lines = text_encoder.native_to_unicode(\n      tf.gfile.Open(ref_filename, \"r\").read()).split(\"\\n\")\n  hyp_lines = text_encoder.native_to_unicode(\n      tf.gfile.Open(hyp_filename, \"r\").read()).split(\"\\n\")\n  assert len(ref_lines) == len(hyp_lines), (\"{} != {}\".format(\n      len(ref_lines), len(hyp_lines)))\n  if not case_sensitive:\n    ref_lines = [x.lower() for x in ref_lines]\n    hyp_lines = [x.lower() for x in hyp_lines]\n  ref_tokens = [bleu_tokenize(x) for x in ref_lines]\n  hyp_tokens = [bleu_tokenize(x) for x in hyp_lines]\n  return compute_bleu(ref_tokens, hyp_tokens)", "output": "Compute BLEU for two files (reference and hypothesis translation).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_checkpoint(model_dir, timeout_mins=240):\n  \"\"\"\n  \"\"\"\n  last_ckpt = None\n  timeout_secs = None\n  if timeout_mins != -1:\n    timeout_secs = timeout_mins * 60\n  while True:\n    last_ckpt = tf.contrib.training.wait_for_new_checkpoint(\n        model_dir, last_ckpt, seconds_to_sleep=60, timeout=timeout_secs)\n\n    if last_ckpt is None:\n      tf.logging.info(\n          \"Eval timeout: no new checkpoints within %dm\" % timeout_mins)\n      break\n\n    yield last_ckpt", "output": "Yields successive checkpoints from model_dir.\n\n  Args:\n    model_dir: The directory in which checkpoints are saved.\n    timeout_mins: The maximum amount of time in minutes to wait\n                  between checkpoints. Set this to -1 to wait indefinitely.\n  Yields:\n    last_ckpt: a new checkpoint path, or None if the timeout was reached.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_states(self, states):\n        \"\"\"\"\"\"\n        states = pickle.loads(states)\n        if isinstance(states, tuple) and len(states) == 2:\n            self.states, self.optimizer = states\n        else:\n            self.states = states\n        self.states_synced = dict.fromkeys(self.states.keys(), False)", "output": "Sets updater states.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_unique_tags(tag_list):\n  \"\"\"\"\"\"\n  frozen_tags_seen = set()\n  for tags in tag_list:\n    frozen_tags = frozenset(tags)\n    if frozen_tags in frozen_tags_seen:\n      raise ValueError(\"Tags %r used repeatedly\" % tags)\n    frozen_tags_seen.add(frozen_tags)", "output": "Checks that tag list contains each set of tags only once.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_order_params(self,\n                              asset,\n                              amount,\n                              limit_price,\n                              stop_price,\n                              style):\n        \"\"\"\n        \n        \"\"\"\n\n        if not self.initialized:\n            raise OrderDuringInitialize(\n                msg=\"order() can only be called from within handle_data()\"\n            )\n\n        if style:\n            if limit_price:\n                raise UnsupportedOrderParameters(\n                    msg=\"Passing both limit_price and style is not supported.\"\n                )\n\n            if stop_price:\n                raise UnsupportedOrderParameters(\n                    msg=\"Passing both stop_price and style is not supported.\"\n                )\n\n        for control in self.trading_controls:\n            control.validate(asset,\n                             amount,\n                             self.portfolio,\n                             self.get_datetime(),\n                             self.trading_client.current_data)", "output": "Helper method for validating parameters to the order API function.\n\n        Raises an UnsupportedOrderParameters if invalid arguments are found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_user_gnupghome(user):\n    '''\n    \n    '''\n    if user == 'salt':\n        gnupghome = os.path.join(__salt__['config.get']('config_dir'), 'gpgkeys')\n    else:\n        gnupghome = os.path.join(_get_user_info(user)['home'], '.gnupg')\n\n    return gnupghome", "output": "Return default GnuPG home directory path for a user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_account(self, account):\n        ''\n        if account.account_cookie not in self.account_list:\n            if self.cash_available > account.init_cash:\n                account.portfolio_cookie = self.portfolio_cookie\n                account.user_cookie = self.user_cookie\n                self.cash.append(self.cash_available - account.init_cash)\n                self.account_list.append(account.account_cookie)\n                account.save()\n                return account\n        else:\n            pass", "output": "portfolio add a account/stratetgy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option.'\n        )\n\n    templates = {}\n    vm_properties = [\n        \"name\",\n        \"config.template\",\n        \"config.guestFullName\",\n        \"config.hardware.numCPU\",\n        \"config.hardware.memoryMB\"\n    ]\n\n    vm_list = salt.utils.vmware.get_mors_with_properties(_get_si(), vim.VirtualMachine, vm_properties)\n\n    for vm in vm_list:\n        if \"config.template\" in vm and vm[\"config.template\"]:\n            templates[vm[\"name\"]] = {\n                'name': vm[\"name\"],\n                'guest_fullname': vm[\"config.guestFullName\"] if \"config.guestFullName\" in vm else \"N/A\",\n                'cpus': vm[\"config.hardware.numCPU\"] if \"config.hardware.numCPU\" in vm else \"N/A\",\n                'ram': vm[\"config.hardware.memoryMB\"] if \"config.hardware.memoryMB\" in vm else \"N/A\"\n            }\n\n    return templates", "output": "Return a list of all the templates present in this VMware environment with basic\n    details\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud --list-images my-vmware-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_from(self, parent_path):\n        \"\"\"\"\"\"\n        path_cls = type(parent_path)\n        is_dir = path_cls.is_dir\n        exists = path_cls.exists\n        scandir = parent_path._accessor.scandir\n        if not is_dir(parent_path):\n            return iter([])\n        return self._select_from(parent_path, is_dir, exists, scandir)", "output": "Iterate over all child paths of `parent_path` matched by this\n        selector.  This can contain parent_path itself.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_padding(builder, layer, input_names, output_names, keras_layer):\n    \"\"\"\n    \n    \"\"\"\n    _check_data_format(keras_layer)\n    # Get input and output names\n    input_name, output_name = (input_names[0], output_names[0])\n\n    is_1d = isinstance(keras_layer, _keras.layers.ZeroPadding1D)\n\n    padding = keras_layer.padding\n    top = left = bottom = right = 0\n    if is_1d:\n        if type(padding) is int:\n            left = right = padding\n        elif type(padding) is tuple:\n            if type(padding[0]) is int:\n                left, right = padding\n            elif type(padding[0]) is tuple and len(padding[0]) == 2:\n                left, right = padding[0]\n            else:\n                raise ValueError(\"Unrecognized padding option: %s\" % (str(padding)))\n        else:\n            raise ValueError(\"Unrecognized padding option: %s\" % (str(padding)))\n    else:\n        if type(padding) is int:\n            top = left = bottom = right = padding\n        elif type(padding) is tuple:\n            if type(padding[0]) is int:\n                top, left = padding\n                bottom, right = padding\n            elif type(padding[0]) is tuple:\n                top, bottom = padding[0]\n                left, right = padding[1]\n            else:\n                raise ValueError(\"Unrecognized padding option: %s\" % (str(padding)))\n        else:\n            raise ValueError(\"Unrecognized padding option: %s\" % (str(padding)))\n\n    # Now add the layer\n    builder.add_padding(name = layer,\n        left = left, right=right, top=top, bottom=bottom, value = 0,\n        input_name = input_name, output_name=output_name\n        )", "output": "Convert padding layer from keras to coreml.\n    Keras only supports zero padding at this time.\n    Parameters\n    ----------\n    keras_layer: layer\n        A keras layer object.\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_backward_begin(self, last_loss, last_output, **kwargs):\n        \"\"\n        last_loss = last_loss.detach().cpu()\n        if self.gen_mode:\n            self.smoothenerG.add_value(last_loss)\n            self.glosses.append(self.smoothenerG.smooth)\n            self.last_gen = last_output.detach().cpu()\n        else:\n            self.smoothenerC.add_value(last_loss)\n            self.closses.append(self.smoothenerC.smooth)", "output": "Record `last_loss` in the proper list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_nested_object(obj):\n    \"\"\"\n    \n\n    \"\"\"\n\n    if isinstance(obj, ABCSeries) and is_object_dtype(obj):\n\n        if any(isinstance(v, ABCSeries) for v in obj.values):\n            return True\n\n    return False", "output": "return a boolean if we have a nested object, e.g. a Series with 1 or\n    more Series elements\n\n    This may not be necessarily be performant.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_argument(self, dest, nargs=1, obj=None):\n        \"\"\"\n        \"\"\"\n        if obj is None:\n            obj = dest\n        self._args.append(Argument(dest=dest, nargs=nargs, obj=obj))", "output": "Adds a positional argument named `dest` to the parser.\n\n        The `obj` can be used to identify the option in the order list\n        that is returned from the parser.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map_column(self, value):\n        \"\"\"\n        \n        \"\"\"\n        if value in self.null_values:\n            return r'\\\\N'\n        else:\n            return default_escape(six.text_type(value))", "output": "Applied to each column of every row returned by `rows`.\n\n        Default behaviour is to escape special characters and identify any self.null_values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_positional_embedding(x, max_length, name=None, positions=None):\n  \"\"\"\n  \"\"\"\n  with tf.name_scope(\"add_positional_embedding\"):\n    _, length, depth = common_layers.shape_list(x)\n    var = tf.cast(tf.get_variable(name, [max_length, depth]), x.dtype)\n    if positions is None:\n      pad_length = tf.maximum(0, length - max_length)\n      sliced = tf.cond(\n          tf.less(length, max_length),\n          lambda: tf.slice(var, [0, 0], [length, -1]),\n          lambda: tf.pad(var, [[0, pad_length], [0, 0]]))\n      return x + tf.expand_dims(sliced, 0)\n    else:\n      return x + tf.gather(var, tf.to_int32(positions))", "output": "Adds positional embedding.\n\n  Args:\n    x: Tensor with shape [batch, length, depth].\n    max_length: int representing static maximum size of any dimension.\n    name: str representing name of the embedding tf.Variable.\n    positions: Tensor with shape [batch, length].\n\n  Returns:\n    Tensor of same shape as x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def multinomial_sample(x, vocab_size=None, sampling_method=\"random\",\n                       temperature=1.0):\n  \"\"\"\n  \"\"\"\n  vocab_size = vocab_size or common_layers.shape_list(x)[-1]\n  if sampling_method == \"random\" and temperature > 0.0:\n    samples = tf.multinomial(tf.reshape(x, [-1, vocab_size]) / temperature, 1)\n  else:\n    samples = tf.argmax(x, axis=-1)\n  reshaped_samples = tf.reshape(samples, common_layers.shape_list(x)[:-1])\n  return reshaped_samples", "output": "Multinomial sampling from a n-dimensional tensor.\n\n  Args:\n    x: Tensor of shape [..., vocab_size]. Parameterizes logits of multinomial.\n    vocab_size: Number of classes in multinomial distribution.\n    sampling_method: String, \"random\" or otherwise deterministic.\n    temperature: Positive float.\n\n  Returns:\n    Tensor of shape [...].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_cached_response(self, request, response):\n        \"\"\"\n        \"\"\"\n        cache_url = self.cache_url(request.url)\n\n        cached_response = self.serializer.loads(request, self.cache.get(cache_url))\n\n        if not cached_response:\n            # we didn't have a cached response\n            return response\n\n        # Lets update our headers with the headers from the new request:\n        # http://tools.ietf.org/html/draft-ietf-httpbis-p4-conditional-26#section-4.1\n        #\n        # The server isn't supposed to send headers that would make\n        # the cached body invalid. But... just in case, we'll be sure\n        # to strip out ones we know that might be problmatic due to\n        # typical assumptions.\n        excluded_headers = [\"content-length\"]\n\n        cached_response.headers.update(\n            dict(\n                (k, v)\n                for k, v in response.headers.items()\n                if k.lower() not in excluded_headers\n            )\n        )\n\n        # we want a 200 b/c we have content via the cache\n        cached_response.status = 200\n\n        # update our cache\n        self.cache.set(cache_url, self.serializer.dumps(request, cached_response))\n\n        return cached_response", "output": "On a 304 we will get a new set of headers that we want to\n        update our cached value with, assuming we have one.\n\n        This should only ever be called when we've sent an ETag and\n        gotten a 304 as the response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(argv=None):\n  \"\"\"\n  \n  \"\"\"\n  try:\n    _name_of_script, filepath = argv\n  except ValueError:\n    raise ValueError(argv)\n  print_accuracies(filepath=filepath, test_start=FLAGS.test_start,\n                   test_end=FLAGS.test_end, which_set=FLAGS.which_set,\n                   nb_iter=FLAGS.nb_iter, base_eps_iter=FLAGS.base_eps_iter,\n                   batch_size=FLAGS.batch_size)", "output": "Print accuracies", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_dups(head):\n    \"\"\"\n    \n    \"\"\"\n    hashset = set()\n    prev = Node()\n    while head:\n        if head.val in hashset:\n            prev.next = head.next\n        else:\n            hashset.add(head.val)\n            prev = head\n        head = head.next", "output": "Time Complexity: O(N)\n    Space Complexity: O(N)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpause_trial(self, trial):\n        \"\"\"\"\"\"\n        assert trial.status == Trial.PAUSED, trial.status\n        self.set_status(trial, Trial.PENDING)", "output": "Sets PAUSED trial to pending to allow scheduler to start.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_history(self, directory):\r\n        \"\"\"\"\"\"\r\n        try:\r\n            directory = osp.abspath(to_text_string(directory))\r\n            if directory in self.history:\r\n                self.histindex = self.history.index(directory)\r\n        except Exception:\r\n            user_directory = get_home_dir()\r\n            self.chdir(directory=user_directory, browsing_history=True)", "output": "Update browse history", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(cont=None, path=None, local_file=None, return_bin=False, profile=None):\n    '''\n    \n\n    '''\n    swift_conn = _auth(profile)\n\n    if cont is None:\n        return swift_conn.get_account()\n\n    if path is None:\n        return swift_conn.get_container(cont)\n\n    if return_bin is True:\n        return swift_conn.get_object(cont, path, return_bin)\n\n    if local_file is not None:\n        return swift_conn.get_object(cont, path, local_file)\n\n    return False", "output": "List the contents of a container, or return an object from a container. Set\n    return_bin to True in order to retrieve an object wholesale. Otherwise,\n    Salt will attempt to parse an XML response.\n\n    CLI Example to list containers:\n\n    .. code-block:: bash\n\n        salt myminion swift.get\n\n    CLI Example to list the contents of a container:\n\n    .. code-block:: bash\n\n        salt myminion swift.get mycontainer\n\n    CLI Example to return the binary contents of an object:\n\n    .. code-block:: bash\n\n        salt myminion swift.get mycontainer myfile.png return_bin=True\n\n    CLI Example to save the binary contents of an object to a local file:\n\n    .. code-block:: bash\n\n        salt myminion swift.get mycontainer myfile.png local_file=/tmp/myfile.png", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def server_delete(s_name, **connection_args):\n    '''\n    \n    '''\n    ret = True\n    server = _server_get(s_name, **connection_args)\n    if server is None:\n        return False\n    nitro = _connect(**connection_args)\n    if nitro is None:\n        return False\n    try:\n        NSServer.delete(nitro, server)\n    except NSNitroError as error:\n        log.debug('netscaler module error - NSServer.delete() failed: %s', error)\n        ret = False\n    _disconnect(nitro)\n    return ret", "output": "Delete a server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.server_delete 'serverName'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mapPartitions(self, f, preservesPartitioning=False):\n        \"\"\"\n        \n        \"\"\"\n        def func(s, iterator):\n            return f(iterator)\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)", "output": "Return a new RDD by applying a function to each partition of this RDD.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        >>> rdd.mapPartitions(f).collect()\n        [3, 7]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unread_data(self, data: bytes) -> None:\n        \"\"\" \n        \"\"\"\n        warnings.warn(\"unread_data() is deprecated \"\n                      \"and will be removed in future releases (#3260)\",\n                      DeprecationWarning,\n                      stacklevel=2)\n        if not data:\n            return\n\n        if self._buffer_offset:\n            self._buffer[0] = self._buffer[0][self._buffer_offset:]\n            self._buffer_offset = 0\n        self._size += len(data)\n        self._cursor -= len(data)\n        self._buffer.appendleft(data)\n        self._eof_counter = 0", "output": "rollback reading some data from stream, inserting it to buffer head.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reverse_url(self, name: str, *args: Any) -> str:\n        \"\"\"\"\"\"\n        return self.application.reverse_url(name, *args)", "output": "Alias for `Application.reverse_url`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_mixed_type_key(obj):\n    \"\"\"\n\n    \"\"\"\n    if isinstance(obj, _BaseNetwork):\n        return obj._get_networks_key()\n    elif isinstance(obj, _BaseAddress):\n        return obj._get_address_key()\n    return NotImplemented", "output": "Return a key suitable for sorting between networks and addresses.\n\n    Address and Network objects are not sortable by default; they're\n    fundamentally different so the expression\n\n        IPv4Address('192.0.2.0') <= IPv4Network('192.0.2.0/24')\n\n    doesn't make any sense.  There are some times however, where you may wish\n    to have ipaddress sort these for you anyway. If you need to do this, you\n    can use this function as the key= argument to sorted().\n\n    Args:\n      obj: either a Network or Address object.\n    Returns:\n      appropriate key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group(self, *args, **kwargs):\n        \"\"\"\n        \"\"\"\n        def decorator(f):\n            cmd = group(*args, **kwargs)(f)\n            self.add_command(cmd)\n            return cmd\n        return decorator", "output": "A shortcut decorator for declaring and attaching a group to\n        the group.  This takes the same arguments as :func:`group` but\n        immediately registers the created command with this instance by\n        calling into :meth:`add_command`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hash_tuples(vals, encoding='utf8', hash_key=None):\n    \"\"\"\n    \n    \"\"\"\n    is_tuple = False\n    if isinstance(vals, tuple):\n        vals = [vals]\n        is_tuple = True\n    elif not is_list_like(vals):\n        raise TypeError(\"must be convertible to a list-of-tuples\")\n\n    from pandas import Categorical, MultiIndex\n\n    if not isinstance(vals, ABCMultiIndex):\n        vals = MultiIndex.from_tuples(vals)\n\n    # create a list-of-Categoricals\n    vals = [Categorical(vals.codes[level],\n                        vals.levels[level],\n                        ordered=False,\n                        fastpath=True)\n            for level in range(vals.nlevels)]\n\n    # hash the list-of-ndarrays\n    hashes = (_hash_categorical(cat,\n                                encoding=encoding,\n                                hash_key=hash_key)\n              for cat in vals)\n    h = _combine_hash_arrays(hashes, len(vals))\n    if is_tuple:\n        h = h[0]\n\n    return h", "output": "Hash an MultiIndex / list-of-tuples efficiently\n\n    .. versionadded:: 0.20.0\n\n    Parameters\n    ----------\n    vals : MultiIndex, list-of-tuples, or single tuple\n    encoding : string, default 'utf8'\n    hash_key : string key to encode, default to _default_hash_key\n\n    Returns\n    -------\n    ndarray of hashed values array", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def crossproduct(d):\n    \"\"\"\n    \n    \"\"\"\n\n    from .. import SArray\n    d = [list(zip(list(d.keys()), x)) for x in _itertools.product(*list(d.values()))]\n    sa = [{k:v for (k,v) in x} for x in d]\n    return SArray(sa).unpack(column_name_prefix='')", "output": "Create an SFrame containing the crossproduct of all provided options.\n\n    Parameters\n    ----------\n    d : dict\n        Each key is the name of an option, and each value is a list\n        of the possible values for that option.\n\n    Returns\n    -------\n    out : SFrame\n        There will be a column for each key in the provided dictionary,\n        and a row for each unique combination of all values.\n\n    Example\n    -------\n    settings = {'argument_1':[0, 1],\n                'argument_2':['a', 'b', 'c']}\n    print crossproduct(settings)\n    +------------+------------+\n    | argument_2 | argument_1 |\n    +------------+------------+\n    |     a      |     0      |\n    |     a      |     1      |\n    |     b      |     0      |\n    |     b      |     1      |\n    |     c      |     0      |\n    |     c      |     1      |\n    +------------+------------+\n    [6 rows x 2 columns]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_info_file(tensorboard_info):\n  \"\"\"\n  \"\"\"\n  payload = \"%s\\n\" % _info_to_string(tensorboard_info)\n  with open(_get_info_file_path(), \"w\") as outfile:\n    outfile.write(payload)", "output": "Write TensorBoardInfo to the current process's info file.\n\n  This should be called by `main` once the server is ready. When the\n  server shuts down, `remove_info_file` should be called.\n\n  Args:\n    tensorboard_info: A valid `TensorBoardInfo` object.\n\n  Raises:\n    ValueError: If any field on `info` is not of the correct type.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(self):\n        \"\"\"\n        \n        \"\"\"\n\n        swagger = None\n\n        # First check if there is inline swagger\n        if self.definition_body:\n            swagger = self._read_from_definition_body()\n\n        if not swagger and self.definition_uri:\n            # If not, then try to download it from the given URI\n            swagger = self._download_swagger(self.definition_uri)\n\n        return swagger", "output": "Gets the Swagger document from either of the given locations. If we fail to retrieve or parse the Swagger\n        file, this method will return None.\n\n        Returns\n        -------\n        dict:\n            Swagger document. None, if we cannot retrieve the document", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_keys_to_action(self):\n    \"\"\"\n    \"\"\"\n    # Based on gym AtariEnv.get_keys_to_action()\n    keyword_to_key = {\n        \"UP\": ord(\"w\"),\n        \"DOWN\": ord(\"s\"),\n        \"LEFT\": ord(\"a\"),\n        \"RIGHT\": ord(\"d\"),\n        \"FIRE\": ord(\" \"),\n    }\n\n    keys_to_action = {}\n\n    for action_id, action_meaning in enumerate(self.action_meanings):\n      keys_tuple = tuple(sorted([\n          key for keyword, key in keyword_to_key.items()\n          if keyword in action_meaning]))\n      assert keys_tuple not in keys_to_action\n      keys_to_action[keys_tuple] = action_id\n\n    # Special actions:\n    keys_to_action[(ord(\"r\"),)] = self.RETURN_DONE_ACTION\n    keys_to_action[(ord(\"c\"),)] = self.TOGGLE_WAIT_ACTION\n    keys_to_action[(ord(\"n\"),)] = self.WAIT_MODE_NOOP_ACTION\n\n    return keys_to_action", "output": "Get mapping from keyboard keys to actions.\n\n    Required by gym.utils.play in environment or top level wrapper.\n\n    Returns:\n      {\n        Unicode code point for keyboard key: action (formatted for step()),\n        ...\n      }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _absent(name, dataset_type, force=False, recursive=False, recursive_all=False):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    ## log configuration\n    dataset_type = dataset_type.lower()\n    log.debug('zfs.%s_absent::%s::config::force = %s',\n              dataset_type, name, force)\n    log.debug('zfs.%s_absent::%s::config::recursive = %s',\n              dataset_type, name, recursive)\n\n    ## destroy dataset if needed\n    if __salt__['zfs.exists'](name, **{'type': dataset_type}):\n        ## NOTE: dataset found with the name and dataset_type\n        if not __opts__['test']:\n            mod_res = __salt__['zfs.destroy'](name, **{'force': force, 'recursive': recursive, 'recursive_all': recursive_all})\n        else:\n            mod_res = OrderedDict([('destroyed', True)])\n\n        ret['result'] = mod_res['destroyed']\n        if ret['result']:\n            ret['changes'][name] = 'destroyed'\n            ret['comment'] = '{0} {1} was destroyed'.format(\n                dataset_type,\n                name,\n            )\n        else:\n            ret['comment'] = 'failed to destroy {0} {1}'.format(\n                dataset_type,\n                name,\n            )\n            if 'error' in mod_res:\n                ret['comment'] = mod_res['error']\n    else:\n        ## NOTE: no dataset found with name of the dataset_type\n        ret['comment'] = '{0} {1} is absent'.format(\n            dataset_type,\n            name\n        )\n\n    return ret", "output": "internal shared function for *_absent\n\n    name : string\n        name of dataset\n    dataset_type : string [filesystem, volume, snapshot, or bookmark]\n        type of dataset to remove\n    force : boolean\n        try harder to destroy the dataset\n    recursive : boolean\n        also destroy all the child datasets\n    recursive_all : boolean\n        recursively destroy all dependents, including cloned file systems\n        outside the target hierarchy. (-R)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_create(alias, passwd, usrgrps, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'user.create'\n            params = {\"alias\": alias, \"passwd\": passwd, \"usrgrps\": []}\n            # User groups\n            if not isinstance(usrgrps, list):\n                usrgrps = [usrgrps]\n            for usrgrp in usrgrps:\n                params['usrgrps'].append({\"usrgrpid\": usrgrp})\n\n            params = _params_extend(params, _ignore_name=True, **kwargs)\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['userids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": ".. versionadded:: 2016.3.0\n\n    Create new zabbix user\n\n    .. note::\n        This function accepts all standard user properties: keyword argument\n        names differ depending on your zabbix version, see here__.\n\n        .. __: https://www.zabbix.com/documentation/2.0/manual/appendix/api/user/definitions#user\n\n    :param alias: user alias\n    :param passwd: user's password\n    :param usrgrps: user groups to add the user to\n\n    :param _connection_user: zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: url of zabbix frontend (can also be set in opts or pillar, see module's docstring)\n\n    :param firstname: string with firstname of the user, use 'firstname' instead of 'name' parameter to not mess\n                      with value supplied from Salt sls file.\n\n    :return: On success string with id of the created user.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.user_create james password007 '[7, 12]' firstname='James Bond'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_weight_param_summary(wp):\n    \"\"\"\n    \"\"\"\n    summary_str = ''\n    if wp.HasField('quantization'):\n        nbits = wp.quantization.numberOfBits\n        quant_type = 'linearly' if wp.quantization.HasField('linearQuantization') else 'lookup-table'\n        summary_str += '{}-bit {} quantized'.format(nbits, quant_type)\n\n    if len(wp.floatValue) > 0:\n        summary_str += '({} floatValues)'.format(len(wp.floatValue))\n    if len(wp.float16Value) > 0:\n        summary_str += '({} bytes float16Values)'.format(len(wp.float16Value))\n    if len(wp.rawValue) > 0:\n        summary_str += '({} bytes rawValues)'.format(len(wp.rawValue))\n\n    return summary_str", "output": "Get a summary of _NeuralNetwork_pb2.WeightParams\n    Args:\n    wp : _NeuralNetwork_pb2.WeightParams - the _NeuralNetwork_pb2.WeightParams message to display\n    Returns:\n    a str summary for wp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_batch(self, dataloader):\n        \"\"\"\n        \n        \"\"\"\n        sum_losses = 0\n        len_losses = 0\n        for input_data, input_label in tqdm(dataloader):\n            data = gluon.utils.split_and_load(input_data, self.ctx, even_split=False)\n            label = gluon.utils.split_and_load(input_label, self.ctx, even_split=False)\n            batch_size = input_data.shape[0]\n            sum_losses, len_losses = self.train(data, label, batch_size)\n            sum_losses += sum_losses\n            len_losses += len_losses\n\n        return sum_losses, len_losses", "output": "Description : training for LipNet", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def offset(self, num_to_skip):\n        \"\"\"\n        \"\"\"\n        return self.__class__(\n            self._parent,\n            projection=self._projection,\n            field_filters=self._field_filters,\n            orders=self._orders,\n            limit=self._limit,\n            offset=num_to_skip,\n            start_at=self._start_at,\n            end_at=self._end_at,\n        )", "output": "Skip to an offset in a query.\n\n        If the current query already has specified an offset, this will\n        overwrite it.\n\n        Args:\n            num_to_skip (int): The number of results to skip at the beginning\n                of query results. (Must be non-negative.)\n\n        Returns:\n            ~.firestore_v1beta1.query.Query: An offset query. Acts as a\n            copy of the current query, modified with the newly added\n            \"offset\" field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_org_details(organization_id):\n    '''\n    \n    '''\n\n    qdata = salt.utils.http.query(\n        '{0}/organization/{1}'.format(_base_url(), organization_id),\n        method='GET',\n        decode=True,\n        decode_type='json',\n        header_dict={\n            'X-DC-DEVKEY': _api_key(),\n            'Content-Type': 'application/json',\n        },\n    )\n    return qdata", "output": "Return the details for an organization\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run digicert.get_org_details 34\n\n    Returns a dictionary with the org details, or with 'error' and 'status' keys.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deploy_ray_func(func, partition, kwargs):  # pragma: no cover\n    \"\"\"\n    \"\"\"\n    try:\n        return func(partition, **kwargs)\n    # Sometimes Arrow forces us to make a copy of an object before we operate\n    # on it. We don't want the error to propagate to the user, and we want to\n    # avoid copying unless we absolutely have to.\n    except ValueError:\n        return func(partition.copy(), **kwargs)", "output": "Deploy a function to a partition in Ray.\n\n    Note: Ray functions are not detected by codecov (thus pragma: no cover)\n\n    Args:\n        func: The function to apply.\n        partition: The partition to apply the function to.\n        kwargs: A dictionary of keyword arguments for the function.\n\n    Returns:\n        The result of the function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def describe_api_models(restApiId, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        models = _multi_call(conn.get_models, 'items', restApiId=restApiId)\n        return {'models': [_convert_datetime_str(model) for model in models]}\n    except ClientError as e:\n        return {'error': __utils__['boto3.get_error'](e)}", "output": "Get all models for a given API\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_apigateway.describe_api_models restApiId", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_authorizer(self, restapi, uri, authorizer):\n        \"\"\"\n        \n        \"\"\"\n        authorizer_type = authorizer.get(\"type\", \"TOKEN\").upper()\n        identity_validation_expression = authorizer.get('validation_expression', None)\n\n        authorizer_resource = troposphere.apigateway.Authorizer(\"Authorizer\")\n        authorizer_resource.RestApiId = troposphere.Ref(restapi)\n        authorizer_resource.Name = authorizer.get(\"name\", \"ZappaAuthorizer\")\n        authorizer_resource.Type = authorizer_type\n        authorizer_resource.AuthorizerUri = uri\n        authorizer_resource.IdentitySource = \"method.request.header.%s\" % authorizer.get('token_header', 'Authorization')\n        if identity_validation_expression:\n            authorizer_resource.IdentityValidationExpression = identity_validation_expression\n\n        if authorizer_type == 'TOKEN':\n            if not self.credentials_arn:\n                self.get_credentials_arn()\n            authorizer_resource.AuthorizerResultTtlInSeconds = authorizer.get('result_ttl', 300)\n            authorizer_resource.AuthorizerCredentials = self.credentials_arn\n        if authorizer_type == 'COGNITO_USER_POOLS':\n            authorizer_resource.ProviderARNs = authorizer.get('provider_arns')\n\n        self.cf_api_resources.append(authorizer_resource.title)\n        self.cf_template.add_resource(authorizer_resource)\n\n        return authorizer_resource", "output": "Create Authorizer for API gateway", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bb_pad_collate(samples:BatchSamples, pad_idx:int=0) -> Tuple[FloatTensor, Tuple[LongTensor, LongTensor]]:\n    \"\"\n    if isinstance(samples[0][1], int): return data_collate(samples)\n    max_len = max([len(s[1].data[1]) for s in samples])\n    bboxes = torch.zeros(len(samples), max_len, 4)\n    labels = torch.zeros(len(samples), max_len).long() + pad_idx\n    imgs = []\n    for i,s in enumerate(samples):\n        imgs.append(s[0].data[None])\n        bbs, lbls = s[1].data\n        if not (bbs.nelement() == 0):\n            bboxes[i,-len(lbls):] = bbs\n            labels[i,-len(lbls):] = tensor(lbls)\n    return torch.cat(imgs,0), (bboxes,labels)", "output": "Function that collect `samples` of labelled bboxes and adds padding with `pad_idx`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_global_config_dir():\n        \"\"\"\n        \"\"\"\n        from appdirs import user_config_dir\n\n        return user_config_dir(\n            appname=Config.APPNAME, appauthor=Config.APPAUTHOR\n        )", "output": "Returns global config location. E.g. ~/.config/dvc/config.\n\n        Returns:\n            str: path to the global config directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_symbol(prototxt_fname):\n    \"\"\"\n    \"\"\"\n    sym, output_name, input_dim = _parse_proto(prototxt_fname)\n    exec(sym)                   # pylint: disable=exec-used\n    _locals = locals()\n    exec(\"ret = \" + output_name, globals(), _locals)  # pylint: disable=exec-used\n    ret = _locals['ret']\n    return ret, input_dim", "output": "Convert caffe model definition into Symbol\n\n    Parameters\n    ----------\n    prototxt_fname : str\n        Filename of the prototxt file\n\n    Returns\n    -------\n    Symbol\n        Converted Symbol\n    tuple\n        Input shape", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_router(router_class, router_name):\n    \"\"\"\n    \"\"\"\n    handle = router_class.remote(router_name)\n    ray.experimental.register_actor(router_name, handle)\n    handle.start.remote()\n    return handle", "output": "Wrapper for starting a router and register it.\n\n    Args:\n        router_class: The router class to instantiate.\n        router_name: The name to give to the router.\n\n    Returns:\n        A handle to newly started router actor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hybrid_forward(self, F, data, gamma, beta):\n        \"\"\"\"\"\"\n        # TODO(haibin): LayerNorm does not support fp16 safe reduction. Issue is tracked at:\n        # https://github.com/apache/incubator-mxnet/issues/14073\n        if self._dtype:\n            data = data.astype('float32')\n            gamma = gamma.astype('float32')\n            beta = beta.astype('float32')\n        norm_data = F.LayerNorm(data, gamma=gamma, beta=beta, axis=self._axis, eps=self._epsilon)\n        if self._dtype:\n            norm_data = norm_data.astype(self._dtype)\n        return norm_data", "output": "forward computation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_portfolio_info(self, portfolio_code):\n        \"\"\"\n        \n        \"\"\"\n        url = self.config[\"portfolio_url\"] + portfolio_code\n        html = self._get_html(url)\n        match_info = re.search(r\"(?<=SNB.cubeInfo = ).*(?=;\\n)\", html)\n        if match_info is None:\n            raise Exception(\n                \"cant get portfolio info, portfolio html : {}\".format(html)\n            )\n        try:\n            portfolio_info = json.loads(match_info.group())\n        except Exception as e:\n            raise Exception(\"get portfolio info error: {}\".format(e))\n        return portfolio_info", "output": "\u83b7\u53d6\u7ec4\u5408\u4fe1\u606f\n        :return: \u5b57\u5178", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_allowed_shape_ranges(spec):\n    \"\"\"\n    \n    \"\"\"\n\n    shaper = NeuralNetworkShaper(spec, False)\n    inputs = _get_input_names(spec)\n    output = {}\n\n    for input in inputs:\n        output[input] = shaper.shape(input)\n\n    return output", "output": "For a given model specification, returns a dictionary with a shape range object for each input feature name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.log_file != PIPE and not (self.log_file == DEVNULL and _HAS_NATIVE_DEVNULL):\n            try:\n                self.log_file.close()\n            except Exception:\n                pass\n\n        if self.process is None:\n            return\n\n        try:\n            self.send_remote_shutdown_command()\n        except TypeError:\n            pass\n\n        try:\n            if self.process:\n                for stream in [self.process.stdin,\n                               self.process.stdout,\n                               self.process.stderr]:\n                    try:\n                        stream.close()\n                    except AttributeError:\n                        pass\n                self.process.terminate()\n                self.process.wait()\n                self.process.kill()\n                self.process = None\n        except OSError:\n            pass", "output": "Stops the service.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialize_schema(connection):\n  \"\"\"\n  \"\"\"\n  cursor = connection.cursor()\n  cursor.execute(\"PRAGMA application_id={}\".format(_TENSORBOARD_APPLICATION_ID))\n  cursor.execute(\"PRAGMA user_version={}\".format(_TENSORBOARD_USER_VERSION))\n  with connection:\n    for statement in _SCHEMA_STATEMENTS:\n      lines = statement.strip('\\n').split('\\n')\n      message = lines[0] + ('...' if len(lines) > 1 else '')\n      logger.debug('Running DB init statement: %s', message)\n      cursor.execute(statement)", "output": "Initializes the TensorBoard sqlite schema using the given connection.\n\n  Args:\n    connection: A sqlite DB connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _clone(self):\n        \"\"\"\n        \"\"\"\n        cloned_self = self.__class__(\n            *self.flat_path, project=self.project, namespace=self.namespace\n        )\n        # If the current parent has already been set, we re-use\n        # the same instance\n        cloned_self._parent = self._parent\n        return cloned_self", "output": "Duplicates the Key.\n\n        Most attributes are simple types, so don't require copying. Other\n        attributes like ``parent`` are long-lived and so we re-use them.\n\n        :rtype: :class:`google.cloud.datastore.key.Key`\n        :returns: A new ``Key`` instance with the same data as the current one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __reg_query_value(handle, value_name):\n        '''\n        \n        '''\n        # item_value, item_type = win32api.RegQueryValueEx(self.__reg_uninstall_handle, value_name)\n        item_value, item_type = win32api.RegQueryValueEx(handle, value_name)  # pylint: disable=no-member\n        if six.PY2 and isinstance(item_value, six.string_types) and not isinstance(item_value, six.text_type):\n            try:\n                item_value = six.text_type(item_value, encoding='mbcs')\n            except UnicodeError:\n                pass\n        if item_type == win32con.REG_EXPAND_SZ:\n            # expects Unicode input\n            win32api.ExpandEnvironmentStrings(item_value)  # pylint: disable=no-member\n            item_type = win32con.REG_SZ\n        return item_value, item_type", "output": "Calls RegQueryValueEx\n\n        If PY2 ensure unicode string and expand REG_EXPAND_SZ before returning\n        Remember to catch not found exceptions when calling.\n\n        Args:\n            handle (object): open registry handle.\n            value_name (str): Name of the value you wished returned\n\n        Returns:\n            tuple: type, value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def connect_database(url):\n    \"\"\"\n    \n\n    \"\"\"\n    db = _connect_database(url)\n    db.copy = lambda: _connect_database(url)\n    return db", "output": "create database object by url\n\n    mysql:\n        mysql+type://user:passwd@host:port/database\n    sqlite:\n        # relative path\n        sqlite+type:///path/to/database.db\n        # absolute path\n        sqlite+type:////path/to/database.db\n        # memory database\n        sqlite+type://\n    mongodb:\n        mongodb+type://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]]\n        more: http://docs.mongodb.org/manual/reference/connection-string/\n    sqlalchemy:\n        sqlalchemy+postgresql+type://user:passwd@host:port/database\n        sqlalchemy+mysql+mysqlconnector+type://user:passwd@host:port/database\n        more: http://docs.sqlalchemy.org/en/rel_0_9/core/engines.html\n    redis:\n        redis+taskdb://host:port/db\n    elasticsearch:\n        elasticsearch+type://host:port/?index=pyspider\n    local:\n        local+projectdb://filepath,filepath\n\n    type:\n        taskdb\n        projectdb\n        resultdb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_present(name, description=None, enabled=True, profile=None,\n                    **connection_args):\n    '''\n    \n\n    '''\n\n    return tenant_present(name, description=description, enabled=enabled, profile=profile,\n                          **connection_args)", "output": "Ensures that the keystone project exists\n    Alias for tenant_present from V2 API to fulfill\n    V3 API naming convention.\n\n    .. versionadded:: 2016.11.0\n\n    name\n        The name of the project to manage\n\n    description\n        The description to use for this project\n\n    enabled\n        Availability state for this project\n\n    .. code-block:: yaml\n\n        nova:\n            keystone.project_present:\n                - enabled: True\n                - description: 'Nova Compute Service'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __create_orget_address(conn, name, region):\n    '''\n    \n    '''\n    try:\n        addy = conn.ex_get_address(name, region)\n    except ResourceNotFoundError:  # pylint: disable=W0703\n        addr_kwargs = {\n            'name': name,\n            'region': region\n        }\n        new_addy = create_address(addr_kwargs, \"function\")\n        addy = conn.ex_get_address(new_addy['name'], new_addy['region'])\n\n    return addy", "output": "Reuse or create a static IP address.\n    Returns a native GCEAddress construct to use with libcloud.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def queue_put_stoppable(self, q, obj):\n        \"\"\" \"\"\"\n        while not self.stopped():\n            try:\n                q.put(obj, timeout=5)\n                break\n            except queue.Full:\n                pass", "output": "Put obj to queue, but will give up when the thread is stopped", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expires(self):\n        \"\"\"\n        \"\"\"\n        expiration_time = self._properties.get(\"expirationTime\")\n        if expiration_time is not None:\n            # expiration_time will be in milliseconds.\n            return google.cloud._helpers._datetime_from_microseconds(\n                1000.0 * float(expiration_time)\n            )", "output": "Union[datetime.datetime, None]: Datetime at which the table will be\n        deleted.\n\n        Raises:\n            ValueError: For invalid value types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_settings(self, index=None, name=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(index, \"_settings\", name), params=params\n        )", "output": "Retrieve settings for one or more (or all) indices.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-settings.html>`_\n\n        :arg index: A comma-separated list of index names; use `_all` or empty\n            string to perform the operation on all indices\n        :arg name: The name of the settings that should be included\n        :arg allow_no_indices: Whether to ignore if a wildcard indices\n            expression resolves into no concrete indices. (This includes `_all`\n            string or when no indices have been specified)\n        :arg expand_wildcards: Whether to expand wildcard expression to concrete\n            indices that are open, closed or both., default ['open', 'closed'],\n            valid choices are: 'open', 'closed', 'none', 'all'\n        :arg flat_settings: Return settings in flat format (default: false)\n        :arg ignore_unavailable: Whether specified concrete indices should be\n            ignored when unavailable (missing or closed)\n        :arg include_defaults: Whether to return all default setting for each of\n            the indices., default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _decode_input_tensor_to_features_dict(feature_map, hparams):\n  \"\"\"\n  \"\"\"\n  inputs = tf.convert_to_tensor(feature_map[\"inputs\"])\n  input_is_image = False\n\n  x = inputs\n  p_hparams = hparams.problem_hparams\n  # Add a third empty dimension\n  x = tf.expand_dims(x, axis=[2])\n  x = tf.to_int32(x)\n  input_space_id = tf.constant(p_hparams.input_space_id)\n  target_space_id = tf.constant(p_hparams.target_space_id)\n\n  features = {}\n  features[\"input_space_id\"] = input_space_id\n  features[\"target_space_id\"] = target_space_id\n  features[\"decode_length\"] = (\n      IMAGE_DECODE_LENGTH if input_is_image else tf.shape(x)[1] + 50)\n  features[\"inputs\"] = x\n  return features", "output": "Convert the interactive input format (see above) to a dictionary.\n\n  Args:\n    feature_map: dict with inputs.\n    hparams: model hyperparameters\n\n  Returns:\n    a features dictionary, as expected by the decoder.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insertInto(self, tableName, overwrite=False):\n        \"\"\"\n        \"\"\"\n        self._jwrite.mode(\"overwrite\" if overwrite else \"append\").insertInto(tableName)", "output": "Inserts the content of the :class:`DataFrame` to the specified table.\n\n        It requires that the schema of the class:`DataFrame` is the same as the\n        schema of the table.\n\n        Optionally overwriting any existing data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancel(self, client=None):\n        \"\"\"\n        \"\"\"\n        client = self._require_client(client)\n\n        extra_params = {}\n        if self.location:\n            extra_params[\"location\"] = self.location\n\n        api_response = client._connection.api_request(\n            method=\"POST\", path=\"%s/cancel\" % (self.path,), query_params=extra_params\n        )\n        self._set_properties(api_response[\"job\"])\n        # The Future interface requires that we return True if the *attempt*\n        # to cancel was successful.\n        return True", "output": "API call:  cancel job via a POST request\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/cancel\n\n        :type client: :class:`~google.cloud.bigquery.client.Client` or\n                      ``NoneType``\n        :param client: the client to use.  If not passed, falls back to the\n                       ``client`` stored on the current dataset.\n\n        :rtype: bool\n        :returns: Boolean indicating that the cancel request was sent.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_lock(self, lock_type='update'):\n        '''\n        \n        '''\n        lock_file = self._get_lock_file(lock_type=lock_type)\n\n        def _add_error(errlist, exc):\n            msg = ('Unable to remove update lock for {0} ({1}): {2} '\n                   .format(self.url, lock_file, exc))\n            log.debug(msg)\n            errlist.append(msg)\n\n        success = []\n        failed = []\n\n        try:\n            os.remove(lock_file)\n        except OSError as exc:\n            if exc.errno == errno.ENOENT:\n                # No lock file present\n                pass\n            elif exc.errno == errno.EISDIR:\n                # Somehow this path is a directory. Should never happen\n                # unless some wiseguy manually creates a directory at this\n                # path, but just in case, handle it.\n                try:\n                    shutil.rmtree(lock_file)\n                except OSError as exc:\n                    _add_error(failed, exc)\n            else:\n                _add_error(failed, exc)\n        else:\n            msg = 'Removed {0} lock for {1} remote \\'{2}\\''.format(\n                lock_type,\n                self.role,\n                self.id\n            )\n            log.debug(msg)\n            success.append(msg)\n        return success, failed", "output": "Clear update.lk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_args(args, root):\n    \"\"\"\n    \n    \"\"\"\n\n    extension_args = {}\n\n    for arg in args:\n        parse_extension_arg(arg, extension_args)\n\n    for name in sorted(extension_args, key=len):\n        path = name.split('.')\n        update_namespace(root, path, extension_args[name])", "output": "Encapsulates a set of custom command line arguments in key=value\n    or key.namespace=value form into a chain of Namespace objects,\n    where each next level is an attribute of the Namespace object on the\n    current level\n\n    Parameters\n    ----------\n    args : list\n        A list of strings representing arguments in key=value form\n    root : Namespace\n        The top-level element of the argument tree", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_state_changed(self, state):\n        \"\"\"\"\"\"\n        if state:\n            self.editor.sig_key_pressed.connect(self._on_key_pressed)\n        else:\n            self.editor.sig_key_pressed.disconnect(self._on_key_pressed)", "output": "Connect/disconnect sig_key_pressed signal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _query_response_to_snapshot(response_pb, collection, expected_prefix):\n    \"\"\"\n    \"\"\"\n    if not response_pb.HasField(\"document\"):\n        return None\n\n    document_id = _helpers.get_doc_id(response_pb.document, expected_prefix)\n    reference = collection.document(document_id)\n    data = _helpers.decode_dict(response_pb.document.fields, collection._client)\n    snapshot = document.DocumentSnapshot(\n        reference,\n        data,\n        exists=True,\n        read_time=response_pb.read_time,\n        create_time=response_pb.document.create_time,\n        update_time=response_pb.document.update_time,\n    )\n    return snapshot", "output": "Parse a query response protobuf to a document snapshot.\n\n    Args:\n        response_pb (google.cloud.proto.firestore.v1beta1.\\\n            firestore_pb2.RunQueryResponse): A\n        collection (~.firestore_v1beta1.collection.CollectionReference): A\n            reference to the collection that initiated the query.\n        expected_prefix (str): The expected prefix for fully-qualified\n            document names returned in the query results. This can be computed\n            directly from ``collection`` via :meth:`_parent_info`.\n\n    Returns:\n        Optional[~.firestore.document.DocumentSnapshot]: A\n        snapshot of the data returned in the query. If ``response_pb.document``\n        is not set, the snapshot will be :data:`None`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resume(profile_process='worker'):\n    \"\"\"\n    \n    \"\"\"\n    profile_process2int = {'worker': 0, 'server': 1}\n    check_call(_LIB.MXProcessProfilePause(int(0),\n                                          profile_process2int[profile_process],\n                                          profiler_kvstore_handle))", "output": "Resume paused profiling.\n\n    Parameters\n    ----------\n    profile_process : string\n        whether to profile kvstore `server` or `worker`.\n        server can only be profiled when kvstore is of type dist.\n        if this is not passed, defaults to `worker`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sparse_series_to_coo(ss, row_levels=(0, ), column_levels=(1, ),\n                          sort_labels=False):\n    \"\"\"\n    \n    \"\"\"\n\n    import scipy.sparse\n\n    if ss.index.nlevels < 2:\n        raise ValueError('to_coo requires MultiIndex with nlevels > 2')\n    if not ss.index.is_unique:\n        raise ValueError('Duplicate index entries are not allowed in to_coo '\n                         'transformation.')\n\n    # to keep things simple, only rely on integer indexing (not labels)\n    row_levels = [ss.index._get_level_number(x) for x in row_levels]\n    column_levels = [ss.index._get_level_number(x) for x in column_levels]\n\n    v, i, j, rows, columns = _to_ijv(ss, row_levels=row_levels,\n                                     column_levels=column_levels,\n                                     sort_labels=sort_labels)\n    sparse_matrix = scipy.sparse.coo_matrix(\n        (v, (i, j)), shape=(len(rows), len(columns)))\n    return sparse_matrix, rows, columns", "output": "Convert a SparseSeries to a scipy.sparse.coo_matrix using index\n    levels row_levels, column_levels as the row and column\n    labels respectively. Returns the sparse_matrix, row and column labels.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aligned_umap(activations, umap_options={}, normalize=True, verbose=False):\n    \"\"\"\"\"\"\n\n    umap_defaults = dict(\n        n_components=2, n_neighbors=50, min_dist=0.05, verbose=verbose, metric=\"cosine\"\n    )\n    umap_defaults.update(umap_options)\n\n    # if passed a list of activations, we combine them and later split the layouts\n    if type(activations) is list or type(activations) is tuple:\n        num_activation_groups = len(activations)\n        combined_activations = np.concatenate(activations)\n    else:\n        num_activation_groups = 1\n        combined_activations = activations\n    try:\n        layout = UMAP(**umap_defaults).fit_transform(combined_activations)\n    except (RecursionError, SystemError) as exception:\n        log.error(\"UMAP failed to fit these activations. We're not yet sure why this sometimes occurs.\")\n        raise ValueError(\"UMAP failed to fit activations: %s\", exception)\n\n    if normalize:\n        layout = normalize_layout(layout)\n\n    if num_activation_groups > 1:\n        layouts = np.split(layout, num_activation_groups, axis=0)\n        return layouts\n    else:\n        return layout", "output": "`activations` can be a list of ndarrays. In that case a list of layouts is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_train_begin(self, **kwargs:Any)->None:\n        \"\"\n        self.best = float('inf') if self.operator == np.less else -float('inf')", "output": "Initializes the best value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def position_to_value(self, y):\n        \"\"\"\"\"\"\n        vsb = self.editor.verticalScrollBar()\n        return vsb.minimum()+max([0, (y-self.offset)/self.get_scale_factor()])", "output": "Convert position in pixels to value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def running(opts):\n    '''\n    \n    '''\n\n    ret = []\n    proc_dir = os.path.join(opts['cachedir'], 'proc')\n    if not os.path.isdir(proc_dir):\n        return ret\n    for fn_ in os.listdir(proc_dir):\n        path = os.path.join(proc_dir, fn_)\n        try:\n            data = _read_proc_file(path, opts)\n            if data is not None:\n                ret.append(data)\n        except (IOError, OSError):\n            # proc files may be removed at any time during this process by\n            # the minion process that is executing the JID in question, so\n            # we must ignore ENOENT during this process\n            pass\n    return ret", "output": "Return the running jobs on this minion", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mtf_image_transformer_base_imagenet():\n  \"\"\"\"\"\"\n  hparams = mtf_image_transformer_base_cifar()\n  hparams.mesh_shape = \"batch:32\"\n  hparams.layout = \"batch:batch\"\n  hparams.batch_size = 128\n  hparams.d_ff = 2048\n  hparams.hidden_size = 512\n  hparams.num_decoder_layers = 12\n  hparams.learning_rate = 0.5\n  hparams.learning_rate_warmup_steps = 31250\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.unconditional = True\n  return hparams", "output": "Data parallel CIFAR parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lstrip_word(word, prefix):\n    '''\n    \n    '''\n\n    if six.text_type(word).startswith(prefix):\n        return six.text_type(word)[len(prefix):]\n    return word", "output": "Return a copy of the string after the specified prefix was removed\n    from the beginning of the string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_bytes_list(cls, function_descriptor_list):\n        \"\"\"\n        \"\"\"\n        assert isinstance(function_descriptor_list, list)\n        if len(function_descriptor_list) == 0:\n            # This is a function descriptor of driver task.\n            return FunctionDescriptor.for_driver_task()\n        elif (len(function_descriptor_list) == 3\n              or len(function_descriptor_list) == 4):\n            module_name = ensure_str(function_descriptor_list[0])\n            class_name = ensure_str(function_descriptor_list[1])\n            function_name = ensure_str(function_descriptor_list[2])\n            if len(function_descriptor_list) == 4:\n                return cls(module_name, function_name, class_name,\n                           function_descriptor_list[3])\n            else:\n                return cls(module_name, function_name, class_name)\n        else:\n            raise Exception(\n                \"Invalid input for FunctionDescriptor.from_bytes_list\")", "output": "Create a FunctionDescriptor instance from list of bytes.\n\n        This function is used to create the function descriptor from\n        backend data.\n\n        Args:\n            cls: Current class which is required argument for classmethod.\n            function_descriptor_list: list of bytes to represent the\n                function descriptor.\n\n        Returns:\n            The FunctionDescriptor instance created from the bytes list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def deviation(self, series, start, limit, mean):\n        '''\n        \n        '''\n        d = []\n        for x in range(start, limit):\n            d.append(float(series[x] - mean))\n        return d", "output": ":type start: int\n        :type limit: int\n        :type mean: int\n        :rtype: list()", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_logical(self, rule, field, value):\n        \"\"\"  \"\"\"\n        if not isinstance(value, Sequence):\n            self._error(field, errors.BAD_TYPE)\n            return\n\n        validator = self._get_child_validator(\n            document_crumb=rule, allow_unknown=False,\n            schema=self.target_validator.validation_rules)\n\n        for constraints in value:\n            _hash = (mapping_hash({'turing': constraints}),\n                     mapping_hash(self.target_validator.types_mapping))\n            if _hash in self.target_validator._valid_schemas:\n                continue\n\n            validator(constraints, normalize=False)\n            if validator._errors:\n                self._error(validator._errors)\n            else:\n                self.target_validator._valid_schemas.add(_hash)", "output": "{'allowed': ('allof', 'anyof', 'noneof', 'oneof')}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def group_get(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.get_group(**kwargs)", "output": "Get a single group\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.group_get name=group1\n        salt '*' keystoneng.group_get name=group2 domain_id=b62e76fbeeff4e8fb77073f591cf211e\n        salt '*' keystoneng.group_get name=0e4febc2a5ab4f2c8f374b054162506d", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, remotepath, localpath, callback=None):\n        \"\"\"\n        \n        \"\"\"\n        with open(localpath, \"wb\") as fl:\n            size = self.getfo(remotepath, fl, callback)\n        s = os.stat(localpath)\n        if s.st_size != size:\n            raise IOError(\n                \"size mismatch in get!  {} != {}\".format(s.st_size, size)\n            )", "output": "Copy a remote file (``remotepath``) from the SFTP server to the local\n        host as ``localpath``.  Any exception raised by operations will be\n        passed through.  This method is primarily provided as a convenience.\n\n        :param str remotepath: the remote file to copy\n        :param str localpath: the destination path on the local host\n        :param callable callback:\n            optional callback function (form: ``func(int, int)``) that accepts\n            the bytes transferred so far and the total bytes to be transferred\n\n        .. versionadded:: 1.4\n        .. versionchanged:: 1.7.4\n            Added the ``callback`` param", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zip_html(self):\n        \"\"\"\n        \n        \"\"\"\n        zip_fname = os.path.join(BUILD_PATH, 'html', 'pandas.zip')\n        if os.path.exists(zip_fname):\n            os.remove(zip_fname)\n        dirname = os.path.join(BUILD_PATH, 'html')\n        fnames = os.listdir(dirname)\n        os.chdir(dirname)\n        self._run_os('zip',\n                     zip_fname,\n                     '-r',\n                     '-q',\n                     *fnames)", "output": "Compress HTML documentation into a zip file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_schedule(schedule):\n  \"\"\"\n  \"\"\"\n  interpolation, steps, pmfs = schedule\n  return interpolation + ' ' + ' '.join(\n      '@' + str(s) + ' ' + ' '.join(map(str, p)) for s, p in zip(steps, pmfs))", "output": "Encodes a schedule tuple into a string.\n\n  Args:\n    schedule: A tuple containing (interpolation, steps, pmfs), where\n      interpolation is a string specifying the interpolation strategy, steps\n      is an int array_like of shape [N] specifying the global steps, and pmfs is\n      an array_like of shape [N, M] where pmf[i] is the sampling distribution\n      at global step steps[i]. N is the number of schedule requirements to\n      interpolate and M is the size of the probability space.\n\n  Returns:\n    The string encoding of the schedule tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def RandomNormalInitializer(stddev=1e-2):\n  \"\"\"\"\"\"\n  def init(shape, rng):\n    return (stddev * backend.random.normal(rng, shape)).astype('float32')\n  return init", "output": "An initializer function for random normal coefficients.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_path_sum(root, sum):\n    \"\"\"\n    \n    \"\"\"\n    if root is None:\n        return False\n    if root.left is None and root.right is None and root.val == sum:\n        return True\n    sum -= root.val\n    return has_path_sum(root.left, sum) or has_path_sum(root.right, sum)", "output": ":type root: TreeNode\n    :type sum: int\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_generator(filepath,\n                      dataset,\n                      chunk_size=1,\n                      start_idx=None,\n                      end_idx=None):\n  \"\"\"\"\"\"\n  encoder = dna_encoder.DNAEncoder(chunk_size=chunk_size)\n  with h5py.File(filepath, \"r\") as h5_file:\n    # Get input keys from h5_file\n    src_keys = [s % dataset for s in [\"%s_in\", \"%s_na\", \"%s_out\"]]\n    src_values = [h5_file[k] for k in src_keys]\n    inp_data, mask_data, out_data = src_values\n    assert len(set([v.len() for v in src_values])) == 1\n\n    if start_idx is None:\n      start_idx = 0\n    if end_idx is None:\n      end_idx = inp_data.len()\n\n    for i in range(start_idx, end_idx):\n      if i % 100 == 0:\n        print(\"Generating example %d for %s\" % (i, dataset))\n      inputs, mask, outputs = inp_data[i], mask_data[i], out_data[i]\n      ex_dict = to_example_dict(encoder, inputs, mask, outputs)\n      # Original data has one output for every 128 input bases. Ensure that the\n      # ratio has been maintained given the chunk size and removing EOS.\n      assert (len(ex_dict[\"inputs\"]) - 1) == ((\n          128 // chunk_size) * ex_dict[\"targets_shape\"][0])\n      yield ex_dict", "output": "Generate example dicts.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(app_id):\n    '''\n    \n    '''\n    cmd = 'sqlite3 \"/Library/Application Support/com.apple.TCC/TCC.db\" ' \\\n          '\"DELETE from access where client=\\'{0}\\'\"'.format(app_id)\n    call = __salt__['cmd.run_all'](\n        cmd,\n        output_loglevel='debug',\n        python_shell=False\n    )\n\n    if call['retcode'] != 0:\n        comment = ''\n        if 'stderr' in call:\n            comment += call['stderr']\n        if 'stdout' in call:\n            comment += call['stdout']\n\n        raise CommandExecutionError('Error removing app: {0}'.format(comment))\n\n    return True", "output": "Remove a bundle ID or command as being allowed to use assistive access.\n\n    app_id\n        The bundle ID or command to remove from assistive access list.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' assistive.remove /usr/bin/osascript\n        salt '*' assistive.remove com.smileonmymac.textexpander", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_recent_repeated_responses(chatbot, conversation, sample=10, threshold=3, quantity=3):\n    \"\"\"\n    \n    \"\"\"\n    from collections import Counter\n\n    # Get the most recent statements from the conversation\n    conversation_statements = list(chatbot.storage.filter(\n        conversation=conversation,\n        order_by=['id']\n    ))[sample * -1:]\n\n    text_of_recent_responses = [\n        statement.text for statement in conversation_statements\n    ]\n\n    counter = Counter(text_of_recent_responses)\n\n    # Find the n most common responses from the conversation\n    most_common = counter.most_common(quantity)\n\n    return [\n        counted[0] for counted in most_common\n        if counted[1] >= threshold\n    ]", "output": "A filter that eliminates possibly repetitive responses to prevent\n    a chat bot from repeating statements that it has recently said.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_np(v):\n    ''''''\n    if isinstance(v, float): return np.array(v)\n    if isinstance(v, (np.ndarray, np.generic)): return v\n    if isinstance(v, (list,tuple)): return [to_np(o) for o in v]\n    if isinstance(v, Variable): v=v.data\n    if torch.cuda.is_available():\n        if is_half_tensor(v): v=v.float()\n    if isinstance(v, torch.FloatTensor): v=v.float()\n    return v.cpu().numpy()", "output": "returns an np.array object given an input of np.array, list, tuple, torch variable or tensor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fill(self, background_shape, img):\n        \"\"\"\n        \n        \"\"\"\n        background_shape = tuple(background_shape)\n        return self._fill(background_shape, img)", "output": "Return a proper background image of background_shape, given img.\n\n        Args:\n            background_shape (tuple): a shape (h, w)\n            img: an image\n        Returns:\n            a background image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_trial(self, name, specification):\n        \"\"\"\"\"\"\n        payload = {\"name\": name, \"spec\": specification}\n        response = requests.post(urljoin(self._path, \"trials\"), json=payload)\n        return self._deserialize(response)", "output": "Adds a trial by name and specification (dict).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_from_dt(self, data, nthread):\n        \"\"\"\n        \n        \"\"\"\n        ptrs = (ctypes.c_void_p * data.ncols)()\n        if hasattr(data, \"internal\") and hasattr(data.internal, \"column\"):\n            # datatable>0.8.0\n            for icol in range(data.ncols):\n                col = data.internal.column(icol)\n                ptr = col.data_pointer\n                ptrs[icol] = ctypes.c_void_p(ptr)\n        else:\n            # datatable<=0.8.0\n            from datatable.internal import frame_column_data_r  # pylint: disable=no-name-in-module,import-error\n            for icol in range(data.ncols):\n                ptrs[icol] = frame_column_data_r(data, icol)\n\n        # always return stypes for dt ingestion\n        feature_type_strings = (ctypes.c_char_p * data.ncols)()\n        for icol in range(data.ncols):\n            feature_type_strings[icol] = ctypes.c_char_p(data.stypes[icol].name.encode('utf-8'))\n\n        handle = ctypes.c_void_p()\n        _check_call(_LIB.XGDMatrixCreateFromDT(\n            ptrs, feature_type_strings,\n            c_bst_ulong(data.shape[0]),\n            c_bst_ulong(data.shape[1]),\n            ctypes.byref(handle),\n            nthread))\n        self.handle = handle", "output": "Initialize data from a datatable Frame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pandas_dtype(dtype):\n    \"\"\"\n    \n    \"\"\"\n    # short-circuit\n    if isinstance(dtype, np.ndarray):\n        return dtype.dtype\n    elif isinstance(dtype, (np.dtype, PandasExtensionDtype, ExtensionDtype)):\n        return dtype\n\n    # registered extension types\n    result = registry.find(dtype)\n    if result is not None:\n        return result\n\n    # try a numpy dtype\n    # raise a consistent TypeError if failed\n    try:\n        npdtype = np.dtype(dtype)\n    except Exception:\n        # we don't want to force a repr of the non-string\n        if not isinstance(dtype, str):\n            raise TypeError(\"data type not understood\")\n        raise TypeError(\"data type '{}' not understood\".format(\n            dtype))\n\n    # Any invalid dtype (such as pd.Timestamp) should raise an error.\n    # np.dtype(invalid_type).kind = 0 for such objects. However, this will\n    # also catch some valid dtypes such as object, np.object_ and 'object'\n    # which we safeguard against by catching them earlier and returning\n    # np.dtype(valid_dtype) before this condition is evaluated.\n    if is_hashable(dtype) and dtype in [object, np.object_, 'object', 'O']:\n        # check hashability to avoid errors/DeprecationWarning when we get\n        # here and `dtype` is an array\n        return npdtype\n    elif npdtype.kind == 'O':\n        raise TypeError(\"dtype '{}' not understood\".format(dtype))\n\n    return npdtype", "output": "Convert input into a pandas only dtype object or a numpy dtype object.\n\n    Parameters\n    ----------\n    dtype : object to be converted\n\n    Returns\n    -------\n    np.dtype or a pandas dtype\n\n    Raises\n    ------\n    TypeError if not a dtype", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_optimizer(self, kvstore='local', optimizer='sgd',\n                       optimizer_params=(('learning_rate', 0.01),),\n                       force_init=False):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized\n        if self.optimizer_initialized and not force_init:\n            self.logger.warning('optimizer already initialized, ignoring.')\n            return\n\n        for module in self._modules:\n            module.init_optimizer(kvstore=kvstore, optimizer=optimizer,\n                                  optimizer_params=optimizer_params, force_init=force_init)\n\n        self.optimizer_initialized = True", "output": "Installs and initializes optimizers.\n\n        Parameters\n        ----------\n        kvstore : str or KVStore\n            Default `'local'`.\n        optimizer : str or Optimizer\n            Default `'sgd'`\n        optimizer_params : dict\n            Default ``(('learning_rate', 0.01),)``. The default value is not a dictionary,\n            just to avoid pylint warning of dangerous default values.\n        force_init : bool\n            Default ``False``, indicating whether we should force re-initializing the\n            optimizer in the case an optimizer is already installed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def join_options(options):\n    \"\"\"\n    \"\"\"\n    rv = []\n    any_prefix_is_slash = False\n    for opt in options:\n        prefix = split_opt(opt)[0]\n        if prefix == '/':\n            any_prefix_is_slash = True\n        rv.append((len(prefix), opt))\n\n    rv.sort(key=lambda x: x[0])\n\n    rv = ', '.join(x[1] for x in rv)\n    return rv, any_prefix_is_slash", "output": "Given a list of option strings this joins them in the most appropriate\n    way and returns them in the form ``(formatted_string,\n    any_prefix_is_slash)`` where the second item in the tuple is a flag that\n    indicates if any of the option prefixes was a slash.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mimic_adam_with_adafactor(hparams):\n  \"\"\"\n  \"\"\"\n  assert \"adam\" in hparams.optimizer\n  hparams.optimizer = \"adafactor\"\n  hparams.optimizer_adafactor_beta1 = hparams.optimizer_adam_beta1\n  hparams.optimizer_adafactor_beta2 = hparams.optimizer_adam_beta2\n  hparams.optimizer_adafactor_multiply_by_parameter_scale = False\n  hparams.optimizer_adafactor_factored = False\n  hparams.optimizer_adafactor_clipping_threshold = None\n  hparams.optimizer_adafactor_decay_type = \"adam\"", "output": "Switch from Adam to Adafactor, approximating the behavior of Adam.\n\n  Some minor things may be different, like epsilon and beta1 correction.\n\n  Args:\n    hparams: model hyperparameters where \"adam\" in hparams.optimizer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_ipv6_filter(ip, options=None):\n    '''\n    \n    '''\n    _is_ipv6 = _is_ipv(ip, 6, options=options)\n    return isinstance(_is_ipv6, six.string_types)", "output": "Returns a bool telling if the value passed to it was a valid IPv6 address.\n\n    ip\n        The IP address.\n\n    net: False\n        Consider IP addresses followed by netmask.\n\n    options\n        CSV of options regarding the nature of the IP address. E.g.: loopback, multicast, private etc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_eval():\n    \"\"\" \n    \"\"\"\n\n    global trainloader\n    global testloader\n    global net\n\n    (x_train, y_train) = trainloader\n    (x_test, y_test) = testloader\n\n    # train procedure\n    net.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=args.batch_size,\n        validation_data=(x_test, y_test),\n        epochs=args.epochs,\n        shuffle=True,\n        callbacks=[\n            SendMetrics(),\n            EarlyStopping(min_delta=0.001, patience=10),\n            TensorBoard(log_dir=TENSORBOARD_DIR),\n        ],\n    )\n\n    # trial report final acc to tuner\n    _, acc = net.evaluate(x_test, y_test)\n    logger.debug(\"Final result is: %.3f\", acc)\n    nni.report_final_result(acc)", "output": "train and eval the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_series_or_dataframe_operations(cls):\n        \"\"\"\n        \n        \"\"\"\n\n        from pandas.core import window as rwindow\n\n        @Appender(rwindow.rolling.__doc__)\n        def rolling(self, window, min_periods=None, center=False,\n                    win_type=None, on=None, axis=0, closed=None):\n            axis = self._get_axis_number(axis)\n            return rwindow.rolling(self, window=window,\n                                   min_periods=min_periods,\n                                   center=center, win_type=win_type,\n                                   on=on, axis=axis, closed=closed)\n\n        cls.rolling = rolling\n\n        @Appender(rwindow.expanding.__doc__)\n        def expanding(self, min_periods=1, center=False, axis=0):\n            axis = self._get_axis_number(axis)\n            return rwindow.expanding(self, min_periods=min_periods,\n                                     center=center, axis=axis)\n\n        cls.expanding = expanding\n\n        @Appender(rwindow.ewm.__doc__)\n        def ewm(self, com=None, span=None, halflife=None, alpha=None,\n                min_periods=0, adjust=True, ignore_na=False,\n                axis=0):\n            axis = self._get_axis_number(axis)\n            return rwindow.ewm(self, com=com, span=span, halflife=halflife,\n                               alpha=alpha, min_periods=min_periods,\n                               adjust=adjust, ignore_na=ignore_na, axis=axis)\n\n        cls.ewm = ewm", "output": "Add the series or dataframe only operations to the cls; evaluate\n        the doc strings again.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def symlink_remove(link):\n    \"\"\"\n    \"\"\"\n    # https://stackoverflow.com/q/26554135/6400719\n    if os.path.isdir(path2str(link)) and is_windows:\n        # this should only be on Py2.7 and windows\n        os.rmdir(path2str(link))\n    else:\n        os.unlink(path2str(link))", "output": "Remove a symlink. Used for model shortcut links.\n\n    link (unicode / Path): The path to the symlink.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cmd(tgt,\n        fun,\n        arg=(),\n        timeout=None,\n        tgt_type='glob',\n        kwarg=None):\n    '''\n    \n    '''\n    client = salt.client.ssh.client.SSHClient(mopts=__opts__)\n    return client.cmd(\n            tgt,\n            fun,\n            arg,\n            timeout,\n            tgt_type,\n            kwarg)", "output": ".. versionadded:: 2015.5.0\n    .. versionchanged:: 2017.7.0\n        The ``expr_form`` argument has been renamed to ``tgt_type``, earlier\n        releases must use ``expr_form``.\n\n    Execute a single command via the salt-ssh subsystem and return all\n    routines at once\n\n    A wrapper around the :py:meth:`SSHClient.cmd\n    <salt.client.ssh.client.SSHClient.cmd>` method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_actions_from_items(self, items):\r\n        \"\"\"\"\"\"\r\n        fromcursor_act = create_action(self, text=_('Go to cursor position'),\r\n                                       icon=ima.icon('fromcursor'),\r\n                                       triggered=self.go_to_cursor_position)\r\n        fullpath_act = create_action(self, text=_('Show absolute path'),\r\n                                     toggled=self.toggle_fullpath_mode)\r\n        fullpath_act.setChecked(self.show_fullpath)\r\n        allfiles_act = create_action(self, text=_('Show all files'),\r\n                                     toggled=self.toggle_show_all_files)\r\n        allfiles_act.setChecked(self.show_all_files)\r\n        comment_act = create_action(self, text=_('Show special comments'),\r\n                                    toggled=self.toggle_show_comments)\r\n        comment_act.setChecked(self.show_comments)\r\n        group_cells_act = create_action(self, text=_('Group code cells'),\r\n                                        toggled=self.toggle_group_cells)\r\n        group_cells_act.setChecked(self.group_cells)\r\n        sort_files_alphabetically_act = create_action(\r\n            self, text=_('Sort files alphabetically'),\r\n            toggled=self.toggle_sort_files_alphabetically)\r\n        sort_files_alphabetically_act.setChecked(\r\n            self.sort_files_alphabetically)\r\n        actions = [fullpath_act, allfiles_act, group_cells_act, comment_act,\r\n                   sort_files_alphabetically_act, fromcursor_act]\r\n        return actions", "output": "Reimplemented OneColumnTree method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def index_fields(self, vocab: Vocabulary) -> None:\n        \"\"\"\n        \n        \"\"\"\n        if not self.indexed:\n            self.indexed = True\n            for field in self.fields.values():\n                field.index(vocab)", "output": "Indexes all fields in this ``Instance`` using the provided ``Vocabulary``.\n        This `mutates` the current object, it does not return a new ``Instance``.\n        A ``DataIterator`` will call this on each pass through a dataset; we use the ``indexed``\n        flag to make sure that indexing only happens once.\n\n        This means that if for some reason you modify your vocabulary after you've\n        indexed your instances, you might get unexpected behavior.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def volume_detach(name, profile=None, timeout=300, **kwargs):\n    '''\n    \n\n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.volume_detach(\n        name,\n        timeout\n    )", "output": "Attach a block storage volume\n\n    name\n        Name of the new volume to attach\n\n    server_name\n        Name of the server to detach from\n\n    profile\n        Profile to build on\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.volume_detach myblock profile=openstack", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_lib(self, version):\n        \"\"\"\n        \n        \"\"\"\n        if os.path.exists(POETRY_LIB_BACKUP):\n            shutil.rmtree(POETRY_LIB_BACKUP)\n\n        # Backup the current installation\n        if os.path.exists(POETRY_LIB):\n            shutil.copytree(POETRY_LIB, POETRY_LIB_BACKUP)\n            shutil.rmtree(POETRY_LIB)\n\n        try:\n            self._make_lib(version)\n        except Exception:\n            if not os.path.exists(POETRY_LIB_BACKUP):\n                raise\n\n            shutil.copytree(POETRY_LIB_BACKUP, POETRY_LIB)\n            shutil.rmtree(POETRY_LIB_BACKUP)\n\n            raise\n        finally:\n            if os.path.exists(POETRY_LIB_BACKUP):\n                shutil.rmtree(POETRY_LIB_BACKUP)", "output": "Packs everything into a single lib/ directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _StructPackEncoder(wire_type, format):\n  \"\"\"\n  \"\"\"\n\n  value_size = struct.calcsize(format)\n\n  def SpecificEncoder(field_number, is_repeated, is_packed):\n    local_struct_pack = struct.pack\n    if is_packed:\n      tag_bytes = TagBytes(field_number, wire_format.WIRETYPE_LENGTH_DELIMITED)\n      local_EncodeVarint = _EncodeVarint\n      def EncodePackedField(write, value):\n        write(tag_bytes)\n        local_EncodeVarint(write, len(value) * value_size)\n        for element in value:\n          write(local_struct_pack(format, element))\n      return EncodePackedField\n    elif is_repeated:\n      tag_bytes = TagBytes(field_number, wire_type)\n      def EncodeRepeatedField(write, value):\n        for element in value:\n          write(tag_bytes)\n          write(local_struct_pack(format, element))\n      return EncodeRepeatedField\n    else:\n      tag_bytes = TagBytes(field_number, wire_type)\n      def EncodeField(write, value):\n        write(tag_bytes)\n        return write(local_struct_pack(format, value))\n      return EncodeField\n\n  return SpecificEncoder", "output": "Return a constructor for an encoder for a fixed-width field.\n\n  Args:\n      wire_type:  The field's wire type, for encoding tags.\n      format:  The format string to pass to struct.pack().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reference_image_path(cls, project, location, product, reference_image):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/products/{product}/referenceImages/{reference_image}\",\n            project=project,\n            location=location,\n            product=product,\n            reference_image=reference_image,\n        )", "output": "Return a fully-qualified reference_image string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_simple_binding(jboss_config, binding_name, profile=None):\n    '''\n    \n       '''\n    log.debug(\"======================== MODULE FUNCTION: jboss7.read_simple_binding, %s\", binding_name)\n    return __read_simple_binding(jboss_config, binding_name, profile=profile)", "output": "Read jndi binding in the running jboss instance\n\n    jboss_config\n        Configuration dictionary with properties specified above.\n    binding_name\n        Binding name to be created\n    profile\n        The profile name (JBoss domain mode only)\n\n    CLI Example:\n\n        .. code-block:: bash\n\n        salt '*' jboss7.read_simple_binding '{\"cli_path\": \"integration.modules.sysmod.SysModuleTest.test_valid_docs\", \"controller\": \"10.11.12.13:9999\", \"cli_user\": \"jbossadm\", \"cli_password\": \"jbossadm\"}' my_binding_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lookup_default(self, name):\n        \"\"\"\n        \"\"\"\n        if self.default_map is not None:\n            rv = self.default_map.get(name)\n            if callable(rv):\n                rv = rv()\n            return rv", "output": "Looks up the default for a parameter name.  This by default\n        looks into the :attr:`default_map` if available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(self):\n        \"\"\"\n        \"\"\"\n        client = self._instance._client\n        client.instance_admin_client.delete_cluster(self.name)", "output": "Delete this cluster.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_delete_cluster]\n            :end-before: [END bigtable_delete_cluster]\n\n        Marks a cluster and all of its tables for permanent deletion in 7 days.\n\n        Immediately upon completion of the request:\n\n        * Billing will cease for all of the cluster's reserved resources.\n        * The cluster's ``delete_time`` field will be set 7 days in the future.\n\n        Soon afterward:\n\n        * All tables within the cluster will become unavailable.\n\n        At the cluster's ``delete_time``:\n\n        * The cluster and **all of its tables** will immediately and\n          irrevocably disappear from the API, and their data will be\n          permanently deleted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_begin(self, train, **kwargs):\n        \"\"\n        if not self.learn.gan_trainer.gen_mode and train: self.learn.opt.lr *= self.mult_lr", "output": "Multiply the current lr if necessary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def undeploy(self, no_confirm=False, remove_logs=False):\n        \"\"\"\n        \n        \"\"\"\n\n        if not no_confirm: # pragma: no cover\n            confirm = input(\"Are you sure you want to undeploy? [y/n] \")\n            if confirm != 'y':\n                return\n\n        if self.use_alb:\n            self.zappa.undeploy_lambda_alb(self.lambda_name)\n\n        if self.use_apigateway:\n            if remove_logs:\n                self.zappa.remove_api_gateway_logs(self.lambda_name)\n\n            domain_name = self.stage_config.get('domain', None)\n            base_path = self.stage_config.get('base_path', None)\n\n            # Only remove the api key when not specified\n            if self.api_key_required and self.api_key is None:\n                api_id = self.zappa.get_api_id(self.lambda_name)\n                self.zappa.remove_api_key(api_id, self.api_stage)\n\n            gateway_id = self.zappa.undeploy_api_gateway(\n                self.lambda_name,\n                domain_name=domain_name,\n                base_path=base_path\n            )\n\n        self.unschedule()  # removes event triggers, including warm up event.\n\n        self.zappa.delete_lambda_function(self.lambda_name)\n        if remove_logs:\n            self.zappa.remove_lambda_function_logs(self.lambda_name)\n\n        click.echo(click.style(\"Done\", fg=\"green\", bold=True) + \"!\")", "output": "Tear down an existing deployment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_eval_and_decode(self):\n    \"\"\"\"\"\"\n    eval_steps = self._hparams.eval_freq_in_steps\n    packed_dataset = \"_packed\" in self._hparams.problem.name\n    mlperf_log.transformer_print(key=mlperf_log.TRAIN_LOOP)\n    for i in range(0, self._train_spec.max_steps, eval_steps):\n      mlperf_log.transformer_print(\n          key=mlperf_log.TRAIN_EPOCH, value=i // eval_steps)\n      if packed_dataset and i > 0:\n        problem = registry.problem(self._hparams.problem.name + \"_packed\")\n        p_hparams = problem.get_hparams(self._hparams)\n        self._hparams.problem = problem\n        self._hparams.problem_hparams = p_hparams\n      self._estimator.train(\n          self._train_spec.input_fn,\n          steps=eval_steps,\n          hooks=self._train_spec.hooks)\n      self._set_eval_dir_name(\"eval\")\n      self._estimator.evaluate(\n          self._eval_spec.input_fn,\n          steps=self._eval_spec.steps,\n          hooks=self._eval_spec.hooks,\n          name=\"eval\")\n      if packed_dataset:\n        problem = registry.problem(\n            self._hparams.problem.name.replace(\"_packed\", \"\"))\n        p_hparams = problem.get_hparams(self._hparams)\n        self._hparams.problem = problem\n        self._hparams.problem_hparams = p_hparams\n      mlperf_log.transformer_print(key=mlperf_log.EVAL_START)\n      if self._hparams.mlperf_mode:\n        self._decode_hparams.mlperf_decode_step = i + eval_steps\n      self.decode(dataset_split=tf.estimator.ModeKeys.EVAL)\n      d_hparams = self._decode_hparams\n      if self._hparams.mlperf_mode and d_hparams.mlperf_success:\n        mlperf_log.transformer_print(\n            key=mlperf_log.RUN_STOP, value={\"success\": \"true\"})\n        break\n\n    d_hparams = self._decode_hparams\n    if self._hparams.mlperf_mode and not d_hparams.mlperf_success:\n      mlperf_log.transformer_print(\n          key=mlperf_log.RUN_STOP, value={\"success\": \"false\"})", "output": "Does eval and decode after training every eval_freq_in_steps.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def orc(self, path):\n        \"\"\"\n        \"\"\"\n        if isinstance(path, basestring):\n            path = [path]\n        return self._df(self._jreader.orc(_to_seq(self._spark._sc, path)))", "output": "Loads ORC files, returning the result as a :class:`DataFrame`.\n\n        >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n        >>> df.dtypes\n        [('a', 'bigint'), ('b', 'int'), ('c', 'int')]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _player_step_tuple(self, envs_step_tuples):\n    \"\"\"\"\"\"\n    ob, reward, done, info = envs_step_tuples[\"env\"]\n    ob = self._augment_observation(ob, reward, self.cumulative_reward)\n    return ob, reward, done, info", "output": "Augment observation, return usual step tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _escalation_rules_to_string(escalation_rules):\n    ''\n    result = ''\n    for rule in escalation_rules:\n        result += 'escalation_delay_in_minutes: {0} '.format(rule['escalation_delay_in_minutes'])\n        for target in rule['targets']:\n            result += '{0}:{1} '.format(target['type'], target['id'])\n    return result", "output": "convert escalation_rules dict to a string for comparison", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_properties(self, response):\n        \"\"\"\n        \"\"\"\n        self._properties.clear()\n        self._properties.update(response)", "output": "Helper for :meth:`reload`.\n\n        :type response: dict\n        :param response: resource mapping from server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def winrm_cmd(session, command, flags, **kwargs):\n    '''\n    \n    '''\n    log.debug('Executing WinRM command: %s %s', command, flags)\n    # rebuild the session to ensure we haven't timed out\n    session.protocol.transport.build_session()\n    r = session.run_cmd(command, flags)\n    return r.status_code", "output": "Wrapper for commands to be run against Windows boxes using WinRM.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_locations(self, provider=None):\n        '''\n        \n        '''\n        mapper = salt.cloud.Map(self._opts_defaults())\n        return salt.utils.data.simple_types_filter(\n            mapper.location_list(provider)\n        )", "output": "List all available locations in configured cloud systems", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_dict(self):\n        \"\"\"\n        \"\"\"\n        if self.all_:\n            return {\"all\": True}\n\n        return {\n            \"keys\": self.keys,\n            \"ranges\": [keyrange._to_dict() for keyrange in self.ranges],\n        }", "output": "Return keyset's state as a dict.\n\n        The result can be used to serialize the instance and reconstitute\n        it later using :meth:`_from_dict`.\n\n        :rtype: dict\n        :returns: state of this instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keypair_add(name, pubfile=None, pubkey=None, profile=None, **kwargs):\n    '''\n    \n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.keypair_add(\n        name,\n        pubfile,\n        pubkey\n    )", "output": "Add a keypair to nova (nova keypair-add)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' nova.keypair_add mykey pubfile='/home/myuser/.ssh/id_rsa.pub'\n        salt '*' nova.keypair_add mykey pubkey='ssh-rsa <key> myuser@mybox'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_list(self, name: str) -> List[str]:\n        \"\"\"\"\"\"\n        norm_name = _normalized_headers[name]\n        return self._as_list.get(norm_name, [])", "output": "Returns all values for the given header as a list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_market_open(self, session_label, data_portal):\n        \"\"\"\n        \"\"\"\n        ledger = self._ledger\n        ledger.start_of_session(session_label)\n\n        adjustment_reader = data_portal.adjustment_reader\n        if adjustment_reader is not None:\n            # this is None when running with a dataframe source\n            ledger.process_dividends(\n                session_label,\n                self._asset_finder,\n                adjustment_reader,\n            )\n\n        self._current_session = session_label\n\n        cal = self._trading_calendar\n        self._market_open, self._market_close = self._execution_open_and_close(\n            cal,\n            session_label,\n        )\n\n        self.start_of_session(ledger, session_label, data_portal)", "output": "Handles the start of each session.\n\n        Parameters\n        ----------\n        session_label : Timestamp\n            The label of the session that is about to begin.\n        data_portal : DataPortal\n            The current data portal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reader(self, name, stream, outbuf):\n        \"\"\"\n        \n        \"\"\"\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            s = s.decode('utf-8').rstrip()\n            outbuf.append(s)\n            logger.debug('%s: %s' % (name, s))\n        stream.close()", "output": "Thread runner for reading lines of from a subprocess into a buffer.\n\n        :param name: The logical name of the stream (used for logging only).\n        :param stream: The stream to read from. This will typically a pipe\n                       connected to the output stream of a subprocess.\n        :param outbuf: The list to append the read lines to.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_numeric_feature_names(example):\n  \"\"\"\n  \"\"\"\n  numeric_features = ('float_list', 'int64_list')\n  features = get_example_features(example)\n  return sorted([\n      feature_name for feature_name in features\n      if features[feature_name].WhichOneof('kind') in numeric_features\n  ])", "output": "Returns a list of feature names for float and int64 type features.\n\n  Args:\n    example: An example.\n\n  Returns:\n    A list of strings of the names of numeric features.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def profile_device_get(name, device_name, remote_addr=None,\n                       cert=None, key=None, verify_cert=True):\n    ''' \n    '''\n    profile = profile_get(\n        name,\n        remote_addr,\n        cert,\n        key,\n        verify_cert,\n        _raw=True\n    )\n\n    return _get_property_dict_item(profile, 'devices', device_name)", "output": "Get a profile device.\n\n        name :\n            The name of the profile to get the device from.\n\n        device_name :\n            The name of the device to retrieve.\n\n        remote_addr :\n            An URL to a remote Server, you also have to give cert and key if\n            you provide remote_addr and its a TCP Address!\n\n            Examples:\n                https://myserver.lan:8443\n                /var/lib/mysocket.sock\n\n        cert :\n            PEM Formatted SSL Certificate.\n\n            Examples:\n                ~/.config/lxc/client.crt\n\n        key :\n            PEM Formatted SSL Key.\n\n            Examples:\n                ~/.config/lxc/client.key\n\n        verify_cert : True\n            Wherever to verify the cert, this is by default True\n            but in the most cases you want to set it off as LXD\n            normaly uses self-signed certificates.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            $ salt '*' lxd.profile_device_get default eth0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_colors(use=True, theme=None):\n    '''\n    \n\n    '''\n\n    colors = {\n        'BLACK': TextFormat('black'),\n        'DARK_GRAY': TextFormat('bold', 'black'),\n        'RED': TextFormat('red'),\n        'LIGHT_RED': TextFormat('bold', 'red'),\n        'GREEN': TextFormat('green'),\n        'LIGHT_GREEN': TextFormat('bold', 'green'),\n        'YELLOW': TextFormat('yellow'),\n        'LIGHT_YELLOW': TextFormat('bold', 'yellow'),\n        'BLUE': TextFormat('blue'),\n        'LIGHT_BLUE': TextFormat('bold', 'blue'),\n        'MAGENTA': TextFormat('magenta'),\n        'LIGHT_MAGENTA': TextFormat('bold', 'magenta'),\n        'CYAN': TextFormat('cyan'),\n        'LIGHT_CYAN': TextFormat('bold', 'cyan'),\n        'LIGHT_GRAY': TextFormat('white'),\n        'WHITE': TextFormat('bold', 'white'),\n        'DEFAULT_COLOR': TextFormat('default'),\n        'ENDC': TextFormat('reset'),\n    }\n    if theme:\n        colors.update(get_color_theme(theme))\n\n    if not use:\n        for color in colors:\n            colors[color] = ''\n    if isinstance(use, six.string_types):\n        # Try to set all of the colors to the passed color\n        if use in colors:\n            for color in colors:\n                # except for color reset\n                if color == 'ENDC':\n                    continue\n                colors[color] = colors[use]\n\n    return colors", "output": "Return the colors as an easy to use dict. Pass `False` to deactivate all\n    colors by setting them to empty strings. Pass a string containing only the\n    name of a single color to be used in place of all colors. Examples:\n\n    .. code-block:: python\n\n        colors = get_colors()  # enable all colors\n        no_colors = get_colors(False)  # disable all colors\n        red_colors = get_colors('RED')  # set all colors to red", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sink(self, name, filter_=None, destination=None):\n        \"\"\"\n        \"\"\"\n        return Sink(name, filter_, destination, client=self)", "output": "Creates a sink bound to the current client.\n\n        :type name: str\n        :param name: the name of the sink to be constructed.\n\n        :type filter_: str\n        :param filter_: (optional) the advanced logs filter expression\n                        defining the entries exported by the sink.  If not\n                        passed, the instance should already exist, to be\n                        refreshed via :meth:`Sink.reload`.\n\n        :type destination: str\n        :param destination: destination URI for the entries exported by\n                            the sink.  If not passed, the instance should\n                            already exist, to be refreshed via\n                            :meth:`Sink.reload`.\n\n        :rtype: :class:`google.cloud.logging.sink.Sink`\n        :returns: Sink created with the current client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def super(self):\n        \"\"\"\"\"\"\n        if self._depth + 1 >= len(self._stack):\n            return self._context.environment. \\\n                undefined('there is no parent block called %r.' %\n                          self.name, name='super')\n        return BlockReference(self.name, self._context, self._stack,\n                              self._depth + 1)", "output": "Super the block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _size_fmt(num):\n    '''\n    \n    '''\n    try:\n        num = int(num)\n        if num < 1024:\n            return '{0} bytes'.format(num)\n        num /= 1024.0\n        for unit in ('KiB', 'MiB', 'GiB', 'TiB', 'PiB'):\n            if num < 1024.0:\n                return '{0:3.1f} {1}'.format(num, unit)\n            num /= 1024.0\n    except Exception:\n        log.error('Unable to format file size for \\'%s\\'', num)\n        return 'unknown'", "output": "Format bytes as human-readable file sizes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_job_cache(hours=24):\n    '''\n    \n    '''\n    threshold = time.time() - hours * 60 * 60\n    for root, dirs, files in salt.utils.files.safe_walk(os.path.join(__opts__['cachedir'], 'minion_jobs'),\n                                                  followlinks=False):\n        for name in dirs:\n            try:\n                directory = os.path.join(root, name)\n                mtime = os.path.getmtime(directory)\n                if mtime < threshold:\n                    shutil.rmtree(directory)\n            except OSError as exc:\n                log.error('Attempt to clear cache with saltutil.clear_job_cache FAILED with: %s', exc)\n                return False\n    return True", "output": "Forcibly removes job cache folders and files on a minion.\n\n    .. versionadded:: 2018.3.0\n\n    WARNING: The safest way to clear a minion cache is by first stopping\n    the minion and then deleting the cache files before restarting it.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.clear_job_cache hours=12", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def can_user_run(self, user, command, groups):\n        '''\n        \n\n        '''\n        log.info('%s wants to run %s with groups %s', user, command, groups)\n        for key, val in groups.items():\n            if user not in val['users']:\n                if '*' not in val['users']:\n                    continue  # this doesn't grant permissions, pass\n            if (command not in val['commands']) and (command not in val.get('aliases', {}).keys()):\n                if '*' not in val['commands']:\n                    continue  # again, pass\n            log.info('Slack user %s permitted to run %s', user, command)\n            return (key, val,)  # matched this group, return the group\n        log.info('Slack user %s denied trying to run %s', user, command)\n        return ()", "output": "Break out the permissions into the following:\n\n        Check whether a user is in any group, including whether a group has the '*' membership\n\n        :type user: str\n        :param user: The username being checked against\n\n        :type command: str\n        :param command: The command that is being invoked (e.g. test.ping)\n\n        :type groups: dict\n        :param groups: the dictionary with groups permissions structure.\n\n        :rtype: tuple\n        :returns: On a successful permitting match, returns 2-element tuple that contains\n            the name of the group that successfully matched, and a dictionary containing\n            the configuration of the group so it can be referenced.\n\n            On failure it returns an empty tuple", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_mirrors(config_path=_DEFAULT_CONFIG_PATH):\n    '''\n    \n    '''\n    _validate_config(config_path)\n\n    cmd = ['mirror', 'list', '-config={}'.format(config_path), '-raw=true']\n\n    cmd_ret = _cmd_run(cmd)\n    ret = [line.strip() for line in cmd_ret.splitlines()]\n\n    log.debug('Found mirrors: %s', len(ret))\n    return ret", "output": "Get a list of all the mirrored remote repositories.\n\n    :param str config_path: The path to the configuration file for the aptly instance.\n\n    :return: A list of the mirror names.\n    :rtype: list\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' aptly.list_mirrors", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def infer(self, setup: QuaRelType, answer_0: QuaRelType, answer_1: QuaRelType) -> int:\n        \"\"\"\n        \n        \"\"\"\n        if self._check_quarels_compatible(setup, answer_0):\n            if self._check_quarels_compatible(setup, answer_1):\n                # Found two answers\n                return -2\n            else:\n                return 0\n        elif self._check_quarels_compatible(setup, answer_1):\n            return 1\n        else:\n            return -1", "output": "Take the question and check if it is compatible with either of the answer choices.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decompress_decoder_1d(x, hparams, name=None):\n  \"\"\"\n  \"\"\"\n  x = tf.expand_dims(x, axis=2)\n  output = decompress_decoder(x, hparams,\n                              strides=(2, 1),\n                              kernel=(hparams.kernel_size, 1),\n                              name=name)\n  return tf.squeeze(output, axis=2)", "output": "Decoder that decompresses 1-D inputs by 2**num_compress_steps.\n\n  Args:\n    x: Tensor of shape [batch, compress_length, channels].\n    hparams: HParams.\n    name: string, variable scope.\n\n  Returns:\n    Tensor of shape [batch, length, hparams.hidden_size].", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_weight(name, backend, socket=DEFAULT_SOCKET_URL):\n    '''\n    \n    '''\n    ha_conn = _get_conn(socket)\n    ha_cmd = haproxy.cmds.getWeight(server=name, backend=backend)\n    return ha_conn.sendCmd(ha_cmd)", "output": "Get server weight\n\n    name\n        Server name\n\n    backend\n        haproxy backend\n\n    socket\n        haproxy stats socket, default ``/var/run/haproxy.sock``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' haproxy.get_weight web1.example.com www", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_network_settings():\n    '''\n    \n    '''\n    if __grains__['lsb_distrib_id'] == 'nilrt':\n        raise salt.exceptions.CommandExecutionError('Not supported in this version.')\n    settings = []\n    networking = 'no' if _get_state() == 'offline' else 'yes'\n    settings.append('networking={0}'.format(networking))\n    hostname = __salt__['network.get_hostname']\n    settings.append('hostname={0}'.format(hostname))\n    return settings", "output": "Return the contents of the global network script.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.get_network_settings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _passwd_opts(self):\n        '''\n        \n        '''\n        # TODO ControlMaster does not work without ControlPath\n        # user could take advantage of it if they set ControlPath in their\n        # ssh config.  Also, ControlPersist not widely available.\n        options = ['ControlMaster=auto',\n                   'StrictHostKeyChecking=no',\n                   ]\n        if self.opts['_ssh_version'] > (4, 9):\n            options.append('GSSAPIAuthentication=no')\n        options.append('ConnectTimeout={0}'.format(self.timeout))\n        if self.opts.get('ignore_host_keys'):\n            options.append('StrictHostKeyChecking=no')\n        if self.opts.get('no_host_keys'):\n            options.extend(['StrictHostKeyChecking=no',\n                            'UserKnownHostsFile=/dev/null'])\n\n        if self.passwd:\n            options.extend(['PasswordAuthentication=yes',\n                            'PubkeyAuthentication=yes'])\n        else:\n            options.extend(['PasswordAuthentication=no',\n                            'PubkeyAuthentication=yes',\n                            'KbdInteractiveAuthentication=no',\n                            'ChallengeResponseAuthentication=no',\n                            'BatchMode=yes'])\n        if self.port:\n            options.append('Port={0}'.format(self.port))\n        if self.user:\n            options.append('User={0}'.format(self.user))\n        if self.identities_only:\n            options.append('IdentitiesOnly=yes')\n\n        ret = []\n        for option in options:\n            ret.append('-o {0} '.format(option))\n        return ''.join(ret)", "output": "Return options to pass to ssh", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render_as_text(self, limit=None):\n        \"\"\"\"\"\"\n        lines = traceback.format_exception(self.exc_type, self.exc_value,\n                                           self.frames[0], limit=limit)\n        return ''.join(lines).rstrip()", "output": "Return a string with the traceback.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_backward_begin(self, smooth_loss:Tensor, **kwargs:Any)->None:\n        \"\"\n        self.losses.append(smooth_loss)\n        if self.pbar is not None and hasattr(self.pbar,'child'):\n            self.pbar.child.comment = f'{smooth_loss:.4f}'", "output": "Record the loss before any other callback has a chance to modify it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_checkout_target(self):\n        '''\n        \n        '''\n        if self.role == 'git_pillar' and self.branch == '__env__':\n            try:\n                return self.all_saltenvs\n            except AttributeError:\n                # all_saltenvs not configured for this remote\n                pass\n            target = self.opts.get('pillarenv') \\\n                or self.opts.get('saltenv') \\\n                or 'base'\n            return self.base \\\n                if target == 'base' \\\n                else six.text_type(target)\n        return self.branch", "output": "Resolve dynamically-set branch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\n        \"\"\"\n        _logger.info('Start dispatcher')\n        if dispatcher_env_vars.NNI_MODE == 'resume':\n            self.load_checkpoint()\n\n        while True:\n            command, data = receive()\n            if data:\n                data = json_tricks.loads(data)\n\n            if command is None or command is CommandType.Terminate:\n                break\n            if multi_thread_enabled():\n                result = self.pool.map_async(self.process_command_thread, [(command, data)])\n                self.thread_results.append(result)\n                if any([thread_result.ready() and not thread_result.successful() for thread_result in self.thread_results]):\n                    _logger.debug('Caught thread exception')\n                    break\n            else:\n                self.enqueue_command(command, data)\n                if self.worker_exceptions:\n                    break\n\n        _logger.info('Dispatcher exiting...')\n        self.stopping = True\n        if multi_thread_enabled():\n            self.pool.close()\n            self.pool.join()\n        else:\n            self.default_worker.join()\n            self.assessor_worker.join()\n\n        _logger.info('Terminated by NNI manager')", "output": "Run the tuner.\n        This function will never return unless raise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate_regressor(model, data, target=\"target\", verbose=False):\n    \"\"\"\n    \n    \"\"\"\n    model = _get_model(model)\n\n    if verbose:\n        print(\"\")\n        print(\"Other Framework\\t\\tPredicted\\t\\tDelta\")\n\n    max_error = 0\n    error_squared = 0\n\n    for index,row in data.iterrows():\n        predicted = model.predict(dict(row))[_to_unicode(target)]\n        other_framework = row[\"prediction\"]\n        delta = predicted - other_framework\n\n        if verbose:\n            print(\"%s\\t\\t\\t\\t%s\\t\\t\\t%0.4f\" % (other_framework, predicted, delta))\n\n        max_error = max(abs(delta), max_error)\n        error_squared = error_squared + (delta * delta)\n\n    ret = {\n        \"samples\": len(data),\n        \"rmse\": _math.sqrt(error_squared / len(data)),\n        \"max_error\": max_error\n    }\n\n    if verbose:\n        print(\"results: %s\" % ret)\n    return ret", "output": "Evaluate a CoreML regression model and compare against predictions\n    from the original framework (for testing correctness of conversion)\n\n    Parameters\n    ----------\n    filename: [str | MLModel]\n        File path from which to load the MLModel from (OR) a loaded version of\n        MLModel.\n\n    data: [str | Dataframe]\n        Test data on which to evaluate the models (dataframe,\n        or path to a .csv file).\n\n    target: str\n       Name of the column in the dataframe that must be interpreted\n       as the target column.\n\n    verbose: bool\n       Set to true for a more verbose output.\n\n    See Also\n    --------\n    evaluate_classifier\n\n    Examples\n    --------\n    .. sourcecode:: python\n\n        >>> metrics = coremltools.utils.evaluate_regressor(spec, 'data_and_predictions.csv', 'target')\n        >>> print(metrics)\n        {\"samples\": 10, \"rmse\": 0.0, max_error: 0.0}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_request(self, request):\n        \"\"\"\n        \"\"\"\n        cookies = request.cookies or {}\n\n        # Bootstrap CookieJar.\n        if not isinstance(cookies, cookielib.CookieJar):\n            cookies = cookiejar_from_dict(cookies)\n\n        # Merge with session cookies\n        merged_cookies = merge_cookies(\n            merge_cookies(RequestsCookieJar(), self.cookies), cookies)\n\n        # Set environment's basic authentication if not explicitly set.\n        auth = request.auth\n        if self.trust_env and not auth and not self.auth:\n            auth = get_netrc_auth(request.url)\n\n        p = PreparedRequest()\n        p.prepare(\n            method=request.method.upper(),\n            url=request.url,\n            files=request.files,\n            data=request.data,\n            json=request.json,\n            headers=merge_setting(request.headers, self.headers, dict_class=CaseInsensitiveDict),\n            params=merge_setting(request.params, self.params),\n            auth=merge_setting(auth, self.auth),\n            cookies=merged_cookies,\n            hooks=merge_hooks(request.hooks, self.hooks),\n        )\n        return p", "output": "Constructs a :class:`PreparedRequest <PreparedRequest>` for\n        transmission and returns it. The :class:`PreparedRequest` has settings\n        merged from the :class:`Request <Request>` instance and those of the\n        :class:`Session`.\n\n        :param request: :class:`Request` instance to prepare with this\n            session's settings.\n        :rtype: requests.PreparedRequest", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_module(prefix, epoch, data_names, data_shapes):\n    \"\"\"\n    \"\"\"\n    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n\n    # We don't need CTC loss for prediction, just a simple softmax will suffice.\n    # We get the output of the layer just before the loss layer ('pred_fc') and add softmax on top\n    pred_fc = sym.get_internals()['pred_fc_output']\n    sym = mx.sym.softmax(data=pred_fc)\n\n    mod = mx.mod.Module(symbol=sym, context=mx.cpu(), data_names=data_names, label_names=None)\n    mod.bind(for_training=False, data_shapes=data_shapes)\n    mod.set_params(arg_params, aux_params, allow_missing=False)\n    return mod", "output": "Loads the model from checkpoint specified by prefix and epoch, binds it\n    to an executor, and sets its parameters and returns a mx.mod.Module", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write(self, to_write):\n        \"\"\"\n        \n        \"\"\"\n        self._file.write(to_write.encode(self._encoding or\n                                         self._default_encoding))", "output": "Helper to call encode before writing to file for Python 3 compat.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def meanApprox(self, timeout, confidence=0.95):\n        \"\"\"\n        \n        \"\"\"\n        jrdd = self.map(float)._to_java_object_rdd()\n        jdrdd = self.ctx._jvm.JavaDoubleRDD.fromRDD(jrdd.rdd())\n        r = jdrdd.meanApprox(timeout, confidence).getFinalValue()\n        return BoundedFloat(r.mean(), r.confidence(), r.low(), r.high())", "output": ".. note:: Experimental\n\n        Approximate operation to return the mean within a timeout\n        or meet the confidence.\n\n        >>> rdd = sc.parallelize(range(1000), 10)\n        >>> r = sum(range(1000)) / 1000.0\n        >>> abs(rdd.meanApprox(1000) - r) / r < 0.05\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_is_chained_assignment_possible(self):\n        \"\"\"\n        \n        \"\"\"\n        if self._is_view and self._is_cached:\n            ref = self._get_cacher()\n            if ref is not None and ref._is_mixed_type:\n                self._check_setitem_copy(stacklevel=4, t='referant',\n                                         force=True)\n            return True\n        elif self._is_copy:\n            self._check_setitem_copy(stacklevel=4, t='referant')\n        return False", "output": "Check if we are a view, have a cacher, and are of mixed type.\n        If so, then force a setitem_copy check.\n\n        Should be called just near setting a value\n\n        Will return a boolean if it we are a view and are cached, but a\n        single-dtype meaning that the cacher should be updated following\n        setting.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_firewall_rule(self, firewall_rule, protocol=None, action=None,\n                             name=None, description=None, ip_version=None,\n                             source_ip_address=None, destination_ip_address=None, source_port=None,\n                             destination_port=None, shared=None, enabled=None):\n        '''\n        \n        '''\n        body = {}\n        if protocol:\n            body['protocol'] = protocol\n        if action:\n            body['action'] = action\n        if name:\n            body['name'] = name\n        if description:\n            body['description'] = description\n        if ip_version:\n            body['ip_version'] = ip_version\n        if source_ip_address:\n            body['source_ip_address'] = source_ip_address\n        if destination_ip_address:\n            body['destination_ip_address'] = destination_ip_address\n        if source_port:\n            body['source_port'] = source_port\n        if destination_port:\n            body['destination_port'] = destination_port\n        if shared:\n            body['shared'] = shared\n        if enabled:\n            body['enabled'] = enabled\n        return self.network_conn.update_firewall_rule(firewall_rule, body={'firewall_rule': body})", "output": "Update a firewall rule", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_new_line(self):\r\n        \"\"\"\"\"\"\r\n        self.set_cursor_position('eof')\r\n        self.current_prompt_pos = self.get_position('cursor')\r\n        self.new_input_line = False", "output": "On new input line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ping(dest_ip=None, **kwargs):\n    '''\n    \n    '''\n    conn = __proxy__['junos.conn']()\n    ret = {}\n\n    if dest_ip is None:\n        ret['message'] = 'Please specify the destination ip to ping.'\n        ret['out'] = False\n        return ret\n\n    op = {'host': dest_ip}\n    if '__pub_arg' in kwargs:\n        if kwargs['__pub_arg']:\n            if isinstance(kwargs['__pub_arg'][-1], dict):\n                op.update(kwargs['__pub_arg'][-1])\n    else:\n        op.update(kwargs)\n\n    op['count'] = six.text_type(op.pop('count', 5))\n    if 'ttl' in op:\n        op['ttl'] = six.text_type(op['ttl'])\n\n    ret['out'] = True\n    try:\n        ret['message'] = jxmlease.parse(etree.tostring(conn.rpc.ping(**op)))\n    except Exception as exception:\n        ret['message'] = 'Execution failed due to \"{0}\"'.format(exception)\n        ret['out'] = False\n    return ret", "output": "Send a ping RPC to a device\n\n    dest_ip\n      The IP of the device to ping\n\n    dev_timeout : 30\n        The NETCONF RPC timeout (in seconds)\n\n    rapid : False\n        When ``True``, executes ping at 100pps instead of 1pps\n\n    ttl\n        Maximum number of IP routers (IP hops) allowed between source and\n        destination\n\n    routing_instance\n      Name of the routing instance to use to send the ping\n\n    interface\n      Interface used to send traffic\n\n    count : 5\n      Number of packets to send\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt 'device_name' junos.ping '8.8.8.8' count=5\n        salt 'device_name' junos.ping '8.8.8.8' ttl=1 rapid=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_raw(path, raw):\n    '''\n    \n    '''\n    log.debug('Writing vault secrets for %s at %s', __grains__['id'], path)\n    try:\n        url = 'v1/{0}'.format(path)\n        response = __utils__['vault.make_request']('POST', url, json=raw)\n        if response.status_code == 200:\n            return response.json()['data']\n        elif response.status_code != 204:\n            response.raise_for_status()\n        return True\n    except Exception as err:\n        log.error('Failed to write secret! %s: %s', type(err).__name__, err)\n        return False", "output": "Set raw data at the path in vault. The vault policy used must allow this.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n            salt '*' vault.write_raw \"secret/my/secret\" '{\"user\":\"foo\",\"password\": \"bar\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_dict(cls, data, intersect=False, orient='items', dtype=None):\n        \"\"\"\n        \n        \"\"\"\n        from collections import defaultdict\n\n        orient = orient.lower()\n        if orient == 'minor':\n            new_data = defaultdict(OrderedDict)\n            for col, df in data.items():\n                for item, s in df.items():\n                    new_data[item][col] = s\n            data = new_data\n        elif orient != 'items':  # pragma: no cover\n            raise ValueError('Orientation must be one of {items, minor}.')\n\n        d = cls._homogenize_dict(cls, data, intersect=intersect, dtype=dtype)\n        ks = list(d['data'].keys())\n        if not isinstance(d['data'], OrderedDict):\n            ks = list(sorted(ks))\n        d[cls._info_axis_name] = Index(ks)\n        return cls(**d)", "output": "Construct Panel from dict of DataFrame objects.\n\n        Parameters\n        ----------\n        data : dict\n            {field : DataFrame}\n        intersect : boolean\n            Intersect indexes of input DataFrames\n        orient : {'items', 'minor'}, default 'items'\n            The \"orientation\" of the data. If the keys of the passed dict\n            should be the items of the result panel, pass 'items'\n            (default). Otherwise if the columns of the values of the passed\n            DataFrame objects should be the items (which in the case of\n            mixed-dtype data you should do), instead pass 'minor'\n        dtype : dtype, default None\n            Data type to force, otherwise infer\n\n        Returns\n        -------\n        Panel", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _format_operation_dict(operation, parameters):\n    \"\"\"\n    \"\"\"\n    formatted_params = {}\n    for name in parameters:\n        escaped_name = name.replace(\"`\", r\"\\`\")\n        formatted_params[name] = \"@`{}`\".format(escaped_name)\n\n    try:\n        return operation % formatted_params\n    except KeyError as exc:\n        raise exceptions.ProgrammingError(exc)", "output": "Formats parameters in operation in the way BigQuery expects.\n\n    The input operation will be a query like ``SELECT %(namedparam)s`` and\n    the output will be a query like ``SELECT @namedparam``.\n\n    :type operation: str\n    :param operation: A Google BigQuery query string.\n\n    :type parameters: Mapping[str, Any]\n    :param parameters: Dictionary of parameter values.\n\n    :rtype: str\n    :returns: A formatted query string.\n    :raises: :class:`~google.cloud.bigquery.dbapi.ProgrammingError`\n        if a parameter used in the operation is not found in the\n        ``parameters`` argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(cls, fpath):\n        \"\"\n        fpath = _expand_path(fpath)\n        assert(fpath.suffix == '.yml')\n        if fpath.exists(): return\n        fpath.parent.mkdir(parents=True, exist_ok=True)\n        with open(fpath, 'w') as yaml_file:\n            yaml.dump(cls.DEFAULT_CONFIG, yaml_file, default_flow_style=False)", "output": "Creates a `Config` from `fpath`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_graph_terms(self, default_screen):\n        \"\"\"\"\"\"\n        columns = self.columns.copy()\n        screen = self.screen\n        if screen is None:\n            screen = default_screen\n        columns[SCREEN_NAME] = screen\n        return columns", "output": "Helper for to_graph and to_execution_plan.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validator(self, meth):\n        \"\"\"\n        \n        \"\"\"\n        if self._validator is None:\n            self._validator = meth\n        else:\n            self._validator = and_(self._validator, meth)\n        return meth", "output": "Decorator that adds *meth* to the list of validators.\n\n        Returns *meth* unchanged.\n\n        .. versionadded:: 17.1.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_data_stock_to_fq(__data, type_='01'):\n\n    def __QA_fetch_stock_xdxr(\n            code,\n            format_='pd',\n            collections=DATABASE.stock_xdxr\n    ):\n        '\u83b7\u53d6\u80a1\u7968\u9664\u6743\u4fe1\u606f/\u6570\u636e\u5e93'\n        try:\n            data = pd.DataFrame(\n                [item for item in collections.find({'code': code})]\n            ).drop(['_id'],\n                   axis=1)\n            data['date'] = pd.to_datetime(data['date'])\n            return data.set_index(['date', 'code'], drop=False)\n        except:\n            return pd.DataFrame(\n                data=[],\n                columns=[\n                    'category',\n                    'category_meaning',\n                    'code',\n                    'date',\n                    'fenhong',\n                    'fenshu',\n                    'liquidity_after',\n                    'liquidity_before',\n                    'name',\n                    'peigu',\n                    'peigujia',\n                    'shares_after',\n                    'shares_before',\n                    'songzhuangu',\n                    'suogu',\n                    'xingquanjia'\n                ]\n            )\n\n    ''\n\n    code = __data.index.remove_unused_levels().levels[1][0] if isinstance(\n        __data.index,\n        pd.core.indexes.multi.MultiIndex\n    ) else __data['code'][0]\n\n    return _QA_data_stock_to_fq(\n        bfq_data=__data,\n        xdxr_data=__QA_fetch_stock_xdxr(code),\n        fqtype=type_\n    )", "output": "\u80a1\u7968 \u65e5\u7ebf/\u5206\u949f\u7ebf \u52a8\u6001\u590d\u6743\u63a5\u53e3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def initialized(name, **kwargs):\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': 'The VM is already correctly defined'\n           }\n\n    # define a machine to start later\n    ret, kwargs = _find_init_change(name, ret, **kwargs)\n\n    if ret['changes'] == {}:\n        return ret\n\n    kwargs['start'] = False\n    __salt__['vagrant.init'](name, **kwargs)\n    ret['changes'][name] = 'Node initialized'\n    ret['comment'] = 'Node {0} defined but not started.'.format(name)\n\n    return ret", "output": "r'''\n    Defines a new VM with specified arguments, but does not start it.\n\n    :param name: the Salt_id node name you wish your VM to have.\n\n    Each machine must be initialized individually using this function\n    or the \"vagrant.running\" function, or the vagrant.init execution module call.\n\n    This command will not change the state of a running or paused machine.\n\n    Possible keyword arguments:\n\n    - cwd: The directory (path) containing the Vagrantfile\n    - machine: ('') the name of the machine (in the Vagrantfile) if not default\n    - vagrant_runas: ('root') the username who owns the vagrantbox file\n    - vagrant_provider: the provider to run the VM (usually 'virtualbox')\n    - vm: ({}) a dictionary containing these or other keyword arguments\n\n    .. code-block:: yaml\n\n        node_name1:\n          vagrant.initialized\n            - cwd: /projects/my_project\n            - vagrant_runas: my_username\n            - machine: machine1\n\n        node_name2:\n          vagrant.initialized\n            - cwd: /projects/my_project\n            - vagrant_runas: my_username\n            - machine: machine2\n\n        start_nodes:\n          vagrant.start:\n            - name: node_name?", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open(name, device, keyfile):\n    '''\n    \n    '''\n    if keyfile is None or keyfile == 'none' or keyfile == '-':\n        raise CommandExecutionError('For immediate crypt device mapping, keyfile must not be none')\n\n    code = __salt__['cmd.retcode']('cryptsetup open --key-file {0} {1} {2}'\n                                   .format(keyfile, device, name))\n    return code == 0", "output": "Open a crypt device using ``cryptsetup``. The ``keyfile`` must not be\n    ``None`` or ``'none'``, because ``cryptsetup`` will otherwise ask for the\n    password interactively.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cryptdev.open foo /dev/sdz1 /path/to/keyfile", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prefix_from_prefix_string(self, prefixlen_str):\n        \"\"\"\n        \"\"\"\n        # int allows a leading +/- as well as surrounding whitespace,\n        # so we ensure that isn't the case\n        if not _BaseV4._DECIMAL_DIGITS.issuperset(prefixlen_str):\n            self._report_invalid_netmask(prefixlen_str)\n        try:\n            prefixlen = int(prefixlen_str)\n        except ValueError:\n            self._report_invalid_netmask(prefixlen_str)\n        if not (0 <= prefixlen <= self._max_prefixlen):\n            self._report_invalid_netmask(prefixlen_str)\n        return prefixlen", "output": "Return prefix length from a numeric string\n\n        Args:\n            prefixlen_str: The string to be converted\n\n        Returns:\n            An integer, the prefix length.\n\n        Raises:\n            NetmaskValueError: If the input is not a valid netmask", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_fill_indexer_searchsorted(self, target, method, limit=None):\n        \"\"\"\n        \n        \"\"\"\n        if limit is not None:\n            raise ValueError('limit argument for %r method only well-defined '\n                             'if index and target are monotonic' % method)\n\n        side = 'left' if method == 'pad' else 'right'\n\n        # find exact matches first (this simplifies the algorithm)\n        indexer = self.get_indexer(target)\n        nonexact = (indexer == -1)\n        indexer[nonexact] = self._searchsorted_monotonic(target[nonexact],\n                                                         side)\n        if side == 'left':\n            # searchsorted returns \"indices into a sorted array such that,\n            # if the corresponding elements in v were inserted before the\n            # indices, the order of a would be preserved\".\n            # Thus, we need to subtract 1 to find values to the left.\n            indexer[nonexact] -= 1\n            # This also mapped not found values (values of 0 from\n            # np.searchsorted) to -1, which conveniently is also our\n            # sentinel for missing values\n        else:\n            # Mark indices to the right of the largest value as not found\n            indexer[indexer == len(self)] = -1\n        return indexer", "output": "Fallback pad/backfill get_indexer that works for monotonic decreasing\n        indexes and non-monotonic targets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ctxtReadMemory(self, buffer, size, URL, encoding, options):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlCtxtReadMemory(self._o, buffer, size, URL, encoding, options)\n        if ret is None:raise treeError('xmlCtxtReadMemory() failed')\n        __tmp = xmlDoc(_obj=ret)\n        return __tmp", "output": "parse an XML in-memory document and build a tree. This\n           reuses the existing @ctxt parser context", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def boto_resource(self, service, *args, **kwargs):\n        \"\"\"\"\"\"\n        return self.boto_session.resource(service, *args, **self.configure_boto_session_method_kwargs(service, kwargs))", "output": "A wrapper to apply configuration options to boto resources", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_role(username, role, **kwargs):\n    '''\n    \n    '''\n    role_line = 'username {0} role {1}'.format(username, role)\n    return config(role_line, **kwargs)", "output": "Assign role to username.\n\n    username\n        Username for role configuration\n\n    role\n        Configure role for username\n\n    no_save_config\n        If True, don't save configuration commands to startup configuration.\n        If False, save configuration to startup configuration.\n        Default: False\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd set_role username=daniel role=vdc-admin.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _netstat_aix():\n    '''\n    \n    '''\n    ret = []\n    ## AIX 6.1 - 7.2, appears to ignore addr_family field contents\n    ## for addr_family in ('inet', 'inet6'):\n    for addr_family in ('inet',):\n        # Lookup connections\n        cmd = 'netstat -n -a -f {0} | tail -n +3'.format(addr_family)\n        out = __salt__['cmd.run'](cmd, python_shell=True)\n        for line in out.splitlines():\n            comps = line.split()\n            if len(comps) < 5:\n                continue\n\n            proto_seen = None\n            tcp_flag = True\n            if 'tcp' == comps[0] or 'tcp4' == comps[0]:\n                proto_seen = 'tcp'\n            elif 'tcp6' == comps[0]:\n                proto_seen = 'tcp6'\n            elif 'udp' == comps[0] or 'udp4' == comps[0]:\n                proto_seen = 'udp'\n                tcp_flag = False\n            elif 'udp6' == comps[0]:\n                proto_seen = 'udp6'\n                tcp_flag = False\n\n            if tcp_flag:\n                if len(comps) >= 6:\n                    ret.append({\n                        'proto': proto_seen,\n                        'recv-q': comps[1],\n                        'send-q': comps[2],\n                        'local-address': comps[3],\n                        'remote-address': comps[4],\n                        'state': comps[5]})\n            else:\n                if len(comps) >= 5:\n                    ret.append({\n                        'proto': proto_seen,\n                        'local-address': comps[3],\n                        'remote-address': comps[4]})\n    return ret", "output": "Return netstat information for SunOS flavors", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scalars_route(self, request):\n    \"\"\"\"\"\"\n    # TODO: return HTTP status code for malformed requests\n    tag = request.args.get('tag')\n    run = request.args.get('run')\n    experiment = request.args.get('experiment')\n    output_format = request.args.get('format')\n    (body, mime_type) = self.scalars_impl(tag, run, experiment, output_format)\n    return http_util.Respond(request, body, mime_type)", "output": "Given a tag and single run, return array of ScalarEvents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_properties_from_api_repr(self, resource):\n        \"\"\"\"\"\"\n        self.name = resource.get(\"name\")\n        self.number = resource[\"projectNumber\"]\n        self.labels = resource.get(\"labels\", {})\n        self.status = resource[\"lifecycleState\"]\n        if \"parent\" in resource:\n            self.parent = resource[\"parent\"]", "output": "Update specific properties from its API representation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_common_type(types):\n    \"\"\"\n    \n\n    \"\"\"\n\n    if len(types) == 0:\n        raise ValueError('no types given')\n\n    first = types[0]\n\n    # workaround for find_common_type([np.dtype('datetime64[ns]')] * 2)\n    # => object\n    if all(is_dtype_equal(first, t) for t in types[1:]):\n        return first\n\n    if any(isinstance(t, (PandasExtensionDtype, ExtensionDtype))\n           for t in types):\n        return np.object\n\n    # take lowest unit\n    if all(is_datetime64_dtype(t) for t in types):\n        return np.dtype('datetime64[ns]')\n    if all(is_timedelta64_dtype(t) for t in types):\n        return np.dtype('timedelta64[ns]')\n\n    # don't mix bool / int or float or complex\n    # this is different from numpy, which casts bool with float/int as int\n    has_bools = any(is_bool_dtype(t) for t in types)\n    if has_bools:\n        for t in types:\n            if is_integer_dtype(t) or is_float_dtype(t) or is_complex_dtype(t):\n                return np.object\n\n    return np.find_common_type(types, [])", "output": "Find a common data type among the given dtypes.\n\n    Parameters\n    ----------\n    types : list of dtypes\n\n    Returns\n    -------\n    pandas extension or numpy dtype\n\n    See Also\n    --------\n    numpy.find_common_type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expect(self, pattern, timeout=-1):\n        \"\"\"\"\"\"\n\n        if self.blocking:\n            raise RuntimeError(\"expect can only be used on non-blocking commands.\")\n\n        try:\n            self.subprocess.expect(pattern=pattern, timeout=timeout)\n        except pexpect.EOF:\n            pass", "output": "Waits on the given pattern to appear in std_out", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _process_all_1(self, texts:Collection[str]) -> List[List[str]]:\n        \"\"\n        tok = self.tok_func(self.lang)\n        if self.special_cases: tok.add_special_cases(self.special_cases)\n        return [self.process_text(str(t), tok) for t in texts]", "output": "Process a list of `texts` in one process.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_outputs(self):\n        \"\"\"\n        \"\"\"\n        out_size = mx_uint()\n        handles = ctypes.POINTER(NDArrayHandle)()\n        check_call(_LIB.MXExecutorOutputs(self.handle,\n                                          ctypes.byref(out_size), ctypes.byref(handles)))\n        num_output = out_size.value\n        outputs = [_ndarray_cls(NDArrayHandle(handles[i])) for i in range(num_output)]\n        return outputs", "output": "List all the output NDArray.\n\n        Returns\n        -------\n        A list of ndarray bound to the heads of executor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_info(call=None, kwargs=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The image_info function must be called with -f or --function.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    name = kwargs.get('name', None)\n    image_id = kwargs.get('image_id', None)\n\n    if image_id:\n        if name:\n            log.warning(\n                'Both the \\'image_id\\' and \\'name\\' arguments were provided. '\n                '\\'image_id\\' will take precedence.'\n            )\n    elif name:\n        image_id = get_image_id(kwargs={'name': name})\n    else:\n        raise SaltCloudSystemExit(\n            'The image_info function requires either a \\'name or an \\'image_id\\' '\n            'to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n\n    info = {}\n    response = server.one.image.info(auth, int(image_id))[1]\n    tree = _get_xml(response)\n    info[tree.find('NAME').text] = _xml_to_dict(tree)\n\n    return info", "output": "Retrieves information for a given image. Either a name or an image_id must be\n    supplied.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the image for which to gather information. Can be used instead\n        of ``image_id``.\n\n    image_id\n        The ID of the image for which to gather information. Can be used instead of\n        ``name``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f image_info opennebula name=my-image\n        salt-cloud --function image_info opennebula image_id=5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention(targets_shifted, inputs_encoded, norm_fn, hparams, bias=None):\n  \"\"\"\"\"\"\n  separabilities = [hparams.separability, hparams.separability]\n  if hparams.separability < 0:\n    separabilities = [hparams.separability - 1, hparams.separability]\n  targets_timed = common_layers.subseparable_conv_block(\n      common_layers.add_timing_signal(targets_shifted),\n      hparams.hidden_size, [((1, 1), (5, 1)), ((4, 1), (5, 1))],\n      normalizer_fn=norm_fn,\n      padding=\"LEFT\",\n      separabilities=separabilities,\n      name=\"targets_time\")\n  if hparams.attention_type == \"transformer\":\n    targets_timed = tf.squeeze(targets_timed, 2)\n    target_shape = tf.shape(targets_timed)\n    targets_segment = tf.zeros([target_shape[0], target_shape[1]])\n    target_attention_bias = common_attention.attention_bias(\n        targets_segment, targets_segment, lower_triangular=True)\n    inputs_attention_bias = tf.zeros([\n        tf.shape(inputs_encoded)[0], hparams.num_heads,\n        tf.shape(targets_segment)[1],\n        tf.shape(inputs_encoded)[1]\n    ])\n\n    qv = common_attention.multihead_attention(\n        targets_timed,\n        None,\n        target_attention_bias,\n        hparams.hidden_size,\n        hparams.hidden_size,\n        hparams.hidden_size,\n        hparams.num_heads,\n        hparams.attention_dropout,\n        name=\"self_attention\")\n    qv = common_attention.multihead_attention(\n        qv,\n        inputs_encoded,\n        inputs_attention_bias,\n        hparams.hidden_size,\n        hparams.hidden_size,\n        hparams.hidden_size,\n        hparams.num_heads,\n        hparams.attention_dropout,\n        name=\"encdec_attention\")\n    return tf.expand_dims(qv, 2)\n  elif hparams.attention_type == \"simple\":\n    targets_with_attention = common_layers.simple_attention(\n        targets_timed, inputs_encoded, bias=bias)\n    return norm_fn(targets_shifted + targets_with_attention, name=\"attn_norm\")", "output": "Complete attention layer with preprocessing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _posix_split_name(self, name):\n        \"\"\"\n        \"\"\"\n        prefix = name[:LENGTH_PREFIX + 1]\n        while prefix and prefix[-1] != \"/\":\n            prefix = prefix[:-1]\n\n        name = name[len(prefix):]\n        prefix = prefix[:-1]\n\n        if not prefix or len(name) > LENGTH_NAME:\n            raise ValueError(\"name is too long\")\n        return prefix, name", "output": "Split a name longer than 100 chars into a prefix\n           and a name part.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_pb(self):\n        \"\"\"\n        \"\"\"\n        max_age = _helpers._timedelta_to_duration_pb(self.max_age)\n        return table_v2_pb2.GcRule(max_age=max_age)", "output": "Converts the garbage collection rule to a protobuf.\n\n        :rtype: :class:`.table_v2_pb2.GcRule`\n        :returns: The converted current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_len(self, key):\r\n        \"\"\"\"\"\"\r\n        data = self.model.get_data()\r\n        return len(data[key])", "output": "Return sequence length", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def RelaxNGValidateCtxt(self, ctxt, options):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        ret = libxml2mod.xmlTextReaderRelaxNGValidateCtxt(self._o, ctxt__o, options)\n        return ret", "output": "Use RelaxNG schema context to validate the document as it\n          is processed. Activation is only possible before the first\n          Read(). If @ctxt is None, then RelaxNG schema validation is\n           deactivated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _import_api():\n    '''\n    \n    '''\n    global api\n    full_url = 'https://{0}:{1}/pve-docs/api-viewer/apidoc.js'.format(url, port)\n    returned_data = requests.get(full_url, verify=verify_ssl)\n\n    re_filter = re.compile('(?<=pveapi =)(.*)(?=^;)', re.DOTALL | re.MULTILINE)\n    api_json = re_filter.findall(returned_data.text)[0]\n    api = salt.utils.json.loads(api_json)", "output": "Download https://<url>/pve-docs/api-viewer/apidoc.js\n    Extract content of pveapi var (json formated)\n    Load this json content into global variable \"api\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lst_avg(lst):\n    '''\n    \n    '''\n    salt.utils.versions.warn_until(\n        'Neon',\n        'This results of this function are currently being rounded.'\n        'Beginning in the Salt Neon release, results will no longer be '\n        'rounded and this warning will be removed.',\n        stacklevel=3\n    )\n\n    if not isinstance(lst, collections.Hashable):\n        return float(sum(lst)/len(lst))\n    return float(lst)", "output": "Returns the average value of a list.\n\n    .. code-block:: jinja\n\n        {% my_list = [1,2,3,4] -%}\n        {{ set my_list | avg }}\n\n    will be rendered as:\n\n    .. code-block:: yaml\n\n        2.5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_ssh_key(host,\n                username,\n                password,\n                protocol=None,\n                port=None,\n                certificate_verify=False):\n    '''\n    \n\n    '''\n    if protocol is None:\n        protocol = 'https'\n    if port is None:\n        port = 443\n\n    url = '{0}://{1}:{2}/host/ssh_root_authorized_keys'.format(protocol,\n                                                               host,\n                                                               port)\n    ret = {}\n    try:\n        result = salt.utils.http.query(url,\n                                       status=True,\n                                       text=True,\n                                       method='GET',\n                                       username=username,\n                                       password=password,\n                                       verify_ssl=certificate_verify)\n        if result.get('status') == 200:\n            ret['status'] = True\n            ret['key'] = result['text']\n        else:\n            ret['status'] = False\n            ret['Error'] = result['error']\n    except Exception as msg:\n        ret['status'] = False\n        ret['Error'] = msg\n\n    return ret", "output": "Retrieve the authorized_keys entry for root.\n    This function only works for ESXi, not vCenter.\n\n    :param host: The location of the ESXi Host\n    :param username: Username to connect as\n    :param password: Password for the ESXi web endpoint\n    :param protocol: defaults to https, can be http if ssl is disabled on ESXi\n    :param port: defaults to 443 for https\n    :param certificate_verify: If true require that the SSL connection present\n                               a valid certificate\n    :return: True if upload is successful\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vsphere.get_ssh_key my.esxi.host root bad-password certificate_verify=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_copy(self, connection):\n        \"\"\"\n        \n        \"\"\"\n        if not self.does_schema_exist(connection):\n            logger.info(\"Creating schema for %s\", self.table)\n            self.create_schema(connection)\n\n        if not self.does_table_exist(connection):\n            logger.info(\"Creating table %s\", self.table)\n            self.create_table(connection)\n\n        if self.enable_metadata_columns:\n            self._add_metadata_columns(connection)\n\n        if self.do_truncate_table:\n            logger.info(\"Truncating table %s\", self.table)\n            self.truncate_table(connection)\n\n        if self.do_prune():\n            logger.info(\"Removing %s older than %s from %s\", self.prune_column, self.prune_date, self.prune_table)\n            self.prune(connection)", "output": "Perform pre-copy sql - such as creating table, truncating, or removing data older than x.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_dictlist(data):\n    '''\n    \n    '''\n    if isinstance(data, list):\n        for element in data:\n            if isinstance(element, dict):\n                if len(element) != 1:\n                    return False\n            else:\n                return False\n        return True\n    return False", "output": "Returns True if data is a list of one-element dicts (as found in many SLS\n    schemas), otherwise returns False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def join_cluster(host, user='rabbit', ram_node=None, runas=None):\n    '''\n    \n    '''\n    cmd = [RABBITMQCTL, 'join_cluster']\n    if ram_node:\n        cmd.append('--ram')\n    cmd.append('{0}@{1}'.format(user, host))\n\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n    stop_app(runas)\n    res = __salt__['cmd.run_all'](cmd, reset_system_locale=False, runas=runas, python_shell=False)\n    start_app(runas)\n\n    return _format_response(res, 'Join')", "output": "Join a rabbit cluster\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.join_cluster rabbit.example.com rabbit", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def abs_error(predictions, labels, weights_fn=None):\n  \"\"\"\"\"\"\n  del weights_fn  # Unused\n  targets = tf.squeeze(labels, axis=[2, 3])\n  batch_abs_error = tf.abs(predictions - targets)\n  den = tf.ones(tf.shape(batch_abs_error), dtype=tf.float32)\n  return (batch_abs_error, den)", "output": "Computes mean(abs(preds-target)).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_notifications(self, messages):\n        \"\"\"\"\"\"\n        for object_id, object_size, metadata_size in messages:\n            if object_size > 0 and object_id in self._waiting_dict:\n                linked_list = self._waiting_dict[object_id]\n                self._complete_future(linked_list)", "output": "Process notifications.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def roc_curve(input:Tensor, targ:Tensor):\n    \"\"\n    targ = (targ == 1)\n    desc_score_indices = torch.flip(input.argsort(-1), [-1])\n    input = input[desc_score_indices]\n    targ = targ[desc_score_indices]\n    d = input[1:] - input[:-1]\n    distinct_value_indices = torch.nonzero(d).transpose(0,1)[0]\n    threshold_idxs = torch.cat((distinct_value_indices, LongTensor([len(targ) - 1]).to(targ.device)))\n    tps = torch.cumsum(targ * 1, dim=-1)[threshold_idxs]\n    fps = (1 + threshold_idxs - tps)\n    if tps[0] != 0 or fps[0] != 0:\n        fps = torch.cat((LongTensor([0]), fps))\n        tps = torch.cat((LongTensor([0]), tps))\n    fpr, tpr = fps.float() / fps[-1], tps.float() / tps[-1]\n    return fpr, tpr", "output": "Returns the false positive and true positive rates", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def morsel_to_cookie(morsel):\n    \"\"\"\"\"\"\n\n    expires = None\n    if morsel['max-age']:\n        try:\n            expires = int(time.time() + int(morsel['max-age']))\n        except ValueError:\n            raise TypeError('max-age: %s must be integer' % morsel['max-age'])\n    elif morsel['expires']:\n        time_template = '%a, %d-%b-%Y %H:%M:%S GMT'\n        expires = calendar.timegm(\n            time.strptime(morsel['expires'], time_template)\n        )\n    return create_cookie(\n        comment=morsel['comment'],\n        comment_url=bool(morsel['comment']),\n        discard=False,\n        domain=morsel['domain'],\n        expires=expires,\n        name=morsel.key,\n        path=morsel['path'],\n        port=None,\n        rest={'HttpOnly': morsel['httponly']},\n        rfc2109=False,\n        secure=bool(morsel['secure']),\n        value=morsel.value,\n        version=morsel['version'] or 0,\n    )", "output": "Convert a Morsel object into a Cookie containing the one k/v pair.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_imglist(self, fname=None, root=None, shuffle=False):\n        \"\"\"\n        \n        \"\"\"\n        def progress_bar(count, total, suffix=''):\n            import sys\n            bar_len = 24\n            filled_len = int(round(bar_len * count / float(total)))\n\n            percents = round(100.0 * count / float(total), 1)\n            bar = '=' * filled_len + '-' * (bar_len - filled_len)\n            sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', suffix))\n            sys.stdout.flush()\n\n        str_list = []\n        for index in range(self.num_images):\n            progress_bar(index, self.num_images)\n            label = self.label_from_index(index)\n            if label.size < 1:\n                continue\n            path = self.image_path_from_index(index)\n            if root:\n                path = osp.relpath(path, root)\n            str_list.append('\\t'.join([str(index), str(2), str(label.shape[1])] \\\n              + [\"{0:.4f}\".format(x) for x in label.ravel()] + [path,]) + '\\n')\n        if str_list:\n            if shuffle:\n                import random\n                random.shuffle(str_list)\n            if not fname:\n                fname = self.name + '.lst'\n            with open(fname, 'w') as f:\n                for line in str_list:\n                    f.write(line)\n        else:\n            raise RuntimeError(\"No image in imdb\")", "output": "save imglist to disk\n\n        Parameters:\n        ----------\n        fname : str\n            saved filename", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def recovery(self, index=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request('GET', _make_path('_cat',\n            'recovery', index), params=params)", "output": "recovery is a view of shard replication.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-recovery.html>`_\n\n        :arg index: A comma-separated list of index names to limit the returned\n            information\n        :arg bytes: The unit in which to display byte values, valid choices are:\n            'b', 'k', 'kb', 'm', 'mb', 'g', 'gb', 't', 'tb', 'p', 'pb'\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg v: Verbose mode. Display column headers, default False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_node_from(self, path):\n        \"\"\" \n        \"\"\"\n        context = self\n        for key in path:\n            context = context[key]\n            if context is None:\n                break\n        return context", "output": "Returns a node for a path.\n\n        :param path: Tuple of :term:`hashable` s.\n        :rtype: :class:`~cerberus.errors.ErrorTreeNode` or :obj:`None`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_mimetype(mimetype: str) -> MimeType:\n    \"\"\"\n\n    \"\"\"\n    if not mimetype:\n        return MimeType(type='', subtype='', suffix='',\n                        parameters=MultiDictProxy(MultiDict()))\n\n    parts = mimetype.split(';')\n    params = MultiDict()  # type: MultiDict[str]\n    for item in parts[1:]:\n        if not item:\n            continue\n        key, value = cast(Tuple[str, str],\n                          item.split('=', 1) if '=' in item else (item, ''))\n        params.add(key.lower().strip(), value.strip(' \"'))\n\n    fulltype = parts[0].strip().lower()\n    if fulltype == '*':\n        fulltype = '*/*'\n\n    mtype, stype = (cast(Tuple[str, str], fulltype.split('/', 1))\n                    if '/' in fulltype else (fulltype, ''))\n    stype, suffix = (cast(Tuple[str, str], stype.split('+', 1))\n                     if '+' in stype else (stype, ''))\n\n    return MimeType(type=mtype, subtype=stype, suffix=suffix,\n                    parameters=MultiDictProxy(params))", "output": "Parses a MIME type into its components.\n\n    mimetype is a MIME type string.\n\n    Returns a MimeType object.\n\n    Example:\n\n    >>> parse_mimetype('text/html; charset=utf-8')\n    MimeType(type='text', subtype='html', suffix='',\n             parameters={'charset': 'utf-8'})", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def should_stop(self, result):\n        \"\"\"\"\"\"\n\n        if result.get(DONE):\n            return True\n\n        for criteria, stop_value in self.stopping_criterion.items():\n            if criteria not in result:\n                raise TuneError(\n                    \"Stopping criteria {} not provided in result {}.\".format(\n                        criteria, result))\n            if result[criteria] >= stop_value:\n                return True\n\n        return False", "output": "Whether the given result meets this trial's stopping criteria.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expand_composites (properties):\n    \"\"\" \n    \"\"\"\n    if __debug__:\n        from .property import Property\n        assert is_iterable_typed(properties, Property)\n    explicit_features = set(p.feature for p in properties)\n\n    result = []\n\n    # now expand composite features\n    for p in properties:\n        expanded = expand_composite(p)\n\n        for x in expanded:\n            if not x in result:\n                f = x.feature\n\n                if f.free:\n                    result.append (x)\n                elif not x in properties:  # x is the result of expansion\n                    if not f in explicit_features:  # not explicitly-specified\n                        if any(r.feature == f for r in result):\n                            raise FeatureConflict(\n                                \"expansions of composite features result in \"\n                                \"conflicting values for '%s'\\nvalues: '%s'\\none contributing composite property was '%s'\" %\n                                (f.name, [r.value for r in result if r.feature == f] + [x.value], p))\n                        else:\n                            result.append (x)\n                elif any(r.feature == f for r in result):\n                    raise FeatureConflict (\"explicitly-specified values of non-free feature '%s' conflict\\n\"\n                    \"existing values: '%s'\\nvalue from expanding '%s': '%s'\" % (f,\n                    [r.value for r in result if r.feature == f], p, x.value))\n                else:\n                    result.append (x)\n\n    return result", "output": "Expand all composite properties in the set so that all components\n        are explicitly expressed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename_file(self, fname):\r\n        \"\"\"\"\"\"\r\n        path, valid = QInputDialog.getText(self, _('Rename'),\r\n                              _('New name:'), QLineEdit.Normal,\r\n                              osp.basename(fname))\r\n        if valid:\r\n            path = osp.join(osp.dirname(fname), to_text_string(path))\r\n            if path == fname:\r\n                return\r\n            if osp.exists(path):\r\n                if QMessageBox.warning(self, _(\"Rename\"),\r\n                         _(\"Do you really want to rename <b>%s</b> and \"\r\n                           \"overwrite the existing file <b>%s</b>?\"\r\n                           ) % (osp.basename(fname), osp.basename(path)),\r\n                         QMessageBox.Yes|QMessageBox.No) == QMessageBox.No:\r\n                    return\r\n            try:\r\n                misc.rename_file(fname, path)\r\n                if osp.isfile(fname):\r\n                    self.sig_renamed.emit(fname, path)\r\n                else:\r\n                    self.sig_renamed_tree.emit(fname, path)\r\n                return path\r\n            except EnvironmentError as error:\r\n                QMessageBox.critical(self, _(\"Rename\"),\r\n                            _(\"<b>Unable to rename file <i>%s</i></b>\"\r\n                              \"<br><br>Error message:<br>%s\"\r\n                              ) % (osp.basename(fname), to_text_string(error)))", "output": "Rename file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validate_read(self, kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs = super().validate_read(kwargs)\n        if 'start' in kwargs or 'stop' in kwargs:\n            raise NotImplementedError(\"start and/or stop are not supported \"\n                                      \"in fixed Sparse reading\")\n        return kwargs", "output": "we don't support start, stop kwds in Sparse", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _expand_disk(disk):\n    '''\n    \n    '''\n    ret = {}\n    ret.update(disk.__dict__)\n    zone = ret['extra']['zone']\n    ret['extra']['zone'] = {}\n    ret['extra']['zone'].update(zone.__dict__)\n    return ret", "output": "Convert the libcloud Volume object into something more serializable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_order(field_path, direction):\n        \"\"\"\"\"\"\n        return query_pb2.StructuredQuery.Order(\n            field=query_pb2.StructuredQuery.FieldReference(field_path=field_path),\n            direction=_enum_from_direction(direction),\n        )", "output": "Helper for :meth:`order_by`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def experiment_pb(\n    hparam_infos,\n    metric_infos,\n    user='',\n    description='',\n    time_created_secs=None):\n  \"\"\"\n  \"\"\"\n  if time_created_secs is None:\n    time_created_secs = time.time()\n  experiment = api_pb2.Experiment(\n      description=description,\n      user=user,\n      time_created_secs=time_created_secs,\n      hparam_infos=hparam_infos,\n      metric_infos=metric_infos)\n  return _summary(metadata.EXPERIMENT_TAG,\n                  plugin_data_pb2.HParamsPluginData(experiment=experiment))", "output": "Creates a summary that defines a hyperparameter-tuning experiment.\n\n  Args:\n    hparam_infos: Array of api_pb2.HParamInfo messages. Describes the\n        hyperparameters used in the experiment.\n    metric_infos: Array of api_pb2.MetricInfo messages. Describes the metrics\n        used in the experiment. See the documentation at the top of this file\n        for how to populate this.\n    user: String. An id for the user running the experiment\n    description: String. A description for the experiment. May contain markdown.\n    time_created_secs: float. The time the experiment is created in seconds\n    since the UNIX epoch. If None uses the current time.\n\n  Returns:\n    A summary protobuffer containing the experiment definition.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MessageToJson(message,\n                  including_default_value_fields=False,\n                  preserving_proto_field_name=False):\n  \"\"\"\n  \"\"\"\n  printer = _Printer(including_default_value_fields,\n                     preserving_proto_field_name)\n  return printer.ToJsonString(message)", "output": "Converts protobuf message to JSON format.\n\n  Args:\n    message: The protocol buffers message instance to serialize.\n    including_default_value_fields: If True, singular primitive fields,\n        repeated fields, and map fields will always be serialized.  If\n        False, only serialize non-empty fields.  Singular message fields\n        and oneof fields are not affected by this option.\n    preserving_proto_field_name: If True, use the original proto field\n        names as defined in the .proto file. If False, convert the field\n        names to lowerCamelCase.\n\n  Returns:\n    A string containing the JSON formatted protocol buffer message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid_url(url):\n    \"\"\"\"\"\"\n    pieces = urlparse(url)\n    return all([pieces.scheme, pieces.netloc])", "output": "Checks if a given string is an url", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_config(self):\r\n        \"\"\"\"\"\"\r\n        for option, value in list(self.explorer.get_options().items()):\r\n            self.set_option(option, value)\r\n        self.set_option('expanded_state',\r\n                        self.explorer.treewidget.get_expanded_state())\r\n        self.set_option('scrollbar_position',\r\n                        self.explorer.treewidget.get_scrollbar_position())", "output": "Save configuration: tree widget state", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dump(self, fmap='', with_stats=False, dump_format=\"text\"):\n        \"\"\"\n        \n        \"\"\"\n        length = c_bst_ulong()\n        sarr = ctypes.POINTER(ctypes.c_char_p)()\n        if self.feature_names is not None and fmap == '':\n            flen = len(self.feature_names)\n\n            fname = from_pystr_to_cstr(self.feature_names)\n\n            if self.feature_types is None:\n                # use quantitative as default\n                # {'q': quantitative, 'i': indicator}\n                ftype = from_pystr_to_cstr(['q'] * flen)\n            else:\n                ftype = from_pystr_to_cstr(self.feature_types)\n            _check_call(_LIB.XGBoosterDumpModelExWithFeatures(\n                self.handle,\n                ctypes.c_int(flen),\n                fname,\n                ftype,\n                ctypes.c_int(with_stats),\n                c_str(dump_format),\n                ctypes.byref(length),\n                ctypes.byref(sarr)))\n        else:\n            if fmap != '' and not os.path.exists(fmap):\n                raise ValueError(\"No such file: {0}\".format(fmap))\n            _check_call(_LIB.XGBoosterDumpModelEx(self.handle,\n                                                  c_str(fmap),\n                                                  ctypes.c_int(with_stats),\n                                                  c_str(dump_format),\n                                                  ctypes.byref(length),\n                                                  ctypes.byref(sarr)))\n        res = from_cstr_to_pystr(sarr, length)\n        return res", "output": "Returns the model dump as a list of strings.\n\n        Parameters\n        ----------\n        fmap : string, optional\n            Name of the file containing feature map names.\n        with_stats : bool, optional\n            Controls whether the split statistics are output.\n        dump_format : string, optional\n            Format of model dump. Can be 'text' or 'json'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _layer_norm(self, name, x):\n    \"\"\"\"\"\"\n    if self.init_layers:\n      bn = LayerNorm()\n      bn.name = name\n      self.layers += [bn]\n    else:\n      bn = self.layers[self.layer_idx]\n      self.layer_idx += 1\n    bn.device_name = self.device_name\n    bn.set_training(self.training)\n    x = bn.fprop(x)\n    return x", "output": "Layer normalization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export(self, description, local=False, path='/tmp', format='qcow2'):\n        '''\n        \n        '''\n        kiwiproc.__salt__ = __salt__\n        return kiwiproc.KiwiExporter(grains=__grains__,\n                                     format=format).load(**description).export('something')", "output": "Export description for Kiwi.\n\n        :param local:\n        :param path:\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _slice(self, slicer):\n        \"\"\"  \"\"\"\n\n        # slice the category\n        # return same dims as we currently have\n\n        if isinstance(slicer, tuple) and len(slicer) == 2:\n            if not com.is_null_slice(slicer[0]):\n                raise AssertionError(\"invalid slicing for a 1-ndim \"\n                                     \"categorical\")\n            slicer = slicer[1]\n\n        return self.values[slicer]", "output": "return a slice of my values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def export_to_dict(self, recursive=True, include_parent_ref=False,\n                       include_defaults=False):\n        \"\"\"\"\"\"\n        cls = self.__class__\n        parent_excludes = {}\n        if recursive and not include_parent_ref:\n            parent_ref = cls.__mapper__.relationships.get(cls.export_parent)\n            if parent_ref:\n                parent_excludes = {c.name for c in parent_ref.local_columns}\n        dict_rep = {c.name: getattr(self, c.name)\n                    for c in cls.__table__.columns\n                    if (c.name in self.export_fields and\n                        c.name not in parent_excludes and\n                        (include_defaults or (\n                            getattr(self, c.name) is not None and\n                            (not c.default or\n                                getattr(self, c.name) != c.default.arg))))\n                    }\n        if recursive:\n            for c in self.export_children:\n                # sorting to make lists of children stable\n                dict_rep[c] = sorted(\n                    [\n                        child.export_to_dict(\n                            recursive=recursive,\n                            include_parent_ref=include_parent_ref,\n                            include_defaults=include_defaults,\n                        ) for child in getattr(self, c)\n                    ],\n                    key=lambda k: sorted(k.items()))\n\n        return dict_rep", "output": "Export obj to dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def scalars_route(self, request):\n    \"\"\"\n    \"\"\"\n    # TODO: return HTTP status code for malformed requests\n    tag_regex_string = request.args.get('tag')\n    run = request.args.get('run')\n    mime_type = 'application/json'\n\n    try:\n      body = self.scalars_impl(run, tag_regex_string)\n    except ValueError as e:\n      return http_util.Respond(\n          request=request,\n          content=str(e),\n          content_type='text/plain',\n          code=500)\n\n    # Produce the response.\n    return http_util.Respond(request, body, mime_type)", "output": "Given a tag regex and single run, return ScalarEvents.\n\n    This route takes 2 GET params:\n    run: A run string to find tags for.\n    tag: A string that is a regex used to find matching tags.\n    The response is a JSON object:\n    {\n      // Whether the regular expression is valid. Also false if empty.\n      regexValid: boolean,\n\n      // An object mapping tag name to a list of ScalarEvents.\n      payload: Object<string, ScalarEvent[]>,\n    }", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _svrg_grads_update_rule(self, g_curr_batch_curr_weight, g_curr_batch_special_weight,\n                                g_special_weight_all_batch):\n        \"\"\"\n        \"\"\"\n        for index, grad in enumerate(g_curr_batch_curr_weight):\n            grad -= g_curr_batch_special_weight[index]\n            grad += g_special_weight_all_batch[index]\n        return g_curr_batch_curr_weight", "output": "Calculates the gradient based on the SVRG update rule.\n        Parameters\n        ----------\n        g_curr_batch_curr_weight : NDArray\n            gradients of current weight of self.mod w.r.t current batch of data\n        g_curr_batch_special_weight: NDArray\n            gradients of the weight of past m epochs of self._mod_special w.r.t current batch of data\n        g_special_weight_all_batch: NDArray\n            average of full gradients over full pass of data\n\n        Returns\n        ----------\n        Gradients calculated using SVRG update rule:\n        grads = g_curr_batch_curr_weight - g_curr_batch_special_weight + g_special_weight_all_batch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_services():\n    '''\n    \n    '''\n    serv = []\n    services = pyconnman.ConnManager().get_services()\n    for path, _ in services:\n        serv.append(six.text_type(path[len(SERVICE_PATH):]))\n    return serv", "output": "Returns a list with all connman services", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_absent(name, profile=None, **connection_args):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': 'User \"{0}\" is already absent'.format(name)}\n\n    # Check if user is present\n    user = __salt__['keystone.user_get'](name=name, profile=profile,\n                                         **connection_args)\n    if 'Error' not in user:\n        if __opts__.get('test'):\n            ret['result'] = None\n            ret['comment'] = 'User \"{0}\" will be deleted'.format(name)\n            return ret\n        # Delete that user!\n        __salt__['keystone.user_delete'](name=name, profile=profile,\n                                         **connection_args)\n        ret['comment'] = 'User \"{0}\" has been deleted'.format(name)\n        ret['changes']['User'] = 'Deleted'\n\n    return ret", "output": "Ensure that the keystone user is absent.\n\n    name\n        The name of the user that should not exist", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def track_strategy_worker(self, strategy, name, interval=10, **kwargs):\n        \"\"\"\"\"\"\n        while True:\n            try:\n                transactions = self.query_strategy_transaction(\n                    strategy, **kwargs\n                )\n            # pylint: disable=broad-except\n            except Exception as e:\n                log.exception(\"\u65e0\u6cd5\u83b7\u53d6\u7b56\u7565 %s \u8c03\u4ed3\u4fe1\u606f, \u9519\u8bef: %s, \u8df3\u8fc7\u6b64\u6b21\u8c03\u4ed3\u67e5\u8be2\", name, e)\n                time.sleep(3)\n                continue\n            for transaction in transactions:\n                trade_cmd = {\n                    \"strategy\": strategy,\n                    \"strategy_name\": name,\n                    \"action\": transaction[\"action\"],\n                    \"stock_code\": transaction[\"stock_code\"],\n                    \"amount\": transaction[\"amount\"],\n                    \"price\": transaction[\"price\"],\n                    \"datetime\": transaction[\"datetime\"],\n                }\n                if self.is_cmd_expired(trade_cmd):\n                    continue\n                log.info(\n                    \"\u7b56\u7565 [%s] \u53d1\u9001\u6307\u4ee4\u5230\u4ea4\u6613\u961f\u5217, \u80a1\u7968: %s \u52a8\u4f5c: %s \u6570\u91cf: %s \u4ef7\u683c: %s \u4fe1\u53f7\u4ea7\u751f\u65f6\u95f4: %s\",\n                    name,\n                    trade_cmd[\"stock_code\"],\n                    trade_cmd[\"action\"],\n                    trade_cmd[\"amount\"],\n                    trade_cmd[\"price\"],\n                    trade_cmd[\"datetime\"],\n                )\n                self.trade_queue.put(trade_cmd)\n                self.add_cmd_to_expired_cmds(trade_cmd)\n            try:\n                for _ in range(interval):\n                    time.sleep(1)\n            except KeyboardInterrupt:\n                log.info(\"\u7a0b\u5e8f\u9000\u51fa\")\n                break", "output": "\u8ddf\u8e2a\u4e0b\u5355worker\n        :param strategy: \u7b56\u7565id\n        :param name: \u7b56\u7565\u540d\u5b57\n        :param interval: \u8f6e\u8be2\u7b56\u7565\u7684\u65f6\u95f4\u95f4\u9694\uff0c\u5355\u4f4d\u4e3a\u79d2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lmx_relative():\n  \"\"\"\"\"\"\n  hparams = lmx_base()\n  hparams.self_attention_type = \"dot_product_relative_v2\"\n  hparams.activation_dtype = \"float32\"\n  hparams.weight_dtype = \"float32\"\n  return hparams", "output": "Language model using relative attention.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nansem(values, axis=None, skipna=True, ddof=1, mask=None):\n    \"\"\"\n    \n    \"\"\"\n\n    # This checks if non-numeric-like data is passed with numeric_only=False\n    # and raises a TypeError otherwise\n    nanvar(values, axis, skipna, ddof=ddof, mask=mask)\n\n    if mask is None:\n        mask = isna(values)\n    if not is_float_dtype(values.dtype):\n        values = values.astype('f8')\n    count, _ = _get_counts_nanvar(mask, axis, ddof, values.dtype)\n    var = nanvar(values, axis, skipna, ddof=ddof)\n\n    return np.sqrt(var) / np.sqrt(count)", "output": "Compute the standard error in the mean along given axis while ignoring NaNs\n\n    Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    ddof : int, default 1\n        Delta Degrees of Freedom. The divisor used in calculations is N - ddof,\n        where N represents the number of elements.\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : float64\n        Unless input is a float array, in which case use the same\n        precision as the input array.\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, np.nan, 2, 3])\n    >>> nanops.nansem(s)\n     0.5773502691896258", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train_supervised(problem, model_name, hparams, data_dir, output_dir,\n                     train_steps, eval_steps, local_eval_frequency=None,\n                     schedule=\"continuous_train_and_eval\"):\n  \"\"\"\"\"\"\n  if local_eval_frequency is None:\n    local_eval_frequency = FLAGS.local_eval_frequency\n\n  exp_fn = trainer_lib.create_experiment_fn(\n      model_name, problem, data_dir, train_steps, eval_steps,\n      min_eval_frequency=local_eval_frequency\n  )\n  run_config = trainer_lib.create_run_config(model_name, model_dir=output_dir)\n  exp = exp_fn(run_config, hparams)\n  getattr(exp, schedule)()", "output": "Train supervised.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shift(self, periods=1, freq=None, axis=0, fill_value=None):\n        \"\"\"\n        \n        \"\"\"\n\n        if freq is not None or axis != 0 or not isna(fill_value):\n            return self.apply(lambda x: x.shift(periods, freq,\n                                                axis, fill_value))\n\n        return self._get_cythonized_result('group_shift_indexer',\n                                           self.grouper, cython_dtype=np.int64,\n                                           needs_ngroups=True,\n                                           result_is_index=True,\n                                           periods=periods)", "output": "Shift each group by periods observations.\n\n        Parameters\n        ----------\n        periods : integer, default 1\n            number of periods to shift\n        freq : frequency string\n        axis : axis to shift, default 0\n        fill_value : optional\n\n            .. versionadded:: 0.24.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def words_and_tags_from_wsj_tree(tree_string):\n  \"\"\"\n  \"\"\"\n  stack, tags, words = [], [], []\n  for tok in tree_string.strip().split():\n    if tok[0] == \"(\":\n      symbol = tok[1:]\n      tags.append(symbol)\n      stack.append(symbol)\n    else:\n      assert tok[-1] == \")\"\n      stack.pop()  # Pop the POS-tag.\n      while tok[-2] == \")\":\n        tags.append(\"/\" + stack.pop())\n        tok = tok[:-1]\n      words.append(tok[:-1])\n  return str.join(\" \", words), str.join(\" \", tags[1:-1])", "output": "Generates linearized trees and tokens from the wsj tree format.\n\n  It uses the linearized algorithm described in https://arxiv.org/abs/1412.7449.\n\n  Args:\n    tree_string: tree in wsj format\n\n  Returns:\n    tuple: (words, linearized tree)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_available_time_string(availabilities):\n    \"\"\"\n    \n    \"\"\"\n    prefix = 'We have availabilities at '\n    if len(availabilities) > 3:\n        prefix = 'We have plenty of availability, including '\n\n    prefix += build_time_output_string(availabilities[0])\n    if len(availabilities) == 2:\n        return '{} and {}'.format(prefix, build_time_output_string(availabilities[1]))\n\n    return '{}, {} and {}'.format(prefix, build_time_output_string(availabilities[1]), build_time_output_string(availabilities[2]))", "output": "Build a string eliciting for a possible time slot among at least two availabilities.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_lifecycle(self, policy=None, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"PUT\", _make_path(\"_ilm\", \"policy\", policy), params=params, body=body\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-put-lifecycle.html>`_\n\n        :arg policy: The name of the index lifecycle policy\n        :arg body: The lifecycle policy definition to register", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def divide(n, iterable):\n    \"\"\"\n    \n    \"\"\"\n\n    seq = tuple(iterable)\n    q, r = divmod(len(seq), n)\n\n    ret = []\n    for i in range(n):\n        start = (i * q) + (i if i < r else r)\n        stop = ((i + 1) * q) + (i + 1 if i + 1 < r else r)\n        ret.append(iter(seq[start:stop]))\n\n    return ret", "output": "split an iterable into n groups, per https://more-itertools.readthedocs.io/en/latest/api.html#grouping\n\n    :param int n: Number of unique groups\n    :param iter iterable: An iterable to split up\n    :return: a list of new iterables derived from the original iterable\n    :rtype: list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create(self):\n        \"\"\"\n        \"\"\"\n        if self.path is not None:\n            logger.debug(\n                \"Skipped creation of temporary directory: {}\".format(self.path)\n            )\n            return\n        # We realpath here because some systems have their default tmpdir\n        # symlinked to another directory.  This tends to confuse build\n        # scripts, so we canonicalize the path by traversing potential\n        # symlinks here.\n        self.path = os.path.realpath(\n            tempfile.mkdtemp(prefix=\"pip-{}-\".format(self.kind))\n        )\n        self._register_finalizer()\n        logger.debug(\"Created temporary directory: {}\".format(self.path))", "output": "Create a temporary directory and store its path in self.path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def postprocess_keyevent(self, event):\r\n        \"\"\"\"\"\"\r\n        ShellBaseWidget.postprocess_keyevent(self, event)\r\n        if QToolTip.isVisible():\r\n            _event, _text, key, _ctrl, _shift = restore_keyevent(event)\r\n            self.hide_tooltip_if_necessary(key)", "output": "Process keypress event", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cp(device, from_minor, to_minor):  # pylint: disable=C0103\n    '''\n    \n    '''\n    _validate_device(device)\n\n    try:\n        int(from_minor)\n        int(to_minor)\n    except Exception:\n        raise CommandExecutionError(\n            'Invalid minor number passed to partition.cp'\n        )\n\n    cmd = 'parted -m -s {0} cp {1} {2}'.format(device, from_minor, to_minor)\n    out = __salt__['cmd.run'](cmd).splitlines()\n    return out", "output": "Copies the file system on the partition <from-minor> to partition\n    <to-minor>, deleting the original contents of the destination\n    partition.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' partition.cp /dev/sda 2 3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def GSSAuth(auth_method, gss_deleg_creds=True):\n    \"\"\"\n    \n    \"\"\"\n    if _API == \"MIT\":\n        return _SSH_GSSAPI(auth_method, gss_deleg_creds)\n    elif _API == \"SSPI\" and os.name == \"nt\":\n        return _SSH_SSPI(auth_method, gss_deleg_creds)\n    else:\n        raise ImportError(\"Unable to import a GSS-API / SSPI module!\")", "output": "Provide SSH2 GSS-API / SSPI authentication.\n\n    :param str auth_method: The name of the SSH authentication mechanism\n                            (gssapi-with-mic or gss-keyex)\n    :param bool gss_deleg_creds: Delegate client credentials or not.\n                                 We delegate credentials by default.\n    :return: Either an `._SSH_GSSAPI` (Unix) object or an\n             `_SSH_SSPI` (Windows) object\n\n    :raises: ``ImportError`` -- If no GSS-API / SSPI module could be imported.\n\n    :see: `RFC 4462 <http://www.ietf.org/rfc/rfc4462.txt>`_\n    :note: Check for the available API and return either an `._SSH_GSSAPI`\n           (MIT GSSAPI) object or an `._SSH_SSPI` (MS SSPI) object. If you\n           get python-gssapi working on Windows, python-gssapi\n           will be used and a `._SSH_GSSAPI` object will be returned.\n           If there is no supported API available,\n           ``None`` will be returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expect_element(__funcname=_qualified_name, **named):\n    \"\"\"\n    \n    \"\"\"\n    def _expect_element(collection):\n        if isinstance(collection, (set, frozenset)):\n            # Special case the error message for set and frozen set to make it\n            # less verbose.\n            collection_for_error_message = tuple(sorted(collection))\n        else:\n            collection_for_error_message = collection\n\n        template = (\n            \"%(funcname)s() expected a value in {collection} \"\n            \"for argument '%(argname)s', but got %(actual)s instead.\"\n        ).format(collection=collection_for_error_message)\n        return make_check(\n            ValueError,\n            template,\n            complement(op.contains(collection)),\n            repr,\n            funcname=__funcname,\n        )\n    return preprocess(**valmap(_expect_element, named))", "output": "Preprocessing decorator that verifies inputs are elements of some\n    expected collection.\n\n    Examples\n    --------\n    >>> @expect_element(x=('a', 'b'))\n    ... def foo(x):\n    ...    return x.upper()\n    ...\n    >>> foo('a')\n    'A'\n    >>> foo('b')\n    'B'\n    >>> foo('c')  # doctest: +NORMALIZE_WHITESPACE +ELLIPSIS\n    Traceback (most recent call last):\n       ...\n    ValueError: ...foo() expected a value in ('a', 'b') for argument 'x',\n    but got 'c' instead.\n\n    Notes\n    -----\n    A special argument, __funcname, can be provided as a string to override the\n    function name shown in error messages.  This is most often used on __init__\n    or __new__ methods to make errors refer to the class name instead of the\n    function name.\n\n    This uses the `in` operator (__contains__) to make the containment check.\n    This allows us to use any custom container as long as the object supports\n    the container protocol.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _extract_apis_from_events(function_logical_id, serverless_function_events, collector):\n        \"\"\"\n        \n        \"\"\"\n        count = 0\n        for _, event in serverless_function_events.items():\n\n            if SamApiProvider._FUNCTION_EVENT_TYPE_API == event.get(SamApiProvider._TYPE):\n                api_resource_id, api = SamApiProvider._convert_event_api(function_logical_id, event.get(\"Properties\"))\n                collector.add_apis(api_resource_id, [api])\n                count += 1\n\n        LOG.debug(\"Found '%d' API Events in Serverless function with name '%s'\", count, function_logical_id)", "output": "Given an AWS::Serverless::Function Event Dictionary, extract out all 'Api' events and store  within the\n        collector\n\n        Parameters\n        ----------\n        function_logical_id : str\n            LogicalId of the AWS::Serverless::Function\n\n        serverless_function_events : dict\n            Event Dictionary of a AWS::Serverless::Function\n\n        collector : ApiCollector\n            Instance of the API collector that where we will save the API information", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_training(train_mode): #pylint: disable=redefined-outer-name\n    \"\"\"\n    \"\"\"\n    prev = ctypes.c_int()\n    check_call(_LIB.MXAutogradSetIsTraining(\n        ctypes.c_int(train_mode), ctypes.byref(prev)))\n    return bool(prev.value)", "output": "Set status to training/predicting. This affects ctx.is_train in operator\n    running context. For example, Dropout will drop inputs randomly when\n    train_mode=True while simply passing through if train_mode=False.\n\n    Parameters\n    ----------\n    train_mode: bool\n\n    Returns\n    -------\n    previous state before this set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config(name,\n           config,\n           write=True):\n    '''\n    \n\n    '''\n\n    _build_config_tree(name, config)\n    configs = _render_configuration()\n\n    if __opts__.get('test', False):\n        comment = 'State syslog_ng will write \\'{0}\\' into {1}'.format(\n            configs,\n            __SYSLOG_NG_CONFIG_FILE\n        )\n        return _format_state_result(name, result=None, comment=comment)\n\n    succ = write\n    if write:\n        succ = _write_config(config=configs)\n\n    return _format_state_result(name, result=succ,\n                                changes={'new': configs, 'old': ''})", "output": "Builds syslog-ng configuration. This function is intended to be used from\n    the state module, users should not use it directly!\n\n    name : the id of the Salt document or it is the format of <statement name>.id\n    config : the parsed YAML code\n    write : if True, it writes  the config into the configuration file,\n    otherwise just returns it\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' syslog_ng.config name='s_local' config=\"[{'tcp':[{'ip':'127.0.0.1'},{'port':1233}]}]\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mouseMoveEvent(self, event):\n        \"\"\"\n        \"\"\"\n        self.line_number_hint = self.editor.get_linenumber_from_mouse_event(\n            event)\n        self.update()", "output": "Override Qt method.\n\n        Draw semitransparent breakpoint hint.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def plot(self, key, funcname):\r\n        \"\"\"\"\"\"\r\n        data = self.model.get_data()\r\n        import spyder.pyplot as plt\r\n        plt.figure()\r\n        getattr(plt, funcname)(data[key])\r\n        plt.show()", "output": "Plot item", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_element(driver, by):\n    \"\"\"\"\"\"\n    try:\n        return driver.find_element(*by)\n    except NoSuchElementException as e:\n        raise e\n    except WebDriverException as e:\n        raise e", "output": "Looks up an element. Logs and re-raises ``WebDriverException``\n    if thrown.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_root_path(self, root_path):\r\n        \"\"\"\"\"\"\r\n        self.root_path = root_path\r\n        self.install_model()\r\n        index = self.fsmodel.setRootPath(root_path)\r\n        self.proxymodel.setup_filter(self.root_path, [])\r\n        self.setRootIndex(self.proxymodel.mapFromSource(index))", "output": "Set root path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_current_text(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        text = self.currentText()\r\n        if osp.isdir(text) and text:\r\n            if text[-1] == os.sep:\r\n                text = text[:-1]\r\n        self.add_text(text)", "output": "Add current text to combo box history (convenient method).\r\n        If path ends in os separator (\"\\\" windows, \"/\" unix) remove it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enter_maintenance_mode(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The enter_maintenance_mode function must be called with '\n            '-f or --function.'\n        )\n\n    host_name = kwargs.get('host') if kwargs and 'host' in kwargs else None\n\n    host_ref = salt.utils.vmware.get_mor_by_property(_get_si(), vim.HostSystem, host_name)\n\n    if not host_name or not host_ref:\n        raise SaltCloudSystemExit(\n            'You must specify a valid name of the host system.'\n        )\n\n    if host_ref.runtime.inMaintenanceMode:\n        return {host_name: 'already in maintenance mode'}\n\n    try:\n        task = host_ref.EnterMaintenanceMode(timeout=0, evacuatePoweredOffVms=True)\n        salt.utils.vmware.wait_for_task(task, host_name, 'enter maintenance mode')\n    except Exception as exc:\n        log.error(\n            'Error while moving host system %s in maintenance mode: %s',\n            host_name, exc,\n            # Show the traceback if the debug logging level is enabled\n            exc_info_on_loglevel=logging.DEBUG\n        )\n        return {host_name: 'failed to enter maintenance mode'}\n\n    return {host_name: 'entered maintenance mode'}", "output": "To put the specified host system in maintenance mode in this VMware environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f enter_maintenance_mode my-vmware-config host=\"myHostSystemName\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put(self, entity):\n        \"\"\"\n        \"\"\"\n        if self._status != self._IN_PROGRESS:\n            raise ValueError(\"Batch must be in progress to put()\")\n\n        if entity.key is None:\n            raise ValueError(\"Entity must have a key\")\n\n        if self.project != entity.key.project:\n            raise ValueError(\"Key must be from same project as batch\")\n\n        if entity.key.is_partial:\n            entity_pb = self._add_partial_key_entity_pb()\n            self._partial_key_entities.append(entity)\n        else:\n            entity_pb = self._add_complete_key_entity_pb()\n\n        _assign_entity_to_pb(entity_pb, entity)", "output": "Remember an entity's state to be saved during :meth:`commit`.\n\n        .. note::\n           Any existing properties for the entity will be replaced by those\n           currently set on this instance.  Already-stored properties which do\n           not correspond to keys set on this instance will be removed from\n           the datastore.\n\n        .. note::\n           Property values which are \"text\" ('unicode' in Python2, 'str' in\n           Python3) map to 'string_value' in the datastore;  values which are\n           \"bytes\" ('str' in Python2, 'bytes' in Python3) map to 'blob_value'.\n\n        When an entity has a partial key, calling :meth:`commit` sends it as\n        an ``insert`` mutation and the key is completed. On return,\n        the key for the ``entity`` passed in is updated to match the key ID\n        assigned by the server.\n\n        :type entity: :class:`google.cloud.datastore.entity.Entity`\n        :param entity: the entity to be saved.\n\n        :raises: :class:`~exceptions.ValueError` if the batch is not in\n                 progress, if entity has no key assigned, or if the key's\n                 ``project`` does not match ours.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_permission(FunctionName, StatementId, Qualifier=None,\n                      region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        kwargs = {}\n        if Qualifier is not None:\n            kwargs['Qualifier'] = Qualifier\n        conn.remove_permission(FunctionName=FunctionName, StatementId=StatementId,\n                               **kwargs)\n        return {'updated': True}\n    except ClientError as e:\n        return {'updated': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Remove a permission from a lambda function.\n\n    Returns {removed: true} if the permission was removed and returns\n    {removed: False} if the permission was not removed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_lamba.remove_permission my_function my_id", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self, fn:PathOrStr):\n        \"\"\n        x = image2np(self.data*255).astype(np.uint8)\n        PIL.Image.fromarray(x).save(fn)", "output": "Save the image to `fn`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dir_list(self, tgt_env):\n        '''\n        \n        '''\n        def _traverse(tree, blobs, prefix):\n            '''\n            Traverse through a pygit2 Tree object recursively, accumulating all\n            the empty directories within it in the \"blobs\" list\n            '''\n            for entry in iter(tree):\n                if entry.oid not in self.repo:\n                    # Entry is a submodule, skip it\n                    continue\n                blob = self.repo[entry.oid]\n                if not isinstance(blob, pygit2.Tree):\n                    continue\n                blobs.append(\n                    salt.utils.path.join(prefix, entry.name, use_posixpath=True)\n                )\n                if blob:\n                    _traverse(\n                        blob, blobs, salt.utils.path.join(\n                            prefix, entry.name, use_posixpath=True)\n                    )\n\n        ret = set()\n        tree = self.get_tree(tgt_env)\n        if not tree:\n            return ret\n        if self.root(tgt_env):\n            try:\n                oid = tree[self.root(tgt_env)].oid\n                tree = self.repo[oid]\n            except KeyError:\n                return ret\n            if not isinstance(tree, pygit2.Tree):\n                return ret\n            relpath = lambda path: os.path.relpath(path, self.root(tgt_env))\n        else:\n            relpath = lambda path: path\n        blobs = []\n        if tree:\n            _traverse(tree, blobs, self.root(tgt_env))\n        add_mountpoint = lambda path: salt.utils.path.join(\n            self.mountpoint(tgt_env), path, use_posixpath=True)\n        for blob in blobs:\n            ret.add(add_mountpoint(relpath(blob)))\n        if self.mountpoint(tgt_env):\n            ret.add(self.mountpoint(tgt_env))\n        return ret", "output": "Get a list of directories for the target environment using pygit2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install_translator(qapp):\r\n    \"\"\"\"\"\"\r\n    global QT_TRANSLATOR\r\n    if QT_TRANSLATOR is None:\r\n        qt_translator = QTranslator()\r\n        if qt_translator.load(\"qt_\"+QLocale.system().name(),\r\n                      QLibraryInfo.location(QLibraryInfo.TranslationsPath)):\r\n            QT_TRANSLATOR = qt_translator # Keep reference alive\r\n    if QT_TRANSLATOR is not None:\r\n        qapp.installTranslator(QT_TRANSLATOR)", "output": "Install Qt translator to the QApplication instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def options(self, **options):\n        \"\"\"\n        \"\"\"\n        for k in options:\n            self._jreader = self._jreader.option(k, to_str(options[k]))\n        return self", "output": "Adds input options for the underlying data source.\n\n        You can set the following option(s) for reading files:\n            * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n                in the JSON/CSV datasources or partition values.\n                If it isn't set, it uses the default value, session local timezone.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_visible_toolbars(self):\r\n        \"\"\"\"\"\"\r\n        toolbars = []\r\n        for toolbar in self.toolbarslist:\r\n            if toolbar.toggleViewAction().isChecked():\r\n                toolbars.append(toolbar)\r\n        self.visible_toolbars = toolbars", "output": "Collects the visible toolbars.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop_volume(name, force=False):\n    '''\n    \n    '''\n    volinfo = info()\n    if name not in volinfo:\n        log.error('Cannot stop non-existing volume %s', name)\n        return False\n    if int(volinfo[name]['status']) != 1:\n        log.warning('Attempt to stop already stopped volume %s', name)\n        return True\n\n    cmd = 'volume stop {0}'.format(name)\n    if force:\n        cmd += ' force'\n\n    return _gluster(cmd)", "output": "Stop a gluster volume\n\n    name\n        Volume name\n\n    force\n        Force stop the volume\n\n        .. versionadded:: 2015.8.4\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glusterfs.stop_volume mycluster", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def click_and_hold(self, on_element=None):\n        \"\"\"\n        \n        \"\"\"\n        if on_element:\n            self.move_to_element(on_element)\n        if self._driver.w3c:\n            self.w3c_actions.pointer_action.click_and_hold()\n            self.w3c_actions.key_action.pause()\n        else:\n            self._actions.append(lambda: self._driver.execute(\n                                 Command.MOUSE_DOWN, {}))\n        return self", "output": "Holds down the left mouse button on an element.\n\n        :Args:\n         - on_element: The element to mouse down.\n           If None, clicks on current mouse position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def insert_option_group(self, idx, *args, **kwargs):\n        \"\"\"\"\"\"\n        group = self.add_option_group(*args, **kwargs)\n\n        self.option_groups.pop()\n        self.option_groups.insert(idx, group)\n\n        return group", "output": "Insert an OptionGroup at a given position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _normalize_cursor(self, cursor, orders):\n        \"\"\"\"\"\"\n        if cursor is None:\n            return\n\n        if not orders:\n            raise ValueError(_NO_ORDERS_FOR_CURSOR)\n\n        document_fields, before = cursor\n\n        order_keys = [order.field.field_path for order in orders]\n\n        if isinstance(document_fields, document.DocumentSnapshot):\n            snapshot = document_fields\n            document_fields = snapshot.to_dict()\n            document_fields[\"__name__\"] = snapshot.reference\n\n        if isinstance(document_fields, dict):\n            # Transform to list using orders\n            values = []\n            data = document_fields\n            for order_key in order_keys:\n                try:\n                    values.append(field_path_module.get_nested_value(order_key, data))\n                except KeyError:\n                    msg = _MISSING_ORDER_BY.format(order_key, data)\n                    raise ValueError(msg)\n            document_fields = values\n\n        if len(document_fields) != len(orders):\n            msg = _MISMATCH_CURSOR_W_ORDER_BY.format(document_fields, order_keys)\n            raise ValueError(msg)\n\n        _transform_bases = (transforms.Sentinel, transforms._ValueList)\n\n        for index, key_field in enumerate(zip(order_keys, document_fields)):\n            key, field = key_field\n\n            if isinstance(field, _transform_bases):\n                msg = _INVALID_CURSOR_TRANSFORM\n                raise ValueError(msg)\n\n            if key == \"__name__\" and isinstance(field, six.string_types):\n                document_fields[index] = self._parent.document(field)\n\n        return document_fields, before", "output": "Helper: convert cursor to a list of values based on orders.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _beam_decode(self,\n                   features,\n                   decode_length,\n                   beam_size,\n                   top_beams,\n                   alpha,\n                   use_tpu=False):\n    \"\"\"\n    \"\"\"\n    return self._beam_decode_slow(features, decode_length, beam_size, top_beams,\n                                  alpha, use_tpu)", "output": "Beam search decoding.\n\n    Models should ideally implement a more efficient version of this function.\n\n    Args:\n      features: an map of string to `Tensor`\n      decode_length: an integer.  How many additional timesteps to decode.\n      beam_size: number of beams.\n      top_beams: an integer. How many of the beams to return.\n      alpha: Float that controls the length penalty. larger the alpha, stronger\n        the preference for longer translations.\n      use_tpu: A bool, whether to do beam decode on TPU.\n\n    Returns:\n       samples: an integer `Tensor`. Top samples from the beam search", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def satisfier(self, term):  # type: (Term) -> Assignment\n        \"\"\"\n        \n        \"\"\"\n        assigned_term = None  # type: Term\n\n        for assignment in self._assignments:\n            if assignment.dependency.name != term.dependency.name:\n                continue\n\n            if (\n                not assignment.dependency.is_root\n                and not assignment.dependency.name == term.dependency.name\n            ):\n                if not assignment.is_positive():\n                    continue\n\n                assert not term.is_positive()\n\n                return assignment\n\n            if assigned_term is None:\n                assigned_term = assignment\n            else:\n                assigned_term = assigned_term.intersect(assignment)\n\n            # As soon as we have enough assignments to satisfy term, return them.\n            if assigned_term.satisfies(term):\n                return assignment\n\n        raise RuntimeError(\"[BUG] {} is not satisfied.\".format(term))", "output": "Returns the first Assignment in this solution such that the sublist of\n        assignments up to and including that entry collectively satisfies term.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def unpin(self):\n        \"\"\"\n        \"\"\"\n\n        await self._state.http.unpin_message(self.channel.id, self.id)\n        self.pinned = False", "output": "|coro|\n\n        Unpins the message.\n\n        You must have the :attr:`~Permissions.manage_messages` permission to do\n        this in a non-private channel context.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to unpin the message.\n        NotFound\n            The message or channel was not found or deleted.\n        HTTPException\n            Unpinning the message failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_circuit_provider(name, asn=None):\n    '''\n    \n    '''\n\n    nb_circuit_provider = get_('circuits', 'providers', name=name)\n    payload = {}\n    if nb_circuit_provider:\n        if nb_circuit_provider['asn'] == asn:\n            return False\n        else:\n            log.error('Duplicate provider with different ASN: %s: %s', name, asn)\n            raise CommandExecutionError(\n                'Duplicate provider with different ASN: {}: {}'.format(name, asn)\n            )\n    else:\n        payload = {\n            'name': name,\n            'slug': slugify(name)\n        }\n        if asn:\n            payload['asn'] = asn\n        circuit_provider = _add('circuits', 'providers', payload)\n        if circuit_provider:\n            return {'circuits': {'providers': {circuit_provider['id']: payload}}}\n        else:\n            return circuit_provider", "output": ".. versionadded:: 2019.2.0\n\n    Create a new Netbox circuit provider\n\n    name\n        The name of the circuit provider\n    asn\n        The ASN of the circuit provider\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion netbox.create_circuit_provider Telia 1299", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_frame(self, index=True, name=None):\n        \"\"\"\n        \n        \"\"\"\n\n        from pandas import DataFrame\n        if name is not None:\n            if not is_list_like(name):\n                raise TypeError(\"'name' must be a list / sequence \"\n                                \"of column names.\")\n\n            if len(name) != len(self.levels):\n                raise ValueError(\"'name' should have same length as \"\n                                 \"number of levels on index.\")\n            idx_names = name\n        else:\n            idx_names = self.names\n\n        # Guarantee resulting column order\n        result = DataFrame(\n            OrderedDict([\n                ((level if lvlname is None else lvlname),\n                 self._get_level_values(level))\n                for lvlname, level in zip(idx_names, range(len(self.levels)))\n            ]),\n            copy=False\n        )\n\n        if index:\n            result.index = self\n        return result", "output": "Create a DataFrame with the levels of the MultiIndex as columns.\n\n        Column ordering is determined by the DataFrame constructor with data as\n        a dict.\n\n        .. versionadded:: 0.24.0\n\n        Parameters\n        ----------\n        index : boolean, default True\n            Set the index of the returned DataFrame as the original MultiIndex.\n\n        name : list / sequence of strings, optional\n            The passed names should substitute index level names.\n\n        Returns\n        -------\n        DataFrame : a DataFrame containing the original MultiIndex data.\n\n        See Also\n        --------\n        DataFrame", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _exclusion_indices_for_range(self, start_idx, end_idx):\n        \"\"\"\n        \n        \"\"\"\n        itree = self._minute_exclusion_tree\n        if itree.overlaps(start_idx, end_idx):\n            ranges = []\n            intervals = itree[start_idx:end_idx]\n            for interval in intervals:\n                ranges.append(interval.data)\n            return sorted(ranges)\n        else:\n            return None", "output": "Returns\n        -------\n        List of tuples of (start, stop) which represent the ranges of minutes\n        which should be excluded when a market minute window is requested.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_absent(name, htpasswd_file=None, runas=None):\n    '''\n    \n\n    '''\n    ret = {'name': name,\n           'changes': {},\n           'comment': '',\n           'result': None}\n\n    exists = __salt__['file.grep'](\n        htpasswd_file, '^{0}:'.format(name))['retcode'] == 0\n\n    if not exists:\n        if __opts__['test']:\n            ret['result'] = None\n        else:\n            ret['result'] = True\n        ret['comment'] = 'User already not in file'\n    else:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'User \\'{0}\\' is set to be removed from htpasswd file'.format(name)\n            ret['changes'] = {name: True}\n        else:\n            userdel_ret = __salt__['webutil.userdel'](\n                htpasswd_file, name, runas=runas, all_results=True)\n\n            ret['result'] = userdel_ret['retcode'] == 0\n            ret['comment'] = userdel_ret['stderr']\n\n            if ret['result']:\n                ret['changes'] = {name: True}\n\n    return ret", "output": "Make sure the user is not in the specified htpasswd file\n\n    name\n        User name\n\n    htpasswd_file\n        Path to the htpasswd file\n\n    runas\n        The system user to run htpasswd command with", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def requote_redirect_url(self, val: bool) -> None:\n        \"\"\"\"\"\"\n        warnings.warn(\"session.requote_redirect_url modification \"\n                      \"is deprecated #2778\",\n                      DeprecationWarning,\n                      stacklevel=2)\n        self._requote_redirect_url = val", "output": "Do URL requoting on redirection handling.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_resource_from_properties(obj, filter_fields):\n    \"\"\"\n    \"\"\"\n    partial = {}\n    for filter_field in filter_fields:\n        api_field = obj._PROPERTY_TO_API_FIELD.get(filter_field)\n        if api_field is None and filter_field not in obj._properties:\n            raise ValueError(\"No property %s\" % filter_field)\n        elif api_field is not None:\n            partial[api_field] = obj._properties.get(api_field)\n        else:\n            # allows properties that are not defined in the library\n            # and properties that have the same name as API resource key\n            partial[filter_field] = obj._properties[filter_field]\n\n    return partial", "output": "Build a resource based on a ``_properties`` dictionary, filtered by\n    ``filter_fields``, which follow the name of the Python object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_svn_page(html):\n    # type: (Union[str, Text]) -> Optional[Match[Union[str, Text]]]\n    \"\"\"\n    \n    \"\"\"\n    return (re.search(r'<title>[^<]*Revision \\d+:', html) and\n            re.search(r'Powered by (?:<a[^>]*?>)?Subversion', html, re.I))", "output": "Returns true if the page appears to be the index page of an svn repository", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_audit_sink(self, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_audit_sink_with_http_info(body, **kwargs)\n        else:\n            (data) = self.create_audit_sink_with_http_info(body, **kwargs)\n            return data", "output": "create an AuditSink\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_audit_sink(body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param V1alpha1AuditSink body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1alpha1AuditSink\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map(self, map_fn, name=\"Map\"):\n        \"\"\"\n        \"\"\"\n        op = Operator(\n            _generate_uuid(),\n            OpType.Map,\n            name,\n            map_fn,\n            num_instances=self.env.config.parallelism)\n        return self.__register(op)", "output": "Applies a map operator to the stream.\n\n        Attributes:\n             map_fn (function): The user-defined logic of the map.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_cmd(cmd, args, ret):\n        \"\"\"\n        \"\"\"\n        from dvc.daemon import daemon\n\n        if not Analytics._is_enabled(cmd):\n            return\n\n        analytics = Analytics()\n        analytics.collect_cmd(args, ret)\n        daemon([\"analytics\", analytics.dump()])", "output": "Collect and send analytics for CLI command.\n\n        Args:\n            args (list): parsed args for the CLI command.\n            ret (int): return value of the CLI command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _smartos_zone_pkgin_data():\n    '''\n    \n    '''\n    # Provides:\n    #   pkgin_repositories\n\n    grains = {\n        'pkgin_repositories': [],\n    }\n\n    pkginrepo = re.compile('^(?:https|http|ftp|file)://.*$')\n    if os.path.isfile('/opt/local/etc/pkgin/repositories.conf'):\n        with salt.utils.files.fopen('/opt/local/etc/pkgin/repositories.conf', 'r') as fp_:\n            for line in fp_:\n                line = salt.utils.stringutils.to_unicode(line)\n                if pkginrepo.match(line):\n                    grains['pkgin_repositories'].append(line)\n\n    return grains", "output": "SmartOS zone pkgsrc information", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(self, where=None, columns=None, **kwargs):\n        \"\"\"\n        \"\"\"\n\n        if not self.read_axes(where=where, **kwargs):\n            return None\n\n        raise NotImplementedError(\"Panel is removed in pandas 0.25.0\")", "output": "we have n indexable columns, with an arbitrary number of data\n        axes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ensure_index_from_sequences(sequences, names=None):\n    \"\"\"\n    \n    \"\"\"\n    from .multi import MultiIndex\n\n    if len(sequences) == 1:\n        if names is not None:\n            names = names[0]\n        return Index(sequences[0], name=names)\n    else:\n        return MultiIndex.from_arrays(sequences, names=names)", "output": "Construct an index from sequences of data.\n\n    A single sequence returns an Index. Many sequences returns a\n    MultiIndex.\n\n    Parameters\n    ----------\n    sequences : sequence of sequences\n    names : sequence of str\n\n    Returns\n    -------\n    index : Index or MultiIndex\n\n    Examples\n    --------\n    >>> ensure_index_from_sequences([[1, 2, 3]], names=['name'])\n    Int64Index([1, 2, 3], dtype='int64', name='name')\n\n    >>> ensure_index_from_sequences([['a', 'a'], ['a', 'b']],\n                                    names=['L1', 'L2'])\n    MultiIndex(levels=[['a'], ['a', 'b']],\n               codes=[[0, 0], [0, 1]],\n               names=['L1', 'L2'])\n\n    See Also\n    --------\n    ensure_index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_bas8l_8h_big_uncond_dr03_imgnet():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_base_14l_8h_big_dr01()\n  # num_hidden_layers\n  hparams.num_decoder_layers = 8\n  hparams.num_heads = 8\n  hparams.hidden_size = 512\n  hparams.filter_size = 2048\n  hparams.layer_prepostprocess_dropout = 0.3\n  return hparams", "output": "big 1d model for conditional image generation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def volume_list(search_opts=None, profile=None, **kwargs):\n    '''\n    \n\n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.volume_list(search_opts=search_opts)", "output": "List storage volumes\n\n    search_opts\n        Dictionary of search options\n\n    profile\n        Profile to use\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.volume_list search_opts='{\"display_name\": \"myblock\"}' profile=openstack", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def str_decode(arr, encoding, errors=\"strict\"):\n    \"\"\"\n    \n    \"\"\"\n    if encoding in _cpython_optimized_decoders:\n        # CPython optimized implementation\n        f = lambda x: x.decode(encoding, errors)\n    else:\n        decoder = codecs.getdecoder(encoding)\n        f = lambda x: decoder(x, errors)[0]\n    return _na_map(f, arr)", "output": "Decode character string in the Series/Index using indicated encoding.\n    Equivalent to :meth:`str.decode` in python2 and :meth:`bytes.decode` in\n    python3.\n\n    Parameters\n    ----------\n    encoding : str\n    errors : str, optional\n\n    Returns\n    -------\n    Series or Index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next_frame_basic_recurrent():\n  \"\"\"\"\"\"\n  hparams = basic_stochastic.next_frame_basic_stochastic_discrete()\n  hparams.filter_double_steps = 2\n  hparams.hidden_size = 64\n  hparams.video_num_input_frames = 4\n  hparams.video_num_target_frames = 4\n  hparams.concat_internal_states = False\n  hparams.add_hparam(\"num_lstm_layers\", 2)\n  hparams.add_hparam(\"num_lstm_filters\", 256)\n  return hparams", "output": "Basic 2-frame recurrent model with stochastic tower.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_sample_weight(pipeline_steps, sample_weight=None):\n    \"\"\"\n\n    \"\"\"\n    sample_weight_dict = {}\n    if not isinstance(sample_weight, type(None)):\n        for (pname, obj) in pipeline_steps:\n            if inspect.getargspec(obj.fit).args.count('sample_weight'):\n                step_sw = pname + '__sample_weight'\n                sample_weight_dict[step_sw] = sample_weight\n\n    if sample_weight_dict:\n        return sample_weight_dict\n    else:\n        return None", "output": "Recursively iterates through all objects in the pipeline and sets sample weight.\n\n    Parameters\n    ----------\n    pipeline_steps: array-like\n        List of (str, obj) tuples from a scikit-learn pipeline or related object\n    sample_weight: array-like\n        List of sample weight\n    Returns\n    -------\n    sample_weight_dict:\n        A dictionary of sample_weight", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def highlight_block(self, text):\r\n        \"\"\"\"\"\"\r\n        text = to_text_string(text)\r\n        inside_comment = tbh.get_state(self.currentBlock().previous()) == self.INSIDE_COMMENT\r\n        self.setFormat(0, len(text),\r\n                       self.formats[\"comment\" if inside_comment else \"normal\"])\r\n        \r\n        match = self.PROG.search(text)\r\n        index = 0\r\n        while match:\r\n            for key, value in list(match.groupdict().items()):\r\n                if value:\r\n                    start, end = match.span(key)\r\n                    index += end-start\r\n                    if key == \"comment_start\":\r\n                        inside_comment = True\r\n                        self.setFormat(start, len(text)-start,\r\n                                       self.formats[\"comment\"])\r\n                    elif key == \"comment_end\":\r\n                        inside_comment = False\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"comment\"])\r\n                    elif inside_comment:\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"comment\"])\r\n                    elif key == \"define\":\r\n                        self.setFormat(start, end-start,\r\n                                       self.formats[\"number\"])\r\n                    else:\r\n                        self.setFormat(start, end-start, self.formats[key])\r\n                    \r\n            match = self.PROG.search(text, match.end())\r\n        \r\n        self.highlight_spaces(text)\r\n        \r\n        last_state = self.INSIDE_COMMENT if inside_comment else self.NORMAL\r\n        tbh.set_state(self.currentBlock(), last_state)", "output": "Implement highlight specific for C/C++.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_class_docstrings(app, what, name, obj, options, lines):\n    \"\"\"\n    \n\n    \"\"\"\n    if what == \"class\":\n        joined = '\\n'.join(lines)\n\n        templates = [\n            \"\"\".. rubric:: Attributes\n\n.. autosummary::\n   :toctree:\n\n   None\n\"\"\",\n            \"\"\".. rubric:: Methods\n\n.. autosummary::\n   :toctree:\n\n   None\n\"\"\"\n        ]\n\n        for template in templates:\n            if template in joined:\n                joined = joined.replace(template, '')\n        lines[:] = joined.split('\\n')", "output": "For those classes for which we use ::\n\n    :template: autosummary/class_without_autosummary.rst\n\n    the documented attributes/methods have to be listed in the class\n    docstring. However, if one of those lists is empty, we use 'None',\n    which then generates warnings in sphinx / ugly html output.\n    This \"autodoc-process-docstring\" event connector removes that part\n    from the processed docstring.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def normalVectorRDD(sc, numRows, numCols, numPartitions=None, seed=None):\n        \"\"\"\n        \n        \"\"\"\n        return callMLlibFunc(\"normalVectorRDD\", sc._jsc, numRows, numCols, numPartitions, seed)", "output": "Generates an RDD comprised of vectors containing i.i.d. samples drawn\n        from the standard normal distribution.\n\n        :param sc: SparkContext used to create the RDD.\n        :param numRows: Number of Vectors in the RDD.\n        :param numCols: Number of elements in each Vector.\n        :param numPartitions: Number of partitions in the RDD (default: `sc.defaultParallelism`).\n        :param seed: Random seed (default: a random long integer).\n        :return: RDD of Vector with vectors containing i.i.d. samples ~ `N(0.0, 1.0)`.\n\n        >>> import numpy as np\n        >>> mat = np.matrix(RandomRDDs.normalVectorRDD(sc, 100, 100, seed=1).collect())\n        >>> mat.shape\n        (100, 100)\n        >>> abs(mat.mean() - 0.0) < 0.1\n        True\n        >>> abs(mat.std() - 1.0) < 0.1\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_table_columns(metadata):\n    \"\"\" \n    \"\"\"\n    cols = OrderedDict()\n    for col in metadata.c:\n        name = str(col).rpartition(\".\")[2]\n        cols[name] = col.type.python_type.__name__\n    return cols", "output": "Extract columns names and python typos from metadata\n\n    Args:\n        metadata: Table metadata\n\n    Returns:\n        dict with columns names and python types", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def namespace_match(pattern: str, namespace: str):\n    \"\"\"\n    \n    \"\"\"\n    if pattern[0] == '*' and namespace.endswith(pattern[1:]):\n        return True\n    elif pattern == namespace:\n        return True\n    return False", "output": "Matches a namespace pattern against a namespace string.  For example, ``*tags`` matches\n    ``passage_tags`` and ``question_tags`` and ``tokens`` matches ``tokens`` but not\n    ``stemmed_tokens``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_element(tup, elem):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        return tup, tup.index(elem)\n    except ValueError:\n        return tuple(chain(tup, (elem,))), len(tup)", "output": "Create a tuple containing all elements of tup, plus elem.\n\n    Returns the new tuple and the index of elem in the new tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_calltip(self, signature, doc='', parameter='', parameter_doc='',\r\n                     color=_DEFAULT_TITLE_COLOR, is_python=False):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        # Find position of calltip\r\n        point = self._calculate_position()\r\n\r\n        # Format text\r\n        tiptext, wrapped_lines = self._format_signature(\r\n            signature,\r\n            doc,\r\n            parameter,\r\n            parameter_doc,\r\n            color,\r\n            is_python,\r\n        )\r\n\r\n        self._update_stylesheet(self.calltip_widget)\r\n\r\n        # Show calltip\r\n        self.calltip_widget.show_tip(point, tiptext, wrapped_lines)", "output": "Show calltip.\r\n\r\n        Calltips look like tooltips but will not disappear if mouse hovers\r\n        them. They are useful for displaying signature information on methods\r\n        and functions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def yank_fields_from_attrs(attrs, _as=None, sort=True):\n    \"\"\"\n    \n    \"\"\"\n    fields_with_names = []\n    for attname, value in list(attrs.items()):\n        field = get_field_as(value, _as)\n        if not field:\n            continue\n        fields_with_names.append((attname, field))\n\n    if sort:\n        fields_with_names = sorted(fields_with_names, key=lambda f: f[1])\n    return OrderedDict(fields_with_names)", "output": "Extract all the fields in given attributes (dict)\n    and return them ordered", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _remove_persistent_module(mod, comment):\n    '''\n    \n    '''\n    conf = _get_modules_conf()\n    mod_name = _strip_module_name(mod)\n    if not mod_name or mod_name not in mod_list(True):\n        return set()\n    escape_mod = re.escape(mod)\n    if comment:\n        __salt__['file.comment'](conf, '^[\\t ]*{0}[\\t ]?'.format(escape_mod))\n    else:\n        __salt__['file.sed'](conf, '^[\\t ]*{0}[\\t ]?'.format(escape_mod), '')\n    return set([mod_name])", "output": "Remove module from configuration file. If comment is true only comment line\n    where module is.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_dependencies_met():\n    \"\"\"\n    \n    \"\"\"\n    # Method added in `cryptography==1.1`; not available in older versions\n    from cryptography.x509.extensions import Extensions\n    if getattr(Extensions, \"get_extension_for_class\", None) is None:\n        raise ImportError(\"'cryptography' module missing required functionality.  \"\n                          \"Try upgrading to v1.3.4 or newer.\")\n\n    # pyOpenSSL 0.14 and above use cryptography for OpenSSL bindings. The _x509\n    # attribute is only present on those versions.\n    from OpenSSL.crypto import X509\n    x509 = X509()\n    if getattr(x509, \"_x509\", None) is None:\n        raise ImportError(\"'pyOpenSSL' module missing required functionality. \"\n                          \"Try upgrading to v0.14 or newer.\")", "output": "Verifies that PyOpenSSL's package-level dependencies have been met.\n    Throws `ImportError` if they are not met.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_val(self, val, result_score_list):\n        \"\"\"\n        \"\"\"\n        self._update_pbar()\n        if val == 'Timeout':\n            self._update_pbar(pbar_msg=('Skipped pipeline #{0} due to time out. '\n                                        'Continuing to the next pipeline.'.format(self._pbar.n)))\n            result_score_list.append(-float('inf'))\n        else:\n            result_score_list.append(val)\n        return result_score_list", "output": "Update values in the list of result scores and self._pbar during pipeline evaluation.\n\n        Parameters\n        ----------\n        val: float or \"Timeout\"\n            CV scores\n        result_score_list: list\n            A list of CV scores\n\n        Returns\n        -------\n        result_score_list: list\n            A updated list of CV scores", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _exponential_timeout_generator(initial, maximum, multiplier, deadline):\n    \"\"\"\n    \"\"\"\n    if deadline is not None:\n        deadline_datetime = datetime_helpers.utcnow() + datetime.timedelta(\n            seconds=deadline\n        )\n    else:\n        deadline_datetime = datetime.datetime.max\n\n    timeout = initial\n    while True:\n        now = datetime_helpers.utcnow()\n        yield min(\n            # The calculated timeout based on invocations.\n            timeout,\n            # The set maximum timeout.\n            maximum,\n            # The remaining time before the deadline is reached.\n            float((deadline_datetime - now).seconds),\n        )\n        timeout = timeout * multiplier", "output": "A generator that yields exponential timeout values.\n\n    Args:\n        initial (float): The initial timeout.\n        maximum (float): The maximum timeout.\n        multiplier (float): The multiplier applied to the timeout.\n        deadline (float): The overall deadline across all invocations.\n\n    Yields:\n        float: A timeout value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sections_to_variance_sections(self, sections_over_time):\n    '''\n    '''\n    variance_sections = []\n\n    for i in range(len(sections_over_time[0])):\n      time_sections = [sections[i] for sections in sections_over_time]\n      variance = np.var(time_sections, axis=0)\n      variance_sections.append(variance)\n\n    return variance_sections", "output": "Computes the variance of corresponding sections over time.\n\n    Returns:\n      a list of np arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean(ctx, state, dry_run=False, bare=False, user=False):\n    \"\"\"\"\"\"\n    from ..core import do_clean\n    do_clean(ctx=ctx, three=state.three, python=state.python, dry_run=dry_run,\n             system=state.system)", "output": "Uninstalls all packages not specified in Pipfile.lock.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def table(self):\n        \"\"\"\n        \n        \"\"\"\n        return pd.DataFrame([\n            self.message,\n        ]).set_index(\n            'account_cookie',\n            drop=False\n        ).T", "output": "\u6253\u5370\u51faaccount\u7684\u5185\u5bb9", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _gather_group_members(group, groups, users):\n    '''\n    \n    '''\n    _group = __salt__['group.info'](group)\n\n    if not _group:\n        log.warning('Group %s does not exist, ignoring.', group)\n        return\n\n    for member in _group['members']:\n        if member not in users:\n            users[member] = groups[group]", "output": "Gather group members", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pdb_has_stopped(self, fname, lineno, shellwidget):\r\n        \"\"\"\"\"\"\r\n        # This is a unique form of the edit_goto signal that is intended to\r\n        # prevent keyboard input from accidentally entering the editor\r\n        # during repeated, rapid entry of debugging commands.\r\n        self.edit_goto[str, int, str, bool].emit(fname, lineno, '', False)\r\n        self.activateWindow()\r\n        shellwidget._control.setFocus()", "output": "Python debugger has just stopped at frame (fname, lineno)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _smooth_distribution(p, eps=0.0001):\n    \"\"\"\n    \"\"\"\n    is_zeros = (p == 0).astype(np.float32)\n    is_nonzeros = (p != 0).astype(np.float32)\n    n_zeros = is_zeros.sum()\n    n_nonzeros = p.size - n_zeros\n    if not n_nonzeros:\n        raise ValueError('The discrete probability distribution is malformed. All entries are 0.')\n    eps1 = eps * float(n_zeros) / float(n_nonzeros)\n    assert eps1 < 1.0, 'n_zeros=%d, n_nonzeros=%d, eps1=%f' % (n_zeros, n_nonzeros, eps1)\n    hist = p.astype(np.float32)\n    hist += eps * is_zeros + (-eps1) * is_nonzeros\n    assert (hist <= 0).sum() == 0\n    return hist", "output": "Given a discrete distribution (may have not been normalized to 1),\n    smooth it by replacing zeros with eps multiplied by a scaling factor and taking the\n    corresponding amount off the non-zero values.\n    Ref: http://web.engr.illinois.edu/~hanj/cs412/bk3/KL-divergence.pdf", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def poll_event(self):\n        \"\"\"\n        \"\"\"\n        try:\n            msg = await self.recv()\n            await self.received_message(msg)\n        except websockets.exceptions.ConnectionClosed as exc:\n            if self._can_handle_close(exc.code):\n                log.info('Websocket closed with %s (%s), attempting a reconnect.', exc.code, exc.reason)\n                raise ResumeWebSocket(self.shard_id) from exc\n            else:\n                log.info('Websocket closed with %s (%s), cannot reconnect.', exc.code, exc.reason)\n                raise ConnectionClosed(exc, shard_id=self.shard_id) from exc", "output": "Polls for a DISPATCH event and handles the general gateway loop.\n\n        Raises\n        ------\n        ConnectionClosed\n            The websocket connection was terminated for unhandled reasons.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete(queue_id):\n    '''\n    \n\n    '''\n\n    ret = {'message': '',\n           'result': True\n           }\n\n    if not queue_id:\n        log.error('Require argument queue_id')\n\n    if not queue_id == 'ALL':\n        queue = show_queue()\n        _message = None\n        for item in queue:\n            if item['queue_id'] == queue_id:\n                _message = item\n\n        if not _message:\n            ret['message'] = 'No message in queue with ID {0}'.format(queue_id)\n            ret['result'] = False\n            return ret\n\n    cmd = 'postsuper -d {0}'.format(queue_id)\n    result = __salt__['cmd.run_all'](cmd)\n\n    if result['retcode'] == 0:\n        if queue_id == 'ALL':\n            ret['message'] = 'Successfully removed all messages'\n        else:\n            ret['message'] = 'Successfully removed message with queue id {0}'.format(queue_id)\n    else:\n        if queue_id == 'ALL':\n            ret['message'] = 'Unable to removed all messages'\n        else:\n            ret['message'] = 'Unable to remove message with queue id {0}: {1}'.format(queue_id, result['stderr'])\n    return ret", "output": "Delete message(s) from the mail queue\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' postfix.delete 5C33CA0DEA\n\n        salt '*' postfix.delete ALL", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_parsed_show_output(parsed_data):\n    '''\n    \n    '''\n    # Match lines like \"main: xenial [snapshot]\" or \"test [local]\".\n    source_pattern = re.compile(r'(?:(?P<component>\\S+):)?\\s*(?P<name>\\S+)\\s+\\[(?P<type>\\S+)\\]')\n    sources = list()\n\n    if 'architectures' in parsed_data:\n        parsed_data['architectures'] = [item.strip() for item in parsed_data['architectures'].split()]\n        parsed_data['architectures'] = sorted(parsed_data['architectures'])\n\n    for source in parsed_data.get('sources', []):\n        # Retain the key/value of only the matching named groups.\n        matches = source_pattern.search(source)\n\n        if matches:\n            groups = matches.groupdict()\n            sources.append({key: groups[key] for key in groups if groups[key]})\n    if sources:\n        parsed_data['sources'] = sources\n\n    return parsed_data", "output": "Convert matching string values to lists/dictionaries.\n\n    :param dict parsed_data: The text of the command output that needs to be parsed.\n\n    :return: A dictionary containing the modified configuration data.\n    :rtype: dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_thing_type(thingTypeName,\n            region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_thing_type(thingTypeName=thingTypeName)\n        return {'deleted': True}\n    except ClientError as e:\n        err = __utils__['boto3.get_error'](e)\n        if e.response.get('Error', {}).get('Code') == 'ResourceNotFoundException':\n            return {'deleted': True}\n        return {'deleted': False, 'error': err}", "output": "Given a thing type name, delete it.\n\n    Returns {deleted: true} if the thing type was deleted and returns\n    {deleted: false} if the thing type was not deleted.\n\n    .. versionadded:: 2016.11.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.delete_thing_type mythingtype", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_logger(self):\n        \"\"\"\"\"\"\n\n        if not self.result_logger:\n            if not os.path.exists(self.local_dir):\n                os.makedirs(self.local_dir)\n            if not self.logdir:\n                self.logdir = tempfile.mkdtemp(\n                    prefix=\"{}_{}\".format(\n                        str(self)[:MAX_LEN_IDENTIFIER], date_str()),\n                    dir=self.local_dir)\n            elif not os.path.exists(self.logdir):\n                os.makedirs(self.logdir)\n\n            self.result_logger = UnifiedLogger(\n                self.config,\n                self.logdir,\n                upload_uri=self.upload_dir,\n                loggers=self.loggers,\n                sync_function=self.sync_function)", "output": "Init logger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def upgrade(self, using=None, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        return self._get_connection(using).indices.upgrade(index=self._name, **kwargs)", "output": "Upgrade the index to the latest format.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.upgrade`` unchanged.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ping():\n    '''\n    \n    '''\n\n    dev = conn()\n    # Check that the underlying netconf connection still exists.\n    if dev._conn is None:\n        return False\n\n    # call rpc only if ncclient queue is empty. If not empty that means other\n    # rpc call is going on.\n    if hasattr(dev._conn, '_session'):\n        if dev._conn._session._transport.is_active():\n            # there is no on going rpc call. buffer tell can be 1 as it stores\n            # remaining char after \"]]>]]>\" which can be a new line char\n            if dev._conn._session._buffer.tell() <= 1 and \\\n                    dev._conn._session._q.empty():\n                return _rpc_file_list(dev)\n            else:\n                log.debug('skipped ping() call as proxy already getting data')\n                return True\n        else:\n            # ssh connection is lost\n            return False\n    else:\n        # other connection modes, like telnet\n        return _rpc_file_list(dev)", "output": "Ping?  Pong!", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_char_states(self, char_embed, is_training, reuse, char_ids, char_lengths):\n        \"\"\"\"\"\"\n        max_char_length = self.cfg.max_char_length\n\n        inputs = dropout(tf.nn.embedding_lookup(char_embed, char_ids),\n                         self.cfg.dropout, is_training)\n        inputs = tf.reshape(\n            inputs, shape=[max_char_length, -1, self.cfg.char_embed_dim])\n        char_lengths = tf.reshape(char_lengths, shape=[-1])\n        with tf.variable_scope('char_encoding', reuse=reuse):\n            cell_fw = XGRUCell(hidden_dim=self.cfg.char_embed_dim)\n            cell_bw = XGRUCell(hidden_dim=self.cfg.char_embed_dim)\n            _, (left_right, right_left) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=cell_fw,\n                cell_bw=cell_bw,\n                sequence_length=char_lengths,\n                inputs=inputs,\n                time_major=True,\n                dtype=tf.float32\n            )\n\n        left_right = tf.reshape(left_right, shape=[-1, self.cfg.char_embed_dim])\n\n        right_left = tf.reshape(right_left, shape=[-1, self.cfg.char_embed_dim])\n\n        states = tf.concat([left_right, right_left], axis=1)\n        out_shape = tf.shape(char_ids)[1:3]\n        out_shape = tf.concat([out_shape, tf.constant(\n            value=[self.cfg.char_embed_dim * 2], dtype=tf.int32)], axis=0)\n        return tf.reshape(states, shape=out_shape)", "output": "Build char embedding network for the QA model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_default_storage_policy_of_datastore(datastore, service_instance=None):\n    '''\n    \n    '''\n    log.trace('Listing the default storage policy of datastore \\'%s\\'', datastore)\n    # Find datastore\n    target_ref = _get_proxy_target(service_instance)\n    ds_refs = salt.utils.vmware.get_datastores(service_instance, target_ref,\n                                               datastore_names=[datastore])\n    if not ds_refs:\n        raise VMwareObjectRetrievalError('Datastore \\'{0}\\' was not '\n                                         'found'.format(datastore))\n    profile_manager = salt.utils.pbm.get_profile_manager(service_instance)\n    policy = salt.utils.pbm.get_default_storage_policy_of_datastore(\n        profile_manager, ds_refs[0])\n    return _get_policy_dict(policy)", "output": "Returns a list of datastores assign the the storage policies.\n\n    datastore\n        Name of the datastore to assign.\n        The datastore needs to be visible to the VMware entity the proxy\n        points to.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.list_default_storage_policy_of_datastore datastore=ds1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SkipFieldValue(tokenizer):\n  \"\"\"\n  \"\"\"\n  # String/bytes tokens can come in multiple adjacent string literals.\n  # If we can consume one, consume as many as we can.\n  if tokenizer.TryConsumeByteString():\n    while tokenizer.TryConsumeByteString():\n      pass\n    return\n\n  if (not tokenizer.TryConsumeIdentifier() and\n      not _TryConsumeInt64(tokenizer) and not _TryConsumeUint64(tokenizer) and\n      not tokenizer.TryConsumeFloat()):\n    raise ParseError('Invalid field value: ' + tokenizer.token)", "output": "Skips over a field value.\n\n  Args:\n    tokenizer: A tokenizer to parse the field name and values.\n\n  Raises:\n    ParseError: In case an invalid field value is found.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reindex_axis(self, labels, axis=0, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        # for compatibility with higher dims\n        if axis != 0:\n            raise ValueError(\"cannot reindex series on non-zero axis!\")\n        msg = (\"'.reindex_axis' is deprecated and will be removed in a future \"\n               \"version. Use '.reindex' instead.\")\n        warnings.warn(msg, FutureWarning, stacklevel=2)\n\n        return self.reindex(index=labels, **kwargs)", "output": "Conform Series to new index with optional filling logic.\n\n        .. deprecated:: 0.21.0\n            Use ``Series.reindex`` instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_show(id=None, name=None, profile=None):  # pylint: disable=C0103\n    '''\n    \n    '''\n    g_client = _auth(profile)\n    ret = {}\n    if name:\n        for image in g_client.images.list():\n            if image.name == name:\n                id = image.id  # pylint: disable=C0103\n                continue\n    if not id:\n        return {\n            'result': False,\n            'comment':\n                'Unable to resolve image ID '\n                'for name \\'{0}\\''.format(name)\n            }\n    try:\n        image = g_client.images.get(id)\n    except exc.HTTPNotFound:\n        return {\n            'result': False,\n            'comment': 'No image with ID {0}'.format(id)\n            }\n    log.debug(\n        'Properties of image %s:\\n%s',\n        image.name, pprint.PrettyPrinter(indent=4).pformat(image)\n    )\n\n    schema = image_schema(profile=profile)\n    if len(schema.keys()) == 1:\n        schema = schema['image']\n    for key in schema:\n        if key in image:\n            ret[key] = image[key]\n    return ret", "output": "Return details about a specific image (glance image-show)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glance.image_show", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _delivery_report(err, msg):\n    '''  '''\n    if err is not None:\n        log.error('Message delivery failed: %s', err)\n    else:\n        log.debug('Message delivered to %s [%s]', msg.topic(), msg.partition())", "output": "Called once for each message produced to indicate delivery result.\n        Triggered by poll() or flush().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_hosts(self, host):\n        \"\"\"\n        \n        \"\"\"\n        try:\n            return shlex.split(host)\n        except ValueError:\n            raise Exception(\"Unparsable host {}\".format(host))", "output": "Return a list of host_names from host value.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _virt_call(domain, function, section, comment,\n               connection=None, username=None, password=None, **kwargs):\n    '''\n    \n    '''\n    ret = {'name': domain, 'changes': {}, 'result': True, 'comment': ''}\n    targeted_domains = fnmatch.filter(__salt__['virt.list_domains'](), domain)\n    changed_domains = list()\n    ignored_domains = list()\n    for targeted_domain in targeted_domains:\n        try:\n            response = __salt__['virt.{0}'.format(function)](targeted_domain,\n                                                             connection=connection,\n                                                             username=username,\n                                                             password=password,\n                                                             **kwargs)\n            if isinstance(response, dict):\n                response = response['name']\n            changed_domains.append({'domain': targeted_domain, function: response})\n        except libvirt.libvirtError as err:\n            ignored_domains.append({'domain': targeted_domain, 'issue': six.text_type(err)})\n    if not changed_domains:\n        ret['result'] = False\n        ret['comment'] = 'No changes had happened'\n        if ignored_domains:\n            ret['changes'] = {'ignored': ignored_domains}\n    else:\n        ret['changes'] = {section: changed_domains}\n        ret['comment'] = comment\n\n    return ret", "output": "Helper to call the virt functions. Wildcards supported.\n\n    :param domain:\n    :param function:\n    :param section:\n    :param comment:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _linux_wwns():\n    '''\n    \n    '''\n    ret = []\n    for fc_file in glob.glob('/sys/class/fc_host/*/port_name'):\n        with salt.utils.files.fopen(fc_file, 'r') as _wwn:\n            content = _wwn.read()\n            for line in content.splitlines():\n                ret.append(line.rstrip()[2:])\n    return ret", "output": "Return Fibre Channel port WWNs from a Linux host.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def foreach_model(self, fn):\n        \"\"\"\n        \"\"\"\n\n        results = ray.get([w.foreach_model.remote(fn) for w in self.workers])\n        out = []\n        for r in results:\n            out.extend(r)\n        return out", "output": "Apply the given function to each model replica in each worker.\n\n        Returns:\n            List of results from applying the function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def del_password(name, root=None):\n    '''\n    \n    '''\n    cmd = ['passwd']\n    if root is not None:\n        cmd.extend(('-R', root))\n    cmd.extend(('-d', name))\n\n    __salt__['cmd.run'](cmd, python_shell=False, output_loglevel='quiet')\n    uinfo = info(name, root=root)\n    return not uinfo['passwd'] and uinfo['name'] == name", "output": ".. versionadded:: 2014.7.0\n\n    Delete the password from name user\n\n    name\n        User to delete\n\n    root\n        Directory to chroot into\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.del_password username", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_pretty_usage(usage):\n    \"\"\"\n    \n    \"\"\"\n\n    if usage is not None and usage.strip() != \"\":\n        usage = \"\\n\".join(map(lambda u: u.strip(), usage.split(\"\\n\")))\n        return \"%s\\n\\n\" % usage", "output": "Makes the usage description pretty and returns a formatted string if `usage`\n    is not an empty string. Otherwise, returns None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(self, targets, jobs=None, remote=None, show_checksums=False):\n        \"\"\"\n        \"\"\"\n        cloud = self._get_cloud(remote, \"status\")\n        return self.repo.cache.local.status(\n            targets, jobs=jobs, remote=cloud, show_checksums=show_checksums\n        )", "output": "Check status of data items in a cloud-agnostic way.\n\n        Args:\n            targets (list): list of targets to check status for.\n            jobs (int): number of jobs that can be running simultaneously.\n            remote (dvc.remote.base.RemoteBase): optional remote to compare\n                targets to. By default remote from core.remote config option\n                is used.\n            show_checksums (bool): show checksums instead of file names in\n                information messages.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def post_card(message,\n              hook_url=None,\n              title=None,\n              theme_color=None):\n    '''\n    \n    '''\n\n    if not hook_url:\n        hook_url = _get_hook_url()\n\n    if not message:\n        log.error('message is a required option.')\n\n    payload = {\n        \"text\": message,\n        \"title\": title,\n        \"themeColor\": theme_color\n    }\n\n    result = salt.utils.http.query(hook_url,\n                                   method='POST',\n                                   data=salt.utils.json.dumps(payload),\n                                   status=True)\n\n    if result['status'] <= 201:\n        return True\n    else:\n        return {\n            'res': False,\n            'message': result.get('body', result['status'])\n        }", "output": "Send a message to an MS Teams channel.\n    :param message:     The message to send to the MS Teams channel.\n    :param hook_url:    The Teams webhook URL, if not specified in the configuration.\n    :param title:       Optional title for the posted card\n    :param theme_color:  Optional hex color highlight for the posted card\n    :return:            Boolean if message was sent successfully.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' msteams.post_card message=\"Build is done\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_with_random_selector(x, func, num_cases):\n  \"\"\"\n  \"\"\"\n  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n  # Pass the real x only to one of the func calls.\n  return control_flow_ops.merge([\n      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n      for case in range(num_cases)\n  ])[0]", "output": "Computes func(x, sel), with sel sampled from [0...num_cases-1].\n\n  Args:\n    x: input Tensor.\n    func: Python function to apply.\n    num_cases: Python int32, number of cases to sample sel from.\n\n  Returns:\n    The result of func(x, sel), where func receives the value of the\n    selector as a python integer, but sel is sampled dynamically.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_tags(filesystemid,\n                tags,\n                keyid=None,\n                key=None,\n                profile=None,\n                region=None,\n                **kwargs):\n    '''\n    \n    '''\n\n    client = _get_conn(key=key, keyid=keyid, profile=profile, region=region)\n\n    new_tags = []\n    for k, v in six.iteritems(tags):\n        new_tags.append({'Key': k, 'Value': v})\n\n    client.create_tags(FileSystemId=filesystemid, Tags=new_tags)", "output": "Creates or overwrites tags associated with a file system.\n    Each tag is a key-value pair. If a tag key specified in the request\n    already exists on the file system, this operation overwrites\n    its value with the value provided in the request.\n\n    filesystemid\n        (string) - ID of the file system for whose tags will be modified.\n\n    tags\n        (dict) - The tags to add to the file system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'my-minion' boto_efs.create_tags", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_table_data(self):\r\n        \"\"\"\"\"\"\r\n        data = self._simplify_shape(\r\n                self.table_widget.get_data())\r\n        if self.table_widget.array_btn.isChecked():\r\n            return array(data)\r\n        elif pd and self.table_widget.df_btn.isChecked():\r\n            info = self.table_widget.pd_info\r\n            buf = io.StringIO(self.table_widget.pd_text)\r\n            return pd.read_csv(buf, **info)\r\n        return data", "output": "Return clipboard processed as data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_delete(user_id=None, name=None, profile=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    if name:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n    kstone.users.delete(user_id)\n    ret = 'User ID {0} deleted'.format(user_id)\n    if name:\n\n        ret += ' ({0})'.format(name)\n    return ret", "output": "Delete a user (keystone user-delete)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_delete user_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.user_delete name=nova", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hook(self, m:nn.Module, i:Tensors, o:Tensors)->Tuple[Rank0Tensor,Rank0Tensor]:\n        \"\"\n        return o.mean().item(),o.std().item()", "output": "Take the mean and std of `o`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _interfaces_ipconfig(out):\n    '''\n    \n    '''\n    ifaces = dict()\n    iface = None\n    adapter_iface_regex = re.compile(r'adapter (\\S.+):$')\n\n    for line in out.splitlines():\n        if not line:\n            continue\n        # TODO what does Windows call Infiniband and 10/40gige adapters\n        if line.startswith('Ethernet'):\n            iface = ifaces[adapter_iface_regex.search(line).group(1)]\n            iface['up'] = True\n            addr = None\n            continue\n        if iface:\n            key, val = line.split(',', 1)\n            key = key.strip(' .')\n            val = val.strip()\n            if addr and key == 'Subnet Mask':\n                addr['netmask'] = val\n            elif key in ('IP Address', 'IPv4 Address'):\n                if 'inet' not in iface:\n                    iface['inet'] = list()\n                addr = {'address': val.rstrip('(Preferred)'),\n                        'netmask': None,\n                        'broadcast': None}  # TODO find the broadcast\n                iface['inet'].append(addr)\n            elif 'IPv6 Address' in key:\n                if 'inet6' not in iface:\n                    iface['inet'] = list()\n                # XXX What is the prefixlen!?\n                addr = {'address': val.rstrip('(Preferred)'),\n                        'prefixlen': None}\n                iface['inet6'].append(addr)\n            elif key == 'Physical Address':\n                iface['hwaddr'] = val\n            elif key == 'Media State':\n                # XXX seen used for tunnel adaptors\n                # might be useful\n                iface['up'] = (val != 'Media disconnected')", "output": "Returns a dictionary of interfaces with various information about each\n    (up/down state, ip address, netmask, and hwaddr)\n\n    NOTE: This is not used by any function and may be able to be removed in the\n    future.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_(saltenv='base', test=None):\n    '''\n    \n    '''\n    sevent = salt.utils.event.get_event(\n            'master',\n            __opts__['sock_dir'],\n            __opts__['transport'],\n            opts=__opts__,\n            listen=True)\n\n    master_key = salt.utils.master.get_master_key('root', __opts__)\n\n    __jid_event__.fire_event({'key': master_key}, 'salt/reactors/manage/list')\n\n    results = sevent.get_event(wait=30, tag='salt/reactors/manage/list-results')\n    reactors = results['reactors']\n    return reactors", "output": "List currently configured reactors\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run reactor.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wiki_2x2_base():\n  \"\"\"\n  \"\"\"\n  hparams = mtf_transformer.mtf_transformer_base_lm()\n  hparams.shared_embedding_and_softmax_weights = False\n  # no dropout - dataset is big enough to avoid overfitting.\n  hparams.attention_dropout = 0.0\n  hparams.relu_dropout = 0.0\n  hparams.layer_prepostprocess_dropout = 0.0\n  hparams.max_length = 1024\n  # 4 sequences per core\n  hparams.batch_size = 32\n  # We don't use linear decay in these experiments, since we don't want\n  # a sharp jump in quality at the end of the training schedule.\n  # You can insert this once you find the right architecture.\n  hparams.learning_rate_schedule = \"rsqrt_decay\"\n  hparams.mesh_shape = \"all:8\"\n  hparams.layout = \"batch:all;experts:all\"\n\n  # parameters for mixture-of-experts\n  moe.set_default_moe_hparams(hparams)\n  hparams.moe_num_experts = 16\n  hparams.moe_hidden_size = 8192\n\n  hparams.decoder_layers = [\"att\", \"drd\"] * 6\n  hparams.d_model = 1024\n  hparams.d_ff = 2048\n  hparams.d_kv = 128\n  hparams.num_heads = 4\n\n  return hparams", "output": "Set of architectural experiments - language model on wikipedia on a 2x2.\n\n  1 epoch = ~180k steps at batch size 32 - we may never finish an epoch!\n\n  Returns:\n    a hparams", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_header_accept(self, accepts):\n        \"\"\"\n        \n        \"\"\"\n        if not accepts:\n            return\n\n        accepts = [x.lower() for x in accepts]\n\n        if 'application/json' in accepts:\n            return 'application/json'\n        else:\n            return ', '.join(accepts)", "output": "Returns `Accept` based on an array of accepts provided.\n\n        :param accepts: List of headers.\n        :return: Accept (e.g. application/json).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_plugin_title(self):\n        \"\"\"\"\"\"\n        if self.dockwidget is not None:\n            win = self.dockwidget\n        elif self.undocked_window is not None:\n            win = self.undocked_window\n        else:\n            return\n        win.setWindowTitle(self.get_plugin_title())", "output": "Update plugin title, i.e. dockwidget or window title", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rank():\n    \"\"\" \n    \"\"\"\n    rank_data_url = 'https://raw.githubusercontent.com/Microsoft/LightGBM/master/examples/lambdarank/'\n    x_train, y_train = sklearn.datasets.load_svmlight_file(cache(rank_data_url + 'rank.train'))\n    x_test, y_test = sklearn.datasets.load_svmlight_file(cache(rank_data_url + 'rank.test'))\n    q_train = np.loadtxt(cache(rank_data_url + 'rank.train.query'))\n    q_test = np.loadtxt(cache(rank_data_url + 'rank.test.query'))\n    return x_train, y_train, x_test, y_test, q_train, q_test", "output": "Ranking datasets from lightgbm repository.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def annotation_path(cls, project, incident, annotation):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/incidents/{incident}/annotations/{annotation}\",\n            project=project,\n            incident=incident,\n            annotation=annotation,\n        )", "output": "Return a fully-qualified annotation string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gpu_memory_info(device_id=0):\n    \"\"\"\n\n    \"\"\"\n    free = ctypes.c_uint64()\n    total = ctypes.c_uint64()\n    dev_id = ctypes.c_int(device_id)\n    check_call(_LIB.MXGetGPUMemoryInformation64(dev_id, ctypes.byref(free), ctypes.byref(total)))\n    return (free.value, total.value)", "output": "Query CUDA for the free and total bytes of GPU global memory.\n\n    Parameters\n    ----------\n    device_id : int, optional\n        The device id of the GPU device.\n\n    Raises\n    ------\n    Will raise an exception on any CUDA error.\n\n    Returns\n    -------\n    (free, total) : (int, int)\n        The number of GPUs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_and_uncompress(self, fileobj, dst_path):\n    \"\"\"\n    \"\"\"\n    try:\n      with tarfile.open(mode=\"r|*\", fileobj=fileobj) as tgz:\n        for tarinfo in tgz:\n          abs_target_path = _merge_relative_path(dst_path, tarinfo.name)\n\n          if tarinfo.isfile():\n            self._extract_file(tgz, tarinfo, abs_target_path)\n          elif tarinfo.isdir():\n            tf_v1.gfile.MakeDirs(abs_target_path)\n          else:\n            # We do not support symlinks and other uncommon objects.\n            raise ValueError(\n                \"Unexpected object type in tar archive: %s\" % tarinfo.type)\n\n        total_size_str = tf_utils.bytes_to_readable_str(\n            self._total_bytes_downloaded, True)\n        self._print_download_progress_msg(\n            \"Downloaded %s, Total size: %s\" % (self._url, total_size_str),\n            flush=True)\n    except tarfile.ReadError:\n      raise IOError(\"%s does not appear to be a valid module.\" % self._url)", "output": "Streams the content for the 'fileobj' and stores the result in dst_path.\n\n    Args:\n      fileobj: File handle pointing to .tar/.tar.gz content.\n      dst_path: Absolute path where to store uncompressed data from 'fileobj'.\n\n    Raises:\n      ValueError: Unknown object encountered inside the TAR file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_hg_revision(repopath):\r\n    \"\"\"\r\n    \"\"\"\r\n    try:\r\n        assert osp.isdir(osp.join(repopath, '.hg'))\r\n        proc = programs.run_program('hg', ['id', '-nib', repopath])\r\n        output, _err = proc.communicate()\r\n        # output is now: ('eba7273c69df+ 2015+ default\\n', None)\r\n        # Split 2 times max to allow spaces in branch names.\r\n        return tuple(output.decode().strip().split(None, 2))\r\n    except (subprocess.CalledProcessError, AssertionError, AttributeError,\r\n            OSError):\r\n        return (None, None, None)", "output": "Return Mercurial revision for the repository located at repopath\r\n       Result is a tuple (global, local, branch), with None values on error\r\n       For example:\r\n           >>> get_hg_revision(\".\")\r\n           ('eba7273c69df+', '2015+', 'default')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reshape(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    if len(inputs) == 1:\n        return 'reshape', attrs, inputs[0]\n    reshape_shape = list(proto_obj._params[inputs[1].name].asnumpy())\n    reshape_shape = [int(i) for i in reshape_shape]\n    new_attrs = {'shape': reshape_shape}\n    return 'reshape', new_attrs, inputs[:1]", "output": "Reshape the given array by the shape attribute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assign(name, value):\n    '''\n    \n    '''\n    ret = {}\n    cmd = 'sysctl {0}=\"{1}\"'.format(name, value)\n    data = __salt__['cmd.run_all'](cmd)\n\n    # Certain values cannot be set from this console, at the current\n    # securelevel or there are other restrictions that prevent us\n    # from applying the setting rightaway.\n    if re.match(r'^sysctl:.*: Operation not permitted$', data['stderr']) or \\\n      data['retcode'] != 0:\n        raise CommandExecutionError('sysctl failed: {0}'.format(\n            data['stderr']))\n    new_name, new_value = data['stdout'].split(':', 1)\n    ret[new_name] = new_value.split(' -> ')[-1]\n    return ret", "output": "Assign a single sysctl parameter for this minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sysctl.assign net.inet.ip.forwarding 1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def AddRow(self, *args):\n        '''  '''\n        NumRows = len(self.Rows)  # number of existing rows is our row number\n        CurrentRowNumber = NumRows  # this row's number\n        CurrentRow = []  # start with a blank row and build up\n        # -------------------------  Add the elements to a row  ------------------------- #\n        for i, element in enumerate(args):  # Loop through list of elements and add them to the row\n            element.Position = (CurrentRowNumber, i)\n            element.ParentContainer = self\n            CurrentRow.append(element)\n        # -------------------------  Append the row to list of Rows  ------------------------- #\n        self.Rows.append(CurrentRow)", "output": "Parms are a variable number of Elements", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def multi_split(s, split):\n    # type: (S, Iterable[S]) -> List[S]\n    \"\"\"\"\"\"\n    for r in split:\n        s = s.replace(r, \"|\")\n    return [i for i in s.split(\"|\") if len(i) > 0]", "output": "Splits on multiple given separators.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_instancenorm(net, node, model, builder):\n    \"\"\"\n    \"\"\"\n    import numpy as _np\n    input_name, output_name = _get_input_output_name(net, node)\n\n    name = node['name']\n    inputs = node['inputs']\n    outputs = node['outputs']\n\n\n    data_blob_name = _get_node_name(net, inputs[0][0])\n    gamma_blob_name = _get_node_name(net, inputs[1][0])\n    beta_blob_name = _get_node_name(net, inputs[2][0])\n    channels = _get_node_channels(net, inputs[0][0])\n\n    bn_output_name = output_name + '_bn_'\n\n    builder.add_batchnorm(\n        name = name + '_normalize',\n        channels = channels,\n        gamma = _np.ones((channels, )),\n        beta = _np.zeros((channels, )),\n        mean = None,\n        variance = None,\n        input_name = input_name,\n        output_name = bn_output_name,\n        compute_mean_var = True,\n        instance_normalization = True)\n\n    gamma_input_names = [bn_output_name, gamma_blob_name]\n    gamma_output_name = output_name + '_mult_gamma'\n    builder.add_elementwise(name=name+'_mult_gamma', input_names=gamma_input_names,\n        output_name = gamma_output_name, mode='MULTIPLY', alpha = None)\n    beta_input_names = [gamma_output_name, beta_blob_name]\n    builder.add_elementwise(name=name+'_add_beta', input_names=beta_input_names,\n        output_name = output_name, mode='ADD', alpha=None)", "output": "Convert an instance norm layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    net: network\n        A mxnet network object.\n\n    node: layer\n        Node to convert.\n\n    model: model\n        An model for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rand_zoom(scale:uniform=1.0, p:float=1.):\n    \"\"\n    return zoom(scale=scale, **rand_pos, p=p)", "output": "Randomized version of `zoom`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_vocab_from_list(self, vocab_list):\n    \"\"\"\n    \"\"\"\n    def token_gen():\n      for token in vocab_list:\n        if token not in RESERVED_TOKENS:\n          yield token\n\n    self._init_vocab(token_gen())", "output": "Initialize tokens from a list of tokens.\n\n    It is ok if reserved tokens appear in the vocab list. They will be\n    removed. The set of tokens in vocab_list should be unique.\n\n    Args:\n      vocab_list: A list of tokens.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __clean_tmp(sfn):\n    '''\n    \n    '''\n    if sfn.startswith(os.path.join(tempfile.gettempdir(),\n                                   salt.utils.files.TEMPFILE_PREFIX)):\n        # Don't remove if it exists in file_roots (any saltenv)\n        all_roots = itertools.chain.from_iterable(\n                six.itervalues(__opts__['file_roots']))\n        in_roots = any(sfn.startswith(root) for root in all_roots)\n        # Only clean up files that exist\n        if os.path.exists(sfn) and not in_roots:\n            os.remove(sfn)", "output": "Clean out a template temp file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dzip_exact(*dicts):\n    \"\"\"\n    \n    \"\"\"\n    if not same(*map(viewkeys, dicts)):\n        raise ValueError(\n            \"dict keys not all equal:\\n\\n%s\" % _format_unequal_keys(dicts)\n        )\n    return {k: tuple(d[k] for d in dicts) for k in dicts[0]}", "output": "Parameters\n    ----------\n    *dicts : iterable[dict]\n        A sequence of dicts all sharing the same keys.\n\n    Returns\n    -------\n    zipped : dict\n        A dict whose keys are the union of all keys in *dicts, and whose values\n        are tuples of length len(dicts) containing the result of looking up\n        each key in each dict.\n\n    Raises\n    ------\n    ValueError\n        If dicts don't all have the same keys.\n\n    Examples\n    --------\n    >>> result = dzip_exact({'a': 1, 'b': 2}, {'a': 3, 'b': 4})\n    >>> result == {'a': (1, 3), 'b': (2, 4)}\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(self):\n        \"\"\"\"\"\"\n        id = self.get(\"id\")\n        if not id:\n            id = \"(none)\"\n        else:\n            id = id[0]\n\n        parent = self.get(\"parent\")\n        if not parent:\n            parent = \"(none)\"\n        else:\n            parent = parent[0]\n\n        print \"'%s'\" % id\n        print \"Parent project:%s\", parent\n        print \"Requirements:%s\", self.get(\"requirements\")\n        print \"Default build:%s\", string.join(self.get(\"debuild-build\"))\n        print \"Source location:%s\", string.join(self.get(\"source-location\"))\n        print \"Projects to build:%s\", string.join(self.get(\"projects-to-build\").sort());", "output": "Prints the project attributes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_search_space(path):\n    ''''''\n    content = json.dumps(get_json_content(path))\n    if not content:\n        raise ValueError('searchSpace file should not be empty')\n    return content", "output": "load search space content", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_bool(val):\n    '''\n    \n    '''\n    if val is None:\n        return False\n    if isinstance(val, bool):\n        return val\n    if isinstance(val, (six.text_type, six.string_types)):\n        return val.lower() in ('yes', '1', 'true')\n    if isinstance(val, six.integer_types):\n        return val > 0\n    if not isinstance(val, collections.Hashable):\n        return bool(val)\n    return False", "output": "Returns the logical value.\n\n    .. code-block:: jinja\n\n        {{ 'yes' | to_bool }}\n\n    will be rendered as:\n\n    .. code-block:: text\n\n        True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def masked_rec_array_to_mgr(data, index, columns, dtype, copy):\n    \"\"\"\n    \n    \"\"\"\n\n    # essentially process a record array then fill it\n    fill_value = data.fill_value\n    fdata = ma.getdata(data)\n    if index is None:\n        index = get_names_from_index(fdata)\n        if index is None:\n            index = ibase.default_index(len(data))\n    index = ensure_index(index)\n\n    if columns is not None:\n        columns = ensure_index(columns)\n    arrays, arr_columns = to_arrays(fdata, columns)\n\n    # fill if needed\n    new_arrays = []\n    for fv, arr, col in zip(fill_value, arrays, arr_columns):\n        mask = ma.getmaskarray(data[col])\n        if mask.any():\n            arr, fv = maybe_upcast(arr, fill_value=fv, copy=True)\n            arr[mask] = fv\n        new_arrays.append(arr)\n\n    # create the manager\n    arrays, arr_columns = reorder_arrays(new_arrays, arr_columns, columns)\n    if columns is None:\n        columns = arr_columns\n\n    mgr = arrays_to_mgr(arrays, arr_columns, index, columns, dtype)\n\n    if copy:\n        mgr = mgr.copy()\n    return mgr", "output": "Extract from a masked rec array and create the manager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_running():\n    '''\n    \n    '''\n    ret = []\n    for line in _machinectl('list')['stdout'].splitlines():\n        try:\n            ret.append(line.split()[0])\n        except IndexError:\n            pass\n    return sorted(ret)", "output": "Lists running nspawn containers\n\n    .. note::\n\n        ``nspawn.list`` also works to list running containers\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion nspawn.list_running\n        salt myminion nspawn.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _nodetool(cmd):\n    '''\n    \n    '''\n    nodetool = __salt__['config.option']('cassandra.nodetool')\n    host = __salt__['config.option']('cassandra.host')\n    return __salt__['cmd.run_stdout']('{0} -h {1} {2}'.format(nodetool, host, cmd))", "output": "Internal cassandra nodetool wrapper. Some functions are not\n    available via pycassa so we must rely on nodetool.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def instance_norm(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs, {'epsilon' : 'eps'})\n    new_attrs['eps'] = attrs.get('epsilon', 1e-5)\n    return 'InstanceNorm', new_attrs, inputs", "output": "Instance Normalization.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward_backward(self, x):\n        \"\"\"\"\"\"\n        (src_seq, tgt_seq, src_valid_length, tgt_valid_length), batch_size = x\n        with mx.autograd.record():\n            out, _ = self._model(src_seq, tgt_seq[:, :-1],\n                                 src_valid_length, tgt_valid_length - 1)\n            smoothed_label = self._label_smoothing(tgt_seq[:, 1:])\n            ls = self._loss(out, smoothed_label, tgt_valid_length - 1).sum()\n            ls = (ls * (tgt_seq.shape[1] - 1)) / batch_size / self._rescale_loss\n        ls.backward()\n        return ls", "output": "Perform forward and backward computation for a batch of src seq and dst seq", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def daemonize(redirect_out=True):\n    '''\n    \n    '''\n    # Avoid circular import\n    import salt.utils.crypt\n    try:\n        pid = os.fork()\n        if pid > 0:\n            # exit first parent\n            salt.utils.crypt.reinit_crypto()\n            os._exit(salt.defaults.exitcodes.EX_OK)\n    except OSError as exc:\n        log.error('fork #1 failed: %s (%s)', exc.errno, exc)\n        sys.exit(salt.defaults.exitcodes.EX_GENERIC)\n\n    # decouple from parent environment\n    os.chdir('/')\n    # noinspection PyArgumentList\n    os.setsid()\n    os.umask(0o022)  # pylint: disable=blacklisted-function\n\n    # do second fork\n    try:\n        pid = os.fork()\n        if pid > 0:\n            salt.utils.crypt.reinit_crypto()\n            sys.exit(salt.defaults.exitcodes.EX_OK)\n    except OSError as exc:\n        log.error('fork #2 failed: %s (%s)', exc.errno, exc)\n        sys.exit(salt.defaults.exitcodes.EX_GENERIC)\n\n    salt.utils.crypt.reinit_crypto()\n\n    # A normal daemonization redirects the process output to /dev/null.\n    # Unfortunately when a python multiprocess is called the output is\n    # not cleanly redirected and the parent process dies when the\n    # multiprocessing process attempts to access stdout or err.\n    if redirect_out:\n        with salt.utils.files.fopen('/dev/null', 'r+') as dev_null:\n            # Redirect python stdin/out/err\n            # and the os stdin/out/err which can be different\n            os.dup2(dev_null.fileno(), sys.stdin.fileno())\n            os.dup2(dev_null.fileno(), sys.stdout.fileno())\n            os.dup2(dev_null.fileno(), sys.stderr.fileno())\n            os.dup2(dev_null.fileno(), 0)\n            os.dup2(dev_null.fileno(), 1)\n            os.dup2(dev_null.fileno(), 2)", "output": "Daemonize a process", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def catalog(self):\n        \"\"\"\n        \"\"\"\n        from pyspark.sql.catalog import Catalog\n        if not hasattr(self, \"_catalog\"):\n            self._catalog = Catalog(self)\n        return self._catalog", "output": "Interface through which the user may create, drop, alter or query underlying\n        databases, tables, functions etc.\n\n        :return: :class:`Catalog`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_mp4(from_idx, to_idx, _params):\n    \"\"\"\n    \n    \"\"\"\n    succ = set()\n    fail = set()\n    for idx in range(from_idx, to_idx):\n        name = 's' + str(idx)\n        save_folder = '{src_path}/{nm}'.format(src_path=_params['src_path'], nm=name)\n        if idx == 0 or os.path.isdir(save_folder):\n            continue\n        script = \"http://spandh.dcs.shef.ac.uk/gridcorpus/{nm}/video/{nm}.mpg_vcd.zip\".format( \\\n                    nm=name)\n        down_sc = 'cd {src_path} && curl {script} --output {nm}.mpg_vcd.zip && \\\n                    unzip {nm}.mpg_vcd.zip'.format(script=script,\n                                                   nm=name,\n                                                   src_path=_params['src_path'])\n        try:\n            print(down_sc)\n            os.system(down_sc)\n            succ.add(idx)\n        except OSError as error:\n            print(error)\n            fail.add(idx)\n    return (succ, fail)", "output": "download mp4s", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\"\"\"\n        self.curr_idx = 0\n        #shuffle data in each bucket\n        random.shuffle(self.idx)\n        for i, buck in enumerate(self.sentences):\n            self.indices[i], self.sentences[i], self.characters[i], self.label[i] = shuffle(self.indices[i],\n                                                                                            self.sentences[i],\n                                                                                            self.characters[i],\n                                                                                            self.label[i])\n\n        self.ndindex = []\n        self.ndsent = []\n        self.ndchar = []\n        self.ndlabel = []\n\n        #for each bucket of data\n        for i, buck in enumerate(self.sentences):\n            #append the lists with an array\n            self.ndindex.append(ndarray.array(self.indices[i], dtype=self.dtype))\n            self.ndsent.append(ndarray.array(self.sentences[i], dtype=self.dtype))\n            self.ndchar.append(ndarray.array(self.characters[i], dtype=self.dtype))\n            self.ndlabel.append(ndarray.array(self.label[i], dtype=self.dtype))", "output": "Resets the iterator to the beginning of the data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dropna(self, how='any', thresh=None, subset=None):\n        \"\"\"\n        \"\"\"\n        if how is not None and how not in ['any', 'all']:\n            raise ValueError(\"how ('\" + how + \"') should be 'any' or 'all'\")\n\n        if subset is None:\n            subset = self.columns\n        elif isinstance(subset, basestring):\n            subset = [subset]\n        elif not isinstance(subset, (list, tuple)):\n            raise ValueError(\"subset should be a list or tuple of column names\")\n\n        if thresh is None:\n            thresh = len(subset) if how == 'any' else 1\n\n        return DataFrame(self._jdf.na().drop(thresh, self._jseq(subset)), self.sql_ctx)", "output": "Returns a new :class:`DataFrame` omitting rows with null values.\n        :func:`DataFrame.dropna` and :func:`DataFrameNaFunctions.drop` are aliases of each other.\n\n        :param how: 'any' or 'all'.\n            If 'any', drop a row if it contains any nulls.\n            If 'all', drop a row only if all its values are null.\n        :param thresh: int, default None\n            If specified, drop rows that have less than `thresh` non-null values.\n            This overwrites the `how` parameter.\n        :param subset: optional list of column names to consider.\n\n        >>> df4.na.drop().show()\n        +---+------+-----+\n        |age|height| name|\n        +---+------+-----+\n        | 10|    80|Alice|\n        +---+------+-----+", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_update_available(self):\n        \"\"\"\n        \"\"\"\n        # Don't perform any check for development versions\n        if 'dev' in self.version:\n            return (False, latest_release)\n\n        # Filter releases\n        if is_stable_version(self.version):\n            releases = [r for r in self.releases if is_stable_version(r)]\n        else:\n            releases = [r for r in self.releases\n                        if not is_stable_version(r) or r in self.version]\n\n        latest_release = releases[-1]\n\n        return (check_version(self.version, latest_release, '<'),\n                latest_release)", "output": "Checks if there is an update available.\n\n        It takes as parameters the current version of Spyder and a list of\n        valid cleaned releases in chronological order.\n        Example: ['2.3.2', '2.3.3' ...] or with github ['2.3.4', '2.3.3' ...]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_principal(name, enctypes=None):\n    '''\n    \n    '''\n    ret = {}\n\n    krb_cmd = 'addprinc -randkey'\n\n    if enctypes:\n        krb_cmd += ' -e {0}'.format(enctypes)\n\n    krb_cmd += ' {0}'.format(name)\n\n    cmd = __execute_kadmin(krb_cmd)\n\n    if cmd['retcode'] != 0 or cmd['stderr']:\n        if not cmd['stderr'].splitlines()[-1].startswith('WARNING:'):\n            ret['comment'] = cmd['stderr'].splitlines()[-1]\n            ret['result'] = False\n\n            return ret\n\n    return True", "output": "Create Principal\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'kdc.example.com' kerberos.create_principal host/example.com", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _load_params(params, logger=logging):\n    \"\"\"\n    \"\"\"\n    if isinstance(params, str):\n        cur_path = os.path.dirname(os.path.realpath(__file__))\n        param_file_path = os.path.join(cur_path, params)\n        logger.info('Loading params from file %s' % param_file_path)\n        save_dict = nd_load(param_file_path)\n        arg_params = {}\n        aux_params = {}\n        for k, v in save_dict.items():\n            tp, name = k.split(':', 1)\n            if tp == 'arg':\n                arg_params[name] = v\n            if tp == 'aux':\n                aux_params[name] = v\n        return arg_params, aux_params\n    elif isinstance(params, (tuple, list)) and len(params) == 2:\n        return params[0], params[1]\n    else:\n        raise ValueError('Unsupported params provided. Must be either a path to the param file or'\n                         ' a pair of dictionaries representing arg_params and aux_params')", "output": "Given a str as a path to the .params file or a pair of params,\n    returns two dictionaries representing arg_params and aux_params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self):\n        \"\"\n        if not self.removed:\n            self.hook.remove()\n            self.removed=True", "output": "Remove the hook from the model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shuffle_step(entries, step):\n    '''\n    \n    '''\n    answer = []\n    for i in range(0, len(entries), step):\n        sub = entries[i:i+step]\n        shuffle(sub)\n        answer += sub\n    return answer", "output": "Shuffle the step", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformerpp_base_8l_8h_big_cond_dr03_dan():\n  \"\"\"\"\"\"\n  hparams = imagetransformerpp_sep_channels_8l_8h()\n  hparams.hidden_size = 512\n  hparams.num_heads = 8\n  hparams.filter_size = 2048\n  hparams.batch_size = 4\n  hparams.max_length = 3075\n  hparams.layer_prepostprocess_dropout = 0.3\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.summarize_grads = True\n  hparams.learning_rate = 0.01\n  return hparams", "output": "big 1d model for conditional image generation.2.99 on cifar10.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _enum_elements(name, server=None):\n    '''\n    \n    '''\n    elements = []\n    data = _api_get(name, server)\n\n    if any(data['extraProperties']['childResources']):\n        for element in data['extraProperties']['childResources']:\n            elements.append(element)\n        return elements\n    return None", "output": "Enum elements", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample(path, start, length):\n    \"\"\"\n    \n    \"\"\"\n    # initialize vocabulary and sequence length\n    param.seq_len = 1\n    ds = CharRNNData(param.corpus, 100000)\n\n    pred = OfflinePredictor(PredictConfig(\n        model=Model(),\n        session_init=SaverRestore(path),\n        input_names=['input', 'c0', 'h0', 'c1', 'h1'],\n        output_names=['prob', 'last_state']))\n\n    # feed the starting sentence\n    initial = np.zeros((1, param.rnn_size))\n    for c in start[:-1]:\n        x = np.array([[ds.char2idx[c]]], dtype='int32')\n        _, state = pred(x, initial, initial, initial, initial)\n\n    def pick(prob):\n        t = np.cumsum(prob)\n        s = np.sum(prob)\n        return(int(np.searchsorted(t, np.random.rand(1) * s)))\n\n    # generate more\n    ret = start\n    c = start[-1]\n    for k in range(length):\n        x = np.array([[ds.char2idx[c]]], dtype='int32')\n        prob, state = pred(x, state[0, 0], state[0, 1], state[1, 0], state[1, 1])\n        c = ds.chars[pick(prob[0])]\n        ret += c\n    print(ret)", "output": ":param path: path to the model\n    :param start: a `str`. the starting characters\n    :param length: a `int`. the length of text to generate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_feature(value):\n  \"\"\"\"\"\"\n  if isinstance(value, FeatureConnector):\n    return value\n  elif utils.is_dtype(value):  # tf.int32, tf.string,...\n    return Tensor(shape=(), dtype=tf.as_dtype(value))\n  elif isinstance(value, dict):\n    return FeaturesDict(value)\n  else:\n    raise ValueError('Feature not supported: {}'.format(value))", "output": "Convert the given value to Feature if necessary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_multi(self, entities):\n        \"\"\"\n        \"\"\"\n        if isinstance(entities, Entity):\n            raise ValueError(\"Pass a sequence of entities\")\n\n        if not entities:\n            return\n\n        current = self.current_batch\n        in_batch = current is not None\n\n        if not in_batch:\n            current = self.batch()\n            current.begin()\n\n        for entity in entities:\n            current.put(entity)\n\n        if not in_batch:\n            current.commit()", "output": "Save entities in the Cloud Datastore.\n\n        :type entities: list of :class:`google.cloud.datastore.entity.Entity`\n        :param entities: The entities to be saved to the datastore.\n\n        :raises: :class:`ValueError` if ``entities`` is a single entity.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_coeffs(orig_pts:Points, targ_pts:Points)->Tensor:\n    \"\"\n    matrix = []\n    #The equations we'll need to solve.\n    for p1, p2 in zip(targ_pts, orig_pts):\n        matrix.append([p1[0], p1[1], 1, 0, 0, 0, -p2[0]*p1[0], -p2[0]*p1[1]])\n        matrix.append([0, 0, 0, p1[0], p1[1], 1, -p2[1]*p1[0], -p2[1]*p1[1]])\n\n    A = FloatTensor(matrix)\n    B = FloatTensor(orig_pts).view(8, 1)\n    #The 8 scalars we seek are solution of AX = B\n    return _solve_func(B,A)[0][:,0]", "output": "Find 8 coeff mentioned [here](https://web.archive.org/web/20150222120106/xenia.media.mit.edu/~cwren/interpolator/).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sorted(self, wantdirs=False):\n        \"\"\"\n        \n        \"\"\"\n\n        def add_dir(dirs, d):\n            dirs.add(d)\n            logger.debug('add_dir added %s', d)\n            if d != self.base:\n                parent, _ = os.path.split(d)\n                assert parent not in ('', '/')\n                add_dir(dirs, parent)\n\n        result = set(self.files)    # make a copy!\n        if wantdirs:\n            dirs = set()\n            for f in result:\n                add_dir(dirs, os.path.dirname(f))\n            result |= dirs\n        return [os.path.join(*path_tuple) for path_tuple in\n                sorted(os.path.split(path) for path in result)]", "output": "Return sorted files in directory order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_policy(policyName, policyDocument,\n            region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        if not isinstance(policyDocument, string_types):\n            policyDocument = salt.utils.json.dumps(policyDocument)\n        policy = conn.create_policy(policyName=policyName,\n                                    policyDocument=policyDocument)\n        if policy:\n            log.info('The newly created policy version is %s', policy['policyVersionId'])\n\n            return {'created': True, 'versionId': policy['policyVersionId']}\n        else:\n            log.warning('Policy was not created')\n            return {'created': False}\n    except ClientError as e:\n        return {'created': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Given a valid config, create a policy.\n\n    Returns {created: true} if the policy was created and returns\n    {created: False} if the policy was not created.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.create_policy my_policy \\\\\n              '{\"Version\":\"2015-12-12\",\\\\\n              \"Statement\":[{\"Effect\":\"Allow\",\\\\\n                            \"Action\":[\"iot:Publish\"],\\\\\n                            \"Resource\":[\"arn:::::topic/foo/bar\"]}]}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _convert_to_border(cls, border_dict):\n        \"\"\"\n        \n        \"\"\"\n\n        from openpyxl.styles import Border\n\n        _border_key_map = {\n            'diagonalup': 'diagonalUp',\n            'diagonaldown': 'diagonalDown',\n        }\n\n        border_kwargs = {}\n        for k, v in border_dict.items():\n            if k in _border_key_map:\n                k = _border_key_map[k]\n            if k == 'color':\n                v = cls._convert_to_color(v)\n            if k in ['left', 'right', 'top', 'bottom', 'diagonal']:\n                v = cls._convert_to_side(v)\n            border_kwargs[k] = v\n\n        return Border(**border_kwargs)", "output": "Convert ``border_dict`` to an openpyxl v2 Border object\n        Parameters\n        ----------\n        border_dict : dict\n            A dict with zero or more of the following keys (or their synonyms).\n                'left'\n                'right'\n                'top'\n                'bottom'\n                'diagonal'\n                'diagonal_direction'\n                'vertical'\n                'horizontal'\n                'diagonalUp' ('diagonalup')\n                'diagonalDown' ('diagonaldown')\n                'outline'\n        Returns\n        -------\n        border : openpyxl.styles.Border", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(domain, key, user=None):\n    '''\n    \n\n    '''\n    cmd = 'defaults read \"{0}\" \"{1}\"'.format(domain, key)\n    return __salt__['cmd.run'](cmd, runas=user)", "output": "Write a default to the system\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' macdefaults.read com.apple.CrashReporter DialogType\n\n        salt '*' macdefaults.read NSGlobalDomain ApplePersistence\n\n    domain\n        The name of the domain to read from\n\n    key\n        The key of the given domain to read from\n\n    user\n        The user to write the defaults to", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def diff(self, n, axis=0):\n        \"\"\"\n        \"\"\"\n        if axis == 0:\n            # Cannot currently calculate diff across multiple blocks since this\n            # function is invoked via apply\n            raise NotImplementedError\n        new_values = (self.values - self.shift(n, axis=axis)[0].values).asi8\n\n        # Reshape the new_values like how algos.diff does for timedelta data\n        new_values = new_values.reshape(1, len(new_values))\n        new_values = new_values.astype('timedelta64[ns]')\n        return [TimeDeltaBlock(new_values, placement=self.mgr_locs.indexer)]", "output": "1st discrete difference\n\n        Parameters\n        ----------\n        n : int, number of periods to diff\n        axis : int, axis to diff upon. default 0\n\n        Return\n        ------\n        A list with a new TimeDeltaBlock.\n\n        Note\n        ----\n        The arguments here are mimicking shift so they are called correctly\n        by apply.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def minimum(lhs, rhs):\n    \"\"\"\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_minimum,\n        lambda x, y: x if x < y else y,\n        _internal._minimum_scalar,\n        None)", "output": "Returns element-wise minimum of the input arrays with broadcasting.\n\n    Equivalent to ``mx.nd.broadcast_minimum(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First array to be compared.\n    rhs : scalar or mxnet.ndarray.array\n         Second array to be compared. If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        The element-wise minimum of the input arrays.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> mx.nd.minimum(x, 2).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.minimum(x, y).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.minimum(z, y).asnumpy()\n    array([[ 0.,  0.],\n           [ 0.,  1.]], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def MapDecoder(field_descriptor, new_default, is_message_map):\n  \"\"\"\"\"\"\n\n  key = field_descriptor\n  tag_bytes = encoder.TagBytes(field_descriptor.number,\n                               wire_format.WIRETYPE_LENGTH_DELIMITED)\n  tag_len = len(tag_bytes)\n  local_DecodeVarint = _DecodeVarint\n  # Can't read _concrete_class yet; might not be initialized.\n  message_type = field_descriptor.message_type\n\n  def DecodeMap(buffer, pos, end, message, field_dict):\n    submsg = message_type._concrete_class()\n    value = field_dict.get(key)\n    if value is None:\n      value = field_dict.setdefault(key, new_default(message))\n    while 1:\n      # Read length.\n      (size, pos) = local_DecodeVarint(buffer, pos)\n      new_pos = pos + size\n      if new_pos > end:\n        raise _DecodeError('Truncated message.')\n      # Read sub-message.\n      submsg.Clear()\n      if submsg._InternalParse(buffer, pos, new_pos) != new_pos:\n        # The only reason _InternalParse would return early is if it\n        # encountered an end-group tag.\n        raise _DecodeError('Unexpected end-group tag.')\n\n      if is_message_map:\n        value[submsg.key].MergeFrom(submsg.value)\n      else:\n        value[submsg.key] = submsg.value\n\n      # Predict that the next tag is another copy of the same repeated field.\n      pos = new_pos + tag_len\n      if buffer[new_pos:pos] != tag_bytes or new_pos == end:\n        # Prediction failed.  Return.\n        return new_pos\n\n  return DecodeMap", "output": "Returns a decoder for a map field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_summary_metadata(display_name, description, num_thresholds):\n  \"\"\"\n  \"\"\"\n  pr_curve_plugin_data = plugin_data_pb2.PrCurvePluginData(\n      version=PROTO_VERSION, num_thresholds=num_thresholds)\n  content = pr_curve_plugin_data.SerializeToString()\n  return summary_pb2.SummaryMetadata(\n      display_name=display_name,\n      summary_description=description,\n      plugin_data=summary_pb2.SummaryMetadata.PluginData(\n          plugin_name=PLUGIN_NAME,\n          content=content))", "output": "Create a `summary_pb2.SummaryMetadata` proto for pr_curves plugin data.\n\n  Arguments:\n    display_name: The display name used in TensorBoard.\n    description: The description to show in TensorBoard.\n    num_thresholds: The number of thresholds to use for PR curves.\n\n  Returns:\n    A `summary_pb2.SummaryMetadata` protobuf object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def error_msg_from_exception(e):\n    \"\"\"\n    \"\"\"\n    msg = ''\n    if hasattr(e, 'message'):\n        if isinstance(e.message, dict):\n            msg = e.message.get('message')\n        elif e.message:\n            msg = '{}'.format(e.message)\n    return msg or '{}'.format(e)", "output": "Translate exception into error message\n\n    Database have different ways to handle exception. This function attempts\n    to make sense of the exception object and construct a human readable\n    sentence.\n\n    TODO(bkyryliuk): parse the Presto error message from the connection\n                     created via create_engine.\n    engine = create_engine('presto://localhost:3506/silver') -\n      gives an e.message as the str(dict)\n    presto.connect('localhost', port=3506, catalog='silver') - as a dict.\n    The latter version is parsed correctly by this function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _connect(**kwargs):\n    '''\n    \n    '''\n    connargs = {}\n    for name in ['uri', 'server', 'port', 'tls', 'no_verify', 'binddn',\n                 'bindpw', 'anonymous']:\n        connargs[name] = _config(name, **kwargs)\n\n    return _LDAPConnection(**connargs).ldap", "output": "Instantiate LDAP Connection class and return an LDAP connection object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pprint(self, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        pprint.pprint(self.asList(), *args, **kwargs)", "output": "Pretty-printer for parsed results as a list, using the\n        `pprint <https://docs.python.org/3/library/pprint.html>`_ module.\n        Accepts additional positional or keyword args as defined for\n        `pprint.pprint <https://docs.python.org/3/library/pprint.html#pprint.pprint>`_ .\n\n        Example::\n\n            ident = Word(alphas, alphanums)\n            num = Word(nums)\n            func = Forward()\n            term = ident | num | Group('(' + func + ')')\n            func <<= ident + Group(Optional(delimitedList(term)))\n            result = func.parseString(\"fna a,b,(fnb c,d,200),100\")\n            result.pprint(width=40)\n\n        prints::\n\n            ['fna',\n             ['a',\n              'b',\n              ['(', 'fnb', ['c', 'd', '200'], ')'],\n              '100']]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _path_condition_name(self, api_id, path):\n        \"\"\"\n        \n        \"\"\"\n        # only valid characters for CloudFormation logical id are [A-Za-z0-9], but swagger paths can contain\n        # slashes and curly braces for templated params, e.g., /foo/{customerId}. So we'll replace\n        # non-alphanumeric characters.\n        path_logical_id = path.replace('/', 'SLASH').replace('{', 'OB').replace('}', 'CB')\n        return '{}{}PathCondition'.format(api_id, path_logical_id)", "output": "Generate valid condition logical id from the given API logical id and swagger resource path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_request_body(fn):\n    '''\n    \n    '''\n    @functools.wraps(fn)\n    def wrapped(*args, **kwargs):  # pylint: disable=C0111\n        if cherrypy.request.process_request_body is not False:\n            fn(*args, **kwargs)\n    return wrapped", "output": "A decorator to skip a processor function if process_request_body is False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode(self, s):\n    \"\"\"\n    \"\"\"\n    try:\n      import matplotlib.image as im  # pylint: disable=g-import-not-at-top\n    except ImportError as e:\n      tf.logging.warning(\n          \"Reading an image requires matplotlib to be installed: %s\", e)\n      raise NotImplementedError(\"Image reading not implemented.\")\n    return im.imread(s)", "output": "Transform a string with a filename into a list of RGB integers.\n\n    Args:\n      s: path to the file with an image.\n\n    Returns:\n      ids: list of integers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_column(self, data, column_name=\"\", inplace=False):\n        \"\"\"\n        \n        \"\"\"\n        # Check type for pandas dataframe or SArray?\n        if not isinstance(data, SArray):\n            raise TypeError(\"Must give column as SArray\")\n        if not isinstance(column_name, str):\n            raise TypeError(\"Invalid column name: must be str\")\n\n        if inplace:\n            self.__is_dirty__ = True\n            with cython_context():\n                if self._is_vertex_frame():\n                    graph_proxy = self.__graph__.__proxy__.add_vertex_field(data.__proxy__, column_name)\n                    self.__graph__.__proxy__ = graph_proxy\n                elif self._is_edge_frame():\n                    graph_proxy = self.__graph__.__proxy__.add_edge_field(data.__proxy__, column_name)\n                    self.__graph__.__proxy__ = graph_proxy\n            return self\n        else:\n            return super(GFrame, self).add_column(data, column_name, inplace=inplace)", "output": "Adds the specified column to this SFrame.  The number of elements in\n        the data given must match every other column of the SFrame.\n\n        If inplace == False (default) this operation does not modify the\n        current SFrame, returning a new SFrame.\n\n        If inplace == True, this operation modifies the current\n        SFrame, returning self.\n\n        Parameters\n        ----------\n        data : SArray\n            The 'column' of data.\n\n        column_name : string\n            The name of the column. If no name is given, a default name is chosen.\n\n        inplace : bool, optional. Defaults to False.\n            Whether the SFrame is modified in place.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _memoize(f):\n  \"\"\"\"\"\"\n  nothing = object()  # Unique \"no value\" sentinel object.\n  cache = {}\n  # Use a reentrant lock so that if f references the resulting wrapper we die\n  # with recursion depth exceeded instead of deadlocking.\n  lock = threading.RLock()\n  @functools.wraps(f)\n  def wrapper(arg):\n    if cache.get(arg, nothing) is nothing:\n      with lock:\n        if cache.get(arg, nothing) is nothing:\n          cache[arg] = f(arg)\n    return cache[arg]\n  return wrapper", "output": "Memoizing decorator for f, which must have exactly 1 hashable argument.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_close(self):\n        ''' \n\n        '''\n        log.info('WebSocket connection closed: code=%s, reason=%r', self.close_code, self.close_reason)\n        if self.connection is not None:\n            self.application.client_lost(self.connection)", "output": "Clean up when the connection is closed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lossless_float_to_int(funcname, func, argname, arg):\n    \"\"\"\n    \n    \"\"\"\n    if not isinstance(arg, float):\n        return arg\n\n    arg_as_int = int(arg)\n    if arg == arg_as_int:\n        warnings.warn(\n            \"{f} expected an int for argument {name!r}, but got float {arg}.\"\n            \" Coercing to int.\".format(\n                f=funcname,\n                name=argname,\n                arg=arg,\n            ),\n        )\n        return arg_as_int\n\n    raise TypeError(arg)", "output": "A preprocessor that coerces integral floats to ints.\n\n    Receipt of non-integral floats raises a TypeError.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_MFI(DataFrame, N=14):\n    \"\"\"\n    \n    \"\"\"\n    C = DataFrame['close']\n    H = DataFrame['high']\n    L = DataFrame['low']\n    VOL = DataFrame['volume']\n    TYP = (C + H + L) / 3\n    V1 = SUM(IF(TYP > REF(TYP, 1), TYP * VOL, 0), N) / \\\n        SUM(IF(TYP < REF(TYP, 1), TYP * VOL, 0), N)\n    mfi = 100 - (100 / (1 + V1))\n    DICT = {'MFI': mfi}\n\n    return pd.DataFrame(DICT)", "output": "\u8d44\u91d1\u6307\u6807\n    TYP := (HIGH + LOW + CLOSE)/3;\n    V1:=SUM(IF(TYP>REF(TYP,1),TYP*VOL,0),N)/SUM(IF(TYP<REF(TYP,1),TYP*VOL,0),N);\n    MFI:100-(100/(1+V1));\n    \u8d4b\u503c: (\u6700\u9ad8\u4ef7 + \u6700\u4f4e\u4ef7 + \u6536\u76d8\u4ef7)/3\n    V1\u8d4b\u503c:\u5982\u679cTYP>1\u65e5\u524d\u7684TYP,\u8fd4\u56deTYP*\u6210\u4ea4\u91cf(\u624b),\u5426\u5219\u8fd4\u56de0\u7684N\u65e5\u7d2f\u548c/\u5982\u679cTYP<1\u65e5\u524d\u7684TYP,\u8fd4\u56deTYP*\u6210\u4ea4\u91cf(\u624b),\u5426\u5219\u8fd4\u56de0\u7684N\u65e5\u7d2f\u548c\n    \u8f93\u51fa\u8d44\u91d1\u6d41\u91cf\u6307\u6807:100-(100/(1+V1))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def items(self):\n        \"\"\"\"\"\"\n        result = [(key, self._mapping[key]) for key in list(self._queue)]\n        result.reverse()\n        return result", "output": "Return a list of items.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_author(self, *, name, url=EmptyEmbed, icon_url=EmptyEmbed):\n        \"\"\"\n        \"\"\"\n\n        self._author = {\n            'name': str(name)\n        }\n\n        if url is not EmptyEmbed:\n            self._author['url'] = str(url)\n\n        if icon_url is not EmptyEmbed:\n            self._author['icon_url'] = str(icon_url)\n\n        return self", "output": "Sets the author for the embed content.\n\n        This function returns the class instance to allow for fluent-style\n        chaining.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The name of the author.\n        url: :class:`str`\n            The URL for the author.\n        icon_url: :class:`str`\n            The URL of the author icon. Only HTTP(S) is supported.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reraise(additional_msg):\n  \"\"\"\"\"\"\n  exc_type, exc_value, exc_traceback = sys.exc_info()\n  msg = str(exc_value) + \"\\n\" + additional_msg\n  six.reraise(exc_type, exc_type(msg), exc_traceback)", "output": "Reraise an exception with an additional message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _SetPath(self, path):\n    \"\"\"\n    \"\"\"\n    old_path = self._path\n    if old_path and not io_wrapper.IsCloudPath(old_path):\n      try:\n        # We're done with the path, so store its size.\n        size = tf.io.gfile.stat(old_path).length\n        logger.debug('Setting latest size of %s to %d', old_path, size)\n        self._finalized_sizes[old_path] = size\n      except tf.errors.OpError as e:\n        logger.error('Unable to get size of %s: %s', old_path, e)\n\n    self._path = path\n    self._loader = self._loader_factory(path)", "output": "Sets the current path to watch for new events.\n\n    This also records the size of the old path, if any. If the size can't be\n    found, an error is logged.\n\n    Args:\n      path: The full path of the file to watch.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_user(name, **client_args):\n    '''\n    \n    '''\n    if not user_exists(name, **client_args):\n        log.info('User \\'%s\\' does not exist', name)\n        return False\n\n    client = _client(**client_args)\n    client.drop_user(name)\n\n    return True", "output": "Remove a user.\n\n    name\n        Name of the user to remove\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' influxdb.remove_user <name>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_tf_checkpoint(path):\n    \"\"\"\"\"\"\n    from tensorflow.python import pywrap_tensorflow\n    tensors = {}\n    reader = pywrap_tensorflow.NewCheckpointReader(path)\n    var_to_shape_map = reader.get_variable_to_shape_map()\n    for key in sorted(var_to_shape_map):\n        tensor = reader.get_tensor(key)\n        tensors[key] = tensor\n    return tensors", "output": "read tensorflow checkpoint", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bias(self, arr:Collection, is_item:bool=True):\n        \"\"\n        idx = self.get_idx(arr, is_item)\n        m = self.model\n        layer = m.i_bias if is_item else m.u_bias\n        return layer(idx).squeeze()", "output": "Bias for item or user (based on `is_item`) for all in `arr`. (Set model to `cpu` and no grad.)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def indexables(self):\n        \"\"\"  \"\"\"\n        if self._indexables is None:\n\n            d = self.description\n\n            # the index columns is just a simple index\n            self._indexables = [GenericIndexCol(name='index', axis=0)]\n\n            for i, n in enumerate(d._v_names):\n\n                dc = GenericDataIndexableCol(\n                    name=n, pos=i, values=[n], version=self.version)\n                self._indexables.append(dc)\n\n        return self._indexables", "output": "create the indexables from the table description", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def install(cert,\n            password,\n            keychain=\"/Library/Keychains/System.keychain\",\n            allow_any=False,\n            keychain_password=None):\n    '''\n    \n    '''\n    if keychain_password is not None:\n        unlock_keychain(keychain, keychain_password)\n\n    cmd = 'security import {0} -P {1} -k {2}'.format(cert, password, keychain)\n    if allow_any:\n        cmd += ' -A'\n    return __salt__['cmd.run'](cmd)", "output": "Install a certificate\n\n    cert\n        The certificate to install\n\n    password\n        The password for the certificate being installed formatted in the way\n        described for openssl command in the PASS PHRASE ARGUMENTS section.\n\n        Note: The password given here will show up as plaintext in the job returned\n        info.\n\n    keychain\n        The keychain to install the certificate to, this defaults to\n        /Library/Keychains/System.keychain\n\n    allow_any\n        Allow any application to access the imported certificate without warning\n\n    keychain_password\n        If your keychain is likely to be locked pass the password and it will be unlocked\n        before running the import\n\n        Note: The password given here will show up as plaintext in the returned job\n        info.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keychain.install test.p12 test123", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_template_data(template_file):\n    \"\"\"\n    \n    \"\"\"\n\n    if not pathlib.Path(template_file).exists():\n        raise ValueError(\"Template file not found at {}\".format(template_file))\n\n    with open(template_file, 'r') as fp:\n        try:\n            return yaml_parse(fp.read())\n        except (ValueError, yaml.YAMLError) as ex:\n            raise ValueError(\"Failed to parse template: {}\".format(str(ex)))", "output": "Read the template file, parse it as JSON/YAML and return the template as a dictionary.\n\n    Parameters\n    ----------\n    template_file : string\n        Path to the template to read\n\n    Returns\n    -------\n    Template data as a dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_factors(n):\n    \"\"\"\n    \"\"\"\n\n    def factor(n, i, combi, res):\n        \"\"\"[summary]\n        helper function\n\n        Arguments:\n            n {[int]} -- [number]\n            i {[int]} -- [to tested divisor]\n            combi {[list]} -- [catch divisors]\n            res {[list]} -- [all factors of the number n]\n        \n        Returns:\n            [list] -- [res]\n        \"\"\"\n\n        while i * i <= n:\n            if n % i == 0:\n                res += combi + [i, int(n/i)],\n                factor(n/i, i, combi+[i], res)\n            i += 1\n        return res\n    return factor(n, 2, [], [])", "output": "[summary]\n    \n    Arguments:\n        n {[int]} -- [to analysed number]\n    \n    Returns:\n        [list of lists] -- [all factors of the number n]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait_input(self, prompt=''):\r\n        \"\"\"\"\"\"\r\n        self.new_prompt(prompt)\r\n        self.setFocus()\r\n        self.input_mode = True\r\n        self.input_loop = QEventLoop()\r\n        self.input_loop.exec_()\r\n        self.input_loop = None", "output": "Wait for input (raw_input support)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_path(python):\n    \"\"\"\n    \"\"\"\n\n    python = Path(python).as_posix()\n    out, err = run(\n        [python, \"-c\", \"import json, sys; print(json.dumps(sys.path))\"], nospin=True\n    )\n    if out:\n        return json.loads(out)\n    else:\n        return []", "output": "Load the :mod:`sys.path` from the given python executable's environment as json\n\n    :param str python: Path to a valid python executable\n    :return: A python representation of the `sys.path` value of the given python executable.\n    :rtype: list\n\n    >>> load_path(\"/home/user/.virtualenvs/requirementslib-5MhGuG3C/bin/python\")\n    ['', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python37.zip', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python3.7', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python3.7/lib-dynload', '/home/user/.pyenv/versions/3.7.0/lib/python3.7', '/home/user/.virtualenvs/requirementslib-5MhGuG3C/lib/python3.7/site-packages', '/home/user/git/requirementslib/src']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def optionally(preprocessor):\n    \"\"\"\n    \"\"\"\n    @wraps(preprocessor)\n    def wrapper(func, argname, arg):\n        return arg if arg is None else preprocessor(func, argname, arg)\n\n    return wrapper", "output": "Modify a preprocessor to explicitly allow `None`.\n\n    Parameters\n    ----------\n    preprocessor : callable[callable, str, any -> any]\n        A preprocessor to delegate to when `arg is not None`.\n\n    Returns\n    -------\n    optional_preprocessor : callable[callable, str, any -> any]\n        A preprocessor that delegates to `preprocessor` when `arg is not None`.\n\n    Examples\n    --------\n    >>> def preprocessor(func, argname, arg):\n    ...     if not isinstance(arg, int):\n    ...         raise TypeError('arg must be int')\n    ...     return arg\n    ...\n    >>> @preprocess(a=optionally(preprocessor))\n    ... def f(a):\n    ...     return a\n    ...\n    >>> f(1)  # call with int\n    1\n    >>> f('a')  # call with not int\n    Traceback (most recent call last):\n       ...\n    TypeError: arg must be int\n    >>> f(None) is None  # call with explicit None\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stats_timing(stats_key, stats_logger):\n    \"\"\"\"\"\"\n    start_ts = now_as_float()\n    try:\n        yield start_ts\n    except Exception as e:\n        raise e\n    finally:\n        stats_logger.timing(stats_key, now_as_float() - start_ts)", "output": "Provide a transactional scope around a series of operations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def isID(self, elem, attr):\n        \"\"\" \"\"\"\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        if attr is None: attr__o = None\n        else: attr__o = attr._o\n        ret = libxml2mod.xmlIsID(self._o, elem__o, attr__o)\n        return ret", "output": "Determine whether an attribute is of type ID. In case we\n          have DTD(s) then this is done if DTD loading has been\n          requested. In the case of HTML documents parsed with the\n           HTML parser, then ID detection is done systematically.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _StructMessageToJsonObject(self, message):\n    \"\"\"\"\"\"\n    fields = message.fields\n    ret = {}\n    for key in fields:\n      ret[key] = self._ValueMessageToJsonObject(fields[key])\n    return ret", "output": "Converts Struct message according to Proto3 JSON Specification.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_system_date(newdate):\n    '''\n    \n    '''\n    fmts = ['%Y-%m-%d', '%m-%d-%Y', '%m-%d-%y',\n            '%m/%d/%Y', '%m/%d/%y', '%Y/%m/%d']\n    # Get date/time object from newdate\n    dt_obj = _try_parse_datetime(newdate, fmts)\n    if dt_obj is None:\n        return False\n\n    # Set time using set_system_date_time()\n    return set_system_date_time(years=dt_obj.year,\n                                months=dt_obj.month,\n                                days=dt_obj.day)", "output": "Set the Windows system date. Use <mm-dd-yy> format for the date.\n\n    Args:\n        newdate (str):\n            The date to set. Can be any of the following formats\n\n            - YYYY-MM-DD\n            - MM-DD-YYYY\n            - MM-DD-YY\n            - MM/DD/YYYY\n            - MM/DD/YY\n            - YYYY/MM/DD\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.set_system_date '03-28-13'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ConvertScalarFieldValue(value, field, require_str=False):\n  \"\"\"\n  \"\"\"\n  if field.cpp_type in _INT_TYPES:\n    return _ConvertInteger(value)\n  elif field.cpp_type in _FLOAT_TYPES:\n    return _ConvertFloat(value)\n  elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_BOOL:\n    return _ConvertBool(value, require_str)\n  elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_STRING:\n    if field.type == descriptor.FieldDescriptor.TYPE_BYTES:\n      return base64.b64decode(value)\n    else:\n      # Checking for unpaired surrogates appears to be unreliable,\n      # depending on the specific Python version, so we check manually.\n      if _UNPAIRED_SURROGATE_PATTERN.search(value):\n        raise ParseError('Unpaired surrogate')\n      return value\n  elif field.cpp_type == descriptor.FieldDescriptor.CPPTYPE_ENUM:\n    # Convert an enum value.\n    enum_value = field.enum_type.values_by_name.get(value, None)\n    if enum_value is None:\n      try:\n        number = int(value)\n        enum_value = field.enum_type.values_by_number.get(number, None)\n      except ValueError:\n        raise ParseError('Invalid enum value {0} for enum type {1}.'.format(\n            value, field.enum_type.full_name))\n      if enum_value is None:\n        raise ParseError('Invalid enum value {0} for enum type {1}.'.format(\n            value, field.enum_type.full_name))\n    return enum_value.number", "output": "Convert a single scalar field value.\n\n  Args:\n    value: A scalar value to convert the scalar field value.\n    field: The descriptor of the field to convert.\n    require_str: If True, the field value must be a str.\n\n  Returns:\n    The converted scalar field value\n\n  Raises:\n    ParseError: In case of convert problems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _expand_sources(sources):\n    '''\n    \n    '''\n    if sources is None:\n        return []\n    if isinstance(sources, six.string_types):\n        sources = [x.strip() for x in sources.split(',')]\n    elif isinstance(sources, (float, six.integer_types)):\n        sources = [six.text_type(sources)]\n    return [path\n            for source in sources\n            for path in _glob(source)]", "output": "Expands a user-provided specification of source files into a list of paths.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\"\"\"\n        self.alive.value = False\n        qsize = 0\n        try:\n            while True:\n                self.queue.get(timeout=0.1)\n                qsize += 1\n        except QEmptyExcept:\n            pass\n        print(\"Queue size on reset: {}\".format(qsize))\n        for i, p in enumerate(self.proc):\n            p.join()\n        self.proc.clear()", "output": "Resets the generator by stopping all processes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(conn=None):\n    '''\n    \n    '''\n    if not conn:\n        conn = get_conn()\n\n    all_images = []\n    # The list of public image projects can be found via:\n    #   % gcloud compute images list\n    # and looking at the \"PROJECT\" column in the output.\n    public_image_projects = (\n        'centos-cloud', 'coreos-cloud', 'debian-cloud', 'google-containers',\n        'opensuse-cloud', 'rhel-cloud', 'suse-cloud', 'ubuntu-os-cloud',\n        'windows-cloud'\n    )\n    for project in public_image_projects:\n        all_images.extend(conn.list_images(project))\n\n    # Finally, add the images in this current project last so that it overrides\n    # any image that also exists in any public project.\n    all_images.extend(conn.list_images())\n\n    ret = {}\n    for img in all_images:\n        ret[img.name] = {}\n        for attr in dir(img):\n            if attr.startswith('_'):\n                continue\n            ret[img.name][attr] = getattr(img, attr)\n    return ret", "output": "Return a dict of all available VM images on the cloud provider with\n    relevant data.\n\n    Note that for GCE, there are custom images within the project, but the\n    generic images are in other projects.  This returns a dict of images in\n    the project plus images in well-known public projects that provide supported\n    images, as listed on this page:\n    https://cloud.google.com/compute/docs/operating-systems/\n\n    If image names overlap, the image in the current project is used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info(vm, info_type='all', key='uuid'):\n    '''\n    \n    '''\n    ret = {}\n    if info_type not in ['all', 'block', 'blockstats', 'chardev', 'cpus', 'kvm', 'pci', 'spice', 'version', 'vnc']:\n        ret['Error'] = 'Requested info_type is not available'\n        return ret\n    if key not in ['uuid', 'alias', 'hostname']:\n        ret['Error'] = 'Key must be either uuid, alias or hostname'\n        return ret\n    vm = lookup('{0}={1}'.format(key, vm), one=True)\n    if 'Error' in vm:\n        return vm\n    # vmadm info <uuid> [type,...]\n    cmd = 'vmadm info {uuid} {type}'.format(\n        uuid=vm,\n        type=info_type\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = res['stderr'] if 'stderr' in res else _exit_status(retcode)\n        return ret\n    return salt.utils.json.loads(res['stdout'])", "output": "Lookup info on running kvm\n\n    vm : string\n        vm to be targeted\n    info_type : string [all|block|blockstats|chardev|cpus|kvm|pci|spice|version|vnc]\n        info type to return\n    key : string [uuid|alias|hostname]\n        value type of 'vm' parameter\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.info 186da9ab-7392-4f55-91a5-b8f1fe770543\n        salt '*' vmadm.info 186da9ab-7392-4f55-91a5-b8f1fe770543 vnc\n        salt '*' vmadm.info nacl key=alias\n        salt '*' vmadm.info nacl vnc key=alias", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_selected_cb(parents, combobox):\n    \"\"\"\n    \n    \"\"\"\n    if parents is not None and len(parents) == 0:\n        combobox.setCurrentIndex(0)\n    else:\n        item = parents[-1]\n        for i in range(combobox.count()):\n            if combobox.itemData(i) == item:\n                combobox.setCurrentIndex(i)\n                break", "output": "Update the combobox with the selected item based on the parents.\n\n    Parameters\n    ----------\n    parents : list of :class:`FoldScopeHelper`\n    combobox : :class:`qtpy.QtWidets.QComboBox`\n        The combobox to populate\n\n    Returns\n    -------\n    None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ParseAbstractInteger(text, is_long=False):\n  \"\"\"\n  \"\"\"\n  # Do the actual parsing. Exception handling is propagated to caller.\n  try:\n    # We force 32-bit values to int and 64-bit values to long to make\n    # alternate implementations where the distinction is more significant\n    # (e.g. the C++ implementation) simpler.\n    if is_long:\n      return long(text, 0)\n    else:\n      return int(text, 0)\n  except ValueError:\n    raise ValueError('Couldn\\'t parse integer: %s' % text)", "output": "Parses an integer without checking size/signedness.\n\n  Args:\n    text: The text to parse.\n    is_long: True if the value should be returned as a long integer.\n\n  Returns:\n    The integer value.\n\n  Raises:\n    ValueError: Thrown Iff the text is not a valid integer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rae(label, pred):\n    \"\"\"\"\"\"\n    numerator = np.mean(np.abs(label - pred), axis=None)\n    denominator = np.mean(np.abs(label - np.mean(label, axis=None)), axis=None)\n    return numerator / denominator", "output": "computes the relative absolute error (condensed using standard deviation formula)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_rpmmacros(runas='root'):\n    '''\n    \n    '''\n    home = os.path.expanduser('~')\n    rpmbuilddir = os.path.join(home, 'rpmbuild')\n    if not os.path.isdir(rpmbuilddir):\n        __salt__['file.makedirs_perms'](name=rpmbuilddir, user=runas, group='mock')\n\n    mockdir = os.path.join(home, 'mock')\n    if not os.path.isdir(mockdir):\n        __salt__['file.makedirs_perms'](name=mockdir, user=runas, group='mock')\n\n    rpmmacros = os.path.join(home, '.rpmmacros')\n    with salt.utils.files.fopen(rpmmacros, 'w') as afile:\n        afile.write(\n            salt.utils.stringutils.to_str('%_topdir {0}\\n'.format(rpmbuilddir))\n        )\n        afile.write('%signature gpg\\n')\n        afile.write('%_source_filedigest_algorithm 8\\n')\n        afile.write('%_binary_filedigest_algorithm 8\\n')\n        afile.write('%_gpg_name packaging@saltstack.com\\n')", "output": "Create the .rpmmacros file in user's home directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_protobuf_value(value_pb, val):\n    \"\"\"\n    \"\"\"\n    attr, val = _pb_attr_value(val)\n    if attr == \"key_value\":\n        value_pb.key_value.CopyFrom(val)\n    elif attr == \"timestamp_value\":\n        value_pb.timestamp_value.CopyFrom(val)\n    elif attr == \"entity_value\":\n        entity_pb = entity_to_protobuf(val)\n        value_pb.entity_value.CopyFrom(entity_pb)\n    elif attr == \"array_value\":\n        if len(val) == 0:\n            array_value = entity_pb2.ArrayValue(values=[])\n            value_pb.array_value.CopyFrom(array_value)\n        else:\n            l_pb = value_pb.array_value.values\n            for item in val:\n                i_pb = l_pb.add()\n                _set_protobuf_value(i_pb, item)\n    elif attr == \"geo_point_value\":\n        value_pb.geo_point_value.CopyFrom(val)\n    else:  # scalar, just assign\n        setattr(value_pb, attr, val)", "output": "Assign 'val' to the correct subfield of 'value_pb'.\n\n    The Protobuf API uses different attribute names based on value types\n    rather than inferring the type.\n\n    Some value types (entities, keys, lists) cannot be directly\n    assigned; this function handles them correctly.\n\n    :type value_pb: :class:`.entity_pb2.Value`\n    :param value_pb: The value protobuf to which the value is being assigned.\n\n    :type val: :class:`datetime.datetime`, boolean, float, integer, string,\n               :class:`google.cloud.datastore.key.Key`,\n               :class:`google.cloud.datastore.entity.Entity`\n    :param val: The value to be assigned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contextMenuEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        if self.model.showndata:\r\n            self.refresh_menu()\r\n            self.menu.popup(event.globalPos())\r\n            event.accept()\r\n        else:\r\n            self.empty_ws_menu.popup(event.globalPos())\r\n            event.accept()", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_img(object, handle, **kwargs):\n    \"\"\"\"\"\"\n\n    if isinstance(object, np.ndarray):\n        normalized = _normalize_array(object)\n        object = PIL.Image.fromarray(normalized)\n\n    if isinstance(object, PIL.Image.Image):\n        object.save(handle, **kwargs)  # will infer format from handle's url ext.\n    else:\n        raise ValueError(\"Can only save_img for numpy arrays or PIL.Images!\")", "output": "Save numpy array as image file on CNS.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def uniform_binning_correction(x, n_bits=8):\n  \"\"\"\n  \"\"\"\n  n_bins = 2**n_bits\n  batch_size, height, width, n_channels = common_layers.shape_list(x)\n  hwc = float(height * width * n_channels)\n\n  x = x + tf.random_uniform(\n      shape=(batch_size, height, width, n_channels),\n      minval=0.0, maxval=1.0/n_bins)\n  objective = -np.log(n_bins) * hwc * tf.ones(batch_size)\n  return x, objective", "output": "Replaces x^i with q^i(x) = U(x, x + 1.0 / 256.0).\n\n  Args:\n    x: 4-D Tensor of shape (NHWC)\n    n_bits: optional.\n  Returns:\n    x: x ~ U(x, x + 1.0 / 256)\n    objective: Equivalent to -q(x)*log(q(x)).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def draw_boxes(im, boxes, labels=None, color=None):\n    \"\"\"\n    \n    \"\"\"\n    boxes = np.asarray(boxes, dtype='int32')\n    if labels is not None:\n        assert len(labels) == len(boxes), \"{} != {}\".format(len(labels), len(boxes))\n    areas = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n    sorted_inds = np.argsort(-areas)    # draw large ones first\n    assert areas.min() > 0, areas.min()\n    # allow equal, because we are not very strict about rounding error here\n    assert boxes[:, 0].min() >= 0 and boxes[:, 1].min() >= 0 \\\n        and boxes[:, 2].max() <= im.shape[1] and boxes[:, 3].max() <= im.shape[0], \\\n        \"Image shape: {}\\n Boxes:\\n{}\".format(str(im.shape), str(boxes))\n\n    im = im.copy()\n    if color is None:\n        color = (15, 128, 15)\n    if im.ndim == 2 or (im.ndim == 3 and im.shape[2] == 1):\n        im = cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)\n    for i in sorted_inds:\n        box = boxes[i, :]\n        if labels is not None:\n            im = draw_text(im, (box[0], box[1]), labels[i], color=color)\n        cv2.rectangle(im, (box[0], box[1]), (box[2], box[3]),\n                      color=color, thickness=1)\n    return im", "output": "Args:\n        im (np.ndarray): a BGR image in range [0,255]. It will not be modified.\n        boxes (np.ndarray): a numpy array of shape Nx4 where each row is [x1, y1, x2, y2].\n        labels: (list[str] or None)\n        color: a 3-tuple BGR color (in range [0, 255])\n\n    Returns:\n        np.ndarray: a new image.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_blobs(self, prefix=''):\n    \"\"\"\"\"\"\n    return [b.name for b in self.bucket.list_blobs(prefix=prefix)]", "output": "Lists names of all blobs by their prefix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_service(protocol=None, service_address=None, scheduler='wlc'):\n    '''\n    \n    '''\n\n    cmd = '{0} -A {1}'.format(__detect_os(),\n                              _build_cmd(protocol=protocol,\n                                         service_address=service_address,\n                                         scheduler=scheduler))\n    out = __salt__['cmd.run_all'](cmd, python_shell=False)\n\n    # A non-zero return code means fail\n    if out['retcode']:\n        ret = out['stderr'].strip()\n    else:\n        ret = True\n    return ret", "output": "Add a virtual service.\n\n    protocol\n        The service protocol(only support tcp, udp and fwmark service).\n\n    service_address\n        The LVS service address.\n\n    scheduler\n        Algorithm for allocating TCP connections and UDP datagrams to real servers.\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' lvs.add_service tcp 1.1.1.1:80 rr", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dataset(self, dataset_ref, retry=DEFAULT_RETRY):\n        \"\"\"\n        \"\"\"\n        if isinstance(dataset_ref, str):\n            dataset_ref = DatasetReference.from_string(\n                dataset_ref, default_project=self.project\n            )\n\n        api_response = self._call_api(retry, method=\"GET\", path=dataset_ref.path)\n        return Dataset.from_api_repr(api_response)", "output": "Fetch the dataset referenced by ``dataset_ref``\n\n        Args:\n            dataset_ref (Union[ \\\n                :class:`~google.cloud.bigquery.dataset.DatasetReference`, \\\n                str, \\\n            ]):\n                A reference to the dataset to fetch from the BigQuery API.\n                If a string is passed in, this method attempts to create a\n                dataset reference from a string using\n                :func:`~google.cloud.bigquery.dataset.DatasetReference.from_string`.\n            retry (:class:`google.api_core.retry.Retry`):\n                (Optional) How to retry the RPC.\n\n        Returns:\n            google.cloud.bigquery.dataset.Dataset:\n                A ``Dataset`` instance.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_persistent_module(mod):\n    '''\n    \n    '''\n    conf = _get_modules_conf()\n    if not os.path.exists(conf):\n        __salt__['file.touch'](conf)\n    mod_name = _strip_module_name(mod)\n    if not mod_name or mod_name in mod_list(True) or mod_name \\\n            not in available():\n        return set()\n    escape_mod = re.escape(mod)\n    # If module is commented only uncomment it\n    if __salt__['file.search'](conf,\n                               '^#[\\t ]*{0}[\\t ]*$'.format(escape_mod),\n                               multiline=True):\n        __salt__['file.uncomment'](conf, escape_mod)\n    else:\n        __salt__['file.append'](conf, mod)\n    return set([mod_name])", "output": "Add module to configuration file to make it persistent. If module is\n    commented uncomment it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_plugin(self):\r\n        \"\"\"\"\"\"\r\n        self.main.add_dockwidget(self)\r\n\r\n        self.focus_changed.connect(self.main.plugin_focus_changed)\r\n        self.edit_goto.connect(self.main.editor.load)\r\n        self.edit_goto[str, int, str, bool].connect(\r\n                         lambda fname, lineno, word, processevents:\r\n                         self.main.editor.load(fname, lineno, word,\r\n                                               processevents=processevents))\r\n        self.main.editor.breakpoints_saved.connect(self.set_spyder_breakpoints)\r\n        self.main.editor.run_in_current_ipyclient.connect(self.run_script)\r\n        self.main.editor.run_cell_in_ipyclient.connect(self.run_cell)\r\n        self.main.workingdirectory.set_current_console_wd.connect(\r\n                                     self.set_current_client_working_directory)\r\n\r\n        self.tabwidget.currentChanged.connect(self.update_working_directory)\r\n\r\n        self._remove_old_stderr_files()", "output": "Register plugin in Spyder's main window", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_elements_by_class_name(self, name):\n        \"\"\"\n        \n        \"\"\"\n        return self.find_elements(by=By.CLASS_NAME, value=name)", "output": "Finds elements by class name.\n\n        :Args:\n         - name: The class name of the elements to find.\n\n        :Returns:\n         - list of WebElement - a list with elements if any was found.  An\n           empty list if not\n\n        :Usage:\n            ::\n\n                elements = driver.find_elements_by_class_name('foo')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ntile(n):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.ntile(int(n)))", "output": "Window function: returns the ntile group id (from 1 to `n` inclusive)\n    in an ordered window partition. For example, if `n` is 4, the first\n    quarter of the rows will get value 1, the second quarter will get 2,\n    the third quarter will get 3, and the last quarter will get 4.\n\n    This is equivalent to the NTILE function in SQL.\n\n    :param n: an integer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gather_file_data(name):\n    \"\"\"\n    \n    \"\"\"\n    res = {'name': name}\n    try:\n        res['mtime'] = osp.getmtime(name)\n        res['size'] = osp.getsize(name)\n    except OSError:\n        pass\n    return res", "output": "Gather data about a given file.\n\n    Returns a dict with fields name, mtime and size, containing the relevant\n    data for the fiel.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PositionalEncoding(x, params, **unused_kwargs):\n  \"\"\"\"\"\"\n  if not isinstance(x, (list, tuple)):  # non-chunked inputs\n    symbol_size = np.shape(x)[1]\n    return x + params[:, :symbol_size, :]\n  # Chunked case: apply to all chunks selecting as much as needed.\n  offset = 0\n  results = []\n  for chunk in x:\n    symbol_size = np.shape(chunk)[1]\n    results.append(chunk + params[:, offset:offset + symbol_size, :])\n    offset += symbol_size\n  return results", "output": "Implements bare positional encoding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_shell(pid=None, max_depth=6):\n    \"\"\"\n    \"\"\"\n    pid = str(pid or os.getpid())\n    mapping = _get_process_mapping()\n    login_shell = os.environ.get('SHELL', '')\n    for _ in range(max_depth):\n        try:\n            proc = mapping[pid]\n        except KeyError:\n            break\n        name = os.path.basename(proc.args[0]).lower()\n        if name in SHELL_NAMES:\n            return (name, proc.args[0])\n        elif proc.args[0].startswith('-'):\n            # This is the login shell. Use the SHELL environ if possible\n            # because it provides better information.\n            if login_shell:\n                name = login_shell.lower()\n            else:\n                name = proc.args[0][1:].lower()\n            return (os.path.basename(name), name)\n        pid = proc.ppid     # Go up one level.\n    return None", "output": "Get the shell that the supplied pid or os.getpid() is running in.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_project(self, path):\r\n        \"\"\"\"\"\"\r\n        self.open_project(path=path)\r\n        self.setup_menu_actions()\r\n        self.add_to_recent(path)", "output": "Create a new project.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, name_or_klass):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(name_or_klass, str):\n            name_or_klass = name_or_klass.__name__\n        return self._extensions[name_or_klass]", "output": "Get a extension by name (or class).\n\n        :param name_or_klass: The name or the class of the extension to get\n        :type name_or_klass: str or type\n        :rtype: spyder.api.mode.EditorExtension", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read_data(label, image):\n    \"\"\"\n    \n    \"\"\"\n    base_url = 'http://yann.lecun.com/exdb/mnist/'\n    with gzip.open(download_file(base_url+label, os.path.join('data',label))) as flbl:\n        magic, num = struct.unpack(\">II\", flbl.read(8))\n        label = np.fromstring(flbl.read(), dtype=np.int8)\n    with gzip.open(download_file(base_url+image, os.path.join('data',image)), 'rb') as fimg:\n        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n    return (label, image)", "output": "download and read data into numpy", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def utime(self, path, times):\n        \"\"\"\n        \n        \"\"\"\n        path = self._adjust_cwd(path)\n        if times is None:\n            times = (time.time(), time.time())\n        self._log(DEBUG, \"utime({!r}, {!r})\".format(path, times))\n        attr = SFTPAttributes()\n        attr.st_atime, attr.st_mtime = times\n        self._request(CMD_SETSTAT, path, attr)", "output": "Set the access and modified times of the file specified by ``path``.\n        If ``times`` is ``None``, then the file's access and modified times\n        are set to the current time.  Otherwise, ``times`` must be a 2-tuple\n        of numbers, of the form ``(atime, mtime)``, which is used to set the\n        access and modified times, respectively.  This bizarre API is mimicked\n        from Python for the sake of consistency -- I apologize.\n\n        :param str path: path of the file to modify\n        :param tuple times:\n            ``None`` or a tuple of (access time, modified time) in standard\n            internet epoch time (seconds since 01 January 1970 GMT)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def acquire(\n        self, timeout: Union[float, datetime.timedelta] = None\n    ) -> Awaitable[_ReleasingContextManager]:\n        \"\"\"\n        \"\"\"\n        waiter = Future()  # type: Future[_ReleasingContextManager]\n        if self._value > 0:\n            self._value -= 1\n            waiter.set_result(_ReleasingContextManager(self))\n        else:\n            self._waiters.append(waiter)\n            if timeout:\n\n                def on_timeout() -> None:\n                    if not waiter.done():\n                        waiter.set_exception(gen.TimeoutError())\n                    self._garbage_collect()\n\n                io_loop = ioloop.IOLoop.current()\n                timeout_handle = io_loop.add_timeout(timeout, on_timeout)\n                waiter.add_done_callback(\n                    lambda _: io_loop.remove_timeout(timeout_handle)\n                )\n        return waiter", "output": "Decrement the counter. Returns an awaitable.\n\n        Block if the counter is zero and wait for a `.release`. The awaitable\n        raises `.TimeoutError` after the deadline.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dorefa(bitW, bitA, bitG):\n    \"\"\"\n    \n    \"\"\"\n    def quantize(x, k):\n        n = float(2 ** k - 1)\n\n        @tf.custom_gradient\n        def _quantize(x):\n            return tf.round(x * n) / n, lambda dy: dy\n\n        return _quantize(x)\n\n    def fw(x):\n        if bitW == 32:\n            return x\n\n        if bitW == 1:   # BWN\n            E = tf.stop_gradient(tf.reduce_mean(tf.abs(x)))\n\n            @tf.custom_gradient\n            def _sign(x):\n                return tf.where(tf.equal(x, 0), tf.ones_like(x), tf.sign(x / E)) * E, lambda dy: dy\n\n            return _sign(x)\n\n        x = tf.tanh(x)\n        x = x / tf.reduce_max(tf.abs(x)) * 0.5 + 0.5\n        return 2 * quantize(x, bitW) - 1\n\n    def fa(x):\n        if bitA == 32:\n            return x\n        return quantize(x, bitA)\n\n    def fg(x):\n        if bitG == 32:\n            return x\n\n        @tf.custom_gradient\n        def _identity(input):\n            def grad_fg(x):\n                rank = x.get_shape().ndims\n                assert rank is not None\n                maxx = tf.reduce_max(tf.abs(x), list(range(1, rank)), keep_dims=True)\n                x = x / maxx\n                n = float(2**bitG - 1)\n                x = x * 0.5 + 0.5 + tf.random_uniform(\n                    tf.shape(x), minval=-0.5 / n, maxval=0.5 / n)\n                x = tf.clip_by_value(x, 0.0, 1.0)\n                x = quantize(x, bitG) - 0.5\n                return x * maxx * 2\n\n            return input, grad_fg\n\n        return _identity(x)\n    return fw, fa, fg", "output": "Return the three quantization functions fw, fa, fg, for weights, activations and gradients respectively", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chmod(self, mode):\n        \"\"\"\n        \n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        self._accessor.chmod(self, mode)", "output": "Change the permissions of the path, like os.chmod().", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mouseReleaseEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        self.QT_CLASS.mouseReleaseEvent(self, event)\r\n        text = self.get_line_at(event.pos())\r\n        if get_error_match(text) and not self.has_selected_text():\r\n            if self.go_to_error is not None:\r\n                self.go_to_error.emit(text)", "output": "Go to error", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(\n    ctx,\n    state,\n    bare=False,\n    dry_run=None,\n    outdated=False,\n    **kwargs\n):\n    \"\"\"\"\"\"\n    from ..core import (\n        ensure_project,\n        do_outdated,\n        do_lock,\n        do_sync,\n        project,\n    )\n\n    ensure_project(three=state.three, python=state.python, warn=True, pypi_mirror=state.pypi_mirror)\n    if not outdated:\n        outdated = bool(dry_run)\n    if outdated:\n        do_outdated(pypi_mirror=state.pypi_mirror)\n    packages = [p for p in state.installstate.packages if p]\n    editable = [p for p in state.installstate.editables if p]\n    if not packages:\n        echo(\n            \"{0} {1} {2} {3}{4}\".format(\n                crayons.white(\"Running\", bold=True),\n                crayons.red(\"$ pipenv lock\", bold=True),\n                crayons.white(\"then\", bold=True),\n                crayons.red(\"$ pipenv sync\", bold=True),\n                crayons.white(\".\", bold=True),\n            )\n        )\n    else:\n        for package in packages + editable:\n            if package not in project.all_packages:\n                echo(\n                    \"{0}: {1} was not found in your Pipfile! Aborting.\"\n                    \"\".format(\n                        crayons.red(\"Warning\", bold=True),\n                        crayons.green(package, bold=True),\n                    ),\n                    err=True,\n                )\n                ctx.abort()\n\n    do_lock(\n        clear=state.clear,\n        pre=state.installstate.pre,\n        keep_outdated=state.installstate.keep_outdated,\n        pypi_mirror=state.pypi_mirror,\n    )\n    do_sync(\n        ctx=ctx,\n        dev=state.installstate.dev,\n        three=state.three,\n        python=state.python,\n        bare=bare,\n        dont_upgrade=not state.installstate.keep_outdated,\n        user=False,\n        clear=state.clear,\n        unused=False,\n        sequential=state.installstate.sequential,\n        pypi_mirror=state.pypi_mirror,\n    )", "output": "Runs lock, then sync.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_lars_path(weighted_data, weighted_labels):\n        \"\"\"\n        \"\"\"\n        x_vector = weighted_data\n        alphas, _, coefs = lars_path(x_vector,\n                                     weighted_labels,\n                                     method='lasso',\n                                     verbose=False)\n        return alphas, coefs", "output": "Generates the lars path for weighted data.\n\n        Args:\n            weighted_data: data that has been weighted by kernel\n            weighted_label: labels, weighted by kernel\n\n        Returns:\n            (alphas, coefs), both are arrays corresponding to the\n            regularization parameter and coefficients, respectively", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dummy_dataloader(dataloader, target_shape):\n    \"\"\"\"\"\"\n    data_iter = enumerate(dataloader)\n    _, data_batch = next(data_iter)\n    logging.debug('Searching target batch shape: %s', target_shape)\n    while data_batch[0].shape != target_shape:\n        logging.debug('Skip batch with shape %s', data_batch[0].shape)\n        _, data_batch = next(data_iter)\n    logging.debug('Found target dummy batch.')\n\n    class DummyIter():\n        def __init__(self, batch):\n            self._batch = batch\n\n        def __iter__(self):\n            while True:\n                yield self._batch\n\n    return DummyIter(data_batch)", "output": "Return a dummy data loader which returns a fixed data batch of target shape", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_model_snapshots(self, job_id, snapshot_id=None, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"GET\",\n            _make_path(\n                \"_ml\", \"anomaly_detectors\", job_id, \"model_snapshots\", snapshot_id\n            ),\n            params=params,\n            body=body,\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-get-snapshot.html>`_\n\n        :arg job_id: The ID of the job to fetch\n        :arg snapshot_id: The ID of the snapshot to fetch\n        :arg body: Model snapshot selection criteria\n        :arg desc: True if the results should be sorted in descending order\n        :arg end: The filter 'end' query parameter\n        :arg from_: Skips a number of documents\n        :arg size: The default number of documents returned in queries as a\n            string.\n        :arg sort: Name of the field to sort on\n        :arg start: The filter 'start' query parameter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def train(cls, rdd, k=10, maxIterations=20, docConcentration=-1.0,\n              topicConcentration=-1.0, seed=None, checkpointInterval=10, optimizer=\"em\"):\n        \"\"\"\n        \"\"\"\n        model = callMLlibFunc(\"trainLDAModel\", rdd, k, maxIterations,\n                              docConcentration, topicConcentration, seed,\n                              checkpointInterval, optimizer)\n        return LDAModel(model)", "output": "Train a LDA model.\n\n        :param rdd:\n          RDD of documents, which are tuples of document IDs and term\n          (word) count vectors. The term count vectors are \"bags of\n          words\" with a fixed-size vocabulary (where the vocabulary size\n          is the length of the vector). Document IDs must be unique\n          and >= 0.\n        :param k:\n          Number of topics to infer, i.e., the number of soft cluster\n          centers.\n          (default: 10)\n        :param maxIterations:\n          Maximum number of iterations allowed.\n          (default: 20)\n        :param docConcentration:\n          Concentration parameter (commonly named \"alpha\") for the prior\n          placed on documents' distributions over topics (\"theta\").\n          (default: -1.0)\n        :param topicConcentration:\n          Concentration parameter (commonly named \"beta\" or \"eta\") for\n          the prior placed on topics' distributions over terms.\n          (default: -1.0)\n        :param seed:\n          Random seed for cluster initialization. Set as None to generate\n          seed based on system time.\n          (default: None)\n        :param checkpointInterval:\n          Period (in iterations) between checkpoints.\n          (default: 10)\n        :param optimizer:\n          LDAOptimizer used to perform the actual calculation. Currently\n          \"em\", \"online\" are supported.\n          (default: \"em\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_yielded(yielded: _Yieldable) -> Future:\n    \"\"\"\n\n    \"\"\"\n    if yielded is None or yielded is moment:\n        return moment\n    elif yielded is _null_future:\n        return _null_future\n    elif isinstance(yielded, (list, dict)):\n        return multi(yielded)  # type: ignore\n    elif is_future(yielded):\n        return typing.cast(Future, yielded)\n    elif isawaitable(yielded):\n        return _wrap_awaitable(yielded)  # type: ignore\n    else:\n        raise BadYieldError(\"yielded unknown object %r\" % (yielded,))", "output": "Convert a yielded object into a `.Future`.\n\n    The default implementation accepts lists, dictionaries, and\n    Futures. This has the side effect of starting any coroutines that\n    did not start themselves, similar to `asyncio.ensure_future`.\n\n    If the `~functools.singledispatch` library is available, this function\n    may be extended to support additional types. For example::\n\n        @convert_yielded.register(asyncio.Future)\n        def _(asyncio_future):\n            return tornado.platform.asyncio.to_tornado_future(asyncio_future)\n\n    .. versionadded:: 4.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_parent_scope(block):\n        \"\"\"\"\"\"\n        original = block\n        if not TextBlockHelper.is_fold_trigger(block):\n            # search level of next non blank line\n            while block.text().strip() == '' and block.isValid():\n                block = block.next()\n            ref_lvl = TextBlockHelper.get_fold_lvl(block) - 1\n            block = original\n            while (block.blockNumber() and\n                   (not TextBlockHelper.is_fold_trigger(block) or\n                    TextBlockHelper.get_fold_lvl(block) > ref_lvl)):\n                block = block.previous()\n        return block", "output": "Find parent scope, if the block is not a fold trigger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_keyid(keyname):\n    '''\n    \n    '''\n    if not keyname:\n        return None\n    keypairs = list_keypairs(call='function')\n    keyid = keypairs[keyname]['id']\n    if keyid:\n        return keyid\n    raise SaltCloudNotFound('The specified ssh key could not be found.')", "output": "Return the ID of the keyname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cached_path(url_or_filename: Union[str, Path], cache_dir: str = None) -> str:\n    \"\"\"\n    \n    \"\"\"\n    if cache_dir is None:\n        cache_dir = CACHE_DIRECTORY\n    if isinstance(url_or_filename, Path):\n        url_or_filename = str(url_or_filename)\n\n    url_or_filename = os.path.expanduser(url_or_filename)\n    parsed = urlparse(url_or_filename)\n\n    if parsed.scheme in ('http', 'https', 's3'):\n        # URL, so get it from the cache (downloading if necessary)\n        return get_from_cache(url_or_filename, cache_dir)\n    elif os.path.exists(url_or_filename):\n        # File, and it exists.\n        return url_or_filename\n    elif parsed.scheme == '':\n        # File, but it doesn't exist.\n        raise FileNotFoundError(\"file {} not found\".format(url_or_filename))\n    else:\n        # Something unknown\n        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))", "output": "Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_node(graph, text):\n    \"\"\"\"\"\"\n    match = _NODEPAT.match(text)\n    if match is not None:\n        node = match.group(1)\n        graph.node(node, label=match.group(2), shape='circle')\n        return node\n    match = _LEAFPAT.match(text)\n    if match is not None:\n        node = match.group(1)\n        graph.node(node, label=match.group(2), shape='box')\n        return node\n    raise ValueError('Unable to parse node: {0}'.format(text))", "output": "parse dumped node", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_pkgs(versions_as_list=False, **kwargs):\n    '''\n    \n    '''\n    versions_as_list = salt.utils.data.is_true(versions_as_list)\n    # 'removed', 'purge_desired' not yet implemented or not applicable\n    if any([salt.utils.data.is_true(kwargs.get(x))\n            for x in ('removed', 'purge_desired')]):\n        return {}\n\n    if 'pkg.list_pkgs' in __context__:\n        if versions_as_list:\n            return __context__['pkg.list_pkgs']\n        else:\n            ret = copy.deepcopy(__context__['pkg.list_pkgs'])\n            __salt__['pkg_resource.stringify'](ret)\n            return ret\n\n    ret = {}\n    cmd = ['port', 'installed']\n    out = salt.utils.mac_utils.execute_return_result(cmd)\n    for line in out.splitlines():\n        try:\n            name, version_num, active = re.split(r'\\s+', line.lstrip())[0:3]\n            version_num = version_num[1:]\n        except ValueError:\n            continue\n        if not LIST_ACTIVE_ONLY or active == '(active)':\n            __salt__['pkg_resource.add_pkg'](ret, name, version_num)\n\n    __salt__['pkg_resource.sort_pkglist'](ret)\n    __context__['pkg.list_pkgs'] = copy.deepcopy(ret)\n    if not versions_as_list:\n        __salt__['pkg_resource.stringify'](ret)\n    return ret", "output": "List the packages currently installed in a dict::\n\n        {'<package_name>': '<version>'}\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.list_pkgs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def empty(self):\n        \"\"\"\n        \n        \"\"\"\n        self._lock.acquire()\n        try:\n            out = self._buffer_tobytes()\n            del self._buffer[:]\n            if (self._event is not None) and not self._closed:\n                self._event.clear()\n            return out\n        finally:\n            self._lock.release()", "output": "Clear out the buffer and return all data that was in it.\n\n        :return:\n            any data that was in the buffer prior to clearing it out, as a\n            `str`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_save_containers(platforms, registry, load_cache) -> int:\n    \"\"\"\n    \n    \"\"\"\n    from joblib import Parallel, delayed\n    if len(platforms) == 0:\n        return 0\n\n    platform_results = Parallel(n_jobs=PARALLEL_BUILDS, backend=\"multiprocessing\")(\n        delayed(_build_save_container)(platform, registry, load_cache)\n        for platform in platforms)\n\n    is_error = False\n    for platform_result in platform_results:\n        if platform_result is not None:\n            logging.error('Failed to generate %s', platform_result)\n            is_error = True\n\n    return 1 if is_error else 0", "output": "Entry point to build and upload all built dockerimages in parallel\n    :param platforms: List of platforms\n    :param registry: Docker registry name\n    :param load_cache: Load cache before building\n    :return: 1 if error occurred, 0 otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_global_logging(serialization_dir: str, file_friendly_logging: bool) -> logging.FileHandler:\n    \"\"\"\n    \n    \"\"\"\n\n    # If we don't have a terminal as stdout,\n    # force tqdm to be nicer.\n    if not sys.stdout.isatty():\n        file_friendly_logging = True\n\n    Tqdm.set_slower_interval(file_friendly_logging)\n    std_out_file = os.path.join(serialization_dir, \"stdout.log\")\n    sys.stdout = TeeLogger(std_out_file, # type: ignore\n                           sys.stdout,\n                           file_friendly_logging)\n    sys.stderr = TeeLogger(os.path.join(serialization_dir, \"stderr.log\"), # type: ignore\n                           sys.stderr,\n                           file_friendly_logging)\n\n    stdout_handler = logging.FileHandler(std_out_file)\n    stdout_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))\n    logging.getLogger().addHandler(stdout_handler)\n\n    return stdout_handler", "output": "This function configures 3 global logging attributes - streaming stdout and stderr\n    to a file as well as the terminal, setting the formatting for the python logging\n    library and setting the interval frequency for the Tqdm progress bar.\n\n    Note that this function does not set the logging level, which is set in ``allennlp/run.py``.\n\n    Parameters\n    ----------\n    serialization_dir : ``str``, required.\n        The directory to stream logs to.\n    file_friendly_logging : ``bool``, required.\n        Whether logs should clean the output to prevent carriage returns\n        (used to update progress bars on a single terminal line). This\n        option is typically only used if you are running in an environment\n        without a terminal.\n\n    Returns\n    -------\n    ``logging.FileHandler``\n        A logging file handler that can later be closed and removed from the global logger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_group(path, follow_symlinks=True):\n    '''\n    \n    '''\n    func_name = '{0}.get_group'.format(__virtualname__)\n    if __opts__.get('fun', '') == func_name:\n        log.info('The function %s should not be used on Windows systems; '\n                 'see function docs for details. The value returned is the '\n                 'user (owner).', func_name)\n\n    return get_user(path, follow_symlinks)", "output": "Return the group that owns a given file\n\n    Under Windows, this will return the user (owner) of the file.\n\n    While a file in Windows does have a 'primary group', this rarely used\n    attribute generally has no bearing on permissions unless intentionally\n    configured and is only used to support Unix compatibility features (e.g.\n    Services For Unix, NFS services).\n\n    Salt, therefore, remaps this function to provide functionality that\n    somewhat resembles Unix behavior for API compatibility reasons. When\n    managing Windows systems, this function is superfluous and will generate\n    an info level log entry if used directly.\n\n    If you do actually want to access the 'primary group' of a file, use\n    `file.get_pgroup`.\n\n    Args:\n        path (str): The path to the file or directory\n\n        follow_symlinks (bool):\n            If the object specified by ``path`` is a symlink, get attributes of\n            the linked file instead of the symlink itself. Default is True\n\n    Returns:\n        str: The name of the owner\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.get_group c:\\\\temp\\\\test.txt", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def addCondition(self, *fns, **kwargs):\n        \"\"\"\n        \"\"\"\n        msg = kwargs.get(\"message\", \"failed user-defined condition\")\n        exc_type = ParseFatalException if kwargs.get(\"fatal\", False) else ParseException\n        for fn in fns:\n            fn = _trim_arity(fn)\n            def pa(s,l,t):\n                if not bool(fn(s,l,t)):\n                    raise exc_type(s,l,msg)\n            self.parseAction.append(pa)\n        self.callDuringTry = self.callDuringTry or kwargs.get(\"callDuringTry\", False)\n        return self", "output": "Add a boolean predicate function to expression's list of parse actions. See\n        :class:`setParseAction` for function call signatures. Unlike ``setParseAction``,\n        functions passed to ``addCondition`` need to return boolean success/fail of the condition.\n\n        Optional keyword arguments:\n        - message = define a custom message to be used in the raised exception\n        - fatal   = if True, will raise ParseFatalException to stop parsing immediately; otherwise will raise ParseException\n\n        Example::\n\n            integer = Word(nums).setParseAction(lambda toks: int(toks[0]))\n            year_int = integer.copy()\n            year_int.addCondition(lambda toks: toks[0] >= 2000, message=\"Only support years 2000 and later\")\n            date_str = year_int + '/' + integer + '/' + integer\n\n            result = date_str.parseString(\"1999/12/31\")  # -> Exception: Only support years 2000 and later (at char 0), (line:1, col:1)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_running(name, no_start=False, path=None):\n    '''\n    \n    '''\n    _ensure_exists(name, path=path)\n    pre = state(name, path=path)\n    if pre == 'running':\n        # This will be a no-op but running the function will give us a pretty\n        # return dict.\n        return start(name, path=path)\n    elif pre == 'stopped':\n        if no_start:\n            raise CommandExecutionError(\n                'Container \\'{0}\\' is not running'.format(name)\n            )\n        return start(name, path=path)\n    elif pre == 'frozen':\n        if no_start:\n            raise CommandExecutionError(\n                'Container \\'{0}\\' is not running'.format(name)\n            )\n        return unfreeze(name, path=path)", "output": "If the container is not currently running, start it. This function returns\n    the state that the container was in before changing\n\n    path\n        path to the container parent directory\n        default: /var/lib/lxc (system)\n\n        .. versionadded:: 2015.8.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _split_result_for_readers(axis, num_splits, df):  # pragma: no cover\n    \"\"\"\n    \"\"\"\n    splits = split_result_of_axis_func_pandas(axis, num_splits, df)\n    if not isinstance(splits, list):\n        splits = [splits]\n    return splits", "output": "Splits the DataFrame read into smaller DataFrames and handles all edge cases.\n\n    Args:\n        axis: Which axis to split over.\n        num_splits: The number of splits to create.\n        df: The DataFrame after it has been read.\n\n    Returns:\n        A list of pandas DataFrames.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_backup(name):\n    \n    '''\n    if name in list_backups():\n        raise CommandExecutionError('Backup already present: {0}'.format(name))\n\n    ps_cmd = ['Backup-WebConfiguration',\n              '-Name', \"'{0}'\".format(name)]\n\n    cmd_ret = _srvmgr(ps_cmd)\n\n    if cmd_ret['retcode'] != 0:\n        msg = 'Unable to backup web configuration: {0}\\nError: {1}' \\\n              ''.format(name, cmd_ret['stderr'])\n        raise CommandExecutionError(msg)\n\n    return name in list_backups()", "output": "r'''\n    Backup an IIS Configuration on the System.\n\n    .. versionadded:: 2017.7.0\n\n    .. note::\n        Backups are stored in the ``$env:Windir\\System32\\inetsrv\\backup``\n        folder.\n\n    Args:\n        name (str): The name to give the backup\n\n    Returns:\n        bool: True if successful, otherwise False\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.create_backup good_config_20170209", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def active():\n    '''\n    \n    '''\n    ret = {}\n    # TODO: This command should be extended to collect more information, such as UUID.\n    devices = __salt__['cmd.run_stdout']('dmsetup ls --target crypt')\n    out_regex = re.compile(r'(?P<devname>\\w+)\\W+\\((?P<major>\\d+), (?P<minor>\\d+)\\)')\n\n    log.debug(devices)\n    for line in devices.split('\\n'):\n        match = out_regex.match(line)\n        if match:\n            dev_info = match.groupdict()\n            ret[dev_info['devname']] = dev_info\n        else:\n            log.warning('dmsetup output does not match expected format')\n\n    return ret", "output": "List existing device-mapper device details.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_exists(name, user=None, password=None, host=None, port=None,\n                database='admin', authdb=None):\n    '''\n    \n    '''\n    users = user_list(user, password, host, port, database, authdb)\n\n    if isinstance(users, six.string_types):\n        return 'Failed to connect to mongo database'\n\n    for user in users:\n        if name == dict(user).get('user'):\n            return True\n\n    return False", "output": "Checks if a user exists in MongoDB\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.user_exists <name> <user> <password> <host> <port> <database>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def DownloadResource(url, path):\n    ''''''\n    import requests\n    from six import BytesIO\n    import zipfile\n    print(\"Downloading... {} to {}\".format(url, path))\n    r = requests.get(url, stream=True)\n    z = zipfile.ZipFile(BytesIO(r.content))\n    z.extractall(path)\n    print(\"Completed download and extraction.\")", "output": "Downloads resources from s3 by url and unzips them to the provided path", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_disk(self, path, **kwargs):\n        \"\"\"\n        \"\"\"\n        path = ensure_path(path)\n        path = path.with_suffix(\".jsonl\")\n        srsly.write_jsonl(path, self.patterns)", "output": "Save the entity ruler patterns to a directory. The patterns will be\n        saved as newline-delimited JSON (JSONL).\n\n        path (unicode / Path): The JSONL file to load.\n        **kwargs: Other config paramters, mostly for consistency.\n        RETURNS (EntityRuler): The loaded entity ruler.\n\n        DOCS: https://spacy.io/api/entityruler#to_disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(format, string, extra_types=None, evaluate_result=True, case_sensitive=False):\n    '''\n    '''\n    p = Parser(format, extra_types=extra_types, case_sensitive=case_sensitive)\n    return p.parse(string, evaluate_result=evaluate_result)", "output": "Using \"format\" attempt to pull values from \"string\".\n\n    The format must match the string contents exactly. If the value\n    you're looking for is instead just a part of the string use\n    search().\n\n    If ``evaluate_result`` is True the return value will be an Result instance with two attributes:\n\n     .fixed - tuple of fixed-position values from the string\n     .named - dict of named values from the string\n\n    If ``evaluate_result`` is False the return value will be a Match instance with one method:\n\n     .evaluate_result() - This will return a Result instance like you would get\n                          with ``evaluate_result`` set to True\n\n    The default behaviour is to match strings case insensitively. You may match with\n    case by specifying case_sensitive=True.\n\n    If the format is invalid a ValueError will be raised.\n\n    See the module documentation for the use of \"extra_types\".\n\n    In the case there is no match parse() will return None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _aws_encode_changebatch(o):\n    '''\n    \n    '''\n    change_idx = 0\n    while change_idx < len(o['Changes']):\n        o['Changes'][change_idx]['ResourceRecordSet']['Name'] = aws_encode(o['Changes'][change_idx]['ResourceRecordSet']['Name'])\n        if 'ResourceRecords' in o['Changes'][change_idx]['ResourceRecordSet']:\n            rr_idx = 0\n            while rr_idx < len(o['Changes'][change_idx]['ResourceRecordSet']['ResourceRecords']):\n                o['Changes'][change_idx]['ResourceRecordSet']['ResourceRecords'][rr_idx]['Value'] = aws_encode(o['Changes'][change_idx]['ResourceRecordSet']['ResourceRecords'][rr_idx]['Value'])\n                rr_idx += 1\n        if 'AliasTarget' in o['Changes'][change_idx]['ResourceRecordSet']:\n            o['Changes'][change_idx]['ResourceRecordSet']['AliasTarget']['DNSName'] = aws_encode(o['Changes'][change_idx]['ResourceRecordSet']['AliasTarget']['DNSName'])\n        change_idx += 1\n    return o", "output": "helper method to process a change batch & encode the bits which need encoding.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def strip_extras_markers_from_requirement(req):\n    # type: (TRequirement) -> TRequirement\n    \"\"\"\n    \n    \"\"\"\n    if req is None:\n        raise TypeError(\"Must pass in a valid requirement, received {0!r}\".format(req))\n    if getattr(req, \"marker\", None) is not None:\n        marker = req.marker  # type: TMarker\n        marker._markers = _strip_extras_markers(marker._markers)\n        if not marker._markers:\n            req.marker = None\n        else:\n            req.marker = marker\n    return req", "output": "Given a :class:`~packaging.requirements.Requirement` instance with markers defining\n    *extra == 'name'*, strip out the extras from the markers and return the cleaned\n    requirement\n\n    :param PackagingRequirement req: A packaging requirement to clean\n    :return: A cleaned requirement\n    :rtype: PackagingRequirement", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exe(self):\n        \"\"\"\n        \"\"\"\n        return self._buckets[self.curr_bucket_key]['exe'][tuple(self.data_shapes.items())]", "output": "Get the current executor\n\n        Returns\n        -------\n        exe : mxnet.executor.Executor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safe_isinstance(value, types=None, class_names=None):\n    \"\"\"\n    \"\"\"\n    # inspect is being imported here because I seriously doubt\n    # that this function will be used outside of the type\n    # checking below.\n    import inspect\n    result = False\n    if types is not None:\n        result = result or isinstance(value, types)\n    if class_names is not None and not result:\n        # this doesn't work with inheritance, but normally\n        # either the class will already be imported within the module,\n        # or the class doesn't have any subclasses. For example: PropertySet\n        if isinstance(class_names, basestring):\n            class_names = [class_names]\n        # this is the part that makes it \"safe\".\n        try:\n            base_names = [class_.__name__ for class_ in inspect.getmro(value.__class__)]\n            for name in class_names:\n                if name in base_names:\n                    return True\n        except AttributeError:\n            pass\n    return result", "output": "To prevent circular imports, this extends isinstance()\n    by checking also if `value` has a particular class name (or inherits from a\n    particular class name). This check is safe in that an AttributeError is not\n    raised in case `value` doesn't have a __class__ attribute.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def revnet_step(name, x, hparams, reverse=True):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n    if hparams.coupling == \"additive\":\n      coupling_layer = functools.partial(\n          additive_coupling, name=\"additive\", reverse=reverse,\n          mid_channels=hparams.coupling_width,\n          activation=hparams.activation, dropout=hparams.coupling_dropout)\n    else:\n      coupling_layer = functools.partial(\n          affine_coupling, name=\"affine\", reverse=reverse,\n          mid_channels=hparams.coupling_width,\n          activation=hparams.activation, dropout=hparams.coupling_dropout)\n    ops = [\n        functools.partial(actnorm, name=\"actnorm\", reverse=reverse),\n        functools.partial(invertible_1x1_conv, name=\"invertible\",\n                          reverse=reverse), coupling_layer]\n\n    if reverse:\n      ops = ops[::-1]\n\n    objective = 0.0\n    for op in ops:\n      x, curr_obj = op(x=x)\n      objective += curr_obj\n    return x, objective", "output": "One step of glow generative flow.\n\n  Actnorm + invertible 1X1 conv + affine_coupling.\n\n  Args:\n    name: used for variable scope.\n    x: input\n    hparams: coupling_width is the only hparam that is being used in\n             this function.\n    reverse: forward or reverse pass.\n  Returns:\n    z: Output of one step of reversible flow.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _transform_should_cast(self, func_nm):\n        \"\"\"\n        \n        \"\"\"\n        return (self.size().fillna(0) > 0).any() and (\n            func_nm not in base.cython_cast_blacklist)", "output": "Parameters:\n        -----------\n        func_nm: str\n            The name of the aggregation function being performed\n\n        Returns:\n        --------\n        bool\n            Whether transform should attempt to cast the result of aggregation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __get_managed_files_rpm(self):\n        '''\n        \n        '''\n        dirs = set()\n        links = set()\n        files = set()\n\n        for line in salt.utils.stringutils.to_str(self._syscall(\"rpm\", None, None, '-qlav')[0]).split(os.linesep):\n            line = line.strip()\n            if not line:\n                continue\n            line = line.replace(\"\\t\", \" \").split(\" \")\n            if line[0][0] == \"d\":\n                dirs.add(line[-1])\n            elif line[0][0] == \"l\":\n                links.add(line[-1])\n            elif line[0][0] == \"-\":\n                files.add(line[-1])\n\n        return sorted(files), sorted(dirs), sorted(links)", "output": "Get a list of all system files, belonging to the RedHat package manager.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chown(self, path, owner, group, recursive=False):\n        \"\"\"\n        \n        \"\"\"\n        bite = self.get_bite()\n        if owner:\n            if group:\n                return all(bite.chown(self.list_path(path), \"%s:%s\" % (owner, group),\n                                      recurse=recursive))\n            return all(bite.chown(self.list_path(path), owner, recurse=recursive))\n        return list(bite.chgrp(self.list_path(path), group, recurse=recursive))", "output": "Use snakebite.chown/chgrp, if available.\n\n        One of owner or group must be set. Just setting group calls chgrp.\n\n        :param path: update-able file(s)\n        :type path: either a string or sequence of strings\n        :param owner: new owner, can be blank\n        :type owner: string\n        :param group: new group, can be blank\n        :type group: string\n        :param recursive: change just listed entry(ies) or all in directories\n        :type recursive: boolean, default is False\n        :return: list of all changed items", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def image_list(auth=None, **kwargs):\n    '''\n    \n    '''\n    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    return cloud.list_images(**kwargs)", "output": "List images\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glanceng.image_list\n        salt '*' glanceng.image_list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_gl_class_type(obj_class):\n    \"\"\"\n    \n\n    \"\"\"\n\n    if obj_class == _SFrame:\n        return \"SFrame\"\n    elif obj_class == _SGraph:\n        return \"SGraph\"\n    elif obj_class == _SArray:\n        return \"SArray\"\n    elif _is_not_pickle_safe_gl_model_class(obj_class):\n        return \"Model\"\n    else:\n        return None", "output": "Internal util to get the type of the GLC class. The pickle file stores\n    this name so that it knows how to construct the object on unpickling.\n\n    Parameters\n    ----------\n    obj_class    : Class which has to be categorized.\n\n    Returns\n    ----------\n    A class type for the pickle file to save.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readerWalker(self):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlReaderWalker(self._o)\n        if ret is None:raise treeError('xmlReaderWalker() failed')\n        __tmp = xmlTextReader(_obj=ret)\n        return __tmp", "output": "Create an xmltextReader for a preparsed document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_gcs_file(path, out_fname=None, prefix_filter=None):\n  \"\"\"\"\"\"\n  url = posixpath.join(GCS_BUCKET, path)\n  if prefix_filter:\n    url += \"?prefix=%s\" % prefix_filter\n  stream = bool(out_fname)\n  resp = requests.get(url, stream=stream)\n  if not resp.ok:\n    raise ValueError(\"GCS bucket inaccessible\")\n  if out_fname:\n    with tf.io.gfile.GFile(out_fname, \"wb\") as f:\n      for chunk in resp.iter_content(1024):\n        f.write(chunk)\n  else:\n    return resp.content", "output": "Download a file from GCS, optionally to a file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def construct_error_message(driver_id, error_type, message, timestamp):\n    \"\"\"\n    \"\"\"\n    builder = flatbuffers.Builder(0)\n    driver_offset = builder.CreateString(driver_id.binary())\n    error_type_offset = builder.CreateString(error_type)\n    message_offset = builder.CreateString(message)\n\n    ray.core.generated.ErrorTableData.ErrorTableDataStart(builder)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddDriverId(\n        builder, driver_offset)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddType(\n        builder, error_type_offset)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddErrorMessage(\n        builder, message_offset)\n    ray.core.generated.ErrorTableData.ErrorTableDataAddTimestamp(\n        builder, timestamp)\n    error_data_offset = ray.core.generated.ErrorTableData.ErrorTableDataEnd(\n        builder)\n    builder.Finish(error_data_offset)\n\n    return bytes(builder.Output())", "output": "Construct a serialized ErrorTableData object.\n\n    Args:\n        driver_id: The ID of the driver that the error should go to. If this is\n            nil, then the error will go to all drivers.\n        error_type: The type of the error.\n        message: The error message.\n        timestamp: The time of the error.\n\n    Returns:\n        The serialized object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_all(recommended=False, restart=True):\n    '''\n    \n    '''\n    to_update = _get_available(recommended, restart)\n\n    if not to_update:\n        return {}\n\n    for _update in to_update:\n        cmd = ['softwareupdate', '--install', _update]\n        salt.utils.mac_utils.execute_return_success(cmd)\n\n    ret = {}\n    updates_left = _get_available()\n\n    for _update in to_update:\n        ret[_update] = True if _update not in updates_left else False\n\n    return ret", "output": "Install all available updates. Returns a dictionary containing the name\n    of the update and the status of its installation.\n\n    :param bool recommended: If set to True, only install the recommended\n        updates. If set to False (default) all updates are installed.\n\n    :param bool restart: Set this to False if you do not want to install updates\n        that require a restart. Default is True\n\n    :return: A dictionary containing the updates that were installed and the\n        status of its installation. If no updates were installed an empty\n        dictionary is returned.\n\n    :rtype: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n       salt '*' softwareupdate.update_all", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_get_trade_gap(start, end):\n    ''\n    start, end = QA_util_get_real_datelist(start, end)\n    if start is not None:\n        return trade_date_sse.index(end) + 1 - trade_date_sse.index(start)\n    else:\n        return 0", "output": "\u8fd4\u56destart_day\u5230end_day\u4e2d\u95f4\u6709\u591a\u5c11\u4e2a\u4ea4\u6613\u5929 \u7b97\u9996\u5c3e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toVector(value):\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(value, Vector):\n            return value\n        elif TypeConverters._can_convert_to_list(value):\n            value = TypeConverters.toList(value)\n            if all(map(lambda v: TypeConverters._is_numeric(v), value)):\n                return DenseVector(value)\n        raise TypeError(\"Could not convert %s to vector\" % value)", "output": "Convert a value to a MLlib Vector, if possible.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_subdirectories(path, include, exclude, show_all):\r\n    \"\"\"\"\"\"\r\n    try:\r\n        # > 1 because of '..'\r\n        return len( listdir(path, include, exclude,\r\n                            show_all, folders_only=True) ) > 1\r\n    except (IOError, OSError):\r\n        return False", "output": "Return True if path has subdirectories", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def push_note(device=None, title=None, body=None):\n    '''\n    \n    '''\n    spb = _SaltPushbullet(device)\n    res = spb.push_note(title, body)\n\n    return res", "output": "Pushing a text note.\n\n    :param device:   Pushbullet target device\n    :param title:    Note title\n    :param body:     Note body\n\n    :return:            Boolean if message was sent successfully.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt \"*\" pushbullet.push_note device=\"Chrome\" title=\"Example title\" body=\"Example body.\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pipe(obj, func, *args, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(func, tuple):\n        func, target = func\n        if target in kwargs:\n            msg = '%s is both the pipe target and a keyword argument' % target\n            raise ValueError(msg)\n        kwargs[target] = obj\n        return func(*args, **kwargs)\n    else:\n        return func(obj, *args, **kwargs)", "output": "Apply a function ``func`` to object ``obj`` either by passing obj as the\n    first argument to the function or, in the case that the func is a tuple,\n    interpret the first element of the tuple as a function and pass the obj to\n    that function as a keyword argument whose key is the value of the second\n    element of the tuple.\n\n    Parameters\n    ----------\n    func : callable or tuple of (callable, string)\n        Function to apply to this object or, alternatively, a\n        ``(callable, data_keyword)`` tuple where ``data_keyword`` is a\n        string indicating the keyword of `callable`` that expects the\n        object.\n    args : iterable, optional\n        positional arguments passed into ``func``.\n    kwargs : dict, optional\n        a dictionary of keyword arguments passed into ``func``.\n\n    Returns\n    -------\n    object : the return type of ``func``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def PopupGetText(message, title=None, default_text='', password_char='', size=(None, None), button_color=None,\n                 background_color=None, text_color=None, icon=DEFAULT_WINDOW_ICON, font=None, no_titlebar=False,\n                 grab_anywhere=False, keep_on_top=False, location=(None, None)):\n    \"\"\"\n    \n    \"\"\"\n\n    layout = [[Text(message, auto_size_text=True, text_color=text_color, background_color=background_color, font=font)],\n              [InputText(default_text=default_text, size=size, password_char=password_char)],\n              [CloseButton('Ok', size=(60, 20), bind_return_key=True), CloseButton('Cancel', size=(60, 20))]]\n\n    _title = title if title is not None else message\n\n    window = Window(title=_title, icon=icon, auto_size_text=True, button_color=button_color, no_titlebar=no_titlebar,\n                    background_color=background_color, grab_anywhere=grab_anywhere, keep_on_top=keep_on_top,\n                    location=location)\n\n    (button, input_values) = window.Layout(layout).Read()\n\n    if button != 'Ok':\n        return None\n    else:\n        return input_values[0]", "output": "Display Popup with text entry field\n    :param message:\n    :param default_text:\n    :param password_char:\n    :param size:\n    :param button_color:\n    :param background_color:\n    :param text_color:\n    :param icon:\n    :param font:\n    :param no_titlebar:\n    :param grab_anywhere:\n    :param keep_on_top:\n    :param location:\n    :return: Text entered or None if window was closed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compress_for_rename(paths):\n    \"\"\"\n    \"\"\"\n    case_map = dict((os.path.normcase(p), p) for p in paths)\n    remaining = set(case_map)\n    unchecked = sorted(set(os.path.split(p)[0]\n                           for p in case_map.values()), key=len)\n    wildcards = set()\n\n    def norm_join(*a):\n        return os.path.normcase(os.path.join(*a))\n\n    for root in unchecked:\n        if any(os.path.normcase(root).startswith(w)\n               for w in wildcards):\n            # This directory has already been handled.\n            continue\n\n        all_files = set()\n        all_subdirs = set()\n        for dirname, subdirs, files in os.walk(root):\n            all_subdirs.update(norm_join(root, dirname, d)\n                               for d in subdirs)\n            all_files.update(norm_join(root, dirname, f)\n                             for f in files)\n        # If all the files we found are in our remaining set of files to\n        # remove, then remove them from the latter set and add a wildcard\n        # for the directory.\n        if not (all_files - remaining):\n            remaining.difference_update(all_files)\n            wildcards.add(root + os.sep)\n\n    return set(map(case_map.__getitem__, remaining)) | wildcards", "output": "Returns a set containing the paths that need to be renamed.\n\n    This set may include directories when the original sequence of paths\n    included every file on disk.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch_defense_data(self):\n    \"\"\"\"\"\"\n    if self.defenses_data_initialized:\n      return\n    logging.info('Fetching defense data from datastore')\n    # init data from datastore\n    self.submissions.init_from_datastore()\n    self.dataset_batches.init_from_datastore()\n    self.adv_batches.init_from_datastore()\n    # read dataset metadata\n    self.read_dataset_metadata()\n    # mark as initialized\n    self.defenses_data_initialized = True", "output": "Lazy initialization of data necessary to execute defenses.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def createFileParserCtxt(filename):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlCreateFileParserCtxt(filename)\n    if ret is None:raise parserError('xmlCreateFileParserCtxt() failed')\n    return parserCtxt(_obj=ret)", "output": "Create a parser context for a file content. Automatic\n      support for ZLIB/Compress compressed document is provided\n       by default if found at compile-time.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def value_opt_step(i,\n                   opt_state,\n                   opt_update,\n                   value_net_apply,\n                   padded_observations,\n                   padded_rewards,\n                   reward_mask,\n                   gamma=0.99):\n  \"\"\"\"\"\"\n  value_params = trax_opt.get_params(opt_state)\n  # Note this partial application here and argnums above in ppo_opt_step.\n  g = grad(functools.partial(value_loss, value_net_apply))(\n      value_params,\n      padded_observations,\n      padded_rewards,\n      reward_mask,\n      gamma=gamma)\n  return opt_update(i, g, opt_state)", "output": "Value optimizer step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def state_apply(state_name, **kwargs):\n    '''\n    \n    '''\n    # A new salt client is instantiated with the default configuration because the main module's\n    # client is hardcoded to local\n    # If the minion is running with a master, a non-local client is needed to lookup states\n    caller = salt.client.Caller()\n    if kwargs:\n        return caller.cmd('state.apply', state_name, **kwargs)\n    else:\n        return caller.cmd('state.apply', state_name)", "output": "Runs :py:func:`state.apply <salt.modules.state.apply>` with given options to set up test data.\n    Intended to be used for optional test setup or teardown\n\n    Reference the :py:func:`state.apply <salt.modules.state.apply>` module documentation for arguments and usage options\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltcheck.state_apply postfix", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_pax_header(self, info, encoding):\n        \"\"\"\n        \"\"\"\n        info[\"magic\"] = POSIX_MAGIC\n        pax_headers = self.pax_headers.copy()\n\n        # Test string fields for values that exceed the field length or cannot\n        # be represented in ASCII encoding.\n        for name, hname, length in (\n                (\"name\", \"path\", LENGTH_NAME), (\"linkname\", \"linkpath\", LENGTH_LINK),\n                (\"uname\", \"uname\", 32), (\"gname\", \"gname\", 32)):\n\n            if hname in pax_headers:\n                # The pax header has priority.\n                continue\n\n            # Try to encode the string as ASCII.\n            try:\n                info[name].encode(\"ascii\", \"strict\")\n            except UnicodeEncodeError:\n                pax_headers[hname] = info[name]\n                continue\n\n            if len(info[name]) > length:\n                pax_headers[hname] = info[name]\n\n        # Test number fields for values that exceed the field limit or values\n        # that like to be stored as float.\n        for name, digits in ((\"uid\", 8), (\"gid\", 8), (\"size\", 12), (\"mtime\", 12)):\n            if name in pax_headers:\n                # The pax header has priority. Avoid overflow.\n                info[name] = 0\n                continue\n\n            val = info[name]\n            if not 0 <= val < 8 ** (digits - 1) or isinstance(val, float):\n                pax_headers[name] = str(val)\n                info[name] = 0\n\n        # Create a pax extended header if necessary.\n        if pax_headers:\n            buf = self._create_pax_generic_header(pax_headers, XHDTYPE, encoding)\n        else:\n            buf = b\"\"\n\n        return buf + self._create_header(info, USTAR_FORMAT, \"ascii\", \"replace\")", "output": "Return the object as a ustar header block. If it cannot be\n           represented this way, prepend a pax extended header sequence\n           with supplement information.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_trial_complete(self,\n                          trial_id,\n                          result=None,\n                          error=False,\n                          early_terminated=False):\n        \"\"\"\n        \"\"\"\n        ho_trial = self._get_hyperopt_trial(trial_id)\n        if ho_trial is None:\n            return\n        ho_trial[\"refresh_time\"] = hpo.utils.coarse_utcnow()\n        if error:\n            ho_trial[\"state\"] = hpo.base.JOB_STATE_ERROR\n            ho_trial[\"misc\"][\"error\"] = (str(TuneError), \"Tune Error\")\n        elif early_terminated:\n            ho_trial[\"state\"] = hpo.base.JOB_STATE_ERROR\n            ho_trial[\"misc\"][\"error\"] = (str(TuneError), \"Tune Removed\")\n        else:\n            ho_trial[\"state\"] = hpo.base.JOB_STATE_DONE\n            hp_result = self._to_hyperopt_result(result)\n            ho_trial[\"result\"] = hp_result\n        self._hpopt_trials.refresh()\n        del self._live_trial_mapping[trial_id]", "output": "Passes the result to HyperOpt unless early terminated or errored.\n\n        The result is internally negated when interacting with HyperOpt\n        so that HyperOpt can \"maximize\" this value, as it minimizes on default.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_valid_label(self, label):\n        \"\"\"\"\"\"\n        if len(label.shape) != 2 or label.shape[1] < 5:\n            msg = \"Label with shape (1+, 5+) required, %s received.\" % str(label)\n            raise RuntimeError(msg)\n        valid_label = np.where(np.logical_and(label[:, 0] >= 0, label[:, 3] > label[:, 1],\n                                              label[:, 4] > label[:, 2]))[0]\n        if valid_label.size < 1:\n            raise RuntimeError('Invalid label occurs.')", "output": "Validate label and its shape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def label_from_re(self, pat:str, full_path:bool=False, label_cls:Callable=None, **kwargs)->'LabelList':\n        \"\"\n        pat = re.compile(pat)\n        def _inner(o):\n            s = str((os.path.join(self.path,o) if full_path else o).as_posix())\n            res = pat.search(s)\n            assert res,f'Failed to find \"{pat}\" in \"{s}\"'\n            return res.group(1)\n        return self.label_from_func(_inner, label_cls=label_cls, **kwargs)", "output": "Apply the re in `pat` to determine the label of every filename.  If `full_path`, search in the full name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def port_policy_present(name, sel_type, protocol=None, port=None, sel_range=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': False, 'changes': {}, 'comment': ''}\n    old_state = __salt__['selinux.port_get_policy'](\n        name=name,\n        sel_type=sel_type,\n        protocol=protocol,\n        port=port, )\n    if old_state:\n        ret.update({'result': True,\n                    'comment': 'SELinux policy for \"{0}\" already present '.format(name) +\n                               'with specified sel_type \"{0}\", protocol \"{1}\" and port \"{2}\".'.format(\n                                   sel_type, protocol, port)})\n        return ret\n    if __opts__['test']:\n        ret.update({'result': None})\n    else:\n        add_ret = __salt__['selinux.port_add_policy'](\n            name=name,\n            sel_type=sel_type,\n            protocol=protocol,\n            port=port,\n            sel_range=sel_range, )\n        if add_ret['retcode'] != 0:\n            ret.update({'comment': 'Error adding new policy: {0}'.format(add_ret)})\n        else:\n            ret.update({'result': True})\n            new_state = __salt__['selinux.port_get_policy'](\n                name=name,\n                sel_type=sel_type,\n                protocol=protocol,\n                port=port, )\n            ret['changes'].update({'old': old_state, 'new': new_state})\n    return ret", "output": ".. versionadded:: 2019.2.0\n\n    Makes sure an SELinux port policy for a given port, protocol and SELinux context type is present.\n\n    name\n        The protocol and port spec. Can be formatted as ``(tcp|udp)/(port|port-range)``.\n\n    sel_type\n        The SELinux Type.\n\n    protocol\n        The protocol for the port, ``tcp`` or ``udp``. Required if name is not formatted.\n\n    port\n        The port or port range. Required if name is not formatted.\n\n    sel_range\n        The SELinux MLS/MCS Security Range.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wait(name, ignore_already_stopped=False, fail_on_exit_status=False):\n    '''\n    \n    '''\n    try:\n        pre = state(name)\n    except CommandExecutionError:\n        # Container doesn't exist anymore\n        return {'result': ignore_already_stopped,\n                'comment': 'Container \\'{0}\\' absent'.format(name)}\n    already_stopped = pre == 'stopped'\n    response = _client_wrapper('wait', name)\n    _clear_context()\n    try:\n        post = state(name)\n    except CommandExecutionError:\n        # Container doesn't exist anymore\n        post = None\n\n    if already_stopped:\n        success = ignore_already_stopped\n    elif post == 'stopped':\n        success = True\n    else:\n        success = False\n\n    result = {'result': success,\n              'state': {'old': pre, 'new': post},\n              'exit_status': response}\n    if already_stopped:\n        result['comment'] = 'Container \\'{0}\\' already stopped'.format(name)\n    if fail_on_exit_status and result['result']:\n        result['result'] = result['exit_status'] == 0\n    return result", "output": "Wait for the container to exit gracefully, and return its exit code\n\n    .. note::\n\n        This function will block until the container is stopped.\n\n    name\n        Container name or ID\n\n    ignore_already_stopped\n        Boolean flag that prevents execution to fail, if a container\n        is already stopped.\n\n    fail_on_exit_status\n        Boolean flag to report execution as failure if ``exit_status``\n        is different than 0.\n\n    **RETURN DATA**\n\n    A dictionary will be returned, containing the following keys:\n\n    - ``status`` - A dictionary showing the prior state of the container as\n      well as the new state\n    - ``result`` - A boolean noting whether or not the action was successful\n    - ``exit_status`` - Exit status for the container\n    - ``comment`` - Only present if the container is already stopped\n\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion docker.wait mycontainer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_(key, value, profile=None):\n    '''\n    \n    '''\n    db = _get_db(profile)\n    return db.save({'_id': uuid4().hex, key: value})", "output": "Set a key/value pair in couchdb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _combine_args(self):\n        \"\"\"\n        \"\"\"\n        child_path = self._parse_path(self._flat_path)\n\n        if self._parent is not None:\n            if self._parent.is_partial:\n                raise ValueError(\"Parent key must be complete.\")\n\n            # We know that _parent.path() will return a copy.\n            child_path = self._parent.path + child_path\n            self._flat_path = self._parent.flat_path + self._flat_path\n            if (\n                self._namespace is not None\n                and self._namespace != self._parent.namespace\n            ):\n                raise ValueError(\"Child namespace must agree with parent's.\")\n            self._namespace = self._parent.namespace\n            if self._project is not None and self._project != self._parent.project:\n                raise ValueError(\"Child project must agree with parent's.\")\n            self._project = self._parent.project\n\n        return child_path", "output": "Sets protected data by combining raw data set from the constructor.\n\n        If a ``_parent`` is set, updates the ``_flat_path`` and sets the\n        ``_namespace`` and ``_project`` if not already set.\n\n        :rtype: :class:`list` of :class:`dict`\n        :returns: A list of key parts with kind and ID or name set.\n        :raises: :class:`ValueError` if the parent key is not complete.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_file_ifaces(iface, data, **settings):\n    '''\n    \n    '''\n    try:\n        eth_template = JINJA.get_template('debian_eth.jinja')\n        source_template = JINJA.get_template('debian_source.jinja')\n    except jinja2.exceptions.TemplateNotFound:\n        log.error('Could not load template debian_eth.jinja')\n        return ''\n\n    # Read /etc/network/interfaces into a dict\n    adapters = _parse_interfaces()\n    # Apply supplied settings over on-disk settings\n    adapters[iface] = data\n\n    ifcfg = ''\n    for adapter in adapters:\n        if 'type' in adapters[adapter] and adapters[adapter]['type'] == 'source':\n            tmp = source_template.render({'name': adapter, 'data': adapters[adapter]})\n        else:\n            tmp = eth_template.render({'name': adapter, 'data': adapters[adapter]})\n        ifcfg = ifcfg + tmp\n        if adapter == iface:\n            saved_ifcfg = tmp\n\n    _SEPARATE_FILE = False\n    if 'filename' in settings:\n        if not settings['filename'].startswith('/'):\n            filename = '{0}/{1}'.format(_DEB_NETWORK_DIR, settings['filename'])\n        else:\n            filename = settings['filename']\n        _SEPARATE_FILE = True\n    else:\n        if 'filename' in adapters[adapter]['data']:\n            filename = adapters[adapter]['data']\n        else:\n            filename = _DEB_NETWORK_FILE\n\n    if not os.path.exists(os.path.dirname(filename)):\n        msg = '{0} cannot be written.'\n        msg = msg.format(os.path.dirname(filename))\n        log.error(msg)\n        raise AttributeError(msg)\n    with salt.utils.files.flopen(filename, 'w') as fout:\n        if _SEPARATE_FILE:\n            fout.write(salt.utils.stringutils.to_str(saved_ifcfg))\n        else:\n            fout.write(salt.utils.stringutils.to_str(ifcfg))\n\n    # Return as an array so the difflib works\n    return saved_ifcfg.split('\\n')", "output": "Writes a file to disk", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_namespaced_pod_preset(self, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_pod_preset_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_pod_preset_with_http_info(namespace, body, **kwargs)\n            return data", "output": "create a PodPreset\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_pod_preset(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1alpha1PodPreset body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1alpha1PodPreset\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mask_zero_div_zero(x, y, result, copy=False):\n    \"\"\"\n    \n    \"\"\"\n    if is_scalar(y):\n        y = np.array(y)\n\n    zmask = y == 0\n    if zmask.any():\n        shape = result.shape\n\n        nan_mask = (zmask & (x == 0)).ravel()\n        neginf_mask = (zmask & (x < 0)).ravel()\n        posinf_mask = (zmask & (x > 0)).ravel()\n\n        if nan_mask.any() or neginf_mask.any() or posinf_mask.any():\n            # Fill negative/0 with -inf, positive/0 with +inf, 0/0 with NaN\n            result = result.astype('float64', copy=copy).ravel()\n\n            np.putmask(result, nan_mask, np.nan)\n            np.putmask(result, posinf_mask, np.inf)\n            np.putmask(result, neginf_mask, -np.inf)\n\n            result = result.reshape(shape)\n\n    return result", "output": "Set results of 0 / 0 or 0 // 0 to np.nan, regardless of the dtypes\n    of the numerator or the denominator.\n\n    Parameters\n    ----------\n    x : ndarray\n    y : ndarray\n    result : ndarray\n    copy : bool (default False)\n        Whether to always create a new array or try to fill in the existing\n        array if possible.\n\n    Returns\n    -------\n    filled_result : ndarray\n\n    Examples\n    --------\n    >>> x = np.array([1, 0, -1], dtype=np.int64)\n    >>> y = 0       # int 0; numpy behavior is different with float\n    >>> result = x / y\n    >>> result      # raw numpy result does not fill division by zero\n    array([0, 0, 0])\n    >>> mask_zero_div_zero(x, y, result)\n    array([ inf,  nan, -inf])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def max_date(self, rows: List[Row], column: DateColumn) -> Date:\n        \"\"\"\n        \n        \"\"\"\n        cell_values = [row.values[column.name] for row in rows]\n        if not cell_values:\n            return Date(-1, -1, -1)\n        if not all([isinstance(value, Date) for value in cell_values]):\n            raise ExecutionError(f\"Invalid values for date selection function: {cell_values}\")\n        return max(cell_values)", "output": "Takes a list of rows and a column and returns the max of the values under that column in\n        those rows.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _adorn_subplots(self):\n        \"\"\"\"\"\"\n        if len(self.axes) > 0:\n            all_axes = self._get_subplots()\n            nrows, ncols = self._get_axes_layout()\n            _handle_shared_axes(axarr=all_axes, nplots=len(all_axes),\n                                naxes=nrows * ncols, nrows=nrows,\n                                ncols=ncols, sharex=self.sharex,\n                                sharey=self.sharey)\n\n        for ax in self.axes:\n            if self.yticks is not None:\n                ax.set_yticks(self.yticks)\n\n            if self.xticks is not None:\n                ax.set_xticks(self.xticks)\n\n            if self.ylim is not None:\n                ax.set_ylim(self.ylim)\n\n            if self.xlim is not None:\n                ax.set_xlim(self.xlim)\n\n            ax.grid(self.grid)\n\n        if self.title:\n            if self.subplots:\n                if is_list_like(self.title):\n                    if len(self.title) != self.nseries:\n                        msg = ('The length of `title` must equal the number '\n                               'of columns if using `title` of type `list` '\n                               'and `subplots=True`.\\n'\n                               'length of title = {}\\n'\n                               'number of columns = {}').format(\n                            len(self.title), self.nseries)\n                        raise ValueError(msg)\n\n                    for (ax, title) in zip(self.axes, self.title):\n                        ax.set_title(title)\n                else:\n                    self.fig.suptitle(self.title)\n            else:\n                if is_list_like(self.title):\n                    msg = ('Using `title` of type `list` is not supported '\n                           'unless `subplots=True` is passed')\n                    raise ValueError(msg)\n                self.axes[0].set_title(self.title)", "output": "Common post process unrelated to data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __interact_writen(self, fd, data):\n        '''\n        '''\n\n        while data != b'' and self.isalive():\n            n = os.write(fd, data)\n            data = data[n:]", "output": "This is used by the interact() method.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list(self, parts: Any) -> str:\n        \"\"\"\n        \"\"\"\n        _ = self.translate\n        if len(parts) == 0:\n            return \"\"\n        if len(parts) == 1:\n            return parts[0]\n        comma = u\" \\u0648 \" if self.code.startswith(\"fa\") else u\", \"\n        return _(\"%(commas)s and %(last)s\") % {\n            \"commas\": comma.join(parts[:-1]),\n            \"last\": parts[len(parts) - 1],\n        }", "output": "Returns a comma-separated list for the given list of parts.\n\n        The format is, e.g., \"A, B and C\", \"A and B\" or just \"A\" for lists\n        of size 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlNewDocNoDtD(URI, ExternalID):\n    \"\"\" \"\"\"\n    ret = libxml2mod.htmlNewDocNoDtD(URI, ExternalID)\n    if ret is None:raise treeError('htmlNewDocNoDtD() failed')\n    return xmlDoc(_obj=ret)", "output": "Creates a new HTML document without a DTD node if @URI and\n       @ExternalID are None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_dash(self, dashboard_id):\n        \"\"\"\"\"\"\n        session = db.session()\n        data = json.loads(request.form.get('data'))\n        dash = models.Dashboard()\n        original_dash = (\n            session\n            .query(models.Dashboard)\n            .filter_by(id=dashboard_id).first())\n\n        dash.owners = [g.user] if g.user else []\n        dash.dashboard_title = data['dashboard_title']\n\n        if data['duplicate_slices']:\n            # Duplicating slices as well, mapping old ids to new ones\n            old_to_new_sliceids = {}\n            for slc in original_dash.slices:\n                new_slice = slc.clone()\n                new_slice.owners = [g.user] if g.user else []\n                session.add(new_slice)\n                session.flush()\n                new_slice.dashboards.append(dash)\n                old_to_new_sliceids['{}'.format(slc.id)] = \\\n                    '{}'.format(new_slice.id)\n\n            # update chartId of layout entities\n            # in v2_dash positions json data, chartId should be integer,\n            # while in older version slice_id is string type\n            for value in data['positions'].values():\n                if (\n                    isinstance(value, dict) and value.get('meta') and\n                    value.get('meta').get('chartId')\n                ):\n                    old_id = '{}'.format(value.get('meta').get('chartId'))\n                    new_id = int(old_to_new_sliceids[old_id])\n                    value['meta']['chartId'] = new_id\n        else:\n            dash.slices = original_dash.slices\n        dash.params = original_dash.params\n\n        self._set_dash_metadata(dash, data)\n        session.add(dash)\n        session.commit()\n        dash_json = json.dumps(dash.data)\n        session.close()\n        return json_success(dash_json)", "output": "Copy dashboard", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_resource_attribute(self, attr):\n        \"\"\"\n        \"\"\"\n        if attr not in self.resource_attributes:\n            raise KeyError(\"%s is not in resource attributes\" % attr)\n\n        return self.resource_attributes[attr]", "output": "Gets the resource attribute if available\n\n        :param attr: Name of the attribute\n        :return: Value of the attribute, if set in the resource. None otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(key, default=None):\n    '''\n    \n    '''\n    store = load()\n\n    if isinstance(key, six.string_types):\n        return store.get(key, default)\n    elif default is None:\n        return [store[k] for k in key if k in store]\n    else:\n        return [store.get(k, default) for k in key]", "output": "Get a (list of) value(s) from the minion datastore\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' data.get key\n        salt '*' data.get '[\"key1\", \"key2\"]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nearest_neighbor(self, x, means):\n    \"\"\"\n    \"\"\"\n    x_norm_sq = tf.reduce_sum(tf.square(x), axis=-1, keep_dims=True)\n    means_norm_sq = tf.reduce_sum(tf.square(means), axis=-1, keep_dims=True)\n    scalar_prod = tf.matmul(\n        tf.transpose(x, perm=[1, 0, 2]), tf.transpose(means, perm=[0, 2, 1]))\n    scalar_prod = tf.transpose(scalar_prod, perm=[1, 0, 2])\n    dist = x_norm_sq + tf.transpose(\n        means_norm_sq, perm=[2, 0, 1]) - 2 * scalar_prod\n\n    if self.hparams.soft_em:\n      nearest_idx = tf.stack(\n          [\n              tf.multinomial(\n                  -dist[:, i, :], num_samples=self.hparams.num_samples)\n              for i in range(self.hparams.num_blocks)\n          ],\n          axis=1)\n      nearest_hot = tf.one_hot(nearest_idx, depth=self.hparams.block_v_size)\n      nearest_hot = tf.reduce_mean(nearest_hot, axis=-2)\n    else:\n      if self.hparams.random_top_k > 1:\n        _, top_k_idx = tf.nn.top_k(-dist, k=self.hparams.random_top_k)\n        nearest_idx = tf.gather(\n            top_k_idx,\n            tf.random_uniform(\n                [1],\n                minval=0,\n                maxval=self.hparams.random_top_k - 1,\n                dtype=tf.int32),\n            axis=-1)\n      else:\n        if self.hparams.use_scales:\n          dist /= tf.reshape(self.hparams.scales,\n                             [1, 1, self.hparams.moe_num_experts])\n        nearest_idx = tf.argmax(-dist, axis=-1)\n      nearest_hot = tf.one_hot(nearest_idx, self.hparams.block_v_size)\n    return nearest_hot", "output": "Find the nearest element in means to elements in x.\n\n    Args:\n        x: Batch of encoder continuous latent states sliced/projected into\n           shape [-1, num_blocks, block_dim].\n        means: Embedding means of shape.\n\n    Returns:\n      Tensor with nearest element in mean encoded in one-hot notation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def as_objective(obj):\n  \"\"\"\n  \"\"\"\n  if isinstance(obj, Objective):\n    return obj\n  elif callable(obj):\n    return obj\n  elif isinstance(obj, str):\n    layer, n = obj.split(\":\")\n    layer, n = layer.strip(), int(n)\n    return channel(layer, n)", "output": "Convert obj into Objective class.\n\n  Strings of the form \"layer:n\" become the Objective channel(layer, n).\n  Objectives are returned unchanged.\n\n  Args:\n    obj: string or Objective.\n\n  Returns:\n    Objective", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute_only_once():\n    \"\"\"\n    \n    \"\"\"\n    f = inspect.currentframe().f_back\n    ident = (f.f_code.co_filename, f.f_lineno)\n    if ident in _EXECUTE_HISTORY:\n        return False\n    _EXECUTE_HISTORY.add(ident)\n    return True", "output": "Each called in the code to this function is guaranteed to return True the\n    first time and False afterwards.\n\n    Returns:\n        bool: whether this is the first time this function gets called from this line of code.\n\n    Example:\n        .. code-block:: python\n\n            if execute_only_once():\n                # do something only once", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def task_absent(name):\n    '''\n    \n    '''\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n\n    task = __salt__['kapacitor.get_task'](name)\n\n    if task:\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = 'Task would have been deleted'\n        else:\n            result = __salt__['kapacitor.delete_task'](name)\n            ret['result'] = result['success']\n            if not ret['result']:\n                ret['comment'] = 'Could not disable task'\n                if result.get('stderr'):\n                    ret['comment'] += '\\n' + result['stderr']\n                return ret\n            ret['comment'] = 'Task was deleted'\n        ret['changes'][name] = 'deleted'\n    else:\n        ret['comment'] = 'Task does not exist'\n\n    return ret", "output": "Ensure that a task is absent from Kapacitor.\n\n    name\n        Name of the task.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def combine_concat_plans(plans, concat_axis):\n    \"\"\"\n    \n    \"\"\"\n    if len(plans) == 1:\n        for p in plans[0]:\n            yield p[0], [p[1]]\n\n    elif concat_axis == 0:\n        offset = 0\n        for plan in plans:\n            last_plc = None\n\n            for plc, unit in plan:\n                yield plc.add(offset), [unit]\n                last_plc = plc\n\n            if last_plc is not None:\n                offset += last_plc.as_slice.stop\n\n    else:\n        num_ended = [0]\n\n        def _next_or_none(seq):\n            retval = next(seq, None)\n            if retval is None:\n                num_ended[0] += 1\n            return retval\n\n        plans = list(map(iter, plans))\n        next_items = list(map(_next_or_none, plans))\n\n        while num_ended[0] != len(next_items):\n            if num_ended[0] > 0:\n                raise ValueError(\"Plan shapes are not aligned\")\n\n            placements, units = zip(*next_items)\n\n            lengths = list(map(len, placements))\n            min_len, max_len = min(lengths), max(lengths)\n\n            if min_len == max_len:\n                yield placements[0], units\n                next_items[:] = map(_next_or_none, plans)\n            else:\n                yielded_placement = None\n                yielded_units = [None] * len(next_items)\n                for i, (plc, unit) in enumerate(next_items):\n                    yielded_units[i] = unit\n                    if len(plc) > min_len:\n                        # trim_join_unit updates unit in place, so only\n                        # placement needs to be sliced to skip min_len.\n                        next_items[i] = (plc[min_len:],\n                                         trim_join_unit(unit, min_len))\n                    else:\n                        yielded_placement = plc\n                        next_items[i] = _next_or_none(plans[i])\n\n                yield yielded_placement, yielded_units", "output": "Combine multiple concatenation plans into one.\n\n    existing_plan is updated in-place.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zfill(self, width):\n        \"\"\"\n        \n        \"\"\"\n        result = str_pad(self._parent, width, side='left', fillchar='0')\n        return self._wrap_result(result)", "output": "Pad strings in the Series/Index by prepending '0' characters.\n\n        Strings in the Series/Index are padded with '0' characters on the\n        left of the string to reach a total string length  `width`. Strings\n        in the Series/Index with length greater or equal to `width` are\n        unchanged.\n\n        Parameters\n        ----------\n        width : int\n            Minimum length of resulting string; strings with length less\n            than `width` be prepended with '0' characters.\n\n        Returns\n        -------\n        Series/Index of objects\n\n        See Also\n        --------\n        Series.str.rjust : Fills the left side of strings with an arbitrary\n            character.\n        Series.str.ljust : Fills the right side of strings with an arbitrary\n            character.\n        Series.str.pad : Fills the specified sides of strings with an arbitrary\n            character.\n        Series.str.center : Fills boths sides of strings with an arbitrary\n            character.\n\n        Notes\n        -----\n        Differs from :meth:`str.zfill` which has special handling\n        for '+'/'-' in the string.\n\n        Examples\n        --------\n        >>> s = pd.Series(['-1', '1', '1000', 10, np.nan])\n        >>> s\n        0      -1\n        1       1\n        2    1000\n        3      10\n        4     NaN\n        dtype: object\n\n        Note that ``10`` and ``NaN`` are not strings, therefore they are\n        converted to ``NaN``. The minus sign in ``'-1'`` is treated as a\n        regular character and the zero is added to the left of it\n        (:meth:`str.zfill` would have moved it to the left). ``1000``\n        remains unchanged as it is longer than `width`.\n\n        >>> s.str.zfill(3)\n        0     0-1\n        1     001\n        2    1000\n        3     NaN\n        4     NaN\n        dtype: object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_proto(self, message, **kw):\n        \"\"\"\n        \"\"\"\n        self.entries.append(ProtobufEntry(payload=message, **kw))", "output": "Add a protobuf entry to be logged during :meth:`commit`.\n\n        :type message: protobuf message\n        :param message: the protobuf entry\n\n        :type kw: dict\n        :param kw: (optional) additional keyword arguments for the entry.\n                   See :class:`~google.cloud.logging.entries.LogEntry`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def close(self):\n        \"\"\"\n        \"\"\"\n        if self._closed:\n            return\n\n        await self.http.close()\n        self._closed = True\n\n        for voice in self.voice_clients:\n            try:\n                await voice.disconnect()\n            except Exception:\n                # if an error happens during disconnects, disregard it.\n                pass\n\n        if self.ws is not None and self.ws.open:\n            await self.ws.close()\n\n        self._ready.clear()", "output": "|coro|\n\n        Closes the connection to discord.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setNNIManagerIp(experiment_config, port, config_file_name):\n    ''''''\n    if experiment_config.get('nniManagerIp') is None:\n        return True, None\n    ip_config_dict = dict()\n    ip_config_dict['nni_manager_ip'] = { 'nniManagerIp' : experiment_config['nniManagerIp'] }\n    response = rest_put(cluster_metadata_url(port), json.dumps(ip_config_dict), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    return True, None", "output": "set nniManagerIp", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename_client_tab(self, client, given_name):\r\n        \"\"\"\"\"\"\r\n        index = self.get_client_index_from_id(id(client))\r\n\r\n        if given_name is not None:\r\n            client.given_name = given_name\r\n        self.tabwidget.setTabText(index, client.get_name())", "output": "Rename client's tab", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cached_manylinux_wheel(self, package_name, package_version, disable_progress=False):\n        \"\"\"\n        \n        \"\"\"\n        cached_wheels_dir = os.path.join(tempfile.gettempdir(), 'cached_wheels')\n        if not os.path.isdir(cached_wheels_dir):\n            os.makedirs(cached_wheels_dir)\n\n        wheel_file = '{0!s}-{1!s}-{2!s}'.format(package_name, package_version, self.manylinux_wheel_file_suffix)\n        wheel_path = os.path.join(cached_wheels_dir, wheel_file)\n\n        if not os.path.exists(wheel_path) or not zipfile.is_zipfile(wheel_path):\n            # The file is not cached, download it.\n            wheel_url = self.get_manylinux_wheel_url(package_name, package_version)\n            if not wheel_url:\n                return None\n\n            print(\" - {}=={}: Downloading\".format(package_name, package_version))\n            with open(wheel_path, 'wb') as f:\n                self.download_url_with_progress(wheel_url, f, disable_progress)\n\n            if not zipfile.is_zipfile(wheel_path):\n                return None\n        else:\n            print(\" - {}=={}: Using locally cached manylinux wheel\".format(package_name, package_version))\n\n        return wheel_path", "output": "Gets the locally stored version of a manylinux wheel. If one does not exist, the function downloads it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _render_filenames(filenames, zip_file, saltenv, template):\n    '''\n    \n    '''\n    if not template:\n        return (filenames, zip_file)\n\n    # render the path as a template using path_template_engine as the engine\n    if template not in salt.utils.templates.TEMPLATE_REGISTRY:\n        raise CommandExecutionError(\n            'Attempted to render file paths with unavailable engine '\n            '{0}'.format(template)\n        )\n\n    kwargs = {}\n    kwargs['salt'] = __salt__\n    kwargs['pillar'] = __pillar__\n    kwargs['grains'] = __grains__\n    kwargs['opts'] = __opts__\n    kwargs['saltenv'] = saltenv\n\n    def _render(contents):\n        '''\n        Render :param:`contents` into a literal pathname by writing it to a\n        temp file, rendering that file, and returning the result.\n        '''\n        # write out path to temp file\n        tmp_path_fn = salt.utils.files.mkstemp()\n        with salt.utils.files.fopen(tmp_path_fn, 'w+') as fp_:\n            fp_.write(salt.utils.stringutils.to_str(contents))\n        data = salt.utils.templates.TEMPLATE_REGISTRY[template](\n            tmp_path_fn,\n            to_str=True,\n            **kwargs\n        )\n        salt.utils.files.safe_rm(tmp_path_fn)\n        if not data['result']:\n            # Failed to render the template\n            raise CommandExecutionError(\n                'Failed to render file path with error: {0}'.format(\n                    data['data']\n                )\n            )\n        else:\n            return data['data']\n\n    filenames = _render(filenames)\n    zip_file = _render(zip_file)\n    return (filenames, zip_file)", "output": "Process markup in the :param:`filenames` and :param:`zipfile` variables (NOT the\n    files under the paths they ultimately point to) according to the markup\n    format provided by :param:`template`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_scanner (type, prop_set):\n    \"\"\" \n    \"\"\"\n    if __debug__:\n        from .property_set import PropertySet\n        assert isinstance(type, basestring)\n        assert isinstance(prop_set, PropertySet)\n    if registered (type):\n        scanner_type = __types [type]['scanner']\n        if scanner_type:\n            return scanner.get (scanner_type, prop_set.raw ())\n            pass\n\n    return None", "output": "Returns a scanner instance appropriate to 'type' and 'property_set'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def gae_advantages(td_deltas, mask, lambda_=0.95, gamma=0.99):\n  \n  \"\"\"\n\n  return rewards_to_go(td_deltas, mask, lambda_ * gamma)", "output": "r\"\"\"Computes the GAE advantages given the one step TD-residuals.\n\n  The formula for a GAE advantage estimator is as follows:\n\n  A_{bt} = \\sum_{l=0}^{\\infty}(\\gamma * \\lambda)^{l}(\\delta_{b,t+l}).\n\n  Internally we just call rewards_to_go, since it is the same computation.\n\n  Args:\n    td_deltas: np.ndarray of shape (B, T) of one step TD-residuals.\n    mask: np.ndarray of shape (B, T) of mask for the residuals. It maybe the\n      case that the `td_deltas` are already masked correctly since they are\n      produced by `deltas(...)`\n    lambda_: float, lambda parameter for GAE estimators.\n    gamma: float, lambda parameter for GAE estimators.\n\n  Returns:\n    GAE advantage estimates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __init_from_np2d(self, mat, params_str, ref_dataset):\n        \"\"\"\"\"\"\n        if len(mat.shape) != 2:\n            raise ValueError('Input numpy.ndarray must be 2 dimensional')\n\n        self.handle = ctypes.c_void_p()\n        if mat.dtype == np.float32 or mat.dtype == np.float64:\n            data = np.array(mat.reshape(mat.size), dtype=mat.dtype, copy=False)\n        else:\n            # change non-float data to float data, need to copy\n            data = np.array(mat.reshape(mat.size), dtype=np.float32)\n\n        ptr_data, type_ptr_data, _ = c_float_array(data)\n        _safe_call(_LIB.LGBM_DatasetCreateFromMat(\n            ptr_data,\n            ctypes.c_int(type_ptr_data),\n            ctypes.c_int(mat.shape[0]),\n            ctypes.c_int(mat.shape[1]),\n            ctypes.c_int(C_API_IS_ROW_MAJOR),\n            c_str(params_str),\n            ref_dataset,\n            ctypes.byref(self.handle)))\n        return self", "output": "Initialize data from a 2-D numpy matrix.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def public_url(self):\n        \"\"\"\n        \"\"\"\n        return \"{storage_base_url}/{bucket_name}/{quoted_name}\".format(\n            storage_base_url=_API_ACCESS_ENDPOINT,\n            bucket_name=self.bucket.name,\n            quoted_name=quote(self.name.encode(\"utf-8\")),\n        )", "output": "The public URL for this blob.\n\n        Use :meth:`make_public` to enable anonymous access via the returned\n        URL.\n\n        :rtype: `string`\n        :returns: The public URL for this blob.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def request_cert(name, master, ticket, port=\"5665\"):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n    cert = \"{0}ca.crt\".format(get_certs_path())\n\n    # Checking if execution is needed.\n    if os.path.isfile(cert):\n        ret['comment'] = 'No execution needed. Cert: {0} already exists.'.format(cert)\n        return ret\n    if __opts__['test']:\n        ret['result'] = None\n        ret['comment'] = 'Certificate request from icinga2 master would be executed'\n        return ret\n\n    # Executing the command.\n    cert_request = __salt__['icinga2.request_cert'](name, master, ticket, port)\n    if not cert_request['retcode']:\n        ret['comment'] = \"Certificate request from icinga2 master executed\"\n        ret['changes']['cert'] = \"Executed. Certificate requested: {0}\".format(cert)\n        return ret\n\n    ret['comment'] = \"FAILED. Certificate requested failed with output: {0}\".format(cert_request['stdout'])\n    ret['result'] = False\n    return ret", "output": "Request CA certificate from master icinga2 node.\n\n    name\n        The domain name for which this certificate will be saved\n\n    master\n        Icinga2 master node for which this certificate will be saved\n\n    ticket\n        Authentication ticket generated on icinga2 master\n\n    port\n        Icinga2 port, defaults to 5665", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all(self):\n        \"\"\"\n        \"\"\"\n        if not self.vars:\n            return self.parent\n        if not self.parent:\n            return self.vars\n        return dict(self.parent, **self.vars)", "output": "Return the complete context as dict including the exported\n        variables.  For optimizations reasons this might not return an\n        actual copy so be careful with using it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _GetMessageFromFactory(factory, full_name):\n  \"\"\"\n  \"\"\"\n  proto_descriptor = factory.pool.FindMessageTypeByName(full_name)\n  proto_cls = factory.GetPrototype(proto_descriptor)\n  return proto_cls", "output": "Get a proto class from the MessageFactory by name.\n\n  Args:\n    factory: a MessageFactory instance.\n    full_name: str, the fully qualified name of the proto type.\n  Returns:\n    A class, for the type identified by full_name.\n  Raises:\n    KeyError, if the proto is not found in the factory's descriptor pool.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename(self, new_name):\r\n        \"\"\"\"\"\"\r\n        old_name = self.name\r\n        self.name = new_name\r\n        pypath = self.relative_pythonpath  # ??\r\n        self.root_path = self.root_path[:-len(old_name)]+new_name\r\n        self.relative_pythonpath = pypath  # ??\r\n        self.save()", "output": "Rename project and rename its root path accordingly.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Expand(self, macro_ref_str):\n    \"\"\"\n    \"\"\"\n    match = _MACRO_RE.match(macro_ref_str)\n    if match is None or match.group(0) != macro_ref_str:\n      raise PDDMError('Failed to parse macro reference: \"%s\"' % macro_ref_str)\n    if match.group('name') not in self._macros:\n      raise PDDMError('No macro named \"%s\".' % match.group('name'))\n    return self._Expand(match, [], macro_ref_str)", "output": "Expands the macro reference.\n\n    Args:\n      macro_ref_str: String of a macro reference (i.e. foo(a, b)).\n\n    Returns:\n      The text from the expansion.\n\n    Raises:\n      PDDMError if there are any issues.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set(config, section, opt, value):\n        \"\"\"\n        \"\"\"\n        if section not in config.keys():\n            config[section] = {}\n\n        config[section][opt] = value", "output": "Sets specified option in the config.\n\n        Args:\n            config (configobj.ConfigObj): config to work on.\n            section (str): section name.\n            opt (str): option name.\n            value: value to set option to.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def format(self, source):\n        \"\"\"\n        \"\"\"\n        self._jwrite = self._jwrite.format(source)\n        return self", "output": "Specifies the underlying output data source.\n\n        :param source: string, name of the data source, e.g. 'json', 'parquet'.\n\n        >>> df.write.format('json').save(os.path.join(tempfile.mkdtemp(), 'data'))", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init_params(net):\n    \"\"\"\"\"\"\n    for module in net.modules():\n        if isinstance(module, nn.Conv2d):\n            init.kaiming_normal(module.weight, mode=\"fan_out\")\n            if module.bias:\n                init.constant(module.bias, 0)\n        elif isinstance(module, nn.BatchNorm2d):\n            init.constant(module.weight, 1)\n            init.constant(module.bias, 0)\n        elif isinstance(module, nn.Linear):\n            init.normal(module.weight, std=1e-3)\n            if module.bias:\n                init.constant(module.bias, 0)", "output": "Init layer parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_warning_menu(self):\r\n        \"\"\"\"\"\"\r\n        editor = self.get_current_editor()\r\n        check_results = editor.get_current_warnings()\r\n        self.warning_menu.clear()\r\n        filename = self.get_current_filename()\r\n        for message, line_number in check_results:\r\n            error = 'syntax' in message\r\n            text = message[:1].upper() + message[1:]\r\n            icon = ima.icon('error') if error else ima.icon('warning')\r\n            slot = lambda _checked, _l=line_number: self.load(filename, goto=_l)\r\n            action = create_action(self, text=text, icon=icon, triggered=slot)\r\n            self.warning_menu.addAction(action)", "output": "Update warning list menu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chugid_and_umask(runas, umask, group=None):\n    '''\n    \n    '''\n    set_runas = False\n    set_grp = False\n\n    current_user = getpass.getuser()\n    if runas and runas != current_user:\n        set_runas = True\n        runas_user = runas\n    else:\n        runas_user = current_user\n\n    current_grp = grp.getgrgid(pwd.getpwnam(getpass.getuser()).pw_gid).gr_name\n    if group and group != current_grp:\n        set_grp = True\n        runas_grp = group\n    else:\n        runas_grp = current_grp\n\n    if set_runas or set_grp:\n        chugid(runas_user, runas_grp)\n    if umask is not None:\n        os.umask(umask)", "output": "Helper method for for subprocess.Popen to initialise uid/gid and umask\n    for the new process.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_chinese_char(self, cp):\n        \"\"\"\"\"\"\n        # This defines a \"chinese character\" as anything in the CJK Unicode block:\n        #   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n        #\n        # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n        # despite its name. The modern Korean Hangul alphabet is a different block,\n        # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n        # space-separated words, so they are not treated specially and handled\n        # like the all of the other languages.\n        if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n                (cp >= 0x3400 and cp <= 0x4DBF) or  #\n                (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n                (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n                (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n                (cp >= 0x2B820 and cp <= 0x2CEAF) or\n                (cp >= 0xF900 and cp <= 0xFAFF) or  #\n                (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n            return True\n\n        return False", "output": "Checks whether CP is the codepoint of a CJK character.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    \n    \"\"\"\n    if model == \"cnn\":\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n    else:\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]", "output": "Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_numeric_methods_binary(cls):\n        \"\"\"\n        \n        \"\"\"\n        cls.__add__ = _make_arithmetic_op(operator.add, cls)\n        cls.__radd__ = _make_arithmetic_op(ops.radd, cls)\n        cls.__sub__ = _make_arithmetic_op(operator.sub, cls)\n        cls.__rsub__ = _make_arithmetic_op(ops.rsub, cls)\n        cls.__rpow__ = _make_arithmetic_op(ops.rpow, cls)\n        cls.__pow__ = _make_arithmetic_op(operator.pow, cls)\n\n        cls.__truediv__ = _make_arithmetic_op(operator.truediv, cls)\n        cls.__rtruediv__ = _make_arithmetic_op(ops.rtruediv, cls)\n\n        # TODO: rmod? rdivmod?\n        cls.__mod__ = _make_arithmetic_op(operator.mod, cls)\n        cls.__floordiv__ = _make_arithmetic_op(operator.floordiv, cls)\n        cls.__rfloordiv__ = _make_arithmetic_op(ops.rfloordiv, cls)\n        cls.__divmod__ = _make_arithmetic_op(divmod, cls)\n        cls.__mul__ = _make_arithmetic_op(operator.mul, cls)\n        cls.__rmul__ = _make_arithmetic_op(ops.rmul, cls)", "output": "Add in numeric methods.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def masked_max(vector: torch.Tensor,\n               mask: torch.Tensor,\n               dim: int,\n               keepdim: bool = False,\n               min_val: float = -1e7) -> torch.Tensor:\n    \"\"\"\n    \n    \"\"\"\n    one_minus_mask = (1.0 - mask).byte()\n    replaced_vector = vector.masked_fill(one_minus_mask, min_val)\n    max_value, _ = replaced_vector.max(dim=dim, keepdim=keepdim)\n    return max_value", "output": "To calculate max along certain dimensions on masked values\n\n    Parameters\n    ----------\n    vector : ``torch.Tensor``\n        The vector to calculate max, assume unmasked parts are already zeros\n    mask : ``torch.Tensor``\n        The mask of the vector. It must be broadcastable with vector.\n    dim : ``int``\n        The dimension to calculate max\n    keepdim : ``bool``\n        Whether to keep dimension\n    min_val : ``float``\n        The minimal value for paddings\n\n    Returns\n    -------\n    A ``torch.Tensor`` of including the maximum values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_parameters(self, parameter_id):\n        \"\"\"\n        \n        \"\"\"\n        if not self.history:\n            self.init_search()\n\n        new_father_id = None\n        generated_graph = None\n        if not self.training_queue:\n            new_father_id, generated_graph = self.generate()\n            new_model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append((generated_graph, new_father_id, new_model_id))\n            self.descriptors.append(generated_graph.extract_descriptor())\n\n        graph, father_id, model_id = self.training_queue.pop(0)\n\n        # from graph to json\n        json_model_path = os.path.join(self.path, str(model_id) + \".json\")\n        json_out = graph_to_json(graph, json_model_path)\n        self.total_data[parameter_id] = (json_out, father_id, model_id)\n\n        return json_out", "output": "Returns a set of trial neural architecture, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_in_notebook(self,\n                         labels=None,\n                         predict_proba=True,\n                         show_predicted_value=True,\n                         **kwargs):\n        \"\"\"\"\"\"\n\n        from IPython.core.display import display, HTML\n        display(HTML(self.as_html(labels=labels,\n                                  predict_proba=predict_proba,\n                                  show_predicted_value=show_predicted_value,\n                                  **kwargs)))", "output": "Shows html explanation in ipython notebook.\n\n        See as_html() for parameters.\n        This will throw an error if you don't have IPython installed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_rabit_checkpoint(self):\n        \"\"\"\n        \"\"\"\n        version = ctypes.c_int()\n        _check_call(_LIB.XGBoosterLoadRabitCheckpoint(\n            self.handle, ctypes.byref(version)))\n        return version.value", "output": "Initialize the model by load from rabit checkpoint.\n\n        Returns\n        -------\n        version: integer\n            The version number of the model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_bulk_size(size):\n    \"\"\"\n    \"\"\"\n    prev = ctypes.c_int()\n    check_call(_LIB.MXEngineSetBulkSize(\n        ctypes.c_int(size), ctypes.byref(prev)))\n    return prev.value", "output": "Set size limit on bulk execution.\n\n    Bulk execution bundles many operators to run together.\n    This can improve performance when running a lot of small\n    operators sequentially.\n\n    Parameters\n    ----------\n    size : int\n        Maximum number of operators that can be bundled in a bulk.\n\n    Returns\n    -------\n    int\n        Previous bulk size.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getConf(self, key, defaultValue=_NoValue):\n        \"\"\"\n        \"\"\"\n        return self.sparkSession.conf.get(key, defaultValue)", "output": "Returns the value of Spark SQL configuration property for the given key.\n\n        If the key is not set and defaultValue is set, return\n        defaultValue. If the key is not set and defaultValue is not set, return\n        the system default value.\n\n        >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\")\n        u'200'\n        >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\n        u'10'\n        >>> sqlContext.setConf(\"spark.sql.shuffle.partitions\", u\"50\")\n        >>> sqlContext.getConf(\"spark.sql.shuffle.partitions\", u\"10\")\n        u'50'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def delete_invite(self, invite):\n        \"\"\"\n        \"\"\"\n\n        invite_id = utils.resolve_invite(invite)\n        await self.http.delete_invite(invite_id)", "output": "|coro|\n\n        Revokes an :class:`.Invite`, URL, or ID to an invite.\n\n        You must have the :attr:`~.Permissions.manage_channels` permission in\n        the associated guild to do this.\n\n        Parameters\n        ----------\n        invite: Union[:class:`.Invite`, :class:`str`]\n            The invite to revoke.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to revoke invites.\n        NotFound\n            The invite is invalid or expired.\n        HTTPException\n            Revoking the invite failed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def preserve_channel_dim(func):\n    \"\"\"\"\"\"\n    @wraps(func)\n    def wrapped_function(img, *args, **kwargs):\n        shape = img.shape\n        result = func(img, *args, **kwargs)\n        if len(shape) == 3 and shape[-1] == 1 and len(result.shape) == 2:\n            result = np.expand_dims(result, axis=-1)\n        return result\n\n    return wrapped_function", "output": "Preserve dummy channel dim.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def translate_bytes(val):\n    '''\n    \n    '''\n    try:\n        val = int(val)\n    except (TypeError, ValueError):\n        if not isinstance(val, six.string_types):\n            val = six.text_type(val)\n    return val", "output": "These values can be expressed as an integer number of bytes, or a string\n    expression (i.e. 100mb, 1gb, etc.).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def addlist(self, key, valuelist=[]):\n        \"\"\"\n        \n        \"\"\"\n        for value in valuelist:\n            self.add(key, value)\n        return self", "output": "Add the values in <valuelist> to the list of values for <key>. If <key>\n        is not in the dictionary, the values in <valuelist> become the values\n        for <key>.\n\n        Example:\n          omd = omdict([(1,1)])\n          omd.addlist(1, [11, 111])\n          omd.allitems() == [(1, 1), (1, 11), (1, 111)]\n          omd.addlist(2, [2])\n          omd.allitems() == [(1, 1), (1, 11), (1, 111), (2, 2)]\n\n        Returns: <self>.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def channel_interpolate(layer1, n_channel1, layer2, n_channel2):\n  \"\"\"\n  \"\"\"\n  def inner(T):\n    batch_n = T(layer1).get_shape().as_list()[0]\n    arr1 = T(layer1)[..., n_channel1]\n    arr2 = T(layer2)[..., n_channel2]\n    weights = (np.arange(batch_n)/float(batch_n-1))\n    S = 0\n    for n in range(batch_n):\n      S += (1-weights[n]) * tf.reduce_mean(arr1[n])\n      S += weights[n] * tf.reduce_mean(arr2[n])\n    return S\n  return inner", "output": "Interpolate between layer1, n_channel1 and layer2, n_channel2.\n\n  Optimize for a convex combination of layer1, n_channel1 and\n  layer2, n_channel2, transitioning across the batch.\n\n  Args:\n    layer1: layer to optimize 100% at batch=0.\n    n_channel1: neuron index to optimize 100% at batch=0.\n    layer2: layer to optimize 100% at batch=N.\n    n_channel2: neuron index to optimize 100% at batch=N.\n\n  Returns:\n    Objective", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextParent(self, ctxt):\n        \"\"\" \"\"\"\n        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        ret = libxml2mod.xmlXPathNextParent(ctxt__o, self._o)\n        if ret is None:raise xpathError('xmlXPathNextParent() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"parent\" direction The parent\n          axis contains the parent of the context node, if there is\n           one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_root_path(self, language):\n        \"\"\"\n        \n        \"\"\"\n        path = None\n\n        # Get path of the current project\n        if self.main and self.main.projects:\n            path = self.main.projects.get_active_project_path()\n\n        # If there's no project, use the output of getcwd_or_home.\n        if not path:\n            # We can't use getcwd_or_home for Python because if it\n            # returns home and you have a lot of Python files on it\n            # then computing Rope completions takes a long time\n            # and blocks the PyLS server.\n            # Instead we use an empty directory inside our config one,\n            # just like we did for Rope in Spyder 3.\n            if language == 'python':\n                path = get_conf_path('lsp_root_path')\n                if not osp.exists(path):\n                    os.mkdir(path)\n            else:\n                path = getcwd_or_home()\n\n        return path", "output": "Get root path to pass to the LSP servers.\n\n        This can be the current project path or the output of\n        getcwd_or_home (except for Python, see below).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _indent(s, indent='  '):\n    \"\"\"\"\"\"\n    if isinstance(s, six.string_types):\n        return '\\n'.join(('%s%s' % (indent, line) for line in s.splitlines()))\n    return s", "output": "Intent each line with indent", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_cpu_config(config_spec, cpu_props):\n    '''\n    \n    '''\n    log.trace('Configuring virtual machine CPU '\n              'settings cpu_props=%s', cpu_props)\n    if 'count' in cpu_props:\n        config_spec.numCPUs = int(cpu_props['count'])\n    if 'cores_per_socket' in cpu_props:\n        config_spec.numCoresPerSocket = int(cpu_props['cores_per_socket'])\n    if 'nested' in cpu_props and cpu_props['nested']:\n        config_spec.nestedHVEnabled = cpu_props['nested']  # True\n    if 'hotadd' in cpu_props and cpu_props['hotadd']:\n        config_spec.cpuHotAddEnabled = cpu_props['hotadd']  # True\n    if 'hotremove' in cpu_props and cpu_props['hotremove']:\n        config_spec.cpuHotRemoveEnabled = cpu_props['hotremove']", "output": "Sets CPU core count to the given value\n\n    config_spec\n        vm.ConfigSpec object\n\n    cpu_props\n        CPU properties dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove(self, key, where=None, start=None, stop=None):\n        \"\"\"\n        \n\n        \"\"\"\n        where = _ensure_term(where, scope_level=1)\n        try:\n            s = self.get_storer(key)\n        except KeyError:\n            # the key is not a valid store, re-raising KeyError\n            raise\n        except Exception:\n\n            if where is not None:\n                raise ValueError(\n                    \"trying to remove a node with a non-None where clause!\")\n\n            # we are actually trying to remove a node (with children)\n            s = self.get_node(key)\n            if s is not None:\n                s._f_remove(recursive=True)\n                return None\n\n        # remove the node\n        if com._all_none(where, start, stop):\n            s.group._f_remove(recursive=True)\n\n        # delete from the table\n        else:\n            if not s.is_table:\n                raise ValueError(\n                    'can only remove with where on objects written as tables')\n            return s.delete(where=where, start=start, stop=stop)", "output": "Remove pandas object partially by specifying the where condition\n\n        Parameters\n        ----------\n        key : string\n            Node to remove or delete rows from\n        where : list of Term (or convertible) objects, optional\n        start : integer (defaults to None), row number to start selection\n        stop  : integer (defaults to None), row number to stop selection\n\n        Returns\n        -------\n        number of rows removed (or None if not a Table)\n\n        Exceptions\n        ----------\n        raises KeyError if key is not a valid store", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lock(backend=None, remote=None):\n    '''\n    \n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    locked, errors = fileserver.lock(back=backend, remote=remote)\n    ret = {}\n    if locked:\n        ret['locked'] = locked\n    if errors:\n        ret['errors'] = errors\n    if not ret:\n        return 'No locks were set'\n    return ret", "output": ".. versionadded:: 2015.5.0\n\n    Set a fileserver update lock for VCS fileserver backends (:mod:`git\n    <salt.fileserver.gitfs>`, :mod:`hg <salt.fileserver.hgfs>`, :mod:`svn\n    <salt.fileserver.svnfs>`).\n\n    .. note::\n\n        This will only operate on enabled backends (those configured in\n        :conf_master:`fileserver_backend`).\n\n    backend\n        Only set the update lock for the specified backend(s).\n\n    remote\n        If not None, then any remotes which contain the passed string will have\n        their lock cleared. For example, a ``remote`` value of ``*github.com*``\n        will remove the lock from all github.com remotes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.lock\n        salt-run fileserver.lock backend=git,hg\n        salt-run fileserver.lock backend=git remote='*github.com*'\n        salt-run fileserver.lock remote=bitbucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def link(src, path):\n    '''\n    \n    '''\n    src = os.path.expanduser(src)\n\n    if not os.path.isabs(src):\n        raise SaltInvocationError('File path must be absolute.')\n\n    try:\n        os.link(src, path)\n        return True\n    except (OSError, IOError):\n        raise CommandExecutionError('Could not create \\'{0}\\''.format(path))\n    return False", "output": ".. versionadded:: 2014.1.0\n\n    Create a hard link to a file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.link /path/to/file /path/to/link", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_image(name):\n    '''\n    \n    '''\n    try:\n        images = __salt__['glance.image_list'](name=name)\n    except kstone_Unauthorized:\n        return False, 'keystoneclient: Unauthorized'\n    except glance_Unauthorized:\n        return False, 'glanceclient: Unauthorized'\n    log.debug('Got images: %s', images)\n\n    if type(images) is dict and len(images) == 1 and 'images' in images:\n        images = images['images']\n\n    images_list = images.values() if type(images) is dict else images\n\n    if not images_list:\n        return None, 'No image with name \"{0}\"'.format(name)\n    elif len(images_list) == 1:\n        return images_list[0], 'Found image {0}'.format(name)\n    elif len(images_list) > 1:\n        return False, 'Found more than one image with given name'\n    else:\n        raise NotImplementedError", "output": "Tries to find image with given name, returns\n        - image, 'Found image <name>'\n        - None, 'No such image found'\n        - False, 'Found more than one image with given name'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _update_expression_reference(self, # pylint: disable=no-self-use\n                                     grammar: Grammar,\n                                     parent_expression_nonterminal: str,\n                                     child_expression_nonterminal: str) -> None:\n        \"\"\"\n        \n        \"\"\"\n        grammar[parent_expression_nonterminal].members = \\\n                [member if member.name != child_expression_nonterminal\n                 else grammar[child_expression_nonterminal]\n                 for member in grammar[parent_expression_nonterminal].members]", "output": "When we add a new expression, there may be other expressions that refer to\n        it, and we need to update those to point to the new expression.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def table_spec_path(cls, project, location, dataset, table_spec):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/datasets/{dataset}/tableSpecs/{table_spec}\",\n            project=project,\n            location=location,\n            dataset=dataset,\n            table_spec=table_spec,\n        )", "output": "Return a fully-qualified table_spec string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def arrays_to_mgr(arrays, arr_names, index, columns, dtype=None):\n    \"\"\"\n    \n    \"\"\"\n    # figure out the index, if necessary\n    if index is None:\n        index = extract_index(arrays)\n    else:\n        index = ensure_index(index)\n\n    # don't force copy because getting jammed in an ndarray anyway\n    arrays = _homogenize(arrays, index, dtype)\n\n    # from BlockManager perspective\n    axes = [ensure_index(columns), index]\n\n    return create_block_manager_from_arrays(arrays, arr_names, axes)", "output": "Segregate Series based on type and coerce into matrices.\n\n    Needs to handle a lot of exceptional cases.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _common_prefix(names):\n    \"\"\"\"\"\"\n    if not names:\n        return ''\n    prefix = names[0]\n    for name in names:\n        i = 0\n        while i < len(prefix) and i < len(name) and prefix[i] == name[i]:\n            i += 1\n        prefix = prefix[:i]\n    return prefix", "output": "Get the common prefix for all names", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def registerJavaUDAF(self, name, javaClassName):\n        \"\"\"\n        \"\"\"\n\n        self.sparkSession._jsparkSession.udf().registerJavaUDAF(name, javaClassName)", "output": "Register a Java user-defined aggregate function as a SQL function.\n\n        :param name: name of the user-defined aggregate function\n        :param javaClassName: fully qualified name of java class\n\n        >>> spark.udf.registerJavaUDAF(\"javaUDAF\", \"test.org.apache.spark.sql.MyDoubleAvg\")\n        >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"a\")],[\"id\", \"name\"])\n        >>> df.createOrReplaceTempView(\"df\")\n        >>> spark.sql(\"SELECT name, javaUDAF(id) as avg from df group by name\").collect()\n        [Row(name=u'b', avg=102.0), Row(name=u'a', avg=102.0)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_(prefix='', ruby=None, runas=None, gem_bin=None):\n    '''\n    \n    '''\n    cmd = ['list']\n    if prefix:\n        cmd.append(prefix)\n    stdout = _gem(cmd,\n                  ruby,\n                  gem_bin=gem_bin,\n                  runas=runas)\n    ret = {}\n    for line in salt.utils.itertools.split(stdout, '\\n'):\n        match = re.match(r'^([^ ]+) \\((.+)\\)', line)\n        if match:\n            gem = match.group(1)\n            versions = match.group(2).split(', ')\n            ret[gem] = versions\n    return ret", "output": "List locally installed gems.\n\n    :param prefix: string :\n        Only list gems when the name matches this prefix.\n    :param gem_bin: string : None\n        Full path to ``gem`` binary to use.\n    :param ruby: string : None\n        If RVM or rbenv are installed, the ruby version and gemset to use.\n        Ignored if ``gem_bin`` is specified.\n    :param runas: string : None\n        The user to run gem as.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gem.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_df(self, data_file):\n        \"\"\"\n        \n        \"\"\"\n\n        crawler = QAHistoryFinancialCrawler()\n\n        with open(data_file, 'rb') as df:\n            data = crawler.parse(download_file=df)\n\n        return crawler.to_df(data)", "output": "\u8bfb\u53d6\u5386\u53f2\u8d22\u52a1\u6570\u636e\u6587\u4ef6\uff0c\u5e76\u8fd4\u56depandas\u7ed3\u679c \uff0c \u7c7b\u4f3cgpcw20171231.zip\u683c\u5f0f\uff0c\u5177\u4f53\u5b57\u6bb5\u542b\u4e49\u53c2\u8003\n\n        https://github.com/rainx/pytdx/issues/133\n\n        :param data_file: \u6570\u636e\u6587\u4ef6\u5730\u5740\uff0c \u6570\u636e\u6587\u4ef6\u7c7b\u578b\u53ef\u4ee5\u4e3a .zip \u6587\u4ef6\uff0c\u4e5f\u53ef\u4ee5\u4e3a\u89e3\u538b\u540e\u7684 .dat\n        :return: pandas DataFrame\u683c\u5f0f\u7684\u5386\u53f2\u8d22\u52a1\u6570\u636e", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def seek(self, idx):\n        \"\"\"\"\"\"\n        assert not self.writable\n        self._check_pid(allow_reset=True)\n        pos = ctypes.c_size_t(self.idx[idx])\n        check_call(_LIB.MXRecordIOReaderSeek(self.handle, pos))", "output": "Sets the current read pointer position.\n\n        This function is internally called by `read_idx(idx)` to find the current\n        reader pointer position. It doesn't return anything.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_render_wrapper(func):\n    \"\"\"\n    \"\"\"\n    global RENDER_WRAPPER\n    if not hasattr(func, \"__call__\"):\n        raise ValueError(Errors.E110.format(obj=type(func)))\n    RENDER_WRAPPER = func", "output": "Set an optional wrapper function that is called around the generated\n    HTML markup on displacy.render. This can be used to allow integration into\n    other platforms, similar to Jupyter Notebooks that require functions to be\n    called around the HTML. It can also be used to implement custom callbacks\n    on render, or to embed the visualization in a custom page.\n\n    func (callable): Function to call around markup before rendering it. Needs\n        to take one argument, the HTML markup, and should return the desired\n        output of displacy.render.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup(app):\n    '''  '''\n    app.add_autodocumenter(ColorDocumenter)\n    app.add_autodocumenter(EnumDocumenter)\n    app.add_autodocumenter(PropDocumenter)\n    app.add_autodocumenter(ModelDocumenter)", "output": "Required Sphinx extension setup function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_train_knns(self, data_activations):\n    \"\"\"\n    \n    \"\"\"\n    knns_ind = {}\n    knns_labels = {}\n\n    for layer in self.layers:\n      # Pre-process representations of data to normalize and remove training data mean.\n      data_activations_layer = copy.copy(data_activations[layer])\n      nb_data = data_activations_layer.shape[0]\n      data_activations_layer /= np.linalg.norm(\n          data_activations_layer, axis=1).reshape(-1, 1)\n      data_activations_layer -= self.centers[layer]\n\n      # Use FALCONN to find indices of nearest neighbors in training data.\n      knns_ind[layer] = np.zeros(\n          (data_activations_layer.shape[0], self.neighbors), dtype=np.int32)\n      knn_errors = 0\n      for i in range(data_activations_layer.shape[0]):\n        query_res = self.query_objects[layer].find_k_nearest_neighbors(\n            data_activations_layer[i], self.neighbors)\n        try:\n          knns_ind[layer][i, :] = query_res\n        except:  # pylint: disable-msg=W0702\n          knns_ind[layer][i, :len(query_res)] = query_res\n          knn_errors += knns_ind[layer].shape[1] - len(query_res)\n\n      # Find labels of neighbors found in the training data.\n      knns_labels[layer] = np.zeros((nb_data, self.neighbors), dtype=np.int32)\n      for data_id in range(nb_data):\n        knns_labels[layer][data_id, :] = self.train_labels[knns_ind[layer][data_id]]\n\n    return knns_ind, knns_labels", "output": "Given a data_activation dictionary that contains a np array with activations for each layer,\n    find the knns in the training data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_multi_precision(self, index, weight, grad, state):\n        \"\"\"\n        \"\"\"\n        if self.multi_precision and weight.dtype == numpy.float16:\n            # Wrapper for mixed precision\n            weight_master_copy = state[0]\n            original_state = state[1]\n            grad32 = grad.astype(numpy.float32)\n            self.update(index, weight_master_copy, grad32, original_state)\n            cast(weight_master_copy, dtype=weight.dtype, out=weight)\n        else:\n            self.update(index, weight, grad, state)", "output": "Updates the given parameter using the corresponding gradient and state.\n        Mixed precision version.\n\n        Parameters\n        ----------\n        index : int\n            The unique index of the parameter into the individual learning\n            rates and weight decays. Learning rates and weight decay\n            may be set via `set_lr_mult()` and `set_wd_mult()`, respectively.\n        weight : NDArray\n            The parameter to be updated.\n        grad : NDArray\n            The gradient of the objective with respect to this parameter.\n        state : any obj\n            The state returned by `create_state()`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _service_is_chkconfig(name):\n    '''\n    \n    '''\n    cmdline = '/sbin/chkconfig --list {0}'.format(name)\n    return __salt__['cmd.retcode'](cmdline, python_shell=False, ignore_retcode=True) == 0", "output": "Return True if the service is managed by chkconfig.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_categorical(y, nb_classes, num_classes=None):\n  \"\"\"\n  \n  \"\"\"\n  if num_classes is not None:\n    if nb_classes is not None:\n      raise ValueError(\"Should not specify both nb_classes and its deprecated \"\n                       \"alias, num_classes\")\n    warnings.warn(\"`num_classes` is deprecated. Switch to `nb_classes`.\"\n                  \" `num_classes` may be removed on or after 2019-04-23.\")\n    nb_classes = num_classes\n    del num_classes\n  y = np.array(y, dtype='int').ravel()\n  n = y.shape[0]\n  categorical = np.zeros((n, nb_classes))\n  categorical[np.arange(n), y] = 1\n  return categorical", "output": "Converts a class vector (integers) to binary class matrix.\n  This is adapted from the Keras function with the same name.\n  :param y: class vector to be converted into a matrix\n            (integers from 0 to nb_classes).\n  :param nb_classes: nb_classes: total number of classes.\n  :param num_classses: depricated version of nb_classes\n  :return: A binary matrix representation of the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _minion_event(self, load):\n        '''\n        \n        '''\n        if 'id' not in load:\n            return False\n        if 'events' not in load and ('tag' not in load or 'data' not in load):\n            return False\n        if 'events' in load:\n            for event in load['events']:\n                if 'data' in event:\n                    event_data = event['data']\n                else:\n                    event_data = event\n                self.event.fire_event(event_data, event['tag'])  # old dup event\n                if load.get('pretag') is not None:\n                    self.event.fire_event(event_data, salt.utils.event.tagify(event['tag'], base=load['pretag']))\n        else:\n            tag = load['tag']\n            self.event.fire_event(load, tag)\n        return True", "output": "Receive an event from the minion and fire it on the master event\n        interface", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open_fileswitcher_dlg(self):\r\n        \"\"\"\"\"\"\r\n        if not self.tabs.count():\r\n            return\r\n        if self.fileswitcher_dlg is not None and \\\r\n          self.fileswitcher_dlg.is_visible:\r\n            self.fileswitcher_dlg.hide()\r\n            self.fileswitcher_dlg.is_visible = False\r\n            return\r\n        self.fileswitcher_dlg = FileSwitcher(self, self, self.tabs, self.data,\r\n                                             ima.icon('TextFileIcon'))\r\n        self.fileswitcher_dlg.sig_goto_file.connect(self.set_stack_index)\r\n        self.fileswitcher_dlg.show()\r\n        self.fileswitcher_dlg.is_visible = True", "output": "Open file list management dialog box", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_rsa_key(path, passphrase):\n    '''\n    \n    '''\n    log.debug('salt.crypt.get_rsa_key: Loading private key')\n    return _get_key_with_evict(path, six.text_type(os.path.getmtime(path)), passphrase)", "output": "Read a private key off the disk.  Poor man's simple cache in effect here,\n    we memoize the result of calling _get_rsa_with_evict.  This means the first\n    time _get_key_with_evict is called with a path and a timestamp the result\n    is cached.  If the file (the private key) does not change then its\n    timestamp will not change and the next time the result is returned from the\n    cache.  If the key DOES change the next time _get_rsa_with_evict is called\n    it is called with different parameters and the fn is run fully to retrieve\n    the key from disk.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_namespaced_replication_controller(self, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.create_namespaced_replication_controller_with_http_info(namespace, body, **kwargs)\n        else:\n            (data) = self.create_namespaced_replication_controller_with_http_info(namespace, body, **kwargs)\n            return data", "output": "create a ReplicationController\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.create_namespaced_replication_controller(namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param V1ReplicationController body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1ReplicationController\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_resources_vms(call=None, resFilter=None, includeConfig=True):\n    '''\n    \n    '''\n\n    timeoutTime = time.time() + 60\n    while True:\n        log.debug('Getting resource: vms.. (filter: %s)', resFilter)\n        resources = query('get', 'cluster/resources')\n        ret = {}\n        badResource = False\n        for resource in resources:\n            if 'type' in resource and resource['type'] in ['openvz', 'qemu',\n                                                           'lxc']:\n                try:\n                    name = resource['name']\n                except KeyError:\n                    badResource = True\n                    log.debug('No name in VM resource %s', repr(resource))\n                    break\n\n                ret[name] = resource\n\n                if includeConfig:\n                    # Requested to include the detailed configuration of a VM\n                    ret[name]['config'] = get_vmconfig(\n                        ret[name]['vmid'],\n                        ret[name]['node'],\n                        ret[name]['type']\n                    )\n\n        if time.time() > timeoutTime:\n            raise SaltCloudExecutionTimeout('FAILED to get the proxmox '\n                                            'resources vms')\n\n        # Carry on if there wasn't a bad resource return from Proxmox\n        if not badResource:\n            break\n\n        time.sleep(0.5)\n\n    if resFilter is not None:\n        log.debug('Filter given: %s, returning requested '\n                  'resource: nodes', resFilter)\n        return ret[resFilter]\n\n    log.debug('Filter not given: %s, returning all resource: nodes', ret)\n    return ret", "output": "Retrieve all VMs available on this environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_resources_vms my-proxmox-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def evaluate(data_loader):\n    \"\"\"\n    \"\"\"\n    translation_out = []\n    all_inst_ids = []\n    avg_loss_denom = 0\n    avg_loss = 0.0\n    for _, (src_seq, tgt_seq, src_valid_length, tgt_valid_length, inst_ids) \\\n            in enumerate(data_loader):\n        src_seq = src_seq.as_in_context(ctx)\n        tgt_seq = tgt_seq.as_in_context(ctx)\n        src_valid_length = src_valid_length.as_in_context(ctx)\n        tgt_valid_length = tgt_valid_length.as_in_context(ctx)\n        # Calculating Loss\n        out, _ = model(src_seq, tgt_seq[:, :-1], src_valid_length, tgt_valid_length - 1)\n        loss = loss_function(out, tgt_seq[:, 1:], tgt_valid_length - 1).mean().asscalar()\n        all_inst_ids.extend(inst_ids.asnumpy().astype(np.int32).tolist())\n        avg_loss += loss * (tgt_seq.shape[1] - 1)\n        avg_loss_denom += (tgt_seq.shape[1] - 1)\n        # Translate\n        samples, _, sample_valid_length =\\\n            translator.translate(src_seq=src_seq, src_valid_length=src_valid_length)\n        max_score_sample = samples[:, 0, :].asnumpy()\n        sample_valid_length = sample_valid_length[:, 0].asnumpy()\n        for i in range(max_score_sample.shape[0]):\n            translation_out.append(\n                [tgt_vocab.idx_to_token[ele] for ele in\n                 max_score_sample[i][1:(sample_valid_length[i] - 1)]])\n    avg_loss = avg_loss / avg_loss_denom\n    real_translation_out = [None for _ in range(len(all_inst_ids))]\n    for ind, sentence in zip(all_inst_ids, translation_out):\n        real_translation_out[ind] = sentence\n    return avg_loss, real_translation_out", "output": "Evaluate given the data loader\n\n    Parameters\n    ----------\n    data_loader : DataLoader\n\n    Returns\n    -------\n    avg_loss : float\n        Average loss\n    real_translation_out : list of list of str\n        The translation output", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ensure_timestamp_field(dataset_expr, deltas, checkpoints):\n    \"\"\"\n    \"\"\"\n    measure = dataset_expr.dshape.measure\n    if TS_FIELD_NAME not in measure.names:\n        dataset_expr = bz.transform(\n            dataset_expr,\n            **{TS_FIELD_NAME: dataset_expr[AD_FIELD_NAME]}\n        )\n        deltas = _ad_as_ts(deltas)\n        checkpoints = _ad_as_ts(checkpoints)\n    else:\n        _check_datetime_field(TS_FIELD_NAME, measure)\n\n    return dataset_expr, deltas, checkpoints", "output": "Verify that the baseline and deltas expressions have a timestamp field.\n\n    If there is not a ``TS_FIELD_NAME`` on either of the expressions, it will\n    be copied from the ``AD_FIELD_NAME``. If one is provided, then we will\n    verify that it is the correct dshape.\n\n    Parameters\n    ----------\n    dataset_expr : Expr\n        The baseline expression.\n    deltas : Expr or None\n        The deltas expression if any was provided.\n    checkpoints : Expr or None\n        The checkpoints expression if any was provided.\n\n    Returns\n    -------\n    dataset_expr, deltas : Expr\n        The new baseline and deltas expressions to use.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setIfMissing(self, key, value):\n        \"\"\"\"\"\"\n        if self.get(key) is None:\n            self.set(key, value)\n        return self", "output": "Set a configuration property, if not already set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_api_repr(cls, resource, client):\n        \"\"\"\n        \"\"\"\n        job_id, config_resource = cls._get_resource_config(resource)\n        config = CopyJobConfig.from_api_repr(config_resource)\n        # Copy required fields to the job.\n        copy_resource = config_resource[\"copy\"]\n        destination = TableReference.from_api_repr(copy_resource[\"destinationTable\"])\n        sources = []\n        source_configs = copy_resource.get(\"sourceTables\")\n        if source_configs is None:\n            single = copy_resource.get(\"sourceTable\")\n            if single is None:\n                raise KeyError(\"Resource missing 'sourceTables' / 'sourceTable'\")\n            source_configs = [single]\n        for source_config in source_configs:\n            table_ref = TableReference.from_api_repr(source_config)\n            sources.append(table_ref)\n        job = cls(job_id, sources, destination, client=client, job_config=config)\n        job._set_properties(resource)\n        return job", "output": "Factory:  construct a job given its API representation\n\n        .. note:\n\n           This method assumes that the project found in the resource matches\n           the client's project.\n\n        :type resource: dict\n        :param resource: dataset job representation returned from the API\n\n        :type client: :class:`google.cloud.bigquery.client.Client`\n        :param client: Client which holds credentials and project\n                       configuration for the dataset.\n\n        :rtype: :class:`google.cloud.bigquery.job.CopyJob`\n        :returns: Job parsed from ``resource``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_repository(self, repository, body, params=None):\n        \"\"\"\n        \n        \"\"\"\n        for param in (repository, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request('PUT', _make_path('_snapshot',\n            repository), params=params, body=body)", "output": "Registers a shared file system repository.\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-snapshots.html>`_\n\n        :arg repository: A repository name\n        :arg body: The repository definition\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg timeout: Explicit operation timeout\n        :arg verify: Whether to verify the repository after creation", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def _retrieve_guilds_after_strategy(self, retrieve):\n        \"\"\"\"\"\"\n        after = self.after.id if self.after else None\n        data = await self.get_guilds(retrieve, after=after)\n        if len(data):\n            if self.limit is not None:\n                self.limit -= retrieve\n            self.after = Object(id=int(data[0]['id']))\n        return data", "output": "Retrieve guilds using after parameter.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def datasets_list_files(self, owner_slug, dataset_slug, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.datasets_list_files_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.datasets_list_files_with_http_info(owner_slug, dataset_slug, **kwargs)  # noqa: E501\n            return data", "output": "List dataset files  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.datasets_list_files(owner_slug, dataset_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str owner_slug: Dataset owner (required)\n        :param str dataset_slug: Dataset name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path(self):\n        \"\"\"\n        \n        \"\"\"\n        location = self.client.table_location(self.table, self.database)\n        if not location:\n            raise Exception(\"Couldn't find location for table: {0}\".format(str(self)))\n        return location", "output": "Returns the path to this table in HDFS.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_valid_project(self, path):\r\n        \"\"\"\"\"\"\r\n        spy_project_dir = osp.join(path, '.spyproject')\r\n        if osp.isdir(path) and osp.isdir(spy_project_dir):\r\n            return True\r\n        else:\r\n            return False", "output": "Check if a directory is a valid Spyder project", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name, **connection_args):\n    '''\n    \n    '''\n    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    #check if db exists and remove it\n    if __salt__['mysql.db_exists'](name, **connection_args):\n        if __opts__['test']:\n            ret['result'] = None\n            ret['comment'] = \\\n                'Database {0} is present and needs to be removed'.format(name)\n            return ret\n        if __salt__['mysql.db_remove'](name, **connection_args):\n            ret['comment'] = 'Database {0} has been removed'.format(name)\n            ret['changes'][name] = 'Absent'\n            return ret\n        else:\n            err = _get_mysql_error()\n            if err is not None:\n                ret['comment'] = 'Unable to remove database {0} ' \\\n                                 '({1})'.format(name, err)\n                ret['result'] = False\n                return ret\n    else:\n        err = _get_mysql_error()\n        if err is not None:\n            ret['comment'] = err\n            ret['result'] = False\n            return ret\n\n    # fallback\n    ret['comment'] = ('Database {0} is not present, so it cannot be removed'\n            ).format(name)\n    return ret", "output": "Ensure that the named database is absent\n\n    name\n        The name of the database to remove", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_pos(token):\n    \"\"\"\n    \"\"\"\n    # TODO: This is a first take. The rules here are crude approximations.\n    # For many of these, full dependencies are needed to properly resolve\n    # PoS mappings.\n    if token.pos == \"\u9023\u4f53\u8a5e,*,*,*\":\n        if re.match(r\"[\u3053\u305d\u3042\u3069\u6b64\u5176\u5f7c]\u306e\", token.surface):\n            return token.pos + \",DET\"\n        if re.match(r\"[\u3053\u305d\u3042\u3069\u6b64\u5176\u5f7c]\", token.surface):\n            return token.pos + \",PRON\"\n        return token.pos + \",ADJ\"\n    return token.pos", "output": "If necessary, add a field to the POS tag for UD mapping.\n    Under Universal Dependencies, sometimes the same Unidic POS tag can\n    be mapped differently depending on the literal token or its context\n    in the sentence. This function adds information to the POS tag to\n    resolve ambiguous mappings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def DiagonalGate(x, params, **kwargs):\n  \"\"\"\"\"\"\n  del params\n  del kwargs\n  # x : [batch, 1, length, depth]\n  x = np.pad(\n      x, [(0, 0), (0, 0), (1, 1), (0, 0)], mode='constant', constant_values=0.0)\n  depth = x.shape[-1] // 3\n  assert 3 * depth == x.shape[-1], ('Depth must be divisible by 3', depth,\n                                    x.shape)\n  xs = [\n      x[:, :, :-2, :depth], x[:, :, 1:-1, depth:2 * depth],\n      x[:, :, 2:, 2 * depth:3 * depth]\n  ]\n  return np.concatenate(xs, axis=3)", "output": "Split channels in 3 parts. Shifts 1st and 3rd sections to left/right.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_thing_shadow(self, **kwargs):\n        \n        \"\"\"\n        thing_name = self._get_required_parameter('thingName', **kwargs)\n        payload = self._get_required_parameter('payload', **kwargs)\n\n        return self._shadow_op('update', thing_name, payload)", "output": "r\"\"\"\n        Updates the thing shadow for the specified thing.\n\n        :Keyword Arguments:\n            * *thingName* (``string``) --\n              [REQUIRED]\n              The name of the thing.\n            * *payload* (``bytes or seekable file-like object``) --\n              [REQUIRED]\n              The state information, in JSON format.\n\n        :returns: (``dict``) --\n        The output from the UpdateThingShadow operation\n            * *payload* (``bytes``) --\n              The state information, in JSON format.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh(self):\r\n        \"\"\"\"\"\"\r\n        self._update_id_list()\r\n        for _id in self.history[:]:\r\n            if _id not in self.id_list:\r\n                self.history.remove(_id)", "output": "Remove editors that are not longer open.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def continuous_future(self,\n                          root_symbol_str,\n                          offset=0,\n                          roll='volume',\n                          adjustment='mul'):\n        \"\"\"\n        \"\"\"\n        return self.asset_finder.create_continuous_future(\n            root_symbol_str,\n            offset,\n            roll,\n            adjustment,\n        )", "output": "Create a specifier for a continuous contract.\n\n        Parameters\n        ----------\n        root_symbol_str : str\n            The root symbol for the future chain.\n\n        offset : int, optional\n            The distance from the primary contract. Default is 0.\n\n        roll_style : str, optional\n            How rolls are determined. Default is 'volume'.\n\n        adjustment : str, optional\n            Method for adjusting lookback prices between rolls. Options are\n            'mul', 'add', and None. Default is 'mul'.\n\n        Returns\n        -------\n        continuous_future : ContinuousFuture\n            The continuous future specifier.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pow(self, other, axis=\"columns\", level=None, fill_value=None):\r\n        \"\"\"\r\n        \"\"\"\r\n        return self._binary_op(\r\n            \"pow\", other, axis=axis, level=level, fill_value=fill_value\r\n        )", "output": "Pow this DataFrame against another DataFrame/Series/scalar.\r\n\r\n        Args:\r\n            other: The object to use to apply the pow against this.\r\n            axis: The axis to pow over.\r\n            level: The Multilevel index level to apply pow over.\r\n            fill_value: The value to fill NaNs with.\r\n\r\n        Returns:\r\n            A new DataFrame with the Pow applied.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schema(self, schema):\n        \"\"\"\n        \"\"\"\n        from pyspark.sql import SparkSession\n        spark = SparkSession.builder.getOrCreate()\n        if isinstance(schema, StructType):\n            jschema = spark._jsparkSession.parseDataType(schema.json())\n            self._jreader = self._jreader.schema(jschema)\n        elif isinstance(schema, basestring):\n            self._jreader = self._jreader.schema(schema)\n        else:\n            raise TypeError(\"schema should be StructType or string\")\n        return self", "output": "Specifies the input schema.\n\n        Some data sources (e.g. JSON) can infer the input schema automatically from data.\n        By specifying the schema here, the underlying data source can skip the schema\n        inference step, and thus speed up data loading.\n\n        :param schema: a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n                       (For example ``col0 INT, col1 DOUBLE``).\n\n        >>> s = spark.read.schema(\"col0 INT, col1 DOUBLE\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _blobs_page_start(iterator, page, response):\n    \"\"\"\n    \"\"\"\n    page.prefixes = tuple(response.get(\"prefixes\", ()))\n    iterator.prefixes.update(page.prefixes)", "output": "Grab prefixes after a :class:`~google.cloud.iterator.Page` started.\n\n    :type iterator: :class:`~google.api_core.page_iterator.Iterator`\n    :param iterator: The iterator that is currently in use.\n\n    :type page: :class:`~google.cloud.api.core.page_iterator.Page`\n    :param page: The page that was just created.\n\n    :type response: dict\n    :param response: The JSON API response for a page of blobs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config(_config=None, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    if _config is None:\n        _config = {}\n    _config.update(kwargs)\n\n    def wrapper(func):\n        func._config = _config\n        return func\n    return wrapper", "output": "A decorator for setting the default kwargs of `BaseHandler.crawl`.\n    Any self.crawl with this callback will use this config.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pairwise_euclid_distance(A, B):\n    \"\"\"\n    \"\"\"\n    batchA = tf.shape(A)[0]\n    batchB = tf.shape(B)[0]\n\n    sqr_norm_A = tf.reshape(tf.reduce_sum(tf.pow(A, 2), 1), [1, batchA])\n    sqr_norm_B = tf.reshape(tf.reduce_sum(tf.pow(B, 2), 1), [batchB, 1])\n    inner_prod = tf.matmul(B, A, transpose_b=True)\n\n    tile_1 = tf.tile(sqr_norm_A, [batchB, 1])\n    tile_2 = tf.tile(sqr_norm_B, [1, batchA])\n    return (tile_1 + tile_2 - 2 * inner_prod)", "output": "Pairwise Euclidean distance between two matrices.\n    :param A: a matrix.\n    :param B: a matrix.\n\n    :returns: A tensor for the pairwise Euclidean between A and B.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def local_attention2d_spatial_decoder(x, kv_dim, heads_dim,\n                                      feedforward_dim, hparams):\n  \"\"\"\"\"\"\n  batch_dim, length_dim, model_dim = x.shape.dims\n  blocks_h_dim = mtf.Dimension(\"blocksh\", hparams.block_height)\n  blocks_w_dim = mtf.Dimension(\"blocksw\", hparams.block_width)\n  num_h_blocks_dim = mtf.Dimension(\"num_h_blocks\",\n                                   hparams.img_len // hparams.block_height)\n  num_w_blocks_dim = mtf.Dimension(\n      \"num_w_blocks\",\n      hparams.img_len * hparams.num_channels // hparams.block_width)\n  x = mtf.transpose(\n      mtf.reshape(\n          x,\n          mtf.Shape([\n              batch_dim, num_h_blocks_dim, blocks_h_dim,\n              num_w_blocks_dim, blocks_w_dim, model_dim\n          ])),\n      mtf.Shape([\n          batch_dim, num_h_blocks_dim, num_w_blocks_dim,\n          blocks_h_dim, blocks_w_dim, model_dim\n      ]))\n  # Image Transformer Decoder\n  # [ self attention - ffn - residual + dropout] x n\n  for layer in range(hparams.num_decoder_layers):\n    layer_name = \"decoder_layer_%d\" % layer\n    with tf.variable_scope(layer_name):\n      # Self attention layer\n      x += layer_prepostprocess_dropout(\n          mtf.layers.local_2d_self_attention_spatial_blocks(\n              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_att\"),\n              kv_dim,\n              heads_dim,\n              memory_h_dim=num_h_blocks_dim,\n              memory_w_dim=num_w_blocks_dim,\n              name=\"self_att\"), hparams)\n      # ffn layer\n      x += layer_prepostprocess_dropout(\n          mtf.layers.dense_relu_dense(\n              mtf.layers.layer_norm(x, model_dim, name=\"layer_norm_ffn\"),\n              feedforward_dim,\n              hparams.dropout,\n              dropout_broadcast_dims=[length_dim]), hparams)\n\n  output = mtf.layers.layer_norm(x, model_dim, name=\"final_layer_norm\")\n  return output", "output": "Image Transformer decoder with local2D spatial layers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def resolve(self, host: str,\n                      port: int, family: int) -> List[Dict[str, Any]]:\n        \"\"\"\"\"\"", "output": "Return IP address for given hostname", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def instr(str, substr):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.instr(_to_java_column(str), substr))", "output": "Locate the position of the first occurrence of substr column in the given string.\n    Returns null if either of the arguments are null.\n\n    .. note:: The position is not zero based, but 1 based index. Returns 0 if substr\n        could not be found in str.\n\n    >>> df = spark.createDataFrame([('abcd',)], ['s',])\n    >>> df.select(instr(df.s, 'b').alias('s')).collect()\n    [Row(s=2)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_java_object_rdd(self):\n        \"\"\" \n        \"\"\"\n        rdd = self._pickled()\n        return self.ctx._jvm.SerDeUtil.pythonToJava(rdd._jrdd, True)", "output": "Return a JavaRDD of Object by unpickling\n\n        It will convert each Python object into Java object by Pyrolite, whenever the\n        RDD is serialized in batch or not.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def chhome(name, home, persist=False):\n    '''\n    \n    '''\n    pre_info = info(name)\n    if not pre_info:\n        raise CommandExecutionError(\n            'User \\'{0}\\' does not exist'.format(name)\n        )\n    if home == pre_info['home']:\n        return True\n    cmd = ['usermod', '-d', home]\n    if persist:\n        cmd.append('-m')\n    cmd.append(name)\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return info(name).get('home') == home", "output": "Set a new home directory for an existing user\n\n    name\n        Username to modify\n\n    home\n        New home directory to set\n\n    persist : False\n        Set to ``True`` to prevent configuration files in the new home\n        directory from being overwritten by the files from the skeleton\n        directory.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.chhome foo /home/users/foo True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_breakpoints(self):\n        \"\"\"\"\"\"\n        breakpoints = []\n        block = self.editor.document().firstBlock()\n        for line_number in range(1, self.editor.document().blockCount()+1):\n            data = block.userData()\n            if data and data.breakpoint:\n                breakpoints.append((line_number, data.breakpoint_condition))\n            block = block.next()\n        return breakpoints", "output": "Get breakpoints", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_sep_channels_16l_16h_imgnet_lrg_loc():\n  \"\"\"\"\"\"\n  hparams = imagetransformer_sep_channels_12l_16h_imagenet_large()\n  hparams.num_hidden_layers = 16\n  hparams.local_attention = True\n  hparams.batch_size = 1\n  hparams.block_length = 256\n  return hparams", "output": "separate rgb embeddings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stats(local=False, remote=False, jail=None, chroot=None, root=None):\n    '''\n    \n    '''\n\n    opts = ''\n    if local:\n        opts += 'l'\n    if remote:\n        opts += 'r'\n\n    cmd = _pkg(jail, chroot, root)\n    cmd.append('stats')\n    if opts:\n        cmd.append('-' + opts)\n    out = __salt__['cmd.run'](cmd, output_loglevel='trace', python_shell=False)\n    return [x.strip('\\t') for x in salt.utils.itertools.split(out, '\\n')]", "output": "Return pkgng stats.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.stats\n\n    local\n        Display stats only for the local package database.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.stats local=True\n\n    remote\n        Display stats only for the remote package database(s).\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.stats remote=True\n\n    jail\n        Retrieve stats from the specified jail.\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.stats jail=<jail name or id>\n            salt '*' pkg.stats jail=<jail name or id> local=True\n            salt '*' pkg.stats jail=<jail name or id> remote=True\n\n    chroot\n        Retrieve stats from the specified chroot (ignored if ``jail`` is\n        specified).\n\n    root\n        Retrieve stats from the specified root (ignored if ``jail`` is\n        specified).\n\n        CLI Example:\n\n        .. code-block:: bash\n\n            salt '*' pkg.stats chroot=/path/to/chroot\n            salt '*' pkg.stats chroot=/path/to/chroot local=True\n            salt '*' pkg.stats chroot=/path/to/chroot remote=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toArray(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.isTransposed:\n            return np.asfortranarray(\n                self.values.reshape((self.numRows, self.numCols)))\n        else:\n            return self.values.reshape((self.numRows, self.numCols), order='F')", "output": "Return an numpy.ndarray\n\n        >>> m = DenseMatrix(2, 2, range(4))\n        >>> m.toArray()\n        array([[ 0.,  2.],\n               [ 1.,  3.]])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def identify_repo(repo_url):\n    \"\"\"\n    \"\"\"\n    repo_url_values = repo_url.split('+')\n    if len(repo_url_values) == 2:\n        repo_type = repo_url_values[0]\n        if repo_type in [\"git\", \"hg\"]:\n            return repo_type, repo_url_values[1]\n        else:\n            raise UnknownRepoType\n    else:\n        if 'git' in repo_url:\n            return 'git', repo_url\n        elif 'bitbucket' in repo_url:\n            return 'hg', repo_url\n        else:\n            raise UnknownRepoType", "output": "Determine if `repo_url` should be treated as a URL to a git or hg repo.\n\n    Repos can be identified by prepending \"hg+\" or \"git+\" to the repo URL.\n\n    :param repo_url: Repo URL of unknown type.\n    :returns: ('git', repo_url), ('hg', repo_url), or None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_punctuation(self, char):\n        \"\"\"\"\"\"\n        cp = ord(char)\n        # We treat all non-letter/number ASCII as punctuation.\n        # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n        # Punctuation class but we treat them as punctuation anyways, for\n        # consistency.\n        group0 = cp >= 33 and cp <= 47\n        group1 = cp >= 58 and cp <= 64\n        group2 = cp >= 91 and cp <= 96\n        group3 = cp >= 123 and cp <= 126\n        if (group0 or group1 or group2 or group3):\n            return True\n        cat = unicodedata.category(char)\n        if cat.startswith('P'):\n            return True\n        return False", "output": "Checks whether `chars` is a punctuation character.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def blink_figure(self):\n        \"\"\"\"\"\"\n        if self.fig:\n            self._blink_flag = not self._blink_flag\n            self.repaint()\n            if self._blink_flag:\n                timer = QTimer()\n                timer.singleShot(40, self.blink_figure)", "output": "Blink figure once.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metric(self, name, filter_=None, description=\"\"):\n        \"\"\"\n        \"\"\"\n        return Metric(name, filter_, client=self, description=description)", "output": "Creates a metric bound to the current client.\n\n        :type name: str\n        :param name: the name of the metric to be constructed.\n\n        :type filter_: str\n        :param filter_: the advanced logs filter expression defining the\n                        entries tracked by the metric.  If not\n                        passed, the instance should already exist, to be\n                        refreshed via :meth:`Metric.reload`.\n\n        :type description: str\n        :param description: the description of the metric to be constructed.\n                            If not passed, the instance should already exist,\n                            to be refreshed via :meth:`Metric.reload`.\n\n        :rtype: :class:`google.cloud.logging.metric.Metric`\n        :returns: Metric created with the current client.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _add_existing_weight(self, weight, trainable=None):\n    \"\"\"\"\"\"\n    if trainable is None: trainable = weight.trainable\n    self.add_weight(name=weight.name, shape=weight.shape, dtype=weight.dtype,\n                    trainable=trainable, getter=lambda *_, **__: weight)", "output": "Calls add_weight() to register but not create an existing weight.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remote_addr(self):\n        \"\"\"\n        \"\"\"\n        if not hasattr(self, \"_remote_addr\"):\n            if self.app.config.PROXIES_COUNT == 0:\n                self._remote_addr = \"\"\n            elif self.app.config.REAL_IP_HEADER and self.headers.get(\n                self.app.config.REAL_IP_HEADER\n            ):\n                self._remote_addr = self.headers[\n                    self.app.config.REAL_IP_HEADER\n                ]\n            elif self.app.config.FORWARDED_FOR_HEADER:\n                forwarded_for = self.headers.get(\n                    self.app.config.FORWARDED_FOR_HEADER, \"\"\n                ).split(\",\")\n                remote_addrs = [\n                    addr\n                    for addr in [addr.strip() for addr in forwarded_for]\n                    if addr\n                ]\n                if self.app.config.PROXIES_COUNT == -1:\n                    self._remote_addr = remote_addrs[0]\n                elif len(remote_addrs) >= self.app.config.PROXIES_COUNT:\n                    self._remote_addr = remote_addrs[\n                        -self.app.config.PROXIES_COUNT\n                    ]\n                else:\n                    self._remote_addr = \"\"\n            else:\n                self._remote_addr = \"\"\n        return self._remote_addr", "output": "Attempt to return the original client ip based on X-Forwarded-For\n        or X-Real-IP. If HTTP headers are unavailable or untrusted, returns\n        an empty string.\n\n        :return: original client ip.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_to_distribution(dist):\n    \"\"\"\"\"\"\n    try:\n        dist.add_qt_bindings()\n    except AttributeError:\n        raise ImportError(\"This script requires guidata 1.5+\")\n    for _modname in ('spyder', 'spyderplugins'):\n        dist.add_module_data_files(_modname, (\"\", ),\n                                   ('.png', '.svg', '.html', '.png', '.txt',\n                                    '.js', '.inv', '.ico', '.css', '.doctree',\n                                    '.qm', '.py',),\n                                   copy_to_root=False)", "output": "Add package to py2exe/cx_Freeze distribution object\n    Extension to guidata.disthelpers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reset_bind(self):\n        \"\"\"\"\"\"\n        self.binded = False\n        self._exec_group = None\n        self._data_shapes = None\n        self._label_shapes = None", "output": "Internal function to reset binded state.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xpathNextFollowing(self, cur):\n        \"\"\" \"\"\"\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        ret = libxml2mod.xmlXPathNextFollowing(self._o, cur__o)\n        if ret is None:raise xpathError('xmlXPathNextFollowing() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Traversal function for the \"following\" direction The\n          following axis contains all nodes in the same document as\n          the context node that are after the context node in\n          document order, excluding any descendants and excluding\n          attribute nodes and namespace nodes; the nodes are ordered\n           in document order", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample_view(min_dist, max_dist=None):\n  '''\n  '''\n  if max_dist is None:\n    max_dist = min_dist\n  dist = np.random.uniform(min_dist, max_dist)\n  eye = np.random.normal(size=3)\n  eye = normalize(eye)*dist\n  return lookat(eye)", "output": "Sample random camera position.\n  \n  Sample origin directed camera position in given distance\n  range from the origin. ModelView matrix is returned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _record_hyper_configs(self, hyper_configs):\n        \"\"\"\n        \"\"\"\n        self.hyper_configs.append(hyper_configs)\n        self.configs_perf.append(dict())\n        self.num_finished_configs.append(0)\n        self.num_configs_to_run.append(len(hyper_configs))\n        self.increase_i()", "output": "after generating one round of hyperconfigs, this function records the generated hyperconfigs,\n        creates a dict to record the performance when those hyperconifgs are running, set the number of finished configs\n        in this round to be 0, and increase the round number.\n\n        Parameters\n        ----------\n        hyper_configs: list\n            the generated hyperconfigs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_blank_page(self):\r\n        \"\"\"\"\"\"\r\n        loading_template = Template(BLANK)\r\n        page = loading_template.substitute(css_path=self.css_path)\r\n        return page", "output": "Create html page to show while the kernel is starting", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self):\n        \"\"\"\n        \"\"\"\n        if len(self.call_queue):\n            return self.apply(lambda x: x).get()\n        try:\n            return ray.get(self.oid)\n        except RayTaskError as e:\n            handle_ray_task_error(e)", "output": "Gets the object out of the plasma store.\n\n        Returns:\n            The object from the plasma store.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_balancers(profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    balancers = conn.list_balancers(**libcloud_kwargs)\n    ret = []\n    for balancer in balancers:\n        ret.append(_simple_balancer(balancer))\n    return ret", "output": "Return a list of load balancers.\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's list_balancers method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_storage.list_balancers profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_data(self, minion):\n        '''\n        \n        '''\n        if isinstance(self.raw[minion], six.string_types):\n            return {'host': self.raw[minion]}\n        if isinstance(self.raw[minion], dict):\n            return self.raw[minion]\n        return False", "output": "Return the configured ip", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_list(user=None, password=None, host=None, port=None, database='admin', authdb=None):\n    '''\n    \n    '''\n    conn = _connect(user, password, host, port, authdb=authdb)\n    if not conn:\n        return 'Failed to connect to mongo database'\n\n    try:\n        log.info('Listing users')\n        mdb = pymongo.database.Database(conn, database)\n\n        output = []\n        mongodb_version = _version(mdb)\n\n        if _LooseVersion(mongodb_version) >= _LooseVersion('2.6'):\n            for user in mdb.command('usersInfo')['users']:\n                output.append(\n                    {'user': user['user'],\n                     'roles': user['roles']}\n                )\n        else:\n            for user in mdb.system.users.find():\n                output.append(\n                    {'user': user['user'],\n                     'readOnly': user.get('readOnly', 'None')}\n                )\n        return output\n\n    except pymongo.errors.PyMongoError as err:\n        log.error('Listing users failed with error: %s', err)\n        return six.text_type(err)", "output": "List users of a MongoDB database\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mongodb.user_list <user> <password> <host> <port> <database>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_summary_struct(self):\n        \"\"\"\n        \n        \"\"\"\n\n        model_fields = [\n            (\"Method\", 'method'),\n            (\"Number of distance components\", 'num_distance_components'),\n            (\"Number of examples\", 'num_examples'),\n            (\"Number of feature columns\", 'num_features'),\n            (\"Number of unpacked features\", 'num_unpacked_features'),\n            (\"Total training time (seconds)\", 'training_time')]\n\n        ball_tree_fields = [\n            (\"Tree depth\", 'tree_depth'),\n            (\"Leaf size\", 'leaf_size')]\n\n        lsh_fields = [\n            (\"Number of hash tables\", 'num_tables'),\n            (\"Number of projections per table\", 'num_projections_per_table')]\n\n        sections = [model_fields]\n        section_titles = ['Attributes']\n\n        if (self.method == 'ball_tree'):\n            sections.append(ball_tree_fields)\n            section_titles.append('Ball Tree Attributes')\n\n        if (self.method == 'lsh'):\n            sections.append(lsh_fields)\n            section_titles.append('LSH Attributes')\n\n        return (sections, section_titles)", "output": "Returns a structured description of the model, including (where\n        relevant) the schema of the training data, description of the training\n        data, training statistics, and model hyperparameters.\n\n        Returns\n        -------\n        sections : list (of list of tuples)\n            A list of summary sections.\n              Each section is a list.\n                Each item in a section list is a tuple of the form:\n                  ('<label>','<field>')\n        section_titles: list\n            A list of section titles.\n              The order matches that of the 'sections' object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse(self, input):\n        \"\"\"\n        \n        \"\"\"\n        result = self._parseIso8601(input)\n        if not result:\n            result = self._parseSimple(input)\n        if result is not None:\n            return result\n        else:\n            raise ParameterException(\"Invalid time delta - could not parse %s\" % input)", "output": "Parses a time delta from the input.\n\n        See :py:class:`TimeDeltaParameter` for details on supported formats.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run_migrations_online():\n    \"\"\"\n\n    \"\"\"\n\n    # this callback is used to prevent an auto-migration from being generated\n    # when there are no changes to the schema\n    # reference: https://alembic.sqlalchemy.org/en/latest/cookbook.html\n    def process_revision_directives(context, revision, directives):\n        if getattr(config.cmd_opts, 'autogenerate', False):\n            script = directives[0]\n            if script.upgrade_ops.is_empty():\n                directives[:] = []\n                logger.info('No changes in schema detected.')\n\n    engine = engine_from_config(config.get_section(config.config_ini_section),\n                                prefix='sqlalchemy.',\n                                poolclass=pool.NullPool)\n\n    connection = engine.connect()\n    kwargs = {}\n    if engine.name in ('sqlite', 'mysql'):\n        kwargs = {\n            'transaction_per_migration': True,\n            'transactional_ddl': True,\n        }\n    configure_args = current_app.extensions['migrate'].configure_args\n    if configure_args:\n        kwargs.update(configure_args)\n\n    context.configure(connection=connection,\n                      target_metadata=target_metadata,\n                      # compare_type=True,\n                      process_revision_directives=process_revision_directives,\n                      **kwargs)\n\n    try:\n        with context.begin_transaction():\n            context.run_migrations()\n    finally:\n        connection.close()", "output": "Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def center_crop(im, min_sz=None):\n    \"\"\"  \"\"\"\n    r,c,*_ = im.shape\n    if min_sz is None: min_sz = min(r,c)\n    start_r = math.ceil((r-min_sz)/2)\n    start_c = math.ceil((c-min_sz)/2)\n    return crop(im, start_r, start_c, min_sz)", "output": "Return a center crop of an image", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lazily_initialize(self):\n    \"\"\"\"\"\"\n    # TODO(nickfelt): remove on-demand imports once dep situation is fixed.\n    import tensorflow.compat.v1 as tf\n    with self._initialization_lock:\n      if self._session:\n        return\n      graph = tf.Graph()\n      with graph.as_default():\n        self.initialize_graph()\n      # Don't reserve GPU because libpng can't run on GPU.\n      config = tf.ConfigProto(device_count={'GPU': 0})\n      self._session = tf.Session(graph=graph, config=config)", "output": "Initialize the graph and session, if this has not yet been done.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def global_lppooling(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    p_value = attrs.get('p', 2)\n    new_attrs = translation_utils._add_extra_attributes(attrs, {'global_pool': True,\n                                                                'kernel': (1, 1),\n                                                                'pool_type': 'lp',\n                                                                'p_value': p_value})\n    new_attrs = translation_utils._remove_attributes(new_attrs, ['p'])\n    return 'Pooling', new_attrs, inputs", "output": "Performs global lp pooling on the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def num_data(self):\n        \"\"\"\n        \"\"\"\n        if self.handle is not None:\n            ret = ctypes.c_int()\n            _safe_call(_LIB.LGBM_DatasetGetNumData(self.handle,\n                                                   ctypes.byref(ret)))\n            return ret.value\n        else:\n            raise LightGBMError(\"Cannot get num_data before construct dataset\")", "output": "Get the number of rows in the Dataset.\n\n        Returns\n        -------\n        number_of_rows : int\n            The number of rows in the Dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_train_begin(self, **kwargs: Any) -> None:\n        \"\"\n        self.path.parent.mkdir(parents=True, exist_ok=True)      \n        self.file = self.path.open('a') if self.append else self.path.open('w')\n        self.file.write(','.join(self.learn.recorder.names[:(None if self.add_time else -1)]) + '\\n')", "output": "Prepare file with metric names.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _field_to_json(field, row_value):\n    \"\"\"\n    \"\"\"\n    if row_value is None:\n        return None\n\n    if field.mode == \"REPEATED\":\n        return _repeated_field_to_json(field, row_value)\n\n    if field.field_type == \"RECORD\":\n        return _record_field_to_json(field.fields, row_value)\n\n    return _scalar_field_to_json(field, row_value)", "output": "Convert a field into JSON-serializable values.\n\n    Args:\n        field ( \\\n            :class:`~google.cloud.bigquery.schema.SchemaField`, \\\n        ):\n            The SchemaField to use for type conversion and field name.\n\n        row_value (Union[ \\\n            Sequence[list], \\\n            any, \\\n        ]):\n            Row data to be inserted. If the SchemaField's mode is\n            REPEATED, assume this is a list. If not, the type\n            is inferred from the SchemaField's field_type.\n\n    Returns:\n        any:\n            A JSON-serializable object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, filepath):\n    \"\"\"\n    \"\"\"\n    # Simultaneously iterating through the different data sets in the hdf5\n    # file is >100x slower and the data set is small (26.7MB). Hence, we first\n    # load everything into memory before yielding the samples.\n    image_array, class_array, values_array = _load_data(filepath)\n    for image, classes, values in moves.zip(image_array, class_array,\n                                            values_array):\n      yield dict(\n          image=np.expand_dims(image, -1),\n          label_shape=classes[1],\n          label_scale=classes[2],\n          label_orientation=classes[3],\n          label_x_position=classes[4],\n          label_y_position=classes[5],\n          value_shape=values[1],\n          value_scale=values[2],\n          value_orientation=values[3],\n          value_x_position=values[4],\n          value_y_position=values[5])", "output": "Generates examples for the dSprites data set.\n\n    Args:\n      filepath: path to the dSprites hdf5 file.\n\n    Yields:\n      Dictionaries with images, latent classes, and latent values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy(self, existing_inputs):\n        \"\"\"\"\"\"\n        return PPOPolicyGraph(\n            self.observation_space,\n            self.action_space,\n            self.config,\n            existing_inputs=existing_inputs)", "output": "Creates a copy of self using existing input placeholders.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sinks_api(self):\n        \"\"\"\n        \"\"\"\n        if self._sinks_api is None:\n            if self._use_grpc:\n                self._sinks_api = _gapic.make_sinks_api(self)\n            else:\n                self._sinks_api = JSONSinksAPI(self)\n        return self._sinks_api", "output": "Helper for log sink-related API calls.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.sinks", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def detect_django_settings():\n    \"\"\"\n    \n    \"\"\"\n\n    matches = []\n    for root, dirnames, filenames in os.walk(os.getcwd()):\n        for filename in fnmatch.filter(filenames, '*settings.py'):\n            full = os.path.join(root, filename)\n            if 'site-packages' in full:\n                continue\n            full = os.path.join(root, filename)\n            package_path = full.replace(os.getcwd(), '')\n            package_module = package_path.replace(os.sep, '.').split('.', 1)[1].replace('.py', '')\n\n            matches.append(package_module)\n    return matches", "output": "Automatically try to discover Django settings files,\n    return them as relative module paths.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self, path):\n        \"\"\"\n        \n        \"\"\"\n        import hdfs\n        try:\n            self.client.status(path)\n            return True\n        except hdfs.util.HdfsError as e:\n            if str(e).startswith('File does not exist: '):\n                return False\n            else:\n                raise e", "output": "Returns true if the path exists and false otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dump(self, directory=''):\n        \"\"\"\n        \n        \"\"\"\n        with self.no_unpicklable_properties():\n            file_name = os.path.join(directory, 'job-instance.pickle')\n            if self.__module__ == '__main__':\n                d = pickle.dumps(self)\n                module_name = os.path.basename(sys.argv[0]).rsplit('.', 1)[0]\n                d = d.replace(b'(c__main__', \"(c\" + module_name)\n                open(file_name, \"wb\").write(d)\n\n            else:\n                pickle.dump(self, open(file_name, \"wb\"))", "output": "Dump instance to file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply(\n        self,\n        func,\n        num_splits=None,\n        other_axis_partition=None,\n        maintain_partitioning=True,\n        **kwargs\n    ):\n        \"\"\"\n        \"\"\"\n        import dask\n\n        if num_splits is None:\n            num_splits = len(self.list_of_blocks)\n\n        if other_axis_partition is not None:\n            return [\n                DaskFramePartition(dask.delayed(obj))\n                for obj in deploy_func_between_two_axis_partitions(\n                    self.axis,\n                    func,\n                    num_splits,\n                    len(self.list_of_blocks),\n                    kwargs,\n                    *dask.compute(\n                        *tuple(\n                            self.list_of_blocks + other_axis_partition.list_of_blocks\n                        )\n                    )\n                )\n            ]\n\n        args = [self.axis, func, num_splits, kwargs, maintain_partitioning]\n\n        args.extend(dask.compute(*self.list_of_blocks))\n        return [\n            DaskFramePartition(dask.delayed(obj)) for obj in deploy_axis_func(*args)\n        ]", "output": "Applies func to the object.\n\n        See notes in Parent class about this method.\n\n        Args:\n            func: The function to apply.\n            num_splits: The number of times to split the result object.\n            other_axis_partition: Another `DaskFrameAxisPartition` object to apply to\n                func with this one.\n\n        Returns:\n            A list of `DaskFramePartition` objects.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def counting_sort(arr):\n    \"\"\"\n    \n    \"\"\"\n\n    m = min(arr)\n    # in case there are negative elements, change the array to all positive element\n    different = 0\n    if m < 0:\n        # save the change, so that we can convert the array back to all positive number\n        different = -m\n        for i in range(len(arr)):\n            arr[i] += -m\n    k = max(arr)\n    temp_arr = [0] * (k + 1)\n    for i in range(0, len(arr)):\n        temp_arr[arr[i]] = temp_arr[arr[i]] + 1\n    # temp_array[i] contain the times the number i appear in arr\n\n    for i in range(1, k + 1):\n        temp_arr[i] = temp_arr[i] + temp_arr[i - 1]\n    # temp_array[i] contain the number of element less than or equal i in arr\n\n    result_arr = arr.copy()\n    # creating a result_arr an put the element in a correct positon\n    for i in range(len(arr) - 1, -1, -1):\n        result_arr[temp_arr[arr[i]] - 1] = arr[i] - different\n        temp_arr[arr[i]] = temp_arr[arr[i]] - 1\n\n    return result_arr", "output": "Counting_sort\n    Sorting a array which has no element greater than k\n    Creating a new temp_arr,where temp_arr[i] contain the number of\n    element less than or equal to i in the arr\n    Then placing the number i into a correct position in the result_arr\n    return the result_arr\n    Complexity: 0(n)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_directory(self):\r\n        \"\"\"\"\"\"\r\n        self.redirect_stdio.emit(False)\r\n        directory = getexistingdirectory(self.main, _(\"\"),\r\n                                         getcwd_or_home())\r\n        if directory:\r\n            self.chdir(directory)\r\n        self.redirect_stdio.emit(True)", "output": "Select directory", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _affine_inv_mult(c, m):\n    \"\"\n    size = c.flow.size()\n    h,w = c.size\n    m[0,1] *= h/w\n    m[1,0] *= w/h\n    c.flow = c.flow.view(-1,2)\n    a = torch.inverse(m[:2,:2].t())\n    c.flow = torch.mm(c.flow - m[:2,2], a).view(size)\n    return c", "output": "Applies the inverse affine transform described in `m` to `c`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_command(self, ctx, args):\n        \"\"\"\n        \n        \"\"\"\n        original_cmd_name = click.utils.make_str(args[0])\n\n        try:\n            return super(DYMMixin, self).resolve_command(ctx, args)\n        except click.exceptions.UsageError as error:\n            error_msg = str(error)\n            matches = difflib.get_close_matches(original_cmd_name,\n                                                self.list_commands(ctx), self.max_suggestions, self.cutoff)\n            if matches:\n                error_msg += '\\n\\nDid you mean one of these?\\n    %s' % '\\n    '.join(matches)  # pylint: disable=line-too-long\n\n            raise click.exceptions.UsageError(error_msg, error.ctx)", "output": "Overrides clicks ``resolve_command`` method\n        and appends *Did you mean ...* suggestions\n        to the raised exception message.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _env_is_exposed(env):\n    '''\n    \n    '''\n    if __opts__['svnfs_env_whitelist']:\n        salt.utils.versions.warn_until(\n            'Neon',\n            'The svnfs_env_whitelist config option has been renamed to '\n            'svnfs_saltenv_whitelist. Please update your configuration.'\n        )\n        whitelist = __opts__['svnfs_env_whitelist']\n    else:\n        whitelist = __opts__['svnfs_saltenv_whitelist']\n\n    if __opts__['svnfs_env_blacklist']:\n        salt.utils.versions.warn_until(\n            'Neon',\n            'The svnfs_env_blacklist config option has been renamed to '\n            'svnfs_saltenv_blacklist. Please update your configuration.'\n        )\n        blacklist = __opts__['svnfs_env_blacklist']\n    else:\n        blacklist = __opts__['svnfs_saltenv_blacklist']\n\n    return salt.utils.stringutils.check_whitelist_blacklist(\n        env,\n        whitelist=whitelist,\n        blacklist=blacklist,\n    )", "output": "Check if an environment is exposed by comparing it against a whitelist and\n    blacklist.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def predict_array(self, arr):\n        \"\"\"\n        \n        \"\"\"\n        precompute = self.precompute\n        self.precompute = False\n        pred = super().predict_array(arr)\n        self.precompute = precompute\n        return pred", "output": "This over-ride is necessary because otherwise the learner method accesses the wrong model when it is called\n        with precompute set to true\n\n        Args:\n            arr: a numpy array to be used as input to the model for prediction purposes\n        Returns:\n            a numpy array containing the predictions from the model", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rar(rarfile, sources, template=None, cwd=None, runas=None):\n    '''\n    \n    '''\n    cmd = ['rar', 'a', '-idp', '{0}'.format(rarfile)]\n    cmd.extend(_expand_sources(sources))\n    return __salt__['cmd.run'](cmd,\n                               cwd=cwd,\n                               template=template,\n                               runas=runas,\n                               python_shell=False).splitlines()", "output": "Uses `rar for Linux`_ to create rar files\n\n    .. _`rar for Linux`: http://www.rarlab.com/\n\n    rarfile\n        Path of rar file to be created\n\n    sources\n        Comma-separated list of sources to include in the rar file. Sources can\n        also be passed in a Python list.\n\n        .. versionchanged:: 2017.7.0\n            Globbing is now supported for this argument\n\n    cwd : None\n        Run the rar command from the specified directory. Use this argument\n        along with relative file paths to create rar files which do not\n        contain the leading directories. If not specified, this will default\n        to the home directory of the user under which the salt minion process\n        is running.\n\n        .. versionadded:: 2014.7.1\n\n    template : None\n        Can be set to 'jinja' or another supported template engine to render\n        the command arguments before execution:\n\n        .. code-block:: bash\n\n            salt '*' archive.rar template=jinja /tmp/rarfile.rar '/tmp/sourcefile1,/tmp/{{grains.id}}.txt'\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' archive.rar /tmp/rarfile.rar /tmp/sourcefile1,/tmp/sourcefile2\n        # Globbing for sources (2017.7.0 and later)\n        salt '*' archive.rar /tmp/rarfile.rar '/tmp/sourcefile*'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def def_emb_sz(classes, n, sz_dict=None):\n    \"\"\n    sz_dict = ifnone(sz_dict, {})\n    n_cat = len(classes[n])\n    sz = sz_dict.get(n, int(emb_sz_rule(n_cat)))  # rule of thumb\n    return n_cat,sz", "output": "Pick an embedding size for `n` depending on `classes` if not given in `sz_dict`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_references_from_wets(wet_files, metadata_dir, out_dir,\n                                 tmp_dir=None):\n  \"\"\"\"\"\"\n  # Setup output files\n  shard_files = make_ref_shard_files(out_dir)\n\n  num_refs = 0\n  for i, wet_file in enumerate(wet_files):\n    num_refs_in_wet = 0\n    tf.logging.info(\"Processing file %d\", i)\n\n    # Read metadata file\n    metadata_fname = os.path.join(\n        metadata_dir, os.path.basename(wet_file)) + cc_utils.METADTA_SUFFIX\n    with tf.gfile.Open(cc_utils.readahead(metadata_fname)) as f:\n      wet_metadata = json.loads(f.read())\n\n    if not wet_metadata:\n      # No references in this WET file\n      continue\n\n    if wet_file.startswith(\"http\"):\n      # download\n      if not tmp_dir:\n        tmp_dir = tempfile.gettempdir()\n      record_gen = cc_utils.wet_records_from_url(wet_file, tmp_dir)\n    else:\n      # local\n      record_gen = cc_utils.wet_records_from_file_obj(\n          cc_utils.gzip_memfile(wet_file), take_ownership=True)\n\n    for wet_record in record_gen:\n      shard_ids = wet_metadata.get(wet_record.url)\n      if not shard_ids:\n        # URL not in dataset\n        continue\n\n      # Serialize and write out\n      ex = _make_example_from_record(wet_record)\n      ex_str = ex.SerializeToString()\n      for shard_id in shard_ids:\n        shard_files[shard_id].write(ex_str)\n      num_refs += 1\n      num_refs_in_wet += 1\n\n    tf.logging.info(\"Wrote out %d references for this WET\", num_refs_in_wet)\n\n  tf.logging.info(\"Wrote out %d references total\", num_refs)\n\n  # Cleanup\n  for shard_file in shard_files:\n    shard_file.close()", "output": "Extract references from WET files into sharded output files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_all_subscriptions_by_topic(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    \n    '''\n    cache_key = _subscriptions_cache_key(name)\n    try:\n        return __context__[cache_key]\n    except KeyError:\n        pass\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    ret = conn.get_all_subscriptions_by_topic(get_arn(name, region, key, keyid, profile))\n    __context__[cache_key] = ret['ListSubscriptionsByTopicResponse']['ListSubscriptionsByTopicResult']['Subscriptions']\n    return __context__[cache_key]", "output": "Get list of all subscriptions to a specific topic.\n\n    CLI example to delete a topic::\n\n        salt myminion boto_sns.get_all_subscriptions_by_topic mytopic region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _thread_main(self):\n        \"\"\"\n        \"\"\"\n        _LOGGER.debug(\"Background thread started.\")\n\n        quit_ = False\n        while True:\n            batch = self._cloud_logger.batch()\n            items = _get_many(\n                self._queue,\n                max_items=self._max_batch_size,\n                max_latency=self._max_latency,\n            )\n\n            for item in items:\n                if item is _WORKER_TERMINATOR:\n                    quit_ = True\n                    # Continue processing items, don't break, try to process\n                    # all items we got back before quitting.\n                else:\n                    batch.log_struct(**item)\n\n            self._safely_commit_batch(batch)\n\n            for _ in range(len(items)):\n                self._queue.task_done()\n\n            if quit_:\n                break\n\n        _LOGGER.debug(\"Background thread exited gracefully.\")", "output": "The entry point for the worker thread.\n\n        Pulls pending log entries off the queue and writes them in batches to\n        the Cloud Logger.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_evaluation(period=1, show_stdv=True):\n    \"\"\"\n    \"\"\"\n    def callback(env):\n        \"\"\"internal function\"\"\"\n        if env.rank != 0 or (not env.evaluation_result_list) or period is False or period == 0:\n            return\n        i = env.iteration\n        if i % period == 0 or i + 1 == env.begin_iteration or i + 1 == env.end_iteration:\n            msg = '\\t'.join([_fmt_metric(x, show_stdv) for x in env.evaluation_result_list])\n            rabit.tracker_print('[%d]\\t%s\\n' % (i, msg))\n    return callback", "output": "Create a callback that print evaluation result.\n\n    We print the evaluation results every **period** iterations\n    and on the first and the last iterations.\n\n    Parameters\n    ----------\n    period : int\n        The period to log the evaluation results\n\n    show_stdv : bool, optional\n         Whether show stdv if provided\n\n    Returns\n    -------\n    callback : function\n        A callback that print evaluation every period iterations.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autocorrelation_plot(series, ax=None, **kwds):\n    \"\"\"\n    \n    \"\"\"\n    import matplotlib.pyplot as plt\n    n = len(series)\n    data = np.asarray(series)\n    if ax is None:\n        ax = plt.gca(xlim=(1, n), ylim=(-1.0, 1.0))\n    mean = np.mean(data)\n    c0 = np.sum((data - mean) ** 2) / float(n)\n\n    def r(h):\n        return ((data[:n - h] - mean) *\n                (data[h:] - mean)).sum() / float(n) / c0\n    x = np.arange(n) + 1\n    y = lmap(r, x)\n    z95 = 1.959963984540054\n    z99 = 2.5758293035489004\n    ax.axhline(y=z99 / np.sqrt(n), linestyle='--', color='grey')\n    ax.axhline(y=z95 / np.sqrt(n), color='grey')\n    ax.axhline(y=0.0, color='black')\n    ax.axhline(y=-z95 / np.sqrt(n), color='grey')\n    ax.axhline(y=-z99 / np.sqrt(n), linestyle='--', color='grey')\n    ax.set_xlabel(\"Lag\")\n    ax.set_ylabel(\"Autocorrelation\")\n    ax.plot(x, y, **kwds)\n    if 'label' in kwds:\n        ax.legend()\n    ax.grid()\n    return ax", "output": "Autocorrelation plot for time series.\n\n    Parameters:\n    -----------\n    series: Time series\n    ax: Matplotlib axis object, optional\n    kwds : keywords\n        Options to pass to matplotlib plotting method\n\n    Returns:\n    -----------\n    class:`matplotlib.axis.Axes`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _lcs(p, l):\n        \"\"\" \"\"\"\n        # Dynamic Programming Finding LCS\n        if len(p) == 0:\n            return 0\n        P = np.array(list(p)).reshape((1, len(p)))\n        L = np.array(list(l)).reshape((len(l), 1))\n        M = np.ndarray(shape=(len(P), len(L)), dtype=np.int32)\n        for i in range(M.shape[0]):\n            for j in range(M.shape[1]):\n                up = 0 if i == 0 else M[i-1, j]\n                left = 0 if j == 0 else M[i, j-1]\n\n                if i == 0 or j == 0:\n                    M[i, j] = max(up, left, M[i, j])\n                else:\n                    M[i, j] = M[i, j] + M[i - 1, j - 1]\n        return M.max()", "output": "Calculates the Longest Common Subsequence between p and l (both list of int) and returns its length", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relaxNGNewMemParserCtxt(buffer, size):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlRelaxNGNewMemParserCtxt(buffer, size)\n    if ret is None:raise parserError('xmlRelaxNGNewMemParserCtxt() failed')\n    return relaxNgParserCtxt(_obj=ret)", "output": "Create an XML RelaxNGs parse context for that memory buffer\n       expected to contain an XML RelaxNGs file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_market_order(self, share_counts):\n        \"\"\"\n        \"\"\"\n        style = MarketOrder()\n        order_args = [\n            (asset, amount, style)\n            for (asset, amount) in iteritems(share_counts)\n            if amount\n        ]\n        return self.blotter.batch_order(order_args)", "output": "Place a batch market order for multiple assets.\n\n        Parameters\n        ----------\n        share_counts : pd.Series[Asset -> int]\n            Map from asset to number of shares to order for that asset.\n\n        Returns\n        -------\n        order_ids : pd.Index[str]\n            Index of ids for newly-created orders.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform (list, pattern, indices = [1]):\n    \"\"\" \n    \"\"\"\n    result = []\n\n    for e in list:\n        m = re.match (pattern, e)\n\n        if m:\n            for i in indices:\n                result.append (m.group (i))\n\n    return result", "output": "Matches all elements of 'list' agains the 'pattern'\n        and returns a list of the elements indicated by indices of\n        all successfull matches. If 'indices' is omitted returns\n        a list of first paranthethised groups of all successfull\n        matches.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_bst(root):\n    \"\"\"\n    \n    \"\"\"\n\n    stack = []\n    pre = None\n    \n    while root or stack:\n        while root:\n            stack.append(root)\n            root = root.left\n        root = stack.pop()\n        if pre and root.val <= pre.val:\n            return False\n        pre = root\n        root = root.right\n\n    return True", "output": ":type root: TreeNode\n    :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serve_file(self, load):\n        '''\n        \n        '''\n        ret = {'data': '',\n               'dest': ''}\n\n        if 'env' in load:\n            # \"env\" is not supported; Use \"saltenv\".\n            load.pop('env')\n\n        if 'path' not in load or 'loc' not in load or 'saltenv' not in load:\n            return ret\n        if not isinstance(load['saltenv'], six.string_types):\n            load['saltenv'] = six.text_type(load['saltenv'])\n\n        fnd = self.find_file(load['path'], load['saltenv'])\n        if not fnd.get('back'):\n            return ret\n        fstr = '{0}.serve_file'.format(fnd['back'])\n        if fstr in self.servers:\n            return self.servers[fstr](load, fnd)\n        return ret", "output": "Serve up a chunk of a file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pad_sentences(sentences, padding_word=\"</s>\"):\n    \"\"\"\n    \"\"\"\n    sequence_length = max(len(x) for x in sentences)\n    padded_sentences = []\n    for i, sentence in enumerate(sentences):\n        num_padding = sequence_length - len(sentence)\n        new_sentence = sentence + [padding_word] * num_padding\n        padded_sentences.append(new_sentence)\n    return padded_sentences", "output": "Pads all sentences to the same length. The length is defined by the longest sentence.\n    Returns padded sentences.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reboot(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The restart action must be called with -a or --action.'\n        )\n\n    data = show_instance(name, call='action')\n    if data.get('status') == 'off':\n        return {'success': True,\n                'action': 'stop',\n                'status': 'off',\n                'msg': 'Machine is already off.'}\n\n    ret = query(droplet_id=data['id'],\n                command='actions',\n                args={'type': 'reboot'},\n                http_method='post')\n\n    return {'success': True,\n            'action': ret['action']['type'],\n            'state': ret['action']['status']}", "output": "Reboot a droplet in DigitalOcean.\n\n    .. versionadded:: 2015.8.8\n\n    name\n        The name of the droplet to restart.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a reboot droplet_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_host_hostname(name, domains=None, **api_opts):\n    '''\n    \n    '''\n    name = name.lower().rstrip('.')\n    if not domains:\n        return name.split('.')[0]\n    domain = get_host_domainname(name, domains, **api_opts)\n    if domain and domain in name:\n        return name.rsplit('.' + domain)[0]\n    return name", "output": "Get hostname\n\n    If no domains are passed, the hostname is checked for a zone in infoblox,\n    if no zone split on first dot.\n\n    If domains are provided, the best match out of the list is truncated from\n    the fqdn leaving the hostname.\n\n    If no matching domains are found the fqdn is returned.\n\n    dots at end of names are ignored.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call infoblox.get_host_hostname fqdn=localhost.xxx.t.domain.com \\\n            domains=\"['domain.com', 't.domain.com']\"\n        #returns: localhost.xxx\n\n        salt-call infoblox.get_host_hostname fqdn=localhost.xxx.t.domain.com\n        #returns: localhost", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _apply_dvs_network_resource_pools(network_resource_pools, resource_dicts):\n    '''\n    \n    '''\n    for res_dict in resource_dicts:\n        ress = [r for r in network_resource_pools if r.key == res_dict['key']]\n        if ress:\n            res = ress[0]\n        else:\n            res = vim.DVSNetworkResourcePoolConfigSpec()\n            res.key = res_dict['key']\n            res.allocationInfo = \\\n                    vim.DVSNetworkResourcePoolAllocationInfo()\n            network_resource_pools.append(res)\n        if res_dict.get('limit'):\n            res.allocationInfo.limit = res_dict['limit']\n        if res_dict.get('num_shares') and res_dict.get('share_level'):\n            if not res.allocationInfo.shares:\n                res.allocationInfo.shares = vim.SharesInfo()\n            res.allocationInfo.shares.shares = res_dict['num_shares']\n            res.allocationInfo.shares.level = \\\n                    vim.SharesLevel(res_dict['share_level'])", "output": "Applies the values of the resource dictionaries to network resource pools,\n    creating the resource pools if required\n    (vim.DVSNetworkResourcePoolConfigSpec)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_jids():\n    '''\n    \n    '''\n    with _get_serv(ret=None, commit=True) as cur:\n\n        sql = '''SELECT jid, load\n                FROM jids'''\n\n        cur.execute(sql)\n        data = cur.fetchall()\n        ret = {}\n        for jid, load in data:\n            ret[jid] = salt.utils.jid.format_jid_instance(jid,\n                                                          salt.utils.json.loads(load))\n        return ret", "output": "Return a list of all job ids", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _paste_mask(box, mask, shape):\n    \"\"\"\n    \n    \"\"\"\n    # int() is floor\n    # box fpcoor=0.0 -> intcoor=0.0\n    x0, y0 = list(map(int, box[:2] + 0.5))\n    # box fpcoor=h -> intcoor=h-1, inclusive\n    x1, y1 = list(map(int, box[2:] - 0.5))    # inclusive\n    x1 = max(x0, x1)    # require at least 1x1\n    y1 = max(y0, y1)\n\n    w = x1 + 1 - x0\n    h = y1 + 1 - y0\n\n    # rounding errors could happen here, because masks were not originally computed for this shape.\n    # but it's hard to do better, because the network does not know the \"original\" scale\n    mask = (cv2.resize(mask, (w, h)) > 0.5).astype('uint8')\n    ret = np.zeros(shape, dtype='uint8')\n    ret[y0:y1 + 1, x0:x1 + 1] = mask\n    return ret", "output": "Args:\n        box: 4 float\n        mask: MxM floats\n        shape: h,w\n    Returns:\n        A uint8 binary image of hxw.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def feature_importances_(self):\n        \"\"\"\n        \n\n        \"\"\"\n        if getattr(self, 'booster', None) is not None and self.booster != 'gbtree':\n            raise AttributeError('Feature importance is not defined for Booster type {}'\n                                 .format(self.booster))\n        b = self.get_booster()\n        score = b.get_score(importance_type=self.importance_type)\n        all_features = [score.get(f, 0.) for f in b.feature_names]\n        all_features = np.array(all_features, dtype=np.float32)\n        return all_features / all_features.sum()", "output": "Feature importances property\n\n        .. note:: Feature importance is defined only for tree boosters\n\n            Feature importance is only defined when the decision tree model is chosen as base\n            learner (`booster=gbtree`). It is not defined for other base learner types, such\n            as linear learners (`booster=gblinear`).\n\n        Returns\n        -------\n        feature_importances_ : array of shape ``[n_features]``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_unscoped_hparams(scopes_and_hparams):\n  \"\"\"\"\"\"\n  merged_values = {}\n  for (scope, hparams) in scopes_and_hparams:\n    for key, value in six.iteritems(hparams.values()):\n      scoped_key = \"%s.%s\" % (scope, key)\n      merged_values[scoped_key] = value\n\n  return hparam.HParams(**merged_values)", "output": "Merge multiple HParams into one with scopes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def since(version):\n    \"\"\"\n    \n    \"\"\"\n    import re\n    indent_p = re.compile(r'\\n( +)')\n\n    def deco(f):\n        indents = indent_p.findall(f.__doc__)\n        indent = ' ' * (min(len(m) for m in indents) if indents else 0)\n        f.__doc__ = f.__doc__.rstrip() + \"\\n\\n%s.. versionadded:: %s\" % (indent, version)\n        return f\n    return deco", "output": "A decorator that annotates a function to append the version of Spark the function was added.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _drop_nodes_from_errorpaths(self, _errors, dp_items, sp_items):\n        \"\"\" \n        \"\"\"\n        dp_basedepth = len(self.document_path)\n        sp_basedepth = len(self.schema_path)\n        for error in _errors:\n            for i in sorted(dp_items, reverse=True):\n                error.document_path = \\\n                    drop_item_from_tuple(error.document_path, dp_basedepth + i)\n            for i in sorted(sp_items, reverse=True):\n                error.schema_path = \\\n                    drop_item_from_tuple(error.schema_path, sp_basedepth + i)\n            if error.child_errors:\n                self._drop_nodes_from_errorpaths(error.child_errors,\n                                                 dp_items, sp_items)", "output": "Removes nodes by index from an errorpath, relatively to the\n            basepaths of self.\n\n        :param errors: A list of :class:`errors.ValidationError` instances.\n        :param dp_items: A list of integers, pointing at the nodes to drop from\n                         the :attr:`document_path`.\n        :param sp_items: Alike ``dp_items``, but for :attr:`schema_path`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def __process_acl(self, load, auth_list):\n        '''\n        \n        '''\n        if 'eauth' not in load:\n            return auth_list\n        fstr = '{0}.process_acl'.format(load['eauth'])\n        if fstr not in self.auth:\n            return auth_list\n        try:\n            return self.auth[fstr](auth_list, self.opts)\n        except Exception as e:\n            log.debug('Authentication module threw %s', e)\n            return auth_list", "output": "Allows eauth module to modify the access list right before it'll be applied to the request.\n        For example ldap auth module expands entries", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_sqlite_connection_provider(db_uri):\n  \"\"\"\n  \"\"\"\n  uri = urlparse.urlparse(db_uri)\n  if uri.scheme != 'sqlite':\n    raise ValueError('Scheme is not sqlite: ' + db_uri)\n  if uri.netloc:\n    raise ValueError('Can not connect to SQLite over network: ' + db_uri)\n  if uri.path == ':memory:':\n    raise ValueError('Memory mode SQLite not supported: ' + db_uri)\n  path = os.path.expanduser(uri.path)\n  params = _get_connect_params(uri.query)\n  # TODO(@jart): Add thread-local pooling.\n  return lambda: sqlite3.connect(path, **params)", "output": "Returns function that returns SQLite Connection objects.\n\n  Args:\n    db_uri: A string URI expressing the DB file, e.g. \"sqlite:~/tb.db\".\n\n  Returns:\n    A function that returns a new PEP-249 DB Connection, which must be closed,\n    each time it is called.\n\n  Raises:\n    ValueError: If db_uri is not a valid sqlite file URI.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rm_automaster(name, device, config='/etc/auto_salt'):\n    '''\n    \n    '''\n    contents = automaster(config)\n    if name not in contents:\n        return True\n    # The entry is present, get rid of it\n    lines = []\n    try:\n        with salt.utils.files.fopen(config, 'r') as ifile:\n            for line in ifile:\n                line = salt.utils.stringutils.to_unicode(line)\n                if line.startswith('#'):\n                    # Commented\n                    lines.append(line)\n                    continue\n                if not line.strip():\n                    # Blank line\n                    lines.append(line)\n                    continue\n                comps = line.split()\n                if len(comps) != 3:\n                    # Invalid entry\n                    lines.append(line)\n                    continue\n\n                comps = line.split()\n                prefix = \"/..\"\n                name_chk = comps[0].replace(prefix, \"\")\n                device_fmt = comps[2].split(\":\")\n\n                if device:\n                    if name_chk == name and device_fmt[1] == device:\n                        continue\n                else:\n                    if name_chk == name:\n                        continue\n                lines.append(line)\n    except (IOError, OSError) as exc:\n        msg = \"Couldn't read from {0}: {1}\"\n        raise CommandExecutionError(msg.format(config, exc))\n\n    try:\n        with salt.utils.files.fopen(config, 'wb') as ofile:\n            ofile.writelines(salt.utils.data.encode(lines))\n    except (IOError, OSError) as exc:\n        msg = \"Couldn't write to {0}: {1}\"\n        raise CommandExecutionError(msg.format(config, exc))\n\n    # Update automount\n    __salt__['cmd.run']('automount -cv')\n    return True", "output": "Remove the mount point from the auto_master\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' mount.rm_automaster /mnt/foo /dev/sdg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_fold_trigger(block):\n        \"\"\"\n        \n        \"\"\"\n        if block is None:\n            return False\n        state = block.userState()\n        if state == -1:\n            state = 0\n        return bool(state & 0x04000000)", "output": "Checks if the block is a fold trigger.\n\n        :param block: block to check\n        :return: True if the block is a fold trigger (represented as a node in\n            the fold panel)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def activated(self, item):\r\n        \"\"\"\"\"\"\r\n        itemdata = self.data.get(id(self.currentItem()))\r\n        if itemdata is not None:\r\n            filename, lineno, colno = itemdata\r\n            self.sig_edit_goto.emit(filename, lineno, self.search_text)", "output": "Double-click event", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shapeless_placeholder(x, axis, name):\n    \"\"\"\n    \n    \"\"\"\n    shp = x.get_shape().as_list()\n    if not isinstance(axis, list):\n        axis = [axis]\n    for a in axis:\n        if shp[a] is None:\n            raise ValueError(\"Axis {} of shape {} is already unknown!\".format(a, shp))\n        shp[a] = None\n    x = tf.placeholder_with_default(x, shape=shp, name=name)\n    return x", "output": "Make the static shape of a tensor less specific.\n\n    If you want to feed to a tensor, the shape of the feed value must match\n    the tensor's static shape. This function creates a placeholder which\n    defaults to x if not fed, but has a less specific static shape than x.\n    See also `tensorflow#5680 <https://github.com/tensorflow/tensorflow/issues/5680>`_.\n\n    Args:\n        x: a tensor\n        axis(int or list of ints): these axes of ``x.get_shape()`` will become\n            None in the output.\n        name(str): name of the output tensor\n\n    Returns:\n        a tensor equal to x, but shape information is partially cleared.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(cyg_arch='x86_64', mirrors=None):\n    '''\n    \n    '''\n    args = []\n    args.append('--upgrade-also')\n\n    # Can't update something that isn't installed\n    if not _check_cygwin_installed(cyg_arch):\n        LOG.debug('Cygwin (%s) not installed, could not update', cyg_arch)\n        return False\n\n    return _run_silent_cygwin(cyg_arch=cyg_arch, args=args, mirrors=mirrors)", "output": "Update all packages.\n\n    cyg_arch : x86_64\n        Specify the cygwin architecture update\n        Current options are x86 and x86_64\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' cyg.update\n        salt '*' cyg.update dos2unix mirrors=\"[{'http://mirror': 'http://url/to/public/key}]\"", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def Reload(self):\n    \"\"\"\"\"\"\n    logger.info('Beginning EventMultiplexer.Reload()')\n    self._reload_called = True\n    # Build a list so we're safe even if the list of accumulators is modified\n    # even while we're reloading.\n    with self._accumulators_mutex:\n      items = list(self._accumulators.items())\n    items_queue = queue.Queue()\n    for item in items:\n      items_queue.put(item)\n\n    # Methods of built-in python containers are thread-safe so long as the GIL\n    # for the thread exists, but we might as well be careful.\n    names_to_delete = set()\n    names_to_delete_mutex = threading.Lock()\n\n    def Worker():\n      \"\"\"Keeps reloading accumulators til none are left.\"\"\"\n      while True:\n        try:\n          name, accumulator = items_queue.get(block=False)\n        except queue.Empty:\n          # No more runs to reload.\n          break\n\n        try:\n          accumulator.Reload()\n        except (OSError, IOError) as e:\n          logger.error('Unable to reload accumulator %r: %s', name, e)\n        except directory_watcher.DirectoryDeletedError:\n          with names_to_delete_mutex:\n            names_to_delete.add(name)\n        finally:\n          items_queue.task_done()\n\n    if self._max_reload_threads > 1:\n      num_threads = min(\n          self._max_reload_threads, len(items))\n      logger.info('Starting %d threads to reload runs', num_threads)\n      for i in xrange(num_threads):\n        thread = threading.Thread(target=Worker, name='Reloader %d' % i)\n        thread.daemon = True\n        thread.start()\n      items_queue.join()\n    else:\n      logger.info(\n          'Reloading runs serially (one after another) on the main '\n          'thread.')\n      Worker()\n\n    with self._accumulators_mutex:\n      for name in names_to_delete:\n        logger.warn('Deleting accumulator %r', name)\n        del self._accumulators[name]\n    logger.info('Finished with EventMultiplexer.Reload()')\n    return self", "output": "Call `Reload` on every `EventAccumulator`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_text(initial_config=None,\n               initial_path=None,\n               merge_config=None,\n               merge_path=None,\n               saltenv='base'):\n    '''\n    \n    '''\n    candidate_tree = merge_tree(initial_config=initial_config,\n                                initial_path=initial_path,\n                                merge_config=merge_config,\n                                merge_path=merge_path,\n                                saltenv=saltenv)\n    return _print_config_text(candidate_tree)", "output": "Return the merge result of the ``initial_config`` with the ``merge_config``,\n    as plain text.\n\n    initial_config\n        The initial configuration sent as text. This argument is ignored when\n        ``initial_path`` is set.\n\n    initial_path\n        Absolute or remote path from where to load the initial configuration\n        text. This argument allows any URI supported by\n        :py:func:`cp.get_url <salt.modules.cp.get_url>`), e.g., ``salt://``,\n        ``https://``, ``s3://``, ``ftp:/``, etc.\n\n    merge_config\n        The config to be merged into the initial config, sent as text. This\n        argument is ignored when ``merge_path`` is set.\n\n    merge_path\n        Absolute or remote path from where to load the merge configuration\n        text. This argument allows any URI supported by\n        :py:func:`cp.get_url <salt.modules.cp.get_url>`), e.g., ``salt://``,\n        ``https://``, ``s3://``, ``ftp:/``, etc.\n\n    saltenv: ``base``\n        Salt fileserver environment from which to retrieve the file.\n        Ignored if ``initial_path`` or ``merge_path`` is not a ``salt://`` URL.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' iosconfig.merge_text initial_path=salt://path/to/running.cfg merge_path=salt://path/to/merge.cfg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def equals(self, other):\n        \"\"\"\n        \n        \"\"\"\n        if self.is_(other):\n            return True\n\n        if not isinstance(other, Index):\n            return False\n\n        if not isinstance(other, MultiIndex):\n            other_vals = com.values_from_object(ensure_index(other))\n            return array_equivalent(self._ndarray_values, other_vals)\n\n        if self.nlevels != other.nlevels:\n            return False\n\n        if len(self) != len(other):\n            return False\n\n        for i in range(self.nlevels):\n            self_codes = self.codes[i]\n            self_codes = self_codes[self_codes != -1]\n            self_values = algos.take_nd(np.asarray(self.levels[i]._values),\n                                        self_codes, allow_fill=False)\n\n            other_codes = other.codes[i]\n            other_codes = other_codes[other_codes != -1]\n            other_values = algos.take_nd(\n                np.asarray(other.levels[i]._values),\n                other_codes, allow_fill=False)\n\n            # since we use NaT both datetime64 and timedelta64\n            # we can have a situation where a level is typed say\n            # timedelta64 in self (IOW it has other values than NaT)\n            # but types datetime64 in other (where its all NaT)\n            # but these are equivalent\n            if len(self_values) == 0 and len(other_values) == 0:\n                continue\n\n            if not array_equivalent(self_values, other_values):\n                return False\n\n        return True", "output": "Determines if two MultiIndex objects have the same labeling information\n        (the levels themselves do not necessarily have to be the same)\n\n        See Also\n        --------\n        equal_levels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_train(batch_id, batch_num, metric, step_loss, log_interval, epoch_id, learning_rate):\n    \"\"\"\n    \"\"\"\n    metric_nm, metric_val = metric.get()\n    if not isinstance(metric_nm, list):\n        metric_nm = [metric_nm]\n        metric_val = [metric_val]\n\n    train_str = '[Epoch %d Batch %d/%d] loss=%.4f, lr=%.7f, metrics:' + \\\n                ','.join([i + ':%.4f' for i in metric_nm])\n    logging.info(train_str, epoch_id + 1, batch_id + 1, batch_num, \\\n                 step_loss / log_interval, \\\n                 learning_rate, \\\n                 *metric_val)", "output": "Generate and print out the log message for training.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grad_to_image(gradient):\n    \"\"\"\"\"\"\n    gradient = gradient - gradient.min()\n    gradient /= gradient.max()\n    gradient = np.uint8(gradient * 255).transpose(1, 2, 0)\n    gradient = gradient[..., ::-1]\n    return gradient", "output": "Convert gradients of image obtained using `get_image_grad`\n    into image. This shows parts of the image that is most strongly activating\n    the output neurons.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def path_exists_glob(path):\n    '''\n    \n\n    '''\n    return True if glob.glob(os.path.expanduser(path)) else False", "output": "Tests to see if path after expansion is a valid path (file or directory).\n    Expansion allows usage of ? * and character ranges []. Tilde expansion\n    is not supported. Returns True/False.\n\n    .. versionadded:: 2014.7.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.path_exists_glob /etc/pam*/pass*", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_global_steps():\n        \"\"\"\"\"\"\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False, dtype=tf.int32)\n        increment_step = tf.assign(global_step, tf.add(global_step, 1))\n        return global_step, increment_step", "output": "Creates TF ops to track and increment global training step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _send_error_report(\n        self, message, report_location=None, http_context=None, user=None\n    ):\n        \"\"\"\n        \"\"\"\n        error_report = self._build_error_report(\n            message, report_location, http_context, user\n        )\n        self.report_errors_api.report_error_event(error_report)", "output": "Makes the call to the Error Reporting API.\n\n        This is the lower-level interface to build and send the payload,\n        generally users will use either report() or report_exception() to\n        automatically gather the parameters for this method.\n\n        :type message: str\n        :param message: The stack trace that was reported or logged by the\n                   service.\n\n        :type report_location: dict\n        :param report_location:  The location in the source code where the\n               decision was made to report the error, usually the place\n               where it was logged. For a logged exception this would be the\n               source line where the exception is logged, usually close to\n               the place where it was caught.\n\n               This should be a Python dict that contains the keys 'filePath',\n               'lineNumber', and 'functionName'\n\n        :type http_context: :class`google.cloud.error_reporting.HTTPContext`\n        :param http_context: The HTTP request which was processed when the\n                             error was triggered.\n\n        :type user: str\n        :param user: The user who caused or was affected by the crash. This can\n                     be a user ID, an email address, or an arbitrary token that\n                     uniquely identifies the user. When sending an error\n                     report, leave this field empty if the user was not\n                     logged in. In this  case the Error Reporting system will\n                     use other data, such as remote IP address,\n                     to distinguish affected users.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def array_repeat(col, count):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.array_repeat(_to_java_column(col), count))", "output": "Collection function: creates an array containing a column repeated count times.\n\n    >>> df = spark.createDataFrame([('ab',)], ['data'])\n    >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n    [Row(r=[u'ab', u'ab', u'ab'])]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_subnet(subnet_id=None, subnet_name=None, region=None, key=None,\n                  keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    return _delete_resource(resource='subnet', name=subnet_name,\n                            resource_id=subnet_id, region=region, key=key,\n                            keyid=keyid, profile=profile)", "output": "Given a subnet ID or name, delete the subnet.\n\n    Returns True if the subnet was deleted and returns False if the subnet was not deleted.\n\n    .. versionchanged:: 2015.8.0\n        Added subnet_name argument\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.delete_subnet 'subnet-6a1fe403'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def build_options(slot, appointment_type, date, booking_map):\n    \"\"\"\n    \n    \"\"\"\n    day_strings = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n    if slot == 'AppointmentType':\n        return [\n            {'text': 'cleaning (30 min)', 'value': 'cleaning'},\n            {'text': 'root canal (60 min)', 'value': 'root canal'},\n            {'text': 'whitening (30 min)', 'value': 'whitening'}\n        ]\n    elif slot == 'Date':\n        # Return the next five weekdays.\n        options = []\n        potential_date = datetime.datetime.today()\n        while len(options) < 5:\n            potential_date = potential_date + datetime.timedelta(days=1)\n            if potential_date.weekday() < 5:\n                options.append({'text': '{}-{} ({})'.format((potential_date.month), potential_date.day, day_strings[potential_date.weekday()]),\n                                'value': potential_date.strftime('%A, %B %d, %Y')})\n        return options\n    elif slot == 'Time':\n        # Return the availabilities on the given date.\n        if not appointment_type or not date:\n            return None\n\n        availabilities = try_ex(lambda: booking_map[date])\n        if not availabilities:\n            return None\n\n        availabilities = get_availabilities_for_duration(get_duration(appointment_type), availabilities)\n        if len(availabilities) == 0:\n            return None\n\n        options = []\n        for i in range(min(len(availabilities), 5)):\n            options.append({'text': build_time_output_string(availabilities[i]), 'value': build_time_output_string(availabilities[i])})\n\n        return options", "output": "Build a list of potential options for a given slot, to be used in responseCard generation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate_table(self):\n        \"\"\"\n        \n        \"\"\"\n\n        gso_table = self._gso_table\n        gso_df = self.df\n        columns = list(gso_df.columns)\n        selected = gso_df[self.columns]\n        col_index = [(col, columns.index(col)) for col in self.columns]\n        keys = np.empty(selected.shape, dtype=np.uint64)\n        for o, (idx, row) in enumerate(selected.iterrows()):\n            for j, (col, v) in enumerate(col_index):\n                val = row[col]\n                # Allow columns with mixed str and None (GH 23633)\n                val = '' if val is None else val\n                key = gso_table.get(val, None)\n                if key is None:\n                    # Stata prefers human numbers\n                    key = (v + 1, o + 1)\n                    gso_table[val] = key\n                keys[o, j] = self._convert_key(key)\n        for i, col in enumerate(self.columns):\n            gso_df[col] = keys[:, i]\n\n        return gso_table, gso_df", "output": "Generates the GSO lookup table for the DataFRame\n\n        Returns\n        -------\n        gso_table : OrderedDict\n            Ordered dictionary using the string found as keys\n            and their lookup position (v,o) as values\n        gso_df : DataFrame\n            DataFrame where strl columns have been converted to\n            (v,o) values\n\n        Notes\n        -----\n        Modifies the DataFrame in-place.\n\n        The DataFrame returned encodes the (v,o) values as uint64s. The\n        encoding depends on teh dta version, and can be expressed as\n\n        enc = v + o * 2 ** (o_size * 8)\n\n        so that v is stored in the lower bits and o is in the upper\n        bits. o_size is\n\n          * 117: 4\n          * 118: 6\n          * 119: 5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def del_export(exports='/etc/exports', path=None):\n    '''\n    \n    '''\n    edict = list_exports(exports)\n    del edict[path]\n    _write_exports(exports, edict)\n    return edict", "output": "Remove an export\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nfs.del_export /media/storage", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rand(x_bounds, x_types):\n    '''\n    \n    '''\n    outputs = []\n\n    for i, _ in enumerate(x_bounds):\n        if x_types[i] == \"discrete_int\":\n            temp = x_bounds[i][random.randint(0, len(x_bounds[i]) - 1)]\n            outputs.append(temp)\n        elif x_types[i] == \"range_int\":\n            temp = random.randint(x_bounds[i][0], x_bounds[i][1])\n            outputs.append(temp)\n        elif x_types[i] == \"range_continuous\":\n            temp = random.uniform(x_bounds[i][0], x_bounds[i][1])\n            outputs.append(temp)\n        else:\n            return None\n\n    return outputs", "output": "Random generate variable value within their bounds", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def name(self, pretty=False):\n        \"\"\"\n        \n        \"\"\"\n        name = self.os_release_attr('name') \\\n            or self.lsb_release_attr('distributor_id') \\\n            or self.distro_release_attr('name') \\\n            or self.uname_attr('name')\n        if pretty:\n            name = self.os_release_attr('pretty_name') \\\n                or self.lsb_release_attr('description')\n            if not name:\n                name = self.distro_release_attr('name') \\\n                       or self.uname_attr('name')\n                version = self.version(pretty=True)\n                if version:\n                    name = name + ' ' + version\n        return name or ''", "output": "Return the name of the OS distribution, as a string.\n\n        For details, see :func:`distro.name`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_url(self):\n        '''\n        \n        '''\n        if self.role in ('git_pillar', 'winrepo'):\n            # With winrepo and git_pillar, the remote is specified in the\n            # format '<branch> <url>', so that we can get a unique identifier\n            # to hash for each remote.\n            try:\n                self.branch, self.url = self.id.split(None, 1)\n            except ValueError:\n                self.branch = self.conf['branch']\n                self.url = self.id\n        else:\n            self.url = self.id", "output": "Examine self.id and assign self.url (and self.branch, for git_pillar)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_has_been_edited(self, text):\r\n        \"\"\"\"\"\"\r\n        self.find(changed=True, forward=True, start_highlight_timer=True)", "output": "Find text has been edited (this slot won't be triggered when \r\n        setting the search pattern combo box text programmatically)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_pb(self):\n        \"\"\"\n        \"\"\"\n        union = table_v2_pb2.GcRule.Union(rules=[rule.to_pb() for rule in self.rules])\n        return table_v2_pb2.GcRule(union=union)", "output": "Converts the union into a single GC rule as a protobuf.\n\n        :rtype: :class:`.table_v2_pb2.GcRule`\n        :returns: The converted current object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_option(**kwargs):\n        \"\"\"\n        \"\"\"\n        if len(kwargs) != 1:\n            raise TypeError(_BAD_OPTION_ERR)\n\n        name, value = kwargs.popitem()\n        if name == \"last_update_time\":\n            return _helpers.LastUpdateOption(value)\n        elif name == \"exists\":\n            return _helpers.ExistsOption(value)\n        else:\n            extra = \"{!r} was provided\".format(name)\n            raise TypeError(_BAD_OPTION_ERR, extra)", "output": "Create a write option for write operations.\n\n        Write operations include :meth:`~.DocumentReference.set`,\n        :meth:`~.DocumentReference.update` and\n        :meth:`~.DocumentReference.delete`.\n\n        One of the following keyword arguments must be provided:\n\n        * ``last_update_time`` (:class:`google.protobuf.timestamp_pb2.\\\n               Timestamp`): A timestamp. When set, the target document must\n               exist and have been last updated at that time. Protobuf\n               ``update_time`` timestamps are typically returned from methods\n               that perform write operations as part of a \"write result\"\n               protobuf or directly.\n        * ``exists`` (:class:`bool`): Indicates if the document being modified\n              should already exist.\n\n        Providing no argument would make the option have no effect (so\n        it is not allowed). Providing multiple would be an apparent\n        contradiction, since ``last_update_time`` assumes that the\n        document **was** updated (it can't have been updated if it\n        doesn't exist) and ``exists`` indicate that it is unknown if the\n        document exists or not.\n\n        Args:\n            kwargs (Dict[str, Any]): The keyword arguments described above.\n\n        Raises:\n            TypeError: If anything other than exactly one argument is\n                provided by the caller.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def dataset_status_cli(self, dataset, dataset_opt=None):\n        \"\"\" \n        \"\"\"\n        dataset = dataset or dataset_opt\n        return self.dataset_status(dataset)", "output": "wrapper for client for dataset_status, with additional\n            dataset_opt to get the status of a dataset from the API\n             Parameters\n            ==========\n            dataset_opt: an alternative to dataset", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rotate_v2(array, k):\n    \"\"\"\n    \n    \"\"\"\n    array = array[:]\n\n    def reverse(arr, a, b):\n        while a < b:\n            arr[a], arr[b] = arr[b], arr[a]\n            a += 1\n            b -= 1\n\n    n = len(array)\n    k = k % n\n    reverse(array, 0, n - k - 1)\n    reverse(array, n - k, n - 1)\n    reverse(array, 0, n - 1)\n    return array", "output": "Reverse segments of the array, followed by the entire array\n    T(n)- O(n)\n    :type array: List[int]\n    :type k: int\n    :rtype: void Do not return anything, modify nums in-place instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_employees(order_by='id'):\n    '''\n    \n    '''\n    ret = {}\n    status, result = _query(action='employees', command='directory')\n    root = ET.fromstring(result)\n    directory = root.getchildren()\n    for cat in directory:\n        if cat.tag != 'employees':\n            continue\n        for item in cat:\n            emp_id = item.items()[0][1]\n            emp_ret = {'id': emp_id}\n            for details in item.getchildren():\n                emp_ret[details.items()[0][1]] = details.text\n            ret[emp_ret[order_by]] = emp_ret\n    return ret", "output": "Show all employees for this company.\n\n    CLI Example:\n\n        salt myminion bamboohr.list_employees\n\n    By default, the return data will be keyed by ID. However, it can be ordered\n    by any other field. Keep in mind that if the field that is chosen contains\n    duplicate values (i.e., location is used, for a company which only has one\n    location), then each duplicate value will be overwritten by the previous.\n    Therefore, it is advisable to only sort by fields that are guaranteed to be\n    unique.\n\n    CLI Examples:\n\n        salt myminion bamboohr.list_employees order_by=id\n        salt myminion bamboohr.list_employees order_by=displayName\n        salt myminion bamboohr.list_employees order_by=workEmail", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_commit_response(commit_response_pb):\n    \"\"\"\n    \"\"\"\n    mut_results = commit_response_pb.mutation_results\n    index_updates = commit_response_pb.index_updates\n    completed_keys = [\n        mut_result.key for mut_result in mut_results if mut_result.HasField(\"key\")\n    ]  # Message field (Key)\n    return index_updates, completed_keys", "output": "Extract response data from a commit response.\n\n    :type commit_response_pb: :class:`.datastore_pb2.CommitResponse`\n    :param commit_response_pb: The protobuf response from a commit request.\n\n    :rtype: tuple\n    :returns: The pair of the number of index updates and a list of\n              :class:`.entity_pb2.Key` for each incomplete key\n              that was completed in the commit.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def expect_dimensions(__funcname=_qualified_name, **dimensions):\n    \"\"\"\n    \n    \"\"\"\n    if isinstance(__funcname, str):\n        def get_funcname(_):\n            return __funcname\n    else:\n        get_funcname = __funcname\n\n    def _expect_dimension(expected_ndim):\n        def _check(func, argname, argvalue):\n            actual_ndim = argvalue.ndim\n            if actual_ndim != expected_ndim:\n                if actual_ndim == 0:\n                    actual_repr = 'scalar'\n                else:\n                    actual_repr = \"%d-D array\" % actual_ndim\n                raise ValueError(\n                    \"{func}() expected a {expected:d}-D array\"\n                    \" for argument {argname!r}, but got a {actual}\"\n                    \" instead.\".format(\n                        func=get_funcname(func),\n                        expected=expected_ndim,\n                        argname=argname,\n                        actual=actual_repr,\n                    )\n                )\n            return argvalue\n        return _check\n    return preprocess(**valmap(_expect_dimension, dimensions))", "output": "Preprocessing decorator that verifies inputs are numpy arrays with a\n    specific dimensionality.\n\n    Examples\n    --------\n    >>> from numpy import array\n    >>> @expect_dimensions(x=1, y=2)\n    ... def foo(x, y):\n    ...    return x[0] + y[0, 0]\n    ...\n    >>> foo(array([1, 1]), array([[1, 1], [2, 2]]))\n    2\n    >>> foo(array([1, 1]), array([1, 1]))  # doctest: +NORMALIZE_WHITESPACE\n    ...                                    # doctest: +ELLIPSIS\n    Traceback (most recent call last):\n       ...\n    ValueError: ...foo() expected a 2-D array for argument 'y',\n    but got a 1-D array instead.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cond_init(m:nn.Module, init_func:LayerFunc):\n    \"\"\n    if (not isinstance(m, bn_types)) and requires_grad(m): init_default(m, init_func)", "output": "Initialize the non-batchnorm layers of `m` with `init_func`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _checkpoint_trial_if_needed(self, trial):\n        \"\"\"\"\"\"\n        if trial.should_checkpoint():\n            # Save trial runtime if possible\n            if hasattr(trial, \"runner\") and trial.runner:\n                self.trial_executor.save(trial, storage=Checkpoint.DISK)\n            self.trial_executor.try_checkpoint_metadata(trial)", "output": "Checkpoints trial based off trial.last_result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, len, buf):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlOutputBufferWrite(self._o, len, buf)\n        return ret", "output": "Write the content of the array in the output I/O buffer\n          This routine handle the I18N transcoding from internal\n          UTF-8 The buffer is lossless, i.e. will store in case of\n           partial or delayed writes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear_lock(self, back=None, remote=None):\n        '''\n        \n        '''\n        back = self.backends(back)\n        cleared = []\n        errors = []\n        for fsb in back:\n            fstr = '{0}.clear_lock'.format(fsb)\n            if fstr in self.servers:\n                good, bad = clear_lock(self.servers[fstr],\n                                       fsb,\n                                       remote=remote)\n                cleared.extend(good)\n                errors.extend(bad)\n        return cleared, errors", "output": "Clear the update lock for the enabled fileserver backends\n\n        back\n            Only clear the update lock for the specified backend(s). The\n            default is to clear the lock for all enabled backends\n\n        remote\n            If specified, then any remotes which contain the passed string will\n            have their lock cleared.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call_color(*args, **kwargs):\n    '''\n    \n    '''\n    res = dict()\n\n    colormap = {\n        'red': Const.COLOR_RED,\n        'green': Const.COLOR_GREEN,\n        'blue': Const.COLOR_BLUE,\n        'orange': Const.COLOR_ORANGE,\n        'pink': Const.COLOR_PINK,\n        'white': Const.COLOR_WHITE,\n        'yellow': Const.COLOR_YELLOW,\n        'daylight': Const.COLOR_DAYLIGHT,\n        'purple': Const.COLOR_PURPLE,\n    }\n\n    devices = _get_lights()\n    color = kwargs.get(\"gamut\")\n    if color:\n        color = color.split(\",\")\n        if len(color) == 2:\n            try:\n                color = {\"xy\": [float(color[0]), float(color[1])]}\n            except Exception as ex:\n                color = None\n        else:\n            color = None\n\n    if not color:\n        color = colormap.get(kwargs.get(\"color\", 'white'), Const.COLOR_WHITE)\n    color.update({\"transitiontime\": max(min(kwargs.get(\"transition\", 0), 200), 0)})\n\n    for dev_id in 'id' not in kwargs and sorted(devices.keys()) or _get_devices(kwargs):\n        res[dev_id] = _set(dev_id, color)\n\n    return res", "output": "Set a color to the lamp.\n\n    Options:\n\n    * **id**: Specifies a device ID. Can be a comma-separated values. All, if omitted.\n    * **color**: Fixed color. Values are: red, green, blue, orange, pink, white,\n                 yellow, daylight, purple. Default white.\n    * **transition**: Transition 0~200.\n\n    Advanced:\n\n    * **gamut**: XY coordinates. Use gamut according to the Philips HUE devices documentation.\n                 More: http://www.developers.meethue.com/documentation/hue-xy-values\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' hue.color\n        salt '*' hue.color id=1\n        salt '*' hue.color id=1,2,3 oolor=red transition=30\n        salt '*' hue.color id=1 gamut=0.3,0.5", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_netmask(cls, arg):\n        \"\"\"\n        \"\"\"\n        if arg not in cls._netmask_cache:\n            if isinstance(arg, _compat_int_types):\n                prefixlen = arg\n            else:\n                try:\n                    # Check for a netmask in prefix length form\n                    prefixlen = cls._prefix_from_prefix_string(arg)\n                except NetmaskValueError:\n                    # Check for a netmask or hostmask in dotted-quad form.\n                    # This may raise NetmaskValueError.\n                    prefixlen = cls._prefix_from_ip_string(arg)\n            netmask = IPv4Address(cls._ip_int_from_prefix(prefixlen))\n            cls._netmask_cache[arg] = netmask, prefixlen\n        return cls._netmask_cache[arg]", "output": "Make a (netmask, prefix_len) tuple from the given argument.\n\n        Argument can be:\n        - an integer (the prefix length)\n        - a string representing the prefix length (e.g. \"24\")\n        - a string representing the prefix netmask (e.g. \"255.255.255.0\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_role_mapping(self, name=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        return self.transport.perform_request(\n            \"GET\", _make_path(\"_security\", \"role_mapping\", name), params=params\n        )", "output": "`<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role-mapping.html>`_\n\n        :arg name: Role-Mapping name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _all_indexes_same(indexes):\n    \"\"\"\n    \n    \"\"\"\n    first = indexes[0]\n    for index in indexes[1:]:\n        if not first.equals(index):\n            return False\n    return True", "output": "Determine if all indexes contain the same elements.\n\n    Parameters\n    ----------\n    indexes : list of Index objects\n\n    Returns\n    -------\n    bool\n        True if all indexes contain the same elements, False otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def installed(name, default=False, user=None):\n    '''\n    \n    '''\n    ret = {'name': name, 'result': None, 'comment': '', 'changes': {}}\n\n    if name.startswith('python-'):\n        name = re.sub(r'^python-', '', name)\n\n    if __opts__['test']:\n        ret['comment'] = 'python {0} is set to be installed'.format(name)\n        return ret\n\n    ret = _check_pyenv(ret, user)\n    if ret['result'] is False:\n        if not __salt__['pyenv.install'](user):\n            ret['comment'] = 'pyenv failed to install'\n            return ret\n        else:\n            return _check_and_install_python(ret, name, default, user=user)\n    else:\n        return _check_and_install_python(ret, name, default, user=user)", "output": "Verify that the specified python is installed with pyenv. pyenv is\n    installed if necessary.\n\n    name\n        The version of python to install\n\n    default : False\n        Whether to make this python the default.\n\n    user: None\n        The user to run pyenv as.\n\n        .. versionadded:: 0.17.0\n\n    .. versionadded:: 0.16.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _id_map(minion_id, dns_name):\n    '''\n    \n    '''\n    bank = 'digicert/minions'\n    cache = salt.cache.Cache(__opts__, syspaths.CACHE_DIR)\n    dns_names = cache.fetch(bank, minion_id)\n    if not isinstance(dns_names, list):\n        dns_names = []\n    if dns_name not in dns_names:\n        dns_names.append(dns_name)\n    cache.store(bank, minion_id, dns_names)", "output": "Maintain a relationship between a minion and a dns name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def emitCurrentToken(self):\n        \"\"\"\n        \"\"\"\n        token = self.currentToken\n        # Add token to the queue to be yielded\n        if (token[\"type\"] in tagTokenTypes):\n            token[\"name\"] = token[\"name\"].translate(asciiUpper2Lower)\n            if token[\"type\"] == tokenTypes[\"EndTag\"]:\n                if token[\"data\"]:\n                    self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                            \"data\": \"attributes-in-end-tag\"})\n                if token[\"selfClosing\"]:\n                    self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                            \"data\": \"self-closing-flag-on-end-tag\"})\n        self.tokenQueue.append(token)\n        self.state = self.dataState", "output": "This method is a generic handler for emitting the tags. It also sets\n        the state to \"data\" because that's what's needed after a token has been\n        emitted.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _write_config(config, newlines=2):\n    '''\n    \n    '''\n    text = config\n    if isinstance(config, dict) and len(list(list(config.keys()))) == 1:\n        key = next(six.iterkeys(config))\n        text = config[key]\n\n    try:\n        with salt.utils.files.fopen(__SYSLOG_NG_CONFIG_FILE, 'a') as fha:\n            fha.write(salt.utils.stringutils.to_str(text))\n\n            for _ in range(0, newlines):\n                fha.write(salt.utils.stringutils.to_str(os.linesep))\n        return True\n    except Exception as err:\n        log.error(six.text_type(err))\n        return False", "output": "Writes the given parameter config into the config file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adjust_learning_rate(optimizer, epoch):\n    \"\"\"\"\"\"\n    lr = args.lr * (0.1 ** (epoch // 30))\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr", "output": "Sets the learning rate to the initial LR decayed by 10 every 30 epochs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_dtype(self, dtype):\n        \"\"\"\n        \n        \"\"\"\n        codes = _recode_for_categories(self.codes, self.categories,\n                                       dtype.categories)\n        return type(self)(codes, dtype=dtype, fastpath=True)", "output": "Internal method for directly updating the CategoricalDtype\n\n        Parameters\n        ----------\n        dtype : CategoricalDtype\n\n        Notes\n        -----\n        We don't do any validation here. It's assumed that the dtype is\n        a (valid) instance of `CategoricalDtype`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def table_data_client(self):\n        \"\"\"\n        \"\"\"\n        if self._table_data_client is None:\n            self._table_data_client = _create_gapic_client(bigtable_v2.BigtableClient)(\n                self\n            )\n        return self._table_data_client", "output": "Getter for the gRPC stub used for the Table Admin API.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_table_data_client]\n            :end-before: [END bigtable_table_data_client]\n\n        :rtype: :class:`.bigtable_v2.BigtableClient`\n        :returns: A BigtableClient object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _shallow_copy_with_infer(self, values, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        attributes = self._get_attributes_dict()\n        attributes.update(kwargs)\n        attributes['copy'] = False\n        if not len(values) and 'dtype' not in kwargs:\n            attributes['dtype'] = self.dtype\n        if self._infer_as_myclass:\n            try:\n                return self._constructor(values, **attributes)\n            except (TypeError, ValueError):\n                pass\n        return Index(values, **attributes)", "output": "Create a new Index inferring the class with passed value, don't copy\n        the data, use the same object attributes with passed in attributes\n        taking precedence.\n\n        *this is an internal non-public method*\n\n        Parameters\n        ----------\n        values : the values to create the new Index, optional\n        kwargs : updates the default attributes for this Index", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def posttrans_hook(conduit):\n    \"\"\"\n    \n    \"\"\"\n    # Integrate Yum with Salt\n    if 'SALT_RUNNING' not in os.environ:\n        with open(CK_PATH, 'w') as ck_fh:\n            ck_fh.write('{chksum} {mtime}\\n'.format(chksum=_get_checksum(), mtime=_get_mtime()))", "output": "Hook after the package installation transaction.\n\n    :param conduit:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _closest_ref_length(references, trans_length):\n    \"\"\"\n    \"\"\"\n    ref_lengths = (len(reference) for reference in references)\n    closest_ref_len = min(ref_lengths,\n                          key=lambda ref_length: (abs(ref_length - trans_length), ref_length))\n\n    return closest_ref_len", "output": "Find the reference that has the closest length to the translation.\n\n    Parameters\n    ----------\n    references: list(list(str))\n        A list of references.\n    trans_length: int\n        Length of the translation.\n\n    Returns\n    -------\n    closest_ref_len: int\n        Length of the reference that is closest to the translation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def store(bank, key, data, cachedir):\n    '''\n    \n    '''\n    base = os.path.join(cachedir, os.path.normpath(bank))\n    try:\n        os.makedirs(base)\n    except OSError as exc:\n        if exc.errno != errno.EEXIST:\n            raise SaltCacheError(\n                'The cache directory, {0}, could not be created: {1}'.format(\n                    base, exc\n                )\n            )\n\n    outfile = os.path.join(base, '{0}.p'.format(key))\n    tmpfh, tmpfname = tempfile.mkstemp(dir=base)\n    os.close(tmpfh)\n    try:\n        with salt.utils.files.fopen(tmpfname, 'w+b') as fh_:\n            fh_.write(__context__['serial'].dumps(data))\n        # On Windows, os.rename will fail if the destination file exists.\n        salt.utils.atomicfile.atomic_rename(tmpfname, outfile)\n    except IOError as exc:\n        raise SaltCacheError(\n            'There was an error writing the cache file, {0}: {1}'.format(\n                base, exc\n            )\n        )", "output": "Store information in a file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def host_status(hostname=None, **kwargs):\n    '''\n    \n    '''\n\n    if not hostname:\n        raise CommandExecutionError('Missing hostname parameter')\n\n    target = 'host'\n    numeric = kwargs.get('numeric')\n    data = _status_query(target, hostname, enumerate=numeric)\n\n    ret = {'result': data['result']}\n    if ret['result']:\n        ret['status'] = data.get('json_data', {}).get('data', {}).get(target, {}).get('status',\n                                                                                      not numeric and 'Unknown' or 2)\n    else:\n        ret['error'] = data['error']\n    return ret", "output": "Check status of a particular host By default\n    statuses are returned in a numeric format.\n\n    Parameters:\n\n    hostname\n        The hostname to check the status of the service in Nagios.\n\n    numeric\n        Turn to false in order to return status in text format\n        ('OK' instead of 0, 'Warning' instead of 1 etc)\n\n    :return: status:     'OK', 'Warning', 'Critical' or 'Unknown'\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nagios_rpc.host_status hostname=webserver.domain.com\n        salt '*' nagios_rpc.host_status hostname=webserver.domain.com numeric=False", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _try_get_state_scope(name, mark_name_scope_used=True):\n  \"\"\"\n  \"\"\"\n  tmp_scope_name = tf_v1.get_variable_scope().name\n  if tmp_scope_name:\n    tmp_scope_name += \"/\"\n  with tf.name_scope(tmp_scope_name):\n    # Pick an unused variable scope.\n    with tf_v1.variable_scope(\n        None, default_name=name, auxiliary_name_scope=False) as vs:\n      abs_state_scope = vs.name + \"/\"\n    # Verify that the name scope is available and mark it used if requested.\n    graph = tf_v1.get_default_graph()\n    unique_name_scope = graph.unique_name(name, mark_name_scope_used) + \"/\"\n    if unique_name_scope != abs_state_scope:\n      raise RuntimeError(\n          \"variable_scope %s was unused but the corresponding \"\n          \"name_scope was already taken.\" % abs_state_scope)\n  return abs_state_scope", "output": "Returns a fresh variable/name scope for a module's state.\n\n  In order to import a module into a given scope without major complications\n  we require the scope to be empty. This function deals with deciding an unused\n  scope where to define the module state. This is non trivial in cases where\n  name_scope and variable_scopes are out of sync, e.g. tpus or re-entering\n  scopes.\n\n  Args:\n    name: A string with the name of the module as supplied by the client.\n    mark_name_scope_used: a boolean, indicating whether to mark the name\n        scope of the returned value as used.\n\n  Raises:\n    RuntimeError: if the name scope of the freshly created variable scope is\n        already used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_attrgetter(self, obj):\n        \"\"\"\"\"\"\n        class Dummy(object):\n            def __init__(self, attrs, index=None):\n                self.attrs = attrs\n                self.index = index\n            def __getattribute__(self, item):\n                attrs = object.__getattribute__(self, \"attrs\")\n                index = object.__getattribute__(self, \"index\")\n                if index is None:\n                    index = len(attrs)\n                    attrs.append(item)\n                else:\n                    attrs[index] = \".\".join([attrs[index], item])\n                return type(self)(attrs, index)\n        attrs = []\n        obj(Dummy(attrs))\n        return self.save_reduce(operator.attrgetter, tuple(attrs))", "output": "attrgetter serializer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exists(self, path):\n        \"\"\"\n        \n        \"\"\"\n\n        cmd = load_hadoop_cmd() + ['fs', '-stat', path]\n        logger.debug('Running file existence check: %s', subprocess.list2cmdline(cmd))\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True, universal_newlines=True)\n        stdout, stderr = p.communicate()\n        if p.returncode == 0:\n            return True\n        else:\n            not_found_pattern = \"^.*No such file or directory$\"\n            not_found_re = re.compile(not_found_pattern)\n            for line in stderr.split('\\n'):\n                if not_found_re.match(line):\n                    return False\n            raise hdfs_error.HDFSCliError(cmd, p.returncode, stdout, stderr)", "output": "Use ``hadoop fs -stat`` to check file existence.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def send_message(self, message):\n        ''' \n\n        '''\n        try:\n            if _message_test_port is not None:\n                _message_test_port.sent.append(message)\n            yield message.send(self)\n        except (WebSocketClosedError, StreamClosedError): # Tornado 4.x may raise StreamClosedError\n            # on_close() is / will be called anyway\n            log.warning(\"Failed sending message as connection was closed\")\n        raise gen.Return(None)", "output": "Send a Bokeh Server protocol message to the connected client.\n\n        Args:\n            message (Message) : a message to send", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prf_divide(numerator, denominator):\n    \"\"\"\n    \"\"\"\n    result = numerator / denominator\n    mask = denominator == 0.0\n    if not mask.any():\n        return result\n\n    # remove nan\n    result[mask] = 0.0\n    return result", "output": "Performs division and handles divide-by-zero.\n\n    On zero-division, sets the corresponding result elements to zero.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def double_tab_complete(self):\r\n        \"\"\"\"\"\"\r\n        opts = self._complete_options()\r\n        if len(opts) > 1:\r\n            self.completer().complete()", "output": "If several options available a double tab displays options.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_rnn_layer(mode, num_layers, input_size, hidden_size, dropout, weight_dropout):\n    \"\"\"\"\"\"\n    if mode == 'rnn_relu':\n        rnn_block = functools.partial(rnn.RNN, activation='relu')\n    elif mode == 'rnn_tanh':\n        rnn_block = functools.partial(rnn.RNN, activation='tanh')\n    elif mode == 'lstm':\n        rnn_block = rnn.LSTM\n    elif mode == 'gru':\n        rnn_block = rnn.GRU\n\n    block = rnn_block(hidden_size, num_layers, dropout=dropout,\n                      input_size=input_size)\n\n    if weight_dropout:\n        apply_weight_drop(block, '.*h2h_weight', rate=weight_dropout)\n\n    return block", "output": "create rnn layer given specs", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_certificate(result):\n    \"\"\"\n    \n    \"\"\"\n    cert_body = \"\"\"-----BEGIN CERTIFICATE-----\\n{0}\\n-----END CERTIFICATE-----\\n\"\"\".format(\n        \"\\n\".join(textwrap.wrap(base64.b64encode(result).decode('utf8'), 64)))\n    signed_crt = open(\"{}/signed.crt\".format(gettempdir()), \"w\")\n    signed_crt.write(cert_body)\n    signed_crt.close()\n\n    return True", "output": "Encode cert bytes to PEM encoded cert file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlCreatePushParser(SAX, chunk, size, URI):\n    \"\"\" \"\"\"\n    ret = libxml2mod.htmlCreatePushParser(SAX, chunk, size, URI)\n    if ret is None:raise parserError('htmlCreatePushParser() failed')\n    return parserCtxt(_obj=ret)", "output": "Create a progressive HTML parser context to build either an\n      event flow if the SAX object is not None, or a DOM tree\n       otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_experiment_from_runs(self):\n    \"\"\"\"\"\"\n    hparam_infos = self._compute_hparam_infos()\n    if not hparam_infos:\n      return None\n    metric_infos = self._compute_metric_infos()\n    return api_pb2.Experiment(hparam_infos=hparam_infos,\n                              metric_infos=metric_infos)", "output": "Computes a minimal Experiment protocol buffer by scanning the runs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "async def estimate_pruned_members(self, *, days):\n        \"\"\"\n        \"\"\"\n\n        if not isinstance(days, int):\n            raise InvalidArgument('Expected int for ``days``, received {0.__class__.__name__} instead.'.format(days))\n\n        data = await self._state.http.estimate_pruned_members(self.id, days)\n        return data['pruned']", "output": "|coro|\n\n        Similar to :meth:`prune_members` except instead of actually\n        pruning members, it returns how many members it would prune\n        from the guild had it been called.\n\n        Parameters\n        -----------\n        days: :class:`int`\n            The number of days before counting as inactive.\n\n        Raises\n        -------\n        Forbidden\n            You do not have permissions to prune members.\n        HTTPException\n            An error occurred while fetching the prune members estimate.\n        InvalidArgument\n            An integer was not passed for ``days``.\n\n        Returns\n        ---------\n        :class:`int`\n            The number of members estimated to be pruned.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def version_msg():\n    \"\"\"\"\"\"\n    python_version = sys.version[:3]\n    location = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n    message = u'Cookiecutter %(version)s from {} (Python {})'\n    return message.format(location, python_version)", "output": "Return the Cookiecutter version, location and Python powering it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_gid(group=None):\n    '''\n    \n    '''\n    if not HAS_GRP:\n        return None\n    if group is None:\n        try:\n            return os.getegid()\n        except AttributeError:\n            return None\n    else:\n        try:\n            return grp.getgrnam(group).gr_gid\n        except KeyError:\n            return None", "output": "Get the gid for a given group name. If no group given, the current egid\n    will be returned. If the group does not exist, None will be returned. On\n    systems which do not support grp or os.getegid it will return None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_kexgss_hostkey(self, m):\n        \"\"\"\n        \n        \"\"\"\n        # client mode\n        host_key = m.get_string()\n        self.transport.host_key = host_key\n        sig = m.get_string()\n        self.transport._verify_key(host_key, sig)\n        self.transport._expect_packet(MSG_KEXGSS_CONTINUE, MSG_KEXGSS_COMPLETE)", "output": "Parse the SSH2_MSG_KEXGSS_HOSTKEY message (client mode).\n\n        :param `.Message` m: The content of the SSH2_MSG_KEXGSS_HOSTKEY message", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def from_csv(cls, path:PathOrStr, csv_name:str, cols:IntsOrStrs=0, delimiter:str=None, header:str='infer',\n                 processor:PreProcessors=None, **kwargs)->'ItemList':\n        \"\"\"\"\"\"\n        df = pd.read_csv(Path(path)/csv_name, delimiter=delimiter, header=header)\n        return cls.from_df(df, path=path, cols=cols, processor=processor, **kwargs)", "output": "Create an `ItemList` in `path` from the inputs in the `cols` of `path/csv_name`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set(self, reference, document_data, merge=False):\n        \"\"\"\n        \"\"\"\n        if merge is not False:\n            write_pbs = _helpers.pbs_for_set_with_merge(\n                reference._document_path, document_data, merge\n            )\n        else:\n            write_pbs = _helpers.pbs_for_set_no_merge(\n                reference._document_path, document_data\n            )\n\n        self._add_write_pbs(write_pbs)", "output": "Add a \"change\" to replace a document.\n\n        See\n        :meth:`~.firestore_v1beta1.document.DocumentReference.set` for\n        more information on how ``option`` determines how the change is\n        applied.\n\n        Args:\n            reference (~.firestore_v1beta1.document.DocumentReference):\n                A document reference that will have values set in this batch.\n            document_data (dict):\n                Property names and values to use for replacing a document.\n            merge (Optional[bool] or Optional[List<apispec>]):\n                If True, apply merging instead of overwriting the state\n                of the document.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wider_pre_dense(layer, n_add, weighted=True):\n    '''\n    '''\n    if not weighted:\n        return StubDense(layer.input_units, layer.units + n_add)\n\n    n_units2 = layer.units\n\n    teacher_w, teacher_b = layer.get_weights()\n    rand = np.random.randint(n_units2, size=n_add)\n    student_w = teacher_w.copy()\n    student_b = teacher_b.copy()\n\n    # target layer update (i)\n    for i in range(n_add):\n        teacher_index = rand[i]\n        new_weight = teacher_w[teacher_index, :]\n        new_weight = new_weight[np.newaxis, :]\n        student_w = np.concatenate((student_w, add_noise(new_weight, student_w)), axis=0)\n        student_b = np.append(student_b, add_noise(teacher_b[teacher_index], student_b))\n\n    new_pre_layer = StubDense(layer.input_units, n_units2 + n_add)\n    new_pre_layer.set_weights((student_w, student_b))\n\n    return new_pre_layer", "output": "wider previous dense layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_rowcount(self, query_results):\n        \"\"\"\n        \"\"\"\n        total_rows = 0\n        num_dml_affected_rows = query_results.num_dml_affected_rows\n\n        if query_results.total_rows is not None and query_results.total_rows > 0:\n            total_rows = query_results.total_rows\n        if num_dml_affected_rows is not None and num_dml_affected_rows > 0:\n            total_rows = num_dml_affected_rows\n        self.rowcount = total_rows", "output": "Set the rowcount from query results.\n\n        Normally, this sets rowcount to the number of rows returned by the\n        query, but if it was a DML statement, it sets rowcount to the number\n        of modified rows.\n\n        :type query_results:\n            :class:`~google.cloud.bigquery.query._QueryResults`\n        :param query_results: results of a query", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bind(_self, **kwargs):\n        \"\"\"\n        \"\"\"\n        return Logger(\n            {**_self._extra, **kwargs},\n            _self._exception,\n            _self._record,\n            _self._lazy,\n            _self._ansi,\n            _self._raw,\n            _self._depth,\n        )", "output": "Bind attributes to the ``extra`` dict of each logged message record.\n\n        This is used to add custom context to each logging call.\n\n        Parameters\n        ----------\n        **kwargs\n            Mapping between keys and values that will be added to the ``extra`` dict.\n\n        Returns\n        -------\n        :class:`~Logger`\n            A logger wrapping the core logger, but which sends record with the customized ``extra``\n            dict.\n\n        Examples\n        --------\n        >>> logger.add(sys.stderr, format=\"{extra[ip]} - {message}\")\n        1\n        >>> class Server:\n        ...     def __init__(self, ip):\n        ...         self.ip = ip\n        ...         self.logger = logger.bind(ip=ip)\n        ...     def call(self, message):\n        ...         self.logger.info(message)\n        ...\n        >>> instance_1 = Server(\"192.168.0.200\")\n        >>> instance_2 = Server(\"127.0.0.1\")\n        >>> instance_1.call(\"First instance\")\n        192.168.0.200 - First instance\n        >>> instance_2.call(\"Second instance\")\n        127.0.0.1 - Second instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_event(event_file_name):\n    \"\"\"\n    \n    \"\"\"\n\n    if event_file_name == STDIN_FILE_NAME:\n        # If event is empty, listen to stdin for event data until EOF\n        LOG.info(\"Reading invoke payload from stdin (you can also pass it from file with --event)\")\n\n    # click.open_file knows to open stdin when filename is '-'. This is safer than manually opening streams, and\n    # accidentally closing a standard stream\n    with click.open_file(event_file_name, 'r') as fp:\n        return fp.read()", "output": "Read the event JSON data from the given file. If no file is provided, read the event from stdin.\n\n    :param string event_file_name: Path to event file, or '-' for stdin\n    :return string: Contents of the event file or stdin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_availabilities(date):\n    \"\"\"\n    \n    \"\"\"\n    day_of_week = dateutil.parser.parse(date).weekday()\n    availabilities = []\n    available_probability = 0.3\n    if day_of_week == 0:\n        start_hour = 10\n        while start_hour <= 16:\n            if random.random() < available_probability:\n                # Add an availability window for the given hour, with duration determined by another random number.\n                appointment_type = get_random_int(1, 4)\n                if appointment_type == 1:\n                    availabilities.append('{}:00'.format(start_hour))\n                elif appointment_type == 2:\n                    availabilities.append('{}:30'.format(start_hour))\n                else:\n                    availabilities.append('{}:00'.format(start_hour))\n                    availabilities.append('{}:30'.format(start_hour))\n            start_hour += 1\n\n    if day_of_week == 2 or day_of_week == 4:\n        availabilities.append('10:00')\n        availabilities.append('16:00')\n        availabilities.append('16:30')\n\n    return availabilities", "output": "Helper function which in a full implementation would  feed into a backend API to provide query schedule availability.\n    The output of this function is an array of 30 minute periods of availability, expressed in ISO-8601 time format.\n\n    In order to enable quick demonstration of all possible conversation paths supported in this example, the function\n    returns a mixture of fixed and randomized results.\n\n    On Mondays, availability is randomized; otherwise there is no availability on Tuesday / Thursday and availability at\n    10:00 - 10:30 and 4:00 - 5:00 on Wednesday / Friday.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ending_long_process(self, message=\"\"):\n        \"\"\"\n        \n        \"\"\"\n        QApplication.restoreOverrideCursor()\n        self.show_message(message, timeout=2000)\n        QApplication.processEvents()", "output": "Clear main window's status bar and restore mouse cursor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def spinner(\n    spinner_name=None,\n    start_text=None,\n    handler_map=None,\n    nospin=False,\n    write_to_stdout=True,\n):\n    \"\"\"\n    \"\"\"\n\n    from .spin import create_spinner\n\n    has_yaspin = None\n    try:\n        import yaspin\n    except ImportError:\n        has_yaspin = False\n        if not nospin:\n            raise RuntimeError(\n                \"Failed to import spinner! Reinstall vistir with command:\"\n                \" pip install --upgrade vistir[spinner]\"\n            )\n        else:\n            spinner_name = \"\"\n    else:\n        has_yaspin = True\n        spinner_name = \"\"\n    use_yaspin = (has_yaspin is False) or (nospin is True)\n    if has_yaspin is None or has_yaspin is True and not nospin:\n        use_yaspin = True\n    if start_text is None and use_yaspin is True:\n        start_text = \"Running...\"\n    with create_spinner(\n        spinner_name=spinner_name,\n        text=start_text,\n        handler_map=handler_map,\n        nospin=nospin,\n        use_yaspin=use_yaspin,\n        write_to_stdout=write_to_stdout,\n    ) as _spinner:\n        yield _spinner", "output": "Get a spinner object or a dummy spinner to wrap a context.\n\n    :param str spinner_name: A spinner type e.g. \"dots\" or \"bouncingBar\" (default: {\"bouncingBar\"})\n    :param str start_text: Text to start off the spinner with (default: {None})\n    :param dict handler_map: Handler map for signals to be handled gracefully (default: {None})\n    :param bool nospin: If true, use the dummy spinner (default: {False})\n    :param bool write_to_stdout: Writes to stdout if true, otherwise writes to stderr (default: True)\n    :return: A spinner object which can be manipulated while alive\n    :rtype: :class:`~vistir.spin.VistirSpinner`\n\n    Raises:\n        RuntimeError -- Raised if the spinner extra is not installed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_levels(self, arcs):\n        \"\"\"\n        \"\"\"\n        levels = set(map(lambda arc: arc[\"end\"] - arc[\"start\"], arcs))\n        return sorted(list(levels))", "output": "Calculate available arc height \"levels\".\n        Used to calculate arrow heights dynamically and without wasting space.\n\n        args (list): Individual arcs and their start, end, direction and label.\n        RETURNS (list): Arc levels sorted from lowest to highest.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def network_interface_get(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n    try:\n        nic = netconn.network_interfaces.get(\n            network_interface_name=name,\n            resource_group_name=resource_group\n        )\n        result = nic.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get details about a specific network interface.\n\n    :param name: The name of the network interface to query.\n\n    :param resource_group: The resource group name assigned to the\n        network interface.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.network_interface_get test-iface0 testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process_raw_trace(raw_trace):\n  \"\"\"\"\"\"\n  trace = trace_events_pb2.Trace()\n  trace.ParseFromString(raw_trace)\n  return ''.join(trace_events_json.TraceEventsJsonStream(trace))", "output": "Processes raw trace data and returns the UI data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_splits(self, assets, dt):\n        \"\"\"\n        \n        \"\"\"\n        if self._adjustment_reader is None or not assets:\n            return []\n\n        # convert dt to # of seconds since epoch, because that's what we use\n        # in the adjustments db\n        seconds = int(dt.value / 1e9)\n\n        splits = self._adjustment_reader.conn.execute(\n            \"SELECT sid, ratio FROM SPLITS WHERE effective_date = ?\",\n            (seconds,)).fetchall()\n\n        splits = [split for split in splits if split[0] in assets]\n        splits = [(self.asset_finder.retrieve_asset(split[0]), split[1])\n                  for split in splits]\n\n        return splits", "output": "Returns any splits for the given sids and the given dt.\n\n        Parameters\n        ----------\n        assets : container\n            Assets for which we want splits.\n        dt : pd.Timestamp\n            The date for which we are checking for splits. Note: this is\n            expected to be midnight UTC.\n\n        Returns\n        -------\n        splits : list[(asset, float)]\n            List of splits, where each split is a (asset, ratio) tuple.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_tags(self):\n    \"\"\"\"\"\"\n    return sorted([frozenset(meta_graph.meta_info_def.tags)\n                   for meta_graph in self.meta_graphs])", "output": "Returns a list of set of tags.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ToMicroseconds(self):\n    \"\"\"\"\"\"\n    micros = _RoundTowardZero(self.nanos, _NANOS_PER_MICROSECOND)\n    return self.seconds * _MICROS_PER_SECOND + micros", "output": "Converts a Duration to microseconds.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clip_lower(self, threshold):\n        \"\"\"\n        \n        \"\"\"\n        with cython_context():\n            return SArray(_proxy=self.__proxy__.clip(threshold, float('nan')))", "output": "Create new SArray with all values clipped to the given lower bound. This\n        function can operate on numeric arrays, as well as vector arrays, in\n        which case each individual element in each vector is clipped. Throws an\n        exception if the SArray is empty or the types are non-numeric.\n\n        Parameters\n        ----------\n        threshold : float\n            The lower bound used to clip values.\n\n        Returns\n        -------\n        out : SArray\n\n        See Also\n        --------\n        clip, clip_upper\n\n        Examples\n        --------\n        >>> sa = turicreate.SArray([1,2,3])\n        >>> sa.clip_lower(2)\n        dtype: int\n        Rows: 3\n        [2, 2, 3]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_max_position_size(self,\n                              asset=None,\n                              max_shares=None,\n                              max_notional=None,\n                              on_error='fail'):\n        \"\"\"\n        \"\"\"\n        control = MaxPositionSize(asset=asset,\n                                  max_shares=max_shares,\n                                  max_notional=max_notional,\n                                  on_error=on_error)\n        self.register_trading_control(control)", "output": "Set a limit on the number of shares and/or dollar value held for the\n        given sid. Limits are treated as absolute values and are enforced at\n        the time that the algo attempts to place an order for sid. This means\n        that it's possible to end up with more than the max number of shares\n        due to splits/dividends, and more than the max notional due to price\n        improvement.\n\n        If an algorithm attempts to place an order that would result in\n        increasing the absolute value of shares/dollar value exceeding one of\n        these limits, raise a TradingControlException.\n\n        Parameters\n        ----------\n        asset : Asset, optional\n            If provided, this sets the guard only on positions in the given\n            asset.\n        max_shares : int, optional\n            The maximum number of shares to hold for an asset.\n        max_notional : float, optional\n            The maximum value to hold for an asset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load(fh, encoding=None, is_verbose=False):\n    \"\"\"\n    \"\"\"\n\n    try:\n        fh.seek(0)\n        if encoding is not None:\n            up = Unpickler(fh, encoding=encoding)\n        else:\n            up = Unpickler(fh)\n        up.is_verbose = is_verbose\n\n        return up.load()\n    except (ValueError, TypeError):\n        raise", "output": "load a pickle, with a provided encoding\n\n    if compat is True:\n       fake the old class hierarchy\n       if it works, then return the new type objects\n\n    Parameters\n    ----------\n    fh : a filelike object\n    encoding : an optional encoding\n    is_verbose : show exception output", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_all_packages(mirror=DEFAULT_MIRROR,\n                      cyg_arch='x86_64'):\n    '''\n    \n    '''\n    if 'cyg.all_packages' not in __context__:\n        __context__['cyg.all_packages'] = {}\n    if mirror not in __context__['cyg.all_packages']:\n        __context__['cyg.all_packages'][mirror] = []\n    if not __context__['cyg.all_packages'][mirror]:\n        pkg_source = '/'.join([mirror, cyg_arch, 'setup.bz2'])\n\n        file_data = _urlopen(pkg_source).read()\n        file_lines = bz2.decompress(file_data).decode('utf_8',\n                                                      errors='replace'\n                                                     ).splitlines()\n\n        packages = [re.search('^@ ([^ ]+)', line).group(1) for\n                    line in file_lines if re.match('^@ [^ ]+', line)]\n\n        __context__['cyg.all_packages'][mirror] = packages\n\n    return __context__['cyg.all_packages'][mirror]", "output": "Return the list of packages based on the mirror provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write(self, bytestring):\n        ''''''\n        with self._lock:\n            if self._closed:\n                raise IOError('Writer is closed')\n            self._byte_queue.put(bytestring)", "output": "Enqueue the given bytes to be written asychronously", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rest_put(url, data, timeout):\n    ''''''\n    try:\n        response = requests.put(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                data=data, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http put to url {1}'.format(str(e), url))\n        return None", "output": "Call rest put method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def download_google_images(path:PathOrStr, search_term:str, size:str='>400*300', n_images:int=10, format:str='jpg',\n                            max_workers:int=defaults.cpus, timeout:int=4) -> FilePathList:\n    \"\"\"\n    \n    \"\"\"\n    label_path = Path(path)/search_term\n    search_url = _search_url(search_term, size=size, format=format)\n    if n_images <= 100: img_tuples = _fetch_img_tuples(search_url, format=format, n_images=n_images)\n    else:               img_tuples = _fetch_img_tuples_webdriver(search_url, format=format, n_images=n_images)\n    downloaded_images = _download_images(label_path, img_tuples, max_workers=max_workers, timeout=timeout)\n    if len(downloaded_images) == 0: raise RuntimeError(f\"Couldn't download any images.\")\n    verify_images(label_path, max_workers=max_workers)\n    return get_image_files(label_path)", "output": "Search for `n_images` images on Google, matching `search_term` and `size` requirements,\n    download them into `path`/`search_term` and verify them, using `max_workers` threads.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def under_variable_scope():\n    \"\"\"\n    \n\n    \"\"\"\n\n    def _impl(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            name = func.__name__\n            with tf.variable_scope(name):\n                return func(*args, **kwargs)\n        return wrapper\n    return _impl", "output": "Returns:\n        A decorator which makes the function happen under a variable scope,\n        which is named by the function itself.\n\n    Example:\n\n    .. code-block:: python\n\n        @under_variable_scope()\n        def mid_level(x):\n            with argscope(Conv2D, kernel_shape=3, nl=BNReLU):\n                x = Conv2D('conv1', x, 512, stride=1)\n                x = Conv2D('conv2', x, 256, stride=1)\n            return x", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def raw_mentions(self):\n        \"\"\"\n        \"\"\"\n        return [int(x) for x in re.findall(r'<@!?([0-9]+)>', self.content)]", "output": "A property that returns an array of user IDs matched with\n        the syntax of <@user_id> in the message content.\n\n        This allows you to receive the user IDs of mentioned users\n        even in a private message context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def RetrievePluginAsset(self, run, plugin_name, asset_name):\n    \"\"\"\n    \"\"\"\n    accumulator = self.GetAccumulator(run)\n    return accumulator.RetrievePluginAsset(plugin_name, asset_name)", "output": "Return the contents for a specific plugin asset from a run.\n\n    Args:\n      run: The string name of the run.\n      plugin_name: The string name of a plugin.\n      asset_name: The string name of an asset.\n\n    Returns:\n      The string contents of the plugin asset.\n\n    Raises:\n      KeyError: If the asset is not available.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attention_lm_moe_base_memeff():\n  \"\"\"\"\"\"\n  hparams = attention_lm_moe_base_long_seq()\n  hparams.use_sepconv = False\n\n  hparams.diet_experts = True\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  hparams.layer_prepostprocess_dropout = 0.0\n  hparams.memory_efficient_ffn = True\n  hparams.attention_type = AttentionType.MEMORY_EFFICIENT\n  hparams.num_heads = 8\n  hparams.factored_logits = True\n  return hparams", "output": "Base model with attention expert.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def render(dson_input, saltenv='base', sls='', **kwargs):\n    '''\n    \n    '''\n    if not isinstance(dson_input, six.string_types):\n        dson_input = dson_input.read()\n\n    log.debug('DSON input = %s', dson_input)\n\n    if dson_input.startswith('#!'):\n        dson_input = dson_input[(dson_input.find('\\n') + 1):]\n    if not dson_input.strip():\n        return {}\n    return dson.loads(dson_input)", "output": "Accepts DSON data as a string or as a file object and runs it through the\n    JSON parser.\n\n    :rtype: A Python data structure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_home_dir():\n    \"\"\"\"\"\"\n    _home_dir = os.environ.get('MXNET_HOME', os.path.join('~', '.mxnet'))\n    # expand ~ to actual path\n    _home_dir = os.path.expanduser(_home_dir)\n    return _home_dir", "output": "Get home directory for storing datasets/models/pre-trained word embeddings", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _cancel_grpc(operations_stub, operation_name):\n    \"\"\"\n    \"\"\"\n    request_pb = operations_pb2.CancelOperationRequest(name=operation_name)\n    operations_stub.CancelOperation(request_pb)", "output": "Cancel an operation using a gRPC client.\n\n    Args:\n        operations_stub (google.longrunning.operations_pb2.OperationsStub):\n            The gRPC operations stub.\n        operation_name (str): The name of the operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def serialize(self):\n        '''\n        \n        '''\n        serialized = {'type': self.__type__}\n        for argname in self._attributes:\n            if argname == 'required':\n                # This is handled elsewhere\n                continue\n            argvalue = self._get_argname_value(argname)\n            if argvalue is not None:\n                if argvalue is Null:\n                    argvalue = None\n                # None values are not meant to be included in the\n                # serialization, since this is not None...\n                if self.__serialize_attr_aliases__ and argname in self.__serialize_attr_aliases__:\n                    argname = self.__serialize_attr_aliases__[argname]\n                serialized[argname] = argvalue\n        return serialized", "output": "Return a serializable form of the config instance", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_setitem_lengths(indexer, value, values):\n    \"\"\"\n    \n    \"\"\"\n    # boolean with truth values == len of the value is ok too\n    if isinstance(indexer, (np.ndarray, list)):\n        if is_list_like(value) and len(indexer) != len(value):\n            if not (isinstance(indexer, np.ndarray) and\n                    indexer.dtype == np.bool_ and\n                    len(indexer[indexer]) == len(value)):\n                raise ValueError(\"cannot set using a list-like indexer \"\n                                 \"with a different length than the value\")\n    # slice\n    elif isinstance(indexer, slice):\n\n        if is_list_like(value) and len(values):\n            if len(value) != length_of_indexer(indexer, values):\n                raise ValueError(\"cannot set using a slice indexer with a \"\n                                 \"different length than the value\")", "output": "Validate that value and indexer are the same length.\n\n    An special-case is allowed for when the indexer is a boolean array\n    and the number of true values equals the length of ``value``. In\n    this case, no exception is raised.\n\n    Parameters\n    ----------\n    indexer : sequence\n        The key for the setitem\n    value : array-like\n        The value for the setitem\n    values : array-like\n        The values being set into\n\n    Returns\n    -------\n    None\n\n    Raises\n    ------\n    ValueError\n        When the indexer is an ndarray or list and the lengths don't\n        match.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def select_current_cell(self):\r\n        \"\"\"\"\"\"\r\n        cursor = self.textCursor()\r\n        cursor.movePosition(QTextCursor.StartOfBlock)\r\n        cur_pos = prev_pos = cursor.position()\r\n\r\n        # Moving to the next line that is not a separator, if we are\r\n        # exactly at one of them\r\n        while self.is_cell_separator(cursor):\r\n            cursor.movePosition(QTextCursor.NextBlock)\r\n            prev_pos = cur_pos\r\n            cur_pos = cursor.position()\r\n            if cur_pos == prev_pos:\r\n                return cursor, False\r\n        prev_pos = cur_pos\r\n        # If not, move backwards to find the previous separator\r\n        while not self.is_cell_separator(cursor):\r\n            cursor.movePosition(QTextCursor.PreviousBlock)\r\n            prev_pos = cur_pos\r\n            cur_pos = cursor.position()\r\n            if cur_pos == prev_pos:\r\n                if self.is_cell_separator(cursor):\r\n                    return cursor, False\r\n                else:\r\n                    break\r\n        cursor.setPosition(prev_pos)\r\n        cell_at_file_start = cursor.atStart()\r\n        # Once we find it (or reach the beginning of the file)\r\n        # move to the next separator (or the end of the file)\r\n        # so we can grab the cell contents\r\n        while not self.is_cell_separator(cursor):\r\n            cursor.movePosition(QTextCursor.NextBlock,\r\n                                QTextCursor.KeepAnchor)\r\n            cur_pos = cursor.position()\r\n            if cur_pos == prev_pos:\r\n                cursor.movePosition(QTextCursor.EndOfBlock,\r\n                                    QTextCursor.KeepAnchor)\r\n                break\r\n            prev_pos = cur_pos\r\n        cell_at_file_end = cursor.atEnd()\r\n        return cursor, cell_at_file_start and cell_at_file_end", "output": "Select cell under cursor\r\n        cell = group of lines separated by CELL_SEPARATORS\r\n        returns the textCursor and a boolean indicating if the\r\n        entire file is selected", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_object(filename, obj):\n    \"\"\"\"\"\"\n    logging.info('saving {}...'.format(filename))\n    try:\n        with gzip.GzipFile(filename, 'wb') as f:\n            f.write(pickle.dumps(obj, 1))\n    except Exception as e:\n        logging.error('save failure: {}'.format(e))\n        raise", "output": "Compresses and pickles given object to the given filename.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _config_getter(get_opt,\n                   key,\n                   value_regex=None,\n                   cwd=None,\n                   user=None,\n                   password=None,\n                   ignore_retcode=False,\n                   output_encoding=None,\n                   **kwargs):\n    '''\n    \n    '''\n    kwargs = salt.utils.args.clean_kwargs(**kwargs)\n    global_ = kwargs.pop('global', False)\n    if kwargs:\n        salt.utils.args.invalid_kwargs(kwargs)\n\n    if cwd is None:\n        if not global_:\n            raise SaltInvocationError(\n                '\\'cwd\\' argument required unless global=True'\n            )\n    else:\n        cwd = _expand_path(cwd, user)\n\n    if get_opt == '--get-regexp':\n        if value_regex is not None \\\n                and not isinstance(value_regex, six.string_types):\n            value_regex = six.text_type(value_regex)\n    else:\n        # Ignore value_regex\n        value_regex = None\n\n    command = ['git', 'config']\n    command.extend(_which_git_config(global_, cwd, user, password,\n                                     output_encoding=output_encoding))\n    command.append(get_opt)\n    command.append(key)\n    if value_regex is not None:\n        command.append(value_regex)\n    return _git_run(command,\n                    cwd=cwd,\n                    user=user,\n                    password=password,\n                    ignore_retcode=ignore_retcode,\n                    failhard=False,\n                    output_encoding=output_encoding)", "output": "Common code for config.get_* functions, builds and runs the git CLI command\n    and returns the result dict for the calling function to parse.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_job(self, job_id, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if job_id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'job_id'.\")\n        return self.transport.perform_request(\n            \"DELETE\", _make_path(\"_ml\", \"anomaly_detectors\", job_id), params=params\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-delete-job.html>`_\n\n        :arg job_id: The ID of the job to delete\n        :arg force: True if the job should be forcefully deleted, default False\n        :arg wait_for_completion: Should this request wait until the operation\n            has completed before returning, default True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _draw_rect(ax:plt.Axes, b:Collection[int], color:str='white', text=None, text_size=14):\n    \"\"\n    patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill=False, edgecolor=color, lw=2))\n    _draw_outline(patch, 4)\n    if text is not None:\n        patch = ax.text(*b[:2], text, verticalalignment='top', color=color, fontsize=text_size, weight='bold')\n        _draw_outline(patch,1)", "output": "Draw bounding box on `ax`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _start(self, worker=None):\n        \"\"\"\"\"\"\n        if worker:\n            self._queue_workers.append(worker)\n\n        if self._queue_workers and self._running_threads < self._max_threads:\n            #print('Queue: {0} Running: {1} Workers: {2} '\n            #       'Threads: {3}'.format(len(self._queue_workers),\n            #                                 self._running_threads,\n            #                                 len(self._workers),\n            #                                 len(self._threads)))\n            self._running_threads += 1\n            worker = self._queue_workers.popleft()\n            thread = QThread()\n            if isinstance(worker, PythonWorker):\n                worker.moveToThread(thread)\n                worker.sig_finished.connect(thread.quit)\n                thread.started.connect(worker._start)\n                thread.start()\n            elif isinstance(worker, ProcessWorker):\n                thread.quit()\n                worker._start()\n            self._threads.append(thread)\n        else:\n            self._timer.start()\n\n        if self._workers:\n            for w in self._workers:\n                if w.is_finished():\n                    self._bag_collector.append(w)\n                    self._workers.remove(w)\n\n        if self._threads:\n            for t in self._threads:\n                if t.isFinished():\n                    self._threads.remove(t)\n                    self._running_threads -= 1\n\n        if len(self._threads) == 0 and len(self._workers) == 0:\n            self._timer.stop()\n            self._timer_worker_delete.start()", "output": "Start threads and check for inactive workers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_service_exec():\n    '''\n    \n    '''\n    contextkey = 'systemd._get_service_exec'\n    if contextkey not in __context__:\n        executables = ('update-rc.d', 'chkconfig')\n        for executable in executables:\n            service_exec = salt.utils.path.which(executable)\n            if service_exec is not None:\n                break\n        else:\n            raise CommandExecutionError(\n                'Unable to find sysv service manager (tried {0})'.format(\n                    ', '.join(executables)\n                )\n            )\n        __context__[contextkey] = service_exec\n    return __context__[contextkey]", "output": "Returns the path to the sysv service manager (either update-rc.d or\n    chkconfig)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_key_file(user,\n                   source,\n                   config='.ssh/authorized_keys',\n                   saltenv='base',\n                   fingerprint_hash_type=None):\n    '''\n    \n    '''\n    keyfile = __salt__['cp.cache_file'](source, saltenv)\n    if not keyfile:\n        return {}\n    s_keys = _validate_keys(keyfile, fingerprint_hash_type)\n    if not s_keys:\n        err = 'No keys detected in {0}. Is file properly ' \\\n              'formatted?'.format(source)\n        log.error(err)\n        __context__['ssh_auth.error'] = err\n        return {}\n    else:\n        ret = {}\n        for key in s_keys:\n            ret[key] = check_key(\n                user,\n                key,\n                s_keys[key]['enc'],\n                s_keys[key]['comment'],\n                s_keys[key]['options'],\n                config=config,\n                fingerprint_hash_type=fingerprint_hash_type)\n        return ret", "output": "Check a keyfile from a source destination against the local keys and\n    return the keys to change\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ssh.check_key_file root salt://ssh/keyfile", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _tokenize_wordpiece(self, text):\n        \"\"\"\n        \"\"\"\n\n        output_tokens = []\n        for token in self.basic_tokenizer._whitespace_tokenize(text):\n            chars = list(token)\n            if len(chars) > self.max_input_chars_per_word:\n                output_tokens.append(self.vocab.unknown_token)\n                continue\n            is_bad = False\n            start = 0\n            sub_tokens = []\n            while start < len(chars):\n                end = len(chars)\n                cur_substr = None\n                while start < end:\n                    substr = ''.join(chars[start:end])\n                    if start > 0:\n                        substr = '##' + substr\n                    if substr in self.vocab:\n                        cur_substr = substr\n                        break\n                    end -= 1\n                if cur_substr is None:\n                    is_bad = True\n                    break\n                sub_tokens.append(cur_substr)\n                start = end\n            if is_bad:\n                output_tokens.append(self.vocab.unknown_token)\n            else:\n                output_tokens.extend(sub_tokens)\n        return output_tokens", "output": "Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BERTBasicTokenizer.\n\n        Returns:\n          A list of wordpiece tokens.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def type(self):\n        \"\"\" \n        \"\"\"\n        kv_type = ctypes.c_char_p()\n        check_call(_LIB.MXKVStoreGetType(self.handle, ctypes.byref(kv_type)))\n        return py_str(kv_type.value)", "output": "Returns the type of this kvstore.\n\n        Returns\n        -------\n        type : str\n            the string type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def discriminator(self, inputs, outputs):\n        \"\"\" \"\"\"\n        l = tf.concat([inputs, outputs], 3)\n        with argscope(Conv2D, kernel_size=4, strides=2, activation=BNLReLU):\n            l = (LinearWrap(l)\n                 .Conv2D('conv0', NF, activation=tf.nn.leaky_relu)\n                 .Conv2D('conv1', NF * 2)\n                 .Conv2D('conv2', NF * 4)\n                 .Conv2D('conv3', NF * 8, strides=1, padding='VALID')\n                 .Conv2D('convlast', 1, strides=1, padding='VALID', activation=tf.identity)())\n        return l", "output": "return a (b, 1) logits", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_nonnegative(value):\n  \"\"\"\"\"\"\n  if isinstance(value, tf.Tensor):\n    with tf.control_dependencies([tf.assert_greater_equal(value, 0)]):\n      value = tf.identity(value)\n  elif value < 0:\n    raise ValueError(\"Value must be non-negative.\")\n  return value", "output": "Check that the value is nonnegative.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pidfile(pidfile):\n    '''\n    \n    '''\n    try:\n        with salt.utils.files.fopen(pidfile) as pdf:\n            pid = pdf.read().strip()\n        return int(pid)\n    except (OSError, IOError, TypeError, ValueError):\n        return -1", "output": "Return the pid from a pidfile as an integer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bytenet_base():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.batch_size = 2048\n  hparams.hidden_size = 768\n  hparams.dropout = 0.2\n  hparams.symbol_dropout = 0.2\n  hparams.label_smoothing = 0.1\n  hparams.clip_grad_norm = 2.0\n  hparams.num_hidden_layers = 4\n  hparams.kernel_height = 3\n  hparams.kernel_width = 1\n  hparams.learning_rate_decay_scheme = \"exp\"\n  hparams.learning_rate = 0.05\n  hparams.learning_rate_warmup_steps = 3000\n  hparams.initializer_gain = 1.0\n  hparams.weight_decay = 3.0\n  hparams.num_sampled_classes = 0\n  hparams.sampling_method = \"argmax\"\n  hparams.optimizer_adam_epsilon = 1e-6\n  hparams.optimizer_adam_beta1 = 0.85\n  hparams.optimizer_adam_beta2 = 0.997\n  hparams.add_hparam(\"num_block_repeat\", 4)\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shuffle_models(self, start_iteration=0, end_iteration=-1):\n        \"\"\"\n        \"\"\"\n        _safe_call(_LIB.LGBM_BoosterShuffleModels(\n            self.handle,\n            ctypes.c_int(start_iteration),\n            ctypes.c_int(end_iteration)))\n        return self", "output": "Shuffle models.\n\n        Parameters\n        ----------\n        start_iteration : int, optional (default=0)\n            The first iteration that will be shuffled.\n        end_iteration : int, optional (default=-1)\n            The last iteration that will be shuffled.\n            If <= 0, means the last available iteration.\n\n        Returns\n        -------\n        self : Booster\n            Booster with shuffled models.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter(self, func, dropna=True, *args, **kwargs):  # noqa\n        \"\"\"\n        \n        \"\"\"\n        if isinstance(func, str):\n            wrapper = lambda x: getattr(x, func)(*args, **kwargs)\n        else:\n            wrapper = lambda x: func(x, *args, **kwargs)\n\n        # Interpret np.nan as False.\n        def true_and_notna(x, *args, **kwargs):\n            b = wrapper(x, *args, **kwargs)\n            return b and notna(b)\n\n        try:\n            indices = [self._get_index(name) for name, group in self\n                       if true_and_notna(group)]\n        except ValueError:\n            raise TypeError(\"the filter must return a boolean result\")\n        except TypeError:\n            raise TypeError(\"the filter must return a boolean result\")\n\n        filtered = self._apply_filter(indices, dropna)\n        return filtered", "output": "Return a copy of a Series excluding elements from groups that\n        do not satisfy the boolean criterion specified by func.\n\n        Parameters\n        ----------\n        func : function\n            To apply to each group. Should return True or False.\n        dropna : Drop groups that do not pass the filter. True by default;\n            if False, groups that evaluate False are filled with NaNs.\n\n        Examples\n        --------\n        >>> df = pd.DataFrame({'A' : ['foo', 'bar', 'foo', 'bar',\n        ...                           'foo', 'bar'],\n        ...                    'B' : [1, 2, 3, 4, 5, 6],\n        ...                    'C' : [2.0, 5., 8., 1., 2., 9.]})\n        >>> grouped = df.groupby('A')\n        >>> df.groupby('A').B.filter(lambda x: x.mean() > 3.)\n        1    2\n        3    4\n        5    6\n        Name: B, dtype: int64\n\n        Returns\n        -------\n        filtered : Series", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_aliased_type(cls, other_base):\n        \"\"\"\n        \n        \"\"\"\n        docstring = dedent(\n            \"\"\"\n            A {t} that names another {t}.\n\n            Parameters\n            ----------\n            term : {t}\n            {{name}}\n            \"\"\"\n        ).format(t=other_base.__name__)\n\n        doc = format_docstring(\n            owner_name=other_base.__name__,\n            docstring=docstring,\n            formatters={'name': PIPELINE_ALIAS_NAME_DOC},\n        )\n\n        return type(\n            'Aliased' + other_base.__name__,\n            (cls, other_base),\n            {'__doc__': doc,\n             '__module__': other_base.__module__},\n        )", "output": "Factory for making Aliased{Filter,Factor,Classifier}.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decode_base64(data: str) -> bytes:\n    \"\"\"\n    \"\"\"\n    missing_padding = len(data) % 4\n    if missing_padding != 0:\n        data += \"=\" * (4 - missing_padding)\n    return base64.decodebytes(data.encode(\"utf-8\"))", "output": "Decode base64, padding being optional.\n\n    :param data: Base64 data as an ASCII byte string\n    :returns: The decoded byte string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_images(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudSystemExit(\n            'The avail_images function must be called with '\n            '-f or --function, or with the --list-images option'\n        )\n\n    fetch = True\n    page = 1\n    ret = {}\n\n    while fetch:\n        items = query(method='images', command='?page=' + six.text_type(page) + '&per_page=200')\n\n        for image in items['images']:\n            ret[image['name']] = {}\n            for item in six.iterkeys(image):\n                ret[image['name']][item] = image[item]\n\n        page += 1\n        try:\n            fetch = 'next' in items['links']['pages']\n        except KeyError:\n            fetch = False\n\n    return ret", "output": "Return a list of the images that are on the provider", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def refresh(self, index=None):\r\n        \"\"\"\"\"\"\r\n        if index is None:\r\n            index = self.get_stack_index()\r\n        # Set current editor\r\n        if self.get_stack_count():\r\n            index = self.get_stack_index()\r\n            finfo = self.data[index]\r\n            editor = finfo.editor\r\n            editor.setFocus()\r\n            self._refresh_outlineexplorer(index, update=False)\r\n            self.__refresh_statusbar(index)\r\n            self.__refresh_readonly(index)\r\n            self.__check_file_status(index)\r\n            self.__modify_stack_title()\r\n            self.update_plugin_title.emit()\r\n        else:\r\n            editor = None\r\n        # Update the modification-state-dependent parameters\r\n        self.modification_changed()\r\n        # Update FindReplace binding\r\n        self.find_widget.set_editor(editor, refresh=False)", "output": "Refresh tabwidget", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resizeEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        if not self.isMaximized() and not self.fullscreen_flag:\r\n            self.window_size = self.size()\r\n        QMainWindow.resizeEvent(self, event)\r\n\r\n        # To be used by the tour to be able to resize\r\n        self.sig_resized.emit(event)", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def textConcat(self, content, len):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlTextConcat(self._o, content, len)\n        return ret", "output": "Concat the given string at the end of the existing node\n           content", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_game(game_name, game_mode=\"NoFrameskip-v4\"):\n  \"\"\"\n  \"\"\"\n  if game_name not in ATARI_GAMES:\n    raise ValueError(\"Game %s not in ATARI_GAMES\" % game_name)\n  if game_mode not in ATARI_GAME_MODES:\n    raise ValueError(\"Unknown ATARI game mode: %s.\" % game_mode)\n  camel_game_name = misc_utils.snakecase_to_camelcase(game_name) + game_mode\n  # Create and register the Problem\n  cls = type(\"Gym%sRandom\" % camel_game_name,\n             (T2TGymEnv,), {\"base_env_name\": camel_game_name})\n  registry.register_problem(cls)", "output": "Create and register problems for the game.\n\n  Args:\n    game_name: str, one of the games in ATARI_GAMES, e.g. \"bank_heist\".\n    game_mode: the frame skip and sticky keys config.\n\n  Raises:\n    ValueError: if game_name or game_mode are wrong.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def availability_set_delete(name, resource_group, **kwargs):\n    '''\n    \n\n    '''\n    result = False\n    compconn = __utils__['azurearm.get_client']('compute', **kwargs)\n    try:\n        compconn.availability_sets.delete(\n            resource_group_name=resource_group,\n            availability_set_name=name\n        )\n        result = True\n\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('compute', str(exc), **kwargs)\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Delete an availability set.\n\n    :param name: The availability set to delete.\n\n    :param resource_group: The resource group name assigned to the\n        availability set.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_compute.availability_set_delete testset testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_or_create_vocab(self, data_dir, tmp_dir, force_get=False):\n    \"\"\"\"\"\"\n    # We assume that vocab file is present in data_dir directory where the\n    # data generated will be stored.\n    vocab_filepath = os.path.join(data_dir, self.vocab_filename)\n    encoder = text_encoder.SubwordTextEncoder(vocab_filepath)\n    return encoder", "output": "Get vocab for distill problems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_requirements(metadata, extras):\n    \"\"\"\n    \"\"\"\n    extras = extras or ()\n    requirements = []\n    for entry in metadata.run_requires:\n        if isinstance(entry, six.text_type):\n            entry = {\"requires\": [entry]}\n            extra = None\n        else:\n            extra = entry.get(\"extra\")\n        if extra is not None and extra not in extras:\n            continue\n        for line in entry.get(\"requires\", []):\n            r = requirementslib.Requirement.from_line(line)\n            if r.markers:\n                contained = get_contained_extras(r.markers)\n                if (contained and not any(e in contained for e in extras)):\n                    continue\n                marker = get_without_extra(r.markers)\n                r.markers = str(marker) if marker else None\n                line = r.as_line(include_hashes=False)\n            requirements.append(line)\n    return requirements", "output": "Read wheel metadata to know what it depends on.\n\n    The `run_requires` attribute contains a list of dict or str specifying\n    requirements. For dicts, it may contain an \"extra\" key to specify these\n    requirements are for a specific extra. Unfortunately, not all fields are\n    specificed like this (I don't know why); some are specified with markers.\n    So we jump though these terrible hoops to know exactly what we need.\n\n    The extra extraction is not comprehensive. Tt assumes the marker is NEVER\n    something like `extra == \"foo\" and extra == \"bar\"`. I guess this never\n    makes sense anyway? Markers are just terrible.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QUANTILE(src_column, *args):\n    \"\"\"\n    \n    \"\"\"\n    if len(args) == 1:\n        quantiles = args[0]\n    else:\n        quantiles = list(args)\n\n    if not _is_non_string_iterable(quantiles):\n        quantiles = [quantiles]\n    query = \",\".join([str(i) for i in quantiles])\n    return (\"__builtin__quantile__[\" + query + \"]\", [src_column])", "output": "Builtin approximate quantile aggregator for groupby.\n    Accepts as an argument, one or more of a list of quantiles to query.\n    For instance:\n\n    To extract the median\n\n    >>> sf.groupby(\"user\",\n    ...   {'rating_quantiles':tc.aggregate.QUANTILE('rating', 0.5)})\n\n    To extract a few quantiles\n\n    >>> sf.groupby(\"user\",\n    ...   {'rating_quantiles':tc.aggregate.QUANTILE('rating', [0.25,0.5,0.75])})\n\n    Or equivalently\n\n    >>> sf.groupby(\"user\",\n    ...     {'rating_quantiles':tc.aggregate.QUANTILE('rating', 0.25,0.5,0.75)})\n\n    The returned quantiles are guaranteed to have 0.5% accuracy. That is to say,\n    if the requested quantile is 0.50, the resultant quantile value may be\n    between 0.495 and 0.505 of the true quantile.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def assertStructIsInline(self, obj):\n        \"\"\"\n        \n        \"\"\"\n\n        N.enforce_number(obj, N.UOffsetTFlags)\n        if obj != self.Offset():\n            msg = (\"flatbuffers: Tried to write a Struct at an Offset that \"\n                   \"is different from the current Offset of the Builder.\")\n            raise StructIsNotInlineError(msg)", "output": "Structs are always stored inline, so need to be created right\n        where they are used. You'll get this error if you created it\n        elsewhere.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_vocab_from_args(args: argparse.Namespace):\n    \"\"\"\n    \n    \"\"\"\n    parameter_path = args.param_path\n    overrides = args.overrides\n    serialization_dir = args.serialization_dir\n\n    params = Params.from_file(parameter_path, overrides)\n\n    make_vocab_from_params(params, serialization_dir)", "output": "Just converts from an ``argparse.Namespace`` object to params.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_tall_finetune_textclass():\n  \"\"\"\"\"\"\n  hparams = transformer_tall()\n  hparams.learning_rate_constant = 6.25e-5\n  hparams.learning_rate_schedule = (\"linear_warmup*constant*linear_decay\")\n  hparams.multiproblem_schedule_max_examples = 0\n  hparams.multiproblem_target_eval_only = True\n  hparams.learning_rate_warmup_steps = 50\n  # Set train steps to learning_rate_decay_steps or less\n  hparams.learning_rate_decay_steps = 25000\n  hparams.multiproblem_reweight_label_loss = True\n  hparams.multiproblem_label_weight = 0.95\n  return hparams", "output": "Hparams for transformer on LM for finetuning on text class problems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def post_work_result(self, function_arn, work_item):\n        \"\"\"\n        \n        \"\"\"\n        url = self._get_work_url(function_arn)\n\n        runtime_logger.info('Posting work result for invocation id [{}] to {}'.format(work_item.invocation_id, url))\n        request = Request(url, work_item.payload or b'')\n\n        request.add_header(HEADER_INVOCATION_ID, work_item.invocation_id)\n        request.add_header(HEADER_AUTH_TOKEN, self.auth_token)\n\n        urlopen(request)\n\n        runtime_logger.info('Posted work result for invocation id [{}]'.format(work_item.invocation_id))", "output": "Post the result of processing work item by :code:`function_arn`.\n\n        :param function_arn: Arn of the Lambda function intended to receive the work for processing.\n        :type function_arn: string\n\n        :param work_item: The WorkItem holding the results of the work being posted.\n        :type work_item: WorkItem\n\n        :returns: None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_tcp_line(line):\n    '''\n    \n    '''\n    ret = {}\n    comps = line.strip().split()\n    sl = comps[0].rstrip(':')\n    ret[sl] = {}\n    l_addr, l_port = comps[1].split(':')\n    r_addr, r_port = comps[2].split(':')\n    ret[sl]['local_addr'] = hex2ip(l_addr, True)\n    ret[sl]['local_port'] = int(l_port, 16)\n    ret[sl]['remote_addr'] = hex2ip(r_addr, True)\n    ret[sl]['remote_port'] = int(r_port, 16)\n    ret[sl]['state'] = int(comps[3], 16)\n    return ret", "output": "Parse a single line from the contents of /proc/net/tcp or /proc/net/tcp6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def htmlAutoCloseTag(self, name, elem):\n        \"\"\" \"\"\"\n        ret = libxml2mod.htmlAutoCloseTag(self._o, name, elem)\n        return ret", "output": "The HTML DTD allows a tag to implicitly close other tags.\n          The list is kept in htmlStartClose array. This function\n          checks if the element or one of it's children would\n           autoclose the given tag.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def safe_listdir(path):\n    \"\"\"\n    \n    \"\"\"\n    try:\n        return os.listdir(path)\n    except (PermissionError, NotADirectoryError):\n        pass\n    except OSError as e:\n        # Ignore the directory if does not exist, not a directory or\n        # permission denied\n        ignorable = (\n            e.errno in (errno.ENOTDIR, errno.EACCES, errno.ENOENT)\n            # Python 2 on Windows needs to be handled this way :(\n            or getattr(e, \"winerror\", None) == 267\n        )\n        if not ignorable:\n            raise\n    return ()", "output": "Attempt to list contents of path, but suppress some exceptions.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_metadata_for_build_wheel(metadata_directory, config_settings):\n    \"\"\"\n    \"\"\"\n    backend = _build_backend()\n    try:\n        hook = backend.prepare_metadata_for_build_wheel\n    except AttributeError:\n        return _get_wheel_metadata_from_wheel(backend, metadata_directory,\n                                              config_settings)\n    else:\n        return hook(metadata_directory, config_settings)", "output": "Invoke optional prepare_metadata_for_build_wheel\n\n    Implements a fallback by building a wheel if the hook isn't defined.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def step(self, actions):\n    \"\"\"\"\"\"\n    self._elapsed_steps += 1\n    obs, rewards, dones = \\\n        [np.array(r) for r in self.batch_env.step(actions)]\n    if self._elapsed_steps > self._max_episode_steps:\n      done = True\n      if self._elapsed_steps > self._max_episode_steps + 1:\n        rewards.fill(0)\n    else:\n      done = dones[0]\n      assert np.all(done == dones), (\"Current modifications of Dopamine \"\n                                     \"require same number of steps for each \"\n                                     \"environment in batch\")\n      del dones\n\n    self.game_over = done\n    return obs, rewards, done, {}", "output": "Step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _addsub_int_array(self, other, op):\n        \"\"\"\n        \n        \"\"\"\n        # _addsub_int_array is overriden by PeriodArray\n        assert not is_period_dtype(self)\n        assert op in [operator.add, operator.sub]\n\n        if self.freq is None:\n            # GH#19123\n            raise NullFrequencyError(\"Cannot shift with no freq\")\n\n        elif isinstance(self.freq, Tick):\n            # easy case where we can convert to timedelta64 operation\n            td = Timedelta(self.freq)\n            return op(self, td * other)\n\n        # We should only get here with DatetimeIndex; dispatch\n        # to _addsub_offset_array\n        assert not is_timedelta64_dtype(self)\n        return op(self, np.array(other) * self.freq)", "output": "Add or subtract array-like of integers equivalent to applying\n        `_time_shift` pointwise.\n\n        Parameters\n        ----------\n        other : Index, ExtensionArray, np.ndarray\n            integer-dtype\n        op : {operator.add, operator.sub}\n\n        Returns\n        -------\n        result : same class as self", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unpersist(self, blocking=False):\n        \"\"\"\n        \"\"\"\n        self.is_cached = False\n        self._jdf.unpersist(blocking)\n        return self", "output": "Marks the :class:`DataFrame` as non-persistent, and remove all blocks for it from\n        memory and disk.\n\n        .. note:: `blocking` default has changed to False to match Scala in 2.0.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def colum_avg(self, state):\r\n        \"\"\"\"\"\"\r\n        self.colum_avg_enabled = state > 0\r\n        if self.colum_avg_enabled:\r\n            self.return_max = lambda col_vals, index: col_vals[index]\r\n        else:\r\n            self.return_max = global_max\r\n        self.reset()", "output": "Toggle backgroundcolor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mouseDoubleClickEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        index_clicked = self.indexAt(event.pos())\r\n        if self.model.breakpoints:\r\n            filename = self.model.breakpoints[index_clicked.row()][0]\r\n            line_number_str = self.model.breakpoints[index_clicked.row()][1]\r\n            self.edit_goto.emit(filename, int(line_number_str), '')\r\n        if index_clicked.column()==2:\r\n            self.set_or_edit_conditional_breakpoint.emit()", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def docEntity(self, name):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlGetDocEntity(self._o, name)\n        if ret is None:raise treeError('xmlGetDocEntity() failed')\n        __tmp = xmlEntity(_obj=ret)\n        return __tmp", "output": "Do an entity lookup in the document entity hash table and", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_cert(self, key_file=None, cert_file=None,\n                 cert_reqs=None, ca_certs=None,\n                 assert_hostname=None, assert_fingerprint=None,\n                 ca_cert_dir=None):\n        \"\"\"\n        \n        \"\"\"\n        # If cert_reqs is not provided, we can try to guess. If the user gave\n        # us a cert database, we assume they want to use it: otherwise, if\n        # they gave us an SSL Context object we should use whatever is set for\n        # it.\n        if cert_reqs is None:\n            if ca_certs or ca_cert_dir:\n                cert_reqs = 'CERT_REQUIRED'\n            elif self.ssl_context is not None:\n                cert_reqs = self.ssl_context.verify_mode\n\n        self.key_file = key_file\n        self.cert_file = cert_file\n        self.cert_reqs = cert_reqs\n        self.assert_hostname = assert_hostname\n        self.assert_fingerprint = assert_fingerprint\n        self.ca_certs = ca_certs and os.path.expanduser(ca_certs)\n        self.ca_cert_dir = ca_cert_dir and os.path.expanduser(ca_cert_dir)", "output": "This method should only be called once, before the connection is used.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def current_iteration(self):\n        \"\"\"\n        \"\"\"\n        out_cur_iter = ctypes.c_int(0)\n        _safe_call(_LIB.LGBM_BoosterGetCurrentIteration(\n            self.handle,\n            ctypes.byref(out_cur_iter)))\n        return out_cur_iter.value", "output": "Get the index of the current iteration.\n\n        Returns\n        -------\n        cur_iter : int\n            The index of the current iteration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_spyderplugins(plugin_path, is_io, modnames, modlist):\r\n    \"\"\"\"\"\"\r\n    if not osp.isdir(plugin_path):\r\n        return\r\n\r\n    for name in os.listdir(plugin_path):\r\n        # This is needed in order to register the spyder_io_hdf5 plugin.\r\n        # See issue 4487\r\n        # Is this a Spyder plugin?\r\n        if not name.startswith(PLUGIN_PREFIX):\r\n            continue\r\n\r\n        # Ensure right type of plugin\r\n        if is_io != name.startswith(IO_PREFIX):\r\n            continue\r\n\r\n        # Skip names that end in certain suffixes\r\n        forbidden_suffixes = ['dist-info', 'egg.info', 'egg-info', 'egg-link',\r\n                              'kernels']\r\n        if any([name.endswith(s) for s in forbidden_suffixes]):\r\n            continue\r\n\r\n        # Import the plugin\r\n        _import_plugin(name, plugin_path, modnames, modlist)", "output": "Scan the directory `plugin_path` for plugin packages and loads them.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def adafactor_optimizer_from_hparams(hparams, lr):\n  \"\"\"\n  \"\"\"\n  if hparams.optimizer_adafactor_decay_type == \"adam\":\n    decay_rate = adafactor_decay_rate_adam(\n        hparams.optimizer_adafactor_beta2)\n  elif hparams.optimizer_adafactor_decay_type == \"pow\":\n    decay_rate = adafactor_decay_rate_pow(\n        hparams.optimizer_adafactor_memory_exponent)\n  else:\n    raise ValueError(\"unknown optimizer_adafactor_decay_type\")\n  if hparams.weight_dtype == \"bfloat16\":\n    parameter_encoding = quantization.EighthPowerEncoding()\n  else:\n    parameter_encoding = None\n  return AdafactorOptimizer(\n      multiply_by_parameter_scale=(\n          hparams.optimizer_adafactor_multiply_by_parameter_scale),\n      learning_rate=lr,\n      decay_rate=decay_rate,\n      beta1=hparams.optimizer_adafactor_beta1,\n      clipping_threshold=hparams.optimizer_adafactor_clipping_threshold,\n      factored=hparams.optimizer_adafactor_factored,\n      simulated_quantize_bits=getattr(\n          hparams, \"simulated_parameter_quantize_bits\", 0),\n      parameter_encoding=parameter_encoding,\n      use_locking=False,\n      name=\"Adafactor\")", "output": "Create an Adafactor optimizer based on model hparams.\n\n  Args:\n    hparams: model hyperparameters\n    lr: learning rate scalar.\n  Returns:\n    an AdafactorOptimizer\n  Raises:\n    ValueError: on illegal values", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def crosstab(self, col1, col2):\n        \"\"\"\n        \n        \"\"\"\n        if not isinstance(col1, basestring):\n            raise ValueError(\"col1 should be a string.\")\n        if not isinstance(col2, basestring):\n            raise ValueError(\"col2 should be a string.\")\n        return DataFrame(self._jdf.stat().crosstab(col1, col2), self.sql_ctx)", "output": "Computes a pair-wise frequency table of the given columns. Also known as a contingency\n        table. The number of distinct values for each column should be less than 1e4. At most 1e6\n        non-zero pair frequencies will be returned.\n        The first column of each row will be the distinct values of `col1` and the column names\n        will be the distinct values of `col2`. The name of the first column will be `$col1_$col2`.\n        Pairs that have no occurrences will have zero as their counts.\n        :func:`DataFrame.crosstab` and :func:`DataFrameStatFunctions.crosstab` are aliases.\n\n        :param col1: The name of the first column. Distinct items will make the first item of\n            each row.\n        :param col2: The name of the second column. Distinct items will make the column names\n            of the DataFrame.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call_status(*args, **kwargs):\n    '''\n    \n    '''\n    res = dict()\n    devices = _get_lights()\n    for dev_id in 'id' not in kwargs and sorted(devices.keys()) or _get_devices(kwargs):\n        dev_id = six.text_type(dev_id)\n        res[dev_id] = {\n            'on': devices[dev_id]['state']['on'],\n            'reachable': devices[dev_id]['state']['reachable']\n        }\n\n    return res", "output": "Return the status of the lamps.\n\n    Options:\n\n    * **id**: Specifies a device ID. Can be a comma-separated values. All, if omitted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' hue.status\n        salt '*' hue.status id=1\n        salt '*' hue.status id=1,2,3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_from_list(self, subwords):\n    \"\"\"\"\"\"\n    subwords = [tf.compat.as_text(s) for s in subwords if s]\n    self._subwords = subwords\n    # Note that internally everything is 0-indexed. Padding is dealt with at the\n    # end of encode and the beginning of decode.\n    self._subword_to_id = {s: i for i, s in enumerate(subwords)}\n\n    # We remember the maximum length of any subword to avoid having to\n    # check arbitrarily long strings.\n    self._max_subword_len = max(\n        len(_UNDERSCORE_REPLACEMENT), max([len(s) for s in subwords] or [1]))\n\n    # Initialize the cache\n    self._cache_size = 2**20\n    self._token_to_ids_cache = [(None, None)] * self._cache_size\n\n    # Setup tokenizer\n    # Reserved tokens are all tokens that are mixed alphanum and non-alphanum.\n    reserved_tokens = set([_UNDERSCORE_REPLACEMENT])\n    for t in self._subwords:\n      if text_encoder.is_mixed_alphanum(t):\n        reserved_tokens.add(t)\n    self._tokenizer = text_encoder.Tokenizer(\n        alphanum_only=False, reserved_tokens=reserved_tokens)", "output": "Initializes the encoder from a list of subwords.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def open_fileswitcher(self, symbol=False):\r\n        \"\"\"\"\"\"\r\n        if self.fileswitcher is not None and \\\r\n          self.fileswitcher.is_visible:\r\n            self.fileswitcher.hide()\r\n            self.fileswitcher.is_visible = False\r\n            return\r\n        if symbol:\r\n            self.fileswitcher.plugin = self.editor\r\n            self.fileswitcher.set_search_text('@')\r\n        else:\r\n            self.fileswitcher.set_search_text('')\r\n        self.fileswitcher.show()\r\n        self.fileswitcher.is_visible = True", "output": "Open file list management dialog box.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def close_all_but_this(self):\r\n        \"\"\"\"\"\"\r\n        self.close_all_right()\r\n        for i in range(0, self.get_stack_count()-1  ):\r\n            self.close_file(0)", "output": "Close all files but the current one", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _repr_html_(self):\n        \"\"\"\n        \"\"\"\n        import cgi\n        if not self._support_repr_html:\n            self._support_repr_html = True\n        if self.sql_ctx._conf.isReplEagerEvalEnabled():\n            max_num_rows = max(self.sql_ctx._conf.replEagerEvalMaxNumRows(), 0)\n            sock_info = self._jdf.getRowsToPython(\n                max_num_rows, self.sql_ctx._conf.replEagerEvalTruncate())\n            rows = list(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n            head = rows[0]\n            row_data = rows[1:]\n            has_more_data = len(row_data) > max_num_rows\n            row_data = row_data[:max_num_rows]\n\n            html = \"<table border='1'>\\n\"\n            # generate table head\n            html += \"<tr><th>%s</th></tr>\\n\" % \"</th><th>\".join(map(lambda x: cgi.escape(x), head))\n            # generate table rows\n            for row in row_data:\n                html += \"<tr><td>%s</td></tr>\\n\" % \"</td><td>\".join(\n                    map(lambda x: cgi.escape(x), row))\n            html += \"</table>\\n\"\n            if has_more_data:\n                html += \"only showing top %d %s\\n\" % (\n                    max_num_rows, \"row\" if max_num_rows == 1 else \"rows\")\n            return html\n        else:\n            return None", "output": "Returns a dataframe with html code when you enabled eager evaluation\n        by 'spark.sql.repl.eagerEval.enabled', this only called by REPL you are\n        using support eager evaluation with HTML.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _mkanchors(ws, hs, x_ctr, y_ctr):\n        \"\"\"\n        \n        \"\"\"\n        ws = ws[:, np.newaxis]\n        hs = hs[:, np.newaxis]\n        anchors = np.hstack((x_ctr - 0.5 * (ws - 1),\n                             y_ctr - 0.5 * (hs - 1),\n                             x_ctr + 0.5 * (ws - 1),\n                             y_ctr + 0.5 * (hs - 1)))\n        return anchors", "output": "Given a vector of widths (ws) and heights (hs) around a center\n        (x_ctr, y_ctr), output a set of anchors (windows).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_stores():\n    '''\n    \n    '''\n    ret = dict()\n    cmd = r\"Get-ChildItem -Path 'Cert:\\' | \" \\\n          r\"Select-Object LocationName, StoreNames\"\n\n    items = _cmd_run(cmd=cmd, as_json=True)\n\n    for item in items:\n        ret[item['LocationName']] = list()\n\n        for store in item['StoreNames']:\n            ret[item['LocationName']].append(store)\n    return ret", "output": "Get the certificate location contexts and their corresponding stores.\n\n    :return: A dictionary of the certificate location contexts and stores.\n    :rtype: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_pki.get_stores", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(cls, fpath=None, create_missing=True):\n        \"\"\n        fpath = _expand_path(fpath or cls.DEFAULT_CONFIG_PATH)\n        if not fpath.exists() and create_missing: cls.create(fpath)\n        assert fpath.exists(), f'Could not find config at: {fpath}. Please create'\n        with open(fpath, 'r') as yaml_file: return yaml.safe_load(yaml_file)", "output": "Retrieve the `Config` in `fpath`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_split_adjustments_with_overwrites(\n        self,\n        pre,\n        post,\n        overwrites,\n        requested_split_adjusted_columns\n    ):\n        \"\"\"\n        \n        \"\"\"\n        for column_name in requested_split_adjusted_columns:\n            # We can do a merge here because the timestamps in 'pre' and\n            # 'post' are guaranteed to not overlap.\n            if pre:\n                # Either empty or contains all columns.\n                for ts in pre[column_name]:\n                    add_new_adjustments(\n                        overwrites,\n                        pre[column_name][ts],\n                        column_name,\n                        ts\n                    )\n            if post:\n                # Either empty or contains all columns.\n                for ts in post[column_name]:\n                    add_new_adjustments(\n                        overwrites,\n                        post[column_name][ts],\n                        column_name,\n                        ts\n                    )", "output": "Merge split adjustments with the dict containing overwrites.\n\n        Parameters\n        ----------\n        pre : dict[str -> dict[int -> list]]\n            The adjustments that occur before the split-adjusted-asof-date.\n        post : dict[str -> dict[int -> list]]\n            The adjustments that occur after the split-adjusted-asof-date.\n        overwrites : dict[str -> dict[int -> list]]\n            The overwrites across all time. Adjustments will be merged into\n            this dictionary.\n        requested_split_adjusted_columns : list of str\n            List of names of split adjusted columns that are being requested.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_report_metric_data(self, data):\n        \"\"\"\n        \"\"\"\n        logger.debug('handle report metric data = %s', data)\n\n        assert 'value' in data\n        value = extract_scalar_reward(data['value'])\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -value\n        else:\n            reward = value\n        assert 'parameter_id' in data\n        s, i, _ = data['parameter_id'].split('_')\n\n        logger.debug('bracket id = %s, metrics value = %s, type = %s', s, value, data['type'])\n        s = int(s)\n\n        assert 'type' in data\n        if data['type'] == 'FINAL':\n            # and PERIODICAL metric are independent, thus, not comparable.\n            assert 'sequence' in data\n            self.brackets[s].set_config_perf(\n                int(i), data['parameter_id'], sys.maxsize, value)\n            self.completed_hyper_configs.append(data)\n       \n            _parameters = self.parameters[data['parameter_id']]\n            _parameters.pop(_KEY)\n            # update BO with loss, max_s budget, hyperparameters\n            self.cg.new_result(loss=reward, budget=data['sequence'], parameters=_parameters, update_model=True)\n        elif data['type'] == 'PERIODICAL':\n            self.brackets[s].set_config_perf(\n                int(i), data['parameter_id'], data['sequence'], value)\n        else:\n            raise ValueError(\n                'Data type not supported: {}'.format(data['type']))", "output": "reveice the metric data and update Bayesian optimization with final result\n\n        Parameters\n        ----------\n        data:\n            it is an object which has keys 'parameter_id', 'value', 'trial_job_id', 'type', 'sequence'.\n\n        Raises\n        ------\n        ValueError\n            Data type not supported", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cint32_array_to_numpy(cptr, length):\n    \"\"\"\"\"\"\n    if isinstance(cptr, ctypes.POINTER(ctypes.c_int32)):\n        return np.fromiter(cptr, dtype=np.int32, count=length)\n    else:\n        raise RuntimeError('Expected int pointer')", "output": "Convert a ctypes int pointer array to a numpy array.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start_raylet_monitor(self):\n        \"\"\"\"\"\"\n        stdout_file, stderr_file = self.new_log_files(\"raylet_monitor\")\n        process_info = ray.services.start_raylet_monitor(\n            self._redis_address,\n            stdout_file=stdout_file,\n            stderr_file=stderr_file,\n            redis_password=self._ray_params.redis_password,\n            config=self._config)\n        assert (ray_constants.PROCESS_TYPE_RAYLET_MONITOR not in\n                self.all_processes)\n        self.all_processes[ray_constants.PROCESS_TYPE_RAYLET_MONITOR] = [\n            process_info\n        ]", "output": "Start the raylet monitor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def draw(self):\n        \"\"\"\"\"\"\n        if sys.stdout.isatty():  # pragma: no cover\n            from asciimatics.screen import Screen\n\n            Screen.wrapper(self._do_draw)\n        else:\n            for line in self.canvas:\n                print(\"\".join(line))", "output": "Draws ASCII canvas on the screen.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_default_parameter_values(sam_template):\n        \"\"\"\n        \n        \"\"\"\n\n        default_values = {}\n\n        parameter_definition = sam_template.get(\"Parameters\", None)\n        if not parameter_definition or not isinstance(parameter_definition, dict):\n            LOG.debug(\"No Parameters detected in the template\")\n            return default_values\n\n        for param_name, value in parameter_definition.items():\n            if isinstance(value, dict) and \"Default\" in value:\n                default_values[param_name] = value[\"Default\"]\n\n        LOG.debug(\"Collected default values for parameters: %s\", default_values)\n        return default_values", "output": "Method to read default values for template parameters and return it\n        Example:\n        If the template contains the following parameters defined\n        Parameters:\n            Param1:\n                Type: String\n                Default: default_value1\n            Param2:\n                Type: String\n                Default: default_value2\n\n        then, this method will grab default value for Param1 and return the following result:\n        {\n            Param1: \"default_value1\",\n            Param2: \"default_value2\"\n        }\n        :param dict sam_template: SAM template\n        :return dict: Default values for parameters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_profile(profile):\n    '''\n    \n    '''\n    if isinstance(profile, string_types):\n        _profile = __salt__['config.option'](profile)\n        if not _profile:\n            msg = 'Pillar key for profile {0} not found.'.format(profile)\n            raise SaltInvocationError(msg)\n    else:\n        _profile = profile\n    hosts = _profile.get('hosts')\n    index = _profile.get('index')\n    return (hosts, index)", "output": "From a pillar key, or a dictionary, return index and host keys.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rollback(self):\n        \"\"\"\"\"\"\n        self._check_state()\n        database = self._session._database\n        api = database.spanner_api\n        metadata = _metadata_with_prefix(database.name)\n        api.rollback(self._session.name, self._transaction_id, metadata=metadata)\n        self._rolled_back = True\n        del self._session._transaction", "output": "Roll back a transaction on the database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prepare_value_nd(self, value, vshape):\n        \"\"\"\"\"\"\n        if isinstance(value, numeric_types):\n            value_nd = full(shape=vshape, val=value, ctx=self.context, dtype=self.dtype)\n        elif isinstance(value, NDArray):\n            value_nd = value.as_in_context(self.context)\n            if value_nd.dtype != self.dtype:\n                value_nd = value_nd.astype(self.dtype)\n        else:\n            try:\n                value_nd = array(value, ctx=self.context, dtype=self.dtype)\n            except:\n                raise TypeError('NDArray does not support assignment with non-array-like'\n                                ' object %s of type %s' % (str(value), str(type(value))))\n        if value_nd.shape != vshape:\n            value_nd = value_nd.broadcast_to(vshape)\n        return value_nd", "output": "Given value and vshape, create an `NDArray` from value with the same\n        context and dtype as the current one and broadcast it to vshape.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle_trial_end(self, data):\n        \"\"\"\n        \"\"\"\n        logger.debug('Tuner handle trial end, result is %s', data)\n\n        hyper_params = json_tricks.loads(data['hyper_params'])\n        s, i, _ = hyper_params['parameter_id'].split('_')\n        hyper_configs = self.brackets[int(s)].inform_trial_end(int(i))\n\n        if hyper_configs is not None:\n            logger.debug(\n                'bracket %s next round %s, hyper_configs: %s', s, i, hyper_configs)\n            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs\n            for _ in range(self.credit):\n                self._request_one_trial_job()\n        # Finish this bracket and generate a new bracket\n        elif self.brackets[int(s)].no_more_trial:\n            self.curr_s -= 1\n            self.generate_new_bracket()\n            for _ in range(self.credit):\n                self._request_one_trial_job()", "output": "receive the information of trial end and generate next configuaration.\n\n        Parameters\n        ----------\n        data: dict()\n            it has three keys: trial_job_id, event, hyper_params\n            trial_job_id: the id generated by training service\n            event: the job's state\n            hyper_params: the hyperparameters (a string) generated and returned by tuner", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_only_image_column(sframe):\n    \"\"\"\n    \n    \"\"\"\n    from turicreate import Image\n    return _find_only_column_of_type(sframe, target_type=Image,\n                                     type_name='image', col_name='feature')", "output": "Finds the only column in `sframe` with a type of turicreate.Image. \n    If there are zero or more than one image columns, an exception will \n    be raised.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_link(self, url):\n        \"\"\"\"\"\"\n        return self._clean_re.sub(lambda match: \"%%%2x\" % ord(match.group(0)), url)", "output": "Makes sure a link is fully encoded.  That is, if a ' ' shows up in\n        the link, it will be rewritten to %20 (while not over-quoting\n        % or other characters).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_impl_ver(env):\n    \"\"\"\"\"\"\n    impl_ver = env.config_var(\"py_version_nodot\")\n    if not impl_ver or get_abbr_impl(env) == \"pp\":\n        impl_ver = \"\".join(map(str, get_impl_version_info(env)))\n\n    return impl_ver", "output": "Return implementation version.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_function_tuple(self, func):\n        \"\"\"  \n        \"\"\"\n        if is_tornado_coroutine(func):\n            self.save_reduce(_rebuild_tornado_coroutine, (func.__wrapped__,),\n                             obj=func)\n            return\n\n        save = self.save\n        write = self.write\n\n        code, f_globals, defaults, closure_values, dct, base_globals = self.extract_func_data(func)\n\n        save(_fill_function)  # skeleton function updater\n        write(pickle.MARK)    # beginning of tuple that _fill_function expects\n\n        self._save_subimports(\n            code,\n            itertools.chain(f_globals.values(), closure_values or ()),\n        )\n\n        # create a skeleton function object and memoize it\n        save(_make_skel_func)\n        save((\n            code,\n            len(closure_values) if closure_values is not None else -1,\n            base_globals,\n        ))\n        write(pickle.REDUCE)\n        self.memoize(func)\n\n        # save the rest of the func data needed by _fill_function\n        save(f_globals)\n        save(defaults)\n        save(dct)\n        save(func.__module__)\n        save(closure_values)\n        write(pickle.TUPLE)\n        write(pickle.REDUCE)", "output": "Pickles an actual func object.\n\n        A func comprises: code, globals, defaults, closure, and dict.  We\n        extract and save these, injecting reducing functions at certain points\n        to recreate the func object.  Keep in mind that some of these pieces\n        can contain a ref to the func itself.  Thus, a naive save on these\n        pieces could trigger an infinite loop of save's.  To get around that,\n        we first create a skeleton func object using just the code (this is\n        safe, since this won't contain a ref to the func), and memoize it as\n        soon as it's created.  The other stuff can then be filled in later.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fast_exit(code):\n    \"\"\"\n    \"\"\"\n    sys.stdout.flush()\n    sys.stderr.flush()\n    os._exit(code)", "output": "Exit without garbage collection, this speeds up exit by about 10ms for\n    things like bash completion.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_indicator_DMI(DataFrame, M1=14, M2=6):\n    \"\"\"\n    \n    \"\"\"\n    HIGH = DataFrame.high\n    LOW = DataFrame.low\n    CLOSE = DataFrame.close\n    OPEN = DataFrame.open\n\n    TR = SUM(MAX(MAX(HIGH-LOW, ABS(HIGH-REF(CLOSE, 1))),\n                 ABS(LOW-REF(CLOSE, 1))), M1)\n    HD = HIGH-REF(HIGH, 1)\n    LD = REF(LOW, 1)-LOW\n    DMP = SUM(IFAND(HD>0,HD>LD,HD,0), M1)\n    DMM = SUM(IFAND(LD>0,LD>HD,LD,0), M1)\n    DI1 = DMP*100/TR\n    DI2 = DMM*100/TR\n    ADX = MA(ABS(DI2-DI1)/(DI1+DI2)*100, M2)\n    ADXR = (ADX+REF(ADX, M2))/2\n\n    return pd.DataFrame({\n        'DI1': DI1, 'DI2': DI2,\n        'ADX': ADX, 'ADXR': ADXR\n    })", "output": "\u8d8b\u5411\u6307\u6807 DMI", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nodeDumpOutput(self, buf, cur, level, format, encoding):\n        \"\"\" \"\"\"\n        if buf is None: buf__o = None\n        else: buf__o = buf._o\n        if cur is None: cur__o = None\n        else: cur__o = cur._o\n        libxml2mod.xmlNodeDumpOutput(buf__o, self._o, cur__o, level, format, encoding)", "output": "Dump an XML node, recursive behaviour, children are printed\n          too. Note that @format = 1 provide node indenting only if\n          xmlIndentTreeOutput = 1 or xmlKeepBlanksDefault(0) was\n           called", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shakeshake(xs, equal_grad=False):\n  \"\"\"\"\"\"\n  if len(xs) == 1:\n    return xs[0]\n  div = (len(xs) + 1) // 2\n  arg1 = shakeshake(xs[:div], equal_grad=equal_grad)\n  arg2 = shakeshake(xs[div:], equal_grad=equal_grad)\n  if equal_grad:\n    return shakeshake2_eqgrad(arg1, arg2)\n  return shakeshake2(arg1, arg2)", "output": "Multi-argument shake-shake, currently approximated by sums of 2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def html_format(data, out, opts=None, **kwargs):\n    '''\n    \n    '''\n    ansi_escaped_string = string_format(data, out, opts, **kwargs)\n    return ansi_escaped_string.replace(' ', '&nbsp;').replace('\\n', '<br />')", "output": "Return the formatted string as HTML.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def avail_locations(call=None):\n    '''\n    \n    '''\n    if call == 'action':\n        raise SaltCloudException(\n            'The avail_locations function must be called with -f or --function.'\n        )\n\n    response = _query('avail', 'datacenters')\n\n    ret = {}\n    for item in response['DATA']:\n        name = item['LOCATION']\n        ret[name] = item\n\n    return ret", "output": "Return available Linode datacenter locations.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud --list-locations my-linode-config\n        salt-cloud -f avail_locations my-linode-config", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def text_embedding_column(key, module_spec, trainable=False):\n  \"\"\"\n  \"\"\"\n  module_spec = module.as_module_spec(module_spec)\n  _check_module_is_text_embedding(module_spec)\n  return _TextEmbeddingColumn(key=key, module_spec=module_spec,\n                              trainable=trainable)", "output": "Uses a Module to construct a dense representation from a text feature.\n\n  This feature column can be used on an input feature whose values are strings\n  of arbitrary size.\n\n  The result of this feature column is the result of passing its `input`\n  through the module `m` instantiated from `module_spec`, as per\n  `result = m(input)`. The `result` must have dtype float32 and shape\n  `[batch_size, num_features]` with a known value of num_features.\n\n  Example:\n\n  ```python\n    comment = text_embedding_column(\"comment\", \"/tmp/text-module\")\n    feature_columns = [comment, ...]\n    ...\n    features = {\n      \"comment\": np.array([\"wow, much amazing\", \"so easy\", ...]),\n      ...\n    }\n    labels = np.array([[1], [0], ...])\n    # If running TF 2.x, use `tf.compat.v1.estimator.inputs.numpy_input_fn`\n    input_fn = tf.estimator.inputs.numpy_input_fn(features, labels,\n                                                  shuffle=True)\n    estimator = tf.estimator.DNNClassifier(hidden_units, feature_columns)\n    estimator.train(input_fn, max_steps=100)\n  ```\n\n  Args:\n    key: A string or `_FeatureColumn` identifying the text feature.\n    module_spec: A ModuleSpec defining the Module to instantiate or a path where\n      to load a ModuleSpec via `load_module_spec`\n    trainable: Whether or not the Module is trainable. False by default,\n      meaning the pre-trained weights are frozen. This is different from the\n      ordinary tf.feature_column.embedding_column(), but that one is intended\n      for training from scratch.\n\n  Returns:\n    `_DenseColumn` that converts from text input.\n\n  Raises:\n     ValueError: if module_spec is not suitable for use in this feature column.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def call_binop(self, context, operator, left, right):\n        \"\"\"\n        \"\"\"\n        return self.binop_table[operator](left, right)", "output": "For intercepted binary operator calls (:meth:`intercepted_binops`)\n        this function is executed instead of the builtin operator.  This can\n        be used to fine tune the behavior of certain operators.\n\n        .. versionadded:: 2.6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shutdown_abort():\n    '''\n    \n    '''\n    try:\n        win32api.AbortSystemShutdown('127.0.0.1')\n        return True\n    except pywintypes.error as exc:\n        (number, context, message) = exc.args\n        log.error('Failed to abort system shutdown')\n        log.error('nbr: %s', number)\n        log.error('ctx: %s', context)\n        log.error('msg: %s', message)\n        return False", "output": "Abort a shutdown. Only available while the dialog box is being\n    displayed to the user. Once the shutdown has initiated, it cannot be\n    aborted.\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' system.shutdown_abort", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_string(self, content, destination_s3_path, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        self._check_deprecated_argument(**kwargs)\n        (bucket, key) = self._path_to_bucket_and_key(destination_s3_path)\n\n        # put the file\n        self.s3.meta.client.put_object(\n            Key=key, Bucket=bucket, Body=content, **kwargs)", "output": "Put a string to an S3 path.\n        :param content: Data str\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto3 function `put_object`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check(name, port=None, **kwargs):\n\n    '''\n    \n\n    '''\n\n    # set name to host as required by the module\n    host = name\n\n    ret = {'name': name,\n           'result': True,\n           'changes': {},\n           'comment': ''}\n\n    if 'test' not in kwargs:\n        kwargs['test'] = __opts__.get('test', False)\n\n    # check the connection\n    if kwargs['test']:\n        ret['result'] = True\n        ret['comment'] = 'The connection will be tested'\n    else:\n        results = __salt__['network.connect'](host, port, **kwargs)\n        ret['result'] = results['result']\n        ret['comment'] = results['comment']\n\n    return ret", "output": "Checks if there is an open connection from the minion to the defined\n    host on a specific port.\n\n    name\n      host name or ip address to test connection to\n\n    port\n      The port to test the connection on\n\n    kwargs\n      Additional parameters, parameters allowed are:\n        proto (tcp or udp)\n        family (ipv4 or ipv6)\n        timeout\n\n    .. code-block:: yaml\n\n      testgoogle:\n        firewall.check:\n          - name: 'google.com'\n          - port: 80\n          - proto: 'tcp'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def entropy(string):\n    \"\"\"\"\"\"\n    entropy = 0\n    for number in range(256):\n        result = float(string.encode('utf-8').count(\n            chr(number))) / len(string.encode('utf-8'))\n        if result != 0:\n            entropy = entropy - result * math.log(result, 2)\n    return entropy", "output": "Calculate the entropy of a string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _file_dict(self, fn_):\n        '''\n        \n        '''\n        if not os.path.isfile(fn_):\n            err = 'The referenced file, {0} is not available.'.format(fn_)\n            sys.stderr.write(err + '\\n')\n            sys.exit(42)\n        with salt.utils.files.fopen(fn_, 'r') as fp_:\n            data = fp_.read()\n        return {fn_: data}", "output": "Take a path and return the contents of the file as a string", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_median_session_metrics(session_group, aggregation_metric):\n  \"\"\"\n  \"\"\"\n  measurements = sorted(_measurements(session_group, aggregation_metric),\n                        key=operator.attrgetter('metric_value.value'))\n  median_session = measurements[(len(measurements) - 1) // 2].session_index\n  del session_group.metric_values[:]\n  session_group.metric_values.MergeFrom(\n      session_group.sessions[median_session].metric_values)", "output": "Sets the metrics for session_group to those of its \"median session\".\n\n  The median session is the session in session_group with the median value\n  of the metric given by 'aggregation_metric'. The median is taken over the\n  subset of sessions in the group whose 'aggregation_metric' was measured\n  at the largest training step among the sessions in the group.\n\n  Args:\n    session_group: A SessionGroup protobuffer.\n    aggregation_metric: A MetricName protobuffer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def generate(self, x, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    # Parse and save attack-specific parameters\n    assert self.parse_params(**kwargs)\n    labels, _nb_classes = self.get_or_guess_labels(x, kwargs)\n    return self.fgm(x, labels=labels, targeted=(self.y_target is not None))", "output": "Generates the adversarial sample for the given input.\n    :param x: The model's inputs.\n    :param eps: (optional float) attack step size (input variation)\n    :param ord: (optional) Order of the norm (mimics NumPy).\n                Possible values: np.inf, 1 or 2.\n    :param y: (optional) A tf variable` with the model labels. Only provide\n              this parameter if you'd like to use true labels when crafting\n              adversarial samples. Otherwise, model predictions are used as\n              labels to avoid the \"label leaking\" effect (explained in this\n              paper: https://arxiv.org/abs/1611.01236). Default is None.\n              Labels should be one-hot-encoded.\n    :param y_target: (optional) A tf variable` with the labels to target.\n                        Leave y_target=None if y is also set.\n                        Labels should be one-hot-encoded.\n    :param clip_min: (optional float) Minimum input component value\n    :param clip_max: (optional float) Maximum input component value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clear(self) -> None:\n        \"\"\"\n        \n        \"\"\"\n        self._best_so_far = None\n        self._epochs_with_no_improvement = 0\n        self._is_best_so_far = True\n        self._epoch_number = 0\n        self.best_epoch = None", "output": "Clears out the tracked metrics, but keeps the patience and should_decrease settings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def toggle_hscrollbar(self, checked):\r\n        \"\"\"\"\"\"\r\n        self.parent_widget.sig_option_changed.emit('show_hscrollbar', checked)\r\n        self.show_hscrollbar = checked\r\n        self.header().setStretchLastSection(not checked)\r\n        self.header().setHorizontalScrollMode(QAbstractItemView.ScrollPerPixel)\r\n        try:\r\n            self.header().setSectionResizeMode(QHeaderView.ResizeToContents)\r\n        except:  # support for qtpy<1.2.0\r\n            self.header().setResizeMode(QHeaderView.ResizeToContents)", "output": "Toggle horizontal scrollbar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _make_reserved_tokens_re(reserved_tokens):\n  \"\"\"\"\"\"\n  if not reserved_tokens:\n    return None\n  escaped_tokens = [_re_escape(rt) for rt in reserved_tokens]\n  pattern = \"(%s)\" % \"|\".join(escaped_tokens)\n  reserved_tokens_re = _re_compile(pattern)\n  return reserved_tokens_re", "output": "Constructs compiled regex to parse out reserved tokens.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_vertex_tuple(s):\n  \"\"\"\"\"\"\n  vt = [0, 0, 0]\n  for i, c in enumerate(s.split('/')):\n    if c:\n      vt[i] = int(c)\n  return tuple(vt)", "output": "Parse vertex indices in '/' separated form (like 'i/j/k', 'i//k' ...).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _build_master(cls):\n        \"\"\"\n        \n        \"\"\"\n        ws = cls()\n        try:\n            from __main__ import __requires__\n        except ImportError:\n            # The main program does not list any requirements\n            return ws\n\n        # ensure the requirements are met\n        try:\n            ws.require(__requires__)\n        except VersionConflict:\n            return cls._build_from_requirements(__requires__)\n\n        return ws", "output": "Prepare the master working set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def phi_symbolic(self, i):\n        \"\"\" \n        \"\"\"\n        if self.phi_symbolics[i] is None:\n\n            # replace the gradients for all the non-linear activations\n            # we do this by hacking our way into the registry (TODO: find a public API for this if it exists)\n            reg = tf_ops._gradient_registry._registry\n            for n in op_handlers:\n                if n in reg:\n                    self.orig_grads[n] = reg[n][\"type\"]\n                    if op_handlers[n] is not passthrough:\n                        reg[n][\"type\"] = self.custom_grad\n                elif n in self.used_types:\n                    raise Exception(n + \" was used in the model but is not in the gradient registry!\")\n            # In TensorFlow 1.10 they started pruning out nodes that they think can't be backpropped\n            # unfortunately that includes the index of embedding layers so we disable that check here\n            if hasattr(tf_gradients_impl, \"_IsBackpropagatable\"):\n                orig_IsBackpropagatable = tf_gradients_impl._IsBackpropagatable\n                tf_gradients_impl._IsBackpropagatable = lambda tensor: True\n            \n            # define the computation graph for the attribution values using custom a gradient-like computation\n            try:\n                out = self.model_output[:,i] if self.multi_output else self.model_output\n                self.phi_symbolics[i] = tf.gradients(out, self.model_inputs)\n\n            finally:\n\n                # reinstate the backpropagatable check\n                if hasattr(tf_gradients_impl, \"_IsBackpropagatable\"):\n                    tf_gradients_impl._IsBackpropagatable = orig_IsBackpropagatable\n\n                # restore the original gradient definitions\n                for n in op_handlers:\n                    if n in reg:\n                        reg[n][\"type\"] = self.orig_grads[n]\n        return self.phi_symbolics[i]", "output": "Get the SHAP value computation graph for a given model output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\"\"\"\n        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n        return self.sublayer[1](x, self.feed_forward)", "output": "Follow Figure 1 (left) for connections.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def filter_event(tag, data, defaults):\n    '''\n    \n    '''\n    ret = {}\n    keys = []\n    use_defaults = True\n\n    for ktag in __opts__.get('filter_events', {}):\n        if tag != ktag:\n            continue\n        keys = __opts__['filter_events'][ktag]['keys']\n        use_defaults = __opts__['filter_events'][ktag].get('use_defaults', True)\n\n    if use_defaults is False:\n        defaults = []\n\n    # For PY3, if something like \".keys()\" or \".values()\" is used on a dictionary,\n    # it returns a dict_view and not a list like in PY2. \"defaults\" should be passed\n    # in with the correct data type, but don't stack-trace in case it wasn't.\n    if not isinstance(defaults, list):\n        defaults = list(defaults)\n\n    defaults = list(set(defaults + keys))\n\n    for key in defaults:\n        if key in data:\n            ret[key] = data[key]\n\n    return ret", "output": "Accept a tag, a dict and a list of default keys to return from the dict, and\n    check them against the cloud configuration for that tag", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def wrap(cls, value):\n        ''' \n\n        '''\n        if isinstance(value, dict):\n            if isinstance(value, PropertyValueColumnData):\n                return value\n            else:\n                return PropertyValueColumnData(value)\n        else:\n            return value", "output": "Some property types need to wrap their values in special containers, etc.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_feature(feature, remove_payload=False, image=None, restart=False):\n    '''\n    \n    '''\n    cmd = ['DISM',\n           '/Quiet',\n           '/Image:{0}'.format(image) if image else '/Online',\n           '/Disable-Feature',\n           '/FeatureName:{0}'.format(feature)]\n\n    if remove_payload:\n        cmd.append('/Remove')\n    if not restart:\n        cmd.append('/NoRestart')\n\n    return __salt__['cmd.run_all'](cmd)", "output": "Disables the feature.\n\n    Args:\n        feature (str): The feature to uninstall\n        remove_payload (Optional[bool]): Remove the feature's payload. Must\n            supply source when enabling in the future.\n        image (Optional[str]): The path to the root directory of an offline\n            Windows image. If `None` is passed, the running operating system is\n            targeted. Default is None.\n        restart (Optional[bool]): Reboot the machine if required by the install\n\n    Returns:\n        dict: A dictionary containing the results of the command\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dism.remove_feature NetFx3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def substitute_any_type(self, basic_types: Set[BasicType]) -> List[Type]:\n        \"\"\"\n        \n        \"\"\"\n        substitutions = []\n        for first_type in substitute_any_type(self.first, basic_types):\n            for second_type in substitute_any_type(self.second, basic_types):\n                substitutions.append(self.__class__(first_type, second_type))\n        return substitutions", "output": "Takes a set of ``BasicTypes`` and replaces any instances of ``ANY_TYPE`` inside this\n        complex type with each of those basic types.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_password(vm_):\n    \n    '''\n    return config.get_cloud_config_value(\n        'password', vm_, __opts__,\n        default=config.get_cloud_config_value(\n            'passwd', vm_, __opts__,\n            search_global=False\n        ),\n        search_global=False\n    )", "output": "r'''\n    Return the password to use for a VM.\n\n    vm\\_\n        The configuration to obtain the password from.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bundle_models(models):\n    \"\"\" \"\"\"\n    custom_models = _get_custom_models(models)\n    if custom_models is None:\n        return None\n\n    key = calc_cache_key(custom_models)\n    bundle = _bundle_cache.get(key, None)\n    if bundle is None:\n        try:\n            _bundle_cache[key] = bundle = _bundle_models(custom_models)\n        except CompilationError as error:\n            print(\"Compilation failed:\", file=sys.stderr)\n            print(str(error), file=sys.stderr)\n            sys.exit(1)\n    return bundle", "output": "Create a bundle of selected `models`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xmlrpc_run(self, port=24444, bind='127.0.0.1', logRequests=False):\n        ''''''\n        import umsgpack\n        from pyspider.libs.wsgi_xmlrpc import WSGIXMLRPCApplication\n        try:\n            from xmlrpc.client import Binary\n        except ImportError:\n            from xmlrpclib import Binary\n\n        application = WSGIXMLRPCApplication()\n\n        application.register_function(self.quit, '_quit')\n        application.register_function(self.size)\n\n        def sync_fetch(task):\n            result = self.sync_fetch(task)\n            result = Binary(umsgpack.packb(result))\n            return result\n        application.register_function(sync_fetch, 'fetch')\n\n        def dump_counter(_time, _type):\n            return self._cnt[_time].to_dict(_type)\n        application.register_function(dump_counter, 'counter')\n\n        import tornado.wsgi\n        import tornado.ioloop\n        import tornado.httpserver\n\n        container = tornado.wsgi.WSGIContainer(application)\n        self.xmlrpc_ioloop = tornado.ioloop.IOLoop()\n        self.xmlrpc_server = tornado.httpserver.HTTPServer(container, io_loop=self.xmlrpc_ioloop)\n        self.xmlrpc_server.listen(port=port, address=bind)\n        logger.info('fetcher.xmlrpc listening on %s:%s', bind, port)\n        self.xmlrpc_ioloop.start()", "output": "Run xmlrpc server", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ancestor(self, value):\n        \"\"\"\n        \"\"\"\n        if not isinstance(value, Key):\n            raise TypeError(\"Ancestor must be a Key\")\n        self._ancestor = value", "output": "Set the ancestor for the query\n\n        :type value: :class:`~google.cloud.datastore.key.Key`\n        :param value: the new ancestor key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_enabled(self):\n        \"\"\"\n\n        \"\"\"\n        if self.name in settings.exclude_rules:\n            return False\n        elif self.name in settings.rules:\n            return True\n        elif self.enabled_by_default and ALL_ENABLED in settings.rules:\n            return True\n        else:\n            return False", "output": "Returns `True` when rule enabled.\n\n        :rtype: bool", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def logger_delete(self, project, logger_name):\n        \"\"\"\n        \"\"\"\n        path = \"projects/%s/logs/%s\" % (project, logger_name)\n        self._gapic_api.delete_log(path)", "output": "API call:  delete all entries in a logger via a DELETE request\n\n        :type project: str\n        :param project: ID of project containing the log entries to delete\n\n        :type logger_name: str\n        :param logger_name: name of logger containing the log entries to delete", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(backend=None):\n    '''\n    \n    '''\n    fileserver = salt.fileserver.Fileserver(__opts__)\n    fileserver.update(back=backend)\n    return True", "output": "Update the fileserver cache. If no backend is provided, then the cache for\n    all configured backends will be updated.\n\n    backend\n        Narrow fileserver backends to a subset of the enabled ones.\n\n        .. versionchanged:: 2015.5.0\n            If all passed backends start with a minus sign (``-``), then these\n            backends will be excluded from the enabled backends. However, if\n            there is a mix of backends with and without a minus sign (ex:\n            ``backend=-roots,git``) then the ones starting with a minus\n            sign will be disregarded.\n\n            Additionally, fileserver backends can now be passed as a\n            comma-separated list. In earlier versions, they needed to be passed\n            as a python list (ex: ``backend=\"['roots', 'git']\"``)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run fileserver.update\n        salt-run fileserver.update backend=roots,git", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_path_completion_type(cwords, cword, opts):\n    \"\"\"\n    \"\"\"\n    if cword < 2 or not cwords[cword - 2].startswith('-'):\n        return\n    for opt in opts:\n        if opt.help == optparse.SUPPRESS_HELP:\n            continue\n        for o in str(opt).split('/'):\n            if cwords[cword - 2].split('=')[0] == o:\n                if not opt.metavar or any(\n                        x in ('path', 'file', 'dir')\n                        for x in opt.metavar.split('/')):\n                    return opt.metavar", "output": "Get the type of path completion (``file``, ``dir``, ``path`` or None)\n\n    :param cwords: same as the environmental variable ``COMP_WORDS``\n    :param cword: same as the environmental variable ``COMP_CWORD``\n    :param opts: The available options to check\n    :return: path completion type (``file``, ``dir``, ``path`` or None)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def encode_record_with_schema_id(self, schema_id, record, is_key=False):\n        \"\"\"\n        \n        \"\"\"\n        serialize_err = KeySerializerError if is_key else ValueSerializerError\n\n        # use slow avro\n        if schema_id not in self.id_to_writers:\n            # get the writer + schema\n\n            try:\n                schema = self.registry_client.get_by_id(schema_id)\n                if not schema:\n                    raise serialize_err(\"Schema does not exist\")\n                self.id_to_writers[schema_id] = self._get_encoder_func(schema)\n            except ClientError:\n                exc_type, exc_value, exc_traceback = sys.exc_info()\n                raise serialize_err(repr(traceback.format_exception(exc_type, exc_value, exc_traceback)))\n\n        # get the writer\n        writer = self.id_to_writers[schema_id]\n        with ContextStringIO() as outf:\n            # Write the magic byte and schema ID in network byte order (big endian)\n            outf.write(struct.pack('>bI', MAGIC_BYTE, schema_id))\n\n            # write the record to the rest of the buffer\n            writer(record, outf)\n\n            return outf.getvalue()", "output": "Encode a record with a given schema id.  The record must\n        be a python dictionary.\n        :param int schema_id: integer ID\n        :param dict record: An object to serialize\n        :param bool is_key: If the record is a key\n        :returns: decoder function\n        :rtype: func", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def status(stats=False, config=False, internals=False, superblock=False, alldevs=False):\n    '''\n    \n    '''\n    bdevs = []\n    for _, links, _ in salt.utils.path.os_walk('/sys/block/'):\n        for block in links:\n            if 'bcache' in block:\n                continue\n\n            for spath, sdirs, _ in salt.utils.path.os_walk('/sys/block/{0}'.format(block), followlinks=False):\n                if 'bcache' in sdirs:\n                    bdevs.append(os.path.basename(spath))\n    statii = {}\n    for bcache in bdevs:\n        statii[bcache] = device(bcache, stats, config, internals, superblock)\n\n    cuuid = uuid()\n    cdev = _bdev()\n    if cdev:\n        count = 0\n        for dev in statii:\n            if dev != cdev:\n                # it's a backing dev\n                if statii[dev]['cache'] == cuuid:\n                    count += 1\n        statii[cdev]['attached_backing_devices'] = count\n\n        if not alldevs:\n            statii = statii[cdev]\n\n    return statii", "output": "Show the full status of the BCache system and optionally all it's involved devices\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt '*' bcache.status\n        salt '*' bcache.status stats=True\n        salt '*' bcache.status internals=True alldevs=True\n\n    :param stats: include statistics\n    :param config: include settings\n    :param internals: include internals\n    :param superblock: include superblock", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve(self, other: Type) -> Type:\n        \"\"\"\"\"\"\n        if not isinstance(other, NltkComplexType):\n            return None\n        resolved_second = NUMBER_TYPE.resolve(other.second)\n        if not resolved_second:\n            return None\n        return CountType(other.first)", "output": "See ``PlaceholderType.resolve``", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_bottleneck_file(bottleneck_path, image_lists, label_name, index,\n                           image_dir, category, sess, jpeg_data_tensor,\n                           decoded_image_tensor, resized_input_tensor,\n                           bottleneck_tensor):\n  \"\"\"\"\"\"\n  tf.logging.debug('Creating bottleneck at ' + bottleneck_path)\n  image_path = get_image_path(image_lists, label_name, index,\n                              image_dir, category)\n  if not tf.gfile.Exists(image_path):\n    tf.logging.fatal('File does not exist %s', image_path)\n  image_data = tf.gfile.GFile(image_path, 'rb').read()\n  try:\n    bottleneck_values = run_bottleneck_on_image(\n        sess, image_data, jpeg_data_tensor, decoded_image_tensor,\n        resized_input_tensor, bottleneck_tensor)\n  except Exception as e:\n    raise RuntimeError('Error during processing file %s (%s)' % (image_path,\n                                                                 str(e)))\n  bottleneck_string = ','.join(str(x) for x in bottleneck_values)\n  with open(bottleneck_path, 'w') as bottleneck_file:\n    bottleneck_file.write(bottleneck_string)", "output": "Create a single bottleneck file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_legacy_build_wheel_path(\n    names,  # type: List[str]\n    temp_dir,  # type: str\n    req,  # type: InstallRequirement\n    command_args,  # type: List[str]\n    command_output,  # type: str\n):\n    # type: (...) -> Optional[str]\n    \"\"\"\n    \n    \"\"\"\n    # Sort for determinism.\n    names = sorted(names)\n    if not names:\n        msg = (\n            'Legacy build of wheel for {!r} created no files.\\n'\n        ).format(req.name)\n        msg += format_command(command_args, command_output)\n        logger.warning(msg)\n        return None\n\n    if len(names) > 1:\n        msg = (\n            'Legacy build of wheel for {!r} created more than one file.\\n'\n            'Filenames (choosing first): {}\\n'\n        ).format(req.name, names)\n        msg += format_command(command_args, command_output)\n        logger.warning(msg)\n\n    return os.path.join(temp_dir, names[0])", "output": "Return the path to the wheel in the temporary build directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def apply_to_model(self, model):\n        ''' \n\n        '''\n        model.apply_theme(self._for_class(model.__class__))\n\n        # a little paranoia because it would be Bad(tm) to mess\n        # this up... would be nicer if python had a way to freeze\n        # the dict.\n        if len(_empty_dict) > 0:\n            raise RuntimeError(\"Somebody put stuff in _empty_dict\")", "output": "Apply this theme to a model.\n\n        .. warning::\n            Typically, don't call this method directly. Instead, set the theme\n            on the :class:`~bokeh.document.Document` the model is a part of.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def entropy_from_samples(samples, vec):\n    \"\"\"\n    \n    \"\"\"\n    samples_cat = tf.argmax(samples[:, :NUM_CLASS], axis=1, output_type=tf.int32)\n    samples_uniform = samples[:, NUM_CLASS:]\n    cat, uniform = get_distributions(vec[:, :NUM_CLASS], vec[:, NUM_CLASS:])\n\n    def neg_logprob(dist, sample, name):\n        nll = -dist.log_prob(sample)\n        # average over batch\n        return tf.reduce_sum(tf.reduce_mean(nll, axis=0), name=name)\n\n    entropies = [neg_logprob(cat, samples_cat, 'nll_cat'),\n                 neg_logprob(uniform, samples_uniform, 'nll_uniform')]\n    return entropies", "output": "Estimate H(x|s) ~= -E_{x \\sim P(x|s)}[\\log Q(x|s)], where x are samples, and Q is parameterized by vec.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_text_encoder(inputs,\n                             target_space,\n                             hparams,\n                             name=None):\n  \"\"\"\n  \"\"\"\n  with tf.variable_scope(name, default_name=\"transformer_text_encoder\"):\n    inputs = common_layers.flatten4d3d(inputs)\n    [\n        encoder_input,\n        encoder_self_attention_bias,\n        ed,\n    ] = transformer_layers.transformer_prepare_encoder(\n        inputs, target_space=target_space, hparams=hparams)\n    encoder_input = tf.nn.dropout(encoder_input, 1.0 - hparams.dropout)\n    encoder_output = transformer_layers.transformer_encoder(\n        encoder_input, encoder_self_attention_bias, hparams)\n    return encoder_output, ed", "output": "Transformer text encoder over inputs with unmasked full attention.\n\n  Args:\n    inputs: Tensor of shape [batch, length, 1, hparams.hidden_size].\n    target_space: int. Used for encoding inputs under a target space id.\n    hparams: HParams.\n    name: string, variable scope.\n\n  Returns:\n    encoder_output: Tensor of shape [batch, length, hparams.hidden_size].\n    ed: Tensor of shape [batch, 1, 1, length]. Encoder-decoder attention bias\n      for any padded tokens.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_intro(self):\r\n        \"\"\"\"\"\"\r\n        from IPython.core.usage import interactive_usage\r\n        self.main.help.show_rich_text(interactive_usage)", "output": "Show intro to IPython help", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_on_change(self, attr, *callbacks):\n        '''  '''\n        if len(callbacks) == 0:\n            raise ValueError(\"remove_on_change takes an attribute name and one or more callbacks, got only one parameter\")\n        _callbacks = self._callbacks.setdefault(attr, [])\n        for callback in callbacks:\n            _callbacks.remove(callback)", "output": "Remove a callback from this object", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_file_combobox(self, text, choices, option, default=NoDefault,\r\n                             tip=None, restart=False, filters=None,\r\n                             adjust_to_contents=False,\r\n                             default_line_edit=False):\r\n        \"\"\"\"\"\"\r\n        combobox = FileComboBox(self, adjust_to_contents=adjust_to_contents,\r\n                                default_line_edit=default_line_edit)\r\n        combobox.restart_required = restart\r\n        combobox.label_text = text\r\n        edit = combobox.lineEdit()\r\n        edit.label_text = text\r\n        edit.restart_required = restart\r\n        self.lineedits[edit] = (option, default)\r\n\r\n        if tip is not None:\r\n            combobox.setToolTip(tip)\r\n        combobox.addItems(choices)\r\n\r\n        msg = _('Invalid file path')\r\n        self.validate_data[edit] = (osp.isfile, msg)\r\n        browse_btn = QPushButton(ima.icon('FileIcon'), '', self)\r\n        browse_btn.setToolTip(_(\"Select file\"))\r\n        browse_btn.clicked.connect(lambda: self.select_file(edit, filters))\r\n\r\n        layout = QGridLayout()\r\n        layout.addWidget(combobox, 0, 0, 0, 9)\r\n        layout.addWidget(browse_btn, 0, 10)\r\n        layout.setContentsMargins(0, 0, 0, 0)\r\n        widget = QWidget(self)\r\n        widget.combobox = combobox\r\n        widget.browse_btn = browse_btn\r\n        widget.setLayout(layout)\r\n\r\n        return widget", "output": "choices: couples (name, key)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_shellwidget(self, shellwidget):\n        \"\"\"\n        \n        \"\"\"\n        shellwidget_id = id(shellwidget)\n        if shellwidget_id not in self.shellwidgets:\n            self.options_button.setVisible(True)\n            fig_browser = FigureBrowser(\n                self, options_button=self.options_button,\n                background_color=MAIN_BG_COLOR)\n            fig_browser.set_shellwidget(shellwidget)\n            fig_browser.setup(**self.get_settings())\n            fig_browser.sig_option_changed.connect(\n                self.sig_option_changed.emit)\n            fig_browser.thumbnails_sb.redirect_stdio.connect(\n                self.main.redirect_internalshell_stdio)\n            self.register_widget_shortcuts(fig_browser)\n            self.add_widget(fig_browser)\n            self.shellwidgets[shellwidget_id] = fig_browser\n            self.set_shellwidget_from_id(shellwidget_id)\n            return fig_browser", "output": "Register shell with figure explorer.\n\n        This function opens a new FigureBrowser for browsing the figures\n        in the shell.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_extra_loss(self, latent_means=None, latent_stds=None,\n                     true_frames=None, gen_frames=None):\n    \"\"\"\"\"\"\n    if not self.is_training:\n      return 0.0\n\n    vae_loss, d_vae_loss, d_gan_loss = 0.0, 0.0, 0.0\n    # Use sv2p's KL divergence computation.\n    if self.hparams.use_vae:\n      vae_loss = super(NextFrameSavpBase, self).get_extra_loss(\n          latent_means=latent_means, latent_stds=latent_stds)\n\n    if self.hparams.use_gan:\n      # Strip out the first context_frames for the true_frames\n      # Strip out the first context_frames - 1 for the gen_frames\n      context_frames = self.hparams.video_num_input_frames\n      true_frames = tf.stack(\n          tf.unstack(true_frames, axis=0)[context_frames:])\n\n      # discriminator for VAE.\n      if self.hparams.use_vae:\n        gen_enc_frames = tf.stack(\n            tf.unstack(gen_frames, axis=0)[context_frames-1:])\n        d_vae_loss = self.get_gan_loss(true_frames, gen_enc_frames, name=\"vae\")\n\n      # discriminator for GAN.\n      gen_prior_frames = tf.stack(\n          tf.unstack(self.gen_prior_video, axis=0)[context_frames-1:])\n      d_gan_loss = self.get_gan_loss(true_frames, gen_prior_frames, name=\"gan\")\n\n    return (\n        vae_loss + self.hparams.gan_loss_multiplier * d_gan_loss +\n        self.hparams.gan_vae_loss_multiplier * d_vae_loss)", "output": "Gets extra loss from VAE and GAN.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pre_fork(self, process_manager):\n        '''\n        \n        '''\n        salt.transport.mixins.auth.AESReqServerMixin.pre_fork(self, process_manager)\n        if USE_LOAD_BALANCER:\n            self.socket_queue = multiprocessing.Queue()\n            process_manager.add_process(\n                LoadBalancerServer, args=(self.opts, self.socket_queue)\n            )\n        elif not salt.utils.platform.is_windows():\n            self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            _set_tcp_keepalive(self._socket, self.opts)\n            self._socket.setblocking(0)\n            self._socket.bind((self.opts['interface'], int(self.opts['ret_port'])))", "output": "Pre-fork we need to create the zmq router device", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def bail_out(self, message, from_error=False):\n        \"\"\"\n        \n        \"\"\"\n        if from_error or self.transport is None or self.transport.is_closing():\n            logger.error(\n                \"Transport closed @ %s and exception \"\n                \"experienced during error handling\",\n                (\n                    self.transport.get_extra_info(\"peername\")\n                    if self.transport is not None\n                    else \"N/A\"\n                ),\n            )\n            logger.debug(\"Exception:\", exc_info=True)\n        else:\n            self.write_error(ServerError(message))\n            logger.error(message)", "output": "In case if the transport pipes are closed and the sanic app encounters\n        an error while writing data to the transport pipe, we log the error\n        with proper details.\n\n        :param message: Error message to display\n        :param from_error: If the bail out was invoked while handling an\n            exception scenario.\n\n        :type message: str\n        :type from_error: bool\n\n        :return: None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_host_ipv4addr_info(ipv4addr=None, mac=None,\n                                        discovered_data=None,\n                                        return_fields=None, **api_opts):\n    '''\n    \n    '''\n    infoblox = _get_infoblox(**api_opts)\n    return infoblox.get_host_ipv4addr_object(ipv4addr, mac, discovered_data, return_fields)", "output": "Get host ipv4addr information\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call infoblox.get_ipv4addr ipv4addr=123.123.122.12\n        salt-call infoblox.get_ipv4addr mac=00:50:56:84:6e:ae\n        salt-call infoblox.get_ipv4addr mac=00:50:56:84:6e:ae return_fields=host return_fields='mac,host,configure_for_dhcp,ipv4addr'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def extract_kwargs(names:Collection[str], kwargs:KWArgs):\n    \"\"\n    new_kwargs = {}\n    for arg_name in names:\n        if arg_name in kwargs:\n            arg_val = kwargs.pop(arg_name)\n            new_kwargs[arg_name] = arg_val\n    return new_kwargs, kwargs", "output": "Extract the keys in `names` from the `kwargs`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_client():\n    \"\"\"\n    \"\"\"\n    if client is not None:\n        return\n\n    global _mysql_kwargs, _table_name\n    _mysql_kwargs = {\n        'host': __opts__.get('mysql.host', '127.0.0.1'),\n        'user': __opts__.get('mysql.user', None),\n        'passwd': __opts__.get('mysql.password', None),\n        'db': __opts__.get('mysql.database', _DEFAULT_DATABASE_NAME),\n        'port': __opts__.get('mysql.port', 3306),\n        'unix_socket': __opts__.get('mysql.unix_socket', None),\n        'connect_timeout': __opts__.get('mysql.connect_timeout', None),\n        'autocommit': True,\n    }\n    _table_name = __opts__.get('mysql.table_name', _table_name)\n    # TODO: handle SSL connection parameters\n\n    for k, v in _mysql_kwargs.items():\n        if v is None:\n            _mysql_kwargs.pop(k)\n    kwargs_copy = _mysql_kwargs.copy()\n    kwargs_copy['passwd'] = \"<hidden>\"\n    log.info(\"mysql_cache: Setting up client with params: %r\", kwargs_copy)\n    # The MySQL client is created later on by run_query\n    _create_table()", "output": "Initialize connection and create table if needed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stack_annotations(annotations_sarray):\n    \"\"\"\n    \n    \"\"\"\n    _raise_error_if_not_sarray(annotations_sarray, variable_name='annotations_sarray')\n    sf = _tc.SFrame({'annotations': annotations_sarray}).add_row_number('row_id')\n    sf = sf.stack('annotations', new_column_name='annotations', drop_na=True)\n    if len(sf) == 0:\n        cols = ['row_id', 'confidence', 'label', 'height', 'width', 'x', 'y']\n        return _tc.SFrame({k: [] for k in cols})\n    sf = sf.unpack('annotations', column_name_prefix='')\n    sf = sf.unpack('coordinates', column_name_prefix='')\n    del sf['type']\n    return sf", "output": "Converts object detection annotations (ground truth or predictions) to\n    stacked format (an `SFrame` where each row is one object instance).\n\n    Parameters\n    ----------\n    annotations_sarray: SArray\n        An `SArray` with unstacked predictions, exactly formatted as the\n        annotations column when training an object detector or when making\n        predictions.\n\n    Returns\n    -------\n    annotations_sframe: An `SFrame` with stacked annotations.\n\n    See also\n    --------\n    unstack_annotations\n\n    Examples\n    --------\n    Predictions are returned by the object detector in unstacked format:\n\n    >>> predictions = detector.predict(images)\n\n    By converting it to stacked format, it is easier to get an overview of\n    object instances:\n\n    >>> turicreate.object_detector.util.stack_annotations(predictions)\n    Data:\n    +--------+------------+-------+-------+-------+-------+--------+\n    | row_id | confidence | label |   x   |   y   | width | height |\n    +--------+------------+-------+-------+-------+-------+--------+\n    |   0    |    0.98    |  dog  | 123.0 | 128.0 |  80.0 | 182.0  |\n    |   0    |    0.67    |  cat  | 150.0 | 183.0 | 129.0 | 101.0  |\n    |   1    |    0.8     |  dog  |  50.0 | 432.0 |  65.0 |  98.0  |\n    +--------+------------+-------+-------+-------+-------+--------+\n    [3 rows x 7 columns]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rest_post(url, data, timeout, show_error=False):\n    ''''''\n    try:\n        response = requests.post(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                 data=data, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None", "output": "Call rest post method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def teardown_tempdir(dir):\n    \n    '''\n\n    if ssh_conn:\n        delete_tree(dir)\n\n    assert_valid_dir(dir)\n    shutil.rmtree(dir)", "output": "r'''\n    Cleanup temporary directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _insert(self, key, records, count):\n        \"\"\"  \"\"\"\n        if key not in self.records:\n            assert key not in self.counts\n            self.records[key] = records\n            self.counts[key] = count\n        else:\n            self.records[key] = np.vstack((self.records[key], records))\n            assert key in self.counts\n            self.counts[key] += count", "output": "Insert records according to key", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def json_iso_dttm_ser(obj, pessimistic: Optional[bool] = False):\n    \"\"\"\n    \n    \"\"\"\n    val = base_json_conv(obj)\n    if val is not None:\n        return val\n    if isinstance(obj, (datetime, date, time, pd.Timestamp)):\n        obj = obj.isoformat()\n    else:\n        if pessimistic:\n            return 'Unserializable [{}]'.format(type(obj))\n        else:\n            raise TypeError(\n                'Unserializable object {} of type {}'.format(obj, type(obj)))\n    return obj", "output": "json serializer that deals with dates\n\n    >>> dttm = datetime(1970, 1, 1)\n    >>> json.dumps({'dttm': dttm}, default=json_iso_dttm_ser)\n    '{\"dttm\": \"1970-01-01T00:00:00\"}'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_token(url, email, secret_key):\n    '''\n    \n    '''\n    url = urlparse.urljoin(url, 'authenticate')\n    data_dict = {\"email\": email, \"secret_key\": secret_key}\n    query = salt.utils.http.query(url, data=data_dict, method='POST',\n                                  decode=True)\n    error = query.get('error')\n    if error:\n        log.error('Cannot obtain NSoT authentication token due to: %s.', error)\n        log.debug('Please verify NSoT URL %s is reachable and email %s is valid', url, email)\n        return False\n    else:\n        log.debug('successfully obtained token from nsot!')\n        return query['dict'].get('auth_token')", "output": "retrieve the auth_token from nsot\n\n    :param url: str\n    :param email: str\n    :param secret_key: str\n    :return: str", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_auto_login(name, password):\n    '''\n    \n    '''\n    # Make the entry into the defaults file\n    cmd = ['defaults',\n           'write',\n           '/Library/Preferences/com.apple.loginwindow.plist',\n           'autoLoginUser',\n           name]\n    __salt__['cmd.run'](cmd)\n    current = get_auto_login()\n\n    # Create/Update the kcpassword file with an obfuscated password\n    o_password = _kcpassword(password=password)\n    with salt.utils.files.set_umask(0o077):\n        with salt.utils.files.fopen('/etc/kcpassword', 'w' if six.PY2 else 'wb') as fd:\n            fd.write(o_password)\n\n    return current if isinstance(current, bool) else current.lower() == name.lower()", "output": ".. versionadded:: 2016.3.0\n\n    Configures the machine to auto login with the specified user\n\n    Args:\n\n        name (str): The user account use for auto login\n\n        password (str): The password to user for auto login\n\n            .. versionadded:: 2017.7.3\n\n    Returns:\n        bool: ``True`` if successful, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.enable_auto_login stevej", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _find_frame(stack, start=0):\n  \"\"\"\"\"\"\n  # We want to find the first place where the layer was called\n  # that is *not* an __init__ function of an inheriting layer.\n  frame = inspect.getframeinfo(stack[start][0])\n  # If we are in an init, move on.\n  if frame.function == '__init__':\n    return _find_frame(stack, start + 1)\n  return frame", "output": "Find the frame with the caller on the stack.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(df, path):\n        \"\"\"\n        \n        \"\"\"\n        buffer = []\n        size = _reset_df_and_get_size(df)\n        with get_tqdm(total=size) as pbar:\n            for dp in df:\n                buffer.append(dp)\n                pbar.update()\n        np.savez_compressed(path, buffer=np.asarray(buffer, dtype=np.object))", "output": "Args:\n            df (DataFlow): the DataFlow to serialize.\n            path (str): output npz file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_setup() -> Tuple[PackagesType, PackagesType, Set[str], Set[str]]:\n    \"\"\"\"\"\"\n    essential_packages: PackagesType = {}\n    test_packages: PackagesType = {}\n    essential_duplicates: Set[str] = set()\n    test_duplicates: Set[str] = set()\n\n    with open('setup.py') as setup_file:\n        contents = setup_file.read()\n\n    # Parse out essential packages.\n    package_string = re.search(r\"\"\"install_requires=\\[[\\s\\n]*['\"](.*?)['\"],?[\\s\\n]*\\]\"\"\",\n                               contents, re.DOTALL).groups()[0].strip()\n    for package in re.split(r\"\"\"['\"],[\\s\\n]+['\"]\"\"\", package_string):\n        module, version = parse_package(package)\n        if module in essential_packages:\n            essential_duplicates.add(module)\n        else:\n            essential_packages[module] = version\n\n    # Parse packages only needed for testing.\n    package_string = re.search(r\"\"\"tests_require=\\[[\\s\\n]*['\"](.*?)['\"],?[\\s\\n]*\\]\"\"\",\n                               contents, re.DOTALL).groups()[0].strip()\n    for package in re.split(r\"\"\"['\"],[\\s\\n]+['\"]\"\"\", package_string):\n        module, version = parse_package(package)\n        if module in test_packages:\n            test_duplicates.add(module)\n        else:\n            test_packages[module] = version\n\n    return essential_packages, test_packages, essential_duplicates, test_duplicates", "output": "Parse all dependencies out of the setup.py script.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_id(name=None, cidr=None, tags=None, region=None, key=None, keyid=None,\n           profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        return {'id': _get_id(vpc_name=name, cidr=cidr, tags=tags, region=region,\n                              key=key, keyid=keyid, profile=profile)}\n    except BotoServerError as e:\n        return {'error': __utils__['boto.get_error'](e)}", "output": "Given VPC properties, return the VPC id if a match is found.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.get_id myvpc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autoencoder_basic():\n  \"\"\"\"\"\"\n  hparams = common_hparams.basic_params1()\n  hparams.optimizer = \"adam\"\n  hparams.learning_rate_constant = 0.0002\n  hparams.learning_rate_warmup_steps = 500\n  hparams.learning_rate_schedule = \"constant * linear_warmup\"\n  hparams.label_smoothing = 0.0\n  hparams.batch_size = 128\n  hparams.hidden_size = 64\n  hparams.num_hidden_layers = 5\n  hparams.initializer = \"uniform_unit_scaling\"\n  hparams.initializer_gain = 1.0\n  hparams.weight_decay = 0.0\n  hparams.kernel_height = 4\n  hparams.kernel_width = 4\n  hparams.dropout = 0.05\n  hparams.add_hparam(\"max_hidden_size\", 1024)\n  hparams.add_hparam(\"bottleneck_bits\", 128)\n  hparams.add_hparam(\"bottleneck_shared_bits\", 0)\n  hparams.add_hparam(\"bottleneck_shared_bits_start_warmup\", 0)\n  hparams.add_hparam(\"bottleneck_shared_bits_stop_warmup\", 0)\n  hparams.add_hparam(\"bottleneck_noise\", 0.1)\n  hparams.add_hparam(\"bottleneck_warmup_steps\", 2000)\n  hparams.add_hparam(\"sample_height\", 32)\n  hparams.add_hparam(\"sample_width\", 32)\n  hparams.add_hparam(\"discriminator_batchnorm\", True)\n  hparams.add_hparam(\"num_sliced_vecs\", 20000)\n  hparams.add_hparam(\"sliced_do_tanh\", int(True))\n  hparams.add_hparam(\"discriminator_size\", 256)\n  hparams.add_hparam(\"discriminator_kernel_size\", 6)\n  hparams.add_hparam(\"discriminator_strides\", 4)\n  hparams.add_hparam(\"discriminator_pure_mean\", int(False))\n  hparams.add_hparam(\"code_loss_factor\", 1.0)\n  hparams.add_hparam(\"gan_codes_warmup_steps\", 16000)\n  hparams.add_hparam(\"gan_loss_factor\", 0.0)\n  hparams.add_hparam(\"bottleneck_l2_factor\", 0.05)\n  hparams.add_hparam(\"gumbel_temperature\", 0.5)\n  hparams.add_hparam(\"gumbel_noise_factor\", 0.5)\n  hparams.add_hparam(\"vq_temperature\", 0.001)\n  hparams.add_hparam(\"use_vq_loss\", int(False))\n  hparams.add_hparam(\"discriminator\", \"double\")\n  return hparams", "output": "Basic autoencoder model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ext_pillar(minion_id, pillar, **kwargs):\n    '''\n    \n    '''\n\n    # If reclass is installed, __virtual__ put it onto the search path, so we\n    # don't need to protect against ImportError:\n    # pylint: disable=3rd-party-module-not-gated\n    from reclass.adapters.salt import ext_pillar as reclass_ext_pillar\n    from reclass.errors import ReclassException\n    # pylint: enable=3rd-party-module-not-gated\n\n    try:\n        # the source path we used above isn't something reclass needs to care\n        # about, so filter it:\n        filter_out_source_path_option(kwargs)\n\n        # if no inventory_base_uri was specified, initialize it to the first\n        # file_roots of class 'base' (if that exists):\n        set_inventory_base_uri_default(__opts__, kwargs)\n\n        # I purposely do not pass any of __opts__ or __salt__ or __grains__\n        # to reclass, as I consider those to be Salt-internal and reclass\n        # should not make any assumptions about it.\n        return reclass_ext_pillar(minion_id, pillar, **kwargs)\n\n    except TypeError as e:\n        if 'unexpected keyword argument' in six.text_type(e):\n            arg = six.text_type(e).split()[-1]\n            raise SaltInvocationError('ext_pillar.reclass: unexpected option: '\n                                      + arg)\n        else:\n            raise\n\n    except KeyError as e:\n        if 'id' in six.text_type(e):\n            raise SaltInvocationError('ext_pillar.reclass: __opts__ does not '\n                                      'define minion ID')\n        else:\n            raise\n\n    except ReclassException as e:\n        raise SaltInvocationError('ext_pillar.reclass: {0}'.format(e))", "output": "Obtain the Pillar data from **reclass** for the given ``minion_id``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def server_show(server_id, profile=None, **kwargs):\n    '''\n    \n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.server_show(server_id)", "output": "Return detailed information for an active server\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.server_show <server_id>", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_tokens_to_ids(self, tokens):\n        \"\"\"\"\"\"\n        ids = []\n        for token in tokens:\n            ids.append(self.vocab[token])\n        if len(ids) > self.max_len:\n            logger.warning(\n                \"Token indices sequence length is longer than the specified maximum \"\n                \" sequence length for this BERT model ({} > {}). Running this\"\n                \" sequence through BERT will result in indexing errors\".format(len(ids), self.max_len)\n            )\n        return ids", "output": "Converts a sequence of tokens into ids using the vocab.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def merge_diff(initial_config=None,\n               initial_path=None,\n               merge_config=None,\n               merge_path=None,\n               saltenv='base'):\n    '''\n    \n    '''\n    if initial_path:\n        initial_config = __salt__['cp.get_file_str'](initial_path, saltenv=saltenv)\n    candidate_config = merge_text(initial_config=initial_config,\n                                  merge_config=merge_config,\n                                  merge_path=merge_path,\n                                  saltenv=saltenv)\n    clean_running_dict = tree(config=initial_config)\n    clean_running = _print_config_text(clean_running_dict)\n    return _get_diff_text(clean_running, candidate_config)", "output": "Return the merge diff, as text, after merging the merge config into the\n    initial config.\n\n    initial_config\n        The initial configuration sent as text. This argument is ignored when\n        ``initial_path`` is set.\n\n    initial_path\n        Absolute or remote path from where to load the initial configuration\n        text. This argument allows any URI supported by\n        :py:func:`cp.get_url <salt.modules.cp.get_url>`), e.g., ``salt://``,\n        ``https://``, ``s3://``, ``ftp:/``, etc.\n\n    merge_config\n        The config to be merged into the initial config, sent as text. This\n        argument is ignored when ``merge_path`` is set.\n\n    merge_path\n        Absolute or remote path from where to load the merge configuration\n        text. This argument allows any URI supported by\n        :py:func:`cp.get_url <salt.modules.cp.get_url>`), e.g., ``salt://``,\n        ``https://``, ``s3://``, ``ftp:/``, etc.\n\n    saltenv: ``base``\n        Salt fileserver environment from which to retrieve the file.\n        Ignored if ``initial_path`` or ``merge_path`` is not a ``salt://`` URL.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' iosconfig.merge_diff initial_path=salt://path/to/running.cfg merge_path=salt://path/to/merge.cfg", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def new(self):\n        '''\n        \n        '''\n        dbname = self._label()\n        self.db_path = os.path.join(self.path, dbname)\n        if not os.path.exists(self.db_path):\n            os.makedirs(self.db_path)\n        self._opened = True\n        self.list_tables()\n\n        return dbname", "output": "Create a new database and opens it.\n\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resize_imgs(self, targ, new_path, resume=True, fn=None):\n        \"\"\"\n        \n        \"\"\"\n        dest = resize_imgs(self.fnames, targ, self.path, new_path, resume, fn)\n        return self.__class__(self.fnames, self.y, self.transform, dest)", "output": "resize all images in the dataset and save them to `new_path`\n        \n        Arguments:\n        targ (int): the target size\n        new_path (string): the new folder to save the images\n        resume (bool): if true (default), allow resuming a partial resize operation by checking for the existence\n        of individual images rather than the existence of the directory\n        fn (function): custom resizing function Img -> Img", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_package_set(package_set, should_ignore=None):\n    # type: (PackageSet, Optional[Callable[[str], bool]]) -> CheckResult\n    \"\"\"\n    \"\"\"\n    if should_ignore is None:\n        def should_ignore(name):\n            return False\n\n    missing = dict()\n    conflicting = dict()\n\n    for package_name in package_set:\n        # Info about dependencies of package_name\n        missing_deps = set()  # type: Set[Missing]\n        conflicting_deps = set()  # type: Set[Conflicting]\n\n        if should_ignore(package_name):\n            continue\n\n        for req in package_set[package_name].requires:\n            name = canonicalize_name(req.project_name)  # type: str\n\n            # Check if it's missing\n            if name not in package_set:\n                missed = True\n                if req.marker is not None:\n                    missed = req.marker.evaluate()\n                if missed:\n                    missing_deps.add((name, req))\n                continue\n\n            # Check if there's a conflict\n            version = package_set[name].version  # type: str\n            if not req.specifier.contains(version, prereleases=True):\n                conflicting_deps.add((name, version, req))\n\n        if missing_deps:\n            missing[package_name] = sorted(missing_deps, key=str)\n        if conflicting_deps:\n            conflicting[package_name] = sorted(conflicting_deps, key=str)\n\n    return missing, conflicting", "output": "Check if a package set is consistent\n\n    If should_ignore is passed, it should be a callable that takes a\n    package name and returns a boolean.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_host_keys(self, filename):\n        \"\"\"\n        \n        \"\"\"\n        self._host_keys_filename = filename\n        self._host_keys.load(filename)", "output": "Load host keys from a local host-key file.  Host keys read with this\n        method will be checked after keys loaded via `load_system_host_keys`,\n        but will be saved back by `save_host_keys` (so they can be modified).\n        The missing host key policy `.AutoAddPolicy` adds keys to this set and\n        saves them, when connecting to a previously-unknown server.\n\n        This method can be called multiple times.  Each new set of host keys\n        will be merged with the existing set (new replacing old if there are\n        conflicts).  When automatically saving, the last hostname is used.\n\n        :param str filename: the filename to read\n\n        :raises: ``IOError`` -- if the filename could not be read", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def to_bytes(self, exclude=tuple(), disable=None, **kwargs):\n        \"\"\"\n        \"\"\"\n        if disable is not None:\n            deprecation_warning(Warnings.W014)\n            exclude = disable\n        serializers = OrderedDict()\n        serializers[\"vocab\"] = lambda: self.vocab.to_bytes()\n        serializers[\"tokenizer\"] = lambda: self.tokenizer.to_bytes(exclude=[\"vocab\"])\n        serializers[\"meta.json\"] = lambda: srsly.json_dumps(self.meta)\n        for name, proc in self.pipeline:\n            if name in exclude:\n                continue\n            if not hasattr(proc, \"to_bytes\"):\n                continue\n            serializers[name] = lambda proc=proc: proc.to_bytes(exclude=[\"vocab\"])\n        exclude = util.get_serialization_exclude(serializers, exclude, kwargs)\n        return util.to_bytes(serializers, exclude)", "output": "Serialize the current state to a binary string.\n\n        exclude (list): Names of components or serialization fields to exclude.\n        RETURNS (bytes): The serialized form of the `Language` object.\n\n        DOCS: https://spacy.io/api/language#to_bytes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def out_format(data, out, opts=None, **kwargs):\n    '''\n    \n    '''\n    return try_printout(data, out, opts, **kwargs)", "output": "Return the formatted outputter string for the passed data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset ():\n    \"\"\" \n    \"\"\"\n    global __prefixes_suffixes, __suffixes_to_types, __types, __rule_names_to_types, __target_suffixes_cache\n\n    __register_features ()\n\n    # Stores suffixes for generated targets.\n    __prefixes_suffixes = [property.PropertyMap(), property.PropertyMap()]\n\n    # Maps suffixes to types\n    __suffixes_to_types = {}\n\n    # A map with all the registered types, indexed by the type name\n    # Each entry is a dictionary with following values:\n    # 'base': the name of base type or None if type has no base\n    # 'derived': a list of names of type which derive from this one\n    # 'scanner': the scanner class registered for this type, if any\n    __types = {}\n\n    # Caches suffixes for targets with certain properties.\n    __target_suffixes_cache = {}", "output": "Clear the module state. This is mainly for testing purposes.\n        Note that this must be called _after_ resetting the module 'feature'.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _sorted_resource_labels(labels):\n    \"\"\"\"\"\"\n    head = [label for label in TOP_RESOURCE_LABELS if label in labels]\n    tail = sorted(label for label in labels if label not in TOP_RESOURCE_LABELS)\n    return head + tail", "output": "Sort label names, putting well-known resource labels first.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def attach(zone, force=False, brand_opts=None):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    ## attach zone\n    res = __salt__['cmd.run_all']('zoneadm -z {zone} attach{force}{brand_opts}'.format(\n        zone=zone,\n        force=' -F' if force else '',\n        brand_opts=' {0}'.format(brand_opts) if brand_opts else '',\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    ret['message'] = ret['message'].replace('zoneadm: ', '')\n    if ret['message'] == '':\n        del ret['message']\n\n    return ret", "output": "Attach the specified zone.\n\n    zone : string\n        name of the zone\n    force : boolean\n        force the zone into the \"installed\" state with no validation\n    brand_opts : string\n        brand specific options to pass\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zoneadm.attach lawrence\n        salt '*' zoneadm.attach lawrence True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _multiple_callbacks(callbacks, *args, **kwargs):\n    \"\"\"\n    \"\"\"\n    if isinstance(callbacks, list):\n        for cb in callbacks:\n            cb(*args, **kwargs)\n        return\n    if callbacks:\n        callbacks(*args, **kwargs)", "output": "Sends args and kwargs to any configured callbacks.\n    This handles the cases where the 'callbacks' variable\n    is ``None``, a single function, or a list.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_installable_file(path):\n    # type: (PipfileType) -> bool\n    \"\"\"\"\"\"\n    from packaging import specifiers\n\n    if isinstance(path, Mapping):\n        path = convert_entry_to_path(path)\n\n    # If the string starts with a valid specifier operator, test if it is a valid\n    # specifier set before making a path object (to avoid breaking windows)\n    if any(path.startswith(spec) for spec in \"!=<>~\"):\n        try:\n            specifiers.SpecifierSet(path)\n        # If this is not a valid specifier, just move on and try it as a path\n        except specifiers.InvalidSpecifier:\n            pass\n        else:\n            return False\n\n    parsed = urlparse(path)\n    is_local = (\n        not parsed.scheme\n        or parsed.scheme == \"file\"\n        or (len(parsed.scheme) == 1 and os.name == \"nt\")\n    )\n    if parsed.scheme and parsed.scheme == \"file\":\n        path = vistir.compat.fs_decode(vistir.path.url_to_path(path))\n    normalized_path = vistir.path.normalize_path(path)\n    if is_local and not os.path.exists(normalized_path):\n        return False\n\n    is_archive = pip_shims.shims.is_archive_file(normalized_path)\n    is_local_project = os.path.isdir(normalized_path) and is_installable_dir(\n        normalized_path\n    )\n    if is_local and is_local_project or is_archive:\n        return True\n\n    if not is_local and pip_shims.shims.is_archive_file(parsed.path):\n        return True\n\n    return False", "output": "Determine if a path can potentially be installed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def size(self):\n        \"\"\"\"\"\"\n        size = {}\n        if self._w3c:\n            size = self._execute(Command.GET_ELEMENT_RECT)['value']\n        else:\n            size = self._execute(Command.GET_ELEMENT_SIZE)['value']\n        new_size = {\"height\": size[\"height\"],\n                    \"width\": size[\"width\"]}\n        return new_size", "output": "The size of the element.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_frame(url, skiprows):\n    \"\"\"\n    \n    \"\"\"\n    return pd.read_csv(\n        url,\n        skiprows=skiprows,\n        skipinitialspace=True,\n        na_values=[\"Bank holiday\", \"Not available\"],\n        parse_dates=[\"Date\"],\n        index_col=\"Date\",\n    ).dropna(how='all') \\\n     .tz_localize('UTC') \\\n     .rename(columns=COLUMN_NAMES)", "output": "Load a DataFrame of data from a Bank of Canada site.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_outputs(self, merge_multi_context=True):\n        \"\"\"\n        \"\"\"\n        assert self.binded and self.params_initialized\n        return self._curr_module.get_outputs(merge_multi_context=merge_multi_context)", "output": "Gets outputs from a previous forward computation.\n\n        Parameters\n        ----------\n        merge_multi_context : bool\n            Defaults to ``True``. In the case when data-parallelism is used, the outputs\n            will be collected from multiple devices. A ``True`` value indicate that we\n            should merge the collected results so that they look like from a single\n            executor.\n\n        Returns\n        -------\n        list of numpy arrays or list of list of numpy arrays\n            If `merge_multi_context` is ``True``, it is like ``[out1, out2]``. Otherwise, it\n            is like ``[[out1_dev1, out1_dev2], [out2_dev1, out2_dev2]]``. All the output\n            elements are numpy arrays.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_values(self, dtype=None):\n        \"\"\"\n        \n        \"\"\"\n        values = self.values\n        if is_object_dtype(dtype):\n            values = values._box_values(values._data)\n\n        values = np.asarray(values)\n\n        if self.ndim == 2:\n            # Ensure that our shape is correct for DataFrame.\n            # ExtensionArrays are always 1-D, even in a DataFrame when\n            # the analogous NumPy-backed column would be a 2-D ndarray.\n            values = values.reshape(1, -1)\n        return values", "output": "Returns an ndarray of values.\n\n        Parameters\n        ----------\n        dtype : np.dtype\n            Only `object`-like dtypes are respected here (not sure\n            why).\n\n        Returns\n        -------\n        values : ndarray\n            When ``dtype=object``, then and object-dtype ndarray of\n            boxed values is returned. Otherwise, an M8[ns] ndarray\n            is returned.\n\n            DatetimeArray is always 1-d. ``get_values`` will reshape\n            the return value to be the same dimensionality as the\n            block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def package(self):\n        \"\"\"\n        \"\"\"\n        cmake = self.configure_cmake()\n        cmake.install()\n        self.copy(pattern=\"LICENSE.txt\", dst=\"licenses\")\n        self.copy(pattern=\"FindFlatBuffers.cmake\", dst=os.path.join(\"lib\", \"cmake\", \"flatbuffers\"), src=\"CMake\")\n        self.copy(pattern=\"flathash*\", dst=\"bin\", src=\"bin\")\n        self.copy(pattern=\"flatc*\", dst=\"bin\", src=\"bin\")\n        if self.settings.os == \"Windows\" and self.options.shared:\n            if self.settings.compiler == \"Visual Studio\":\n                shutil.move(os.path.join(self.package_folder, \"lib\", \"%s.dll\" % self.name),\n                            os.path.join(self.package_folder, \"bin\", \"%s.dll\" % self.name))\n            elif self.settings.compiler == \"gcc\":\n                shutil.move(os.path.join(self.package_folder, \"lib\", \"lib%s.dll\" % self.name),\n                            os.path.join(self.package_folder, \"bin\", \"lib%s.dll\" % self.name))", "output": "Copy Flatbuffers' artifacts to package folder", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def neuron(layer_name, channel_n, x=None, y=None, batch=None):\n  \"\"\"\n  \"\"\"\n  def inner(T):\n    layer = T(layer_name)\n    shape = tf.shape(layer)\n    x_ = shape[1] // 2 if x is None else x\n    y_ = shape[2] // 2 if y is None else y\n\n    if batch is None:\n      return layer[:, x_, y_, channel_n]\n    else:\n      return layer[batch, x_, y_, channel_n]\n  return inner", "output": "Visualize a single neuron of a single channel.\n\n  Defaults to the center neuron. When width and height are even numbers, we\n  choose the neuron in the bottom right of the center 2x2 neurons.\n\n  Odd width & height:               Even width & height:\n\n  +---+---+---+                     +---+---+---+---+\n  |   |   |   |                     |   |   |   |   |\n  +---+---+---+                     +---+---+---+---+\n  |   | X |   |                     |   |   |   |   |\n  +---+---+---+                     +---+---+---+---+\n  |   |   |   |                     |   |   | X |   |\n  +---+---+---+                     +---+---+---+---+\n                                    |   |   |   |   |\n                                    +---+---+---+---+", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_attach_volumes(name, kwargs, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The create_attach_volumes action must be called with '\n            '-a or --action.'\n        )\n\n    volumes = literal_eval(kwargs['volumes'])\n    node = kwargs['node']\n    conn = get_conn()\n    node_data = _expand_node(conn.ex_get_node(node))\n    letter = ord('a') - 1\n\n    for idx, volume in enumerate(volumes):\n        volume_name = '{0}-sd{1}'.format(name, chr(letter + 2 + idx))\n\n        volume_dict = {\n          'disk_name': volume_name,\n          'location': node_data['extra']['zone']['name'],\n          'size': volume['size'],\n          'type': volume.get('type', 'pd-standard'),\n          'image': volume.get('image', None),\n          'snapshot': volume.get('snapshot', None),\n          'auto_delete': volume.get('auto_delete', False)\n        }\n\n        create_disk(volume_dict, 'function')\n        attach_disk(name, volume_dict, 'action')", "output": ".. versionadded:: 2017.7.0\n\n    Create and attach multiple volumes to a node. The 'volumes' and 'node'\n    arguments are required, where 'node' is a libcloud node, and 'volumes'\n    is a list of maps, where each map contains:\n\n    size\n        The size of the new disk in GB. Required.\n\n    type\n        The disk type, either pd-standard or pd-ssd. Optional, defaults to pd-standard.\n\n    image\n        An image to use for this new disk. Optional.\n\n    snapshot\n        A snapshot to use for this new disk. Optional.\n\n    auto_delete\n        An option(bool) to keep or remove the disk upon instance deletion.\n        Optional, defaults to False.\n\n    Volumes are attached in the order in which they are given, thus on a new\n    node the first volume will be /dev/sdb, the second /dev/sdc, and so on.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_to_object(self):\n        \"\"\"\n        \"\"\"\n\n        tmpdir = tempfile.mkdtemp(\"save_to_object\", dir=self.logdir)\n        checkpoint_prefix = self.save(tmpdir)\n\n        data = {}\n        base_dir = os.path.dirname(checkpoint_prefix)\n        for path in os.listdir(base_dir):\n            path = os.path.join(base_dir, path)\n            if path.startswith(checkpoint_prefix):\n                with open(path, \"rb\") as f:\n                    data[os.path.basename(path)] = f.read()\n\n        out = io.BytesIO()\n        data_dict = pickle.dumps({\n            \"checkpoint_name\": os.path.basename(checkpoint_prefix),\n            \"data\": data,\n        })\n        if len(data_dict) > 10e6:  # getting pretty large\n            logger.info(\"Checkpoint size is {} bytes\".format(len(data_dict)))\n        out.write(data_dict)\n\n        shutil.rmtree(tmpdir)\n        return out.getvalue()", "output": "Saves the current model state to a Python object. It also\n        saves to disk but does not return the checkpoint path.\n\n        Returns:\n            Object holding checkpoint data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def metric_create(self, project, metric_name, filter_, description):\n        \"\"\"\n        \"\"\"\n        parent = \"projects/%s\" % (project,)\n        metric_pb = LogMetric(name=metric_name, filter=filter_, description=description)\n        self._gapic_api.create_log_metric(parent, metric_pb)", "output": "API call:  create a metric resource.\n\n        See\n        https://cloud.google.com/logging/docs/reference/v2/rest/v2/projects.metrics/create\n\n        :type project: str\n        :param project: ID of the project in which to create the metric.\n\n        :type metric_name: str\n        :param metric_name: the name of the metric\n\n        :type filter_: str\n        :param filter_: the advanced logs filter expression defining the\n                        entries exported by the metric.\n\n        :type description: str\n        :param description: description of the metric.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eq_or_parent(self, other):\n        \"\"\"\n        \"\"\"\n        return self.parts[: len(other.parts)] == other.parts[: len(self.parts)]", "output": "Check whether ``other`` is an ancestor.\n\n        Returns:\n            (bool) True IFF ``other`` is an ancestor or equal to ``self``,\n            else False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pressed(self, evt):\n        \"\"\"\"\"\"\n        x, y, widget = evt.x, evt.y, evt.widget\n        item = widget.identify_row(y)\n        column = widget.identify_column(x)\n\n        if not column or not item in self._items:\n            # clicked in the weekdays row or just outside the columns\n            return\n\n        item_values = widget.item(item)['values']\n        if not len(item_values):  # row is empty for this month\n            return\n\n        text = item_values[int(column[1]) - 1]\n        if not text:  # date is empty\n            return\n\n        bbox = widget.bbox(item, column)\n        if not bbox:  # calendar not visible yet\n            return\n\n        # update and then show selection\n        text = '%02d' % text\n        self._selection = (text, item, column)\n        self._show_selection(text, bbox)\n        year, month = self._date.year, self._date.month\n        try:\n            self._TargetElement.Update(self.datetime(year, month, int(self._selection[0])))\n            if self._TargetElement.ChangeSubmits:\n                self._TargetElement.ParentForm.LastButtonClicked = self._TargetElement.Key\n                self._TargetElement.ParentForm.FormRemainedOpen = True\n                self._TargetElement.ParentForm.TKroot.quit()  # kick the users out of the mainloop\n        except:\n            pass\n        if self.close_when_chosen:\n            self._master.destroy()", "output": "Clicked somewhere in the calendar.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setdefault(self, key, default=None):\n        \"\"\"\n        \"\"\"\n        self._wlock.acquire()\n        try:\n            try:\n                return self[key]\n            except KeyError:\n                self[key] = default\n                return default\n        finally:\n            self._wlock.release()", "output": "Set `default` if the key is not in the cache otherwise\n        leave unchanged. Return the value of this key.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def data_names(self):\n        \"\"\"\"\"\"\n        if self.binded:\n            return self._curr_module.data_names\n        else:\n            _, data_names, _ = self._call_sym_gen(self._default_bucket_key)\n            return data_names", "output": "A list of names for data required by this module.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _kernel_versions_redhat():\n    '''\n    \n    '''\n    kernel_get_last = __salt__['cmd.run']('rpm -q --last kernel')\n    kernels = []\n    kernel_versions = []\n    for line in kernel_get_last.splitlines():\n        if 'kernel-' in line:\n            kernels.append(line)\n\n    kernel = kernels[0].split(' ', 1)[0]\n    kernel = kernel.strip('kernel-')\n    kernel_versions.append(kernel)\n\n    return kernel_versions", "output": "Name of the last installed kernel, for Red Hat based systems.\n\n    Returns:\n            List with name of last installed kernel as it is interpreted in output of `uname -a` command.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, archive):\n    \"\"\"\n    \"\"\"\n\n    for fname, fobj in archive:\n      res = _NAME_RE.match(fname)\n      if not res:  # if anything other than .png; skip\n        continue\n      label = res.group(2).lower()\n      yield {\n          \"image\": fobj,\n          \"label\": label,\n      }", "output": "Generate rock, paper or scissors images and labels given the directory path.\n\n    Args:\n      archive: object that iterates over the zip.\n\n    Yields:\n      The image path and its corresponding label.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_search_space(self, search_space):\n        \"\"\"\n        \n        \"\"\"\n        self.json = search_space\n        search_space_instance = json2space(self.json)\n        rstate = np.random.RandomState()\n        trials = hp.Trials()\n        domain = hp.Domain(None, search_space_instance,\n                           pass_expr_memo_ctrl=None)\n        algorithm = self._choose_tuner(self.algorithm_name)\n        self.rval = hp.FMinIter(algorithm, domain, trials,\n                                max_evals=-1, rstate=rstate, verbose=0)\n        self.rval.catch_eval_exceptions = False", "output": "Update search space definition in tuner by search_space in parameters.\n\n        Will called when first setup experiemnt or update search space in WebUI.\n\n        Parameters\n        ----------\n        search_space : dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_enum_doc(elt, full_name:str)->str:\n    \"\"\n    vals = ', '.join(elt.__members__.keys())\n    return f'{code_esc(full_name)}',f'<code>Enum</code> = [{vals}]'", "output": "Formatted enum documentation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def loss(self, xs, ys):\n        \"\"\"\"\"\"\n        return float(\n            self.sess.run(\n                self.cross_entropy, feed_dict={\n                    self.x: xs,\n                    self.y_: ys\n                }))", "output": "Computes the loss of the network.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def display_output(data, out=None, opts=None, **kwargs):\n    '''\n    \n    '''\n    if opts is None:\n        opts = {}\n    display_data = try_printout(data, out, opts, **kwargs)\n\n    output_filename = opts.get('output_file', None)\n    log.trace('data = %s', data)\n    try:\n        # output filename can be either '' or None\n        if output_filename:\n            if not hasattr(output_filename, 'write'):\n                ofh = salt.utils.files.fopen(output_filename, 'a')  # pylint: disable=resource-leakage\n                fh_opened = True\n            else:\n                # Filehandle/file-like object\n                ofh = output_filename\n                fh_opened = False\n\n            try:\n                fdata = display_data\n                if isinstance(fdata, six.text_type):\n                    try:\n                        fdata = fdata.encode('utf-8')\n                    except (UnicodeDecodeError, UnicodeEncodeError):\n                        # try to let the stream write\n                        # even if we didn't encode it\n                        pass\n                if fdata:\n                    ofh.write(salt.utils.stringutils.to_str(fdata))\n                    ofh.write('\\n')\n            finally:\n                if fh_opened:\n                    ofh.close()\n            return\n        if display_data:\n            salt.utils.stringutils.print_cli(display_data)\n    except IOError as exc:\n        # Only raise if it's NOT a broken pipe\n        if exc.errno != errno.EPIPE:\n            raise exc", "output": "Print the passed data using the desired output", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_credentials():\n    '''\n    \n    '''\n\n    # if the username and password were already found don't fo though the\n    # connection process again\n    if 'username' in DETAILS and 'password' in DETAILS:\n        return DETAILS['username'], DETAILS['password']\n\n    passwords = DETAILS['passwords']\n    for password in passwords:\n        DETAILS['password'] = password\n        if not __salt__['vsphere.test_vcenter_connection']():\n            # We are unable to authenticate\n            continue\n        # If we have data returned from above, we've successfully authenticated.\n        return DETAILS['username'], password\n    # We've reached the end of the list without successfully authenticating.\n    raise salt.exceptions.VMwareConnectionError('Cannot complete login due to '\n                                                'incorrect credentials.')", "output": "Cycle through all the possible credentials and return the first one that\n    works.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cell_values(self, column_family_id, column, max_count=None):\n        \"\"\"\n        \"\"\"\n        cells = self.find_cells(column_family_id, column)\n        if max_count is None:\n            max_count = len(cells)\n\n        for index, cell in enumerate(cells):\n            if index == max_count:\n                break\n\n            yield cell.value, cell.timestamp_micros", "output": "Get a time series of cells stored on this instance.\n\n        For example:\n\n        .. literalinclude:: snippets_table.py\n            :start-after: [START bigtable_row_cell_values]\n            :end-before: [END bigtable_row_cell_values]\n\n        Args:\n            column_family_id (str): The ID of the column family. Must be of the\n                form ``[_a-zA-Z0-9][-_.a-zA-Z0-9]*``.\n            column (bytes): The column within the column family where the cells\n                are located.\n            max_count (int): The maximum number of cells to use.\n\n        Returns:\n            A generator which provides: cell.value, cell.timestamp_micros\n                for each cell in the list of cells\n\n        Raises:\n            KeyError: If ``column_family_id`` is not among the cells stored\n                in this row.\n            KeyError: If ``column`` is not among the cells stored in this row\n                for the given ``column_family_id``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init(opts):\n    '''\n    \n    '''\n    proxy_dict = opts.get('proxy', {})\n    opts['multiprocessing'] = proxy_dict.get('multiprocessing', False)\n    netmiko_connection_args = proxy_dict.copy()\n    netmiko_connection_args.pop('proxytype', None)\n    netmiko_device['always_alive'] = netmiko_connection_args.pop('always_alive',\n                                                                 opts.get('proxy_always_alive', True))\n    try:\n        connection = ConnectHandler(**netmiko_connection_args)\n        netmiko_device['connection'] = connection\n        netmiko_device['initialized'] = True\n        netmiko_device['args'] = netmiko_connection_args\n        netmiko_device['up'] = True\n        if not netmiko_device['always_alive']:\n            netmiko_device['connection'].disconnect()\n    except NetMikoTimeoutException as t_err:\n        log.error('Unable to setup the netmiko connection', exc_info=True)\n    except NetMikoAuthenticationException as au_err:\n        log.error('Unable to setup the netmiko connection', exc_info=True)\n    return True", "output": "Open the connection to the network device\n    managed through netmiko.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _set_properties(self, resource):\n        \"\"\"\n        \"\"\"\n        resource = resource.copy()\n        self._additions = tuple(\n            [\n                ResourceRecordSet.from_api_repr(added_res, self.zone)\n                for added_res in resource.pop(\"additions\", ())\n            ]\n        )\n        self._deletions = tuple(\n            [\n                ResourceRecordSet.from_api_repr(added_res, self.zone)\n                for added_res in resource.pop(\"deletions\", ())\n            ]\n        )\n        self._properties = resource", "output": "Helper method for :meth:`from_api_repr`, :meth:`create`, etc.\n\n        :type resource: dict\n        :param resource: change set representation returned from the API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def should_use_ephemeral_cache(\n    req,  # type: InstallRequirement\n    format_control,  # type: FormatControl\n    autobuilding,  # type: bool\n    cache_available  # type: bool\n):\n    # type: (...) -> Optional[bool]\n    \"\"\"\n    \n    \"\"\"\n    if req.constraint:\n        return None\n    if req.is_wheel:\n        if not autobuilding:\n            logger.info(\n                'Skipping %s, due to already being wheel.', req.name,\n            )\n        return None\n    if not autobuilding:\n        return False\n\n    if req.editable or not req.source_dir:\n        return None\n\n    if req.link and not req.link.is_artifact:\n        # VCS checkout. Build wheel just for this run.\n        return True\n\n    if \"binary\" not in format_control.get_allowed_formats(\n            canonicalize_name(req.name)):\n        logger.info(\n            \"Skipping bdist_wheel for %s, due to binaries \"\n            \"being disabled for it.\", req.name,\n        )\n        return None\n\n    link = req.link\n    base, ext = link.splitext()\n    if cache_available and _contains_egg_info(base):\n        return False\n\n    # Otherwise, build the wheel just for this run using the ephemeral\n    # cache since we are either in the case of e.g. a local directory, or\n    # no cache directory is available to use.\n    return True", "output": "Return whether to build an InstallRequirement object using the\n    ephemeral cache.\n\n    :param cache_available: whether a cache directory is available for the\n        autobuilding=True case.\n\n    :return: True or False to build the requirement with ephem_cache=True\n        or False, respectively; or None not to build the requirement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_answer_begin_end(data):\n    '''\n    \n    '''\n    begin = []\n    end = []\n    for qa_pair in data:\n        tokens = qa_pair['passage_tokens']\n        char_begin = qa_pair['answer_begin']\n        char_end = qa_pair['answer_end']\n        word_begin = get_word_index(tokens, char_begin)\n        word_end = get_word_index(tokens, char_end)\n        begin.append(word_begin)\n        end.append(word_end)\n    return np.asarray(begin), np.asarray(end)", "output": "Get answer's index of begin and end.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_collection(self):\n        \"\"\"\n        \n        \"\"\"\n        db_mongo = self._mongo_client[self._index]\n        return db_mongo[self._collection]", "output": "Return targeted mongo collection to query on", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def balancer_detach_member(balancer_id, member_id, profile, **libcloud_kwargs):\n    '''\n    \n    '''\n    conn = _get_driver(profile=profile)\n    balancer = conn.get_balancer(balancer_id)\n    members = conn.balancer_list_members(balancer=balancer)\n    match = [member for member in members if member.id == member_id]\n    if len(match) > 1:\n        raise ValueError(\"Ambiguous argument, found mulitple records\")\n    elif not match:\n        raise ValueError(\"Bad argument, found no records\")\n    else:\n        member = match[0]\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    return conn.balancer_detach_member(balancer=balancer, member=member, **libcloud_kwargs)", "output": "Add a new member to the load balancer\n\n    :param balancer_id: id of a load balancer you want to fetch\n    :type  balancer_id: ``str``\n\n    :param ip: IP address for the new member\n    :type  ip: ``str``\n\n    :param port: Port for the new member\n    :type  port: ``int``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's balancer_detach_member method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_storage.balancer_detach_member balancer123 member123 profile1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def imagetransformer_cifar10_base_dmol():\n  \"\"\"\"\"\"\n  hparams = image_transformer_base()\n  hparams.likelihood = cia.DistributionType.DMOL\n  hparams.num_channels = 1\n  hparams.bottom[\"targets\"] = modalities.image_channel_compress_targets_bottom\n  hparams.top[\"targets\"] = modalities.identity_top\n  hparams.num_heads = 8\n  hparams.batch_size = 8\n  hparams.sampling_method = \"random\"\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  hparams.summarize_grads = True\n  hparams.hidden_size = 256\n  hparams.filter_size = 512\n  hparams.attention_key_channels = 512\n  hparams.attention_value_channels = 512\n  hparams.num_decoder_layers = 12\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.learning_rate = 0.1\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.pos = \"emb\"\n  hparams.unconditional = True\n  return hparams", "output": "Best config for 2.90 bits/dim on CIFAR10 using DMOL.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_roster_file(options):\n    '''\n    \n    '''\n    template = None\n    # The __disable_custom_roster is always True if Salt SSH Client comes\n    # from Salt API. In that case no way to define own 'roster_file', instead\n    # this file needs to be chosen from already validated rosters\n    # (see /etc/salt/master config).\n    if options.get('__disable_custom_roster') and options.get('roster_file'):\n        roster = options.get('roster_file').strip('/')\n        for roster_location in options.get('rosters'):\n            r_file = os.path.join(roster_location, roster)\n            if os.path.isfile(r_file):\n                template = r_file\n                break\n        del options['roster_file']\n\n    if not template:\n        if options.get('roster_file'):\n            template = options.get('roster_file')\n        elif 'config_dir' in options.get('__master_opts__', {}):\n            template = os.path.join(options['__master_opts__']['config_dir'],\n                                    'roster')\n        elif 'config_dir' in options:\n            template = os.path.join(options['config_dir'], 'roster')\n        else:\n            template = os.path.join(salt.syspaths.CONFIG_DIR, 'roster')\n\n    if not os.path.isfile(template):\n        raise IOError('Roster file \"{0}\" not found'.format(template))\n\n    if not os.access(template, os.R_OK):\n        raise IOError('Access denied to roster \"{0}\"'.format(template))\n\n    return template", "output": "Find respective roster file.\n\n    :param options:\n    :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cycle(self, *args):\n        \"\"\"\"\"\"\n        if not args:\n            raise TypeError('no items for cycling given')\n        return args[self.index0 % len(args)]", "output": "Cycles among the arguments with the current loop index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_cache_subnet_groups(name=None, region=None, key=None,\n                             keyid=None, profile=None):\n    '''\n    \n    '''\n    return [g['CacheSubnetGroupName'] for g in\n            get_all_cache_subnet_groups(name, region, key, keyid, profile)]", "output": "Return a list of all cache subnet group names\n\n    CLI example::\n\n        salt myminion boto_elasticache.list_subnet_groups region=us-east-1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def config_filter_lines(parent_regex, child_regex, source='running'):\n    \n    '''\n    config_txt = __salt__['net.config'](source=source)['out'][source]\n    return __salt__['ciscoconfparse.filter_lines'](config=config_txt,\n                                                   parent_regex=parent_regex,\n                                                   child_regex=child_regex)", "output": "r'''\n    .. versionadded:: 2019.2.0\n\n    Return a list of detailed matches, for the configuration blocks (parent-child\n    relationship) whose parent respects the regular expressions configured via\n    the ``parent_regex`` argument, and the child matches the ``child_regex``\n    regular expression. The result is a list of dictionaries with the following\n    keys:\n\n    - ``match``: a boolean value that tells whether ``child_regex`` matched any\n      children lines.\n    - ``parent``: the parent line (as text).\n    - ``child``: the child line (as text). If no child line matched, this field\n      will be ``None``.\n\n    .. note::\n        This function is only available only when the underlying library\n        `ciscoconfparse <http://www.pennington.net/py/ciscoconfparse/index.html>`_\n        is installed. See\n        :py:func:`ciscoconfparse module <salt.modules.ciscoconfparse_mod>` for\n        more details.\n\n    parent_regex\n        The regular expression to match the parent configuration lines against.\n\n    child_regex\n        The regular expression to match the child configuration lines against.\n\n    source: ``running``\n        The configuration type to retrieve from the network device. Default:\n        ``running``. Available options: ``running``, ``startup``, ``candidate``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.config_filter_lines '^interface' 'ip address'\n        salt '*' napalm.config_filter_lines '^interface' 'shutdown' source=candidate", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_domains(container_id=None):\n    '''\n    \n    '''\n    if container_id:\n        url = '{0}/domain?{1}'.format(_base_url(), container_id)\n    else:\n        url = '{0}/domain'.format(_base_url())\n\n    orgs = _paginate(url,\n                     \"domains\",\n                     method='GET',\n                     decode=True,\n                     decode_type='json',\n                     header_dict={\n                         'X-DC-DEVKEY': _api_key(),\n                         'Content-Type': 'application/json',\n                     }\n    )\n\n    ret = {'domains': orgs}\n    return ret", "output": "List domains that CertCentral knows about. You can filter by\n    container_id (also known as \"Division\") by passing a container_id.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run digicert.list_domains", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _round_robin_write(writers, generator):\n  \"\"\"\"\"\"\n  for i, example in enumerate(utils.tqdm(\n      generator, unit=\" examples\", leave=False)):\n    writers[i % len(writers)].write(example)", "output": "Write records from generator round-robin across writers.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(key, profile=None):\n    '''\n    \n    '''\n    if not profile:\n        return None\n    _, cur, table = _connect(profile)\n    q = profile.get('get_query', ('SELECT value FROM {0} WHERE '\n                                  'key=:key'.format(table)))\n    res = cur.execute(q, {'key': key})\n    res = res.fetchone()\n    if not res:\n        return None\n    return salt.utils.msgpack.unpackb(res[0])", "output": "Get a value from sqlite3", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _init_nxapi(opts):\n    '''\n    \n    '''\n    proxy_dict = opts.get('proxy', {})\n    conn_args = copy.deepcopy(proxy_dict)\n    conn_args.pop('proxytype', None)\n    opts['multiprocessing'] = conn_args.pop('multiprocessing', True)\n    # This is not a SSH-based proxy, so it should be safe to enable\n    # multiprocessing.\n    try:\n        rpc_reply = __utils__['nxos.nxapi_request']('show clock', **conn_args)\n        # Execute a very simple command to confirm we are able to connect properly\n        DEVICE_DETAILS['conn_args'] = conn_args\n        DEVICE_DETAILS['initialized'] = True\n        DEVICE_DETAILS['up'] = True\n        DEVICE_DETAILS['no_save_config'] = opts['proxy'].get('no_save_config', False)\n    except Exception as ex:\n        log.error('Unable to connect to %s', conn_args['host'])\n        log.error('Please check the following:\\n')\n        log.error('-- Verify that \"feature nxapi\" is enabled on your NX-OS device: %s', conn_args['host'])\n        log.error('-- Verify that nxapi settings on the NX-OS device and proxy minion config file match')\n        log.error('-- Exception Generated: %s', ex)\n        exit()\n    log.info('nxapi DEVICE_DETAILS info: %s', DEVICE_DETAILS)\n    return True", "output": "Open a connection to the NX-OS switch over NX-API.\n\n    As the communication is HTTP(S) based, there is no connection to maintain,\n    however, in order to test the connectivity and make sure we are able to\n    bring up this Minion, we are executing a very simple command (``show clock``)\n    which doesn't come with much overhead and it's sufficient to confirm we are\n    indeed able to connect to the NX-API endpoint as configured.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def focusOutEvent(self, event):\n        \"\"\"\n        \n        \"\"\"\n        self.clicked_outside = True\n        return super(QLineEdit, self).focusOutEvent(event)", "output": "Detect when the focus goes out of this widget.\n\n        This is used to make the file switcher leave focus on the\n        last selected file by the user.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def decons_obs_group_ids(comp_ids, obs_ids, shape, labels, xnull):\n    \"\"\"\n    \n    \"\"\"\n\n    if not xnull:\n        lift = np.fromiter(((a == -1).any() for a in labels), dtype='i8')\n        shape = np.asarray(shape, dtype='i8') + lift\n\n    if not is_int64_overflow_possible(shape):\n        # obs ids are deconstructable! take the fast route!\n        out = decons_group_index(obs_ids, shape)\n        return out if xnull or not lift.any() \\\n            else [x - y for x, y in zip(out, lift)]\n\n    i = unique_label_indices(comp_ids)\n    i8copy = lambda a: a.astype('i8', subok=False, copy=True)\n    return [i8copy(lab[i]) for lab in labels]", "output": "reconstruct labels from observed group ids\n\n    Parameters\n    ----------\n    xnull: boolean,\n        if nulls are excluded; i.e. -1 labels are passed through", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _temp_exists(method, ip):\n    '''\n    \n    '''\n    _type = method.replace('temp', '').upper()\n    cmd = \"csf -t | awk -v code=1 -v type=_type -v ip=ip '$1==type && $2==ip {{code=0}} END {{exit code}}'\".format(_type=_type, ip=ip)\n    exists = __salt__['cmd.run_all'](cmd)\n    return not bool(exists['retcode'])", "output": "Checks if the ip exists as a temporary rule based\n    on the method supplied, (tempallow, tempdeny).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_batch_end(self, train, **kwargs:Any)->None:\n        \"\"\n        if train:\n            if self.idx_s >= len(self.scheds): return {'stop_training': True, 'stop_epoch': True}\n            sched = self.scheds[self.idx_s]\n            for k,v in sched.items(): self.opt.set_stat(k, v.step())\n            if list(sched.values())[0].is_done: self.idx_s += 1", "output": "Take a step in lr,mom sched, start next stepper when the current one is complete.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exit_dfu():\n    \"\"\"\"\"\"\n\n    # set jump address\n    set_address(0x08000000)\n\n    # Send DNLOAD with 0 length to exit DFU\n    __dev.ctrl_transfer(0x21, __DFU_DNLOAD, 0, __DFU_INTERFACE,\n                        None, __TIMEOUT)\n\n    try:\n        # Execute last command\n        if get_status() != __DFU_STATE_DFU_MANIFEST:\n            print(\"Failed to reset device\")\n\n        # Release device\n        usb.util.dispose_resources(__dev)\n    except:\n        pass", "output": "Exit DFU mode, and start running the program.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_optimizer_states(self, fname):\n        \"\"\"\n        \"\"\"\n        assert self._updater is not None, \"Cannot load states for distributed training\"\n        self._updater.set_states(open(fname, 'rb').read())", "output": "Loads the optimizer (updater) state from the file.\n\n        Parameters\n        ----------\n        fname : str\n            Path to input states file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def free_memory(self):\r\n        \"\"\"\"\"\"\r\n        self.main.free_memory()\r\n        QTimer.singleShot(self.INITIAL_FREE_MEMORY_TIME_TRIGGER,\r\n                          lambda: self.main.free_memory())\r\n        QTimer.singleShot(self.SECONDARY_FREE_MEMORY_TIME_TRIGGER,\r\n                          lambda: self.main.free_memory())", "output": "Free memory signal.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_spyder(self):\r\n        \"\"\"\r\n        \r\n        \"\"\"\r\n        answer = QMessageBox.warning(self, _(\"Warning\"),\r\n             _(\"Spyder will restart and reset to default settings: <br><br>\"\r\n               \"Do you want to continue?\"),\r\n             QMessageBox.Yes | QMessageBox.No)\r\n        if answer == QMessageBox.Yes:\r\n            self.restart(reset=True)", "output": "Quit and reset Spyder and then Restart application.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self):\n        \"\"\"\n        \"\"\"\n        client = self._instance._client\n        # We are passing `None` for second argument location.\n        # Location is set only at the time of creation of a cluster\n        # and can not be changed after cluster has been created.\n        return client.instance_admin_client.update_cluster(\n            self.name, self.serve_nodes, None\n        )", "output": "Update this cluster.\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_update_cluster]\n            :end-before: [END bigtable_update_cluster]\n\n        .. note::\n\n            Updates the ``serve_nodes``. If you'd like to\n            change them before updating, reset the values via\n\n            .. code:: python\n\n                cluster.serve_nodes = 8\n\n            before calling :meth:`update`.\n\n        :type location: :str:``CreationOnly``\n        :param location: The location where this cluster's nodes and storage\n                reside. For best performance, clients should be located as\n                close as possible to this cluster. Currently only zones are\n                supported, so values should be of the form\n                ``projects/<project>/locations/<zone>``.\n\n        :type serve_nodes: :int\n        :param serve_nodes: The number of nodes allocated to this cluster.\n                More nodes enable higher throughput and more consistent\n                performance.\n\n        :rtype: :class:`Operation`\n        :returns: The long-running operation corresponding to the\n                  update operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_alarms(deployment_id, profile=\"telemetry\"):\n    '''\n    \n\n    '''\n    auth = _auth(profile=profile)\n\n    try:\n        response = requests.get(_get_telemetry_base(profile) + \"/alerts?deployment={0}\".format(deployment_id), headers=auth)\n    except requests.exceptions.RequestException as e:\n        log.error(six.text_type(e))\n        return False\n\n    if response.status_code == 200:\n        alarms = response.json()\n\n        if alarms:\n            return alarms\n\n        return 'No alarms defined for deployment: {0}'.format(deployment_id)\n    else:\n        # Non 200 response, sent back the error response'\n        return {'err_code': response.status_code, 'err_msg': salt.utils.json.loads(response.text).get('err', '')}", "output": "get all the alarms set up against the current deployment\n\n    Returns dictionary of alarm information\n\n    CLI Example:\n\n        salt myminion telemetry.get_alarms rs-ds033197 profile=telemetry", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_file(self, path, tgt_env):\n        '''\n        \n        '''\n        tree = self.get_tree(tgt_env)\n        if not tree:\n            # Branch/tag/SHA not found in repo\n            return None, None, None\n        blob = None\n        depth = 0\n        while True:\n            depth += 1\n            if depth > SYMLINK_RECURSE_DEPTH:\n                blob = None\n                break\n            try:\n                file_blob = tree / path\n                if stat.S_ISLNK(file_blob.mode):\n                    # Path is a symlink. The blob data corresponding to\n                    # this path's object ID will be the target of the\n                    # symlink. Follow the symlink and set path to the\n                    # location indicated in the blob data.\n                    stream = six.StringIO()\n                    file_blob.stream_data(stream)\n                    stream.seek(0)\n                    link_tgt = stream.read()\n                    stream.close()\n                    path = salt.utils.path.join(\n                        os.path.dirname(path), link_tgt, use_posixpath=True)\n                else:\n                    blob = file_blob\n                    if isinstance(blob, git.Tree):\n                        # Path is a directory, not a file.\n                        blob = None\n                    break\n            except KeyError:\n                # File not found or repo_path points to a directory\n                blob = None\n                break\n        if isinstance(blob, git.Blob):\n            return blob, blob.hexsha, blob.mode\n        return None, None, None", "output": "Find the specified file in the specified environment", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _pep440_version_cmp(pkg1, pkg2, ignore_epoch=False):\n    '''\n    \n    '''\n    normalize = lambda x: six.text_type(x).split('!', 1)[-1] \\\n        if ignore_epoch else six.text_type(x)\n    pkg1 = normalize(pkg1)\n    pkg2 = normalize(pkg2)\n\n    try:\n        if pkg_resources.parse_version(pkg1) < pkg_resources.parse_version(pkg2):\n            return -1\n        if pkg_resources.parse_version(pkg1) == pkg_resources.parse_version(pkg2):\n            return 0\n        if pkg_resources.parse_version(pkg1) > pkg_resources.parse_version(pkg2):\n            return 1\n    except Exception as exc:\n        log.exception(exc)\n    return None", "output": "Compares two version strings using pkg_resources.parse_version.\n    Return -1 if version1 < version2, 0 if version1 ==version2,\n    and 1 if version1 > version2. Return None if there was a problem\n    making the comparison.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def denyMethodWithConditions(self, verb, resource, conditions):\n        \"\"\"\"\"\"\n        self._addMethod(\"Deny\", verb, resource, conditions)", "output": "Adds an API Gateway method (Http verb + Resource path) to the list of denied\n        methods and includes a condition for the policy statement. More on AWS policy\n        conditions here: http://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html#Condition", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def validNormalizeAttributeValue(self, elem, name, value):\n        \"\"\" \"\"\"\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidNormalizeAttributeValue(self._o, elem__o, name, value)\n        return ret", "output": "Does the validation related extra step of the normalization\n          of attribute values:  If the declared value is not CDATA,\n          then the XML processor must further process the normalized\n          attribute value by discarding any leading and trailing\n          space (#x20) characters, and by replacing sequences of\n           space (#x20) characters by single space (#x20) character.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def has_value(key, value=None):\n    '''\n    \n    '''\n    if not isinstance(key, six.string_types):\n        log.debug('%s: \\'key\\' argument is not a string type: \\'%s\\'', __name__, key)\n        return False\n    try:\n        cur_val = os.environ[key]\n        if value is not None:\n            if cur_val == value:\n                return True\n            else:\n                return False\n    except KeyError:\n        return False\n    return True", "output": "Determine whether the key exists in the current salt process\n    environment dictionary. Optionally compare the current value\n    of the environment against the supplied value string.\n\n    key\n        Must be a string. Used as key for environment lookup.\n\n    value:\n        Optional. If key exists in the environment, compare the\n        current value with this value. Return True if they are equal.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' environ.has_value foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def maybe_infer_tz(tz, inferred_tz):\n    \"\"\"\n    \n    \"\"\"\n    if tz is None:\n        tz = inferred_tz\n    elif inferred_tz is None:\n        pass\n    elif not timezones.tz_compare(tz, inferred_tz):\n        raise TypeError('data is already tz-aware {inferred_tz}, unable to '\n                        'set specified tz: {tz}'\n                        .format(inferred_tz=inferred_tz, tz=tz))\n    return tz", "output": "If a timezone is inferred from data, check that it is compatible with\n    the user-provided timezone, if any.\n\n    Parameters\n    ----------\n    tz : tzinfo or None\n    inferred_tz : tzinfo or None\n\n    Returns\n    -------\n    tz : tzinfo or None\n\n    Raises\n    ------\n    TypeError : if both timezones are present but do not match", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_response(self):\n        \"\"\"\n        \n        \"\"\"\n        user_input = self.usr_input.get()\n        self.usr_input.delete(0, tk.END)\n\n        response = self.chatbot.get_response(user_input)\n\n        self.conversation['state'] = 'normal'\n        self.conversation.insert(\n            tk.END, \"Human: \" + user_input + \"\\n\" + \"ChatBot: \" + str(response.text) + \"\\n\"\n        )\n        self.conversation['state'] = 'disabled'\n\n        time.sleep(0.5)", "output": "Get a response from the chatbot and display it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def remove_datastore(datastore, service_instance=None):\n    '''\n    \n    '''\n    log.trace('Removing datastore \\'%s\\'', datastore)\n    target = _get_proxy_target(service_instance)\n    datastores = salt.utils.vmware.get_datastores(\n        service_instance,\n        reference=target,\n        datastore_names=[datastore])\n    if not datastores:\n        raise VMwareObjectRetrievalError(\n            'Datastore \\'{0}\\' was not found'.format(datastore))\n    if len(datastores) > 1:\n        raise VMwareObjectRetrievalError(\n            'Multiple datastores \\'{0}\\' were found'.format(datastore))\n    salt.utils.vmware.remove_datastore(service_instance, datastores[0])\n    return True", "output": "Removes a datastore. If multiple datastores an error is raised.\n\n    datastore\n        Datastore name\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.remove_datastore ds_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy_value(self, orig_name, new_name):\r\n        \"\"\"\"\"\"\r\n        self.shellwidget.copy_value(orig_name, new_name)\r\n        self.shellwidget.refresh_namespacebrowser()", "output": "Copy value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restore(self):\n        \"\"\"\"\"\"\n        current, self.nlp.pipeline = self.nlp.pipeline, self.original_pipeline\n        unexpected = [name for name, pipe in current if not self.nlp.has_pipe(name)]\n        if unexpected:\n            # Don't change the pipeline if we're raising an error.\n            self.nlp.pipeline = current\n            raise ValueError(Errors.E008.format(names=unexpected))\n        self[:] = []", "output": "Restore the pipeline to its state when DisabledPipes was created.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def logical_and(lhs, rhs):\n    \"\"\"\n    \"\"\"\n    # pylint: disable= no-member, protected-access\n    return _ufunc_helper(\n        lhs,\n        rhs,\n        op.broadcast_logical_and,\n        lambda x, y: 1 if x and y else 0,\n        _internal._logical_and_scalar,\n        None)", "output": "Returns the result of element-wise **logical and** comparison\n    operation with broadcasting.\n\n    For each element in input arrays, return 1(true) if lhs elements and rhs elements\n    are true, otherwise return 0(false).\n\n    Equivalent to ``lhs and rhs`` and ``mx.nd.broadcast_logical_and(lhs, rhs)``.\n\n    .. note::\n\n       If the corresponding dimensions of two arrays have the same size or one of them has size 1,\n       then the arrays are broadcastable to a common shape.\n\n    Parameters\n    ----------\n    lhs : scalar or mxnet.ndarray.array\n        First input of the function.\n    rhs : scalar or mxnet.ndarray.array\n         Second input of the function. If ``lhs.shape != rhs.shape``, they must be\n        broadcastable to a common shape.\n\n    Returns\n    -------\n    NDArray\n        Output array of boolean values.\n\n    Examples\n    --------\n    >>> x = mx.nd.ones((2,3))\n    >>> y = mx.nd.arange(2).reshape((2,1))\n    >>> z = mx.nd.arange(2).reshape((1,2))\n    >>> x.asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> y.asnumpy()\n    array([[ 0.],\n           [ 1.]], dtype=float32)\n    >>> z.asnumpy()\n    array([[ 0.,  1.]], dtype=float32)\n    >>> mx.nd.logical_and(x, 1).asnumpy()\n    array([[ 1.,  1.,  1.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.logical_and(x, y).asnumpy()\n    array([[ 0.,  0.,  0.],\n           [ 1.,  1.,  1.]], dtype=float32)\n    >>> mx.nd.logical_and(z, y).asnumpy()\n    array([[ 0.,  0.],\n           [ 0.,  1.]], dtype=float32)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def done(self):\n        \"\"\"\n        \"\"\"\n        return self._exception != self._SENTINEL or self._result != self._SENTINEL", "output": "Return True the future is done, False otherwise.\n\n        This still returns True in failure cases; checking :meth:`result` or\n        :meth:`exception` is the canonical way to assess success or failure.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fit_transform(self, data):\n        \"\"\"\n        \n\n        \"\"\"\n        if not self._transformers:\n            return self._preprocess(data)\n\n        transformed_data = self._preprocess(data)\n        final_step = self._transformers[-1]\n        return final_step[1].fit_transform(transformed_data)", "output": "First fit a transformer using the SFrame `data` and then return a transformed\n        version of `data`.\n\n        Parameters\n        ----------\n        data : SFrame\n            The data used to fit the transformer. The same data is then also\n            transformed.\n\n        Returns\n        -------\n        Transformed SFrame.\n\n        See Also\n        --------\n        transform, fit_transform\n\n        Notes\n        -----\n        - The default implementation calls fit() and then calls transform().\n          You may override this function with a more efficient implementation.\"\n\n        Examples\n        --------\n        .. sourcecode:: python\n\n          >> transformed_sf = chain.fit_transform(sf)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set(self, name, value):\n        \"\"\"\"\"\"\n        name = self._convert_name(name)\n\n        if ((name in _ELEMENTSFIELD or name == 'Platform') and\n            not isinstance(value, (list, tuple))):\n            if isinstance(value, string_types):\n                value = [v.strip() for v in value.split(',')]\n            else:\n                value = []\n        elif (name in _LISTFIELDS and\n              not isinstance(value, (list, tuple))):\n            if isinstance(value, string_types):\n                value = [value]\n            else:\n                value = []\n\n        if logger.isEnabledFor(logging.WARNING):\n            project_name = self['Name']\n\n            scheme = get_scheme(self.scheme)\n            if name in _PREDICATE_FIELDS and value is not None:\n                for v in value:\n                    # check that the values are valid\n                    if not scheme.is_valid_matcher(v.split(';')[0]):\n                        logger.warning(\n                            \"'%s': '%s' is not valid (field '%s')\",\n                            project_name, v, name)\n            # FIXME this rejects UNKNOWN, is that right?\n            elif name in _VERSIONS_FIELDS and value is not None:\n                if not scheme.is_valid_constraint_list(value):\n                    logger.warning(\"'%s': '%s' is not a valid version (field '%s')\",\n                                   project_name, value, name)\n            elif name in _VERSION_FIELDS and value is not None:\n                if not scheme.is_valid_version(value):\n                    logger.warning(\"'%s': '%s' is not a valid version (field '%s')\",\n                                   project_name, value, name)\n\n        if name in _UNICODEFIELDS:\n            if name == 'Description':\n                value = self._remove_line_prefix(value)\n\n        self._fields[name] = value", "output": "Control then set a metadata field.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_dense_tensor(self, inputs, weight_collections=None, trainable=None):\n    \"\"\"\"\"\"\n    del weight_collections\n    text_batch = tf.reshape(inputs.get(self), shape=[-1])\n    m = module.Module(self.module_spec, trainable=self.trainable and trainable)\n    return m(text_batch)", "output": "Returns a `Tensor`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_optional_arg_param(self, param_name, layer_index, blob_index):\n        \"\"\"\"\"\"\n        blobs = self.layers[layer_index].blobs\n        if blob_index < len(blobs):\n            self.add_arg_param(param_name, layer_index, blob_index)", "output": "Add an arg param. If there is no such param in .caffemodel fie, silently ignore it.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_image(kwargs, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The show_images function must be called with '\n            '-f or --function'\n        )\n\n    if not isinstance(kwargs, dict):\n        kwargs = {}\n\n    images = kwargs['image']\n    images = images.split(',')\n\n    params = {\n        'action': 'DescribeImages',\n        'images': images,\n        'zone': _get_specified_zone(kwargs, get_configured_provider()),\n    }\n\n    items = query(params=params)\n\n    if not items['image_set']:\n        raise SaltCloudNotFound('The specified image could not be found.')\n\n    result = {}\n    for image in items['image_set']:\n        result[image['image_id']] = {}\n        for key in image:\n            result[image['image_id']][key] = image[key]\n\n    return result", "output": "Show the details from QingCloud concerning an image.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f show_image my-qingcloud image=trustysrvx64c\n        salt-cloud -f show_image my-qingcloud image=trustysrvx64c,coreos4\n        salt-cloud -f show_image my-qingcloud image=trustysrvx64c zone=ap1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def debug_print(*message):\r\n    \"\"\"\"\"\"\r\n    warnings.warn(\"debug_print is deprecated; use the logging module instead.\")\r\n    if get_debug_level():\r\n        ss = STDOUT\r\n        if PY3:\r\n            # This is needed after restarting and using debug_print\r\n            for m in message:\r\n                ss.buffer.write(str(m).encode('utf-8'))\r\n            print('', file=ss)\r\n        else:\r\n            print(*message, file=ss)", "output": "Output debug messages to stdout", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fixed_crop(src, x0, y0, w, h, size=None, interpolation=cv2.INTER_CUBIC):\n    \"\"\"\"\"\"\n    out = mx.nd.crop(src, begin=(y0, x0, 0), end=(y0+h, x0+w, int(src.shape[2])))\n    if size is not None and (w, h) != size:\n        out = resize(out, size, interpolation=interpolation)\n    return out", "output": "Crop src at fixed location, and (optionally) resize it to size", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_sparse(arr, kind='block', fill_value=None, dtype=None, copy=False):\n    \"\"\"\n    \n    \"\"\"\n\n    arr = _sanitize_values(arr)\n\n    if arr.ndim > 1:\n        raise TypeError(\"expected dimension <= 1 data\")\n\n    if fill_value is None:\n        fill_value = na_value_for_dtype(arr.dtype)\n\n    if isna(fill_value):\n        mask = notna(arr)\n    else:\n        # cast to object comparison to be safe\n        if is_string_dtype(arr):\n            arr = arr.astype(object)\n\n        if is_object_dtype(arr.dtype):\n            # element-wise equality check method in numpy doesn't treat\n            # each element type, eg. 0, 0.0, and False are treated as\n            # same. So we have to check the both of its type and value.\n            mask = splib.make_mask_object_ndarray(arr, fill_value)\n        else:\n            mask = arr != fill_value\n\n    length = len(arr)\n    if length != len(mask):\n        # the arr is a SparseArray\n        indices = mask.sp_index.indices\n    else:\n        indices = mask.nonzero()[0].astype(np.int32)\n\n    index = _make_index(length, indices, kind)\n    sparsified_values = arr[mask]\n    if dtype is not None:\n        sparsified_values = astype_nansafe(sparsified_values, dtype=dtype)\n    # TODO: copy\n    return sparsified_values, index, fill_value", "output": "Convert ndarray to sparse format\n\n    Parameters\n    ----------\n    arr : ndarray\n    kind : {'block', 'integer'}\n    fill_value : NaN or another value\n    dtype : np.dtype, optional\n    copy : bool, default False\n\n    Returns\n    -------\n    (sparse_values, index, fill_value) : (ndarray, SparseIndex, Scalar)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_datetime64_ns_dtype(arr_or_dtype):\n    \"\"\"\n    \n    \"\"\"\n\n    if arr_or_dtype is None:\n        return False\n    try:\n        tipo = _get_dtype(arr_or_dtype)\n    except TypeError:\n        if is_datetime64tz_dtype(arr_or_dtype):\n            tipo = _get_dtype(arr_or_dtype.dtype)\n        else:\n            return False\n    return tipo == _NS_DTYPE or getattr(tipo, 'base', None) == _NS_DTYPE", "output": "Check whether the provided array or dtype is of the datetime64[ns] dtype.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array or dtype to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array or dtype is of the datetime64[ns] dtype.\n\n    Examples\n    --------\n    >>> is_datetime64_ns_dtype(str)\n    False\n    >>> is_datetime64_ns_dtype(int)\n    False\n    >>> is_datetime64_ns_dtype(np.datetime64)  # no unit\n    False\n    >>> is_datetime64_ns_dtype(DatetimeTZDtype(\"ns\", \"US/Eastern\"))\n    True\n    >>> is_datetime64_ns_dtype(np.array(['a', 'b']))\n    False\n    >>> is_datetime64_ns_dtype(np.array([1, 2]))\n    False\n    >>> is_datetime64_ns_dtype(np.array([], dtype=np.datetime64))  # no unit\n    False\n    >>> is_datetime64_ns_dtype(np.array([],\n                               dtype=\"datetime64[ps]\"))  # wrong unit\n    False\n    >>> is_datetime64_ns_dtype(pd.DatetimeIndex([1, 2, 3],\n                               dtype=np.datetime64))  # has 'ns' unit\n    True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schema_get(name, profile=None):\n    '''\n    \n    '''\n    g_client = _auth(profile)\n    schema_props = {}\n    for prop in g_client.schemas.get(name).properties:\n        schema_props[prop.name] = prop.description\n    log.debug(\n        'Properties of schema %s:\\n%s',\n        name, pprint.PrettyPrinter(indent=4).pformat(schema_props)\n    )\n    return {name: schema_props}", "output": "Known valid names of schemas are:\n      - image\n      - images\n      - member\n      - members\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' glance.schema_get name=f16-jeos", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def process(self, processor:PreProcessors=None):\n        \"\"\n        if processor is not None: self.processor = processor\n        self.processor = listify(self.processor)\n        for p in self.processor: p.process(self)\n        return self", "output": "Apply `processor` or `self.processor` to `self`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def relabel(self, change):\n        \"\"\n        class_new,class_old,file_path = change.new,change.old,change.owner.file_path\n        fp = Path(file_path)\n        parent = fp.parents[1]\n        self._csv_dict[fp] = class_new", "output": "Relabel images by moving from parent dir with old label `class_old` to parent dir with new label `class_new`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def view(self, dtype=None):\n        \"\"\"\n        \n        \"\"\"\n        return self._constructor(self._values.view(dtype),\n                                 index=self.index).__finalize__(self)", "output": "Create a new view of the Series.\n\n        This function will return a new Series with a view of the same\n        underlying values in memory, optionally reinterpreted with a new data\n        type. The new data type must preserve the same size in bytes as to not\n        cause index misalignment.\n\n        Parameters\n        ----------\n        dtype : data type\n            Data type object or one of their string representations.\n\n        Returns\n        -------\n        Series\n            A new Series object as a view of the same data in memory.\n\n        See Also\n        --------\n        numpy.ndarray.view : Equivalent numpy function to create a new view of\n            the same data in memory.\n\n        Notes\n        -----\n        Series are instantiated with ``dtype=float64`` by default. While\n        ``numpy.ndarray.view()`` will return a view with the same data type as\n        the original array, ``Series.view()`` (without specified dtype)\n        will try using ``float64`` and may fail if the original data type size\n        in bytes is not the same.\n\n        Examples\n        --------\n        >>> s = pd.Series([-2, -1, 0, 1, 2], dtype='int8')\n        >>> s\n        0   -2\n        1   -1\n        2    0\n        3    1\n        4    2\n        dtype: int8\n\n        The 8 bit signed integer representation of `-1` is `0b11111111`, but\n        the same bytes represent 255 if read as an 8 bit unsigned integer:\n\n        >>> us = s.view('uint8')\n        >>> us\n        0    254\n        1    255\n        2      0\n        3      1\n        4      2\n        dtype: uint8\n\n        The views share the same underlying values:\n\n        >>> us[0] = 128\n        >>> s\n        0   -128\n        1     -1\n        2      0\n        3      1\n        4      2\n        dtype: int8", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, statement):\n        \"\"\"\n        \n        \"\"\"\n        Statement = self.get_model('statement')\n        Tag = self.get_model('tag')\n\n        if hasattr(statement, 'id'):\n            statement.save()\n        else:\n            statement = Statement.objects.create(\n                text=statement.text,\n                search_text=self.tagger.get_bigram_pair_string(statement.text),\n                conversation=statement.conversation,\n                in_response_to=statement.in_response_to,\n                search_in_response_to=self.tagger.get_bigram_pair_string(statement.in_response_to),\n                created_at=statement.created_at\n            )\n\n        for _tag in statement.tags.all():\n            tag, _ = Tag.objects.get_or_create(name=_tag)\n\n            statement.tags.add(tag)\n\n        return statement", "output": "Update the provided statement.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _equals(v1_indices, v1_values, v2_indices, v2_values):\n        \"\"\"\n        \n        \"\"\"\n        v1_size = len(v1_values)\n        v2_size = len(v2_values)\n        k1 = 0\n        k2 = 0\n        all_equal = True\n        while all_equal:\n            while k1 < v1_size and v1_values[k1] == 0:\n                k1 += 1\n            while k2 < v2_size and v2_values[k2] == 0:\n                k2 += 1\n\n            if k1 >= v1_size or k2 >= v2_size:\n                return k1 >= v1_size and k2 >= v2_size\n\n            all_equal = v1_indices[k1] == v2_indices[k2] and v1_values[k1] == v2_values[k2]\n            k1 += 1\n            k2 += 1\n        return all_equal", "output": "Check equality between sparse/dense vectors,\n        v1_indices and v2_indices assume to be strictly increasing.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _string_width(string, *, _IS_ASCII=_IS_ASCII):\n    \"\"\"\"\"\"\n    match = _IS_ASCII.match(string)\n    if match:\n        return match.endpos\n\n    UNICODE_WIDE_CHAR_TYPE = 'WFA'\n    width = 0\n    func = unicodedata.east_asian_width\n    for char in string:\n        width += 2 if func(char) in UNICODE_WIDE_CHAR_TYPE else 1\n    return width", "output": "Returns string's width.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def context_menu_requested(self, event):\n        \"\"\"\"\"\"\n        if self.fig:\n            pos = QPoint(event.x(), event.y())\n            context_menu = QMenu(self)\n            context_menu.addAction(ima.icon('editcopy'), \"Copy Image\",\n                                   self.copy_figure,\n                                   QKeySequence(\n                                       get_shortcut('plots', 'copy')))\n            context_menu.popup(self.mapToGlobal(pos))", "output": "Popup context menu.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _compute_edge_transforms(node_states,\n                             depth,\n                             num_transforms,\n                             name=\"transform\"):\n  \"\"\"\n  \"\"\"\n  node_shapes = common_layers.shape_list(node_states)\n  x = common_layers.dense(\n      node_states,\n      depth * num_transforms,\n      use_bias=False,\n      name=name)\n\n  batch = node_shapes[0]  # B.\n  length = node_shapes[1]  # N.\n\n  # Making the fourth dimension explicit by separating the vectors of size\n  # K*T (in k) and V*T (in v) into two-dimensional matrices with shape [K, T]\n  # (in k) and [V, T] in v.\n  #\n  x = tf.reshape(x, [batch, length, num_transforms, depth])\n\n  # Flatten out the fourth dimension.\n  x = tf.reshape(x, [batch, length * num_transforms, depth])\n\n  return x", "output": "Helper function that computes transformation for keys and values.\n\n  Let B be the number of batches.\n  Let N be the number of nodes in the graph.\n  Let D be the size of the node hidden states.\n  Let K be the size of the attention keys/queries (total_key_depth).\n  Let V be the size of the attention values (total_value_depth).\n  Let T be the total number of transforms (num_transforms).\n\n  Computes the transforms for keys or values for attention.\n  * For each node N_j and edge type t, a key K_jt of size K is computed. When an\n    edge of type t goes from node N_j to any other node, K_jt is the key that is\n    in the attention process.\n  * For each node N_j and edge type t, a value V_jt of size V is computed. When\n    an edge of type t goes from node N_j to node N_i, Attention(Q_i, K_jt)\n    produces a weight w_ijt. The message sent along this edge is w_ijt * V_jt.\n\n  Args:\n    node_states: A tensor of shape [B, L, D]\n    depth: An integer (K or V)\n    num_transforms: An integer (T),\n    name: A name for the function\n\n  Returns:\n    x: A The attention keys or values for each node and edge type\n      (shape [B, N*T, K or V])", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def sample_from_discretized_mix_logistic(pred, seed=None):\n  \"\"\"\n  \"\"\"\n\n  logits, locs, log_scales, coeffs = split_to_discretized_mix_logistic_params(\n      pred)\n\n  # Sample mixture indicator given logits using the gumbel max trick.\n  num_mixtures = shape_list(logits)[-1]\n  gumbel_noise = -tf.log(-tf.log(\n      tf.random_uniform(\n          tf.shape(logits), minval=1e-5, maxval=1. - 1e-5, seed=seed)))\n  sel = tf.one_hot(\n      tf.argmax(logits + gumbel_noise, -1),\n      depth=num_mixtures,\n      dtype=tf.float32)\n\n  # Select mixture component's parameters.\n  sel = tf.expand_dims(sel, -1)\n  locs = tf.reduce_sum(locs * sel, 3)\n  log_scales = tf.reduce_sum(log_scales * sel, 3)\n  coeffs = tf.reduce_sum(coeffs * sel, 3)\n\n  # Sample from 3-D logistic & clip to interval. Note we don't round to the\n  # nearest 8-bit value when sampling.\n  uniform_noise = tf.random_uniform(\n      tf.shape(locs), minval=1e-5, maxval=1. - 1e-5, seed=seed)\n  logistic_noise = tf.log(uniform_noise) - tf.log1p(-uniform_noise)\n  x = locs + tf.exp(log_scales) * logistic_noise\n  x0 = x[..., 0]\n  x1 = x[..., 1] + coeffs[..., 0] * x0\n  x2 = x[..., 2] + coeffs[..., 1] * x0 + coeffs[..., 2] * x1\n  x = tf.stack([x0, x1, x2], axis=-1)\n  x = tf.clip_by_value(x, -1., 1.)\n  return x", "output": "Sampling from a discretized mixture of logistics.\n\n  Args:\n    pred: A [batch, height, width, num_mixtures*10] tensor of floats\n      comprising one unconstrained mixture probability, three means\n      (one per channel), three standard deviations (one per channel),\n      and three coefficients which linearly parameterize dependence across\n      channels.\n    seed: Random seed.\n\n  Returns:\n    A tensor of shape [batch, height, width, 3] with real intensities scaled\n    between -1 and 1.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show(commands,\n         raw_text=True,\n         **kwargs):\n    '''\n    \n    '''\n    ret = []\n    if raw_text:\n        method = 'cli_ascii'\n        key = 'msg'\n    else:\n        method = 'cli'\n        key = 'body'\n    response_list = _cli_command(commands,\n                                 method=method,\n                                 **kwargs)\n    ret = [response[key] for response in response_list if response]\n    return ret", "output": "Execute one or more show (non-configuration) commands.\n\n    commands\n        The commands to be executed.\n\n    raw_text: ``True``\n        Whether to return raw text or structured data.\n\n    transport: ``https``\n        Specifies the type of connection transport to use. Valid values for the\n        connection are ``http``, and  ``https``.\n\n    host: ``localhost``\n        The IP address or DNS host name of the connection device.\n\n    username: ``admin``\n        The username to pass to the device to authenticate the NX-API connection.\n\n    password\n        The password to pass to the device to authenticate the NX-API connection.\n\n    port\n        The TCP port of the endpoint for the NX-API connection. If this keyword is\n        not specified, the default value is automatically determined by the\n        transport type (``80`` for ``http``, or ``443`` for ``https``).\n\n    timeout: ``60``\n        Time in seconds to wait for the device to respond. Default: 60 seconds.\n\n    verify: ``True``\n        Either a boolean, in which case it controls whether we verify the NX-API\n        TLS certificate, or a string, in which case it must be a path to a CA bundle\n        to use. Defaults to ``True``.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call --local nxos_api.show 'show version'\n        salt '*' nxos_api.show 'show bgp sessions' 'show processes' raw_text=False\n        salt 'regular-minion' nxos_api.show 'show interfaces' host=sw01.example.com username=test password=test", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def batch_fn(dataset, training, shapes, target_names,\n             batch_size=32, eval_batch_size=32, bucket_batch_length=32,\n             bucket_max_length=256, bucket_min_length=8,\n             bucket_length_step=1.1, buckets=None):\n  \"\"\"\"\"\"\n  del target_names\n  # If bucketing is not specified, check if target shapes are variable.\n  cur_batch_size = batch_size if training else eval_batch_size\n  if buckets is None:\n    variable_target_shapes = False\n    target_shape = shapes[1]\n    for dim in target_shape:\n      if dim is None:\n        variable_target_shapes = True\n    tf.logging.info(\"Heuristically setting bucketing to %s based on shapes \"\n                    \"of target tensors.\" % variable_target_shapes)\n    if variable_target_shapes:\n      batch_size_per_token = cur_batch_size * bucket_batch_length\n      scheme = data_reader.batching_scheme(batch_size_per_token,\n                                           bucket_max_length,\n                                           bucket_min_length,\n                                           bucket_length_step,\n                                           drop_long_sequences=training)\n      buckets = (scheme[\"boundaries\"], scheme[\"batch_sizes\"])\n\n  if buckets:\n    tf.logging.info(\"Bucketing with buckets %s.\" % str(buckets))\n    def example_length(_, target):\n      return tf.shape(target)[0]\n    boundaries, batch_sizes = buckets\n    dataset = dataset.apply(tf.data.experimental.bucket_by_sequence_length(\n        example_length, boundaries, batch_sizes))\n  else:\n    dataset = dataset.padded_batch(cur_batch_size, shapes)\n  return dataset", "output": "Batching function.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def keypoint_vflip(kp, rows, cols):\n    \"\"\"\"\"\"\n    x, y, angle, scale = kp\n    c = math.cos(angle)\n    s = math.sin(angle)\n    angle = math.atan2(-s, c)\n    return [x, (rows - 1) - y, angle, scale]", "output": "Flip a keypoint vertically around the x-axis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_django_admin(bin_env):\n    '''\n    \n    '''\n    if not bin_env:\n        if salt.utils.path.which('django-admin.py'):\n            return 'django-admin.py'\n        elif salt.utils.path.which('django-admin'):\n            return 'django-admin'\n        else:\n            raise salt.exceptions.CommandExecutionError(\n                    \"django-admin or django-admin.py not found on PATH\")\n\n    # try to get django-admin.py bin from env\n    if os.path.exists(os.path.join(bin_env, 'bin', 'django-admin.py')):\n        return os.path.join(bin_env, 'bin', 'django-admin.py')\n    return bin_env", "output": "Return the django admin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_bokehjs(bokehjs_action, develop=False):\n    ''' \n\n    '''\n    print()\n    if develop:\n        print(\"Installed Bokeh for DEVELOPMENT:\")\n    else:\n        print(\"Installed Bokeh:\")\n    if bokehjs_action in ['built', 'installed']:\n        print(\"  - using %s built BokehJS from bokehjs/build\\n\" % (bright(yellow(\"NEWLY\")) if bokehjs_action=='built' else bright(yellow(\"PREVIOUSLY\"))))\n    else:\n        print(\"  - using %s BokehJS, located in 'bokeh.server.static'\\n\" % bright(yellow(\"PACKAGED\")))\n    print()", "output": "Print a useful report after setuptools output describing where and how\n    BokehJS is installed.\n\n    Args:\n        bokehjs_action (str) : one of 'built', 'installed', or 'packaged'\n            how (or if) BokehJS was installed into the python source tree\n\n        develop (bool, optional) :\n            whether the command was for \"develop\" mode (default: False)\n\n    Returns:\n        None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def xmoe_tr_dense_2k():\n  \"\"\"\n  \"\"\"\n  hparams = mtf_transformer2.mtf_bitransformer_base()\n  hparams.encoder_layers = [\"self_att\", \"drd\"] * 4\n  hparams.decoder_layers = [\"self_att\", \"enc_att\", \"drd\"] * 4\n  hparams.batch_size = 64\n  hparams.shared_embedding_and_softmax_weights = True\n  hparams.mesh_shape = \"batch:8\"\n  return hparams", "output": "Series of architectural experiments on Translation.\n\n  # run on 8-core setup\n\n  119M params, einsum=0.95e13\n\n  Returns:\n    a hparams", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def explore(self, title=None):\n        \"\"\"\n        \n        \"\"\"\n\n        import sys\n        import os\n\n        if sys.platform != 'darwin' and sys.platform != 'linux2' and sys.platform != 'linux':\n            raise NotImplementedError('Visualization is currently supported only on macOS and Linux.')\n\n        path_to_client = _get_client_app_path()\n\n        if title is None:\n            title = \"\"\n        self.__proxy__.explore(path_to_client, title)", "output": "Explore the SFrame in an interactive GUI. Opens a new app window.\n\n        Parameters\n        ----------\n        title : str\n            The plot title to show for the resulting visualization. Defaults to None.\n            If the title is None, a default title will be provided.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Suppose 'sf' is an SFrame, we can view it using:\n\n        >>> sf.explore()\n\n        To override the default plot title and axis labels:\n\n        >>> sf.explore(title=\"My Plot Title\")", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _reverse_op(name, doc=\"binary operator\"):\n    \"\"\" \n    \"\"\"\n    def _(self, other):\n        jother = _create_column_from_literal(other)\n        jc = getattr(jother, name)(self._jc)\n        return Column(jc)\n    _.__doc__ = doc\n    return _", "output": "Create a method for binary operator (this object is on right side)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exception(self, timeout=None):\n        \"\"\"\n        \"\"\"\n        # Wait until the future is done.\n        if not self._completed.wait(timeout=timeout):\n            raise exceptions.TimeoutError(\"Timed out waiting for result.\")\n\n        # If the batch completed successfully, this should return None.\n        if self._result != self._SENTINEL:\n            return None\n\n        # Okay, this batch had an error; this should return it.\n        return self._exception", "output": "Return the exception raised by the call, if any.\n\n        This blocks until the message has successfully been published, and\n        returns the exception. If the call succeeded, return None.\n\n        Args:\n            timeout (Union[int, float]): The number of seconds before this call\n                times out and raises TimeoutError.\n\n        Raises:\n            TimeoutError: If the request times out.\n\n        Returns:\n            Exception: The exception raised by the call, if any.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _read_info(info_path):\n  \"\"\"\"\"\"\n  if not tf.io.gfile.exists(info_path):\n    return None\n  with tf.io.gfile.GFile(info_path) as info_f:\n    return json.load(info_f)", "output": "Returns info dict or None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def run(self):\n        \"\"\"\n        \n        \"\"\"\n        connection = self.output().connect()\n        # kill any sessions other than ours and\n        # internal Redshift sessions (rdsdb)\n        query = (\"select pg_terminate_backend(process) \"\n                 \"from STV_SESSIONS \"\n                 \"where db_name=%s \"\n                 \"and user_name != 'rdsdb' \"\n                 \"and process != pg_backend_pid()\")\n        cursor = connection.cursor()\n        logger.info('Killing all open Redshift sessions for database: %s', self.database)\n        try:\n            cursor.execute(query, (self.database,))\n            cursor.close()\n            connection.commit()\n        except psycopg2.DatabaseError as e:\n            if e.message and 'EOF' in e.message:\n                # sometimes this operation kills the current session.\n                # rebuild the connection. Need to pause for 30-60 seconds\n                # before Redshift will allow us back in.\n                connection.close()\n                logger.info('Pausing %s seconds for Redshift to reset connection', self.connection_reset_wait_seconds)\n                time.sleep(self.connection_reset_wait_seconds)\n                logger.info('Reconnecting to Redshift')\n                connection = self.output().connect()\n            else:\n                raise\n\n        try:\n            self.output().touch(connection)\n            connection.commit()\n        finally:\n            connection.close()\n\n        logger.info('Done killing all open Redshift sessions for database: %s', self.database)", "output": "Kill any open Redshift sessions for the given database.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rfc_cv(n_estimators, min_samples_split, max_features, data, targets):\n    \"\"\"\n    \"\"\"\n    estimator = RFC(\n        n_estimators=n_estimators,\n        min_samples_split=min_samples_split,\n        max_features=max_features,\n        random_state=2\n    )\n    cval = cross_val_score(estimator, data, targets,\n                           scoring='neg_log_loss', cv=4)\n    return cval.mean()", "output": "Random Forest cross validation.\n\n    This function will instantiate a random forest classifier with parameters\n    n_estimators, min_samples_split, and max_features. Combined with data and\n    targets this will in turn be used to perform cross validation. The result\n    of cross validation is returned.\n\n    Our goal is to find combinations of n_estimators, min_samples_split, and\n    max_features that minimzes the log loss.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_network_interface(name, resource_group):\n    '''\n    \n    '''\n    public_ips = []\n    private_ips = []\n    netapi_versions = get_api_versions(kwargs={\n        'resource_provider': 'Microsoft.Network',\n        'resource_type': 'publicIPAddresses'\n        }\n    )\n    netapi_version = netapi_versions[0]\n    netconn = get_conn(client_type='network')\n    netiface_query = netconn.network_interfaces.get(\n        resource_group_name=resource_group,\n        network_interface_name=name\n    )\n\n    netiface = netiface_query.as_dict()\n    for index, ip_config in enumerate(netiface['ip_configurations']):\n        if ip_config.get('private_ip_address') is not None:\n            private_ips.append(ip_config['private_ip_address'])\n        if 'id' in ip_config.get('public_ip_address', {}):\n            public_ip_name = get_resource_by_id(\n                ip_config['public_ip_address']['id'],\n                netapi_version,\n                'name'\n            )\n            public_ip = _get_public_ip(public_ip_name, resource_group)\n            public_ips.append(public_ip['ip_address'])\n            netiface['ip_configurations'][index]['public_ip_address'].update(public_ip)\n\n    return netiface, public_ips, private_ips", "output": "Get a network interface.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ID(self, ID):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlGetID(self._o, ID)\n        if ret is None:raise treeError('xmlGetID() failed')\n        __tmp = xmlAttr(_obj=ret)\n        return __tmp", "output": "Search the attribute declaring the given ID", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_full_slice(obj, l):\n    \"\"\"\n    \n    \"\"\"\n    return (isinstance(obj, slice) and obj.start == 0 and obj.stop == l and\n            obj.step is None)", "output": "We have a full length slice.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_(profile=\"splunk\"):\n    '''\n    \n    '''\n    client = _get_splunk(profile)\n    searches = [x['name'] for x in client.saved_searches]\n    return searches", "output": "List splunk searches (names only)\n\n    CLI Example:\n        splunk_search.list", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_toolbar(self, title, object_name, iconsize=24):\r\n        \"\"\"\"\"\"\r\n        toolbar = self.addToolBar(title)\r\n        toolbar.setObjectName(object_name)\r\n        toolbar.setIconSize(QSize(iconsize, iconsize))\r\n        self.toolbarslist.append(toolbar)\r\n        return toolbar", "output": "Create and return toolbar with *title* and *object_name*", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resource_group_get(name, **kwargs):\n    '''\n    \n\n    '''\n    result = {}\n    resconn = __utils__['azurearm.get_client']('resource', **kwargs)\n    try:\n        group = resconn.resource_groups.get(name)\n        result = group.as_dict()\n\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('resource', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "output": ".. versionadded:: 2019.2.0\n\n    Get a dictionary representing a resource group's properties.\n\n    :param name: The resource group name to get.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_resource.resource_group_get testgroup", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def handle(self, message, connection):\n        ''' \n\n        '''\n\n        handler = self._handlers.get((message.msgtype, message.revision))\n\n        if handler is None:\n            handler = self._handlers.get(message.msgtype)\n\n        if handler is None:\n            raise ProtocolError(\"%s not expected on server\" % message)\n\n        try:\n            work = yield handler(message, connection)\n        except Exception as e:\n            log.error(\"error handling message %r: %r\", message, e)\n            log.debug(\"  message header %r content %r\", message.header, message.content, exc_info=1)\n            work = connection.error(message, repr(e))\n        raise gen.Return(work)", "output": "Delegate a received message to the appropriate handler.\n\n        Args:\n            message (Message) :\n                The message that was receive that needs to be handled\n\n            connection (ServerConnection) :\n                The connection that received this message\n\n        Raises:\n            ProtocolError", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_internet_gateway(internet_gateway_name=None, vpc_id=None,\n                            vpc_name=None, tags=None, region=None, key=None,\n                            keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        if vpc_id or vpc_name:\n            vpc_id = check_vpc(vpc_id, vpc_name, region, key, keyid, profile)\n            if not vpc_id:\n                return {'created': False,\n                        'error': {'message': 'VPC {0} does not exist.'.format(vpc_name or vpc_id)}}\n\n        r = _create_resource('internet_gateway', name=internet_gateway_name,\n                             tags=tags, region=region, key=key, keyid=keyid,\n                             profile=profile)\n        if r.get('created') and vpc_id:\n            conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n            conn.attach_internet_gateway(r['id'], vpc_id)\n            log.info(\n                'Attached internet gateway %s to VPC %s',\n                r['id'], vpc_name or vpc_id\n            )\n        return r\n    except BotoServerError as e:\n        return {'created': False, 'error': __utils__['boto.get_error'](e)}", "output": "Create an Internet Gateway, optionally attaching it to an existing VPC.\n\n    Returns the internet gateway id if the internet gateway was created and\n    returns False if the internet gateways was not created.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.create_internet_gateway \\\\\n                internet_gateway_name=myigw vpc_name=myvpc", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def iter_find_files(directory, patterns, ignored=None):\n    \"\"\"\n\n    \"\"\"\n    if isinstance(patterns, basestring):\n        patterns = [patterns]\n    pats_re = re.compile('|'.join([fnmatch.translate(p) for p in patterns]))\n\n    if not ignored:\n        ignored = []\n    elif isinstance(ignored, basestring):\n        ignored = [ignored]\n    ign_re = re.compile('|'.join([fnmatch.translate(p) for p in ignored]))\n    for root, dirs, files in os.walk(directory):\n        for basename in files:\n            if pats_re.match(basename):\n                if ignored and ign_re.match(basename):\n                    continue\n                filename = os.path.join(root, basename)\n                yield filename\n    return", "output": "Returns a generator that yields file paths under a *directory*,\n    matching *patterns* using `glob`_ syntax (e.g., ``*.txt``). Also\n    supports *ignored* patterns.\n\n    Args:\n        directory (str): Path that serves as the root of the\n            search. Yielded paths will include this as a prefix.\n        patterns (str or list): A single pattern or list of\n            glob-formatted patterns to find under *directory*.\n        ignored (str or list): A single pattern or list of\n            glob-formatted patterns to ignore.\n\n    For example, finding Python files in the directory of this module:\n\n    >>> files = set(iter_find_files(os.path.dirname(__file__), '*.py'))\n\n    Or, Python files while ignoring emacs lockfiles:\n\n    >>> filenames = iter_find_files('.', '*.py', ignored='.#*')\n\n    .. _glob: https://en.wikipedia.org/wiki/Glob_%28programming%29", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mask_sigint():\n    \"\"\"\n    \n    \"\"\"\n    if is_main_thread():\n        sigint_handler = signal.signal(signal.SIGINT, signal.SIG_IGN)\n        yield True\n        signal.signal(signal.SIGINT, sigint_handler)\n    else:\n        yield False", "output": "Returns:\n        If called in main thread, returns a context where ``SIGINT`` is ignored, and yield True.\n        Otherwise yield False.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_sid_string(principal):\n    '''\n    \n    '''\n    # If None is passed, use the Universal Well-known SID \"Null SID\"\n    if principal is None:\n        principal = 'NULL SID'\n\n    try:\n        return win32security.ConvertSidToStringSid(principal)\n    except TypeError:\n        # Not a PySID object\n        principal = get_sid(principal)\n\n    try:\n        return win32security.ConvertSidToStringSid(principal)\n    except pywintypes.error:\n        log.exception('Invalid principal %s', principal)\n        raise CommandExecutionError('Invalid principal {0}'.format(principal))", "output": "Converts a PySID object to a string SID.\n\n    Args:\n\n        principal(str):\n            The principal to lookup the sid. Must be a PySID object.\n\n    Returns:\n        str: A string sid\n\n    Usage:\n\n    .. code-block:: python\n\n        # Get a PySID object\n        py_sid = salt.utils.win_dacl.get_sid('jsnuffy')\n\n        # Get the string version of the SID\n        salt.utils.win_dacl.get_sid_string(py_sid)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def absent(name):\n    '''\n    \n    '''\n\n    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n\n    try:\n        index = __salt__['elasticsearch.index_get'](index=name)\n        if index and name in index:\n            if __opts__['test']:\n                ret['comment'] = 'Index {0} will be removed'.format(name)\n                ret['changes']['old'] = index[name]\n                ret['result'] = None\n            else:\n                ret['result'] = __salt__['elasticsearch.index_delete'](index=name)\n                if ret['result']:\n                    ret['comment'] = 'Successfully removed index {0}'.format(name)\n                    ret['changes']['old'] = index[name]\n                else:\n                    ret['comment'] = 'Failed to remove index {0} for unknown reasons'.format(name)\n        else:\n            ret['comment'] = 'Index {0} is already absent'.format(name)\n    except Exception as err:\n        ret['result'] = False\n        ret['comment'] = six.text_type(err)\n\n    return ret", "output": "Ensure that the named index is absent.\n\n    name\n        Name of the index to remove", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def init(complete_options=False, match_incomplete=None):\n    \"\"\"\n    \"\"\"\n    global _initialized\n    if not _initialized:\n        _patch()\n        completion_configuration.complete_options = complete_options\n        if match_incomplete is not None:\n            completion_configuration.match_incomplete = match_incomplete\n        _initialized = True", "output": "Initialize the enhanced click completion\n\n    Parameters\n    ----------\n    complete_options : bool\n        always complete the options, even when the user hasn't typed a first dash (Default value = False)\n    match_incomplete : func\n        a function with two parameters choice and incomplete. Must return True\n        if incomplete is a correct match for choice, False otherwise.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def with_rank_at_most(self, rank):\n        \"\"\"\n        \"\"\"\n        if self.ndims is not None and self.ndims > rank:\n            raise ValueError(\"Shape %s must have rank at most %d\" % (self, rank))\n        else:\n            return self", "output": "Returns a shape based on `self` with at most the given rank.\n\n        Args:\n          rank: An integer.\n\n        Returns:\n          A shape that is at least as specific as `self` with at most the given\n          rank.\n\n        Raises:\n          ValueError: If `self` does not represent a shape with at most the given\n            `rank`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def f_comb(self, pos, sample):\n        \"\"\"\n        \"\"\"\n        ret = 0\n        for i in range(self.effective_model_num):\n            model = self.effective_model[i]\n            y = self.predict_y(model, pos)\n            ret += sample[i] * y\n        return ret", "output": "return the value of the f_comb when epoch = pos\n\n        Parameters\n        ----------\n        pos: int\n            the epoch number of the position you want to predict\n        sample: list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        int\n            The expected matrix at pos with all the active function's prediction", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def vanilla_lstm(num_hidden, indata, prev_state, param, seqidx, layeridx, is_batchnorm=False, gamma=None, beta=None, name=None):\n    \"\"\"\"\"\"\n    i2h = mx.sym.FullyConnected(data=indata,\n                                weight=param.i2h_weight,\n                                bias=param.i2h_bias,\n                                num_hidden=num_hidden * 4,\n                                name=\"t%d_l%d_i2h\" % (seqidx, layeridx))\n    if is_batchnorm:\n        if name is not None:\n            i2h = batchnorm(net=i2h, gamma=gamma, beta=beta, name=\"%s_batchnorm\" % name)\n        else:\n            i2h = batchnorm(net=i2h, gamma=gamma, beta=beta)\n    h2h = mx.sym.FullyConnected(data=prev_state.h,\n                                weight=param.h2h_weight,\n                                bias=param.h2h_bias,\n                                num_hidden=num_hidden * 4,\n                                name=\"t%d_l%d_h2h\" % (seqidx, layeridx))\n    gates = i2h + h2h\n    slice_gates = mx.sym.SliceChannel(gates, num_outputs=4,\n                                      name=\"t%d_l%d_slice\" % (seqidx, layeridx))\n    in_gate = mx.sym.Activation(slice_gates[0], act_type=\"sigmoid\")\n    in_transform = mx.sym.Activation(slice_gates[1], act_type=\"tanh\")\n    forget_gate = mx.sym.Activation(slice_gates[2], act_type=\"sigmoid\")\n    out_gate = mx.sym.Activation(slice_gates[3], act_type=\"sigmoid\")\n    next_c = (forget_gate * prev_state.c) + (in_gate * in_transform)\n    next_h = out_gate * mx.sym.Activation(next_c, act_type=\"tanh\")\n    return LSTMState(c=next_c, h=next_h)", "output": "LSTM Cell symbol", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, label_images):\n    \"\"\"\"\"\"\n\n    for label, image_paths in label_images.items():\n      for image_path in image_paths:\n        yield {\n            \"image\": image_path,\n            \"label\": label,\n        }", "output": "Generate example for each image in the dict.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_website(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_website(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}", "output": "Remove the website configuration from the given bucket\n\n    Returns {deleted: true} if website configuration was deleted and returns\n    {deleted: False} if website configuration was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_website my_bucket", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parameter_objects(parameter_objects_from_pillars, parameter_object_overrides):\n    '''\n    \n    '''\n    from_pillars = copy.deepcopy(__salt__['pillar.get'](parameter_objects_from_pillars))\n    from_pillars.update(parameter_object_overrides)\n    parameter_objects = _standardize(_dict_to_list_ids(from_pillars))\n    for parameter_object in parameter_objects:\n        parameter_object['attributes'] = _properties_from_dict(parameter_object['attributes'])\n    return parameter_objects", "output": "Return a list of parameter objects that configure the pipeline\n\n    parameter_objects_from_pillars\n        The pillar key to use for lookup\n\n    parameter_object_overrides\n        Parameter objects to use. Will override objects read from pillars.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clone(zone, source, snapshot=None):\n    '''\n    \n    '''\n    ret = {'status': True}\n\n    ## install zone\n    res = __salt__['cmd.run_all']('zoneadm -z {zone} clone {snapshot}{source}'.format(\n        zone=zone,\n        source=source,\n        snapshot='-s {0} '.format(snapshot) if snapshot else '',\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    ret['message'] = ret['message'].replace('zoneadm: ', '')\n    if ret['message'] == '':\n        del ret['message']\n\n    return ret", "output": "Install a zone by copying an existing installed zone.\n\n    zone : string\n        name of the zone\n    source : string\n        zone to clone from\n    snapshot : string\n        optional name of snapshot to use as source\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zoneadm.clone clementine dolores", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rounding_sequence_accuracy(predictions,\n                               labels,\n                               weights_fn=common_layers.weights_nonzero):\n  \"\"\"\"\"\"\n  outputs = tf.squeeze(tf.to_int32(predictions), axis=-1)\n  weights = weights_fn(labels)\n  labels = tf.to_int32(labels)\n  not_correct = tf.to_float(tf.not_equal(outputs, labels)) * weights\n  axis = list(range(1, len(outputs.get_shape())))\n  correct_seq = 1.0 - tf.minimum(1.0, tf.reduce_sum(not_correct, axis=axis))\n  return correct_seq, tf.constant(1.0)", "output": "Sequence accuracy for L1/L2 losses: round down the predictions to ints.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_args_for_reloading():\n    \"\"\"\"\"\"\n    rv = [sys.executable]\n    main_module = sys.modules[\"__main__\"]\n    mod_spec = getattr(main_module, \"__spec__\", None)\n    if mod_spec:\n        # Parent exe was launched as a module rather than a script\n        rv.extend([\"-m\", mod_spec.name])\n        if len(sys.argv) > 1:\n            rv.extend(sys.argv[1:])\n    else:\n        rv.extend(sys.argv)\n    return rv", "output": "Returns the executable.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _worker(reader: DatasetReader,\n            input_queue: Queue,\n            output_queue: Queue,\n            index: int) -> None:\n    \"\"\"\n    \n    \"\"\"\n    # Keep going until you get a file_path that's None.\n    while True:\n        file_path = input_queue.get()\n        if file_path is None:\n            # Put my index on the queue to signify that I'm finished\n            output_queue.put(index)\n            break\n\n        logger.info(f\"reading instances from {file_path}\")\n        for instance in reader.read(file_path):\n            output_queue.put(instance)", "output": "A worker that pulls filenames off the input queue, uses the dataset reader\n    to read them, and places the generated instances on the output queue.\n    When there are no filenames left on the input queue, it puts its ``index``\n    on the output queue and doesn't do anything else.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_valid_relpath(\n        relpath,\n        maxdepth=None):\n    '''\n    \n    '''\n    # Check relpath surrounded by slashes, so that `..` can be caught as\n    # a path component at the start, end, and in the middle of the path.\n    sep, pardir = posixpath.sep, posixpath.pardir\n    if sep + pardir + sep in sep + relpath + sep:\n        return False\n\n    # Check that the relative path's depth does not exceed maxdepth\n    if maxdepth is not None:\n        path_depth = relpath.strip(sep).count(sep)\n        if path_depth > maxdepth:\n            return False\n\n    return True", "output": "Performs basic sanity checks on a relative path.\n\n    Requires POSIX-compatible paths (i.e. the kind obtained through\n    cp.list_master or other such calls).\n\n    Ensures that the path does not contain directory transversal, and\n    that it does not exceed a stated maximum depth (if specified).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _byte_encode(self, token):\n    \"\"\"\"\"\"\n    # Vocab ids for all bytes follow ids for the subwords\n    offset = len(self._subwords)\n    if token == \"_\":\n      return [len(self._subwords) + ord(\" \")]\n    return [i + offset for i in list(bytearray(tf.compat.as_bytes(token)))]", "output": "Encode a single token byte-wise into integer ids.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(tmp_dir, dataset_split):\n  \"\"\"\n  \"\"\"\n  if dataset_split == problem.DatasetSplit.TRAIN:\n    file_name = _TRAINING_SET\n  else:\n    file_name = _DEV_SET\n  squad_file = generator_utils.maybe_download(tmp_dir,\n                                              file_name,\n                                              os.path.join(_URL, file_name))\n  with tf.gfile.GFile(squad_file, mode=\"r\") as fp:\n    squad = json.load(fp)\n\n  version = squad[\"version\"]\n  for article in squad[\"data\"]:\n    if \"title\" in article:\n      title = article[\"title\"].strip()\n    else:\n      title = \"no title\"\n    for paragraph in article[\"paragraphs\"]:\n      context = paragraph[\"context\"].strip()\n      for qa in paragraph[\"qas\"]:\n        question = qa[\"question\"].strip()\n        id_ = qa[\"id\"]\n        answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"]]\n        answers = [answer[\"text\"].strip() for answer in qa[\"answers\"]]\n\n        # Features currently used are \"context\", \"question\", and \"answers\".\n        # Others are extracted here for the ease of future expansions.\n        example = {\n            \"version\": version,\n            \"title\": title,\n            \"context\": context,\n            \"question\": question,\n            \"id\": id_,\n            \"answer_starts\": answer_starts,\n            \"answers\": answers,\n            \"num_answers\": len(answers),\n            \"is_supervised\": True,\n        }\n        yield example", "output": "Generate squad examples.\n\n  Args:\n    tmp_dir: a string\n    dataset_split: problem.DatasetSplit.TRAIN or problem.DatasetSplit.EVAL\n  Yields:\n    dictionaries representing examples", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def CopyFrom(self, other_msg):\n    \"\"\"\n    \"\"\"\n    if self is other_msg:\n      return\n    self.Clear()\n    self.MergeFrom(other_msg)", "output": "Copies the content of the specified message into the current message.\n\n    The method clears the current message and then merges the specified\n    message using MergeFrom.\n\n    Args:\n      other_msg: Message to copy into the current one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _prep_acl_for_compare(ACL):\n    '''\n    \n    '''\n    ret = copy.deepcopy(ACL)\n    ret['Owner'] = _normalize_user(ret['Owner'])\n    for item in ret.get('Grants', ()):\n        item['Grantee'] = _normalize_user(item.get('Grantee'))\n    return ret", "output": "Prepares the ACL returned from the AWS API for comparison with a given one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_xsrf_cookie(self) -> None:\n        \"\"\"\n        \"\"\"\n        # Prior to release 1.1.1, this check was ignored if the HTTP header\n        # ``X-Requested-With: XMLHTTPRequest`` was present.  This exception\n        # has been shown to be insecure and has been removed.  For more\n        # information please see\n        # http://www.djangoproject.com/weblog/2011/feb/08/security/\n        # http://weblog.rubyonrails.org/2011/2/8/csrf-protection-bypass-in-ruby-on-rails\n        token = (\n            self.get_argument(\"_xsrf\", None)\n            or self.request.headers.get(\"X-Xsrftoken\")\n            or self.request.headers.get(\"X-Csrftoken\")\n        )\n        if not token:\n            raise HTTPError(403, \"'_xsrf' argument missing from POST\")\n        _, token, _ = self._decode_xsrf_token(token)\n        _, expected_token, _ = self._get_raw_xsrf_token()\n        if not token:\n            raise HTTPError(403, \"'_xsrf' argument has invalid format\")\n        if not hmac.compare_digest(utf8(token), utf8(expected_token)):\n            raise HTTPError(403, \"XSRF cookie does not match POST argument\")", "output": "Verifies that the ``_xsrf`` cookie matches the ``_xsrf`` argument.\n\n        To prevent cross-site request forgery, we set an ``_xsrf``\n        cookie and include the same value as a non-cookie\n        field with all ``POST`` requests. If the two do not match, we\n        reject the form submission as a potential forgery.\n\n        The ``_xsrf`` value may be set as either a form field named ``_xsrf``\n        or in a custom HTTP header named ``X-XSRFToken`` or ``X-CSRFToken``\n        (the latter is accepted for compatibility with Django).\n\n        See http://en.wikipedia.org/wiki/Cross-site_request_forgery\n\n        .. versionchanged:: 3.2.2\n           Added support for cookie version 2.  Both versions 1 and 2 are\n           supported.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_cache_key(self, name, filename=None):\n        \"\"\"\"\"\"\n        hash = sha1(name.encode('utf-8'))\n        if filename is not None:\n            filename = '|' + filename\n            if isinstance(filename, text_type):\n                filename = filename.encode('utf-8')\n            hash.update(filename)\n        return hash.hexdigest()", "output": "Returns the unique hash key for this template name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_permissions(vhost, runas=None):\n    '''\n    \n    '''\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n    res = __salt__['cmd.run_all'](\n        [RABBITMQCTL, 'list_permissions', '-q', '-p', vhost],\n        reset_system_locale=False,\n        runas=runas,\n        python_shell=False)\n\n    return _output_to_dict(res)", "output": "Lists permissions for vhost via rabbitmqctl list_permissions\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.list_permissions /myvhost", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernel_status(self, user_name, kernel_slug, **kwargs):  # noqa: E501\n        \"\"\"\n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.kernel_status_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.kernel_status_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n            return data", "output": "Get the status of the latest kernel version  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.kernel_status(user_name, kernel_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str user_name: Kernel owner (required)\n        :param str kernel_slug: Kernel name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def explode(col):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.explode(_to_java_column(col))\n    return Column(jc)", "output": "Returns a new row for each element in the given array or map.\n    Uses the default column name `col` for elements in the array and\n    `key` and `value` for elements in the map unless specified otherwise.\n\n    >>> from pyspark.sql import Row\n    >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n    >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n    [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n\n    >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n    +---+-----+\n    |key|value|\n    +---+-----+\n    |  a|    b|\n    +---+-----+", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_exists(username, domain='', database=None, **kwargs):\n    '''\n    \n    '''\n    if domain:\n        username = '{0}\\\\{1}'.format(domain, username)\n    if database:\n        kwargs['database'] = database\n    # We should get one, and only one row\n    return len(tsql_query(query=\"SELECT name FROM sysusers WHERE name='{0}'\".format(username), **kwargs)) == 1", "output": "Find if an user exists in a specific database on the MS SQL server.\n    domain, if provided, will be prepended to username\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minion mssql.user_exists 'USERNAME' [database='DBNAME']", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def fetch2(self, path, api='public', method='GET', params={}, headers=None, body=None):\n        \"\"\"\"\"\"\n        if self.enableRateLimit:\n            self.throttle()\n        self.lastRestRequestTimestamp = self.milliseconds()\n        request = self.sign(path, api, method, params, headers, body)\n        return self.fetch(request['url'], request['method'], request['headers'], request['body'])", "output": "A better wrapper over request for deferred signing", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def project_delete(project_id=None, name=None, profile=None, **connection_args):\n    '''\n    \n    '''\n    auth(profile, **connection_args)\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        return tenant_delete(tenant_id=project_id, name=name, profile=None, **connection_args)\n    else:\n        return False", "output": "Delete a project (keystone project-delete).\n    Overrides keystone tenant-delete form api V2. For keystone api V3 only.\n\n    .. versionadded:: 2016.11.0\n\n    project_id\n        The project id.\n\n    name\n        The project name.\n\n    profile\n        Configuration profile - if configuration for multiple openstack accounts required.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.project_delete c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.project_delete project_id=c965f79c4f864eaaa9c3b41904e67082\n        salt '*' keystone.project_delete name=demo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def property_data_zpool():\n    '''\n    \n\n    '''\n    # NOTE: man page also mentions a few short forms\n    property_data = _property_parse_cmd(_zpool_cmd(), {\n        'allocated': 'alloc',\n        'autoexpand': 'expand',\n        'autoreplace': 'replace',\n        'listsnapshots': 'listsnaps',\n        'fragmentation': 'frag',\n    })\n\n    # NOTE: zpool status/iostat has a few extra fields\n    zpool_size_extra = [\n        'capacity-alloc', 'capacity-free',\n        'operations-read', 'operations-write',\n        'bandwith-read', 'bandwith-write',\n        'read', 'write',\n    ]\n    zpool_numeric_extra = [\n        'cksum', 'cap',\n    ]\n\n    for prop in zpool_size_extra:\n        property_data[prop] = {\n            'edit': False,\n            'type': 'size',\n            'values': '<size>',\n        }\n\n    for prop in zpool_numeric_extra:\n        property_data[prop] = {\n            'edit': False,\n            'type': 'numeric',\n            'values': '<count>',\n        }\n\n    return property_data", "output": "Return a dict of zpool properties\n\n    .. note::\n\n        Each property will have an entry with the following info:\n            - edit : boolean - is this property editable after pool creation\n            - type : str - either bool, bool_alt, size, numeric, or string\n            - values : str - list of possible values\n\n    .. warning::\n\n        This data is probed from the output of 'zpool get' with some suplimental\n        data that is hardcoded. There is no better way to get this informatio aside\n        from reading the code.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_result(self, result):\n        \"\"\"\"\"\"\n        self._result = result\n        self._result_set = True\n        self._invoke_callbacks(self)", "output": "Set the Future's result.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def perform_iteration(self):\n        \"\"\"\"\"\"\n        stats = self.get_all_stats()\n\n        self.redis_client.publish(\n            self.redis_key,\n            jsonify_asdict(stats),\n        )", "output": "Get any changes to the log files and push updates to Redis.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def summary(self):\n        \"\"\"\n        \n        \"\"\"\n        if self.hasSummary:\n            return LinearRegressionTrainingSummary(super(LinearRegressionModel, self).summary)\n        else:\n            raise RuntimeError(\"No training summary available for this %s\" %\n                               self.__class__.__name__)", "output": "Gets summary (e.g. residuals, mse, r-squared ) of model on\n        training set. An exception is thrown if\n        `trainingSummary is None`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def random_chars(number):\n    \"\"\"\"\"\"\n    char_map = {\n        k: v for k, v in chars.CHARS.iteritems()\n        if not format_character(k).startswith('\\\\x')\n    }\n\n    char_num = sum(char_map.values())\n    return (\n        format_character(nth_char(char_map, random.randint(0, char_num - 1)))\n        for _ in xrange(0, number)\n    )", "output": "Generate random characters", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nanmedian(values, axis=None, skipna=True, mask=None):\n    \"\"\"\n    \n    \"\"\"\n    def get_median(x):\n        mask = notna(x)\n        if not skipna and not mask.all():\n            return np.nan\n        return np.nanmedian(x[mask])\n\n    values, mask, dtype, dtype_max, _ = _get_values(values, skipna, mask=mask)\n    if not is_float_dtype(values):\n        values = values.astype('f8')\n        values[mask] = np.nan\n\n    if axis is None:\n        values = values.ravel()\n\n    notempty = values.size\n\n    # an array from a frame\n    if values.ndim > 1:\n\n        # there's a non-empty array to apply over otherwise numpy raises\n        if notempty:\n            if not skipna:\n                return _wrap_results(\n                    np.apply_along_axis(get_median, axis, values), dtype)\n\n            # fastpath for the skipna case\n            return _wrap_results(np.nanmedian(values, axis), dtype)\n\n        # must return the correct shape, but median is not defined for the\n        # empty set so return nans of shape \"everything but the passed axis\"\n        # since \"axis\" is where the reduction would occur if we had a nonempty\n        # array\n        shp = np.array(values.shape)\n        dims = np.arange(values.ndim)\n        ret = np.empty(shp[dims != axis])\n        ret.fill(np.nan)\n        return _wrap_results(ret, dtype)\n\n    # otherwise return a scalar value\n    return _wrap_results(get_median(values) if notempty else np.nan, dtype)", "output": "Parameters\n    ----------\n    values : ndarray\n    axis: int, optional\n    skipna : bool, default True\n    mask : ndarray[bool], optional\n        nan-mask if known\n\n    Returns\n    -------\n    result : float\n        Unless input is a float array, in which case use the same\n        precision as the input array.\n\n    Examples\n    --------\n    >>> import pandas.core.nanops as nanops\n    >>> s = pd.Series([1, np.nan, 2, 2])\n    >>> nanops.nanmedian(s)\n    2.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def autologin(self, limit=10):\n        \"\"\"\n        \"\"\"\n        for _ in range(limit):\n            if self.login():\n                break\n        else:\n            raise exceptions.NotLoginError(\n                \"\u767b\u5f55\u5931\u8d25\u6b21\u6570\u8fc7\u591a, \u8bf7\u68c0\u67e5\u5bc6\u7801\u662f\u5426\u6b63\u786e / \u5238\u5546\u670d\u52a1\u5668\u662f\u5426\u5904\u4e8e\u7ef4\u62a4\u4e2d / \u7f51\u7edc\u8fde\u63a5\u662f\u5426\u6b63\u5e38\"\n            )\n        self.keepalive()", "output": "\u5b9e\u73b0\u81ea\u52a8\u767b\u5f55\n        :param limit: \u767b\u5f55\u6b21\u6570\u9650\u5236", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_user(name, runas=None):\n    '''\n    \n    '''\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n    res = __salt__['cmd.run_all'](\n        [RABBITMQCTL, 'delete_user', name],\n        reset_system_locale=False,\n        python_shell=False,\n        runas=runas)\n    msg = 'Deleted'\n\n    return _format_response(res, msg)", "output": "Deletes a user via rabbitmqctl delete_user.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.delete_user rabbit_user", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def timed_operation(msg, log_start=False):\n    \"\"\"\n    \n    \"\"\"\n    assert len(msg)\n    if log_start:\n        logger.info('Start {} ...'.format(msg))\n    start = timer()\n    yield\n    msg = msg[0].upper() + msg[1:]\n    logger.info('{} finished, time:{:.4f} sec.'.format(\n        msg, timer() - start))", "output": "Surround a context with a timer.\n\n    Args:\n        msg(str): the log to print.\n        log_start(bool): whether to print also at the beginning.\n\n    Example:\n        .. code-block:: python\n\n            with timed_operation('Good Stuff'):\n                time.sleep(1)\n\n        Will print:\n\n        .. code-block:: python\n\n            Good stuff finished, time:1sec.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_python_interpreter_valid_name(filename):\r\n    \"\"\"\"\"\"\r\n    pattern = r'.*python(\\d\\.?\\d*)?(w)?(.exe)?$'\r\n    if re.match(pattern, filename, flags=re.I) is None:\r\n        return False\r\n    else:\r\n        return True", "output": "Check that the python interpreter file has a valid name.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup():\n    \"\"\"\n    \n    \"\"\"\n    # create an index template\n    index_template = BlogPost._index.as_template(ALIAS, PATTERN)\n    # upload the template into elasticsearch\n    # potentially overriding the one already there\n    index_template.save()\n\n    # create the first index if it doesn't exist\n    if not BlogPost._index.exists():\n        migrate(move_data=False)", "output": "Create the index template in elasticsearch specifying the mappings and any\n    settings to be used. This can be run at any time, ideally at every new code\n    deploy.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def focusInEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        show_status = getattr(self.lineEdit(), 'show_status_icon', None)\r\n        if show_status:\r\n            show_status()\r\n        QComboBox.focusInEvent(self, event)", "output": "Handle focus in event restoring to display the status icon.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unregister(name, delete=False):\n    '''\n    \n    '''\n    nodes = list_nodes_min()\n    if name not in nodes:\n        raise CommandExecutionError(\n            'The specified VM ({0}) is not registered.'.format(name)\n        )\n\n    cmd = '{0} unregistervm {1}'.format(vboxcmd(), name)\n    if delete is True:\n        cmd += ' --delete'\n    ret = salt.modules.cmdmod.run_all(cmd)\n    if ret['retcode'] == 0:\n        return True\n    return ret['stderr']", "output": "Unregister a VM\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vboxmanage.unregister my_vm_filename", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _filter_nodes(superclass, all_nodes=_all_nodes):\n    \"\"\"\"\"\"\n    node_names = (node.__name__ for node in all_nodes\n                  if issubclass(node, superclass))\n    return frozenset(node_names)", "output": "Filter out AST nodes that are subclasses of ``superclass``.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_vm(name, datacenter=None, vm_properties=None, traversal_spec=None,\n           parent_ref=None, service_instance=None):\n    '''\n    \n    '''\n    virtual_machine = salt.utils.vmware.get_vm_by_property(\n        service_instance,\n        name,\n        datacenter=datacenter,\n        vm_properties=vm_properties,\n        traversal_spec=traversal_spec,\n        parent_ref=parent_ref)\n    return virtual_machine", "output": "Returns vm object properties.\n\n    name\n        Name of the virtual machine.\n\n    datacenter\n        Datacenter name\n\n    vm_properties\n        List of vm properties.\n\n    traversal_spec\n        Traversal Spec object(s) for searching.\n\n    parent_ref\n        Container Reference object for searching under a given object.\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def embed_sentences(self,\n                        sentences: Iterable[List[str]],\n                        batch_size: int = DEFAULT_BATCH_SIZE) -> Iterable[numpy.ndarray]:\n        \"\"\"\n        \n        \"\"\"\n        for batch in lazy_groups_of(iter(sentences), batch_size):\n            yield from self.embed_batch(batch)", "output": "Computes the ELMo embeddings for a iterable of sentences.\n\n        Please note that ELMo has internal state and will give different results for the same input.\n        See the comment under the class definition.\n\n        Parameters\n        ----------\n        sentences : ``Iterable[List[str]]``, required\n            An iterable of tokenized sentences.\n        batch_size : ``int``, required\n            The number of sentences ELMo should process at once.\n\n        Returns\n        -------\n            A list of tensors, each representing the ELMo vectors for the input sentence at the same index.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_code(self, code):\n        \"\"\"\n        \"\"\"\n        # code= [code] if isinstance(code,str) else\n        return self.new(self.data.loc[(slice(None), code), :])", "output": "getcode \u83b7\u53d6\u67d0\u4e00\u53ea\u80a1\u7968\u7684\u677f\u5757\n\n        Arguments:\n            code {str} -- \u80a1\u7968\u4ee3\u7801\n\n        Returns:\n            DataStruct -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def revoke_auth(preserve_minion_cache=False):\n    '''\n    \n    '''\n    masters = list()\n    ret = True\n    if 'master_uri_list' in __opts__:\n        for master_uri in __opts__['master_uri_list']:\n            masters.append(master_uri)\n    else:\n        masters.append(__opts__['master_uri'])\n\n    for master in masters:\n        channel = salt.transport.client.ReqChannel.factory(__opts__, master_uri=master)\n        tok = channel.auth.gen_token(b'salt')\n        load = {'cmd': 'revoke_auth',\n                'id': __opts__['id'],\n                'tok': tok,\n                'preserve_minion_cache': preserve_minion_cache}\n        try:\n            channel.send(load)\n        except SaltReqTimeoutError:\n            ret = False\n        finally:\n            channel.close()\n    return ret", "output": "The minion sends a request to the master to revoke its own key.\n    Note that the minion session will be revoked and the minion may\n    not be able to return the result of this command back to the master.\n\n    If the 'preserve_minion_cache' flag is set to True, the master\n    cache for this minion will not be removed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.revoke_auth", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _slice(attrs, inputs, proto_obj):\n    \"\"\"\"\"\"\n    new_attrs = translation_utils._fix_attribute_names(attrs,\n                                                       {'axes' : 'axis',\n                                                        'ends' : 'end',\n                                                        'starts' : 'begin'})\n    # onnx slice provides slicing on multiple axis. Adding multiple slice_axis operator\n    # for multiple axes from mxnet\n    begin = new_attrs.get('begin')\n    end = new_attrs.get('end')\n    axes = new_attrs.get('axis', tuple(range(len(begin))))\n    slice_op = symbol.slice_axis(inputs[0], axis=axes[0], begin=begin[0], end=end[0])\n    if len(axes) > 1:\n        for i, axis in enumerate(axes):\n            slice_op = symbol.slice_axis(slice_op, axis=axis, begin=begin[i], end=end[i])\n    return slice_op, new_attrs, inputs", "output": "Returns a slice of the input tensor along multiple axes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def find_learning_rate_from_args(args: argparse.Namespace) -> None:\n    \"\"\"\n    \n    \"\"\"\n    params = Params.from_file(args.param_path, args.overrides)\n    find_learning_rate_model(params, args.serialization_dir,\n                             start_lr=args.start_lr,\n                             end_lr=args.end_lr,\n                             num_batches=args.num_batches,\n                             linear_steps=args.linear,\n                             stopping_factor=args.stopping_factor,\n                             force=args.force)", "output": "Start learning rate finder for given args", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def delete_subject(self, subject):\n        \"\"\"\n        \n        \"\"\"\n\n        url = '/'.join([self.url, 'subjects', subject])\n\n        result, code = self._send_request(url, method=\"DELETE\")\n        if not (code >= 200 and code <= 299):\n            raise ClientError('Unable to delete subject: {}'.format(result))\n        return result", "output": "DELETE /subjects/(string: subject)\n        Deletes the specified subject and its associated compatibility level if registered.\n        It is recommended to use this API only when a topic needs to be recycled or in development environments.\n        :param subject: subject name\n        :returns: version of the schema deleted under this subject\n        :rtype: (int)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _run_hook_from_repo_dir(repo_dir, hook_name, project_dir, context,\n                            delete_project_on_failure):\n    \"\"\"\n    \"\"\"\n    with work_in(repo_dir):\n        try:\n            run_hook(hook_name, project_dir, context)\n        except FailedHookException:\n            if delete_project_on_failure:\n                rmtree(project_dir)\n            logger.error(\n                \"Stopping generation because {} hook \"\n                \"script didn't exit successfully\".format(hook_name)\n            )\n            raise", "output": "Run hook from repo directory, clean project directory if hook fails.\n\n    :param repo_dir: Project template input directory.\n    :param hook_name: The hook to execute.\n    :param project_dir: The directory to execute the script from.\n    :param context: Cookiecutter project context.\n    :param delete_project_on_failure: Delete the project directory on hook\n        failure?", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_root_path(self, root_path):\r\n        \"\"\"\"\"\"\r\n        if self.name is None:\r\n            self.name = osp.basename(root_path)\r\n        self.root_path = to_text_string(root_path)\r\n        config_path = self.__get_project_config_path()\r\n        if osp.exists(config_path):\r\n            self.load()\r\n        else:\r\n            if not osp.isdir(self.root_path):\r\n                os.mkdir(self.root_path)\r\n            self.save()", "output": "Set project root path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def key_type(key, host=None, port=None, db=None, password=None):\n    '''\n    \n    '''\n    server = _connect(host, port, db, password)\n    return server.type(key)", "output": "Get redis key type\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' redis.type foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def authenticate(remote_addr, password, cert, key, verify_cert=True):\n    '''\n    \n\n    '''\n    client = pylxd_client_get(remote_addr, cert, key, verify_cert)\n\n    if client.trusted:\n        return True\n\n    try:\n        client.authenticate(password)\n    except pylxd.exceptions.LXDAPIException as e:\n        # Wrong password\n        raise CommandExecutionError(six.text_type(e))\n\n    return client.trusted", "output": "Authenticate with a remote LXDaemon.\n\n    remote_addr :\n        An URL to a remote Server, you also have to give cert and key if you\n        provide remote_addr and its a TCP Address!\n\n        Examples:\n            https://myserver.lan:8443\n\n    password :\n        The password of the remote.\n\n    cert :\n        PEM Formatted SSL Certificate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Wherever to verify the cert, this is by default True\n        but in the most cases you want to set it off as LXD\n        normaly uses self-signed certificates.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        $ salt '*' lxd.authenticate https://srv01:8443 <yourpass> ~/.config/lxc/client.crt ~/.config/lxc/client.key false\n\n    See the `requests-docs`_ for the SSL stuff.\n\n    .. _requests-docs: http://docs.python-requests.org/en/master/user/advanced/#ssl-cert-verification", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_docstring_at_first_line_of_function(self):\r\n        \"\"\"\"\"\"\r\n        result = self.get_function_definition_from_first_line()\r\n        editor = self.code_editor\r\n        if result:\r\n            func_text, number_of_line_func = result\r\n            line_number_function = (self.line_number_cursor +\r\n                                    number_of_line_func - 1)\r\n\r\n            cursor = editor.textCursor()\r\n            line_number_cursor = cursor.blockNumber() + 1\r\n            offset = line_number_function - line_number_cursor\r\n            if offset > 0:\r\n                for __ in range(offset):\r\n                    cursor.movePosition(QTextCursor.NextBlock)\r\n            else:\r\n                for __ in range(abs(offset)):\r\n                    cursor.movePosition(QTextCursor.PreviousBlock)\r\n            cursor.movePosition(QTextCursor.EndOfLine, QTextCursor.MoveAnchor)\r\n            editor.setTextCursor(cursor)\r\n\r\n            indent = get_indent(func_text)\r\n            editor.insert_text('\\n{}{}\"\"\"'.format(indent, editor.indent_chars))\r\n            self.write_docstring()", "output": "Write docstring to editor at mouse position.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_config(**api_opts):\n    '''\n    \n    '''\n    config = {\n        'api_sslverify': True,\n        'api_url': 'https://INFOBLOX/wapi/v1.2.1',\n        'api_user': '',\n        'api_key': '',\n    }\n    if '__salt__' in globals():\n        config_key = '{0}.config'.format(__virtualname__)\n        config.update(__salt__['config.get'](config_key, {}))\n    # pylint: disable=C0201\n    for k in set(config.keys()) & set(api_opts.keys()):\n        config[k] = api_opts[k]\n    return config", "output": "Return configuration\n    user passed api_opts override salt config.get vars", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_connection_close(self) -> None:\n        \"\"\"\n        \"\"\"\n        if _has_stream_request_body(self.__class__):\n            if not self.request._body_future.done():\n                self.request._body_future.set_exception(iostream.StreamClosedError())\n                self.request._body_future.exception()", "output": "Called in async handlers if the client closed the connection.\n\n        Override this to clean up resources associated with\n        long-lived connections.  Note that this method is called only if\n        the connection was closed during asynchronous processing; if you\n        need to do cleanup after every request override `on_finish`\n        instead.\n\n        Proxies may keep a connection open for a time (perhaps\n        indefinitely) after the client has gone away, so this method\n        may not be called promptly after the end user closes their\n        connection.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_post(self, path: str, handler: _WebHandler,\n                 **kwargs: Any) -> AbstractRoute:\n        \"\"\"\n        \n        \"\"\"\n        return self.add_route(hdrs.METH_POST, path, handler, **kwargs)", "output": "Shortcut for add_route with method POST", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_dtype_kinds(l):\n    \"\"\"\n    \n    \"\"\"\n\n    typs = set()\n    for arr in l:\n\n        dtype = arr.dtype\n        if is_categorical_dtype(dtype):\n            typ = 'category'\n        elif is_sparse(arr):\n            typ = 'sparse'\n        elif isinstance(arr, ABCRangeIndex):\n            typ = 'range'\n        elif is_datetime64tz_dtype(arr):\n            # if to_concat contains different tz,\n            # the result must be object dtype\n            typ = str(arr.dtype)\n        elif is_datetime64_dtype(dtype):\n            typ = 'datetime'\n        elif is_timedelta64_dtype(dtype):\n            typ = 'timedelta'\n        elif is_object_dtype(dtype):\n            typ = 'object'\n        elif is_bool_dtype(dtype):\n            typ = 'bool'\n        elif is_extension_array_dtype(dtype):\n            typ = str(arr.dtype)\n        else:\n            typ = dtype.kind\n        typs.add(typ)\n    return typs", "output": "Parameters\n    ----------\n    l : list of arrays\n\n    Returns\n    -------\n    a set of kinds that exist in this list of arrays", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cut(self):\n        \"\"\"\"\"\"\n        self.truncate_selection(self.header_end_pos)\n        if self.has_selected_text():\n            CodeEditor.cut(self)", "output": "Cut text", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _to_predictor(self, pred_parameter=None):\n        \"\"\"\"\"\"\n        predictor = _InnerPredictor(booster_handle=self.handle, pred_parameter=pred_parameter)\n        predictor.pandas_categorical = self.pandas_categorical\n        return predictor", "output": "Convert to predictor.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def read(self):\n        \"\"\"\n        \n        \"\"\"\n        cursor = self.get_collection().find(\n            {\n                '_id': {'$in': self._document_ids},\n                self._field: {'$exists': True}\n            },\n            {self._field: True}\n        )\n\n        return {doc['_id']: doc[self._field] for doc in cursor}", "output": "Read the targets value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(key, profile=None):\n    '''\n    \n    '''\n    conn = salt.utils.memcached.get_conn(profile)\n    return salt.utils.memcached.get(conn, key)", "output": "Get a value from memcached", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_base_vq1_16_nb1_packed_dan_b01_scales():\n  \"\"\"\"\"\"\n  hparams = transformer_base_vq_ada_32ex_packed()\n  hparams.use_scales = int(True)\n  hparams.moe_num_experts = 16\n  hparams.moe_k = 1\n  hparams.beta = 0.1\n  hparams.ema = False\n  return hparams", "output": "Set of hyperparameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_worker_processes(apppool):\n    '''\n    \n    '''\n    ps_cmd = ['Get-ChildItem',\n              r\"'IIS:\\AppPools\\{0}\\WorkerProcesses'\".format(apppool)]\n\n    cmd_ret = _srvmgr(cmd=ps_cmd, return_json=True)\n\n    try:\n        items = salt.utils.json.loads(cmd_ret['stdout'], strict=False)\n    except ValueError:\n        raise CommandExecutionError('Unable to parse return data as Json.')\n\n    ret = dict()\n    for item in items:\n        ret[item['processId']] = item['appPoolName']\n\n    if not ret:\n        log.warning('No backups found in output: %s', cmd_ret)\n\n    return ret", "output": "Returns a list of worker processes that correspond to the passed\n    application pool.\n\n    .. versionadded:: 2017.7.0\n\n    Args:\n        apppool (str): The application pool to query\n\n    Returns:\n        dict: A dictionary of worker processes with their process IDs\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' win_iis.list_worker_processes 'My App Pool'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _handle_sub_action(self, input_dict, handler):\n        \"\"\"\n        \n        \"\"\"\n        if not self.can_handle(input_dict):\n            return input_dict\n\n        key = self.intrinsic_name\n        sub_value = input_dict[key]\n\n        input_dict[key] = self._handle_sub_value(sub_value, handler)\n\n        return input_dict", "output": "Handles resolving replacements in the Sub action based on the handler that is passed as an input.\n\n        :param input_dict: Dictionary to be resolved\n        :param supported_values: One of several different objects that contain the supported values that\n            need to be changed. See each method above for specifics on these objects.\n        :param handler: handler that is specific to each implementation.\n        :return: Resolved value of the Sub dictionary", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def split_by_fname_file(self, fname:PathOrStr, path:PathOrStr=None)->'ItemLists':\n        \"\"\n        path = Path(ifnone(path, self.path))\n        valid_names = loadtxt_str(path/fname)\n        return self.split_by_files(valid_names)", "output": "Split the data by using the names in `fname` for the validation set. `path` will override `self.path`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _tmp_access_rule(method,\n                    ip=None,\n                    ttl=None,\n                    port=None,\n                    direction='in',\n                    port_origin='d',\n                    ip_origin='d',\n                    comment=''):\n    '''\n    \n    '''\n    if _status_csf():\n        if ip is None:\n            return {'error': 'You must supply an ip address or CIDR.'}\n        if ttl is None:\n            return {'error': 'You must supply a ttl.'}\n        args = _build_tmp_access_args(method, ip, ttl, port, direction, comment)\n        return __csf_cmd(args)", "output": "Handles the cmd execution for tempdeny and tempallow commands.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def skip_connection_distance(a, b):\n    \"\"\"\"\"\"\n    if a[2] != b[2]:\n        return 1.0\n    len_a = abs(a[1] - a[0])\n    len_b = abs(b[1] - b[0])\n    return (abs(a[0] - b[0]) + abs(len_a - len_b)) / (max(a[0], b[0]) + max(len_a, len_b))", "output": "The distance between two skip-connections.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def patch_traces(self, traces, project_id=None):\n        \"\"\"\n        \"\"\"\n        if project_id is None:\n            project_id = self.project\n\n        self.trace_api.patch_traces(project_id=project_id, traces=traces)", "output": "Sends new traces to Stackdriver Trace or updates existing traces.\n\n        Args:\n            traces (dict): Required. The traces to be patched in the API call.\n\n            project_id (Optional[str]): ID of the Cloud project where the trace\n                data is stored.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def parse_services(rule):\n    '''\n    \n    '''\n    parser = argparse.ArgumentParser()\n    rules = shlex.split(rule)\n    rules.pop(0)\n    parser.add_argument('--disabled', dest='disabled', action='store')\n    parser.add_argument('--enabled', dest='enabled', action='store')\n\n    args = clean_args(vars(parser.parse_args(rules)))\n    parser = None\n    return args", "output": "Parse the services line", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernels_list_cli(self,\n                         mine=False,\n                         page=1,\n                         page_size=20,\n                         search=None,\n                         csv_display=False,\n                         parent=None,\n                         competition=None,\n                         dataset=None,\n                         user=None,\n                         language=None,\n                         kernel_type=None,\n                         output_type=None,\n                         sort_by=None):\n        \"\"\" \n        \"\"\"\n        kernels = self.kernels_list(\n            page=page,\n            page_size=page_size,\n            search=search,\n            mine=mine,\n            dataset=dataset,\n            competition=competition,\n            parent_kernel=parent,\n            user=user,\n            language=language,\n            kernel_type=kernel_type,\n            output_type=output_type,\n            sort_by=sort_by)\n        fields = ['ref', 'title', 'author', 'lastRunTime', 'totalVotes']\n        if kernels:\n            if csv_display:\n                self.print_csv(kernels, fields)\n            else:\n                self.print_table(kernels, fields)\n        else:\n            print('No kernels found')", "output": "client wrapper for kernels_list, see this function for arguments.\n            Additional arguments are provided here.\n             Parameters\n            ==========\n            csv_display: if True, print comma separated values instead of table", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def list_users(root=None):\n    '''\n    \n    '''\n    if root is not None:\n        getspall = functools.partial(_getspall, root=root)\n    else:\n        getspall = functools.partial(spwd.getspall)\n\n    return sorted([user.sp_namp if hasattr(user, 'sp_namp') else user.sp_nam\n                   for user in getspall()])", "output": ".. versionadded:: 2018.3.0\n\n    Return a list of all shadow users\n\n    root\n        Directory to chroot into\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' shadow.list_users", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_user(username, **kwargs):\n    '''\n    \n    '''\n    command = 'show run | include \"^username {0} password 5 \"'.format(username)\n    info = ''\n    info = show(command, **kwargs)\n    if isinstance(info, list):\n        info = info[0]\n    return info", "output": "Get username line from switch.\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd get_user username=admin", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def authorize_url(self, state=None):\n        '''\n        \n        '''\n        if not self._client_id:\n            raise ApiAuthError('No client id.')\n        kw = dict(client_id=self._client_id)\n        if self._redirect_uri:\n            kw['redirect_uri'] = self._redirect_uri\n        if self._scope:\n            kw['scope'] = self._scope\n        if state:\n            kw['state'] = state\n        return 'https://github.com/login/oauth/authorize?%s' % _encode_params(kw)", "output": "Generate authorize_url.\n\n        >>> GitHub(client_id='3ebf94c5776d565bcf75').authorize_url()\n        'https://github.com/login/oauth/authorize?client_id=3ebf94c5776d565bcf75'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def classify(self, dataset, verbose=True, batch_size=64):\n        \"\"\"\n        \n        \"\"\"\n        prob_vector = self.predict(dataset, output_type='probability_vector',\n                                   verbose=verbose, batch_size=batch_size)\n        id_to_label = self._id_to_class_label\n\n        return _tc.SFrame({\n            'class': prob_vector.apply(lambda v: id_to_label[_np.argmax(v)]),\n            'probability': prob_vector.apply(_np.max)\n        })", "output": "Return the classification for each examples in the ``dataset``.\n        The output SFrame contains predicted class labels and its probability.\n\n        Parameters\n        ----------\n        dataset : SFrame | SArray | dict\n            The audio data to be classified.\n            If dataset is an SFrame, it must have a column with the same name as\n            the feature used for model training, but does not require a target\n            column. Additional columns are ignored.\n\n        verbose : bool, optional\n            If True, prints progress updates and model details.\n\n        batch_size : int, optional\n            If you are getting memory errors, try decreasing this value. If you\n            have a powerful computer, increasing this value may improve performance.\n\n        Returns\n        -------\n        out : SFrame\n            An SFrame with model predictions, both class labels and probabilities.\n\n        See Also\n        ----------\n        create, evaluate, predict\n\n        Examples\n        ----------\n        >>> classes = model.classify(data)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def crop_or_pad_to(height, width):\n    \"\"\"\n    \"\"\"\n    def inner(t_image):\n        return tf.image.resize_image_with_crop_or_pad(t_image, height, width)\n    return inner", "output": "Ensures the specified spatial shape by either padding or cropping.\n    Meant to be used as a last transform for architectures insisting on a specific\n    spatial shape of their inputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _validate_schema(self, schema, field, value):\n        \"\"\"  \"\"\"\n        if schema is None:\n            return\n\n        if isinstance(value, Sequence) and not isinstance(value, _str_type):\n            self.__validate_schema_sequence(field, schema, value)\n        elif isinstance(value, Mapping):\n            self.__validate_schema_mapping(field, schema, value)", "output": "{'type': ['dict', 'string'],\n             'anyof': [{'validator': 'schema'},\n                       {'validator': 'bulk_schema'}]}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def show_lb(kwargs=None, call=None):\n    '''\n    \n    '''\n    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The show_lb function must be called with -f or --function.'\n        )\n    if not kwargs or 'name' not in kwargs:\n        log.error(\n            'Must specify name of load-balancer.'\n        )\n        return False\n\n    lb_conn = get_lb_conn(get_conn())\n    return _expand_balancer(lb_conn.get_balancer(kwargs['name']))", "output": "Show the details of an existing load-balancer.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f show_lb gce name=lb", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def json2space(x, oldy=None, name=NodeType.Root.value):\n    \"\"\"\n    \"\"\"\n    y = list()\n    if isinstance(x, dict):\n        if NodeType.Type.value in x.keys():\n            _type = x[NodeType.Type.value]\n            name = name + '-' + _type\n            if _type == 'choice':\n                if oldy != None:\n                    _index = oldy[NodeType.Index.value]\n                    y += json2space(x[NodeType.Value.value][_index],\n                                    oldy[NodeType.Value.value], name=name+'[%d]' % _index)\n                else:\n                    y += json2space(x[NodeType.Value.value], None, name=name)\n            y.append(name)\n        else:\n            for key in x.keys():\n                y += json2space(x[key], (oldy[key] if oldy !=\n                                         None else None), name+\"[%s]\" % str(key))\n    elif isinstance(x, list):\n        for i, x_i in enumerate(x):\n            y += json2space(x_i, (oldy[i] if oldy !=\n                                  None else None), name+\"[%d]\" % i)\n    else:\n        pass\n    return y", "output": "Change search space from json format to hyperopt format", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def signature(self):\n        \"\"\"\"\"\"\n        if self.usage is not None:\n            return self.usage\n\n\n        params = self.clean_params\n        if not params:\n            return ''\n\n        result = []\n        for name, param in params.items():\n            greedy = isinstance(param.annotation, converters._Greedy)\n\n            if param.default is not param.empty:\n                # We don't want None or '' to trigger the [name=value] case and instead it should\n                # do [name] since [name=None] or [name=] are not exactly useful for the user.\n                should_print = param.default if isinstance(param.default, str) else param.default is not None\n                if should_print:\n                    result.append('[%s=%s]' % (name, param.default) if not greedy else\n                                  '[%s=%s]...' % (name, param.default))\n                    continue\n                else:\n                    result.append('[%s]' % name)\n\n            elif param.kind == param.VAR_POSITIONAL:\n                result.append('[%s...]' % name)\n            elif greedy:\n                result.append('[%s]...' % name)\n            elif self._is_typing_optional(param.annotation):\n                result.append('[%s]' % name)\n            else:\n                result.append('<%s>' % name)\n\n        return ' '.join(result)", "output": "Returns a POSIX-like signature useful for help command output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def join(self, timeout: Union[float, datetime.timedelta] = None) -> Awaitable[None]:\n        \"\"\"\n        \"\"\"\n        return self._finished.wait(timeout)", "output": "Block until all items in the queue are processed.\n\n        Returns an awaitable, which raises `tornado.util.TimeoutError` after a\n        timeout.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def choose_trial_to_run(self, trial_runner):\n        \"\"\"\n        \"\"\"\n\n        candidates = []\n        for trial in trial_runner.get_trials():\n            if trial.status in [Trial.PENDING, Trial.PAUSED] and \\\n                    trial_runner.has_resources(trial.resources):\n                candidates.append(trial)\n        candidates.sort(\n            key=lambda trial: self._trial_state[trial].last_perturbation_time)\n        return candidates[0] if candidates else None", "output": "Ensures all trials get fair share of time (as defined by time_attr).\n\n        This enables the PBT scheduler to support a greater number of\n        concurrent trials than can fit in the cluster at any given time.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def createURLParserCtxt(filename, options):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlCreateURLParserCtxt(filename, options)\n    if ret is None:raise parserError('xmlCreateURLParserCtxt() failed')\n    return parserCtxt(_obj=ret)", "output": "Create a parser context for a file or URL content.\n      Automatic support for ZLIB/Compress compressed document is\n      provided by default if found at compile-time and for file\n       accesses", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _log_error_and_abort(ret, obj):\n    '''\n    \n    '''\n    ret['result'] = False\n    ret['abort'] = True\n    if 'error' in obj:\n        ret['comment'] = '{0}'.format(obj.get('error'))\n    return ret", "output": "helper function to update errors in the return structure", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_instances(name, lifecycle_state=\"InService\", health_status=\"Healthy\",\n                  attribute=\"private_ip_address\", attributes=None, region=None,\n                  key=None, keyid=None, profile=None):\n    '''\n    \n\n    '''\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    ec2_conn = _get_ec2_conn(region=region, key=key, keyid=keyid, profile=profile)\n    retries = 30\n    while True:\n        try:\n            asgs = conn.get_all_groups(names=[name])\n            break\n        except boto.exception.BotoServerError as e:\n            if retries and e.code == 'Throttling':\n                log.debug('Throttled by AWS API, retrying in 5 seconds...')\n                time.sleep(5)\n                retries -= 1\n                continue\n            log.error(e)\n            return False\n    if len(asgs) != 1:\n        log.debug(\"name '%s' returns multiple ASGs: %s\", name, [asg.name for asg in asgs])\n        return False\n    asg = asgs[0]\n    instance_ids = []\n    # match lifecycle_state and health_status\n    for i in asg.instances:\n        if lifecycle_state is not None and i.lifecycle_state != lifecycle_state:\n            continue\n        if health_status is not None and i.health_status != health_status:\n            continue\n        instance_ids.append(i.instance_id)\n    # get full instance info, so that we can return the attribute\n    instances = ec2_conn.get_only_instances(instance_ids=instance_ids)\n    if attributes:\n        return [[_convert_attribute(instance, attr) for attr in attributes] for instance in instances]\n    else:\n        # properly handle case when not all instances have the requested attribute\n        return [_convert_attribute(instance, attribute) for instance in instances if getattr(instance, attribute)]", "output": "return attribute of all instances in the named autoscale group.\n\n    CLI example::\n\n        salt-call boto_asg.get_instances my_autoscale_group_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shell_split(text):\r\n    \"\"\"\r\n    \r\n    \"\"\"\r\n    assert is_text_string(text)  # in case a QString is passed...\r\n    pattern = r'(\\s+|(?<!\\\\)\".*?(?<!\\\\)\"|(?<!\\\\)\\'.*?(?<!\\\\)\\')'\r\n    out = []\r\n    for token in re.split(pattern, text):\r\n        if token.strip():\r\n            out.append(token.strip('\"').strip(\"'\"))\r\n    return out", "output": "Split the string `text` using shell-like syntax\r\n\r\n    This avoids breaking single/double-quoted strings (e.g. containing\r\n    strings with spaces). This function is almost equivalent to the shlex.split\r\n    function (see standard library `shlex`) except that it is supporting\r\n    unicode strings (shlex does not support unicode until Python 2.7.3).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stop(self, wait=True):\n        ''' \n\n        '''\n        assert not self._stopped, \"Already stopped\"\n        self._stopped = True\n        self._tornado.stop(wait)\n        self._http.stop()", "output": "Stop the Bokeh Server.\n\n        This stops and removes all Bokeh Server ``IOLoop`` callbacks, as well\n        as stops the ``HTTPServer`` that this instance was configured with.\n\n        Args:\n            fast (bool):\n                Whether to wait for orderly cleanup (default: True)\n\n        Returns:\n            None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transform(self, X):\n        \"\"\"\n        \"\"\"\n        if isinstance(X, pd.DataFrame):\n            X_transformed = X[self.feat_list].values\n        elif isinstance(X, np.ndarray):\n            X_transformed = X[:, self.feat_list_idx]\n\n        return X_transformed.astype(np.float64)", "output": "Make subset after fit\n\n        Parameters\n        ----------\n        X: numpy ndarray, {n_samples, n_features}\n            New data, where n_samples is the number of samples and n_features is the number of features.\n\n        Returns\n        -------\n        X_transformed: array-like, shape (n_samples, n_features + 1) or (n_samples, n_features + 1 + n_classes) for classifier with predict_proba attribute\n            The transformed feature set.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def transformer_parsing_base():\n  \"\"\"\"\"\"\n  hparams = transformer_base()\n  hparams.attention_dropout = 0.2\n  hparams.layer_prepostprocess_dropout = 0.2\n  hparams.max_length = 512\n  hparams.learning_rate_warmup_steps = 16000\n  hparams.hidden_size = 1024\n  hparams.learning_rate = 0.05\n  hparams.shared_embedding_and_softmax_weights = False\n  return hparams", "output": "HParams for parsing on WSJ only.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def info_installed(*names, **kwargs):\n    '''\n    \n    '''\n    all_versions = kwargs.get('all_versions', False)\n    ret = dict()\n    for pkg_name, pkgs_nfo in __salt__['lowpkg.info'](*names, **kwargs).items():\n        pkg_nfo = pkgs_nfo if all_versions else [pkgs_nfo]\n        for _nfo in pkg_nfo:\n            t_nfo = dict()\n            # Translate dpkg-specific keys to a common structure\n            for key, value in _nfo.items():\n                if key == 'source_rpm':\n                    t_nfo['source'] = value\n                else:\n                    t_nfo[key] = value\n            if not all_versions:\n                ret[pkg_name] = t_nfo\n            else:\n                ret.setdefault(pkg_name, []).append(t_nfo)\n    return ret", "output": ".. versionadded:: 2015.8.1\n\n    Return the information of the named package(s), installed on the system.\n\n    :param all_versions:\n        Include information for all versions of the packages installed on the minion.\n\n    CLI example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.info_installed <package1>\n        salt '*' pkg.info_installed <package1> <package2> <package3> ...\n        salt '*' pkg.info_installed <package1> <package2> <package3> all_versions=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_focus_widget_properties(self):\r\n        \"\"\"\"\"\"\r\n        from spyder.plugins.editor.widgets.editor import TextEditBaseWidget\r\n        from spyder.plugins.ipythonconsole.widgets import ControlWidget\r\n        widget = QApplication.focusWidget()\r\n\r\n        textedit_properties = None\r\n        if isinstance(widget, (TextEditBaseWidget, ControlWidget)):\r\n            console = isinstance(widget, ControlWidget)\r\n            not_readonly = not widget.isReadOnly()\r\n            readwrite_editor = not_readonly and not console\r\n            textedit_properties = (console, not_readonly, readwrite_editor)\r\n        return widget, textedit_properties", "output": "Get properties of focus widget\r\n        Returns tuple (widget, properties) where properties is a tuple of\r\n        booleans: (is_console, not_readonly, readwrite_editor)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_pyproject_toml(self):\n        # type: () -> None\n        \"\"\"\n        \"\"\"\n        pep517_data = load_pyproject_toml(\n            self.use_pep517,\n            self.pyproject_toml,\n            self.setup_py,\n            str(self)\n        )\n\n        if pep517_data is None:\n            self.use_pep517 = False\n        else:\n            self.use_pep517 = True\n            requires, backend, check = pep517_data\n            self.requirements_to_check = check\n            self.pyproject_requires = requires\n            self.pep517_backend = Pep517HookCaller(self.setup_py_dir, backend)\n\n            # Use a custom function to call subprocesses\n            self.spin_message = \"\"\n\n            def runner(cmd, cwd=None, extra_environ=None):\n                with open_spinner(self.spin_message) as spinner:\n                    call_subprocess(\n                        cmd,\n                        cwd=cwd,\n                        extra_environ=extra_environ,\n                        show_stdout=False,\n                        spinner=spinner\n                    )\n                self.spin_message = \"\"\n\n            self.pep517_backend._subprocess_runner = runner", "output": "Load the pyproject.toml file.\n\n        After calling this routine, all of the attributes related to PEP 517\n        processing for this requirement have been set. In particular, the\n        use_pep517 attribute can be used to determine whether we should\n        follow the PEP 517 or legacy (setup.py) code path.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def full_data(tgt,\n              fun,\n              arg=None,\n              tgt_type='glob',\n              returner='',\n              timeout=5):\n    '''\n    \n\n    '''\n    return _publish(tgt,\n                    fun,\n                    arg=arg,\n                    tgt_type=tgt_type,\n                    returner=returner,\n                    timeout=timeout,\n                    form='full',\n                    wait=True)", "output": "Return the full data about the publication, this is invoked in the same\n    way as the publish function\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt system.example.com publish.full_data '*' cmd.run 'ls -la /tmp'\n\n    .. admonition:: Attention\n\n        If you need to pass a value to a function argument and that value\n        contains an equal sign, you **must** include the argument name.\n        For example:\n\n        .. code-block:: bash\n\n            salt '*' publish.full_data test.kwarg arg='cheese=spam'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def log_inference(batch_id, batch_num, metric, step_loss, log_interval):\n    \"\"\"\n    \"\"\"\n    metric_nm, metric_val = metric.get()\n    if not isinstance(metric_nm, list):\n        metric_nm = [metric_nm]\n        metric_val = [metric_val]\n\n    eval_str = '[Batch %d/%d] loss=%.4f, metrics:' + \\\n               ','.join([i + ':%.4f' for i in metric_nm])\n    logging.info(eval_str, batch_id + 1, batch_num, \\\n                 step_loss / log_interval, \\\n                 *metric_val)", "output": "Generate and print out the log message for inference.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dictionary_to_stringlist(input_dict):\n    '''\n    \n    '''\n    string_value = \"\"\n    for s in input_dict:\n        string_value += \"{0}={1},\".format(s, input_dict[s])\n    string_value = string_value[:-1]\n    return string_value", "output": "Convert a dictionary to a stringlist (comma separated settings)\n\n    The result of the dictionary {'setting1':'value1','setting2':'value2'} will be:\n\n    setting1=value1,setting2=value2", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_resources(name, expr, resources):\n    \"\"\"\n    \"\"\"\n    if expr is None:\n        return\n    bound = expr._resources()\n    if not bound and resources is None:\n        raise ValueError('no resources provided to compute %s' % name)\n    if bound and resources:\n        raise ValueError(\n            'explicit and implicit resources provided to compute %s' % name,\n        )", "output": "Validate that the expression and resources passed match up.\n\n    Parameters\n    ----------\n    name : str\n        The name of the argument we are checking.\n    expr : Expr\n        The potentially bound expr.\n    resources\n        The explicitly passed resources to compute expr.\n\n    Raises\n    ------\n    ValueError\n        If the resources do not match for an expression.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_jids():\n    '''\n    \n    '''\n    serv = _get_serv(ret=None)\n    sql = \"select distinct(jid) from jids group by load\"\n\n    # [{u'points': [[0, jid, load],\n    #               [0, jid, load]],\n    #   u'name': u'jids',\n    #   u'columns': [u'time', u'distinct', u'load']}]\n    data = serv.query(sql)\n    ret = {}\n    if data:\n        for _, jid, load in data[0]['points']:\n            ret[jid] = salt.utils.jid.format_jid_instance(jid, salt.utils.json.loads(load))\n    return ret", "output": "Return a list of all job ids", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_external_workers(worker):\n    \"\"\"\n    \n    \"\"\"\n    worker_that_blocked_task = collections.defaultdict(set)\n    get_work_response_history = worker._get_work_response_history\n    for get_work_response in get_work_response_history:\n        if get_work_response['task_id'] is None:\n            for running_task in get_work_response['running_tasks']:\n                other_worker_id = running_task['worker']\n                other_task_id = running_task['task_id']\n                other_task = worker._scheduled_tasks.get(other_task_id)\n                if other_worker_id == worker._id or not other_task:\n                    continue\n                worker_that_blocked_task[other_worker_id].add(other_task)\n    return worker_that_blocked_task", "output": "This returns a dict with a set of tasks for all of the other workers", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def power_on_vm(name, datacenter=None, service_instance=None):\n    '''\n    \n\n    '''\n    log.trace('Powering on virtual machine %s', name)\n    vm_properties = [\n        'name',\n        'summary.runtime.powerState'\n    ]\n    virtual_machine = salt.utils.vmware.get_vm_by_property(\n        service_instance,\n        name,\n        datacenter=datacenter,\n        vm_properties=vm_properties)\n    if virtual_machine['summary.runtime.powerState'] == 'poweredOn':\n        result = {'comment': 'Virtual machine is already powered on',\n                  'changes': {'power_on': True}}\n        return result\n    salt.utils.vmware.power_cycle_vm(virtual_machine['object'], action='on')\n    result = {'comment': 'Virtual machine power on action succeeded',\n              'changes': {'power_on': True}}\n    return result", "output": "Powers on a virtual machine specified by it's name.\n\n    name\n        Name of the virtual machine\n\n    datacenter\n        Datacenter of the virtual machine\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.power_on_vm name=my_vm", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def clean_whitespace(statement):\n    \"\"\"\n    \n    \"\"\"\n    import re\n\n    # Replace linebreaks and tabs with spaces\n    statement.text = statement.text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n\n    # Remove any leeding or trailing whitespace\n    statement.text = statement.text.strip()\n\n    # Remove consecutive spaces\n    statement.text = re.sub(' +', ' ', statement.text)\n\n    return statement", "output": "Remove any consecutive whitespace characters from the statement text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def try_one_generator (project, name, generator, target_type, properties, sources):\n    \"\"\" \n    \"\"\"\n    if __debug__:\n        from .targets import ProjectTarget\n        assert isinstance(project, ProjectTarget)\n        assert isinstance(name, basestring) or name is None\n        assert isinstance(generator, Generator)\n        assert isinstance(target_type, basestring)\n        assert isinstance(properties, property_set.PropertySet)\n        assert is_iterable_typed(sources, virtual_target.VirtualTarget)\n    source_types = []\n\n    for s in sources:\n        source_types.append (s.type ())\n\n    viable_source_types = viable_source_types_for_generator (generator)\n\n    if source_types and viable_source_types != ['*'] and\\\n           not set_.intersection (source_types, viable_source_types):\n        if project.manager ().logger ().on ():\n            id = generator.id ()\n            project.manager ().logger ().log (__name__, \"generator '%s' pruned\" % id)\n            project.manager ().logger ().log (__name__, \"source_types\" '%s' % source_types)\n            project.manager ().logger ().log (__name__, \"viable_source_types '%s'\" % viable_source_types)\n\n        return []\n\n    else:\n        return try_one_generator_really (project, name, generator, target_type, properties, sources)", "output": "Checks if generator invocation can be pruned, because it's guaranteed\n        to fail. If so, quickly returns empty list. Otherwise, calls\n        try_one_generator_really.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def load_external_data_for_tensor(tensor, base_dir):  # type: (TensorProto, Text) -> None\n    \"\"\"\n    \n    \"\"\"\n    if tensor.HasField(\"raw_data\"):  # already loaded\n        return\n    info = ExternalDataInfo(tensor)\n    file_location = _sanitize_path(info.location)\n    external_data_file_path = os.path.join(base_dir, file_location)\n\n    with open(external_data_file_path, 'rb') as data_file:\n\n        if info.offset:\n            data_file.seek(info.offset)\n\n        if info.length:\n            tensor.raw_data = data_file.read(info.length)\n        else:\n            tensor.raw_data = data_file.read()", "output": "Load data from an external file for tensor.\n\n    @params\n    tensor: a TensorProto object.\n    base_dir: directory that contains the external data.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def param_dict_to_str(data):\n    \"\"\"\"\"\"\n    if data is None or not data:\n        return \"\"\n    pairs = []\n    for key, val in data.items():\n        if isinstance(val, (list, tuple, set)) or is_numpy_1d_array(val):\n            pairs.append(str(key) + '=' + ','.join(map(str, val)))\n        elif isinstance(val, string_type) or isinstance(val, numeric_types) or is_numeric(val):\n            pairs.append(str(key) + '=' + str(val))\n        elif val is not None:\n            raise TypeError('Unknown type of parameter:%s, got:%s'\n                            % (key, type(val).__name__))\n    return ' '.join(pairs)", "output": "Convert Python dictionary to string, which is passed to C API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getPermissionBit(self, t, m):\n        '''\n        \n        '''\n        try:\n            if isinstance(m, string_types):\n                return self.rights[t][m]['BITS']\n            else:\n                return m\n        except KeyError:\n            raise CommandExecutionError((\n                'No right \"{0}\".  It should be one of the following:  {1}')\n                .format(m, ', '.join(self.rights[t])))", "output": "returns a permission bit of the string permission value for the specified object type", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def all(self, axis=0, bool_only=None, skipna=True, level=None, **kwargs):\r\n        \"\"\"\r\n        \"\"\"\r\n        if axis is not None:\r\n            axis = self._get_axis_number(axis)\r\n            if bool_only and axis == 0:\r\n                if hasattr(self, \"dtype\"):\r\n                    raise NotImplementedError(\r\n                        \"{}.{} does not implement numeric_only.\".format(\r\n                            self.__name__, \"all\"\r\n                        )\r\n                    )\r\n                data_for_compute = self[self.columns[self.dtypes == np.bool]]\r\n                return data_for_compute.all(\r\n                    axis=axis, bool_only=False, skipna=skipna, level=level, **kwargs\r\n                )\r\n            return self._reduce_dimension(\r\n                self._query_compiler.all(\r\n                    axis=axis, bool_only=bool_only, skipna=skipna, level=level, **kwargs\r\n                )\r\n            )\r\n        else:\r\n            if bool_only:\r\n                raise ValueError(\"Axis must be 0 or 1 (got {})\".format(axis))\r\n            # Reduce to a scalar if axis is None.\r\n            result = self._reduce_dimension(\r\n                self._query_compiler.all(\r\n                    axis=0, bool_only=bool_only, skipna=skipna, level=level, **kwargs\r\n                )\r\n            )\r\n            if isinstance(result, BasePandasDataset):\r\n                return result.all(\r\n                    axis=axis, bool_only=bool_only, skipna=skipna, level=level, **kwargs\r\n                )\r\n            return result", "output": "Return whether all elements are True over requested axis\r\n\r\n        Note:\r\n            If axis=None or axis=0, this call applies df.all(axis=1)\r\n                to the transpose of df.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _generate_examples(self, filepath):\n    \"\"\"\n    \"\"\"\n    with tf.io.gfile.GFile(filepath, \"rb\") as f:\n      data = tfds.core.lazy_imports.scipy.io.loadmat(f)\n\n    # Maybe should shuffle ?\n\n    assert np.max(data[\"y\"]) <= 10  # Sanity check\n    assert np.min(data[\"y\"]) > 0\n\n    for image, label in zip(np.rollaxis(data[\"X\"], -1), data[\"y\"]):\n      yield {\n          \"image\": image,\n          \"label\": label % 10,  # digit 0 is saved as 0 (instead of 10)\n      }", "output": "Generate examples as dicts.\n\n    Args:\n      filepath: `str` path of the file to process.\n\n    Yields:\n      Generator yielding the next samples", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def next(self) -> Future:\n        \"\"\"\n        \"\"\"\n        self._running_future = Future()\n\n        if self._finished:\n            self._return_result(self._finished.popleft())\n\n        return self._running_future", "output": "Returns a `.Future` that will yield the next available result.\n\n        Note that this `.Future` will not be the same object as any of\n        the inputs.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def compress_encoder_2d(x, hparams, name=None):\n  \"\"\"\n  \"\"\"\n  return compress_encoder(\n      x,\n      hparams,\n      strides=(2, 2),\n      kernel_size=(hparams.kernel_size, hparams.kernel_size),\n      name=name)", "output": "Encoder that compresses 2-D inputs by 2**num_compress_steps.\n\n  Args:\n    x: Tensor of shape [batch, height, width, channels].\n    hparams: HParams.\n    name: string, variable scope.\n\n  Returns:\n    Tensor of shape [batch, latent_length, hparams.hidden_size], where\n      latent_length is\n      hparams.num_latents * (height*width) / 2**(hparams.num_compress_steps).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rlmb_base_sv2p():\n  \"\"\"\"\"\"\n  hparams = rlmb_base()\n  hparams.learning_rate_bump = 1.0\n  hparams.generative_model = \"next_frame_sv2p\"\n  hparams.generative_model_params = \"next_frame_sv2p_atari\"\n  return hparams", "output": "Base setting with sv2p as world model.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _params_extend(params, _ignore_name=False, **kwargs):\n    '''\n    \n\n    '''\n    # extend params value by optional zabbix API parameters\n    for key in kwargs:\n        if not key.startswith('_'):\n            params.setdefault(key, kwargs[key])\n\n    # ignore name parameter passed from Salt state module, use firstname or visible_name instead\n    if _ignore_name:\n        params.pop('name', None)\n        if 'firstname' in params:\n            params['name'] = params.pop('firstname')\n        elif 'visible_name' in params:\n            params['name'] = params.pop('visible_name')\n\n    return params", "output": "Extends the params dictionary by values from keyword arguments.\n\n    .. versionadded:: 2016.3.0\n\n    :param params: Dictionary with parameters for zabbix API.\n    :param _ignore_name: Salt State module is passing first line as 'name' parameter. If API uses optional parameter\n    'name' (for ex. host_create, user_create method), please use 'visible_name' or 'firstname' instead of 'name' to\n    not mess these values.\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: Extended params dictionary with parameters.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def unload_extension(self, name):\n        \"\"\"\n        \"\"\"\n\n        lib = self.__extensions.get(name)\n        if lib is None:\n            raise errors.ExtensionNotLoaded(name)\n\n        self._remove_module_references(lib.__name__)\n        self._call_module_finalizers(lib, name)", "output": "Unloads an extension.\n\n        When the extension is unloaded, all commands, listeners, and cogs are\n        removed from the bot and the module is un-imported.\n\n        The extension can provide an optional global function, ``teardown``,\n        to do miscellaneous clean-up if necessary. This function takes a single\n        parameter, the ``bot``, similar to ``setup`` from\n        :meth:`~.Bot.load_extension`.\n\n        Parameters\n        ------------\n        name: :class:`str`\n            The extension name to unload. It must be dot separated like\n            regular Python imports if accessing a sub-module. e.g.\n            ``foo.test`` if you want to import ``foo/test.py``.\n\n        Raises\n        -------\n        ExtensionNotLoaded\n            The extension was not loaded.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _indent(lines, prefix=\"  \"):\n    \"\"\"\n    \"\"\"\n    indented = []\n    for line in lines.split(\"\\n\"):\n        indented.append(prefix + line)\n    return \"\\n\".join(indented)", "output": "Indent some text.\n\n    Note that this is present as ``textwrap.indent``, but not in Python 2.\n\n    Args:\n        lines (str): The newline delimited string to be indented.\n        prefix (Optional[str]): The prefix to indent each line with. Default\n            to two spaces.\n\n    Returns:\n        str: The newly indented content.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def getconfig():\n    '''\n    \n    '''\n    try:\n        config = '/etc/selinux/config'\n        with salt.utils.files.fopen(config, 'r') as _fp:\n            for line in _fp:\n                line = salt.utils.stringutils.to_unicode(line)\n                if line.strip().startswith('SELINUX='):\n                    return line.split('=')[1].capitalize().strip()\n    except (IOError, OSError, AttributeError):\n        return None\n    return None", "output": "Return the selinux mode from the config file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.getconfig", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def simple_cnn(actns:Collection[int], kernel_szs:Collection[int]=None,\n               strides:Collection[int]=None, bn=False) -> nn.Sequential:\n    \"\"\n    nl = len(actns)-1\n    kernel_szs = ifnone(kernel_szs, [3]*nl)\n    strides    = ifnone(strides   , [2]*nl)\n    layers = [conv_layer(actns[i], actns[i+1], kernel_szs[i], stride=strides[i],\n              norm_type=(NormType.Batch if bn and i<(len(strides)-1) else None)) for i in range_of(strides)]\n    layers.append(PoolFlatten())\n    return nn.Sequential(*layers)", "output": "CNN with `conv_layer` defined by `actns`, `kernel_szs` and `strides`, plus batchnorm if `bn`.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def volume_show(name, profile=None, **kwargs):\n    '''\n    \n\n    '''\n    conn = _auth(profile, **kwargs)\n    return conn.volume_show(name)", "output": "Create a block storage volume\n\n    name\n        Name of the volume\n\n    profile\n        Profile to use\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.volume_show myblock profile=openstack", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_apply_state(meta_graph, state_ops, feed_map):\n  \"\"\"\"\"\"\n  for node in meta_graph.graph_def.node:\n    keys_to_purge = []\n    tensor_name = node.name + \":0\"\n    # Verify that the node is a state op and that its due to be rewired\n    # in the feedmap.\n    if node.op in state_ops and tensor_name in feed_map:\n      node.op = \"Placeholder\"\n      for key in node.attr:\n        # Only shape and dtype are required for Placeholder. Remove other\n        # attributes.\n        if key != \"shape\":\n          keys_to_purge.append(key)\n      for key in keys_to_purge:\n        del node.attr[key]\n      node.attr[\"dtype\"].type = types_pb2.DT_RESOURCE", "output": "Replaces state ops with non state Placeholder ops for the apply graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def rename_set(set=None, new_set=None, family='ipv4'):\n    '''\n    \n    '''\n\n    if not set:\n        return 'Error: Set needs to be specified'\n\n    if not new_set:\n        return 'Error: New name for set needs to be specified'\n\n    settype = _find_set_type(set)\n    if not settype:\n        return 'Error: Set does not exist'\n\n    settype = _find_set_type(new_set)\n    if settype:\n        return 'Error: New Set already exists'\n\n    cmd = '{0} rename {1} {2}'.format(_ipset_cmd(), set, new_set)\n    out = __salt__['cmd.run'](cmd, python_shell=False)\n\n    if not out:\n        out = True\n    return out", "output": ".. versionadded:: 2014.7.0\n\n    Delete ipset set.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ipset.rename_set custom_set new_set=new_set_name\n\n        IPv6:\n        salt '*' ipset.rename_set custom_set new_set=new_set_name family=ipv6", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def user_password_update(user_id=None, name=None, password=None,\n                         profile=None, **connection_args):\n    '''\n    \n    '''\n    kstone = auth(profile, **connection_args)\n    if name:\n        for user in kstone.users.list():\n            if user.name == name:\n                user_id = user.id\n                break\n    if not user_id:\n        return {'Error': 'Unable to resolve user id'}\n\n    if _OS_IDENTITY_API_VERSION > 2:\n        kstone.users.update(user=user_id, password=password)\n    else:\n        kstone.users.update_password(user=user_id, password=password)\n    ret = 'Password updated for user ID {0}'.format(user_id)\n    if name:\n        ret += ' ({0})'.format(name)\n    return ret", "output": "Update a user's password (keystone user-password-update)\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' keystone.user_password_update c965f79c4f864eaaa9c3b41904e67082 password=12345\n        salt '*' keystone.user_password_update user_id=c965f79c4f864eaaa9c3b41904e67082 password=12345\n        salt '*' keystone.user_password_update name=nova password=12345", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def kernels_initialize(self, folder):\n        \"\"\" \n        \"\"\"\n        if not os.path.isdir(folder):\n            raise ValueError('Invalid folder: ' + folder)\n\n        resources = []\n        resource = {'path': 'INSERT_SCRIPT_PATH_HERE'}\n        resources.append(resource)\n\n        username = self.get_config_value(self.CONFIG_NAME_USER)\n        meta_data = {\n            'id': username + '/INSERT_KERNEL_SLUG_HERE',\n            'title': 'INSERT_TITLE_HERE',\n            'code_file': 'INSERT_CODE_FILE_PATH_HERE',\n            'language': 'INSERT_LANGUAGE_HERE',\n            'kernel_type': 'INSERT_KERNEL_TYPE_HERE',\n            'is_private': 'true',\n            'enable_gpu': 'false',\n            'enable_internet': 'false',\n            'dataset_sources': [],\n            'competition_sources': [],\n            'kernel_sources': [],\n        }\n        meta_file = os.path.join(folder, self.KERNEL_METADATA_FILE)\n        with open(meta_file, 'w') as f:\n            json.dump(meta_data, f, indent=2)\n\n        return meta_file", "output": "create a new kernel in a specified folder from template, including\n            json metadata that grabs values from the configuration.\n             Parameters\n            ==========\n            folder: the path of the folder", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def itemsize(self):\n        \"\"\"\n        \n        \"\"\"\n        warnings.warn(\"{obj}.itemsize is deprecated and will be removed \"\n                      \"in a future version\".format(obj=type(self).__name__),\n                      FutureWarning, stacklevel=2)\n        return self._ndarray_values.itemsize", "output": "Return the size of the dtype of the item of the underlying data.\n\n        .. deprecated:: 0.23.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def video_l1_loss(top_out, targets, model_hparams, vocab_size, weights_fn):\n  \"\"\"\"\"\"\n  del vocab_size  # unused arg\n  logits = top_out\n  logits = tf.reshape(logits, [-1] + common_layers.shape_list(logits)[2:-1])\n  targets = tf.reshape(targets, [-1] + common_layers.shape_list(targets)[2:])\n  weights = weights_fn(targets)\n  # Shift targets by 0.5 so later just casting to int gives the prediction.\n  # So for int targets, say 0 and 7, we actually train to predict 0.5 and 7.5.\n  # Later (in merics or infer) this is cast to int anyway. Also, we have no\n  # loss beyond cutoff = 0.2 as these are already correct predictions.\n  targets = tf.to_float(targets) + 0.5\n  loss = video_l1_internal_loss(logits, targets, model_hparams)\n  return tf.reduce_sum(loss * weights), tf.reduce_sum(weights)", "output": "Compute loss numerator and denominator for one shard of output.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _legacy_init(self, name, arr):\n        \"\"\"\n        \"\"\"\n        warnings.warn(\n            \"\\033[91mCalling initializer with init(str, NDArray) has been deprecated.\" \\\n            \"please use init(mx.init.InitDesc(...), NDArray) instead.\\033[0m\",\n            DeprecationWarning, stacklevel=3)\n        if not isinstance(name, string_types):\n            raise TypeError('name must be string')\n        if not isinstance(arr, NDArray):\n            raise TypeError('arr must be NDArray')\n        if name.startswith('upsampling'):\n            self._init_bilinear(name, arr)\n        elif name.startswith('stn_loc') and name.endswith('weight'):\n            self._init_zero(name, arr)\n        elif name.startswith('stn_loc') and name.endswith('bias'):\n            self._init_loc_bias(name, arr)\n        elif name.endswith('bias'):\n            self._init_bias(name, arr)\n        elif name.endswith('gamma'):\n            self._init_gamma(name, arr)\n        elif name.endswith('beta'):\n            self._init_beta(name, arr)\n        elif name.endswith('weight'):\n            self._init_weight(name, arr)\n        elif name.endswith(\"moving_mean\"):\n            self._init_zero(name, arr)\n        elif name.endswith(\"moving_var\"):\n            self._init_one(name, arr)\n        elif name.endswith(\"moving_inv_var\"):\n            self._init_zero(name, arr)\n        elif name.endswith(\"moving_avg\"):\n            self._init_zero(name, arr)\n        elif name.endswith('min'):\n            self._init_zero(name, arr)\n        elif name.endswith('max'):\n            self._init_one(name, arr)\n        else:\n            self._init_default(name, arr)", "output": "Legacy initialization method.\n\n        Parameters\n        ----------\n        name : str\n            Name of corresponding NDArray.\n\n        arr : NDArray\n            NDArray to be initialized.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def set_field_at(self, index, *, name, value, inline=True):\n        \"\"\"\n        \"\"\"\n\n        try:\n            field = self._fields[index]\n        except (TypeError, IndexError, AttributeError):\n            raise IndexError('field index out of range')\n\n        field['name'] = str(name)\n        field['value'] = str(value)\n        field['inline'] = inline\n        return self", "output": "Modifies a field to the embed object.\n\n        The index must point to a valid pre-existing field.\n\n        This function returns the class instance to allow for fluent-style\n        chaining.\n\n        Parameters\n        -----------\n        index: :class:`int`\n            The index of the field to modify.\n        name: :class:`str`\n            The name of the field.\n        value: :class:`str`\n            The value of the field.\n        inline: :class:`bool`\n            Whether the field should be displayed inline.\n\n        Raises\n        -------\n        IndexError\n            An invalid index was provided.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shift_right(x, pad_value=None):\n  \"\"\"\"\"\"\n  if pad_value is None:\n    shifted_targets = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])[:, :-1, :, :]\n  else:\n    shifted_targets = tf.concat([pad_value, x], axis=1)[:, :-1, :, :]\n  return shifted_targets", "output": "Shift the second dimension of x right by one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def readerForMemory(buffer, size, URL, encoding, options):\n    \"\"\" \"\"\"\n    ret = libxml2mod.xmlReaderForMemory(buffer, size, URL, encoding, options)\n    if ret is None:raise treeError('xmlReaderForMemory() failed')\n    return xmlTextReader(_obj=ret)", "output": "Create an xmltextReader for an XML in-memory document. The\n      parsing flags @options are a combination of xmlParserOption.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cmd(command, *args, **kwargs):\n    '''\n    \n    '''\n    for k in list(kwargs):\n        if k.startswith('__pub_'):\n            kwargs.pop(k)\n    local_command = '.'.join(['nxos', command])\n    log.info('local command: %s', local_command)\n    if local_command not in __salt__:\n        return False\n    return __salt__[local_command](*args, **kwargs)", "output": "NOTE: This function is preserved for backwards compatibilty.  This allows\n    commands to be executed using either of the following syntactic forms.\n\n    salt '*' nxos.cmd <function>\n\n    or\n\n    salt '*' nxos.<function>\n\n    command\n        function from `salt.modules.nxos` to run\n\n    args\n        positional args to pass to `command` function\n\n    kwargs\n        key word arguments to pass to `command` function\n\n    .. code-block:: bash\n\n        salt '*' nxos.cmd sendline 'show ver'\n        salt '*' nxos.cmd show_run\n        salt '*' nxos.cmd check_password username=admin password='$5$lkjsdfoi$blahblahblah' encrypted=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resolve_revision(self, dest, url, rev_options):\n        \"\"\"\n        \n        \"\"\"\n        rev = rev_options.arg_rev\n        sha, is_branch = self.get_revision_sha(dest, rev)\n\n        if sha is not None:\n            rev_options = rev_options.make_new(sha)\n            rev_options.branch_name = rev if is_branch else None\n\n            return rev_options\n\n        # Do not show a warning for the common case of something that has\n        # the form of a Git commit hash.\n        if not looks_like_hash(rev):\n            logger.warning(\n                \"Did not find branch or tag '%s', assuming revision or ref.\",\n                rev,\n            )\n\n        if not rev.startswith('refs/'):\n            return rev_options\n\n        # If it looks like a ref, we have to fetch it explicitly.\n        self.run_command(\n            ['fetch', '-q', url] + rev_options.to_args(),\n            cwd=dest,\n        )\n        # Change the revision to the SHA of the ref we fetched\n        sha = self.get_revision(dest, rev='FETCH_HEAD')\n        rev_options = rev_options.make_new(sha)\n\n        return rev_options", "output": "Resolve a revision to a new RevOptions object with the SHA1 of the\n        branch, tag, or ref if found.\n\n        Args:\n          rev_options: a RevOptions object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def current_changed(self, index):\r\n        \"\"\"\"\"\"\r\n#        count = self.get_stack_count()\r\n#        for btn in (self.filelist_btn, self.previous_btn, self.next_btn):\r\n#            btn.setEnabled(count > 1)\r\n\r\n        editor = self.get_current_editor()\r\n        if editor.lsp_ready and not editor.document_opened:\r\n            editor.document_did_open()\r\n        if index != -1:\r\n            editor.setFocus()\r\n            logger.debug(\"Set focus to: %s\" % editor.filename)\r\n        else:\r\n            self.reset_statusbar.emit()\r\n        self.opened_files_list_changed.emit()\r\n\r\n        self.stack_history.refresh()\r\n        self.stack_history.remove_and_append(index)\r\n\r\n        # Needed to avoid an error generated after moving/renaming\r\n        # files outside Spyder while in debug mode.\r\n        # See issue 8749.\r\n        try:\r\n            logger.debug(\"Current changed: %d - %s\" %\r\n                         (index, self.data[index].editor.filename))\r\n        except IndexError:\r\n            pass\r\n\r\n        self.update_plugin_title.emit()\r\n        if editor is not None:\r\n            # Needed in order to handle the close of files open in a directory\r\n            # that has been renamed. See issue 5157\r\n            try:\r\n                self.current_file_changed.emit(self.data[index].filename,\r\n                                               editor.get_position('cursor'))\r\n            except IndexError:\r\n                pass", "output": "Stack index has changed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert_activation(net, node, module, builder):\n    \"\"\"\n    \"\"\"\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    mx_non_linearity = _get_attrs(node)['act_type']\n    #TODO add SCALED_TANH, SOFTPLUS, SOFTSIGN, SIGMOID_HARD, LEAKYRELU, PRELU, ELU, PARAMETRICSOFTPLUS, THRESHOLDEDRELU, LINEAR\n    if mx_non_linearity == 'relu':\n        non_linearity = 'RELU'\n    elif mx_non_linearity == 'tanh':\n        non_linearity = 'TANH'\n    elif mx_non_linearity == 'sigmoid':\n        non_linearity = 'SIGMOID'\n    else:\n        raise TypeError('Unknown activation type %s' % mx_non_linearity)\n    builder.add_activation(name = name,\n                           non_linearity = non_linearity,\n                           input_name = input_name,\n                           output_name = output_name)", "output": "Convert an activation layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    network: net\n        A mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        An module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_hash():\n    '''\n    \n    '''\n    # Using bitmask to emulate rollover behavior of C unsigned 32 bit int\n    bitmask = 0xffffffff\n    h = 0\n\n    for i in bytearray(salt.utils.stringutils.to_bytes(__grains__['id'])):\n        h = (h + i) & bitmask\n        h = (h + (h << 10)) & bitmask\n        h = (h ^ (h >> 6)) & bitmask\n\n    h = (h + (h << 3)) & bitmask\n    h = (h ^ (h >> 11)) & bitmask\n    h = (h + (h << 15)) & bitmask\n\n    return (h & (_HASH_SIZE - 1)) & bitmask", "output": "Jenkins One-At-A-Time Hash Function\n    More Info: http://en.wikipedia.org/wiki/Jenkins_hash_function#one-at-a-time", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def entity_from_dict(self, entity_dict):\n        \"\"\"\n        \"\"\"\n        entity = entity_dict[\"entity\"]\n        role = entity_dict[\"role\"]\n\n        if entity == \"allUsers\":\n            entity = self.all()\n\n        elif entity == \"allAuthenticatedUsers\":\n            entity = self.all_authenticated()\n\n        elif \"-\" in entity:\n            entity_type, identifier = entity.split(\"-\", 1)\n            entity = self.entity(entity_type=entity_type, identifier=identifier)\n\n        if not isinstance(entity, _ACLEntity):\n            raise ValueError(\"Invalid dictionary: %s\" % entity_dict)\n\n        entity.grant(role)\n        return entity", "output": "Build an _ACLEntity object from a dictionary of data.\n\n        An entity is a mutable object that represents a list of roles\n        belonging to either a user or group or the special types for all\n        users and all authenticated users.\n\n        :type entity_dict: dict\n        :param entity_dict: Dictionary full of data from an ACL lookup.\n\n        :rtype: :class:`_ACLEntity`\n        :returns: An Entity constructed from the dictionary.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_namespaced_ingress(self, name, namespace, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_namespaced_ingress_with_http_info(name, namespace, body, **kwargs)\n        else:\n            (data) = self.replace_namespaced_ingress_with_http_info(name, namespace, body, **kwargs)\n            return data", "output": "replace the specified Ingress\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_namespaced_ingress(name, namespace, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Ingress (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param NetworkingV1beta1Ingress body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: NetworkingV1beta1Ingress\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_get_macroindex_list(ip=None, port=None):\n    \"\"\"\n\n\n    \"\"\"\n    global extension_market_list\n    extension_market_list = QA_fetch_get_extensionmarket_list(\n    ) if extension_market_list is None else extension_market_list\n\n    return extension_market_list.query('market==38')", "output": "\u5b8f\u89c2\u6307\u6807\u5217\u8868\n\n    Keyword Arguments:\n        ip {[type]} -- [description] (default: {None})\n        port {[type]} -- [description] (default: {None})\n\n        38        10      \u5b8f\u89c2\u6307\u6807         HG", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _crop_pad_default(x, size, padding_mode='reflection', row_pct:uniform = 0.5, col_pct:uniform = 0.5):\n    \"\"\n    padding_mode = _pad_mode_convert[padding_mode]\n    size = tis2hw(size)\n    if x.shape[1:] == torch.Size(size): return x\n    rows,cols = size\n    row_pct,col_pct = _minus_epsilon(row_pct,col_pct)\n    if x.size(1)<rows or x.size(2)<cols:\n        row_pad = max((rows-x.size(1)+1)//2, 0)\n        col_pad = max((cols-x.size(2)+1)//2, 0)\n        x = F.pad(x[None], (col_pad,col_pad,row_pad,row_pad), mode=padding_mode)[0]\n    row = int((x.size(1)-rows+1)*row_pct)\n    col = int((x.size(2)-cols+1)*col_pct)\n    x = x[:, row:row+rows, col:col+cols]\n    return x.contiguous()", "output": "Crop and pad tfm - `row_pct`,`col_pct` sets focal point.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save(self, config=None):\n        \"\"\"\n        \"\"\"\n        if config is not None:\n            clist = [config]\n        else:\n            clist = [\n                self._system_config,\n                self._global_config,\n                self._repo_config,\n                self._local_config,\n            ]\n\n        for conf in clist:\n            if conf.filename is None:\n                continue\n\n            try:\n                logger.debug(\"Writing '{}'.\".format(conf.filename))\n                dname = os.path.dirname(os.path.abspath(conf.filename))\n                try:\n                    os.makedirs(dname)\n                except OSError as exc:\n                    if exc.errno != errno.EEXIST:\n                        raise\n                conf.write()\n            except Exception as exc:\n                msg = \"failed to write config '{}'\".format(conf.filename)\n                raise ConfigError(msg, exc)", "output": "Saves config to config files.\n\n        Args:\n            config (configobj.ConfigObj): optional config object to save.\n\n        Raises:\n            dvc.config.ConfigError: thrown if failed to write config file.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def winsorize(row, min_percentile, max_percentile):\n    \"\"\"\n    \n    \"\"\"\n    a = row.copy()\n    nan_count = isnan(row).sum()\n    nonnan_count = a.size - nan_count\n\n    # NOTE: argsort() sorts nans to the end of the array.\n    idx = a.argsort()\n\n    # Set values at indices below the min percentile to the value of the entry\n    # at the cutoff.\n    if min_percentile > 0:\n        lower_cutoff = int(min_percentile * nonnan_count)\n        a[idx[:lower_cutoff]] = a[idx[lower_cutoff]]\n\n    # Set values at indices above the max percentile to the value of the entry\n    # at the cutoff.\n    if max_percentile < 1:\n        upper_cutoff = int(ceil(nonnan_count * max_percentile))\n        # if max_percentile is close to 1, then upper_cutoff might not\n        # remove any values.\n        if upper_cutoff < nonnan_count:\n            start_of_nans = (-nan_count) if nan_count else None\n            a[idx[upper_cutoff:start_of_nans]] = a[idx[upper_cutoff - 1]]\n\n    return a", "output": "This implementation is based on scipy.stats.mstats.winsorize", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_pillar_minions(self, expr, delimiter, greedy):\n        '''\n        \n        '''\n        return self._check_cache_minions(expr, delimiter, greedy, 'pillar')", "output": "Return the minions found by looking via pillar", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def copy2(src, dst):\n    \"\"\"\n\n    \"\"\"\n    if os.path.isdir(dst):\n        dst = os.path.join(dst, os.path.basename(src))\n    copyfile(src, dst)\n    copystat(src, dst)", "output": "Copy data and all stat info (\"cp -p src dst\").\n\n    The destination may be a directory.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset(self):\n        \"\"\"\n        \"\"\"\n        info = self._env.reset()[self.brain_name]\n        n_agents = len(info.agents)\n        self._check_agents(n_agents)\n        self.game_over = False\n\n        if not self._multiagent:\n            obs, reward, done, info = self._single_step(info)\n        else:\n            obs, reward, done, info = self._multi_step(info)\n        return obs", "output": "Resets the state of the environment and returns an initial observation.\n        In the case of multi-agent environments, this is a list.\n        Returns: observation (object/list): the initial observation of the\n            space.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _ConsumeInteger(tokenizer, is_signed=False, is_long=False):\n  \"\"\"\n  \"\"\"\n  try:\n    result = ParseInteger(tokenizer.token, is_signed=is_signed, is_long=is_long)\n  except ValueError as e:\n    raise tokenizer.ParseError(str(e))\n  tokenizer.NextToken()\n  return result", "output": "Consumes an integer number from tokenizer.\n\n  Args:\n    tokenizer: A tokenizer used to parse the number.\n    is_signed: True if a signed integer must be parsed.\n    is_long: True if a long integer must be parsed.\n\n  Returns:\n    The integer parsed.\n\n  Raises:\n    ParseError: If an integer with given characteristics couldn't be consumed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def eval_valid(self, feval=None):\n        \"\"\"\n        \"\"\"\n        return [item for i in range_(1, self.__num_dataset)\n                for item in self.__inner_eval(self.name_valid_sets[i - 1], i, feval)]", "output": "Evaluate for validation data.\n\n        Parameters\n        ----------\n        feval : callable or None, optional (default=None)\n            Customized evaluation function.\n            Should accept two parameters: preds, train_data,\n            and return (eval_name, eval_result, is_higher_better) or list of such tuples.\n            For multi-class task, the preds is group by class_id first, then group by row_id.\n            If you want to get i-th row preds in j-th class, the access way is preds[j * num_data + i].\n\n        Returns\n        -------\n        result : list\n            List with evaluation results.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def schema(self):\n        \"\"\"\n        \"\"\"\n        prop = self._properties.get(\"schema\", {})\n        return [SchemaField.from_api_repr(field) for field in prop.get(\"fields\", [])]", "output": "List[:class:`~google.cloud.bigquery.schema.SchemaField`]: The schema\n        for the data.\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs#configuration.query.tableDefinitions.(key).schema\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/tables#externalDataConfiguration.schema", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_banks_to_remove(redis_server, bank, path=''):\n    '''\n    \n    '''\n    current_path = bank if not path else '{path}/{bank}'.format(path=path, bank=bank)\n    bank_paths_to_remove = [current_path]\n    # as you got here, you'll be removed\n\n    bank_key = _get_bank_redis_key(current_path)\n    child_banks = redis_server.smembers(bank_key)\n    if not child_banks:\n        return bank_paths_to_remove  # this bank does not have any child banks so we stop here\n    for child_bank in child_banks:\n        bank_paths_to_remove.extend(_get_banks_to_remove(redis_server, child_bank, path=current_path))\n        # go one more level deeper\n        # and also remove the children of this child bank (if any)\n    return bank_paths_to_remove", "output": "A simple tree tarversal algorithm that builds the list of banks to remove,\n    starting from an arbitrary node in the tree.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def write_pid_to_pidfile(pidfile_path):\n    \"\"\" \n\n        \"\"\"\n    open_flags = (os.O_CREAT | os.O_EXCL | os.O_WRONLY)\n    open_mode = 0o644\n    pidfile_fd = os.open(pidfile_path, open_flags, open_mode)\n    pidfile = os.fdopen(pidfile_fd, 'w')\n\n    # According to the FHS 2.3 section on PID files in /var/run:\n    #\n    #   The file must consist of the process identifier in\n    #   ASCII-encoded decimal, followed by a newline character. For\n    #   example, if crond was process number 25, /var/run/crond.pid\n    #   would contain three characters: two, five, and newline.\n\n    pid = os.getpid()\n    pidfile.write(\"%s\\n\" % pid)\n    pidfile.close()", "output": "Write the PID in the named PID file.\n\n        Get the numeric process ID (\u201cPID\u201d) of the current process\n        and write it to the named file as a line of text.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def map_across_blocks(self, map_func):\n        \"\"\"\n        \"\"\"\n        preprocessed_map_func = self.preprocess_func(map_func)\n        new_partitions = np.array(\n            [\n                [part.apply(preprocessed_map_func) for part in row_of_parts]\n                for row_of_parts in self.partitions\n            ]\n        )\n        return self.__constructor__(new_partitions)", "output": "Applies `map_func` to every partition.\n\n        Args:\n            map_func: The function to apply.\n\n        Returns:\n            A new BaseFrameManager object, the type of object that called this.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mnist_common_generator(tmp_dir,\n                           training,\n                           how_many,\n                           data_filename,\n                           label_filename,\n                           start_from=0):\n  \"\"\"\n  \"\"\"\n  data_path = os.path.join(tmp_dir, data_filename)\n  labels_path = os.path.join(tmp_dir, label_filename)\n  images = _extract_mnist_images(data_path, 60000 if training else 10000)\n  labels = _extract_mnist_labels(labels_path, 60000 if training else 10000)\n  # Shuffle the data to make sure classes are well distributed.\n  data = list(zip(images, labels))\n  random.shuffle(data)\n  images, labels = list(zip(*data))\n  return image_utils.image_generator(images[start_from:start_from + how_many],\n                                     labels[start_from:start_from + how_many])", "output": "Image generator for MNIST.\n\n  Args:\n    tmp_dir: path to temporary storage directory.\n    training: a Boolean; if true, we use the train set, otherwise the test set.\n    how_many: how many images and labels to generate.\n    data_filename: file that contains features data.\n    label_filename: file that contains labels.\n    start_from: from which image to start.\n\n  Returns:\n    An instance of image_generator that produces MNIST images.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _create_resource(resource, name=None, tags=None, region=None, key=None,\n                     keyid=None, profile=None, **kwargs):\n    '''\n    \n    '''\n\n    try:\n        try:\n            conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n            create_resource = getattr(conn, 'create_' + resource)\n        except AttributeError:\n            raise AttributeError('{0} function does not exist for boto VPC '\n                                 'connection.'.format('create_' + resource))\n\n        if name and _get_resource_id(resource, name, region=region, key=key,\n                                     keyid=keyid, profile=profile):\n            return {'created': False, 'error': {'message':\n                    'A {0} named {1} already exists.'.format(\n                        resource, name)}}\n\n        r = create_resource(**kwargs)\n\n        if r:\n            if isinstance(r, bool):\n                return {'created': True}\n            else:\n                log.info('A %s with id %s was created', resource, r.id)\n                _maybe_set_name_tag(name, r)\n                _maybe_set_tags(tags, r)\n\n                if name:\n                    _cache_id(name,\n                              sub_resource=resource,\n                              resource_id=r.id,\n                              region=region,\n                              key=key, keyid=keyid,\n                              profile=profile)\n                return {'created': True, 'id': r.id}\n        else:\n            if name:\n                e = '{0} {1} was not created.'.format(resource, name)\n            else:\n                e = '{0} was not created.'.format(resource)\n            log.warning(e)\n            return {'created': False, 'error': {'message': e}}\n    except BotoServerError as e:\n        return {'created': False, 'error': __utils__['boto.get_error'](e)}", "output": "Create a VPC resource. Returns the resource id if created, or False\n    if not created.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def conv_bn_lrelu(ni:int, nf:int, ks:int=3, stride:int=1)->nn.Sequential:\n    \"\"\n    return nn.Sequential(\n        nn.Conv2d(ni, nf, kernel_size=ks, bias=False, stride=stride, padding=ks//2),\n        nn.BatchNorm2d(nf),\n        nn.LeakyReLU(negative_slope=0.1, inplace=True))", "output": "Create a seuence Conv2d->BatchNorm2d->LeakyReLu layer.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def restore(self):\n        \"\"\"\"\"\"\n        signal.signal(signal.SIGINT, self.original_sigint)\n        signal.signal(signal.SIGTERM, self.original_sigterm)\n        if os.name == 'nt':\n            signal.signal(signal.SIGBREAK, self.original_sigbreak)", "output": "Restore signal handlers to their original settings.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def response_class(self, cls):\n        \"\"\"\n        \n        \"\"\"\n        s = self._clone()\n        s._response_class = cls\n        return s", "output": "Override the default wrapper used for the response.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update_employee(emp_id, key=None, value=None, items=None):\n    '''\n    \n    '''\n    if items is None:\n        if key is None or value is None:\n            return {'Error': 'At least one key/value pair is required'}\n        items = {key: value}\n    elif isinstance(items, six.string_types):\n        items = salt.utils.yaml.safe_load(items)\n\n    xml_items = ''\n    for pair in items:\n        xml_items += '<field id=\"{0}\">{1}</field>'.format(pair, items[pair])\n    xml_items = '<employee>{0}</employee>'.format(xml_items)\n\n    status, result = _query(\n        action='employees',\n        command=emp_id,\n        data=xml_items,\n        method='POST',\n    )\n\n    return show_employee(emp_id, ','.join(items.keys()))", "output": "Update one or more items for this employee. Specifying an empty value will\n    clear it for that employee.\n\n    CLI Examples:\n\n        salt myminion bamboohr.update_employee 1138 nickname Curly\n        salt myminion bamboohr.update_employee 1138 nickname ''\n        salt myminion bamboohr.update_employee 1138 items='{\"nickname\": \"Curly\"}\n        salt myminion bamboohr.update_employee 1138 items='{\"nickname\": \"\"}", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _dispatch(name, *args, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n\n        def outer(self, *args, **kwargs):\n            def f(x):\n                x = self._shallow_copy(x, groupby=self._groupby)\n                return getattr(x, name)(*args, **kwargs)\n            return self._groupby.apply(f)\n        outer.__name__ = name\n        return outer", "output": "Dispatch to apply.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _bytes_to_unicode(value):\n    \"\"\"\n    \"\"\"\n    result = value.decode(\"utf-8\") if isinstance(value, six.binary_type) else value\n    if isinstance(result, six.text_type):\n        return result\n    else:\n        raise ValueError(\"%r could not be converted to unicode\" % (value,))", "output": "Converts bytes to a unicode value, if necessary.\n\n    :type value: bytes\n    :param value: bytes value to attempt string conversion on.\n\n    :rtype: str\n    :returns: The original value converted to unicode (if bytes) or as passed\n              in if it started out as unicode.\n\n    :raises ValueError: if the value could not be converted to unicode.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ext_pillar(minion_id,\n               pillar,\n               *args,\n               **kwargs):\n    '''\n    \n    '''\n    return SQLite3ExtPillar().fetch(minion_id, pillar, *args, **kwargs)", "output": "Execute queries against SQLite3, merge and return as a dict", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _parse_resource_reference(cls, ref_value):\n        \"\"\"\n        \n\n        \"\"\"\n        no_result = (None, None)\n\n        if not isinstance(ref_value, string_types):\n            return no_result\n\n        splits = ref_value.split(cls._resource_ref_separator, 1)\n\n        # Either there is no 'dot' (or) one of the values is empty string (Ex: when you split \"LogicalId.\")\n        if len(splits) != 2 or not all(splits):\n            return no_result\n\n        return splits[0], splits[1]", "output": "Splits a resource reference of structure \"LogicalId.Property\" and returns the \"LogicalId\" and \"Property\"\n        separately.\n\n        :param string ref_value: Input reference value which *may* contain the structure \"LogicalId.Property\"\n        :return string, string: Returns two values - logical_id, property. If the input does not contain the structure,\n            then both `logical_id` and property will be None", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def main(args):\n  \"\"\"\n  \n  \"\"\"\n  print_in_box('Validating submission ' + args.submission_filename)\n  random.seed()\n  temp_dir = args.temp_dir\n  delete_temp_dir = False\n  if not temp_dir:\n    temp_dir = tempfile.mkdtemp()\n    logging.info('Created temporary directory: %s', temp_dir)\n    delete_temp_dir = True\n  validator = submission_validator_lib.SubmissionValidator(temp_dir,\n                                                           args.use_gpu)\n  if validator.validate_submission(args.submission_filename,\n                                   args.submission_type):\n    print_in_box('Submission is VALID!')\n  else:\n    print_in_box('Submission is INVALID, see log messages for details')\n  if delete_temp_dir:\n    logging.info('Deleting temporary directory: %s', temp_dir)\n    subprocess.call(['rm', '-rf', temp_dir])", "output": "Validates the submission.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def add_aux_param(self, param_name, layer_index, blob_index):\n        \"\"\" \"\"\"\n        self.add_param('aux:%s' % param_name, layer_index, blob_index)", "output": "Add an aux param to .params file. Example: moving_mean in BatchNorm layer", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def prepare_defenses(self):\n    \"\"\"\"\"\"\n    print_header('PREPARING DEFENSE DATA')\n    # verify that defense data not written yet\n    if not self.ask_when_work_is_populated(self.defense_work):\n      return\n    self.defense_work = eval_lib.DefenseWorkPieces(\n        datastore_client=self.datastore_client)\n    # load results of attacks\n    self.submissions.init_from_datastore()\n    self.dataset_batches.init_from_datastore()\n    self.adv_batches.init_from_datastore()\n    self.attack_work.read_all_from_datastore()\n    # populate classification results\n    print_header('Initializing classification batches')\n    self.class_batches.init_from_adversarial_batches_write_to_datastore(\n        self.submissions, self.adv_batches)\n    if self.verbose:\n      print(self.class_batches)\n    # populate work pieces\n    print_header('Preparing defense work pieces')\n    self.defense_work.init_from_class_batches(\n        self.class_batches.data, num_shards=self.num_defense_shards)\n    self.defense_work.write_all_to_datastore()\n    if self.verbose:\n      print(self.defense_work)", "output": "Prepares all data needed for evaluation of defenses.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setup_menu(self):\r\n        \"\"\"\"\"\"\r\n        self.copy_action = create_action(self, _('Copy'),\r\n                                         shortcut=keybinding('Copy'),\r\n                                         icon=ima.icon('editcopy'),\r\n                                         triggered=self.copy,\r\n                                         context=Qt.WidgetShortcut)\r\n        menu = QMenu(self)\r\n        add_actions(menu, [self.copy_action, ])\r\n        return menu", "output": "Setup context menu", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ssh_usernames(vm_, opts, default_users=None):\n    '''\n    \n    '''\n    if default_users is None:\n        default_users = ['root']\n\n    usernames = salt.config.get_cloud_config_value(\n        'ssh_username', vm_, opts\n    )\n\n    if not isinstance(usernames, list):\n        usernames = [usernames]\n\n    # get rid of None's or empty names\n    usernames = [x for x in usernames if x]\n    # Keep a copy of the usernames the user might have provided\n    initial = usernames[:]\n\n    # Add common usernames to the list to be tested\n    for name in default_users:\n        if name not in usernames:\n            usernames.append(name)\n    # Add the user provided usernames to the end of the list since enough time\n    # might need to pass before the remote service is available for logins and\n    # the proper username might have passed its iteration.\n    # This has detected in a CentOS 5.7 EC2 image\n    usernames.extend(initial)\n    return usernames", "output": "Return the ssh_usernames. Defaults to a built-in list of users for trying.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_weight(self):\n        \"\"\"\n        \"\"\"\n        if self.weight is None:\n            self.weight = self.get_field('weight')\n        return self.weight", "output": "Get the weight of the Dataset.\n\n        Returns\n        -------\n        weight : numpy array or None\n            Weight for each data point from the Dataset.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def regexpExec(self, content):\n        \"\"\" \"\"\"\n        ret = libxml2mod.xmlRegexpExec(self._o, content)\n        return ret", "output": "Check if the regular expression generates the value", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def coalesce(self, numPartitions, shuffle=False):\n        \"\"\"\n        \n        \"\"\"\n        if shuffle:\n            # Decrease the batch size in order to distribute evenly the elements across output\n            # partitions. Otherwise, repartition will possibly produce highly skewed partitions.\n            batchSize = min(10, self.ctx._batchSize or 1024)\n            ser = BatchedSerializer(PickleSerializer(), batchSize)\n            selfCopy = self._reserialize(ser)\n            jrdd_deserializer = selfCopy._jrdd_deserializer\n            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)\n        else:\n            jrdd_deserializer = self._jrdd_deserializer\n            jrdd = self._jrdd.coalesce(numPartitions, shuffle)\n        return RDD(jrdd, self.ctx, jrdd_deserializer)", "output": "Return a new RDD that is reduced into `numPartitions` partitions.\n\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n        [[1], [2, 3], [4, 5]]\n        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n        [[1, 2, 3, 4, 5]]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get(self, end_ix):\n        \"\"\"\n        \n        \"\"\"\n        if self.most_recent_ix == end_ix:\n            return self.current\n\n        target = end_ix - self.cal_start - self.offset + 1\n        self.current = self.window.seek(target)\n\n        self.most_recent_ix = end_ix\n        return self.current", "output": "Returns\n        -------\n        out : A np.ndarray of the equity pricing up to end_ix after adjustments\n              and rounding have been applied.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def nac(x, depth, name=None, reuse=None):\n  \"\"\"\"\"\"\n  with tf.variable_scope(name, default_name=\"nac\", values=[x], reuse=reuse):\n    x_shape = shape_list(x)\n    w = tf.get_variable(\"w\", [x_shape[-1], depth])\n    m = tf.get_variable(\"m\", [x_shape[-1], depth])\n    w = tf.tanh(w) * tf.nn.sigmoid(m)\n    x_flat = tf.reshape(x, [-1, x_shape[-1]])\n    res_flat = tf.matmul(x_flat, w)\n    return tf.reshape(res_flat, x_shape[:-1] + [depth])", "output": "NAC as in https://arxiv.org/abs/1808.00508.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def aggregate_task_lm_losses(hparams,\n                             problem_hparams,\n                             logits,\n                             feature_name,\n                             feature):\n  \"\"\"\"\"\"\n  summaries = []\n  vocab_size = problem_hparams.vocab_size[feature_name]\n  if vocab_size is not None and hasattr(hparams, \"vocab_divisor\"):\n    vocab_size += (-vocab_size) % hparams.vocab_divisor\n  modality = problem_hparams.modality[feature_name]\n  loss = hparams.loss.get(feature_name, modalities.get_loss(modality))\n  weights_fn = hparams.weights_fn.get(\n      feature_name, modalities.get_weights_fn(modality))\n  loss_num = 0.\n  loss_den = 0.\n  for task in hparams.problem.task_list:\n    loss_num_, loss_den_ = loss(\n        logits, feature,\n        lambda x: common_layers.weights_multi_problem_all(x, task.task_id),  # pylint: disable=cell-var-from-loop\n        hparams, vocab_size, weights_fn)\n\n    loss_num += loss_num_\n    loss_den += loss_den_\n\n    loss_val = loss_num_ / tf.maximum(1.0, loss_den_)\n    summaries.append([task.name+\"_loss\", loss_val])\n\n  return loss_num, loss_den, summaries", "output": "LM loss for multiproblems.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def pad_to_multiple_2d(x, block_shape):\n  \"\"\"\n  \"\"\"\n  old_shape = x.get_shape().dims\n  last = old_shape[-1]\n  if len(old_shape) == 4:\n    height_padding = -common_layers.shape_list(x)[1] % block_shape[0]\n    width_padding = -common_layers.shape_list(x)[2] % block_shape[1]\n    paddings = [[0, 0], [0, height_padding], [0, width_padding], [0, 0]]\n  elif len(old_shape) == 5:\n    height_padding = -common_layers.shape_list(x)[2] % block_shape[0]\n    width_padding = -common_layers.shape_list(x)[3] % block_shape[1]\n    paddings = [[0, 0], [0, 0], [0, height_padding], [0, width_padding], [0, 0]]\n\n  padded_x = tf.pad(x, paddings)\n  padded_shape = padded_x.get_shape().as_list()\n  padded_shape = padded_shape[:-1] + [last]\n  padded_x.set_shape(padded_shape)\n  return padded_x", "output": "Making sure x is a multiple of shape.\n\n  Args:\n    x: a [batch, heads, h, w, depth] or [batch, h, w, depth] tensor\n    block_shape: a 2-d list of integer shapes\n\n  Returns:\n    padded_x: a [batch, heads, h, w, depth] or [batch, h, w, depth] tensor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def convert(model, feature_names = None, target = 'target', force_32bit_float = True):\n    \"\"\"\n    \n    \"\"\"\n    return _MLModel(_convert_tree_ensemble(model, feature_names, target, force_32bit_float = force_32bit_float))", "output": "Convert a trained XGBoost model to Core ML format.\n\n    Parameters\n    ----------\n    decision_tree : Booster\n        A trained XGboost tree model.\n\n    feature_names: [str] | str\n        Names of input features that will be exposed in the Core ML model\n        interface.\n\n        Can be set to one of the following:\n\n        - None for using the feature names from the model.\n        - List of names of the input features that should be exposed in the\n          interface to the Core ML model. These input features are in the same\n          order as the XGboost model.\n\n    target: str\n        Name of the output feature name exposed to the Core ML model.\n\n    force_32bit_float: bool\n        If True, then the resulting CoreML model will use 32 bit floats internally.\n\n    Returns\n    -------\n    model:MLModel\n        Returns an MLModel instance representing a Core ML model.\n\n    Examples\n    --------\n    .. sourcecode:: python\n\n\t\t# Convert it with default input and output names\n\t\t>>> import coremltools\n\t\t>>> coreml_model = coremltools.converters.xgboost.convert(model)\n\n\t\t# Saving the Core ML model to a file.\n\t\t>>> coremltools.save('my_model.mlmodel')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def make_simple_equity_info(sids,\n                            start_date,\n                            end_date,\n                            symbols=None,\n                            names=None,\n                            exchange='TEST'):\n    \"\"\"\n    \n    \"\"\"\n    num_assets = len(sids)\n    if symbols is None:\n        symbols = list(ascii_uppercase[:num_assets])\n    else:\n        symbols = list(symbols)\n\n    if names is None:\n        names = [str(s) + \" INC.\" for s in symbols]\n\n    return pd.DataFrame(\n        {\n            'symbol': symbols,\n            'start_date': pd.to_datetime([start_date] * num_assets),\n            'end_date': pd.to_datetime([end_date] * num_assets),\n            'asset_name': list(names),\n            'exchange': exchange,\n        },\n        index=sids,\n        columns=(\n            'start_date',\n            'end_date',\n            'symbol',\n            'exchange',\n            'asset_name',\n        ),\n    )", "output": "Create a DataFrame representing assets that exist for the full duration\n    between `start_date` and `end_date`.\n\n    Parameters\n    ----------\n    sids : array-like of int\n    start_date : pd.Timestamp, optional\n    end_date : pd.Timestamp, optional\n    symbols : list, optional\n        Symbols to use for the assets.\n        If not provided, symbols are generated from the sequence 'A', 'B', ...\n    names : list, optional\n        Names to use for the assets.\n        If not provided, names are generated by adding \" INC.\" to each of the\n        symbols (which might also be auto-generated).\n    exchange : str, optional\n        The exchange name.\n\n    Returns\n    -------\n    info : pd.DataFrame\n        DataFrame representing newly-created assets.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_data(self, filename=None):\r\n        \"\"\"\"\"\"\r\n        if filename is None:\r\n            filename = self.filename\r\n            if filename is None:\r\n                filename = getcwd_or_home()\r\n            filename, _selfilter = getsavefilename(self, _(\"\"),\r\n                                                   filename,\r\n                                                   iofunctions.save_filters)\r\n            if filename:\r\n                self.filename = filename\r\n            else:\r\n                return False\r\n        QApplication.setOverrideCursor(QCursor(Qt.WaitCursor))\r\n        QApplication.processEvents()\r\n\r\n        error_message = self.shellwidget.save_namespace(self.filename)\r\n        self.shellwidget._kernel_reply = None\r\n\r\n        QApplication.restoreOverrideCursor()\r\n        QApplication.processEvents()\r\n        if error_message is not None:\r\n            if 'Some objects could not be saved:' in error_message:\r\n                save_data_message = (\r\n                    _('<b>Some objects could not be saved:</b>')\r\n                    + '<br><br><code>{obj_list}</code>'.format(\r\n                        obj_list=error_message.split(': ')[1]))\r\n            else:\r\n                save_data_message = _(\r\n                    '<b>Unable to save current workspace</b>'\r\n                    '<br><br>Error message:<br>') + error_message\r\n            QMessageBox.critical(self, _(\"\"), save_data_message)\r\n        self.save_button.setEnabled(self.filename is not None)", "output": "Save data", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_cond_latents(cond_latents, hparams):\n  \"\"\"\"\"\"\n  if cond_latents is None:\n    return\n  if not isinstance(cond_latents[0], list):\n    cond_latents = [cond_latents]\n  exp_num_latents = hparams.num_cond_latents\n  if hparams.latent_dist_encoder == \"conv_net\":\n    exp_num_latents += int(hparams.cond_first_frame)\n  if len(cond_latents) != exp_num_latents:\n    raise ValueError(\"Expected number of cond_latents: %d, got %d\" %\n                     (exp_num_latents, len(cond_latents)))\n  for cond_latent in cond_latents:\n    if len(cond_latent) != hparams.n_levels - 1:\n      raise ValueError(\"Expected level_latents to be %d, got %d\" %\n                       (hparams.n_levels - 1, len(cond_latent)))", "output": "Shape checking for cond_latents.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def highlight_max(self, subset=None, color='yellow', axis=0):\n        \"\"\"\n        \n        \"\"\"\n        return self._highlight_handler(subset=subset, color=color, axis=axis,\n                                       max_=True)", "output": "Highlight the maximum by shading the background.\n\n        Parameters\n        ----------\n        subset : IndexSlice, default None\n            a valid slice for ``data`` to limit the style application to.\n        color : str, default 'yellow'\n        axis : {0 or 'index', 1 or 'columns', None}, default 0\n            apply to each column (``axis=0`` or ``'index'``), to each row\n            (``axis=1`` or ``'columns'``), or to the entire DataFrame at once\n            with ``axis=None``.\n\n        Returns\n        -------\n        self : Styler", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snapshots_create(container, name=None, remote_addr=None,\n                     cert=None, key=None, verify_cert=True):\n    '''\n    \n    '''\n    cont = container_get(\n        container, remote_addr, cert, key, verify_cert, _raw=True\n    )\n    if not name:\n        name = datetime.now().strftime('%Y%m%d%H%M%S')\n\n    cont.snapshots.create(name)\n\n    for c in snapshots_all(container).get(container):\n        if c.get('name') == name:\n            return {'name': name}\n\n    return {'name': False}", "output": "Create a snapshot for a container\n\n    container :\n        The name of the container to get.\n\n    name :\n        The name of the snapshot.\n\n    remote_addr :\n        An URL to a remote server. The 'cert' and 'key' fields must also be\n        provided if 'remote_addr' is defined.\n\n        Examples:\n            https://myserver.lan:8443\n            /var/lib/mysocket.sock\n\n    cert :\n        PEM Formatted SSL Certificate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Verify the ssl certificate.  Default: True\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        $ salt '*' lxd.snapshots_create test-container test-snapshot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_fun(fun):\n    '''\n    \n    '''\n    conn, mdb = _get_conn(ret=None)\n    ret = {}\n    rdata = mdb.saltReturns.find_one({'fun': fun}, {'_id': 0})\n    if rdata:\n        ret = rdata\n    return ret", "output": "Return the most recent jobs that have executed the named function", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_minions():\n    '''\n    \n    '''\n    query = '''SELECT DISTINCT minion_id\n               FROM {keyspace}.minions;'''.format(keyspace=_get_keyspace())\n\n    ret = []\n\n    # cassandra_cql.cql_query may raise a CommandExecutionError\n    try:\n        data = __salt__['cassandra_cql.cql_query'](query)\n        if data:\n            for row in data:\n                minion = row.get('minion_id')\n                if minion:\n                    ret.append(minion)\n    except CommandExecutionError:\n        log.critical('Could not get the list of minions.')\n        raise\n    except Exception as e:\n        log.critical(\n            'Unexpected error while getting list of minions: %s', e)\n        raise\n\n    return ret", "output": "Return a list of minions", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def print_row(self, **kwargs):\n        '''\n        \n        '''\n        meta_string = '|'\n        for key in self.column_names:\n            float_specifier = ''\n            if isinstance(kwargs[key], float):\n                float_specifier = '.3f'\n            meta_string += \" {%s:<{width}%s}|\" % (key, float_specifier)\n        kwargs['width'] = self.column_width - 1\n\n        print(meta_string.format(**kwargs))\n        print(self.hr)", "output": "keys of kwargs must be the names passed to __init__(...) as `column_names`", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_load(jid, load, minions=None):\n    '''\n    \n    '''\n    serv = _get_serv(ret=None)\n\n    # create legacy request in case an InfluxDB 0.8.x version is used\n    if \"influxdb08\" in serv.__module__:\n        req = [\n            {\n                'name': 'jids',\n                'columns': ['jid', 'load'],\n                'points': [\n                    [jid, salt.utils.json.dumps(load)]\n                ],\n            }\n        ]\n    # create InfluxDB 0.9+ version request\n    else:\n        req = [\n            {\n                'measurement': 'jids',\n                'tags': {\n                    'jid': jid\n                },\n                'fields': {\n                    'load': salt.utils.json.dumps(load)\n                }\n            }\n        ]\n\n    try:\n        serv.write_points(req)\n    except Exception as ex:\n        log.critical('Failed to store load with InfluxDB returner: %s', ex)", "output": "Save the load to the specified jid", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _encrypt_private(self, ret, dictkey, target):\n        '''\n        \n        '''\n        # encrypt with a specific AES key\n        pubfn = os.path.join(self.opts['pki_dir'],\n                             'minions',\n                             target)\n        key = salt.crypt.Crypticle.generate_key_string()\n        pcrypt = salt.crypt.Crypticle(\n            self.opts,\n            key)\n        try:\n            pub = salt.crypt.get_rsa_pub_key(pubfn)\n        except (ValueError, IndexError, TypeError):\n            return self.crypticle.dumps({})\n        except IOError:\n            log.error('AES key not found')\n            return {'error': 'AES key not found'}\n\n        pret = {}\n        if not six.PY2:\n            key = salt.utils.stringutils.to_bytes(key)\n        if HAS_M2:\n            pret['key'] = pub.public_encrypt(key, RSA.pkcs1_oaep_padding)\n        else:\n            cipher = PKCS1_OAEP.new(pub)\n            pret['key'] = cipher.encrypt(key)\n        pret[dictkey] = pcrypt.dumps(\n            ret if ret is not False else {}\n        )\n        return pret", "output": "The server equivalent of ReqChannel.crypted_transfer_decode_dictentry", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def exit_interpreter(self):\r\n        \"\"\"\"\"\"\r\n        self.interpreter.exit_flag = True\r\n        if self.multithreaded:\r\n            self.interpreter.stdin_write.write(to_binary_string('\\n'))\r\n        self.interpreter.restore_stds()", "output": "Exit interpreter", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def row(*args, **kwargs):\n    \"\"\" \n    \"\"\"\n\n    sizing_mode = kwargs.pop('sizing_mode', None)\n    children = kwargs.pop('children', None)\n\n    children = _handle_children(*args, children=children)\n\n    row_children = []\n    for item in children:\n        if isinstance(item, LayoutDOM):\n            if sizing_mode is not None and _has_auto_sizing(item):\n                item.sizing_mode = sizing_mode\n            row_children.append(item)\n        else:\n            raise ValueError(\"\"\"Only LayoutDOM items can be inserted into a row. Tried to insert: %s of type %s\"\"\" % (item, type(item)))\n\n    return Row(children=row_children, sizing_mode=sizing_mode, **kwargs)", "output": "Create a row of Bokeh Layout objects. Forces all objects to\n    have the same sizing_mode, which is required for complex layouts to work.\n\n    Args:\n        children (list of :class:`~bokeh.models.layouts.LayoutDOM` ): A list of instances for\n            the row. Can be any of the following - :class:`~bokeh.models.plots.Plot`,\n            :class:`~bokeh.models.widgets.widget.Widget`,\n            :class:`~bokeh.models.layouts.Row`,\n            :class:`~bokeh.models.layouts.Column`,\n            :class:`~bokeh.models.tools.ToolbarBox`,\n            :class:`~bokeh.models.layouts.Spacer`.\n\n        sizing_mode (``\"fixed\"``, ``\"stretch_both\"``, ``\"scale_width\"``, ``\"scale_height\"``, ``\"scale_both\"`` ): How\n            will the items in the layout resize to fill the available space.\n            Default is ``\"fixed\"``. For more information on the different\n            modes see :attr:`~bokeh.models.layouts.LayoutDOM.sizing_mode`\n            description on :class:`~bokeh.models.layouts.LayoutDOM`.\n\n    Returns:\n        Row: A row of LayoutDOM objects all with the same sizing_mode.\n\n    Examples:\n\n        >>> row([plot_1, plot_2])\n        >>> row(children=[widget_box_1, plot_1], sizing_mode='stretch_both')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def snapshots_delete(container, name, remote_addr=None,\n                     cert=None, key=None, verify_cert=True):\n    '''\n    \n    '''\n    cont = container_get(\n        container, remote_addr, cert, key, verify_cert, _raw=True\n    )\n\n    try:\n        for s in cont.snapshots.all():\n            if s.name == name:\n                s.delete()\n                return True\n    except pylxd.exceptions.LXDAPIException:\n        pass\n\n    return False", "output": "Delete a snapshot for a container\n\n    container :\n        The name of the container to get.\n\n    name :\n        The name of the snapshot.\n\n    remote_addr :\n        An URL to a remote server. The 'cert' and 'key' fields must also be\n        provided if 'remote_addr' is defined.\n\n        Examples:\n            https://myserver.lan:8443\n            /var/lib/mysocket.sock\n\n    cert :\n        PEM Formatted SSL Certificate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Verify the ssl certificate.  Default: True\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        $ salt '*' lxd.snapshots_delete test-container test-snapshot", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def enable_plugin(name, runas=None):\n    '''\n    \n    '''\n    if runas is None and not salt.utils.platform.is_windows():\n        runas = salt.utils.user.get_user()\n    cmd = [_get_rabbitmq_plugin(), 'enable', name]\n    ret = __salt__['cmd.run_all'](cmd, reset_system_locale=False, runas=runas, python_shell=False)\n    return _format_response(ret, 'Enabled')", "output": "Enable a RabbitMQ plugin via the rabbitmq-plugins command.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rabbitmq.enable_plugin foo", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_code_tolist(code, auto_fill=True):\n    \"\"\"\n    \"\"\"\n\n    if isinstance(code, str):\n        if auto_fill:\n            return [QA_util_code_tostr(code)]\n        else:\n            return [code]\n\n    elif isinstance(code, list):\n        if auto_fill:\n            return [QA_util_code_tostr(item) for item in code]\n        else:\n            return [item for item in code]", "output": "\u8f6c\u6362code==> list\n\n    Arguments:\n        code {[type]} -- [description]\n\n    Keyword Arguments:\n        auto_fill {bool} -- \u662f\u5426\u81ea\u52a8\u8865\u5168(\u4e00\u822c\u662f\u7528\u4e8e\u80a1\u7968/\u6307\u6570/etf\u7b496\u4f4d\u6570,\u671f\u8d27\u4e0d\u9002\u7528) (default: {True})\n\n    Returns:\n        [list] -- [description]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def const_rand(size, seed=23980):\n    \"\"\" \n    \"\"\"\n    old_seed = np.random.seed()\n    np.random.seed(seed)\n    out = np.random.rand(size)\n    np.random.seed(old_seed)\n    return out", "output": "Generate a random array with a fixed seed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def replace_node_status(self, name, body, **kwargs):\n        \"\"\"\n        \n        \"\"\"\n        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.replace_node_status_with_http_info(name, body, **kwargs)\n        else:\n            (data) = self.replace_node_status_with_http_info(name, body, **kwargs)\n            return data", "output": "replace status of the specified Node\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.replace_node_status(name, body, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the Node (required)\n        :param V1Node body: (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n        :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n        :return: V1Node\n                 If the method is called asynchronously,\n                 returns the request thread.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def contextMenuEvent(self, event):\r\n        \"\"\"\"\"\"\r\n        self.menu.popup(event.globalPos())\r\n        event.accept()", "output": "Reimplement Qt method", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def slice_shift(self, periods=1, axis=0):\n        \"\"\"\n        \n        \"\"\"\n        if periods == 0:\n            return self\n\n        if periods > 0:\n            vslicer = slice(None, -periods)\n            islicer = slice(periods, None)\n        else:\n            vslicer = slice(-periods, None)\n            islicer = slice(None, periods)\n\n        new_obj = self._slice(vslicer, axis=axis)\n        shifted_axis = self._get_axis(axis)[islicer]\n        new_obj.set_axis(shifted_axis, axis=axis, inplace=True)\n\n        return new_obj.__finalize__(self)", "output": "Equivalent to `shift` without copying data. The shifted data will\n        not include the dropped periods and the shifted axis will be smaller\n        than the original.\n\n        Parameters\n        ----------\n        periods : int\n            Number of periods to move, can be positive or negative\n\n        Returns\n        -------\n        shifted : same type as caller\n\n        Notes\n        -----\n        While the `slice_shift` is faster than `shift`, you may pay for it\n        later during alignment.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def hostinterface_delete(interfaceids, **kwargs):\n    '''\n    \n    '''\n    conn_args = _login(**kwargs)\n    ret = {}\n    try:\n        if conn_args:\n            method = 'hostinterface.delete'\n            if isinstance(interfaceids, list):\n                params = interfaceids\n            else:\n                params = [interfaceids]\n            ret = _query(method, params, conn_args['url'], conn_args['auth'])\n            return ret['result']['interfaceids']\n        else:\n            raise KeyError\n    except KeyError:\n        return ret", "output": "Delete host interface\n\n    .. versionadded:: 2016.3.0\n\n    :param interfaceids: IDs of the host interfaces to delete\n    :param _connection_user: Optional - zabbix user (can also be set in opts or pillar, see module's docstring)\n    :param _connection_password: Optional - zabbix password (can also be set in opts or pillar, see module's docstring)\n    :param _connection_url: Optional - url of zabbix frontend (can also be set in opts, pillar, see module's docstring)\n\n    :return: ID of deleted host interfaces, False on failure.\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' zabbix.hostinterface_delete 50", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(self, other):\n        \"\"\"\"\"\"\n        if isinstance(other, cookielib.CookieJar):\n            for cookie in other:\n                self.set_cookie(copy.copy(cookie))\n        else:\n            super(RequestsCookieJar, self).update(other)", "output": "Updates this jar with cookies from another CookieJar or dict-like", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_load(jid):\n    '''\n    \n    '''\n    options = _get_options()\n\n    index = options['master_job_cache_index']\n    doc_type = options['master_job_cache_doc_type']\n\n    if options['index_date']:\n        index = '{0}-{1}'.format(index,\n            datetime.date.today().strftime('%Y.%m.%d'))\n\n    data = __salt__['elasticsearch.document_get'](index=index,\n                                                  id=jid,\n                                                  doc_type=doc_type)\n    if data:\n        # Use salt.utils.json.dumps to convert elasticsearch unicode json to standard json\n        return salt.utils.json.loads(salt.utils.json.dumps(data))\n    return {}", "output": "Return the load data that marks a specified jid\n\n    .. versionadded:: 2015.8.1", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _connect(user=None, password=None, host=None, port=None, database='admin', authdb=None):\n    '''\n    \n    '''\n    if not user:\n        user = __salt__['config.option']('mongodb.user')\n    if not password:\n        password = __salt__['config.option']('mongodb.password')\n    if not host:\n        host = __salt__['config.option']('mongodb.host')\n    if not port:\n        port = __salt__['config.option']('mongodb.port')\n    if not authdb:\n        authdb = database\n\n    try:\n        conn = pymongo.MongoClient(host=host, port=port)\n        mdb = pymongo.database.Database(conn, database)\n        if user and password:\n            mdb.authenticate(user, password, source=authdb)\n    except pymongo.errors.PyMongoError:\n        log.error('Error connecting to database %s', database)\n        return False\n\n    return conn", "output": "Returns a tuple of (user, host, port) with config, pillar, or default\n    values assigned to missing values.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def execute_script(self, script, *args):\n        \"\"\"\n        \n        \"\"\"\n        converted_args = list(args)\n        command = None\n        if self.w3c:\n            command = Command.W3C_EXECUTE_SCRIPT\n        else:\n            command = Command.EXECUTE_SCRIPT\n\n        return self.execute(command, {\n            'script': script,\n            'args': converted_args})['value']", "output": "Synchronously Executes JavaScript in the current window/frame.\n\n        :Args:\n         - script: The JavaScript to execute.\n         - \\\\*args: Any applicable arguments for your JavaScript.\n\n        :Usage:\n            ::\n\n                driver.execute_script('return document.title;')", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _raw_open(self, flags, mode=0o777):\n        \"\"\"\n        \n        \"\"\"\n        if self._closed:\n            self._raise_closed()\n        return self._accessor.open(self, flags, mode)", "output": "Open the file pointed by this path and return a file descriptor,\n        as os.open() does.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def reset_cooldown(self, ctx):\n        \"\"\"\n        \"\"\"\n        if self._buckets.valid:\n            bucket = self._buckets.get_bucket(ctx.message)\n            bucket.reset()", "output": "Resets the cooldown on this command.\n\n        Parameters\n        -----------\n        ctx: :class:`.Context`\n            The invocation context to reset the cooldown under.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def shutdown(self):\r\n        \"\"\"\"\"\"\r\n        if self.get_kernel() is not None and not self.slave:\r\n            self.shellwidget.kernel_manager.shutdown_kernel()\r\n        if self.shellwidget.kernel_client is not None:\r\n            background(self.shellwidget.kernel_client.stop_channels)", "output": "Shutdown kernel", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def grain_funcs(opts, proxy=None):\n    '''\n    \n    '''\n    ret = LazyLoader(\n        _module_dirs(\n            opts,\n            'grains',\n            'grain',\n            ext_type_dirs='grains_dirs',\n        ),\n        opts,\n        tag='grains',\n    )\n    ret.pack['__utils__'] = utils(opts, proxy=proxy)\n    return ret", "output": "Returns the grain functions\n\n      .. code-block:: python\n\n          import salt.config\n          import salt.loader\n\n          __opts__ = salt.config.minion_config('/etc/salt/minion')\n          grainfuncs = salt.loader.grain_funcs(__opts__)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_imported_modules(cells, nb_module_name=''):\n    \"\"\n    module_names = get_top_level_modules()\n    nb_imports = [match.group(1) for cell in cells for match in IMPORT_RE.finditer(cell['source']) if cell['cell_type'] == 'code']\n    parts = nb_module_name.split('.')\n    parent_modules = ['.'.join(parts[:(x+1)]) for x in range_of(parts)] # Imports parent modules - a.b.c = [a, a.b, a.b.c]\n    all_modules = module_names + nb_imports + parent_modules\n    mods = [import_mod(m, ignore_errors=True) for m in all_modules]\n    return [m for m in mods if m is not None]", "output": "Finds all submodules of notebook - sorted by submodules > top level modules > manual imports. This gives notebook imports priority", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_pending(self, tag, match_func=None):\n        \"\"\"\n        \"\"\"\n        if match_func is None:\n            match_func = self._get_match_func()\n        old_events = self.pending_events\n        self.pending_events = []\n        ret = None\n        for evt in old_events:\n            if match_func(evt['tag'], tag):\n                if ret is None:\n                    ret = evt\n                    log.trace('get_event() returning cached event = %s', ret)\n                else:\n                    self.pending_events.append(evt)\n            elif any(pmatch_func(evt['tag'], ptag) for ptag, pmatch_func in self.pending_tags):\n                self.pending_events.append(evt)\n            else:\n                log.trace('get_event() discarding cached event that no longer has any subscriptions = %s', evt)\n        return ret", "output": "Check the pending_events list for events that match the tag\n\n        :param tag: The tag to search for\n        :type tag: str\n        :param tags_regex: List of re expressions to search for also\n        :type tags_regex: list[re.compile()]\n        :return:", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def div(a, b):\n  \"\"\"\n  \n  \"\"\"\n  def divide(a, b):\n    \"\"\"Division\"\"\"\n    return a / b\n  return op_with_scalar_cast(a, b, divide)", "output": "A wrapper around tf division that does more automatic casting of\n  the input.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def start(name, call=None):\n    '''\n    \n    '''\n    if call != 'action':\n        raise SaltCloudException(\n            'The start action must be called with -a or --action.'\n        )\n\n    node_id = get_linode_id_from_name(name)\n    node = get_linode(kwargs={'linode_id': node_id})\n\n    if node['STATUS'] == 1:\n        return {'success': True,\n                'action': 'start',\n                'state': 'Running',\n                'msg': 'Machine already running'}\n\n    response = _query('linode', 'boot', args={'LinodeID': node_id})['DATA']\n\n    if _wait_for_job(node_id, response['JobID']):\n        return {'state': 'Running',\n                'action': 'start',\n                'success': True}\n    else:\n        return {'action': 'start',\n                'success': False}", "output": "Start a VM in Linode.\n\n    name\n        The name of the VM to start.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a stop vm_name", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def purge(name=None, slot=None, fromrepo=None, pkgs=None, **kwargs):\n    '''\n    \n    '''\n    ret = remove(name=name, slot=slot, fromrepo=fromrepo, pkgs=pkgs)\n    ret.update(depclean(name=name, slot=slot, fromrepo=fromrepo, pkgs=pkgs))\n    return ret", "output": ".. versionchanged:: 2015.8.12,2016.3.3,2016.11.0\n        On minions running systemd>=205, `systemd-run(1)`_ is now used to\n        isolate commands which modify installed packages from the\n        ``salt-minion`` daemon's control group. This is done to keep systemd\n        from killing any emerge commands spawned by Salt when the\n        ``salt-minion`` service is restarted. (see ``KillMode`` in the\n        `systemd.kill(5)`_ manpage for more information). If desired, usage of\n        `systemd-run(1)`_ can be suppressed by setting a :mod:`config option\n        <salt.modules.config.get>` called ``systemd.scope``, with a value of\n        ``False`` (no quotes).\n\n    .. _`systemd-run(1)`: https://www.freedesktop.org/software/systemd/man/systemd-run.html\n    .. _`systemd.kill(5)`: https://www.freedesktop.org/software/systemd/man/systemd.kill.html\n\n    Portage does not have a purge, this function calls remove followed\n    by depclean to emulate a purge process\n\n    name\n        The name of the package to be deleted.\n\n    slot\n        Restrict the remove to a specific slot. Ignored if name is None.\n\n    fromrepo\n        Restrict the remove to a specific slot. Ignored if ``name`` is None.\n\n    Multiple Package Options:\n\n    pkgs\n        Uninstall multiple packages. ``slot`` and ``fromrepo`` arguments are\n        ignored if this argument is present. Must be passed as a python list.\n\n    .. versionadded:: 0.16.0\n\n\n    Returns a dict containing the changes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pkg.purge <package name>\n        salt '*' pkg.purge <package name> slot=4.4\n        salt '*' pkg.purge <package1>,<package2>,<package3>\n        salt '*' pkg.purge pkgs='[\"foo\", \"bar\"]'", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lambda_not_found_response(*args):\n        \"\"\"\n        \n        \"\"\"\n        response_data = jsonify(ServiceErrorResponses._NO_LAMBDA_INTEGRATION)\n        return make_response(response_data, ServiceErrorResponses.HTTP_STATUS_CODE_502)", "output": "Constructs a Flask Response for when a Lambda function is not found for an endpoint\n\n        :return: a Flask Response", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def RelaxNGSetSchema(self, reader):\n        \"\"\" \"\"\"\n        if reader is None: reader__o = None\n        else: reader__o = reader._o\n        ret = libxml2mod.xmlTextReaderRelaxNGSetSchema(reader__o, self._o)\n        return ret", "output": "Use RelaxNG to validate the document as it is processed.\n          Activation is only possible before the first Read(). if\n          @schema is None, then RelaxNG validation is desactivated. @\n          The @schema should not be freed until the reader is\n           deallocated or its use has been deactivated.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def is_ubuntu():\n    \"\"\"\"\"\"\n    if sys.platform.startswith('linux') and osp.isfile('/etc/lsb-release'):\n        release_info = open('/etc/lsb-release').read()\n        if 'Ubuntu' in release_info:\n            return True\n        else:\n            return False\n    else:\n        return False", "output": "Detect if we are running in an Ubuntu-based distribution", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def not_allowed(subset=None, show_ip=False, show_ipv4=None):\n    '''\n    \n    '''\n    show_ip = _show_ip_migration(show_ip, show_ipv4)\n    return list_not_state(subset=subset, show_ip=show_ip)", "output": ".. versionadded:: 2015.8.0\n    .. versionchanged:: 2019.2.0\n        The 'show_ipv4' argument has been renamed to 'show_ip' as it now\n        includes IPv6 addresses for IPv6-connected minions.\n\n    Print a list of all minions that are NOT up according to Salt's presence\n    detection (no commands will be sent)\n\n    subset : None\n        Pass in a CIDR range to filter minions by IP address.\n\n    show_ip : False\n        Also show the IP address each minion is connecting from.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run manage.not_allowed", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def live_processes(self):\n        \"\"\"\n        \"\"\"\n        result = []\n        for process_type, process_infos in self.all_processes.items():\n            for process_info in process_infos:\n                if process_info.process.poll() is None:\n                    result.append((process_type, process_info.process))\n        return result", "output": "Return a list of the live processes.\n\n        Returns:\n            A list of the live processes.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    \"\"\"\n    \n    \"\"\"\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss", "output": "Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def human_to_bytes(size):\n    '''\n    \n    '''\n    sbytes = size[:-1]\n    unit = size[-1]\n    if sbytes.isdigit():\n        sbytes = int(sbytes)\n        if unit == 'P':\n            sbytes *= 1125899906842624\n        elif unit == 'T':\n            sbytes *= 1099511627776\n        elif unit == 'G':\n            sbytes *= 1073741824\n        elif unit == 'M':\n            sbytes *= 1048576\n        else:\n            sbytes = 0\n    else:\n        sbytes = 0\n    return sbytes", "output": "Given a human-readable byte string (e.g. 2G, 30M),\n    return the number of bytes.  Will return 0 if the argument has\n    unexpected form.\n\n    .. versionadded:: 2018.3.0", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def ls_(active=None, cache=True, path=None):\n    '''\n    \n    '''\n    contextvar = 'lxc.ls{0}'.format(path)\n    if active:\n        contextvar += '.active'\n    if cache and (contextvar in __context__):\n        return __context__[contextvar]\n    else:\n        ret = []\n        cmd = 'lxc-ls'\n        if path:\n            cmd += ' -P {0}'.format(pipes.quote(path))\n        if active:\n            cmd += ' --active'\n        output = __salt__['cmd.run_stdout'](cmd, python_shell=False)\n        for line in output.splitlines():\n            ret.extend(line.split())\n        __context__[contextvar] = ret\n        return ret", "output": "Return a list of the containers available on the minion\n\n    path\n        path to the container parent directory\n        default: /var/lib/lxc (system)\n\n        .. versionadded:: 2015.8.0\n\n    active\n        If ``True``, return only active (i.e. running) containers\n\n        .. versionadded:: 2015.5.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' lxc.ls\n        salt '*' lxc.ls active=True", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def register_trainable(name, trainable):\n    \"\"\"\n    \"\"\"\n\n    from ray.tune.trainable import Trainable\n    from ray.tune.function_runner import wrap_function\n\n    if isinstance(trainable, type):\n        logger.debug(\"Detected class for trainable.\")\n    elif isinstance(trainable, FunctionType):\n        logger.debug(\"Detected function for trainable.\")\n        trainable = wrap_function(trainable)\n    elif callable(trainable):\n        logger.warning(\n            \"Detected unknown callable for trainable. Converting to class.\")\n        trainable = wrap_function(trainable)\n\n    if not issubclass(trainable, Trainable):\n        raise TypeError(\"Second argument must be convertable to Trainable\",\n                        trainable)\n    _global_registry.register(TRAINABLE_CLASS, name, trainable)", "output": "Register a trainable function or class.\n\n    Args:\n        name (str): Name to register.\n        trainable (obj): Function or tune.Trainable class. Functions must\n            take (config, status_reporter) as arguments and will be\n            automatically converted into a class during registration.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def on_epoch_end(self, epoch, logs=None):\n        \"\"\"\n        \n        \"\"\"\n        if logs is None:\n            logs = dict()\n        logger.debug(logs)\n        nni.report_intermediate_result(logs[\"val_acc\"])", "output": "Run on end of each epoch", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_fetch_user(user_cookie, db=DATABASE):\n    \"\"\"\n    \n    \"\"\"\n    collection = DATABASE.account\n\n    return [res for res in collection.find({'user_cookie': user_cookie}, {\"_id\": 0})]", "output": "get the user\n\n    Arguments:\n        user_cookie : str the unique cookie_id for a user\n    Keyword Arguments:\n        db: database for query\n\n    Returns:\n        list ---  [ACCOUNT]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_logits(self, x, **kwargs):\n    \"\"\"\n    \n    \"\"\"\n    outputs = self.fprop(x, **kwargs)\n    if self.O_LOGITS in outputs:\n      return outputs[self.O_LOGITS]\n    raise NotImplementedError(str(type(self)) + \"must implement `get_logits`\"\n                              \" or must define a \" + self.O_LOGITS +\n                              \" output in `fprop`\")", "output": ":param x: A symbolic representation (Tensor) of the network input\n    :return: A symbolic representation (Tensor) of the output logits\n    (i.e., the values fed as inputs to the softmax layer).", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _is_intrinsic_dict(self, input):\n        \"\"\"\n        \n        \"\"\"\n        # All intrinsic functions are dictionaries with just one key\n        return isinstance(input, dict) \\\n            and len(input) == 1 \\\n            and list(input.keys())[0] in self.supported_intrinsics", "output": "Can the input represent an intrinsic function in it?\n\n        :param input: Object to be checked\n        :return: True, if the input contains a supported intrinsic function.  False otherwise", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_request(name=None):\n    '''\n    \n    '''\n    notify_path = os.path.join(__opts__['cachedir'], 'req_state.p')\n    serial = salt.payload.Serial(__opts__)\n    if os.path.isfile(notify_path):\n        with salt.utils.files.fopen(notify_path, 'rb') as fp_:\n            req = serial.load(fp_)\n        if name:\n            return req[name]\n        return req\n    return {}", "output": ".. versionadded:: 2015.5.0\n\n    Return the state request information, if any\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.check_request", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def addSibling(self, elem):\n        \"\"\" \"\"\"\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlAddSibling(self._o, elem__o)\n        if ret is None:raise treeError('xmlAddSibling() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "output": "Add a new element @elem to the list of siblings of @cur\n          merging adjacent TEXT nodes (@elem may be freed) If the new\n          element was already inserted in a document it is first\n           unlinked from its existing context.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def resources(self):\n        '''  '''\n        used_resources = self._used_resources()\n        ret = collections.defaultdict(dict)\n        for resource, total in six.iteritems(self._resources):\n            ret[resource]['total'] = total\n            if resource in used_resources:\n                ret[resource]['used'] = used_resources[resource]\n            else:\n                ret[resource]['used'] = 0\n        return ret", "output": "get total resources and available ones", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def returner(ret):\n    '''\n    \n    '''\n    log.debug('sqlite3 returner <returner> called with data: %s', ret)\n    conn = _get_conn(ret)\n    cur = conn.cursor()\n    sql = '''INSERT INTO salt_returns\n             (fun, jid, id, fun_args, date, full_ret, success)\n             VALUES (:fun, :jid, :id, :fun_args, :date, :full_ret, :success)'''\n    cur.execute(sql,\n                {'fun': ret['fun'],\n                 'jid': ret['jid'],\n                 'id': ret['id'],\n                 'fun_args': six.text_type(ret['fun_args']) if ret.get('fun_args') else None,\n                 'date': six.text_type(datetime.datetime.now()),\n                 'full_ret': salt.utils.json.dumps(ret['return']),\n                 'success': ret.get('success', '')})\n    _close_conn(conn)", "output": "Insert minion return data into the sqlite3 database", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def stopped(name,\n            kill=False,\n            remote_addr=None,\n            cert=None,\n            key=None,\n            verify_cert=True):\n    '''\n    \n    '''\n    ret = {\n        'name': name,\n        'kill': kill,\n\n        'remote_addr': remote_addr,\n        'cert': cert,\n        'key': key,\n        'verify_cert': verify_cert,\n\n        'changes': {}\n    }\n\n    try:\n        container = __salt__['lxd.container_get'](\n            name, remote_addr, cert, key, verify_cert, _raw=True\n        )\n    except CommandExecutionError as e:\n        return _error(ret, six.text_type(e))\n    except SaltInvocationError as e:\n        # Container not found\n        return _error(ret, 'Container \"{0}\" not found'.format(name))\n\n    if container.status_code == CONTAINER_STATUS_STOPPED:\n        return _success(ret, 'Container \"{0}\" is already stopped'.format(name))\n\n    if __opts__['test']:\n        ret['changes']['stopped'] = \\\n            'Would stop the container \"{0}\"'.format(name)\n        return _unchanged(ret, ret['changes']['stopped'])\n\n    container.stop(force=kill, wait=True)\n    ret['changes']['stopped'] = \\\n        'Stopped the container \"{0}\"'.format(name)\n    return _success(ret, ret['changes']['stopped'])", "output": "Ensure a LXD container is stopped, kill it if kill is true else stop it\n\n    name :\n        The name of the container to stop\n\n    kill :\n        kill if true\n\n    remote_addr :\n        An URL to a remote Server, you also have to give cert and key if you\n        provide remote_addr!\n\n        Examples:\n            https://myserver.lan:8443\n            /var/lib/mysocket.sock\n\n    cert :\n        PEM Formatted SSL Zertifikate.\n\n        Examples:\n            ~/.config/lxc/client.crt\n\n    key :\n        PEM Formatted SSL Key.\n\n        Examples:\n            ~/.config/lxc/client.key\n\n    verify_cert : True\n        Wherever to verify the cert, this is by default True\n        but in the most cases you want to set it off as LXD\n        normaly uses self-signed certificates.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def create_from_user_input(raw_properties, jamfile_module, location):\n    \"\"\"'\"\"\"\n    assert is_iterable_typed(raw_properties, basestring)\n    assert isinstance(jamfile_module, basestring)\n    assert isinstance(location, basestring)\n    properties = property.create_from_strings(raw_properties, True)\n    properties = property.translate_paths(properties, location)\n    properties = property.translate_indirect(properties, jamfile_module)\n\n    project_id = get_manager().projects().attributeDefault(jamfile_module, 'id', None)\n    if not project_id:\n        project_id = os.path.abspath(location)\n    properties = property.translate_dependencies(properties, project_id, location)\n    properties = property.expand_subfeatures_in_conditions(properties)\n    return create(properties)", "output": "Creates a property-set from the input given by the user, in the\n    context of 'jamfile-module' at 'location", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def trim_join_unit(join_unit, length):\n    \"\"\"\n    \n    \"\"\"\n\n    if 0 not in join_unit.indexers:\n        extra_indexers = join_unit.indexers\n\n        if join_unit.block is None:\n            extra_block = None\n        else:\n            extra_block = join_unit.block.getitem_block(slice(length, None))\n            join_unit.block = join_unit.block.getitem_block(slice(length))\n    else:\n        extra_block = join_unit.block\n\n        extra_indexers = copy.copy(join_unit.indexers)\n        extra_indexers[0] = extra_indexers[0][length:]\n        join_unit.indexers[0] = join_unit.indexers[0][:length]\n\n    extra_shape = (join_unit.shape[0] - length,) + join_unit.shape[1:]\n    join_unit.shape = (length,) + join_unit.shape[1:]\n\n    return JoinUnit(block=extra_block, indexers=extra_indexers,\n                    shape=extra_shape)", "output": "Reduce join_unit's shape along item axis to length.\n\n    Extra items that didn't fit are returned as a separate block.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def save_all(self):\r\n        \"\"\"\r\n        \"\"\"\r\n        for index in range(self.get_stack_count()):\r\n            if self.data[index].editor.document().isModified():\r\n                self.save(index)", "output": "Save all opened files.\r\n\r\n        Iterate through self.data and call save() on any modified files.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _rules_from_env(self, val):\n        \"\"\"\"\"\"\n        val = val.split(':')\n        if 'DEFAULT_RULES' in val:\n            val = const.DEFAULT_RULES + [rule for rule in val if rule != 'DEFAULT_RULES']\n        return val", "output": "Transforms rules list from env-string to python.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def accepts(self, package):  # type: (poetry.packages.Package) -> bool\n        \"\"\"\n        \n        \"\"\"\n        return (\n            self._name == package.name\n            and self._constraint.allows(package.version)\n            and (not package.is_prerelease() or self.allows_prereleases())\n        )", "output": "Determines if the given package matches this dependency.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def registry_path(cls, project, location, registry):\n        \"\"\"\"\"\"\n        return google.api_core.path_template.expand(\n            \"projects/{project}/locations/{location}/registries/{registry}\",\n            project=project,\n            location=location,\n            registry=registry,\n        )", "output": "Return a fully-qualified registry string.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def graph_memoized(func):\n    \"\"\"\n    \n    \"\"\"\n\n    # TODO it keeps the graph alive\n    from ..compat import tfv1\n    GRAPH_ARG_NAME = '__IMPOSSIBLE_NAME_FOR_YOU__'\n\n    @memoized\n    def func_with_graph_arg(*args, **kwargs):\n        kwargs.pop(GRAPH_ARG_NAME)\n        return func(*args, **kwargs)\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        assert GRAPH_ARG_NAME not in kwargs, \"No Way!!\"\n        graph = tfv1.get_default_graph()\n        kwargs[GRAPH_ARG_NAME] = graph\n        return func_with_graph_arg(*args, **kwargs)\n    return wrapper", "output": "Like memoized, but keep one cache per default graph.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_edit_filetypes():\n    \"\"\"\"\"\"\n    # The filter details are not hidden on Windows, so we can't use\n    # all Pygments extensions on that platform\n    if os.name == 'nt':\n        supported_exts = []\n    else:\n        try:\n            supported_exts = _get_pygments_extensions()\n        except Exception:\n            supported_exts = []\n\n    # NOTE: Try to not add too much extensions to this list to not\n    # make the filter look too big on Windows\n    favorite_exts = ['.py', '.R', '.jl', '.ipynb', '.md', '.pyw', '.pyx',\n                     '.c', '.cpp', '.json', '.dat', '.csv', '.tsv', '.txt',\n                     '.ini', '.html', '.js', '.h', '.bat']\n\n    other_exts = [ext for ext in supported_exts if ext not in favorite_exts]\n    all_exts = tuple(favorite_exts + other_exts)\n    text_filetypes = (_(\"Supported text files\"), all_exts)\n    return [text_filetypes] + EDIT_FILETYPES", "output": "Get all file types supported by the Editor", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def slice_indexer(self, start=None, end=None, step=None, kind=None):\n        \"\"\"\n        \n        \"\"\"\n        start_slice, end_slice = self.slice_locs(start, end, step=step,\n                                                 kind=kind)\n\n        # return a slice\n        if not is_scalar(start_slice):\n            raise AssertionError(\"Start slice bound is non-scalar\")\n        if not is_scalar(end_slice):\n            raise AssertionError(\"End slice bound is non-scalar\")\n\n        return slice(start_slice, end_slice, step)", "output": "For an ordered or unique index, compute the slice indexer for input\n        labels and step.\n\n        Parameters\n        ----------\n        start : label, default None\n            If None, defaults to the beginning\n        end : label, default None\n            If None, defaults to the end\n        step : int, default None\n        kind : string, default None\n\n        Returns\n        -------\n        indexer : slice\n\n        Raises\n        ------\n        KeyError : If key does not exist, or key is not unique and index is\n            not ordered.\n\n        Notes\n        -----\n        This function assumes that the data is sorted, so use at your own peril\n\n        Examples\n        ---------\n        This is a method on all index types. For example you can do:\n\n        >>> idx = pd.Index(list('abcd'))\n        >>> idx.slice_indexer(start='b', end='c')\n        slice(1, 3)\n\n        >>> idx = pd.MultiIndex.from_arrays([list('abcd'), list('efgh')])\n        >>> idx.slice_indexer(start='b', end=('c', 'g'))\n        slice(1, 3)", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _get_technologies():\n    '''\n    \n    '''\n    tech = ''\n    technologies = pyconnman.ConnManager().get_technologies()\n    for path, params in technologies:\n        tech += '{0}\\n\\tName = {1}\\n\\tType = {2}\\n\\tPowered = {3}\\n\\tConnected = {4}\\n'.format(\n            path, params['Name'], params['Type'], params['Powered'] == 1, params['Connected'] == 1)\n    return tech", "output": "Returns the technologies of connman", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def check_ownership(obj, raise_if_false=True):\n    \"\"\"\n    \"\"\"\n    if not obj:\n        return False\n\n    security_exception = SupersetSecurityException(\n        \"You don't have the rights to alter [{}]\".format(obj))\n\n    if g.user.is_anonymous:\n        if raise_if_false:\n            raise security_exception\n        return False\n    roles = [r.name for r in get_user_roles()]\n    if 'Admin' in roles:\n        return True\n    session = db.create_scoped_session()\n    orig_obj = session.query(obj.__class__).filter_by(id=obj.id).first()\n\n    # Making a list of owners that works across ORM models\n    owners = []\n    if hasattr(orig_obj, 'owners'):\n        owners += orig_obj.owners\n    if hasattr(orig_obj, 'owner'):\n        owners += [orig_obj.owner]\n    if hasattr(orig_obj, 'created_by'):\n        owners += [orig_obj.created_by]\n\n    owner_names = [o.username for o in owners if o]\n\n    if (\n            g.user and hasattr(g.user, 'username') and\n            g.user.username in owner_names):\n        return True\n    if raise_if_false:\n        raise security_exception\n    else:\n        return False", "output": "Meant to be used in `pre_update` hooks on models to enforce ownership\n\n    Admin have all access, and other users need to be referenced on either\n    the created_by field that comes with the ``AuditMixin``, or in a field\n    named ``owners`` which is expected to be a one-to-many with the User\n    model. It is meant to be used in the ModelView's pre_update hook in\n    which raising will abort the update.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def QA_util_add_months(dt, months):\n    \"\"\"\n    \n    \"\"\"\n    dt = datetime.datetime.strptime(\n        dt, \"%Y-%m-%d\") + relativedelta(months=months)\n    return(dt)", "output": "#\u8fd4\u56dedt\u9694months\u4e2a\u6708\u540e\u7684\u65e5\u671f\uff0cmonths\u76f8\u5f53\u4e8e\u6b65\u957f", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def months_between(date1, date2, roundOff=True):\n    \"\"\"\n    \n    \"\"\"\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.months_between(\n        _to_java_column(date1), _to_java_column(date2), roundOff))", "output": "Returns number of months between dates date1 and date2.\n    If date1 is later than date2, then the result is positive.\n    If date1 and date2 are on the same day of month, or both are the last day of month,\n    returns an integer (time of day will be ignored).\n    The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n\n    >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n    >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n    [Row(months=3.94959677)]\n    >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n    [Row(months=3.9495967741935485)]", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def update(database, table, key):\n    \"\"\"\"\"\"\n    field = random.randrange(10)\n    value = ''.join(random.choice(string.printable) for i in range(100))\n    with database.batch() as batch:\n        batch.update(table=table, columns=('id', 'field%d' % field),\n                     values=[(key, value)])", "output": "Does a single update operation.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_pythonpath(self, at_start=False):\r\n        \"\"\"\"\"\"\r\n        if at_start:\r\n            current_path = self.get_option('current_project_path',\r\n                                           default=None)\r\n        else:\r\n            current_path = self.get_active_project_path()\r\n        if current_path is None:\r\n            return []\r\n        else:\r\n            return [current_path]", "output": "Get project path as a list to be added to PYTHONPATH", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_valid_version():\n    '''\n    \n    '''\n\n    # Locate the full path to npm\n    npm_path = salt.utils.path.which('npm')\n\n    # pylint: disable=no-member\n    res = salt.modules.cmdmod.run('{npm} --version'.format(npm=npm_path), output_loglevel='quiet')\n    npm_version, valid_version = _LooseVersion(res), _LooseVersion('1.2')\n    # pylint: enable=no-member\n    if npm_version < valid_version:\n        raise CommandExecutionError(\n            '\\'npm\\' is not recent enough({0} < {1}). Please Upgrade.'.format(\n                npm_version, valid_version\n            )\n        )", "output": "Check the version of npm to ensure this module will work. Currently\n    npm must be at least version 1.2.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def websocket_url_for_server_url(url):\n    ''' \n\n    '''\n    if url.startswith(\"http:\"):\n        reprotocoled = \"ws\" + url[4:]\n    elif url.startswith(\"https:\"):\n        reprotocoled = \"wss\" + url[5:]\n    else:\n        raise ValueError(\"URL has unknown protocol \" + url)\n    if reprotocoled.endswith(\"/\"):\n        return reprotocoled + \"ws\"\n    else:\n        return reprotocoled + \"/ws\"", "output": "Convert an ``http(s)`` URL for a Bokeh server websocket endpoint into\n    the appropriate ``ws(s)`` URL\n\n    Args:\n        url (str):\n            An ``http(s)`` URL\n\n    Returns:\n        str:\n            The corresponding ``ws(s)`` URL ending in ``/ws``\n\n    Raises:\n        ValueError:\n            If the input URL is not of the proper form.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cancel_job(self, job_id, project=None, location=None, retry=DEFAULT_RETRY):\n        \"\"\"\n        \"\"\"\n        extra_params = {\"projection\": \"full\"}\n\n        if project is None:\n            project = self.project\n\n        if location is None:\n            location = self.location\n\n        if location is not None:\n            extra_params[\"location\"] = location\n\n        path = \"/projects/{}/jobs/{}/cancel\".format(project, job_id)\n\n        resource = self._call_api(\n            retry, method=\"POST\", path=path, query_params=extra_params\n        )\n\n        return self.job_from_resource(resource[\"job\"])", "output": "Attempt to cancel a job from a job ID.\n\n        See\n        https://cloud.google.com/bigquery/docs/reference/rest/v2/jobs/cancel\n\n        Arguments:\n            job_id (str): Unique job identifier.\n\n        Keyword Arguments:\n            project (str):\n                (Optional) ID of the project which owns the job (defaults to\n                the client's project).\n            location (str): Location where the job was run.\n            retry (google.api_core.retry.Retry):\n                (Optional) How to retry the RPC.\n\n        Returns:\n            Union[google.cloud.bigquery.job.LoadJob, \\\n                  google.cloud.bigquery.job.CopyJob, \\\n                  google.cloud.bigquery.job.ExtractJob, \\\n                  google.cloud.bigquery.job.QueryJob]:\n                Job instance, based on the resource returned by the API.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def raw_command(netfn, command, bridge_request=None, data=(), retry=True, delay_xmit=None, **kwargs):\n    '''\n    \n    '''\n    with _IpmiSession(**kwargs) as s:\n        r = s.raw_command(netfn=int(netfn),\n                        command=int(command),\n                        bridge_request=bridge_request,\n                        data=data,\n                        retry=retry,\n                        delay_xmit=delay_xmit)\n        return r", "output": "Send raw ipmi command\n\n    This allows arbitrary IPMI bytes to be issued.  This is commonly used\n    for certain vendor specific commands.\n\n    :param netfn: Net function number\n    :param command: Command value\n    :param bridge_request: The target slave address and channel number for\n                        the bridge request.\n    :param data: Command data as a tuple or list\n    :param kwargs:\n        - api_host=127.0.0.1\n        - api_user=admin\n        - api_pass=example\n        - api_port=623\n        - api_kg=None\n\n    :returns: dict -- The response from IPMI device\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-call ipmi.raw_command netfn=0x06 command=0x46 data=[0x02]\n        # this will return the name of the user with id 2 in bytes", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_conn(profile):\n    '''\n    \n    '''\n    params = {}\n    for key in ('host', 'port', 'token', 'scheme', 'consistency', 'dc', 'verify'):\n        if key in profile:\n            params[key] = profile[key]\n\n    if HAS_CONSUL:\n        return consul.Consul(**params)\n    else:\n        raise CommandExecutionError(\n            '(unable to import consul, '\n            'module most likely not installed. PLease install python-consul)'\n        )", "output": "Return a client object for accessing consul", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _agent_import_failed(trace):\n    \"\"\"\"\"\"\n\n    class _AgentImportFailed(Trainer):\n        _name = \"AgentImportFailed\"\n        _default_config = with_common_config({})\n\n        def _setup(self, config):\n            raise ImportError(trace)\n\n    return _AgentImportFailed", "output": "Returns dummy agent class for if PyTorch etc. is not installed.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def flush(self, fsync=False):\n        \"\"\"\n        \n        \"\"\"\n        if self._handle is not None:\n            self._handle.flush()\n            if fsync:\n                try:\n                    os.fsync(self._handle.fileno())\n                except OSError:\n                    pass", "output": "Force all buffered modifications to be written to disk.\n\n        Parameters\n        ----------\n        fsync : bool (default False)\n          call ``os.fsync()`` on the file handle to force writing to disk.\n\n        Notes\n        -----\n        Without ``fsync=True``, flushing may not guarantee that the OS writes\n        to disk. With fsync, the operation will block until the OS claims the\n        file has been written; however, other caching layers may still\n        interfere.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_screen_resolution(self):\n        \"\"\"\"\"\"\n        widget = QDesktopWidget()\n        geometry = widget.availableGeometry(widget.primaryScreen())\n        return geometry.width(), geometry.height()", "output": "Return the screen resolution of the primary screen.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def cell_value(self, column_family_id, column, index=0):\n        \"\"\"\n        \"\"\"\n        cells = self.find_cells(column_family_id, column)\n\n        try:\n            cell = cells[index]\n        except (TypeError, IndexError):\n            num_cells = len(cells)\n            msg = _MISSING_INDEX.format(index, column, column_family_id, num_cells)\n            raise IndexError(msg)\n\n        return cell.value", "output": "Get a single cell value stored on this instance.\n\n        For example:\n\n        .. literalinclude:: snippets_table.py\n            :start-after: [START bigtable_row_cell_value]\n            :end-before: [END bigtable_row_cell_value]\n\n        Args:\n            column_family_id (str): The ID of the column family. Must be of the\n                form ``[_a-zA-Z0-9][-_.a-zA-Z0-9]*``.\n            column (bytes): The column within the column family where the cell\n                is located.\n            index (Optional[int]): The offset within the series of values. If\n                not specified, will return the first cell.\n\n        Returns:\n            ~google.cloud.bigtable.row_data.Cell value: The cell value stored\n            in the specified column and specified index.\n\n        Raises:\n            KeyError: If ``column_family_id`` is not among the cells stored\n                in this row.\n            KeyError: If ``column`` is not among the cells stored in this row\n                for the given ``column_family_id``.\n            IndexError: If ``index`` cannot be found within the cells stored\n                in this row for the given ``column_family_id``, ``column``\n                pair.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def mmodule(saltenv, fun, *args, **kwargs):\n    '''\n    \n    '''\n    mminion = _MMinion(saltenv)\n    return mminion.functions[fun](*args, **kwargs)", "output": "Loads minion modules from an environment so that they can be used in pillars\n    for that environment\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' saltutil.mmodule base test.ping", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def setData(self, index, value, role=Qt.EditRole, change_type=None):\r\n        \"\"\"\"\"\"\r\n        column = index.column()\r\n        row = index.row()\r\n\r\n        if index in self.display_error_idxs:\r\n            return False\r\n        if change_type is not None:\r\n            try:\r\n                value = self.data(index, role=Qt.DisplayRole)\r\n                val = from_qvariant(value, str)\r\n                if change_type is bool:\r\n                    val = bool_false_check(val)\r\n                self.df.iloc[row, column] = change_type(val)\r\n            except ValueError:\r\n                self.df.iloc[row, column] = change_type('0')\r\n        else:\r\n            val = from_qvariant(value, str)\r\n            current_value = self.get_value(row, column)\r\n            if isinstance(current_value, (bool, np.bool_)):\r\n                val = bool_false_check(val)\r\n            supported_types = (bool, np.bool_) + REAL_NUMBER_TYPES\r\n            if (isinstance(current_value, supported_types) or\r\n                    is_text_string(current_value)):\r\n                try:\r\n                    self.df.iloc[row, column] = current_value.__class__(val)\r\n                except (ValueError, OverflowError) as e:\r\n                    QMessageBox.critical(self.dialog, \"Error\",\r\n                                         str(type(e).__name__) + \": \" + str(e))\r\n                    return False\r\n            else:\r\n                QMessageBox.critical(self.dialog, \"Error\",\r\n                                     \"Editing dtype {0!s} not yet supported.\"\r\n                                     .format(type(current_value).__name__))\r\n                return False\r\n        self.max_min_col_update()\r\n        self.dataChanged.emit(index, index)\r\n        return True", "output": "Cell content change", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def get_tickers_from_file(self, filename):\n        \"\"\"\"\"\"\n        if not os.path.exists(filename):\n            log.error(\"Ticker List file does not exist: %s\", filename)\n\n        tickers = []\n        with io.open(filename, 'r') as fd:\n            for ticker in fd:\n                tickers.append(ticker.rstrip())\n        return tickers", "output": "Load ticker list from txt file", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def _check_for_invalid_keys(fname, kwargs, compat_args):\n    \"\"\"\n    \n\n    \"\"\"\n    # set(dict) --> set of the dictionary's keys\n    diff = set(kwargs) - set(compat_args)\n\n    if diff:\n        bad_arg = list(diff)[0]\n        raise TypeError((\"{fname}() got an unexpected \"\n                         \"keyword argument '{arg}'\".\n                         format(fname=fname, arg=bad_arg)))", "output": "Checks whether 'kwargs' contains any keys that are not\n    in 'compat_args' and raises a TypeError if there is one.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def zoom_in(self):\n        \"\"\"\"\"\"\n        if self._scalefactor <= self._sfmax:\n            self._scalefactor += 1\n            self.scale_image()\n            self._adjust_scrollbar(self._scalestep)\n            self.sig_zoom_changed.emit(self.get_scaling())", "output": "Scale the image up by one scale step.", "category": "Python"}, {"instruction": "Summarizing the code.", "input": "def put_watch(self, id, body=None, params=None):\n        \"\"\"\n        \n        \"\"\"\n        if id in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'id'.\")\n        return self.transport.perform_request(\n            \"PUT\", _make_path(\"_watcher\", \"watch\", id), params=params, body=body\n        )", "output": "`<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-put-watch.html>`_\n\n        :arg id: Watch ID\n        :arg body: The watch\n        :arg active: Specify whether the watch is in/active by default\n        :arg if_primary_term: only update the watch if the last operation that\n            has changed the watch has the specified primary term\n        :arg if_seq_no: only update the watch if the last operation that has\n            changed the watch has the specified sequence number\n        :arg version: Explicit version number for concurrency control", "category": "Python"}]