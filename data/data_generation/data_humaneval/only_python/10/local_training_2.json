[{"instruction": "def close(self, timeout: int = 5) -> None:\n        \"\"\"Stop a ffmpeg instance.\n\n        Return a coroutine\n        \"\"\"\n", "input": "", "output": "        if self._read_task is not None and not self._read_task.cancelled():\n            self._read_task.cancel()\n\n        return super().close(timeout)", "category": "Python"}, {"instruction": "def logout(self):\n        \"\"\"\n        Log currently authenticated user out, invalidating any existing tokens.\n        \"\"\"\n", "input": "", "output": "        # Remove token from local cache\n        # MAINT: need to expire token on server\n        data = self._read_uaa_cache()\n        if self.uri in data:\n            for client in data[self.uri]:\n                if client['id'] == self.client['id']:\n                    data[self.uri].remove(client)\n\n        with open(self._cache_path, 'w') as output:\n            output.write(json.dumps(data, sort_keys=True, indent=4))", "category": "Python"}, {"instruction": "def read_byte(self):\n        \"\"\"Read one byte of cooked data\n        \"\"\"\n", "input": "", "output": "        buf = b''\n        if len(self.cookedq) > 0:\n            buf = bytes([self.cookedq[0]])\n            self.cookedq = self.cookedq[1:]\n        else:\n            yield from self.process_rawq()\n            if not self.eof:\n                yield from self.fill_rawq()\n                yield from self.process_rawq()\n                # There now should be data so lets read again\n                buf = yield from self.read_byte()\n\n        return buf", "category": "Python"}, {"instruction": "def _words_by_distinctiveness_score(vocab, topic_word_distrib, doc_topic_distrib, doc_lengths, n=None,\n                                    least_to_most=False):\n    \"\"\"Return words in `vocab` ordered by distinctiveness score.\"\"\"\n", "input": "", "output": "    p_t = get_marginal_topic_distrib(doc_topic_distrib, doc_lengths)\n    distinct = get_word_distinctiveness(topic_word_distrib, p_t)\n\n    return _words_by_score(vocab, distinct, least_to_most=least_to_most, n=n)", "category": "Python"}, {"instruction": "def estimate_params(self,burn=None,clip=10.0,alpha=0.32):\n        \"\"\" Estimate all source parameters \"\"\"\n", "input": "", "output": "        mle = self.get_mle()\n        out = odict()\n        for param in mle.keys():\n            out[param] = self.estimate(param,burn=burn,clip=clip,alpha=alpha)\n        return out", "category": "Python"}, {"instruction": "def to_raw(self, value, context=None):\n        \"\"\"Convert the value to a JSON compatible value\"\"\"\n", "input": "", "output": "        if value is None:\n            return None\n        res = {}\n        value = value.copy()\n        errors = []\n        for field in list(set(value) & set(self._fields)):\n            schema = self._fields.get(field)\n            name = schema.get_attr('name', field)\n            try:\n                res[name] = \\\n                    schema.to_raw(value.pop(field), context=context)\n            except exceptions.ValidationErrors as ex:\n                self._update_errors_by_exception(errors, ex, name)\n\n        self._raise_exception_when_errors(errors, value)\n        res.update(value)\n        return res", "category": "Python"}, {"instruction": "def get(self, url=None, delimiter=\"/\"):\n        \"\"\"Path is an s3 url. Ommiting the path or providing \"s3://\" as the\n        path will return a list of all buckets. Otherwise, all subdirectories\n        and their contents will be shown.\n        \"\"\"\n", "input": "", "output": "        params = {'Delimiter': delimiter}\n        bucket, obj_key = _parse_url(url)\n\n        if bucket:\n            params['Bucket'] = bucket\n        else:\n            return self.call(\"ListBuckets\", response_data_key=\"Buckets\")\n\n        if obj_key:\n            params['Prefix'] = obj_key\n\n        objects = self.call(\"ListObjects\", response_data_key=\"Contents\",\n                            **params)\n        if objects:\n            for obj in objects:\n                obj['url'] = \"s3://{0}/{1}\".format(bucket, obj['Key'])\n\n        return objects", "category": "Python"}, {"instruction": "def run(self, command, application):\n        \"\"\"Get or set the profile.\n\n        If .profile is called with no args, the current profile\n        is displayed.  If the .profile command is called with a\n        single arg, then the current profile for the application\n        will be set to the new value.\n        \"\"\"\n", "input": "", "output": "        if len(command) == 1:\n            profile = application.profile\n            if profile is None:\n                self._output.write(\n                    \"Current shell profile: no profile configured\\n\"\n                    \"You can change profiles using: .profile profile-name\\n\")\n            else:\n                self._output.write(\"Current shell profile: %s\\n\" % profile)\n        elif len(command) == 2:\n            new_profile_name = command[1]\n            application.profile = new_profile_name\n            self._output.write(\"Current shell profile changed to: %s\\n\" %\n                               new_profile_name)\n        else:\n            self._err.write(\"Usage:\\n%s\\n\" % self.USAGE)", "category": "Python"}, {"instruction": "def parse(cls, fptr, offset, length):\n        \"\"\"Parse bits per component box.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n        offset : int\n            Start position of box in bytes.\n        length : int\n            Length of the box in bytes.\n\n        Returns\n        -------\n        BitsPerComponent\n            Instance of the current bits per component box.\n        \"\"\"\n", "input": "", "output": "        nbytes = length - 8\n        data = fptr.read(nbytes)\n        bpc = tuple(((x & 0x7f) + 1) for x in bytearray(data))\n        signed = tuple(((x & 0x80) > 0) for x in bytearray(data))\n\n        return cls(bpc, signed, length=length, offset=offset)", "category": "Python"}, {"instruction": "def get_user_by_username(self, username):\n        \"\"\"\n        Returns details for user of the given username.\n\n        If there is more than one match will only return the first.  Use\n        get_users() for full result set.\n        \"\"\"\n", "input": "", "output": "        results = self.get_users(filter='username eq \"%s\"' % (username))\n        if results['totalResults'] == 0:\n            logging.warning(\"Found no matches for given username.\")\n            return\n        elif results['totalResults'] > 1:\n            logging.warning(\"Found %s matches for username %s\" %\n                (results['totalResults'], username))\n\n        return results['resources'][0]", "category": "Python"}, {"instruction": "def _video_part_rIds(self):\n        \"\"\"Return the rIds for relationships to media part for video.\n\n        This is where the media part and its relationships to the slide are\n        actually created.\n        \"\"\"\n", "input": "", "output": "        media_rId, video_rId = self._slide_part.get_or_add_video_media_part(\n            self._video\n        )\n        return media_rId, video_rId", "category": "Python"}, {"instruction": "def fields(self) -> GraphQLInputFieldMap:\n        \"\"\"Get provided fields, wrap them as GraphQLInputField if needed.\"\"\"\n", "input": "", "output": "        try:\n            fields = resolve_thunk(self._fields)\n        except GraphQLError:\n            raise\n        except Exception as error:\n            raise TypeError(f\"{self.name} fields cannot be resolved: {error}\")\n        if not isinstance(fields, dict) or not all(\n            isinstance(key, str) for key in fields\n        ):\n            raise TypeError(\n                f\"{self.name} fields must be a dict with field names as keys\"\n                \" or a function which returns such an object.\"\n            )\n        if not all(\n            isinstance(value, GraphQLInputField) or is_input_type(value)\n            for value in fields.values()\n        ):\n            raise TypeError(\n                f\"{self.name} fields must be\"\n                \" GraphQLInputField or input type objects.\"\n            )\n        return {\n            name: value\n            if isinstance(value, GraphQLInputField)\n            else GraphQLInputField(value)\n            for name, value in fields.items()\n        }", "category": "Python"}, {"instruction": "def do_inspect(self, arg):\n        \"\"\"\n        i(nspect) object\n        Inspect an object\n        \"\"\"\n", "input": "", "output": "        if arg in self.curframe.f_locals:\n            obj = self.curframe.f_locals[arg]\n        elif arg in self.curframe.f_globals:\n            obj = self.curframe.f_globals[arg]\n        else:\n            obj = WebPdb.null\n        if obj is not WebPdb.null:\n            self.console.writeline(\n                '{0} = {1}:\\n'.format(arg, type(obj))\n            )\n            for name, value in inspect.getmembers(obj):\n                if not (name.startswith('__') and (name.endswith('__'))):\n                    self.console.writeline('    {0}: {1}\\n'.format(\n                        name, self._get_repr(value, pretty=True, indent=8)\n                    ))\n        else:\n            self.console.writeline(\n                'NameError: name \"{0}\" is not defined\\n'.format(arg)\n            )\n        self.console.flush()", "category": "Python"}, {"instruction": "def select_char_code_table(self, table):\n        '''Select character code table, from tree built in ones.\n        \n        Args:\n            table: The desired character code table. Choose from 'standard', 'eastern european', 'western european', and 'spare'\n        Returns:\n            None\n        Raises:\n            RuntimeError: Invalid chartable.\n        '''\n", "input": "", "output": "        tables = {'standard': 0,\n                  'eastern european': 1,\n                  'western european': 2,\n                  'spare': 3\n                  }\n        if table in tables:\n            self.send(chr(27)+'t'+chr(tables[table]))\n        else:\n            raise RuntimeError('Invalid char table.')", "category": "Python"}, {"instruction": "def reference(self):\n        \"\"\"The Didl object this favorite refers to.\"\"\"\n", "input": "", "output": "\n        # Import from_didl_string if it isn't present already. The import\n        # happens here because it would cause cyclic import errors if the\n        # import happened at load time.\n        global _FROM_DIDL_STRING_FUNCTION  # pylint: disable=global-statement\n        if not _FROM_DIDL_STRING_FUNCTION:\n            from . import data_structures_entry\n            _FROM_DIDL_STRING_FUNCTION = data_structures_entry.from_didl_string\n\n        ref = _FROM_DIDL_STRING_FUNCTION(\n            getattr(self, 'resource_meta_data'))[0]\n        # The resMD metadata lacks a <res> tag, so we use the resources from\n        # the favorite to make 'reference' playable.\n        ref.resources = self.resources\n        return ref", "category": "Python"}, {"instruction": "def store_data(data):\n    \"\"\"Use this function to store data in a JSON file.\n\n    This function is used for loading up a JSON file and appending additional\n    data to the JSON file.\n\n    :param data: the data to add to the JSON file.\n    :type data: dict\n    \"\"\"\n", "input": "", "output": "    with open(url_json_path) as json_file:\n        try:\n            json_file_data = load(json_file)\n            json_file_data.update(data)\n        except (AttributeError, JSONDecodeError):\n            json_file_data = data\n    with open(url_json_path, 'w') as json_file:\n        dump(json_file_data, json_file, indent=4, sort_keys=True)", "category": "Python"}, {"instruction": "def upd_data(self, *args):\n        \"\"\"Update to match new entity data\"\"\"\n", "input": "", "output": "        data = [self.munge(k, v) for k, v in self.iter_data()]\n        self.data = sorted(data, key=lambda d: d['key'])", "category": "Python"}, {"instruction": "def addSubprocess(self, fds, name, factory):\n        \"\"\"\n        Public method for _addSubprocess.\n        Wraps reactor.adoptStreamConnection in \n        a simple DeferredLock to guarantee\n        workers play well together.\n        \"\"\"\n", "input": "", "output": "        self._lock.run(self._addSubprocess, self, fds, name, factory)", "category": "Python"}, {"instruction": "def pulse(self):\n        \"\"\"\n        Calls when_rotated callback if detected changes\n        \"\"\"\n", "input": "", "output": "        new_b_value = self.gpio_b.is_active\n        new_a_value = self.gpio_a.is_active\n\n        value = self.table_values.value(new_b_value, new_a_value, self.old_b_value, self.old_a_value)\n\n        self.old_b_value = new_b_value\n        self.old_a_value = new_a_value\n\n        if value != 0:\n            self.when_rotated(value)", "category": "Python"}, {"instruction": "def _generate_statistics(self, out_path, results_path):\n        \"\"\"Writes a statistics report for the results at `results_path` to\n        `out_path`.\n\n        Reuses an existing statistics report if one exists at\n        `out_path`.\n\n        :param out_path: path to output statistics report to\n        :type out_path: `str`\n        :param results_path: path of results to generate statistics for\n        :type results_path: `str`\n\n        \"\"\"\n", "input": "", "output": "        if not os.path.exists(out_path):\n            report = StatisticsReport(self._corpus, self._tokenizer,\n                                      results_path)\n            report.generate_statistics()\n            with open(out_path, mode='w', encoding='utf-8', newline='') as fh:\n                report.csv(fh)", "category": "Python"}, {"instruction": "def __parse_blacklist(self, json):\n        \"\"\"Parse blacklist entries using Sorting Hat format.\n\n        The Sorting Hat blacklist format is a JSON stream that\n        stores a list of blacklisted entries.\n\n        Next, there is an example of a valid stream:\n\n        {\n            \"blacklist\": [\n                \"John Doe\",\n                \"John Smith\",\n                \"root@example.com\"\n            ]\n        }\n\n        :param stream: stream to parse\n\n        :raises InvalidFormatError: raised when the format of the stream is\n            not valid.\n        \"\"\"\n", "input": "", "output": "        try:\n            for entry in json['blacklist']:\n                if not entry:\n                    msg = \"invalid json format. Blacklist entries cannot be null or empty\"\n                    raise InvalidFormatError(cause=msg)\n\n                excluded = self.__encode(entry)\n\n                bl = self._blacklist.get(excluded, None)\n\n                if not bl:\n                    bl = MatchingBlacklist(excluded=excluded)\n                    self._blacklist[excluded] = bl\n        except KeyError as e:\n            msg = \"invalid json format. Attribute %s not found\" % e.args\n            raise InvalidFormatError(cause=msg)", "category": "Python"}, {"instruction": "def rescale(img, size, mode=None):\n    \"\"\"\n    Rescale an image to given size, crop if mode specified\n     * img: a PIL image object\n     * size: a 2-tuple of (width, height); at least one must be specified\n     * mode: CROP_TL keep the top left part, CROP_BR the bottom right part\n    \"\"\"\n", "input": "", "output": "\n    assert size[0] or size[1], \"Must provide a width or a height\"\n\n    size = list(size)\n    crop = size[0] and size[1]\n\n    if not size[0]:\n        size[0] = int(img.size[0] * size[1] / float(img.size[1]))\n    if not size[1]:\n        size[1] = int(img.size[1] * size[0] / float(img.size[0]))\n\n    if crop:\n        b = img.size[0] > img.size[1]\n        crop_size = (img.size[b], int(img.size[b] * size[0] / float(size[1])))\n\n        if mode ==  CROP_TL:\n            img = img.crop((0, 0, crop_size[0], crop_size[1]))\n        elif mode == CROP_BR:\n            img = img.crop((\n                img.size[0] - crop_size[0],\n                img.size[1] - crop_size[1],\n                img.size[0],\n                img.size[1]\n            ))\n\n    return img.resize(size, Image.ANTIALIAS)", "category": "Python"}, {"instruction": "def fetch_html(self, msg_nums):\n        \"\"\"\n        Given a message number that we found with imap_search,\n        get the text/html content.\n        @Params\n        msg_nums - message number to get html message for\n        @Returns\n        HTML content of message matched by message number\n        \"\"\"\n", "input": "", "output": "        if not msg_nums:\n            raise Exception(\"Invalid Message Number!\")\n\n        return self.__imap_fetch_content_type(msg_nums, self.HTML)", "category": "Python"}, {"instruction": "def get_descriptor(ctx):\n    \"\"\" construct and return descriptor \"\"\"\n", "input": "", "output": "    d_net = gluon.nn.Sequential()\n    with d_net.name_scope():\n\n        d_net.add(SNConv2D(num_filter=64, kernel_size=4, strides=2, padding=1, in_channels=3, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=128, kernel_size=4, strides=2, padding=1, in_channels=64, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=256, kernel_size=4, strides=2, padding=1, in_channels=128, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=512, kernel_size=4, strides=2, padding=1, in_channels=256, ctx=ctx))\n        d_net.add(gluon.nn.LeakyReLU(0.2))\n\n        d_net.add(SNConv2D(num_filter=1, kernel_size=4, strides=1, padding=0, in_channels=512, ctx=ctx))\n\n    return d_net", "category": "Python"}, {"instruction": "def _new_cls_attr(self, clazz, name, cls=None, mult=MULT_ONE, cont=True,\n                      ref=False, bool_assignment=False, position=0):\n        \"\"\"Creates new meta attribute of this class.\"\"\"\n", "input": "", "output": "        attr = MetaAttr(name, cls, mult, cont, ref, bool_assignment,\n                        position)\n        clazz._tx_attrs[name] = attr\n        return attr", "category": "Python"}, {"instruction": "def is_indexed(self, identifier):\n        \"\"\" Returns True if identifier is already indexed. Otherwise returns False. \"\"\"\n", "input": "", "output": "        query = text(", "category": "Python"}, {"instruction": "def get_command_and_args(self):\n        r\"\"\"We want to get the command and the args with ! splitting.\n        but don't forget to protect against the \\! to avoid splitting on them\n\n        Remember: A Nagios-like command is command_name!arg1!arg2!...\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        # First protect\n        p_call = self.call.replace(r'\\!', '___PROTECT_EXCLAMATION___')\n        tab = p_call.split('!')\n        return tab[0].strip(), [s.replace('___PROTECT_EXCLAMATION___', '!') for s in tab[1:]]", "category": "Python"}, {"instruction": "def _apply_incoming_copying_manipulators(self, son, collection):\n        \"\"\"Apply incoming copying manipulators to `son`.\"\"\"\n", "input": "", "output": "        for manipulator in self.__incoming_copying_manipulators:\n            son = manipulator.transform_incoming(son, collection)\n        return son", "category": "Python"}, {"instruction": "def log(ltype, method, page, user_agent):\n    \"\"\"Writes to the log a message in the following format::\n        \n        \"<datetime>: <exception> method <HTTP method> page <path> \\\nuser agent <user_agent>\"\n                \n\"\"\"\n", "input": "", "output": "\n    try:\n        f = open(settings.DJANGOSPAM_LOG, \"a\")\n        f.write(\"%s: %s method %s page %s user agent %s\\n\" % \\\n                (datetime.datetime.now(), ltype, method, page, user_agent))\n        f.close()\n    except:\n        if settings.DJANGOSPAM_FAIL_ON_LOG:\n            exc_type, exc_value = sys.exc_info()[:2]\n            raise LogError(exc_type, exc_value)", "category": "Python"}, {"instruction": "def add_item(c, name, item):\n        \"\"\"\n        add_item adds MenuItems to the menu identified by 'name'\n        \"\"\"\n", "input": "", "output": "        if isinstance(item, MenuItem):\n            if name not in c.items:\n                c.items[name] = []\n            c.items[name].append(item)\n            c.sorted[name] = False", "category": "Python"}, {"instruction": "def show_vpnservice(self, vpnservice, **kwargs):\n        '''\n        Fetches information of a specific VPN service\n        '''\n", "input": "", "output": "        vpnservice_id = self._find_vpnservice_id(vpnservice)\n        return self.network_conn.show_vpnservice(vpnservice_id, **kwargs)", "category": "Python"}, {"instruction": "def min_height(self) -> int:\n        \"\"\"Minimum height necessary to render the block's contents.\"\"\"\n", "input": "", "output": "        return max(\n            len(self.content.split('\\n')) if self.content else 0,\n            # Only vertical lines can cross 0 height blocks.\n            int(any([self.left, self.right]))\n        )", "category": "Python"}, {"instruction": "def _population_load_script(work_bams, names, chrom, pairmode, items):\n    \"\"\"Prepare BAMs for assessing CNVs in a population.\n    \"\"\"\n", "input": "", "output": "    bed_file = _get_regional_bed_file(items[0])\n    if bed_file:\n        return _population_prep_targeted.format(bam_file_str=\",\".join(work_bams), names_str=\",\".join(names),\n                                                chrom=chrom, num_cores=0, pairmode=pairmode, bed_file=bed_file)\n    else:\n        return _population_prep.format(bam_file_str=\",\".join(work_bams), names_str=\",\".join(names),\n                                       chrom=chrom, num_cores=0, pairmode=pairmode)", "category": "Python"}, {"instruction": "def associar_assinatura(self, sequencia_cnpj, assinatura_ac):\n        \"\"\"Sobrep\u00f5e :meth:`~satcfe.base.FuncoesSAT.associar_assinatura`.\n\n        :return: Uma resposta SAT padr\u00e3o.\n        :rtype: satcfe.resposta.padrao.RespostaSAT\n        \"\"\"\n", "input": "", "output": "        resp = self._http_post('associarassinatura',\n                sequencia_cnpj=sequencia_cnpj, assinatura_ac=assinatura_ac)\n        # (!) resposta baseada na reda\u00e7\u00e3o com efeitos at\u00e9 31-12-2016\n        conteudo = resp.json()\n        return RespostaSAT.associar_assinatura(conteudo.get('retorno'))", "category": "Python"}, {"instruction": "def populate_username(self, request, user):\n        \"\"\"\n        Fills in a valid username, if required and missing.  If the\n        username is already present it is assumed to be valid\n        (unique).\n        \"\"\"\n", "input": "", "output": "        from .utils import user_username, user_email, user_field\n        first_name = user_field(user, 'first_name')\n        last_name = user_field(user, 'last_name')\n        email = user_email(user)\n        username = user_username(user)\n        if app_settings.USER_MODEL_USERNAME_FIELD:\n            user_username(\n                user,\n                username or self.generate_unique_username([\n                    first_name,\n                    last_name,\n                    email,\n                    username,\n                    'user']))", "category": "Python"}, {"instruction": "def on_touch_move(self, touch):\n        \"\"\"If an entity is selected, drag it.\"\"\"\n", "input": "", "output": "        if hasattr(self, '_lasttouch') and self._lasttouch == touch:\n            return\n        if self.app.selection in self.selection_candidates:\n            self.selection_candidates.remove(self.app.selection)\n        if self.app.selection:\n            if not self.selection_candidates:\n                self.keep_selection = True\n            ret = super().on_touch_move(touch)\n            return ret\n        elif self.selection_candidates:\n            for cand in self.selection_candidates:\n                if cand.collide_point(*touch.pos):\n                    self.app.selection = cand\n                    cand.selected = True\n                    touch.grab(cand)\n                    ret = super().on_touch_move(touch)\n                    return ret", "category": "Python"}, {"instruction": "def validate_reaction(self):\n        \"\"\"Ensure reaction is of a certain type.\n\n        Mainly for future expansion.\n        \"\"\"\n", "input": "", "output": "        if self.reaction not in self._reaction_valid_values:\n            raise ValueError(\"reaction should be one of: {valid}\".format(\n                valid=\", \".join(self._reaction_valid_values)\n            ))", "category": "Python"}, {"instruction": "def runListPeers(self, request):\n        \"\"\"\n        Takes a ListPeersRequest and returns a ListPeersResponse using\n        a page_token and page_size if provided.\n        \"\"\"\n", "input": "", "output": "        return self.runSearchRequest(\n            request,\n            protocol.ListPeersRequest,\n            protocol.ListPeersResponse,\n            self.peersGenerator)", "category": "Python"}, {"instruction": "def get_indices(s: Union[str, 'ChainedBase']) -> Dict[int, str]:\n    \"\"\" Retrieve a dict of characters and escape codes with their real index\n        into the string as the key.\n    \"\"\"\n", "input": "", "output": "    codes = get_code_indices(s)\n    if not codes:\n        # This function is not for non-escape-code stuff, but okay.\n        return {i: c for i, c in enumerate(s)}\n\n    indices = {}\n    for codeindex in sorted(codes):\n        code = codes[codeindex]\n        if codeindex == 0:\n            indices[codeindex] = code\n            continue\n        # Grab characters before codeindex.\n        start = max(indices or {0: ''}, key=int)\n        startcode = indices.get(start, '')\n        startlen = start + len(startcode)\n        indices.update({i: s[i] for i in range(startlen, codeindex)})\n        indices[codeindex] = code\n\n    if not indices:\n        return {i: c for i, c in enumerate(s)}\n    lastindex = max(indices, key=int)\n    lastitem = indices[lastindex]\n    start = lastindex + len(lastitem)\n    textlen = len(s)\n    if start < (textlen - 1):\n        # Grab chars after last code.\n        indices.update({i: s[i] for i in range(start, textlen)})\n    return indices", "category": "Python"}, {"instruction": "def pop(self, instance):\r\n        '''Remove ``instance`` from the :class:`SessionModel`. Instance\r\ncould be a :class:`Model` or an id.\r\n\r\n:parameter instance: a :class:`Model` or an ``id``.\r\n:rtype: the :class:`Model` removed from session or ``None`` if\r\n    it was not in the session.\r\n'''\n", "input": "", "output": "        if isinstance(instance, self.model):\r\n            iid = instance.get_state().iid\r\n        else:\r\n            iid = instance\r\n        instance = None\r\n        for d in (self._new, self._modified, self._deleted):\r\n            if iid in d:\r\n                inst = d.pop(iid)\r\n                if instance is None:\r\n                    instance = inst\r\n                elif inst is not instance:\r\n                    raise ValueError('Critical error: %s is duplicated' % iid)\r\n        return instance", "category": "Python"}, {"instruction": "def unload(self, key):\n        \"\"\"\n        Unload a loaded key and its subkeys.\n\n        The easiest way to do this is to select a key using :py:meth:`PGPKeyring.key` first::\n\n            with keyring.key(\"DSA von TestKey\") as key:\n                keyring.unload(key)\n\n        :param key: The key to unload.\n        :type key: :py:obj:`PGPKey`\n        \"\"\"\n", "input": "", "output": "        assert isinstance(key, PGPKey)\n        pkid = id(key)\n        if pkid in self._keys:\n            # remove references\n            [ kd.remove(pkid) for kd in [self._pubkeys, self._privkeys] if pkid in kd ]\n            # remove the key\n            self._keys.pop(pkid)\n\n            # remove aliases\n            for m, a in [ (m, a) for m in self._aliases for a, p in m.items() if p == pkid ]:\n                m.pop(a)\n                # do a re-sort of this alias if it was not unique\n                if a in self:\n                    self._sort_alias(a)\n\n            # if key is a primary key, unload its subkeys as well\n            if key.is_primary:\n                [ self.unload(sk) for sk in key.subkeys.values() ]", "category": "Python"}, {"instruction": "def _extract_psf_fitting_names(psf):\n    \"\"\"\n    Determine the names of the x coordinate, y coordinate, and flux from\n    a model.  Returns (xname, yname, fluxname)\n    \"\"\"\n", "input": "", "output": "\n    if hasattr(psf, 'xname'):\n        xname = psf.xname\n    elif 'x_0' in psf.param_names:\n        xname = 'x_0'\n    else:\n        raise ValueError('Could not determine x coordinate name for '\n                         'psf_photometry.')\n\n    if hasattr(psf, 'yname'):\n        yname = psf.yname\n    elif 'y_0' in psf.param_names:\n        yname = 'y_0'\n    else:\n        raise ValueError('Could not determine y coordinate name for '\n                         'psf_photometry.')\n\n    if hasattr(psf, 'fluxname'):\n        fluxname = psf.fluxname\n    elif 'flux' in psf.param_names:\n        fluxname = 'flux'\n    else:\n        raise ValueError('Could not determine flux name for psf_photometry.')\n\n    return xname, yname, fluxname", "category": "Python"}, {"instruction": "def _load_rsp(rsp):\n        \"\"\"\n        Converts raw Flickr string response to Python dict\n        \"\"\"\n", "input": "", "output": "        first = rsp.find('(') + 1\n        last = rsp.rfind(')')\n        return json.loads(rsp[first:last])", "category": "Python"}, {"instruction": "def Deserialize(self, reader):\n        \"\"\"\n        Deserialize full object.\n\n        Args:\n            reader (neo.IO.BinaryReader):\n        \"\"\"\n", "input": "", "output": "        self.AssetId = reader.ReadUInt256()\n        self.Value = reader.ReadFixed8()\n        self.ScriptHash = reader.ReadUInt160()\n        if self.ScriptHash is None:\n            raise Exception(\"Script hash is required from deserialize!!!!!!!!\")", "category": "Python"}, {"instruction": "def _CollectTypeChecks(function, parent_type_check_dict, stack_location,\n                      self_name):\n  \"\"\"Collect all type checks for this function.\"\"\"\n", "input": "", "output": "  type_check_dict = dict(parent_type_check_dict)\n  type_check_dict.update(_ParseDocstring(function))\n\n  # Convert any potential string based checks into python instances.\n  for key, value in type_check_dict.items():\n    if isinstance(value, str):\n      type_check_dict[key] = _ParseTypeCheckString(value, stack_location + 1,\n                                                  self_name)\n\n  return type_check_dict", "category": "Python"}, {"instruction": "def parseFragment(self, stream, *args, **kwargs):\n        \"\"\"Parse a HTML fragment into a well-formed tree fragment\n\n        :arg container: name of the element we're setting the innerHTML\n            property if set to None, default to 'div'\n\n        :arg stream: a file-like object or string containing the HTML to be parsed\n\n            The optional encoding parameter must be a string that indicates\n            the encoding.  If specified, that encoding will be used,\n            regardless of any BOM or later declaration (such as in a meta\n            element)\n\n        :arg scripting: treat noscript elements as if JavaScript was turned on\n\n        :returns: parsed tree\n\n        Example:\n\n        >>> from html5lib.html5libparser import HTMLParser\n        >>> parser = HTMLParser()\n        >>> parser.parseFragment('<b>this is a fragment</b>')\n        <Element u'DOCUMENT_FRAGMENT' at 0x7feac484b090>\n\n        \"\"\"\n", "input": "", "output": "        self._parse(stream, True, *args, **kwargs)\n        return self.tree.getFragment()", "category": "Python"}, {"instruction": "def set_temperature(self, temp):\n        \"\"\"Set both the driver and passenger temperature to temp.\"\"\"\n", "input": "", "output": "        temp = round(temp, 1)\n        self.__manual_update_time = time.time()\n        data = self._controller.command(self._id, 'set_temps',\n                                        {\"driver_temp\": temp,\n                                         \"passenger_temp\": temp},\n                                        wake_if_asleep=True)\n        if data['response']['result']:\n            self.__driver_temp_setting = temp\n            self.__passenger_temp_setting = temp", "category": "Python"}, {"instruction": "def _QueryHash(self, digest):\n    \"\"\"Queries the Viper Server for a specfic hash.\n\n    Args:\n      digest (str): hash to look up.\n\n    Returns:\n      dict[str, object]: JSON response or None on error.\n    \"\"\"\n", "input": "", "output": "    if not self._url:\n      self._url = '{0:s}://{1:s}:{2:d}/file/find'.format(\n          self._protocol, self._host, self._port)\n\n    request_data = {self.lookup_hash: digest}\n\n    try:\n      json_response = self.MakeRequestAndDecodeJSON(\n          self._url, 'POST', data=request_data)\n\n    except errors.ConnectionError as exception:\n      json_response = None\n      logger.error('Unable to query Viper with error: {0!s}.'.format(\n          exception))\n\n    return json_response", "category": "Python"}, {"instruction": "def drop_table(self, table_name, database=None, force=False):\n        \"\"\"\n        Drop an Impala table\n\n        Parameters\n        ----------\n        table_name : string\n        database : string, default None (optional)\n        force : boolean, default False\n          Database may throw exception if table does not exist\n\n        Examples\n        --------\n        >>> table = 'my_table'\n        >>> db = 'operations'\n        >>> con.drop_table(table, database=db, force=True)  # doctest: +SKIP\n        \"\"\"\n", "input": "", "output": "        statement = ddl.DropTable(\n            table_name, database=database, must_exist=not force\n        )\n        self._execute(statement)", "category": "Python"}, {"instruction": "def classname(ob):\n    \"\"\"Get the object's class's name as package.module.Class\"\"\"\n", "input": "", "output": "    import inspect\n    if inspect.isclass(ob):\n        return '.'.join([ob.__module__, ob.__name__])\n    else:\n        return '.'.join([ob.__class__.__module__, ob.__class__.__name__])", "category": "Python"}, {"instruction": "def delete_user(self, auth, username):\n        \"\"\"\n        Deletes the user with username ``username``. Should only be called if the\n        to-be-deleted user has no repositories.\n\n        :param auth.Authentication auth: authentication object, must be admin-level\n        :param str username: username of user to delete\n        \"\"\"\n", "input": "", "output": "        path = \"/admin/users/{}\".format(username)\n        self.delete(path, auth=auth)", "category": "Python"}, {"instruction": "def piece(piece: chess.Piece, size: Optional[int] = None) -> str:\n    \"\"\"\n    Renders the given :class:`chess.Piece` as an SVG image.\n\n    >>> import chess\n    >>> import chess.svg\n    >>>\n    >>> chess.svg.piece(chess.Piece.from_symbol(\"R\"))  # doctest: +SKIP\n\n    .. image:: ../docs/wR.svg\n    \"\"\"\n", "input": "", "output": "    svg = _svg(SQUARE_SIZE, size)\n    svg.append(ET.fromstring(PIECES[piece.symbol()]))\n    return SvgWrapper(ET.tostring(svg).decode(\"utf-8\"))", "category": "Python"}, {"instruction": "def detectAndroid(self):\n        \"\"\"Return detection of an Android device\n\n        Detects *any* Android OS-based device: phone, tablet, and multi-media player.\n        Also detects Google TV.\n        \"\"\"\n", "input": "", "output": "        if UAgentInfo.deviceAndroid in self.__userAgent \\\n           or self.detectGoogleTV():\n            return True\n\n        return False", "category": "Python"}, {"instruction": "def hostcmd_push(base_path, project_name, engine_name, vars_files=None, config_file=None, **kwargs):\n    \"\"\"\n    Push images to a registry. Requires authenticating with the registry prior to starting\n    the push. If your engine's config file does not already contain an authorization for the\n    registry, pass username and/or password. If you exclude password, you will be prompted.\n    \"\"\"\n", "input": "", "output": "    assert_initialized(base_path, config_file)\n    config = get_config(base_path, vars_files=vars_files, engine_name=engine_name, project_name=project_name,\n                        config_file=config_file)\n\n    engine_obj = load_engine(['LOGIN', 'PUSH'],\n                             engine_name, config.project_name,\n                             config['services'], **kwargs)\n    logger.debug('PROJECT NAME', project_name=config.project_name)\n    push_images(base_path,\n                config.image_namespace,\n                engine_obj,\n                config,\n                save_conductor=config.save_conductor,\n                **kwargs)", "category": "Python"}, {"instruction": "def computeCovarianceMatrixPlink(plink_path,out_dir,bfile,cfile,sim_type='RRM'):\n    \"\"\"\n    computing the covariance matrix via plink\n    \"\"\"\n", "input": "", "output": "\n    print(\"Using plink to create covariance matrix\")\n    cmd = '%s --bfile %s '%(plink_path,bfile)\n\n    if sim_type=='RRM':\n        # using variance standardization\n        cmd += '--make-rel square '\n    else:\n        raise Exception('sim_type %s is not known'%sim_type)\n\n    cmd+= '--out %s'%(os.path.join(out_dir,'plink'))\n\n    subprocess.call(cmd,shell=True)\n\n    # move file to specified file\n    if sim_type=='RRM':\n        old_fn = os.path.join(out_dir, 'plink.rel')\n        os.rename(old_fn,cfile+'.cov')\n\n        old_fn = os.path.join(out_dir, 'plink.rel.id')\n        os.rename(old_fn,cfile+'.cov.id')\n\n    if sim_type=='IBS':\n        old_fn = os.path.join(out_dir, 'plink.mibs')\n        os.rename(old_fn,cfile+'.cov')\n\n        old_fn = os.path.join(out_dir, 'plink.mibs.id')\n        os.rename(old_fn,cfile+'.cov.id')", "category": "Python"}, {"instruction": "def calcSMA(self):\n        \"\"\" Calculates the semi-major axis from Keplers Third Law\n        \"\"\"\n", "input": "", "output": "        try:\n            return eq.KeplersThirdLaw(None, self.star.M, self.P).a\n        except HierarchyError:\n            return np.nan", "category": "Python"}, {"instruction": "def set_slats_level(self, slatsLevel=0.0, shutterLevel=None):\n        \"\"\" sets the slats and shutter level\n\n        Args:\n            slatsLevel(float): the new level of the slats. 0.0 = open, 1.0 = closed,\n            shutterLevel(float): the new level of the shutter. 0.0 = open, 1.0 = closed, None = use the current value\n        Returns:\n            the result of the _restCall\n        \"\"\"\n", "input": "", "output": "        if shutterLevel is None:\n            shutterLevel = self.shutterLevel\n        data = {\n            \"channelIndex\": 1,\n            \"deviceId\": self.id,\n            \"slatsLevel\": slatsLevel,\n            \"shutterLevel\": shutterLevel,\n        }\n        return self._restCall(\"device/control/setSlatsLevel\", json.dumps(data))", "category": "Python"}, {"instruction": "def updateWCS(drizwcs,inwcs):\n    \"\"\" Copy output WCS array from Drizzle into WCSObject.\"\"\"\n", "input": "", "output": "    crpix = np.array([drizwcs[0],drizwcs[2]], dtype=np.float64)\n    crval = np.array([drizwcs[1],drizwcs[3]], dtype=np.float64)\n    cd = np.array([[drizwcs[4],drizwcs[6]],[drizwcs[5],drizwcs[7]]], dtype=np.float64)\n    inwcs.cd = cd\n    inwcs.crval = crval\n    inwc.crpix = crpix\n    inwcs.pscale = N.sqrt(N.power(inwcs.cd[0][0],2)+N.power(inwcs.cd[1][0],2)) * 3600.\n    inwcs.orient = N.arctan2(inwcs.cd[0][1],inwcs.cd[1][1]) * 180./N.pi", "category": "Python"}, {"instruction": "def _len_set(obj):\n    '''Length of frozen/set (estimate).\n    '''\n", "input": "", "output": "    n = len(obj)\n    if n > 8:  # assume half filled\n       n = _power2(n + n - 2)\n    elif n:  # at least 8\n       n = 8\n    return n", "category": "Python"}, {"instruction": "def min_pos(self):\n        '''Returns minimal positive value or None.'''\n", "input": "", "output": "        if self.__len__() == 0:\n            return ArgumentError('empty set has no minimum positive value.')\n        if self.contains(0):\n            return None\n        positive = [interval for interval in self.intervals\n                    if interval.left > 0]\n        if len(positive) == 0:\n            return None\n        return numpy.min(list(map(lambda i: i.left, positive)))", "category": "Python"}, {"instruction": "def addons(cls, recurse=True):\n        \"\"\"\n        Returns a dictionary containing all the available addons\n        for this mixin class.  If the optional recurse flag is set to True,\n        then all the base classes will be searched for the given addon as well.\n        \n        :param      recurse | <bool>\n        \n        :return     {<str> name: <variant> addon, ..}\n        \"\"\"\n", "input": "", "output": "        cls.initAddons()\n        prop = '_{0}__addons'.format(cls.__name__)\n        out = {}\n\n        # lookup base classes\n        if recurse:\n            for base in cls.__bases__:\n                if issubclass(base, AddonManager):\n                    out.update(base.addons(recurse))\n\n        # always use the highest level for any given key\n        out.update(getattr(cls, prop, {}))\n        return out", "category": "Python"}, {"instruction": "def wkt_polygon(value):\n    \"\"\"\n    Convert a string with a comma separated list of coordinates into\n    a WKT polygon, by closing the ring.\n    \"\"\"\n", "input": "", "output": "    points = ['%s %s' % (lon, lat) for lon, lat, dep in coordinates(value)]\n    # close the linear polygon ring by appending the first coord to the end\n    points.append(points[0])\n    return 'POLYGON((%s))' % ', '.join(points)", "category": "Python"}, {"instruction": "def check_string(value, min_length=None, max_length=None, pattern=None):\n    \"\"\"\n    verify that a string has a particular size and conforms\n    to a particular alphabet\n\n    >>> check_string(1)\n    False\n    >>> check_string(None)\n    False\n    >>> check_string(True)\n    False\n    >>> check_string({})\n    False\n    >>> check_string([])\n    False\n    >>> check_string((1,2))\n    False\n    >>> check_string('abc')\n    True\n    >>> check_string('')\n    True\n    >>> check_string(u'')\n    True\n    >>> check_string('abc', min_length=0, max_length=3)\n    True\n    >>> check_string('abc', min_length=3, max_length=3)\n    True\n    >>> check_string('abc', min_length=4, max_length=5)\n    False\n    >>> check_string('abc', min_length=0, max_length=2)\n    False\n    >>> check_string('abc', pattern='^abc$')\n    True\n    >>> check_string('abc', pattern='^abd$')\n    False\n    \"\"\"\n", "input": "", "output": "    if type(value) not in [str, unicode]:\n        return False\n\n    if min_length and len(value) < min_length:\n        return False\n\n    if max_length and len(value) > max_length:\n        return False\n\n    if pattern and not re.match(pattern, value):\n        return False\n\n    return True", "category": "Python"}, {"instruction": "def overlapping(self, start, stop):\n        \"\"\"Iterates (in chronological order) over every event that has an intersection\n        with the timespan between `start` and `stop`\n\n        Args:\n            start : (Arrow object)\n            stop : (Arrow object)\n        \"\"\"\n", "input": "", "output": "        for event in self:\n            if ((start <= event.begin <= stop # if start is between the bonds\n            or start <= event.end <= stop) # or stop is between the bonds\n            or event.begin <= start and event.end >= stop): # or event is a superset of [start,stop]\n                yield event", "category": "Python"}, {"instruction": "def get(self, path, default=None):\n        \"\"\"\n        Returns given path value.\n\n        :param path: Path name.\n        :type path: unicode\n        :param default: Default value if path is not found.\n        :type default: object\n        :return: Action.\n        :rtype: QAction\n        \"\"\"\n", "input": "", "output": "\n        try:\n            return self.__getitem__(path)\n        except KeyError as error:\n            return default", "category": "Python"}, {"instruction": "def freqpoly_plot(data):\n        \"\"\"make freqpoly plot of merged read lengths\"\"\"\n", "input": "", "output": "        rel_data = OrderedDict()\n        for key, val in data.items():\n            tot = sum(val.values(), 0)\n            rel_data[key] = {k: v / tot for k, v in val.items()}\n        fplotconfig = {\n            'data_labels': [\n                {'name': 'Absolute', 'ylab': 'Frequency', 'xlab': 'Merged Read Length'},\n                {'name': 'Relative', 'ylab': 'Relative Frequency', 'xlab': 'Merged Read Length'}\n                ],\n            'id': 'flash_freqpoly_plot', 'title': 'FLASh: Frequency of merged read lengths',\n            'colors': dict(zip(data.keys(), MultiqcModule.get_colors(len(data))))\n            }\n        return linegraph.plot([data, rel_data], fplotconfig)", "category": "Python"}, {"instruction": "def __protocolize(base_url):\n        \"\"\"Internal add-protocol-to-url helper\"\"\"\n", "input": "", "output": "        if not base_url.startswith(\"http://\") and not base_url.startswith(\"https://\"):\n            base_url = \"https://\" + base_url\n\n        # Some API endpoints can't handle extra /'s in path requests\n        base_url = base_url.rstrip(\"/\")\n        return base_url", "category": "Python"}, {"instruction": "def remove_feature_flag_courses(self, feature, course_id):\r\n        \"\"\"\r\n        Remove feature flag.\r\n\r\n        Remove feature flag for a given Account, Course, or User.  (Note that the flag must\r\n        be defined on the Account, Course, or User directly.)  The object will then inherit\r\n        the feature flags from a higher account, if any exist.  If this flag was 'on' or 'off',\r\n        then lower-level account flags that were masked by this one will apply again.\r\n        \"\"\"\n", "input": "", "output": "        path = {}\r\n        data = {}\r\n        params = {}\r\n\r\n        # REQUIRED - PATH - course_id\r\n        ", "category": "Python"}, {"instruction": "def cmd_version(unused_conf):\n    \"\"\"Print out version information about YABT and detected builders.\"\"\"\n", "input": "", "output": "    import pkg_resources\n    print('This is {} version {}, imported from {}'\n          .format(__oneliner__, __version__, __file__))\n    if len(Plugin.builders) > 0:\n        print('setuptools registered builders:')\n    for entry_point in pkg_resources.iter_entry_points('yabt.builders'):\n        print('  {0.module_name}.{0.name} (dist {0.dist})'.format(entry_point))", "category": "Python"}, {"instruction": "def destroy_status(self):\n        \"\"\" :reference: https://developer.twitter.com/en/docs/tweets/post-and-engage/api-reference/post-statuses-destroy-id\n            :allowed_param:'id'\n        \"\"\"\n", "input": "", "output": "        return bind_api(\n            api=self,\n            path='/statuses/destroy/{id}.json',\n            method='POST',\n            payload_type='status',\n            allowed_param=['id'],\n            require_auth=True\n        )", "category": "Python"}, {"instruction": "def check(self, file):\n        \"\"\"\n        Checks a given file against all available yara rules\n\n        :param file: Path to file\n        :type file:str\n        :returns: Python dictionary containing the results\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        result = []\n        for rule in self.ruleset:\n            matches = rule.match(file)\n            for match in matches:\n                result.append(str(match))\n\n        return result", "category": "Python"}, {"instruction": "def use_plenary_objective_view(self):\n        \"\"\"Pass through to provider ObjectiveLookupSession.use_plenary_objective_view\"\"\"\n", "input": "", "output": "        self._object_views['objective'] = PLENARY\n        # self._get_provider_session('objective_lookup_session') # To make sure the session is tracked\n        for session in self._get_provider_sessions():\n            try:\n                session.use_plenary_objective_view()\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def load_design_docs():\n    \"\"\"\n    Load design docs for registered views\n    \"\"\"\n", "input": "", "output": "    url = ':'.join([options.url_registry_db, str(options.db_port)])\n    client = partial(couch.BlockingCouch, couch_url=url)\n\n    for name, docs in _views.items():\n        db = client(db_name=name)\n        views = []\n\n        for doc in docs:\n            try:\n                current_doc = db.get_doc(doc['_id'])\n\n                # use the current _rev if not provided\n                if '_rev' not in doc:\n                    doc['_rev'] = current_doc['_rev']\n            except couch.NotFound:\n                pass\n\n            views.append(doc)\n\n        db.save_docs(views)", "category": "Python"}, {"instruction": "def argdistincts(self, nested=False):\n        \"\"\"\n        Return all unique combinations (up to permutation) of two elements,\n        taken without replacement from the indices of the jagged dimension.\n        Combinations are ordered lexicographically.\n        nested: Return a doubly-jagged array where the first jagged dimension\n        matches the shape of this array\n        \"\"\"\n", "input": "", "output": "        out = self._argdistincts(absolute=False)\n\n        if nested:\n            out = self.JaggedArray.fromcounts(self.numpy.maximum(0, self.counts - 1), self.JaggedArray.fromcounts(self.index[:, :0:-1].flatten(), out._content))\n\n        return out", "category": "Python"}, {"instruction": "def moments(data,cntr):\n    \"\"\"\n    Returns (height, x, y, width_x, width_y)\n    the gaussian parameters of a 2D distribution by calculating its\n    moments.\n\n    \"\"\"\n", "input": "", "output": "    total = data.sum()\n    #X, Y = np.indices(data.shape)\n    #x = (X*data).sum()/total\n    #y = (Y*data).sum()/total\n    x,y = cntr\n    xi = int(x)\n    yi = int(y)\n    if xi < 0 or xi >= data.shape[1] or yi < 0 or yi >= data.shape[0]:\n        raise ValueError\n    col = data[:, xi]\n    width_x = np.sqrt(abs(((np.arange(col.size)-y)**2*col).sum()/col.sum()))\n    row = data[yi, :]\n    width_y = np.sqrt(abs(((np.arange(row.size)-x)**2*row).sum()/row.sum()))\n    height = data.max()\n    return height, x, y, width_x, width_y", "category": "Python"}, {"instruction": "def pushdown_not(self):\n        \"\"\"Return an expression with NOT operators pushed down thru dual ops.\n\n        Specifically, perform the following transformations:\n            ~(a | b | c ...) <=> ~a & ~b & ~c ...\n            ~(a & b & c ...) <=> ~a | ~b | ~c ...\n            ~(s ? d1 : d0) <=> s ? ~d1 : ~d0\n        \"\"\"\n", "input": "", "output": "        node = self.node.pushdown_not()\n        if node is self.node:\n            return self\n        else:\n            return _expr(node)", "category": "Python"}, {"instruction": "def rpc_method(func, doc=None, format='json', request_handler=None):\n    '''A decorator which exposes a function ``func`` as an rpc function.\n\n    :param func: The function to expose.\n    :param doc: Optional doc string. If not provided the doc string of\n        ``func`` will be used.\n    :param format: Optional output format.\n    :param request_handler: function which takes ``request``, ``format``\n        and ``kwargs`` and return a new ``kwargs`` to be passed to ``func``.\n        It can be used to add additional parameters based on request and\n        format.\n    '''\n", "input": "", "output": "    def _(self, *args, **kwargs):\n        request = args[0]\n        if request_handler:\n            kwargs = request_handler(request, format, kwargs)\n        try:\n            return func(*args, **kwargs)\n        except TypeError:\n            msg = checkarity(func, args, kwargs)\n            if msg:\n                raise InvalidParams('Invalid Parameters. %s' % msg)\n            else:\n                raise\n\n    _.__doc__ = doc or func.__doc__\n    _.__name__ = func.__name__\n    _.FromApi = True\n    return _", "category": "Python"}, {"instruction": "def has_successor(self, u, v, t=None):\n        \"\"\"Return True if node u has successor v at time t (optional).\n\n        This is true if graph has the edge u->v.\n\n        Parameters\n        ----------\n        u, v : nodes\n            Nodes can be, for example, strings or numbers.\n            Nodes must be hashable (and not None) Python objects.\n        t : snapshot id (default=None)\n            If None will be returned the presence of the interaction on the flattened graph.\n\n        \"\"\"\n", "input": "", "output": "        return self.has_interaction(u, v, t)", "category": "Python"}, {"instruction": "def calcThermo(seq1, seq2, calc_type='ANY', mv_conc=50, dv_conc=0,\n                 dntp_conc=0.8, dna_conc=50, temp_c=37, max_loop=30,\n                 temp_only=False):\n    \"\"\" Main subprocess wrapper for calls to the ntthal executable.\n\n    Returns a named tuple with tm, ds, dh, and dg values or None if no\n    structure / complex could be computed.\n    \"\"\"\n", "input": "", "output": "    args = [pjoin(PRIMER3_HOME, 'ntthal'),\n            '-a',       str(calc_type),\n            '-mv',      str(mv_conc),\n            '-dv',      str(dv_conc),\n            '-n',       str(dntp_conc),\n            '-d',       str(dna_conc),\n            '-t',       str(temp_c),\n            '-maxloop', str(max_loop),\n            '-path',    THERMO_PATH,\n            '-s1',      seq1,\n            '-s2',      seq2]\n    if temp_only:\n        args += ['-r']\n    out = subprocess.check_output(args, stderr=DEV_NULL,\n                                  env=os.environ)\n    return _parse_ntthal(out)", "category": "Python"}, {"instruction": "def rename(self, old_name, new_name):\n        \"\"\"Rename key to a new name.\"\"\"\n", "input": "", "output": "        try:\n            self.api.rename(mkey(old_name), mkey(new_name))\n        except ResponseError, exc:\n            if \"no such key\" in exc.args:\n                raise KeyError(old_name)\n            raise", "category": "Python"}, {"instruction": "def insert_into_last_element(html, element):\n    \"\"\"\n    function to insert an html element into another html fragment\n    example:\n        html = '<p>paragraph1</p><p>paragraph2...</p>'\n        element = '<a href=\"/read-more/\">read more</a>'\n        ---> '<p>paragraph1</p><p>paragraph2...<a href=\"/read-more/\">read more</a></p>'\n    \"\"\"\n", "input": "", "output": "    try:\n        item = fragment_fromstring(element)\n    except (ParserError, TypeError) as e:\n        item = fragment_fromstring('<span></span>')\n\n    try:\n        doc = fragments_fromstring(html)\n        doc[-1].append(item)\n\n        return ''.join(tostring(e) for e in doc)\n    except (ParserError, TypeError) as e:\n        return ''", "category": "Python"}, {"instruction": "def enviar_dados_venda(self, dados_venda):\n        \"\"\"Sobrep\u00f5e :meth:`~satcfe.base.FuncoesSAT.enviar_dados_venda`.\n\n        :return: Uma resposta SAT especializada em ``EnviarDadosVenda``.\n        :rtype: satcfe.resposta.enviardadosvenda.RespostaEnviarDadosVenda\n        \"\"\"\n", "input": "", "output": "        resp = self._http_post('enviardadosvenda',\n                dados_venda=dados_venda.documento())\n        conteudo = resp.json()\n        return RespostaEnviarDadosVenda.analisar(conteudo.get('retorno'))", "category": "Python"}, {"instruction": "def nuc_p(msg):\n    \"\"\"Calculate NUCp, Navigation Uncertainty Category - Position (ADS-B version 1)\n\n    Args:\n        msg (string): 28 bytes hexadecimal message string,\n\n    Returns:\n        int: Horizontal Protection Limit\n        int: 95% Containment Radius - Horizontal (meters)\n        int: 95% Containment Radius - Vertical (meters)\n\n    \"\"\"\n", "input": "", "output": "    tc = typecode(msg)\n\n    if typecode(msg) < 5 or typecode(msg) > 22:\n        raise RuntimeError(\n            \"%s: Not a surface position message (5<TC<8), \\\n            airborne position message (8<TC<19), \\\n            or airborne position with GNSS height (20<TC<22)\" % msg\n        )\n\n    try:\n        NUCp = uncertainty.TC_NUCp_lookup[tc]\n        HPL = uncertainty.NUCp[NUCp]['HPL']\n        RCu = uncertainty.NUCp[NUCp]['RCu']\n        RCv = uncertainty.NUCp[NUCp]['RCv']\n    except KeyError:\n        HPL, RCu, RCv = uncertainty.NA, uncertainty.NA, uncertainty.NA\n\n\n    if tc in [20, 21]:\n        RCv = uncertainty.NA\n\n    return HPL, RCu, RCv", "category": "Python"}, {"instruction": "def process_priority(self, process_priority):\n        \"\"\"\n        Sets the process priority.\n\n        :param process_priority: string\n        \"\"\"\n", "input": "", "output": "\n        log.info('QEMU VM \"{name}\" [{id}] has set the process priority to {priority}'.format(name=self._name,\n                                                                                             id=self._id,\n                                                                                             priority=process_priority))\n        self._process_priority = process_priority", "category": "Python"}, {"instruction": "def exact(self, *args, **kwargs):\n        \"\"\"Compare attributes of pairs exactly.\n\n        Shortcut of :class:`recordlinkage.compare.Exact`::\n\n            from recordlinkage.compare import Exact\n\n            indexer = recordlinkage.Compare()\n            indexer.add(Exact())\n\n        \"\"\"\n", "input": "", "output": "        compare = Exact(*args, **kwargs)\n        self.add(compare)\n\n        return self", "category": "Python"}, {"instruction": "def cli(env, sortby, columns, datacenter, username, storage_type):\n    \"\"\"List block storage.\"\"\"\n", "input": "", "output": "    block_manager = SoftLayer.BlockStorageManager(env.client)\n    block_volumes = block_manager.list_block_volumes(datacenter=datacenter,\n                                                     username=username,\n                                                     storage_type=storage_type,\n                                                     mask=columns.mask())\n\n    table = formatting.Table(columns.columns)\n    table.sortby = sortby\n\n    for block_volume in block_volumes:\n        table.add_row([value or formatting.blank()\n                       for value in columns.row(block_volume)])\n\n    env.fout(table)", "category": "Python"}, {"instruction": "def lessgreater(x, y):\n    \"\"\"\n    Return True if x < y or x > y and False otherwise.\n\n    This function returns False whenever x and/or y is a NaN.\n\n    \"\"\"\n", "input": "", "output": "    x = BigFloat._implicit_convert(x)\n    y = BigFloat._implicit_convert(y)\n    return mpfr.mpfr_lessgreater_p(x, y)", "category": "Python"}, {"instruction": "def example_2_compute(self):\n        \"\"\"\n        \u5b9e\u73b0\u524d\u5411\u4f20\u64ad\u7b97\u6cd5\uff0c\u51cf\u5c11\u8ba1\u7b97\u56fe\u4e2d\u8282\u70b9\u7684\u4e2a\u6570\n        \"\"\"\n", "input": "", "output": "        a = matmul(self.x, self.w1) # \u8ba1\u7b97\u795e\u7ecf\u7f51\u7edc\u7684\u7b2c\u4e00\u5c42\u7684\u8f93\u51fa\u5411\u91cf\n        y = matmul(a, self.w2) # \u4ee5\u7b2c\u4e00\u5c42\u7684\u8f93\u51fa\u4f5c\u4e3a\u7b2c\u4e8c\u5c42\u7684\u8f93\u5165\u518d\u6b21\u8ba1\u7b97\u7b2c\u4e8c\u5c42\u7684\u8f93\u51fa\n\n        sess = Session()\n        initOp = global_variables_initializer() # \u521d\u59cb\u5316\u6240\u6709\u53d8\u91cf\n        sess.run(initOp)\n        #print('\u7b2c\u4e8c\u79cd\uff1a', sess.run(y, feed_dict={self.x: [[0.7, 0.9]]}))\n        print('\u7b2c\u4e8c\u79cd\uff1a', sess.run(y, feed_dict={self.x: [[0.7, 0.9], [0.1, 0.4], [0.5, 0.8]]}))\n        sess.close()", "category": "Python"}, {"instruction": "def is_valid(self):\n        \"\"\"\n        Tests if the dependency is in a valid state\n        \"\"\"\n", "input": "", "output": "        return super(SimpleDependency, self).is_valid() or (\n            self.requirement.immediate_rebind and self._pending_ref is not None\n        )", "category": "Python"}, {"instruction": "def measurements(self, value):\n        \"\"\"The measurements property.\n        \n        Args:\n            value (hash). the property value.\n        \"\"\"\n", "input": "", "output": "        if value == self._defaults['measurements'] and 'measurements' in self._values:\n            del self._values['measurements']\n        else:\n            self._values['measurements'] = value", "category": "Python"}, {"instruction": "def view_path(self):\n        \"\"\"\n        It returns view_path as string like: 'app_name.module_mane.func_name'\n        \"\"\"\n", "input": "", "output": "        return u\"{0}.{1}.{2}\".format(self.app_name, self.module_name, self.func_name)", "category": "Python"}, {"instruction": "def merged(self):\n        '''The clean stats from all the hosts reporting to this host.'''\n", "input": "", "output": "        stats = {}\n        for topic in self.client.topics()['topics']:\n            for producer in self.client.lookup(topic)['producers']:\n                hostname = producer['broadcast_address']\n                port = producer['http_port']\n                host = '%s_%s' % (hostname, port)\n                stats[host] = nsqd.Client(\n                    'http://%s:%s/' % (hostname, port)).clean_stats()\n        return stats", "category": "Python"}, {"instruction": "def form_valid(self, form):\n        \"\"\"\n        If the request is ajax, save the form and return a json response.\n        Otherwise return super as expected.\n        \"\"\"\n", "input": "", "output": "        self.object = form.save(commit=False)\n        self.pre_save()\n        self.object.save()\n        if hasattr(form, 'save_m2m'):\n            form.save_m2m()\n        self.post_save()\n\n        if self.request.is_ajax():\n            return self.render_json_response(self.get_success_result())\n        return HttpResponseRedirect(self.get_success_url())", "category": "Python"}, {"instruction": "def input_has_value(self, field_name, value):\n    \"\"\"\n    Assert the form input with label (recommended), name or id has given value.\n    \"\"\"\n", "input": "", "output": "    text_field = find_any_field(world.browser,\n                                DATE_FIELDS + TEXT_FIELDS,\n                                field_name)\n    if text_field is False:\n        raise AssertionError(\n            \"Can not find a field named {!r}.\".format(field_name))\n\n    actual = text_field.get_attribute('value')\n    if actual != value:\n        raise AssertionError(\n            \"Field value expected to be {!r}, got {!r}.\".format(\n                value, actual))", "category": "Python"}, {"instruction": "def append(self, transitions, rows=None):\n    \"\"\"Append a batch of transitions to rows of the memory.\n\n    Args:\n      transitions: Tuple of transition quantities with batch dimension.\n      rows: Episodes to append to, defaults to all.\n\n    Returns:\n      Operation.\n    \"\"\"\n", "input": "", "output": "    rows = tf.range(self._capacity) if rows is None else rows\n    assert rows.shape.ndims == 1\n    assert_capacity = tf.assert_less(\n        rows, self._capacity,\n        message='capacity exceeded')\n    with tf.control_dependencies([assert_capacity]):\n      assert_max_length = tf.assert_less(\n          tf.gather(self._length, rows), self._max_length,\n          message='max length exceeded')\n    with tf.control_dependencies([assert_max_length]):\n      timestep = tf.gather(self._length, rows)\n      indices = tf.stack([rows, timestep], 1)\n      append_ops = tools.nested.map(\n          lambda var, val: tf.scatter_nd_update(var, indices, val),\n          self._buffers, transitions, flatten=True)\n    with tf.control_dependencies(append_ops):\n      episode_mask = tf.reduce_sum(tf.one_hot(\n          rows, self._capacity, dtype=tf.int32), 0)\n      return self._length.assign_add(episode_mask)", "category": "Python"}, {"instruction": "def auto_string_formatter(func):\n    \"\"\"\n    Decorator that handles automatic string formatting.\n\n    By converting a string argument to a function that does formatting on said\n    string.\n    \"\"\"\n", "input": "", "output": "    @wraps(func)\n    def auto_string_formatter_wrapper(function, *args, **kwargs):\n        if isinstance(function, string_types):\n            function = StringFormatter(function)\n\n        return func(function, *args, **kwargs)\n\n    return auto_string_formatter_wrapper", "category": "Python"}, {"instruction": "def schema_columns(self):\n        \"\"\"Return column informatino only from this schema\"\"\"\n", "input": "", "output": "        t = self.schema_term\n\n        columns = []\n\n\n        if t:\n            for i, c in enumerate(t.children):\n\n                if c.term_is(\"Table.Column\"):\n                    p = c.all_props\n                    p['pos'] = i\n                    p['name'] = c.value\n                    p['header'] = self._name_for_col_term(c, i)\n                    \n\n                    columns.append(p)\n\n        return columns", "category": "Python"}, {"instruction": "def _get_ssh_public_key(self):\n        \"\"\"Generate SSH public key from private key.\"\"\"\n", "input": "", "output": "        key = ipa_utils.generate_public_ssh_key(self.ssh_private_key_file)\n        return '{user}:{key} {user}'.format(\n            user=self.ssh_user,\n            key=key.decode()\n        )", "category": "Python"}, {"instruction": "def mangled_name(self):\n        \"\"\"Return the mangled name for the entity referenced by this cursor.\"\"\"\n", "input": "", "output": "        if not hasattr(self, '_mangled_name'):\n            self._mangled_name = conf.lib.clang_Cursor_getMangling(self)\n\n        return self._mangled_name", "category": "Python"}, {"instruction": "def handle(self, *args, **options):\n        \"\"\" Forward to the right sub-handler \"\"\"\n", "input": "", "output": "        if options[\"sub_command\"] == \"add\":\n            self.handle_add(options)\n        elif options[\"sub_command\"] == \"update\":\n            self.handle_update(options)\n        elif options[\"sub_command\"] == \"details\":\n            self.handle_details(options[\"username\"])\n        elif options[\"sub_command\"] == \"list\":\n            self.handle_list(options[\"all\"], options[\"csv\"])", "category": "Python"}, {"instruction": "def remove_user_from_user_groups(self, id, **kwargs):  # noqa: E501\n        \"\"\"Removes specific user groups from the user  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.remove_user_from_user_groups(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :param list[str] body: The list of user groups that should be removed from the user\n        :return: UserModel\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.remove_user_from_user_groups_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.remove_user_from_user_groups_with_http_info(id, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def _run(self, referenceGroupId, referenceId=None):\n        \"\"\"\n        automatically guess reference id if not passed\n        \"\"\"\n", "input": "", "output": "        # check if we can get reference id from rg\n        if referenceId is None:\n            referenceId = self._referenceId\n        if referenceId is None:\n            rg = self._client.get_read_group(\n                read_group_id=referenceGroupId)\n            iterator = self._client.search_references(rg.reference_set_id)\n            for reference in iterator:\n                self._run(referenceGroupId, reference.id)\n        else:\n            iterator = self._client.search_reads(\n                read_group_ids=[referenceGroupId],\n                reference_id=referenceId,\n                start=self._start, end=self._end)\n            self._output(iterator)", "category": "Python"}, {"instruction": "def remove_phenotype(self, ind_obj, phenotypes=None):\n        \"\"\"Remove multiple phenotypes from an individual.\"\"\"\n", "input": "", "output": "        if phenotypes is None:\n            logger.info(\"delete all phenotypes related to %s\", ind_obj.ind_id)\n            self.query(PhenotypeTerm).filter_by(ind_id=ind_obj.id).delete()\n        else:\n            for term in ind_obj.phenotypes:\n                if term.phenotype_id in phenotypes:\n                    logger.info(\"delete phenotype: %s from %s\",\n                                term.phenotype_id, ind_obj.ind_id)\n                    self.session.delete(term)\n        logger.debug('persist removals')\n        self.save()\n        for case_obj in ind_obj.cases:\n            self.update_hpolist(case_obj)", "category": "Python"}, {"instruction": "def _load_cache(self):\n        \"\"\"\n        the method is implemented for the purpose of optimization, byte positions will not be re-read from a file\n        that has already been used, if the content of the file has changed, and the name has been left the same,\n        the old version of byte offsets will be loaded\n        :return: list of byte offsets from existing file\n        \"\"\"\n", "input": "", "output": "        try:\n            with open(self.__cache_path, 'rb') as f:\n                return load(f)\n        except FileNotFoundError:\n            return\n        except IsADirectoryError as e:\n            raise IsADirectoryError(f'Please delete {self.__cache_path} directory') from e\n        except (UnpicklingError, EOFError) as e:\n            raise UnpicklingError(f'Invalid cache file {self.__cache_path}. Please delete it') from e", "category": "Python"}, {"instruction": "def Auth(email=None, password=None):\n    \"\"\"Get a reusable google data client.\"\"\"\n", "input": "", "output": "    gd_client = SpreadsheetsService()\n    gd_client.source = \"texastribune-ttspreadimporter-1\"\n    if email is None:\n        email = os.environ.get('GOOGLE_ACCOUNT_EMAIL')\n    if password is None:\n        password = os.environ.get('GOOGLE_ACCOUNT_PASSWORD')\n    if email and password:\n        gd_client.ClientLogin(email, password)\n    return gd_client", "category": "Python"}, {"instruction": "def _FieldSkipper():\n  \"\"\"Constructs the SkipField function.\"\"\"\n", "input": "", "output": "\n  WIRETYPE_TO_SKIPPER = [\n      _SkipVarint,\n      _SkipFixed64,\n      _SkipLengthDelimited,\n      _SkipGroup,\n      _EndGroup,\n      _SkipFixed32,\n      _RaiseInvalidWireType,\n      _RaiseInvalidWireType,\n      ]\n\n  wiretype_mask = wire_format.TAG_TYPE_MASK\n\n  def SkipField(buffer, pos, end, tag_bytes):\n    ", "category": "Python"}, {"instruction": "def add_auth(f):\n    \"\"\"A decorator that adds the authentication header to requests arguments\"\"\"\n", "input": "", "output": "\n    def add_auth_decorator(*args, **kwargs):\n        token = get_user_token()\n        if 'headers' not in kwargs:\n            kwargs['headers'] = {}\n        kwargs['headers']['Authorization'] = \"Bearer %s\" % token\n        return f(*args, **kwargs)\n\n    return add_auth_decorator", "category": "Python"}, {"instruction": "def request(schema):\n    \"\"\"\n    Decorate a function with a request schema.\n\n    \"\"\"\n", "input": "", "output": "    def wrapper(func):\n        setattr(func, REQUEST, schema)\n        return func\n    return wrapper", "category": "Python"}, {"instruction": "def __xinclude_libxml2(target, source, env):\n    \"\"\"\n    Resolving XIncludes, using the libxml2 module.\n    \"\"\"\n", "input": "", "output": "    doc = libxml2.readFile(str(source[0]), None, libxml2.XML_PARSE_NOENT)\n    doc.xincludeProcessFlags(libxml2.XML_PARSE_NOENT)\n    doc.saveFile(str(target[0]))\n    doc.freeDoc()\n\n    return None", "category": "Python"}, {"instruction": "def create_option_pool(self):\n        \"\"\"Get an instance of option_pool services facade.\"\"\"\n", "input": "", "output": "        return OptionPool(\n            self.networkapi_url,\n            self.user,\n            self.password,\n            self.user_ldap)", "category": "Python"}, {"instruction": "def timeframe(self, start, end):\n        r\"\"\"\n            When you want to search bugs for a certain time frame.\n\n            :param start:\n            :param end:\n            :returns: :class:`Search`\n        \"\"\"\n", "input": "", "output": "        if start:\n            self._time_frame['chfieldfrom'] = start\n        if end:\n            self._time_frame['chfieldto'] = end\n        return self", "category": "Python"}, {"instruction": "def currentStore(self, store_view=None):\n        \"\"\"\n        Set/Get current store view\n\n        :param store_view: Store view ID or Code\n        :return: int\n        \"\"\"\n", "input": "", "output": "        args = [store_view] if store_view else []\n        return int(self.call('catalog_category_attribute.currentStore', args))", "category": "Python"}, {"instruction": "def to_utc(a_datetime, keep_utc_tzinfo=False):\n    \"\"\"\n    Convert a time awared datetime to utc datetime.\n\n    :param a_datetime: a timezone awared datetime. (If not, then just returns)\n    :param keep_utc_tzinfo: whether to retain the utc time zone information.\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u5c06\u4e00\u4e2a\u5e26\u65f6\u533a\u7684\u65f6\u95f4\u8f6c\u5316\u6210UTC\u65f6\u95f4\u3002\u800c\u5bf9\u4e8eUTC\u65f6\u95f4\u800c\u8a00, \u6709\u6ca1\u6709\u65f6\u533a\u4fe1\u606f\u90fd\u65e0\u6240\u8c13\u4e86\u3002\n    \"\"\"\n", "input": "", "output": "    if a_datetime.tzinfo:\n        utc_datetime = a_datetime.astimezone(utc)  # convert to utc time\n        if keep_utc_tzinfo is False:\n            utc_datetime = utc_datetime.replace(tzinfo=None)\n        return utc_datetime\n    else:\n        return a_datetime", "category": "Python"}, {"instruction": "def equivalent_diameter(target, throat_area='throat.area',\n                        throat_shape='circle'):\n    r\"\"\"\n    Calculates the diameter of a cirlce or edge-length of a sqaure with same\n    area as the throat.\n\n    Parameters\n    ----------\n    target : OpenPNM Object\n        The object which this model is associated with. This controls the\n        length of the calculated array, and also provides access to other\n        necessary properties.\n\n    thorat_area : string\n        The dictionary key to the throat area values\n\n    throat_shape : string\n        The shape cross-sectional shape of the throat to assume when\n        back-calculating from the area.  Options are 'circle' (default) or\n        'square'.\n    \"\"\"\n", "input": "", "output": "    area = target[throat_area]\n    if throat_shape.startswith('circ'):\n        value = 2*_np.sqrt(area/_np.pi)\n    elif throat_shape.startswith('square'):\n        value = _np.sqrt(area)\n    return value", "category": "Python"}, {"instruction": "def typedef(\n            self,\n            name=None,\n            function=None,\n            header_dir=None,\n            header_file=None,\n            recursive=None):\n        \"\"\"returns reference to typedef declaration, that is matched\n        defined criteria\"\"\"\n", "input": "", "output": "        return (\n            self._find_single(\n                self._impl_matchers[scopedef_t.typedef],\n                name=name,\n                function=function,\n                decl_type=self._impl_decl_types[\n                    scopedef_t.typedef],\n                header_dir=header_dir,\n                header_file=header_file,\n                recursive=recursive)\n        )", "category": "Python"}, {"instruction": "def U(Document, __raw__=None, **update):\n\t\"\"\"Generate a MongoDB update document through paramater interpolation.\n\t\n\tArguments passed by name have their name interpreted as an optional operation prefix (defaulting to `set`, e.g.\n\t`push`), a double-underscore separated field reference (e.g. `foo`, or `foo__bar`, or `foo__S__bar`, or\n\t`foo__27__bar`)\n\t\n\tBecause this utility is likely going to be used frequently it has been given a single-character name.\n\t\"\"\"\n", "input": "", "output": "\t\n\tops = Update(__raw__)\n\targs = _process_arguments(Document, UPDATE_ALIASES, {}, update, UPDATE_PASSTHROUGH)\n\t\n\tfor operation, _, field, value in args:\n\t\tif not operation:\n\t\t\toperation = DEFAULT_UPDATE\n\t\t\n\t\tif isinstance(operation, tuple):\n\t\t\toperation, cast = ('$' + operation[0]), operation[1]\n\t\t\tif cast in UPDATE_MAGIC:\n\t\t\t\tvalue = cast(value, field)\n\t\t\telse:\n\t\t\t\tvalue = cast(value)\n\t\t\t\n\t\t\tif operation in ops and ~field in ops[operation] and isinstance(value, Mapping):\n\t\t\t\tops[operation][~field].update(value)\n\t\t\t\tcontinue\n\t\t\n\t\telse:\n\t\t\toperation = '$' + operation\n\t\t\n\t\tops &= Update({operation: {~field: value}})\n\t\n\treturn ops", "category": "Python"}, {"instruction": "def block_ip(ip_address):\n    \"\"\" given the ip, block it \"\"\"\n", "input": "", "output": "    if not ip_address:\n        # no reason to continue when there is no ip\n        return\n    if config.DISABLE_IP_LOCKOUT:\n        # no need to block, we disabled it.\n        return\n    key = get_ip_blocked_cache_key(ip_address)\n    if config.COOLOFF_TIME:\n        REDIS_SERVER.set(key, 'blocked', config.COOLOFF_TIME)\n    else:\n        REDIS_SERVER.set(key, 'blocked')\n    send_ip_block_signal(ip_address)", "category": "Python"}, {"instruction": "def clean_slug(self):\n        \"\"\"\n        Generate a valid slug, in case the given one is taken\n        \"\"\"\n", "input": "", "output": "        source = self.cleaned_data.get('slug', '')\n        lang_choice = self.language_code\n        if not source:\n            source = slugify(self.cleaned_data.get('title', ''))\n        qs = Post._default_manager.active_translations(lang_choice).language(lang_choice)\n        used = list(qs.values_list('translations__slug', flat=True))\n        slug = source\n        i = 1\n        while slug in used:\n            slug = '%s-%s' % (source, i)\n            i += 1\n        return slug", "category": "Python"}, {"instruction": "def getArguments(names, local_dict=None, global_dict=None):\n    \"\"\"Get the arguments based on the names.\"\"\"\n", "input": "", "output": "    call_frame = sys._getframe(2)\n\n    clear_local_dict = False\n    if local_dict is None:\n        local_dict = call_frame.f_locals\n        clear_local_dict = True\n    try:\n        frame_globals = call_frame.f_globals\n        if global_dict is None:\n            global_dict = frame_globals\n\n        # If `call_frame` is the top frame of the interpreter we can't clear its \n        # `local_dict`, because it is actually the `global_dict`.\n        clear_local_dict = clear_local_dict and not frame_globals is local_dict\n\n        arguments = []\n        for name in names:\n            try:\n                a = local_dict[name]\n            except KeyError:\n                a = global_dict[name]\n            arguments.append(numpy.asarray(a))\n    finally:\n        # If we generated local_dict via an explicit reference to f_locals,\n        # clear the dict to prevent creating extra ref counts in the caller's scope\n        # See https://github.com/pydata/numexpr/issues/310\n        if clear_local_dict:\n            local_dict.clear()\n\n    return arguments", "category": "Python"}, {"instruction": "def _write_header(data, fp, relation_name, index):\n    \"\"\"Write header containing attribute names and types\"\"\"\n", "input": "", "output": "    fp.write(\"@relation {0}\\n\\n\".format(relation_name))\n\n    if index:\n        data = data.reset_index()\n\n    attribute_names = _sanitize_column_names(data)\n\n    for column, series in data.iteritems():\n        name = attribute_names[column]\n        fp.write(\"@attribute {0}\\t\".format(name))\n\n        if is_categorical_dtype(series) or is_object_dtype(series):\n            _write_attribute_categorical(series, fp)\n        elif numpy.issubdtype(series.dtype, numpy.floating):\n            fp.write(\"real\")\n        elif numpy.issubdtype(series.dtype, numpy.integer):\n            fp.write(\"integer\")\n        elif numpy.issubdtype(series.dtype, numpy.datetime64):\n            fp.write(\"date 'yyyy-MM-dd HH:mm:ss'\")\n        else:\n            raise TypeError('unsupported type %s' % series.dtype)\n\n        fp.write(\"\\n\")\n    return data", "category": "Python"}, {"instruction": "def crc16(data):\n    \"\"\"\n    Calculate an ISO13239 CRC checksum of the input buffer.\n    \"\"\"\n", "input": "", "output": "    m_crc = 0xffff\n    for this in data:\n        m_crc ^= ord(this)\n        for _ in range(8):\n            j = m_crc & 1\n            m_crc >>= 1\n            if j:\n                m_crc ^= 0x8408\n    return m_crc", "category": "Python"}, {"instruction": "def string2time(text):\n    \"\"\"Return :class:`datetime.datetime` object from given string,\n    or ``None`` if failed to translate.\"\"\"\n", "input": "", "output": "    length = len(TIME_FORMAT)\n    result = None\n    while length:\n        try:\n            result = datetime.strptime(text, TIME_FORMAT[:length])\n        except:\n            length -= 1\n        else:\n            break\n    return result", "category": "Python"}, {"instruction": "def libvlc_vlm_seek_media(p_instance, psz_name, f_percentage):\n    '''Seek in the named broadcast.\n    @param p_instance: the instance.\n    @param psz_name: the name of the broadcast.\n    @param f_percentage: the percentage to seek to.\n    @return: 0 on success, -1 on error.\n    '''\n", "input": "", "output": "    f = _Cfunctions.get('libvlc_vlm_seek_media', None) or \\\n        _Cfunction('libvlc_vlm_seek_media', ((1,), (1,), (1,),), None,\n                    ctypes.c_int, Instance, ctypes.c_char_p, ctypes.c_float)\n    return f(p_instance, psz_name, f_percentage)", "category": "Python"}, {"instruction": "def extract_from_text(text):\n    \"\"\"\n    Extract arXiv IDs from a text.\n\n    :param text: The text to extract arXiv IDs from.\n    :returns: A list of matching arXiv IDs, in canonical form.\n\n    >>> sorted(extract_from_text('1506.06690 1506.06690v1 arXiv:1506.06690 arXiv:1506.06690v1 arxiv:1506.06690 arxiv:1506.06690v1 math.GT/0309136 abcdf bar1506.06690foo mare.GG/0309136'))\n    ['1506.06690', '1506.06690v1', 'math.GT/0309136']\n    \"\"\"\n", "input": "", "output": "    # Remove the leading \"arxiv:\".\n    return tools.remove_duplicates([re.sub(\"arxiv:\", \"\", i[0],\n                                           flags=re.IGNORECASE)\n                                    for i in REGEX.findall(text) if i[0] != ''])", "category": "Python"}, {"instruction": "def get_kline(self, symbol, period, size=150, _async=False):\n        \"\"\"\n        \u83b7\u53d6KLine\n        :param symbol\n        :param period: \u53ef\u9009\u503c\uff1a{1min, 5min, 15min, 30min, 60min, 1day, 1mon, 1week, 1year }\n        :param size: \u53ef\u9009\u503c\uff1a [1,2000]\n        :return:\n        \"\"\"\n", "input": "", "output": "        params = {'symbol': symbol, 'period': period, 'size': size}\n\n        url = u.MARKET_URL + '/market/history/kline'\n        return http_get_request(url, params, _async=_async)", "category": "Python"}, {"instruction": "def get_candidates(self, obj):\n        \"\"\"\n        CandidateElections.\n        \"\"\"\n", "input": "", "output": "        return CandidateElectionSerializer(\n            obj.candidate_elections.all(), many=True\n        ).data", "category": "Python"}, {"instruction": "def register_json_encoder(self, encoder_type: type, encoder: JSONEncoder):\n        \"\"\"\n        Register the given JSON encoder for use with the given object type.\n        :param encoder_type: the type of object to encode\n        :param encoder: the JSON encoder\n        :return: this builder\n        \"\"\"\n", "input": "", "output": "        self._json_encoders[encoder_type] = encoder\n        return self", "category": "Python"}, {"instruction": "def select_tag(self, uid):\n        \"\"\"\n        Selects tag for further usage.\n        uid -- list or tuple with four bytes tag ID\n        Returns error state.\n        \"\"\"\n", "input": "", "output": "        back_data = []\n        buf = []\n\n        buf.append(self.act_select)\n        buf.append(0x70)\n\n        for i in range(5):\n            buf.append(uid[i])\n\n        crc = self.calculate_crc(buf)\n        buf.append(crc[0])\n        buf.append(crc[1])\n\n        (error, back_data, back_length) = self.card_write(self.mode_transrec, buf)\n\n        if (not error) and (back_length == 0x18):\n            return False\n        else:\n            return True", "category": "Python"}, {"instruction": "def separation(X,y,samples=False):\n    \"\"\" return the sum of the between-class squared distance\"\"\"\n", "input": "", "output": "    # pdb.set_trace()\n    num_classes = len(np.unique(y))\n    total_dist = (X.max()-X.min())**2\n    if samples:\n        # return intra-class distance for each sample\n        separation = np.zeros(y.shape)\n        for label in np.unique(y):\n            for outsider in np.unique(y[y!=label]):\n                separation[y==label] += (X[y==label] - np.mean(X[y==outsider])) ** 2\n\n        #normalize between 0 and 1\n        print('separation:',separation)\n        print('num_classes:',num_classes)\n        print('total_dist:',total_dist)\n        separation = separation#/separation.max()\n\n        print('separation after normalization:',separation)\n\n    else:\n        # return aggregate score\n        separation = 0\n        for i,label in enumerate(np.unique(y)):\n            for outsider in np.unique(y[y!=label]):\n                separation += np.sum((X[y==label] - np.mean(X[y==outsider])) ** 2)/len(y[y==label])\n        separation = separation/len(np.unique(y))\n\n    return separation", "category": "Python"}, {"instruction": "def regex_opt(strings, prefix='', suffix=''):\n    \"\"\"Return a compiled regex that matches any string in the given list.\n\n    The strings to match must be literal strings, not regexes.  They will be\n    regex-escaped.\n\n    *prefix* and *suffix* are pre- and appended to the final regex.\n    \"\"\"\n", "input": "", "output": "    strings = sorted(strings)\n    return prefix + regex_opt_inner(strings, '(') + suffix", "category": "Python"}, {"instruction": "def check_hints(self, reply):\n        \"\"\"Parse the hints to check for errors. \"\"\"\n", "input": "", "output": "        try:\n            f_ind = reply.index(\"hints\")\n            l_ind = reply.rindex(\"hints\")\n        except Exception:\n            fail_reason = vdp_const.hints_failure_reason % (reply)\n            LOG.error(\"%s\", fail_reason)\n            return False, fail_reason\n        if f_ind != l_ind:\n            # Currently not supported if reply contains a filter keyword\n            fail_reason = vdp_const.multiple_hints_failure_reason % (reply)\n            LOG.error(\"%s\", fail_reason)\n            return False, fail_reason\n        try:\n            hints_compl = reply.partition(\"hints\")[2]\n            hints_val = reply.partition(\"hints\")[2][0:4]\n            len_hints = int(hints_val)\n            hints_val = hints_compl[4:4 + len_hints]\n            hints = int(hints_val)\n            if hints != 0:\n                fail_reason = vdp_const.nonzero_hints_failure % (hints)\n                return False, fail_reason\n        except ValueError:\n            fail_reason = vdp_const.format_failure_reason % (reply)\n            LOG.error(\"%s\", fail_reason)\n            return False, fail_reason\n        return True, None", "category": "Python"}, {"instruction": "def delete_group(self, name, url_prefix, auth, session, send_opts):\n        \"\"\"Delete given group.\n\n        Args:\n            name (string): Name of group.\n            url_prefix (string): Protocol + host such as https://api.theboss.io\n            auth (string): Token to send in the request header.\n            session (requests.Session): HTTP session to use for request.\n            send_opts (dictionary): Additional arguments to pass to session.send().\n\n        Raises:\n            requests.HTTPError on failure.\n        \"\"\"\n", "input": "", "output": "        req = self.get_group_request(\n            'DELETE', 'application/json', url_prefix, auth, name)\n\n        prep = session.prepare_request(req)\n        resp = session.send(prep, **send_opts)\n        if resp.status_code == 204:\n            return\n\n        msg = ('Delete failed for group {}, got HTTP response: ({}) - {}'.format(\n            name, resp.status_code, resp.text))\n        raise HTTPError(msg, request = req, response = resp)", "category": "Python"}, {"instruction": "def difference(self, *others):\n        \"\"\" Calculates the difference between this set and @others\n\n            @others: one or several #str keynames or :class:RedisSet objects\n\n            -> set resulting from the difference between the first set and\n                all @others.\n        \"\"\"\n", "input": "", "output": "        others = self._typesafe_others(others)\n        return set(map(\n            self._loads, self._client.sdiff(self.key_prefix, *others)))", "category": "Python"}, {"instruction": "def _post_md5_skip_on_check(self, key, md5_match):\n        # type: (Uploader, str, bool) -> None\n        \"\"\"Perform post MD5 skip on check\n        :param Uploader self: this\n        :param str key: md5 map key\n        :param bool md5_match: if MD5 matches\n        \"\"\"\n", "input": "", "output": "        with self._md5_meta_lock:\n            src, rfile = self._md5_map.pop(key)\n        uid = blobxfer.operations.upload.Uploader.create_unique_id(src, rfile)\n        if md5_match:\n            with self._upload_lock:\n                self._upload_set.remove(uid)\n                self._upload_total -= 1\n            if self._general_options.dry_run:\n                logger.info('[DRY RUN] MD5 match, skipping: {} -> {}'.format(\n                    src.absolute_path, rfile.path))\n        else:\n            if self._general_options.dry_run:\n                with self._upload_lock:\n                    self._upload_set.remove(uid)\n                    self._upload_total -= 1\n                logger.info('[DRY RUN] MD5 mismatch, upload: {} -> {}'.format(\n                    src.absolute_path, rfile.path))\n            else:\n                self._add_to_upload_queue(src, rfile, uid)", "category": "Python"}, {"instruction": "def string_to_date(value):\n    \"\"\"\n    Return a Python date that corresponds to the specified string\n    representation.\n\n    @param value: string representation of a date.\n\n    @return: an instance ``datetime.datetime`` represented by the string.\n    \"\"\"\n", "input": "", "output": "    if isinstance(value, datetime.date):\n        return value\n\n    return dateutil.parser.parse(value).date()", "category": "Python"}, {"instruction": "def cmd_rollback(context):\n  \"\"\"\n  Roll back by finding the most recent \"stable\" tagged version, and putting it again, so that\n  it's the new \"current\" version.\n  Args:\n    context: a populated EFVersionContext object\n  \"\"\"\n", "input": "", "output": "  last_stable = get_versions(context, return_stable=True)\n  if len(last_stable) != 1:\n    fail(\"Didn't find a version marked stable for key: {} in env/service: {}/{}\".format(\n         context.key, context.env, context.service_name))\n  context.value = last_stable[0].value\n  context.commit_hash = last_stable[0].commit_hash\n  context.build_number = last_stable[0].build_number\n  context.location = last_stable[0].location\n  context.stable = True\n  cmd_set(context)", "category": "Python"}, {"instruction": "def emitCurrentToken(self):\n        \"\"\"This method is a generic handler for emitting the tags. It also sets\n        the state to \"data\" because that's what's needed after a token has been\n        emitted.\n        \"\"\"\n", "input": "", "output": "        token = self.currentToken\n        # Add token to the queue to be yielded\n        if (token[\"type\"] in tagTokenTypes):\n            token[\"name\"] = token[\"name\"].translate(asciiUpper2Lower)\n            if token[\"type\"] == tokenTypes[\"EndTag\"]:\n                if token[\"data\"]:\n                    self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                            \"data\": \"attributes-in-end-tag\"})\n                if token[\"selfClosing\"]:\n                    self.tokenQueue.append({\"type\": tokenTypes[\"ParseError\"],\n                                            \"data\": \"self-closing-flag-on-end-tag\"})\n        self.tokenQueue.append(token)\n        self.state = self.dataState", "category": "Python"}, {"instruction": "def rt(nu, size=None):\n    \"\"\"\n    Student's t random variates.\n    \"\"\"\n", "input": "", "output": "    return rnormal(0, 1, size) / np.sqrt(rchi2(nu, size) / nu)", "category": "Python"}, {"instruction": "def update(self, heap):\n        \"\"\"Update the item descriptors and items from an incoming heap.\n\n        Parameters\n        ----------\n        heap : :class:`spead2.recv.Heap`\n            Incoming heap\n\n        Returns\n        -------\n        dict\n            Items that have been updated from this heap, indexed by name\n        \"\"\"\n", "input": "", "output": "        for descriptor in heap.get_descriptors():\n            item = Item.from_raw(descriptor, flavour=heap.flavour)\n            self._add_item(item)\n        updated_items = {}\n        for raw_item in heap.get_items():\n            if raw_item.id <= STREAM_CTRL_ID:\n                continue     # Special fields, not real items\n            try:\n                item = self._by_id[raw_item.id]\n            except KeyError:\n                _logger.warning('Item with ID %#x received but there is no descriptor', raw_item.id)\n            else:\n                item.set_from_raw(raw_item)\n                item.version += 1\n                updated_items[item.name] = item\n        return updated_items", "category": "Python"}, {"instruction": "def set(self, is_on=None, brightness=None, cancel_transition=True):\n        \"\"\"\n        Set properties of the led simultaneously before updating pwm values.\n\n        :param is_on: On-off state of the led.\n        :param brightness: Brightness of the led.\n        :param cancel_transition: Cancel active transitions.\n        \"\"\"\n", "input": "", "output": "        if cancel_transition:\n            self._cancel_active_transition()\n\n        if is_on is not None:\n            self._is_on = is_on\n\n        if brightness is not None:\n            self._assert_is_brightness(brightness)\n            self._brightness = brightness\n\n        self._update_pwm()", "category": "Python"}, {"instruction": "def url_fix(s, charset='utf-8'):\n    r\"\"\"Sometimes you get an URL by a user that just isn't a real URL because\n    it contains unsafe characters like ' ' and so on. This function can fix\n    some of the problems in a similar way browsers handle data entered by the\n    user:\n\n    >>> url_fix(u'http://de.wikipedia.org/wiki/Elf (Begriffskl\\xe4rung)')\n    'http://de.wikipedia.org/wiki/Elf%20(Begriffskl%C3%A4rung)'\n\n    :param s: the string with the URL to fix.\n    :param charset: The target charset for the URL if the url was given as\n                    unicode string.\n    \"\"\"\n", "input": "", "output": "    scheme, netloc, path, qs, anchor = url_parse(to_unicode(s, charset, 'replace'))\n    path = url_quote(path, charset, safe='/%+$!*\\'(),')\n    qs = url_quote_plus(qs, charset, safe=':&%=+$!*\\'(),')\n    return to_native(url_unparse((scheme, netloc, path, qs, anchor)))", "category": "Python"}, {"instruction": "def bpopmax(self, timeout=0):\n        \"\"\"\n        Atomically remove the highest-scoring item from the set, blocking until\n        an item becomes available or timeout is reached (0 for no timeout,\n        default).\n\n        Returns a 2-tuple of (item, score).\n        \"\"\"\n", "input": "", "output": "        res = self.database.bzpopmax(self.key, timeout)\n        if res is not None:\n            return (res[1], res[2])", "category": "Python"}, {"instruction": "def validate_config(config):\n    \"\"\"\n    Validates the extractor configuration file. Ensures that there are no duplicate field names, etc.\n\n    :param config: The configuration file that contains the specification of the extractor\n    :return: True if config is valid, else raises a exception that specifies the correction to be made\n\n    \"\"\"\n", "input": "", "output": "    fields = [f for f in get_fields(config)]\n    if len(fields) != len(set(fields)):\n        raise InvalidConfigException(\n            \"Invalid configuration file - %d duplicate field names\" % len(fields) - len(set(fields))\n        )\n    return True", "category": "Python"}, {"instruction": "def iteritems(self, key=_absent):\n        \"\"\"\n        Parity with dict.iteritems() except the optional <key> parameter has\n        been added. If <key> is provided, only items with the provided key are\n        iterated over. KeyError is raised if <key> is provided and not in the\n        dictionary.\n\n        Example:\n          omd = omdict([(1,1), (1,11), (1,111), (2,2), (3,3)])\n          omd.iteritems(1) -> (1,1) -> (1,11) -> (1,111)\n          omd.iteritems() -> (1,1) -> (2,2) -> (3,3)\n\n        Raises: KeyError if <key> is provided and not in the dictionary.\n        Returns: An iterator over the items() of the dictionary, or only items\n          with the key <key> if <key> is provided.\n        \"\"\"\n", "input": "", "output": "        if key is not _absent:\n            if key in self:\n                items = [(node.key, node.value) for node in self._map[key]]\n                return iter(items)\n            raise KeyError(key)\n        items = six.iteritems(self._map)\n        return iter((key, nodes[0].value) for (key, nodes) in items)", "category": "Python"}, {"instruction": "def bedrooms(self):\n        \"\"\"\n        This method gets the number of bedrooms.\n        :return:\n        \"\"\"\n", "input": "", "output": "        try:\n            if self._data_from_search:\n                info = self._data_from_search.find(\n                    'ul', {\"class\": \"info\"}).text\n                s = info.split('|')\n                nb = s[1].strip()\n                return int(nb.split()[0])\n            else:\n                div = self._ad_page_content.find(\n                    'div', {'id': 'smi-summary-items'})\n                spans = div.find_all('span', {'class': 'header_text'})\n                for span in spans:\n                    # print(span.text)\n                    if 'bed' in span.text.lower():\n                        return int(''.join([n for n in span.text if n.isdigit()]))\n                return\n        except Exception as e:\n            if self._debug:\n                logging.error(\n                    \"Error getting bedrooms. Error message: \" + e.args[0])\n            return 'N/A'", "category": "Python"}, {"instruction": "def delete(self):\n        \"\"\"\n            Destructor.\n        \"\"\"\n", "input": "", "output": "\n        if self.maplesat:\n            pysolvers.maplechrono_del(self.maplesat)\n            self.maplesat = None\n\n            if self.prfile:\n                self.prfile.close()", "category": "Python"}, {"instruction": "def set_learning_objective_ids(self, learning_objective_ids):\n        \"\"\"the learning objective to find related items for\n\n        This can only be set if there are no items specifically set\n\n        \"\"\"\n", "input": "", "output": "        if self.get_learning_objective_ids_metadata().is_read_only():\n            raise NoAccess()\n        for learning_objective_id in learning_objective_ids:\n            if not self.my_osid_object_form._is_valid_id(learning_objective_id):\n                raise InvalidArgument()\n        if self.my_osid_object_form._my_map['itemIds'][0]:\n            raise IllegalState()\n        self.my_osid_object_form._my_map['learningObjectiveIds'] = [str(lo) for lo in learning_objective_ids]", "category": "Python"}, {"instruction": "def activate(self, resource=None, timeout=3, wait_for_finish=False):\n        \"\"\"\n        Activate this package on the SMC\n\n        :param list resource: node href's to activate on. Resource is only\n               required for software upgrades\n        :param int timeout: timeout between queries\n        :raises TaskRunFailed: failure during activation (downloading, etc)\n        :rtype: TaskOperationPoller\n        \"\"\"\n", "input": "", "output": "        return Task.execute(self, 'activate', json={'resource': resource},\n            timeout=timeout, wait_for_finish=wait_for_finish)", "category": "Python"}, {"instruction": "def relative_ref(self, baseURI):\n        \"\"\"\n        Return string containing relative reference to package item from\n        *baseURI*. E.g. PackURI('/ppt/slideLayouts/slideLayout1.xml') would\n        return '../slideLayouts/slideLayout1.xml' for baseURI '/ppt/slides'.\n        \"\"\"\n", "input": "", "output": "        # workaround for posixpath bug in 2.6, doesn't generate correct\n        # relative path when *start* (second) parameter is root ('/')\n        if baseURI == '/':\n            relpath = self[1:]\n        else:\n            relpath = posixpath.relpath(self, baseURI)\n        return relpath", "category": "Python"}, {"instruction": "def is_dragon(host, timeout=1):\n        \"\"\"\n        Check if host is a dragon.\n\n        Check if the specified host is a dragon based on simple heuristic.\n        The code simply checks if particular strings are in the index page.\n        It should work for DragonMint or Innosilicon branded miners.\n        \"\"\"\n", "input": "", "output": "        try:\n            r = requests.get('http://{}/'.format(host), timeout=timeout)\n            if r.status_code == 200:\n                if '<title>DragonMint</title>' in r.text or \\\n                        '<title>AsicMiner</title>' in r.text:\n                    return True\n        except requests.exceptions.RequestException:\n            pass\n        return False", "category": "Python"}, {"instruction": "def enbase64(byte_str):\n    \"\"\"\n    Encode bytes/strings to base64.\n\n    Args:\n        - ``byte_str``:  The string or bytes to base64 encode.\n\n    Returns:\n        - byte_str encoded as base64.\n    \"\"\"\n", "input": "", "output": "\n    # Python 3: base64.b64encode() expects type byte\n    if isinstance(byte_str, str) and not PYTHON2:\n        byte_str = bytes(byte_str, 'utf-8')\n    return base64.b64encode(byte_str)", "category": "Python"}, {"instruction": "def log(self, *args):\n        \"\"\"Log a log message.  Used for debugging recurring events.\"\"\"\n", "input": "", "output": "        if _canShortcutLogging(self.logCategory, LOG):\n            return\n        logObject(self.logObjectName(), self.logCategory,\n            *self.logFunction(*args))", "category": "Python"}, {"instruction": "def getPrices(self, *args, **kwargs):\n        \"\"\"\n        Request prices for EC2\n\n        Return a list of possible prices for EC2\n\n        This method gives output: ``v1/prices.json#``\n\n        This method is ``experimental``\n        \"\"\"\n", "input": "", "output": "\n        return self._makeApiCall(self.funcinfo[\"getPrices\"], *args, **kwargs)", "category": "Python"}, {"instruction": "def terminate(instance_id=None, name=None, region=None,\n              key=None, keyid=None, profile=None, filters=None):\n    '''\n    Terminate the instance described by instance_id or name.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_ec2.terminate name=myinstance\n        salt myminion boto_ec2.terminate instance_id=i-a46b9f\n    '''\n", "input": "", "output": "    instances = find_instances(instance_id=instance_id, name=name,\n                               region=region, key=key, keyid=keyid,\n                               profile=profile, return_objs=True,\n                               filters=filters)\n    if instances in (False, None, []):\n        return instances\n\n    if len(instances) == 1:\n        instances[0].terminate()\n        return True\n    else:\n        log.warning('Refusing to terminate multiple instances at once')\n        return False", "category": "Python"}, {"instruction": "def execute(self, query, *args, **kwargs):\n        \"\"\"Asynchronously execute the specified CQL query.\n\n        The execute command also takes optional parameters and trace\n        keyword arguments. See cassandra-python documentation for\n        definition of those parameters.\n        \"\"\"\n", "input": "", "output": "        tornado_future = Future()\n        cassandra_future = self._session.execute_async(query, *args, **kwargs)\n        self._ioloop.add_callback(\n            self._callback, cassandra_future, tornado_future)\n        return tornado_future", "category": "Python"}, {"instruction": "def parse_blockwise(value):\n    \"\"\"\n    Parse Blockwise option.\n\n    :param value: option value\n    :return: num, m, size\n    \"\"\"\n", "input": "", "output": "\n    length = byte_len(value)\n    if length == 1:\n        num = value & 0xF0\n        num >>= 4\n        m = value & 0x08\n        m >>= 3\n        size = value & 0x07\n    elif length == 2:\n        num = value & 0xFFF0\n        num >>= 4\n        m = value & 0x0008\n        m >>= 3\n        size = value & 0x0007\n    else:\n        num = value & 0xFFFFF0\n        num >>= 4\n        m = value & 0x000008\n        m >>= 3\n        size = value & 0x000007\n    return num, int(m), pow(2, (size + 4))", "category": "Python"}, {"instruction": "def parse(\n    version, strict=False  # type: str  # type: bool\n):  # type:(...) -> Union[Version, LegacyVersion]\n    \"\"\"\n    Parse the given version string and return either a :class:`Version` object\n    or a LegacyVersion object depending on if the given version is\n    a valid PEP 440 version or a legacy version.\n\n    If strict=True only PEP 440 versions will be accepted.\n    \"\"\"\n", "input": "", "output": "    try:\n        return Version(version)\n    except InvalidVersion:\n        if strict:\n            raise\n\n        return LegacyVersion(version)", "category": "Python"}, {"instruction": "def _deserialize(self, value, environment=None):\n        \"\"\"A collection traverses over something to deserialize its value.\"\"\"\n", "input": "", "output": "\n        if not isinstance(value, CollectionABC):\n            raise exc.Invalid(self)\n\n        items_class = self.items.__class__\n        invalids = []\n\n        # traverse items and match against validated struct\n        collection = self.collection_type()\n\n        for i, subvalue in enumerate(value):\n            item = items_class(name=i)\n            try:\n                self.collection_pusher(collection, item.deserialize(subvalue, environment=environment))\n\n            except exc.Invalid as ex:\n                invalids.append(ex)\n\n        if invalids:\n            # on invalids this item is also ``Invalid``\n            raise exc.InvalidChildren(self, invalids)\n\n        return collection", "category": "Python"}, {"instruction": "def save_sensors(self):\n        \"\"\"Save sensors to file.\"\"\"\n", "input": "", "output": "        if not self.need_save:\n            return\n        fname = os.path.realpath(self.persistence_file)\n        exists = os.path.isfile(fname)\n        dirname = os.path.dirname(fname)\n        if (not os.access(dirname, os.W_OK) or exists and\n                not os.access(fname, os.W_OK)):\n            _LOGGER.error('Permission denied when writing to %s', fname)\n            return\n        split_fname = os.path.splitext(fname)\n        tmp_fname = '{}.tmp{}'.format(split_fname[0], split_fname[1])\n        _LOGGER.debug('Saving sensors to persistence file %s', fname)\n        self._perform_file_action(tmp_fname, 'save')\n        if exists:\n            os.rename(fname, self.persistence_bak)\n        os.rename(tmp_fname, fname)\n        if exists:\n            os.remove(self.persistence_bak)\n        self.need_save = False", "category": "Python"}, {"instruction": "def badgify_badges(**kwargs):\n    \"\"\"\n    Returns all badges or only awarded badges for the given user.\n    \"\"\"\n", "input": "", "output": "    User = get_user_model()\n    user = kwargs.get('user', None)\n    username = kwargs.get('username', None)\n    if username:\n        try:\n            user = User.objects.get(username=username)\n        except User.DoesNotExist:\n            pass\n    if user:\n        awards = Award.objects.filter(user=user).select_related('badge')\n        badges = [award.badge for award in awards]\n        return badges\n    return Badge.objects.all()", "category": "Python"}, {"instruction": "def load_user_config(args, log):\n    \"\"\"Load settings from the user's confiuration file, and add them to `args`.\n\n    Settings are loaded from the configuration file in the user's home\n    directory.  Those parameters are added (as attributes) to the `args`\n    object.\n\n    Arguments\n    ---------\n    args : `argparse.Namespace`\n        Namespace object to which configuration attributes will be added.\n\n    Returns\n    -------\n    args : `argparse.Namespace`\n        Namespace object with added attributes.\n\n    \"\"\"\n", "input": "", "output": "    if not os.path.exists(_CONFIG_PATH):\n        err_str = (\n            \"Configuration file does not exists ({}).\\n\".format(_CONFIG_PATH) +\n            \"Run `python -m astrocats setup` to configure.\")\n        log_raise(log, err_str)\n\n    config = json.load(open(_CONFIG_PATH, 'r'))\n    setattr(args, _BASE_PATH_KEY, config[_BASE_PATH_KEY])\n    log.debug(\"Loaded configuration: {}: {}\".format(_BASE_PATH_KEY, config[\n        _BASE_PATH_KEY]))\n    return args", "category": "Python"}, {"instruction": "def combine_images(imgs, register=True):\n    \"\"\"Combine similar images into one to reduce the noise\n\n    Parameters\n    ----------\n    imgs: list of 2d array\n        Series of images\n    register: Boolean, default False\n        True if the images should be register before combination\n\n    Returns\n    -------\n    im: 2d array\n        The result image\n\n    Notes\n    -----\n    This is an example of the usage of the library\n    \"\"\"\n", "input": "", "output": "    imgs = np.asarray(imgs, dtype=\"float\")\n    if register:\n        for i in range(1, imgs.shape[0]):\n            ret = register_images(imgs[0, :, :], imgs[i, :, :])\n            imgs[i, :, :] = rotate_scale_shift(imgs[i, :, :], *ret[:3], np.nan)\n    return np.mean(imgs, 0)", "category": "Python"}, {"instruction": "def get_password_gui(device, options):\n    \"\"\"Get the password to unlock a device from GUI.\"\"\"\n", "input": "", "output": "    text = _('Enter password for {0.device_presentation}: ', device)\n    try:\n        return password_dialog(device.id_uuid, 'udiskie', text, options)\n    except RuntimeError:\n        return None", "category": "Python"}, {"instruction": "def depth(sequence, func=max, _depth=0):\n    \"\"\"\n    Find the nesting depth of a nested sequence\n    \"\"\"\n", "input": "", "output": "    if isinstance(sequence, dict):\n        sequence = list(sequence.values())\n    depth_list = [depth(item, func=func, _depth=_depth + 1)\n                  for item in sequence if (isinstance(item, dict) or util_type.is_listlike(item))]\n    if len(depth_list) > 0:\n        return func(depth_list)\n    else:\n        return _depth", "category": "Python"}, {"instruction": "def _ps(osdata):\n    '''\n    Return the ps grain\n    '''\n", "input": "", "output": "    grains = {}\n    bsd_choices = ('FreeBSD', 'NetBSD', 'OpenBSD', 'MacOS')\n    if osdata['os'] in bsd_choices:\n        grains['ps'] = 'ps auxwww'\n    elif osdata['os_family'] == 'Solaris':\n        grains['ps'] = '/usr/ucb/ps auxwww'\n    elif osdata['os'] == 'Windows':\n        grains['ps'] = 'tasklist.exe'\n    elif osdata.get('virtual', '') == 'openvzhn':\n        grains['ps'] = (\n            'ps -fH -p $(grep -l \\\"^envID:[[:space:]]*0\\\\$\\\" '\n            '/proc/[0-9]*/status | sed -e \\\"s=/proc/\\\\([0-9]*\\\\)/.*=\\\\1=\\\")  '\n            '| awk \\'{ $7=\\\"\\\"; print }\\''\n        )\n    elif osdata['os_family'] == 'AIX':\n        grains['ps'] = '/usr/bin/ps auxww'\n    elif osdata['os_family'] == 'NILinuxRT':\n        grains['ps'] = 'ps -o user,pid,ppid,tty,time,comm'\n    else:\n        grains['ps'] = 'ps -efHww'\n    return grains", "category": "Python"}, {"instruction": "def pom_contains_modules():\n    \"\"\"\n    Reads pom.xml in current working directory and checks, if there is non-empty modules tag.\n    \"\"\"\n", "input": "", "output": "    pom_file = None\n    try:\n        pom_file = open(\"pom.xml\")\n        pom = pom_file.read()\n    finally:\n        if pom_file:\n            pom_file.close()\n\n    artifact = MavenArtifact(pom=pom)\n    if artifact.modules:\n        return True\n    else:\n        return False", "category": "Python"}, {"instruction": "def go_offline(connected=None):\n    \"\"\"\n    connected : bool\n        If True, the plotly.js library will be loaded\n        from an online CDN. If False, the plotly.js library will be loaded locally\n        from the plotly python package\n    \"\"\"\n", "input": "", "output": "    from .auth import get_config_file\n    if connected is None:\n        try:\n            connected=True if get_config_file()['offline_connected'] is None else get_config_file()['offline_connected']\n        except:\n            connected=True\n    if run_from_ipython():\n        try:\n            py_offline.init_notebook_mode(connected)\n        except TypeError:\n            #For older versions of plotly\n            py_offline.init_notebook_mode()\n        py_offline.__PLOTLY_OFFLINE_INITIALIZED=True", "category": "Python"}, {"instruction": "def add_facts_stream(self, assessment, data_type, options, data):\n        \"\"\"\n        To add  facts data stream to a Assessment\n        :param datastream: string\n        :param data_type: string\n        :param options: dict\n        :param data: Stream\n        \"\"\"\n", "input": "", "output": "\n        url = self.get_fact_url(assessment,options)\n\n        form_data = {\n            'files': {\n                'data': (\n                    Utils.random_string(10)+('.json' if data_type == 'json' else '.csv'),\n                    data,\n                    'text/plain;charset=UTF-8',\n                    {'Expires': '0'}\n                )\n            }\n        }\n        response = self.http.upstream(url,form_data)\n        return response", "category": "Python"}, {"instruction": "def write_fieldtrip(data, filename):\n    \"\"\"Export data to FieldTrip.\n\n    Parameters\n    ----------\n    data : instance of ChanTime\n        data with only one trial\n    filename : path to file\n        file to export to (include '.mat')\n\n    Notes\n    -----\n    It saves mat file using Version 6 ('-v7') because it relies on scipy.io\n    functions. Therefore it cannot store data larger than 2 GB.\n    \"\"\"\n", "input": "", "output": "    n_trl = data.number_of('trial')\n    trial = empty(n_trl, dtype='O')\n    time = empty(n_trl, dtype='O')\n\n    for trl in range(n_trl):\n        trial[trl] = data.data[trl]\n        time[trl] = data.axis['time'][trl]\n\n    ft_data = {'fsample': float(data.s_freq),\n               'label': data.axis['chan'][0].astype('O'),\n               'trial': trial,\n               'time': time,\n               'cfg': 'Converted from wonambi on ' + str(datetime.now()),\n               }\n\n    savemat(filename, {VAR: ft_data})", "category": "Python"}, {"instruction": "def detect_parser_type(cls, file, encoding=None):\n        '''Get the suitable parser type for the document.\n\n        Returns:\n            str\n        '''\n", "input": "", "output": "        is_xml = XMLDetector.is_file(file)\n        doctype = cls.parse_doctype(file, encoding=encoding) or ''\n\n        if not doctype and is_xml:\n            return 'xml'\n\n        if 'XHTML' in doctype:\n            return 'xhtml'\n\n        return 'html'", "category": "Python"}, {"instruction": "def putNetworkVisualPropBypass(self, networkId, viewId, visualProperty, body, verbose=None):\n        \"\"\"\n        Bypasses the Visual Style of the Network with the Visual Property specificed by the `visualProperty`, `viewId`, and `networkId` parameters.\n        \n        Additional details on common Visual Properties can be found in the [Basic Visual Lexicon JavaDoc API](http://chianti.ucsd.edu/cytoscape-3.6.1/API/org/cytoscape/view/presentation/property/BasicVisualLexicon.html)\n\n        :param networkId: SUID of the Network\n        :param viewId: SUID of the Network View\n        :param visualProperty: Name of the Visual Property\n        :param body: A Visual Property and its value.\n        :param verbose: print more\n\n        :returns: 200: successful operation\n        \"\"\"\n", "input": "", "output": "\n        response=api(url=self.___url+'networks/'+str(networkId)+'/views/'+str(viewId)+'/network/'+str(visualProperty)+'/bypass', method=\"PUT\", body=body, verbose=verbose)\n        return response", "category": "Python"}, {"instruction": "def stop(self, timeout=1.0):\n        \"\"\"Stop the handler thread (from another thread).\n\n        Parameters\n        ----------\n        timeout : float, optional\n            Seconds to wait for server to have *started*.\n\n        \"\"\"\n", "input": "", "output": "        if timeout:\n            self._running.wait(timeout)\n        self._running.clear()\n        # Make sure to wake the run thread.\n        self._wake.set()", "category": "Python"}, {"instruction": "def from_grayscale_and_depth(gray_im, depth_im):\n        \"\"\" Creates an G-D image from a separate grayscale and depth image. \"\"\"\n", "input": "", "output": "        # check shape\n        if gray_im.height != depth_im.height or gray_im.width != depth_im.width:\n            raise ValueError(\n                'Grayscale and depth images must have the same shape')\n\n        # check frame\n        if gray_im.frame != depth_im.frame:\n            raise ValueError(\n                'Grayscale and depth images must have the same frame')\n\n        # form composite data\n        gd_data = np.zeros([gray_im.height, gray_im.width, 2])\n        gd_data[:, :, 0] = gray_im.data.astype(np.float64)\n        gd_data[:, :, 1] = depth_im.data\n        return GdImage(gd_data, frame=gray_im.frame)", "category": "Python"}, {"instruction": "def get_objects(self, ids, **args):\n        \"\"\"Fetches all of the given object from the graph.\n\n        We return a map from ID to object. If any of the IDs are\n        invalid, we raise an exception.\n        \"\"\"\n", "input": "", "output": "        args[\"ids\"] = \",\".join(ids)\n        return self.request(self.version + \"/\", args)", "category": "Python"}, {"instruction": "def validate_v2_svc_catalog_endpoint_data(self, expected, actual):\n        \"\"\"Validate service catalog endpoint data.\n\n           Validate a list of actual service catalog endpoints vs a list of\n           expected service catalog endpoints.\n           \"\"\"\n", "input": "", "output": "        self.log.debug('Validating service catalog endpoint data...')\n        self.log.debug('actual: {}'.format(repr(actual)))\n        for k, v in six.iteritems(expected):\n            if k in actual:\n                ret = self._validate_dict_data(expected[k][0], actual[k][0])\n                if ret:\n                    return self.endpoint_error(k, ret)\n            else:\n                return \"endpoint {} does not exist\".format(k)\n        return ret", "category": "Python"}, {"instruction": "def _is_viable_phone_number(number):\n    \"\"\"Checks to see if a string could possibly be a phone number.\n\n    At the moment, checks to see that the string begins with at least 2\n    digits, ignoring any punctuation commonly found in phone numbers.  This\n    method does not require the number to be normalized in advance - but does\n    assume that leading non-number symbols have been removed, such as by the\n    method _extract_possible_number.\n\n    Arguments:\n    number -- string to be checked for viability as a phone number\n\n    Returns True if the number could be a phone number of some sort, otherwise\n    False\n    \"\"\"\n", "input": "", "output": "    if len(number) < _MIN_LENGTH_FOR_NSN:\n        return False\n    match = fullmatch(_VALID_PHONE_NUMBER_PATTERN, number)\n    return bool(match)", "category": "Python"}, {"instruction": "def _D2O_Sublimation_Pressure(T):\n    \"\"\"Sublimation Pressure correlation for heavy water\n\n    Parameters\n    ----------\n    T : float\n        Temperature, [K]\n\n    Returns\n    -------\n    P : float\n        Pressure at sublimation line, [MPa]\n\n    Notes\n    ------\n    Raise :class:`NotImplementedError` if input isn't in limit:\n\n        * 210 \u2264 T \u2264 276.969\n\n    Examples\n    --------\n    >>> _Sublimation_Pressure(245)\n    3.27390934e-5\n\n    References\n    ----------\n    IAPWS, Revised Release on the IAPWS Formulation 2017 for the Thermodynamic\n    Properties of Heavy Water, http://www.iapws.org/relguide/Heavy.html.\n    \"\"\"\n", "input": "", "output": "    if 210 <= T <= 276.969:\n        Tita = T/276.969\n        suma = 0\n        ai = [-0.1314226e2, 0.3212969e2]\n        ti = [-1.73, -1.42]\n        for a, t in zip(ai, ti):\n            suma += a*(1-Tita**t)\n        return exp(suma)*0.00066159\n    else:\n        raise NotImplementedError(\"Incoming out of bound\")", "category": "Python"}, {"instruction": "def jsonload(path, **kw):\n    \"\"\"python 2 + 3 compatible version of json.load.\n\n    :return: The python object read from path.\n    \"\"\"\n", "input": "", "output": "    _kw = {}\n    if not PY2:\n        _kw['encoding'] = 'utf8'\n    with io.open(path, **_kw) as fp:\n        return json.load(fp, **kw)", "category": "Python"}, {"instruction": "def from_string(cls, token_string):\n        \"\"\" `token_string` should be the string representation from the server. \"\"\"\n", "input": "", "output": "        # unhexlify works fine with unicode input in everythin but pypy3, where it Raises \"TypeError: 'str' does not support the buffer interface\"\n        if isinstance(token_string, six.text_type):\n            token_string = token_string.encode('ascii')\n        # The BOP stores a hex string\n        return cls(unhexlify(token_string))", "category": "Python"}, {"instruction": "def from_parameter(cls: Type[UnlockParameterType], parameter: str) -> Optional[Union[SIGParameter, XHXParameter]]:\n        \"\"\"\n        Return UnlockParameter instance from parameter string\n\n        :param parameter: Parameter string\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        sig_param = SIGParameter.from_parameter(parameter)\n        if sig_param:\n            return sig_param\n        else:\n            xhx_param = XHXParameter.from_parameter(parameter)\n            if xhx_param:\n                return xhx_param\n\n        return None", "category": "Python"}, {"instruction": "def cmp_dict(d1, d2, ignore_keys=[]):\n    \"\"\"Compare dicts ignoring select keys\"\"\"\n", "input": "", "output": "    # https://stackoverflow.com/questions/10480806/compare-dictionaries-ignoring-specific-keys\n    return {k: v for k, v in d1.items() if k not in ignore_keys} \\\n        == {k: v for k, v in d2.items() if k not in ignore_keys}", "category": "Python"}, {"instruction": "def decode_iter_request(data: dict) -> Optional[Union[str, int]]:\n    \"\"\"\n    Decode incoming response from an iteration request\n\n    Args:\n        data: Response data\n\n    Returns:\n        Next itervalue\n    \"\"\"\n", "input": "", "output": "    if \"response_metadata\" in data:\n        return data[\"response_metadata\"].get(\"next_cursor\")\n    elif \"paging\" in data:\n        current_page = int(data[\"paging\"].get(\"page\", 1))\n        max_page = int(data[\"paging\"].get(\"pages\", 1))\n\n        if current_page < max_page:\n            return current_page + 1\n    elif \"has_more\" in data and data[\"has_more\"] and \"latest\" in data:\n        return data[\"messages\"][-1][\"ts\"]\n\n    return None", "category": "Python"}, {"instruction": "def received_message(self, address, data):\n        \"\"\"Process a message received from the KNX bus.\"\"\"\n", "input": "", "output": "        self.value_cache.set(address, data)\n        if self.notify:\n            self.notify(address, data)\n\n        try:\n            listeners = self.address_listeners[address]\n        except KeyError:\n            listeners = []\n\n        for listener in listeners:\n            listener(address, data)", "category": "Python"}, {"instruction": "def visit_IfExp(self, node):\n        '''\n        Resulting node alias to either branch\n\n        >>> from pythran import passmanager\n        >>> pm = passmanager.PassManager('demo')\n        >>> module = ast.parse('def foo(a, b, c): return a if c else b')\n        >>> result = pm.gather(Aliases, module)\n        >>> Aliases.dump(result, filter=ast.IfExp)\n        (a if c else b) => ['a', 'b']\n        '''\n", "input": "", "output": "        self.visit(node.test)\n        rec = [self.visit(n) for n in (node.body, node.orelse)]\n        return self.add(node, set.union(*rec))", "category": "Python"}, {"instruction": "def nato(sentence, pad=' ', format='telephony'):\n    '''\n    Transform a sentence using the NATO spelling alphabet.\n\n    :param sentence: input sentence\n    :param pad: default ``' '``\n    :param format: default ``telephony``, options ``telephony`` or ``phonetic``\n\n    >>> print(nato('Python'))\n    papa yankee tango hotel oscar november\n\n    >>> print(nato('Python', format='phonetic'))\n    pah-pah yang-key tang-go hoh-tel oss-cah no-vem-ber\n\n    '''\n", "input": "", "output": "    try:\n        return '' + ALPHABET['nato'][format](sentence, pad)\n    except KeyError:\n        raise TypeError('Unsupported NATO alphabet \"%s\"' % (format,))", "category": "Python"}, {"instruction": "def info(self, cloud=None, api_key=None, version=None, **kwargs):\n        \"\"\"\n        Return the current state of the model associated with a given collection\n        \"\"\"\n", "input": "", "output": "        url_params = {\"batch\": False, \"api_key\": api_key, \"version\": version, \"method\": \"info\"}\n        return self._api_handler(None, cloud=cloud, api=\"custom\", url_params=url_params, **kwargs)", "category": "Python"}, {"instruction": "def sliding_window(time_series, width, step, order='F'):\n    '''\n    Segments univariate time series with sliding window\n\n    Parameters\n    ----------\n    time_series : array like shape [n_samples]\n        time series or sequence\n    width : int > 0\n        segment width in samples\n    step : int > 0\n        stepsize for sliding in samples\n\n    Returns\n    -------\n    w : array like shape [n_segments, width]\n        resampled time series segments\n    '''\n", "input": "", "output": "    w = np.hstack([time_series[i:1 + i - width or None:step] for i in range(0, width)])\n    result = w.reshape((int(len(w) / width), width), order='F')\n    if order == 'F':\n        return result\n    else:\n        return np.ascontiguousarray(result)", "category": "Python"}, {"instruction": "def enabled(job_label, runas=None):\n    '''\n    Return True if the named service is enabled, false otherwise\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enabled <service label>\n    '''\n", "input": "", "output": "    overrides_data = dict(plistlib.readPlist(\n        '/var/db/launchd.db/com.apple.launchd/overrides.plist'\n    ))\n    if overrides_data.get(job_label, False):\n        if overrides_data[job_label]['Disabled']:\n            return False\n        else:\n            return True\n    else:\n        return False", "category": "Python"}, {"instruction": "def remove_all_gap_columns( self ):\n        \"\"\"\n        Remove any columns containing only gaps from alignment components,\n        text of components is modified IN PLACE.\n        \"\"\"\n", "input": "", "output": "        seqs = []\n        for c in self.components:\n            try:\n                seqs.append( list( c.text ) )\n            except TypeError:\n                seqs.append( None )\n        i = 0\n        text_size = self.text_size\n        while i < text_size:\n            all_gap = True\n            for seq in seqs:\n                if seq is None: continue\n                if seq[i] != '-': all_gap = False\n            if all_gap:\n                for seq in seqs:\n                    if seq is None: continue\n                    del seq[i]\n                text_size -= 1\n            else:\n                i += 1\n        for i in range( len( self.components ) ):\n            if seqs[i] is None: continue\n            self.components[i].text = ''.join( seqs[i] )\n        self.text_size = text_size", "category": "Python"}, {"instruction": "def bulk_cursor_execute(self, bulk_cursor):\n        \"\"\"\n            Executes the bulk_cursor\n\n            :param bulk_cursor: Cursor to perform bulk operations\n            :type bulk_cursor: pymongo bulk cursor object\n\n            :returns: pymongo bulk cursor object (for bulk operations)\n        \"\"\"\n", "input": "", "output": "        try:\n            result = bulk_cursor.execute()\n        except BulkWriteError as bwe:\n            msg = \"bulk_cursor_execute: Exception in executing Bulk cursor to mongo with {error}\".format(\n                error=str(bwe))\n            raise Exception(msg)\n        except Exception as e:\n            msg = \"Mongo Bulk cursor could not be fetched, Error: {error}\".format(\n                error=str(e))\n            raise Exception(msg)", "category": "Python"}, {"instruction": "def log(array, cutoff):\n    \"\"\"\n    Compute the logarithm of an array with a cutoff on the small values\n    \"\"\"\n", "input": "", "output": "    arr = numpy.copy(array)\n    arr[arr < cutoff] = cutoff\n    return numpy.log(arr)", "category": "Python"}, {"instruction": "def flush(table='filter', chain='', family='ipv4'):\n    '''\n    Flush the chain in the specified table, flush all chains in the specified\n    table if not specified chain.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' iptables.flush filter INPUT\n\n        IPv6:\n        salt '*' iptables.flush filter INPUT family=ipv6\n    '''\n", "input": "", "output": "\n    wait = '--wait' if _has_option('--wait', family) else ''\n    cmd = '{0} {1} -t {2} -F {3}'.format(_iptables_cmd(family), wait, table, chain)\n    out = __salt__['cmd.run'](cmd)\n    return out", "category": "Python"}, {"instruction": "def run_get_data_background(macs, queue):\n    \"\"\"\n    Background process from RuuviTag Sensors\n    \"\"\"\n", "input": "", "output": "\n    def callback(data):\n        data[1]['time'] = str(datetime.now())\n        queue.put(data)\n\n    RuuviTagSensor.get_datas(callback, macs)", "category": "Python"}, {"instruction": "def execute_greenlet_async(func, *args, **kwargs):\n  \"\"\"\n  Executes `func` in a separate greenlet in the same process. Memory and other\n  resources are available (e.g. TCP connections etc.) `args` and `kwargs` are\n  passed to `func`.\n  \"\"\"\n", "input": "", "output": "  global _GREENLET_EXECUTOR\n  if _GREENLET_EXECUTOR is None:\n    _GREENLET_EXECUTOR = GreenletExecutor(\n      num_greenlets=settings.node.greenlet_pool_size)\n  return _GREENLET_EXECUTOR.submit(func, *args, **kwargs)", "category": "Python"}, {"instruction": "def __printCommandSyntax(self, command):\n        \"\"\" Command-completion helper method: print command syntax \"\"\"\n", "input": "", "output": "        commandHelp = self.completion[command]\n        if commandHelp != None and len(commandHelp) > 2:\n            commandValues = commandHelp[2]\n            #commandDefault = commandHelp[3]\n            displayHelp = [self._color(self.COLOR_WHITE, command)]\n            if commandValues != None:\n                valuesIsEnum = len(commandHelp) >= 6\n                if '+' in command or command.upper() in ['ATS0']:\n                    displayHelp.append(self._color(self.COLOR_WHITE, '='))\n                displayHelp.append(('|' if valuesIsEnum else ',').join([value[0] for value in commandValues]))\n            sys.stdout.write('\\r Syntax: {0}\\n'.format(self._color(self.COLOR_WHITE, ''.join(displayHelp))))\n            sys.stdout.flush()\n            self._refreshInputPrompt(len(self.inputBuffer))", "category": "Python"}, {"instruction": "def query_induction(self, nodes: List[Node]) -> List[Edge]:\n        \"\"\"Get all edges between any of the given nodes (minimum length of 2).\"\"\"\n", "input": "", "output": "        if len(nodes) < 2:\n            raise ValueError('not enough nodes given to induce over')\n\n        return self.session.query(Edge).filter(self._edge_both_nodes(nodes)).all()", "category": "Python"}, {"instruction": "def H(self) -> 'QubitVector':\n        \"\"\"Return the conjugate transpose of this tensor.\"\"\"\n", "input": "", "output": "        N = self.qubit_nb\n        R = self.rank\n\n        # (super) operator transpose\n        tensor = self.tensor\n        tensor = bk.reshape(tensor, [2**(N*R//2)] * 2)\n        tensor = bk.transpose(tensor)\n        tensor = bk.reshape(tensor, [2] * R * N)\n\n        tensor = bk.conj(tensor)\n\n        return QubitVector(tensor, self.qubits)", "category": "Python"}, {"instruction": "def get_current_path(index=2):  # type: (int) -> str\n    \"\"\"\n    Get the caller's path to sys.path\n    If the caller is a CLI through stdin, the current working directory is used\n    \"\"\"\n", "input": "", "output": "    try:\n        path = _caller_path(index)\n    except RuntimeError:\n        path = os.getcwd()\n    return path", "category": "Python"}, {"instruction": "def wallet_work_get(self, wallet):\n        \"\"\"\n        Returns a list of pairs of account and work from **wallet**\n\n        .. enable_control required\n        .. version 8.0 required\n\n        :param wallet: Wallet to return work for\n        :type wallet: str\n\n        :raises: :py:exc:`nano.rpc.RPCException`\n\n        >>> rpc.wallet_work_get(\n        ...     wallet=\"000D1BAEC8EC208142C99059B393051BAC8380F9B5A2E6B2489A277D81789F3F\"\n        ... )\n        {\n            \"xrb_1111111111111111111111111111111111111111111111111111hifc8npp\":\n                \"432e5cf728c90f4f\"\n        }\n\n        \"\"\"\n", "input": "", "output": "\n        wallet = self._process_value(wallet, 'wallet')\n\n        payload = {\"wallet\": wallet}\n\n        resp = self.call('wallet_work_get', payload)\n\n        return resp.get('works') or {}", "category": "Python"}, {"instruction": "def shiftAccent(self, shiftAmount):\n        '''\n        Move the whole accent earlier or later\n        '''\n", "input": "", "output": "        if shiftAmount == 0:\n            return\n        \n        self.pointList = [(time + shiftAmount, pitch)\n                          for time, pitch in self.pointList]\n        \n        # Update shift amounts\n        if shiftAmount < 0:\n            self.netLeftShift += shiftAmount\n        elif shiftAmount >= 0:\n            self.netRightShift += shiftAmount", "category": "Python"}, {"instruction": "def get_asset_lookup_session_for_repository(self, repository_id):\n        \"\"\"Gets the ``OsidSession`` associated with the asset lookup service for the given repository.\n\n        arg:    repository_id (osid.id.Id): the ``Id`` of the repository\n        return: (osid.repository.AssetLookupSession) - the new\n                ``AssetLookupSession``\n        raise:  NotFound - ``repository_id`` not found\n        raise:  NullArgument - ``repository_id`` is ``null``\n        raise:  OperationFailed - ``unable to complete request``\n        raise:  Unimplemented - ``supports_asset_lookup()`` or\n                ``supports_visible_federation()`` is ``false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_asset_lookup()`` and\n        ``supports_visible_federation()`` are ``true``.*\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_asset_lookup():\n            raise errors.Unimplemented()\n        ##\n        # Also include check to see if the catalog Id is found otherwise raise errors.NotFound\n        ##\n        # pylint: disable=no-member\n        return sessions.AssetLookupSession(repository_id, runtime=self._runtime)", "category": "Python"}, {"instruction": "def valid_batch(self):\n        \"\"\" Returns a single batch with all the validation cases.\"\"\"\n", "input": "", "output": "\n        valid_fns = list(zip(*self.corpus.get_valid_fns()))\n        return self.load_batch(valid_fns)", "category": "Python"}, {"instruction": "def _guess_header(info):\n    \"\"\"Add the first group to get report with some factor\"\"\"\n", "input": "", "output": "    value = \"group\"\n    if \"metadata\" in info:\n        if info[\"metadata\"]:\n            return \",\".join(map(str, info[\"metadata\"].keys()))\n    return value", "category": "Python"}, {"instruction": "def contains_shebang(f):\n    \"\"\"\n    Returns true if any shebang line is present in the first line of the file.\n    \"\"\"\n", "input": "", "output": "    first_line = f.readline()\n    if first_line in shebangs.values():\n        return True\n    return False", "category": "Python"}, {"instruction": "def load_pygame(filename, *args, **kwargs):\n    \"\"\" Load a TMX file, images, and return a TiledMap class\n\n    PYGAME USERS: Use me.\n\n    this utility has 'smart' tile loading.  by default any tile without\n    transparent pixels will be loaded for quick blitting.  if the tile has\n    transparent pixels, then it will be loaded with per-pixel alpha.  this is\n    a per-tile, per-image check.\n\n    if a color key is specified as an argument, or in the tmx data, the\n    per-pixel alpha will not be used at all. if the tileset's image has colorkey\n    transparency set in Tiled, the util_pygam will return images that have their\n    transparency already set.\n\n    TL;DR:\n    Don't attempt to convert() or convert_alpha() the individual tiles.  It is\n    already done for you.\n    \"\"\"\n", "input": "", "output": "    kwargs['image_loader'] = pygame_image_loader\n    return pytmx.TiledMap(filename, *args, **kwargs)", "category": "Python"}, {"instruction": "def __create_tcp_top(self, packet):\n        \"\"\"\n        witch the complete packet set top header\n        \"\"\"\n", "input": "", "output": "        length = len(packet)\n        top = pack('<HHI', const.MACHINE_PREPARE_DATA_1, const.MACHINE_PREPARE_DATA_2, length)\n        return top + packet", "category": "Python"}, {"instruction": "def create(self, asset_content, friendly_name, tags='', optimize=False):\n        \"\"\"\n        Create an asset on the server\n        You must provide the asset with a friendly name\n        for the server to generate a path from.\n        \"\"\"\n", "input": "", "output": "\n        return self._create_asset({\n            'asset': b64encode(asset_content),\n            'friendly-name': friendly_name,\n            'tags': tags,\n            'optimize': optimize,\n            'type': 'base64'\n        })", "category": "Python"}, {"instruction": "def get_absolute_name(self):\n        \"\"\" Returns the full dotted name of this field \"\"\"\n", "input": "", "output": "        res = []\n        current = self\n\n        while type(current) != type(None):\n            if current.__matched_index:\n                res.append('$')\n            res.append(current.get_type().db_field)\n            current = current._get_parent()\n        return '.'.join(reversed(res))", "category": "Python"}, {"instruction": "def set_file(self, file=None, is_modified=False, is_untitled=False):\n        \"\"\"\n        Sets the editor file.\n\n        :param File: File to set.\n        :type File: unicode\n        :param is_modified: File modified state.\n        :type is_modified: bool\n        :param is_untitled: File untitled state.\n        :type is_untitled: bool\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        LOGGER.debug(\"> Setting '{0}' editor file.\".format(file))\n        self.__file = file\n        self.__is_untitled = is_untitled\n        self.set_modified(is_modified)\n        self.set_title()\n        return True", "category": "Python"}, {"instruction": "def integer(cls, integer, bits = None):\n        \"\"\"\n        @type  integer: int\n        @param integer: Integer.\n\n        @type  bits: int\n        @param bits:\n            (Optional) Number of bits of the target architecture.\n            The default is platform dependent. See: L{HexDump.integer_size}\n\n        @rtype:  str\n        @return: Text output.\n        \"\"\"\n", "input": "", "output": "        if bits is None:\n            integer_size = cls.integer_size\n        else:\n            integer_size = bits / 4\n        return ('%%.%dX' % integer_size) % integer", "category": "Python"}, {"instruction": "def schema_map(schema):\n    \"\"\"Return a valid ICachedItemMapper.map for schema\"\"\"\n", "input": "", "output": "    mapper = {}\n    for name in getFieldNames(schema):\n        mapper[name] = name\n    return mapper", "category": "Python"}, {"instruction": "def close(self, filehandle):\n        \"\"\"Close openend file if no longer used.\"\"\"\n", "input": "", "output": "        with self.lock:\n            if filehandle in self.files:\n                self.files[filehandle] -= 1\n                # trim the file cache\n                index = 0\n                size = len(self.past)\n                while size > self.size and index < size:\n                    filehandle = self.past[index]\n                    if self.files[filehandle] == 0:\n                        filehandle.close()\n                        del self.files[filehandle]\n                        del self.past[index]\n                        size -= 1\n                    else:\n                        index += 1", "category": "Python"}, {"instruction": "def __global_logging_exception_handler(exc_type, exc_value, exc_traceback):\n    '''\n    This function will log all un-handled python exceptions.\n    '''\n", "input": "", "output": "    if exc_type.__name__ == \"KeyboardInterrupt\":\n        # Do not log the exception or display the traceback on Keyboard Interrupt\n        # Stop the logging queue listener thread\n        if is_mp_logging_listener_configured():\n            shutdown_multiprocessing_logging_listener()\n    else:\n        # Log the exception\n        logging.getLogger(__name__).error(\n            'An un-handled exception was caught by salt\\'s global exception '\n            'handler:\\n%s: %s\\n%s',\n            exc_type.__name__,\n            exc_value,\n            ''.join(traceback.format_exception(\n                exc_type, exc_value, exc_traceback\n            )).strip()\n        )\n        # Call the original sys.excepthook\n        sys.__excepthook__(exc_type, exc_value, exc_traceback)", "category": "Python"}, {"instruction": "def measure_each(*qubits: raw_types.Qid,\n                 key_func: Callable[[raw_types.Qid], str] = str\n                 ) -> List[gate_operation.GateOperation]:\n    \"\"\"Returns a list of operations individually measuring the given qubits.\n\n    The qubits are measured in the computational basis.\n\n    Args:\n        *qubits: The qubits to measure.\n        key_func: Determines the key of the measurements of each qubit. Takes\n            the qubit and returns the key for that qubit. Defaults to str.\n\n    Returns:\n        A list of operations individually measuring the given qubits.\n    \"\"\"\n", "input": "", "output": "    return [MeasurementGate(1, key_func(q)).on(q) for q in qubits]", "category": "Python"}, {"instruction": "def write(self) -> None:\n        \"\"\"Call method |NetCDFFile.write| of all handled |NetCDFFile| objects.\n        \"\"\"\n", "input": "", "output": "        if self.folders:\n            init = hydpy.pub.timegrids.init\n            timeunits = init.firstdate.to_cfunits('hours')\n            timepoints = init.to_timepoints('hours')\n            for folder in self.folders.values():\n                for file_ in folder.values():\n                    file_.write(timeunits, timepoints)", "category": "Python"}, {"instruction": "def get_hashes(path, exclude=None):\n    '''\n    Get a dictionary of file paths and timestamps.\n\n    Paths matching `exclude` regex will be excluded.\n    '''\n", "input": "", "output": "    out = {}\n    for f in Path(path).rglob('*'):\n        if f.is_dir():\n            # We want to watch files, not directories.\n            continue\n        if exclude and re.match(exclude, f.as_posix()):\n            retox_log.debug(\"excluding '{}'\".format(f.as_posix()))\n            continue\n        pytime = f.stat().st_mtime\n        out[f.as_posix()] = pytime\n    return out", "category": "Python"}, {"instruction": "def fit(self, X, y=None):\n        \"\"\"\n        Fit a transformation from the pipeline\n        :param X (DataSet): the data to fit\n        \"\"\"\n", "input": "", "output": "        for columns, transformer in self.mapping:\n            if transformer is not None:\n                transformer.fit(self._get_columns(X, columns))\n        return self", "category": "Python"}, {"instruction": "def path_from_row_pks(row, pks, use_rowid, quote=True):\n    \"\"\" Generate an optionally URL-quoted unique identifier\n        for a row from its primary keys.\"\"\"\n", "input": "", "output": "    if use_rowid:\n        bits = [row['rowid']]\n    else:\n        bits = [\n            row[pk][\"value\"] if isinstance(row[pk], dict) else row[pk]\n            for pk in pks\n        ]\n    if quote:\n        bits = [urllib.parse.quote_plus(str(bit)) for bit in bits]\n    else:\n        bits = [str(bit) for bit in bits]\n\n    return ','.join(bits)", "category": "Python"}, {"instruction": "def _get_binary_replacement(first, second, cls):\n    \"\"\"Helper function for match_replace_binary\"\"\"\n", "input": "", "output": "    expr = ProtoExpr([first, second], {})\n    if LOG:\n        logger = logging.getLogger('QNET.create')\n    for key, rule in cls._binary_rules.items():\n        pat, replacement = rule\n        match_dict = match_pattern(pat, expr)\n        if match_dict:\n            try:\n                replaced = replacement(**match_dict)\n                if LOG:\n                    logger.debug(\n                        \"%sRule %s.%s: (%s, %s) -> %s\", (\"  \" * (LEVEL)),\n                        cls.__name__, key, expr.args, expr.kwargs, replaced)\n                return replaced\n            except CannotSimplify:\n                if LOG_NO_MATCH:\n                    logger.debug(\n                        \"%sRule %s.%s: no match: CannotSimplify\",\n                        (\"  \" * (LEVEL)), cls.__name__, key)\n                continue\n        else:\n            if LOG_NO_MATCH:\n                logger.debug(\n                    \"%sRule %s.%s: no match: %s\", (\"  \" * (LEVEL)),\n                    cls.__name__, key, match_dict.reason)\n    return None", "category": "Python"}, {"instruction": "def image_query(self, imagename=None):\n        \"\"\"Get the list of image info in image repository\n\n        :param imagename:  Used to retrieve the specified image info,\n               if not specified, all images info will be returned\n\n        :returns: A list that contains the specified or all images info\n        \"\"\"\n", "input": "", "output": "        try:\n            return self._imageops.image_query(imagename)\n        except exception.SDKBaseException:\n            LOG.error(\"Failed to query image\")\n            raise", "category": "Python"}, {"instruction": "def c_source(self):\n        \"\"\"Return strings.\"\"\"\n", "input": "", "output": "        relocs = Relocs(\n            ''.join(self.c_self_relocs()), *self.c_module_relocs()\n        )\n        return Source(\n            ''.join(self.c_typedefs()),\n            '' if self.opts.no_structs else self.c_struct(),\n            ''.join(self.c_hashes()),\n            ''.join(self.c_var_decls()),\n            relocs,\n            self.c_loadlib() + ''.join(self.c_getprocs())\n        )", "category": "Python"}, {"instruction": "def run_command_hooks(cmd_obj, hook_kind):\n    \"\"\"Run hooks registered for that command and phase.\n\n    *cmd_obj* is a finalized command object; *hook_kind* is either\n    'pre_hook' or 'post_hook'.\n    \"\"\"\n", "input": "", "output": "\n    hooks = getattr(cmd_obj, hook_kind, None)\n\n    if not hooks:\n        return\n\n    for modname, hook in hooks:\n        if isinstance(hook, str):\n            try:\n                hook_obj = resolve_name(hook)\n            except ImportError as exc:\n                raise DistutilsModuleError(\n                    'cannot find hook {0}: {1}'.format(hook, exc))\n        else:\n            hook_obj = hook\n\n        if not callable(hook_obj):\n            raise DistutilsOptionError('hook {0!r} is not callable' % hook)\n\n        log.info('running {0} from {1} for {2} command'.format(\n                 hook_kind.rstrip('s'), modname, cmd_obj.get_command_name()))\n\n        try:\n            hook_obj(cmd_obj)\n        except Exception:\n            log.error('{0} command hook {1} raised an exception: %s\\n'.format(\n                hook_obj.__name__, cmd_obj.get_command_name()))\n            log.error(traceback.format_exc())\n            sys.exit(1)", "category": "Python"}, {"instruction": "def walk(self, walker):\n        \"\"\"Walks each step in the underlying graph, in topological order.\n\n        Args:\n            walker (func): a walker function to be passed to\n                :class:`stacker.dag.DAG` to walk the graph.\n        \"\"\"\n", "input": "", "output": "\n        def walk_func(step):\n            # Before we execute the step, we need to ensure that it's\n            # transitive dependencies are all in an \"ok\" state. If not, we\n            # won't execute this step.\n            for dep in self.graph.downstream(step.name):\n                if not dep.ok:\n                    step.set_status(FailedStatus(\"dependency has failed\"))\n                    return step.ok\n\n            return step.run()\n\n        return self.graph.walk(walker, walk_func)", "category": "Python"}, {"instruction": "def save(self, *args, **kwargs):\n        \"\"\"\n        Save the created_by and last_modified_by fields based on the current admin user.\n        \"\"\"\n", "input": "", "output": "        if not self.instance.id:\n            self.instance.created_by = self.user\n        self.instance.last_modified_by = self.user\n        return super(ChangeableContentForm, self).save(*args, **kwargs)", "category": "Python"}, {"instruction": "def use_isolated_book_view(self):\n        \"\"\"Pass through to provider CommentLookupSession.use_isolated_book_view\"\"\"\n", "input": "", "output": "        self._book_view = ISOLATED\n        # self._get_provider_session('comment_lookup_session') # To make sure the session is tracked\n        for session in self._get_provider_sessions():\n            try:\n                session.use_isolated_book_view()\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def add_pending(self, family: str, email: str=None) -> models.Analysis:\n        \"\"\"Add pending entry for an analysis.\"\"\"\n", "input": "", "output": "        started_at = dt.datetime.now()\n        new_log = self.Analysis(family=family, status='pending', started_at=started_at)\n        new_log.user = self.user(email) if email else None\n        self.add_commit(new_log)\n        return new_log", "category": "Python"}, {"instruction": "def linear_warp(X, d, n, *args):\n    r\"\"\"Warp inputs with a linear transformation.\n    \n    Applies the warping\n    \n    .. math::\n        \n        w(x) = \\frac{x-a}{b-a}\n    \n    to each dimension. If you set `a=min(X)` and `b=max(X)` then this is a\n    convenient way to map your inputs to the unit hypercube.\n    \n    Parameters\n    ----------\n    X : array, (`M`,)\n        `M` inputs from dimension `d`.\n    d : non-negative int\n        The index (starting from zero) of the dimension to apply the warping to.\n    n : non-negative int\n        The derivative order to compute.\n    *args : 2N scalars\n        The remaining parameters to describe the warping, given as scalars.\n        These are given as `a_i`, `b_i` for each of the `D` dimensions. Note\n        that these must ALL be provided for each call.\n    \"\"\"\n", "input": "", "output": "    X = scipy.asarray(X, dtype=float)\n    \n    a = args[2 * d]\n    b = args[2 * d + 1]\n    \n    if n == 0:\n        return (X - a) / (b - a)\n    elif n == 1:\n        return 1.0 / (b - a) * scipy.ones_like(X)\n    else:\n        return scipy.zeros_like(X)", "category": "Python"}, {"instruction": "def applyaty(self, content, xty):\n        \"\"\"\n        Apply the type referenced in the I{arrayType} to the content\n        (child nodes) of the array.  Each element (node) in the array\n        that does not have an explicit xsi:type attribute is given one\n        based on the I{arrayType}.\n        @param content: An array content.\n        @type content: L{Content}\n        @param xty: The XSI type reference.\n        @type xty: str\n        @return: self\n        @rtype: L{Encoded}\n        \"\"\"\n", "input": "", "output": "        name = 'type'\n        ns = Namespace.xsins\n        parent = content.node\n        for child in parent.getChildren():\n            ref = child.get(name, ns)\n            if ref is None:\n                parent.addPrefix(ns[0], ns[1])\n                attr = ':'.join((ns[0], name))\n                child.set(attr, xty)\n        return self", "category": "Python"}, {"instruction": "def remove_number_words(text_string):\n    '''\n    Removes any integer represented as a word within text_string and returns the new string as\n    type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n", "input": "", "output": "    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        for word in NUMBER_WORDS:\n            text_string = re.sub(r'[\\S]*\\b'+word+r'[\\S]*', \"\", text_string)\n        return \" \".join(text_string.split())\n    else:\n        raise InputError(\"string not passed as argument\")", "category": "Python"}, {"instruction": "def _append(self, menu):\n        '''append this menu item to a menu'''\n", "input": "", "output": "        menu.Append(self.id(), self.name, self.description)", "category": "Python"}, {"instruction": "def uvw2enu(u: float, v: float, w: float,\n            lat0: float, lon0: float, deg: bool = True) -> Tuple[float, float, float]:\n    \"\"\"\n    Parameters\n    ----------\n\n    u : float or numpy.ndarray of float\n    v : float or numpy.ndarray of float\n    w : float or numpy.ndarray of float\n\n\n    Results\n    -------\n\n    East : float or numpy.ndarray of float\n        target east ENU coordinate (meters)\n    North : float or numpy.ndarray of float\n        target north ENU coordinate (meters)\n    Up : float or numpy.ndarray of float\n        target up ENU coordinate (meters)\n    \"\"\"\n", "input": "", "output": "    if deg:\n        lat0 = radians(lat0)\n        lon0 = radians(lon0)\n\n    t = cos(lon0) * u + sin(lon0) * v\n    East = -sin(lon0) * u + cos(lon0) * v\n    Up = cos(lat0) * t + sin(lat0) * w\n    North = -sin(lat0) * t + cos(lat0) * w\n\n    return East, North, Up", "category": "Python"}, {"instruction": "def list_all(self, name=None, visibility=None, member_status=None,\n            owner=None, tag=None, status=None, size_min=None, size_max=None,\n            sort_key=None, sort_dir=None):\n        \"\"\"\n        Returns all of the images in one call, rather than in paginated batches.\n        The same filtering options available in list() apply here, with the\n        obvious exception of limit and marker.\n        \"\"\"\n", "input": "", "output": "        return self._manager.list_all(name=name, visibility=visibility,\n                member_status=member_status, owner=owner, tag=tag,\n                status=status, size_min=size_min, size_max=size_max,\n                sort_key=sort_key, sort_dir=sort_dir)", "category": "Python"}, {"instruction": "def country_map(code):\n    '''\n    Country mapping\n    '''\n", "input": "", "output": "    code = str(code).upper()\n    global _country_maps\n    return _country_maps.get(code, code)", "category": "Python"}, {"instruction": "def chosen_angle_to_canonical_half_turns(\n        half_turns: Optional[Union[sympy.Basic, float]] = None,\n        rads: Optional[float] = None,\n        degs: Optional[float] = None,\n        default: float = 1.0,\n) -> Union[sympy.Basic, float]:\n    \"\"\"Returns a canonicalized half_turns based on the given arguments.\n\n    At most one of half_turns, rads, degs must be specified. If none are\n    specified, the output defaults to half_turns=1.\n\n    Args:\n        half_turns: The number of half turns to rotate by.\n        rads: The number of radians to rotate by.\n        degs: The number of degrees to rotate by\n        default: The half turns angle to use if nothing else is specified.\n\n    Returns:\n        A number of half turns.\n    \"\"\"\n", "input": "", "output": "    return canonicalize_half_turns(\n            chosen_angle_to_half_turns(\n                half_turns=half_turns,\n                rads=rads,\n                degs=degs,\n                default=default))", "category": "Python"}, {"instruction": "def upgrade_plan_list(self, subid, params=None):\n        ''' /v1/server/upgrade_plan_list\n        GET - account\n        Retrieve a list of the VPSPLANIDs for which a virtual machine\n        can be upgraded. An empty response array means that there are\n        currently no upgrades available.\n\n        Link: https://www.vultr.com/api/#server_upgrade_plan_list\n        '''\n", "input": "", "output": "        params = update_params(params, {'SUBID': subid})\n        return self.request('/v1/server/upgrade_plan_list', params, 'GET')", "category": "Python"}, {"instruction": "def _m(self, word, j):\n        \"\"\"m() measures the number of consonant sequences between k0 and j.\n        if c is a consonant sequence and v a vowel sequence, and <..>\n        indicates arbitrary presence,\n\n           <c><v>       gives 0\n           <c>vc<v>     gives 1\n           <c>vcvc<v>   gives 2\n           <c>vcvcvc<v> gives 3\n           ....\n        \"\"\"\n", "input": "", "output": "        n = 0\n        i = 0\n        while True:\n            if i > j:\n                return n\n            if not self._cons(word, i):\n                break\n            i = i + 1\n        i = i + 1\n\n        while True:\n            while True:\n                if i > j:\n                    return n\n                if self._cons(word, i):\n                    break\n                i = i + 1\n            i = i + 1\n            n = n + 1\n\n            while True:\n                if i > j:\n                    return n\n                if not self._cons(word, i):\n                    break\n                i = i + 1\n            i = i + 1", "category": "Python"}, {"instruction": "def register(PluginClass):\n    \"\"\"\n    Register a plugin class. This function will call back your plugin's\n    constructor.\n    \"\"\"\n", "input": "", "output": "    if PluginClass in _cache.keys():\n        raise Exception(\"Plugin class already registered\")\n    plugin = PluginClass()\n    _cache[PluginClass] = plugin\n\n    if getattr(PluginClass, 'extra_page_actions', False):\n        for key in plugin.extra_page_actions:\n            if key not in _extra_page_actions:\n                _extra_page_actions[key] = []\n            _extra_page_actions[key].extend(plugin.extra_page_actions[key])\n\n    if getattr(PluginClass, 'extra_edit_actions', False):\n        for key in plugin.extra_edit_actions:\n            if key not in _extra_edit_actions:\n                _extra_edit_actions[key] = []\n            _extra_edit_actions[key].extend(plugin.extra_edit_actions[key])\n\n\n    if getattr(PluginClass, 'navbar_links', False):\n        _navbar_links.extend(list(plugin.navbar_links))", "category": "Python"}, {"instruction": "def start_scan(self, active):\n        \"\"\"Start a scan. Will call self._on_device_found for each device scanned.\n        Args:\n            active (bool): Indicate if it is an active scan (probing for scan response) or not.\n        \"\"\"\n", "input": "", "output": "        try:\n            self.bable.start_scan(self._on_device_found, active_scan=active, sync=True)\n        except bable_interface.BaBLEException as err:\n            # If we are already scanning, raise an error only we tried to change the active scan param\n            if self._active_scan != active:\n                raise err\n\n        self._active_scan = active\n        self.scanning = True", "category": "Python"}, {"instruction": "def _CaptureExpression(self, frame, expression):\n    \"\"\"Evalutes the expression and captures it into a Variable object.\n\n    Args:\n      frame: evaluation context.\n      expression: watched expression to compile and evaluate.\n\n    Returns:\n      Variable object (which will have error status if the expression fails\n      to evaluate).\n    \"\"\"\n", "input": "", "output": "    rc, value = _EvaluateExpression(frame, expression)\n    if not rc:\n      return {'name': expression, 'status': value}\n\n    return self.CaptureNamedVariable(expression, value, 0,\n                                     self.expression_capture_limits)", "category": "Python"}, {"instruction": "def insertComponent(self, comp, row=0, col=0):\n        \"\"\"Inserts component into model\n\n        :param comp: Component to insert into the stimulus\n        :type comp: :class:`AbstractStimulusComponent<sparkle.stim.abstract_component.AbstractStimulusComponent>`\n        :param row: Track number to place comp in\n        :type row: int\n        :param col: location in track to insert component to\n        :type col: int\n        \"\"\"\n", "input": "", "output": "        if row > len(self._segments) -1:\n            self.insertEmptyRow()\n        self._segments[row].insert(col, comp)\n\n        # in case of samplerate change, just always update\n        self.updateCalibration()", "category": "Python"}, {"instruction": "def OnDependencies(self, event):\n        \"\"\"Display dependency dialog\"\"\"\n", "input": "", "output": "\n        dlg = DependencyDialog(self.main_window)\n        dlg.ShowModal()\n        dlg.Destroy()", "category": "Python"}, {"instruction": "def train(self, cloud=None, batch=False, api_key=None, version=None, **kwargs):\n        \"\"\"\n        This is the basic training endpoint. Given an existing dataset this endpoint will train a model.\n\n        Inputs\n        api_key (optional) - String: Your API key, required only if the key has not been declared\n          elsewhere. This allows the API to recognize a request as yours and automatically route it\n          to the appropriate destination.\n        cloud (optional) - String: Your private cloud domain, required only if the key has not been declared\n          elsewhere. This allows the API to recognize a request as yours and automatically route it\n          to the appropriate destination.\n        \"\"\"\n", "input": "", "output": "        url_params = {\"batch\": batch, \"api_key\": api_key, \"version\": version, 'method': \"train\"}\n        return self._api_handler(self.keywords['collection'], cloud=cloud, api=\"custom\", url_params=url_params, **kwargs)", "category": "Python"}, {"instruction": "def hcode(*content, sep=' '):\n    \"\"\"\n    Make mono-width text (HTML)\n\n    :param content:\n    :param sep:\n    :return:\n    \"\"\"\n", "input": "", "output": "    return _md(quote_html(_join(*content, sep=sep)), symbols=MD_SYMBOLS[6])", "category": "Python"}, {"instruction": "def validate(instance):\n    \"\"\" Validates a given ``instance``.\n\n    :param object instance: The instance to validate\n    :raises jsonschema.exceptions.ValidationError: On failed validation\n    \"\"\"\n", "input": "", "output": "\n    jsonschema.validate(\n        to_dict(instance, dict_type=dict), build_schema(instance.__class__)\n    )", "category": "Python"}, {"instruction": "def import_class(class_uri):\n    \"\"\"\n    Import a class by string 'from.path.module.class'\n    \"\"\"\n", "input": "", "output": "\n    parts = class_uri.split('.')\n    class_name = parts.pop()\n    module_uri = '.'.join(parts)\n\n    try:\n        module = import_module(module_uri)\n    except ImportError as e:\n        # maybe we are still in a module, test going up one level\n        try:\n            module = import_class(module_uri)\n        except Exception:\n            # if failure raise the original exception\n            raise e\n\n    return getattr(module, class_name)", "category": "Python"}, {"instruction": "def wait_for_keys(self, *keys, timeout=0):\n        \"\"\"Waits until one of the specified keys was pressed, and returns \n        which key was pressed.\n\n        :param keys: iterable of integers of pygame-keycodes, or simply \n            multiple keys passed via multiple arguments\n        :type keys: iterable\n        :param timeout: number of seconds to wait till the function returns\n        :type timeout: float\n\n        :returns: The keycode of the pressed key, or None in case of timeout\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "        if len(keys) == 1 and _is_iterable(keys[0]):\n            keys = keys[0]\n\n        return self.listen_until_return(Handler.key_press(keys), timeout=timeout)", "category": "Python"}, {"instruction": "def time_partitioning(self):\n        \"\"\"google.cloud.bigquery.table.TimePartitioning: Specifies time-based\n        partitioning for the destination table.\n        \"\"\"\n", "input": "", "output": "        prop = self._get_sub_prop(\"timePartitioning\")\n        if prop is not None:\n            prop = TimePartitioning.from_api_repr(prop)\n        return prop", "category": "Python"}, {"instruction": "def prepare_dir(app, directory, delete=False):\n    \"\"\"Create apidoc dir, delete contents if delete is True.\n\n    :param app: the sphinx app\n    :type app: :class:`sphinx.application.Sphinx`\n    :param directory: the apidoc directory. you can use relative paths here\n    :type directory: str\n    :param delete: if True, deletes the contents of apidoc. This acts like an override switch.\n    :type delete: bool\n    :returns: None\n    :rtype: None\n    :raises: None\n    \"\"\"\n", "input": "", "output": "    logger.info(\"Preparing output directories for jinjaapidoc.\")\n    if os.path.exists(directory):\n        if delete:\n            logger.debug(\"Deleting dir %s\", directory)\n            shutil.rmtree(directory)\n            logger.debug(\"Creating dir %s\", directory)\n            os.mkdir(directory)\n    else:\n        logger.debug(\"Creating %s\", directory)\n        os.mkdir(directory)", "category": "Python"}, {"instruction": "def read_prologue(self):\n        \"\"\"Read the prologue metadata.\"\"\"\n", "input": "", "output": "\n        with open(self.filename, \"rb\") as fp_:\n            fp_.seek(self.mda['total_header_length'])\n            data = np.fromfile(fp_, dtype=hrit_prologue, count=1)\n            self.prologue.update(recarray2dict(data))\n            try:\n                impf = np.fromfile(fp_, dtype=impf_configuration, count=1)[0]\n            except IndexError:\n                logger.info('No IMPF configuration field found in prologue.')\n            else:\n                self.prologue.update(recarray2dict(impf))", "category": "Python"}, {"instruction": "def add_node(self, node):\n        \"\"\"Add a new node to the scheduler.\n\n        From now on the node will be assigned work units to be executed.\n\n        Called by the ``DSession.worker_workerready`` hook when it successfully\n        bootstraps a new node.\n        \"\"\"\n", "input": "", "output": "        assert node not in self.assigned_work\n        self.assigned_work[node] = OrderedDict()", "category": "Python"}, {"instruction": "def master_open():\n    \"\"\"master_open() -> (master_fd, slave_name)\n    Open a pty master and return the fd, and the filename of the slave end.\n    Deprecated, use openpty() instead.\"\"\"\n", "input": "", "output": "\n    try:\n        master_fd, slave_fd = os.openpty()\n    except (AttributeError, OSError):\n        pass\n    else:\n        slave_name = os.ttyname(slave_fd)\n        os.close(slave_fd)\n        return master_fd, slave_name\n\n    return _open_terminal()", "category": "Python"}, {"instruction": "def contains(self, x: int, y: int) -> bool:\n        \"\"\"Returns True if this node contains these coordinates.\n\n        Args:\n            x (int): X position to check.\n            y (int): Y position to check.\n\n        Returns:\n            bool: True if this node contains these coordinates.\n                  Otherwise False.\n        \"\"\"\n", "input": "", "output": "        return (\n            self.x <= x < self.x + self.width\n            and self.y <= y < self.y + self.height\n        )", "category": "Python"}, {"instruction": "def format(self, obj, context, maxlevels, level):  # pylint: disable=arguments-differ\n        \"\"\" Mask obj if it looks like an URL, then pass it to the super class.\n        \"\"\"\n", "input": "", "output": "        if isinstance(obj, basestring) and \"://\" in fmt.to_unicode(obj):\n            obj = mask_keys(obj)\n        return pprint.PrettyPrinter.format(self, obj, context, maxlevels, level)", "category": "Python"}, {"instruction": "def _normalize_sort_SQL(self, field_name, field_vals, sort_dir_str):\n        \"\"\"\n        allow sorting by a set of values\n\n        http://stackoverflow.com/questions/3303851/sqlite-and-custom-order-by\n        \"\"\"\n", "input": "", "output": "        fvi = None\n        if sort_dir_str == 'ASC':\n            fvi = (t for t in enumerate(field_vals)) \n\n        else:\n            fvi = (t for t in enumerate(reversed(field_vals))) \n\n        query_sort_str = ['  CASE {}'.format(self._normalize_name(field_name))]\n        query_args = []\n        for i, v in fvi:\n            query_sort_str.append('    WHEN {} THEN {}'.format(self.val_placeholder, i))\n            query_args.append(v)\n\n        query_sort_str.append('  END')\n        query_sort_str = \"\\n\".join(query_sort_str)\n        return query_sort_str, query_args", "category": "Python"}, {"instruction": "def ExpandDims(a, dim):\n    \"\"\"\n    Expand dim op, i.e. add singular axis at dim.\n    \"\"\"\n", "input": "", "output": "    shape = list(a.shape)\n    if dim >= 0:\n        shape.insert(dim, 1)\n    else:\n        shape.insert(len(shape) + dim + 1, 1)\n    return np.copy(a).reshape(*shape),", "category": "Python"}, {"instruction": "def escalation_date(self, escalation_date):\n        \"\"\"\n        Sets the task escalation_date\n        Args:\n            escalation_date: Converted to %Y-%m-%dT%H:%M:%SZ date format\n        \"\"\"\n", "input": "", "output": "        if not self.can_update():\n            self._tcex.handle_error(910, [self.type])\n\n        escalation_date = self._utils.format_datetime(\n            escalation_date, date_format='%Y-%m-%dT%H:%M:%SZ'\n        )\n        self._data['escalationDate'] = escalation_date\n        request = {'escalationDate': escalation_date}\n        return self.tc_requests.update(self.api_type, self.api_sub_type, self.unique_id, request)", "category": "Python"}, {"instruction": "def _asStr(self):\n        '''\n            _asStr - Get the string representation of this style\n\n              @return <str> - A string representation of this style (semicolon separated, key: value format)\n        '''\n", "input": "", "output": "\n        styleDict = self._styleDict\n        if styleDict:\n            return '; '.join([name + ': ' + value for name, value in styleDict.items()])\n        return ''", "category": "Python"}, {"instruction": "def get_module_source(self, fullname):\n        \"\"\"Given the name of a loaded module `fullname`, attempt to find its\n        source code.\n\n        :returns:\n            Tuple of `(module path, source text, is package?)`, or :data:`None`\n            if the source cannot be found.\n        \"\"\"\n", "input": "", "output": "        tup = self._found_cache.get(fullname)\n        if tup:\n            return tup\n\n        for method in self.get_module_methods:\n            tup = method(self, fullname)\n            if tup:\n                #LOG.debug('%r returned %r', method, tup)\n                break\n        else:\n            tup = None, None, None\n            LOG.debug('get_module_source(%r): cannot find source', fullname)\n\n        self._found_cache[fullname] = tup\n        return tup", "category": "Python"}, {"instruction": "def header_length(bytearray):\n    \"\"\"Return the length of s when it is encoded with base64.\"\"\"\n", "input": "", "output": "    groups_of_3, leftover = divmod(len(bytearray), 3)\n    # 4 bytes out for each 3 bytes (or nonzero fraction thereof) in.\n    n = groups_of_3 * 4\n    if leftover:\n        n += 4\n    return n", "category": "Python"}, {"instruction": "def _height_and_width(self):\n        \"\"\"\n        Override for blessings.Terminal._height_and_width\n        Adds caching\n        \"\"\"\n", "input": "", "output": "\n        try:\n            return self._cache['height_and_width']\n        except KeyError:\n            handw = self._cache['height_and_width'] = super(Terminal, self)._height_and_width()\n            return handw", "category": "Python"}, {"instruction": "def reverse_transform(self, column):\n        \"\"\"Applies the natural logarithm function to turn positive values into real ranged values.\n\n        Args:\n            column (pandas.DataFrame): Data to transform.\n\n        Returns:\n            pd.DataFrame\n        \"\"\"\n", "input": "", "output": "        self.check_data_type()\n\n        return pd.DataFrame({self.col_name: np.log(column[self.col_name])})", "category": "Python"}, {"instruction": "def init_remote(self):\n        '''\n        Initialize/attach to a remote using pygit2. Return a boolean which\n        will let the calling function know whether or not a new repo was\n        initialized by this function.\n        '''\n", "input": "", "output": "        # https://github.com/libgit2/pygit2/issues/339\n        # https://github.com/libgit2/libgit2/issues/2122\n        home = os.path.expanduser('~')\n        pygit2.settings.search_path[pygit2.GIT_CONFIG_LEVEL_GLOBAL] = home\n        new = False\n        if not os.listdir(self.cachedir):\n            # Repo cachedir is empty, initialize a new repo there\n            self.repo = pygit2.init_repository(self.cachedir)\n            new = True\n        else:\n            # Repo cachedir exists, try to attach\n            try:\n                self.repo = pygit2.Repository(self.cachedir)\n            except KeyError:\n                log.error(_INVALID_REPO, self.cachedir, self.url, self.role)\n                return new\n\n        self.gitdir = salt.utils.path.join(self.repo.workdir, '.git')\n        self.enforce_git_config()\n\n        return new", "category": "Python"}, {"instruction": "def focus_changed(self):\r\n        \"\"\"Editor focus has changed\"\"\"\n", "input": "", "output": "        fwidget = QApplication.focusWidget()\r\n        for finfo in self.data:\r\n            if fwidget is finfo.editor:\r\n                self.refresh()\r\n        self.editor_focus_changed.emit()", "category": "Python"}, {"instruction": "def node_inclusion_predicate_builder(nodes: Iterable[BaseEntity]) -> NodePredicate:\n    \"\"\"Build a function that returns true for the given nodes.\"\"\"\n", "input": "", "output": "    nodes = set(nodes)\n\n    @node_predicate\n    def node_inclusion_predicate(node: BaseEntity) -> bool:\n        ", "category": "Python"}, {"instruction": "async def invites(self):\n        \"\"\"|coro|\n\n        Returns a list of all active instant invites from this channel.\n\n        You must have :attr:`~.Permissions.manage_guild` to get this information.\n\n        Raises\n        -------\n        Forbidden\n            You do not have proper permissions to get the information.\n        HTTPException\n            An error occurred while fetching the information.\n\n        Returns\n        -------\n        List[:class:`Invite`]\n            The list of invites that are currently active.\n        \"\"\"\n", "input": "", "output": "\n        state = self._state\n        data = await state.http.invites_from_channel(self.id)\n        result = []\n\n        for invite in data:\n            invite['channel'] = self\n            invite['guild'] = self.guild\n            result.append(Invite(state=state, data=invite))\n\n        return result", "category": "Python"}, {"instruction": "def get_ranked_english():\n    '''\n    wikitionary has a list of ~40k English words, ranked by frequency of occurance in TV and movie transcripts.\n    more details at:\n    http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/explanation\n\n    the list is separated into pages of 1000 or 2000 terms each.\n    * the first 10k words are separated into pages of 1000 terms each.\n    * the remainder is separated into pages of 2000 terms each:\n    '''\n", "input": "", "output": "    URL_TMPL = 'http://en.wiktionary.org/wiki/Wiktionary:Frequency_lists/TV/2006/%s'\n    urls = []\n    for i in range(10):\n        freq_range = \"%d-%d\" % (i * 1000 + 1, (i+1) * 1000)\n        urls.append(URL_TMPL % freq_range)\n\n    for i in range(0,15):\n        freq_range = \"%d-%d\" % (10000 + 2 * i * 1000 + 1, 10000 + (2 * i + 2) * 1000)\n        urls.append(URL_TMPL % freq_range)\n\n    urls.append(URL_TMPL % '40001-41284')\n\n    ranked_terms = [] # ordered by rank, in decreasing frequency.\n    for url in urls:\n        html, is_cached = wiki_download(url)\n        if not is_cached:\n            time.sleep(SLEEP_TIME)\n        new_terms = parse_wiki_terms(html)\n        ranked_terms.extend(new_terms)\n\n    return ranked_terms", "category": "Python"}, {"instruction": "def hash_citation(type: str, reference: str) -> str:\n    \"\"\"Create a hash for a type/reference pair of a citation.\n\n    :param type: The corresponding citation type\n    :param reference: The citation reference\n    \"\"\"\n", "input": "", "output": "    s = u'{type}:{reference}'.format(type=type, reference=reference)\n    return hashlib.sha512(s.encode('utf8')).hexdigest()", "category": "Python"}, {"instruction": "def compute_eigenvalues(in_prefix, out_prefix):\n    \"\"\"Computes the Eigenvalues using smartpca from Eigensoft.\n\n    :param in_prefix: the prefix of the input files.\n    :param out_prefix: the prefix of the output files.\n\n    :type in_prefix: str\n    :type out_prefix: str\n\n    Creates a \"parameter file\" used by smartpca and runs it.\n\n    \"\"\"\n", "input": "", "output": "    # First, we create the parameter file\n    with open(out_prefix + \".parameters\", \"w\") as o_file:\n        print >>o_file, \"genotypename:    \" + in_prefix + \".bed\"\n        print >>o_file, \"snpname:         \" + in_prefix + \".bim\"\n        print >>o_file, \"indivname:       \" + in_prefix + \".fam\"\n        print >>o_file, \"evecoutname:     \" + out_prefix + \".evec.txt\"\n        print >>o_file, \"evaloutname:     \" + out_prefix + \".eval.txt\"\n        print >>o_file, \"numoutlieriter:  0\"\n        print >>o_file, \"altnormstyle:    NO\"\n\n    # Executing smartpca\n    command = [\"smartpca\", \"-p\", out_prefix + \".parameters\"]\n    runCommand(command)", "category": "Python"}, {"instruction": "def section(title, bar=OVERLINE, strm=sys.stdout):\n    \"\"\"Helper function for testing demo routines\n    \"\"\"\n", "input": "", "output": "    width = utils.term.width\n    printy(bold(title.center(width)))\n    printy(bold((bar * width)[:width]))", "category": "Python"}, {"instruction": "def DeregisterHelper(cls, analyzer_helper):\n    \"\"\"Deregisters a format analyzer helper.\n\n    Args:\n      analyzer_helper (AnalyzerHelper): analyzer helper.\n\n    Raises:\n      KeyError: if analyzer helper object is not set for the corresponding\n          type indicator.\n    \"\"\"\n", "input": "", "output": "    if analyzer_helper.type_indicator not in cls._analyzer_helpers:\n      raise KeyError(\n          'Analyzer helper object not set for type indicator: {0:s}.'.format(\n              analyzer_helper.type_indicator))\n\n    analyzer_helper = cls._analyzer_helpers[analyzer_helper.type_indicator]\n\n    cls._FlushCache(analyzer_helper.format_categories)\n\n    del cls._analyzer_helpers[analyzer_helper.type_indicator]", "category": "Python"}, {"instruction": "def auto_cleaned_path_stripped_uuid4(instance, filename: str) -> str:\n    \"\"\"\n    Gets upload path in this format: {MODEL_NAME}/{UUID4}{SUFFIX}.\n    Same as `upload_path_uuid4` but deletes the original file name from the user.\n\n    :param instance: Instance of model or model class.\n    :param filename: Uploaded file name.\n    :return: Target upload path.\n    \"\"\"\n", "input": "", "output": "    _, suffix = parse_filename(filename)\n    base_dir = get_base_dir_from_object(instance)\n    rand_uuid = uuid.uuid4()\n\n    return os.path.join(base_dir, \"{rand_uuid}{suffix}\".format(rand_uuid=rand_uuid,\n                                                               suffix=suffix))", "category": "Python"}, {"instruction": "def handler(event):\n    \"\"\"Signal decorator to allow use of callback functions as class decorators.\"\"\"\n", "input": "", "output": "    def decorator(fn):\n        def apply(cls):\n            event.connect(fn, sender=cls)\n            return cls\n\n        fn.apply = apply\n        return fn\n    return decorator", "category": "Python"}, {"instruction": "def get_gql_query(self):\n        \"\"\"Generate GraphQL query\"\"\"\n", "input": "", "output": "        template_string = ", "category": "Python"}, {"instruction": "def hourly_values(self):\n        \"\"\"A list of temperature values for each hour over the design day.\"\"\"\n", "input": "", "output": "        return [self._dry_bulb_max - self._dry_bulb_range * x for\n                x in self.temp_multipliers]", "category": "Python"}, {"instruction": "def reload_system_path(self):\n        # type: () -> None\n        \"\"\"\n        Rebuilds the base system path and all of the contained finders within it.\n\n        This will re-apply any changes to the environment or any version changes on the system.\n        \"\"\"\n", "input": "", "output": "\n        if self._system_path is not None:\n            self._system_path.clear_caches()\n        self._system_path = None\n        six.moves.reload_module(pyfinder_path)\n        self._system_path = self.create_system_path()", "category": "Python"}, {"instruction": "def validate_type_ac_pair(type, ac):\n    \"\"\"validate that accession is correct for variant type AND that\n    accession is fully specified.\n\n    \"\"\"\n", "input": "", "output": "\n    assert type in valid_pairs, \"Unknown variant type \" + type\n    if valid_pairs[type].match(ac):\n        return (ValidationLevel.VALID,\n                \"Accession ({ac}) is compatible with variant type {type}\".format(ac=ac, type=type))\n    elif invalid_pairs[type].match(ac):\n        return (ValidationLevel.ERROR,\n                \"Accession ({ac}) is not compatible with variant type {type}\".format(\n                    ac=ac, type=type))\n    else:\n        return (ValidationLevel.WARNING,\n                \"Accession ({ac}) is not known to be compatible with variant type {type}\".format(\n                    ac=ac, type=type))", "category": "Python"}, {"instruction": "def put_if_empty(self, key, value):\n        \"\"\"\n        Atomically write data only if the key is not already set.\n\n        :param bytes key: Key to check/set.\n        :param bytes value: Arbitrary data.\n        :return: Boolean whether key/value was set.\n        \"\"\"\n", "input": "", "output": "        if self.has_data_for_key(key):\n            return False\n        self.put_data(key, value)\n        return True", "category": "Python"}, {"instruction": "def get_content_string(self):\n        \"\"\" Ge thet Clusterpoint response's content as a string. \"\"\"\n", "input": "", "output": "        return ''.join([ET.tostring(element, encoding=\"utf-8\", method=\"xml\")\n                        for element in list(self._content)])", "category": "Python"}, {"instruction": "def getAttributeValueType(self, index):\n        \"\"\"\n        Return the type of the attribute at the given index\n\n        :param index: index of the attribute\n        \"\"\"\n", "input": "", "output": "        offset = self._get_attribute_offset(index)\n        return self.m_attributes[offset + const.ATTRIBUTE_IX_VALUE_TYPE]", "category": "Python"}, {"instruction": "def require_group(self, name, overwrite=False):\n        \"\"\"Obtain a sub-group, creating one if it doesn't exist.\n\n        Parameters\n        ----------\n        name : string\n            Group name.\n        overwrite : bool, optional\n            Overwrite any existing array with given `name` if present.\n\n        Returns\n        -------\n        g : zarr.hierarchy.Group\n\n        Examples\n        --------\n        >>> import zarr\n        >>> g1 = zarr.group()\n        >>> g2 = g1.require_group('foo')\n        >>> g3 = g1.require_group('foo')\n        >>> g2 == g3\n        True\n\n        \"\"\"\n", "input": "", "output": "\n        return self._write_op(self._require_group_nosync, name,\n                              overwrite=overwrite)", "category": "Python"}, {"instruction": "def display(self):\r\n        \"\"\"A unicode value with the object's data, to be used for displaying \r\n        the object in your application.\"\"\"\n", "input": "", "output": "        country = self.country if self.state else self.country_full\r\n        state = self.state if self.city else self.state_full\r\n        vals = (self.street, self.city, state, country)\r\n        disp = u', '.join(filter(None, vals))\r\n        if self.street and (self.house or self.apartment):\r\n            prefix = u'-'.join([val for val in (self.house, self.apartment) \r\n                                if val])\r\n            disp = prefix + u' ' + (disp or u'')\r\n        if self.po_box and not self.street:\r\n            disp = u' '.join([u'P.O. Box', self.po_box, (disp or u'')])\r\n        return disp", "category": "Python"}, {"instruction": "def pad(self, pad_length):\n        \"\"\"\n        Pad the pianoroll with zeros at the end along the time axis.\n\n        Parameters\n        ----------\n        pad_length : int\n            The length to pad with zeros along the time axis.\n\n        \"\"\"\n", "input": "", "output": "        self.pianoroll = np.pad(\n            self.pianoroll, ((0, pad_length), (0, 0)), 'constant')", "category": "Python"}, {"instruction": "def get_all_active(self):\n        \"\"\"\n        Get all of the active messages ordered by the active_datetime.\n        \"\"\"\n", "input": "", "output": "        now = timezone.now()\n        return self.select_related().filter(active_datetime__lte=now,\n                                              inactive_datetime__gte=now).order_by('active_datetime')", "category": "Python"}, {"instruction": "def get(self):\n        \"\"\"\n        Reloads the measurements from the backing store.\n        :return: 200 if success.\n        \"\"\"\n", "input": "", "output": "        try:\n            self._measurementController.reloadCompletedMeasurements()\n            return None, 200\n        except:\n            logger.exception(\"Failed to reload measurements\")\n            return str(sys.exc_info()), 500", "category": "Python"}, {"instruction": "def parse(chord):\n    \"\"\" Parse a string to get chord component\n\n    :param str chord: str expression of a chord\n    :rtype: (str, pychord.Quality, str, str)\n    :return: (root, quality, appended, on)\n    \"\"\"\n", "input": "", "output": "    if len(chord) > 1 and chord[1] in (\"b\", \"#\"):\n        root = chord[:2]\n        rest = chord[2:]\n    else:\n        root = chord[:1]\n        rest = chord[1:]\n    check_note(root, chord)\n    on_chord_idx = rest.find(\"/\")\n    if on_chord_idx >= 0:\n        on = rest[on_chord_idx + 1:]\n        rest = rest[:on_chord_idx]\n        check_note(on, chord)\n    else:\n        on = None\n    if rest in QUALITY_DICT:\n        quality = Quality(rest)\n    else:\n        raise ValueError(\"Invalid chord {}: Unknown quality {}\".format(chord, rest))\n    # TODO: Implement parser for appended notes\n    appended = []\n    return root, quality, appended, on", "category": "Python"}, {"instruction": "def installedOn(self):\n    \"\"\"\n    If this item is installed on another item, return the install\n    target. Otherwise return None.\n    \"\"\"\n", "input": "", "output": "    try:\n        return self.store.findUnique(_DependencyConnector,\n                                     _DependencyConnector.installee == self\n                                     ).target\n    except ItemNotFound:\n        return None", "category": "Python"}, {"instruction": "def write_client(captures, options):\n    \"\"\"\n    Write the TypeFactory classes to _client.py, along with some\n    imports and tables so that we can look up versioned Facades.\n\n    \"\"\"\n", "input": "", "output": "    with open(\"{}/_client.py\".format(options.output_dir), \"w\") as f:\n        f.write(HEADER)\n        f.write(\"from juju.client._definitions import *\\n\\n\")\n        clients = \", \".join(\"_client{}\".format(v) for v in captures)\n        f.write(\"from juju.client import \" + clients + \"\\n\\n\")\n        f.write(CLIENT_TABLE.format(clients=\",\\n    \".join(\n            ['\"{}\": _client{}'.format(v, v) for v in captures])))\n        f.write(LOOKUP_FACADE)\n        f.write(TYPE_FACTORY)\n        for key in sorted([k for k in factories.keys() if \"Facade\" in k]):\n            print(factories[key], file=f)", "category": "Python"}, {"instruction": "def stop(self):\n        '''\n        Called by the engine to stop the current utterance and clear the queue\n        of commands.\n        '''\n", "input": "", "output": "        # clear queue up to first end loop command\n        while(True):\n            try:\n                mtd, args, name = self._queue[0]\n            except IndexError:\n                break\n            if(mtd == self._engine.endLoop):\n                break\n            self._queue.pop(0)\n        self._driver.stop()", "category": "Python"}, {"instruction": "def get_clone(rec):\n    \"\"\"\n    >>> get_clone(\"Medicago truncatula chromosome 2 clone mth2-48e18\")\n    ('2', 'mth2-48e18')\n    \"\"\"\n", "input": "", "output": "    s = rec.description\n    chr = re.search(chr_pat, s)\n    clone = re.search(clone_pat, s)\n    chr = chr.group(1) if chr else \"\"\n    clone = clone.group(1) if clone else \"\"\n\n    return chr, clone", "category": "Python"}, {"instruction": "def send_message(self, peer: Peer, text: str, reply: int=None, link_preview: bool=None,\n                     on_success: callable=None, reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send message to peer.\n        :param peer: Peer to send message to.\n        :param text: Text to send.\n        :param reply: Message object or message_id to reply to.\n        :param link_preview: Whether or not to show the link preview for this message\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n", "input": "", "output": "        if isinstance(reply, Message):\n            reply = reply.id\n\n        botapi.send_message(chat_id=peer.id, text=text, disable_web_page_preview=not link_preview,\n                            reply_to_message_id=reply, on_success=on_success, reply_markup=reply_markup,\n                            **self.request_args).run()", "category": "Python"}, {"instruction": "def check_call(args, stdin=None, env=None):\n    \"\"\"Runs command and verifies its success.\"\"\"\n", "input": "", "output": "    log.debug('run: %s%s', args, ' {}'.format(env) if env else '')\n    subprocess.check_call(args=args, stdin=stdin, env=env)", "category": "Python"}, {"instruction": "def connect_tcp(self, address, port):\n        \"\"\"Connect to tcp/ip `address`:`port`. Delegated to `_connect_tcp`.\"\"\"\n", "input": "", "output": "        info('Connecting to TCP address: %s:%d', address, port)\n        self._connect_tcp(address, port)", "category": "Python"}, {"instruction": "def print_build_help(build_path, default_build_path):\n    \"\"\"\n    Print help text after configuration step is done.\n    \"\"\"\n", "input": "", "output": "    print('   configure step is done')\n    print('   now you need to compile the sources:')\n    if (build_path == default_build_path):\n        print('   $ cd build')\n    else:\n        print('   $ cd ' + build_path)\n    print('   $ make')", "category": "Python"}, {"instruction": "def vm_state(vm_=None, **kwargs):\n    '''\n    Return list of all the vms and their state.\n\n    If you pass a VM name in as an argument then it will return info\n    for just the named VM, otherwise it will return all VMs.\n\n    :param vm_: name of the domain\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.vm_state <domain>\n    '''\n", "input": "", "output": "    def _info(dom):\n        ", "category": "Python"}, {"instruction": "def max_voltage_step(self):\n        \"\"\"\n        Maximum absolute difference in adjacent voltage steps\n        \"\"\"\n", "input": "", "output": "        steps = [self.voltage_pairs[i].voltage\n                 - self.voltage_pairs[i + 1].voltage\n                 for i in range(len(self.voltage_pairs) - 1)]\n        return max(steps) if len(steps) > 0 else 0", "category": "Python"}, {"instruction": "def _get_users_of_group(config, group):\n    \"\"\" Utility to query fas for users of a group. \"\"\"\n", "input": "", "output": "    if not group:\n        return set()\n    fas = fmn.rules.utils.get_fas(config)\n    return fmn.rules.utils.get_user_of_group(config, fas, group)", "category": "Python"}, {"instruction": "def encrypt(v, key=None, keyfile=None):\n    \"\"\"\n    Encrypt an string\n    \"\"\"\n", "input": "", "output": "    cipher = functions.get_cipher(key, keyfile)\n    return cipher.encrypt(v)", "category": "Python"}, {"instruction": "def dbprint(*args):\n    \"\"\"print only if app.debug is truthy\"\"\"\n", "input": "", "output": "    if app and app.debug:\n        if USING_WINDOWS:\n            print(\"DEBUG: \" + \" \".join(args))\n\n        else:\n            CYELLOW2 = \"\\33[93m\"\n            NORMAL = \"\\033[0m\"\n            print(CYELLOW2 + \"DEBUG: \" + \" \".join(args) + NORMAL)", "category": "Python"}, {"instruction": "def set_range(self, start_index=0, end_index=None):\n        '''\n        Set range of tests to run\n\n        .. deprecated::\n            use :func:`~kitty.fuzzers.base.BaseFuzzer.set_test_list`\n\n        :param start_index: index to start at (default=0)\n        :param end_index: index to end at(default=None)\n        '''\n", "input": "", "output": "        if end_index is not None:\n            end_index += 1\n        self._test_list = StartEndList(start_index, end_index)\n        self.session_info.start_index = start_index\n        self.session_info.current_index = 0\n        self.session_info.end_index = end_index\n        self.session_info.test_list_str = self._test_list.as_test_list_str()\n        return self", "category": "Python"}, {"instruction": "def is_item_public(self, permission_name, view_name):\n        \"\"\"\n            Check if view has public permissions\n\n            :param permission_name:\n                the permission: can_show, can_edit...\n            :param view_name:\n                the name of the class view (child of BaseView)\n        \"\"\"\n", "input": "", "output": "        permissions = self.get_public_permissions()\n        if permissions:\n            for i in permissions:\n                if (view_name == i.view_menu.name) and (\n                    permission_name == i.permission.name\n                ):\n                    return True\n            return False\n        else:\n            return False", "category": "Python"}, {"instruction": "def mainloop(self):\n        \"\"\"\n        Handles events and calls their handler for infinity.\n        \"\"\"\n", "input": "", "output": "        while self.keep_going:\n            with self.lock:\n                if self.on_connect and not self.readable(2):\n                    self.on_connect()\n                    self.on_connect = None\n                if not self.keep_going:\n                    break\n                self.process_once()", "category": "Python"}, {"instruction": "def precision(links_true, links_pred=None):\n    \"\"\"precision(links_true, links_pred)\n\n    Compute the precision.\n\n    The precision is given by TP/(TP+FP).\n\n    Parameters\n    ----------\n    links_true: pandas.MultiIndex, pandas.DataFrame, pandas.Series\n        The true (or actual) collection of links.\n    links_pred: pandas.MultiIndex, pandas.DataFrame, pandas.Series\n        The predicted collection of links.\n\n    Returns\n    -------\n    float\n        The precision\n    \"\"\"\n", "input": "", "output": "\n    if _isconfusionmatrix(links_true):\n\n        confusion_matrix = links_true\n\n        v = confusion_matrix[0, 0] \\\n            / (confusion_matrix[0, 0] + confusion_matrix[1, 0])\n    else:\n\n        tp = true_positives(links_true, links_pred)\n        fp = false_positives(links_true, links_pred)\n        v = tp / (tp + fp)\n\n    return float(v)", "category": "Python"}, {"instruction": "def lt(self, event_property, value):\n        \"\"\"A less-than filter chain.\n\n        >>> request_time = EventExpression('request', 'elapsed_ms')\n        >>> filtered = request_time.lt('elapsed_ms', 500)\n        >>> print(filtered)\n        request(elapsed_ms).lt(elapsed_ms, 500)\n        \"\"\"\n", "input": "", "output": "        c = self.copy()\n        c.filters.append(filters.LT(event_property, value))\n        return c", "category": "Python"}, {"instruction": "def refresh(self, **kwargs):\n        \"\"\"Refresh a single object from server.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns None (updates the object)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the server cannot perform the request\n        \"\"\"\n", "input": "", "output": "        if self._id_attr:\n            path = '%s/%s' % (self.manager.path, self.id)\n        else:\n            path = self.manager.path\n        server_data = self.manager.gitlab.http_get(path, **kwargs)\n        self._update_attrs(server_data)", "category": "Python"}, {"instruction": "def _create_tc_dirs(self):\n        \"\"\"Create app directories for logs and data files.\"\"\"\n", "input": "", "output": "        tc_log_path = self.profile.get('args', {}).get('tc_log_path')\n        if tc_log_path is not None and not os.path.isdir(tc_log_path):\n            os.makedirs(tc_log_path)\n        tc_out_path = self.profile.get('args', {}).get('tc_out_path')\n        if tc_out_path is not None and not os.path.isdir(tc_out_path):\n            os.makedirs(tc_out_path)\n        tc_tmp_path = self.profile.get('args', {}).get('tc_tmp_path')\n        if tc_tmp_path is not None and not os.path.isdir(tc_tmp_path):\n            os.makedirs(tc_tmp_path)", "category": "Python"}, {"instruction": "def _CheckIsDirectory(self, file_entry):\n    \"\"\"Checks the is_directory find specification.\n\n    Args:\n      file_entry (FileEntry): file entry.\n\n    Returns:\n      bool: True if the file entry matches the find specification, False if not.\n    \"\"\"\n", "input": "", "output": "    if definitions.FILE_ENTRY_TYPE_DIRECTORY not in self._file_entry_types:\n      return False\n    return file_entry.IsDirectory()", "category": "Python"}, {"instruction": "def CSS_setKeyframeKey(self, styleSheetId, range, keyText):\n\t\t\"\"\"\n\t\tFunction path: CSS.setKeyframeKey\n\t\t\tDomain: CSS\n\t\t\tMethod name: setKeyframeKey\n\t\t\n\t\t\tParameters:\n\t\t\t\tRequired arguments:\n\t\t\t\t\t'styleSheetId' (type: StyleSheetId) -> No description\n\t\t\t\t\t'range' (type: SourceRange) -> No description\n\t\t\t\t\t'keyText' (type: string) -> No description\n\t\t\tReturns:\n\t\t\t\t'keyText' (type: Value) -> The resulting key text after modification.\n\t\t\n\t\t\tDescription: Modifies the keyframe rule key text.\n\t\t\"\"\"\n", "input": "", "output": "\t\tassert isinstance(keyText, (str,)\n\t\t    ), \"Argument 'keyText' must be of type '['str']'. Received type: '%s'\" % type(\n\t\t    keyText)\n\t\tsubdom_funcs = self.synchronous_command('CSS.setKeyframeKey',\n\t\t    styleSheetId=styleSheetId, range=range, keyText=keyText)\n\t\treturn subdom_funcs", "category": "Python"}, {"instruction": "def on_message(self, opcode, message):\n\t\t\"\"\"\n\t\tThe primary dispatch function to handle incoming WebSocket messages.\n\n\t\t:param int opcode: The opcode of the message that was received.\n\t\t:param bytes message: The data contained within the message.\n\t\t\"\"\"\n", "input": "", "output": "\t\tself.logger.debug(\"processing {0} (opcode: 0x{1:02x}) message\".format(self._opcode_names.get(opcode, 'UNKNOWN'), opcode))\n\t\tif opcode == self._opcode_close:\n\t\t\tself.close()\n\t\telif opcode == self._opcode_ping:\n\t\t\tif len(message) > 125:\n\t\t\t\tself.close()\n\t\t\t\treturn\n\t\t\tself.send_message(self._opcode_pong, message)\n\t\telif opcode == self._opcode_pong:\n\t\t\tpass\n\t\telif opcode == self._opcode_binary:\n\t\t\tself.on_message_binary(message)\n\t\telif opcode == self._opcode_text:\n\t\t\ttry:\n\t\t\t\tmessage = self._decode_string(message)\n\t\t\texcept UnicodeDecodeError:\n\t\t\t\tself.logger.warning('closing connection due to invalid unicode within a text message')\n\t\t\t\tself.close()\n\t\t\telse:\n\t\t\t\tself.on_message_text(message)\n\t\telif opcode == self._opcode_continue:\n\t\t\tself.close()\n\t\telse:\n\t\t\tself.logger.warning(\"received unknown opcode: {0} (0x{0:02x})\".format(opcode))\n\t\t\tself.close()", "category": "Python"}, {"instruction": "def getpeptides(self, chain):\n        \"\"\"If peptide ligand chains are defined via the command line options,\n        try to extract the underlying ligand formed by all residues in the\n        given chain without water\n        \"\"\"\n", "input": "", "output": "        all_from_chain = [o for o in pybel.ob.OBResidueIter(\n            self.proteincomplex.OBMol) if o.GetChain() == chain]  # All residues from chain\n        if len(all_from_chain) == 0:\n            return None\n        else:\n            non_water = [o for o in all_from_chain if not o.GetResidueProperty(9)]\n            ligand = self.extract_ligand(non_water)\n            return ligand", "category": "Python"}, {"instruction": "def map_overlay_forecast(self):\n        \"\"\"Returns capabilities data for forecast map overlays.\"\"\"\n", "input": "", "output": "        return json.loads(self._query(LAYER, FORECAST, ALL, CAPABILITIES, \"\").decode(errors=\"replace\"))", "category": "Python"}, {"instruction": "def size(self):\n        \"\"\"\n        Returns the actual size in pixels as set by matplotlib, or\n        the user provided size if available.\n        \"\"\"\n", "input": "", "output": "        if not hasattr(self, \"_size\") or self._size is None:\n            fig = plt.gcf()\n            self._size = fig.get_size_inches()*fig.dpi\n        return self._size", "category": "Python"}, {"instruction": "def extend(self, v):\n        '''\n        >>> import numpy as np\n        >>> da = DiskArray('/tmp/test.array', shape=(0, 3), capacity=(10, 3), dtype=np.float32)\n        >>> print(da[:])\n\t[[2. 3. 4.]\n\t [0. 0. 0.]\n\t [0. 0. 0.]\n\t [0. 0. 0.]\n\t [0. 0. 0.]\n\t [0. 0. 0.]\n\t [0. 0. 0.]\n\t [0. 0. 0.]\n\t [0. 0. 0.]\n\t [0. 0. 0.]]\n        >>> data = np.array([[2,3,4], [1, 2, 3]])\n        >>> da.extend(data)\n        >>> print(da[:])\n        [[2. 3. 4.]\n         [1. 2. 3.]\n         [0. 0. 0.]\n         [0. 0. 0.]\n         [0. 0. 0.]\n         [0. 0. 0.]\n         [0. 0. 0.]\n         [0. 0. 0.]\n         [0. 0. 0.]\n         [0. 0. 0.]]\n        >>> os.remove('/tmp/test.array')\n        '''\n", "input": "", "output": "\n        nrows = self._shape[0]\n        nrows_capacity = self._capacity_shape[0]\n        remaining_capacity = nrows_capacity - nrows\n\n        if remaining_capacity < len(v):\n            diff = len(v) - remaining_capacity\n            self._capacity_shape = self._incr_shape(self._capacity_shape, diff)\n            self._update_ndarray()\n\n        self.data[nrows:nrows+len(v)] = v\n        self._shape = self._incr_shape(self._shape, len(v))", "category": "Python"}, {"instruction": "def satisfiesExternal(cntxt: Context, n: Node, se: ShExJ.ShapeExternal, c: DebugContext) -> bool:\n    \"\"\" Se is a ShapeExternal and implementation-specific mechansims not defined in this specification indicate\n     success.\n     \"\"\"\n", "input": "", "output": "    if c.debug:\n        print(f\"id: {se.id}\")\n    extern_shape = cntxt.external_shape_for(se.id)\n    if extern_shape:\n        return satisfies(cntxt, n, extern_shape)\n    cntxt.fail_reason = f\"{se.id}: Shape is not in Schema\"\n    return False", "category": "Python"}, {"instruction": "def __postCallAction_codebp(self, event):\n        \"\"\"\n        Handles code breakpoint events on return from the function.\n\n        @type  event: L{ExceptionEvent}\n        @param event: Breakpoint hit event.\n        \"\"\"\n", "input": "", "output": "\n        # If the breakpoint was accidentally hit by another thread,\n        # pass it to the debugger instead of calling the \"post\" callback.\n        #\n        # XXX FIXME:\n        # I suppose this check will fail under some weird conditions...\n        #\n        tid = event.get_tid()\n        if tid not in self.__paramStack:\n            return True\n\n        # Remove the code breakpoint at the return address.\n        pid     = event.get_pid()\n        address = event.breakpoint.get_address()\n        event.debug.dont_break_at(pid, address)\n\n        # Call the \"post\" callback.\n        try:\n            self.__postCallAction(event)\n\n        # Forget the parameters.\n        finally:\n            self.__pop_params(tid)", "category": "Python"}, {"instruction": "def is_correlated(self, threshold=0):\n        \"\"\"\n        Compare with a threshold to determine whether two timeseries correlate to each other.\n        :return: a CorrelationResult object if two time series correlate otherwise false.\n        \"\"\"\n", "input": "", "output": "        return self.correlation_result if self.correlation_result.coefficient >= threshold else False", "category": "Python"}, {"instruction": "def get_coef_indices(self, i=-1):\n        \"\"\"get the indices for the coefficients of a term in the term list\n\n        Parameters\n        ---------\n        i : int\n            by default `int=-1`, meaning that coefficient indices are returned\n            for all terms in the term list\n\n        Returns\n        -------\n        list of integers\n        \"\"\"\n", "input": "", "output": "        if i == -1:\n            return list(range(self.n_coefs))\n\n        if i >= len(self._terms):\n            raise ValueError('requested {}th term, but found only {} terms'\\\n                            .format(i, len(self._terms)))\n\n        start = 0\n        for term in self._terms[:i]:\n            start += term.n_coefs\n        stop = start + self._terms[i].n_coefs\n        return list(range(start, stop))", "category": "Python"}, {"instruction": "def allconcat(self, x, mesh_axis, concat_axis, stack=False):\n    \"\"\"Grouped allconcat (like MPI allgather followed by concat).\n\n    TODO(noam): inefficient - replace with a XLA allconcat when available\n\n    Args:\n      x: a LaidOutTensor\n      mesh_axis: an integer - the mesh axis along which to group\n      concat_axis: an integer (the Tensor axis along which to concatenate)\n      stack: a boolean - whether to stack instead of concat\n    Returns:\n      a LaidOutTensor\n    \"\"\"\n", "input": "", "output": "    x = x.to_laid_out_tensor()\n    coord = self.laid_out_pcoord(mesh_axis)\n    t = x.one_slice\n    old_shape = t.shape.as_list()\n    num_parts = self.shape[mesh_axis].size\n    t = tf.expand_dims(t, concat_axis)\n    t *= tf.reshape(\n        tf.one_hot(coord.one_slice, num_parts, dtype=t.dtype),\n        [num_parts if i == concat_axis else 1\n         for i in xrange(len(old_shape) + 1)])\n    if not stack:\n      new_shape = old_shape[:]\n      new_shape[concat_axis] *= num_parts\n      t = tf.reshape(t, new_shape)\n    return self.allreduce(self.LaidOutTensor([t]), [mesh_axis], \"SUM\")", "category": "Python"}, {"instruction": "def update_reading_list(self, reading_list):\n        \"\"\"Generic behaviors for reading lists before being rendered.\"\"\"\n", "input": "", "output": "\n        # remove the current piece of content from the query.\n        reading_list = reading_list.filter(\n            ~es_filter.Ids(values=[self.id])\n        )\n\n        # remove excluded document types from the query.\n        reading_list_config = getattr(settings, \"READING_LIST_CONFIG\", {})\n        excluded_doc_types = reading_list_config.get(\"excluded_doc_types\", [])\n        for obj in excluded_doc_types:\n            reading_list = reading_list.filter(~es_filter.Type(value=obj))\n\n        return reading_list", "category": "Python"}, {"instruction": "def create(plm, address, cat, subcat, firmware=None):\n    \"\"\"Create a device from device info data.\"\"\"\n", "input": "", "output": "    from insteonplm.devices.ipdb import IPDB\n    ipdb = IPDB()\n    product = ipdb[[cat, subcat]]\n    deviceclass = product.deviceclass\n    device = None\n    if deviceclass is not None:\n        device = deviceclass(plm, address, cat, subcat,\n                             product.product_key,\n                             product.description,\n                             product.model)\n    return device", "category": "Python"}, {"instruction": "def init_celery(project_name):\n    \"\"\" init celery app without the need of redundant code \"\"\"\n", "input": "", "output": "    os.environ.setdefault('DJANGO_SETTINGS_MODULE', '%s.settings' % project_name)\n    app = Celery(project_name)\n    app.config_from_object('django.conf:settings')\n    app.autodiscover_tasks(settings.INSTALLED_APPS, related_name='tasks')\n    return app", "category": "Python"}, {"instruction": "def _to_bytes_or_false(val):  # type: (Union[Text, bytes]) -> Union[bytes, bool]\n    \"\"\"An internal graph to convert the input to a bytes or to False.\n\n    The criteria for conversion is as follows and should be python 2 and 3\n    compatible:\n    - If val is py2 str or py3 bytes: return bytes\n    - If val is py2 unicode or py3 str: return val.decode('utf-8')\n    - Otherwise, return False\n    \"\"\"\n", "input": "", "output": "    if isinstance(val, bytes):\n        return val\n    else:\n        try:\n            return val.encode('utf-8')\n        except AttributeError:\n            return False", "category": "Python"}, {"instruction": "def insert(self, collection, doc, callback=None):\n        \"\"\"Insert an item into a collection\n\n        Arguments:\n        collection - the collection to be modified\n        doc - The document to insert. May not yet have an _id attribute,\n        in which case Meteor will generate one for you.\n\n        Keyword Arguments:\n        callback - Optional. If present, called with an error object as the first argument and,\n        if no error, the _id as the second.\"\"\"\n", "input": "", "output": "        self.call(\"/\" + collection + \"/insert\", [doc], callback=callback)", "category": "Python"}, {"instruction": "def resource_info(self):\n        \"\"\"Get the extended information of this resource.\n\n        :param resource_name: Unique symbolic name of a resource.\n\n        :rtype: :class:`pyvisa.highlevel.ResourceInfo`\n        \"\"\"\n", "input": "", "output": "        return self.visalib.parse_resource_extended(self._resource_manager.session, self.resource_name)", "category": "Python"}, {"instruction": "def init(**config):\n    \"\"\" Initialize the crypto backend.\n\n    The backend can be one of two plugins:\n\n        - 'x509' - Uses x509 certificates.\n        - 'gpg' - Uses GnuPG keys.\n    \"\"\"\n", "input": "", "output": "    global _implementation\n    global _validate_implementations\n\n    if config.get('crypto_backend') == 'gpg':\n        _implementation = gpg\n    else:\n        _implementation = x509\n\n    _validate_implementations = []\n    for mod in config.get('crypto_validate_backends', []):\n        if mod == 'gpg':\n            _validate_implementations.append(gpg)\n        elif mod == 'x509':\n            _validate_implementations.append(x509)\n        else:\n            raise ValueError(\"%r is not a valid crypto backend\" % mod)\n\n    if not _validate_implementations:\n        _validate_implementations.append(_implementation)", "category": "Python"}, {"instruction": "def create(self, sid):\n        \"\"\"\n        Create a new ShortCodeInstance\n\n        :param unicode sid: The SID of a Twilio ShortCode resource\n\n        :returns: Newly created ShortCodeInstance\n        :rtype: twilio.rest.proxy.v1.service.short_code.ShortCodeInstance\n        \"\"\"\n", "input": "", "output": "        data = values.of({'Sid': sid, })\n\n        payload = self._version.create(\n            'POST',\n            self._uri,\n            data=data,\n        )\n\n        return ShortCodeInstance(self._version, payload, service_sid=self._solution['service_sid'], )", "category": "Python"}, {"instruction": "def potential(self, x, y, kwargs, k=None):\n        \"\"\"\n        lensing potential\n\n        :param x: x-position (preferentially arcsec)\n        :type x: numpy array\n        :param y: y-position (preferentially arcsec)\n        :type y: numpy array\n        :param kwargs: list of keyword arguments of lens model parameters matching the lens model classes\n        :param k: only evaluate the k-th lens model\n        :return: lensing potential in units of arcsec^2\n        \"\"\"\n", "input": "", "output": "        return self.lens_model.potential(x, y, kwargs, k=k)", "category": "Python"}, {"instruction": "def Send(self, message):\n    \"\"\"Send one message.\n\n    Deprecated, users should migrate to call self.outgoing.InsertMessage\n    directly.\n    \"\"\"\n", "input": "", "output": "    if not self.outgoing:\n      raise NotConfigured(\"Send address not provided.\")\n    self.outgoing.InsertMessage(message)", "category": "Python"}, {"instruction": "def _get_suggestions(self, filter_word=None):\n        \"\"\"\n        This only gets caled internally from the get_suggestion method.\n        \"\"\"\n", "input": "", "output": "        keys = self.manifest.keys()\n        words = []\n        for key in keys:            \n            if isinstance(self.manifest[key], Manifest):\n                # if this key is another manifest, append a slash to the \n                # suggestion so the user knows theres more items under this key\n                words.append(key + '/')\n            else:\n                words.append(key)\n\n        if filter_word:\n            words = [x for x in words if x.startswith(filter_word)]\n\n        return words", "category": "Python"}, {"instruction": "def get_torrent_upload_limit(self, infohash_list):\n        \"\"\"\n        Get upoload speed limit of the supplied torrents.\n\n        :param infohash_list: Single or list() of infohashes.\n        \"\"\"\n", "input": "", "output": "        data = self._process_infohash_list(infohash_list)\n        return self._post('command/getTorrentsUpLimit', data=data)", "category": "Python"}, {"instruction": "def transform_incoming(self, son, collection):\n        \"\"\"Move _id to the front if it's there.\n        \"\"\"\n", "input": "", "output": "        if not \"_id\" in son:\n            return son\n        transformed = SON({\"_id\": son[\"_id\"]})\n        transformed.update(son)\n        return transformed", "category": "Python"}, {"instruction": "def _det_inference(self):\n        \"\"\"\n        Internal method for determining the inference method\n        \"\"\"\n", "input": "", "output": "        # 2 random effects with complete design -> gp2KronSum\n        # TODO: add check for low-rankness, use GP3KronSumLR and GP2KronSumLR when possible\n        if (self.n_randEffs==2) and (~sp.isnan(self.Y).any()):\n            rv = 'GP2KronSum'\n        else:\n            rv = 'GP'\n        return rv", "category": "Python"}, {"instruction": "def resolve_parameter_refs(self, input_dict, parameters):\n        \"\"\"\n        Resolves references that are present in the parameters and returns the value. If it is not in parameters,\n        this method simply returns the input unchanged.\n\n        :param input_dict: Dictionary representing the Ref function. Must contain only one key and it should be \"Ref\".\n            Ex: {Ref: \"foo\"}\n\n        :param parameters: Dictionary of parameter values for resolution\n        :return:\n        \"\"\"\n", "input": "", "output": "        if not self.can_handle(input_dict):\n            return input_dict\n\n        param_name = input_dict[self.intrinsic_name]\n\n        if not isinstance(param_name, string_types):\n            return input_dict\n\n        if param_name in parameters:\n            return parameters[param_name]\n        else:\n            return input_dict", "category": "Python"}, {"instruction": "def map_overview_header_element(feature, parent):\n    \"\"\"Retrieve map overview header string from definitions.\"\"\"\n", "input": "", "output": "    _ = feature, parent  # NOQA\n    header = map_overview_header['string_format']\n    return header.capitalize()", "category": "Python"}, {"instruction": "def encode(data, mime_type='', charset='utf-8', base64=True):\n    \"\"\"\n    Encode data to DataURL\n    \"\"\"\n", "input": "", "output": "    if isinstance(data, six.text_type):\n        data = data.encode(charset)\n    else:\n        charset = None\n    if base64:\n        data = utils.text(b64encode(data))\n    else:\n        data = utils.text(quote(data))\n\n    result = ['data:', ]\n    if mime_type:\n        result.append(mime_type)\n    if charset:\n        result.append(';charset=')\n        result.append(charset)\n    if base64:\n        result.append(';base64')\n    result.append(',')\n    result.append(data)\n\n    return ''.join(result)", "category": "Python"}, {"instruction": "def index(self, value):\n\t\t\"\"\"\n\t\tReturn the smallest index of the row(s) with this column\n\t\tequal to value.\n\t\t\"\"\"\n", "input": "", "output": "\t\tfor i in xrange(len(self.parentNode)):\n\t\t\tif getattr(self.parentNode[i], self.Name) == value:\n\t\t\t\treturn i\n\t\traise ValueError(value)", "category": "Python"}, {"instruction": "def value(self):\n        \"\"\"returns object as dictionary\"\"\"\n", "input": "", "output": "        return {\n            \"objectIdFieldName\" : self._objectIdFieldName,\n            \"displayFieldName\" : self._displayFieldName,\n            \"globalIdFieldName\" : self._globalIdFieldName,\n            \"geometryType\" : self._geometryType,\n            \"spatialReference\" : self._spatialReference,\n            \"hasZ\" : self._hasZ,\n            \"hasM\" : self._hasM,\n            \"fields\" : self._fields,\n            \"features\" : [f.asDictionary for f in self._features]\n        }", "category": "Python"}, {"instruction": "def data_received(self, data):\n        \"\"\"Process received data from the socket\n\n        Called when we receive data\n        \"\"\"\n", "input": "", "output": "        self.logger.debug('Received data: %s', repr(data))\n\n        try:\n            request = self._parse_headers(data)\n            self._handle_request(request)\n        except InvalidRequestError as e:\n            self._write_response(e.get_http_response())\n\n        if not self.keepalive:\n            if self._timeout_handle:\n                self._timeout_handle.cancel()\n            self.transport.close()\n\n        if self._timeout and self._timeout_handle:\n            self.logger.debug('Delaying timeout event')\n            self._timeout_handle.cancel()\n            self._timout_handle = self._loop.call_later(\n                self._timeout, self._handle_timeout)", "category": "Python"}, {"instruction": "def memoize_by_args(func):\n    \"\"\"Memoizes return value of a func based on args.\"\"\"\n", "input": "", "output": "    memory = {}\n\n    @functools.wraps(func)\n    def memoized(*args):\n        if args not in memory.keys():\n            value = func(*args)\n            memory[args] = value\n\n        return memory[args]\n\n    return memoized", "category": "Python"}, {"instruction": "def update_employee(emp_id, key=None, value=None, items=None):\n    '''\n    Update one or more items for this employee. Specifying an empty value will\n    clear it for that employee.\n\n    CLI Examples:\n\n        salt myminion bamboohr.update_employee 1138 nickname Curly\n        salt myminion bamboohr.update_employee 1138 nickname ''\n        salt myminion bamboohr.update_employee 1138 items='{\"nickname\": \"Curly\"}\n        salt myminion bamboohr.update_employee 1138 items='{\"nickname\": \"\"}\n    '''\n", "input": "", "output": "    if items is None:\n        if key is None or value is None:\n            return {'Error': 'At least one key/value pair is required'}\n        items = {key: value}\n    elif isinstance(items, six.string_types):\n        items = salt.utils.yaml.safe_load(items)\n\n    xml_items = ''\n    for pair in items:\n        xml_items += '<field id=\"{0}\">{1}</field>'.format(pair, items[pair])\n    xml_items = '<employee>{0}</employee>'.format(xml_items)\n\n    status, result = _query(\n        action='employees',\n        command=emp_id,\n        data=xml_items,\n        method='POST',\n    )\n\n    return show_employee(emp_id, ','.join(items.keys()))", "category": "Python"}, {"instruction": "def _render_image(self, spec, alt=''):\n        \"\"\" Given an image spec, try to turn it into a card image per the configuration \"\"\"\n", "input": "", "output": "        # pylint: disable=unused-argument\n\n        try:\n            path, image_args, _ = image.parse_image_spec(spec)\n        except Exception as err:  # pylint: disable=broad-except\n            # we tried\u2122\n            logger.exception(\"Got error on spec %s: %s\", spec, err)\n            return None\n\n        img = image.get_image(path, self._image_search_path)\n        if img:\n            image_config = {**image_args, **self._config, 'absolute': True}\n            return img.get_rendition(1, **image_config)[0]\n\n        return None", "category": "Python"}, {"instruction": "def writer(f):\n    '''CSV writer factory for CADA format'''\n", "input": "", "output": "    return unicodecsv.writer(f, encoding='utf-8', delimiter=b',', quotechar=b'\"')", "category": "Python"}, {"instruction": "def image_url(self, pixel_size=None):\n        \"\"\"\n        Get the URL for the user icon in the desired pixel size, if it exists. If no\n        size is supplied, give the URL for the full-size image.\n        \"\"\"\n", "input": "", "output": "        if \"profile\" not in self._raw:\n            return\n        profile = self._raw[\"profile\"]\n        if (pixel_size):\n            img_key = \"image_%s\" % pixel_size\n            if img_key in profile:\n                return profile[img_key]\n        return profile[self._DEFAULT_IMAGE_KEY]", "category": "Python"}, {"instruction": "def z_axis_transform(compound, new_origin=None,\n                     point_on_z_axis=None,\n                     point_on_zx_plane=None):\n    \"\"\"Move a compound such that the z-axis lies on specified points.\n\n    Parameters\n    ----------\n    compound : mb.Compound\n        The compound to move.\n    new_origin : mb.Compound or list-like of size 3, optional, default=[0.0, 0.0, 0.0]\n        Where to place the new origin of the coordinate system.\n    point_on_z_axis : mb.Compound or list-like of size 3, optional, default=[0.0, 0.0, 1.0]\n        A point on the new z-axis.\n    point_on_zx_plane : mb.Compound or list-like of size 3, optional, default=[0.0, 0.0, 1.0]\n        A point on the new xz-plane.\n\n    \"\"\"\n", "input": "", "output": "    x_axis_transform(compound, new_origin=new_origin,\n                     point_on_x_axis=point_on_z_axis,\n                     point_on_xy_plane=point_on_zx_plane)\n    rotate_around_y(compound, np.pi * 3 / 2)", "category": "Python"}, {"instruction": "def dict_to_instance(self, content):\n        \"\"\"\n        transforms the content to a new instace of\n        object self.schema['title']\n        :param content: valid response\n        :returns new instance of current class\n        \"\"\"\n", "input": "", "output": "        klass = self.schema['title']\n        cls = get_model_class(klass, api=self.__api__)\n        # jdict = json.loads(content, encoding=\"utf-8\")\n        ### check if we have a response\n        properties_dict = content[self.schema['title']][self.schema['title']]\n        #@todo: find a way to handle the data\n        # validation fails if the none values are not removed\n        new_dict = helpers.remove_properties_containing_None(properties_dict)\n        obj = cls(new_dict)\n        #obj.links = content[self.schema['title']]['links']\n        return obj", "category": "Python"}, {"instruction": "def load_plume_package(package, plume_dir, accept_defaults):\n    \"\"\"Loads a canari package into Plume.\"\"\"\n", "input": "", "output": "    from canari.commands.load_plume_package import load_plume_package\n    load_plume_package(package, plume_dir, accept_defaults)", "category": "Python"}, {"instruction": "def register_event(self, event_type, pattern, handler):\n        \"\"\" When ``event_type`` is observed for ``pattern``, triggers ``handler``.\n\n        For \"CHANGE\" events, ``pattern`` should be a tuple of ``min_changed_pixels`` and\n        the base screen state.\n        \"\"\"\n", "input": "", "output": "        if event_type not in self._supported_events:\n            raise ValueError(\"Unsupported event type {}\".format(event_type))\n        if event_type != \"CHANGE\" and not isinstance(pattern, Pattern) and not isinstance(pattern, basestring):\n            raise ValueError(\"Expected pattern to be a Pattern or string\")\n        if event_type == \"CHANGE\" and not (len(pattern)==2 and isinstance(pattern[0], int) and isinstance(pattern[1], numpy.ndarray)):\n            raise ValueError(\"For \\\"CHANGE\\\" events, ``pattern`` should be a tuple of ``min_changed_pixels`` and the base screen state.\")\n\n        # Create event object\n        event = {\n            \"pattern\": pattern,\n            \"event_type\": event_type,\n            \"count\": 0,\n            \"handler\": handler,\n            \"name\": uuid.uuid4(),\n            \"active\": True\n        }\n        self._events[event[\"name\"]] = event\n        return event[\"name\"]", "category": "Python"}, {"instruction": "def read(self, source, errors='strict', clean_paragraphs=True):\n        \"\"\"\n        source: A list of P objects.\n        \"\"\"\n", "input": "", "output": "\n        reader = Rtf15Reader(source, errors, clean_paragraphs)\n        return reader.go()", "category": "Python"}, {"instruction": "def _influxdb_url(self):\n        \"\"\" Return REST API URL to access time series.\n        \"\"\"\n", "input": "", "output": "        url = \"{0}/db/{1}/series\".format(self.influxdb.url.rstrip('/'), self.config.dbname)\n\n        if self.influxdb.user and self.influxdb.password:\n            url += \"?u={0}&p={1}\".format(self.influxdb.user, self.influxdb.password)\n\n        return url", "category": "Python"}, {"instruction": "def _validate(self, writing=False):\n        \"\"\"Verify that the box obeys the specifications.\"\"\"\n", "input": "", "output": "        if self.colorspace is not None and self.icc_profile is not None:\n            msg = (\"Colorspace and icc_profile cannot both be set when \"\n                   \"creating a ColourSpecificationBox.\")\n            self._dispatch_validation_error(msg, writing=writing)\n\n        if self.method not in _COLORSPACE_METHODS.keys():\n            msg = \"Invalid colorspace method value ({method}).\"\n            msg = msg.format(method=self.method)\n            if writing:\n                raise IOError(msg)\n            else:\n                warnings.warn(msg, UserWarning)\n\n        if self.approximation not in _APPROXIMATION_MEASURES.keys():\n            msg = \"Invalid colr approximation value ({approx}).\"\n            msg = msg.format(approx=self.approximation)\n            if not writing:\n                # Don't bother to check this for the case of writing=True\n                # because it's already handles in the wrapping code.\n                warnings.warn(msg, UserWarning)", "category": "Python"}, {"instruction": "def set_message_last_post(cr, uid, pool, models):\n    \"\"\"\n    Given a list of models, set their 'message_last_post' fields to an\n    estimated last post datetime.\n    To be called in post-migration scripts\n\n    :param cr: database cursor\n    :param uid: user id, assumed to be openerp.SUPERUSER_ID\n    :param pool: orm pool, assumed to be openerp.pooler.get_pool(cr.dbname)\n    :param models: a list of model names for which 'message_last_post' needs \\\n    to be filled\n    :return:\n    \"\"\"\n", "input": "", "output": "    if type(models) is not list:\n        models = [models]\n    for model in models:\n        model_pool = pool[model]\n        cr.execute(\n            \"UPDATE {table} \"\n            \"SET message_last_post=(SELECT max(mm.date) \"\n            \"FROM mail_message mm \"\n            \"WHERE mm.model=%s \"\n            \"AND mm.date IS NOT NULL \"\n            \"AND mm.res_id={table}.id)\".format(\n                table=model_pool._table), (model,)\n        )", "category": "Python"}, {"instruction": "def indices_to_labels(self, indices: Sequence[int]) -> List[str]:\n        \"\"\" Converts a sequence of indices into their corresponding labels.\"\"\"\n", "input": "", "output": "\n        return [(self.INDEX_TO_LABEL[index]) for index in indices]", "category": "Python"}, {"instruction": "def ifi_index(self, value):\n        \"\"\"Index setter.\"\"\"\n", "input": "", "output": "        self.bytearray[self._get_slicers(3)] = bytearray(c_int(value or 0))", "category": "Python"}, {"instruction": "def get_query(self, q, request):\n        \"\"\" return a query set searching for the query string q\n            either implement this method yourself or set the search_field\n            in the LookupChannel class definition\n        \"\"\"\n", "input": "", "output": "        return Institute.objects.filter(\n            Q(name__icontains=q)\n        )", "category": "Python"}, {"instruction": "def max(self, axis=None, skipna=True, *args, **kwargs):\n        \"\"\"\n        Return the maximum value of the Index or maximum along\n        an axis.\n\n        See Also\n        --------\n        numpy.ndarray.max\n        Series.max : Return the maximum value in a Series.\n        \"\"\"\n", "input": "", "output": "        nv.validate_max(args, kwargs)\n        nv.validate_minmax_axis(axis)\n\n        if not len(self):\n            return self._na_value\n\n        i8 = self.asi8\n        try:\n            # quick check\n            if len(i8) and self.is_monotonic:\n                if i8[-1] != iNaT:\n                    return self._box_func(i8[-1])\n\n            if self.hasnans:\n                if skipna:\n                    max_stamp = self[~self._isnan].asi8.max()\n                else:\n                    return self._na_value\n            else:\n                max_stamp = i8.max()\n            return self._box_func(max_stamp)\n        except ValueError:\n            return self._na_value", "category": "Python"}, {"instruction": "def _map_table_name(self, model_names):\n        \"\"\"\n        Pre foregin_keys potrbejeme pre z nazvu tabulky zistit class,\n        tak si to namapujme\n        \"\"\"\n", "input": "", "output": "\n        for model in model_names:\n            if isinstance(model, tuple):\n                model = model[0]\n\n            try:\n                model_cls = getattr(self.models, model)\n                self.table_to_class[class_mapper(model_cls).tables[0].name] = model\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def as_ab_write(self, start, data):\n        \"\"\"\n        This is the asynchronous counterpart of Cli_ABWrite.\n        \"\"\"\n", "input": "", "output": "        wordlen = snap7.snap7types.S7WLByte\n        type_ = snap7.snap7types.wordlen_to_ctypes[wordlen]\n        size = len(data)\n        cdata = (type_ * size).from_buffer_copy(data)\n        logger.debug(\"ab write: start: %s: size: %s: \" % (start, size))\n        return self.library.Cli_AsABWrite(\n            self.pointer, start, size, byref(cdata))", "category": "Python"}, {"instruction": "def read_datafiles(files, dtype, column):\n    '''Load the datafiles and return cov, mag, phase and fpi phase values.\n    '''\n", "input": "", "output": "    pha = []\n    pha_fpi = []\n    for filename, filetype in zip(files, dtype):\n        if filetype == 'cov':\n            cov = load_cov(filename)\n        elif filetype == 'mag':\n            mag = load_rho(filename, column)\n        elif filetype == 'pha':\n            pha = load_rho(filename, 2)\n        elif filetype == 'pha_fpi':\n            pha_fpi = load_rho(filename, 2)\n\n    return cov, mag, pha, pha_fpi", "category": "Python"}, {"instruction": "def add(self, x, axis):\n        \"\"\"Function to add 3D View with vector or 2D array (type = numpy.ndarray or 2D Field or 2D View) or 2D View with vector (type = numpy.ndarray)\n        :param x: array(1D, 2D) or field (2D) or View(2D)\n        :param axis: specifies axis, eg. axis = (1,2) plane lies in yz-plane, axis=0 vector along x axis\n        :return: dict with result of operation (same form as view.d)\n        \"\"\"\n", "input": "", "output": "        return self.__array_op(operator.add, x, axis)", "category": "Python"}, {"instruction": "def clear_extra_selections(self, key):\r\n        \"\"\"Remove decorations added through set_extra_selections.\r\n\r\n        Args:\r\n            key (str) name of the extra selections group.\r\n        \"\"\"\n", "input": "", "output": "        for decoration in self.extra_selections_dict.get(key, []):\r\n            self.decorations.remove(decoration)\r\n        self.extra_selections_dict[key] = []", "category": "Python"}, {"instruction": "def bsrchc(value, ndim, lenvals, array):\n    \"\"\"\n    Do a binary earch for a given value within a character string array.\n    Return the index of the first matching array entry, or -1 if the key\n    value was not found.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/bsrchc_c.html\n\n    :param value: Key value to be found in array.\n    :type value: str\n    :param ndim: Dimension of array.\n    :type ndim: int\n    :param lenvals: String length.\n    :type lenvals: int\n    :param array: Character string array to search.\n    :type array: list of strings\n    :return: index\n    :rtype: int\n    \"\"\"\n", "input": "", "output": "    value = stypes.stringToCharP(value)\n    ndim = ctypes.c_int(ndim)\n    lenvals = ctypes.c_int(lenvals)\n    array = stypes.listToCharArrayPtr(array, xLen=lenvals, yLen=ndim)\n    return libspice.bsrchc_c(value, ndim, lenvals, array)", "category": "Python"}, {"instruction": "def _publish_actor_class_to_key(self, key, actor_class_info):\n        \"\"\"Push an actor class definition to Redis.\n\n        The is factored out as a separate function because it is also called\n        on cached actor class definitions when a worker connects for the first\n        time.\n\n        Args:\n            key: The key to store the actor class info at.\n            actor_class_info: Information about the actor class.\n        \"\"\"\n", "input": "", "output": "        # We set the driver ID here because it may not have been available when\n        # the actor class was defined.\n        self._worker.redis_client.hmset(key, actor_class_info)\n        self._worker.redis_client.rpush(\"Exports\", key)", "category": "Python"}, {"instruction": "def save_to_metadata_file(cls, dirpath, key, value):\n        \"\"\"\n        Store `key: value` in metadata file dict.\n        \"\"\"\n", "input": "", "output": "        fullpath = os.path.join(dirpath, cls.metadata_filename)\n        if os.path.exists(fullpath):\n            d = pickle.load(open(fullpath, 'rb'))\n            d.update({key: value})\n            pickle.dump(d, open(fullpath, 'wb'))\n        else:\n            pickle.dump({key: value}, open(fullpath, 'wb'))", "category": "Python"}, {"instruction": "def from_clause(cls, clause):\n        \"\"\" Factory method \"\"\"\n", "input": "", "output": "        [field, low, high] = clause\n        return cls(field, resolve(low), resolve(high))", "category": "Python"}, {"instruction": "def read_nih_image_header(fh, byteorder, dtype, count, offsetsize):\n    \"\"\"Read NIH_IMAGE_HEADER tag from file and return as dict.\"\"\"\n", "input": "", "output": "    a = fh.read_record(TIFF.NIH_IMAGE_HEADER, byteorder=byteorder)\n    a = a.newbyteorder(byteorder)\n    a = recarray2dict(a)\n    a['XUnit'] = a['XUnit'][:a['XUnitSize']]\n    a['UM'] = a['UM'][:a['UMsize']]\n    return a", "category": "Python"}, {"instruction": "def negotiate_safe(self, name, params):\n        \"\"\"\n        `name` and `params` are sent in the HTTP request by the client. Check\n        if the extension name is supported by this extension, and validate the\n        parameters. Returns a dict with accepted parameters, or None if not\n        accepted.\n        \"\"\"\n", "input": "", "output": "        for param in params.iterkeys():\n            if param not in self.defaults:\n                return\n\n        try:\n            return dict(self.negotiate(name, params))\n        except (KeyError, ValueError, AssertionError):\n            pass", "category": "Python"}, {"instruction": "def imprints2marc(self, key, value):\n    \"\"\"Populate the ``260`` MARC field.\"\"\"\n", "input": "", "output": "    return {\n        'a': value.get('place'),\n        'b': value.get('publisher'),\n        'c': value.get('date'),\n    }", "category": "Python"}, {"instruction": "def update(self):\n        \"\"\"\n        Update connection information of all nodes in this cluster.\n\n        It happens, for example, that public ip's are not available\n        immediately, therefore calling this method might help.\n        \"\"\"\n", "input": "", "output": "        for node in self.get_all_nodes():\n            try:\n                node.update_ips()\n\n                # If we previously did not have a preferred_ip or the\n                # preferred_ip is not in the current list, then try to connect\n                # to one of the node ips and update the preferred_ip.\n                if node.ips and \\\n                        not (node.preferred_ip and \\\n                                         node.preferred_ip in node.ips):\n                    node.connect()\n            except InstanceError as ex:\n                log.warning(\"Ignoring error updating information on node %s: %s\",\n                            node, ex)\n        self.repository.save_or_update(self)", "category": "Python"}, {"instruction": "def RybToRgb(hue):\n    '''Maps a hue on Itten's RYB color wheel to the standard RGB wheel.\n\n    Parameters:\n      :hue:\n        The hue on Itten's RYB color wheel [0...360]\n\n    Returns:\n      An approximation of the corresponding hue on the standard RGB wheel.\n\n    >>> Color.RybToRgb(15)\n    8.0\n\n    '''\n", "input": "", "output": "    d = hue % 15\n    i = int(hue / 15)\n    x0 = _RgbWheel[i]\n    x1 = _RgbWheel[i+1]\n    return x0 + (x1-x0) * d / 15", "category": "Python"}, {"instruction": "def gc_velocity_update(particle, social, state):\n    \"\"\" Guaranteed convergence velocity update.\n\n    Args:\n        particle: cipy.algorithms.pso.Particle: Particle to update the velocity\n            for.\n        social: cipy.algorithms.pso.Particle: The social best for the particle.\n        state: cipy.algorithms.pso.State: The state of the PSO algorithm.\n\n    Returns:\n        numpy.ndarray: the calculated velocity.\n    \"\"\"\n", "input": "", "output": "    gbest = state.swarm[gbest_idx(state.swarm)].position\n    if not np.array_equal(gbest, particle.position):\n        return std_velocity(particle, social, state)\n\n    rho = state.params['rho']\n    inertia = state.params['inertia']\n    v_max = state.params['v_max']\n    size = particle.position.size\n\n    r2 = state.rng.uniform(0.0, 1.0, size)\n    velocity = __gc_velocity_equation__(inertia, rho, r2, particle, gbest)\n    return __clamp__(velocity, v_max)", "category": "Python"}, {"instruction": "def get_stdin_data(self):\n        \"\"\"\n        Get words from stdin.\n\n        \"\"\"\n", "input": "", "output": "\n        if self.tty.in_is_tty:\n            # No pipez found\n            return False\n\n        if sys.version_info < (3, 0):\n            stdin_lines = (l.decode('utf-8') for l in sys.stdin.xreadlines())\n        else:\n            stdin_lines = (l for l in sys.stdin.readlines())\n\n        rx_word = re.compile(\"\\w+\", re.UNICODE)\n\n        # If we have stdin data, we should remove everything else!\n        self.words.clear()\n        word_list = [match.group(0)\n                     for line in stdin_lines\n                     for match in rx_word.finditer(line.lower())]\n        if self.ns.filter_stopwords:\n            word_list = self.filter_words(\n                word_list, stopwords=wow.STOPWORDS,\n                min_length=self.ns.min_length)\n\n        self.words.extend(word_list)\n\n        return True", "category": "Python"}, {"instruction": "def list(self):\n        \"\"\"Lists already deployed lambdas\"\"\"\n", "input": "", "output": "        for function in self.client.list_functions().get('Functions', []):\n            lines = json.dumps(function, indent=4, sort_keys=True).split('\\n')\n            for line in lines:\n                logger.info(line)", "category": "Python"}, {"instruction": "def _get_first_header(dicom_directory):\n    \"\"\"\n    Function to get the first dicom file form a directory and return the header\n    Useful to determine the type of data to convert\n\n    :param dicom_directory: directory with dicom files\n    \"\"\"\n", "input": "", "output": "    # looping over all files\n    for root, _, file_names in os.walk(dicom_directory):\n        # go over all the files and try to read the dicom header\n        for file_name in file_names:\n            file_path = os.path.join(root, file_name)\n            # check wither it is a dicom file\n            if not compressed_dicom.is_dicom_file(file_path):\n                continue\n            # read the headers\n            return compressed_dicom.read_file(file_path,\n                                              stop_before_pixels=True,\n                                              force=dicom2nifti.settings.pydicom_read_force)\n    # no dicom files found\n    raise ConversionError('NO_DICOM_FILES_FOUND')", "category": "Python"}, {"instruction": "def send_subscribe(self, dup, topics):\n        \"\"\"Send subscribe COMMAND to server.\"\"\"\n", "input": "", "output": "        pkt = MqttPkt()\n        \n        pktlen = 2 + sum([2+len(topic)+1 for (topic, qos) in topics])\n        pkt.command = NC.CMD_SUBSCRIBE | (dup << 3) | (1 << 1)\n        pkt.remaining_length = pktlen\n        \n        ret = pkt.alloc()\n        if ret != NC.ERR_SUCCESS:\n            return ret\n        \n        #variable header\n        mid = self.mid_generate()\n        pkt.write_uint16(mid)\n        \n        #payload\n        for (topic, qos) in topics:\n            pkt.write_string(topic)\n            pkt.write_byte(qos)\n        \n        return self.packet_queue(pkt)", "category": "Python"}, {"instruction": "def last_modified_time(path):\n    \"\"\"\n    Get the last modified time of path as a Timestamp.\n    \"\"\"\n", "input": "", "output": "    return pd.Timestamp(os.path.getmtime(path), unit='s', tz='UTC')", "category": "Python"}, {"instruction": "def parse_workflow_messages(self):\n        \"\"\"\n        Transmits client message that defined in\n        a workflow task's inputOutput extension\n\n       .. code-block:: xml\n\n            <bpmn2:extensionElements>\n            <camunda:inputOutput>\n            <camunda:inputParameter name=\"client_message\">\n            <camunda:map>\n              <camunda:entry key=\"title\">Te\u015fekk\u00fcrler</camunda:entry>\n              <camunda:entry key=\"body\">\u0130\u015flem Ba\u015far\u0131l\u0131</camunda:entry>\n              <camunda:entry key=\"type\">info</camunda:entry>\n            </camunda:map>\n            </camunda:inputParameter>\n            </camunda:inputOutput>\n            </bpmn2:extensionElements>\n\n        \"\"\"\n", "input": "", "output": "        if 'client_message' in self.current.spec.data:\n            m = self.current.spec.data['client_message']\n            self.current.msg_box(title=m.get('title'),\n                                 msg=m.get('body'),\n                                 typ=m.get('type', 'info'))", "category": "Python"}, {"instruction": "def _communicate(self):\n        \"\"\"Callback for communicate.\"\"\"\n", "input": "", "output": "        if (not self._communicate_first and\n                self._process.state() == QProcess.NotRunning):\n            self.communicate()\n        elif self._fired:\n            self._timer.stop()", "category": "Python"}, {"instruction": "def remove_repo(self, rname):\n        \"\"\"todo: Docstring for remove_repo\n\n        :param repo: arg description\n        :type repo: type description\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"%s\", rname)\n        repo = Repo(name=rname)\n        repo.remove()\n\n        return repo", "category": "Python"}, {"instruction": "def _get_model_metadata(model_class, metadata, version=None):\n    \"\"\"\n    Returns user-defined metadata, making sure information all models should \n    have is also available, as a dictionary\n    \"\"\"\n", "input": "", "output": "    from turicreate import __version__\n    info = {\n        'turicreate_version': __version__,\n        'type': model_class,\n    }\n    if version is not None:\n        info['version'] = str(version)\n    info.update(metadata)\n    return info", "category": "Python"}, {"instruction": "def get_qubits(self):\n        \"\"\"\n        The support of all the operators in the PauliSum object.\n\n        :returns: A list of all the qubits in the sum of terms.\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        return list(set().union(*[term.get_qubits() for term in self.terms]))", "category": "Python"}, {"instruction": "def returnIndexList(self, limit=False):\n        '''Return a list of integers that are list-index references to the\n        original list of dictionaries.\"\n\n        Example of use:\n\n        >>> test = [\n        ...    {\"name\": \"Jim\",   \"age\": 18, \"income\": 93000, \"order\": 2},\n        ...    {\"name\": \"Larry\", \"age\": 18,                  \"order\": 3},\n        ...    {\"name\": \"Joe\",   \"age\": 20, \"income\": 15000, \"order\": 1},\n        ...    {\"name\": \"Bill\",  \"age\": 19, \"income\": 29000, \"order\": 4},\n        ... ]\n        >>> print PLOD(test).returnIndexList()\n        [0, 1, 2, 3]\n        >>> print PLOD(test).sort(\"name\").returnIndexList()\n        [3, 0, 2, 1]\n        \n        :param limit:\n           A number limiting the quantity of entries to return. Defaults to\n           False, which means that the full list is returned.\n        :return:\n           A list of integers representing the original indices.\n        '''\n", "input": "", "output": "        if limit==False:\n            return self.index_track\n        result = []\n        for i in range(limit):\n            if len(self.table)>i:\n                result.append(self.index_track[i])\n        return result", "category": "Python"}, {"instruction": "def extract_blocking(obj):\n    \"\"\"Extract index and watch from :class:`Blocking`\n\n    Parameters:\n        obj (Blocking): the blocking object\n    Returns:\n        tuple: index and watch\n    \"\"\"\n", "input": "", "output": "    if isinstance(obj, tuple):\n        try:\n            a, b = obj\n        except:\n            raise TypeError(\"Not a Blocking object\")\n    else:\n        a, b = obj, None\n    return extract_attr(a, keys=[\"Index\"]), b", "category": "Python"}, {"instruction": "def iexpand(string, keep_escapes=False):\n    \"\"\"Expand braces and return an iterator.\"\"\"\n", "input": "", "output": "\n    if isinstance(string, bytes):\n        is_bytes = True\n        string = string.decode('latin-1')\n\n    else:\n        is_bytes = False\n\n    if is_bytes:\n        return (entry.encode('latin-1') for entry in ExpandBrace(keep_escapes).expand(string))\n\n    else:\n        return (entry for entry in ExpandBrace(keep_escapes).expand(string))", "category": "Python"}, {"instruction": "def calculate_indent(text):\n    \"\"\"\n    :param text:\n    :type text: str\n    :return:\n    \"\"\"\n", "input": "", "output": "    indent = 0\n    for c in text:\n        if c is '\\t':\n            raise ValueError()\n        if c is not ' ':\n            return indent,text[indent:]\n        indent += 1\n    return indent,''", "category": "Python"}, {"instruction": "def sample_observed_state(self, s: pd.Series) -> Dict:\n        \"\"\" Sample observed state vector. This is the implementation of the\n        emission function.\n\n        Args:\n            s: Latent state vector.\n\n        Returns:\n            Observed state vector.\n        \"\"\"\n", "input": "", "output": "\n        return {\n            n[0]: {\n                i.name: np.random.normal(s[n[0]] * i.mean, i.stdev)\n                for i in n[1][\"indicators\"].values()\n            }\n            for n in self.nodes(data=True)\n        }", "category": "Python"}, {"instruction": "def rm_files(path, extension):\n    \"\"\"\n    Remove all files in the given directory with the given extension\n\n    :param str path: Directory\n    :param str extension: File type to remove\n    :return none:\n    \"\"\"\n", "input": "", "output": "    files = list_files(extension, path)\n    for file in files:\n        if file.endswith(extension):\n            os.remove(os.path.join(path, file))\n    return", "category": "Python"}, {"instruction": "def sanitize(value):\n    \"\"\"Strips all undesirable characters out of potential file paths.\"\"\"\n", "input": "", "output": "\n    value = unicodedata.normalize('NFKD', value)\n    value = value.strip()\n    value = re.sub('[^./\\w\\s-]', '', value)\n    value = re.sub('[-\\s]+', '-', value)\n\n    return value", "category": "Python"}, {"instruction": "def _cache_key_select_state(method, self, workflow_id, field_id, field_title):\n    \"\"\"\n    This function returns the key used to decide if select_state has to be recomputed\n    \"\"\"\n", "input": "", "output": "    key = update_timer(), workflow_id, field_id, field_title\n    return key", "category": "Python"}, {"instruction": "def delete_extra_files(self, relpaths, cloud_objs):\n        \"\"\"\n        Deletes any objects from the container that do not exist locally.\n        \"\"\"\n", "input": "", "output": "        for cloud_obj in cloud_objs:\n            if cloud_obj not in relpaths:\n                if not self.test_run:\n                    self.delete_cloud_obj(cloud_obj)\n                self.delete_count += 1\n                if not self.quiet or self.verbosity > 1:\n                    print(\"Deleted: {0}\".format(cloud_obj))", "category": "Python"}, {"instruction": "def accept(self):\n        \"\"\"Method invoked when OK button is clicked.\"\"\"\n", "input": "", "output": "        try:\n            self.save_metadata()\n        except InvalidValidationException as e:\n            display_warning_message_box(\n                self, tr('Invalid Field Mapping'), str(e))\n            return\n        super(FieldMappingDialog, self).accept()", "category": "Python"}, {"instruction": "def pad_pdf_pages(pdf_name, pages_per_q) -> None:\n    \"\"\"\n    Checks if PDF has the correct number of pages. If it has too many, warns\n    the user. If it has too few, adds blank pages until the right length is\n    reached.\n    \"\"\"\n", "input": "", "output": "    pdf = PyPDF2.PdfFileReader(pdf_name)\n    output = PyPDF2.PdfFileWriter()\n    num_pages = pdf.getNumPages()\n    if num_pages > pages_per_q:\n        logging.warning('{} has {} pages. Only the first '\n                        '{} pages will get output.'\n                        .format(pdf_name, num_pages, pages_per_q))\n\n    # Copy over up to pages_per_q pages\n    for page in range(min(num_pages, pages_per_q)):\n        output.addPage(pdf.getPage(page))\n\n    # Pad if necessary\n    if num_pages < pages_per_q:\n        for page in range(pages_per_q - num_pages):\n            output.addBlankPage()\n\n    # Output the PDF\n    with open(pdf_name, 'wb') as out_file:\n        output.write(out_file)", "category": "Python"}, {"instruction": "def require_condition(cls, expr, message, *format_args, **format_kwds):\n        \"\"\"\n        used to assert a certain state. If the expression renders a false\n        value, an exception will be raised with the supplied message\n\n        :param: message:     The failure message to attach to the raised Buzz\n        :param: expr:        A boolean value indicating an evaluated expression\n        :param: format_args: Format arguments. Follows str.format convention\n        :param: format_kwds: Format keyword args. Follows str.format convetion\n        \"\"\"\n", "input": "", "output": "        if not expr:\n            raise cls(message, *format_args, **format_kwds)", "category": "Python"}, {"instruction": "def delete_device_subscriptions(self, device_id):\n        \"\"\"Removes a device's subscriptions\n\n        :param device_id: ID of the device (Required)\n        :returns: None\n        \"\"\"\n", "input": "", "output": "        api = self._get_api(mds.SubscriptionsApi)\n        return api.delete_endpoint_subscriptions(device_id)", "category": "Python"}, {"instruction": "def set_group_status(group_id, status, **kwargs):\n    \"\"\"\n        Set the status of a group to 'X'\n    \"\"\"\n", "input": "", "output": "    user_id = kwargs.get('user_id')\n    try:\n        group_i = db.DBSession.query(ResourceGroup).filter(ResourceGroup.id == group_id).one()\n    except NoResultFound:\n        raise ResourceNotFoundError(\"ResourceGroup %s not found\"%(group_id))\n\n    group_i.network.check_write_permission(user_id)\n\n    group_i.status = status\n\n    db.DBSession.flush()\n\n    return group_i", "category": "Python"}, {"instruction": "def is_sql_equal(sqls1, sqls2):\n    \"\"\"\n    Find out equality of two SQL items.\n\n    See https://docs.djangoproject.com/en/1.8/ref/migration-operations/#runsql.\n    Args:\n        sqls1, sqls2: SQL items, have the same format as supported by Django's RunSQL operation.\n    Returns:\n        (bool) `True` if equal, otherwise `False`.\n    \"\"\"\n", "input": "", "output": "    is_seq1 = isinstance(sqls1, (list, tuple))\n    is_seq2 = isinstance(sqls2, (list, tuple))\n\n    if not is_seq1:\n        sqls1 = (sqls1,)\n    if not is_seq2:\n        sqls2 = (sqls2,)\n\n    if len(sqls1) != len(sqls2):\n        return False\n\n    for sql1, sql2 in zip(sqls1, sqls2):\n        sql1, params1 = _sql_params(sql1)\n        sql2, params2 = _sql_params(sql2)\n        if sql1 != sql2 or params1 != params2:\n            return False\n    return True", "category": "Python"}, {"instruction": "def custom_size(self, minimum: int = 40, maximum: int = 62) -> int:\n        \"\"\"Generate clothing size using custom format.\n\n        :param minimum: Minimum value.\n        :param maximum: Maximum value.\n        :return: Clothing size.\n        \"\"\"\n", "input": "", "output": "        return self.random.randint(minimum, maximum)", "category": "Python"}, {"instruction": "def max_brightness(self):\n        \"\"\"\n        Returns the maximum allowable brightness value.\n        \"\"\"\n", "input": "", "output": "        self._max_brightness, value = self.get_cached_attr_int(self._max_brightness, 'max_brightness')\n        return value", "category": "Python"}, {"instruction": "def addPrivateCertificate(self, subjectName, existingCertificate=None):\n        \"\"\"\n        Add a PrivateCertificate object to this store for this subjectName.\n\n        If existingCertificate is None, add a new self-signed certificate.\n        \"\"\"\n", "input": "", "output": "        if existingCertificate is None:\n            assert '@' not in subjectName, \"Don't self-sign user certs!\"\n            mainDN = DistinguishedName(commonName=subjectName)\n            mainKey = KeyPair.generate()\n            mainCertReq = mainKey.certificateRequest(mainDN)\n            mainCertData = mainKey.signCertificateRequest(\n                mainDN, mainCertReq,\n                lambda dn: True,\n                self.genSerial(subjectName)\n            )\n            mainCert = mainKey.newCertificate(mainCertData)\n        else:\n            mainCert = existingCertificate\n        self.localStore[subjectName] = mainCert", "category": "Python"}, {"instruction": "def abcell (a, b, c):\n    \"\"\"Given the nontrivial parameters for evaluation a 2D Gaussian as a\n    polynomial, compute the equivalent ellipse parameters (major, minor, pa)\n\n    Inputs: (a, b, c), where z = exp (ax\u00b2 + bxy + cy\u00b2)\n\n    Returns:\n\n    * mjr: major axis (sigma not FWHM) of the Gaussian\n    * mnr: minor axis (sigma not FWHM) of the Gaussian\n    * pa: position angle (from +x to +y) of the Gaussian, radians\n\n    \"\"\"\n", "input": "", "output": "    from numpy import arctan2, sqrt\n\n    bad = _abccheck (a, b, c)\n    pa = 0.5 * arctan2 (b, a - c)\n\n    t1 = np.sqrt ((a - c)**2 + b**2)\n    t2 = -t1 - a - c\n    bad |= (t2 <= 0)\n    mjr = t2**-0.5\n\n    t2 = t1 - a - c\n    bad |= (t2 <= 0)\n    mnr = t2**-0.5\n\n    w = np.where (bad)\n    mjr[w] = np.nan\n    mnr[w] = np.nan\n    pa[w] = np.nan\n\n    return ellnorm (mjr, mnr, pa)", "category": "Python"}, {"instruction": "def out_filename(template, n_val, mode):\n    \"\"\"Determine the output filename\"\"\"\n", "input": "", "output": "    return '{0}_{1}_{2}.cpp'.format(template.name, n_val, mode.identifier)", "category": "Python"}, {"instruction": "def iter_links(self, elements):\n        '''Iterate the document root for links.\n\n        Returns:\n            iterable: A iterator of :class:`LinkedInfo`.\n        '''\n", "input": "", "output": "        for element in elements:\n            if not isinstance(element, Element):\n                continue\n\n            for link_infos in self.iter_links_element(element):\n                yield link_infos", "category": "Python"}, {"instruction": "def get_assessment_taken_bank_assignment_session(self, proxy):\n        \"\"\"Gets the session for assigning taken assessments to bank mappings.\n\n        arg:    proxy (osid.proxy.Proxy): a proxy\n        return: (osid.assessment.AssessmentTakenBankAssignmentSession) -\n                an ``AssessmentTakenBankAssignmentSession``\n        raise:  NullArgument - ``proxy`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented -\n                ``supports_assessment_taken_bank_assignment()`` is\n                ``false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_assessment_taken_bank_assignment()`` is ``true``.*\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_assessment_taken_bank_assignment():\n            raise errors.Unimplemented()\n        # pylint: disable=no-member\n        return sessions.AssessmentTakenBankAssignmentSession(proxy=proxy, runtime=self._runtime)", "category": "Python"}, {"instruction": "def _get_signature(self):\n        \"\"\"\n        Returns a 32-character hexadecimal md5 hash of the signature string.\n        \"\"\"\n", "input": "", "output": "\n        keys = list(self.params.keys())\n\n        keys.sort()\n\n        string = \"\"\n\n        for name in keys:\n            string += name\n            string += self.params[name]\n\n        string += self.api_secret\n\n        return md5(string)", "category": "Python"}, {"instruction": "def do_help(self, args):\n        \"\"\"Get help on commands\n        'help' or '?' with no arguments prints a list of commands for which help is available\n        'help <command>' or '? <command>' gives help on <command>\n        \"\"\"\n", "input": "", "output": "        if _debug: ConsoleCmd._debug(\"do_help %r\", args)\n\n        # the only reason to define this method is for the help text in the doc string\n        cmd.Cmd.do_help(self, args)", "category": "Python"}, {"instruction": "def delete_requests_request_name(self, request_name, synchronous=None):\n        \"\"\"DeleteRequestsRequestName.\n        [Preview API] Delete a symbol request by request name.\n        :param str request_name:\n        :param bool synchronous: If true, delete all the debug entries under this request synchronously in the current session. If false, the deletion will be postponed to a later point and be executed automatically by the system.\n        \"\"\"\n", "input": "", "output": "        query_parameters = {}\n        if request_name is not None:\n            query_parameters['requestName'] = self._serialize.query('request_name', request_name, 'str')\n        if synchronous is not None:\n            query_parameters['synchronous'] = self._serialize.query('synchronous', synchronous, 'bool')\n        self._send(http_method='DELETE',\n                   location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                   version='5.0-preview.1',\n                   query_parameters=query_parameters)", "category": "Python"}, {"instruction": "def add_contents_to_zip(zip_file, path):\n    \"\"\"\n    Zip the contents of <path> into <zip_file>.\n    :param str|unicode zip_file: The file name of the zip file\n    :param str|unicode path: Full path of the directory to zip up\n    \"\"\"\n", "input": "", "output": "    path = path.rstrip(os.sep)\n    with zipfile.ZipFile(zip_file, 'a') as zf:\n        for root, dirs, files in os.walk(path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                zf.write(file_path, file_path[len(path)+1:])", "category": "Python"}, {"instruction": "def connection_sync(self, connection_id, connProps=None):\n        \"\"\"Synchronizes connection properties with the server.\n\n        :param connection_id:\n            ID of the current connection.\n\n        :param connProps:\n            Dictionary with the properties that should be changed.\n\n        :returns:\n            A ``common_pb2.ConnectionProperties`` object.\n        \"\"\"\n", "input": "", "output": "        if connProps is None:\n            connProps = {}\n\n        request = requests_pb2.ConnectionSyncRequest()\n        request.connection_id = connection_id\n        request.conn_props.auto_commit = connProps.get('autoCommit', False)\n        request.conn_props.has_auto_commit = True\n        request.conn_props.read_only = connProps.get('readOnly', False)\n        request.conn_props.has_read_only = True\n        request.conn_props.transaction_isolation = connProps.get('transactionIsolation', 0)\n        request.conn_props.catalog = connProps.get('catalog', '')\n        request.conn_props.schema = connProps.get('schema', '')\n\n        response_data = self._apply(request)\n        response = responses_pb2.ConnectionSyncResponse()\n        response.ParseFromString(response_data)\n        return response.conn_props", "category": "Python"}, {"instruction": "def _final_frame_length(header, final_frame_bytes):\n    \"\"\"Calculates the length of a final ciphertext frame, given a complete header\n    and the number of bytes of ciphertext in the final frame.\n\n    :param header: Complete message header object\n    :type header: aws_encryption_sdk.structures.MessageHeader\n    :param int final_frame_bytes: Bytes of ciphertext in the final frame\n    :rtype: int\n    \"\"\"\n", "input": "", "output": "    final_frame_length = 4  # Sequence Number End\n    final_frame_length += 4  # Sequence Number\n    final_frame_length += header.algorithm.iv_len  # IV\n    final_frame_length += 4  # Encrypted Content Length\n    final_frame_length += final_frame_bytes  # Encrypted Content\n    final_frame_length += header.algorithm.auth_len  # Authentication Tag\n    return final_frame_length", "category": "Python"}, {"instruction": "def render_template(self, template_parameters, template_id):\n        \"\"\"RenderTemplate.\n        [Preview API]\n        :param :class:`<TemplateParameters> <azure.devops.v5_1.cix.models.TemplateParameters>` template_parameters:\n        :param str template_id:\n        :rtype: :class:`<Template> <azure.devops.v5_1.cix.models.Template>`\n        \"\"\"\n", "input": "", "output": "        route_values = {}\n        if template_id is not None:\n            route_values['templateId'] = self._serialize.url('template_id', template_id, 'str')\n        content = self._serialize.body(template_parameters, 'TemplateParameters')\n        response = self._send(http_method='POST',\n                              location_id='eb5d6d1d-98a2-4bbd-9028-f9a6b2d66515',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('Template', response)", "category": "Python"}, {"instruction": "def getLoader(user, repo, sha=None, prov=None):\n    \"\"\"Build a fileLoader (LocalLoader or GithubLoader) for the given repository.\"\"\"\n", "input": "", "output": "    if user is None and repo is None:\n        loader = LocalLoader()\n    else:\n        loader = GithubLoader(user, repo, sha, prov)\n    return loader", "category": "Python"}, {"instruction": "def declaration_path(decl):\n    \"\"\"\n    Returns a list of parent declarations names.\n\n    Args:\n        decl (declaration_t): declaration for which declaration path\n                              should be calculated.\n\n    Returns:\n        list[(str | basestring)]: list of names, where first item is the top\n                                  parent name and last item the inputted\n                                  declaration name.\n    \"\"\"\n", "input": "", "output": "\n    if not decl:\n        return []\n    if not decl.cache.declaration_path:\n        result = [decl.name]\n        parent = decl.parent\n        while parent:\n            if parent.cache.declaration_path:\n                result.reverse()\n                decl.cache.declaration_path = parent.cache.declaration_path + \\\n                    result\n                return decl.cache.declaration_path\n            else:\n                result.append(parent.name)\n                parent = parent.parent\n        result.reverse()\n        decl.cache.declaration_path = result\n        return result\n\n    return decl.cache.declaration_path", "category": "Python"}, {"instruction": "def enc_setup(self, enc_alg, msg, auth_data=b'', key=None, iv=\"\"):\n        \"\"\" Encrypt JWE content.\n\n        :param enc_alg: The JWE \"enc\" value specifying the encryption algorithm\n        :param msg: The plain text message\n        :param auth_data: Additional authenticated data\n        :param key: Key (CEK)\n        :return: Tuple (ciphertext, tag), both as bytes\n        \"\"\"\n", "input": "", "output": "\n        iv = self._generate_iv(enc_alg, iv)\n\n        if enc_alg in [\"A192GCM\", \"A128GCM\", \"A256GCM\"]:\n            aes = AES_GCMEncrypter(key=key)\n            ctx, tag = split_ctx_and_tag(aes.encrypt(msg, iv, auth_data))\n        elif enc_alg in [\"A128CBC-HS256\", \"A192CBC-HS384\", \"A256CBC-HS512\"]:\n            aes = AES_CBCEncrypter(key=key)\n            ctx, tag = aes.encrypt(msg, iv, auth_data)\n        else:\n            raise NotSupportedAlgorithm(enc_alg)\n\n        return ctx, tag, aes.key", "category": "Python"}, {"instruction": "def run_event_hooks(event, task):\n    \"\"\" Executes registered task event plugins for the provided event and task.\n\n        `event`\n            Name of the event to trigger for the plugin:\n                ('task_start', 'task_run', 'task_end')\n        `task`\n            ``Task`` instance.\n        \"\"\"\n", "input": "", "output": "\n    # get chain of classes registered for this event\n    call_chain = _event_hooks.get(event)\n\n    if call_chain:\n        # lookup the associated class method for this event\n        event_methods = {\n            'task_start': 'on_taskstart',\n            'task_run': 'on_taskrun',\n            'task_end': 'on_taskend'\n        }\n        method = event_methods.get(event)\n\n        if method:\n            for _, get_plugin in call_chain:\n                plugin_obj = get_plugin()\n\n                if not _is_plugin_disabled(plugin_obj):\n                    try:\n                        getattr(plugin_obj, method)(task)  # execute\n                    except Exception:\n                        # TODO: log these issues for plugin author or user\n                        pass", "category": "Python"}, {"instruction": "def read_array(self, infile, var_name):\n        \"\"\"Directly return a numpy array for a given variable name\"\"\"\n", "input": "", "output": "        file_handle = self.read_cdf(infile)\n        try:\n            # return the data array\n            return file_handle.variables[var_name][:]\n        except KeyError:\n            print(\"Cannot find variable: {0}\".format(var_name))\n            raise KeyError", "category": "Python"}, {"instruction": "def normalize_name(self, name):\n        \"\"\"\n        Normalize a field or level name.\n\n        :param name: The field or level name (a string).\n        :returns: The normalized name (a string).\n\n        Transforms all strings to lowercase and resolves level name aliases\n        (refer to :func:`find_level_aliases()`) to their canonical name:\n\n        >>> from coloredlogs import NameNormalizer\n        >>> from humanfriendly import format_table\n        >>> nn = NameNormalizer()\n        >>> sample_names = ['DEBUG', 'INFO', 'WARN', 'WARNING', 'ERROR', 'FATAL', 'CRITICAL']\n        >>> print(format_table([(n, nn.normalize_name(n)) for n in sample_names]))\n        -----------------------\n        | DEBUG    | debug    |\n        | INFO     | info     |\n        | WARN     | warning  |\n        | WARNING  | warning  |\n        | ERROR    | error    |\n        | FATAL    | critical |\n        | CRITICAL | critical |\n        -----------------------\n        \"\"\"\n", "input": "", "output": "        name = name.lower()\n        if name in self.aliases:\n            name = self.aliases[name]\n        return name", "category": "Python"}, {"instruction": "def in_order_traverse(self):\n        \"\"\"\n        In-order traversal of the tree\n        \"\"\"\n", "input": "", "output": "        result = []\n\n        if not self.node:\n            return result\n\n        result.extend(self.node.left.in_order_traverse())\n        result.append(self.node.key)\n        result.extend(self.node.right.in_order_traverse())\n        return result", "category": "Python"}, {"instruction": "def radio_field(*args, **kwargs):\n    '''\n    Get a password\n    '''\n", "input": "", "output": "    radio_field = wtforms.RadioField(*args, **kwargs)\n    radio_field.input_type = 'radio_field'\n    return radio_field", "category": "Python"}, {"instruction": "def expected(self, observable, beta=1e5):\n        \"\"\"Wrapper to the expected_value function to fix the eigenbasis\"\"\"\n", "input": "", "output": "        return expected_value(observable,\n                         self.eig_energies,\n                         self.eig_states,\n                         beta)", "category": "Python"}, {"instruction": "def process_unset(line, annotations):\n    \"\"\"Process UNSET lines in BEL Script\"\"\"\n", "input": "", "output": "\n    matches = re.match('UNSET\\s+\"?(.*?)\"?\\s*$', line)\n    if matches:\n        val = matches.group(1)\n        if val == \"ALL\" or val == \"STATEMENT_GROUP\":\n            annotations = {}\n        elif re.match(\"{\", val):\n            vals = convert_csv_str_to_list(val)\n            for val in vals:\n                annotations.pop(val, None)\n        else:\n            annotations.pop(val, None)\n\n    else:\n        log.warn(f\"Problem with UNSET line: {line}\")\n\n    return annotations", "category": "Python"}, {"instruction": "def _sendDDEcommand(self, cmd, timeout=None):\n        \"\"\"Send command to DDE client\"\"\"\n", "input": "", "output": "        reply = self.conversation.Request(cmd, timeout)\n        if self.pyver > 2:\n            reply = reply.decode('ascii').rstrip()\n        return reply", "category": "Python"}, {"instruction": "def bed(args):\n    \"\"\"\n    %prog bed maffiles > out.bed\n\n    Convert a folder of maf alignments to the bed features\n    then useful to check coverage, etc.\n    \"\"\"\n", "input": "", "output": "    p = OptionParser(bed.__doc__)\n\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(p.print_help())\n\n    flist = args\n    prefix = flist[0].split(\".\")[0]\n\n    j = 0\n    for f in flist:\n        reader = Maf(f).reader\n        for rec in reader:\n            a, b = rec.components\n\n            for a, tag in zip((a, b), \"ab\"):\n                name = \"{0}_{1:07d}{2}\".format(prefix, j, tag)\n                print(\"\\t\".join(str(x) for x in (a.src, a.forward_strand_start, \\\n                        a.forward_strand_end, name)))\n\n            j += 1", "category": "Python"}, {"instruction": "def change_window(self, size_window):\n        ''' Change the region of interest\n\n        Args:\n            size_window (float): Radius of the region of interest (km)\n\n        Notes:\n            Change the attributes ``size_window`` and ``window`` to\n            correspond to the new region of interest.\n\n        '''\n", "input": "", "output": "        self.size_window = size_window\n        self.window = self.lambert_window(\n            self.size_window, self.lat0, self.lon0)", "category": "Python"}, {"instruction": "def _at_percentile(tau, var_tau, percentile):\n    \"\"\"\n    Returns the value of the inverse chi-2 distribution at the given\n    percentile from the mean and variance of the uncertainty model, as\n    reported in equations 5.1 - 5.3 of Al Atik (2015)\n    \"\"\"\n", "input": "", "output": "    assert (percentile >= 0.0) and (percentile <= 1.0)\n    c_val = _scaling(tau, var_tau)\n    k_val = _dof(tau, var_tau)\n    return np.sqrt(c_val * chi2.ppf(percentile, df=k_val))", "category": "Python"}, {"instruction": "def returner(ret):\n    '''\n    Send a Telegram message with the data.\n\n    :param ret:     The data to be sent.\n    :return:        Boolean if message was sent successfully.\n    '''\n", "input": "", "output": "    _options = _get_options(ret)\n\n    chat_id = _options.get('chat_id')\n    token = _options.get('token')\n\n    if not chat_id:\n        log.error('telegram.chat_id not defined in salt config')\n    if not token:\n        log.error('telegram.token not defined in salt config')\n\n    returns = ret.get('return')\n\n    message = ('id: {0}\\r\\n'\n               'function: {1}\\r\\n'\n               'function args: {2}\\r\\n'\n               'jid: {3}\\r\\n'\n               'return: {4}\\r\\n').format(\n                    ret.get('id'),\n                    ret.get('fun'),\n                    ret.get('fun_args'),\n                    ret.get('jid'),\n                    returns)\n\n    return __salt__['telegram.post_message'](message,\n                                             chat_id=chat_id,\n                                             token=token)", "category": "Python"}, {"instruction": "def turn_physical_on(Pot,ro=None,vo=None):\n    \"\"\"\n    NAME:\n       \n       turn_physical_on\n\n    PURPOSE:\n    \n       turn on automatic returning of outputs in physical units\n    \n    INPUT:\n    \n       ro= reference distance (kpc; can be Quantity)\n       \n       vo= reference velocity (km/s; can be Quantity)\n\n    OUTPUT:\n    \n        (none)\n    \n    HISTORY:\n    \n        2016-01-30 - Written - Bovy (UofT)\n    \n    \"\"\"\n", "input": "", "output": "    if isinstance(Pot,list):\n        for pot in Pot:\n            turn_physical_on(pot,ro=ro,vo=vo)\n    else:\n        Pot.turn_physical_on(ro=ro,vo=vo)\n    return None", "category": "Python"}, {"instruction": "def default_from_address(self):\n        \"\"\"\n        Cache the coinbase address so that we don't make two requests for every\n        single transaction.\n        \"\"\"\n", "input": "", "output": "        if self._coinbase_cache_til is not None:\n            if time.time - self._coinbase_cache_til > 30:\n                self._coinbase_cache_til = None\n                self._coinbase_cache = None\n\n        if self._coinbase_cache is None:\n            self._coinbase_cache = self.get_coinbase()\n\n        return self._coinbase_cache", "category": "Python"}, {"instruction": "def squeeze(self):\n        \"\"\"Remove the degenerate dimensions.\n\n        Note that no changes are made in-place.\n\n        Returns\n        -------\n        squeezed : `IntervalProd`\n            Squeezed set.\n\n        Examples\n        --------\n        >>> min_pt, max_pt = [-1, 0, 2], [-0.5, 1, 3]\n        >>> rbox = IntervalProd(min_pt, max_pt)\n        >>> rbox.collapse(1, 0).squeeze()\n        IntervalProd([-1.,  2.], [-0.5,  3. ])\n        >>> rbox.collapse([1, 2], [0, 2.5]).squeeze()\n        IntervalProd(-1.0, -0.5)\n        >>> rbox.collapse([0, 1, 2], [-1, 0, 2.5]).squeeze()\n        IntervalProd([], [])\n        \"\"\"\n", "input": "", "output": "        b_new = self.min_pt[self.nondegen_byaxis]\n        e_new = self.max_pt[self.nondegen_byaxis]\n        return IntervalProd(b_new, e_new)", "category": "Python"}, {"instruction": "def add_room_alias(self, room_alias):\n        \"\"\"Add an alias to the room and return True if successful.\"\"\"\n", "input": "", "output": "        try:\n            self.client.api.set_room_alias(self.room_id, room_alias)\n            return True\n        except MatrixRequestError:\n            return False", "category": "Python"}, {"instruction": "def get_unit_name(self, surf, name, radius):\n    \"\"\"Get a length limited unit name for drawing units.\"\"\"\n", "input": "", "output": "    key = (name, radius)\n    if key not in self._name_lengths:\n      max_len = surf.world_to_surf.fwd_dist(radius * 1.6)\n      for i in range(len(name)):\n        if self._font_small.size(name[:i + 1])[0] > max_len:\n          self._name_lengths[key] = name[:i]\n          break\n      else:\n        self._name_lengths[key] = name\n    return self._name_lengths[key]", "category": "Python"}, {"instruction": "def to_json(obj, pretty=False):\n  \"\"\"Converts an object to JSON, using the defaults specified in register_json_default.\n\n  :obj: the object to convert to JSON\n  :pretty: if True, extra whitespace is added to make the output easier to read\n  \"\"\"\n", "input": "", "output": "  sort_keys = False\n  indent = None\n  separators = (\",\", \":\")\n\n  if isinstance(pretty, tuple):\n    sort_keys, indent, separators = pretty\n  elif pretty is True:\n    sort_keys = True\n    indent = 2\n    separators = (\", \", \": \")\n\n  return json.dumps(obj, sort_keys=sort_keys, indent=indent, separators=separators,\n                    default=json_default)", "category": "Python"}, {"instruction": "def set_pwm_frequency(self, value):\n        \"\"\"Set the frequency for all PWM output\n\n        :param value: the frequency in Hz\n        \"\"\"\n", "input": "", "output": "        self.__check_range('pwm_frequency', value)\n        reg_val = self.calc_pre_scale(value)\n        logger.debug(\"Calculated prescale value is %s\" % reg_val)\n        self.sleep()\n        self.write(Registers.PRE_SCALE, reg_val)\n        self.wake()", "category": "Python"}, {"instruction": "def create(self, tables, version):\n        \"\"\"Do the actual work of creating the database, filling its tables with\n        values, creating indices, and setting the datacache version metadata.\n\n        Parameters\n        ----------\n        tables : list\n            List of datacache.DatabaseTable objects\n\n        version : int\n        \"\"\"\n", "input": "", "output": "        for table in tables:\n            self._create_table(\n                table_name=table.name,\n                column_types=table.column_types,\n                primary=table.primary_key,\n                nullable=table.nullable)\n            self._fill_table(table.name, table.rows)\n            self._create_indices(table.name, table.indices)\n        self._finalize_database(version)\n        self._commit()", "category": "Python"}, {"instruction": "def custom_search(self, index, doc_type):\n        '''\n        Performs a search on a custom elasticsearch index and mapping. Will not attempt to map result objects.\n        '''\n", "input": "", "output": "        from bungiesearch import Bungiesearch\n        return Bungiesearch(raw_results=True).index(index).doc_type(doc_type)", "category": "Python"}, {"instruction": "def safe_get_user_model():\n    \"\"\"\n    Safe loading of the User model, customized or not.\n    \"\"\"\n", "input": "", "output": "    user_app, user_model = settings.AUTH_USER_MODEL.split('.')\n    return apps.get_registered_model(user_app, user_model)", "category": "Python"}, {"instruction": "async def updateCronJob(self, iden, query):\n        '''\n        Change an existing cron job's query\n\n        Args:\n            iden (bytes):  The iden of the cron job to be changed\n        '''\n", "input": "", "output": "        cron = self.cell.agenda.appts.get(iden)\n        if cron is None:\n            raise s_exc.NoSuchIden()\n        self._trig_auth_check(cron.useriden)\n        await self.cell.agenda.mod(iden, query)", "category": "Python"}, {"instruction": "def sources_remove(source_uri, ruby=None, runas=None, gem_bin=None):\n    '''\n    Remove a gem source.\n\n    :param source_uri: string\n        The source URI to remove.\n    :param gem_bin: string : None\n        Full path to ``gem`` binary to use.\n    :param ruby: string : None\n        If RVM or rbenv are installed, the ruby version and gemset to use.\n        Ignored if ``gem_bin`` is specified.\n    :param runas: string : None\n        The user to run gem as.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gem.sources_remove http://rubygems.org/\n    '''\n", "input": "", "output": "    return _gem(['sources', '--remove', source_uri],\n                ruby,\n                gem_bin=gem_bin,\n                runas=runas)", "category": "Python"}, {"instruction": "def end_index(self):\n        \"\"\"Return the 1-based index of the last item on this page.\"\"\"\n", "input": "", "output": "        paginator = self.paginator\n        # Special case for the last page because there can be orphans.\n        if self.number == paginator.num_pages:\n            return paginator.count\n        return (self.number - 1) * paginator.per_page + paginator.first_page", "category": "Python"}, {"instruction": "def cmd_condition_yaw(self, args):\n        '''yaw angle angular_speed angle_mode'''\n", "input": "", "output": "        if ( len(args) != 3):\n            print(\"Usage: yaw ANGLE ANGULAR_SPEED MODE:[0 absolute / 1 relative]\")\n            return\n\n        if (len(args) == 3):\n            angle = float(args[0])\n            angular_speed = float(args[1])\n            angle_mode = float(args[2])\n            print(\"ANGLE %s\" % (str(angle)))\n\n            self.master.mav.command_long_send(\n                self.settings.target_system,  # target_system\n                mavutil.mavlink.MAV_COMP_ID_SYSTEM_CONTROL, # target_component\n                mavutil.mavlink.MAV_CMD_CONDITION_YAW, # command\n                0, # confirmation\n                angle, # param1 (angle value)\n                angular_speed, # param2 (angular speed value)\n                0, # param3\n                angle_mode, # param4 (mode: 0->absolute / 1->relative)\n                0, # param5\n                0, # param6\n                0)", "category": "Python"}, {"instruction": "def populate_all():\n    \"\"\"Fetch and store all metadata from the AFIP.\"\"\"\n", "input": "", "output": "    ReceiptType.objects.populate()\n    ConceptType.objects.populate()\n    DocumentType.objects.populate()\n    VatType.objects.populate()\n    TaxType.objects.populate()\n    CurrencyType.objects.populate()", "category": "Python"}, {"instruction": "def has_linguist_kwargs(self, kwargs):\n        \"\"\"\n        Parses the given kwargs and returns True if they contain\n        linguist lookups.\n        \"\"\"\n", "input": "", "output": "        for k in kwargs:\n            if self.is_linguist_lookup(k):\n                return True\n        return False", "category": "Python"}, {"instruction": "def activate(self):\n        \"\"\"\n        Activates the bounce instance and updates it with the latest data.\n\n        :return: Activation status.\n        :rtype: `str`\n        \"\"\"\n", "input": "", "output": "        response = self._manager.activate(self.ID)\n        self._update(response[\"Bounce\"])\n        return response[\"Message\"]", "category": "Python"}, {"instruction": "def to_match(self):\n        \"\"\"Return a unicode object with the MATCH representation of this expression.\"\"\"\n", "input": "", "output": "        self.validate()\n\n        mark_name, _ = self.fold_scope_location.get_location_name()\n        validate_safe_string(mark_name)\n\n        template = u'$%(mark_name)s.size()'\n        template_data = {\n            'mark_name': mark_name,\n        }\n        return template % template_data", "category": "Python"}, {"instruction": "def annToMask(self, ann):\n        \"\"\"\n        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\n        :return: binary mask (numpy 2D array)\n        \"\"\"\n", "input": "", "output": "        rle = self.annToRLE(ann)\n        m = maskUtils.decode(rle)\n        return m", "category": "Python"}, {"instruction": "def hash_tuple(val, encoding='utf8', hash_key=None):\n    \"\"\"\n    Hash a single tuple efficiently\n\n    Parameters\n    ----------\n    val : single tuple\n    encoding : string, default 'utf8'\n    hash_key : string key to encode, default to _default_hash_key\n\n    Returns\n    -------\n    hash\n\n    \"\"\"\n", "input": "", "output": "    hashes = (_hash_scalar(v, encoding=encoding, hash_key=hash_key)\n              for v in val)\n\n    h = _combine_hash_arrays(hashes, len(val))[0]\n\n    return h", "category": "Python"}, {"instruction": "def extract_edges_from_callable(fn):\n    \"\"\"\n    This takes args and kwargs provided, and returns the names of the strings\n    assigned. If a string is not provided for a value, an exception is raised.\n\n    This is how we extract the edges provided in the brap call lambdas.\n    \"\"\"\n", "input": "", "output": "\n    def extractor(*args, **kwargs):\n        ", "category": "Python"}, {"instruction": "def add_include(self, name, included_scope, module):\n        \"\"\"Register an imported module into this scope.\n\n        Raises ``ThriftCompilerError`` if the name has already been used.\n        \"\"\"\n", "input": "", "output": "        # The compiler already ensures this. If we still get here with a\n        # conflict, that's a bug.\n        assert name not in self.included_scopes\n\n        self.included_scopes[name] = included_scope\n        self.add_surface(name, module)", "category": "Python"}, {"instruction": "def disable_all(self, disable):\n        \"\"\"Disables all modulation and outputs of the Standford MW func. generator\"\"\"\n", "input": "", "output": "        commands = ['ENBH 0', #disable high freq. rear output\n                    'ENBL 0', #disable low freq. front bnc\n                    'MODL 0'   #disable modulation\n                    ]\n        command_string = '\\n'.join(commands)\n        print_string = '\\n\\t' + command_string.replace('\\n', '\\n\\t')\n        logging.info(print_string)\n        if disable:\n            self.instr.write(command_string)", "category": "Python"}, {"instruction": "def choice(self, subscribers, message):\n        \"\"\"\n        Choose a random connection, favoring those that are reliable from\n        subscriber pool to deliver specified message.\n\n        @param subscribers: Collection of subscribed connections to destination. \n        @type subscribers: C{list} of L{coilmq.server.StompConnection}\n\n        @param message: The message to be delivered. \n        @type message: L{stompclient.frame.Frame}\n\n        @return: A random subscriber from the list or None if list is empty.\n        @rtype: L{coilmq.server.StompConnection}\n        \"\"\"\n", "input": "", "output": "        if not subscribers:\n            return None\n        reliable_subscribers = [\n            s for s in subscribers if s.reliable_subscriber]\n        if reliable_subscribers:\n            return random.choice(reliable_subscribers)\n        else:\n            return random.choice(subscribers)", "category": "Python"}, {"instruction": "def get_feature(name):\n    \"\"\"Get an instance of a ``Features`` class by ``name`` (str).\"\"\"\n", "input": "", "output": "    if name == 'css':\n        return CSSFeatures()\n    elif name == 'kohlschuetter':\n        return KohlschuetterFeatures()\n    elif name == 'readability':\n        return ReadabilityFeatures()\n    elif name == 'weninger':\n        return WeningerFeatures()\n    elif name == 'clustered_weninger':\n        return ClusteredWeningerFeatures()\n    else:\n        raise ValueError('invalid feature name: \"{}\"'.format(name))", "category": "Python"}, {"instruction": "def handle_operators(args):\n    \"\"\"usage: {program} operators\n\n    List the available operator plugins.\n    \"\"\"\n", "input": "", "output": "    assert args\n    print('\\n'.join(cosmic_ray.plugins.operator_names()))\n\n    return ExitCode.OK", "category": "Python"}, {"instruction": "def work_in(dirname=None):\n    \"\"\"\n    Context manager version of os.chdir. When exited, returns to the working\n    directory prior to entering.\n\n    Grabbed from cookiecutter, thanks audreyr!\n    \"\"\"\n", "input": "", "output": "    curdir = os.getcwd()\n    try:\n        if dirname is not None:\n            if dirname not in sys.path:\n                sys.path.insert(0, dirname)\n            os.chdir(dirname)\n        yield\n    finally:\n        os.chdir(curdir)", "category": "Python"}, {"instruction": "def flip(self):\n        \"\"\"This will switch major/minor around, regardless of frequency truth.\n\n        This is intended for forcing one of two populations to relate correctly\n        to the same genotype definitions. When flipped, Ps and Qs will be\n        backward, and the maf will no longer relate to the \"minor\" allele\n        frequency. However, it does allow clients to use the same calls for each\n        population without having to perform checks during those calculations.\n        \"\"\"\n", "input": "", "output": "\n        maj_count = self.maj_allele_count\n        self.maj_allele_count = self.min_allele_count\n        self.min_allele_count = maj_count\n\n        alleles = self.alleles\n        self.alleles = [alleles[1], alleles[0]]", "category": "Python"}, {"instruction": "def _post(self, url, data):\n        \"\"\"\n        Helper method: POST data to a given URL on TBA's API.\n\n        :param url: URL string to post data to and hash.\n        :pararm data: JSON data to post and hash.\n        :return: Requests Response object.\n\n        \"\"\"\n", "input": "", "output": "        return self.session.post(self.WRITE_URL_PRE + url % self.event_key, data=data, headers={'X-TBA-Auth-Sig': md5((self.auth_secret + '/api/trusted/v1/' + url % self.event_key + data).encode('utf-8')).hexdigest()})", "category": "Python"}, {"instruction": "def next(self):\n        \"\"\"return one dict which contains \"data\" and \"label\" \"\"\"\n", "input": "", "output": "        if self.iter_next():\n            self.data, self.label = self._read()\n            return {self.data_name  :  self.data[0][1],\n                    self.label_name :  self.label[0][1]}\n        else:\n            raise StopIteration", "category": "Python"}, {"instruction": "def program_end_action(self, text, loc, arg):\r\n        \"\"\"Checks if there is a 'main' function and the type of 'main' function\"\"\"\n", "input": "", "output": "        exshared.setpos(loc, text)\r\n        if DEBUG > 0:\r\n            print(\"PROGRAM_END:\",arg)\r\n            if DEBUG == 2: self.symtab.display()\r\n            if DEBUG > 2: return\r\n        index = self.symtab.lookup_symbol(\"main\",SharedData.KINDS.FUNCTION)\r\n        if index == None:\r\n            raise SemanticException(\"Undefined reference to 'main'\", False)\r\n        elif self.symtab.get_type(index) != SharedData.TYPES.INT:\r\n            self.warning(\"Return type of 'main' is not int\", False)", "category": "Python"}, {"instruction": "def chain(self, **kwargs):\n        \"\"\"\n        Add patterns chain, using configuration of this rebulk\n\n        :param pattern:\n        :type pattern:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        chain = self.build_chain(**kwargs)\n        self._patterns.append(chain)\n        return chain", "category": "Python"}, {"instruction": "def shutdown(self):\n\t\t\"\"\"Shut down the Executor.  Suspend all waiting jobs.\n\t\t\n\t\tRunning workers will terminate after finishing their current job items.\n\t\tThe call will block until all workers are terminated.\n\t\t\"\"\"\n", "input": "", "output": "\t\twith self.lock:\n\t\t\tself.pool.inqueue.put(StopIteration)   # Stop the pool workers\n\t\t\tself.waitqueue.put(StopIteration)      # Stop the input_feeder\n\t\t\t_iterqueue(self.waitqueue) >> item[-1] # Exhaust the waitqueue\n\t\t\tself.closed = True\n\t\tself.join()", "category": "Python"}, {"instruction": "def add_bond(self, key1, key2, bond):\n        \"\"\"Set a bond. Existing bond will be overwritten.\"\"\"\n", "input": "", "output": "        self.graph.add_edge(key1, key2, bond=bond)", "category": "Python"}, {"instruction": "def split_input_output(self, text):\n        \"\"\"Split code into input lines and output lines, according to the\n        input and output prompt templates.\"\"\"\n", "input": "", "output": "        lines = _to_lines(text)\n        i = 0\n        for line in lines:\n            if _starts_with_regex(line, self.input_prompt_regex):\n                i += 1\n            else:\n                break\n        return lines[:i], lines[i:]", "category": "Python"}, {"instruction": "def visit_ExceptHandler(self, node):\n        \"\"\" Exception may declare a new variable. \"\"\"\n", "input": "", "output": "        if node.name:\n            self.naming[node.name.id] = [frozenset()]\n        for stmt in node.body:\n            self.visit(stmt)", "category": "Python"}, {"instruction": "def is_compressed(path):\n        \"\"\"Test if path represents compressed file(s).\n\n        :param str path: Path to file(s).\n        :return: String specifying compression type if compressed, \"\" otherwise.\n        :rtype: :py:class:`str`\n        \"\"\"\n", "input": "", "output": "        if path.endswith(\".zip\"):\n            return \"zip\"\n        elif path.endswith(\".tar.gz\"):\n            return \"tar.gz\"\n        elif path.endswith(\".tar.bz2\"):\n            return \"tar.bz2\"\n        elif path.endswith(\".gz\"):\n            return \"gz\"\n        elif path.endswith(\".bz2\"):\n            return \"bz2\"\n        elif path.endswith(\".tar\"):\n            return \"tar\"\n        return \"\"", "category": "Python"}, {"instruction": "def CreateTaskStart(self):\n    \"\"\"Creates a task start.\n\n    Returns:\n      TaskStart: task start attribute container.\n    \"\"\"\n", "input": "", "output": "    task_start = TaskStart()\n    task_start.identifier = self.identifier\n    task_start.session_identifier = self.session_identifier\n    task_start.timestamp = self.start_time\n    return task_start", "category": "Python"}, {"instruction": "def get_container_for(self, instance):\n        \"\"\"Returns the container id used in slots to group analyses\n        \"\"\"\n", "input": "", "output": "        if IReferenceAnalysis.providedBy(instance):\n            return api.get_uid(instance.getSample())\n        return instance.getRequestUID()", "category": "Python"}, {"instruction": "def read(self, size_or_buffer, timeout = None):\n        r\"\"\"Read data from the endpoint.\n\n        The parameter size_or_buffer is either the number of bytes to\n        read or an array object where the data will be put in and timeout is the\n        time limit of the operation. The transfer type and endpoint address\n        are automatically inferred.\n\n        The method returns either an array object or the number of bytes\n        actually read.\n\n        For details, see the Device.read() method.\n        \"\"\"\n", "input": "", "output": "        return self.device.read(self, size_or_buffer, timeout)", "category": "Python"}, {"instruction": "def _invert_all(self):\n        \"\"\"Invert every bit.\"\"\"\n", "input": "", "output": "        set = self._datastore.setbyte\n        get = self._datastore.getbyte\n        for p in xrange(self._datastore.byteoffset, self._datastore.byteoffset + self._datastore.bytelength):\n            set(p, 256 + ~get(p))", "category": "Python"}, {"instruction": "async def stop(self):\n        \"\"\"\n        Stops playback from lavalink.\n\n        .. important::\n\n            This method will clear the queue.\n        \"\"\"\n", "input": "", "output": "        await self.node.stop(self.channel.guild.id)\n        self.queue = []\n        self.current = None\n        self.position = 0\n        self._paused = False", "category": "Python"}, {"instruction": "def magspec(frames, NFFT):\n    \"\"\"Compute the magnitude spectrum of each frame in frames. If frames is an NxD matrix, output will be Nx(NFFT/2+1).\n\n    :param frames: the array of frames. Each row is a frame.\n    :param NFFT: the FFT length to use. If NFFT > frame_len, the frames are zero-padded.\n    :returns: If frames is an NxD matrix, output will be Nx(NFFT/2+1). Each row will be the magnitude spectrum of the corresponding frame.\n    \"\"\"\n", "input": "", "output": "    if numpy.shape(frames)[1] > NFFT:\n        logging.warn(\n            'frame length (%d) is greater than FFT size (%d), frame will be truncated. Increase NFFT to avoid.',\n            numpy.shape(frames)[1], NFFT)\n    complex_spec = numpy.fft.rfft(frames, NFFT)\n    return numpy.absolute(complex_spec)", "category": "Python"}, {"instruction": "def translate(self):\n        \"\"\"Compile the template to a Python function.\"\"\"\n", "input": "", "output": "        expressions, varnames, funcnames = self.expr.translate()\n\n        argnames = []\n        for varname in varnames:\n            argnames.append(VARIABLE_PREFIX + varname)\n        for funcname in funcnames:\n            argnames.append(FUNCTION_PREFIX + funcname)\n\n        func = compile_func(\n            argnames,\n            [ast.Return(ast.List(expressions, ast.Load()))],\n        )\n\n        def wrapper_func(values={}, functions={}):\n            args = {}\n            for varname in varnames:\n                args[VARIABLE_PREFIX + varname] = values[varname]\n            for funcname in funcnames:\n                args[FUNCTION_PREFIX + funcname] = functions[funcname]\n            parts = func(**args)\n            return u''.join(parts)\n\n        return wrapper_func", "category": "Python"}, {"instruction": "def add_widget(self, widget):\n        \"\"\"Add the given widget to the tooltip\n\n        :param widget: the widget to add\n        :type widget: QtGui.QWidget\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        if self._buttons.get(widget):\n            return\n        btn = self.create_button(widget)\n        cb = partial(self.focus_widget, w=widget)\n        btn.clicked.connect(cb)\n        self.layout().addWidget(btn)\n        self._buttons[widget] = btn", "category": "Python"}, {"instruction": "def clone(self, id, name=None):\n        \"\"\"Clone a gist\n\n        Arguments:\n            id:   the gist identifier\n            name: the name to give the cloned repo\n\n        \"\"\"\n", "input": "", "output": "        url = 'git@gist.github.com:/{}'.format(id)\n\n        if name is None:\n            os.system('git clone {}'.format(url))\n        else:\n            os.system('git clone {} {}'.format(url, name))", "category": "Python"}, {"instruction": "def identify_core(core):\n    \"\"\"Identify the polynomial argument.\"\"\"\n", "input": "", "output": "    for datatype, identifier in {\n            int: _identify_scaler,\n            numpy.int8: _identify_scaler,\n            numpy.int16: _identify_scaler,\n            numpy.int32: _identify_scaler,\n            numpy.int64: _identify_scaler,\n            float: _identify_scaler,\n            numpy.float16: _identify_scaler,\n            numpy.float32: _identify_scaler,\n            numpy.float64: _identify_scaler,\n            chaospy.poly.base.Poly: _identify_poly,\n            dict: _identify_dict,\n            numpy.ndarray: _identify_iterable,\n            list: _identify_iterable,\n            tuple: _identify_iterable,\n    }.items():\n        if isinstance(core, datatype):\n            return identifier(core)\n\n    raise TypeError(\n        \"Poly arg: 'core' is not a valid type \" + repr(core))", "category": "Python"}, {"instruction": "def add_policy(self, name, policy_type, cooldown, change=None,\n            is_percent=False, desired_capacity=None, args=None):\n        \"\"\"\n        Adds a policy with the given values to this scaling group. The\n        'change' parameter is treated as an absolute amount, unless\n        'is_percent' is True, in which case it is treated as a percentage.\n        \"\"\"\n", "input": "", "output": "        return self.manager.add_policy(self, name, policy_type, cooldown,\n                change=change, is_percent=is_percent,\n                desired_capacity=desired_capacity, args=args)", "category": "Python"}, {"instruction": "def type_str(self, short=False):\n        \"\"\"\n        Returns the type of the attribute as string.\n\n        :return: the type\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        if short:\n            return javabridge.static_call(\n                \"weka/core/Attribute\", \"typeToStringShort\", \"(Lweka/core/Attribute;)Ljava/lang/String;\",\n                self.jobject)\n        else:\n            return javabridge.static_call(\n                \"weka/core/Attribute\", \"typeToString\", \"(Lweka/core/Attribute;)Ljava/lang/String;\",\n                self.jobject)", "category": "Python"}, {"instruction": "def medial_wall_to_nan(in_file, subjects_dir, target_subject, newpath=None):\n    \"\"\" Convert values on medial wall to NaNs\n    \"\"\"\n", "input": "", "output": "    import nibabel as nb\n    import numpy as np\n    import os\n\n    fn = os.path.basename(in_file)\n    if not target_subject.startswith('fs'):\n        return in_file\n\n    cortex = nb.freesurfer.read_label(os.path.join(\n        subjects_dir, target_subject, 'label', '{}.cortex.label'.format(fn[:2])))\n    func = nb.load(in_file)\n    medial = np.delete(np.arange(len(func.darrays[0].data)), cortex)\n    for darray in func.darrays:\n        darray.data[medial] = np.nan\n\n    out_file = os.path.join(newpath or os.getcwd(), fn)\n    func.to_filename(out_file)\n    return out_file", "category": "Python"}, {"instruction": "def findNextSection(self, scope, name):\n        \"\"\" Starts with given par (scope+name) and looks further down the list\n        of parameters until one of a different non-null scope is found.  Upon\n        success, returns the (scope, name) tuple, otherwise (None, None). \"\"\"\n", "input": "", "output": "        # first find index of starting point\n        plist = self._taskParsObj.getParList()\n        start = 0\n        for i in range(len(plist)):\n            if scope == plist[i].scope and name == plist[i].name:\n                start = i\n                break\n        else:\n            print('WARNING: could not find starting par: '+scope+'.'+name)\n            return (None, None)\n\n        # now find first different (non-null) scope in a par, after start\n        for i in range(start, len(plist)):\n            if len(plist[i].scope) > 0 and plist[i].scope != scope:\n                return (plist[i].scope, plist[i].name)\n        # else didn't find it\n        return (None, None)", "category": "Python"}, {"instruction": "def is_model_admin_subclass(node):\n    \"\"\"Checks that node is derivative of ModelAdmin class.\"\"\"\n", "input": "", "output": "    if node.name[-5:] != 'Admin' or isinstance(node.parent, ClassDef):\n        return False\n\n    return node_is_subclass(node, 'django.contrib.admin.options.ModelAdmin')", "category": "Python"}, {"instruction": "def serialize(self):\n        \"\"\"This function serialize into a simple dict object.\n        It is used when transferring data to other daemons over the network (http)\n\n        Here we directly return all attributes\n\n        :return: json representation of a Timerange\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        return {\"hstart\": self.hstart, \"mstart\": self.mstart,\n                \"hend\": self.hend, \"mend\": self.mend,\n                \"is_valid\": self.is_valid}", "category": "Python"}, {"instruction": "def init_defaults(self):\n        \"\"\"\n        Sets the name of the table to the passed in table value\n        \"\"\"\n", "input": "", "output": "        super(SimpleTable, self).init_defaults()\n        self.name = self.table", "category": "Python"}, {"instruction": "def modis1kmto500m(lons1km, lats1km, cores=1):\n    \"\"\"Getting 500m geolocation for modis from 1km tiepoints.\n\n    http://www.icare.univ-lille1.fr/tutorials/MODIS_geolocation\n    \"\"\"\n", "input": "", "output": "    if cores > 1:\n        return _multi(modis1kmto500m, lons1km, lats1km, 10, cores)\n\n    cols1km = np.arange(1354)\n    cols500m = np.arange(1354 * 2) / 2.0\n    lines = lons1km.shape[0]\n    rows1km = np.arange(lines)\n    rows500m = (np.arange(lines * 2) - 0.5) / 2.\n\n    along_track_order = 1\n    cross_track_order = 3\n\n    satint = SatelliteInterpolator((lons1km, lats1km),\n                                   (rows1km, cols1km),\n                                   (rows500m, cols500m),\n                                   along_track_order,\n                                   cross_track_order,\n                                   chunk_size=20)\n    satint.fill_borders(\"y\", \"x\")\n    lons500m, lats500m = satint.interpolate()\n    return lons500m, lats500m", "category": "Python"}, {"instruction": "def from_memdb(buffer):\n        \"\"\"Creates a sourcemap view from MemDB bytes.\"\"\"\n", "input": "", "output": "        buffer = to_bytes(buffer)\n        return View._from_ptr(rustcall(\n            _lib.lsm_view_from_memdb,\n            buffer, len(buffer)))", "category": "Python"}, {"instruction": "def basename(path):\n    \"\"\"Rightmost part of path after separator.\"\"\"\n", "input": "", "output": "    base_path = path.strip(SEP)\n    sep_ind = base_path.rfind(SEP)\n    if sep_ind < 0:\n        return path\n\n    return base_path[sep_ind + 1:]", "category": "Python"}, {"instruction": "def stack_lines(indices):\n    \"\"\"\n    Stack a list of values that represent a polyline into\n    individual line segments with duplicated consecutive values.\n\n    Parameters\n    ----------\n    indices: sequence of items\n\n    Returns\n    ---------\n    stacked: (n,2) set of items\n\n    In [1]: trimesh.util.stack_lines([0,1,2])\n    Out[1]:\n    array([[0, 1],\n           [1, 2]])\n\n    In [2]: trimesh.util.stack_lines([0,1,2,4,5])\n    Out[2]:\n    array([[0, 1],\n           [1, 2],\n           [2, 4],\n           [4, 5]])\n\n    In [3]: trimesh.util.stack_lines([[0,0],[1,1],[2,2], [3,3]])\n    Out[3]:\n    array([[0, 0],\n           [1, 1],\n           [1, 1],\n           [2, 2],\n           [2, 2],\n           [3, 3]])\n\n    \"\"\"\n", "input": "", "output": "    indices = np.asanyarray(indices)\n    if is_sequence(indices[0]):\n        shape = (-1, len(indices[0]))\n    else:\n        shape = (-1, 2)\n    return np.column_stack((indices[:-1],\n                            indices[1:])).reshape(shape)", "category": "Python"}, {"instruction": "def requestPty(self, term=None, rows=0, cols=0, xpixel=0, ypixel=0, modes=''):\n        \"\"\"Request allocation of a pseudo-terminal for a channel\n\n        @param term: TERM environment variable value (e.g., vt100)\n        @param columns: terminal width, characters (e.g., 80)\n        @param rows: terminal height, rows (e.g., 24)\n        @param width: terminal width, pixels (e.g., 640)\n        @param height: terminal height, pixels (e.g., 480)\n        @param modes: encoded terminal modes\n\n        The dimension parameters are only informational.\n        Zero dimension parameters are ignored. The columns/rows dimensions\n        override the pixel dimensions (when nonzero). Pixel dimensions refer\n        to the drawable area of the window.\n        \"\"\"\n", "input": "", "output": "        #TODO: Needs testing!\n        term = term or os.environ.get('TERM', '')\n        data = packRequest_pty_req(term, (rows, cols, xpixel, ypixel), modes)\n        return self.sendRequest('pty-req', data)", "category": "Python"}, {"instruction": "def load_adjusted_array(self, domain, columns, dates, sids, mask):\n        \"\"\"\n        Load data from our stored baseline.\n        \"\"\"\n", "input": "", "output": "        if len(columns) != 1:\n            raise ValueError(\n                \"Can't load multiple columns with DataFrameLoader\"\n            )\n\n        column = columns[0]\n        self._validate_input_column(column)\n\n        date_indexer = self.dates.get_indexer(dates)\n        assets_indexer = self.assets.get_indexer(sids)\n\n        # Boolean arrays with True on matched entries\n        good_dates = (date_indexer != -1)\n        good_assets = (assets_indexer != -1)\n\n        data = self.baseline[ix_(date_indexer, assets_indexer)]\n        mask = (good_assets & as_column(good_dates)) & mask\n\n        # Mask out requested columns/rows that didn't match.\n        data[~mask] = column.missing_value\n\n        return {\n            column: AdjustedArray(\n                # Pull out requested columns/rows from our baseline data.\n                data=data,\n                adjustments=self.format_adjustments(dates, sids),\n                missing_value=column.missing_value,\n            ),\n        }", "category": "Python"}, {"instruction": "def use_gae_thread():\n    \"\"\"Makes ``Client``s started after this use the appengine thread class.\"\"\"\n", "input": "", "output": "    global _THREAD_CLASS  # pylint: disable=global-statement\n    try:\n        from google.appengine.api.background_thread import background_thread\n        _THREAD_CLASS = background_thread.BackgroundThread\n    except ImportError:\n        _logger.error(\n            u'Could not install appengine background threads!'\n            u' Please install the python AppEngine SDK and use this from there')", "category": "Python"}, {"instruction": "def td_type():\n    '''get type of the tomodir (complex or dc and whether fpi)\n    '''\n", "input": "", "output": "    cfg = np.genfromtxt('exe/crtomo.cfg',\n                        skip_header=15,\n                        dtype='str',\n                        usecols=([0]))\n    is_complex = False\n    if cfg[0] == 'F':\n        is_complex = True\n    is_fpi = False\n    if cfg[2] == 'T':\n        is_fpi = True\n\n    return is_complex, is_fpi", "category": "Python"}, {"instruction": "def ystep(self):\n        r\"\"\"Minimise Augmented Lagrangian with respect to\n        :math:`\\mathbf{y}`.\"\"\"\n", "input": "", "output": "\n        self.Y = np.asarray(sp.prox_l1l2(\n            self.AX + self.U, (self.lmbda / self.rho) * self.wl1,\n            self.mu / self.rho, axis=-1), dtype=self.dtype)\n        GenericBPDN.ystep(self)", "category": "Python"}, {"instruction": "def auto_register_search_models():\n    \"\"\"Auto register all search models\"\"\"\n", "input": "", "output": "    for config in models_config.get_all_configs():\n        if config.disable_search_index:\n            continue\n\n        search.register(\n            config.model.objects.get_queryset(),\n            ModelSearchAdapter,\n            fields=config.search_fields,\n            exclude=config.search_exclude_fields,\n        )", "category": "Python"}, {"instruction": "def convert_time_string(date_str):\n    \"\"\" Change a date string from the format 2018-08-15T23:55:17 into a datetime object \"\"\"\n", "input": "", "output": "    dt, _, _ = date_str.partition(\".\")\n    dt = datetime.strptime(dt, \"%Y-%m-%dT%H:%M:%S\")\n    return dt", "category": "Python"}, {"instruction": "def split_docstring(docstring):\n    \"\"\"\n    Separates the method's description and paramter's\n\n    :return: Return description string and list of fields strings\n    \"\"\"\n", "input": "", "output": "    docstring_list = [line.strip() for line in docstring.splitlines()]\n    description_list = list(\n        takewhile(lambda line: not (line.startswith(':') or\n                                    line.startswith('@inherit')), docstring_list))\n    description = ' '.join(description_list).strip()\n    first_field_line_number = len(description_list)\n\n    fields = []\n    if first_field_line_number >= len(docstring_list):\n        return description, fields  # only description, without any field\n    last_field_lines = [docstring_list[first_field_line_number]]\n\n    for line in docstring_list[first_field_line_number + 1:]:\n        if line.strip().startswith(':') or line.strip().startswith('@inherit'):\n            fields.append(' '.join(last_field_lines))\n            last_field_lines = [line]\n        else:\n            last_field_lines.append(line)\n\n    fields.append(' '.join(last_field_lines))\n    return description, fields", "category": "Python"}, {"instruction": "def _unsetLearningMode(self):\n    \"\"\"\n    Unsets the learning mode, to start inference.\n    \"\"\"\n", "input": "", "output": "\n    for region in self.L4Regions:\n      region.setParameter(\"learn\", False)\n    for region in self.L2Regions:\n      region.setParameter(\"learningMode\", False)", "category": "Python"}, {"instruction": "def responses_incremental(self, start_time):\n        \"\"\"\n        Retrieve NPS Responses incremental\n\n        :param start_time: time to retrieve events from.\n        \"\"\"\n", "input": "", "output": "        return self._query_zendesk(self.endpoint.responses_incremental, 'responses', start_time=start_time)", "category": "Python"}, {"instruction": "def lstat(self, path):\n        \"\"\"\n        Retrieve information about a file on the remote system, without\n        following symbolic links (shortcuts).  This otherwise behaves exactly\n        the same as L{stat}.\n\n        @param path: the filename to stat\n        @type path: str\n        @return: an object containing attributes about the given file\n        @rtype: SFTPAttributes\n        \"\"\"\n", "input": "", "output": "        path = self._adjust_cwd(path)\n        self._log(DEBUG, 'lstat(%r)' % path)\n        t, msg = self._request(CMD_LSTAT, path)\n        if t != CMD_ATTRS:\n            raise SFTPError('Expected attributes')\n        return SFTPAttributes._from_msg(msg)", "category": "Python"}, {"instruction": "def _system_parameters(**kwargs):\n    \"\"\"\n    Returns system keyword arguments removing Nones.\n\n    Args:\n        kwargs: system keyword arguments.\n\n    Returns:\n        dict: system keyword arguments.\n    \"\"\"\n", "input": "", "output": "    return {key: value for key, value in kwargs.items()\n            if (value is not None or value == {})}", "category": "Python"}, {"instruction": "def append_string(self, value):\n        \"\"\"Appends a length-prefixed string to our buffer, with the\n        length varint-encoded.\n        \"\"\"\n", "input": "", "output": "        self._stream.append_var_uint32(len(value))\n        self._stream.append_raw_bytes(value)", "category": "Python"}, {"instruction": "def list_npm_modules(collector, no_print=False):\n    \"\"\"List the npm modules that get installed in a docker image for the react server\"\"\"\n", "input": "", "output": "    default = ReactServer().default_npm_deps()\n    for _, module in sorted(collector.configuration[\"__active_modules__\"].items()):\n        default.update(module.npm_deps())\n\n    if not no_print:\n        print(json.dumps(default, indent=4, sort_keys=True))\n    return default", "category": "Python"}, {"instruction": "async def channel_cmd(self, ctx, *, channel : discord.Channel = None):\n        \"\"\"Ignores a specific channel from being processed.\n\n        If no channel is specified, the current channel is ignored.\n        If a channel is ignored then the bot does not process commands in that\n        channel until it is unignored.\n        \"\"\"\n", "input": "", "output": "\n        if channel is None:\n            channel = ctx.message.channel\n\n        ignored = self.config.get('ignored', [])\n        if channel.id in ignored:\n            await self.bot.responses.failure(message='That channel is already ignored.')\n            return\n\n        ignored.append(channel.id)\n        await self.config.put('ignored', ignored)\n        await self.bot.responses.success(message='Channel <#{}> will be ignored.'.format(channel.id))", "category": "Python"}, {"instruction": "def focusInEvent(self, event):\n        \"\"\"Reimplement Qt method to send focus change notification\"\"\"\n", "input": "", "output": "        self.focus_changed.emit()\n        return super(ControlWidget, self).focusInEvent(event)", "category": "Python"}, {"instruction": "def get_battery_state():\n        \"\"\"\n        TODO\n        @return: Tuple (energy_full, energy_now, power_now)\n        \"\"\"\n", "input": "", "output": "        energy_now = float(100.0)\n        power_now = float(100.0)\n        energy_full = float(100.0)\n        return energy_full, energy_now, power_now", "category": "Python"}, {"instruction": "def get_contacts_of_client_per_page(self, client_id, per_page=1000, page=1):\n        \"\"\"\n        Get contacts of client per page\n\n        :param client_id: the client id\n        :param per_page: How many objects per page. Default: 1000\n        :param page: Which page. Default: 1\n        :return: list\n        \"\"\"\n", "input": "", "output": "        return self._get_resource_per_page(\n            resource=CONTACTS,\n            per_page=per_page,\n            page=page,\n            params={'client_id': client_id},\n        )", "category": "Python"}, {"instruction": "def translate(client, event, channel, nick, rest):\n\t\"\"\"\n\tTranslate a phrase using Google Translate. First argument should be\n\tthe language[s]. It is a 2 letter abbreviation. It will auto detect\n\tthe orig lang if you only give one; or two languages joined by a |,\n\tfor example 'en|de' to trans from English to German. Follow this by\n\tthe phrase you want to translate.\n\t\"\"\"\n", "input": "", "output": "\ttry:\n\t\tset_key()\n\texcept Exception:\n\t\treturn (\n\t\t\t\"No API key configured. Google charges for translation. \"\n\t\t\t\"Please register for an API key at \"\n\t\t\t\"https://code.google.com/apis/console/?api=translate&promo=tr \"\n\t\t\t\"and set the 'Google API key' config variable to a valid key\")\n\trest = rest.strip()\n\tlangpair, _, rest = rest.partition(' ')\n\tsource_lang, _, target_lang = langpair.rpartition('|')\n\ttry:\n\t\treturn google.translate(rest.encode('utf-8'), target_lang, source_lang)\n\texcept Exception:\n\t\tlog.exception(\"Error occurred in translate\")\n\t\ttmpl = (\n\t\t\t\"An error occurred. \"\n\t\t\t\"Are you sure {langpair} is a valid language?\")\n\t\treturn tmpl.format(**vars())", "category": "Python"}, {"instruction": "def open(self):\n        \"\"\"\n        Open the Sender using the supplied conneciton.\n        If the handler has previously been redirected, the redirect\n        context will be used to create a new handler before opening it.\n\n        :param connection: The underlying client shared connection.\n        :type: connection: ~uamqp.connection.Connection\n        \"\"\"\n", "input": "", "output": "        self.running = True\n        if self.redirected:\n            self.target = self.redirected.address\n            self._handler = SendClient(\n                self.target,\n                auth=self.client.get_auth(),\n                debug=self.client.debug,\n                msg_timeout=self.timeout,\n                error_policy=self.retry_policy,\n                keep_alive_interval=self.keep_alive,\n                client_name=self.name,\n                properties=self.client.create_properties())\n        self._handler.open()\n        while not self._handler.client_ready():\n            time.sleep(0.05)", "category": "Python"}, {"instruction": "def insert_variables(\n            self, device2variable, exchangespec, selections) -> None:\n        \"\"\"Determine the relevant target or base variables (as defined by\n        the given |ExchangeSpecification| object ) handled by the given\n        |Selections| object and insert them into the given `device2variable`\n        dictionary.\"\"\"\n", "input": "", "output": "        if self.targetspecs.master in ('node', 'nodes'):\n            for node in selections.nodes:\n                variable = self._query_nodevariable(node, exchangespec)\n                device2variable[node] = variable\n        else:\n            for element in self._iter_relevantelements(selections):\n                variable = self._query_elementvariable(element, exchangespec)\n                device2variable[element] = variable", "category": "Python"}, {"instruction": "def from_string(cls, s):\n        \"\"\"Create an istance from string s containing a YAML dictionary.\"\"\"\n", "input": "", "output": "        stream = cStringIO(s)\n        stream.seek(0)\n        return cls(**yaml.safe_load(stream))", "category": "Python"}, {"instruction": "def update_forum_redirects_counter(sender, forum, user, request, response, **kwargs):\n    \"\"\" Handles the update of the link redirects counter associated with link forums. \"\"\"\n", "input": "", "output": "    if forum.is_link and forum.link_redirects:\n        forum.link_redirects_count = F('link_redirects_count') + 1\n        forum.save()", "category": "Python"}, {"instruction": "def make_app(global_conf, full_stack=True, **app_conf):\n    \"\"\"\n    Set depotexample up with the settings found in the PasteDeploy configuration\n    file used.\n    \n    :param global_conf: The global settings for depotexample (those\n        defined under the ``[DEFAULT]`` section).\n    :type global_conf: dict\n    :param full_stack: Should the whole TG2 stack be set up?\n    :type full_stack: str or bool\n    :return: The depotexample application with all the relevant middleware\n        loaded.\n    \n    This is the PasteDeploy factory for the depotexample application.\n    \n    ``app_conf`` contains all the application-specific settings (those defined\n    under ``[app:main]``.\n    \n   \n    \"\"\"\n", "input": "", "output": "    app = make_base_app(global_conf, full_stack=True, **app_conf)\n    \n    # Wrap your base TurboGears 2 application with custom middleware here\n    from depot.manager import DepotManager\n    app = DepotManager.make_middleware(app)\n\n    return app", "category": "Python"}, {"instruction": "def get_observations(params: Dict) -> Dict[str, Any]:\n    \"\"\"Search observations, see: http://api.inaturalist.org/v1/docs/#!/Observations/get_observations.\n\n    Returns the parsed JSON returned by iNaturalist (observations in r['results'], a list of dicts)\n    \"\"\"\n", "input": "", "output": "\n    r = make_inaturalist_api_get_call('observations', params=params)\n    return r.json()", "category": "Python"}, {"instruction": "def daArray(arry, dtype=numpy.float):\n    \"\"\"\n    Array constructor for numpy distributed array\n    @param arry numpy-like array\n    \"\"\"\n", "input": "", "output": "    a = numpy.array(arry, dtype)\n    res = DistArray(a.shape, a.dtype)\n    res[:] = a\n    return res", "category": "Python"}, {"instruction": "def update(self, collection, selector, modifier, callback=None):\n        \"\"\"Insert an item into a collection\n\n        Arguments:\n        collection - the collection to be modified\n        selector - specifies which documents to modify\n        modifier - Specifies how to modify the documents\n\n        Keyword Arguments:\n        callback - Optional. If present, called with an error object as the first argument and,\n        if no error, the number of affected documents as the second.\"\"\"\n", "input": "", "output": "        self.call(\"/\" + collection + \"/update\", [selector, modifier], callback=callback)", "category": "Python"}, {"instruction": "def calculate_batch_normalization_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N, C] ---> [N, C]\n        2. [N, C, H, W] ---> [N, C, H, W]\n\n    This operator just uses the operator input shape as its output shape.\n    '''\n", "input": "", "output": "    check_input_and_output_numbers(operator, input_count_range=1, output_count_range=1)\n    check_input_and_output_types(operator, good_input_types=[FloatTensorType])\n\n    input_shape = operator.inputs[0].type.shape\n    if len(input_shape) not in [2, 4]:\n        raise RuntimeError('Input must be a 2-D or a 4-D tensor')\n\n    operator.outputs[0].type.shape = copy.deepcopy(operator.inputs[0].type.shape)", "category": "Python"}, {"instruction": "def download(self, *ids):\n        \"\"\"\n        Downloads the subtitles with the given ids.\n        :param ids: The subtitles to download\n        :return: Result instances\n        :raises NotOKException\n        \"\"\"\n", "input": "", "output": "        bundles = sublists_of(ids, 20)  # 20 files at once is an API restriction\n\n        for bundle in bundles:\n            download_response = self._rpc.DownloadSubtitles(self._token, bundle)\n\n            assert_status(download_response)\n\n            download_data = download_response.get('data')\n\n            for item in download_data:\n                subtitle_id = item['idsubtitlefile']\n                subtitle_data = item['data']\n\n                decompressed = decompress(subtitle_data)\n\n                yield Result(subtitle_id, decompressed)", "category": "Python"}, {"instruction": "def deactivate_license(key_name=None):\n    '''\n    Deactivates an installed license.\n    Required version 7.0.0 or greater.\n\n    key_name(str): The file name of the license key installed.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' panos.deactivate_license key_name=License_File_Name.key\n\n    '''\n", "input": "", "output": "\n    _required_version = '7.0.0'\n    if not __proxy__['panos.is_required_version'](_required_version):\n        return False, 'The panos device requires version {0} or greater for this command.'.format(_required_version)\n\n    if not key_name:\n        return False, 'You must specify a key_name.'\n    else:\n        query = {'type': 'op', 'cmd': '<request><license><deactivate><key><features><member>{0}</member></features>'\n                                      '</key></deactivate></license></request>'.format(key_name)}\n\n    return __proxy__['panos.call'](query)", "category": "Python"}, {"instruction": "def initialise_shopify_session():\n    \"\"\"\n    Initialise the Shopify session with the Shopify App's API credentials.\n    \"\"\"\n", "input": "", "output": "    if not settings.SHOPIFY_APP_API_KEY or not settings.SHOPIFY_APP_API_SECRET:\n        raise ImproperlyConfigured(\"SHOPIFY_APP_API_KEY and SHOPIFY_APP_API_SECRET must be set in settings\")\n    shopify.Session.setup(api_key=settings.SHOPIFY_APP_API_KEY, secret=settings.SHOPIFY_APP_API_SECRET)", "category": "Python"}, {"instruction": "def mtf_resnet_single():\n  \"\"\"Small single parameters.\"\"\"\n", "input": "", "output": "  hparams = mtf_resnet_tiny()\n  hparams.mesh_shape = \"\"\n  hparams.layout = \"\"\n  hparams.hidden_size = 32\n  hparams.filter_size = 32\n  hparams.batch_size = 1\n  hparams.num_encoder_layers = 1\n  hparams.num_layers = 1\n  hparams.block_length = 16\n  return hparams", "category": "Python"}, {"instruction": "def zoomset_cb(self, setting, value, chviewer, info):\n        \"\"\"This callback is called when a channel window is zoomed.\n        \"\"\"\n", "input": "", "output": "        return self.zoomset(chviewer, info.chinfo)", "category": "Python"}, {"instruction": "def BX(self, params):\n        \"\"\"\n        BX Rj\n\n        Jump to the address in the Link Register\n        \"\"\"\n", "input": "", "output": "        Rj = self.get_one_parameter(self.ONE_PARAMETER, params)\n\n        self.check_arguments(LR_or_general_purpose_registers=(Rj,))\n\n        def BX_func():\n            self.register['PC'] = self.register[Rj]\n\n        return BX_func", "category": "Python"}, {"instruction": "def _compute_magnitude_terms(self, rup, coeffs):\n        \"\"\"\n        First three terms of equation (8) on p. 203:\n\n        ``c1 + c2*(M - 6) + c3*(M - 6)**2``\n        \"\"\"\n", "input": "", "output": "\n        adj_mag = rup.mag - self.CONSTS['ref_mag']\n        return coeffs['c1'] + coeffs['c2']*adj_mag + coeffs['c3']*adj_mag**2", "category": "Python"}, {"instruction": "async def on_raw_433(self, message):\n        \"\"\" Nickname in use. \"\"\"\n", "input": "", "output": "        if not self.registered:\n            self._registration_attempts += 1\n            # Attempt to set new nickname.\n            if self._attempt_nicknames:\n                await self.set_nickname(self._attempt_nicknames.pop(0))\n            else:\n                await self.set_nickname(\n                    self._nicknames[0] + '_' * (self._registration_attempts - len(self._nicknames)))", "category": "Python"}, {"instruction": "def request_syncmodule(blink, network):\n    \"\"\"\n    Request sync module info.\n\n    :param blink: Blink instance.\n    :param network: Sync module network id.\n    \"\"\"\n", "input": "", "output": "    url = \"{}/network/{}/syncmodules\".format(blink.urls.base_url, network)\n    return http_get(blink, url)", "category": "Python"}, {"instruction": "def __get_fault(self, replyroot):\n        \"\"\"\n        Extract fault information from a SOAP reply.\n\n        Returns an I{unmarshalled} fault L{Object} or None in case the given\n        XML document does not contain a SOAP <Fault> element.\n\n        @param replyroot: A SOAP reply message root XML element or None.\n        @type replyroot: L{Element}|I{None}\n        @return: A fault object.\n        @rtype: L{Object}\n\n        \"\"\"\n", "input": "", "output": "        envns = suds.bindings.binding.envns\n        soapenv = replyroot and replyroot.getChild(\"Envelope\", envns)\n        soapbody = soapenv and soapenv.getChild(\"Body\", envns)\n        fault = soapbody and soapbody.getChild(\"Fault\", envns)\n        return fault is not None and UmxBasic().process(fault)", "category": "Python"}, {"instruction": "def global_custom_theme(request):\n    \"\"\"Add custom theme javascript and css.\"\"\"\n", "input": "", "output": "    today = datetime.datetime.now().date()\n    theme = {}\n\n    if today.month == 3 and (14 <= today.day <= 16):\n        theme = {\"css\": \"themes/piday/piday.css\"}\n\n    return {\"theme\": theme}", "category": "Python"}, {"instruction": "async def verify_signature(self, msg: bytes, signature: bytes) -> bool:\n        \"\"\"\n        Verification the signature of a msg\n        :param msg:\n        :param signature:\n        :return: bool\n        \"\"\"\n", "input": "", "output": "        if not hasattr(Connection.verify_signature, \"cb\"):\n            self.logger.debug(\"vcx_connection_verify_signature: Creating callback\")\n            Connection.verify_signature.cb = create_cb(CFUNCTYPE(None, c_uint32, c_uint32, c_bool))\n\n        c_connection_handle = c_uint32(self.handle)\n        c_msg_len = c_uint32(len(msg))\n        c_signature_len = c_uint32(len(signature))\n\n        result = await do_call('vcx_connection_verify_signature',\n                               c_connection_handle,\n                               msg,\n                               c_msg_len,\n                               signature,\n                               c_signature_len,\n                               Connection.verify_signature.cb)\n\n        self.logger.debug(\"vcx_connection_verify_signature completed\")\n        return result", "category": "Python"}, {"instruction": "def visit_Name(self, node: ast.Name) -> None:\n        \"\"\"\n        Resolve the name from the variable look-up and the built-ins.\n\n        Due to possible branching (e.g., If-expressions), some nodes might lack the recomputed values. These nodes\n        are ignored.\n        \"\"\"\n", "input": "", "output": "        if node in self._recomputed_values:\n            value = self._recomputed_values[node]\n\n            # Check if it is a non-built-in\n            is_builtin = True\n            for lookup in self._variable_lookup:\n                if node.id in lookup:\n                    is_builtin = False\n                    break\n\n            if not is_builtin and _representable(value=value):\n                text = self._atok.get_text(node)\n                self.reprs[text] = value\n\n        self.generic_visit(node=node)", "category": "Python"}, {"instruction": "def _keplerian_circular_to_keplerian(cls, coord, center):\n        \"\"\"Conversion from Keplerian near-circular elements to Mean Keplerian\n        \"\"\"\n", "input": "", "output": "        a, ex, ey, i, \u03a9, u = coord\n\n        e = sqrt(ex ** 2 + ey ** 2)\n        \u03c9 = arctan2(ey / e, ex / e)\n        \u03bd = u - \u03c9\n\n        return np.array([a, e, i, \u03a9, \u03c9, \u03bd], dtype=float)", "category": "Python"}, {"instruction": "def do_dir(self, args, unknown):\n        \"\"\"List contents of current directory.\"\"\"\n", "input": "", "output": "        # No arguments for this command\n        if unknown:\n            self.perror(\"dir does not take any positional arguments:\", traceback_war=False)\n            self.do_help('dir')\n            self._last_result = cmd2.CommandResult('', 'Bad arguments')\n            return\n\n        # Get the contents as a list\n        contents = os.listdir(self.cwd)\n\n        fmt = '{} '\n        if args.long:\n            fmt = '{}\\n'\n        for f in contents:\n            self.stdout.write(fmt.format(f))\n        self.stdout.write('\\n')\n\n        self._last_result = cmd2.CommandResult(data=contents)", "category": "Python"}, {"instruction": "def flexifunction_command_send(self, target_system, target_component, command_type, force_mavlink1=False):\n                '''\n                Acknowldge sucess or failure of a flexifunction command\n\n                target_system             : System ID (uint8_t)\n                target_component          : Component ID (uint8_t)\n                command_type              : Flexifunction command type (uint8_t)\n\n                '''\n", "input": "", "output": "                return self.send(self.flexifunction_command_encode(target_system, target_component, command_type), force_mavlink1=force_mavlink1)", "category": "Python"}, {"instruction": "def build_ml_raxml(alignment, outfile, work_dir=\".\", **kwargs):\n    \"\"\"\n    build maximum likelihood tree of DNA seqs with RAxML\n    \"\"\"\n", "input": "", "output": "    work_dir = op.join(work_dir, \"work\")\n    mkdir(work_dir)\n    phy_file = op.join(work_dir, \"aln.phy\")\n    AlignIO.write(alignment, file(phy_file, \"w\"), \"phylip-relaxed\")\n\n    raxml_work = op.abspath(op.join(op.dirname(phy_file), \"raxml_work\"))\n    mkdir(raxml_work)\n    raxml_cl = RaxmlCommandline(cmd=RAXML_BIN(\"raxmlHPC\"), \\\n        sequences=phy_file, algorithm=\"a\", model=\"GTRGAMMA\", \\\n        parsimony_seed=12345, rapid_bootstrap_seed=12345, \\\n        num_replicates=100, name=\"aln\", \\\n        working_dir=raxml_work, **kwargs)\n\n    logging.debug(\"Building ML tree using RAxML: %s\" % raxml_cl)\n    stdout, stderr = raxml_cl()\n\n    tree_file = \"{0}/RAxML_bipartitions.aln\".format(raxml_work)\n    if not op.exists(tree_file):\n        print(\"***RAxML failed.\", file=sys.stderr)\n        sh(\"rm -rf %s\" % raxml_work, log=False)\n        return None\n    sh(\"cp {0} {1}\".format(tree_file, outfile), log=False)\n\n    logging.debug(\"ML tree printed to %s\" % outfile)\n    sh(\"rm -rf %s\" % raxml_work)\n\n    return outfile, phy_file", "category": "Python"}, {"instruction": "def _create_code_edit(self, mimetype, *args, **kwargs):\n        \"\"\"\n        Create a code edit instance based on the mimetype of the file to\n        open/create.\n\n        :type mimetype: mime type\n        :param args: Positional arguments that must be forwarded to the editor\n            widget constructor.\n        :param kwargs: Keyworded arguments that must be forwarded to the editor\n            widget constructor.\n        :return: Code editor widget instance.\n        \"\"\"\n", "input": "", "output": "        if mimetype in self.editors.keys():\n            return self.editors[mimetype](\n                *args, parent=self.main_tab_widget, **kwargs)\n        editor = self.fallback_editor(*args, parent=self.main_tab_widget,\n                                      **kwargs)\n        return editor", "category": "Python"}, {"instruction": "def _validate_configuration_type(self, configuration_type):\n        \"\"\"Validate configuration type\n\n        :param configuration_type: configuration_type, should be Startup or Running\n        :raise Exception:\n        \"\"\"\n", "input": "", "output": "\n        if configuration_type.lower() != 'running' and configuration_type.lower() != 'startup':\n            raise Exception(self.__class__.__name__, 'Configuration Type is invalid. Should be startup or running')", "category": "Python"}, {"instruction": "def parse(self, element):\n        \"\"\"Parses the contents of the specified XML element using template info.\n\n        :arg element: the XML element from the input file being converted.\n        \"\"\"\n", "input": "", "output": "        result = []\n        if element.text is not None and element.tag == self.identifier:\n            l, k = (0, 0)\n            raw = element.text.split()\n            while k < len(self.values):\n                dtype = self.dtype[k]\n                if isinstance(self.values[k], int):\n                    for i in range(self.values[k]):\n                        result.append(self._caster[dtype](raw[i + l]))\n                    l += self.values[k]\n                    k += 1\n                else:\n                    #This is a variable argument line, just use up the rest\n                    #of them as the type of the current line\n                    rest = [ self._caster[dtype](val) for val in raw[l::] ]\n                    result.extend(rest)\n                    break\n        else:\n            msg.warn(\"no results for parsing {} using line {}\".format(element.tag, self.identifier))\n\n        return result", "category": "Python"}, {"instruction": "def fmt_sz(intval):\n    \"\"\" Format a byte sized value.\n    \"\"\"\n", "input": "", "output": "    try:\n        return fmt.human_size(intval)\n    except (ValueError, TypeError):\n        return \"N/A\".rjust(len(fmt.human_size(0)))", "category": "Python"}, {"instruction": "def on_task_status(self, task):\n        \"\"\"Ignore not processing error in interactive mode\"\"\"\n", "input": "", "output": "        if not self.interactive:\n            super(OneScheduler, self).on_task_status(task)\n\n        try:\n            procesok = task['track']['process']['ok']\n        except KeyError as e:\n            logger.error(\"Bad status pack: %s\", e)\n            return None\n\n        if procesok:\n            ret = self.on_task_done(task)\n        else:\n            ret = self.on_task_failed(task)\n        if task['track']['fetch'].get('time'):\n            self._cnt['5m_time'].event((task['project'], 'fetch_time'),\n                                       task['track']['fetch']['time'])\n        if task['track']['process'].get('time'):\n            self._cnt['5m_time'].event((task['project'], 'process_time'),\n                                       task['track']['process'].get('time'))\n        self.projects[task['project']].active_tasks.appendleft((time.time(), task))\n        return ret", "category": "Python"}, {"instruction": "def order_items(self, *args, **kwargs):\n        \"\"\"Pass through to provider methods.\"\"\"\n", "input": "", "output": "        try:\n            self._get_provider_session('assessment_basic_authoring_session').order_items(*args, **kwargs)\n        except InvalidArgument:\n            self._get_sub_package_provider_session(\n                'assessment_authoring', 'assessment_part_item_design_session').order_items(*args, **kwargs)", "category": "Python"}, {"instruction": "def get_frames(self):\n        \"\"\"Rectify and return current frames from cameras.\"\"\"\n", "input": "", "output": "        frames = super(CalibratedPair, self).get_frames()\n        return self.calibration.rectify(frames)", "category": "Python"}, {"instruction": "def _get_full_key(self, key):\n        \"\"\"Returns full key even if table is omitted\"\"\"\n", "input": "", "output": "\n        length = len(key)\n\n        if length == 3:\n            return key\n\n        elif length == 2:\n            row, col = key\n            tab = self.grid.current_table\n            return row, col, tab\n\n        else:\n            msg = _(\"Key length {length}  not in (2, 3)\").format(length=length)\n            raise ValueError(msg)", "category": "Python"}, {"instruction": "def lang_match_json(row, hdr, accepted_languages):\n    '''Find if the JSON row contains acceptable language data'''\n", "input": "", "output": "    if not accepted_languages:\n        return True\n    languages = set([row[c].get('xml:lang') for c in hdr\n                     if c in row and row[c]['type'] == 'literal'])\n    return (not languages) or (languages & accepted_languages)", "category": "Python"}, {"instruction": "def verify_all(df, check, *args, **kwargs):\n    \"\"\"\n    Verify that all the entries in ``check(df, *args, **kwargs)``\n    are true.\n    \"\"\"\n", "input": "", "output": "    result = check(df, *args, **kwargs)\n    try:\n        assert np.all(result)\n    except AssertionError as e:\n        msg = \"{} not true for all\".format(check.__name__)\n        e.args = (msg, df[~result])\n        raise\n    return df", "category": "Python"}, {"instruction": "def search(self, *query, **kwargs):\n        '''\n        Searches based on tags specified by users\n\n\n\n        Parameters\n        ---------\n        query: str\n            tags to search on. If multiple terms, provided in comma delimited\n            string format\n\n        prefix: str\n            start of archive name. Providing a start string improves search\n            speed.\n\n        '''\n", "input": "", "output": "\n        prefix = kwargs.get('prefix')\n\n        if prefix is not None:\n            prefix = fs.path.relpath(prefix)\n\n        return self.manager.search(query, begins_with=prefix)", "category": "Python"}, {"instruction": "def minimal_models(self, alphabet: _Alphabet) -> Set[PLInterpretation]:\n        \"\"\"Find models of min size (i.e. the less number of proposition to True).\n        Very trivial (and inefficient) algorithm: BRUTE FORCE on all the possible interpretations.\"\"\"\n", "input": "", "output": "        models = self.all_models(alphabet)\n\n        minimal_models = set()\n        for m in models:\n            min_m = m\n            for m1 in models:\n                if min_m.true_propositions.issuperset(m1.true_propositions):\n                    min_m = m1\n            minimal_models.add(min_m)\n\n        return minimal_models", "category": "Python"}, {"instruction": "def post_event_ticket_class(self, id, ticket_class_id, **data):\n        \"\"\"\n        POST /events/:id/ticket_classes/:ticket_class_id/\n        Updates an existing ticket class, returning the updated result as a :format:`ticket_class` under the key ``ticket_class``.\n        \"\"\"\n", "input": "", "output": "        \n        return self.post(\"/events/{0}/ticket_classes/{0}/\".format(id,ticket_class_id), data=data)", "category": "Python"}, {"instruction": "def list(self, deployment_sid=values.unset, limit=None, page_size=None):\n        \"\"\"\n        Lists DeviceInstance records from the API as a list.\n        Unlike stream(), this operation is eager and will load `limit` records into\n        memory before returning.\n\n        :param unicode deployment_sid: Find all Devices grouped under the specified Deployment.\n        :param int limit: Upper limit for the number of records to return. list() guarantees\n                          never to return more than limit.  Default is no limit\n        :param int page_size: Number of records to fetch per request, when not set will use\n                              the default value of 50 records.  If no page_size is defined\n                              but a limit is defined, list() will attempt to read the limit\n                              with the most efficient page size, i.e. min(limit, 1000)\n\n        :returns: Generator that will yield up to limit results\n        :rtype: list[twilio.rest.preview.deployed_devices.fleet.device.DeviceInstance]\n        \"\"\"\n", "input": "", "output": "        return list(self.stream(deployment_sid=deployment_sid, limit=limit, page_size=page_size, ))", "category": "Python"}, {"instruction": "def _callbacks_cleanup(cnx, callback_ids):\n    '''\n    Unregister all the registered callbacks\n\n    :param cnx: libvirt connection\n    :param callback_ids: dictionary mapping a libvirt object type to an ID list\n                         of callbacks to deregister\n    '''\n", "input": "", "output": "    for obj, ids in callback_ids.items():\n        register_name = REGISTER_FUNCTIONS[obj]\n        deregister_name = register_name.replace('Reg', 'Dereg')\n        deregister = getattr(cnx, deregister_name)\n        for callback_id in ids:\n            deregister(callback_id)", "category": "Python"}, {"instruction": "def addChildJobFn(self, fn, *args, **kwargs):\n        \"\"\"\n        Adds a job function as a child job. See :class:`toil.job.JobFunctionWrappingJob`\n        for a definition of a job function.\n\n        :param fn: Job function to be run as a child job with ``*args`` and ``**kwargs`` as \\\n        arguments to this function. See toil.job.JobFunctionWrappingJob for reserved \\\n        keyword arguments used to specify resource requirements.\n        :return: The new child job that wraps fn.\n        :rtype: toil.job.JobFunctionWrappingJob\n        \"\"\"\n", "input": "", "output": "        if PromisedRequirement.convertPromises(kwargs):\n            return self.addChild(PromisedRequirementJobFunctionWrappingJob.create(fn, *args, **kwargs))\n        else:\n            return self.addChild(JobFunctionWrappingJob(fn, *args, **kwargs))", "category": "Python"}, {"instruction": "def namedb_open( path ):\n    \"\"\"\n    Open a connection to our database\n    \"\"\"\n", "input": "", "output": "    con = sqlite3.connect( path, isolation_level=None, timeout=2**30 )\n    db_query_execute(con, 'pragma mmap_size=536870912', ())\n    con.row_factory = namedb_row_factory\n\n    version = namedb_get_version(con)\n    if not semver_equal(version, VERSION):\n        # wrong version\n        raise Exception('Database has version {}, but this node is version {}.  Please update your node database (such as with fast_sync).'.format(version, VERSION))\n\n    return con", "category": "Python"}, {"instruction": "def set_cwd(cls, cwd):\n        \"\"\"\n        Set the cwd that is used to manipulate paths.\n        \"\"\"\n", "input": "", "output": "        if not cwd:\n            try:\n                cwd = os.getcwdu()\n            except AttributeError:\n                cwd = os.getcwd()\n        if isinstance(cwd, six.binary_type):\n            cwd = cwd.decode(sys.getdefaultencoding())\n        cls._cwd = cwd\n        cls._root = cls._git_root()", "category": "Python"}, {"instruction": "def MACVOL(self,days,rev=0):\n    \"\"\" Comparing yesterday volume is high, low or equal.\n        return \u2191,\u2193 or -\n        \u8207\u524d\u4e00\u5929 days \u65e5\u6210\u4ea4\u91cf\u79fb\u52d5\u5e73\u5747\u6bd4\u8f03\n        rev = 0\n          \u56de\u50b3 \u2191,\u2193 or -\n        rev = 1\n          \u56de\u50b3 1,-1 or 0\n    \"\"\"\n", "input": "", "output": "    yesterday = self.stock_vol[:]\n    yesterday.pop()\n    yes_MAVOL = float(sum(yesterday[-days:]) / days)\n    today_MAVOL = self.MAVOL(days)\n\n    return self.high_or_low(today_MAVOL, yes_MAVOL,rev)", "category": "Python"}, {"instruction": "def add_parents(self, parents):\n        \"\"\"Adds new parent nodes after filtering for duplicates\n\n        Args:\n            parents (list): list of OmniTree nodes to add as parents\n        \"\"\"\n", "input": "", "output": "\n        self._parents += [p for p in parents if p not in self._parents]", "category": "Python"}, {"instruction": "def k2g(kml_path, output_dir, separate_folders, style_type, \n  style_filename):\n    \"\"\"\n    Given a path to a KML file, convert it to a a GeoJSON FeatureCollection file and save it to the given output directory.\n\n    If ``--separate_folders``, then create several GeoJSON files, one for each folder in the KML file that contains geodata or that has a descendant node that contains geodata.\n    Warning: this can produce GeoJSON files with the same geodata in case the KML file has nested folders with geodata.\n\n    If ``--style_type`` is specified, then also build a JSON style file of the given style type and save it to the output directory under the file name given by ``--style_filename``.\n    \"\"\"\n", "input": "", "output": "    m.convert(kml_path, output_dir, separate_folders, style_type, style_filename)", "category": "Python"}, {"instruction": "def merge_config(\n    config: Mapping[str, Any],\n    override_config: Mapping[str, Any] = None,\n    override_config_fn: str = None,\n) -> Mapping[str, Any]:\n    \"\"\"Override config with additional configuration in override_config or override_config_fn\n\n    Used in script to merge CLI options with Config\n\n    Args:\n        config: original configuration\n        override_config: new configuration to override/extend current config\n        override_config_fn: new configuration filename as YAML file\n    \"\"\"\n", "input": "", "output": "\n    if override_config_fn:\n        with open(override_config_fn, \"r\") as f:\n            override_config = yaml.load(f, Loader=yaml.SafeLoader)\n\n    if not override_config:\n        log.info(\"Missing override_config\")\n\n    return functools.reduce(rec_merge, (config, override_config))", "category": "Python"}, {"instruction": "def _handle_client_error():\n    \"\"\"\n    Handle boto exception and convert to class\n    IO exceptions\n\n    Raises:\n        OSError subclasses: IO error.\n    \"\"\"\n", "input": "", "output": "    try:\n        yield\n\n    except _ClientError as exception:\n        error = exception.response['Error']\n        if error['Code'] in _ERROR_CODES:\n            raise _ERROR_CODES[error['Code']](error['Message'])\n        raise", "category": "Python"}, {"instruction": "def fixed_length(cls, l, allow_empty=False):\n        \"\"\"Create a sedes for text data with exactly `l` encoded characters.\"\"\"\n", "input": "", "output": "        return cls(l, l, allow_empty=allow_empty)", "category": "Python"}, {"instruction": "def create_WCSname(wcsname):\n    \"\"\" Verify that a valid WCSNAME has been provided, and if not, create a\n        default WCSNAME based on current date.\n    \"\"\"\n", "input": "", "output": "    if util.is_blank(wcsname):\n        ptime = fileutil.getDate()\n        wcsname = \"User_\"+ptime\n\n    return wcsname", "category": "Python"}, {"instruction": "def make_timebar(progress=0, duration=0):\n    \"\"\"\n    Makes a new time bar string\n\n    Args:\n        progress: How far through the current song we are (in seconds)\n        duration: The duration of the current song (in seconds)\n\n    Returns:\n        timebar (str): The time bar string\n    \"\"\"\n", "input": "", "output": "\n    duration_string = api_music.duration_to_string(duration)\n    if duration <= 0:\n        return \"---\"\n\n    time_counts = int(round((progress / duration) * TIMEBAR_LENGTH))\n    if time_counts > TIMEBAR_LENGTH:\n        time_counts = TIMEBAR_LENGTH\n\n    if duration > 0:\n        bar = \"\u2502\" + (TIMEBAR_PCHAR * time_counts) + (TIMEBAR_ECHAR * (TIMEBAR_LENGTH - time_counts)) + \"\u2502\"\n        time_bar = \"{} {}\".format(bar, duration_string)\n    else:\n        time_bar = duration_string\n\n    return time_bar", "category": "Python"}, {"instruction": "def tonativefunc(enc='utf-8'):\n    ''' Returns a function that turns everything into 'native' strings using enc '''\n", "input": "", "output": "    if sys.version_info >= (3,0,0):\n        return lambda x: x.decode(enc) if isinstance(x, bytes) else str(x)\n    return lambda x: x.encode(enc) if isinstance(x, unicode) else str(x)", "category": "Python"}, {"instruction": "def delaunay_2d(self, tol=1e-05, alpha=0.0, offset=1.0, bound=False, inplace=False):\n        \"\"\"Apply a delaunay 2D filter along the best fitting plane\"\"\"\n", "input": "", "output": "        alg = vtk.vtkDelaunay2D()\n        alg.SetProjectionPlaneMode(vtk.VTK_BEST_FITTING_PLANE)\n        alg.SetInputDataObject(self)\n        alg.SetTolerance(tol)\n        alg.SetAlpha(alpha)\n        alg.SetOffset(offset)\n        alg.SetBoundingTriangulation(bound)\n        alg.Update()\n\n        mesh = _get_output(alg)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "category": "Python"}, {"instruction": "def _find_player_id(self, row):\n        \"\"\"\n        Find the player's ID.\n\n        Find the player's ID as embedded in the 'data-append-csv' attribute,\n        such as 'zettehe01' for Henrik Zetterberg.\n\n        Parameters\n        ----------\n        row : PyQuery object\n            A PyQuery object representing a single row in a boxscore table for\n            a single player.\n\n        Returns\n        -------\n        str\n            Returns a ``string`` of the player's ID, such as 'zettehe01' for\n            Henrik Zetterberg.\n        \"\"\"\n", "input": "", "output": "        player_id = row('th').attr('data-append-csv')\n        if not player_id:\n            player_id = row('td').attr('data-append-csv')\n        return player_id", "category": "Python"}, {"instruction": "def from_fs_path(path):\n    \"\"\"Returns a URI for the given filesystem path.\"\"\"\n", "input": "", "output": "    scheme = 'file'\n    params, query, fragment = '', '', ''\n    path, netloc = _normalize_win_path(path)\n    return urlunparse((scheme, netloc, path, params, query, fragment))", "category": "Python"}, {"instruction": "def object_issubclass(node, class_or_seq, context=None):\n    \"\"\"Check if a type is a subclass of any node in class_or_seq\n\n    :param node: A given node\n    :param class_or_seq: Union[Nodes.NodeNG, Sequence[nodes.NodeNG]]\n    :rtype: bool\n\n    :raises AstroidTypeError: if the given ``classes_or_seq`` are not types\n    :raises AstroidError: if the type of the given node cannot be inferred\n        or its type's mro doesn't work\n    \"\"\"\n", "input": "", "output": "    if not isinstance(node, nodes.ClassDef):\n        raise TypeError(\"{node} needs to be a ClassDef node\".format(node=node))\n    return _object_type_is_subclass(node, class_or_seq, context=context)", "category": "Python"}, {"instruction": "def next_frame_sv2p_tiny():\n  \"\"\"Tiny SV2P model.\"\"\"\n", "input": "", "output": "  hparams = next_frame_sv2p_atari_softmax()\n  hparams.batch_size = 2\n  hparams.tiny_mode = True\n  hparams.num_masks = 1\n  hparams.video_modality_loss_cutoff = 0.4\n  hparams.video_num_input_frames = 4\n  hparams.video_num_target_frames = 4\n  return hparams", "category": "Python"}, {"instruction": "def occurrences_after(self, after=None):\n        \"\"\"\n        It is often useful to know what the next occurrence is given a list of\n        events.  This function produces a generator that yields the\n        the most recent occurrence after the date ``after`` from any of the\n        events in ``self.events``\n        \"\"\"\n", "input": "", "output": "        from schedule.models import Occurrence\n\n        if after is None:\n            after = timezone.now()\n        occ_replacer = OccurrenceReplacer(\n            Occurrence.objects.filter(event__in=self.events))\n        generators = [event._occurrences_after_generator(after) for event in self.events]\n        occurrences = []\n\n        for generator in generators:\n            try:\n                heapq.heappush(occurrences, (next(generator), generator))\n            except StopIteration:\n                pass\n\n        while occurrences:\n            generator = occurrences[0][1]\n\n            try:\n                next_occurrence = heapq.heapreplace(occurrences, (next(generator), generator))[0]\n            except StopIteration:\n                next_occurrence = heapq.heappop(occurrences)[0]\n            yield occ_replacer.get_occurrence(next_occurrence)", "category": "Python"}, {"instruction": "def variantAnnotationsGenerator(self, request):\n        \"\"\"\n        Returns a generator over the (variantAnnotaitons, nextPageToken) pairs\n        defined by the specified request.\n        \"\"\"\n", "input": "", "output": "        compoundId = datamodel.VariantAnnotationSetCompoundId.parse(\n            request.variant_annotation_set_id)\n        dataset = self.getDataRepository().getDataset(compoundId.dataset_id)\n        variantSet = dataset.getVariantSet(compoundId.variant_set_id)\n        variantAnnotationSet = variantSet.getVariantAnnotationSet(\n            request.variant_annotation_set_id)\n        iterator = paging.VariantAnnotationsIntervalIterator(\n            request, variantAnnotationSet)\n        return iterator", "category": "Python"}, {"instruction": "def replace_launch_config(self, scaling_group, launch_config_type,\n            server_name, image, flavor, disk_config=None, metadata=None,\n            personality=None, networks=None, load_balancers=None,\n            key_name=None):\n        \"\"\"\n        Replace an existing launch configuration. All of the attributes must be\n        specified. If you wish to delete any of the optional attributes, pass\n        them in as None.\n        \"\"\"\n", "input": "", "output": "        return self._manager.replace_launch_config(scaling_group,\n                launch_config_type, server_name, image, flavor,\n                disk_config=disk_config, metadata=metadata,\n                personality=personality, networks=networks,\n                load_balancers=load_balancers, key_name=key_name)", "category": "Python"}, {"instruction": "def from_dict(cls, dictionary, root=False):\n        \"\"\"Convert dictionary (and ordered dictionary) into a ConfigTree\n        :param dictionary: dictionary to convert\n        :type dictionary: dict\n        :return: Config object\n        :type return: Config\n        \"\"\"\n", "input": "", "output": "\n        def create_tree(value):\n            if isinstance(value, dict):\n                res = ConfigTree(root=root)\n                for key, child_value in value.items():\n                    res.put(key, create_tree(child_value))\n                return res\n            if isinstance(value, list):\n                return [create_tree(v) for v in value]\n            else:\n                return value\n\n        return create_tree(dictionary)", "category": "Python"}, {"instruction": "def clear(self):\n        \"\"\"\n        Cleans up the manager. The manager can't be used after this method has\n        been called\n        \"\"\"\n", "input": "", "output": "        # Cancel timer\n        self.__cancel_timer()\n        self.__timer = None\n        self.__timer_args = None\n\n        self.__still_valid = False\n        self._value = None\n        super(TemporalDependency, self).clear()", "category": "Python"}, {"instruction": "def putParamset(self, remote, address, paramset, value):\n        \"\"\"Set paramsets manually\"\"\"\n", "input": "", "output": "        try:\n            return self.proxies[\"%s-%s\" % (self._interface_id, remote)].putParamset(address, paramset, value)\n        except Exception as err:\n            LOG.debug(\"ServerThread.putParamset: Exception: %s\" % str(err))", "category": "Python"}, {"instruction": "def render_to_template(self):\n        \"\"\"\n        Render the current menu instance to a template and return a string\n        \"\"\"\n", "input": "", "output": "        context_data = self.get_context_data()\n        template = self.get_template()\n\n        context_data['current_template'] = template.template.name\n        return template.render(context_data)", "category": "Python"}, {"instruction": "def assert_condition_last_modified(self):\n        \"\"\"If the resource has a last modified date (see\n        :func:`get_last_modified`) the request headers ``If-Modified-Since``\n        and ``If-Unmodified-Since`` are verified. May abort the request with\n        304 or 412 response codes.\n\n        :raises:\n            - :class:`webob.exceptions.ResponseException` of status 304 if the\n              ``If-Modified-Since`` is later than the last modified date.\n            - :class:`webob.exceptions.ResponseException` of status 412 if the\n              last modified date is later than the ``If-Unmodified-Since``\n              header.\n        \"\"\"\n", "input": "", "output": "        rq = self.request\n        rs = self.response\n        if rs.last_modified:\n            rsl = rs.last_modified\n            if rq.if_modified_since and rsl <= rq.if_modified_since:\n                raise_304(self)\n            if rq.if_unmodified_since and rsl > rq.if_unmodified_since:\n                raise_412(self, 'Resource is newer than the '\n                    'If-Unmodified-Since request header.')", "category": "Python"}, {"instruction": "def leave_group_memberships(self, group_id, membership_id):\r\n        \"\"\"\r\n        Leave a group.\r\n\r\n        Leave a group if you are allowed to leave (some groups, such as sets of\r\n        course groups created by teachers, cannot be left). You may also use 'self'\r\n        in place of a membership_id.\r\n        \"\"\"\n", "input": "", "output": "        path = {}\r\n        data = {}\r\n        params = {}\r\n\r\n        # REQUIRED - PATH - group_id\r\n        ", "category": "Python"}, {"instruction": "def show_multi_exposure(self):\n        \"\"\"Show InaSAFE Multi Exposure.\"\"\"\n", "input": "", "output": "        from safe.gui.tools.multi_exposure_dialog import MultiExposureDialog\n        dialog = MultiExposureDialog(\n            self.iface.mainWindow(), self.iface)\n        dialog.exec_()", "category": "Python"}, {"instruction": "def select_distinct(distinct_field=None, **kwargs):\n    \"\"\"\n    select distinct values for a given field for a given a query\n    \"\"\"\n", "input": "", "output": "    results = search_associations(rows=0,\n                                  select_fields=[],\n                                  facet_field_limits = {\n                                      distinct_field : -1\n                                  },\n                                  facet_fields=[distinct_field],\n                                  **kwargs\n    )\n    # TODO: map field\n    return list(results['facet_counts'][distinct_field].keys())", "category": "Python"}, {"instruction": "def create_from_yaml(self, name, yamlfile):\n        \"\"\"\n        Create new environment using conda-env via a yaml specification file.\n\n        Unlike other methods, this calls conda-env, and requires a named\n        environment and uses channels as defined in rcfiles.\n\n        Parameters\n        ----------\n        name : string\n            Environment name\n        yamlfile : string\n            Path to yaml file with package spec (as created by conda env export\n        \"\"\"\n", "input": "", "output": "        logger.debug(str((name, yamlfile)))\n        cmd_list = ['env', 'create', '-n', name, '-f', yamlfile, '--json']\n        return self._call_and_parse(cmd_list)", "category": "Python"}, {"instruction": "def _add_throughput(self, y, x, width, op, title, available, used):\n        \"\"\" Write a single throughput measure to a row \"\"\"\n", "input": "", "output": "        percent = float(used) / available\n        self.win.addstr(y, x, \"[\")\n        # Because we have disabled scrolling, writing the lower right corner\n        # character in a terminal can throw an error (this is inside the curses\n        # implementation). If that happens (and it will only ever happen here),\n        # we should just catch it and continue.\n        try:\n            self.win.addstr(y, x + width - 1, \"]\")\n        except curses.error:\n            pass\n        x += 1\n        right = \"%.1f/%d:%s\" % (used, available, op)\n        pieces = self._progress_bar(width - 2, percent, title, right)\n        for color, text in pieces:\n            self.win.addstr(y, x, text, curses.color_pair(color))\n            x += len(text)", "category": "Python"}, {"instruction": "def recognise(self):\n        \"\"\"\n        Recognise which is Cube's OLL case.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(self.cube, Cube):\n            raise ValueError(\"Use Solver.feed(cube) to feed the cube to solver.\")\n        result = \"\"\n        for face in \"LFRB\":\n            for square in self.cube.get_face(face)[0]:\n                result += str(int(square == self.cube[\"U\"][\"U\"]))\n        if result not in algo_dict:\n            raise ValueError(\"Invalid Cube, probably didn't solve F2L, or wrong input value.\\nUse Solver.feed(cube) to reset the cube.\")\n        self.case = result\n        return result", "category": "Python"}, {"instruction": "def stop_patching(name=None):\n    # type: (Optional[str]) -> None\n    \"\"\"\n    Finish the mocking initiated by `start_patching`\n\n    Kwargs:\n        name (Optional[str]): if given, only unpatch the specified path, else all\n            defined default mocks\n    \"\"\"\n", "input": "", "output": "    global _patchers, _mocks\n    if not _patchers:\n        warnings.warn('stop_patching() called again, already stopped')\n\n    if name is not None:\n        items = [(name, _patchers[name])]\n    else:\n        items = list(_patchers.items())\n\n    for name, patcher in items:\n        patcher.stop()\n        del _patchers[name]\n        del _mocks[name]", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Close the channel to the queue.\"\"\"\n", "input": "", "output": "        self.cancel()\n        self.backend.close()\n        self._closed = True", "category": "Python"}, {"instruction": "def iter_deployments(self, number=-1, etag=None):\n        \"\"\"Iterate over deployments for this repository.\n\n        :param int number: (optional), number of deployments to return.\n            Default: -1, returns all available deployments\n        :param str etag: (optional), ETag from a previous request for all\n            deployments\n        :returns: generator of\n            :class:`Deployment <github3.repos.deployment.Deployment>`\\ s\n        \"\"\"\n", "input": "", "output": "        url = self._build_url('deployments', base_url=self._api)\n        i = self._iter(int(number), url, Deployment, etag=etag)\n        i.headers.update(Deployment.CUSTOM_HEADERS)\n        return i", "category": "Python"}, {"instruction": "def check_resource_subscription(self, device_id, _resource_path, **kwargs):  # noqa: E501\n        \"\"\"Read subscription status  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass asynchronous=True\n        >>> thread = api.check_resource_subscription(device_id, _resource_path, asynchronous=True)\n        >>> result = thread.get()\n\n        :param asynchronous bool\n        :param str device_id: A unique Device Management device ID for the endpoint. Note that the ID must be an exact match. You cannot use wildcards here.  (required)\n        :param str _resource_path: The URL of the resource.  (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('asynchronous'):\n            return self.check_resource_subscription_with_http_info(device_id, _resource_path, **kwargs)  # noqa: E501\n        else:\n            (data) = self.check_resource_subscription_with_http_info(device_id, _resource_path, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def padto8(data):\n    \"\"\"Pads data to the multiplies of 8 bytes.\n\n       This makes x86_64 faster and prevents\n       undefined behavior on other platforms\"\"\"\n", "input": "", "output": "    length = len(data)\n    return data + b'\\xdb' * (roundto8(length) - length)", "category": "Python"}, {"instruction": "def publish_proto_metadata_update(self):\n        \"\"\" Publish protobuf model in ipfs and update existing metadata file \"\"\"\n", "input": "", "output": "        metadata = load_mpe_service_metadata(self.args.metadata_file)\n        ipfs_hash_base58 = utils_ipfs.publish_proto_in_ipfs(self._get_ipfs_client(), self.args.protodir)\n        metadata.set_simple_field(\"model_ipfs_hash\", ipfs_hash_base58)\n        metadata.save_pretty(self.args.metadata_file)", "category": "Python"}, {"instruction": "def has_ndarray_int_columns(features, X):\n    \"\"\" Checks if numeric feature columns exist in ndarray \"\"\"\n", "input": "", "output": "    _, ncols = X.shape\n    if not all(d.isdigit() for d in features if isinstance(d, str)) or not isinstance(X, np.ndarray):\n        return False\n    ndarray_columns = np.arange(0, ncols)\n    feature_cols = np.unique([int(d) for d in features])\n    return all(np.in1d(feature_cols, ndarray_columns))", "category": "Python"}, {"instruction": "def _print_details(extra=None):\n    \"\"\"Return a function that prints node details.\"\"\"\n", "input": "", "output": "    def print_node_handler(name, node, depth):\n        ", "category": "Python"}, {"instruction": "def auto_no_thousands(self):\n        \"\"\"Like self.auto but calculates the next unit if >999.99.\"\"\"\n", "input": "", "output": "        if self._value >= 1000000000000:\n            return self.TiB, 'TiB'\n        if self._value >= 1000000000:\n            return self.GiB, 'GiB'\n        if self._value >= 1000000:\n            return self.MiB, 'MiB'\n        if self._value >= 1000:\n            return self.KiB, 'KiB'\n        else:\n            return self.B, 'B'", "category": "Python"}, {"instruction": "def dictionary_merge(a, b):\n    \"\"\"merges dictionary b into a\n       Like dict.update, but recursive\n    \"\"\"\n", "input": "", "output": "    for key, value in b.items():\n        if key in a and isinstance(a[key], dict) and isinstance(value, dict):\n            dictionary_merge(a[key], b[key])\n            continue\n        a[key] = b[key]\n    return a", "category": "Python"}, {"instruction": "def to_dict(item):\n        \"\"\"\n        Convert an object to a dictionary (recursive).\n        \"\"\"\n", "input": "", "output": "        def convert(item):\n            if isinstance(item, IterableObject):\n                if isinstance(item.source, dict):\n                    return {k: convert(v.source) if hasattr(v, 'source') else convert(v) for k, v in item}\n                else:\n                    return convert(item.source)\n            elif isinstance(item, dict):\n                return {k: convert(v) for k, v in item.items()}\n            elif isinstance(item, list):\n                def yield_convert(item):\n                    for index, value in enumerate(item):\n                        yield convert(value)\n                return list(yield_convert(item))\n            else:\n                return item\n\n        return convert(item)", "category": "Python"}, {"instruction": "def getValidCertifications(self):\n        \"\"\" Returns the certifications fully valid\n        \"\"\"\n", "input": "", "output": "        certs = []\n        today = date.today()\n        for c in self.getCertifications():\n            validfrom = c.getValidFrom() if c else None\n            validto = c.getValidTo() if validfrom else None\n            if not validfrom or not validto:\n                continue\n            validfrom = validfrom.asdatetime().date()\n            validto = validto.asdatetime().date()\n            if (today >= validfrom and today <= validto):\n                certs.append(c)\n        return certs", "category": "Python"}, {"instruction": "def chi_squared(self, p=None):\n        \"\"\"\n        Returns the total chi squared (summed over all massaged data sets).\n        \n        p=None means use the fit results.\n        \"\"\"\n", "input": "", "output": "        chi2s = self.chi_squareds(p)\n        if chi2s == None: return None\n        \n        return sum(self.chi_squareds(p))", "category": "Python"}, {"instruction": "def astype(self, dtype):\n        \"\"\"Return a copy of this element with new ``dtype``.\n\n        Parameters\n        ----------\n        dtype :\n            Scalar data type of the returned space. Can be provided\n            in any way the `numpy.dtype` constructor understands, e.g.\n            as built-in type or as a string. Data types with non-trivial\n            shapes are not allowed.\n\n        Returns\n        -------\n        newelem : `NumpyTensor`\n            Version of this element with given data type.\n        \"\"\"\n", "input": "", "output": "        return self.space.astype(dtype).element(self.data.astype(dtype))", "category": "Python"}, {"instruction": "def _on_expose_event(self, widget, event):\n        '''\n        .. versionchanged:: 0.20\n            Renamed from ``on_widget__expose_event`` to allow wrapping for\n            debouncing to improve responsiveness.\n\n        Called when drawing area is first displayed and, for example, when part\n        of drawing area is uncovered after being covered up by another window.\n\n        Clear canvas, draw frame off screen, and mark dirty.\n        '''\n", "input": "", "output": "        logger.info('on_widget__expose_event')\n\n        # Request immediate paint of pre-rendered off-screen Cairo surface to\n        # drawing area widget, but also mark as dirty to redraw after next\n        # render.\n        self.draw()\n        self._dirty_draw = True", "category": "Python"}, {"instruction": "def get_assignee(self, login):\n        \"\"\"\n        given the user login, looks for a user in assignee list of the repo\n        and return it if was found.\n        \"\"\"\n", "input": "", "output": "        if not login:\n            return GithubObject.NotSet\n        if not hasattr(self, '_assignees'):\n            self._assignees = {c.login: c for c in self.repo.get_assignees()}\n        if login not in self._assignees:\n            # warning\n            print(\"{} doesn't belong to this repo. This issue won't be assigned.\".format(login))\n        return self._assignees.get(login)", "category": "Python"}, {"instruction": "def modify_macho_file_headers(macho_file_path, modificator_func):\n\t\"\"\" Modifies headers of a Mach-O file at the given path by calling\n\tthe modificator function on each header.\n\t\n\tReturns True on success, otherwise rises an exeption (e.g. from macholib)\n\t\"\"\"\n", "input": "", "output": "\tif not os.path.isfile(macho_file_path):\n\t\traise Exception(\"You must specify a real executable path as a target\")\n\t\treturn False\n\t\t\n\tm = MachO(macho_file_path)\n\tapply_to_headers(m, modificator_func)\n\tsave_macho(m, macho_file_path)\n\treturn True", "category": "Python"}, {"instruction": "def main():\n    \"\"\"Function to add or substract the temperature effect to data in a tomodir\n    \"\"\"\n", "input": "", "output": "    options = handle_options()\n\n    # read in temperature and resistivity data\n    tempdata = readin_temp(options.temp_file)\n    magdata = readin_rho(options.filename,\n                         options.rhofile,\n                         aniso=options.aniso)\n    # calculate corrected data\n    mag_corr = calc_correction(temp=tempdata,\n                               mag=magdata,\n                               add=options.add,\n                               T_std=options.T_std,\n                               m=options.m,)\n    # save data\n    save_mag_to_file(mag_corr,\n                     options.output,\n                     options.rhofile)", "category": "Python"}, {"instruction": "def _get_fs(self):\n        '''\n        Get available file systems and their types.\n        '''\n", "input": "", "output": "\n        data = dict()\n        for dev, dev_data in salt.utils.fsutils._blkid().items():\n            dev = self._get_disk_size(dev)\n            device = dev.pop('device')\n            dev['type'] = dev_data['type']\n            data[device] = dev\n\n        return data", "category": "Python"}, {"instruction": "def compute_wcs(key, challenge):\n    \"\"\"\n    Compute an WAMP-CRA authentication signature from an authentication\n    challenge and a (derived) key.\n\n    :param key: The key derived (via PBKDF2) from the secret.\n    :type key: str/bytes\n    :param challenge: The authentication challenge to sign.\n    :type challenge: str/bytes\n\n    :return: The authentication signature.\n    :rtype: bytes\n    \"\"\"\n", "input": "", "output": "    key = key.encode('utf8')\n    challenge = challenge.encode('utf8')\n    sig = hmac.new(key, challenge, hashlib.sha256).digest()\n    return binascii.b2a_base64(sig).strip()", "category": "Python"}, {"instruction": "def V_vertical_ellipsoidal(D, a, h):\n    r'''Calculates volume of a vertical tank with a convex ellipsoidal bottom,\n    according to [1]_. No provision for the top of the tank is made here.\n\n    .. math::\n        V_f = \\frac{\\pi}{4}\\left(\\frac{Dh}{a}\\right)^2 \\left(a - \\frac{h}{3}\\right),\\; h < a\n\n    .. math::\n        V_f = \\frac{\\pi D^2}{4}\\left(h - \\frac{a}{3}\\right),\\; h \\ge a\n\n    Parameters\n    ----------\n    D : float\n        Diameter of the main cylindrical section, [m]\n    a : float\n        Distance the ellipsoid head extends under the main cylinder, [m]\n    h : float\n        Height, as measured up to where the fluid ends, [m]\n\n    Returns\n    -------\n    V : float\n        Volume [m^3]\n\n    Examples\n    --------\n    Matching example from [1]_, with inputs in inches and volume in gallons.\n\n    >>> V_vertical_ellipsoidal(132., 33., 24)/231.\n    783.3581681678445\n\n    References\n    ----------\n    .. [1] Jones, D. \"Calculating Tank Volume.\" Text. Accessed December 22, 2015.\n       http://www.webcalc.com.br/blog/Tank_Volume.PDF'''\n", "input": "", "output": "    if h < a:\n        Vf = pi/4*(D*h/a)**2*(a - h/3.)\n    else:\n        Vf = pi*D**2/4*(h - a/3.)\n    return Vf", "category": "Python"}, {"instruction": "def Handle(self, unused_args, token=None):\n    \"\"\"Fetches and renders current user's settings.\"\"\"\n", "input": "", "output": "\n    result = ApiGrrUser(username=token.username)\n\n    if data_store.RelationalDBEnabled():\n      user_record = data_store.REL_DB.ReadGRRUser(token.username)\n      result.InitFromDatabaseObject(user_record)\n    else:\n      try:\n        user_record = aff4.FACTORY.Open(\n            aff4.ROOT_URN.Add(\"users\").Add(token.username),\n            aff4_users.GRRUser,\n            token=token)\n\n        result.InitFromAff4Object(user_record)\n      except IOError:\n        result.settings = aff4_users.GRRUser.SchemaCls.GUI_SETTINGS()\n\n    result.interface_traits = (\n        self.interface_traits or ApiGrrUserInterfaceTraits())\n\n    return result", "category": "Python"}, {"instruction": "def after(self, after):\n        \"\"\"\n        Sets the after of this EnrollmentIdentities.\n        ID\n\n        :param after: The after of this EnrollmentIdentities.\n        :type: str\n        \"\"\"\n", "input": "", "output": "        if after is None:\n            raise ValueError(\"Invalid value for `after`, must not be `None`\")\n        if after is not None and not re.search('^[A-Za-z0-9]{32}', after):\n            raise ValueError(\"Invalid value for `after`, must be a follow pattern or equal to `/^[A-Za-z0-9]{32}/`\")\n\n        self._after = after", "category": "Python"}, {"instruction": "def _create_gitlab_prometheus_instance(self, instance, init_config):\n        \"\"\"\n        Set up the gitlab instance so it can be used in OpenMetricsBaseCheck\n        \"\"\"\n", "input": "", "output": "        # Mapping from Prometheus metrics names to Datadog ones\n        # For now it's a 1:1 mapping\n        allowed_metrics = init_config.get('allowed_metrics')\n        if allowed_metrics is None:\n            raise CheckException(\"At least one metric must be whitelisted in `allowed_metrics`.\")\n\n        gitlab_instance = deepcopy(instance)\n        # gitlab uses 'prometheus_endpoint' and not 'prometheus_url', so we have to rename the key\n        gitlab_instance['prometheus_url'] = instance.get('prometheus_endpoint')\n\n        gitlab_instance.update(\n            {\n                'namespace': 'gitlab',\n                'metrics': allowed_metrics,\n                # Defaults that were set when gitlab was based on PrometheusCheck\n                'send_monotonic_counter': instance.get('send_monotonic_counter', False),\n                'health_service_check': instance.get('health_service_check', False),\n            }\n        )\n\n        return gitlab_instance", "category": "Python"}, {"instruction": "def request_acquisition(self, acquisition_request):\n        \"\"\"RequestAcquisition.\n        [Preview API]\n        :param :class:`<ExtensionAcquisitionRequest> <azure.devops.v5_1.gallery.models.ExtensionAcquisitionRequest>` acquisition_request:\n        :rtype: :class:`<ExtensionAcquisitionRequest> <azure.devops.v5_1.gallery.models.ExtensionAcquisitionRequest>`\n        \"\"\"\n", "input": "", "output": "        content = self._serialize.body(acquisition_request, 'ExtensionAcquisitionRequest')\n        response = self._send(http_method='POST',\n                              location_id='3adb1f2d-e328-446e-be73-9f6d98071c45',\n                              version='5.1-preview.1',\n                              content=content)\n        return self._deserialize('ExtensionAcquisitionRequest', response)", "category": "Python"}, {"instruction": "def _get_url(method,\n             api_url,\n             api_version):\n    '''\n    Build the API URL.\n    '''\n", "input": "", "output": "    return '{url}/{version}/{method}.json'.format(url=api_url,\n                                                  version=float(api_version),\n                                                  method=method)", "category": "Python"}, {"instruction": "def p_object_literal(self, p):\n        \"\"\"object_literal : LBRACE RBRACE\n                          | LBRACE property_list RBRACE\n                          | LBRACE property_list COMMA RBRACE\n        \"\"\"\n", "input": "", "output": "        if len(p) == 3:\n            p[0] = self.asttypes.Object()\n        else:\n            p[0] = self.asttypes.Object(properties=p[2])\n        p[0].setpos(p)", "category": "Python"}, {"instruction": "def topic_exists(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    Check to see if an SNS topic exists.\n\n    CLI example::\n\n        salt myminion boto3_sns.topic_exists mytopic region=us-east-1\n    '''\n", "input": "", "output": "    topics = list_topics(region=region, key=key, keyid=keyid, profile=profile)\n    return name in list(topics.values() + topics.keys())", "category": "Python"}, {"instruction": "def init_weights(self):\n        \"\"\"Performs He initialization\"\"\"\n", "input": "", "output": "\n        self.W = np.random.randn(self.n_neurons, self.n_inputs) * np.sqrt(2 / self.n_inputs)\n        self.b = np.zeros((self.n_neurons, 1))", "category": "Python"}, {"instruction": "def instruction_BVC(self, opcode, ea):\n        \"\"\"\n        Tests the state of the V (overflow) bit and causes a branch if it is\n        clear. That is, branch if the twos complement result was valid. When\n        used after an operation on twos complement binary values, this\n        instruction will branch if there was no overflow.\n\n        source code forms: BVC dd; LBVC DDDD\n\n        CC bits \"HNZVC\": -----\n        \"\"\"\n", "input": "", "output": "        if self.V == 0:\n#            log.info(\"$%x BVC branch to $%x, because V==0 \\t| %s\" % (\n#                self.program_counter, ea, self.cfg.mem_info.get_shortest(ea)\n#            ))\n            self.program_counter.set(ea)", "category": "Python"}, {"instruction": "def match_in(grammar, text):\n    \"\"\"Determine if there is a match for grammar in text.\"\"\"\n", "input": "", "output": "    for result in grammar.parseWithTabs().scanString(text):\n        return True\n    return False", "category": "Python"}, {"instruction": "def get_open_state_machine_of_file_system_path(self, file_system_path):\n        \"\"\"Return a reference to the state machine with respective path if open\n        \"\"\"\n", "input": "", "output": "        for sm in self.state_machines.values():\n            if sm.file_system_path == file_system_path:\n                return sm", "category": "Python"}, {"instruction": "def get_env(name, default=None):\n    \"\"\"Get the environment variable or return exception\"\"\"\n", "input": "", "output": "    if name in os.environ:\n        return os.environ[name]\n\n    if default is not None:\n        return default\n\n    error_msg = \"Set the {} env variable\".format(name)\n    raise ImproperlyConfigured(error_msg)", "category": "Python"}, {"instruction": "def __bindings(self):\n        \"\"\"Binds events to handlers\"\"\"\n", "input": "", "output": "\n        self.direction_choicectrl.Bind(wx.EVT_CHOICE, self.OnDirectionChoice)\n        self.sec_checkboxctrl.Bind(wx.EVT_CHECKBOX, self.OnSecondaryCheckbox)\n        self.pad_intctrl.Bind(EVT_INT, self.OnPadIntCtrl)\n        self.labelsize_intctrl.Bind(EVT_INT, self.OnLabelSizeIntCtrl)", "category": "Python"}, {"instruction": "def get_cel_to_gal_angle(skydir):\n    \"\"\"Calculate the rotation angle in radians between the longitude\n    axes of a local projection in celestial and galactic coordinates.\n\n    Parameters\n    ----------\n    skydir : `~astropy.coordinates.SkyCoord`\n        Direction of projection center.\n\n    Returns\n    -------\n    angle : float\n        Rotation angle in radians.\n    \"\"\"\n", "input": "", "output": "    wcs0 = create_wcs(skydir, coordsys='CEL')\n    wcs1 = create_wcs(skydir, coordsys='GAL')\n    x, y = SkyCoord.to_pixel(SkyCoord.from_pixel(1.0, 0.0, wcs0), wcs1)\n    return np.arctan2(y, x)", "category": "Python"}, {"instruction": "def get_athlete_stats(self, athlete_id=None):\n        \"\"\"\n        Returns Statistics for the athlete.\n        athlete_id must be the id of the authenticated athlete or left blank.\n        If it is left blank two requests will be made - first to get the\n        authenticated athlete's id and second to get the Stats.\n\n        http://strava.github.io/api/v3/athlete/#stats\n\n        :return: A model containing the Stats\n        :rtype: :py:class:`stravalib.model.AthleteStats`\n        \"\"\"\n", "input": "", "output": "        if athlete_id is None:\n            athlete_id = self.get_athlete().id\n\n        raw = self.protocol.get('/athletes/{id}/stats', id=athlete_id)\n        # TODO: Better error handling - this will return a 401 if this athlete\n        #       is not the authenticated athlete.\n\n        return model.AthleteStats.deserialize(raw)", "category": "Python"}, {"instruction": "def assert_not_called(_mock_self):\n        \"\"\"assert that the mock was never called.\n        \"\"\"\n", "input": "", "output": "        self = _mock_self\n        if self.call_count != 0:\n            msg = (\"Expected '%s' to not have been called. Called %s times.\" %\n                   (self._mock_name or 'mock', self.call_count))\n            raise AssertionError(msg)", "category": "Python"}, {"instruction": "def dict_values(src):\n    \"\"\"\n    Recursively get values in dict.\n\n    Unlike the builtin dict.values() function, this method will descend into\n    nested dicts, returning all nested values.\n\n    Arguments:\n        src (dict): Source dict.\n\n    Returns:\n        list: List of values.\n    \"\"\"\n", "input": "", "output": "    for v in src.values():\n        if isinstance(v, dict):\n            for v in dict_values(v):\n                yield v\n        else:\n            yield v", "category": "Python"}, {"instruction": "def dispatch(self, packet):\n        \"\"\"\n        dispatch: XBee data dict -> None\n\n        When called, dispatch checks the given packet against each\n        registered callback method and calls each callback whose filter\n        function returns true.\n        \"\"\"\n", "input": "", "output": "        for handler in self.handlers:\n            if handler['filter'](packet):\n                # Call the handler method with its associated\n                # name and the packet which passed its filter check\n                handler['callback'](handler['name'], packet)", "category": "Python"}, {"instruction": "def _next_channel_id(self):\n        '''Return the next possible channel id.  Is a circular enumeration.'''\n", "input": "", "output": "        self._channel_counter += 1\n        if self._channel_counter >= self._channel_max:\n            self._channel_counter = 1\n        return self._channel_counter", "category": "Python"}, {"instruction": "def open(self):\n        \"\"\"Opens the stream.\"\"\"\n", "input": "", "output": "        if self.is_active:\n            raise ValueError(\"Can not open an already open stream.\")\n\n        request_generator = _RequestQueueGenerator(\n            self._request_queue, initial_request=self._initial_request\n        )\n        call = self._start_rpc(iter(request_generator), metadata=self._rpc_metadata)\n\n        request_generator.call = call\n\n        # TODO: api_core should expose the future interface for wrapped\n        # callables as well.\n        if hasattr(call, \"_wrapped\"):  # pragma: NO COVER\n            call._wrapped.add_done_callback(self._on_call_done)\n        else:\n            call.add_done_callback(self._on_call_done)\n\n        self._request_generator = request_generator\n        self.call = call", "category": "Python"}, {"instruction": "def concat_generator(filename, up_threshold, low_threshold=10):\n  \"\"\"Generate concatenated lines from file upto up_threshold characters.\"\"\"\n", "input": "", "output": "  txt = \"\"\n  for line in tf.gfile.Open(filename):\n    line = line.strip()\n    if len(txt) + len(line) + 1 >= up_threshold:\n      ret = txt\n      txt = \"\"\n      # We don't yield very short long parts to prevent noisy examples.\n      if len(ret) > low_threshold and len(ret) < up_threshold:\n        yield {\"targets\": ret}\n\n    if not txt:\n      txt = line\n    else:\n      txt = \" \".join([txt, line])", "category": "Python"}, {"instruction": "def shared_databases(self):\n        \"\"\"\n        Retrieves a list containing the names of databases shared\n        with this account.\n\n        :returns: List of database names\n        \"\"\"\n", "input": "", "output": "        endpoint = '/'.join((\n            self.server_url, '_api', 'v2', 'user', 'shared_databases'))\n        resp = self.r_session.get(endpoint)\n        resp.raise_for_status()\n        data = response_to_json_dict(resp)\n        return data.get('shared_databases', [])", "category": "Python"}, {"instruction": "def build(self, output_dir=None, **kwargs):\n        \"\"\"\n        Buildes the recipe and creates needed folder and files.\n        May ask the user for some parameter inputs.\n\n        :param output_dir: Path, where the recipe shall be build. Default is the current working directory\n        :return: location of the installed recipe\n        \"\"\"\n", "input": "", "output": "        if output_dir is None:\n            output_dir = os.getcwd()\n\n        target = cookiecutter(self.path, output_dir=output_dir, **kwargs)\n\n        if self.final_words is not None and len(self.final_words) > 0:\n            print(\"\")\n            print(self.final_words)\n        return target", "category": "Python"}, {"instruction": "def register_transform(self, node_class, transform, predicate=None):\n        \"\"\"Register `transform(node)` function to be applied on the given\n        astroid's `node_class` if `predicate` is None or returns true\n        when called with the node as argument.\n\n        The transform function may return a value which is then used to\n        substitute the original node in the tree.\n        \"\"\"\n", "input": "", "output": "        self.transforms[node_class].append((transform, predicate))", "category": "Python"}, {"instruction": "def decode_solution(self, encoded_solution):\n        \"\"\"Return solution from an encoded representation.\"\"\"\n", "input": "", "output": "        return self._decode_function(encoded_solution, *self._decode_args,\n                                     **self._decode_kwargs)", "category": "Python"}, {"instruction": "def get_area_dates(bbox, date_interval, maxcc=None):\n    \"\"\" Get list of times of existing images from specified area and time range\n\n    :param bbox: bounding box of requested area\n    :type bbox: geometry.BBox\n    :param date_interval: a pair of time strings in ISO8601 format\n    :type date_interval: tuple(str)\n    :param maxcc: filter images by maximum percentage of cloud coverage\n    :type maxcc: float in range [0, 1] or None\n    :return: list of time strings in ISO8601 format\n    :rtype: list[datetime.datetime]\n    \"\"\"\n", "input": "", "output": "\n    area_info = get_area_info(bbox, date_interval, maxcc=maxcc)\n    return sorted({datetime.datetime.strptime(tile_info['properties']['startDate'].strip('Z'),\n                                              '%Y-%m-%dT%H:%M:%S')\n                   for tile_info in area_info})", "category": "Python"}, {"instruction": "def _compute_product(map1, map2):\n        \"\"\" Make a map that is the product of two maps\n        \"\"\"\n", "input": "", "output": "        data = map1.data * map2.data\n        return HpxMap(data, map1.hpx)", "category": "Python"}, {"instruction": "def mthread_submit(nslave, worker_args, worker_envs):\n    \"\"\"\n      customized submit script, that submit nslave jobs, each must contain args as parameter\n      note this can be a lambda function containing additional parameters in input\n      Parameters\n         nslave number of slave process to start up\n         args arguments to launch each job\n              this usually includes the parameters of master_uri and parameters passed into submit\n    \"\"\"\n", "input": "", "output": "    procs = {}\n    for i in range(nslave):\n        procs[i] = Thread(target = exec_cmd, args = (args.command + worker_args, i, worker_envs))\n        procs[i].daemon = True\n        procs[i].start()\n    for i in range(nslave):\n        procs[i].join()", "category": "Python"}, {"instruction": "def get_methods(extension_name):\n    \"\"\" Return all methods in extension that have nago_access set \"\"\"\n", "input": "", "output": "    extension = get_extension(extension_name)\n    methods = {}\n    for name, i in inspect.getmembers(extension):\n        if hasattr(i, 'nago_access'):\n            api_name = i.nago_name\n            methods[api_name] = i\n    return methods", "category": "Python"}, {"instruction": "def mksls(fmt, src, dst=None):\n    '''\n    Convert an installation file/script to an SLS file. Currently supports\n    ``kickstart``, ``preseed``, and ``autoyast``.\n\n    CLI Examples:\n\n        salt <minion> genesis.mksls kickstart /path/to/kickstart.cfg\n        salt <minion> genesis.mksls kickstart /path/to/kickstart.cfg /path/to/dest.sls\n\n    .. versionadded:: Beryllium\n    '''\n", "input": "", "output": "    if fmt == 'kickstart':\n        return salt.utils.kickstart.mksls(src, dst)\n    elif fmt == 'preseed':\n        return salt.utils.preseed.mksls(src, dst)\n    elif fmt == 'autoyast':\n        return salt.utils.yast.mksls(src, dst)", "category": "Python"}, {"instruction": "async def download_media_by_id(self, media_id):\n        \"\"\"Given a message ID, finds the media this message contained and\n           downloads it.\n        \"\"\"\n", "input": "", "output": "        try:\n            msg = self.found_media[int(media_id)]\n        except (ValueError, KeyError):\n            # ValueError when parsing, KeyError when accessing dictionary\n            print('Invalid media ID given or message not found!')\n            return\n\n        print('Downloading media to usermedia/...')\n        os.makedirs('usermedia', exist_ok=True)\n        output = await self.download_media(\n            msg.media,\n            file='usermedia/',\n            progress_callback=self.download_progress_callback\n        )\n        print('Media downloaded to {}!'.format(output))", "category": "Python"}, {"instruction": "def output_results(results, split_id='results', output_stream=None):\n    '''\n    Log `results` readably to `output_stream`, with a header\n    containing `split_id`.\n\n    :param results: a dictionary of summary statistics from an evaluation\n    :type results: dict(str -> object)\n\n    :param str split_id: an identifier for the source of `results` (e.g. 'dev')\n\n    :param file output_stream: the file-like object to which to log the results\n        (default: stdout)\n    :type split_id: str\n    '''\n", "input": "", "output": "    if output_stream is None:\n        output_stream = sys.stdout\n\n    output_stream.write('----- %s -----\\n' % split_id)\n    for name in sorted(results.keys()):\n        output_stream.write('%s: %s\\n' % (name, repr(results[name])))\n\n    output_stream.flush()", "category": "Python"}, {"instruction": "def colAdd(self,name=\"\",desc=\"\",unit=\"\",comment=\"\",coltype=0,data=[],pos=None):\n        \"\"\"\n        column types:\n            0: Y\n            1: Disregard\n            2: Y Error\n            3: X\n            4: Label\n            5: Z\n            6: X Error\n        \"\"\"\n", "input": "", "output": "        if pos is None:\n            pos=len(self.colNames)\n        self.colNames.insert(pos,name)\n        self.colDesc.insert(pos,desc)\n        self.colUnits.insert(pos,unit)\n        self.colComments.insert(pos,comment)\n        self.colTypes.insert(pos,coltype)\n        self.colData.insert(pos,data)\n        return", "category": "Python"}, {"instruction": "def get_name(self, name_case=DdlParseBase.NAME_CASE.original):\n        \"\"\"\n        Get Name converted case\n\n        :param name_case: name case type\n            * DdlParse.NAME_CASE.original : Return to no convert\n            * DdlParse.NAME_CASE.lower : Return to lower\n            * DdlParse.NAME_CASE.upper : Return to upper\n\n        :return: name\n        \"\"\"\n", "input": "", "output": "        if name_case == self.NAME_CASE.lower:\n            return self._name.lower()\n        elif name_case == self.NAME_CASE.upper:\n            return self._name.upper()\n        else:\n            return self._name", "category": "Python"}, {"instruction": "def mouseReleaseEvent(self, event):\n        \"\"\"\n        Reimplements the :meth:`QLabel.mouseReleaseEvent` method.\n\n        :param event: QEvent.\n        :type event: QEvent\n        \"\"\"\n", "input": "", "output": "\n        if self.underMouse():\n            if self.__checkable:\n                self.set_checked(not self.__checked)\n            else:\n                self.setPixmap(self.__active_pixmap)\n        else:\n            self.setPixmap(self.__default_pixmap)\n        self.released.emit()\n        self.clicked.emit()", "category": "Python"}, {"instruction": "def init_mpraw(mpv, npv):\n    \"\"\"Set a global variable as a multiprocessing RawArray in shared\n    memory with a numpy array wrapper and initialise its value.\n\n    Parameters\n    ----------\n    mpv : string\n      Name of global variable to set\n    npv : ndarray\n      Numpy array to use as initialiser for global variable value\n    \"\"\"\n", "input": "", "output": "\n    globals()[mpv] = mpraw_as_np(npv.shape, npv.dtype)\n    globals()[mpv][:] = npv", "category": "Python"}, {"instruction": "def now(utc=False):\n    \"\"\"Returns the current time.\n\n    :param utc: If ``True``, returns a timezone-aware ``datetime`` object in\n       UTC.  When ``False`` (the default), returns a naive ``datetime`` object\n       in local time.\n    :return: A ``datetime`` object representing the current time at the time of\n       the call.\n    \"\"\"\n", "input": "", "output": "    if utc:\n        return datetime.datetime.utcnow().replace(tzinfo=dateutil.tz.tzutc())\n    else:\n        return datetime.datetime.now()", "category": "Python"}, {"instruction": "def remove(self, name_or_klass):\n        \"\"\"\n        Remove a extension from the editor.\n\n        :param name_or_klass: The name (or class) of the extension to remove.\n        :returns: The removed extension.\n        \"\"\"\n", "input": "", "output": "        logger.debug('removing extension {}'.format(name_or_klass))\n        extension = self.get(name_or_klass)\n        extension.on_uninstall()\n        self._extensions.pop(extension.name)\n        return extension", "category": "Python"}, {"instruction": "def get_list(self, name, default=None):\n        \"\"\"Retrieves an environment variable as a list.\n\n        Note that while implicit access of environment variables\n        containing tuples will return tuples, using this method will\n        coerce tuples to lists.\n\n        Args:\n            name (str): The case-insensitive, unprefixed variable name.\n            default: If provided, a default value will be returned\n                instead of throwing ``EnvironmentError``.\n\n        Returns:\n            list: The environment variable's value as a list.\n\n        Raises:\n            EnvironmentError: If the environment variable does not\n                exist, and ``default`` was not provided.\n            ValueError: If the environment variable value is not an\n                integer with base 10.\n\n        \"\"\"\n", "input": "", "output": "        if name not in self:\n            if default is not None:\n                return default\n            raise EnvironmentError.not_found(self._prefix, name)\n        return list(self[name])", "category": "Python"}, {"instruction": "def unregister(name, server_url):\n    '''\n    Unregister specified server from Spacewalk\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run spacewalk.unregister my-test-vm spacewalk01.domain.com\n    '''\n", "input": "", "output": "\n    try:\n        client, key = _get_session(server_url)\n    except Exception as exc:\n        err_msg = 'Exception raised when connecting to spacewalk server ({0}): {1}'.format(server_url, exc)\n        log.error(err_msg)\n        return {name: err_msg}\n\n    systems_list = client.system.getId(key, name)\n\n    if systems_list:\n        for system in systems_list:\n            out = client.system.deleteSystem(key, system['id'])\n            if out == 1:\n                return {name: 'Successfully unregistered from {0}'.format(server_url)}\n            else:\n                return {name: 'Failed to unregister from {0}'.format(server_url)}\n    else:\n        return {name: 'System does not exist in spacewalk server ({0})'.format(server_url)}", "category": "Python"}, {"instruction": "def get_commands(self):\n        \"\"\"\n        Returns commands available to execute\n        :return: list of (name, doc) tuples\n        \"\"\"\n", "input": "", "output": "        commands = []\n        for name, value in inspect.getmembers(self):\n            if not inspect.isgeneratorfunction(value):\n                continue\n            if name.startswith('_') or name == 'run':\n                continue\n            doc = inspect.getdoc(value)\n            commands.append((name, doc))\n        return commands", "category": "Python"}, {"instruction": "def _check_warnings(self, json_response):\n        ''' Extract warnings from the response to make them accessible\n\n        Args:\n            json_response (dict): JSON response\n        \n        '''\n", "input": "", "output": "\n        self.warnings = None\n        if json_response:\n            self.warnings = json_response.get('warnings')\n\n        if self.debug and self.warnings:\n            for w in self.warnings:\n                print(\"WARNING: %s - %s\" % (w['warning_name'], w['warning_msg']))", "category": "Python"}, {"instruction": "def sinatra_path_to_regex(cls, path):\n        \"\"\"\n        Converts a sinatra-style path to a regex with named\n        parameters.\n        \"\"\"\n", "input": "", "output": "        # Return the path if already a (compiled) regex\n        if type(path) is cls.regex_type:\n            return path\n\n        # Build a regular expression string which is split on the '/' character\n        regex = [\n            \"(?P<{}>\\w+)\".format(segment[1:])\n            if cls.sinatra_param_regex.match(segment)\n            else segment\n            for segment in path.split('/')\n        ]\n        return re.compile('/'.join(regex))", "category": "Python"}, {"instruction": "def _handle_decoded_payload(self, data):\n        '''\n        Override this method if you wish to handle the decoded data\n        differently.\n        '''\n", "input": "", "output": "        # TODO: even do this??\n        data['to'] = int(data.get('to', self.opts['timeout'])) - 1\n        # Only forward the command if it didn't originate from ourselves\n        if data.get('master_id', 0) != self.opts.get('master_id', 1):\n            self.syndic_cmd(data)", "category": "Python"}, {"instruction": "def pack_req(cls, modify_order_op, order_id, price, qty,\n                 adjust_limit, trd_env, acc_id, trd_mkt, conn_id):\n        \"\"\"Convert from user request for place order to PLS request\"\"\"\n", "input": "", "output": "        from futuquant.common.pb.Trd_ModifyOrder_pb2 import Request\n        req = Request()\n        serial_no = get_unique_id32()\n        req.c2s.packetID.serialNo = serial_no\n        req.c2s.packetID.connID = conn_id\n\n        req.c2s.header.trdEnv = TRD_ENV_MAP[trd_env]\n        req.c2s.header.accID = acc_id\n        req.c2s.header.trdMarket = TRD_MKT_MAP[trd_mkt]\n\n        req.c2s.orderID = int(order_id)\n        req.c2s.modifyOrderOp = MODIFY_ORDER_OP_MAP[modify_order_op]\n        req.c2s.forAll = False\n\n        if modify_order_op == ModifyOrderOp.NORMAL:\n            req.c2s.qty = qty\n            req.c2s.price = price\n            req.c2s.adjustPrice = adjust_limit != 0\n            req.c2s.adjustSideAndLimit = adjust_limit\n\n        return pack_pb_req(req, ProtoId.Trd_ModifyOrder, conn_id, serial_no)", "category": "Python"}, {"instruction": "def get_status(self):\n        \"\"\"\n        :calls: `GET /user/migrations/:migration_id`_\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        headers, data = self._requester.requestJsonAndCheck(\n            \"GET\",\n            self.url,\n            headers={\n                \"Accept\": Consts.mediaTypeMigrationPreview\n            }\n        )\n        self._useAttributes(data)\n        return self.state", "category": "Python"}, {"instruction": "def logical_chassis_fwdl_status_output_cluster_fwdl_entries_fwdl_entries_blade_slot(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        logical_chassis_fwdl_status = ET.Element(\"logical_chassis_fwdl_status\")\n        config = logical_chassis_fwdl_status\n        output = ET.SubElement(logical_chassis_fwdl_status, \"output\")\n        cluster_fwdl_entries = ET.SubElement(output, \"cluster-fwdl-entries\")\n        fwdl_entries = ET.SubElement(cluster_fwdl_entries, \"fwdl-entries\")\n        blade_slot = ET.SubElement(fwdl_entries, \"blade-slot\")\n        blade_slot.text = kwargs.pop('blade_slot')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def make_password(password, salt=None, hasher='default'):\n    \"\"\"\n    Turn a plain-text password into a hash for database storage\n    Same as encode() but generate a new random salt. If password is None then\n    return a concatenation of UNUSABLE_PASSWORD_PREFIX and a random string,\n    which disallows logins. Additional random string reduces chances of gaining\n    access to staff or superuser accounts. See ticket #20079 for more info.\n    \"\"\"\n", "input": "", "output": "    if password is None:\n        return UNUSABLE_PASSWORD_PREFIX + get_random_string(UNUSABLE_PASSWORD_SUFFIX_LENGTH)\n    hasher = bCryptPasswordHasher\n\n    if not salt:\n        salt = hasher.salt()\n\n    return hasher.encode(password, salt)", "category": "Python"}, {"instruction": "def editor_js_initialization(selector, **extra_settings):\n    \"\"\" Return script tag with initialization code. \"\"\"\n", "input": "", "output": "\n    init_template = loader.get_template(\n        settings.MARKDOWN_EDITOR_INIT_TEMPLATE)\n\n    options = dict(\n        previewParserPath=reverse('django_markdown_preview'),\n        **settings.MARKDOWN_EDITOR_SETTINGS)\n    options.update(extra_settings)\n    ctx = dict(\n        selector=selector,\n        extra_settings=simplejson.dumps(options)\n    )\n    return init_template.render(ctx)", "category": "Python"}, {"instruction": "def get_subject_with_file_validation(jwt_bu64, cert_path):\n    \"\"\"Same as get_subject_with_local_validation() except that the signing certificate\n    is read from a local PEM file.\"\"\"\n", "input": "", "output": "    cert_obj = d1_common.cert.x509.deserialize_pem_file(cert_path)\n    return get_subject_with_local_validation(jwt_bu64, cert_obj)", "category": "Python"}, {"instruction": "def gut_message(message: Message) -> Message:\n    \"\"\"\n    Remove body from a message, and wrap in a message/external-body.\n    \"\"\"\n", "input": "", "output": "    wrapper = Message()\n    wrapper.add_header('Content-Type', 'message/external-body',\n                       access_type='x-spam-deleted',\n                       expiration=time.strftime(\"%a, %d %b %Y %H:%M:%S %z\"),\n                       size=str(len(message.get_payload())))\n\n    message.set_payload('')\n    wrapper.set_payload([message])\n\n    return wrapper", "category": "Python"}, {"instruction": "def tacacs_server_host_hostname(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        tacacs_server = ET.SubElement(config, \"tacacs-server\", xmlns=\"urn:brocade.com:mgmt:brocade-aaa\")\n        host = ET.SubElement(tacacs_server, \"host\")\n        hostname = ET.SubElement(host, \"hostname\")\n        hostname.text = kwargs.pop('hostname')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def adjacent(self, node_a, node_b):\n        \"\"\"Determines whether there is an edge from node_a to node_b.\n        Returns True if such an edge exists, otherwise returns False.\"\"\"\n", "input": "", "output": "        neighbors = self.neighbors(node_a)\n        return node_b in neighbors", "category": "Python"}, {"instruction": "def in_single_path_and_inner(self):\n        \"\"\"\n        Test if a node is not linking to any fan in or out node.\n        \"\"\"\n", "input": "", "output": "        return len(self.successor) == 1 and self.successor[0] is not None and not self.successor[0].in_or_out and \\\n               len(self.precedence) == 1 and self.precedence[0] is not None and not self.successor[0].in_or_out", "category": "Python"}, {"instruction": "def contains_relevant_concept(\n    s: Influence, relevant_concepts: List[str], cutoff=0.7\n) -> bool:\n    \"\"\" Returns true if a given Influence statement has a relevant concept, and\n    false otherwise. \"\"\"\n", "input": "", "output": "\n    return any(\n        map(lambda c: contains_concept(s, c, cutoff=cutoff), relevant_concepts)\n    )", "category": "Python"}, {"instruction": "def parse_block_scalar_empty_line(indent_token_class, content_token_class):\n        \"\"\"Process an empty line in a block scalar.\"\"\"\n", "input": "", "output": "        def callback(lexer, match, context):\n            text = match.group()\n            if (context.block_scalar_indent is None or\n                    len(text) <= context.block_scalar_indent):\n                if text:\n                    yield match.start(), indent_token_class, text\n            else:\n                indentation = text[:context.block_scalar_indent]\n                content = text[context.block_scalar_indent:]\n                yield match.start(), indent_token_class, indentation\n                yield (match.start()+context.block_scalar_indent,\n                       content_token_class, content)\n            context.pos = match.end()\n        return callback", "category": "Python"}, {"instruction": "def _depth_limited_walk(top, max_depth=None):\n    '''\n    Walk the directory tree under root up till reaching max_depth.\n    With max_depth=None (default), do not limit depth.\n    '''\n", "input": "", "output": "    for root, dirs, files in salt.utils.path.os_walk(top):\n        if max_depth is not None:\n            rel_depth = root.count(os.path.sep) - top.count(os.path.sep)\n            if rel_depth >= max_depth:\n                del dirs[:]\n        yield (six.text_type(root), list(dirs), list(files))", "category": "Python"}, {"instruction": "def getOrCreate(cls, sc):\n        \"\"\"\n        Get the existing SQLContext or create a new one with given SparkContext.\n\n        :param sc: SparkContext\n        \"\"\"\n", "input": "", "output": "        if cls._instantiatedContext is None:\n            jsqlContext = sc._jvm.SQLContext.getOrCreate(sc._jsc.sc())\n            sparkSession = SparkSession(sc, jsqlContext.sparkSession())\n            cls(sc, sparkSession, jsqlContext)\n        return cls._instantiatedContext", "category": "Python"}, {"instruction": "def setLabel( self, text ):\n        \"\"\"\n        Sets the label for this widget to the inputed text.  Differs per type.\n        \n        :param      text | <str>\n        \"\"\"\n", "input": "", "output": "        if ( self._editor and hasattr(self._editor, 'setLabel') ):\n            self._editor.setLabel(text)\n            return True\n        return False", "category": "Python"}, {"instruction": "def register_shortcut(self, qaction_or_qshortcut, context, name,\n                          add_sc_to_tip=False):\n        \"\"\"\n        Register QAction or QShortcut to Spyder main application.\n\n        if add_sc_to_tip is True, the shortcut is added to the\n        action's tooltip\n        \"\"\"\n", "input": "", "output": "        self.main.register_shortcut(qaction_or_qshortcut, context,\n                                    name, add_sc_to_tip)", "category": "Python"}, {"instruction": "def transform_around(matrix, point):\n    \"\"\"\n    Given a transformation matrix, apply its rotation\n    around a point in space.\n\n    Parameters\n    ----------\n    matrix: (4,4) or (3, 3) float, transformation matrix\n    point:  (3,) or (2,)  float, point in space\n\n    Returns\n    ---------\n    result: (4,4) transformation matrix\n    \"\"\"\n", "input": "", "output": "    point = np.asanyarray(point)\n    matrix = np.asanyarray(matrix)\n    dim = len(point)\n    if matrix.shape != (dim + 1,\n                        dim + 1):\n        raise ValueError('matrix must be (d+1, d+1)')\n\n    translate = np.eye(dim + 1)\n    translate[:dim, dim] = -point\n    result = np.dot(matrix, translate)\n    translate[:dim, dim] = point\n    result = np.dot(translate, result)\n\n    return result", "category": "Python"}, {"instruction": "def compile_bundle_entry(self, spec, entry):\n        \"\"\"\n        Handler for each entry for the bundle method of the compile\n        process.  This copies the source file or directory into the\n        build directory.\n        \"\"\"\n", "input": "", "output": "\n        modname, source, target, modpath = entry\n        bundled_modpath = {modname: modpath}\n        bundled_target = {modname: target}\n        export_module_name = []\n        if isfile(source):\n            export_module_name.append(modname)\n            copy_target = join(spec[BUILD_DIR], target)\n            if not exists(dirname(copy_target)):\n                makedirs(dirname(copy_target))\n            shutil.copy(source, copy_target)\n        elif isdir(source):\n            copy_target = join(spec[BUILD_DIR], modname)\n            shutil.copytree(source, copy_target)\n\n        return bundled_modpath, bundled_target, export_module_name", "category": "Python"}, {"instruction": "def initialize_weights(self):\n        \"\"\"Randomly initializes the visible-to-hidden connections.\"\"\"\n", "input": "", "output": "        n = self._outputSize\n        m = self._inputSize\n        self._Q = self._random.sample((n,m))\n\n        # Normalize the weights of each units\n        for i in range(n):\n            self._Q[i] /=  np.sqrt( np.dot(self._Q[i], self._Q[i]) )", "category": "Python"}, {"instruction": "def accounts(self):\n        \"\"\"\n        Return an account reference\n        :param account_id:\n        :param accounts_password: The password for decrypting the secret\n        :return:\n        \"\"\"\n", "input": "", "output": "        d = {}\n\n        if False and not self._account_password:\n            from ambry.dbexceptions import ConfigurationError\n            raise ConfigurationError(\n                \"Can't access accounts without setting an account password\"\n                \" either in the accounts.password config, or in the AMBRY_ACCOUNT_PASSWORD\"\n                \" env var.\")\n\n        for act in self.database.session.query(Account).all():\n            if self._account_password:\n                act.secret_password = self._account_password\n            e = act.dict\n            a_id = e['account_id']\n            d[a_id] = e\n\n        return d", "category": "Python"}, {"instruction": "def get_arg_descriptions_from_docstring(cls, obj):\n    \"\"\"Returns an ordered map of arg name -> arg description found in :param: stanzas.\"\"\"\n", "input": "", "output": "\n    ret = OrderedDict()\n    name = ''\n    doc = obj.__doc__ or ''\n    lines = [s.strip() for s in doc.split('\\n')]\n    stanza_first_line_re = cls._get_stanza_first_line_re()\n    for line in lines:\n      m = stanza_first_line_re.match(line)\n      if m and m.group(1) == 'param':\n        # If first line of a parameter description, set name and description.\n        name, description = m.group(3, 4)\n        ret[name] = description\n      elif m and m.group(1) != 'param':\n        # If first line of a description of an item other than a parameter, clear name.\n        name = ''\n      elif name and line:\n        # If subsequent line of a parameter description, add to existing description (if any) for\n        # that parameter.\n        ret[name] += (' ' + line) if ret[name] else line\n      # Ignore subsequent lines of descriptions of items other than parameters.\n    return ret", "category": "Python"}, {"instruction": "def generator(self, size=1000, tree_depth=1):\n        \"\"\" Creates a random #generator\n\n            @size: #int number of random values to include in each @tree_depth\n            @tree_depth: #int dict tree dimensions size, i.e.\n                1=|(value1, value2)|\n                2=|((value1, value2), (value1, value2))|\n\n            -> random :class:collections.deque\n        \"\"\"\n", "input": "", "output": "        if not tree_depth:\n            return self._map_type()\n        return (self.generator(size, tree_depth-1) for x in range(size))", "category": "Python"}, {"instruction": "def to_json(self):\r\n        \"\"\"Create a property dict that is used to recreate an edge dictionary for a :class:`BELGraph`.\r\n\r\n        :return: Property dictionary of an edge that is participant (sub/obj) related.\r\n        :rtype: dict\r\n        \"\"\"\n", "input": "", "output": "        participant = self.side\r\n\r\n        prop_dict = {\r\n            participant: {\r\n                MODIFIER: self.modifier  # FIXME this is probably wrong for location\r\n            }\r\n        }\r\n\r\n        if self.modifier == LOCATION:\r\n            prop_dict[participant] = {\r\n                LOCATION: self.effect.to_json()\r\n            }\r\n        if self.relative_key:  # for translocations\r\n            prop_dict[participant][EFFECT] = {\r\n                self.relative_key: self.effect.to_json()\r\n            }\r\n        elif self.effect:  # for activities\r\n            prop_dict[participant][EFFECT] = self.effect.to_json()\r\n\r\n        # degradations don't have modifications\r\n\r\n        return prop_dict", "category": "Python"}, {"instruction": "def set_figure_size(fig, width, height):\r\n    \"\"\"Sets MatPlotLib figure width and height in pixels\r\n\r\n    Reference: https://github.com/matplotlib/matplotlib/issues/2305/\r\n    \"\"\"\n", "input": "", "output": "    dpi = float(fig.get_dpi())\r\n    fig.set_size_inches(float(width) / dpi, float(height) / dpi)", "category": "Python"}, {"instruction": "def _get_disk_size(self, device):\n        '''\n        Get a size of a disk.\n        '''\n", "input": "", "output": "        out = __salt__['cmd.run_all'](\"df {0}\".format(device))\n        if out['retcode']:\n            msg = \"Disk size info error: {0}\".format(out['stderr'])\n            log.error(msg)\n            raise SIException(msg)\n\n        devpath, blocks, used, available, used_p, mountpoint = [elm for elm in\n                                                                out['stdout'].split(os.linesep)[-1].split(\" \") if elm]\n        return {\n            'device': devpath, 'blocks': blocks, 'used': used,\n            'available': available, 'used (%)': used_p, 'mounted': mountpoint,\n        }", "category": "Python"}, {"instruction": "def _pipeline_needs_fastq(config, data):\n    \"\"\"Determine if the pipeline can proceed with a BAM file, or needs fastq conversion.\n    \"\"\"\n", "input": "", "output": "    aligner = config[\"algorithm\"].get(\"aligner\")\n    support_bam = aligner in alignment.metadata.get(\"support_bam\", [])\n    return aligner and not support_bam", "category": "Python"}, {"instruction": "def play_list_detail(id, limit=20):\n    \"\"\"\u83b7\u53d6\u6b4c\u5355\u4e2d\u7684\u6240\u6709\u97f3\u4e50\u3002\u7531\u4e8e\u83b7\u53d6\u7cbe\u54c1\u4e2d\uff0c\u53ea\u80fd\u770b\u5230\u6b4c\u5355\u540d\u5b57\u548c ID \u5e76\u6ca1\u6709\u6b4c\u5355\u7684\u97f3\u4e50\uff0c\u56e0\u6b64\u589e\u52a0\u8be5\u63a5\u53e3\u4f20\u5165\u6b4c\u5355 ID\n    \u83b7\u53d6\u6b4c\u5355\u4e2d\u7684\u6240\u6709\u97f3\u4e50.\n\n    :param id: \u6b4c\u5355\u7684ID\n    :param limit: (optional) \u6570\u636e\u4e0a\u9650\u591a\u5c11\u884c\uff0c\u9ed8\u8ba4 20\n    \"\"\"\n", "input": "", "output": "    if id is None:\n        raise ParamsError()\n    r = NCloudBot()\n    r.method = 'PLAY_LIST_DETAIL'\n    r.data = {'id': id, 'limit': limit, \"csrf_token\": \"\"}\n    r.send()\n\n    return r.response", "category": "Python"}, {"instruction": "def is_member_of(self, group_name):\n        \"\"\"Return True if member of LDAP group, otherwise return False\"\"\"\n", "input": "", "output": "        group_dn = 'cn=%s,cn=groups,cn=accounts,%s' % (group_name, self._base_dn)\n        if str(group_dn).lower() in [str(i).lower() for i in self.member_of]:\n            return True\n        else:\n            return False", "category": "Python"}, {"instruction": "def update(self, job_id, sql_query):\n        \"\"\"\n        Updates the sql query of a specific job\n\n        :param job_id: The id of the job to be updated\n        :param sql_query: The new SQL query for the job\n        :type job_id: str\n        :type sql_query: str\n\n        :return: Response data, either as json or as a regular response.content\n                    object\n        :rtype: object\n\n        :raise: CartoException\n        \"\"\"\n", "input": "", "output": "        header = {'content-type': 'application/json'}\n        data = self.send(self.api_url + job_id,\n                         http_method=\"PUT\",\n                         json_body={\"query\": sql_query},\n                         http_header=header)\n        return data", "category": "Python"}, {"instruction": "def fitToSize(rect, targetWidth, targetHeight, bounds):\n\t\"\"\"\n\tPads or crops a rectangle as necessary to achieve the target dimensions,\n\tensuring the modified rectangle falls within the specified bounds.\n\t\n\tThe input rectangle, bounds, and return value are all a tuple of (x,y,w,h).\n\t\"\"\"\n", "input": "", "output": "\t\n\t# Determine the difference between the current size and target size\n\tx,y,w,h = rect\n\tdiffX = w - targetWidth\n\tdiffY = h - targetHeight\n\t\n\t# Determine if we are cropping or padding the width\n\tif diffX > 0:\n\t\tcropLeft  = math.floor(diffX / 2)\n\t\tcropRight = diffX - cropLeft\n\t\tx,y,w,h   = cropRect((x,y,w,h), 0, 0, cropLeft, cropRight)\n\telif diffX < 0:\n\t\tpadLeft  = math.floor(abs(diffX) / 2)\n\t\tpadRight = abs(diffX) - padLeft\n\t\tx,y,w,h  = padRect((x,y,w,h), 0, 0, padLeft, padRight, bounds, False)\n\t\n\t# Determine if we are cropping or padding the height\n\tif diffY > 0:\n\t\tcropTop    = math.floor(diffY / 2)\n\t\tcropBottom = diffY - cropTop\n\t\tx,y,w,h    = cropRect((x,y,w,h), cropTop, cropBottom, 0, 0)\n\telif diffY < 0:\n\t\tpadTop    = math.floor(abs(diffY) / 2)\n\t\tpadBottom = abs(diffY) - padTop\n\t\tx,y,w,h   = padRect((x,y,w,h), padTop, padBottom, 0, 0, bounds, False)\n\t\n\treturn (x,y,w,h)", "category": "Python"}, {"instruction": "def print_hex(self, value, justify_right=True):\n        \"\"\"Print a numeric value in hexadecimal.  Value should be from 0 to FFFF.\n        \"\"\"\n", "input": "", "output": "        if value < 0 or value > 0xFFFF:\n            # Ignore out of range values.\n            return\n        self.print_number_str('{0:X}'.format(value), justify_right)", "category": "Python"}, {"instruction": "def _parse_dict(self, data):\n\t\t\"\"\"Helper function to parse blocks of GNTP headers into a dictionary\n\n\t\t:param string data:\n\t\t:return dict: Dictionary of parsed GNTP Headers\n\t\t\"\"\"\n", "input": "", "output": "\t\td = {}\n\t\tfor line in data.split('\\r\\n'):\n\t\t\tmatch = GNTP_HEADER.match(line)\n\t\t\tif not match:\n\t\t\t\tcontinue\n\n\t\t\tkey = match.group(1).strip()\n\t\t\tval = match.group(2).strip()\n\t\t\td[key] = val\n\t\treturn d", "category": "Python"}, {"instruction": "def setComponent(self, component):\n        \"\"\"Re-implemented from :meth:`AbstractComponentWidget<sparkle.gui.stim.abstract_component_editor.AbstractComponentWidget.setComponent>`\"\"\"\n", "input": "", "output": "        details = component.auto_details()\n        state = component.stateDict()\n        for field, detail in details.items():\n            val = state[field]\n            self.inputWidgets[field].setValue(val)\n        self._component = component", "category": "Python"}, {"instruction": "def _ExecuteTransaction(self, transaction):\n    \"\"\"Get connection from pool and execute transaction.\"\"\"\n", "input": "", "output": "\n    def Action(connection):\n      connection.cursor.execute(\"START TRANSACTION\")\n      for query in transaction:\n        connection.cursor.execute(query[\"query\"], query[\"args\"])\n      connection.cursor.execute(\"COMMIT\")\n      return connection.cursor.fetchall()\n\n    return self._RetryWrapper(Action)", "category": "Python"}, {"instruction": "def underline(self, msg):\n        \"\"\"Underline the input\"\"\"\n", "input": "", "output": "        return click.style(msg, underline=True) if self.colorize else msg", "category": "Python"}, {"instruction": "def interface(self):\n        \"\"\"\n        Public method for handling service command argument.\n        \"\"\"\n", "input": "", "output": "        \n        # Possible control arguments\n        controls = {\n            'start': self.do_start,\n            'stop': self.do_stop,\n            'status': self.do_status,\n            'restart': self.do_restart,\n            'reload': self.do_restart    \n        }\n        \n        # Process the control argument\n        try:\n            controls[self.command]()\n        except KeyError:\n            self.write_stdout('Usage: {} {{start|stop|status|restart|reload}}'.format(self.name), 3)\n        exit(0)", "category": "Python"}, {"instruction": "def pdist(objects, dmeasure, diagval = numpy.inf):\n    \"\"\"\n    Compute the pair-wise distances between arbitrary objects.\n    \n    Notes\n    -----\n    ``dmeasure`` is assumed to be *symmetry* i.e. between object *a* and object *b* the\n    function will be called only ones.\n    \n    Parameters\n    ----------\n    objects : sequence\n        A sequence of objects of length *n*.\n    dmeasure : function\n        A callable function that takes two objects as input at returns a number.\n    diagval : number\n        The diagonal values of the resulting array.\n        \n    Returns\n    -------\n    pdists : ndarray\n        An *nxn* symmetric float array containing the pair-wise distances.\n    \"\"\"\n", "input": "", "output": "    out = numpy.zeros([len(objects)] * 2, numpy.float)\n    numpy.fill_diagonal(out, diagval)\n    for idx1, idx2 in combinations(list(range(len(objects))), 2):\n        out[idx1, idx2] = dmeasure(objects[idx1], objects[idx2])\n    return out + out.T", "category": "Python"}, {"instruction": "def including(self, sequence) -> Generator:\n        \"\"\"Include the sequence elements matching the filter set.\"\"\"\n", "input": "", "output": "        return (element for element in sequence\n                if self.indexer(element) in self.predicates)", "category": "Python"}, {"instruction": "def view_announcement_view(request, id):\n    \"\"\"View an announcement.\n\n    id: announcement id\n\n    \"\"\"\n", "input": "", "output": "    announcement = get_object_or_404(Announcement, id=id)\n\n    return render(request, \"announcements/view.html\", {\"announcement\": announcement})", "category": "Python"}, {"instruction": "def _notify(self, log_level, message):\n        \"\"\"\n        Calls the callback function and logs messages using the PySDK logger\n        :param log_level: An integer representing the log level, as specified\n                          in the Python `logging` library\n        :param message:   The actual message to be sent to the logger and the\n                          `callback` function\n        \"\"\"\n", "input": "", "output": "        timestamp = datetime.datetime.utcnow()\n        logger.log(log_level, str(message))\n        try:\n            self._callback(log_level, message, timestamp)\n        except Exception as ex:\n            logger.warning(consts.LOG_MSG_CALLBACK_FAILURE % str(ex))", "category": "Python"}, {"instruction": "def load_xml_conf(self, xml_file, id):\n        '''\n        Creates a new config from xml file.\n        :param xml_file: path to xml file. Format : nutch-site.xml or nutch-default.xml\n        :param id:\n        :return: config object\n        '''\n", "input": "", "output": "\n        # converting nutch-site.xml to key:value pairs\n        import xml.etree.ElementTree as ET\n        tree = ET.parse(xml_file)\n        params = {}\n        for prop in tree.getroot().findall(\".//property\"):\n            params[prop.find('./name').text.strip()] = prop.find('./value').text.strip()\n        return self.proxy.Configs().create(id, configData=params)", "category": "Python"}, {"instruction": "def identifier(self):\n        \"\"\"Set the identifier of the current MOC.\n\n        The new identifier should be given after this option.\n\n        ::\n\n            pymoctool ... --id 'New MOC identifier' --output new_moc.fits\n        \"\"\"\n", "input": "", "output": "\n        if self.moc is None:\n            self.moc = MOC()\n\n        self.moc.id = self.params.pop()", "category": "Python"}, {"instruction": "def image_click_yshift(axes = \"gca\"):\n    \"\"\"\n    Takes a starting and ending point, then shifts the image y by this amount\n    \"\"\"\n", "input": "", "output": "    if axes == \"gca\": axes = _pylab.gca()\n\n    try:\n        p1 = _pylab.ginput()\n        p2 = _pylab.ginput()\n\n        yshift = p2[0][1]-p1[0][1]\n\n        e = axes.images[0].get_extent()\n\n        e[2] = e[2] + yshift\n        e[3] = e[3] + yshift\n\n        axes.images[0].set_extent(e)\n\n        _pylab.draw()\n    except:\n        print(\"whoops\")", "category": "Python"}, {"instruction": "async def get_users(self):\n        \"\"\"Get Tautulli users.\"\"\"\n", "input": "", "output": "        cmd = 'get_users'\n        url = self.base_url + cmd\n        users = []\n        try:\n            async with async_timeout.timeout(8, loop=self._loop):\n                response = await self._session.get(url)\n\n            logger(\"Status from Tautulli: \" + str(response.status))\n            all_user_data = await response.json()\n            for user in all_user_data['response']['data']:\n                if user['username'] != 'Local':\n                    users.append(user['username'])\n            self.tautulli_users = users\n            logger(self.tautulli_users)\n\n        except (asyncio.TimeoutError, aiohttp.ClientError, socket.gaierror,\n                AttributeError) as error:\n            msg = \"Can not load data from Tautulli: {} - {}\".format(url, error)\n            logger(msg, 40)", "category": "Python"}, {"instruction": "def toRanks(A):\n    \"\"\"\n    converts the columns of A to ranks\n    \"\"\"\n", "input": "", "output": "    AA=sp.zeros_like(A)\n    for i in range(A.shape[1]):\n        AA[:,i] = st.rankdata(A[:,i])\n    AA=sp.array(sp.around(AA),dtype=\"int\")-1\n    return AA", "category": "Python"}, {"instruction": "def print_there(x, y, text):\n    \"\"\"\"\n    allows display of a game of life on a console via\n    resetting cursor position to a set point - looks 'ok'\n    for testing but not production quality.\n    \"\"\"\n", "input": "", "output": "    sys.stdout.write(\"\\x1b7\\x1b[%d;%df%s\\x1b8\" % (x, y, text))\n    sys.stdout.flush()", "category": "Python"}, {"instruction": "def set_log_level(verbose, quiet):\n    '''\n    Ses the logging level of the script based on command line options.\n\n    Arguments:\n    - `verbose`:\n    - `quiet`:\n    '''\n", "input": "", "output": "    if quiet:\n        verbose = -1\n    if verbose < 0:\n        verbose = logging.CRITICAL\n    elif verbose == 0:\n        verbose = logging.WARNING\n    elif verbose == 1:\n        verbose = logging.INFO\n    elif 1 < verbose:\n        verbose = logging.DEBUG\n    LOGGER.setLevel(verbose)", "category": "Python"}, {"instruction": "def _set_flow_rate(pipette, params) -> None:\n    \"\"\"\n    Set flow rate in uL/mm, to value obtained from command's params.\n    \"\"\"\n", "input": "", "output": "    flow_rate_param = params['flowRate']\n\n    if not (flow_rate_param > 0):\n        raise RuntimeError('Positive flowRate param required')\n\n    pipette.flow_rate = {\n        'aspirate': flow_rate_param,\n        'dispense': flow_rate_param\n    }", "category": "Python"}, {"instruction": "def verifySignature(ecPublicSigningKey, message, signature):\n        \"\"\"\n        :type ecPublicSigningKey: ECPublicKey\n        :type message: bytearray\n        :type signature: bytearray\n        \"\"\"\n", "input": "", "output": "\n        if ecPublicSigningKey.getType() == Curve.DJB_TYPE:\n            result = _curve.verifySignature(ecPublicSigningKey.getPublicKey(), message, signature)\n            return result == 0\n        else:\n            raise InvalidKeyException(\"Unknown type: %s\" % ecPublicSigningKey.getType())", "category": "Python"}, {"instruction": "def create_filter(self):\n        \"\"\"Get an instance of filter services facade.\"\"\"\n", "input": "", "output": "        return Filter(\n            self.networkapi_url,\n            self.user,\n            self.password,\n            self.user_ldap)", "category": "Python"}, {"instruction": "def _update_job(self, target, args, kwargs):\n        \"\"\"Specify the function this async job is to execute when run.\"\"\"\n", "input": "", "output": "        target_path, options = get_function_path_and_options(target)\n\n        assert isinstance(args, (tuple, list)) or args is None\n        assert isinstance(kwargs, dict) or kwargs is None\n\n        if options:\n            self.update_options(**options)\n\n        self._options['job'] = (target_path, args, kwargs)", "category": "Python"}, {"instruction": "def ie(self):\n        \"\"\"\u4fe1\u606f\u62bd\u53d6\"\"\"\n", "input": "", "output": "\n        self.cdm()\n        if self._config.ADAPTIVE:\n            self.queue_adaptive()\n        self.text_rank()\n        self.title_extractor()\n        if self._config.ADAPTIVE:\n            self.title_adaptive()\n        self.time_extractor()\n\n        log('info', '------------------------------TEDT------------------------------')\n        log('info', '\u6807\u9898\uff1a\u3010{}\u3011'.format(self.title))\n        log('info', '\u65f6\u95f4\uff1a\u3010{}\u3011'.format(self.time))\n        log('info', '\u6b63\u6587\uff1a\u3010{}\u3011'.format(self.corpus))\n        log('info', '*****************************************************************')", "category": "Python"}, {"instruction": "def inject_path(path):\n    \"\"\"\n    Imports :func: from a python file at :path: and executes it with *args, **kwargs arguments. Everytime this function\n    is called the module is reloaded so that you can alter your debug code while the application is running.\n\n    The result of the function is returned, otherwise the exception is returned (if one is raised)\n    \"\"\"\n", "input": "", "output": "    try:\n        dirname = os.path.dirname(path)\n        if dirname not in sys.path:\n            exists_in_sys = False\n            sys.path.append(dirname)\n        else:\n            exists_in_sys = True\n        module_name = os.path.splitext(os.path.split(path)[1])[0]\n        if module_name in sys.modules:\n            reload(sys.modules[module_name])\n        else:\n            __import__(module_name)\n        if not exists_in_sys:\n            sys.path.remove(dirname)\n    except Exception as e:\n        return e", "category": "Python"}, {"instruction": "def GetVShadowStoreByPathSpec(self, path_spec):\n    \"\"\"Retrieves a VSS store for a path specification.\n\n    Args:\n      path_spec (PathSpec): path specification.\n\n    Returns:\n      pyvshadow.store: a VSS store or None if not available.\n    \"\"\"\n", "input": "", "output": "    store_index = vshadow.VShadowPathSpecGetStoreIndex(path_spec)\n    if store_index is None:\n      return None\n\n    return self._vshadow_volume.get_store(store_index)", "category": "Python"}, {"instruction": "def plot_residual(self, x, y1, y2, label1='Raw data', label2='Fit/theory', \n             xlabel=None, ylabel=None, show_legend=True,\n             **kws):\n        \"\"\"plot after clearing current plot \"\"\"\n", "input": "", "output": "\n        panel = self.get_panel('top')\n        panel.plot(x, y1, label=label1, **kws)\n        panel = self.get_panel('top')\n        panel.oplot(x, y2, label=label2, ylabel=ylabel, show_legend=show_legend, **kws)\n        panel = self.get_panel('bottom')\n        panel.plot(x, (y2-y1), ylabel='residual', show_legend=False, **kws)\n        \n        if xlabel is not None:\n            self.xlabel = xlabel\n        if self.xlabel is not None:\n            self.panel_bot.set_xlabel(self.xlabel)", "category": "Python"}, {"instruction": "def save_pdf(self, save_model=True):\n        \"\"\"\n        Save the receipt as a PDF related to this model.\n\n        The related :class:`~.Receipt` should be validated first, of course.\n\n        :param bool save_model: If True, immediately save this model instance.\n        \"\"\"\n", "input": "", "output": "        from django_afip.views import ReceiptPDFView\n\n        if not self.receipt.is_validated:\n            raise exceptions.DjangoAfipException(\n                _('Cannot generate pdf for non-authorized receipt')\n            )\n\n        self.pdf_file = File(BytesIO(), name='{}.pdf'.format(uuid.uuid4().hex))\n        render_pdf(\n            template='receipts/code_{}.html'.format(\n                self.receipt.receipt_type.code,\n            ),\n            file_=self.pdf_file,\n            context=ReceiptPDFView.get_context_for_pk(self.receipt_id),\n        )\n\n        if save_model:\n            self.save()", "category": "Python"}, {"instruction": "def resp_set_label(self, resp, label=None):\n        \"\"\"Default callback for get_label/set_label\n        \"\"\"\n", "input": "", "output": "        if label:\n            self.label=label\n        elif resp:\n            self.label=resp.label.decode().replace(\"\\x00\", \"\")", "category": "Python"}, {"instruction": "def tensor_index(self, datapoint_index):\n        \"\"\" Returns the index of the tensor containing the referenced datapoint. \"\"\"\n", "input": "", "output": "        if datapoint_index >= self._num_datapoints:\n            raise ValueError('Datapoint index %d is greater than the number of datapoints (%d)' %(datapoint_index, self._num_datapoints))\n        return self._index_to_file_num[datapoint_index]", "category": "Python"}, {"instruction": "def add_seperator(self):\n        \"\"\"\n        Add separator between labels in menu that called on right mouse click.\n        \"\"\"\n", "input": "", "output": "        m_item = Gtk.SeparatorMenuItem()\n        self.menu.append(m_item)\n        self.menu.show_all()", "category": "Python"}, {"instruction": "def prepare_for_conf(self):\n        \"\"\"Initialize the pushed configuration dictionary\n        with the inner properties that are to be propagated to the satellite link.\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"- preparing: %s\", self)\n        self.cfg = {\n            'self_conf': self.give_satellite_cfg(),\n            'schedulers': {},\n            'arbiters': {}\n        }\n        logger.debug(\"- prepared: %s\", self.cfg)", "category": "Python"}, {"instruction": "def getNumDownloads(self, fileInfo):\n\t\t\"\"\"Function to get the number of times a file has been downloaded\"\"\"\n", "input": "", "output": "\t\tdownloads = fileInfo[fileInfo.find(\"FILE INFORMATION\"):]\n\t\tif -1 != fileInfo.find(\"not included in ranking\"):\n\t\t\treturn \"0\"\n\t\tdownloads = downloads[:downloads.find(\".<BR>\")]\n\t\tdownloads = downloads[downloads.find(\"</A> with \") + len(\"</A> with \"):]\n\t\treturn downloads", "category": "Python"}, {"instruction": "def deleteReplicationMetadata(\n        self, pid, nodeId, serialVersion, vendorSpecific=None\n    ):\n        \"\"\"See Also: deleteReplicationMetadataResponse()\n\n        Args:\n          pid:\n          nodeId:\n          serialVersion:\n          vendorSpecific:\n\n        Returns:\n\n        \"\"\"\n", "input": "", "output": "        response = self.deleteReplicationMetadataResponse(\n            pid, nodeId, serialVersion, vendorSpecific\n        )\n        return self._read_boolean_response(response)", "category": "Python"}, {"instruction": "def evaluate(data_source, batch_size, params_file_name, ctx=None):\n    \"\"\"Evaluate the model on the dataset.\n\n    Parameters\n    ----------\n    data_source : NDArray\n        The dataset is evaluated on.\n    batch_size : int\n        The size of the mini-batch.\n    params_file_name : str\n        The parameter file to use to evaluate,\n        e.g., val.params or args.save\n    ctx : mx.cpu() or mx.gpu()\n        The context of the computation.\n\n    Returns\n    -------\n    loss: float\n        The loss on the dataset\n    \"\"\"\n", "input": "", "output": "\n    total_L = 0.0\n    ntotal = 0\n\n    model_eval.load_parameters(params_file_name, context)\n\n    hidden = model_eval.begin_state(batch_size=batch_size, func=mx.nd.zeros, ctx=context[0])\n    i = 0\n    while i < len(data_source) - 1 - 1:\n        data, target = get_batch(data_source, i, seq_len=args.bptt)\n        data = data.as_in_context(ctx)\n        target = target.as_in_context(ctx)\n        output, hidden = model_eval(data, hidden)\n        hidden = detach(hidden)\n        L = loss(output.reshape(-3, -1),\n                 target.reshape(-1,))\n        total_L += mx.nd.sum(L).asscalar()\n        ntotal += L.size\n        i += args.bptt\n    return total_L / ntotal", "category": "Python"}, {"instruction": "def config(host, seq, option, value):\n    \"\"\"Set configuration parameters of the drone.\"\"\"\n", "input": "", "output": "    at(host, 'CONFIG', seq, [str(option), str(value)])", "category": "Python"}, {"instruction": "def register_module_alias(self, alias, module_path, after_init=False):\n        \"\"\"Adds an alias for a module.\n\n        http://uwsgi-docs.readthedocs.io/en/latest/PythonModuleAlias.html\n\n        :param str|unicode alias:\n        :param str|unicode module_path:\n        :param bool after_init: add a python module alias after uwsgi module initialization\n        \"\"\"\n", "input": "", "output": "        command = 'post-pymodule-alias' if after_init else 'pymodule-alias'\n        self._set(command, '%s=%s' % (alias, module_path), multi=True)\n\n        return self._section", "category": "Python"}, {"instruction": "async def populate(self, agent_cls, *args, **kwargs):\n        '''Populate all the slave grid environments with agents. Assumes that\n        no agents have been spawned yet to the slave environment grids. This\n        excludes the slave environment managers as they are not in the grids.)\n        '''\n", "input": "", "output": "        n = self.gs[0] * self.gs[1]\n        tasks = []\n        for addr in self.addrs:\n            task = asyncio.ensure_future(self._populate_slave(addr, agent_cls,\n                                                              n, *args,\n                                                              **kwargs))\n            tasks.append(task)\n        rets = await asyncio.gather(*tasks)\n        return rets", "category": "Python"}, {"instruction": "def finish_bag(dir_bag):\n    \"\"\"\n    Closing steps for creating a bag\n    :param obj dir_bag:\n    :return None:\n    \"\"\"\n", "input": "", "output": "    logger_bagit.info(\"enter finish_bag\")\n    # Create a bag for the 3 files\n    new_bag = create_bag(dir_bag)\n    open_bag(dir_bag)\n    new_bag.save(manifests=True)\n    logger_bagit.info(\"exit finish_bag\")\n    return", "category": "Python"}, {"instruction": "def batch(enumerable, batch_size):\r\n    '''\r\n    Breaks enumerable argument into batch size enumerable pieces. The last chunk can\r\n    be of any length up to batch_size.\r\n\r\n    batch(xrange(5), 3)\r\n    # => [0, 1, 2], [3, 4]\r\n    '''\n", "input": "", "output": "    batch_size = max(int(batch_size), 1)\r\n    try:\r\n        enumerable.__getitem__\r\n        total_size = len(enumerable)\r\n    except (TypeError, AttributeError):\r\n        enumerable = list(enumerable)\r\n        total_size = len(enumerable)\r\n    if total_size == 0:\r\n        yield tuple()\r\n    try:\r\n        for batch_index in xrange(0, total_size, batch_size):\r\n            yield enumerable[batch_index:min(batch_index + batch_size, total_size)]\r\n    except TypeError:\r\n        # Fall back on islice, though it's not as efficient the way we're using it\r\n        for batch_start in xrange(0, total_size, batch_size):\r\n            yield tuple(islice(enumerable, batch_start, min(batch_start + batch_size, total_size)))", "category": "Python"}, {"instruction": "def hidden_tags(self):\n        \"\"\" Returns a list of tags to be hidden from the 'ls' output. \"\"\"\n", "input": "", "output": "        hidden_tags = self.cp.get('ls', 'hide_tags')\n        # pylint: disable=no-member\n        return [] if hidden_tags == '' else [tag.strip() for tag in\n                                             hidden_tags.split(',')]", "category": "Python"}, {"instruction": "def _node_has_namespace_helper(node: BaseEntity, namespace: str) -> bool:\n        \"\"\"Check that the node has namespace information.\n\n        Might have cross references in future.\n        \"\"\"\n", "input": "", "output": "        return namespace == node.get(NAMESPACE)", "category": "Python"}, {"instruction": "def get_ax_size(ax, fig):\n    \"\"\"Get axis size\n\n    Parameters\n    ----------\n    ax : matplotlib.axis\n        Axis object from matplotlib.\n    fig : matplotlib.Figure\n        Figure.\n    \"\"\"\n", "input": "", "output": "    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n    width, height = bbox.width, bbox.height\n    width *= fig.dpi\n    height *= fig.dpi", "category": "Python"}, {"instruction": "def getalgo(self, operation, name):\r\n        '''Return the algorithm for *operation* named *name*'''\n", "input": "", "output": "        if operation not in self._algorithms:\r\n            raise NotAvailable('{0} not registered.'.format(operation))\r\n        oper = self._algorithms[operation]\r\n        try:\r\n            return oper[name]\r\n        except KeyError:\r\n            raise NotAvailable('{0} algorithm {1} not registered.'\r\n                               .format(operation, name))", "category": "Python"}, {"instruction": "def bit_flip(\n    p: Optional[float] = None\n) -> Union[common_gates.XPowGate, BitFlipChannel]:\n    r\"\"\"\n    Construct a BitFlipChannel that flips a qubit state\n    with probability of a flip given by p. If p is None, return\n    a guaranteed flip in the form of an X operation.\n\n    This channel evolves a density matrix via\n\n        $$\n        \\rho \\rightarrow M_0 \\rho M_0^\\dagger + M_1 \\rho M_1^\\dagger\n        $$\n\n    With\n\n        $$\n        \\begin{aligned}\n        M_0 =& \\sqrt{p} \\begin{bmatrix}\n                            1 & 0 \\\\\n                            0 & 1\n                       \\end{bmatrix}\n        \\\\\n        M_1 =& \\sqrt{1-p} \\begin{bmatrix}\n                            0 & 1 \\\\\n                            1 & -0\n                         \\end{bmatrix}\n        \\end{aligned}\n        $$\n\n    Args:\n        p: the probability of a bit flip.\n\n    Raises:\n        ValueError: if p is not a valid probability.\n    \"\"\"\n", "input": "", "output": "    if p is None:\n        return pauli_gates.X\n\n    return _bit_flip(p)", "category": "Python"}, {"instruction": "def iteration_stats(self, k, frcxd):\n        \"\"\"Construct iteration stats record tuple.\"\"\"\n", "input": "", "output": "\n        tk = self.timer.elapsed(self.opt['IterTimer'])\n        tpl = (k,) + self.eval_objfn() + \\\n            (frcxd, self.F, self.Q, self.iterBTrack, self.L) + \\\n            self.itstat_extra() + (tk,)\n        return type(self).IterationStats(*tpl)", "category": "Python"}, {"instruction": "def shard_data(self, region):\n        \"\"\"\n        Get League of Legends status for the given shard.\n\n        Requests to this API are not counted against the application Rate Limits.\n\n        :param string region: the region to execute this request on\n\n        :returns: ShardStatus\n        \"\"\"\n", "input": "", "output": "        url, query = LolStatusApiV3Urls.shard_data(region=region)\n        return self._raw_request(self.shard_data.__name__, region, url, query)", "category": "Python"}, {"instruction": "def set_(key, value, profile=None):\n    '''\n    Set a key/value pair in memcached\n    '''\n", "input": "", "output": "    conn = salt.utils.memcached.get_conn(profile)\n    time = profile.get('expire', DEFAULT_EXPIRATION)\n    return salt.utils.memcached.set_(conn, key, value, time=time)", "category": "Python"}, {"instruction": "def cli(env, identifier, wait):\n    \"\"\"Check if a server is ready.\"\"\"\n", "input": "", "output": "\n    compute = SoftLayer.HardwareManager(env.client)\n    compute_id = helpers.resolve_id(compute.resolve_ids, identifier,\n                                    'hardware')\n    ready = compute.wait_for_ready(compute_id, wait)\n    if ready:\n        env.fout(\"READY\")\n    else:\n        raise exceptions.CLIAbort(\"Server %s not ready\" % compute_id)", "category": "Python"}, {"instruction": "def toc():\n    \"\"\"toc() -> float | None\n\n    Returns the total elapsed seconds since the most recent tic(), or\n    None if tic() was not called.\n\n    Examples:\n\n    >>> import time\n\n    >>> tic()\n    >>> time.sleep(1.2)\n    >>> elapsed = toc()\n\n    >>> assert abs(elapsed - 1.2) <= 1e-2\n\n    .. note:: The tic() and toc() functions are simplistic and may introduce\n            significant overhead, especially in tight loops.  Their use should\n            be limited to one-off experiments and rough numbers.  The Python\n            profile package (i.e. 'import profile') should be used for serious\n            and detailed profiling.\n    \"\"\"\n", "input": "", "output": "    end = datetime.datetime.now()\n    return totalSeconds( end - TICs.pop() ) if len(TICs) else None", "category": "Python"}, {"instruction": "def find_chan_in_region(channels, anat, region_name):\n    \"\"\"Find which channels are in a specific region.\n\n    Parameters\n    ----------\n    channels : instance of wonambi.attr.chan.Channels\n        channels, that have locations\n    anat : instance of wonambi.attr.anat.Freesurfer\n        anatomical information taken from freesurfer.\n    region_name : str\n        the name of the region, according to FreeSurferColorLUT.txt\n\n    Returns\n    -------\n    chan_in_region : list of str\n        list of the channels that are in one region.\n    \"\"\"\n", "input": "", "output": "    if 'region' not in channels.chan[0].attr.keys():\n        lg.info('Computing region for each channel.')\n        channels = assign_region_to_channels(channels, anat)\n\n    chan_in_region = []\n    for one_chan in channels.chan:\n        if region_name in one_chan.attr['region']:\n            chan_in_region.append(one_chan.label)\n\n    return chan_in_region", "category": "Python"}, {"instruction": "def get_project(self, project_id):\n        \"\"\" http://confluence.jetbrains.net/display/YTD2/GET+project\n        \"\"\"\n", "input": "", "output": "        return youtrack.Project(self._get(\"/admin/project/\" + urlquote(project_id)), self)", "category": "Python"}, {"instruction": "async def browse(self, device):\n        \"\"\"\n        Launch file manager on the mount path of the specified device.\n\n        :param device: device object, block device path or mount path\n        :returns: whether the program was successfully launched.\n        \"\"\"\n", "input": "", "output": "        device = self._find_device(device)\n        if not device.is_mounted:\n            self._log.error(_(\"not browsing {0}: not mounted\", device))\n            return False\n        if not self._browser:\n            self._log.error(_(\"not browsing {0}: no program\", device))\n            return False\n        self._log.debug(_('opening {0} on {0.mount_paths[0]}', device))\n        self._browser(device.mount_paths[0])\n        self._log.info(_('opened {0} on {0.mount_paths[0]}', device))\n        return True", "category": "Python"}, {"instruction": "def set_cols_dtype(self, array):\n        \"\"\"Set the desired columns datatype for the cols.\n\n        - the elements of the array should be either a callable or any of\n          \"a\", \"t\", \"f\", \"e\" or \"i\":\n\n            * \"a\": automatic (try to use the most appropriate datatype)\n            * \"t\": treat as text\n            * \"f\": treat as float in decimal format\n            * \"e\": treat as float in exponential format\n            * \"i\": treat as int\n            * a callable: should return formatted string for any value given\n\n        - by default, automatic datatyping is used for each column\n        \"\"\"\n", "input": "", "output": "\n        self._check_row_size(array)\n        self._dtype = array\n        return self", "category": "Python"}, {"instruction": "def process_user_input(self):\n        \"\"\"\n        This overrides the method in ConsoleMenu to allow for comma-delimited and range inputs.\n\n        Examples:\n            All of the following inputs would have the same result:\n                * 1,2,3,4\n                * 1-4\n                * 1-2,3-4\n                * 1 - 4\n                * 1, 2, 3, 4\n        Raises:\n            ValueError: If the input cannot be correctly parsed.\n        \"\"\"\n", "input": "", "output": "        user_input = self.screen.input()\n\n        try:\n            indexes = self.__parse_range_list(user_input)\n            # Subtract 1 from each number for its actual index number\n            indexes[:] = [x - 1 for x in indexes if 0 < x < len(self.items) + 1]\n            for index in indexes:\n                self.current_option = index\n                self.select()\n        except Exception as e:\n            return", "category": "Python"}, {"instruction": "def access_storage_create(name, **kwargs):\n    \"\"\"\n    Creates new ACL for the specified collection.\n\n    Does nothing if ACL already exists.\n    \"\"\"\n", "input": "", "output": "    ctx = Context(**kwargs)\n    ctx.execute_action('access:storage:create', **{\n        'storage': ctx.repo.create_secure_service('storage'),\n        'name': name,\n    })", "category": "Python"}, {"instruction": "def images():\n    \"\"\"Upload images via REST interface\n\n    Check if file upload was successful and sanatize user input.\n\n    TODO: return file URL instead of filename\n\n    \"\"\"\n", "input": "", "output": "    if request.method == 'POST':\n        file_upload = request.files['file']\n        if file_upload:\n            image = dict()\n            image['filename'] = secure_filename(file_upload.filename)\n            full_path = os.path.join(session['img_input_dir'],\n                                     image['filename'])\n            file_upload.save(full_path)\n            image['uid'] = session['image_uid_counter']\n            session['image_uid_counter'] += 1\n            current_app.logger.debug('File %d is saved as %s',\n                                     image['uid'],\n                                     image['filename'])\n            session['image_list'].append(image)\n            return jsonify(ok=\"true\", file=image['filename'], uid=image['uid'])\n        return jsonify(ok=\"false\")\n    if request.method == 'GET':\n        return jsonify(images=session['image_list'])", "category": "Python"}, {"instruction": "def mean_absolute_error(df, col_true, col_pred=None):\n    \"\"\"\n    Compute mean absolute error of a predicted DataFrame.\n\n    Note that this method will trigger the defined flow to execute.\n\n    :param df: predicted data frame\n    :type df: DataFrame\n    :param col_true: column name of true value\n    :type col_true: str\n    :param col_true: column name of predicted value, 'prediction_score' by default.\n    :type col_pred: str\n    :return: Mean absolute error\n    :rtype: float\n    \"\"\"\n", "input": "", "output": "    if not col_pred:\n        col_pred = get_field_name_by_role(df, FieldRole.PREDICTED_VALUE)\n    return _run_evaluation_node(df, col_true, col_pred)['mae']", "category": "Python"}, {"instruction": "def delimit_slug(slug, sep=' '):\n    \"\"\" Return a str of separated tokens found within a slugLike_This => 'slug Like This'\n\n    >>> delimit_slug(\"slugLike_ThisW/aTLA's\")\n    'slug Like This W a TLA s'\n    >>> delimit_slug('slugLike_ThisW/aTLA', '|')\n    'slug|Like|This|W|a|TLA'\n    \"\"\"\n", "input": "", "output": "    hyphenated_slug = re.sub(CRE_SLUG_DELIMITTER, sep, slug)\n    return hyphenated_slug", "category": "Python"}, {"instruction": "def init_object(self, args, kwargs):\n        \"\"\"This method is reponsible for setting :attr:`obj`.\n\n        It is called during :meth:`prepare_args`.\n        \"\"\"\n", "input": "", "output": "        self.object_id = kwargs.pop(self.pk, None)\n        if self.object_id is not None:\n            self.obj = self.Model.query.get(self.object_id)\n            actions.context[\"object\"] = self.obj\n\n        return args, kwargs", "category": "Python"}, {"instruction": "def get_array(self, period):\n        \"\"\"\n            Get the value of the variable for the given period.\n\n            If the value is not known, return ``None``.\n        \"\"\"\n", "input": "", "output": "        if self.variable.is_neutralized:\n            return self.default_array()\n        value = self._memory_storage.get(period)\n        if value is not None:\n            return value\n        if self._disk_storage:\n            return self._disk_storage.get(period)", "category": "Python"}, {"instruction": "def disqus_sso_script(context):\n    \"\"\"\n    Provides a generic context variable which adds single-sign-on\n    support to DISQUS if ``COMMENTS_DISQUS_API_PUBLIC_KEY`` and\n    ``COMMENTS_DISQUS_API_SECRET_KEY`` are specified.\n    \"\"\"\n", "input": "", "output": "    settings = context[\"settings\"]\n    public_key = getattr(settings, \"COMMENTS_DISQUS_API_PUBLIC_KEY\", \"\")\n    secret_key = getattr(settings, \"COMMENTS_DISQUS_API_SECRET_KEY\", \"\")\n    user = context[\"request\"].user\n    if public_key and secret_key and user.is_authenticated():\n        context[\"public_key\"] = public_key\n        context[\"sso_data\"] = _get_disqus_sso(user, public_key, secret_key)\n    return context", "category": "Python"}, {"instruction": "def get_image_hash(image):\n    '''\n    Returns an MD5 hash of the image file\n\n    Handles images stored locally and on AWS\n\n    I know this code is ugly.\n    Please don't ask.\n    The rabbit hole is deep.\n    '''\n", "input": "", "output": "    md5 = hashlib.md5()\n    try:\n        for chunk in image.file.chunks():\n            md5.update(chunk)\n        return md5.hexdigest()\n    # this should only occur in tests\n    except ValueError:\n        # see link below for why we try not to use .open()\n        # https://docs.djangoproject.com/en/1.9/ref/files/uploads/#django.core.files.uploadedfile.UploadedFile.chunks  # noqa\n        image.file.open()\n\n        for chunk in image.file.chunks():\n            md5.update(chunk)\n        return md5.hexdigest()\n    finally:\n        image.file.close()", "category": "Python"}, {"instruction": "def add_member(self, urls):\n        \"\"\"\n        Add a member into the cluster.\n\n        :returns: new member\n        :rtype: :class:`.Member`\n        \"\"\"\n", "input": "", "output": "        member_add_request = etcdrpc.MemberAddRequest(peerURLs=urls)\n\n        member_add_response = self.clusterstub.MemberAdd(\n            member_add_request,\n            self.timeout,\n            credentials=self.call_credentials,\n            metadata=self.metadata\n        )\n\n        member = member_add_response.member\n        return etcd3.members.Member(member.ID,\n                                    member.name,\n                                    member.peerURLs,\n                                    member.clientURLs,\n                                    etcd_client=self)", "category": "Python"}, {"instruction": "def deriv(self, x: str, ctype: ContentType) -> SchemaPattern:\n        \"\"\"Return derivative of the receiver.\"\"\"\n", "input": "", "output": "        return (Empty() if\n                self.name == x and self._active(ctype)\n                else NotAllowed())", "category": "Python"}, {"instruction": "def store_oui(self, port_uuid, oui_type, oui_data):\n        \"\"\"Function for storing the OUI.\n\n        param uuid: UUID of the vNIC\n        param oui_type: OUI ID\n        param oui_data: OUI Opaque Data\n        \"\"\"\n", "input": "", "output": "        self.oui_vif_map[port_uuid] = {'oui_id': oui_type,\n                                       'oui_data': oui_data}", "category": "Python"}, {"instruction": "def set_remote_addr(self, dst_mac, dst_ip):\n        \"\"\"\n        Configure remote ethernet and IP addresses.\n        \"\"\"\n", "input": "", "output": "        self.dst_mac = dst_mac\n        self.dst_ip = dst_ip\n\n        if not (dst_mac == \"FF:FF:FF:FF:FF:FF\" or dst_ip == \"255.255.255.255\"):\n            self._remote_addr_config = True\n\n        LOG.info(\"[BFD][%s][REMOTE] Remote address configured: %s, %s.\",\n                 hex(self._local_discr), self.dst_ip, self.dst_mac)", "category": "Python"}, {"instruction": "def connectivity(dataset, largest=False):\n        \"\"\"Find and label connected bodies/volumes. This adds an ID array to\n        the point and cell data to distinguish seperate connected bodies.\n        This applies a ``vtkConnectivityFilter`` filter which extracts cells\n        that share common points and/or meet other connectivity criterion.\n        (Cells that share vertices and meet other connectivity criterion such\n        as scalar range are known as a region.)\n\n        Parameters\n        ----------\n        largest : bool\n            Extract the largest connected part of the mesh.\n        \"\"\"\n", "input": "", "output": "        alg = vtk.vtkConnectivityFilter()\n        alg.SetInputData(dataset)\n        if largest:\n            alg.SetExtractionModeToLargestRegion()\n        else:\n            alg.SetExtractionModeToAllRegions()\n        alg.SetColorRegions(True)\n        alg.Update()\n        return _get_output(alg)", "category": "Python"}, {"instruction": "def on_rabbitmq_channel_open(self, channel):\n        \"\"\"Called when the RabbitMQ accepts the channel open request.\n\n        :param pika.channel.Channel channel: The channel opened with RabbitMQ\n\n        \"\"\"\n", "input": "", "output": "        LOGGER.info('Channel %i is opened for communication with RabbitMQ',\n                    channel.channel_number)\n        self._set_rabbitmq_channel(channel)\n        self._publish_deferred_messages()", "category": "Python"}, {"instruction": "def experiments_fmri_upsert_property(self, experiment_id, properties):\n        \"\"\"Upsert property of fMRI data object associated with given experiment.\n\n        Raises ValueError if given property dictionary results in an illegal\n        operation.\n\n        Parameters\n        ----------\n        experiment_id : string\n            Unique experiment identifier\n        properties : Dictionary()\n            Dictionary of property names and their new values.\n\n        Returns\n        -------\n        FMRIDataHandle\n            Handle for updated object of None if object doesn't exist\n        \"\"\"\n", "input": "", "output": "        # Get experiment fMRI to ensure that it exists. Needed to get fMRI\n        # data object identifier for given experiment identifier\n        fmri = self.experiments_fmri_get(experiment_id)\n        if fmri is None:\n            return None\n        # Update properties for fMRI object using the object identifier\n        return self.funcdata.upsert_object_property(fmri.identifier, properties)", "category": "Python"}, {"instruction": "def evaluate_at(self, eval_at, testcases, mode=None):\n        \"\"\" Sets the evaluation interation indices.\n\n            :param list eval_at: iteration indices where an evaluation should be performed\n            :param numpy.array testcases: testcases used for evaluation\n\n        \"\"\"\n", "input": "", "output": "        self.eval_at = eval_at\n        self.log.eval_at = eval_at\n\n        if mode is None:\n            if self.context_mode is None or (self.context_mode.has_key('choose_m') and self.context_mode['choose_m']):\n                mode = 'inverse'\n            else:\n                mode = self.context_mode[\"mode\"]\n                \n\n        self.evaluation = Evaluation(self.ag, self.env, testcases, mode=mode)\n        for test in testcases:\n            self.log.add('testcases', test)", "category": "Python"}, {"instruction": "async def on_raw_notice(self, message):\n        \"\"\" NOTICE command. \"\"\"\n", "input": "", "output": "        nick, metadata = self._parse_user(message.source)\n        target, message = message.params\n\n        self._sync_user(nick, metadata)\n\n        await self.on_notice(target, nick, message)\n        if self.is_channel(target):\n            await self.on_channel_notice(target, nick, message)\n        else:\n            await self.on_private_notice(target, nick, message)", "category": "Python"}, {"instruction": "def _get_url(cls, name, message_model, dispatch_model):\n        \"\"\"Returns a common pattern sitemessage URL.\n\n        :param str name: URL name\n        :param Message message_model:\n        :param Dispatch|None dispatch_model:\n        :return:\n        \"\"\"\n", "input": "", "output": "        global APP_URLS_ATTACHED\n\n        url = ''\n\n        if dispatch_model is None:\n            return url\n\n        if APP_URLS_ATTACHED != False:  # sic!\n\n            hashed = cls.get_dispatch_hash(dispatch_model.id, message_model.id)\n\n            try:\n                url = reverse(name, args=[message_model.id, dispatch_model.id, hashed])\n                url = '%s%s' % (get_site_url(), url)\n            except NoReverseMatch:\n                if APP_URLS_ATTACHED is None:\n                    APP_URLS_ATTACHED = False\n\n        return url", "category": "Python"}, {"instruction": "def get_samples(self, md5='', sha1='', sha256=''):\n        \"\"\"\n        Searches for a sample in CRITs. Currently only hashes allowed.\n\n        Args:\n            md5: md5sum\n            sha1: sha1sum\n            sha256: sha256sum\n        Returns:\n            JSON response or None if not found\n        \"\"\"\n", "input": "", "output": "        params = {'api_key': self.api_key, 'username': self.username}\n        if md5:\n            params['c-md5'] = md5\n        if sha1:\n            params['c-sha1'] = sha1\n        if sha256:\n            params['c-sha256'] = sha256\n        r = requests.get('{0}/samples/'.format(self.url),\n                         params=params,\n                         verify=self.verify,\n                         proxies=self.proxies)\n        if r.status_code == 200:\n            result_data = json.loads(r.text)\n            if 'meta' in result_data:\n                if 'total_count' in result_data['meta']:\n                    if result_data['meta']['total_count'] > 0:\n                        return result_data\n        else:\n            log.error('Non-200 status code: {}'.format(r.status_code))\n        return None", "category": "Python"}, {"instruction": "def _compute_suftab(self, string):\n        \"\"\"Computes the suffix array of a string in O(n).\n\n        The code is based on that from the pysuffix library (https://code.google.com/p/pysuffix/).\n\n        K\u00e4rkk\u00e4inen & Sanders (2003).\n        \"\"\"\n", "input": "", "output": "        n = len(string)\n        string += (unichr(1) * 3)\n        suftab = np.zeros(n, dtype=np.int)\n        alpha = sorted(set(string))\n        self._kark_sort(string, suftab, n, alpha)\n        return suftab", "category": "Python"}, {"instruction": "def api_version(self):\n        \"\"\"\n        Get Pagure API version.\n        :return:\n        \"\"\"\n", "input": "", "output": "        request_url = \"{}/api/0/version\".format(self.instance)\n        return_value = self._call_api(request_url)\n        return return_value['version']", "category": "Python"}, {"instruction": "def find_file(path, search_path):\n    \"\"\" Find a file by relative path. Arguments:\n\n    path -- the image's filename\n    search_path -- a list of directories to check in\n\n    Returns: the resolved file path\n    \"\"\"\n", "input": "", "output": "\n    if isinstance(search_path, str):\n        search_path = [search_path]\n    for relative in search_path:\n        candidate = os.path.normpath(os.path.join(relative, path))\n        if os.path.isfile(candidate):\n            return candidate\n\n    return None", "category": "Python"}, {"instruction": "def list_directory(self, mdir, limit=None, marker=None):\n        \"\"\"ListDirectory\n        https://apidocs.joyent.com/manta/api.html#ListDirectory\n\n        @param mdir {str} A manta path, e.g. '/trent/stor/mydir'.\n        @param limit {int} Limits the number of records to come back (default\n            and max is 1000).\n        @param marker {str} Key name at which to start the next listing.\n        @returns Directory entries (dirents). E.g.:\n            [{u'mtime': u'2012-12-11T01:54:07Z', u'name': u'play', u'type': u'directory'},\n             ...]\n        \"\"\"\n", "input": "", "output": "        res, dirents = self.list_directory2(mdir, limit=limit, marker=marker)\n        return dirents", "category": "Python"}, {"instruction": "def print_hierarchy(self, level: int = 0, file: IO[str] = sys.stdout) \\\n            -> None:\n        \"\"\"\n        Print this comparison and its children with indentation to represent\n        nesting.\n\n        :param level: The level of indentation to use. This is mostly for\n                      internal use, but you can use it to inset the root\n                      comparison.\n        :param file: The stream to print to. Defaults to stdout.\n        \"\"\"\n", "input": "", "output": "        print(' ' * self._INDENT_SIZE * level + str(self), file=file)", "category": "Python"}, {"instruction": "def __make_security_role_api_request(server_context, api, role, email=None, user_id=None, container_path=None):\n    \"\"\"\n    Execute a request against the LabKey Security Controller Group Membership apis\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param api: Action to execute\n    :param user_id: user ids to apply action to\n    :param role: (from get_roles) to remove user from\n    :param container_path: Additional container context path\n    :return: Request json object\n    \"\"\"\n", "input": "", "output": "    if email is None and user_id is None:\n        raise ValueError(\"Must supply either/both [email] or [user_id]\")\n\n    url = server_context.build_url(security_controller, api, container_path)\n\n    return server_context.make_request(url, {\n        'roleClassName': role['uniqueName'],\n        'principalId': user_id,\n        'email': email\n    })", "category": "Python"}, {"instruction": "def set_display_mode(self, mytz, fmt):\n        \"\"\"Set the display mode for self.\n\n        :arg mytz:        A pytz.timezone object.\n\n        :arg fmt:         A format string for strftime.\n\n        ~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-~-\n\n        PURPOSE:        Modifies self.display_timestamp to first parse\n                        self.timestamp and then format according to given\n                        timezone and format string.\n\n        \"\"\"\n", "input": "", "output": "        my_stamp = dateutil.parser.parse(self.timestamp)\n        tz_stamp = my_stamp.astimezone(\n            mytz) if my_stamp.tzinfo is not None else my_stamp\n        self.display_timestamp = tz_stamp.strftime(fmt)", "category": "Python"}, {"instruction": "def page(self):\n        '''Current page.'''\n", "input": "", "output": "        page = self.request.GET.get(self.page_param)\n        if not page:\n            return 1\n        try:\n            page = int(page)\n        except ValueError:\n            self.invalid_page()\n            return 1\n        if page<1:\n            self.invalid_page()\n            return 1\n        return page", "category": "Python"}, {"instruction": "def approximate_column(self, i):\n        r\"\"\" Computes the Nystroem approximation of column :math:`i` of matrix $A \\in \\mathbb{R}^{n \\times n}$.\n\n        \"\"\"\n", "input": "", "output": "        return np.dot(self._C_k, self._R_k[:, i])", "category": "Python"}, {"instruction": "def no_empty_value(func):\n    \"\"\"Raises an exception if function argument is empty.\"\"\"\n", "input": "", "output": "    @wraps(func)\n    def wrapper(value):\n        if not value:\n            raise Exception(\"Empty value not allowed\")\n        return func(value)\n    return wrapper", "category": "Python"}, {"instruction": "def log_variable(variable, gradient=None):\n    r'''\n    We introduce a function for logging a tensor variable's current state.\n    It logs scalar values for the mean, standard deviation, minimum and maximum.\n    Furthermore it logs a histogram of its state and (if given) of an optimization gradient.\n    '''\n", "input": "", "output": "    name = variable.name.replace(':', '_')\n    mean = tf.reduce_mean(variable)\n    tf.summary.scalar(name='%s/mean'   % name, tensor=mean)\n    tf.summary.scalar(name='%s/sttdev' % name, tensor=tf.sqrt(tf.reduce_mean(tf.square(variable - mean))))\n    tf.summary.scalar(name='%s/max'    % name, tensor=tf.reduce_max(variable))\n    tf.summary.scalar(name='%s/min'    % name, tensor=tf.reduce_min(variable))\n    tf.summary.histogram(name=name, values=variable)\n    if gradient is not None:\n        if isinstance(gradient, tf.IndexedSlices):\n            grad_values = gradient.values\n        else:\n            grad_values = gradient\n        if grad_values is not None:\n            tf.summary.histogram(name='%s/gradients' % name, values=grad_values)", "category": "Python"}, {"instruction": "def detect_gpt(self, filename, offset, fs_guid):\n        \"\"\"Used by rawdisk.session.Session to match gpt partitions agains\n        filesystem plugins.\n\n        Args:\n            filename: device or file that it will read in order to detect the\n            filesystem\n            fs_id: filesystem guid to match\n            (ex. {EBD0A0A2-B9E5-4433-87C0-68B6B72699C7})\n            offset: offset for the filesystem that is being matched\n\n        Returns:\n            Volume object supplied by matched plugin.\n            If there is no match, None is returned\n        \"\"\"\n", "input": "", "output": "        self.logger.debug('Detecting GPT partition type')\n\n        if fs_guid not in self.__gpt_plugins:\n            return None\n        else:\n            plugins = self.__gpt_plugins.get(fs_guid)\n            for plugin in plugins:\n                if plugin.detect(filename, offset):\n                    return plugin.get_volume_object()\n\n        return None", "category": "Python"}, {"instruction": "def get_attributes(self, item_name, attribute_name=None,\n                       consistent_read=False, item=None):\n        \"\"\"\n        Retrieve attributes for a given item.\n\n        :type item_name: string\n        :param item_name: The name of the item whose attributes are being retrieved.\n\n        :type attribute_names: string or list of strings\n        :param attribute_names: An attribute name or list of attribute names.  This\n                                parameter is optional.  If not supplied, all attributes\n                                will be retrieved for the item.\n\n        :rtype: :class:`boto.sdb.item.Item`\n        :return: An Item mapping type containing the requested attribute name/values\n        \"\"\"\n", "input": "", "output": "        return self.connection.get_attributes(self, item_name, attribute_name,\n                                              consistent_read, item)", "category": "Python"}, {"instruction": "def link_directories(root_dir, package_doc_dirs):\n    \"\"\"Create symlinks to package/module documentation directories from the\n    root documentation project.\n\n    Parameters\n    ----------\n    root_dir : `str`\n        Directory in the main documentation project where links will be\n        created. For example, this could be a ``'modules'`` directory\n        in the ``pipelines_lsst_io`` project directory.\n    package_doc_dirs : `dict`\n        Dictionary that maps symlinks to be made in ``root_dir`` with\n        source directories in the packages.\n\n    Notes\n    -----\n    If the link already exists in the ``root_dir`` it will be automatically\n    replaced.\n    \"\"\"\n", "input": "", "output": "    logger = logging.getLogger(__name__)\n\n    for dirname, source_dirname in package_doc_dirs.items():\n        link_name = os.path.join(root_dir, dirname)\n        if os.path.islink(link_name):\n            os.remove(link_name)\n        os.symlink(source_dirname, link_name)\n\n        message = 'Linking {0} -> {1}'.format(link_name, source_dirname)\n        logger.info(message)", "category": "Python"}, {"instruction": "def entries(self, start = datetime.datetime.today(), end =\n    \tdatetime.datetime.today()):\n        '''Retrieves entries from all projects/tasks logged by this person.\n\n        Can be filtered based on time by specifying start/end datetimes.'''\n", "input": "", "output": "        fr = start.strftime('%Y%m%d')\n        to = end.strftime('%Y%m%d')\n        url = str.format(\n            'people/{}/entries?from={}&to={}',\n            str(self.id),\n            fr,\n            to,\n        )\n        response = self.hv.get_request(url)\n        return [Entry(self.hv, ej['day_entry']) for ej in response]", "category": "Python"}, {"instruction": "def line_on_device(\n        device: 'cirq.google.XmonDevice',\n        length: int,\n        method: LinePlacementStrategy = greedy.GreedySequenceSearchStrategy()\n) -> GridQubitLineTuple:\n    \"\"\"Searches for linear sequence of qubits on device.\n\n    Args:\n        device: Google Xmon device instance.\n        length: Desired number of qubits making up the line.\n        method: Line placement method. Defaults to\n                cirq.greedy.GreedySequenceSearchMethod.\n\n    Returns:\n        Line sequences search results.\n    \"\"\"\n", "input": "", "output": "    return method.place_line(device, length)", "category": "Python"}, {"instruction": "def _get_file(self, path, prepend=False):\n        '''Extracts a file from dap to a file-like object'''\n", "input": "", "output": "        if prepend:\n            path = os.path.join(self._dirname(), path)\n        extracted = self._tar.extractfile(path)\n        if extracted:\n            return extracted\n        raise DapFileError(('Could not read %s from %s, maybe it\\'s a directory,' +\n            'bad link or the dap file is corrupted') % (path, self.basename))", "category": "Python"}, {"instruction": "def unpack(cls, msg):\n        \"\"\"Construct an _OpMsg from raw bytes.\"\"\"\n", "input": "", "output": "        flags, first_payload_type, first_payload_size = cls.UNPACK_FROM(msg)\n        if flags != 0:\n            raise ProtocolError(\"Unsupported OP_MSG flags (%r)\" % (flags,))\n        if first_payload_type != 0:\n            raise ProtocolError(\n                \"Unsupported OP_MSG payload type (%r)\" % (first_payload_type,))\n\n        if len(msg) != first_payload_size + 5:\n            raise ProtocolError(\"Unsupported OP_MSG reply: >1 section\")\n\n        # Convert Python 3 memoryview to bytes. Note we should call\n        # memoryview.tobytes() if we start using memoryview in Python 2.7.\n        payload_document = bytes(msg[5:])\n        return cls(flags, payload_document)", "category": "Python"}, {"instruction": "def p_contextualize_items(self, t):\n        \"\"\"contextualize_items : contextualize_items contextualize_item\n                               | contextualize_item\n                               | empty\"\"\"\n", "input": "", "output": "        if len(t) == 3:\n            t[0] = t[1]\n            t[0].append(t[2])\n        elif t[1]:\n            t[0] = [t[1]]\n        else:\n            t[0] = []", "category": "Python"}, {"instruction": "def format_specifications(specifications):\n    # type: (Iterable[str]) -> List[str]\n    \"\"\"\n    Transforms the interfaces names into URI strings, with the interface\n    implementation language as a scheme.\n\n    :param specifications: Specifications to transform\n    :return: The transformed names\n    \"\"\"\n", "input": "", "output": "    transformed = set()\n    for original in specifications:\n        try:\n            lang, spec = _extract_specification_parts(original)\n            transformed.add(_format_specification(lang, spec))\n        except ValueError:\n            # Ignore invalid specifications\n            pass\n\n    return list(transformed)", "category": "Python"}, {"instruction": "def request_system_disarm(blink, network):\n    \"\"\"\n    Disarm system.\n\n    :param blink: Blink instance.\n    :param network: Sync module network id.\n    \"\"\"\n", "input": "", "output": "    url = \"{}/network/{}/disarm\".format(blink.urls.base_url, network)\n    return http_post(blink, url)", "category": "Python"}, {"instruction": "def get_page(self, url, *args, **kwds):\n        \"\"\"\n        Define our own get_page method so that we can easily override the\n        factory when we need to. This was copied from the following:\n            * twisted.web.client.getPage\n            * twisted.web.client._makeGetterFactory\n        \"\"\"\n", "input": "", "output": "        contextFactory = None\n        scheme, host, port, path = parse(url)\n        data = kwds.get('postdata', None)\n        self._method = method = kwds.get('method', 'GET')\n        self.request_headers = self._headers(kwds.get('headers', {}))\n        if (self.body_producer is None) and (data is not None):\n            self.body_producer = FileBodyProducer(StringIO(data))\n        if self.endpoint.ssl_hostname_verification:\n            contextFactory = None\n        else:\n            contextFactory = WebClientContextFactory()\n        agent = _get_agent(scheme, host, self.reactor, contextFactory)\n        if scheme == \"https\":\n            self.client.url = url\n        d = agent.request(method, url, self.request_headers, self.body_producer)\n        d.addCallback(self._handle_response)\n        return d", "category": "Python"}, {"instruction": "def _decode_transfer_data(self, data):\n        \"\"\"\n        Take a byte array and extract the data from it\n\n        Decode the response returned by a DAP_Transfer CMSIS-DAP command\n        and return it as an array of bytes.\n        \"\"\"\n", "input": "", "output": "        assert self.get_empty() is False\n        if data[0] != Command.DAP_TRANSFER:\n            raise ValueError('DAP_TRANSFER response error')\n\n        if data[2] != DAP_TRANSFER_OK:\n            if data[2] == DAP_TRANSFER_FAULT:\n                raise DAPAccessIntf.TransferFaultError()\n            elif data[2] == DAP_TRANSFER_WAIT:\n                raise DAPAccessIntf.TransferTimeoutError()\n            raise DAPAccessIntf.TransferError()\n\n        # Check for count mismatch after checking for DAP_TRANSFER_FAULT\n        # This allows TransferFaultError or TransferTimeoutError to get\n        # thrown instead of TransferFaultError\n        if data[1] != self._read_count + self._write_count:\n            raise DAPAccessIntf.TransferError()\n\n        return data[3:3 + 4 * self._read_count]", "category": "Python"}, {"instruction": "def disconnect(self, message=\"\"):\n        \"\"\"Hang up the connection.\n\n        Arguments:\n            message -- Quit message.\n        \"\"\"\n", "input": "", "output": "        try:\n            del self.connected\n        except AttributeError:\n            return\n\n        self.quit(message)\n\n        self.transport.close()\n\n        self._handle_event(Event(\"disconnect\", self.server, \"\", [message]))", "category": "Python"}, {"instruction": "def GetUserByEmail(self, email):\n    \"\"\"Gets user info by email.\n\n    Args:\n      email: string, the user email.\n\n    Returns:\n      GitkitUser, containing the user info.\n    \"\"\"\n", "input": "", "output": "    user = self.rpc_helper.GetAccountInfoByEmail(email)\n    return GitkitUser.FromApiResponse(user)", "category": "Python"}, {"instruction": "def update(gandi, ip, reverse, background):\n    \"\"\"Update an ip.\"\"\"\n", "input": "", "output": "    if not reverse:\n        return\n    return gandi.ip.update(ip, {'reverse': reverse}, background)", "category": "Python"}, {"instruction": "def get_node_text(self, goid, goobj):\n        \"\"\"Return a string to be printed in a GO term box.\"\"\"\n", "input": "", "output": "        txt = []\n        # Header line: \"GO:0036464 L04 D06\"\n        txt.append(self.get_hdr(goid, goobj))\n        # GO name line: \"cytoplamic ribonucleoprotein\"\n        if 'no_name' not in self.present:\n            txt.append(self._get_go_name(goobj))\n        # study info line: \"24 genes\"\n        if 'objgoea' in self.kws:\n            study_txt = self.kws['objgoea'].get_study_txt(goid)\n            if study_txt is not None:\n                txt.append(study_txt)\n        # Add user-specified text, if needed\n        if 'go2txt' in self.kws and goid in self.kws['go2txt']:\n            txt.append(self.kws['go2txt'][goid])\n        return \"\\n\".join(txt)", "category": "Python"}, {"instruction": "def disconnect(self):\n        \"\"\"Disconnect from the socket.\"\"\"\n", "input": "", "output": "        if self.pipeline:\n            self._send(*self.pipeline)\n        self.pipeline = None", "category": "Python"}, {"instruction": "def log_level(level):\n    \"\"\"\n    Attempt to convert the given argument into a log level.\n\n    Log levels are represented as integers, where higher values are more \n    severe.  If the given level is already an integer, it is simply returned.  \n    If the given level is a string that can be converted into an integer, it is \n    converted and that value is returned.  Finally, if the given level is a \n    string naming one of the levels defined in the logging module, return that \n    level.  If none of those conditions are met, raise a ValueError.\n    \"\"\"\n", "input": "", "output": "    from six import string_types\n\n    if isinstance(level, int):\n        return level\n\n    if isinstance(level, string_types):\n        try: return int(level)\n        except ValueError: pass\n\n        try: return getattr(logging, level.upper())\n        except AttributeError: pass\n\n    raise ValueError(\"cannot convert '{}' into a log level\".format(level))", "category": "Python"}, {"instruction": "def column_to_bq_schema(self):\n        \"\"\"Convert a column to a bigquery schema object.\n        \"\"\"\n", "input": "", "output": "        kwargs = {}\n        if len(self.fields) > 0:\n            fields = [field.column_to_bq_schema() for field in self.fields]\n            kwargs = {\"fields\": fields}\n\n        return google.cloud.bigquery.SchemaField(self.name, self.dtype,\n                                                 self.mode, **kwargs)", "category": "Python"}, {"instruction": "def update_invoice_item(self, invoice_item_id, invoice_item_dict):\n        \"\"\"\n        Updates an invoice item\n\n        :param invoice_item_id: the invoice item id\n        :param invoice_item_dict: dict\n        :return: dict\n        \"\"\"\n", "input": "", "output": "        return self._create_put_request(\n            resource=INVOICE_ITEMS,\n            billomat_id=invoice_item_id,\n            send_data=invoice_item_dict\n        )", "category": "Python"}, {"instruction": "def update_code_analysis_actions(self):\r\n        \"\"\"Update actions in the warnings menu.\"\"\"\n", "input": "", "output": "        editor = self.get_current_editor()\r\n        # To fix an error at startup\r\n        if editor is None:\r\n            return\r\n        results = editor.get_current_warnings()\r\n        # Update code analysis buttons\r\n        state = (self.get_option('code_analysis/pyflakes') \\\r\n                 or self.get_option('code_analysis/pep8')) \\\r\n                 and results is not None and len(results)\r\n        for action in (self.warning_list_action, self.previous_warning_action,\r\n                       self.next_warning_action):\r\n            if state is not None:\r\n                action.setEnabled(state)", "category": "Python"}, {"instruction": "def hdel(self, name, *keys):\n        \"\"\"\n        Delete one or more hash field.\n\n        :param name: str     the name of the redis key\n        :param keys: on or more members to remove from the key.\n        :return: Future()\n        \"\"\"\n", "input": "", "output": "        with self.pipe as pipe:\n            m_encode = self.memberparse.encode\n            keys = [m_encode(m) for m in self._parse_values(keys)]\n            return pipe.hdel(self.redis_key(name), *keys)", "category": "Python"}, {"instruction": "def funname(file):\n    \"\"\"Return variable names from file names.\"\"\"\n", "input": "", "output": "    if isinstance(file, str):\n        files = [file]\n    else:\n        files = file\n    bases = [os.path.basename(f) for f in files]\n    names = [os.path.splitext(b)[0] for b in bases]\n    if isinstance(file, str):\n        return names[0]\n    else:\n        return names", "category": "Python"}, {"instruction": "def has_moderator_permissions(self, request):\n        \"\"\"\n        Find if user have global or per object permission firstly on category instance, \n        if not then on thread instance\n        \"\"\"\n", "input": "", "output": "        return any(request.user.has_perm(perm) for perm in self.permission_required)", "category": "Python"}, {"instruction": "def with_img_type(self, image_type):\n        \"\"\"\n        Returns the search results having the specified image type\n\n        :param image_type: the desired image type (valid values are provided by the\n            `pyowm.commons.enums.ImageTypeEnum` enum)\n        :type image_type: `pyowm.commons.databoxes.ImageType` instance\n        :returns: a list of `pyowm.agroapi10.imagery.MetaImage` instances\n\n        \"\"\"\n", "input": "", "output": "        assert isinstance(image_type, ImageType)\n        return list(filter(lambda x: x.image_type == image_type, self.metaimages))", "category": "Python"}, {"instruction": "def sents(self, filename=None):\n        \"\"\"\n        Returns the file, line by line. Use test_file if no filename specified.\n        \"\"\"\n", "input": "", "output": "        filename = filename if filename else self.test_file\n        with io.open(filename, 'r') as fin:\n            for line in fin:\n                yield line.strip()", "category": "Python"}, {"instruction": "def format_json(json_object, indent):\n    \"\"\" Pretty-format json data \"\"\"\n", "input": "", "output": "    indent_str = \"\\n\" + \" \" * indent\n    json_str = json.dumps(json_object, indent=2, default=serialize_json_var)\n    return indent_str.join(json_str.split(\"\\n\"))", "category": "Python"}, {"instruction": "def createBatchScanner(self, login, tableName, options):\n    \"\"\"\n    Parameters:\n     - login\n     - tableName\n     - options\n    \"\"\"\n", "input": "", "output": "    self.send_createBatchScanner(login, tableName, options)\n    return self.recv_createBatchScanner()", "category": "Python"}, {"instruction": "def create_snapshot(self, volume_id_or_uri, snapshot, timeout=-1):\n        \"\"\"\n        Creates a snapshot for the specified volume.\n\n        Args:\n            volume_id_or_uri:\n                Can be either the volume ID or the volume URI.\n            snapshot (dict):\n                Object to create.\n            timeout:\n                Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation in\n                OneView, just stops waiting for its completion.\n\n        Returns:\n            dict: Storage volume.\n        \"\"\"\n", "input": "", "output": "        uri = self.__build_volume_snapshot_uri(volume_id_or_uri)\n\n        return self._client.create(snapshot, uri=uri, timeout=timeout, default_values=self.DEFAULT_VALUES_SNAPSHOT)", "category": "Python"}, {"instruction": "def file_size(self, name, force_refresh=False):\n        \"\"\"Returns the size of the file.\n\n           For efficiency this operation does not use locking, so may return\n           inconsistent data. Use it for informational purposes.\n        \"\"\"\n", "input": "", "output": "\n        uname, version = split_name(name)\n\n        t = time.time()\n        logger.debug('    querying size of %s', name)\n        try:\n            if not self.remote_store or (version is not None\n                                         and not force_refresh):\n                try:\n                    if self.local_store and self.local_store.exists(name):\n                        return self.local_store.file_size(name)\n                except Exception:\n                    if self.remote_store:\n                        logger.warning(\"Error getting '%s' from local store\",\n                                       name, exc_info=True)\n                    else:\n                        raise\n            if self.remote_store:\n                return self.remote_store.file_size(name)\n            raise FiletrackerError(\"File not available: %s\" % name)\n        finally:\n            logger.debug('    processed %s in %.2fs', name, time.time() - t)", "category": "Python"}, {"instruction": "def add_term(self, t):\n        \"\"\"Add a term to this section and set it's ownership. Should only be used on root level terms\"\"\"\n", "input": "", "output": "        if t not in self.terms:\n            if t.parent_term_lc == 'root':\n                self.terms.append(t)\n\n                self.doc.add_term(t, add_section=False)\n\n                t.set_ownership()\n\n            else:\n                raise GenerateError(\"Can only add or move root-level terms. Term '{}' parent is '{}' \"\n                                    .format(t, t.parent_term_lc))\n\n        assert t.section or t.join_lc == 'root.root', t", "category": "Python"}, {"instruction": "def get_profile_data(self, auth_response):\n        \"\"\" Retrieve profile data from provider \"\"\"\n", "input": "", "output": "        res = auth_response\n        token = res.get('access_token')\n        me = res.get('user')\n\n        if not me.get('id'):\n            raise x.UserException('Instagram must return a user id')\n\n        data = dict(\n            provider=self.provider,\n            email=None,\n            id=me.get('id'),\n            token=token,\n        )\n        return data", "category": "Python"}, {"instruction": "def _tag(val, tag):\n        \"\"\"Surround val with <tag></tag>\"\"\"\n", "input": "", "output": "        if isinstance(val, str):\n            val = bytes(val, 'utf-8')\n        return (bytes('<' + tag + '>', 'utf-8') + val +\n                bytes('</' + tag + '>', 'utf-8'))", "category": "Python"}, {"instruction": "def findPrev( self ):\r\n        \"\"\"\r\n        Looks for the previous occurance of the current search text.\r\n        \"\"\"\n", "input": "", "output": "        text = self.uiFindTXT.text()\r\n        view = self.currentWebView()\r\n        \r\n        options =  QWebPage.FindWrapsAroundDocument\r\n        options |= QWebPage.FindBackward\r\n        \r\n        if ( self.uiCaseSensitiveCHK.isChecked() ):\r\n            options |= QWebPage.FindCaseSensitively\r\n        \r\n        view.page().findText(text, options)", "category": "Python"}, {"instruction": "def keys(self, **kwargs):\n        \"\"\"\n        DEPRECATED\n        Iterator that returns primary keys as a sequence of dicts.\n        \"\"\"\n", "input": "", "output": "        warnings.warn('Use of `rel.fetch.keys()` notation is deprecated. '\n                      'Please use `rel.fetch(\"KEY\")` or `rel.fetch(dj.key)` for equivalent result', stacklevel=2)\n        yield from self._expression.proj().fetch(as_dict=True, **kwargs)", "category": "Python"}, {"instruction": "def tree_entries(self):\n        \"\"\"\n        Returns this DataFrame joined with the tree entries DataSource.\n\n        >>> entries_df = commits_df.tree_entries\n\n        :rtype: TreeEntriesDataFrame\n        \"\"\"\n", "input": "", "output": "        return TreeEntriesDataFrame(self._engine_dataframe.getTreeEntries(), self._session, self._implicits)", "category": "Python"}, {"instruction": "def mix_hash(self, data: bytes):\r\n        \"\"\"\r\n        Sets h = HASH(h + data).\r\n\r\n        :param data: bytes sequence\r\n        \"\"\"\n", "input": "", "output": "        self.h = self.noise_protocol.hash_fn.hash(self.h + data)", "category": "Python"}, {"instruction": "def DeleteTrigger(self, trigger_link, options=None):\n        \"\"\"Deletes a trigger.\n\n        :param str trigger_link:\n            The link to the trigger.\n        :param dict options:\n            The request options for the request.\n\n        :return:\n            The deleted Trigger.\n        :rtype:\n            dict\n\n        \"\"\"\n", "input": "", "output": "        if options is None:\n            options = {}\n\n        path = base.GetPathFromLink(trigger_link)\n        trigger_id = base.GetResourceIdOrFullNameFromLink(trigger_link)\n        return self.DeleteResource(path,\n                                   'triggers',\n                                   trigger_id,\n                                   None,\n                                   options)", "category": "Python"}, {"instruction": "def serialize(cls, share_detail):\n        \"\"\"\n        :type share_detail: object_.ShareDetail\n\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "\n        return {\n            cls._FIELD_PAYMENT: converter.serialize(\n                share_detail._payment_field_for_request),\n            cls._FIELD_READ_ONLY: converter.serialize(\n                share_detail._read_only_field_for_request),\n            cls._FIELD_DRAFT_PAYMENT: converter.serialize(\n                share_detail._draft_payment\n            ),\n        }", "category": "Python"}, {"instruction": "def solid_company(taxonomy, tax_ids):\n    \"\"\"Return a set of non-lonely species tax_ids that will make those in *tax_ids* not lonely.\"\"\"\n", "input": "", "output": "    res = []\n    for t in tax_ids:\n        res.extend(taxonomy.nary_subtree(taxonomy.sibling_of(t), 2) or [])\n    return res", "category": "Python"}, {"instruction": "def _StrftimeGm(value, unused_context, args):\n    \"\"\"Convert a timestamp in seconds to a string based on the format string.\n\n  Returns GM time.\n  \"\"\"\n", "input": "", "output": "    time_tuple = time.gmtime(value)\n    return _StrftimeHelper(args, time_tuple)", "category": "Python"}, {"instruction": "def process(self, data=None):\n        \"\"\"Fetch incoming data from the Flask request object when no data is supplied\n        to the process method.  By default, the RequestHandler expects the\n        incoming data to be sent as JSON.\n        \"\"\"\n", "input": "", "output": "\n        return super(RequestHandler, self).process(data=data or self.get_request_data())", "category": "Python"}, {"instruction": "def unix_word_rubout(event, WORD=True):\n    \"\"\"\n    Kill the word behind point, using whitespace as a word boundary.\n    Usually bound to ControlW.\n    \"\"\"\n", "input": "", "output": "    buff = event.current_buffer\n    pos = buff.document.find_start_of_previous_word(count=event.arg, WORD=WORD)\n\n    if pos is None:\n        # Nothing found? delete until the start of the document.  (The\n        # input starts with whitespace and no words were found before the\n        # cursor.)\n        pos = - buff.cursor_position\n\n    if pos:\n        deleted = buff.delete_before_cursor(count=-pos)\n\n        # If the previous key press was also Control-W, concatenate deleted\n        # text.\n        if event.is_repeat:\n            deleted += event.cli.clipboard.get_data().text\n\n        event.cli.clipboard.set_text(deleted)\n    else:\n        # Nothing to delete. Bell.\n        event.cli.output.bell()", "category": "Python"}, {"instruction": "def clear_all(self):\n        \"\"\" clear all files that were to be injected \"\"\"\n", "input": "", "output": "        self.injections.clear_all()\n        for config_file in CONFIG_FILES:\n            self.injections.clear(os.path.join(\"~\", config_file))", "category": "Python"}, {"instruction": "def expectation(P, obs):\n    r\"\"\"Equilibrium expectation of given observable.\n\n    Parameters\n    ----------\n    P : (M, M) ndarray\n        Transition matrix\n    obs : (M,) ndarray\n        Observable, represented as vector on state space\n\n    Returns\n    -------\n    x : float\n        Expectation value\n\n    \"\"\"\n", "input": "", "output": "    pi = statdist(P)\n    return np.dot(pi, obs)", "category": "Python"}, {"instruction": "def is_obtuse(p1, v, p2):\n    '''Determine whether the angle, p1 - v - p2 is obtuse\n    \n    p1 - N x 2 array of coordinates of first point on edge\n    v - N x 2 array of vertex coordinates\n    p2 - N x 2 array of coordinates of second point on edge\n    \n    returns vector of booleans\n    '''\n", "input": "", "output": "    p1x = p1[:,1]\n    p1y = p1[:,0]\n    p2x = p2[:,1]\n    p2y = p2[:,0]\n    vx = v[:,1]\n    vy = v[:,0]\n    Dx = vx - p2x\n    Dy = vy - p2y\n    Dvp1x = p1x - vx\n    Dvp1y = p1y - vy\n    return Dvp1x * Dx + Dvp1y * Dy > 0", "category": "Python"}, {"instruction": "def connect(url, prefix=None, **kwargs):\n    \"\"\"\n    connect and return a connection instance from url\n\n    arguments:\n        - url (str): xbahn connection url\n    \"\"\"\n", "input": "", "output": "    return connection(url, prefix=get_prefix(prefix), **kwargs)", "category": "Python"}, {"instruction": "def augustus(args):\n    \"\"\"\n    %prog augustus augustus.gff3 > reformatted.gff3\n\n    AUGUSTUS does generate a gff3 (--gff3=on) but need some refinement.\n    \"\"\"\n", "input": "", "output": "    from jcvi.formats.gff import Gff\n\n    p = OptionParser(augustus.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    ingff3, = args\n    gff = Gff(ingff3)\n    fw = must_open(opts.outfile, \"w\")\n    seen = defaultdict(int)\n    for g in gff:\n        if g.type not in (\"gene\", \"transcript\", \"CDS\"):\n            continue\n\n        if g.type == \"transcript\":\n            g.type = \"mRNA\"\n\n        prefix = g.seqid + \"_\"\n        pid = prefix + g.id\n        newid = \"{0}-{1}\".format(pid, seen[pid]) if pid in seen else pid\n        seen[pid] += 1\n        g.attributes[\"ID\"] = [newid]\n        g.attributes[\"Parent\"] = [(prefix + x) for x in g.attributes[\"Parent\"]]\n        g.update_attributes()\n        print(g, file=fw)\n    fw.close()", "category": "Python"}, {"instruction": "def append_attribute(self, name, value, content):\n        \"\"\"\n        Append an attribute name/value into L{Content.data}.\n        @param name: The attribute name\n        @type name: basestring\n        @param value: The attribute's value\n        @type value: basestring\n        @param content: The current content being unmarshalled.\n        @type content: L{Content}\n        \"\"\"\n", "input": "", "output": "        key = name\n        key = '_%s' % reserved.get(key, key)\n        setattr(content.data, key, value)", "category": "Python"}, {"instruction": "def create(host, port, result_converter=None, testcase_converter=None, args=None):\n    \"\"\"\n    Function which is called by Icetea to create an instance of the cloud client. This function\n    must exists.\n    This function myust not return None. Either return an instance of Client or raise.\n    \"\"\"\n", "input": "", "output": "    return SampleClient(host, port, result_converter, testcase_converter, args)", "category": "Python"}, {"instruction": "def box(script):\n    \"\"\"\n    :param es_script:\n    :return: TEXT EXPRESSION WITH NON OBJECTS BOXED\n    \"\"\"\n", "input": "", "output": "    if script.type is BOOLEAN:\n        return \"Boolean.valueOf(\" + text_type(script.expr) + \")\"\n    elif script.type is INTEGER:\n        return \"Integer.valueOf(\" + text_type(script.expr) + \")\"\n    elif script.type is NUMBER:\n        return \"Double.valueOf(\" + text_type(script.expr) + \")\"\n    else:\n        return script.expr", "category": "Python"}, {"instruction": "def proccess_lowstates(**kwargs):\n    '''\n    return proccessed lowstate data that was not blacklisted\n\n    render_module_function is used to provide your own.\n    defaults to from_lowstate\n    '''\n", "input": "", "output": "    states = []\n    config = _get_config(**kwargs)\n    proccesser = config.get('proccesser')\n    ls = __salt__['state.show_lowstate']()\n\n    if not isinstance(ls, list):\n        raise Exception('ERROR: to see details run: [salt-call state.show_lowstate] <-----***-SEE-***')\n    else:\n        if ls:\n            if not isinstance(ls[0], dict):\n                raise Exception('ERROR: to see details run: [salt-call state.show_lowstate] <-----***-SEE-***')\n\n    for s in ls:\n        if _blacklist_filter(s, config):\n            continue\n        doc = __salt__[proccesser](s, config, **kwargs)\n        states.append(doc)\n    return states", "category": "Python"}, {"instruction": "def _format_report_line(self, test, time_taken, color, status, percent):\n        \"\"\"Format a single report line.\"\"\"\n", "input": "", "output": "        return \"[{0}] {3:04.2f}% {1}: {2}\".format(\n            status, test, self._colored_time(time_taken, color), percent\n        )", "category": "Python"}, {"instruction": "def apply_inverse(self, y):\n        \"\"\"\n        Self-consistently apply the inverse of the computed kernel matrix to\n        some vector or matrix of samples. This method subtracts the mean,\n        sorts the samples, then returns the samples in the correct (unsorted)\n        order.\n\n        :param y: ``(nsamples, )`` or ``(nsamples, K)``\n            The vector (or matrix) of sample values.\n\n        \"\"\"\n", "input": "", "output": "        self.recompute(quiet=False)\n        r = np.array(y, dtype=np.float64, order=\"F\")\n        r = self._check_dimensions(r, check_dim=False)\n\n        # Broadcast the mean function\n        m = [slice(None)] + [np.newaxis for _ in range(len(r.shape) - 1)]\n        r -= self._call_mean(self._x)[m]\n\n        # Do the solve\n        if len(r.shape) == 1:\n            b = self.solver.apply_inverse(r, in_place=True).flatten()\n        else:\n            b = self.solver.apply_inverse(r, in_place=True)\n        return b", "category": "Python"}, {"instruction": "def rows_max(self, size=None, focus=False):\n        \"\"\"Return the number of rows for `size`\n\n        If `size` is not given, the currently rendered number of rows is returned.\n        \"\"\"\n", "input": "", "output": "        if size is not None:\n            ow = self._original_widget\n            ow_size = self._get_original_widget_size(size)\n            sizing = ow.sizing()\n            if FIXED in sizing:\n                self._rows_max_cached = ow.pack(ow_size, focus)[1]\n            elif FLOW in sizing:\n                self._rows_max_cached = ow.rows(ow_size, focus)\n            else:\n                raise RuntimeError('Not a flow/box widget: %r' % self._original_widget)\n        return self._rows_max_cached", "category": "Python"}, {"instruction": "def calculate_color_temperature(r, g, b):\n    \"\"\"Converts the raw R/G/B values to color temperature in degrees Kelvin.\"\"\"\n", "input": "", "output": "    # 1. Map RGB values to their XYZ counterparts.\n    # Based on 6500K fluorescent, 3000K fluorescent\n    # and 60W incandescent values for a wide range.\n    # Note: Y = Illuminance or lux\n    X = (-0.14282 * r) + (1.54924 * g) + (-0.95641 * b)\n    Y = (-0.32466 * r) + (1.57837 * g) + (-0.73191 * b)\n    Z = (-0.68202 * r) + (0.77073 * g) + ( 0.56332 * b)\n    # Check for divide by 0 (total darkness) and return None.\n    if (X + Y + Z) == 0:\n        return None\n    # 2. Calculate the chromaticity co-ordinates\n    xc = (X) / (X + Y + Z)\n    yc = (Y) / (X + Y + Z)\n    # Check for divide by 0 again and return None.\n    if (0.1858 - yc) == 0:\n        return None\n    # 3. Use McCamy's formula to determine the CCT\n    n = (xc - 0.3320) / (0.1858 - yc)\n    # Calculate the final CCT\n    cct = (449.0 * (n ** 3.0)) + (3525.0 *(n ** 2.0)) + (6823.3 * n) + 5520.33\n    return int(cct)", "category": "Python"}, {"instruction": "def crude_tokenizer(line):\n    \"\"\"This is a very crude tokenizer from pynlpl\"\"\"\n", "input": "", "output": "    tokens = []\n    buffer = ''\n    for c in line.strip():\n        if c == ' ' or c in string.punctuation:\n            if buffer:\n                tokens.append(buffer)\n                buffer = ''\n        else:\n            buffer += c\n    if buffer: tokens.append(buffer)\n    return tokens", "category": "Python"}, {"instruction": "def clean_history(self, widget, event=None):\n        \"\"\"Triggered when the 'Clean History' button is clicked.\n\n        Empties the execution history tree by adjusting the start index and updates tree store and view.\n        \"\"\"\n", "input": "", "output": "        self.history_tree_store.clear()\n        selected_sm_m = self.model.get_selected_state_machine_model()\n        if selected_sm_m:\n            # the core may continue running without the GUI and for this it needs its execution histories\n            if state_machine_execution_engine.finished_or_stopped():\n                selected_sm_m.state_machine.destroy_execution_histories()\n                self.update()", "category": "Python"}, {"instruction": "def filter_label(label, replace_by_similar=True):\n    \"\"\"Some labels currently don't work together because of LaTeX naming\n       clashes. Those will be replaced by simple strings. \"\"\"\n", "input": "", "output": "    bad_names = ['celsius', 'degree', 'ohm', 'venus', 'mars', 'astrosun',\n                 'fullmoon', 'leftmoon', 'female', 'male', 'checked',\n                 'diameter', 'sun', 'Bowtie', 'sqrt',\n                 'cong', 'copyright', 'dag', 'parr', 'notin', 'dotsc',\n                 'mathds', 'mathfrak']\n    if any(label[1:].startswith(bad) for bad in bad_names):\n        if label == '\\\\dag' and replace_by_similar:\n            return '\\\\dagger'\n        elif label == '\\\\diameter' and replace_by_similar:\n            return '\\\\O'\n        return label[1:]\n    else:\n        return label", "category": "Python"}, {"instruction": "def debug_process(pid):\n    \"\"\"Interrupt a running process and debug it.\"\"\"\n", "input": "", "output": "    os.kill(pid, signal.SIGUSR1)  # Signal process.\n    pipe = NamedPipe(pipename(pid), 1)\n    try:\n        while pipe.is_open():\n            txt=raw_input(pipe.get()) + '\\n'\n            pipe.put(txt)\n    except EOFError:\n        pass # Exit.\n    pipe.close()", "category": "Python"}, {"instruction": "def build_D3treeStandard(old, MAX_DEPTH, level=1, toplayer=None):\n    \"\"\"\n\t  For d3s examples all we need is a json with name, children and size .. eg\n\n\t  {\n\t \"name\": \"flare\",\n\t \"children\": [\n\t  {\n\t   \"name\": \"analytics\",\n\t   \"children\": [\n\t\t{\n\t\t \"name\": \"cluster\",\n\t\t \"children\": [\n\t\t  {\"name\": \"AgglomerativeCluster\", \"size\": 3938},\n\t\t  {\"name\": \"CommunityStructure\", \"size\": 3812},\n\t\t  {\"name\": \"HierarchicalCluster\", \"size\": 6714},\n\t\t  {\"name\": \"MergeEdge\", \"size\": 743}\n\t\t ]\n\t\t},\n\t\tetc...\n\t\"\"\"\n", "input": "", "output": "    out = []\n    if not old:\n        old = toplayer\n    for x in old:\n        d = {}\n        # print \"*\" * level, x.label\n        d['qname'] = x.qname\n        d['name'] = x.bestLabel(quotes=False).replace(\"_\", \" \")\n        d['objid'] = x.id\n        if x.children() and level < MAX_DEPTH:\n            d['size'] = len(x.children()) + 5  # fake size\n            d['realsize'] = len(x.children())  # real size\n            d['children'] = build_D3treeStandard(x.children(), MAX_DEPTH,\n                                                 level + 1)\n        else:\n            d['size'] = 1  # default size\n            d['realsize'] = 0  # default size\n        out += [d]\n\n    return out", "category": "Python"}, {"instruction": "def _mask(self, weights):\n        \"\"\"\n        identifies the mask at which the weights are\n            greater than sqrt(machine epsilon)\n        and\n            not NaN\n        and\n            not Inf\n\n\n        Parameters\n        ---------\n        weights : array-like of shape (n,)\n            containing weights in [0,1]\n\n        Returns\n        -------\n        mask : boolean np.array of shape (n,) of good weight values\n        \"\"\"\n", "input": "", "output": "        mask = (np.abs(weights) >= np.sqrt(EPS)) * np.isfinite(weights)\n        if mask.sum() == 0:\n            raise OptimizationError('PIRLS optimization has diverged.\\n' +\n                'Try increasing regularization, or specifying an initial value for self.coef_')\n        return mask", "category": "Python"}, {"instruction": "def update(collection_name, upsert, multi, spec,\n           doc, safe, last_error_args, check_keys, opts, ctx=None):\n    \"\"\"Get an **update** message.\"\"\"\n", "input": "", "output": "    if ctx:\n        return _update_compressed(\n            collection_name, upsert, multi, spec, doc, check_keys, opts, ctx)\n    return _update_uncompressed(collection_name, upsert, multi, spec,\n                                doc, safe, last_error_args, check_keys, opts)", "category": "Python"}, {"instruction": "def to_dict(self, include_args=True, include_kwargs=True):\n        \"\"\"Converts this object to a dictionary.\n\n        :param include_args: boolean indicating whether to include the\n                             exception args in the output.\n        :param include_kwargs: boolean indicating whether to include the\n                               exception kwargs in the output.\n        \"\"\"\n", "input": "", "output": "        data = {\n            'exception_str': self.exception_str,\n            'traceback_str': self.traceback_str,\n            'exc_type_names': self.exception_type_names,\n            'exc_args': self.exception_args if include_args else tuple(),\n            'exc_kwargs': self.exception_kwargs if include_kwargs else {},\n            'generated_on': self.generated_on,\n        }\n        if self._cause is not None:\n            data['cause'] = self._cause.to_dict(include_args=include_args,\n                                                include_kwargs=include_kwargs)\n        return data", "category": "Python"}, {"instruction": "def find_nested_models(self, model, definitions):\n        '''\n        Prepare dictionary with reference to another definitions, create one dictionary\n        that contains full information about model, with all nested reference\n        :param model: --dictionary that contains information about model\n        :type model: dict\n        :param definitions: --dictionary that contains copy of all definitions\n        :type definitions: dict\n        :return: dictionary with all nested reference\n        :rtype: dict\n        '''\n", "input": "", "output": "        for key, value in model.items():\n            if isinstance(value, dict):\n                model[key] = self.find_nested_models(value, definitions)\n            elif key == '$ref':\n                def_name = value.split('/')[-1]\n                def_property = definitions[def_name]['properties']\n                return self.find_nested_models(def_property, definitions)\n        return model", "category": "Python"}, {"instruction": "def tsort(self):\n        \"\"\"Given a partial ordering, return a totally ordered list.\n\n        part is a dict of partial orderings.  Each value is a set,\n        which the key depends on.\n\n        The return value is a list of sets, each of which has only\n        dependencies on items in previous entries in the list.\n\n        raise ValueError if ordering is not possible (check for circular or missing dependencies)\"\"\"\n", "input": "", "output": "\n        task_dict = {}\n        for key, task in self.tasks.iteritems():\n            task_dict[task] = task.dependencies\n        # parts = parts.copy()\n        parts = task_dict.copy()\n\n        result = []\n        while True:\n            level = set([name for name, deps in parts.iteritems() if not deps])\n            if not level:\n                break\n            result.append(level)\n            parts = dict([(name, deps - level) for name, deps in parts.iteritems() if name not in level])\n        if parts:\n            raise ValueError('total ordering not possible (check for circular or missing dependencies)')\n        return result", "category": "Python"}, {"instruction": "def sample_truncated_gaussian_vector(data, uncertainties, bounds=None):\n    '''\n    Samples a Gaussian distribution subject to boundaries on the data\n\n    :param numpy.ndarray data:\n        Vector of N data values\n    :param numpy.ndarray uncertainties:\n        Vector of N data uncertainties\n    :param int number_bootstraps:\n        Number of bootstrap samples\n    :param tuple bounds:\n        (Lower, Upper) bound of data space\n    '''\n", "input": "", "output": "    nvals = len(data)\n    if bounds:\n        # if bounds[0] or (fabs(bounds[0]) < PRECISION):\n        if bounds[0] is not None:\n            lower_bound = (bounds[0] - data) / uncertainties\n        else:\n            lower_bound = -np.inf * np.ones_like(data)\n\n        # if bounds[1] or (fabs(bounds[1]) < PRECISION):\n        if bounds[1] is not None:\n            upper_bound = (bounds[1] - data) / uncertainties\n        else:\n            upper_bound = np.inf * np.ones_like(data)\n        sample = hmtk_truncnorm.rvs(lower_bound, upper_bound, size=nvals)\n\n    else:\n        sample = np.random.normal(0., 1., nvals)\n    return data + uncertainties * sample", "category": "Python"}, {"instruction": "def save_model(model: AbstractModel, name_suffix: str, on_failure: str) -> None:\n        \"\"\"\n        Save the given model with the given name_suffix. On failure, take the specified action.\n\n        :param model: the model to be saved\n        :param name_suffix: name to be used for saving\n        :param on_failure: action to be taken on failure; one of :py:attr:`SAVE_FAILURE_ACTIONS`\n        :raise IOError: on save failure with ``on_failure`` set to ``error``\n        \"\"\"\n", "input": "", "output": "        try:\n            logging.debug('Saving the model')\n            save_path = model.save(name_suffix)\n            logging.info('Model saved to: %s', save_path)\n        except Exception as ex:  # pylint: disable=broad-except\n            if on_failure == 'error':\n                raise IOError('Failed to save the model.') from ex\n            elif on_failure == 'warn':\n                logging.warning('Failed to save the model.')", "category": "Python"}, {"instruction": "def initiate_sniff(self, initial=False):\n        \"\"\"\n        Initiate a sniffing task. Make sure we only have one sniff request\n        running at any given time. If a finished sniffing request is around,\n        collect its result (which can raise its exception).\n        \"\"\"\n", "input": "", "output": "        if self.sniffing_task and self.sniffing_task.done():\n            try:\n                if self.sniffing_task is not None:\n                    self.sniffing_task.result()\n            except:\n                if self.raise_on_sniff_error:\n                    raise\n            finally:\n                self.sniffing_task = None\n\n        if self.sniffing_task is None:\n            self.sniffing_task = ensure_future(self.sniff_hosts(initial), loop=self.loop)", "category": "Python"}, {"instruction": "def evaluate(self, script):\n        \"\"\"\n        Evaluate script in page frame.\n\n        :param script: The script to evaluate.\n        \"\"\"\n", "input": "", "output": "        if WEBENGINE:\n            return self.dom.runJavaScript(\"{}\".format(script))\n        else:\n            return self.dom.evaluateJavaScript(\"{}\".format(script))", "category": "Python"}, {"instruction": "def subGraphHasField(self, parent_name, graph_name, field_name):\n        \"\"\"Return true if subgraph with name graph_name with parent graph with\n        name parent_name has field with name field_name.\n        \n        @param parent_name: Root Graph Name\n        @param graph_name:  Subgraph Name\n        @param field_name:  Field Name.\n        @return:            Boolean\n        \n        \"\"\"\n", "input": "", "output": "        subgraph = self._getSubGraph(parent_name, graph_name, True)\n        return subgraph.hasField(field_name)", "category": "Python"}, {"instruction": "def type_id(self) -> UnitTypeId:\n        \"\"\" UnitTypeId found in sc2/ids/unit_typeid\n        Caches all type_ids of the same unit type\"\"\"\n", "input": "", "output": "        unit_type = self._proto.unit_type\n        if unit_type not in self._game_data.unit_types:\n            self._game_data.unit_types[unit_type] = UnitTypeId(unit_type)\n        return self._game_data.unit_types[unit_type]", "category": "Python"}, {"instruction": "def get_addon_name(addonxml):\n    '''Parses an addon name from the given addon.xml filename.'''\n", "input": "", "output": "    xml = parse(addonxml)\n    addon_node = xml.getElementsByTagName('addon')[0]\n    return addon_node.getAttribute('name')", "category": "Python"}, {"instruction": "def delete(self, *args, **kwargs):\n        \"\"\"Delete an object\"\"\"\n", "input": "", "output": "        self.before_delete(args, kwargs)\n\n        self.delete_object(kwargs)\n\n        result = {'meta': {'message': 'Object successfully deleted'}}\n\n        final_result = self.after_delete(result)\n\n        return final_result", "category": "Python"}, {"instruction": "def immoments(im, p,q):\n    x = list(range(im.shape[1]))\n    y = list(range(im.shape[0]))\n    #coord=np.array([x.flatten(),y.flatten()]).T\n    \"\"\"\n    moment = 0\n    momentx = 0\n    for i in x.flatten():\n        moment+=momentx\n        sumx=0\n        for j in y.flatten():\n            sumx+=i**0*j**0*star0[i,j]\n    \"\"\"\n", "input": "", "output": "    moment = np.sum([i**p*j**q*im[i,j] for j in x for i in y], dtype=np.float64)\n    return moment", "category": "Python"}, {"instruction": "def get_upload_key_metadata(self):\n        \"\"\"Generate metadata dictionary from a bucket key.\"\"\"\n", "input": "", "output": "        key = self.get_upload_key()\n        metadata = key.metadata.copy()\n\n        # Some http header properties which are stored on the key need to be\n        # copied to the metadata when updating\n        headers = {\n            # http header name, key attribute name\n            'Cache-Control': 'cache_control',\n            'Content-Type': 'content_type',\n            'Content-Disposition': 'content_disposition',\n            'Content-Encoding': 'content_encoding',\n        }\n\n        for header_name, attribute_name in headers.items():\n            attribute_value = getattr(key, attribute_name, False)\n            if attribute_value:\n                metadata.update({b'{0}'.format(header_name):\n                                 b'{0}'.format(attribute_value)})\n        return metadata", "category": "Python"}, {"instruction": "def _dump_list(list_data, jsonify, stream=sys.stdout):\n    '''\n    Dump list to output stream, optionally encoded as JSON.\n\n    Parameters\n    ----------\n    list_data : list\n    jsonify : bool\n    stream : file-like\n    '''\n", "input": "", "output": "    if not jsonify and list_data:\n        print >> stream, '\\n'.join(list_data)\n    else:\n        print >> stream, json.dumps(list_data)", "category": "Python"}, {"instruction": "def _cli_check_ref_format(fmt):\n    '''Checks that a reference format exists and if not, raises a helpful exception'''\n", "input": "", "output": "\n    if fmt is None:\n        return None\n\n    fmt = fmt.lower()\n    if not fmt in api.get_reference_formats():\n        errstr = \"Reference format '\" + fmt + \"' does not exist.\\n\"\n        errstr += \"For a complete list of formats, use the 'bse list-ref-formats' command\"\n        raise RuntimeError(errstr)\n\n    return fmt", "category": "Python"}, {"instruction": "def start_inline(self,stylestack=None):\n        \"\"\" starts an inline entity with an optional style definition \"\"\"\n", "input": "", "output": "        self.stack.append('inline')\n        if self.dirty:\n            self.escpos._raw(' ')\n        if stylestack:\n            self.style(stylestack)", "category": "Python"}, {"instruction": "def ReconcileShadow(self, store_type):\n    \"\"\"Verify that entries that claim to use shadow files have a shadow entry.\n\n    If the entries of the non-shadowed file indicate that a shadow file is used,\n    check that there is actually an entry for that file in shadow.\n\n    Args:\n      store_type: The type of password store that should be used (e.g.\n        /etc/shadow or /etc/gshadow)\n    \"\"\"\n", "input": "", "output": "    for k, v in iteritems(self.entry):\n      if v.pw_entry.store == store_type:\n        shadow_entry = self.shadow.get(k)\n        if shadow_entry is not None:\n          v.pw_entry = shadow_entry\n        else:\n          v.pw_entry.store = \"UNKNOWN\"", "category": "Python"}, {"instruction": "def log_cloud_error(client, message, **kwargs):\n    '''\n    Log an azurearm cloud error exception\n    '''\n", "input": "", "output": "    try:\n        cloud_logger = getattr(log, kwargs.get('azurearm_log_level'))\n    except (AttributeError, TypeError):\n        cloud_logger = getattr(log, 'error')\n\n    cloud_logger(\n         'An AzureARM %s CloudError has occurred: %s',\n         client.capitalize(),\n         message\n    )\n\n    return", "category": "Python"}, {"instruction": "def send_login_email(app_id, token, hook, email=None, user_id=None, lang=\"en_US\",\n                     url_login='https://pswdless.appspot.com/rest/login'):\n    '''\n    Contact password-less server to send user a email containing the login link\n    '''\n", "input": "", "output": "    return SendLoginEmail(app_id, token, hook, email, user_id, lang, url_login)", "category": "Python"}, {"instruction": "def find_obj_by_path(self, si, path, name, type_name):\n        \"\"\"\n        Finds object in the vCenter or returns \"None\"\n\n        :param si:         pyvmomi 'ServiceInstance'\n        :param path:       the path to find the object ('dc' or 'dc/folder' or 'dc/folder/folder/etc...')\n        :param name:       the object name to return\n        :param type_name:   the name of the type, can be (vm, network, host, datastore)\n        \"\"\"\n", "input": "", "output": "\n        folder = self.get_folder(si, path)\n        if folder is None:\n            raise ValueError('vmomi managed object not found at: {0}'.format(path))\n\n        look_in = None\n        if hasattr(folder, type_name):\n            look_in = getattr(folder, type_name)\n        if hasattr(folder, self.ChildEntity):\n            look_in = folder\n        if look_in is None:\n            raise ValueError('vmomi managed object not found at: {0}'.format(path))\n\n        search_index = si.content.searchIndex\n        '#searches for the specific vm in the folder'\n        return search_index.FindChild(look_in, name)", "category": "Python"}, {"instruction": "def train_supervised(problem, model_name, hparams, data_dir, output_dir,\n                     train_steps, eval_steps, local_eval_frequency=None,\n                     schedule=\"continuous_train_and_eval\"):\n  \"\"\"Train supervised.\"\"\"\n", "input": "", "output": "  if local_eval_frequency is None:\n    local_eval_frequency = FLAGS.local_eval_frequency\n\n  exp_fn = trainer_lib.create_experiment_fn(\n      model_name, problem, data_dir, train_steps, eval_steps,\n      min_eval_frequency=local_eval_frequency\n  )\n  run_config = trainer_lib.create_run_config(model_name, model_dir=output_dir)\n  exp = exp_fn(run_config, hparams)\n  getattr(exp, schedule)()", "category": "Python"}, {"instruction": "def save_to_object(self):\n        \"\"\"Saves the current model state to a Python object. It also\n        saves to disk but does not return the checkpoint path.\n\n        Returns:\n            Object holding checkpoint data.\n        \"\"\"\n", "input": "", "output": "\n        tmpdir = tempfile.mkdtemp(\"save_to_object\", dir=self.logdir)\n        checkpoint_prefix = self.save(tmpdir)\n\n        data = {}\n        base_dir = os.path.dirname(checkpoint_prefix)\n        for path in os.listdir(base_dir):\n            path = os.path.join(base_dir, path)\n            if path.startswith(checkpoint_prefix):\n                with open(path, \"rb\") as f:\n                    data[os.path.basename(path)] = f.read()\n\n        out = io.BytesIO()\n        data_dict = pickle.dumps({\n            \"checkpoint_name\": os.path.basename(checkpoint_prefix),\n            \"data\": data,\n        })\n        if len(data_dict) > 10e6:  # getting pretty large\n            logger.info(\"Checkpoint size is {} bytes\".format(len(data_dict)))\n        out.write(data_dict)\n\n        shutil.rmtree(tmpdir)\n        return out.getvalue()", "category": "Python"}, {"instruction": "def has_model(self, model):\n        \"\"\"\n        Check if this object subscribes to the specified content model.\n\n        :param model: URI for the content model, as a string\n                    (currently only accepted in ``info:fedora/foo:###`` format)\n        :rtype: boolean\n        \"\"\"\n", "input": "", "output": "        # TODO:\n        # - accept DigitalObject for model?\n        # - convert model pid to info:fedora/ form if not passed in that way?\n        try:\n            rels = self.rels_ext.content\n        except RequestFailed:\n            # if rels-ext can't be retrieved, confirm this object does not have a RELS-EXT\n            # (in which case, it does not subscribe to the specified content model)\n            if \"RELS-EXT\" not in self.ds_list.keys():\n                return False\n            else:\n                raise\n\n        st = (self.uriref, modelns.hasModel, URIRef(model))\n        return st in rels", "category": "Python"}, {"instruction": "def groupby(expr, by, *bys):\n    \"\"\"\n    Group collection by a series of sequences.\n\n    :param expr: collection\n    :param by: columns to group\n    :param bys: columns to group\n    :return: GroupBy instance\n    :rtype: :class:`odps.df.expr.groupby.GroupBy`\n    \"\"\"\n", "input": "", "output": "\n    if not isinstance(by, list):\n        by = [by, ]\n    if len(bys) > 0:\n        by = by + list(bys)\n    return GroupBy(_input=expr, _by=by)", "category": "Python"}, {"instruction": "def get_calling_file(file_path=None, result='name'):\n    \"\"\"\n    Retrieve file_name or file_path of calling Python script\n    \"\"\"\n", "input": "", "output": "    # Get full path of calling python script\n    if file_path is None:\n        path = inspect.stack()[1][1]\n    else:\n        path = file_path\n\n    name = path.split('/')[-1].split('.')[0]\n    if result == 'name':\n        return name\n    elif result == 'path':\n        return path\n    else:\n        return path, name", "category": "Python"}, {"instruction": "def setTimeStart( self, timeStart ):\r\n        \"\"\"\r\n        Sets the start time for this item.  This will automatically push the\r\n        end time to match the length for this item.  So if the item starts\r\n        at 11a and ends on 1p, and the start time is changed to 12p\r\n        the end time will change to 2p.  To affect the length of the \r\n        item, use either setLength, or setTimeEnd.\r\n        \r\n        :param      timeStart | <QDate>\r\n        \"\"\"\n", "input": "", "output": "        timeStart = QTime(timeStart)\r\n        \r\n        length = self.length() # in minutes\r\n        self._timeStart = timeStart\r\n        self._timeEnd   = timeStart.addSecs(length * 60)\r\n        self.markForRebuild()", "category": "Python"}, {"instruction": "def log(self, msg, error=False):\n        \"\"\"Log message helper.\"\"\"\n", "input": "", "output": "        output = self.stdout\n        if error:\n            output = self.stderr\n\n        output.write(msg)\n        output.write('\\n')", "category": "Python"}, {"instruction": "def _get_annotations(self, text, language=''):\n    \"\"\"Returns the list of annotations retrieved from the given text.\n\n    Args:\n      text (str): Input text.\n      language (:obj:`str`, optional): Language code.\n\n    Returns:\n      Results in a dictionary. :code:`tokens` contains the list of annotations\n      and :code:`language` contains the inferred language from the input.\n    \"\"\"\n", "input": "", "output": "    body = {\n        'document': {\n            'type': 'PLAIN_TEXT',\n            'content': text,\n        },\n        'features': {\n            'extract_syntax': True,\n        },\n        'encodingType': 'UTF32',\n    }\n    if language:\n      body['document']['language'] = language\n\n    request = self.service.documents().annotateText(body=body)\n    response = request.execute()\n    tokens = response.get('tokens', [])\n    language = response.get('language')\n\n    return {'tokens': tokens, 'language': language}", "category": "Python"}, {"instruction": "def rtl_all(*vectorlist):\n    \"\"\" Hardware equivalent of python native \"all\".\n\n    :param WireVector vectorlist: all arguments are WireVectors of length 1\n    :return: WireVector of length 1\n\n    Returns a 1-bit WireVector which will hold a '1' only if all of the\n    inputs are '1' (i.e. it is a big ol' AND gate)\n    \"\"\"\n", "input": "", "output": "    if len(vectorlist) <= 0:\n        raise PyrtlError('rtl_all requires at least 1 argument')\n    converted_vectorlist = [as_wires(v) for v in vectorlist]\n    if any(len(v) != 1 for v in converted_vectorlist):\n        raise PyrtlError('only length 1 WireVectors can be inputs to rtl_all')\n    return and_all_bits(concat_list(converted_vectorlist))", "category": "Python"}, {"instruction": "def _get_sentences_dict(self):\n        \"\"\"\n        Returns sentence objects\n\n        :return: order dict of sentences\n        :rtype: collections.OrderedDict\n\n        \"\"\"\n", "input": "", "output": "        if self._sentences_dict is None:\n            sentences = [Sentence(element) for element in self._xml.xpath('/root/document/sentences/sentence')]\n            self._sentences_dict = OrderedDict([(s.id, s) for s in sentences])\n        return self._sentences_dict", "category": "Python"}, {"instruction": "def _parse_authorization(cls, response, uri=None):\n        \"\"\"\n        Parse an authorization resource.\n        \"\"\"\n", "input": "", "output": "        links = _parse_header_links(response)\n        try:\n            new_cert_uri = links[u'next'][u'url']\n        except KeyError:\n            raise errors.ClientError('\"next\" link missing')\n        return (\n            response.json()\n            .addCallback(\n                lambda body: messages.AuthorizationResource(\n                    body=messages.Authorization.from_json(body),\n                    uri=cls._maybe_location(response, uri=uri),\n                    new_cert_uri=new_cert_uri))\n            )", "category": "Python"}, {"instruction": "def create_index_if_missing(self, index, settings=None):\n        \"\"\"Creates an index if it doesn't already exist.\n\n        If supplied, settings must be a dictionary.\n\n        :param index: the name of the index\n        :keyword settings: a settings object or a dict containing settings\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.create_index(index, settings)\n        except IndexAlreadyExistsException as e:\n            return e.result", "category": "Python"}, {"instruction": "def check_pid_file(self):\n        \"\"\"\n        This function checks if we are already running the :class:`Flow` with a :class:`PyFlowScheduler`.\n        Raises: Flow.Error if the pid file of the scheduler exists.\n        \"\"\"\n", "input": "", "output": "        if not os.path.exists(self.pid_file):\n            return 0\n\n        self.show_status()\n        raise self.Error(", "category": "Python"}, {"instruction": "def generate_uuid_based_oid(length=None):\n    \"\"\"\n    OID generator which uses uuid.uuid4 (random UUIDs) to produce result.\n    \"\"\"\n", "input": "", "output": "    length = min(length or OID_LENGTH, OID_MAX_LENGTH)\n\n    while True:\n        yield uuid.uuid4().hex.upper()[:length]", "category": "Python"}, {"instruction": "def save_to_file(self, text, filename, name=None):\n        '''\n        Adds an utterance to speak to the event queue.\n\n        @param text: Text to sepak\n        @type text: unicode\n        @param filename: the name of file to save.\n        @param name: Name to associate with this utterance. Included in\n            notifications about this utterance.\n        @type name: str\n        '''\n", "input": "", "output": "        self.proxy.save_to_file(text, filename, name)", "category": "Python"}, {"instruction": "def encode_list(key, list_):\n    # type: (str, Iterable) -> Dict[str, str]\n    \"\"\"\n    Converts a list into a space-separated string and puts it in a dictionary\n\n    :param key: Dictionary key to store the list\n    :param list_: A list of objects\n    :return: A dictionary key->string or an empty dictionary\n    \"\"\"\n", "input": "", "output": "    if not list_:\n        return {}\n    return {key: \" \".join(str(i) for i in list_)}", "category": "Python"}, {"instruction": "def parse_kwargs(kwargs, *keys, **keyvalues):\n    \"\"\"Return dict with keys from keys|keyvals and values from kwargs|keyvals.\n\n    Existing keys are deleted from kwargs.\n\n    >>> kwargs = {'one': 1, 'two': 2, 'four': 4}\n    >>> kwargs2 = parse_kwargs(kwargs, 'two', 'three', four=None, five=5)\n    >>> kwargs == {'one': 1}\n    True\n    >>> kwargs2 == {'two': 2, 'four': 4, 'five': 5}\n    True\n\n    \"\"\"\n", "input": "", "output": "    result = {}\n    for key in keys:\n        if key in kwargs:\n            result[key] = kwargs[key]\n            del kwargs[key]\n    for key, value in keyvalues.items():\n        if key in kwargs:\n            result[key] = kwargs[key]\n            del kwargs[key]\n        else:\n            result[key] = value\n    return result", "category": "Python"}, {"instruction": "def add_accent_at(string, accent, index):\n    \"\"\"\n    Add mark to the index-th character of the given string.  Return\n    the new string after applying change.\n    (unused)\n    \"\"\"\n", "input": "", "output": "    if index == -1:\n        return string\n    # Python can handle the case which index is out of range of given string\n    return string[:index] + \\\n        accent.accent.add_accent_char(string[index], accent) + \\\n        string[index+1:]", "category": "Python"}, {"instruction": "def translate_detector(self, vector):\n        \"\"\"Translate the detector by a given vector\"\"\"\n", "input": "", "output": "        vector = np.array(vector, dtype=float)\n        self.pmts.pos_x += vector[0]\n        self.pmts.pos_y += vector[1]\n        self.pmts.pos_z += vector[2]\n        self.reset_caches()", "category": "Python"}, {"instruction": "def export_to_wif(self, compressed=None):\n        \"\"\"Export a key to WIF.\n\n        :param compressed: False if you want a standard WIF export (the most\n            standard option). True if you want the compressed form (Note that\n            not all clients will accept this form). Defaults to None, which\n            in turn uses the self.compressed attribute.\n        :type compressed: bool\n        See https://en.bitcoin.it/wiki/Wallet_import_format for a full\n        description.\n        \"\"\"\n", "input": "", "output": "        # Add the network byte, creating the \"extended key\"\n        extended_key_hex = self.get_extended_key()\n        extended_key_bytes = unhexlify(extended_key_hex)\n        if compressed is None:\n            compressed = self.compressed\n        if compressed:\n            extended_key_bytes += '\\01'\n        # And return the base58-encoded result with a checksum\n        return ensure_str(base58.b58encode_check(extended_key_bytes))", "category": "Python"}, {"instruction": "def _get_select_commands(self, source, tables):\n        \"\"\"\n        Create select queries for all of the tables from a source database.\n\n        :param source: Source database name\n        :param tables: Iterable of table names\n        :return: Dictionary of table keys, command values\n        \"\"\"\n", "input": "", "output": "        # Create dictionary of select queries\n        row_queries = {tbl: self.select_all(tbl, execute=False) for tbl in\n                       tqdm(tables, total=len(tables), desc='Getting {0} select queries'.format(source))}\n\n        # Convert command strings into lists of commands\n        for tbl, command in row_queries.items():\n            if isinstance(command, str):\n                row_queries[tbl] = [command]\n\n        # Pack commands into list of tuples\n        return [(tbl, cmd) for tbl, cmds in row_queries.items() for cmd in cmds]", "category": "Python"}, {"instruction": "def at_match(self, match, predicate=None, index=None):\n        \"\"\"\n        Retrieves a list of matches from given match.\n        \"\"\"\n", "input": "", "output": "        return self.at_span(match.span, predicate, index)", "category": "Python"}, {"instruction": "def get_object(self, group_id, mask=None):\n        \"\"\"Returns a PlacementGroup Object\n\n        https://softlayer.github.io/reference/services/SoftLayer_Virtual_PlacementGroup/getObject\n        \"\"\"\n", "input": "", "output": "        if mask is None:\n            mask = \"mask[id, name, createDate, rule, backendRouter[id, hostname],\" \\\n                   \"guests[activeTransaction[id,transactionStatus[name,friendlyName]]]]\"\n        return self.client.call('SoftLayer_Virtual_PlacementGroup', 'getObject', id=group_id, mask=mask)", "category": "Python"}, {"instruction": "def isinstance(self, types):\n        \"\"\"\n        Checks if the instance if one of the types provided or if any of the inner_field child is one of the types\n        provided, returns True if field or any inner_field is one of ths provided, False otherwise\n        :param types: Iterable of types to check inclusion of instance\n        :return: Boolean\n        \"\"\"\n", "input": "", "output": "        if isinstance(self, types):\n            return True\n        inner_field = getattr(self, 'inner_field', None)\n        while inner_field:\n            if isinstance(inner_field, types):\n                return True\n            inner_field = getattr(inner_field, 'inner_field', None)\n        return False", "category": "Python"}, {"instruction": "def get_datasource(self, source_id, datasource_id):\n        \"\"\"\n        Get a Datasource object\n\n        :rtype: Datasource\n        \"\"\"\n", "input": "", "output": "        target_url = self.client.get_url('DATASOURCE', 'GET', 'single', {'source_id': source_id, 'datasource_id': datasource_id})\n        return self.client.get_manager(Datasource)._get(target_url)", "category": "Python"}, {"instruction": "def commit_or_abort(self, ctx, timeout=None, metadata=None,\n                        credentials=None):\n        \"\"\"Runs commit or abort operation.\"\"\"\n", "input": "", "output": "        return self.stub.CommitOrAbort(ctx, timeout=timeout, metadata=metadata,\n                                       credentials=credentials)", "category": "Python"}, {"instruction": "def get_temperatures(self, units):\n        \"\"\"\n            Returns the temperatures in the specified units\n\n            :param list units: the units for the sensor temperature\n\n            :returns: the sensor temperature in the given units. The order of\n            the temperatures matches the order of the given units.\n            :rtype: list\n\n            :raises UnsupportedUnitError: if the unit is not supported\n            :raises NoSensorFoundError: if the sensor could not be found\n            :raises SensorNotReadyError: if the sensor is not ready yet\n        \"\"\"\n", "input": "", "output": "        sensor_value = self.get_temperature(self.DEGREES_C)\n        return [self._get_unit_factor(unit)(sensor_value) for unit in units]", "category": "Python"}, {"instruction": "def configure_logging(verbosity):\n    '''configure logging via verbosity level of between 0 and 2 corresponding\n    to log levels warning, info and debug respectfully.'''\n", "input": "", "output": "    log_level = max(logging.DEBUG, logging.WARNING - logging.DEBUG*verbosity)\n    logging.basicConfig(\n        stream=sys.stderr, level=log_level,\n        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n    )\n\n    urllib3_logger = logging.getLogger(\n        'requests.packages.urllib3')\n    urllib3_logger.setLevel(log_level)\n\n    # if debug level set then its nice to see the headers of the request\n    if log_level == logging.DEBUG:\n        try:\n            import http.client as http_client\n        except ImportError:\n            # Python 2\n            import httplib as http_client\n        http_client.HTTPConnection.debuglevel = 1", "category": "Python"}, {"instruction": "def _get_item_value(item, fieldName):\n        '''\n            _get_item_value - Returns the value of a given field on #item, using either dict style (e.x. x['a']) aka __getitem__ access if possible,\n              otherwise attribute access (aka implements __getattribute__).\n\n              This allows you to pass a mix of object types, or objects and dicts, in the same collection.\n\n                @param item <???> - The item that needs a value fetched off it. items must support either __getitem__ or __getattribute__\n                @param fieldName <str> - The name of the field on that item which is being requested\n\n            NOTE: if __getitem__ is defined, it will be used, otherwise __getattribute__ will be used.\n\n            @return - The value of the #fieldName key/attribute on #item\n        '''\n", "input": "", "output": "\n        if hasattr(item, '__getitem__'):\n            return QueryableListDicts._get_item_value(item, fieldName)\n\n        return QueryableListObjs._get_item_value(item, fieldName)", "category": "Python"}, {"instruction": "def writetoPAJ(CIJ, fname, directed):\n    '''\n    This function writes a Pajek .net file from a numpy matrix\n\n    Parameters\n    ----------\n    CIJ : NxN np.ndarray\n        adjacency matrix\n    fname : str\n        filename\n    directed : bool\n        True if the network is directed and False otherwise. The data format\n        may be required to know this for some reason so I am afraid to just\n        use directed as the default value.\n    '''\n", "input": "", "output": "    n = np.size(CIJ, axis=0)\n    with open(fname, 'w') as fd:\n        fd.write('*vertices %i \\r' % n)\n        for i in range(1, n + 1):\n            fd.write('%i \"%i\" \\r' % (i, i))\n        if directed:\n            fd.write('*arcs \\r')\n        else:\n            fd.write('*edges \\r')\n        for i in range(n):\n            for j in range(n):\n                if CIJ[i, j] != 0:\n                    fd.write('%i %i %.6f \\r' % (i + 1, j + 1, CIJ[i, j]))", "category": "Python"}, {"instruction": "def build_config(config_file=get_system_config_directory()):\r\n    \"\"\"\r\n    Construct the config object from necessary elements.\r\n    \"\"\"\n", "input": "", "output": "    config = Config(config_file, allow_no_value=True)\r\n    application_versions = find_applications_on_system()\r\n\r\n    # Add found versions to config if they don't exist. Versions found\r\n    # in the config file takes precedence over versions found in PATH.\r\n    for item in application_versions.iteritems():\r\n        if not config.has_option(Config.EXECUTABLES, item[0]):\r\n            config.set(Config.EXECUTABLES, item[0], item[1])\r\n    return config", "category": "Python"}, {"instruction": "def version_by_pip(package):\n    \"\"\"\n    Don't use this for bumping code, unless it is dot installed,\n    this is going to look at pip installed packages?\n    :param package:\n    :return:\n    \"\"\"\n", "input": "", "output": "    command = \"pip show {0}\".format(package)\n    result = execute_get_text(command)\n    for line in result.split(\"\\n\"):\n        if line.startswith(\"Version\"):\n            return line.split(\":\")[0]\n    return None", "category": "Python"}, {"instruction": "def add_to_updatedb_prunepath(path, updatedb_path=UPDATEDB_PATH):\n    \"\"\"Adds the specified path to the mlocate's udpatedb.conf PRUNEPATH list.\n\n    This method has no effect if the path specified by updatedb_path does not\n    exist or is not a file.\n\n    @param path: string the path to add to the updatedb.conf PRUNEPATHS value\n    @param updatedb_path: the path the updatedb.conf file\n    \"\"\"\n", "input": "", "output": "    if not os.path.exists(updatedb_path) or os.path.isdir(updatedb_path):\n        # If the updatedb.conf file doesn't exist then don't attempt to update\n        # the file as the package providing mlocate may not be installed on\n        # the local system\n        return\n\n    with open(updatedb_path, 'r+') as f_id:\n        updatedb_text = f_id.read()\n        output = updatedb(updatedb_text, path)\n        f_id.seek(0)\n        f_id.write(output)\n        f_id.truncate()", "category": "Python"}, {"instruction": "def pointcloud2ply(vertices, normals, out_file=None):\n    \"\"\"Converts the file to PLY format\"\"\"\n", "input": "", "output": "    from pathlib import Path\n    import pandas as pd\n    from pyntcloud import PyntCloud\n    df = pd.DataFrame(np.hstack((vertices, normals)))\n    df.columns = ['x', 'y', 'z', 'nx', 'ny', 'nz']\n    cloud = PyntCloud(df)\n\n    if out_file is None:\n        out_file = Path('pointcloud.ply').resolve()\n\n    cloud.to_file(str(out_file))\n    return out_file", "category": "Python"}, {"instruction": "def add(self, namespace_uri):\n        \"\"\"Add this namespace URI to the mapping, without caring what\n        alias it ends up with\"\"\"\n", "input": "", "output": "        # See if this namespace is already mapped to an alias\n        alias = self.namespace_to_alias.get(namespace_uri)\n        if alias is not None:\n            return alias\n\n        # Fall back to generating a numerical alias\n        i = 0\n        while True:\n            alias = 'ext' + str(i)\n            try:\n                self.addAlias(namespace_uri, alias)\n            except KeyError:\n                i += 1\n            else:\n                return alias\n\n        assert False, \"Not reached\"", "category": "Python"}, {"instruction": "def set_alarm_state(self, alarm_name, state_reason, state_value,\n                        state_reason_data=None):\n        \"\"\"\n        Temporarily sets the state of an alarm. When the updated StateValue\n        differs from the previous value, the action configured for the\n        appropriate state is invoked. This is not a permanent change. The next\n        periodic alarm check (in about a minute) will set the alarm to its\n        actual state.\n\n        :type alarm_name: string\n        :param alarm_name: Descriptive name for alarm.\n\n        :type state_reason: string\n        :param state_reason: Human readable reason.\n\n        :type state_value: string\n        :param state_value: OK | ALARM | INSUFFICIENT_DATA\n\n        :type state_reason_data: string\n        :param state_reason_data: Reason string (will be jsonified).\n        \"\"\"\n", "input": "", "output": "        params = {'AlarmName' : alarm_name,\n                  'StateReason' : state_reason,\n                  'StateValue' : state_value}\n        if state_reason_data:\n            params['StateReasonData'] = json.dumps(state_reason_data)\n\n        return self.get_status('SetAlarmState', params)", "category": "Python"}, {"instruction": "def v1_url_associations(tags, url):\n    '''Retrieve associations for a given URL.\n\n    The associations returned have the exact same structure as defined\n    in the ``v1_tag_associate`` route with one addition: a ``tag``\n    field contains the full tag name for the association.\n    '''\n", "input": "", "output": "    url = urllib.unquote_plus(url.decode('utf-8')).strip()\n    return {'associations': tags.assocs_by_url(url)}", "category": "Python"}, {"instruction": "def getReaderNames(self):\n        \"\"\"Returns the list or PCSC readers on which to wait for cards.\"\"\"\n", "input": "", "output": "\n        # get inserted readers\n        hresult, pcscreaders = SCardListReaders(self.hcontext, [])\n        if 0 != hresult and SCARD_E_NO_READERS_AVAILABLE != hresult:\n            raise ListReadersException(hresult)\n\n        readers = []\n\n        # if no readers asked, use all inserted readers\n        if None == self.readersAsked:\n            readers = pcscreaders\n\n        # otherwise use only the asked readers that are inserted\n        else:\n            for reader in self.readersAsked:\n                if not isinstance(reader, type(\"\")):\n                    reader = str(reader)\n                if reader in pcscreaders:\n                    readers = readers + [reader]\n\n        return readers", "category": "Python"}, {"instruction": "def _get_dtype(arr_or_dtype):\n    \"\"\"\n    Get the dtype instance associated with an array\n    or dtype object.\n\n    Parameters\n    ----------\n    arr_or_dtype : array-like\n        The array-like or dtype object whose dtype we want to extract.\n\n    Returns\n    -------\n    obj_dtype : The extract dtype instance from the\n                passed in array or dtype object.\n\n    Raises\n    ------\n    TypeError : The passed in object is None.\n    \"\"\"\n", "input": "", "output": "\n    if arr_or_dtype is None:\n        raise TypeError(\"Cannot deduce dtype from null object\")\n\n    # fastpath\n    elif isinstance(arr_or_dtype, np.dtype):\n        return arr_or_dtype\n    elif isinstance(arr_or_dtype, type):\n        return np.dtype(arr_or_dtype)\n\n    # if we have an array-like\n    elif hasattr(arr_or_dtype, 'dtype'):\n        arr_or_dtype = arr_or_dtype.dtype\n\n    return pandas_dtype(arr_or_dtype)", "category": "Python"}, {"instruction": "def convert_the_args(raw_args):\n    \"\"\"\n    Function used to convert the arguments of methods\n    \"\"\"\n", "input": "", "output": "    if not raw_args:\n        return \"\"\n    if isinstance(raw_args,dict):\n        out_args = \", \".join([ \"{}={}\".format(k,v) for k,v in raw_args.iteritems() ])\n        \n    elif isinstance(raw_args,(list,tuple)):\n        new_list = []\n        for x in raw_args:\n            if isinstance(x,basestring):\n                new_list.append(x)\n            elif isinstance(x,dict):\n                new_list.append( \", \".join([ \"{}={}\".format(k,v) for k,v in x.iteritems() ]) )\n            else:\n                raise ValueError(\"Error preparing the getters\")\n        out_args = \", \".join(new_list)\n    else:\n        raise ValueError(\"Couldn't recognize list of getters\")\n    return out_args", "category": "Python"}, {"instruction": "def __deserialize_model(self, data, klass):\n        \"\"\"\n        Deserializes list or dict to model.\n\n        :param data: dict, list.\n        :param klass: class literal.\n        :return: model object.\n        \"\"\"\n", "input": "", "output": "        instance = klass()\n\n        if not instance.swagger_types:\n            return data\n\n        for attr, attr_type in iteritems(instance.swagger_types):\n            if data is not None \\\n               and instance.attribute_map[attr] in data\\\n               and isinstance(data, (list, dict)):\n                value = data[instance.attribute_map[attr]]\n                setattr(instance, '_' + attr, self.__deserialize(value, attr_type))\n\n        return instance", "category": "Python"}, {"instruction": "def sum_tbl(tbl, kfield, vfields):\n    \"\"\"\n    Aggregate a composite array and compute the totals on a given key.\n\n    >>> dt = numpy.dtype([('name', (bytes, 10)), ('value', int)])\n    >>> tbl = numpy.array([('a', 1), ('a', 2), ('b', 3)], dt)\n    >>> sum_tbl(tbl, 'name', ['value'])['value']\n    array([3, 3])\n    \"\"\"\n", "input": "", "output": "    pairs = [(n, tbl.dtype[n]) for n in [kfield] + vfields]\n    dt = numpy.dtype(pairs + [('counts', int)])\n\n    def sum_all(group):\n        vals = numpy.zeros(1, dt)[0]\n        for rec in group:\n            for vfield in vfields:\n                vals[vfield] += rec[vfield]\n            vals['counts'] += 1\n        vals[kfield] = rec[kfield]\n        return vals\n    rows = groupby(tbl, operator.itemgetter(kfield), sum_all).values()\n    array = numpy.zeros(len(rows), dt)\n    for i, row in enumerate(rows):\n        for j, name in enumerate(dt.names):\n            array[i][name] = row[j]\n    return array", "category": "Python"}, {"instruction": "def randomLocation(cls, radius, width, height, origin=None):\n        '''\n        :param: radius - float\n        :param: width  - float\n        :param: height - float\n        :param: origin - optional Point subclass\n        :return: Rectangle\n        '''\n", "input": "", "output": "        return cls(width,\n                   height,\n                   Point.randomLocation(radius, origin))", "category": "Python"}, {"instruction": "def signature(cert, sig, body):\n    \"\"\"Validate data request signature.\n\n    See `validate.request` for additional info.\n\n    Args:\n        cert: cryptography.hazmat.backends.openssl.x509._Certificate. The Amazon\n            signing certificate.\n        sig: str. Signature header value sent by request.\n        body: str. HTTPS request body.\n\n    Returns:\n        bool: True if valid, False otherwise.\n    \"\"\"\n", "input": "", "output": "    body = six.b(body)\n\n    sig = base64.decodestring(sig)\n    padder = padding.PKCS1v15()\n    public_key = cert.public_key()\n    try:\n        public_key.verify(sig, body, padder, hashes.SHA1())\n        return True\n    except InvalidSignature:\n        warnings.warn('Signature verification failed.')\n        return False", "category": "Python"}, {"instruction": "def upload_media(self, **params):\n        \"\"\"Uploads media file to Twitter servers. The file will be available to be attached\n        to a status for 60 minutes. To attach to a update, pass a list of returned media ids\n        to the :meth:`update_status` method using the ``media_ids`` param.\n\n        Docs:\n        https://developer.twitter.com/en/docs/media/upload-media/api-reference/post-media-upload\n\n        \"\"\"\n", "input": "", "output": "        # https://developer.twitter.com/en/docs/media/upload-media/api-reference/get-media-upload-status\n        if params and params.get('command', '') == 'STATUS':\n            return self.get('https://upload.twitter.com/1.1/media/upload.json', params=params)\n\n        return self.post('https://upload.twitter.com/1.1/media/upload.json', params=params)", "category": "Python"}, {"instruction": "def get_index_text(self, modname, name_cls):\n        \"\"\"Return text for index entry based on object type.\"\"\"\n", "input": "", "output": "        if self.objtype.endswith('function'):\n            if not modname:\n                return _('%s() (built-in %s)') % \\\n                    (name_cls[0], self.chpl_type_name)\n            return _('%s() (in module %s)') % (name_cls[0], modname)\n        elif self.objtype in ('data', 'type', 'enum'):\n            if not modname:\n                type_name = self.objtype\n                if type_name == 'data':\n                    type_name = 'variable'\n                return _('%s (built-in %s)') % (name_cls[0], type_name)\n            return _('%s (in module %s)') % (name_cls[0], modname)\n        else:\n            return ''", "category": "Python"}, {"instruction": "def _extract_variable_parts(variable_key, variable):\n  \"\"\"Matches a variable to individual parts.\n\n  Args:\n    variable_key: String identifier of the variable in the module scope.\n    variable: Variable tensor.\n\n  Returns:\n    partitioned: Whether the variable is partitioned.\n    name: Name of the variable up to the partitioning.\n    offset: Offset of the variable into the full variable.\n\n  Raises:\n    RuntimeError: In case of unexpected variable format.\n  \"\"\"\n", "input": "", "output": "  name, offset, partitioned = None, None, False\n  # pylint: disable=protected-access\n  if variable._save_slice_info:\n    name = variable_key[:variable_key.rfind(\"/\")]\n    if not variable._save_slice_info.full_name.endswith(name):\n      raise RuntimeError(\"Unexpected handling of partitioned variable.\")\n    offset = variable._save_slice_info.var_offset[0]\n    partitioned = True\n  # pylint: enable=protected-access\n  return partitioned, name, offset", "category": "Python"}, {"instruction": "def __mgmt(name, _type, action):\n    '''\n    Perform zone management\n    '''\n", "input": "", "output": "    # It's permanent because the 4 concerned functions need the permanent option, it's wrong without\n    cmd = '--{0}-{1}={2} --permanent'.format(action, _type, name)\n\n    return __firewall_cmd(cmd)", "category": "Python"}, {"instruction": "def _get_mean_rock(self, mag, _rake, rrup, is_reverse, imt):\n        \"\"\"\n        Calculate and return the mean intensity for rock sites.\n\n        Implements an equation from table 2.\n        \"\"\"\n", "input": "", "output": "        if mag <= self.NEAR_FIELD_SATURATION_MAG:\n            C = self.COEFFS_ROCK_LOWMAG[imt]\n        else:\n            C = self.COEFFS_ROCK_HIMAG[imt]\n        # clip mag if greater than 8.5. This is to avoid\n        # ValueError: negative number cannot be raised to a fractional power\n        mag = 8.5 if mag > 8.5 else mag\n        mean = (\n            C['c1'] + C['c2'] * mag + C['c3'] * ((8.5 - mag) ** 2.5)\n            + C['c4'] * numpy.log(rrup + numpy.exp(C['c5'] + C['c6'] * mag))\n            + C['c7'] * numpy.log(rrup + 2)\n        )\n        if is_reverse:\n            # footnote in table 2 says that for reverse ruptures\n            # the mean amplitude value should be multiplied by 1.2\n            mean += 0.1823215567939546  # == log(1.2)\n        return mean", "category": "Python"}, {"instruction": "def process_mathjax_script(mathjax_settings):\n    \"\"\"Load the mathjax script template from file, and render with the settings\"\"\"\n", "input": "", "output": "\n    # Read the mathjax javascript template from file\n    with open (os.path.dirname(os.path.realpath(__file__))\n            + '/mathjax_script_template', 'r') as mathjax_script_template:\n        mathjax_template = mathjax_script_template.read()\n\n    return mathjax_template.format(**mathjax_settings)", "category": "Python"}, {"instruction": "def delete_alert(self, id, **kwargs):  # noqa: E501\n        \"\"\"Delete a specific alert  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_alert(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :return: ResponseContainerAlert\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_alert_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_alert_with_http_info(id, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def iter(self, reset=False, reverse=False):\n        \"\"\"\n        Yield processed files one at a time, in natural order.\n        \"\"\"\n", "input": "", "output": "        files = os.listdir(self.source)\n        files.sort(reverse=reverse)\n\n        for filename in files:\n            try:\n                yield self.get(filename, reset)\n            except PostNotFound:\n                continue", "category": "Python"}, {"instruction": "def find_patches(modules, recursive=True):\n    \"\"\"Find all the patches created through decorators.\n\n    Parameters\n    ----------\n    modules : list of module\n        Modules and/or packages to search the patches in.\n    recursive : bool\n        ``True`` to search recursively in subpackages.\n\n    Returns\n    -------\n    list of gorilla.Patch\n        Patches found.\n\n    Raises\n    ------\n    TypeError\n        The input is not a valid package or module.\n\n    See Also\n    --------\n    :func:`patch`, :func:`patches`.\n    \"\"\"\n", "input": "", "output": "    out = []\n    modules = (module\n               for package in modules\n               for module in _module_iterator(package, recursive=recursive))\n    for module in modules:\n        members = _get_members(module, filter=None)\n        for _, value in members:\n            base = _get_base(value)\n            decorator_data = get_decorator_data(base)\n            if decorator_data is None:\n                continue\n\n            out.extend(decorator_data.patches)\n\n    return out", "category": "Python"}, {"instruction": "def send(token, title, **kwargs):\n    \"\"\"\n    Site: https://boxcar.io/\n    API: http://help.boxcar.io/knowledgebase/topics/48115-boxcar-api\n    Desc: Best app for system administrators\n    \"\"\"\n", "input": "", "output": "    headers = {\n        \"Content-type\": \"application/x-www-form-urlencoded\",\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n    }\n\n    data = {\n        \"user_credentials\": token,\n        \"notification[title]\": from_unicode(title),\n        \"notification[sound]\": \"notifier-2\"\n    }\n\n    for k, v in kwargs.items():\n        data['notification[%s]' % k] = from_unicode(v)\n\n    http = HTTPSConnection(kwargs.pop(\"api_url\", \"new.boxcar.io\"))\n    http.request(\n        \"POST\", \"/api/notifications\",\n        headers=headers,\n        body=urlencode(data))\n    response = http.getresponse()\n\n    if response.status != 201:\n        raise BoxcarError(response.reason)\n    return True", "category": "Python"}, {"instruction": "def list_deelgemeenten(self, gewest=2):\n        '''\n        List all `deelgemeenten` in a `gewest`.\n\n        :param gewest: The :class:`Gewest` for which the \\\n            `deelgemeenten` are wanted. Currently only Flanders is supported.\n        :rtype: A :class:`list` of :class:`Deelgemeente`.\n        '''\n", "input": "", "output": "        try:\n            gewest_id = gewest.id\n        except AttributeError:\n            gewest_id = gewest\n\n        if gewest_id != 2:\n            raise ValueError('Currently only deelgemeenten in Flanders are known.')\n\n        def creator():\n            return [Deelgemeente(dg['id'], dg['naam'], dg['gemeente_niscode']) for dg in self.deelgemeenten.values()]\n\n        if self.caches['permanent'].is_configured:\n            key = 'ListDeelgemeentenByGewestId#%s' % gewest_id\n            deelgemeenten = self.caches['permanent'].get_or_create(key, creator)\n        else:\n            deelgemeenten = creator()\n        for dg in deelgemeenten:\n            dg.set_gateway(self)\n        return deelgemeenten", "category": "Python"}, {"instruction": "def LookupNamespace(self, prefix):\n        \"\"\"Resolves a namespace prefix in the scope of the current\n           element. \"\"\"\n", "input": "", "output": "        ret = libxml2mod.xmlTextReaderLookupNamespace(self._o, prefix)\n        return ret", "category": "Python"}, {"instruction": "def polynet(num_classes=1000, pretrained='imagenet'):\n    \"\"\"PolyNet architecture from the paper\n    'PolyNet: A Pursuit of Structural Diversity in Very Deep Networks'\n    https://arxiv.org/abs/1611.05725\n    \"\"\"\n", "input": "", "output": "    if pretrained:\n        settings = pretrained_settings['polynet'][pretrained]\n        assert num_classes == settings['num_classes'], \\\n            'num_classes should be {}, but is {}'.format(\n                settings['num_classes'], num_classes)\n        model = PolyNet(num_classes=num_classes)\n        model.load_state_dict(model_zoo.load_url(settings['url']))\n        model.input_space = settings['input_space']\n        model.input_size = settings['input_size']\n        model.input_range = settings['input_range']\n        model.mean = settings['mean']\n        model.std = settings['std']\n    else:\n        model = PolyNet(num_classes=num_classes)\n    return model", "category": "Python"}, {"instruction": "def removeRemoteByName(self, name: str) -> int:\n        \"\"\"\n        Remove the remote by name.\n\n        :param name: the name of the remote to remove\n        :raises: RemoteNotFound\n        \"\"\"\n", "input": "", "output": "        remote = self.getRemote(name)\n        rid = remote.uid\n        self.removeRemote(remote)\n        return rid", "category": "Python"}, {"instruction": "def dephasing_kraus_map(p=0.10):\n    \"\"\"\n    Generate the Kraus operators corresponding to a dephasing channel.\n\n    :params float p: The one-step dephasing probability.\n    :return: A list [k1, k2] of the Kraus operators that parametrize the map.\n    :rtype: list\n    \"\"\"\n", "input": "", "output": "    return [np.sqrt(1 - p) * np.eye(2), np.sqrt(p) * np.diag([1, -1])]", "category": "Python"}, {"instruction": "def show_vcs_output_virtual_ip_address(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        show_vcs = ET.Element(\"show_vcs\")\n        config = show_vcs\n        output = ET.SubElement(show_vcs, \"output\")\n        virtual_ip_address = ET.SubElement(output, \"virtual-ip-address\")\n        virtual_ip_address.text = kwargs.pop('virtual_ip_address')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def check_dependencies():\n    \"\"\"\n    Check that dependencies are installed:\n    - require git 2.7+, so that credential-cache--daemon ignores SIGHUP\n        https://github.com/git/git/blob/v2.7.0/credential-cache--daemon.c\n    \"\"\"\n", "input": "", "output": "\n    # Check that git is installed\n    if not shutil.which(\"git\"):\n        raise Error(_(\"You don't have git. Install git, then re-run!\"))\n\n    # Check that git --version > 2.7\n    version = subprocess.check_output([\"git\", \"--version\"]).decode(\"utf-8\")\n    matches = re.search(r\"^git version (\\d+\\.\\d+\\.\\d+).*$\", version)\n    if not matches or pkg_resources.parse_version(matches.group(1)) < pkg_resources.parse_version(\"2.7.0\"):\n        raise Error(_(\"You have an old version of git. Install version 2.7 or later, then re-run!\"))", "category": "Python"}, {"instruction": "def get_code_num(s: str) -> Optional[int]:\n    \"\"\" Get code number from an escape code.\n        Raises InvalidEscapeCode if an invalid number is found.\n    \"\"\"\n", "input": "", "output": "    if ';' in s:\n        # Extended fore/back codes.\n        numberstr = s.rpartition(';')[-1][:-1]\n    else:\n        # Fore, back, style, codes.\n        numberstr = s.rpartition('[')[-1][:-1]\n\n    num = try_parse_int(\n        numberstr,\n        default=None,\n        minimum=0,\n        maximum=255\n    )\n    if num is None:\n        raise InvalidEscapeCode(numberstr)\n    return num", "category": "Python"}, {"instruction": "def enter_frame(self, frame):\n        \"\"\"Remember all undeclared identifiers.\"\"\"\n", "input": "", "output": "        CodeGenerator.enter_frame(self, frame)\n        for _, (action, param) in iteritems(frame.symbols.loads):\n            if action == 'resolve':\n                self.undeclared_identifiers.add(param)", "category": "Python"}, {"instruction": "def add_task(self, tile_address, coroutine):\n        \"\"\"Add a task into the event loop.\n\n        This is the main entry point for registering background tasks that are\n        associated with a tile. The tasks are added to the EmulationLoop and\n        the tile they are a part of is recorded.  When the tile is reset, all\n        of its background tasks are canceled as part of the reset process.\n\n        If you have a task that should not be associated with any tile, you\n        may pass `None` for tile_address and the task will not be cancelled\n        when any tile is reset.\n\n        Args:\n            tile_address (int): The address of the tile running\n                the task.\n            coroutine (coroutine): A coroutine that will be added\n                to the event loop.\n        \"\"\"\n", "input": "", "output": "\n        self._loop.call_soon_threadsafe(self._add_task, tile_address, coroutine)", "category": "Python"}, {"instruction": "def string_to_timestamp(timestring):\n    \"\"\"\n    Accepts a str, returns an int timestamp.\n    \"\"\"\n", "input": "", "output": "\n    ts = None\n\n    # Uses an extended version of Go's duration string.\n    try:\n        delta = durationpy.from_str(timestring);\n        past = datetime.datetime.utcnow() - delta\n        ts = calendar.timegm(past.timetuple())\n        return ts\n    except Exception as e:\n        pass\n\n    if ts:\n        return ts\n    # else:\n    #     print(\"Unable to parse timestring.\")\n    return 0", "category": "Python"}, {"instruction": "def get_from_relations(self, query,aliases):\n        '''\n        Returns list of the names of all positive relations in the query\n        '''\n", "input": "", "output": "        return [aliases[rel.get_name()] for rel in query.get_relations() if not rel.is_negated()]", "category": "Python"}, {"instruction": "def find_plugin(self, name):\n        \"\"\"Find a plugin named name\"\"\"\n", "input": "", "output": "        suffix = \".py\"\n        if not self.class_name:\n            suffix = \"\"\n        for i in self._get_paths():\n            path = os.path.join(i, \"%s%s\" % (name, suffix))\n            if os.path.exists(path):\n                return path\n        return None", "category": "Python"}, {"instruction": "def fromvector(cls, v):\n        \"\"\"Initialize from euclidean vector\"\"\"\n", "input": "", "output": "        w = v.normalized()\n        return cls(w.x, w.y, w.z)", "category": "Python"}, {"instruction": "def workspace_backup_undo(ctx):\n    \"\"\"\n    Restore the last backup\n    \"\"\"\n", "input": "", "output": "    backup_manager = WorkspaceBackupManager(Workspace(ctx.resolver, directory=ctx.directory, mets_basename=ctx.mets_basename, automatic_backup=ctx.automatic_backup))\n    backup_manager.undo()", "category": "Python"}, {"instruction": "def delete_radiussettings(self, domainid, serverid, settingsid, data):\n        \"\"\"Delete RADIUS settings\"\"\"\n", "input": "", "output": "        return self.api_call(\n            ENDPOINTS['radiussettings']['delete'],\n            dict(domainid=domainid, serverid=serverid, settingsid=settingsid),\n            body=data)", "category": "Python"}, {"instruction": "def __create_dir_property(self, dir_name, docstring):\n        \"\"\"\n        Generate getter and setter for a directory property.\n\n        \"\"\"\n", "input": "", "output": "        property_name = \"{}_dir\".format(dir_name)\n        private_name = \"_\" + property_name\n        setattr(self, private_name, None)\n\n        def fget(self):\n            return getattr(self, private_name)\n\n        def fset(self, path):\n            verify_dir(path, dir_name)\n            setattr(self, private_name, path)\n\n        p = property(fget=fget, fset=fset, doc=docstring)\n        setattr(self.__class__, property_name, p)", "category": "Python"}, {"instruction": "def rgb256(r, g, b):\n    '''\n    Convert an RGB colour to 256 colour ANSI graphics.\n    '''\n", "input": "", "output": "    grey = False\n    poss = True\n    step = 2.5\n\n    while poss: # As long as the colour could be grey scale\n        if r < step or g < step or b < step:\n            grey = r < step and g < step and b < step\n            poss = False\n\n        step += 42.5\n\n    if grey:\n        colour = 232 + int(float(sum([r, g, b]) / 33.0))\n    else:\n        colour = sum([16] + [int (6 * float(val) / 256) * mod\n            for val, mod in ((r, 36), (g, 6), (b, 1))])\n\n    return sequence('m', fields=3)(38, 5, colour)", "category": "Python"}, {"instruction": "def uninstall_ruby(ruby, runas=None):\n    '''\n    Uninstall a ruby implementation.\n\n    ruby\n        The version of ruby to uninstall. Should match one of the versions\n        listed by :py:func:`rbenv.versions <salt.modules.rbenv.versions>`.\n\n    runas\n        The user under which to run rbenv. If not specified, then rbenv will be\n        run as the user under which Salt is running.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rbenv.uninstall_ruby 2.0.0-p0\n    '''\n", "input": "", "output": "    ruby = re.sub(r'^ruby-', '', ruby)\n    _rbenv_exec(['uninstall', '--force', ruby], runas=runas)\n    return True", "category": "Python"}, {"instruction": "def resources_to_json(resources):\n    \"\"\" Make a JSON/catalog representation of the resources db \"\"\"\n", "input": "", "output": "    return {\n        docname: clean_resource_json(resource.__json__(resources))\n        for (docname, resource)\n        in resources.items()\n    }", "category": "Python"}, {"instruction": "def _upload_assets_to_OSF(dlgr_id, osf_id, provider=\"osfstorage\"):\n    \"\"\"Upload experimental assets to the OSF.\"\"\"\n", "input": "", "output": "    root = \"https://files.osf.io/v1\"\n    snapshot_filename = \"{}-code.zip\".format(dlgr_id)\n    snapshot_path = os.path.join(\"snapshots\", snapshot_filename)\n    r = requests.put(\n        \"{}/resources/{}/providers/{}/\".format(root, osf_id, provider),\n        params={\"kind\": \"file\", \"name\": snapshot_filename},\n        headers={\n            \"Authorization\": \"Bearer {}\".format(config.get(\"osf_access_token\")),\n            \"Content-Type\": \"text/plain\",\n        },\n        data=open(snapshot_path, \"rb\"),\n    )\n    r.raise_for_status()", "category": "Python"}, {"instruction": "def split_token(output):\n    \"\"\"\n    Split an output into token tuple, real output tuple.\n\n    :param output:\n    :return: tuple, tuple\n    \"\"\"\n", "input": "", "output": "\n    output = ensure_tuple(output)\n\n    flags, i, len_output, data_allowed = set(), 0, len(output), True\n    while i < len_output and isflag(output[i]):\n        if output[i].must_be_first and i:\n            raise ValueError(\"{} flag must be first.\".format(output[i]))\n        if i and output[i - 1].must_be_last:\n            raise ValueError(\"{} flag must be last.\".format(output[i - 1]))\n        if output[i] in flags:\n            raise ValueError(\"Duplicate flag {}.\".format(output[i]))\n        flags.add(output[i])\n        data_allowed &= output[i].allows_data\n        i += 1\n\n    output = output[i:]\n    if not data_allowed and len(output):\n        raise ValueError(\"Output data provided after a flag that does not allow data.\")\n\n    return flags, output", "category": "Python"}, {"instruction": "def split_taf(txt: str) -> [str]:  # type: ignore\n    \"\"\"\n    Splits a TAF report into each distinct time period\n    \"\"\"\n", "input": "", "output": "    lines = []\n    split = txt.split()\n    last_index = 0\n    for i, item in enumerate(split):\n        if starts_new_line(item) and i != 0 and not split[i - 1].startswith('PROB'):\n            lines.append(' '.join(split[last_index:i]))\n            last_index = i\n    lines.append(' '.join(split[last_index:]))\n    return lines", "category": "Python"}, {"instruction": "def main_group(self, i):\n        \"\"\"The main group: group 0.\"\"\"\n", "input": "", "output": "\n        current = []\n        while True:\n            try:\n                t = next(i)\n                current.extend(self.normal(t, i))\n            except StopIteration:\n                break\n        return current", "category": "Python"}, {"instruction": "def determine_inst(i_info, param_str, command):\n    \"\"\"Determine the instance-id of the target instance.\n\n    Inspect the number of instance-ids collected and take the\n    appropriate action: exit if no ids, return if single id,\n    and call user_picklist function if multiple ids exist.\n\n    Args:\n        i_info (dict): information and details for instances.\n        param_str (str): the title to display in the listing.\n        command (str): command specified on the command line.\n    Returns:\n        tar_inst (str): the AWS instance-id of the target.\n    Raises:\n        SystemExit: if no instances are match parameters specified.\n\n    \"\"\"\n", "input": "", "output": "    qty_instances = len(i_info)\n    if not qty_instances:\n        print(\"No instances found with parameters: {}\".format(param_str))\n        sys.exit(1)\n\n    if qty_instances > 1:\n        print(\"{} instances match these parameters:\".format(qty_instances))\n        tar_idx = user_picklist(i_info, command)\n\n    else:\n        tar_idx = 0\n    tar_inst = i_info[tar_idx]['id']\n    print(\"{0}{3}ing{1} instance id {2}{4}{1}\".\n          format(C_STAT[command], C_NORM, C_TI, command, tar_inst))\n    return (tar_inst, tar_idx)", "category": "Python"}, {"instruction": "def turn_left(self, angle_degrees, rate=RATE):\n        \"\"\"\n        Turn to the left, staying on the spot\n\n        :param angle_degrees: How far to turn (degrees)\n        :param rate: The trurning speed (degrees/second)\n        :return:\n        \"\"\"\n", "input": "", "output": "        flight_time = angle_degrees / rate\n\n        self.start_turn_left(rate)\n        time.sleep(flight_time)\n        self.stop()", "category": "Python"}, {"instruction": "def get_users(self, full_name=None, email=None, username=None):\n        \"\"\"\n        Send GET request to /users for users with optional full_name, email, and/or username filtering.\n        :param full_name: str name of the user we are searching for\n        :param email: str: optional email to filter by\n        :param username: str: optional username to filter by\n        :return: requests.Response containing the successful result\n        \"\"\"\n", "input": "", "output": "        data = {}\n        if full_name:\n            data['full_name_contains'] = full_name\n        if email:\n            data['email'] = email\n        if username:\n            data['username'] = username\n        return self._get_collection('/users', data)", "category": "Python"}, {"instruction": "def per_instance(*args, **kwargs):\n  \"\"\"A memoized key factory that works like `equal_args` except that the first parameter's identity\n  is used when forming the key.\n\n  This is a useful key factory when you want to enforce memoization happens per-instance for an\n  instance method in a class hierarchy that defines a custom `__hash__`/`__eq__`.\n  \"\"\"\n", "input": "", "output": "  instance_and_rest = (InstanceKey(args[0]),) + args[1:]\n  return equal_args(*instance_and_rest, **kwargs)", "category": "Python"}, {"instruction": "def estimate_frequency_for_zero(self, sample_rate: float, nbits=42) -> float:\n        \"\"\"\n        Calculates the frequency of at most nbits logical zeros and returns the mean of these frequencies\n\n        :param nbits:\n        :return:\n        \"\"\"\n", "input": "", "output": "        return self.__estimate_frequency_for_bit(False, sample_rate, nbits)", "category": "Python"}, {"instruction": "def split(x, split_dim, num_or_size_splits, name=None):\n  \"\"\"Like tf.split.\n\n  Args:\n    x: a Tensor\n    split_dim: a Dimension in x.shape.dims\n    num_or_size_splits: either an integer dividing split_dim.size\n       or a list of integers adding up to split_dim.size\n    name: an optional string\n  Returns:\n    a list of Tensors.\n  \"\"\"\n", "input": "", "output": "  return SplitOperation(x, split_dim, num_or_size_splits, name=name).outputs", "category": "Python"}, {"instruction": "def exists(self, path_or_index):\n        \"\"\"\n        Checks if a path exists in the document. This is meant to be used\n        for a corresponding :meth:`~couchbase.subdocument.exists` request.\n\n        :param path_or_index: The path (or index) to check\n        :return: `True` if the path exists, `False` if the path does not exist\n        :raise: An exception if the server-side check failed for a reason other\n            than the path not existing.\n        \"\"\"\n", "input": "", "output": "        result = self._resolve(path_or_index)\n        if not result[0]:\n            return True\n        elif E.SubdocPathNotFoundError._can_derive(result[0]):\n            return False\n        else:\n            raise E.exc_from_rc(result[0])", "category": "Python"}, {"instruction": "def contextMenuEvent(self, event):\r\n        \"\"\"Reimplements ShellBaseWidget method\"\"\"\n", "input": "", "output": "        state = self.has_selected_text()\r\n        self.copy_without_prompts_action.setEnabled(state)\r\n        ShellBaseWidget.contextMenuEvent(self, event)", "category": "Python"}, {"instruction": "def get_events(self, stack_name, chronological=True):\n        \"\"\"Get the events in batches and return in chronological order\"\"\"\n", "input": "", "output": "        next_token = None\n        event_list = []\n        while True:\n            if next_token is not None:\n                events = self.cloudformation.describe_stack_events(\n                    StackName=stack_name, NextToken=next_token\n                )\n            else:\n                events = self.cloudformation.describe_stack_events(\n                    StackName=stack_name\n                )\n            event_list.append(events['StackEvents'])\n            next_token = events.get('NextToken', None)\n            if next_token is None:\n                break\n            time.sleep(GET_EVENTS_SLEEP)\n        if chronological:\n            return reversed(sum(event_list, []))\n        else:\n            return sum(event_list, [])", "category": "Python"}, {"instruction": "def query_list_pager(con, idx, kind='2'):\n        '''\n        Get records of certain pager.\n        '''\n", "input": "", "output": "        all_list = MPost.query_under_condition(con, kind=kind)\n        return all_list[(idx - 1) * CMS_CFG['list_num']: idx * CMS_CFG['list_num']]", "category": "Python"}, {"instruction": "def import_private_key_from_file(filename, passphrase=None):\n    \"\"\"\n    Read a private Elliptic Curve key from a PEM file.\n\n    :param filename: The name of the file\n    :param passphrase: A pass phrase to use to unpack the PEM file.\n    :return: A\n        cryptography.hazmat.primitives.asymmetric.ec.EllipticCurvePrivateKey\n        instance\n    \"\"\"\n", "input": "", "output": "    with open(filename, \"rb\") as key_file:\n        private_key = serialization.load_pem_private_key(\n            key_file.read(),\n            password=passphrase,\n            backend=default_backend())\n\n    return private_key", "category": "Python"}, {"instruction": "def swap_twitter_subject(subject, body):\n    \"\"\"If subject starts from 'Tweet from...'\n    then we need to get first meaning line from the body.\"\"\"\n", "input": "", "output": "\n    if subject.startswith('Tweet from'):\n        lines = body.split('\\n')\n        for idx, line in enumerate(lines):\n            if re.match(r'.*, ?\\d{2}:\\d{2}]]', line) is not None:\n                try:\n                    subject = lines[idx + 1]\n                except IndexError:\n                    pass\n                break\n    return subject, body", "category": "Python"}, {"instruction": "def remove_xml_element_file(name, path):\n    \"\"\" Remove XML elements from a single file \"\"\"\n", "input": "", "output": "    ET.register_namespace(\"\", \"http://soap.sforce.com/2006/04/metadata\")\n    tree = elementtree_parse_file(path)\n    tree = remove_xml_element(name, tree)\n    return tree.write(path, encoding=UTF8, xml_declaration=True)", "category": "Python"}, {"instruction": "def xoscmounts(host_mount):\n    \"\"\"\n    Cross OS compatible mount dirs\n    \"\"\"\n", "input": "", "output": "    callback_lower_drive_letter = lambda pat: pat.group(1).lower()\n    host_mount = re.sub(r\"^([a-zA-Z])\\:\", callback_lower_drive_letter, host_mount)\n    host_mount = re.sub(r\"^([a-z])\", \"//\\\\1\", host_mount)\n    host_mount = re.sub(r\"\\\\\", \"/\", host_mount)\n    return host_mount", "category": "Python"}, {"instruction": "def organize(data, key):\n    \"\"\"\n    Iterate over a collection and group its values by the provided key\n    function. If the key is not callable (a function), then we wrap it in\n    templategetter(), so that you can simply do:\n\n    >>> organize([{'foo': 1}, {'foo': 2}, {'foo': 1}], \"{foo}\")\n    {'1': [{'foo': 1}, {'foo': 1}], '2': [{'foo': 2}]}\n    \"\"\"\n", "input": "", "output": "    groups = {}\n    if not callable(key):\n        key = templategetter(key)\n    for row in data:\n        k = key(row)\n        if groups.has_key(k):\n            groups[k].append(row)\n        else:\n            groups[k] = [row]\n    return groups", "category": "Python"}, {"instruction": "def error_catcher(self, extra_info: Optional[str] = None):\n        \"\"\"\n        Context manager to catch, print and record InstaloaderExceptions.\n\n        :param extra_info: String to prefix error message with.\"\"\"\n", "input": "", "output": "        try:\n            yield\n        except InstaloaderException as err:\n            if extra_info:\n                self.error('{}: {}'.format(extra_info, err))\n            else:\n                self.error('{}'.format(err))\n            if self.raise_all_errors:\n                raise", "category": "Python"}, {"instruction": "def play_NoteContainer(self, nc, channel=1, velocity=100):\n        \"\"\"Play the Notes in the NoteContainer nc.\"\"\"\n", "input": "", "output": "        self.notify_listeners(self.MSG_PLAY_NC, {'notes': nc,\n            'channel': channel, 'velocity': velocity})\n        if nc is None:\n            return True\n        for note in nc:\n            if not self.play_Note(note, channel, velocity):\n                return False\n        return True", "category": "Python"}, {"instruction": "def report_exception(self, filename, exc):\n        \"\"\"\n        This method is used when self.parser raises an Exception so that\n        we can report a customized :class:`EventReport` object with info the exception.\n        \"\"\"\n", "input": "", "output": "        # Build fake event.\n        event = AbinitError(src_file=\"Unknown\", src_line=0, message=str(exc))\n        return EventReport(filename, events=[event])", "category": "Python"}, {"instruction": "def _ensure_content_type():\n    \"\"\" Add the bulldog content type to the database if it's missing. \"\"\"\n", "input": "", "output": "    from django.contrib.contenttypes.models import ContentType\n    try:\n        row = ContentType.objects.get(app_label=PERM_APP_NAME)\n    except ContentType.DoesNotExist:\n        row = ContentType(name=PERM_APP_NAME, app_label=PERM_APP_NAME, model=PERM_APP_NAME)\n        row.save()\n    return row.id", "category": "Python"}, {"instruction": "def merge_schema(first, second):\n    \"\"\"Returns the result of merging the two given schemas.\n    \"\"\"\n", "input": "", "output": "    if not (type(first) == type(second) == dict):\n        raise ValueError(\"Argument is not a schema\")\n\n    if not (first.get('type') == second.get('type') == 'object'):\n        raise NotImplementedError(\"Unsupported root type\")\n\n    return merge_objects(first, second)", "category": "Python"}, {"instruction": "def get_assessment(self, assessment):\n        \"\"\"\n        To get Assessment by id\n        \"\"\"\n", "input": "", "output": "        response = self.http.get('/Assessment/' + str(assessment))\n        assessment = Schemas.Assessment(assessment=response)\n        return assessment", "category": "Python"}, {"instruction": "def try_unbuffered_file(file, _alreadyopen={}):\n    \"\"\" Try re-opening a file in an unbuffered mode and return it.\n        If that fails, just return the original file.\n        This function remembers the file descriptors it opens, so it\n        never opens the same one twice.\n\n        This is meant for files like sys.stdout or sys.stderr.\n    \"\"\"\n", "input": "", "output": "    try:\n        fileno = file.fileno()\n    except (AttributeError, UnsupportedOperation):\n        # Unable to use fileno to re-open unbuffered. Oh well.\n        # The output may be line buffered, which isn't that great for\n        # repeatedly drawing and erasing text, or hiding/showing the cursor.\n        return file\n    filedesc = _alreadyopen.get(fileno, None)\n    if filedesc is not None:\n        return filedesc\n\n    filedesc = fdopen(fileno, 'wb', 0)\n    _alreadyopen[fileno] = filedesc\n    # TODO: sys.stdout/stderr don't need to be closed.\n    #       But would it be worth it to try and close these opened files?\n    return filedesc", "category": "Python"}, {"instruction": "def i2len(self, pkt, val):\n        \"\"\"get the length of the field, including the padding length\"\"\"\n", "input": "", "output": "        fld_len = self._fld.i2len(pkt, val)\n        return fld_len + self.padlen(fld_len)", "category": "Python"}, {"instruction": "def handle_data(self, data):\n        \"\"\"\n        Any space immediately following another collapsible space will be\n        collapsed.\n\n        .. seealso::\n           `CSS Text Module Level 3 - The White Space Processing Rules\n            <http://www.w3.org/TR/css3-text/#egbidiwscollapse>`_\n        \"\"\"\n", "input": "", "output": "\n        tag_stack = self._tag_stack\n        if tag_stack and tag_stack[-1] in _RM_WS_ELEMENTS:\n            # just ignore the content of this element\n            assert data.strip() == ''\n            return\n\n        if self._preserve == 0:\n            if self._remove_begining_ws:\n                data = data.lstrip()\n                if not data:\n                    return\n\n                self._remove_begining_ws = False\n\n            self._last_text_idx = len(self._buffer)\n\n            data = _WS_PATTERN.sub(' ', data)\n            if data and data[-1] == ' ':\n                # immediately followed spaces will be collapsed\n                self._remove_begining_ws = True\n        else:\n            # the content cannot be stripped\n            self._reset_newline_status()\n\n        self._buffer.append(data)", "category": "Python"}, {"instruction": "def _labels_from_pyclusters(self):\n        \"\"\"\n        Computes and returns the list of labels indicating the data points and the corresponding cluster ids.\n\n        :return: The list of labels\n        \"\"\"\n", "input": "", "output": "        clusters = self.model.get_clusters()\n        labels = []\n        for i in range(0, len(clusters)):\n            for j in clusters[i]:\n                labels.insert(int(j), i)\n        return labels", "category": "Python"}, {"instruction": "def _include_term(self, term):\n        \"\"\"Add a single term to the current ontology.\n\n        It is needed to dereference any term in the term's relationship\n        and then to build the reference again to make sure the other\n        terms referenced in the term's relations are the one contained\n        in the ontology (to make sure changes to one term in the ontology\n        will be applied to every other term related to that term).\n        \"\"\"\n", "input": "", "output": "        ref_needed = False\n\n        if term.relations:\n\n            for k,v in six.iteritems(term.relations):\n                for i,t in enumerate(v):\n\n                    #if isinstance(t, Term):\n                    try:\n\n                        if t.id not in self:\n                            self._include_term(t)\n\n                        v[i] = t.id\n\n                    except AttributeError:\n                        pass\n\n                    ref_needed = True\n\n        self.terms[term.id] = term\n        return ref_needed", "category": "Python"}, {"instruction": "def register_agent(self, short_name):\n        \"\"\"Register to act as the RPC agent for this service.\n\n        After this cal succeeds, all requests to send RPCs to this service will be routed\n        through this agent.\n\n        Args:\n            short_name (str): A unique short name for this service that functions\n                as an id\n        \"\"\"\n", "input": "", "output": "\n        self._loop.run_coroutine(self._client.register_agent(short_name))", "category": "Python"}, {"instruction": "def get_deadline_metadata(self):\n        \"\"\"Gets the metadata for the assessment deadline.\n\n        return: (osid.Metadata) - metadata for the end time\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for osid.resource.ResourceForm.get_group_metadata_template\n        metadata = dict(self._mdata['deadline'])\n        metadata.update({'existing_date_time_values': self._my_map['deadline']})\n        return Metadata(**metadata)", "category": "Python"}, {"instruction": "def handle_annotations_url(self, line: str, position: int, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle statements like ``DEFINE ANNOTATION X AS URL \"Y\"``.\n\n        :raises: RedefinedAnnotationError\n        \"\"\"\n", "input": "", "output": "        keyword = tokens['name']\n        self.raise_for_redefined_annotation(line, position, keyword)\n\n        url = tokens['url']\n        self.annotation_url_dict[keyword] = url\n\n        if self.skip_validation:\n            return tokens\n\n        self.annotation_to_term[keyword] = self.manager.get_annotation_entry_names(url)\n\n        return tokens", "category": "Python"}, {"instruction": "def _pick_up_classes(modules, classnames):\n    \"\"\" Given a list of class names to retrieve, try to fetch them from the specified list of\n        modules and returns the list of the fetched classes.\n    \"\"\"\n", "input": "", "output": "    klasses = []\n    for classname in classnames:\n        klass = None\n        for module in modules:\n            if hasattr(module, classname):\n                klass = getattr(module, classname)\n                break\n        if not klass:\n            raise ClassNotFoundError('Error fetching \\'{}\\' in {}'.format(\n                classname, str([module.__name__ for module in modules]))\n            )\n        klasses.append(klass)\n    return klasses", "category": "Python"}, {"instruction": "def lookup_image(wildcard):\n  \"\"\"Returns unique ec2.Image whose name matches wildcard\n  lookup_ami('pytorch*').name => ami-29fa\n  \n  https://boto3.readthedocs.io/en/latest/reference/services/ec2.html#image\n\n  Assert fails if multiple images match or no images match.\n  \"\"\"\n", "input": "", "output": "\n  ec2 = get_ec2_resource()\n  filter_ = {'Name': 'name', 'Values': [wildcard]}\n\n  images = list(ec2.images.filter(Filters=[filter_]))\n\n  # Note, can add filtering by Owners as follows\n  #  images = list(ec2.images.filter_(Filters = [filter_], Owners=['self', 'amazon']))\n\n  assert len(images) <= 1, \"Multiple images match \" + str(wildcard)\n  assert len(images) > 0, \"No images match \" + str(wildcard)\n  return images[0]", "category": "Python"}, {"instruction": "def drop_layer(self, layer):\n        \"\"\"Removes the named layer and the value associated with it from the node.\n\n        Parameters\n        ----------\n        layer : str\n            Name of the layer to drop.\n\n        Raises\n        ------\n        TypeError\n            If the node is frozen\n        KeyError\n            If the named layer does not exist\n        \"\"\"\n", "input": "", "output": "        if self._frozen:\n            raise TypeError('Frozen ConfigTree does not support modification')\n        for child in self._children.values():\n            child.drop_layer(layer)\n        self._layers.remove(layer)", "category": "Python"}, {"instruction": "def get(self, request, *args, **kwargs):\n        \"\"\"\n        redirect user to captive page\n        with the social auth token in the querystring\n        (which will allow the captive page to send the token to freeradius)\n        \"\"\"\n", "input": "", "output": "        if not request.GET.get('cp'):\n            return HttpResponse(_('missing cp GET param'), status=400)\n        self.authorize(request, *args, **kwargs)\n        return HttpResponseRedirect(self.get_redirect_url(request))", "category": "Python"}, {"instruction": "def add_user_to_group(uid, gid, exist_ok=True):\n    \"\"\" Adds a user to a group within DCOS Enterprise.  The group and\n        user must exist.\n\n        :param uid: user id\n        :type uid: str\n        :param gid: group id\n        :type gid: str\n        :param exist_ok: True if it is ok for the relationship to pre-exist.\n        :type exist_ok: bool\n    \"\"\"\n", "input": "", "output": "    acl_url = urljoin(_acl_url(), 'groups/{}/users/{}'.format(gid, uid))\n    try:\n        r = http.put(acl_url)\n        assert r.status_code == 204\n    except DCOSHTTPException as e:\n        if e.response.status_code == 409 and exist_ok:\n            pass\n        else:\n            raise", "category": "Python"}, {"instruction": "def _filter_index_pages(docnames, base_dir):\n    \"\"\"Filter docnames to only yield paths of the form\n    ``<base_dir>/<name>/index``\n\n    Parameters\n    ----------\n    docnames : `list` of `str`\n        List of document names (``env.found_docs``).\n    base_dir : `str`\n        Base directory of all sub-directories containing index pages.\n\n    Yields\n    ------\n    docname : `str`\n        Document name that meets the pattern.\n    \"\"\"\n", "input": "", "output": "    for docname in docnames:\n        parts = docname.split('/')\n        if len(parts) == 3 and parts[0] == base_dir and parts[2] == 'index':\n            yield docname", "category": "Python"}, {"instruction": "def launch(url, wait=False, locate=False):\n    \"\"\"This function launches the given URL (or filename) in the default\n    viewer application for this file type.  If this is an executable, it\n    might launch the executable in a new session.  The return value is\n    the exit code of the launched application.  Usually, ``0`` indicates\n    success.\n\n    Examples::\n\n        click.launch('https://click.palletsprojects.com/')\n        click.launch('/my/downloaded/file', locate=True)\n\n    .. versionadded:: 2.0\n\n    :param url: URL or filename of the thing to launch.\n    :param wait: waits for the program to stop.\n    :param locate: if this is set to `True` then instead of launching the\n                   application associated with the URL it will attempt to\n                   launch a file manager with the file located.  This\n                   might have weird effects if the URL does not point to\n                   the filesystem.\n    \"\"\"\n", "input": "", "output": "    from ._termui_impl import open_url\n    return open_url(url, wait=wait, locate=locate)", "category": "Python"}, {"instruction": "def list_(prefix='', ruby=None, runas=None, gem_bin=None):\n    '''\n    List locally installed gems.\n\n    :param prefix: string :\n        Only list gems when the name matches this prefix.\n    :param gem_bin: string : None\n        Full path to ``gem`` binary to use.\n    :param ruby: string : None\n        If RVM or rbenv are installed, the ruby version and gemset to use.\n        Ignored if ``gem_bin`` is specified.\n    :param runas: string : None\n        The user to run gem as.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gem.list\n    '''\n", "input": "", "output": "    cmd = ['list']\n    if prefix:\n        cmd.append(prefix)\n    stdout = _gem(cmd,\n                  ruby,\n                  gem_bin=gem_bin,\n                  runas=runas)\n    ret = {}\n    for line in salt.utils.itertools.split(stdout, '\\n'):\n        match = re.match(r'^([^ ]+) \\((.+)\\)', line)\n        if match:\n            gem = match.group(1)\n            versions = match.group(2).split(', ')\n            ret[gem] = versions\n    return ret", "category": "Python"}, {"instruction": "def lookup_ip_host(self, mac):\n\t\t\"\"\"Lookup a host object with with given mac address.\n\n\t\t@type mac: str\n\t\t@raises ValueError:\n\t\t@raises OmapiError:\n\t\t@raises OmapiErrorNotFound: if no lease object with the given mac could be found\n\t\t@raises OmapiErrorAttributeNotFound: if lease could be found, but objects lacks a ip\n\t\t@raises socket.error:\n\t\t\"\"\"\n", "input": "", "output": "\t\tres = self.lookup_by_host(mac=mac)\n\t\ttry:\n\t\t\treturn res[\"ip-address\"]\n\t\texcept KeyError:\n\t\t\traise OmapiErrorAttributeNotFound()", "category": "Python"}, {"instruction": "def get_vnetwork_vswitches_output_instance_id(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_vnetwork_vswitches = ET.Element(\"get_vnetwork_vswitches\")\n        config = get_vnetwork_vswitches\n        output = ET.SubElement(get_vnetwork_vswitches, \"output\")\n        instance_id = ET.SubElement(output, \"instance-id\")\n        instance_id.text = kwargs.pop('instance_id')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def msgblock(key, text, side='|'):\n    \"\"\" puts text inside a visual ascii block \"\"\"\n", "input": "", "output": "    blocked_text = ''.join(\n        [' + --- ', key, ' ---\\n'] +\n        [' ' + side + ' ' + line + '\\n' for line in text.split('\\n')] +\n        [' L ___ ', key, ' ___\\n']\n    )\n    return blocked_text", "category": "Python"}, {"instruction": "def save_video(video, fps, save_filename='media/output.avi'):\n    \"\"\"Save a video to disk\"\"\"\n", "input": "", "output": "    # fourcc = cv2.CAP_PROP_FOURCC('M', 'J', 'P', 'G')\n    print(save_filename)\n    video = float_to_uint8(video)\n    fourcc = cv2.VideoWriter_fourcc(*'MJPG')\n    writer = cv2.VideoWriter(save_filename, fourcc, fps, (video.shape[2], video.shape[1]), 1)\n    for x in range(0, video.shape[0]):\n        res = cv2.convertScaleAbs(video[x])\n        writer.write(res)", "category": "Python"}, {"instruction": "def train(cls, data, isotonic=True):\n        \"\"\"\n        Train an isotonic regression model on the given data.\n\n        :param data:\n          RDD of (label, feature, weight) tuples.\n        :param isotonic:\n          Whether this is isotonic (which is default) or antitonic.\n          (default: True)\n        \"\"\"\n", "input": "", "output": "        boundaries, predictions = callMLlibFunc(\"trainIsotonicRegressionModel\",\n                                                data.map(_convert_to_vector), bool(isotonic))\n        return IsotonicRegressionModel(boundaries.toArray(), predictions.toArray(), isotonic)", "category": "Python"}, {"instruction": "def bootstrap(self, config):\n        \"\"\" Initialize a new node from scratch and start it. \"\"\"\n", "input": "", "output": "        pg_hba = config.get('pg_hba', [])\n        method = config.get('method') or 'initdb'\n        self._running_custom_bootstrap = method != 'initdb' and method in config and 'command' in config[method]\n        if self._running_custom_bootstrap:\n            do_initialize = self._custom_bootstrap\n            config = config[method]\n        else:\n            do_initialize = self._initdb\n        return do_initialize(config) and self.append_pg_hba(pg_hba) and self.save_configuration_files() \\\n            and self._configure_server_parameters() and self.start()", "category": "Python"}, {"instruction": "def _work(self):\n        \"\"\"Process the information regarding the available ports.\"\"\"\n", "input": "", "output": "        super(HyperVNeutronAgent, self)._work()\n        if self._nvgre_enabled:\n            self._nvgre_ops.refresh_nvgre_records()\n        self._port_enable_control_metrics()", "category": "Python"}, {"instruction": "def argmin(self):\n        \"\"\"\n        Return the co-ordinates of the bin centre containing the\n        minimum value.  Same as numpy.argmin(), converting the\n        indexes to bin co-ordinates.\n        \"\"\"\n", "input": "", "output": "        return tuple(centres[index] for centres, index in\n                     zip(self.centres(), numpy.unravel_index(self.array.argmin(),\n                                                             self.array.shape)))", "category": "Python"}, {"instruction": "def add_lines(self, lines):\n        '''\n        Add line vertices and edges to the mesh.\n\n        lines: A list of Polyline or Lines objects.\n\n        '''\n", "input": "", "output": "        import numpy as np\n        if not lines:\n            return\n\n        v_lines = np.vstack([l.v for l in lines])\n        v_index_offset = np.cumsum([0] + [len(l.v) for l in lines])\n        e_lines = np.vstack([l.e + v_index_offset[i] for i, l in enumerate(lines)])\n        num_body_verts = self.v.shape[0]\n        self.v = np.vstack([self.v, v_lines])\n        self.e = e_lines + num_body_verts", "category": "Python"}, {"instruction": "def _base64_md5hash(buffer_object):\n    \"\"\"Get MD5 hash of bytes (as base64).\n\n    :type buffer_object: bytes buffer\n    :param buffer_object: Buffer containing bytes used to compute an MD5\n                          hash (as base64).\n\n    :rtype: str\n    :returns: A base64 encoded digest of the MD5 hash.\n    \"\"\"\n", "input": "", "output": "    hash_obj = md5()\n    _write_buffer_to_hash(buffer_object, hash_obj)\n    digest_bytes = hash_obj.digest()\n    return base64.b64encode(digest_bytes)", "category": "Python"}, {"instruction": "def smear(idx, factor):\n    \"\"\"\n    This function will take as input an array of indexes and return every\n    unique index within the specified factor of the inputs.\n\n    E.g.: smear([5,7,100],2) = [3,4,5,6,7,8,9,98,99,100,101,102]\n\n    Parameters\n    -----------\n    idx : numpy.array of ints\n        The indexes to be smeared.\n    factor : idx\n        The factor by which to smear out the input array.\n\n    Returns\n    --------\n    new_idx : numpy.array of ints\n        The smeared array of indexes.\n    \"\"\"\n", "input": "", "output": "\n\n    s = [idx]\n    for i in range(factor+1):\n        a = i - factor/2\n        s += [idx + a]\n    return numpy.unique(numpy.concatenate(s))", "category": "Python"}, {"instruction": "def tempkeys(self):\r\n    \"\"\"Add a new level to make new keys temporary. Used instead of copy in sqex.\r\n    This may *seem* similar to a transaction but the tables are not being duplicated, just referenced.\r\n    At __exit__, old dict is restored (but changes to Tables remain).\r\n    \"\"\"\n", "input": "", "output": "    self.levels.append(dict(self.levels[-1]))\r\n    try: yield\r\n    finally: self.levels.pop()", "category": "Python"}, {"instruction": "def get_git_remote_url(path='.', remote='origin'):\n    \"\"\"\n    Get git remote url\n    :param path: path to repo\n    :param remote:\n    :return: remote url or exception\n    \"\"\"\n", "input": "", "output": "    return dulwich.repo.Repo.discover(path).get_config()\\\n        .get((b'remote', remote.encode('utf-8')), b'url').decode('utf-8')", "category": "Python"}, {"instruction": "def leave_room(room, sid=None, namespace=None):\n    \"\"\"Leave a room.\n\n    This function removes the user from a room, under the current namespace.\n    The user and the namespace are obtained from the event context. Example::\n\n        @socketio.on('leave')\n        def on_leave(data):\n            username = session['username']\n            room = data['room']\n            leave_room(room)\n            send(username + ' has left the room.', room=room)\n\n    :param room: The name of the room to leave.\n    :param sid: The session id of the client. If not provided, the client is\n                obtained from the request context.\n    :param namespace: The namespace for the room. If not provided, the\n                      namespace is obtained from the request context.\n    \"\"\"\n", "input": "", "output": "    socketio = flask.current_app.extensions['socketio']\n    sid = sid or flask.request.sid\n    namespace = namespace or flask.request.namespace\n    socketio.server.leave_room(sid, room, namespace=namespace)", "category": "Python"}, {"instruction": "def ext_language(ext, exts=None):\n    \"\"\"Language of the extension in those extensions\n\n    If exts is supplied, then restrict recognition to those exts only\n    If exts is not supplied, then use all known extensions\n\n    >>> ext_language('.py') == 'python'\n    True\n    \"\"\"\n", "input": "", "output": "    languages = {\n        '.py': 'python',\n        '.py2': 'python2',\n        '.py3': 'python3',\n        '.sh': 'bash',\n        '.bash': 'bash',\n        '.pl': 'perl',\n        '.js': 'javascript',\n        '.txt': 'english',\n    }\n    ext_languages = {_: languages[_] for _ in exts} if exts else languages\n    return ext_languages.get(ext)", "category": "Python"}, {"instruction": "def join(*vectors):\n    r\"\"\"\n    Takes an arbitrary number of aligned vectors of the same length and combines\n    them into a single vector (vertically).\n    \n    E.g. taking two 100-sample feature vectors of once 5 and once 7 features, a 100x12\n    feature vector is created and returned. \n    \n    The feature vectors are expected to have the form samples*features i.e.::\n    \n            s1    s2    s3    [...]\n        f1\n        f2\n        [...]\n    \n    Parameters\n    ----------\n    *vectors : sequences\n        A number of vectors with the same number of samples.\n    \n    Returns\n    -------\n    vector : ndarray\n        The combined vectors.\n    \"\"\"\n", "input": "", "output": "    # check supplied arguments\n    if len(vectors) < 2:\n        return vectors[0]\n\n    # process supplied arguments\n    vectors = list(vectors)\n    for i in range(len(vectors)):\n        vectors[i] = numpy.array(vectors[i], copy=False)\n        if vectors[i].ndim == 1:\n            vectors[i] = numpy.array([vectors[i]], copy=False).T\n    \n    # treat single-value cases special (no squeezing)\n    if 1 == len(vectors[0]):\n        return numpy.concatenate(vectors, 1)\n    \n    return numpy.squeeze(numpy.concatenate(vectors, 1))", "category": "Python"}, {"instruction": "def available():\n    \"\"\"\n        List available types for 'smudge'\n    \"\"\"\n", "input": "", "output": "\n    db = smudge_db.get()\n\n    click.echo('{:<6} {:<6} {:<50}'.format('Type', 'Offset', 'Magic'))\n    for k, v in db.items():\n        click.echo('{type:<6} {offset:<6} {magic}'.format(\n            type=k, magic=v['magic'].encode('hex'), offset=v['offset']))", "category": "Python"}, {"instruction": "def _convert_iterable(self, iterable):\n        \"\"\"Converts elements returned by an iterable into instances of\n        self._wrapper\n\n        \"\"\"\n", "input": "", "output": "        # Return original if _wrapper isn't callable\n        if not callable(self._wrapper):\n            return iterable\n\n        return [self._wrapper(x) for x in iterable]", "category": "Python"}, {"instruction": "def param_request_list_send(self, target_system, target_component, force_mavlink1=False):\n                '''\n                Request all parameters of this component. After this request, all\n                parameters are emitted.\n\n                target_system             : System ID (uint8_t)\n                target_component          : Component ID (uint8_t)\n\n                '''\n", "input": "", "output": "                return self.send(self.param_request_list_encode(target_system, target_component), force_mavlink1=force_mavlink1)", "category": "Python"}, {"instruction": "def getmembers(obj, *predicates):\n    \"\"\" Return all the members of an object as a list of `(key, value)` tuples, sorted by name.\n\n    The optional list of predicates can be used to filter the members.\n\n    The default predicate drops members whose name starts with '_'. To disable it, pass `None` as the first predicate.\n\n    :param obj: Object to list the members for\n    :param predicates: Functions to filter the members.\n\n        If the first value is not None, a default predicate is added that filters private members out (name starts with '_')\n\n    :type predicates: tuple[Callable|None]\n    :returns: Sorted list of (name, value) tuples\n    :rtype: list[(str, *)]\n    \"\"\"\n", "input": "", "output": "    # Add default\n    if not predicates or predicates[0] is not None:\n        predicates = (lambda key, value: not key.startswith('_'),) + predicates\n    # Build composite predicate\n    def predicate(key_value_tuple):\n        key, value = key_value_tuple\n        for p in predicates:\n            if p is not None and not p(key, value):\n                return False\n        return True\n    # Filter\n    return filter(predicate, inspect.getmembers(obj))", "category": "Python"}, {"instruction": "def get_zone_mode(self, zone_name):\n        \"\"\"\n        Get the mode for a zone\n        \"\"\"\n", "input": "", "output": "        zone = self.get_zone(zone_name)\n\n        if zone is None:\n            raise RuntimeError(\"Unknown zone\")\n\n        return ZoneMode(zone['mode'])", "category": "Python"}, {"instruction": "def level(self):\n        \"\"\"Extract log level if available (lazy).\"\"\"\n", "input": "", "output": "        if not self._level_calculated:\n            self._level_calculated = True\n            self._extract_level()\n        return self._level", "category": "Python"}, {"instruction": "def relevant_connections(n, _from, to):\n    \"\"\"Construct a connectivity matrix.\n\n    Args:\n        n (int): The dimensions of the matrix\n        _from (tuple[int]): Nodes with outgoing connections to ``to``\n        to (tuple[int]): Nodes with incoming connections from ``_from``\n\n    Returns:\n        np.ndarray: An |n x n| connectivity matrix with the |i,jth| entry is\n        ``1`` if |i| is in ``_from`` and |j| is in ``to``, and 0 otherwise.\n    \"\"\"\n", "input": "", "output": "    cm = np.zeros((n, n))\n\n    # Don't try and index with empty arrays. Older versions of NumPy\n    # (at least up to 1.9.3) break with empty array indices.\n    if not _from or not to:\n        return cm\n\n    cm[np.ix_(_from, to)] = 1\n    return cm", "category": "Python"}, {"instruction": "def readGraph(filename):\n    p = graph_parser.Parser()\n    \"\"\"\n    input: string, name of a file containing a Bioquali-like graph description\n    output: asp.TermSet, with atoms matching the contents of the input file\n\n    Parses a Bioquali-like graph description, and returns\n    a TermSet object.\n    Written using original Bioquali\n    \"\"\"\n", "input": "", "output": "\n    accu = TermSet()\n    file = open(filename,'r')\n    s = file.readline()\n    while s!=\"\":\n        try:\n            accu = p.parse(s)\n        except EOFError:\n            break\n        s = file.readline()\n\n    return accu", "category": "Python"}, {"instruction": "def TriCross(self, A=[(100, 0, 0), (0, 50, 60)], B=[(50, 50, 0), (0, 0, 100)]):\n\n        '''\n        Return the crosspoint of two line A and B in triangular coord.\n        :param A: first line\n        :type A: a list consist of two tuples, beginning and end point of the line\n        :param B: second line\n        :type B: a list consist of two tuples, beginning and end point of the line\n        :return:  the crosspoint of A and B\n        :rtype:   a list consist of three numbers, the x-y-z of the triangular coord\n        '''\n", "input": "", "output": "\n        x0, y0 = self.TriToBin(A[0][0], A[0][1], A[0][2])\n        x1, y1 = self.TriToBin(A[1][0], A[1][1], A[1][2])\n        x2, y2 = self.TriToBin(B[0][0], B[0][1], B[0][2])\n        x3, y3 = self.TriToBin(B[1][0], B[1][1], B[1][2])\n\n        b1 = (y1 - y0) / (x1 - x0)\n        b2 = (y3 - y2) / (x3 - x2)\n        c1 = y0 - b1 * x0\n        c2 = y2 - b2 * x2\n\n        x = (c2 - c1) / (b1 - b2)\n        y = b1 * x + c1\n\n        result = self.BinToTri(x, y)\n        return (result)", "category": "Python"}, {"instruction": "def utf8(string):\n    \"\"\"\n    Make sure string is utf8 encoded bytes.\n\n    If parameter is a object, object.__str__ will been called before encode as bytes\n    \"\"\"\n", "input": "", "output": "    if isinstance(string, six.text_type):\n        return string.encode('utf8')\n    elif isinstance(string, six.binary_type):\n        return string\n    else:\n        return six.text_type(string).encode('utf8')", "category": "Python"}, {"instruction": "def _change_height(self, ax, new_value):\n        \"\"\"Make bars in horizontal bar chart thinner\"\"\"\n", "input": "", "output": "        for patch in ax.patches:\n            current_height = patch.get_height()\n            diff = current_height - new_value\n\n            # we change the bar height\n            patch.set_height(new_value)\n\n            # we recenter the bar\n            patch.set_y(patch.get_y() + diff * .5)", "category": "Python"}, {"instruction": "def add(self, nickname, email=None, phone_number=None, user_id=None):\n        \"\"\"Add a user to the group.\n\n        You must provide either the email, phone number, or user_id that\n        uniquely identifies a user.\n\n        :param str nickname: new name for the user in the group\n        :param str email: email address of the user\n        :param str phone_number: phone number of the user\n        :param str user_id: user_id of the user\n        :return: a membership request\n        :rtype: :class:`MembershipRequest`\n        \"\"\"\n", "input": "", "output": "        member = {\n            'nickname': nickname,\n            'email': email,\n            'phone_number': phone_number,\n            'user_id': user_id,\n        }\n        return self.add_multiple(member)", "category": "Python"}, {"instruction": "def isreal(obj):\n    \"\"\"\n    Test if the argument is a real number (float or integer).\n\n    :param obj: Object\n    :type  obj: any\n\n    :rtype: boolean\n    \"\"\"\n", "input": "", "output": "    return (\n        (obj is not None)\n        and (not isinstance(obj, bool))\n        and isinstance(obj, (int, float))\n    )", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"\n        Run the plugin\n\n        Parse and validate config.\n        Store in workflow workspace for later retrieval.\n        \"\"\"\n", "input": "", "output": "        if self.reactor_config_map:\n            self.log.info(\"reading config from REACTOR_CONFIG env variable\")\n            conf = read_yaml(self.reactor_config_map, 'schemas/config.json')\n        else:\n            config_filename = os.path.join(self.config_path, self.basename)\n            self.log.info(\"reading config from %s\", config_filename)\n            conf = read_yaml_from_file_path(config_filename, 'schemas/config.json')\n        reactor_conf = ReactorConfig(conf)\n        workspace = self.workflow.plugin_workspace.setdefault(self.key, {})\n        workspace[WORKSPACE_CONF_KEY] = reactor_conf\n\n        self.log.info(\"reading config content %s\", reactor_conf.conf)\n\n        # need to stash this on the workflow for access in a place that can't import this module\n        self.workflow.default_image_build_method = get_default_image_build_method(self.workflow)", "category": "Python"}, {"instruction": "def while_do(self, classical_reg, q_program):\n        \"\"\"\n        While a classical register at index classical_reg is 1, loop q_program\n\n        Equivalent to the following construction:\n\n        .. code::\n\n            WHILE [c]:\n               instr...\n            =>\n              LABEL @START\n              JUMP-UNLESS @END [c]\n              instr...\n              JUMP @START\n              LABEL @END\n\n        :param int classical_reg: The classical register to check\n        :param Program q_program: The Quil program to loop.\n        :return: The Quil Program with the loop instructions added.\n        :rtype: Program\n        \"\"\"\n", "input": "", "output": "        label_start = LabelPlaceholder(\"START\")\n        label_end = LabelPlaceholder(\"END\")\n        self.inst(JumpTarget(label_start))\n        self.inst(JumpUnless(target=label_end, condition=classical_reg))\n        self.inst(q_program)\n        self.inst(Jump(label_start))\n        self.inst(JumpTarget(label_end))\n        return self", "category": "Python"}, {"instruction": "def groups_set_description(self, room_id, description, **kwargs):\n        \"\"\"Sets the description for the private group.\"\"\"\n", "input": "", "output": "        return self.__call_api_post('groups.setDescription', roomId=room_id, description=description, kwargs=kwargs)", "category": "Python"}, {"instruction": "def CreateRunner(self, **kw):\n    \"\"\"Make a new runner.\"\"\"\n", "input": "", "output": "    self.runner = HuntRunner(self, token=self.token, **kw)\n    return self.runner", "category": "Python"}, {"instruction": "def hasColumn(self, column, recurse=True, flags=0):\n        \"\"\"\n        Returns whether or not this column exists within the list of columns\n        for this schema.\n        \n        :return     <bool>\n        \"\"\"\n", "input": "", "output": "        return column in self.columns(recurse=recurse, flags=flags)", "category": "Python"}, {"instruction": "def paintEvent(self, event):\r\n        \"\"\"\r\n        Overloads the paint event to support rendering of hints if there are\r\n        no items in the tree.\r\n        \r\n        :param      event | <QPaintEvent>\r\n        \"\"\"\n", "input": "", "output": "        super(XTextEdit, self).paintEvent(event)\r\n        \r\n        if self.document().isEmpty() and self.hint():\r\n            text    = self.hint()\r\n            rect    = self.rect()\r\n            \r\n            # modify the padding on the rect\r\n            rect.setX(4)\r\n            rect.setY(4)\r\n            align = int(Qt.AlignLeft | Qt.AlignTop)\r\n            \r\n            # setup the coloring options\r\n            clr = self.hintColor()\r\n            \r\n            # paint the hint\r\n            with XPainter(self.viewport()) as painter:\r\n                painter.setPen(clr)\r\n                painter.drawText(rect, align | Qt.TextWordWrap, text)", "category": "Python"}, {"instruction": "def _run_program(self, bin, fastafile, params=None):\n        \"\"\"\n        Get enriched JASPAR motifs in a FASTA file.\n\n        Parameters\n        ----------\n        bin : str\n            Command used to run the tool.\n        \n        fastafile : str\n            Name of the FASTA input file.\n\n        params : dict, optional\n            Optional parameters. For some of the tools required parameters\n            are passed using this dictionary.\n\n        Returns\n        -------\n        motifs : list of Motif instances\n            The predicted motifs.\n\n        stdout : str\n            Standard out of the tool.\n        \n        stderr : str\n            Standard error of the tool.\n        \"\"\"\n", "input": "", "output": "        fname = os.path.join(self.config.get_motif_dir(), \"JASPAR2010_vertebrate.pwm\")\n        motifs = read_motifs(fname, fmt=\"pwm\")\n\n        for motif in motifs:\n            motif.id = \"JASPAR_%s\" % motif.id\n        return motifs, \"\", \"\"", "category": "Python"}, {"instruction": "def email_to(self):\n        \"\"\"\n        Return the value entered for the first field of type EmailField.\n        \"\"\"\n", "input": "", "output": "        for field in self.form_fields:\n            if field.is_a(fields.EMAIL):\n                return self.cleaned_data[field.slug]\n        return None", "category": "Python"}, {"instruction": "def _pre_capture_config(self, override_configs=None):\n        \"\"\"Utility function which configures the wireless interface per the\n        specified configurations. Operation is performed before every capture\n        start using baseline configurations (specified when sniffer initialized)\n        and override configurations specified here.\n        \"\"\"\n", "input": "", "output": "        final_configs = {}\n        if self._base_configs:\n            final_configs.update(self._base_configs)\n        if override_configs:\n            final_configs.update(override_configs)\n\n        if sniffer.Sniffer.CONFIG_KEY_CHANNEL in final_configs:\n            try:\n                subprocess.check_call([\n                    'iwconfig',\n                    self._interface,\n                    'channel',\n                    str(final_configs[sniffer.Sniffer.CONFIG_KEY_CHANNEL])])\n            except Exception as err:\n                raise sniffer.ExecutionError(err)", "category": "Python"}, {"instruction": "def junos_cli(command, format=None, dev_timeout=None, dest=None, **kwargs):\n    '''\n    .. versionadded:: 2019.2.0\n\n    Execute a CLI command and return the output in the specified format.\n\n    command\n        The command to execute on the Junos CLI.\n\n    format: ``text``\n        Format in which to get the CLI output (either ``text`` or ``xml``).\n\n    dev_timeout: ``30``\n        The NETCONF RPC timeout (in seconds).\n\n    dest\n        Destination file where the RPC output is stored. Note that the file will\n        be stored on the Proxy Minion. To push the files to the Master, use\n        :mod:`cp.push <salt.modules.cp.push>`.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' napalm.junos_cli 'show lldp neighbors'\n    '''\n", "input": "", "output": "    prep = _junos_prep_fun(napalm_device)  # pylint: disable=undefined-variable\n    if not prep['result']:\n        return prep\n    return __salt__['junos.cli'](command,\n                                 format=format,\n                                 dev_timeout=dev_timeout,\n                                 dest=dest,\n                                 **kwargs)", "category": "Python"}, {"instruction": "def get_terminal_width():\n    \"\"\" -> #int width of the terminal window \"\"\"\n", "input": "", "output": "    # http://www.brandonrubin.me/2014/03/18/python-snippet-get-terminal-width/\n    command = ['tput', 'cols']\n    try:\n        width = int(subprocess.check_output(command))\n    except OSError as e:\n        print(\n            \"Invalid Command '{0}': exit status ({1})\".format(\n                command[0], e.errno))\n    except subprocess.CalledProcessError as e:\n        print(\n            \"'{0}' returned non-zero exit status: ({1})\".format(\n                command, e.returncode))\n    else:\n        return width", "category": "Python"}, {"instruction": "async def bluetooth_scan():\n    \"\"\"Get nearby bluetooth devices.\"\"\"\n", "input": "", "output": "    async with aiohttp.ClientSession() as session:\n        ghlocalapi = Bluetooth(LOOP, session, IPADDRESS)\n        await ghlocalapi.scan_for_devices()  # Start device scan\n        await ghlocalapi.get_scan_result()  # Returns the result\n\n        print(\"Device info:\", ghlocalapi.devices)", "category": "Python"}, {"instruction": "def should_stop(self, result):\n        \"\"\"Whether the given result meets this trial's stopping criteria.\"\"\"\n", "input": "", "output": "\n        if result.get(DONE):\n            return True\n\n        for criteria, stop_value in self.stopping_criterion.items():\n            if criteria not in result:\n                raise TuneError(\n                    \"Stopping criteria {} not provided in result {}.\".format(\n                        criteria, result))\n            if result[criteria] >= stop_value:\n                return True\n\n        return False", "category": "Python"}, {"instruction": "def __public_objs(self):\n        \"\"\"\n        Returns a dictionary mapping a public identifier name to a\n        Python object.\n        \"\"\"\n", "input": "", "output": "        members = dict(inspect.getmembers(self.module))\n        return dict([(name, obj)\n                     for name, obj in members.items()\n                     if self.__is_exported(name, inspect.getmodule(obj))])", "category": "Python"}, {"instruction": "def D_ds(self, H_0, Om0, Ode0=None):\n        \"\"\"\n        angular diameter from deflector to source\n        :param H_0: Hubble parameter [km/s/Mpc]\n        :param Om0: normalized matter density at present time\n        :return: float [Mpc]\n        \"\"\"\n", "input": "", "output": "        lensCosmo = self._get_cosom(H_0, Om0, Ode0)\n        return lensCosmo.D_ds", "category": "Python"}, {"instruction": "def parent_suite(self):\n        \"\"\"Get the current parent suite.\n\n        A parent suite exists when a context within a suite is active. That is,\n        during execution of a tool within a suite, or after a user has entered\n        an interactive shell in a suite context, for example via the command-\n        line syntax 'tool +i', where 'tool' is an alias in a suite.\n\n        Returns:\n            `Suite` object, or None if there is no current parent suite.\n        \"\"\"\n", "input": "", "output": "        if self.context and self.context.parent_suite_path:\n            return Suite.load(self.context.parent_suite_path)\n        return None", "category": "Python"}]