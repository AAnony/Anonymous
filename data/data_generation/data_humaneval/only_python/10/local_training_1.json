[{"instruction": "def get_all_remote_methods(resolver=None, ns_prefix=''):\n    \"\"\"\n    Returns a dictionary to be used for calling ``djangoCall.configure()``, which itself extends the\n    Angular API to the client, offering him to call remote methods.\n    \"\"\"\n", "input": "", "output": "    if not resolver:\n        resolver = get_resolver(get_urlconf())\n    result = {}\n    for name in resolver.reverse_dict.keys():\n        if not isinstance(name, six.string_types):\n            continue\n        try:\n            url = reverse(ns_prefix + name)\n            resmgr = resolve(url)\n            ViewClass = import_string('{0}.{1}'.format(resmgr.func.__module__, resmgr.func.__name__))\n            if isclass(ViewClass) and issubclass(ViewClass, JSONResponseMixin):\n                result[name] = _get_remote_methods_for(ViewClass, url)\n        except (NoReverseMatch, ImproperlyConfigured):\n            pass\n    for namespace, ns_pattern in resolver.namespace_dict.items():\n        sub_res = get_all_remote_methods(ns_pattern[1], ns_prefix + namespace + ':')\n        if sub_res:\n            result[namespace] = sub_res\n    return result", "category": "Python"}, {"instruction": "def query_starts_with(query, prefixes):\n    \"\"\"Check if the query starts with any item from *prefixes*.\"\"\"\n", "input": "", "output": "    prefixes = [prefix.lower() for prefix in prefixes]\n    formatted_sql = sqlparse.format(query.lower(), strip_comments=True)\n    return bool(formatted_sql) and formatted_sql.split()[0] in prefixes", "category": "Python"}, {"instruction": "def forward_list(self):\n        '''\n        adb forward --list\n        TODO: not tested\n        '''\n", "input": "", "output": "        version = self.version()\n        if int(version[1]) <= 1 and int(version[2]) <= 0 and int(version[3]) < 31:\n            raise EnvironmentError(\"Low adb version.\")\n        lines = self.run_cmd(\"forward\", \"--list\").strip().splitlines()\n        return [line.strip().split() for line in lines]", "category": "Python"}, {"instruction": "def getWmWindowType(self, win, str=False):\n        \"\"\"\n        Get the list of window types of the given window (property\n        _NET_WM_WINDOW_TYPE).\n\n        :param win: the window object\n        :param str: True to get a list of string types instead of int\n        :return: list of (int|str)\n        \"\"\"\n", "input": "", "output": "        types = self._getProperty('_NET_WM_WINDOW_TYPE', win) or []\n        if not str:\n            return types\n        return [self._getAtomName(t) for t in types]", "category": "Python"}, {"instruction": "def parse_resource_from_url(self, url):\n        \"\"\"\n        Returns the appropriate resource name for the given URL.\n\n        :param url:  API URL stub, like: '/api/hosts'\n        :return: Resource name, like 'hosts', or None if not found\n        \"\"\"\n", "input": "", "output": "        # special case for the api root\n        if url == '/api':\n            return 'api'\n        elif url == '/katello':\n            return 'katello'\n\n        match = self.resource_pattern.match(url)\n        if match:\n            return match.groupdict().get('resource', None)", "category": "Python"}, {"instruction": "def transitive_reduction(self):\n        \"\"\" Performs a transitive reduction on the DAG. The transitive\n        reduction of a graph is a graph with as few edges as possible with the\n        same reachability as the original graph.\n\n        See https://en.wikipedia.org/wiki/Transitive_reduction\n        \"\"\"\n", "input": "", "output": "        combinations = []\n        for node, edges in self.graph.items():\n            combinations += [[node, edge] for edge in edges]\n\n        while True:\n            new_combinations = []\n            for comb1 in combinations:\n                for comb2 in combinations:\n                    if not comb1[-1] == comb2[0]:\n                        continue\n                    new_entry = comb1 + comb2[1:]\n                    if new_entry not in combinations:\n                        new_combinations.append(new_entry)\n            if not new_combinations:\n                break\n            combinations += new_combinations\n\n        constructed = {(c[0], c[-1]) for c in combinations if len(c) != 2}\n        for node, edges in self.graph.items():\n            bad_nodes = {e for n, e in constructed if node == n}\n            self.graph[node] = edges - bad_nodes", "category": "Python"}, {"instruction": "def sort_ordered_objects(items, getter=lambda x: x):\n    \"\"\"Sort an iterable of OrderedBase instances.\n\n    Args:\n        items (iterable): the objects to sort\n        getter (callable or None): a function to extract the OrderedBase instance from an object.\n\n    Examples:\n        >>> sort_ordered_objects([x, y, z])\n        >>> sort_ordered_objects(v.items(), getter=lambda e: e[1])\n    \"\"\"\n", "input": "", "output": "    return sorted(items, key=lambda x: getattr(getter(x), OrderedBase.CREATION_COUNTER_FIELD, -1))", "category": "Python"}, {"instruction": "def _build_collapse_to_gene_dict(graph) -> Dict[BaseEntity, Set[BaseEntity]]:\n    \"\"\"Build a collapse dictionary.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :return: A dictionary of {node: set of PyBEL node tuples}\n    \"\"\"\n", "input": "", "output": "    collapse_dict = defaultdict(set)\n    r2g = {}\n\n    for gene_node, rna_node, d in graph.edges(data=True):\n        if d[RELATION] != TRANSCRIBED_TO:\n            continue\n\n        collapse_dict[gene_node].add(rna_node)\n        r2g[rna_node] = gene_node\n\n    for rna_node, protein_node, d in graph.edges(data=True):\n        if d[RELATION] != TRANSLATED_TO:\n            continue\n\n        if rna_node not in r2g:\n            raise ValueError('Should complete origin before running this function')\n\n        collapse_dict[r2g[rna_node]].add(protein_node)\n\n    return collapse_dict", "category": "Python"}, {"instruction": "def clear(self):\n        \"\"\"Clears the paginator to have no pages.\"\"\"\n", "input": "", "output": "        if self.prefix is not None:\n            self._current_page = [self.prefix]\n            self._count = len(self.prefix) + 1 # prefix + newline\n        else:\n            self._current_page = []\n            self._count = 0\n        self._pages = []", "category": "Python"}, {"instruction": "def get_probs(self, x):\n    \"\"\"\n    :param x: A symbolic representation of the network input.\n    :return: A symbolic representation of the probs\n    \"\"\"\n", "input": "", "output": "    name = self._get_softmax_name()\n\n    return self.get_layer(x, name)", "category": "Python"}, {"instruction": "def reflected_light_intensity(self):\n        \"\"\"\n        A measurement of the reflected light intensity, as a percentage.\n        \"\"\"\n", "input": "", "output": "        self._ensure_mode(self.MODE_REFLECT)\n        return self.value(0) * self._scale('REFLECT')", "category": "Python"}, {"instruction": "async def remove_offer(self, **params):\n\t\t\"\"\"Receives offfer after have deal\n\t\tAccepts:\n\t\t\t- cid\n\t\t\t- buyer address\n\t\t\t- coin ID\n\t\t\"\"\"\n", "input": "", "output": "\t\tif params.get(\"message\"):\n\t\t\tparams = json.loads(params.get(\"message\", \"{}\"))\n\t\t\n\t\tif not params:\n\t\t\treturn {\"error\":400, \"reason\":\"Missed required fields\"}\n\n\t\t# Check if required fields exists\n\t\tcid = int(params.get(\"cid\", 0))\n\t\tbuyer_address = params.get(\"buyer_address\")\n\t\tcoinid = params.get(\"coinid\")\n\n\t\ttry:\n\t\t\tcoinid = coinid.replace(\"TEST\", \"\")\n\t\texcept:\n\t\t\tpass\n\t\n\t\t# Check if required fileds \n\t\tif not all([cid, buyer_address]):\n\t\t\treturn {\"error\":400, \"reason\":\"Missed required fields\"}\n\t\t\n\t\t# Try to find offer with account id and cid\t\n\t\toffer = await self.get_offer(cid=cid, buyer_address=buyer_address, coinid=coinid)\n\t\tif \"error\" in offer.keys():\n\t\t\treturn offer\n\n\t\t# Remove offer\n\t\tdatabase = client[coinid]\n\t\toffer_collection = database[settings.OFFER]\n\t\tawait offer_collection.delete_one(\n\t\t\t\t\t\t\t{\"account_id\":offer[\"account_id\"],\n\t\t\t\t\t\t\t\"cid\":cid})\n\t\treturn {\"result\": \"ok\"}", "category": "Python"}, {"instruction": "def resize(self, width, height, **kwargs):\n        \"\"\"Resizes the image to the supplied width/height. Returns the\n        instance. Supports the following optional keyword arguments:\n\n        mode - The resizing mode to use, see Image.MODES\n        filter - The filter to use: see Image.FILTERS\n        background - The hexadecimal background fill color, RGB or ARGB\n        position - The position used to crop: see Image.POSITIONS for\n                   pre-defined positions or a custom position ratio\n        retain - The minimum percentage of the original image to retain\n                 when cropping\n        \"\"\"\n", "input": "", "output": "        opts = Image._normalize_options(kwargs)\n        size = self._get_size(width, height)\n        if opts[\"mode\"] == \"adapt\":\n            self._adapt(size, opts)\n        elif opts[\"mode\"] == \"clip\":\n            self._clip(size, opts)\n        elif opts[\"mode\"] == \"fill\":\n            self._fill(size, opts)\n        elif opts[\"mode\"] == \"scale\":\n            self._scale(size, opts)\n        else:\n            self._crop(size, opts)\n        return self", "category": "Python"}, {"instruction": "def get_by_id(self, business_id, **url_params):\n        \"\"\"Make a request to the business details endpoint. More info at\n        https://www.yelp.com/developers/documentation/v3/business\n\n        Args:\n            business_id (str): The business alias (i.e. yelp-san-francisco) or\n                ID (i.e. 4kMBvIEWPxWkWKFN__8SxQ.\n            **url_params: Dict corresponding to business API params\n                https://www.yelp.com/developers/documentation/v3/business\n\n        Returns:\n            yelp.obj.business.Business object that wraps the response.\n\n        \"\"\"\n", "input": "", "output": "        business_path = BUSINESS_PATH.format(business_id=business_id)\n        response = self.client._make_request(business_path, url_params=url_params)\n        return Business(response)", "category": "Python"}, {"instruction": "def _copy_if_necessary(self, local_path, overwrite):\n        \"\"\"\n        Return cached path to local file, copying it to the cache if necessary.\n        \"\"\"\n", "input": "", "output": "        local_path = abspath(local_path)\n        if not exists(local_path):\n            raise MissingLocalFile(local_path)\n        elif not self.copy_local_files_to_cache:\n            return local_path\n        else:\n            cached_path = self.cached_path(local_path)\n            if exists(cached_path) and not overwrite:\n                return cached_path\n            copy2(local_path, cached_path)\n            return cached_path", "category": "Python"}, {"instruction": "def execute_console_command(frame, thread_id, frame_id, line, buffer_output=True):\n    \"\"\"fetch an interactive console instance from the cache and\n    push the received command to the console.\n\n    create and return an instance of console_message\n    \"\"\"\n", "input": "", "output": "    console_message = ConsoleMessage()\n\n    interpreter = get_interactive_console(thread_id, frame_id, frame, console_message)\n    more, output_messages, error_messages = interpreter.push(line, frame, buffer_output)\n    console_message.update_more(more)\n\n    for message in output_messages:\n        console_message.add_console_message(CONSOLE_OUTPUT, message)\n\n    for message in error_messages:\n        console_message.add_console_message(CONSOLE_ERROR, message)\n\n    return console_message", "category": "Python"}, {"instruction": "def load_file(self, filepath):\n        \"\"\"\n        This function opens any type of a readable file and decompose\n        the file object into a list, for each line, of lists containing\n        splitted line strings using space as a spacer.\n\n        Parameters\n        ----------\n        filepath : :class:`str`\n            The full path or a relative path to any type of file.\n\n        Returns\n        -------\n        :class:`dict`\n            Returns a dictionary containing the molecular information\n            extracted from the input files. This information will\n            vary with file type and information stored in it.\n            The data is sorted into lists that contain one feature\n            for example key atom_id: [atom_id_1, atom_id_2]\n            Over the process of analysis this dictionary will be updated\n            with new data.\n        \"\"\"\n", "input": "", "output": "        self.file_path = filepath\n        _, self.file_type = os.path.splitext(filepath)\n        _, self.file_name = os.path.split(filepath)\n        with open(filepath) as ffile:\n            self.file_content = ffile.readlines()\n\n        return (self._load_funcs[self.file_type]())", "category": "Python"}, {"instruction": "def volume_list(search_opts=None, profile=None, **kwargs):\n    '''\n    List storage volumes\n\n    search_opts\n        Dictionary of search options\n\n    profile\n        Profile to use\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' nova.volume_list search_opts='{\"display_name\": \"myblock\"}' profile=openstack\n\n    '''\n", "input": "", "output": "    conn = _auth(profile, **kwargs)\n    return conn.volume_list(search_opts=search_opts)", "category": "Python"}, {"instruction": "def mul_table(self, other):\n        \"\"\"\n        Fast multiplication using a the LWNAF precomputation table.\n        \"\"\"\n", "input": "", "output": "        # Get a BigInt\n        other = coerceBigInt(other)\n        if not other:\n            return NotImplemented\n        other %= orderG2()\n\n        # Building the precomputation table, if there is not one already.\n        if not self._table:\n            self._table = lwnafTable()\n            librelic.ep2_mul_pre_lwnaf(byref(self._table), byref(self))\n\n\n        result = G2Element()\n        librelic.ep2_mul_fix_lwnaf(byref(result), byref(self._table), \n            byref(other))\n        return result", "category": "Python"}, {"instruction": "def top_terms(results=15):\n    \"\"\"Get a list of the top overall terms\n        \n    Args:\n    \n    Kwargs:\n        results (int): An integer number of results to return\n        \n    Returns:\n        A list of term document dicts\n    \n    Example:\n    \n    >>> terms = artist.top_terms(results=5)\n    >>> terms\n    [{u'frequency': 1.0, u'name': u'rock'},\n     {u'frequency': 0.99054710039307992, u'name': u'electronic'},\n     {u'frequency': 0.96131624654034398, u'name': u'hip hop'},\n     {u'frequency': 0.94358477322411127, u'name': u'jazz'},\n     {u'frequency': 0.94023302416455468, u'name': u'pop rock'}]\n    >>> \n    \"\"\"\n", "input": "", "output": "    \n    kwargs = {}\n    if results:\n        kwargs['results'] = results\n    \n    ", "category": "Python"}, {"instruction": "def jboss_standalone_main_config_files(broker):\n        \"\"\"Command: JBoss standalone main config files\"\"\"\n", "input": "", "output": "        ps = broker[DefaultSpecs.ps_auxww].content\n        results = []\n        search = re.compile(r\"\\-Djboss\\.server\\.base\\.dir=(\\S+)\").search\n        # JBoss progress command content should contain jboss.home.dir\n        for p in ps:\n            if '-D[Standalone]' in p:\n                match = search(p)\n                # Only get the path which is absolute\n                if match and match.group(1)[0] == \"/\":\n                    main_config_path = match.group(1)\n                    main_config_file = \"standalone.xml\"\n                    if \" -c \" in p:\n                        main_config_file = p.split(\" -c \")[1].split()[0]\n                    elif \"--server-config\" in p:\n                        main_config_file = p.split(\"--server-config=\")[1].split()[0]\n                    results.append(main_config_path + \"/\" + main_config_file)\n        return list(set(results))", "category": "Python"}, {"instruction": "def validate_regexp(pattern, flags=0):\n    \"\"\"\n    Validate the field matches the given regular expression.\n    Should work with anything that supports '==' operator.\n\n    :param pattern: Regular expresion to match. String or regular expression instance.\n    :param pattern: Flags for the regular expression.\n    :raises: ``ValidationError('equal')``\n    \"\"\"\n", "input": "", "output": "    regex = re.compile(pattern, flags) if isinstance(pattern, str) else pattern\n\n    def regexp_validator(field, data):\n        if field.value is None:\n            return\n        if regex.match(str(field.value)) is None:\n            raise ValidationError('regexp', pattern=pattern)\n    return regexp_validator", "category": "Python"}, {"instruction": "def validate(self):\n        \"\"\"Validate the edges.\"\"\"\n", "input": "", "output": "\n        for function in compat_itervalues(self.functions):\n            for callee_id in compat_keys(function.calls):\n                assert function.calls[callee_id].callee_id == callee_id\n                if callee_id not in self.functions:\n                    sys.stderr.write('warning: call to undefined function %s from function %s\\n' % (str(callee_id), function.name))\n                    del function.calls[callee_id]", "category": "Python"}, {"instruction": "def _bytes_to_json(value):\n    \"\"\"Coerce 'value' to an JSON-compatible representation.\"\"\"\n", "input": "", "output": "    if isinstance(value, bytes):\n        value = base64.standard_b64encode(value).decode(\"ascii\")\n    return value", "category": "Python"}, {"instruction": "def get_station_temperature_datetime(self, station_id):\n        \"\"\" Return temperature measurement datetime for a given station \"\"\"\n", "input": "", "output": "        request = requests.get(\n            \"{}/station/{}/parameters/temperature/datetime\".format(\n                self.base_url, station_id))\n        if request.status_code != 200:\n            return None\n        return datetime.strptime(request.json(), \"%Y-%m-%dT%H:%M:%S\")", "category": "Python"}, {"instruction": "def put_container(self, url, container, container_headers=None):\n        \"\"\"Create a container if it is not Found.\n\n        :param url:\n        :param container:\n        \"\"\"\n", "input": "", "output": "\n        headers, container_uri = self._return_base_data(\n            url=url,\n            container=container,\n            container_headers=container_headers\n        )\n\n        resp = self._header_getter(\n            uri=container_uri,\n            headers=headers\n        )\n        if resp.status_code == 404:\n            return self._putter(uri=container_uri, headers=headers)\n        else:\n            return resp", "category": "Python"}, {"instruction": "def moveoutletstostrm(np, flowdir, streamRaster, outlet, modifiedOutlet,\n                          workingdir=None, mpiexedir=None,\n                          exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run move the given outlets to stream\"\"\"\n", "input": "", "output": "        fname = TauDEM.func_name('moveoutletstostrm')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-p': flowdir, '-src': streamRaster, '-o': outlet},\n                          workingdir,\n                          None,\n                          {'-om': modifiedOutlet},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "category": "Python"}, {"instruction": "def ensure_routing_table_is_fresh(self, access_mode):\n        \"\"\" Update the routing table if stale.\n\n        This method performs two freshness checks, before and after acquiring\n        the refresh lock. If the routing table is already fresh on entry, the\n        method exits immediately; otherwise, the refresh lock is acquired and\n        the second freshness check that follows determines whether an update\n        is still required.\n\n        This method is thread-safe.\n\n        :return: `True` if an update was required, `False` otherwise.\n        \"\"\"\n", "input": "", "output": "        if self.routing_table.is_fresh(access_mode):\n            return False\n        with self.refresh_lock:\n            if self.routing_table.is_fresh(access_mode):\n                if access_mode == READ_ACCESS:\n                    # if reader is fresh but writers is not fresh, then we are reading in absence of writer\n                    self.missing_writer = not self.routing_table.is_fresh(WRITE_ACCESS)\n                return False\n            self.update_routing_table()\n            self.update_connection_pool()\n            return True", "category": "Python"}, {"instruction": "def group_records_by_domain(self):\r\n        \"\"\"Return the records grouped by the domain they came from.\r\n        \r\n        The return value is a dict, a key in this dict is a domain\r\n        and the value is a list of all the records with this domain.\r\n        \r\n        \"\"\"\n", "input": "", "output": "        key_function = lambda record: record.source.domain\r\n        return self.group_records(key_function)", "category": "Python"}, {"instruction": "def call(self, procedure, *args, **kwargs):\n        \"\"\"Call a remote procedure.\n\n        Replace :meth:`autobahn.wamp.interface.IApplicationSession.call`\n        \"\"\"\n", "input": "", "output": "        return self._async_session.call(procedure, *args, **kwargs)", "category": "Python"}, {"instruction": "def get_print_setup(self, print_data):\n        \"\"\"Opens print setup dialog and returns print_data\"\"\"\n", "input": "", "output": "\n        psd = wx.PageSetupDialogData(print_data)\n        # psd.EnablePrinter(False)\n        psd.CalculatePaperSizeFromId()\n        dlg = wx.PageSetupDialog(self.main_window, psd)\n        dlg.ShowModal()\n\n        # this makes a copy of the wx.PrintData instead of just saving\n        # a reference to the one inside the PrintDialogData that will\n        # be destroyed when the dialog is destroyed\n        data = dlg.GetPageSetupData()\n        new_print_data = wx.PrintData(data.GetPrintData())\n        new_print_data.PaperId = data.PaperId\n        new_print_data.PaperSize = data.PaperSize\n\n        dlg.Destroy()\n\n        return new_print_data", "category": "Python"}, {"instruction": "def retrieve_authorization_code(self, redirect_func=None):\n        \"\"\" retrieve authorization code to get access token\n        \"\"\"\n", "input": "", "output": "        \n        request_param = {\n            \"client_id\": self.client_id,\n            \"redirect_uri\": self.redirect_uri,\n            }\n\n        if self.scope:\n            request_param['scope'] = self.scope\n\n        if self._extra_auth_params:\n            request_param.update(self._extra_auth_params)\n\n        r = requests.get(self.auth_uri, params=request_param,\n                         allow_redirects=False)\n        url = r.headers.get('location') \n        if self.local:\n            webbrowser.open_new_tab(url)\n            authorization_code = raw_input(\"Code: \")\n            if self.validate_code(authorization_code):\n                self.authorization_code = authorization_code\n        else:\n            return redirect_func(url)", "category": "Python"}, {"instruction": "def _setup_opera(self, capabilities):\n        \"\"\"Setup Opera webdriver\n\n        :param capabilities: capabilities object\n        :returns: a new local Opera driver\n        \"\"\"\n", "input": "", "output": "        opera_driver = self.config.get('Driver', 'opera_driver_path')\n        self.logger.debug(\"Opera driver path given in properties: %s\", opera_driver)\n        return webdriver.Opera(executable_path=opera_driver, desired_capabilities=capabilities)", "category": "Python"}, {"instruction": "def _full_to_yearly_ts(self, arr, dt):\n        \"\"\"Average the full timeseries within each year.\"\"\"\n", "input": "", "output": "        time_defined = self.def_time and not ('av' in self.dtype_in_time)\n        if time_defined:\n            arr = utils.times.yearly_average(arr, dt)\n        return arr", "category": "Python"}, {"instruction": "def parse_dates(d, default='today'):\n    \"\"\" Parses one or more dates from d \"\"\"\n", "input": "", "output": "\n    if default == 'today':\n        default = datetime.datetime.today()\n\n    if d is None:\n        return default\n\n    elif isinstance(d, _parsed_date_types):\n        return d\n\n    elif is_number(d):\n        # Treat as milliseconds since 1970\n        d = d if isinstance(d, float) else float(d)\n        return datetime.datetime.utcfromtimestamp(d)\n\n    elif not isinstance(d, STRING_TYPES):\n        if hasattr(d, '__iter__'):\n            return [parse_dates(s, default) for s in d]\n        else:\n            return default\n\n    elif len(d) == 0:\n        # Behaves like dateutil.parser < version 2.5\n        return default\n\n    else:\n        try:\n            return parser.parse(d)\n        except (AttributeError, ValueError):\n            return default", "category": "Python"}, {"instruction": "def translation_activate_block(function=None, language=None):\n    \"\"\"\n    Activate language only for one method or function\n    \"\"\"\n", "input": "", "output": "\n    def _translation_activate_block(function):\n        def _decorator(*args, **kwargs):\n            tmp_language = translation.get_language()\n            try:\n                translation.activate(language or settings.LANGUAGE_CODE)\n                return function(*args, **kwargs)\n            finally:\n                translation.activate(tmp_language)\n        return wraps(function)(_decorator)\n\n    if function:\n        return _translation_activate_block(function)\n    else:\n        return _translation_activate_block", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"\n        Run the application\n        \"\"\"\n", "input": "", "output": "        self.call_plugins(\"on_run\")\n        if vars(self.arguments).get(\"version\", None):\n            self.logger.info(\"{app_name}: {version}\".format(app_name=self.app_name, version=self.version))\n        else:\n            if self.arguments.command == \"main\":\n                self.main()\n            else:\n                self.subcommands[self.arguments.command].run()\n        self.call_plugins(\"on_end\")", "category": "Python"}, {"instruction": "def scratch_file(unlink=True, **kwargs):\n    \"\"\"Create a temporary file and return its name.\n\n    Additional arguments are passed to :class:`tempfile.NamedTemporaryFile`\n\n    At the start of the with block a secure, temporary file is created\n    and its name returned.  At the end of the with block it is\n    deleted.\n    \"\"\"\n", "input": "", "output": "    kwargs['delete'] = False\n    tf = tempfile.NamedTemporaryFile(**kwargs)\n    tf.close()\n    try:\n        yield tf.name\n    finally:\n        if unlink:\n            os.unlink(tf.name)", "category": "Python"}, {"instruction": "def render_dynamic_electrode_state_shapes(self):\n        '''\n        Render **dynamic** states reported by the electrode controller.\n\n        **Dynamic** electrode states are only applied while a protocol is\n        running -- _not_ while in real-time programming mode.\n\n        See also :meth:`render_electrode_shapes()`.\n\n\n        .. versionadded:: 0.12\n        '''\n", "input": "", "output": "        df_shapes = self.canvas.df_canvas_shapes.copy()\n        # Only include shapes for electrodes reported as actuated.\n        on_electrodes = self._dynamic_electrodes[self._dynamic_electrodes > 0]\n        df_shapes = (df_shapes.set_index('id').loc[on_electrodes.index]\n                     .reset_index())\n\n        return self.render_electrode_shapes(df_shapes=df_shapes,\n                                            shape_scale=0.75,\n                                            # Lignt blue\n                                            fill=(136 / 255.,\n                                                  189 / 255.,\n                                                  230 / 255.))", "category": "Python"}, {"instruction": "def unregister(self, fd):\n        \"\"\"\n        Unregister an USB-unrelated fd from poller.\n        Convenience method.\n        \"\"\"\n", "input": "", "output": "        if fd in self.__fd_set:\n            raise ValueError(\n                'This fd is a special USB event fd, it must stay registered.'\n            )\n        self.__poller.unregister(fd)", "category": "Python"}, {"instruction": "def random_id(k=5):\n  \"\"\"Random id to use for AWS identifiers.\"\"\"\n", "input": "", "output": "  #  https://stackoverflow.com/questions/2257441/random-string-generation-with-upper-case-letters-and-digits-in-python\n  return ''.join(random.choices(string.ascii_lowercase + string.digits, k=k))", "category": "Python"}, {"instruction": "def update_throughput(self, read_units, write_units):\n        \"\"\"\n        Update the ProvisionedThroughput for the Amazon DynamoDB Table.\n\n        :type read_units: int\n        :param read_units: The new value for ReadCapacityUnits.\n        \n        :type write_units: int\n        :param write_units: The new value for WriteCapacityUnits.\n        \"\"\"\n", "input": "", "output": "        self.layer2.update_throughput(self, read_units, write_units)", "category": "Python"}, {"instruction": "def path(self):\n    \"\"\"The full path to the map file: directory, filename and file ending.\"\"\"\n", "input": "", "output": "    if self.filename:\n      map_path = os.path.join(self.directory, self.filename)\n      if not map_path.endswith(\".SC2Map\"):\n        map_path += \".SC2Map\"\n      return map_path", "category": "Python"}, {"instruction": "def write_json_to_temp_file(data):\n    \"\"\"Writes JSON data to a temporary file and returns the path to it\"\"\"\n", "input": "", "output": "    fp = tempfile.NamedTemporaryFile(delete=False)\n    fp.write(json.dumps(data).encode('utf-8'))\n    fp.close()\n    return fp.name", "category": "Python"}, {"instruction": "def is_thin_archieve(self):\n        \"\"\"\n        Return the is thin archieve attribute of the BFD file being processed.\n        \"\"\"\n", "input": "", "output": "        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.IS_THIN_ARCHIEVE)", "category": "Python"}, {"instruction": "def check_job(self, job_id):\n        \"\"\"Return the state and number of results of a query by job id.\n\n        Parameters\n        ----------\n        job_id : str\n            The job id of the query to check.\n\n        Returns\n        -------\n        tuple\n            (``bool``, ``int``) Whether or not the query has completed and the\n            total number of rows included in the query table if it has\n            completed (else 0)\n        \"\"\"\n", "input": "", "output": "\n        query_reply = self.get_query_results(job_id, offset=0, limit=0)\n\n        return (query_reply.get('jobComplete', False),\n                int(query_reply.get('totalRows', 0)))", "category": "Python"}, {"instruction": "def get_cache_key(bucket, name, args, kwargs):\n    \"\"\"\n    Gets a unique SHA1 cache key for any call to a native tag.\n    Use args and kwargs in hash so that the same arguments use the same key\n    \"\"\"\n", "input": "", "output": "    u = ''.join(map(str, (bucket, name, args, kwargs)))\n    return 'native_tags.%s' % sha_constructor(u).hexdigest()", "category": "Python"}, {"instruction": "def flasher(msg, severity=None):\n    \"\"\"Flask's flash if available, logging call if not\"\"\"\n", "input": "", "output": "    try:\n        flash(msg, severity)\n    except RuntimeError:\n        if severity == 'danger':\n            logging.error(msg)\n        else:\n            logging.info(msg)", "category": "Python"}, {"instruction": "def get_sampleCross(self, res, DS=None, resMode='abs', ind=None):\n        \"\"\" Sample, with resolution res, the 2D cross-section\n\n        The sampling domain can be limited by DS or ind\n        \"\"\"\n", "input": "", "output": "        args = [self.Poly, self.dgeom['P1Min'][0], self.dgeom['P1Max'][0],\n                self.dgeom['P2Min'][1], self.dgeom['P2Max'][1], res]\n        kwdargs = dict(DS=DS, dSMode=resMode, ind=ind, margin=1.e-9)\n        pts, dS, ind, reseff = _comp._Ves_get_sampleCross(*args, **kwdargs)\n        return pts, dS, ind, reseff", "category": "Python"}, {"instruction": "def _pfp__show(self, level=0, include_offset=False):\n        \"\"\"Show the contents of the struct\n        \"\"\"\n", "input": "", "output": "        res = []\n        res.append(\"{}{} {{\".format(\n            \"{:04x} \".format(self._pfp__offset) if include_offset else \"\",\n            self._pfp__show_name\n        ))\n        for child in self._pfp__children:\n            res.append(\"{}{}{:10s} = {}\".format(\n                \"    \"*(level+1),\n                \"{:04x} \".format(child._pfp__offset) if include_offset else \"\",\n                child._pfp__name,\n                child._pfp__show(level+1, include_offset)\n            ))\n        res.append(\"{}}}\".format(\"    \"*level))\n        return \"\\n\".join(res)", "category": "Python"}, {"instruction": "def get_hosts(self):\n        \"\"\"\n        Return a list of parsed hosts info, with the limit applied if required.\n        \"\"\"\n", "input": "", "output": "        limited_hosts = {}\n        if self.limit is not None:\n            # Find hosts and groups of hosts to include\n            for include in self.limit['include']:\n                # Include whole group\n                for hostname in self.hosts_in_group(include):\n                    limited_hosts[hostname] = self.hosts[hostname]\n                # Include individual host\n                if include in self.hosts:\n                    limited_hosts[include] = self.hosts[include]\n            # Find hosts and groups of hosts to exclude\n            for exclude in self.limit[\"exclude\"]:\n                # Exclude whole group\n                for hostname in self.hosts_in_group(exclude):\n                    if hostname in limited_hosts:\n                        limited_hosts.pop(hostname)\n                # Exclude individual host\n                if exclude in limited_hosts:\n                    limited_hosts.pop(exclude)\n\n            return limited_hosts\n        else:\n            # Return all hosts\n            return self.hosts", "category": "Python"}, {"instruction": "def has_reg(value):\n    \"\"\"Return True if the given key exists in HKEY_LOCAL_MACHINE, False\n    otherwise.\"\"\"\n", "input": "", "output": "    try:\n        SCons.Util.RegOpenKeyEx(SCons.Util.HKEY_LOCAL_MACHINE, value)\n        ret = True\n    except SCons.Util.WinError:\n        ret = False\n    return ret", "category": "Python"}, {"instruction": "def explicit_indexing_adapter(\n        key, shape, indexing_support, raw_indexing_method):\n    \"\"\"Support explicit indexing by delegating to a raw indexing method.\n\n    Outer and/or vectorized indexers are supported by indexing a second time\n    with a NumPy array.\n\n    Parameters\n    ----------\n    key : ExplicitIndexer\n        Explicit indexing object.\n    shape : Tuple[int, ...]\n        Shape of the indexed array.\n    indexing_support : IndexingSupport enum\n        Form of indexing supported by raw_indexing_method.\n    raw_indexing_method: callable\n        Function (like ndarray.__getitem__) that when called with indexing key\n        in the form of a tuple returns an indexed array.\n\n    Returns\n    -------\n    Indexing result, in the form of a duck numpy-array.\n    \"\"\"\n", "input": "", "output": "    raw_key, numpy_indices = decompose_indexer(key, shape, indexing_support)\n    result = raw_indexing_method(raw_key.tuple)\n    if numpy_indices.tuple:\n        # index the loaded np.ndarray\n        result = NumpyIndexingAdapter(np.asarray(result))[numpy_indices]\n    return result", "category": "Python"}, {"instruction": "def get_bidi_paired_bracket_type_property(value, is_bytes=False):\n    \"\"\"Get `BPT` property.\"\"\"\n", "input": "", "output": "\n    obj = unidata.ascii_bidi_paired_bracket_type if is_bytes else unidata.unicode_bidi_paired_bracket_type\n\n    if value.startswith('^'):\n        negated = value[1:]\n        value = '^' + unidata.unicode_alias['bidipairedbrackettype'].get(negated, negated)\n    else:\n        value = unidata.unicode_alias['bidipairedbrackettype'].get(value, value)\n\n    return obj[value]", "category": "Python"}, {"instruction": "def debug_mode(header, data_object, debug=False, halt=False):\n    \"\"\" debug output \"\"\"\n", "input": "", "output": "    if debug:\n        print('\\n  ' + str(header) + '\\n')\n        try:\n            export_json_object(data_object)\n        except Exception:\n            print(data_object)\n        if halt:\n            sys.exit(0)\n    return True", "category": "Python"}, {"instruction": "def rewriteLoadCommands(self, changefunc):\n        \"\"\"\n        Rewrite the load commands based upon a change dictionary\n        \"\"\"\n", "input": "", "output": "        data = changefunc(self.parent.filename)\n        changed = False\n        if data is not None:\n            if self.rewriteInstallNameCommand(\n                    data.encode(sys.getfilesystemencoding())):\n                changed = True\n        for idx, name, filename in self.walkRelocatables():\n            data = changefunc(filename)\n            if data is not None:\n                if self.rewriteDataForCommand(idx, data.encode(\n                        sys.getfilesystemencoding())):\n                    changed = True\n        return changed", "category": "Python"}, {"instruction": "def bulk_query(self, query, *multiparams):\n        \"\"\"Bulk insert or update.\"\"\"\n", "input": "", "output": "\n        self._conn.execute(text(query), *multiparams)", "category": "Python"}, {"instruction": "def interpret(self, infile):\n        \"\"\" Process a file of rest and return json \"\"\"\n", "input": "", "output": "\n        # need row headings\n        data = pandas.read_csv(infile)\n\n        # FIXME find the right foo\n        return json.dumps(data.foo())", "category": "Python"}, {"instruction": "def cleanup():\n    \"\"\"Close all sockets at exit\"\"\"\n", "input": "", "output": "    for sck in list(Wdb._sockets):\n        try:\n            sck.close()\n        except Exception:\n            log.warn('Error in cleanup', exc_info=True)", "category": "Python"}, {"instruction": "def set_shares(self):\n        \"\"\"\n        Setta la variabile membro 'self.samba_shares' il quale e' una lista\n        di dizionari con i dati da passare ai comandi di \"umount\" e \"mount\".\n        I vari dizionari sono popolati o da un file ~/.pygmount.rc e da un\n        file passato dall'utente.\n        \"\"\"\n", "input": "", "output": "        if self.filename is None:\n            self.filenamename = os.path.expanduser(\n                '~%s/%s' % (self.host_username, FILE_RC))\n        if not os.path.exists(self.filename):\n            error_msg = (u\"Impossibile trovare il file di configurazione \"\n                         u\"'%s'.\\nLe unit\u00e0 di rete non saranno collegate.\" % (\n                             FILE_RC.lstrip('.')))\n            if not self.shell_mode:\n                ErrorMessage(error_msg)\n            logging.error(error_msg)\n            sys.exit(5)\n        if self.verbose:\n            logging.warning(\"File RC utilizzato: %s\", self.filename)\n        self.samba_shares = read_config(self.filename)", "category": "Python"}, {"instruction": "def create(cls, path_name=None, name=None, crawlable=True):\n        \"\"\"initialize an instance and save it to db.\"\"\"\n", "input": "", "output": "\n        project = cls(path_name, name, crawlable)\n\n        db.session.add(project)\n        db.session.commit()\n\n        return collect_results(project, force=True)", "category": "Python"}, {"instruction": "def return_features(self, names='all'):\n        \"\"\"\n        Returns a list of extracted features from the database\n\n        Parameters\n        ----------\n        names : list of strings, a list of feature names which are to be retrieved from the database, if equal\n        to 'all', the all features will be returned, default value: 'all'\n\n        Returns\n        -------\n        A list of lists, each 'inside list' corresponds to a single data point, each element of the 'inside list' is a\n        feature (can be of any type)\n        \"\"\"\n", "input": "", "output": "        if self._prepopulated is False:\n            raise errors.EmptyDatabase(self.dbpath)\n        else:\n            return return_features_base(self.dbpath, self._set_object, names)", "category": "Python"}, {"instruction": "def add_symbol(self, name, string=None):\n        \"\"\"\n        Add a symbol with key `name` to `scipy_data_fitting.Model.symbols`.\n        Optionally, specify an alternative `string` to pass to [`sympy.Symbol`][1],\n        otherwise `name` is used.\n\n        [1]: http://docs.sympy.org/dev/modules/core.html#id4\n        \"\"\"\n", "input": "", "output": "        if not string: string = name\n        self.symbols[name] = sympy.Symbol(string)", "category": "Python"}, {"instruction": "def _value_ref(self, column, value, *, dumped=False, inner=False):\n        \"\"\"inner=True uses column.typedef.inner_type instead of column.typedef\"\"\"\n", "input": "", "output": "        ref = \":v{}\".format(self.next_index)\n\n        # Need to dump this value\n        if not dumped:\n            typedef = column.typedef\n            for segment in path_of(column):\n                typedef = typedef[segment]\n            if inner:\n                typedef = typedef.inner_typedef\n            value = self.engine._dump(typedef, value)\n\n        self.attr_values[ref] = value\n        self.counts[ref] += 1\n        return ref, value", "category": "Python"}, {"instruction": "def generate(self, data, *args, **kwargs):\n        \"\"\"\n        \u6839\u636e\u4f20\u5165\u7684\u6570\u636e\u7ed3\u6784\u751f\u6210\u6700\u7ec8\u7528\u4e8e\u63a8\u9001\u7684\u6587\u4ef6\u5b57\u8282\u5b57\u7b26\u4e32( :func:`bytes` )\uff0c\n        MoEar\u4f1a\u5c06\u5176\u6301\u4e45\u5316\u5e76\u7528\u4e8e\u4e4b\u540e\u7684\u63a8\u9001\u4efb\u52a1\n\n        :param dict data: \u5f85\u6253\u5305\u7684\u6570\u636e\u7ed3\u6784\n        :return: \u8fd4\u56de\u751f\u6210\u7684\u4e66\u7c4d\u6253\u5305\u8f93\u51fa\u5b57\u8282\n        :rtype: bytes\n        \"\"\"\n", "input": "", "output": "        with tempfile.TemporaryDirectory() as tmpdirname:\n            self.options.setdefault('package_build_dir', tmpdirname)\n            crawler = CrawlerScript(self.options)\n            crawler.crawl(data, self.spider, *args, **kwargs)\n\n            output_file = os.path.join(\n                self.options['package_build_dir'], 'source', 'moear.mobi')\n            with open(output_file, 'rb') as fh:\n                content = fh.read()\n\n        return content", "category": "Python"}, {"instruction": "def install_hook(path):\n    \"\"\" Auto definition of SCM and hook installation. \"\"\"\n", "input": "", "output": "    git = op.join(path, '.git', 'hooks')\n    hg = op.join(path, '.hg')\n    if op.exists(git):\n        install_git(git)\n        LOGGER.warn('Git hook has been installed.')\n\n    elif op.exists(hg):\n        install_hg(hg)\n        LOGGER.warn('Mercurial hook has been installed.')\n\n    else:\n        LOGGER.error('VCS has not found. Check your path.')\n        sys.exit(1)", "category": "Python"}, {"instruction": "def get_caffe_pb():\n    \"\"\"\n    Get caffe protobuf.\n    Returns:\n        The imported caffe protobuf module.\n    \"\"\"\n", "input": "", "output": "    dir = get_dataset_path('caffe')\n    caffe_pb_file = os.path.join(dir, 'caffe_pb2.py')\n    if not os.path.isfile(caffe_pb_file):\n        download(CAFFE_PROTO_URL, dir)\n        assert os.path.isfile(os.path.join(dir, 'caffe.proto'))\n\n        if sys.version_info.major == 3:\n            cmd = \"protoc --version\"\n            version, ret = subproc_call(cmd, timeout=3)\n            if ret != 0:\n                sys.exit(1)\n            try:\n                version = version.decode('utf-8')\n                version = float('.'.join(version.split(' ')[1].split('.')[:2]))\n                assert version >= 2.7, \"Require protoc>=2.7 for Python3\"\n            except Exception:\n                logger.exception(\"protoc --version gives: \" + str(version))\n                raise\n\n        cmd = 'cd {} && protoc caffe.proto --python_out .'.format(dir)\n        ret = os.system(cmd)\n        assert ret == 0, \\\n            \"Command `{}` failed!\".format(cmd)\n        assert os.path.isfile(caffe_pb_file), caffe_pb_file\n    import imp\n    return imp.load_source('caffepb', caffe_pb_file)", "category": "Python"}, {"instruction": "def save(self, path):\n        ''' Save clip data to file.\n\n        Args:\n            path (str): Filename to save audio data to.\n        '''\n", "input": "", "output": "        self.clip.write_audiofile(path, fps=self.sampling_rate)", "category": "Python"}, {"instruction": "def _initialize(self, funs_to_tally, length):\n        \"\"\"Create a group named ``chain#`` to store all data for this chain.\"\"\"\n", "input": "", "output": "\n        chain = self.nchains\n        self._chains[chain] = self._h5file.create_group(\n            '/', 'chain%d' % chain, 'chain #%d' % chain)\n\n        for name, fun in six.iteritems(funs_to_tally):\n\n            arr = np.asarray(fun())\n\n            assert arr.dtype != np.dtype('object')\n\n            array = self._h5file.createEArray(\n                self._chains[chain], name,\n                tables.Atom.from_dtype(arr.dtype), (0,) + arr.shape,\n                filters=self.filter)\n\n            self._arrays[chain, name] = array\n            self._traces[name] = Trace(name, getfunc=fun, db=self)\n            self._traces[name]._initialize(self.chains, length)\n\n        self.trace_names.append(list(funs_to_tally.keys()))", "category": "Python"}, {"instruction": "def flush_synced(self, using=None, **kwargs):\n        \"\"\"\n        Perform a normal flush, then add a generated unique marker (sync_id) to\n        all shards.\n\n        Any additional keyword arguments will be passed to\n        ``Elasticsearch.indices.flush_synced`` unchanged.\n        \"\"\"\n", "input": "", "output": "        return self._get_connection(using).indices.flush_synced(index=self._name, **kwargs)", "category": "Python"}, {"instruction": "def is_shutit_installed(self,\n\t                        module_id,\n\t                        note=None,\n\t                        loglevel=logging.DEBUG):\n\t\t\"\"\"Helper proc to determine whether shutit has installed already here by placing a file in the db.\n\n\t\t@param module_id: Identifying string of shutit module\n\t\t@param note:      See send()\n\t\t\"\"\"\n", "input": "", "output": "\t\tshutit_global.shutit_global_object.yield_to_draw()\n\t\tshutit_pexpect_session = self.get_current_shutit_pexpect_session()\n\t\treturn shutit_pexpect_session.is_shutit_installed(module_id,note=note,loglevel=loglevel)", "category": "Python"}, {"instruction": "def applyUserPars_steps(configObj, input_dict, step='3a'):\n    \"\"\" Apply logic to turn on use of user-specified output WCS if user provides\n        any parameter on command-line regardless of how final_wcs was set.\n    \"\"\"\n", "input": "", "output": "    step_kws = {'7a': 'final_wcs', '3a': 'driz_sep_wcs'}\n    stepname = getSectionName(configObj,step)\n    finalParDict = configObj[stepname].copy()\n    del finalParDict[step_kws[step]]\n\n    # interpret input_dict to find any parameters for this step specified by the user\n    user_pars = {}\n    for kw in finalParDict:\n        if kw in input_dict: user_pars[kw] = input_dict[kw]\n    if len(user_pars) > 0:\n        configObj[stepname][step_kws[step]] = True", "category": "Python"}, {"instruction": "def add_dicts(*args):\n    \"\"\"\n    Adds two or more dicts together. Common keys will have their values added.\n\n    For example::\n\n        >>> t1 = {'a':1, 'b':2}\n        >>> t2 = {'b':1, 'c':3}\n        >>> t3 = {'d':4}\n\n        >>> add_dicts(t1, t2, t3)\n        {'a': 1, 'c': 3, 'b': 3, 'd': 4}\n\n    \"\"\"\n", "input": "", "output": "\n    counters = [Counter(arg) for arg in args]\n    return dict(reduce(operator.add, counters))", "category": "Python"}, {"instruction": "def all(self):\n        \"\"\"\n        This method retrieve an iterator with all the available types.\n\n        return: iterator of crossref document types\n\n        Example:\n            >>> from crossref.restful import Types\n            >>> types = Types()\n            >>> [i for i in types.all()]\n            [{'label': 'Book Section', 'id': 'book-section'},\n            {'label': 'Monograph', 'id': 'monograph'},\n            {'label': 'Report', 'id': 'report'},\n            {'label': 'Book Track', 'id': 'book-track'},\n            {'label': 'Journal Article', 'id': 'journal-article'},\n            {'label': 'Part', 'id': 'book-part'},\n            ...\n            }]\n        \"\"\"\n", "input": "", "output": "        request_url = build_url_endpoint(self.ENDPOINT, self.context)\n        request_params = dict(self.request_params)\n\n        result = self.do_http_request(\n            'get',\n            request_url,\n            data=request_params,\n            custom_header=str(self.etiquette)\n        )\n\n        if result.status_code == 404:\n            raise StopIteration()\n\n        result = result.json()\n\n        for item in result['message']['items']:\n            yield item", "category": "Python"}, {"instruction": "def _potential_cross_partial_w(moment_index: int,\n                               op: ops.Operation,\n                               state: _OptimizerState) -> None:\n    \"\"\"Cross the held W over a partial W gate.\n\n    [Where W(a) is shorthand for PhasedX(phase_exponent=a).]\n\n    Uses the following identity:\n        \u2500\u2500\u2500W(a)\u2500\u2500\u2500W(b)^t\u2500\u2500\u2500\n        \u2261 \u2500\u2500\u2500Z^-a\u2500\u2500\u2500X\u2500\u2500\u2500Z^a\u2500\u2500\u2500W(b)^t\u2500\u2500\u2500\u2500\u2500\u2500 (expand W(a))\n        \u2261 \u2500\u2500\u2500Z^-a\u2500\u2500\u2500X\u2500\u2500\u2500W(b-a)^t\u2500\u2500\u2500Z^a\u2500\u2500\u2500\u2500 (move Z^a across, phasing axis)\n        \u2261 \u2500\u2500\u2500Z^-a\u2500\u2500\u2500W(a-b)^t\u2500\u2500\u2500X\u2500\u2500\u2500Z^a\u2500\u2500\u2500\u2500 (move X across, negating axis angle)\n        \u2261 \u2500\u2500\u2500W(2a-b)^t\u2500\u2500\u2500Z^-a\u2500\u2500\u2500X\u2500\u2500\u2500Z^a\u2500\u2500\u2500 (move Z^-a across, phasing axis)\n        \u2261 \u2500\u2500\u2500W(2a-b)^t\u2500\u2500\u2500W(a)\u2500\u2500\u2500\n    \"\"\"\n", "input": "", "output": "    a = state.held_w_phases.get(op.qubits[0])\n    if a is None:\n        return\n    exponent, phase_exponent = cast(Tuple[float, float],\n                                    _try_get_known_phased_pauli(op))\n    new_op = ops.PhasedXPowGate(\n        exponent=exponent,\n        phase_exponent=2 * a - phase_exponent).on(op.qubits[0])\n    state.deletions.append((moment_index, op))\n    state.inline_intos.append((moment_index, new_op))", "category": "Python"}, {"instruction": "def command_update(self):\n        \"\"\"Update package lists repositories\n        \"\"\"\n", "input": "", "output": "        if len(self.args) == 1 and self.args[0] == \"update\":\n            Update().repository(only=\"\")\n        elif (len(self.args) == 2 and self.args[0] == \"update\" and\n                self.args[1].startswith(\"--only=\")):\n            repos = self.args[1].split(\"=\")[-1].split(\",\")\n            for rp in repos:\n                if rp not in self.meta.repositories:\n                    repos.remove(rp)\n            Update().repository(repos)\n        else:\n            usage(\"\")", "category": "Python"}, {"instruction": "def find_rotation(points_distorted, points_perfect):\n    \"\"\"\n    This finds the rotation matrix that aligns the (distorted) set of points \"points_distorted\" with respect to the\n    (perfect) set of points \"points_perfect\" in a least-square sense.\n    :param points_distorted: List of points describing a given (distorted) polyhedron for which the rotation that\n                             aligns these points in a least-square sense to the set of perfect points \"points_perfect\"\n    :param points_perfect: List of \"perfect\" points describing a given model polyhedron.\n    :return: The rotation matrix\n    \"\"\"\n", "input": "", "output": "    H = np.matmul(points_distorted.T, points_perfect)\n    [U, S, Vt] = svd(H)\n    rot = np.matmul(Vt.T, U.T)\n    return rot", "category": "Python"}, {"instruction": "def wrap(self, text, **kwargs):\n        '''Wraps each paragraph in ``text`` individually.\n\n        Parameters\n        ----------\n        text : str\n\n        Returns\n        -------\n        str\n            Single string containing the wrapped paragraphs.\n        '''\n", "input": "", "output": "        pilcrow = re.compile(r'(\\n\\s*\\n)', re.MULTILINE)\n        list_prefix = re.compile(r'\\s*(?:\\w|[0-9]+)[\\.\\)]\\s+')\n\n        paragraphs = pilcrow.split(text)\n        wrapped_lines = []\n        for paragraph in paragraphs:\n            if paragraph.isspace():\n                wrapped_lines.append('')\n            else:\n                wrapper = textwrap.TextWrapper(**vars(self))\n                list_item = re.match(list_prefix, paragraph)\n                if list_item:\n                    wrapper.subsequent_indent += ' ' * len(list_item.group(0))\n                wrapped_lines.extend(wrapper.wrap(paragraph))\n\n        return wrapped_lines", "category": "Python"}, {"instruction": "def get_loc(data, attr={'lr_mult':'0.01'}):\n    \"\"\"\n    the localisation network in lenet-stn, it will increase acc about more than 1%,\n    when num-epoch >=15\n    \"\"\"\n", "input": "", "output": "    loc = mx.symbol.Convolution(data=data, num_filter=30, kernel=(5, 5), stride=(2,2))\n    loc = mx.symbol.Activation(data = loc, act_type='relu')\n    loc = mx.symbol.Pooling(data=loc, kernel=(2, 2), stride=(2, 2), pool_type='max')\n    loc = mx.symbol.Convolution(data=loc, num_filter=60, kernel=(3, 3), stride=(1,1), pad=(1, 1))\n    loc = mx.symbol.Activation(data = loc, act_type='relu')\n    loc = mx.symbol.Pooling(data=loc, global_pool=True, kernel=(2, 2), pool_type='avg')\n    loc = mx.symbol.Flatten(data=loc)\n    loc = mx.symbol.FullyConnected(data=loc, num_hidden=6, name=\"stn_loc\", attr=attr)\n    return loc", "category": "Python"}, {"instruction": "def _split_generators(self, dl_manager):\n    \"\"\"Returns splits.\"\"\"\n", "input": "", "output": "    filenames = {\n        \"training_dat\": _TRAINING_URL_TEMPLATE.format(type=\"dat\"),\n        \"training_cat\": _TRAINING_URL_TEMPLATE.format(type=\"cat\"),\n        \"training_info\": _TRAINING_URL_TEMPLATE.format(type=\"info\"),\n        \"testing_dat\": _TESTING_URL_TEMPLATE.format(type=\"dat\"),\n        \"testing_cat\": _TESTING_URL_TEMPLATE.format(type=\"cat\"),\n        \"testing_info\": _TESTING_URL_TEMPLATE.format(type=\"info\"),\n    }\n\n    files = dl_manager.download_and_extract(filenames)\n\n    return [\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TRAIN,\n            num_shards=1,\n            gen_kwargs=dict(\n                dat_path=files[\"training_dat\"],\n                cat_path=files[\"training_cat\"],\n                info_path=files[\"training_info\"])),\n        tfds.core.SplitGenerator(\n            name=tfds.Split.TEST,\n            num_shards=1,\n            gen_kwargs=dict(\n                dat_path=files[\"testing_dat\"],\n                cat_path=files[\"testing_cat\"],\n                info_path=files[\"testing_info\"])),\n    ]", "category": "Python"}, {"instruction": "def add_subkey(self, subkey):\n        \"\"\"\n        Add a new subkey to this key.\n\n        Parameters\n        ----------\n        subkey : AdfKey\n            A new subkey.\n\n        Notes\n        -----\n        Duplicate check will not be performed if this is an 'Atoms' block.\n\n        \"\"\"\n", "input": "", "output": "        if self.key.lower() == 'atoms' or not self.has_subkey(subkey):\n            self.subkeys.append(subkey)", "category": "Python"}, {"instruction": "def disable(self):\n        \"\"\"\n        Disable the sandbox on this engine.\n        \"\"\"\n", "input": "", "output": "        self.engine.data.update(sandbox_type='none')\n        self.pop('cloud_sandbox_settings', None) #pre-6.3\n        self.pop('sandbox_settings', None)", "category": "Python"}, {"instruction": "def quit(self, *args, **kwargs):  # real signature unknown\n        \"\"\"\n        quit the  read_probe thread\n        \"\"\"\n", "input": "", "output": "        self._stop = True\n        super(ReadProbes, self).quit(*args, **kwargs)", "category": "Python"}, {"instruction": "def cmd(send, _, args):\n    \"\"\"Returns stats on the active users.\n\n    Syntax: {command}\n\n    \"\"\"\n", "input": "", "output": "    if args['target'] == 'private':\n        send(\"You're all alone!\")\n        return\n    with args['handler'].data_lock:\n        channel = args['handler'].channels[args['target']]\n        voiced = len([x for x in args['handler'].voiced[args['target']].values() if x])\n        total = len(channel.users())\n    send(\"%d active users, %d total users, %g%% active\" % (voiced, total, voiced / total * 100))", "category": "Python"}, {"instruction": "def loadCurve(data, groups, thresholds, absvals, fs, xlabels):\n        \"\"\"Accepts a data set from a whole test, averages reps and re-creates the \n        progress plot as the same as it was during live plotting. Number of thresholds\n        must match the size of the channel dimension\"\"\"\n", "input": "", "output": "        xlims = (xlabels[0], xlabels[-1])\n        pw = ProgressWidget(groups, xlims)\n        spike_counts = []\n        # skip control\n        for itrace in range(data.shape[0]):\n            count = 0\n            for ichan in range(data.shape[2]):\n                flat_reps = data[itrace,:,ichan,:].flatten()\n                count += len(spikestats.spike_times(flat_reps, thresholds[ichan], fs, absvals[ichan]))\n            spike_counts.append(count/(data.shape[1]*data.shape[2])) #mean spikes per rep\n\n        i = 0\n        for g in groups:\n            for x in xlabels:\n                pw.setPoint(x, g, spike_counts[i])\n                i +=1\n\n        return pw", "category": "Python"}, {"instruction": "def build_plan(description, graph,\n               targets=None, reverse=False):\n    \"\"\"Builds a plan from a list of steps.\n    Args:\n        description (str): an arbitrary string to\n            describe the plan.\n        graph (:class:`Graph`): a list of :class:`Graph` to execute.\n        targets (list): an optional list of step names to filter the graph to.\n            If provided, only these steps, and their transitive dependencies\n            will be executed. If no targets are specified, every node in the\n            graph will be executed.\n        reverse (bool): If provided, the graph will be walked in reverse order\n            (dependencies last).\n    \"\"\"\n", "input": "", "output": "\n    # If we want to execute the plan in reverse (e.g. Destroy), transpose the\n    # graph.\n    if reverse:\n        graph = graph.transposed()\n\n    # If we only want to build a specific target, filter the graph.\n    if targets:\n        nodes = []\n        for target in targets:\n            for k, step in graph.steps.items():\n                if step.name == target:\n                    nodes.append(step.name)\n        graph = graph.filtered(nodes)\n\n    return Plan(description=description, graph=graph)", "category": "Python"}, {"instruction": "def tally(self, name, value):\n        \"\"\"Adds to the \"used\" metric for the given quota.\"\"\"\n", "input": "", "output": "        value = value or 0  # Protection against None.\n        # Start at 0 if this is the first value.\n        if 'used' not in self.usages[name]:\n            self.usages[name]['used'] = 0\n        # Increment our usage and update the \"available\" metric.\n        self.usages[name]['used'] += int(value)  # Fail if can't coerce to int.\n        self.update_available(name)", "category": "Python"}, {"instruction": "def _get_name_from_content_type(self, request):\n        \"\"\" Get name from Content-Type header \"\"\"\n", "input": "", "output": "\n        content_type = request.META.get('CONTENT_TYPE', None)\n        if content_type:\n            # remove the possible charset-encoding info\n            return util.strip_charset(content_type)\n        return None", "category": "Python"}, {"instruction": "def get_section(value):\n    \"\"\" '*' digits\n\n    The formal BNF is more complicated because leading 0s are not allowed.  We\n    check for that and add a defect.  We also assume no CFWS is allowed between\n    the '*' and the digits, though the RFC is not crystal clear on that.\n    The caller should already have dealt with leading CFWS.\n\n    \"\"\"\n", "input": "", "output": "    section = Section()\n    if not value or value[0] != '*':\n        raise errors.HeaderParseError(\"Expected section but found {}\".format(\n                                        value))\n    section.append(ValueTerminal('*', 'section-marker'))\n    value = value[1:]\n    if not value or not value[0].isdigit():\n        raise errors.HeaderParseError(\"Expected section number but \"\n                                      \"found {}\".format(value))\n    digits = ''\n    while value and value[0].isdigit():\n        digits += value[0]\n        value = value[1:]\n    if digits[0] == '0' and digits != '0':\n        section.defects.append(errors.InvalidHeaderError(\"section number\"\n            \"has an invalid leading 0\"))\n    section.number = int(digits)\n    section.append(ValueTerminal(digits, 'digits'))\n    return section, value", "category": "Python"}, {"instruction": "def eventFilter(self, widget, event):\r\n        \"\"\"Event filter for search_text widget.\r\n\r\n        Emits signals when presing Enter and Shift+Enter.\r\n        This signals are used for search forward and backward.\r\n        Also, a crude hack to get tab working in the Find/Replace boxes.\r\n        \"\"\"\n", "input": "", "output": "        if event.type() == QEvent.KeyPress:\r\n            key = event.key()\r\n            shift = event.modifiers() & Qt.ShiftModifier\r\n\r\n            if key == Qt.Key_Return:\r\n                if shift:\r\n                    self.return_shift_pressed.emit()\r\n                else:\r\n                    self.return_pressed.emit()\r\n\r\n            if key == Qt.Key_Tab:\r\n                if self.search_text.hasFocus():\r\n                    self.replace_text.set_current_text(\r\n                        self.search_text.currentText())\r\n                self.focusNextChild()\r\n\r\n        return super(FindReplace, self).eventFilter(widget, event)", "category": "Python"}, {"instruction": "def check_simulation(wdir):\n    \"\"\"\n    Check bhdebug.txt to make sure that you specify enough digits to\n    overcome roundoff errors.\n    \"\"\"\n", "input": "", "output": "    wdir = pathlib.Path(wdir)\n    field = wdir / \"V_0Ereim.dat\"\n    if not (field.exists() and\n            field.stat().st_size > 130):\n        msg = \"Output {} does not exist or is too small!\".format(field)\n        raise BHFIELDExecutionError(msg)", "category": "Python"}, {"instruction": "def prepend(self, key, value, expire=0, noreply=None):\n        \"\"\"\n        The memcached \"prepend\" command.\n\n        Args:\n          key: str, see class docs for details.\n          value: str, see class docs for details.\n          expire: optional int, number of seconds until the item is expired\n                  from the cache, or zero for no expiry (the default).\n          noreply: optional bool, True to not wait for the reply (defaults to\n                   self.default_noreply).\n\n        Returns:\n          True.\n        \"\"\"\n", "input": "", "output": "        if noreply is None:\n            noreply = self.default_noreply\n        return self._store_cmd(b'prepend', {key: value}, expire, noreply)[key]", "category": "Python"}, {"instruction": "def get_gsims(self, trt):\n        \"\"\"\n        :param trt: tectonic region type\n        :returns: sorted list of available GSIMs for that trt\n        \"\"\"\n", "input": "", "output": "        if trt == '*' or trt == b'*':  # fake logictree\n            [trt] = self.values\n        return sorted(self.values[trt])", "category": "Python"}, {"instruction": "def plot(x, y, units='absolute', axis=None, **kwargs):\n  r'''\nPlot.\n\n:arguments:\n\n  **x, y** (``list``)\n    Coordinates.\n\n:options:\n\n  **units** ([``'absolute'``] | ``'relative'``)\n    The type of units in which the coordinates are specified. Relative coordinates correspond to a\n    fraction of the relevant axis. If you use relative coordinates, be sure to set the limits and\n    scale before calling this function!\n\n  ...\n    Any ``plt.plot(...)`` option.\n\n:returns:\n\n  The handle of the ``plt.plot(...)`` command.\n  '''\n", "input": "", "output": "\n  # get current axis\n  if axis is None:\n    axis = plt.gca()\n\n  # transform\n  if units.lower() == 'relative':\n    x = rel2abs_x(x, axis)\n    y = rel2abs_y(y, axis)\n\n  # plot\n  return axis.plot(x, y, **kwargs)", "category": "Python"}, {"instruction": "def derive_child_context(self, whence):\n        \"\"\"Derives a scalar context as a child of the current context.\"\"\"\n", "input": "", "output": "        return _HandlerContext(\n            container=self.container,\n            queue=self.queue,\n            field_name=None,\n            annotations=None,\n            depth=self.depth,\n            whence=whence,\n            value=bytearray(),  # children start without a value\n            ion_type=None,\n            pending_symbol=None\n        )", "category": "Python"}, {"instruction": "def ncr(n, r):\n    \"\"\"\n    Calculate n choose r.\n\n    :param n: n\n    :type n : int\n    :param r: r\n    :type r :int\n    :return: n choose r as int\n    \"\"\"\n", "input": "", "output": "    r = min(r, n - r)\n    numer = reduce(op.mul, range(n, n - r, -1), 1)\n    denom = reduce(op.mul, range(1, r + 1), 1)\n    return numer // denom", "category": "Python"}, {"instruction": "def _get_a1_value(bbar, dbar, slip, beta, mmax):\n        \"\"\"\n        Returns the A1 value defined in I.9 (Table 2)\n        \"\"\"\n", "input": "", "output": "        return ((dbar - bbar) / dbar) * (slip / beta) *\\\n            np.exp(-(dbar / 2.) * mmax)", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"\n        Run all steps\n        \"\"\"\n", "input": "", "output": "        for line_ in self.region_string.split('\\n'):\n            for line in line_.split(\";\"):\n                self.parse_line(line)\n                log.debug('Global state: {}'.format(self))", "category": "Python"}, {"instruction": "def PopItem(self):\n    \"\"\"Pops an item off the queue.\n\n    Returns:\n      object: item from the queue.\n\n    Raises:\n      QueueClose: if the queue has already been closed.\n      QueueEmpty: if no item could be retrieved from the queue within the\n          specified timeout.\n    \"\"\"\n", "input": "", "output": "    try:\n      # If no timeout is specified the queue will block if empty otherwise\n      # a Queue.Empty exception is raised.\n      return self._queue.get(timeout=self._timeout)\n\n    except KeyboardInterrupt:\n      raise errors.QueueClose\n\n    # If close() is called on the multiprocessing.Queue while it is blocking\n    # on get() it will raise IOError.\n    except IOError:\n      raise errors.QueueClose\n\n    except Queue.Empty:\n      raise errors.QueueEmpty", "category": "Python"}, {"instruction": "def save_signal(self,filename=None):\n        \"\"\"\n        Saves TransitSignal.\n\n        Calls :func:`TransitSignal.save`; default filename is\n        ``trsig.pkl`` in ``self.folder``.\n        \"\"\"\n", "input": "", "output": "        if filename is None:\n            filename = os.path.join(self.folder,'trsig.pkl')\n        self.trsig.save(filename)", "category": "Python"}, {"instruction": "def _get(operation: Operation, url=URL, **params):\n    \"\"\"HTTP GET of the FlashAir command.cgi entrypoint\"\"\"\n", "input": "", "output": "    prepped_request = _prep_get(operation, url=url, **params)\n    return cgi.send(prepped_request)", "category": "Python"}, {"instruction": "def remove(self, name):\n        \"\"\"Remove an action.\"\"\"\n", "input": "", "output": "        self.gui.removeAction(self._actions_dict[name].qaction)\n        del self._actions_dict[name]\n        delattr(self, name)", "category": "Python"}, {"instruction": "def length(self):\n        \"\"\"\n        :return: Length of the ``data``.\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "\n        if not self.__length:\n            self.__length = self.__get_length()\n\n        return self.__length", "category": "Python"}, {"instruction": "def fcoe_get_interface_output_fcoe_intf_list_fcoe_intf_rx_logo(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        fcoe_get_interface = ET.Element(\"fcoe_get_interface\")\n        config = fcoe_get_interface\n        output = ET.SubElement(fcoe_get_interface, \"output\")\n        fcoe_intf_list = ET.SubElement(output, \"fcoe-intf-list\")\n        fcoe_intf_fcoe_port_id_key = ET.SubElement(fcoe_intf_list, \"fcoe-intf-fcoe-port-id\")\n        fcoe_intf_fcoe_port_id_key.text = kwargs.pop('fcoe_intf_fcoe_port_id')\n        fcoe_intf_rx_logo = ET.SubElement(fcoe_intf_list, \"fcoe-intf-rx-logo\")\n        fcoe_intf_rx_logo.text = kwargs.pop('fcoe_intf_rx_logo')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def build_list(self):\n        \"\"\"Return a list of tuples taken from self.args.stdout\n        [(plugin, attribute), ... ]\"\"\"\n", "input": "", "output": "        ret = []\n        for p in self.args.stdout.split(','):\n            if '.' in p:\n                p, a = p.split('.')\n            else:\n                a = None\n            ret.append((p, a))\n        return ret", "category": "Python"}, {"instruction": "def get_checksums(path, algorithms):\n    \"\"\"\n    Compute a checksum(s) of given file using specified algorithms.\n\n    :param path: path to file\n    :param algorithms: list of cryptographic hash functions, currently supported: md5, sha256\n    :return: dictionary\n    \"\"\"\n", "input": "", "output": "    if not algorithms:\n        return {}\n\n    compute_md5 = 'md5' in algorithms\n    compute_sha256 = 'sha256' in algorithms\n\n    if compute_md5:\n        md5 = hashlib.md5()\n    if compute_sha256:\n        sha256 = hashlib.sha256()\n    blocksize = 65536\n    with open(path, mode='rb') as f:\n        buf = f.read(blocksize)\n        while len(buf) > 0:\n            if compute_md5:\n                md5.update(buf)\n            if compute_sha256:\n                sha256.update(buf)\n            buf = f.read(blocksize)\n\n    checksums = {}\n    if compute_md5:\n        checksums['md5sum'] = md5.hexdigest()\n        logger.debug('md5sum: %s', checksums['md5sum'])\n    if compute_sha256:\n        checksums['sha256sum'] = sha256.hexdigest()\n        logger.debug('sha256sum: %s', checksums['sha256sum'])\n    return checksums", "category": "Python"}, {"instruction": "async def vafter(self):\n        \"\"\"Function that is called after a song finishes playing\"\"\"\n", "input": "", "output": "        self.logger.debug(\"Finished playing a song\")\n        if self.state != 'ready':\n            self.logger.debug(\"Returning because player is in state {}\".format(self.state))\n            return\n\n        self.pause_time = None\n\n        if self.vclient_task:\n            loop = asyncio.get_event_loop()\n            loop.call_soon(self.vclient_task.cancel)\n            self.vclient_task = None\n\n        try:\n            if self.streamer is None:\n                await self.stop()\n                return\n\n            if self.streamer.error is None:\n                await self.vplay()\n            else:\n                self.statuslog.error(self.streamer.error)\n                await self.destroy()\n        except Exception as e:\n            logger.exception(e)\n            try:\n                await self.destroy()\n            except Exception as e:\n                logger.exception(e)", "category": "Python"}, {"instruction": "def index():\n    \"\"\"Index page with uploader and list of existing depositions.\"\"\"\n", "input": "", "output": "    ctx = mycommunities_ctx()\n\n    p = request.args.get('p', type=str)\n    so = request.args.get('so', type=str)\n    page = request.args.get('page', type=int, default=1)\n\n    so = so or current_app.config.get('COMMUNITIES_DEFAULT_SORTING_OPTION')\n\n    communities = Community.filter_communities(p, so)\n    featured_community = FeaturedCommunity.get_featured_or_none()\n    form = SearchForm(p=p)\n    per_page = 10\n    page = max(page, 1)\n    p = Pagination(page, per_page, communities.count())\n\n    ctx.update({\n        'r_from': max(p.per_page * (p.page - 1), 0),\n        'r_to': min(p.per_page * p.page, p.total_count),\n        'r_total': p.total_count,\n        'pagination': p,\n        'form': form,\n        'title': _('Communities'),\n        'communities': communities.slice(\n            per_page * (page - 1), per_page * page).all(),\n        'featured_community': featured_community,\n    })\n\n    return render_template(\n        current_app.config['COMMUNITIES_INDEX_TEMPLATE'], **ctx)", "category": "Python"}, {"instruction": "def gfonts_repo_structure(fonts):\n  \"\"\" The family at the given font path\n      follows the files and directory structure\n      typical of a font project hosted on\n      the Google Fonts repo on GitHub ? \"\"\"\n", "input": "", "output": "  from fontbakery.utils import get_absolute_path\n\n  # FIXME: Improve this with more details\n  #        about the expected structure.\n  abspath = get_absolute_path(fonts[0])\n  return abspath.split(os.path.sep)[-3] in [\"ufl\", \"ofl\", \"apache\"]", "category": "Python"}, {"instruction": "def set_thresh(thresh,p=False,hostname=None):\n    '''Sets the level of the threshold slider.\n    If ``p==True`` will be interpreted as a _p_-value'''\n", "input": "", "output": "    driver_send(\"SET_THRESHNEW %s *%s\" % (str(thresh),\"p\" if p else \"\"),hostname=hostname)", "category": "Python"}, {"instruction": "def convert_bytes_to_ints(in_bytes, num):\n    \"\"\"Convert a byte array into an integer array. The number of bytes forming an integer\n    is defined by num\n    :param in_bytes: the input bytes\n    :param num: the number of bytes per int\n    :return the integer array\"\"\"\n", "input": "", "output": "    out_arr = []\n    for i in range(len(in_bytes)//num):\n        val = in_bytes[i * num:i * num + num]\n        unpacked = struct.unpack(mmtf.utils.constants.NUM_DICT[num], val)\n        out_arr.append(unpacked[0])\n    return out_arr", "category": "Python"}, {"instruction": "def run_command(self, config_file, sources):\n        \"\"\"\n        :param str config_file: The name of config file.\n        :param list sources: The list with source files.\n        \"\"\"\n", "input": "", "output": "        config = configparser.ConfigParser()\n        config.read(config_file)\n\n        rdbms = config.get('database', 'rdbms').lower()\n\n        loader = self.create_routine_loader(rdbms)\n        status = loader.main(config_file, sources)\n\n        return status", "category": "Python"}, {"instruction": "def ip_rtm_config_route_static_route_nh_static_route_dest(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        ip = ET.SubElement(config, \"ip\", xmlns=\"urn:brocade.com:mgmt:brocade-common-def\")\n        rtm_config = ET.SubElement(ip, \"rtm-config\", xmlns=\"urn:brocade.com:mgmt:brocade-rtm\")\n        route = ET.SubElement(rtm_config, \"route\")\n        static_route_nh = ET.SubElement(route, \"static-route-nh\")\n        static_route_next_hop_key = ET.SubElement(static_route_nh, \"static-route-next-hop\")\n        static_route_next_hop_key.text = kwargs.pop('static_route_next_hop')\n        static_route_dest = ET.SubElement(static_route_nh, \"static-route-dest\")\n        static_route_dest.text = kwargs.pop('static_route_dest')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def _int2coord(x, y, dim):\n    \"\"\"Convert x, y values in dim x dim-grid coordinate system into lng, lat values.\n\n    Parameters:\n        x: int        x value of point [0, dim); corresponds to longitude\n        y: int        y value of point [0, dim); corresponds to latitude\n        dim: int      Number of coding points each x, y value can take.\n                      Corresponds to 2^level of the hilbert curve.\n\n    Returns:\n        Tuple[float, float]: (lng, lat)\n            lng    longitude value of coordinate [-180.0, 180.0]; corresponds to X axis\n            lat    latitude value of coordinate [-90.0, 90.0]; corresponds to Y axis\n    \"\"\"\n", "input": "", "output": "    assert dim >= 1\n    assert x < dim\n    assert y < dim\n\n    lng = x / dim * 360 - 180\n    lat = y / dim * 180 - 90\n\n    return lng, lat", "category": "Python"}, {"instruction": "def remove_cached_item(self, path):\n        \"\"\"\n        Remove cached resource item\n        :param path: str\n        :return: bool\n        \"\"\"\n", "input": "", "output": "        item_path = '%s/%s/%s' % (\n                current_app.static_folder,\n                self.cache_folder,\n                path.strip('/')\n            )\n\n        if os.path.isfile(item_path):\n            os.remove(item_path)\n\n        return True", "category": "Python"}, {"instruction": "def write_input_files(self, fh):\n    \"\"\"\n    Write as a comment into the DAG file the list of input files\n    for this DAG node.\n\n    @param fh: descriptor of open DAG file.\n    \"\"\"\n", "input": "", "output": "    for f in self.__input_files:\n        print >>fh, \"## Job %s requires input file %s\" % (self.__name, f)", "category": "Python"}, {"instruction": "def find_all(self, node_type):\n        \"\"\"Find all the nodes of a given type.  If the type is a tuple,\n        the check is performed for any of the tuple items.\n        \"\"\"\n", "input": "", "output": "        for child in self.iter_child_nodes():\n            if isinstance(child, node_type):\n                yield child\n            for result in child.find_all(node_type):\n                yield result", "category": "Python"}, {"instruction": "def list_all_wish_lists(cls, **kwargs):\n        \"\"\"List WishLists\n\n        Return a list of WishLists\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.list_all_wish_lists(async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param int page: page number\n        :param int size: page size\n        :param str sort: page order\n        :return: page[WishList]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._list_all_wish_lists_with_http_info(**kwargs)\n        else:\n            (data) = cls._list_all_wish_lists_with_http_info(**kwargs)\n            return data", "category": "Python"}, {"instruction": "def table2csv(table_data):\n    \"\"\"\n    :param table_data: expecting a list of tuples\n    :return: text in nice formatted csv\n    \"\"\"\n", "input": "", "output": "    text_data = [tuple(value2json(vals, pretty=True) for vals in rows) for rows in table_data]\n\n    col_widths = [max(len(text) for text in cols) for cols in zip(*text_data)]\n    template = \", \".join(\n        \"{{\" + text_type(i) + \"|left_align(\" + text_type(w) + \")}}\"\n        for i, w in enumerate(col_widths)\n    )\n    text = \"\\n\".join(expand_template(template, d) for d in text_data)\n    return text", "category": "Python"}, {"instruction": "def get_net_size(mask):\n    '''\n    Turns an IPv4 netmask into it's corresponding prefix length\n    (255.255.255.0 -> 24 as in 192.168.1.10/24).\n    '''\n", "input": "", "output": "    binary_str = ''\n    for octet in mask.split('.'):\n        binary_str += bin(int(octet))[2:].zfill(8)\n    return len(binary_str.rstrip('0'))", "category": "Python"}, {"instruction": "def WriteSerializableArray(self, array):\n        \"\"\"\n        Write an array of serializable objects to the stream.\n\n        Args:\n            array(list): a list of serializable objects. i.e. extending neo.IO.Mixins.SerializableMixin\n        \"\"\"\n", "input": "", "output": "        if array is None:\n            self.WriteByte(0)\n        else:\n            self.WriteVarInt(len(array))\n            for item in array:\n                item.Serialize(self)", "category": "Python"}, {"instruction": "def unregister(self, recipe):\n        \"\"\"\n        Unregisters a given recipe class.\n        \"\"\"\n", "input": "", "output": "        recipe = self.get_recipe_instance_from_class(recipe)\n        if recipe.slug in self._registry:\n            del self._registry[recipe.slug]", "category": "Python"}, {"instruction": "def _get(self, q, params=''):\n        '''Generic GET wrapper including the api_key'''\n", "input": "", "output": "        if (q[-1] == '/'): q = q[:-1]\n        headers = {'Content-Type': 'application/json'}\n        r = requests.get('{url}{q}?api_key={key}{params}'.format(url=self.url, q=q, key=self.api_key, params=params),\n                        headers=headers)\n        ret = DotDict(r.json())\n        if (not r.ok or ('error' in ret and ret.error == True)):\n            raise Exception(r.url, r.reason, r.status_code, r.json())\n        return DotDict(r.json())", "category": "Python"}, {"instruction": "def Hash(self):\n        \"\"\"\n        Get the hash value of the Blockbase.\n\n        Returns:\n            UInt256: containing the hash of the data.\n        \"\"\"\n", "input": "", "output": "        if not self.__hash:\n            hashdata = self.RawData()\n            ba = bytearray(binascii.unhexlify(hashdata))\n            hash = bin_dbl_sha256(ba)\n            self.__hash = UInt256(data=hash)\n\n        return self.__hash", "category": "Python"}, {"instruction": "def street_name(self):\n        \"\"\"\n        :example 'Crist Parks'\n        \"\"\"\n", "input": "", "output": "        pattern = self.random_element(self.street_name_formats)\n        return self.generator.parse(pattern)", "category": "Python"}, {"instruction": "def makeLUTfromCTF(sclist, N=None):\n    \"\"\"\n    Use a Color Transfer Function to generate colors in a vtk lookup table.\n    See `here <http://www.vtk.org/doc/nightly/html/classvtkColorTransferFunction.html>`_.\n\n    :param list sclist: a list in the form ``[(scalar1, [r,g,b]), (scalar2, 'blue'), ...]``.\n    :return: the lookup table object ``vtkLookupTable``. This can be fed into ``colorMap``.\n    \"\"\"\n", "input": "", "output": "    ctf = vtk.vtkColorTransferFunction()\n    ctf.SetColorSpaceToDiverging()\n\n    for sc in sclist:\n        scalar, col = sc\n        r, g, b = getColor(col)\n        ctf.AddRGBPoint(scalar, r, g, b)\n\n    if N is None:\n        N = len(sclist)\n\n    lut = vtk.vtkLookupTable()\n    lut.SetNumberOfTableValues(N)\n    lut.Build()\n\n    for i in range(N):\n        rgb = list(ctf.GetColor(float(i) / N)) + [1]\n        lut.SetTableValue(i, rgb)\n\n    return lut", "category": "Python"}, {"instruction": "def debugPreview(self, title=\"Debug\"):\n        \"\"\" Displays the region in a preview window.\n\n        If the region is a Match, circles the target area. If the region is larger than half the\n        primary screen in either dimension, scales it down to half size.\n        \"\"\"\n", "input": "", "output": "        region = self\n        haystack = self.getBitmap()\n        if isinstance(region, Match):\n            cv2.circle(\n                haystack,\n                (region.getTarget().x - self.x, region.getTarget().y - self.y),\n                5,\n                255)\n        if haystack.shape[0] > (Screen(0).getBounds()[2]/2) or haystack.shape[1] > (Screen(0).getBounds()[3]/2):\n            # Image is bigger than half the screen; scale it down\n            haystack = cv2.resize(haystack, (0, 0), fx=0.5, fy=0.5)\n        Image.fromarray(haystack).show()", "category": "Python"}, {"instruction": "def __filter(filterable, filter_, logic_operation='and'):\n        \"\"\" filtering DataFrame using filter_ key-value conditions applying logic_operation\n        only find rows strictly fitting the filter_ criterion\"\"\"\n", "input": "", "output": "        condition = []\n        if not filter_:\n            return filterable\n        elif filter_.get('type') == '__ANY__':\n            return filterable\n        else:\n            for key, value in filter_.items():\n                condition.append('{key} == \"{value}\"'.format(key=key, value=value))\n        try:\n            res = filterable.query(\" {operation} \".format(operation=logic_operation).join(condition))\n        except pd.core.computation.ops.UndefinedVariableError:\n            return pd.DataFrame()\n        else:\n            return res", "category": "Python"}, {"instruction": "def _parse_linux_cx_state(self, lines, tcp_states, state_col, protocol=None, ip_version=None):\n        \"\"\"\n        Parse the output of the command that retrieves the connection state (either `ss` or `netstat`)\n        Returns a dict metric_name -> value\n        \"\"\"\n", "input": "", "output": "        metrics = {}\n        for _, val in iteritems(self.cx_state_gauge):\n            metrics[val] = 0\n        for l in lines:\n            cols = l.split()\n            if cols[0].startswith('tcp') or protocol == 'tcp':\n                proto = \"tcp{0}\".format(ip_version) if ip_version else (\"tcp4\", \"tcp6\")[cols[0] == \"tcp6\"]\n                if cols[state_col] in tcp_states:\n                    metric = self.cx_state_gauge[proto, tcp_states[cols[state_col]]]\n                    metrics[metric] += 1\n            elif cols[0].startswith('udp') or protocol == 'udp':\n                proto = \"udp{0}\".format(ip_version) if ip_version else (\"udp4\", \"udp6\")[cols[0] == \"udp6\"]\n                metric = self.cx_state_gauge[proto, 'connections']\n                metrics[metric] += 1\n\n        return metrics", "category": "Python"}, {"instruction": "def del_alias(self, alias):\n        \"\"\"\n        Delete an alias from the registry. The blobs it points to won't be deleted. Use :meth:`del_blob` for that.\n\n        .. Note::\n           On private registry, garbage collection might need to be run manually; see:\n           https://docs.docker.com/registry/garbage-collection/\n\n        :param alias: Alias name.\n        :type alias: str\n\n        :rtype: list\n        :returns: A list of blob hashes (strings) which were assigned to the alias.\n        \"\"\"\n", "input": "", "output": "        dcd = self._get_dcd(alias)\n        dgsts = self.get_alias(alias)\n        self._request('delete', 'manifests/{}'.format(dcd))\n        return dgsts", "category": "Python"}, {"instruction": "def pathstrip(path, n):\n  \"\"\" Strip n leading components from the given path \"\"\"\n", "input": "", "output": "  pathlist = [path]\n  while os.path.dirname(pathlist[0]) != b'':\n    pathlist[0:1] = os.path.split(pathlist[0])\n  return b'/'.join(pathlist[n:])", "category": "Python"}, {"instruction": "def yum_install(**kwargs):\n    \"\"\"\n        installs a yum package\n    \"\"\"\n", "input": "", "output": "    if 'repo' in kwargs:\n        repo = kwargs['repo']\n\n    for pkg in list(kwargs['packages']):\n        if is_package_installed(distribution='el', pkg=pkg) is False:\n            if 'repo' in locals():\n                log_green(\n                    \"installing %s from repo %s ...\" % (pkg, repo))\n                sudo(\"yum install -y --quiet --enablerepo=%s %s\" % (repo, pkg))\n            else:\n                log_green(\"installing %s ...\" % pkg)\n                sudo(\"yum install -y --quiet %s\" % pkg)", "category": "Python"}, {"instruction": "def _log_in(self):\n        '''Connect and login.\n\n        Coroutine.\n        '''\n", "input": "", "output": "        username = self._request.url_info.username or self._request.username or 'anonymous'\n        password = self._request.url_info.password or self._request.password or '-wpull@'\n\n        cached_login = self._login_table.get(self._control_connection)\n\n        if cached_login and cached_login == (username, password):\n            _logger.debug('Reusing existing login.')\n            return\n\n        try:\n            yield from self._commander.login(username, password)\n        except FTPServerError as error:\n            raise AuthenticationError('Login error: {}'.format(error)) \\\n                from error\n\n        self._login_table[self._control_connection] = (username, password)", "category": "Python"}, {"instruction": "def name(self):\n        \"\"\"Get the enumeration name of this cursor kind.\"\"\"\n", "input": "", "output": "        if self._name_map is None:\n            self._name_map = {}\n            for key, value in self.__class__.__dict__.items():\n                if isinstance(value, self.__class__):\n                    self._name_map[value] = key\n        return self._name_map[self]", "category": "Python"}, {"instruction": "def get(self, container):\n        \"\"\"\n        Returns a Container matching the specified container name. If no such\n        container exists, a NoSuchContainer exception is raised.\n        \"\"\"\n", "input": "", "output": "        name = utils.get_name(container)\n        uri = \"/%s\" % name\n        resp, resp_body = self.api.method_head(uri)\n        hdrs = resp.headers\n        data = {\"total_bytes\": int(hdrs.get(\"x-container-bytes-used\", \"0\")),\n                \"object_count\": int(hdrs.get(\"x-container-object-count\", \"0\")),\n                \"name\": name}\n        return Container(self, data, loaded=False)", "category": "Python"}, {"instruction": "def _parse(self):\n        \"\"\"\n        Loop through all child elements and execute any available parse methods for them\n        \"\"\"\n", "input": "", "output": "        # Is this a shorthand template?\n        if self._element.tag == 'template':\n            return self._parse_template(self._element)\n\n        # Is this a shorthand redirect?\n        if self._element.tag == 'redirect':\n            return self._parse_redirect(self._element)\n\n        for child in self._element:\n            method_name = '_parse_' + child.tag\n\n            if hasattr(self, method_name):\n                parse = getattr(self, method_name)\n                parse(child)", "category": "Python"}, {"instruction": "def all_subs(bounds):\n    \"\"\"given a list of tuples specifying the bounds of an array, all_subs()\n       returns a list of all the tuples of subscripts for that array.\"\"\"\n", "input": "", "output": "    idx_list = []\n    for i in range(len(bounds)):\n        this_dim = bounds[i]\n        lo,hi = this_dim[0],this_dim[1]   # bounds for this dimension\n        this_dim_idxs = range(lo,hi+1)    # indexes for this dimension\n        idx_list.append(this_dim_idxs)\n\n    return idx2subs(idx_list)", "category": "Python"}, {"instruction": "def flatten_colors(colors):\n    \"\"\"Prepare colors to be exported.\n       Flatten dicts and convert colors to util.Color()\"\"\"\n", "input": "", "output": "    all_colors = {\"wallpaper\": colors[\"wallpaper\"],\n                  \"alpha\": colors[\"alpha\"],\n                  **colors[\"special\"],\n                  **colors[\"colors\"]}\n    return {k: util.Color(v) for k, v in all_colors.items()}", "category": "Python"}, {"instruction": "def get_module_name(self, path_args):\n        \"\"\"returns the module_name and remaining path args.\n\n        return -- tuple -- (module_name, path_args)\"\"\"\n", "input": "", "output": "        controller_prefix = self.controller_prefix\n        cset = self.module_names\n        module_name = controller_prefix\n        mod_name = module_name\n        while path_args:\n            mod_name += \".\" + path_args[0]\n            if mod_name in cset:\n                module_name = mod_name\n                path_args.pop(0)\n            else:\n                break\n\n        return module_name, path_args", "category": "Python"}, {"instruction": "def annotate(self, text, lang = None, customParams = None):\n        \"\"\"\n        identify the list of entities and nonentities mentioned in the text\n        @param text: input text to annotate\n        @param lang: language of the provided document (can be an ISO2 or ISO3 code). If None is provided, the language will be automatically detected\n        @param customParams: None or a dict with custom parameters to send to the annotation service\n        @returns: dict\n        \"\"\"\n", "input": "", "output": "        params = {\"lang\": lang, \"text\": text}\n        if customParams:\n            params.update(customParams)\n        return self._er.jsonRequestAnalytics(\"/api/v1/annotate\", params)", "category": "Python"}, {"instruction": "def removeReferenceSet(self):\n        \"\"\"\n        Removes a referenceSet from the repo.\n        \"\"\"\n", "input": "", "output": "        self._openRepo()\n        referenceSet = self._repo.getReferenceSetByName(\n            self._args.referenceSetName)\n\n        def func():\n            self._updateRepo(self._repo.removeReferenceSet, referenceSet)\n        self._confirmDelete(\"ReferenceSet\", referenceSet.getLocalId(), func)", "category": "Python"}, {"instruction": "def generate_custom(self, cpu, vcpu_num, fill_topology):\n        \"\"\"\n        Generate custom CPU model. This method attempts to convert the dict to\n        XML, as defined by ``xmltodict.unparse`` method.\n\n        Args:\n            cpu(dict): CPU spec\n            vcpu_num(int): number of virtual cpus\n            fill_topology(bool): if topology is not defined in ``cpu`` and\n                ``vcpu`` was not set, will add CPU topology to the generated\n                CPU.\n\n        Returns:\n            lxml.etree.Element: CPU XML node\n\n        Raises:\n            :exc:`~LagoInitException`: when failed to convert dict to XML\n        \"\"\"\n", "input": "", "output": "\n        try:\n            cpu = utils.dict_to_xml({'cpu': cpu})\n        except:\n            # TO-DO: print an example here\n            raise LagoInitException('conversion of \\'cpu\\' to XML failed')\n\n        if not cpu.xpath('topology') and fill_topology:\n            cpu.append(self.generate_topology(vcpu_num))\n        return cpu", "category": "Python"}, {"instruction": "def compute_cyclomatic_complexity(function):\n    \"\"\"\n    Compute the cyclomatic complexity of a function\n    Args:\n        function (core.declarations.function.Function)\n    Returns:\n        int\n    \"\"\"\n", "input": "", "output": "    # from https://en.wikipedia.org/wiki/Cyclomatic_complexity\n    # M = E - N + 2P\n    # where M is the complexity\n    # E number of edges\n    # N number of nodes\n    # P number of connected components\n\n    E = compute_number_edges(function)\n    N = len(function.nodes)\n    P = len(compute_strongly_connected_components(function))\n    return E - N + 2 * P", "category": "Python"}, {"instruction": "def inventory(self, modules_inventory=False):\n        \"\"\" Get chassis inventory.\n\n        :param modules_inventory: True - read modules inventory, false - don't read.\n        \"\"\"\n", "input": "", "output": "\n        self.c_info = self.get_attributes()\n        for m_index, m_portcounts in enumerate(self.c_info['c_portcounts'].split()):\n            if int(m_portcounts):\n                module = XenaModule(parent=self, index=m_index)\n                if modules_inventory:\n                    module.inventory()", "category": "Python"}, {"instruction": "def return_period_from_string(arg):\n    \"\"\"\n    Takes a string such as \"days=1,seconds=30\" and strips the quotes\n    and returns a dictionary with the key/value pairs\n\n    \"\"\"\n", "input": "", "output": "    period = {}\n\n    if arg[0] == '\"' and arg[-1] == '\"':\n        opt = arg[1:-1]  # remove quotes\n    else:\n        opt = arg\n\n    for o in opt.split(\",\"):\n        key, value = o.split(\"=\")\n        period[str(key)] = int(value)\n\n    return period", "category": "Python"}, {"instruction": "def process_minion_update(self, event_data):\n        '''\n        Associate grains data with a minion and publish minion update\n        '''\n", "input": "", "output": "        tag = event_data['tag']\n        event_info = event_data['data']\n\n        mid = tag.split('/')[-1]\n\n        if not self.minions.get(mid, None):\n            self.minions[mid] = {}\n\n        minion = self.minions[mid]\n\n        minion.update({'grains': event_info['return']})\n        log.debug(\"In process minion grains update with minions=%s\", self.minions)\n        self.publish_minions()", "category": "Python"}, {"instruction": "def _load_metadata(self):\n        \"\"\"(Re)load metadata from store.\"\"\"\n", "input": "", "output": "        if self._synchronizer is None:\n            self._load_metadata_nosync()\n        else:\n            mkey = self._key_prefix + array_meta_key\n            with self._synchronizer[mkey]:\n                self._load_metadata_nosync()", "category": "Python"}, {"instruction": "def draw_nodes(self):\n        \"\"\"\n        Draws nodes to the screen.\n\n        GeoPlot is the first plot kind to support an Altair backend in addition\n        to the usual matplotlib backend.\n        \"\"\"\n", "input": "", "output": "        if self.backend == \"matplotlib\":\n            node_r = 0.005  # temporarily hardcoded.\n            for i, node in enumerate(self.nodes):\n                x = self.node_coords[\"x\"][i]\n                y = self.node_coords[\"y\"][i]\n                color = self.node_colors[i]\n                node_patch = patches.Ellipse(\n                    (x, y), node_r, node_r, lw=0, color=color, zorder=2\n                )\n                self.ax.add_patch(node_patch)\n        elif self.backend == \"altair\":\n            self.node_chart = (\n                alt.Chart(self.node_df)\n                .mark_point()\n                .encode(\n                    alt.X(f\"{self.node_lon}:Q\", scale=alt.Scale(zero=False)),\n                    alt.Y(f\"{self.node_lat}:Q\", scale=alt.Scale(zero=False)),\n                )\n            )", "category": "Python"}, {"instruction": "def parse(self, xml_str):\n        # type: (str) -> List[EndpointDescription]\n        \"\"\"\n        Parses an EDEF XML string\n\n        :param xml_str: An XML string\n        :return: The list of parsed EndpointDescription\n        \"\"\"\n", "input": "", "output": "        # Parse the document\n        root = ElementTree.fromstring(xml_str)\n        if root.tag != TAG_ENDPOINT_DESCRIPTIONS:\n            raise ValueError(\"Not an EDEF XML: {0}\".format(root.tag))\n\n        # Parse content\n        return [\n            self._parse_description(node)\n            for node in root.findall(TAG_ENDPOINT_DESCRIPTION)\n        ]", "category": "Python"}, {"instruction": "def get_free_memory():\n    \"\"\"Return current free memory on the machine.\n\n    Currently supported for Windows, Linux, MacOS.\n\n    :returns: Free memory in MB unit\n    :rtype: int\n    \"\"\"\n", "input": "", "output": "    if 'win32' in sys.platform:\n        # windows\n        return get_free_memory_win()\n    elif 'linux' in sys.platform:\n        # linux\n        return get_free_memory_linux()\n    elif 'darwin' in sys.platform:\n        # mac\n        return get_free_memory_osx()", "category": "Python"}, {"instruction": "def update_views(self):\n        \"\"\"Update stats views.\"\"\"\n", "input": "", "output": "        # Call the father's method\n        super(Plugin, self).update_views()\n\n        # Add specifics informations\n        # Alert and log\n        for key in ['user', 'system', 'iowait']:\n            if key in self.stats:\n                self.views[key]['decoration'] = self.get_alert_log(self.stats[key], header=key)\n        # Alert only\n        for key in ['steal', 'total']:\n            if key in self.stats:\n                self.views[key]['decoration'] = self.get_alert(self.stats[key], header=key)\n        # Alert only but depend on Core number\n        for key in ['ctx_switches']:\n            if key in self.stats:\n                self.views[key]['decoration'] = self.get_alert(self.stats[key], maximum=100 * self.stats['cpucore'], header=key)\n        # Optional\n        for key in ['nice', 'irq', 'iowait', 'steal', 'ctx_switches', 'interrupts', 'soft_interrupts', 'syscalls']:\n            if key in self.stats:\n                self.views[key]['optional'] = True", "category": "Python"}, {"instruction": "def Geometric(p, tag=None):\n    \"\"\"\n    A Geometric random variate\n    \n    Parameters\n    ----------\n    p : scalar\n        The probability of success\n    \"\"\"\n", "input": "", "output": "    assert (\n        0 < p < 1\n    ), 'Geometric probability \"p\" must be between zero and one, non-inclusive'\n    return uv(ss.geom(p), tag=tag)", "category": "Python"}, {"instruction": "def register(self, func, singleton=False, threadlocal=False, name=None):\n        \"\"\"\n        Register a dependency function\n        \"\"\"\n", "input": "", "output": "        func._giveme_singleton = singleton\n        func._giveme_threadlocal = threadlocal\n\n        if name is None:\n            name = func.__name__\n        self._registered[name] = func\n        return func", "category": "Python"}, {"instruction": "def timefreq(x,fs=200):\n    \"\"\"\n    TIMEFREQ\n    \n    This function takes the time series and the sampling rate and calculates the\n    total number of points, the maximum frequency, the minimum (or change in)\n    frequency, and the vector of frequency points F.\n    \n    Version: 2011may04\n    \"\"\"\n", "input": "", "output": "    from numpy import size, shape, arange, append\n    \n    maxfreq=float(fs)/2.0 # Maximum frequency\n    minfreq=float(fs)/float(size(x,0)) # Minimum and delta frequency -- simply the inverse of the length of the recording in seconds\n    F=arange(minfreq,maxfreq+minfreq,minfreq) # Create frequencies evenly spaced from 0:minfreq:maxfreq\n    F=append(0,F) # Add zero-frequency component\n    \n    return F", "category": "Python"}, {"instruction": "def _list_fonts():\n    \"\"\"List system fonts\"\"\"\n", "input": "", "output": "    stdout_, stderr = run_subprocess(['fc-list', ':scalable=true', 'family'])\n    vals = [v.split(',')[0] for v in stdout_.strip().splitlines(False)]\n    return vals", "category": "Python"}, {"instruction": "def index(self, value):\n        \"\"\"\n        Gets the index in the list for a value\n        \"\"\"\n", "input": "", "output": "        if self.__modified_data__ is not None:\n            return self.__modified_data__.index(value)\n        return self.__original_data__.index(value)", "category": "Python"}, {"instruction": "def soap_action(self, service, action, payloadbody):\n        \"\"\"Do a soap request.\"\"\"\n", "input": "", "output": "        payload = self.soapenvelope.format(body=payloadbody).encode('utf-8')\n        headers = {\"Host\": self.url,\n                   \"Content-Type\": \"text/xml; charset=UTF-8\",\n                   \"Cache-Control\": \"no-cache\",\n                   \"Content-Length\": str(len(payload)),\n                   \"SOAPAction\": action}\n        try:\n            self.last_exception = None\n            response = requests.post(url=self.url + service, headers=headers,\n                                     data=payload, cookies=self.cookies)\n        except requests.exceptions.RequestException as exp:\n            self.last_exception = exp\n            return False\n        if response.status_code != 200:\n            self.last_response = response\n            return False\n        self.cookies = response.cookies\n        try:\n            xdoc = xml.etree.ElementTree.fromstring(response.text)\n        except xml.etree.ElementTree.ParseError as exp:\n            self.last_exception = exp\n            self.last_response = response\n            return False\n        return xdoc", "category": "Python"}, {"instruction": "def document_quote (document):\n    \"\"\"Quote given document.\"\"\"\n", "input": "", "output": "    doc, query = urllib.splitquery(document)\n    doc = url_quote_part(doc, '/=,')\n    if query:\n        return \"%s?%s\" % (doc, query)\n    return doc", "category": "Python"}, {"instruction": "def rc(self):\n     \"\"\"Flip the direction\"\"\"\n", "input": "", "output": "     ntx = self.copy()\n     newstrand = '+'\n     if ntx.strand == '+': newstrand = '-'\n     ntx._options = ntx._options._replace(direction=newstrand)\n     return ntx", "category": "Python"}, {"instruction": "def media(self):\n        \"\"\"\n        Return all media required to render this view, including forms.\n        \"\"\"\n", "input": "", "output": "        media = self._get_common_media()\n        media += self._get_view_media()\n        media += self.get_media_assets()\n        return media", "category": "Python"}, {"instruction": "def get_usage(self, start=None, end=None):\n        \"\"\"\n        Return the usage records for this load balancer. You may optionally\n        include a start datetime or an end datetime, or both, which will limit\n        the records to those on or after the start time, and those before or on\n        the end time. These times should be Python datetime.datetime objects,\n        Python datetime.date objects, or strings in the format:\n        \"YYYY-MM-DD HH:MM:SS\" or \"YYYY-MM-DD\".\n        \"\"\"\n", "input": "", "output": "        return self.manager.get_usage(self, start=start, end=end)", "category": "Python"}, {"instruction": "def __parse_identities(self, stream):\n        \"\"\"Parse identities stream\"\"\"\n", "input": "", "output": "\n        for aliases in self.__parse_stream(stream):\n            identity = self.__parse_alias(aliases[0])\n            uuid = identity.email\n\n            uid = self._identities.get(uuid, None)\n\n            if not uid:\n                uid = UniqueIdentity(uuid=uuid)\n                identity.uuid = uuid\n                uid.identities.append(identity)\n                self._identities[uuid] = uid\n\n                profile = Profile(uuid=uuid, name=identity.name, email=identity.email,\n                                  is_bot=False)\n                uid.profile = profile\n\n            # Aliases\n            for alias in aliases[1:]:\n                identity = self.__parse_alias(alias, uuid)\n                uid.identities.append(identity)\n\n            self._identities[uuid] = uid", "category": "Python"}, {"instruction": "def _write_lock(self):\n        \"\"\"Acquire and release the lock around output calls.\n\n        This should allow multiple threads or processes to write output\n        reliably.  Code that modifies the `_content` attribute should also do\n        so within this context.\n        \"\"\"\n", "input": "", "output": "        if self._lock:\n            lgr.debug(\"Acquiring write lock\")\n            self._lock.acquire()\n        try:\n            yield\n        finally:\n            if self._lock:\n                lgr.debug(\"Releasing write lock\")\n                self._lock.release()", "category": "Python"}, {"instruction": "def scan(self):\n        \"\"\"Trigger the wifi interface to scan.\"\"\"\n", "input": "", "output": "\n        self._logger.info(\"iface '%s' scans\", self.name())\n\n        self._wifi_ctrl.scan(self._raw_obj)", "category": "Python"}, {"instruction": "def localize(dt, force_to_local=True):\n  \"\"\"Localize a datetime to the local timezone.\n\n  If dt is naive, returns the same datetime with the local timezone, otherwise\n  uses astimezone to convert.\n\n  Args:\n    dt: datetime object.\n    force_to_local: Force all results to be in local time.\n\n  Returns:\n    A datetime_tz object.\n  \"\"\"\n", "input": "", "output": "  if not isinstance(dt, datetime_tz):\n    if not dt.tzinfo:\n      return datetime_tz(dt, tzinfo=localtz())\n    dt = datetime_tz(dt)\n  if force_to_local:\n    return dt.astimezone(localtz())\n  return dt", "category": "Python"}, {"instruction": "def load_labware(self, labware: Labware) -> Labware:\n        \"\"\" Specify the presence of a piece of labware on the module.\n\n        :param labware: The labware object. This object should be already\n                        initialized and its parent should be set to this\n                        module's geometry. To initialize and load a labware\n                        onto the module in one step, see\n                        :py:meth:`load_labware_by_name`.\n        :returns: The properly-linked labware object\n        \"\"\"\n", "input": "", "output": "        mod_labware = self._geometry.add_labware(labware)\n        self._ctx.deck.recalculate_high_z()\n        return mod_labware", "category": "Python"}, {"instruction": "def try_ntime(max_try, func, *args, **kwargs):\n    \"\"\"\n    Try execute a function n times, until no exception raised or tried\n    ``max_try`` times.\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u53cd\u590d\u5c1d\u8bd5\u6267\u884c\u4e00\u4e2a\u51fd\u6570\u82e5\u5e72\u6b21\u3002\u76f4\u5230\u6210\u529f\u4e3a\u6b62\u6216\u662f\u91cd\u590d\u5c1d\u8bd5 ``max_try`` \u6b21\u3002\u671f\u95f4\n    \u53ea\u8981\u6709\u4e00\u6b21\u6210\u529f, \u5c31\u6b63\u5e38\u8fd4\u56de\u3002\u5982\u679c\u4e00\u6b21\u90fd\u6ca1\u6709\u6210\u529f, \u5219\u884c\u4e3a\u8ddf\u6700\u540e\u4e00\u6b21\u6267\u884c\u4e86\n    ``func(*args, **kwargs)`` \u4e00\u6837\u3002\n    \"\"\"\n", "input": "", "output": "    if max_try < 1:\n        raise ValueError\n\n    for i in range(max_try):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            last_exception = e\n\n    raise last_exception", "category": "Python"}, {"instruction": "def log_config(verbose=1):\n    \"\"\"Set up logging the way I like it.\"\"\"\n", "input": "", "output": "    # ENH:\n    # - do print levelname before DEBUG and WARNING\n    # - instead of %module, name the currently running script\n    #   - make a subclass of logging.handlers.X instead?\n    #   - tweak %root?\n    #   - take __file__ as an argument?\n    if verbose == 0:\n        level = logging.WARNING\n        fmt = \"%(module)s: %(message)s\"\n    elif verbose == 1:\n        level = logging.INFO\n        fmt = \"%(module)s [@%(lineno)s]: %(message)s\"\n    else:\n        level = logging.DEBUG\n        fmt = \"%(module)s [%(lineno)s]: %(levelname)s: %(message)s\"\n    logging.basicConfig(format=fmt, level=level)", "category": "Python"}, {"instruction": "def rollback(self):\n        \"\"\"Abandon the current transaction.\n\n            Rollback all messages published during the current transaction\n            session to the remote server.\n\n            Note that all messages published during this transaction session\n            will be lost, and will have to be published again.\n\n            A new transaction session starts as soon as the command has\n            been executed.\n\n        :return:\n        \"\"\"\n", "input": "", "output": "        self._tx_active = False\n        return self._channel.rpc_request(specification.Tx.Rollback())", "category": "Python"}, {"instruction": "def validate(cls, mapper_spec):\n    \"\"\"Validates mapper spec.\n\n    Args:\n      mapper_spec: The MapperSpec for this InputReader.\n\n    Raises:\n      BadReaderParamsError: required parameters are missing or invalid.\n    \"\"\"\n", "input": "", "output": "    if mapper_spec.input_reader_class() != cls:\n      raise BadReaderParamsError(\"Input reader class mismatch\")\n    params = _get_params(mapper_spec)\n    if cls.BATCH_SIZE_PARAM in params:\n      try:\n        batch_size = int(params[cls.BATCH_SIZE_PARAM])\n        if batch_size < 1:\n          raise BadReaderParamsError(\"Bad batch size: %s\" % batch_size)\n      except ValueError, e:\n        raise BadReaderParamsError(\"Bad batch size: %s\" % e)", "category": "Python"}, {"instruction": "def sqlite_by_object(self, destination, progress):\n        \"\"\"This is probably not very fast.\"\"\"\n", "input": "", "output": "        db = SQLiteDatabase(destination)\n        db.create()\n        for script in self.sqlite_dump_string(progress): db.cursor.executescript(script)\n        db.close()", "category": "Python"}, {"instruction": "def nowrange_(self, col, timeframe):\n        \"\"\"\n        Returns a Dataswim instance with rows within a date range from now\n        ex: ds.nowrange(\"Date\", \"3D\") for a 3 days range. Units are: S,\n        H, D, W, M, Y\n        \"\"\"\n", "input": "", "output": "        df = self._nowrange(col, timeframe)\n        if df is None:\n            self.err(\"Can not select range data from now\")\n            return\n        return self._duplicate_(df)", "category": "Python"}, {"instruction": "def focused_data_item(self) -> typing.Optional[DataItem.DataItem]:\n        \"\"\"Return the data item with keyboard focus.\"\"\"\n", "input": "", "output": "        return self.__focused_display_item.data_item if self.__focused_display_item else None", "category": "Python"}, {"instruction": "def load_default_tc_plugins(self):\n        \"\"\"\n        Load default test case level plugins from icetea_lib.Plugin.plugins.default_plugins.\n\n        :return: Nothing\n        \"\"\"\n", "input": "", "output": "        for plugin_name, plugin_class in default_plugins.items():\n            if issubclass(plugin_class, PluginBase):\n                try:\n                    self.register_tc_plugins(plugin_name, plugin_class())\n                except PluginException as error:\n                    self.logger.debug(error)\n                    continue", "category": "Python"}, {"instruction": "def addError(self, test, err, capt=None):\n        \"\"\"\n        After a test error, we want to record testcase run information.\n        \"\"\"\n", "input": "", "output": "        self.__insert_test_result(constants.State.ERROR, test, err)", "category": "Python"}, {"instruction": "def __construct_list(self, list_value):\r\n        \"\"\" Loop list/set/tuple and parse values \"\"\"\n", "input": "", "output": "        array = []\r\n        for value in list_value:\r\n            array.append(self.__iterate_value(value))\r\n        return array", "category": "Python"}, {"instruction": "def report_invalid_syntax(self):\n        \"\"\"Check if the syntax is valid.\"\"\"\n", "input": "", "output": "        (exc_type, exc) = sys.exc_info()[:2]\n        if len(exc.args) > 1:\n            offset = exc.args[1]\n            if len(offset) > 2:\n                offset = offset[1:3]\n        else:\n            offset = (1, 0)\n        self.report_error(offset[0], offset[1] or 0,\n                          'E901 %s: %s' % (exc_type.__name__, exc.args[0]),\n                          self.report_invalid_syntax)", "category": "Python"}, {"instruction": "def write_files(self):\n        \"\"\"\n        write all data out into er_* and pmag_* files as appropriate\n        \"\"\"\n", "input": "", "output": "        warnings = self.validate_data()\n\n        print('-I- Writing all saved data to files')\n        if self.measurements:\n            self.write_measurements_file()\n        for dtype in ['specimen', 'sample', 'site']:\n            if self.data_lists[dtype][0]:\n                do_pmag = dtype in self.incl_pmag_data\n                self.write_magic_file(dtype, do_er=True, do_pmag=do_pmag)\n                if not do_pmag:\n                    pmag_file = os.path.join(self.WD, 'pmag_' + dtype + 's.txt')\n                    if os.path.isfile(pmag_file):\n                        os.remove(pmag_file)\n\n        if self.locations:\n            self.write_magic_file('location', do_er=True, do_pmag=False)\n\n        self.write_age_file()\n\n        if self.results:\n            self.write_result_file()\n\n        if warnings:\n            print('-W- ' + str(warnings))\n            return False, warnings\n\n        return True, None", "category": "Python"}, {"instruction": "def end_of_chunk(prev_tag, tag, prev_type, type_):\n    \"\"\"Checks if a chunk ended between the previous and current word.\n\n    Args:\n        prev_tag: previous chunk tag.\n        tag: current chunk tag.\n        prev_type: previous type.\n        type_: current type.\n\n    Returns:\n        chunk_end: boolean.\n    \"\"\"\n", "input": "", "output": "    chunk_end = False\n\n    if prev_tag == 'E': chunk_end = True\n    if prev_tag == 'S': chunk_end = True\n\n    if prev_tag == 'B' and tag == 'B': chunk_end = True\n    if prev_tag == 'B' and tag == 'S': chunk_end = True\n    if prev_tag == 'B' and tag == 'O': chunk_end = True\n    if prev_tag == 'I' and tag == 'B': chunk_end = True\n    if prev_tag == 'I' and tag == 'S': chunk_end = True\n    if prev_tag == 'I' and tag == 'O': chunk_end = True\n\n    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n        chunk_end = True\n\n    return chunk_end", "category": "Python"}, {"instruction": "def _close_writable(self):\n        \"\"\"\n        Close the object in write mode.\n        \"\"\"\n", "input": "", "output": "        # Wait segments upload completion\n        for segment in self._write_futures:\n            segment['etag'] = segment['etag'].result()\n\n        # Upload manifest file\n        with _handle_client_exception():\n            self._client.put_object(self._container, self._object_name, _dumps(\n                self._write_futures), query_string='multipart-manifest=put')", "category": "Python"}, {"instruction": "def rename_module(new, old):\n    \"\"\"\n    Attempts to import the old module and load it under the new name.\n    Used for purely cosmetic name changes in Python 3.x.\n    \"\"\"\n", "input": "", "output": "    try:\n        sys.modules[new] = imp.load_module(old, *imp.find_module(old))\n        return True\n    except ImportError:\n        return False", "category": "Python"}, {"instruction": "def taskfileinfo_descriptor_data(tfi, role):\n    \"\"\"Return the data for descriptor\n\n    :param tfi: the :class:`jukeboxcore.filesys.TaskFileInfo` holds the data\n    :type tfi: :class:`jukeboxcore.filesys.TaskFileInfo`\n    :param role: item data role\n    :type role: QtCore.Qt.ItemDataRole\n    :returns: data for the descriptor\n    :rtype: depending on role\n    :raises: None\n    \"\"\"\n", "input": "", "output": "    if role == QtCore.Qt.DisplayRole or role == QtCore.Qt.EditRole:\n        return tfi.descriptor", "category": "Python"}, {"instruction": "async def _set_persistent_menu(self):\n        \"\"\"\n        Define the persistent menu for all pages\n        \"\"\"\n", "input": "", "output": "\n        page = self.settings()\n\n        if 'menu' in page:\n            await self._send_to_messenger_profile(page, {\n                'persistent_menu': page['menu'],\n            })\n\n            logger.info('Set menu for page %s', page['page_id'])", "category": "Python"}, {"instruction": "def annotate(self, text, annotators=None):\n        \"\"\"Return an AnnotatedDocument from the CoreNLP server.\n\n        :param (str) text: text to be annotated\n        :param (list[str]) annotators: a list of annotator names\n\n        See a list of valid annotator names here:\n          http://stanfordnlp.github.io/CoreNLP/annotators.html\n\n        :return (AnnotatedDocument): an annotated document\n        \"\"\"\n", "input": "", "output": "        doc_pb = self.annotate_proto(text, annotators)\n        return AnnotatedDocument.from_pb(doc_pb)", "category": "Python"}, {"instruction": "def fit(self, X, truncated=3):\n        \"\"\"Fit a vine model to the data.\n\n        Args:\n            X(numpy.ndarray): data to be fitted.\n            truncated(int): max level to build the vine.\n        \"\"\"\n", "input": "", "output": "        self.n_sample, self.n_var = X.shape\n        self.columns = X.columns\n        self.tau_mat = X.corr(method='kendall').values\n        self.u_matrix = np.empty([self.n_sample, self.n_var])\n\n        self.truncated = truncated\n        self.depth = self.n_var - 1\n        self.trees = []\n\n        self.unis, self.ppfs = [], []\n        for i, col in enumerate(X):\n            uni = self.model()\n            uni.fit(X[col])\n            self.u_matrix[:, i] = uni.cumulative_distribution(X[col])\n            self.unis.append(uni)\n            self.ppfs.append(uni.percent_point)\n\n        self.train_vine(self.vine_type)\n        self.fitted = True", "category": "Python"}, {"instruction": "def _energy_distance_imp(x, y, exponent=1):\n    \"\"\"\n    Real implementation of :func:`energy_distance`.\n\n    This function is used to make parameter ``exponent`` keyword-only in\n    Python 2.\n\n    \"\"\"\n", "input": "", "output": "    x = _transform_to_2d(x)\n    y = _transform_to_2d(y)\n\n    _check_valid_energy_exponent(exponent)\n\n    distance_xx = distances.pairwise_distances(x, exponent=exponent)\n    distance_yy = distances.pairwise_distances(y, exponent=exponent)\n    distance_xy = distances.pairwise_distances(x, y, exponent=exponent)\n\n    return _energy_distance_from_distance_matrices(distance_xx=distance_xx,\n                                                   distance_yy=distance_yy,\n                                                   distance_xy=distance_xy)", "category": "Python"}, {"instruction": "def p_try_statement_3(self, p):\n        \"\"\"try_statement : TRY block catch finally\"\"\"\n", "input": "", "output": "        p[0] = self.asttypes.Try(statements=p[2], catch=p[3], fin=p[4])\n        p[0].setpos(p)", "category": "Python"}, {"instruction": "def hyperedge_cardinality_pairs_list(H):\n    \"\"\"Returns a list of 2-tuples of (\\|tail\\|, \\|head\\|) for each hyperedge\n    in the hypergraph.\n\n    :param H: the hypergraph whose cardinality ratios will be\n            operated on.\n    :returns: list -- list of 2-tuples for each hyperedge's cardinality.\n    :raises: TypeError -- Algorithm only applicable to directed hypergraphs\n\n    \"\"\"\n", "input": "", "output": "    if not isinstance(H, DirectedHypergraph):\n        raise TypeError(\"Algorithm only applicable to directed hypergraphs\")\n\n    return [(len(H.get_hyperedge_tail(hyperedge_id)),\n            len(H.get_hyperedge_head(hyperedge_id)))\n            for hyperedge_id in H.hyperedge_id_iterator()]", "category": "Python"}, {"instruction": "def trunc_neg_eigs(self, particle):\n        \"\"\"\n        Given a state represented as a model parameter vector,\n        returns a model parameter vector representing the same\n        state with any negative eigenvalues set to zero.\n\n        :param np.ndarray particle: Vector of length ``(dim ** 2, )``\n            representing a state.\n        :return: The same state with any negative eigenvalues\n            set to zero.\n        \"\"\"\n", "input": "", "output": "        arr = np.tensordot(particle, self._basis.data.conj(), 1)\n        w, v = np.linalg.eig(arr)\n        if np.all(w >= 0):\n            return particle\n        else:\n            w[w < 0] = 0\n            new_arr = np.dot(v * w, v.conj().T)\n            new_particle = np.real(np.dot(self._basis.flat(), new_arr.flatten()))\n            assert new_particle[0] > 0\n            return new_particle", "category": "Python"}, {"instruction": "def print_prefixed_lines(lines: List[Tuple[str, Optional[str]]]) -> str:\n    \"\"\"Print lines specified like this: [\"prefix\", \"string\"]\"\"\"\n", "input": "", "output": "    existing_lines = [line for line in lines if line[1] is not None]\n    pad_len = reduce(lambda pad, line: max(pad, len(line[0])), existing_lines, 0)\n    return \"\\n\".join(\n        map(\n            lambda line: line[0].rjust(pad_len) + line[1], existing_lines  # type:ignore\n        )\n    )", "category": "Python"}, {"instruction": "def get_symlink_luid():\n\t\"\"\"\n\tGet the LUID for the SeCreateSymbolicLinkPrivilege\n\t\"\"\"\n", "input": "", "output": "\tsymlink_luid = privilege.LUID()\n\tres = privilege.LookupPrivilegeValue(\n\t\tNone, \"SeCreateSymbolicLinkPrivilege\", symlink_luid)\n\tif not res > 0:\n\t\traise RuntimeError(\"Couldn't lookup privilege value\")\n\treturn symlink_luid", "category": "Python"}, {"instruction": "def filter_threshold(self, inst_rc, threshold, num_occur=1):\n    '''\n    Filter the matrix rows or columns based on num_occur values being above a\n    threshold (in absolute value).\n    '''\n", "input": "", "output": "    inst_df = self.dat_to_df()\n\n    inst_df = run_filter.filter_threshold(inst_df, inst_rc, threshold,\n      num_occur)\n\n    self.df_to_dat(inst_df)", "category": "Python"}, {"instruction": "def get_from_name(self, name):\n        \"\"\"\n        Returns the item with the given name, or None if no such item\n        is known.\n        \"\"\"\n", "input": "", "output": "        with self.condition:\n            try:\n                item_id = self.name2id[name]\n            except KeyError:\n                return None\n            return self.id2item[item_id]\n        return None", "category": "Python"}, {"instruction": "def safe_dump(data, stream=None, **kwds):\n    \"\"\"\n    Serialize a Python object into a YAML stream.\n    Produce only basic YAML tags.\n    If stream is None, return the produced string instead.\n    \"\"\"\n", "input": "", "output": "    return dump_all([data], stream, Dumper=SafeDumper, **kwds)", "category": "Python"}, {"instruction": "def fn_getds(fn):\n    \"\"\"Wrapper around gdal.Open()\n    \"\"\"\n", "input": "", "output": "    ds = None\n    if fn_check(fn):\n        ds = gdal.Open(fn, gdal.GA_ReadOnly)\n    else:\n        print(\"Unable to find %s\" % fn)\n    return ds", "category": "Python"}, {"instruction": "def remove_message_listener(self, name, fn):\n        \"\"\"\n        Removes a message listener (that was previously added using :py:func:`add_message_listener`).\n\n        See :ref:`mavlink_messages` for more information.\n\n        :param String name: The name of the message for which the listener is to be removed (or '*' to remove an 'all messages' observer).\n        :param fn: The listener callback function to remove.\n\n        \"\"\"\n", "input": "", "output": "        name = str(name)\n        if name in self._message_listeners:\n            if fn in self._message_listeners[name]:\n                self._message_listeners[name].remove(fn)\n                if len(self._message_listeners[name]) == 0:\n                    del self._message_listeners[name]", "category": "Python"}, {"instruction": "def unlock_kinetis_read_until_ack(jlink, address):\n    \"\"\"Polls the device until the request is acknowledged.\n\n    Sends a read request to the connected device to read the register at the\n    given 'address'.  Polls indefinitely until either the request is ACK'd or\n    the request ends in a fault.\n\n    Args:\n      jlink (JLink): the connected J-Link\n      address (int) the address of the register to poll\n\n    Returns:\n      ``SWDResponse`` object on success.\n\n    Raises:\n      KinetisException: when read exits with non-ack or non-wait status.\n\n    Note:\n      This function is required in order to avoid reading corrupt or otherwise\n      invalid data from registers when communicating over SWD.\n    \"\"\"\n", "input": "", "output": "    request = swd.ReadRequest(address, ap=True)\n    response = None\n    while True:\n        response = request.send(jlink)\n        if response.ack():\n            break\n        elif response.wait():\n            continue\n        raise KinetisException('Read exited with status: %s', response.status)\n\n    return response", "category": "Python"}, {"instruction": "def mimebundle_to_html(bundle):\n    \"\"\"\n    Converts a MIME bundle into HTML.\n    \"\"\"\n", "input": "", "output": "    if isinstance(bundle, tuple):\n        data, metadata = bundle\n    else:\n        data = bundle\n    html = data.get('text/html', '')\n    if 'application/javascript' in data:\n        js = data['application/javascript']\n        html += '\\n<script type=\"application/javascript\">{js}</script>'.format(js=js)\n    return html", "category": "Python"}, {"instruction": "def _log(self, name, element):  # pylint: disable=no-self-use\n        \"\"\"\n        Log Response and Tag elements. Do nothing if elements is none of them.\n        \"\"\"\n", "input": "", "output": "        from bs4 import BeautifulSoup, Tag\n        if isinstance(element, Response):\n            LOGGER.debug('%s response: URL=%s Code=%s', name, element.url, element.status_code)\n        elif isinstance(element, (BeautifulSoup, Tag)):\n            LOGGER.debug('%s HTML:\\n%s', name, element)", "category": "Python"}, {"instruction": "def parse_database_url(url):\n    \"\"\"Parses a database URL.\"\"\"\n", "input": "", "output": "\n    if url == 'sqlite://:memory:':\n        # this is a special case, because if we pass this URL into\n        # urlparse, urlparse will choke trying to interpret \"memory\"\n        # as a port number\n        return {\n            'ENGINE': DATABASE_SCHEMES['sqlite'],\n            'NAME': ':memory:'\n        }\n        # note: no other settings are required for sqlite\n\n    # otherwise parse the url as normal\n    config = {}\n\n    url = urlparse.urlparse(url)\n\n    # Remove query strings.\n    path = url.path[1:]\n    path = path.split('?', 2)[0]\n\n    # if we are using sqlite and we have no path, then assume we\n    # want an in-memory database (this is the behaviour of sqlalchemy)\n    if url.scheme == 'sqlite' and path == '':\n        path = ':memory:'\n\n    # Update with environment configuration.\n    config.update({\n        'NAME': path or '',\n        'USER': url.username or '',\n        'PASSWORD': url.password or '',\n        'HOST': url.hostname or '',\n        'PORT': url.port or '',\n    })\n\n    if url.scheme in DATABASE_SCHEMES:\n        config['ENGINE'] = DATABASE_SCHEMES[url.scheme]\n\n    return config", "category": "Python"}, {"instruction": "def mysql(host, user, passwd, db, charset):\n    \"\"\"Set MySQL/MariaDB connection\"\"\"\n", "input": "", "output": "    connection_string = database.set_mysql_connection(host=host, user=user, passwd=passwd, db=db, charset=charset)\n    test_connection(connection_string)", "category": "Python"}, {"instruction": "def set_ecdh_curve(self, curve_name=None):\n        u''' Select a curve to use for ECDH(E) key exchange or set it to auto mode\n\n        Used for server only!\n\n        s.a. openssl.exe ecparam -list_curves\n\n        :param None | str curve_name: None = Auto-mode, \"secp256k1\", \"secp384r1\", ...\n        :return: 1 for success and 0 for failure\n        '''\n", "input": "", "output": "        if curve_name:\n            retVal = SSL_CTX_set_ecdh_auto(self._ctx, 0)\n            avail_curves = get_elliptic_curves()\n            key = [curve for curve in avail_curves if curve.name == curve_name][0].to_EC_KEY()\n            retVal &= SSL_CTX_set_tmp_ecdh(self._ctx, key)\n        else:\n            retVal = SSL_CTX_set_ecdh_auto(self._ctx, 1)\n        return retVal", "category": "Python"}, {"instruction": "def queryMany(self, query, args):\n\t\t\"\"\"\n\t\tExecutes a series of the same Insert Statments\n\t\t\n\t\tEach tuple in the args list will be applied to the query and executed.\n\t\tThis is the equivilant of MySQLDB.cursor.executemany()\n\t\t\n\t\t@author: Nick Verbeck\n\t\t@since: 9/7/2008\n\t\t\"\"\"\n", "input": "", "output": "\t\tself.lastError = None\n\t\tself.affectedRows = None\n\t\tself.rowcount = None\n\t\tself.record = None\n\t\tcursor = None\n\t\t\n\t\ttry:\n\t\t\ttry:\n\t\t\t\tself._GetConnection()\n\t\t\t\tself.conn.query = query\n\t\t\t\t#Execute query and store results\n\t\t\t\tcursor = self.conn.getCursor()\n\t\t\t\tself.affectedRows = cursor.executemany(query, args)\n\t\t\t\tself.conn.updateCheckTime()\n\t\t\texcept Exception, e:\n\t\t\t\tself.lastError = e\n\t\tfinally:\n\t\t\tif cursor is not None:\n\t\t\t\tcursor.close()\n\t\t\tself._ReturnConnection()\n\t\t\tif self.lastError is not None:\n\t\t\t\traise self.lastError\n\t\t\telse:\n\t\t\t\treturn self.affectedRows", "category": "Python"}, {"instruction": "def at(self, timestamp=None, **kwargs):\n        \"\"\"\n        at(timestamp) -> SubRipFile clone\n\n        timestamp argument should be coercible to SubRipFile object.\n\n        A specialization of slice. Return all subtiles visible at the\n        timestamp mark.\n\n        Example:\n            >>> subs.at((0, 0, 20, 0)).shift(seconds=2)\n            >>> subs.at(seconds=20).shift(seconds=2)\n        \"\"\"\n", "input": "", "output": "        time = timestamp or kwargs\n        return self.slice(starts_before=time, ends_after=time)", "category": "Python"}, {"instruction": "def volume_create(self, name, size=100, snapshot=None, voltype=None,\n                      availability_zone=None):\n        '''\n        Create a block device\n        '''\n", "input": "", "output": "        if self.volume_conn is None:\n            raise SaltCloudSystemExit('No cinder endpoint available')\n        nt_ks = self.volume_conn\n        response = nt_ks.volumes.create(\n            size=size,\n            display_name=name,\n            volume_type=voltype,\n            snapshot_id=snapshot,\n            availability_zone=availability_zone\n        )\n\n        return self._volume_get(response.id)", "category": "Python"}, {"instruction": "def fetch_all_first_values(session: Session,\n                           select_statement: Select) -> List[Any]:\n    \"\"\"\n    Returns a list of the first values in each row returned by a ``SELECT``\n    query.\n\n    A Core version of this sort of thing:\n    http://xion.io/post/code/sqlalchemy-query-values.html\n\n    Args:\n        session: SQLAlchemy :class:`Session` object\n        select_statement: SQLAlchemy :class:`Select` object\n\n    Returns:\n        a list of the first value of each result row\n\n    \"\"\"\n", "input": "", "output": "    rows = session.execute(select_statement)  # type: ResultProxy\n    try:\n        return [row[0] for row in rows]\n    except ValueError as e:\n        raise MultipleResultsFound(str(e))", "category": "Python"}, {"instruction": "def _max(self):\n        \"\"\"Getter for the maximum series value\"\"\"\n", "input": "", "output": "        return (\n            self.range[1] if (self.range and self.range[1] is not None) else\n            (max(self.yvals) if self.yvals else None)\n        )", "category": "Python"}, {"instruction": "def map_column(self, keys, func):\n        \"\"\"\n        Args:\n            keys (list or str): the column name(s) to apply the `func` to\n            func (callable): applied to each element in the specified columns\n        \"\"\"\n", "input": "", "output": "        return [[func(v) for v in self[key]] for key in keys]", "category": "Python"}, {"instruction": "def stderr_output(cmd):\n    \"\"\"Wraps the execution of check_output in a way that\n    ignores stderr when not in debug mode\"\"\"\n", "input": "", "output": "\n    handle, gpg_stderr = stderr_handle()\n    try:\n        output = subprocess.check_output(cmd, stderr=gpg_stderr)  # nosec\n        if handle:\n            handle.close()\n\n        return str(polite_string(output))\n    except subprocess.CalledProcessError as exception:\n        LOGGER.debug(\"GPG Command %s\", ' '.join(exception.cmd))\n        LOGGER.debug(\"GPG Output %s\", exception.output)\n        raise CryptoritoError('GPG Execution')", "category": "Python"}, {"instruction": "def _get_route_info(self, request):\n        \"\"\"Return information about the current URL.\"\"\"\n", "input": "", "output": "        resolve_match = resolve(request.path)\n\n        app_name = resolve_match.app_name    # The application namespace for the URL pattern that matches the URL.\n        namespace = resolve_match.namespace  # The instance namespace for the URL pattern that matches the URL.\n        url_name = resolve_match.url_name    # The name of the URL pattern that matches the URL.\n        view_name = resolve_match.view_name  # Name of the view that matches the URL, incl. namespace if there's one.\n\n        return {\n            \"app_name\": app_name or None,\n            \"namespace\": namespace or None,\n            \"url_name\": url_name or None,\n            \"view_name\": view_name or None,\n        }", "category": "Python"}, {"instruction": "def get_keyboard() -> types.InlineKeyboardMarkup:\n    \"\"\"\n    Generate keyboard with list of posts\n    \"\"\"\n", "input": "", "output": "    markup = types.InlineKeyboardMarkup()\n    for post_id, post in POSTS.items():\n        markup.add(\n            types.InlineKeyboardButton(\n                post['title'],\n                callback_data=posts_cb.new(id=post_id, action='view'))\n        )\n    return markup", "category": "Python"}, {"instruction": "def atexit_unregister(func, *args, **kwargs):\n    \"\"\"Python 2/3 compatible method for unregistering exit function.\n\n    Python2 has no atexit.unregister function :/\n    \"\"\"\n", "input": "", "output": "    try:\n        atexit.unregister(func, *args, **kwargs)\n    except AttributeError:\n        # This code runs in Python 2.7 *only*\n        # Only replace with a noop, don't delete during iteration\n        for i in range(len(atexit._exithandlers)):\n            if atexit._exithandlers[i] == (func, args, kwargs):\n                atexit._exithandlers[i] = (lambda: None, [], {})\n                break", "category": "Python"}, {"instruction": "def _create_dim_scales(self):\n        \"\"\"Create all necessary HDF5 dimension scale.\"\"\"\n", "input": "", "output": "        dim_order = self._dim_order.maps[0]\n        for dim in sorted(dim_order, key=lambda d: dim_order[d]):\n            if dim not in self._h5group:\n                size = self._current_dim_sizes[dim]\n                kwargs = {}\n                if self._dim_sizes[dim] is None:\n                    kwargs[\"maxshape\"] = (None,)\n                self._h5group.create_dataset(\n                    name=dim, shape=(size,), dtype='S1', **kwargs)\n\n            h5ds = self._h5group[dim]\n            h5ds.attrs['_Netcdf4Dimid'] = dim_order[dim]\n\n            if len(h5ds.shape) > 1:\n                dims = self._variables[dim].dimensions\n                coord_ids = np.array([dim_order[d] for d in dims], 'int32')\n                h5ds.attrs['_Netcdf4Coordinates'] = coord_ids\n\n            scale_name = dim if dim in self.variables else NOT_A_VARIABLE\n            h5ds.dims.create_scale(h5ds, scale_name)\n\n        for subgroup in self.groups.values():\n            subgroup._create_dim_scales()", "category": "Python"}, {"instruction": "def message(self, to, subject, text):\n        \"\"\"Alias for :meth:`compose`.\"\"\"\n", "input": "", "output": "        return self.compose(to, subject, text)", "category": "Python"}, {"instruction": "def Update(self, data):\n        \"\"\"Updates a Beta distribution.\n\n        data: pair of int (heads, tails)\n        \"\"\"\n", "input": "", "output": "        heads, tails = data\n        self.alpha += heads\n        self.beta += tails", "category": "Python"}, {"instruction": "def is_known_scalar(value):\n    \"\"\"\n    Return True if value is a type we expect in a dataframe\n    \"\"\"\n", "input": "", "output": "    def _is_datetime_or_timedelta(value):\n        # Using pandas.Series helps catch python, numpy and pandas\n        # versions of these types\n        return pd.Series(value).dtype.kind in ('M', 'm')\n\n    return not np.iterable(value) and (isinstance(value, numbers.Number) or\n                                       _is_datetime_or_timedelta(value))", "category": "Python"}, {"instruction": "def auto_correlation(sequence):\n    \"\"\"\n    test for the autocorrelation of a sequence between t and t - 1\n    as the 'auto_correlation' it is less likely that the sequence is\n    generated randomly.\n    :param sequence: any iterable with at most 2 values that can be turned\n                     into a float via np.float . e.g.\n                     '1001001'\n                     [1, 0, 1, 0, 1]\n                     [1.2,.1,.5,1]\n    :rtype: returns a dict of the linear regression stats of sequence[1:] vs.\n            sequence[:-1]\n\n    >>> result = auto_correlation('00000001111111111100000000')\n    >>> result['p'] < 0.05\n    True\n    >>> result['auto_correlation']\n    0.83766233766233755\n\n    \"\"\"\n", "input": "", "output": "    if isinstance(sequence, basestring):\n        sequence = map(int, sequence)\n    seq = np.array(list(sequence), dtype=np.float)\n    dseq = np.column_stack((seq[1:], seq[:-1]))\n    slope, intercept, r, ttp, see = linregress(seq[1:], seq[:-1])\n    cc = np.corrcoef(dseq, rowvar=0)[0][1]\n    return {'slope': slope, 'intercept': intercept, 'r-squared': r ** 2,\n            'p': ttp, 'see': see, 'auto_correlation': cc}", "category": "Python"}, {"instruction": "def get_matches(pattern, language, max_count=8):\n    \"\"\"\n    take a word pattern or a Python regexp and a language name, and return a\n    list of all matching words.\n    \"\"\"\n", "input": "", "output": "    if str(pattern) == pattern:\n        pattern = compile_pattern(pattern)\n\n    results = []\n\n    if not dicts.exists(language):\n        print(\"The language '%s' is not available locally.\" % language)\n        return []\n\n    with open(dicts.filepath(language), 'r') as f:\n        for word in f:\n            if max_count <= 0:\n                break\n            w = word.strip()\n            if pattern.match(w) and w not in results:\n                results.append(w)\n                max_count -= 1\n\n    return results", "category": "Python"}, {"instruction": "def run(self, **kwargs):\n        \"\"\"\n        Run an IDF file with a given EnergyPlus weather file. This is a\n        wrapper for the EnergyPlus command line interface.\n\n        Parameters\n        ----------\n        **kwargs\n            See eppy.runner.functions.run()\n\n        \"\"\"\n", "input": "", "output": "        # write the IDF to the current directory\n        self.saveas('in.idf')\n        # if `idd` is not passed explicitly, use the IDF.iddname\n        idd = kwargs.pop('idd', self.iddname)\n        epw = kwargs.pop('weather', self.epw)\n        try:\n            run(self, weather=epw, idd=idd, **kwargs)\n        finally:\n            os.remove('in.idf')", "category": "Python"}, {"instruction": "async def get_partition_ids_async(self):\n        \"\"\"\n        Returns a list of all the event hub partition IDs.\n\n        :rtype: list[str]\n        \"\"\"\n", "input": "", "output": "        if not self.partition_ids:\n            try:\n                eh_client = EventHubClientAsync(\n                    self.host.eh_config.client_address,\n                    debug=self.host.eph_options.debug_trace,\n                    http_proxy=self.host.eph_options.http_proxy)\n                try:\n                    eh_info = await eh_client.get_eventhub_info_async()\n                    self.partition_ids = eh_info['partition_ids']\n                except Exception as err:  # pylint: disable=broad-except\n                    raise Exception(\"Failed to get partition ids\", repr(err))\n            finally:\n                await eh_client.stop_async()\n        return self.partition_ids", "category": "Python"}, {"instruction": "def validate_template(template_body=None, template_url=None, region=None, key=None, keyid=None, profile=None):\n    '''\n    Validate cloudformation template\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_cfn.validate_template mystack-template\n    '''\n", "input": "", "output": "    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        # Returns an object if json is validated and an exception if its not\n        return conn.validate_template(template_body, template_url)\n    except BotoServerError as e:\n        log.debug(e)\n        msg = 'Error while trying to validate template {0}.'.format(template_body)\n        log.error(msg)\n        return six.text_type(e)", "category": "Python"}, {"instruction": "def cache_backend(self):\n        \"\"\"\n        Get the cache backend\n\n        Returns\n        ~~~~~~~\n        Django cache backend\n\n        \"\"\"\n", "input": "", "output": "        if not hasattr(self, '_cache_backend'):\n            if hasattr(django.core.cache, 'caches'):\n                self._cache_backend = django.core.cache.caches[_cache_name]\n            else:\n                self._cache_backend = django.core.cache.get_cache(_cache_name)\n\n        return self._cache_backend", "category": "Python"}, {"instruction": "def push_back(self, value):\r\n        '''Appends a copy of *value* at the end of the :class:`Sequence`.'''\n", "input": "", "output": "        self.cache.push_back(self.value_pickler.dumps(value))\r\n        return self", "category": "Python"}, {"instruction": "def _split_regex(regex):\n    \"\"\"\n    Return an array of the URL split at each regex match like (?P<id>[\\d]+)\n    Call with a regex of '^/foo/(?P<id>[\\d]+)/bar/$' and you will receive ['/foo/', '/bar/']\n    \"\"\"\n", "input": "", "output": "    if regex[0] == '^':\n        regex = regex[1:]\n    if regex[-1] == '$':\n        regex = regex[0:-1]\n    results = []\n    line = ''\n    for c in regex:\n        if c == '(':\n            results.append(line)\n            line = ''\n        elif c == ')':\n            line = ''\n        else:\n            line = line + c\n    if len(line) > 0:\n        results.append(line)\n    return results", "category": "Python"}, {"instruction": "def make_break(lineno, p):\n    \"\"\" Checks if --enable-break is set, and if so, calls\n    BREAK keyboard interruption for this line if it has not been already\n    checked \"\"\"\n", "input": "", "output": "    global last_brk_linenum\n\n    if not OPTIONS.enableBreak.value or lineno == last_brk_linenum or is_null(p):\n        return None\n\n    last_brk_linenum = lineno\n    return make_sentence('CHKBREAK', make_number(lineno, lineno, TYPE.uinteger))", "category": "Python"}, {"instruction": "def distance_home(GPS_RAW):\n    '''distance from first fix point'''\n", "input": "", "output": "    global first_fix\n    if (hasattr(GPS_RAW, 'fix_type') and GPS_RAW.fix_type < 2) or \\\n       (hasattr(GPS_RAW, 'Status')   and GPS_RAW.Status   < 2):\n        return 0\n\n    if first_fix == None:\n        first_fix = GPS_RAW\n        return 0\n    return distance_two(GPS_RAW, first_fix)", "category": "Python"}, {"instruction": "def removeItem( self, item ):\n        \"\"\"\n        Overloads the default QGraphicsScene method to handle cleanup and \\\n        additional removal options for nodes.\n        \n        :param      item        <QGraphicsItem>\n        \n        :return     <bool>\n        \"\"\"\n", "input": "", "output": "        # for nodes and connections, call the prepareToRemove method before \n        # removing\n        if ( isinstance( item, XNode ) or \n             isinstance( item, XNodeConnection ) ):\n            # make sure this item is ok to remove\n            if ( not item.prepareToRemove() ):\n                return False\n        \n        # remove the item using the base class method\n        try:\n            self._cache.remove(item)\n        except KeyError:\n            pass\n        \n        # mark the scene as modified\n        self.setModified(True)\n        super(XNodeScene, self).removeItem(item)\n        \n        if not self.signalsBlocked():\n            self.itemsRemoved.emit()\n        \n        return True", "category": "Python"}, {"instruction": "def sub_base_uri(self):\n        \"\"\" This will return the sub_base_uri parsed from the base_uri\n        :return: str of the sub_base_uri\n        \"\"\"\n", "input": "", "output": "        return self._base_uri and \\\n               self._base_uri.split('://')[-1].split('.')[0] \\\n               or self._base_uri", "category": "Python"}, {"instruction": "def _make_compile_argv(self, compile_request):\n    \"\"\"Return a list of arguments to use to compile sources. Subclasses can override and append.\"\"\"\n", "input": "", "output": "\n    sources_minus_headers = list(self._iter_sources_minus_headers(compile_request))\n    if len(sources_minus_headers) == 0:\n      raise self._HeaderOnlyLibrary()\n\n    compiler = compile_request.compiler\n    compiler_options = compile_request.compiler_options\n    # We are going to execute in the target output, so get absolute paths for everything.\n    buildroot = get_buildroot()\n    # TODO: add -v to every compiler and linker invocation!\n    argv = (\n      [compiler.exe_filename] +\n      compiler.extra_args +\n      # TODO: If we need to produce static libs, don't add -fPIC! (could use Variants -- see #5788).\n      ['-c', '-fPIC'] +\n      compiler_options +\n      [\n        '-I{}'.format(os.path.join(buildroot, inc_dir))\n        for inc_dir in compile_request.include_dirs\n      ] +\n      [os.path.join(buildroot, src) for src in sources_minus_headers])\n\n    self.context.log.info(\"selected compiler exe name: '{}'\".format(compiler.exe_filename))\n    self.context.log.debug(\"compile argv: {}\".format(argv))\n\n    return argv", "category": "Python"}, {"instruction": "def _build_func(eq, **args):\n    r'''\n    Take a symbolic equation and return the lambdified version plus the\n    linearization of form S1 * x + S2\n    '''\n", "input": "", "output": "    eq_prime = eq.diff(args['x'])\n    s1 = eq_prime\n    s2 = eq - eq_prime*args['x']\n    EQ = _syp.lambdify(args.values(), expr=eq, modules='numpy')\n    S1 = _syp.lambdify(args.values(), expr=s1, modules='numpy')\n    S2 = _syp.lambdify(args.values(), expr=s2, modules='numpy')\n    return EQ, S1, S2", "category": "Python"}, {"instruction": "def _tree_line(self, no_type: bool = False) -> str:\n        \"\"\"Return the receiver's contribution to tree diagram.\"\"\"\n", "input": "", "output": "        return self._tree_line_prefix() + \" \" + self.iname()", "category": "Python"}, {"instruction": "def clone(self, config, **kwargs):\n        \"\"\"Make a clone of this analysis instance.\"\"\"\n", "input": "", "output": "        gta = GTAnalysis(config, **kwargs)\n        gta._roi = copy.deepcopy(self.roi)\n        return gta", "category": "Python"}, {"instruction": "def highlightjs_javascript(jquery=None):\n    \"\"\"\n    Return HTML for highlightjs JavaScript.\n\n    Adjust url in settings. If no url is returned, we don't want this statement to return any HTML.\n    This is intended behavior.\n\n    Default value: ``None``\n\n    This value is configurable, see Settings section\n\n    **Tag name**::\n\n        highlightjs_javascript\n\n    **Parameters**:\n\n        :jquery: Truthy to include jQuery as well as highlightjs\n\n    **usage**::\n\n        {% highlightjs_javascript %}\n\n    **example**::\n\n        {% highlightjs_javascript jquery=1 %}\n    \"\"\"\n", "input": "", "output": "\n    javascript = ''\n    # See if we have to include jQuery\n    if jquery is None:\n        jquery = get_highlightjs_setting('include_jquery', False)\n    if jquery:\n        url = highlightjs_jquery_url()\n        if url:\n            javascript += '<script src=\"{url}\"></script>'.format(url=url)\n    url = highlightjs_url()\n    if url:\n        javascript += '<script src=\"{url}\"></script>'.format(url=url)\n    javascript += '<script>hljs.initHighlightingOnLoad();</script>'\n    return javascript", "category": "Python"}, {"instruction": "def _extract_language(self):\n        \"\"\" Extract language from the HTML ``<head>`` tags. \"\"\"\n", "input": "", "output": "\n        if self.language:\n            return\n\n        found = False\n\n        for pattern in self.config.language:\n            for item in self.parsed_tree.xpath(pattern):\n                stripped_language = item.strip()\n\n                if stripped_language:\n                    self.language = stripped_language\n                    LOGGER.info(u'Language extracted: %s.', stripped_language,\n                                extra={'siteconfig': self.config.host})\n                    found = True\n                    break\n\n            if found:\n                break", "category": "Python"}, {"instruction": "def is_same_vectors(self, vec_set1, vec_set2):\n        \"\"\"\n        Determine if two sets of vectors are the same within length and angle\n        tolerances\n\n        Args:\n            vec_set1(array[array]): an array of two vectors\n            vec_set2(array[array]): second array of two vectors\n        \"\"\"\n", "input": "", "output": "        if (np.absolute(rel_strain(vec_set1[0], vec_set2[0])) >\n                self.max_length_tol):\n            return False\n        elif (np.absolute(rel_strain(vec_set1[1], vec_set2[1])) >\n                  self.max_length_tol):\n            return False\n        elif (np.absolute(rel_angle(vec_set1, vec_set2)) >\n                  self.max_angle_tol):\n            return False\n        else:\n            return True", "category": "Python"}, {"instruction": "def future(self):\n        \"\"\"\n        Return a :class:`asyncio.Future` which has been :meth:`connect`\\\\ -ed\n        using :attr:`AUTO_FUTURE`.\n\n        The token returned by :meth:`connect` is not returned; to remove the\n        future from the signal, just cancel it.\n        \"\"\"\n", "input": "", "output": "        fut = asyncio.Future()\n        self.connect(fut, self.AUTO_FUTURE)\n        return fut", "category": "Python"}, {"instruction": "def get_indent(self, string):\n        \"\"\"\n        Look through the string and count the spaces\n        \"\"\"\n", "input": "", "output": "        indent_amt = 0\n\n        if string[0] == '\\t':\n            return '\\t'\n        for char in string:\n            if char == ' ':\n                indent_amt += 1\n            else:\n                return ' ' * indent_amt", "category": "Python"}, {"instruction": "def is_contains(self, data):\n        \"\"\"\n            Judge the data whether is already exist if each bit of hash code is 1 then data exist.\n        \"\"\"\n", "input": "", "output": "        if not data:\n            return False\n        data = self._compress_by_md5(data)\n        result = True\n        # cut the first two place,route to different block by block_num\n        name = self.key + str(int(data[0:2], 16) % self.block_num)\n        for h in self.hash_function:\n            local_hash = h.hash(data)\n            result = result & self.server.getbit(name, local_hash)\n        return result", "category": "Python"}, {"instruction": "def Close(self):\n    \"\"\"Flushes the flow and all its requests to the data_store.\"\"\"\n", "input": "", "output": "    self._CheckLeaseAndFlush()\n    super(FlowBase, self).Close()\n    # Writing the messages queued in the queue_manager of the runner always has\n    # to be the last thing that happens or we will have a race condition.\n    self.FlushMessages()", "category": "Python"}, {"instruction": "def transit_generate_rand_bytes(self, data_bytes=None, output_format=None, mount_point='transit'):\n        \"\"\"POST /<mount_point>/random(/<data_bytes>)\n\n        :param data_bytes:\n        :type data_bytes:\n        :param output_format:\n        :type output_format:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        if data_bytes is not None:\n            url = '/v1/{0}/random/{1}'.format(mount_point, data_bytes)\n        else:\n            url = '/v1/{0}/random'.format(mount_point)\n\n        params = {}\n        if output_format is not None:\n            params[\"format\"] = output_format\n\n        return self._adapter.post(url, json=params).json()", "category": "Python"}, {"instruction": "def _sel_var(ds, var, upcast_float32=True):\n    \"\"\"Select the specified variable by trying all possible alternative names.\n\n    Parameters\n    ----------\n    ds : Dataset\n        Dataset possibly containing var\n    var : aospy.Var\n        Variable to find data for\n    upcast_float32 : bool (default True)\n        Whether to cast a float32 DataArray up to float64\n\n    Returns\n    -------\n    DataArray\n\n    Raises\n    ------\n    KeyError\n        If the variable is not in the Dataset\n    \"\"\"\n", "input": "", "output": "    for name in var.names:\n        try:\n            da = ds[name].rename(var.name)\n            if upcast_float32:\n                return _maybe_cast_to_float64(da)\n            else:\n                return da\n        except KeyError:\n            pass\n    msg = '{0} not found among names: {1} in\\n{2}'.format(var, var.names, ds)\n    raise LookupError(msg)", "category": "Python"}, {"instruction": "def activate_organizations(self, user):\n        \"\"\"\n        Activates the related organizations for the user.\n\n        It only activates the related organizations by model type - that is, if\n        there are multiple types of organizations then only organizations in\n        the provided model class are activated.\n        \"\"\"\n", "input": "", "output": "        try:\n            relation_name = self.org_model().user_relation_name\n        except TypeError:\n            # No org_model specified, raises a TypeError because NoneType is\n            # not callable. This the most sensible default:\n            relation_name = \"organizations_organization\"\n        organization_set = getattr(user, relation_name)\n        for org in organization_set.filter(is_active=False):\n            org.is_active = True\n            org.save()", "category": "Python"}, {"instruction": "def calculate_rms_means(means, corrected_means):\n    \"\"\"Calculates RMS of means from zero before and after correction\n\n    Parameters\n    ----------\n    means: numpy array of means of gaussians of all PMT combinations\n    corrected_means: numpy array of corrected gaussian means for all PMT combs\n\n    Returns\n    -------\n    rms_means: RMS of means from zero\n    rms_corrected_means: RMS of corrected_means from zero\n    \"\"\"\n", "input": "", "output": "    rms_means = np.sqrt(np.mean((means - 0)**2))\n    rms_corrected_means = np.sqrt(np.mean((corrected_means - 0)**2))\n    return rms_means, rms_corrected_means", "category": "Python"}, {"instruction": "def iterintervals(self, n=2):\n        \"\"\"Iterate over groups of `n` consecutive measurement points in the\n        time series.\n\n        \"\"\"\n", "input": "", "output": "        # tee the original iterator into n identical iterators\n        streams = tee(iter(self), n)\n\n        # advance the \"cursor\" on each iterator by an increasing\n        # offset, e.g. if n=3:\n        #\n        #                   [a, b, c, d, e, f, ..., w, x, y, z]\n        #  first cursor -->  *\n        # second cursor -->     *\n        #  third cursor -->        *\n        for stream_index, stream in enumerate(streams):\n            for i in range(stream_index):\n                next(stream)\n\n        # now, zip the offset streams back together to yield tuples,\n        # in the n=3 example it would yield:\n        # (a, b, c), (b, c, d), ..., (w, x, y), (x, y, z)\n        for intervals in zip(*streams):\n            yield intervals", "category": "Python"}, {"instruction": "def _try_parse_datetime(time_str, fmts):\n    '''\n    A helper function that attempts to parse the input time_str as a date.\n\n    Args:\n\n        time_str (str): A string representing the time\n\n        fmts (list): A list of date format strings\n\n    Returns:\n        datetime: Returns a datetime object if parsed properly, otherwise None\n    '''\n", "input": "", "output": "    result = None\n    for fmt in fmts:\n        try:\n            result = datetime.strptime(time_str, fmt)\n            break\n        except ValueError:\n            pass\n    return result", "category": "Python"}, {"instruction": "def run(name, action):\n    '''\n    Run the specified service with an action.\n\n    .. versionadded:: 2015.8.1\n\n    name\n        Service name.\n\n    action\n        Action name (like start,  stop,  reload,  restart).\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.run apache2 reload\n        salt '*' service.run postgresql initdb\n    '''\n", "input": "", "output": "    cmd = os.path.join(\n        _GRAINMAP.get(__grains__.get('os'), '/etc/init.d'),\n        name\n    ) + ' ' + action\n    return not __salt__['cmd.retcode'](cmd, python_shell=False)", "category": "Python"}, {"instruction": "def template_name(self, path, base):\n        \"\"\"Find out the name of a JS template\"\"\"\n", "input": "", "output": "        if not base:\n            path = os.path.basename(path)\n        if path == base:\n            base = os.path.dirname(path)\n        name = re.sub(r\"^%s[\\/\\\\]?(.*)%s$\" % (\n            re.escape(base), re.escape(settings.TEMPLATE_EXT)\n        ), r\"\\1\", path)\n        return re.sub(r\"[\\/\\\\]\", settings.TEMPLATE_SEPARATOR, name)", "category": "Python"}, {"instruction": "def _get_peer_connection(self, blacklist=None):\n        \"\"\"Find a peer and connect to it.\n\n        Returns a ``(peer, connection)`` tuple.\n\n        Raises ``NoAvailablePeerError`` if no healthy peers are found.\n\n        :param blacklist:\n            If given, a set of hostports for peers that we must not try.\n        \"\"\"\n", "input": "", "output": "\n        blacklist = blacklist or set()\n\n        peer = None\n        connection = None\n\n        while connection is None:\n            peer = self._choose(blacklist)\n\n            if not peer:\n                raise NoAvailablePeerError(\n                    \"Can't find an available peer for '%s'\" % self.service\n                )\n\n            try:\n                connection = yield peer.connect()\n            except NetworkError as e:\n                log.info(\n                    'Failed to connect to %s. Trying a different host.',\n                    peer.hostport,\n                    exc_info=e,\n                )\n                connection = None\n                blacklist.add(peer.hostport)\n\n        raise gen.Return((peer, connection))", "category": "Python"}, {"instruction": "def set_index(self, index):\n        \"\"\"Display the data of the given index\n\n        :param index: the index to paint\n        :type index: QtCore.QModelIndex\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        item = index.internalPointer()\n        note = item.internal_data()\n        self.content_lb.setText(note.content)\n        self.created_dte.setDateTime(dt_to_qdatetime(note.date_created))\n        self.updated_dte.setDateTime(dt_to_qdatetime(note.date_updated))\n        self.username_lb.setText(note.user.username)", "category": "Python"}, {"instruction": "def cmd_output_remove(self, args):\n        '''remove an output'''\n", "input": "", "output": "        device = args[0]\n        for i in range(len(self.mpstate.mav_outputs)):\n            conn = self.mpstate.mav_outputs[i]\n            if str(i) == device or conn.address == device:\n                print(\"Removing output %s\" % conn.address)\n                try:\n                    mp_util.child_fd_list_add(conn.port.fileno())\n                except Exception:\n                    pass\n                conn.close()\n                self.mpstate.mav_outputs.pop(i)\n                return", "category": "Python"}, {"instruction": "def _check_version(self, config):\n        \"\"\"\n        check for a valid version\n        an existing scheme implies an existing version as well\n\n        return True, if version is valid, and False otherwise\n        \"\"\"\n", "input": "", "output": "        try:\n            self.file_version = config.get(\n                escape_for_ini('keyring-setting'),\n                escape_for_ini('version'),\n            )\n        except (configparser.NoSectionError, configparser.NoOptionError):\n            return False\n        return True", "category": "Python"}, {"instruction": "def set_muted(self, status):\n        \"\"\"Set client mute status.\"\"\"\n", "input": "", "output": "        new_volume = self._client['config']['volume']\n        new_volume['muted'] = status\n        self._client['config']['volume']['muted'] = status\n        yield from self._server.client_volume(self.identifier, new_volume)\n        _LOGGER.info('set muted to %s on %s', status, self.friendly_name)", "category": "Python"}, {"instruction": "def should_recover(self):\n        \"\"\"Returns whether the trial qualifies for restoring.\n\n        This is if a checkpoint frequency is set and has not failed more than\n        max_failures. This may return true even when there may not yet\n        be a checkpoint.\n        \"\"\"\n", "input": "", "output": "        return (self.checkpoint_freq > 0\n                and (self.num_failures < self.max_failures\n                     or self.max_failures < 0))", "category": "Python"}, {"instruction": "def calculate_dimensions(self):\n        \"\"\"For a regular grid, calculate the element and node dimensions\n        \"\"\"\n", "input": "", "output": "        x_coordinates = np.sort(self.grid['x'][:, 0])  # first x node\n        self.nr_nodes_z = np.where(x_coordinates == x_coordinates[0])[0].size\n        self.nr_elements_x = self.elements.shape[0] / (self.nr_nodes_z - 1)\n        self.nr_nodes_x = self.nr_elements_x + 1\n        self.nr_elements_z = self.nr_nodes_z - 1", "category": "Python"}, {"instruction": "def distort(value):\n        \"\"\"\n        Distorts a string by randomly replacing characters in it.\n\n        :param value: a string to distort.\n\n        :return: a distored string.\n        \"\"\"\n", "input": "", "output": "        value = value.lower()\n\n        if (RandomBoolean.chance(1, 5)):\n            value = value[0:1].upper() + value[1:]\n\n        if (RandomBoolean.chance(1, 3)):\n            value = value + random.choice(_symbols)\n\n        return value", "category": "Python"}, {"instruction": "def setHeight(self, typeID, height):\n        \"\"\"setHeight(string, double) -> None\n\n        Sets the height in m of vehicles of this type.\n        \"\"\"\n", "input": "", "output": "        self._connection._sendDoubleCmd(\n            tc.CMD_SET_VEHICLETYPE_VARIABLE, tc.VAR_HEIGHT, typeID, height)", "category": "Python"}, {"instruction": "def AddAnalysisReport(self, analysis_report):\n    \"\"\"Adds an analysis report.\n\n    Args:\n      analysis_report (AnalysisReport): analysis report.\n\n    Raises:\n      IOError: when the storage writer is closed.\n      OSError: when the storage writer is closed.\n    \"\"\"\n", "input": "", "output": "    self._RaiseIfNotWritable()\n\n    self._storage_file.AddAnalysisReport(analysis_report)\n\n    report_identifier = analysis_report.plugin_name\n    self._session.analysis_reports_counter['total'] += 1\n    self._session.analysis_reports_counter[report_identifier] += 1\n    self.number_of_analysis_reports += 1", "category": "Python"}, {"instruction": "def send_reply_to(address, reply=EMPTY):\n    \"\"\"Reply to a message previously received\n    \n    :param address: a nw0 address (eg from `nw0.advertise`)\n    :param reply: any simple Python object, including text & tuples\n    \"\"\"\n", "input": "", "output": "    _logger.debug(\"Sending reply %s to %s\", reply, address)\n    return sockets._sockets.send_reply_to(address, reply)", "category": "Python"}, {"instruction": "def getElementsWithAttrValues(self, attrName, attrValues, root='root'):\n        '''\n            getElementsWithAttrValues - Returns elements with an attribute, named by #attrName contains one of the values in the list, #values\n\n            @param attrName <lowercase str> - A lowercase attribute name\n            @param attrValues set<str> - A set of all valid values. \n\n\n            @return - TagCollection of all matching elements\n\n        '''\n", "input": "", "output": "        (root, isFromRoot) = self._handleRootArg(root)\n\n        if type(attrValues) != set:\n            attrValues = set(attrValues)\n        \n        return root.getElementsWithAttrValues(attrName, attrValues)", "category": "Python"}, {"instruction": "def show_faults():\n    \"\"\"\n    Return all valid/active faults ordered by ID to allow the user to pick and choose.\n\n    :return:  List of Tuples where the Tuple elements are:  (fault id, fault template)\n    \"\"\"\n", "input": "", "output": "    cursor = CONN.cursor()\n\n    query = \"select fau_id, fault from surfaults where fau_is_valid = 'y' order by fau_id asc\"\n    cursor.execute(query)\n    result = cursor.fetchall()\n    return result", "category": "Python"}, {"instruction": "def set_transfer_spec(self):\n        ''' run the function to set the transfer spec on error set associated exception '''\n", "input": "", "output": "        _ret = False\n        try:\n            self._args.transfer_spec_func(self._args)\n            _ret = True\n        except Exception as ex:\n            self.notify_exception(AsperaTransferSpecError(ex), False)\n        return _ret", "category": "Python"}, {"instruction": "def console_set_default_background(\n    con: tcod.console.Console, col: Tuple[int, int, int]\n) -> None:\n    \"\"\"Change the default background color for a console.\n\n    Args:\n        con (Console): Any Console instance.\n        col (Union[Tuple[int, int, int], Sequence[int]]):\n            An (r, g, b) sequence or Color instance.\n\n    .. deprecated:: 8.5\n        Use :any:`Console.default_bg` instead.\n    \"\"\"\n", "input": "", "output": "    lib.TCOD_console_set_default_background(_console(con), col)", "category": "Python"}, {"instruction": "def set_weights(self, src_filter, num_taxonomies_by_site):\n        \"\"\"\n        :returns: the weights of the ruptures in the getter\n        \"\"\"\n", "input": "", "output": "        weights = []\n        for rup in self.rup_array:\n            sids = src_filter.close_sids(rup, self.trt, rup['mag'])\n            weights.append(num_taxonomies_by_site[sids].sum())\n        self.weights = numpy.array(weights)\n        self.weight = self.weights.sum()", "category": "Python"}, {"instruction": "def lchown(path, user, group):\n    '''\n    Chown a file, pass the file the desired user and group without following\n    symlinks.\n\n    path\n        path to the file or directory\n\n    user\n        user owner\n\n    group\n        group owner\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.chown /etc/passwd root root\n    '''\n", "input": "", "output": "    path = os.path.expanduser(path)\n\n    uid = user_to_uid(user)\n    gid = group_to_gid(group)\n    err = ''\n    if uid == '':\n        if user:\n            err += 'User does not exist\\n'\n        else:\n            uid = -1\n    if gid == '':\n        if group:\n            err += 'Group does not exist\\n'\n        else:\n            gid = -1\n\n    return os.lchown(path, uid, gid)", "category": "Python"}, {"instruction": "def rd_files(self, study_fn, pop_fn):\n        \"\"\"Read files and return study and population.\"\"\"\n", "input": "", "output": "        study, pop = self._read_geneset(study_fn, pop_fn)\n        print(\"Study: {0} vs. Population {1}\\n\".format(len(study), len(pop)))\n        return study, pop", "category": "Python"}, {"instruction": "def _exec_callers(xinst, result):\n    \"\"\"Adds the dependency calls from the specified executable instance\n    to the results dictionary.\n    \"\"\"\n", "input": "", "output": "    for depkey, depval in xinst.dependencies.items():\n        if depval.target is not None:\n            if depval.target.name in result:\n                if xinst not in result[depval.target.name]:\n                    result[depval.target.name].append(xinst)\n            else:\n                result[depval.target.name] = [xinst]\n\n    for xname, xvalue in xinst.executables:\n        _exec_callers(xvalue, result)", "category": "Python"}, {"instruction": "def licenses(\n        self, token: dict = None, owner_id: str = None, prot: str = \"https\"\n    ) -> dict:\n        \"\"\"Get information about licenses owned by a specific workgroup.\n\n        :param str token: API auth token\n        :param str owner_id: workgroup UUID\n        :param str prot: https [DEFAULT] or http\n         (use it only for dev and tracking needs).\n        \"\"\"\n", "input": "", "output": "        # handling request parameters\n        payload = {\"gid\": owner_id}\n\n        # search request\n        licenses_url = \"{}://v1.{}.isogeo.com/groups/{}/licenses\".format(\n            prot, self.api_url, owner_id\n        )\n        licenses_req = self.get(\n            licenses_url,\n            headers=self.header,\n            params=payload,\n            proxies=self.proxies,\n            verify=self.ssl,\n        )\n\n        # checking response\n        req_check = checker.check_api_response(licenses_req)\n        if isinstance(req_check, tuple):\n            return req_check\n\n        # end of method\n        return licenses_req.json()", "category": "Python"}, {"instruction": "def dependencies(self, path: str) -> Set[str]:\n        \"\"\"Returns a set of all classes referenced by the ClassFile at\n        `path` without reading the entire ClassFile.\n\n        This is an optimization method that does not load a complete ClassFile,\n        nor does it add the results to the ClassLoader cache.\n\n        :param path: Fully-qualified path to a ClassFile.\n        \"\"\"\n", "input": "", "output": "        return set(c.name.value for c in self.search_constant_pool(\n            path=path,\n            type_=ConstantClass\n        ))", "category": "Python"}, {"instruction": "def isotopePattern(sum_formula, threshold=1e-4, rel_threshold=True, desired_prob=None):\n    \"\"\"\n    Calculates isotopic peaks for a sum formula.\n\n    :param sum_formula: text representation of an atomic composition\n    :type sum_formula: str\n    :param threshold: minimum peak abundance\n    :type threshold: float\n    :param rel_threshold: if True, threshold is relative to the highest peak, otherwise it is a probability\n    :type rel_threshold: bool\n    :param desired_prob: total probability covered by the result; if set, threshold parameter is ignored\n    :type desired_prob: float | None\n    \"\"\"\n", "input": "", "output": "    assert threshold >= 0 and threshold < 1\n    assert desired_prob is None or (desired_prob > 0 and desired_prob <= 1)\n\n    if desired_prob:\n        s = ims.spectrum_new_from_sf(sum_formula.encode('ascii'), desired_prob)\n    else:\n        s = ims.spectrum_new_from_sf_thr(sum_formula.encode('ascii'), threshold, rel_threshold)\n    return _new_spectrum(TheoreticalSpectrum, s)", "category": "Python"}, {"instruction": "def from_node(index, data, modify_index=None):\n        \"\"\"\n        >>> ClusterConfig.from_node(1, '{') is None\n        False\n        \"\"\"\n", "input": "", "output": "\n        try:\n            data = json.loads(data)\n        except (TypeError, ValueError):\n            data = None\n            modify_index = 0\n        if not isinstance(data, dict):\n            data = {}\n        return ClusterConfig(index, data, index if modify_index is None else modify_index)", "category": "Python"}, {"instruction": "def _get_channel(self):\n        \"\"\"Returns a channel according to if there is a redirection to do or\n        not.\n        \"\"\"\n", "input": "", "output": "        channel = self._transport.open_session()\n        channel.set_combine_stderr(True)\n        channel.get_pty()\n        return channel", "category": "Python"}, {"instruction": "def finalize(self, **kwargs):\n        \"\"\"\n        Finalize executes any subclass-specific axes finalization steps.\n        The user calls poof and poof calls finalize.\n\n        Parameters\n        ----------\n        kwargs: dict\n            generic keyword arguments\n\n        \"\"\"\n", "input": "", "output": "        # Set the title\n        self.set_title(\n            \"{} Ranking of {} Features\".format(\n                self.ranking_.title(), len(self.features_)\n            )\n        )", "category": "Python"}, {"instruction": "def get_pubkey_hex( privatekey_hex ):\n    \"\"\"\n    Get the uncompressed hex form of a private key\n    \"\"\"\n", "input": "", "output": "    if not isinstance(privatekey_hex, (str, unicode)):\n        raise ValueError(\"private key is not a hex string but {}\".format(str(type(privatekey_hex))))\n\n    # remove 'compressed' hint\n    if len(privatekey_hex) > 64:\n        if privatekey_hex[-2:] != '01':\n            raise ValueError(\"private key does not end in 01\")\n\n        privatekey_hex = privatekey_hex[:64]\n\n    # get hex public key\n    privatekey_int = int(privatekey_hex, 16)\n    privk = ec.derive_private_key(privatekey_int, ec.SECP256K1(), default_backend())\n    pubk = privk.public_key()\n    x = pubk.public_numbers().x\n    y = pubk.public_numbers().y\n\n    pubkey_hex = \"04{:064x}{:064x}\".format(x, y)\n    return pubkey_hex", "category": "Python"}, {"instruction": "def delete_channel_cb(self, viewer, channel):\n        \"\"\"Called when a channel is deleted from the main interface.\n        Parameter is a channel (a Channel object).\"\"\"\n", "input": "", "output": "        chname = channel.name\n        del self.name_dict[chname]\n\n        # Unhighlight\n        un_hilite_set = set([])\n        for path in self._hl_path:\n            if path[0] == chname:\n                un_hilite_set.add(path)\n        self._hl_path -= un_hilite_set\n\n        if self.gui_up:\n            self.recreate_toc()\n\n        self._rebuild_channels()", "category": "Python"}, {"instruction": "def rename(store, src_path, dst_path):\n    \"\"\"Rename all items under the given path. If `store` provides a `rename` method,\n    this will be called, otherwise will fall back to implementation via the\n    `MutableMapping` interface.\"\"\"\n", "input": "", "output": "    src_path = normalize_storage_path(src_path)\n    dst_path = normalize_storage_path(dst_path)\n    if hasattr(store, 'rename'):\n        # pass through\n        store.rename(src_path, dst_path)\n    else:\n        # slow version, delete one key at a time\n        _rename_from_keys(store, src_path, dst_path)", "category": "Python"}, {"instruction": "def create_environment(self, environment):\n        \"\"\"\n        Method to create environment\n        \"\"\"\n", "input": "", "output": "\n        uri = 'api/v3/environment/'\n\n        data = dict()\n        data['environments'] = list()\n        data['environments'].append(environment)\n\n        return super(ApiEnvironment, self).post(uri, data)", "category": "Python"}, {"instruction": "def disassemble_around(self, lpAddress, dwSize = 64):\n        \"\"\"\n        Disassemble around the given address.\n\n        @type  lpAddress: int\n        @param lpAddress: Memory address where to read the code from.\n\n        @type  dwSize: int\n        @param dwSize: Delta offset.\n            Code will be read from lpAddress - dwSize to lpAddress + dwSize.\n\n        @rtype:  list of tuple( long, int, str, str )\n        @return: List of tuples. Each tuple represents an assembly instruction\n            and contains:\n             - Memory address of instruction.\n             - Size of instruction in bytes.\n             - Disassembly line of instruction.\n             - Hexadecimal dump of instruction.\n        \"\"\"\n", "input": "", "output": "        aProcess = self.get_process()\n        return aProcess.disassemble_around(lpAddress, dwSize)", "category": "Python"}, {"instruction": "def print_csm_info(fname):\n    \"\"\"\n    Parse the composite source model without instantiating the sources and\n    prints information about its composition and the full logic tree\n    \"\"\"\n", "input": "", "output": "    oqparam = readinput.get_oqparam(fname)\n    csm = readinput.get_composite_source_model(oqparam, in_memory=False)\n    print(csm.info)\n    print('See http://docs.openquake.org/oq-engine/stable/'\n          'effective-realizations.html for an explanation')\n    rlzs_assoc = csm.info.get_rlzs_assoc()\n    print(rlzs_assoc)\n    dupl = [(srcs[0]['id'], len(srcs)) for srcs in csm.check_dupl_sources()]\n    if dupl:\n        print(rst_table(dupl, ['source_id', 'multiplicity']))\n    tot, pairs = get_pickled_sizes(rlzs_assoc)\n    print(rst_table(pairs, ['attribute', 'nbytes']))", "category": "Python"}, {"instruction": "def make(self):\n        \"\"\"\n        Creates this directory and any of the missing directories in the path.\n        Any errors that may occur are eaten.\n        \"\"\"\n", "input": "", "output": "        try:\n            if not self.exists:\n                logger.info(\"Creating %s\" % self.path)\n                os.makedirs(self.path)\n        except os.error:\n            pass\n        return self", "category": "Python"}, {"instruction": "def has_property(obj, name):\n        \"\"\"\n        Checks if object has a property with specified name.\n\n        :param obj: an object to introspect.\n\n        :param name: a name of the property to check.\n\n        :return: true if the object has the property and false if it doesn't.\n        \"\"\"\n", "input": "", "output": "        if obj == None:\n            raise Exception(\"Object cannot be null\")\n        if name == None:\n            raise Exception(\"Property name cannot be null\")\n\n        name = name.lower()\n\n        for property_name in dir(obj): \n            if property_name.lower() != name:\n                continue\n\n            property = getattr(obj, property_name)\n\n            if PropertyReflector._is_property(property, property_name):\n                return True\n        \n        return False", "category": "Python"}, {"instruction": "def with_pattern(pattern, regex_group_count=None):\n    \"\"\"Attach a regular expression pattern matcher to a custom type converter\n    function.\n\n    This annotates the type converter with the :attr:`pattern` attribute.\n\n    EXAMPLE:\n        >>> import parse\n        >>> @parse.with_pattern(r\"\\d+\")\n        ... def parse_number(text):\n        ...     return int(text)\n\n    is equivalent to:\n\n        >>> def parse_number(text):\n        ...     return int(text)\n        >>> parse_number.pattern = r\"\\d+\"\n\n    :param pattern: regular expression pattern (as text)\n    :param regex_group_count: Indicates how many regex-groups are in pattern.\n    :return: wrapped function\n    \"\"\"\n", "input": "", "output": "    def decorator(func):\n        func.pattern = pattern\n        func.regex_group_count = regex_group_count\n        return func\n    return decorator", "category": "Python"}, {"instruction": "def strip_praw_subscription(subscription):\n        \"\"\"\n        Parse through a subscription and return a dict with data ready to be\n        displayed through the terminal.\n        \"\"\"\n", "input": "", "output": "\n        data = {}\n        data['object'] = subscription\n        if isinstance(subscription, praw.objects.Multireddit):\n            data['type'] = 'Multireddit'\n            data['name'] = subscription.path\n            data['title'] = subscription.description_md\n        else:\n            data['type'] = 'Subscription'\n            data['name'] = \"/r/\" + subscription.display_name\n            data['title'] = subscription.title\n\n        return data", "category": "Python"}, {"instruction": "def validate_required_attributes(fully_qualified_name: str, spec: Dict[str, Any],\n                                 *attributes: str) -> List[RequiredAttributeError]:\n    \"\"\" Validates to ensure that a set of attributes are present in spec \"\"\"\n", "input": "", "output": "    return [\n        RequiredAttributeError(fully_qualified_name, spec, attribute)\n        for attribute in attributes\n        if attribute not in spec\n    ]", "category": "Python"}, {"instruction": "def add_root_log_file(log_file):\n    \"\"\"\n    Add a log file to the root logger.\n\n    Parameters\n    ----------\n    log_file :obj:`str`\n        The path to the log file.\n    \"\"\"\n", "input": "", "output": "    root_logger = logging.getLogger()\n\n    # add a file handle to the root logger\n    hdlr = logging.FileHandler(log_file)\n    formatter = logging.Formatter('%(asctime)s %(name)-10s %(levelname)-8s %(message)s', datefmt='%m-%d %H:%M:%S')\n    hdlr.setFormatter(formatter)\n    root_logger.addHandler(hdlr)\n    root_logger.info('Root logger now logging to {}'.format(log_file))", "category": "Python"}, {"instruction": "def clusterQueues(self):\n        \"\"\" Return a dict of queues in cluster and servers running them\n        \"\"\"\n", "input": "", "output": "        servers = yield self.getClusterServers()\n\n        queues = {}\n\n        for sname in servers:\n            qs = yield self.get('rhumba.server.%s.queues' % sname)\n            uuid = yield self.get('rhumba.server.%s.uuid' % sname)\n       \n            qs = json.loads(qs)\n\n            for q in qs:\n                if q not in queues:\n                    queues[q] = []\n\n                queues[q].append({'host': sname, 'uuid': uuid})\n\n        defer.returnValue(queues)", "category": "Python"}, {"instruction": "def merge_values(values1,values2):\n    '''\n    Merges two numpy arrays by calculating all possible combinations of rows\n    '''\n", "input": "", "output": "    array1 = values_to_array(values1)\n    array2 = values_to_array(values2)\n\n    if array1.size == 0:\n        return array2\n    if array2.size == 0:\n        return array1\n\n    merged_array = []\n    for row_array1 in array1:\n        for row_array2 in array2:\n            merged_row = np.hstack((row_array1,row_array2))\n            merged_array.append(merged_row)\n    return np.atleast_2d(merged_array)", "category": "Python"}, {"instruction": "def gdate(self):\n        \"\"\"Return the Gregorian date for the given Hebrew date object.\"\"\"\n", "input": "", "output": "        if self._last_updated == \"gdate\":\n            return self._gdate\n        return conv.jdn_to_gdate(self._jdn)", "category": "Python"}, {"instruction": "def followers(self, actor, flag=''):\n        \"\"\"\n        Returns a list of User objects who are following the given actor (eg my followers).\n        \"\"\"\n", "input": "", "output": "        return [follow.user for follow in self.followers_qs(actor, flag=flag)]", "category": "Python"}, {"instruction": "def report(issues, show_urls=False):\n    \"\"\"Summary report about a list of issues, printing number and title.\n    \"\"\"\n", "input": "", "output": "    # titles may have unicode in them, so we must encode everything below\n    if show_urls:\n        for i in issues:\n            print(u'#%d: %s' % (i['number'],\n                                      i['title'].replace(u'`', u'``')))\n    else:\n        for i in issues:\n            print(u'* %d: %s' % (i['number'], i['title'].replace(u'`', u'``')))", "category": "Python"}, {"instruction": "def mass_fraction_within_radius(self, kwargs_lens, center_x, center_y, theta_E, numPix=100):\n        \"\"\"\n        computes the mean convergence of all the different lens model components within a spherical aperture\n\n        :param kwargs_lens: lens model keyword argument list\n        :param center_x: center of the aperture\n        :param center_y: center of the aperture\n        :param theta_E: radius of aperture\n        :return: list of average convergences for all the model components\n        \"\"\"\n", "input": "", "output": "        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=2.*theta_E / numPix)\n        x_grid += center_x\n        y_grid += center_y\n        mask = mask_util.mask_sphere(x_grid, y_grid, center_x, center_y, theta_E)\n        kappa_list = []\n        for i in range(len(kwargs_lens)):\n            kappa = self.LensModel.kappa(x_grid, y_grid, kwargs_lens, k=i)\n            kappa_mean = np.sum(kappa * mask) / np.sum(mask)\n            kappa_list.append(kappa_mean)\n        return kappa_list", "category": "Python"}, {"instruction": "def scheduleServices(self, jobGraph):\n        \"\"\"\n        Schedule the services of a job asynchronously.\n        When the job's services are running the jobGraph for the job will\n        be returned by toil.leader.ServiceManager.getJobGraphsWhoseServicesAreRunning.\n\n        :param toil.jobGraph.JobGraph jobGraph: wrapper of job with services to schedule.\n        \"\"\"\n", "input": "", "output": "        # Add jobGraph to set being processed by the service manager\n        self.jobGraphsWithServicesBeingStarted.add(jobGraph)\n\n        # Add number of jobs managed by ServiceManager\n        self.jobsIssuedToServiceManager += sum(map(len, jobGraph.services)) + 1 # The plus one accounts for the root job\n\n        # Asynchronously schedule the services\n        self._jobGraphsWithServicesToStart.put(jobGraph)", "category": "Python"}, {"instruction": "def _box_click(self, event):\n        \"\"\"Check or uncheck box when clicked.\"\"\"\n", "input": "", "output": "        x, y, widget = event.x, event.y, event.widget\n        elem = widget.identify(\"element\", x, y)\n        if \"image\" in elem:\n            # a box was clicked\n            item = self.identify_row(y)\n            if self.tag_has(\"unchecked\", item) or self.tag_has(\"tristate\", item):\n                self._check_ancestor(item)\n                self._check_descendant(item)\n            else:\n                self._uncheck_descendant(item)\n                self._uncheck_ancestor(item)", "category": "Python"}, {"instruction": "def share_application_with_accounts(application_id, account_ids, sar_client=None):\n    \"\"\"\n    Share the application privately with given AWS account IDs.\n\n    :param application_id: The Amazon Resource Name (ARN) of the application\n    :type application_id: str\n    :param account_ids: List of AWS account IDs, or *\n    :type account_ids: list of str\n    :param sar_client: The boto3 client used to access SAR\n    :type sar_client: boto3.client\n    :raises ValueError\n    \"\"\"\n", "input": "", "output": "    if not application_id or not account_ids:\n        raise ValueError('Require application id and list of AWS account IDs to share the app')\n\n    if not sar_client:\n        sar_client = boto3.client('serverlessrepo')\n\n    application_policy = ApplicationPolicy(account_ids, [ApplicationPolicy.DEPLOY])\n    application_policy.validate()\n    sar_client.put_application_policy(\n        ApplicationId=application_id,\n        Statements=[application_policy.to_statement()]\n    )", "category": "Python"}, {"instruction": "def user_field(user, field, *args):\n    \"\"\"\n    Gets or sets (optional) user model fields. No-op if fields do not exist.\n    \"\"\"\n", "input": "", "output": "    if not field:\n        return\n    User = get_user_model()\n    try:\n        field_meta = User._meta.get_field(field)\n        max_length = field_meta.max_length\n    except FieldDoesNotExist:\n        if not hasattr(user, field):\n            return\n        max_length = None\n    if args:\n        # Setter\n        v = args[0]\n        if v:\n            v = v[0:max_length]\n        setattr(user, field, v)\n    else:\n        # Getter\n        return getattr(user, field)", "category": "Python"}, {"instruction": "def collect_results(project, force=False):\n    \"\"\"collect_results.\"\"\"\n", "input": "", "output": "    if not project.crawlable:\n        return project\n\n    now = datetime.datetime.now()\n\n    if (now - project.updated_at).total_seconds() < 4 and (not force):\n        return project\n\n    result_paths = []\n    if os.path.isdir(project.path_name):\n        result_paths.extend(_list_result_paths(project.path_name))\n\n    registered_results = db.session.query(Result.path_name).filter_by(\n        project_id=project.id\n    ).all()\n    registered_paths = {r.path_name for r in registered_results}\n\n    for result_path in result_paths:\n        if result_path not in registered_paths:\n            _register_result(project.id, result_path)\n\n    project.updated_at = datetime.datetime.now()\n\n    db.session.commit()\n\n    return project", "category": "Python"}, {"instruction": "def _format_operation(operation, parameters=None):\n    \"\"\"Formats parameters in operation in way BigQuery expects.\n\n    :type: str\n    :param operation: A Google BigQuery query string.\n\n    :type: Mapping[str, Any] or Sequence[Any]\n    :param parameters: Optional parameter values.\n\n    :rtype: str\n    :returns: A formatted query string.\n    :raises: :class:`~google.cloud.bigquery.dbapi.ProgrammingError`\n        if a parameter used in the operation is not found in the\n        ``parameters`` argument.\n    \"\"\"\n", "input": "", "output": "    if parameters is None:\n        return operation\n\n    if isinstance(parameters, collections_abc.Mapping):\n        return _format_operation_dict(operation, parameters)\n\n    return _format_operation_list(operation, parameters)", "category": "Python"}, {"instruction": "def cmd_func(self, command: str) -> Optional[Callable]:\n        \"\"\"\n        Get the function for a command\n        :param command: the name of the command\n        \"\"\"\n", "input": "", "output": "        func_name = self.cmd_func_name(command)\n        if func_name:\n            return getattr(self, func_name)", "category": "Python"}, {"instruction": "def save_model(self, request, obj, form, change):\n        \"\"\"\n        Surclasse la m\u00e9thode de sauvegarde de l'admin du mod\u00e8le pour y \n        rajouter automatiquement l'auteur qui cr\u00e9\u00e9 l'objet\n        \"\"\"\n", "input": "", "output": "        instance = form.save(commit=False)\n        if not(instance.created):\n            instance.author = request.user\n        instance.save()\n        form.save_m2m()\n\n        return instance", "category": "Python"}, {"instruction": "def get_bip32_address(self, ecdh=False):\n        \"\"\"Compute BIP32 derivation address according to SLIP-0013/0017.\"\"\"\n", "input": "", "output": "        index = struct.pack('<L', self.identity_dict.get('index', 0))\n        addr = index + self.to_bytes()\n        log.debug('bip32 address string: %r', addr)\n        digest = hashlib.sha256(addr).digest()\n        s = io.BytesIO(bytearray(digest))\n\n        hardened = 0x80000000\n        addr_0 = 17 if bool(ecdh) else 13\n        address_n = [addr_0] + list(util.recv(s, '<LLLL'))\n        return [(hardened | value) for value in address_n]", "category": "Python"}, {"instruction": "def _remove_sequences_from_alignment(self, sequence_names, input_alignment_file, output_alignment_file):\n        '''Remove sequences from the alignment file that have names in\n        sequence_names\n\n        Parameters\n        ----------\n        sequence_names: list of str\n            names of sequences to remove\n        input_alignment_file: str\n            path to alignment file to remove from\n        output_alignment_file: str\n            path to alignment file to write to\n\n        Returns\n        -------\n        int: number of sequences written to file'''\n", "input": "", "output": "        nameset = set(sequence_names)\n        num_written = 0\n        with open(output_alignment_file, 'w') as f:\n            for s in SeqIO.parse(open(input_alignment_file), \"fasta\"):\n                if s.name not in nameset:\n                    SeqIO.write(s, f, \"fasta\")\n                    num_written += 1\n        logging.debug(\"After removing sequences from alignment, %i remain\" % num_written)\n        return num_written", "category": "Python"}, {"instruction": "def get_parser(self, prog_name, subcommand):\n        \"\"\"\n        Returns parser for given ``prog_name`` and ``subcommand``.\n\n        :param prog_name: vcs main script name\n        :param subcommand: command name\n        \"\"\"\n", "input": "", "output": "        parser = OptionParser(\n            prog=prog_name,\n            usage=self.usage(subcommand),\n            version=self.get_version(),\n            option_list=sorted(self.get_option_list()))\n        return parser", "category": "Python"}, {"instruction": "def get(self, cid1, cid2, annotator_id):\n        '''Retrieve a relation label from the store.\n        '''\n", "input": "", "output": "        t = (cid1, cid2, annotator_id)\n        for k, v in self.kvl.scan(self.TABLE, (t, t)):\n            return self._label_from_kvlayer(k, v)", "category": "Python"}, {"instruction": "def start_redis(self):\n        \"\"\"Start the Redis servers.\"\"\"\n", "input": "", "output": "        assert self._redis_address is None\n        redis_log_files = [self.new_log_files(\"redis\")]\n        for i in range(self._ray_params.num_redis_shards):\n            redis_log_files.append(self.new_log_files(\"redis-shard_\" + str(i)))\n\n        (self._redis_address, redis_shards,\n         process_infos) = ray.services.start_redis(\n             self._node_ip_address,\n             redis_log_files,\n             port=self._ray_params.redis_port,\n             redis_shard_ports=self._ray_params.redis_shard_ports,\n             num_redis_shards=self._ray_params.num_redis_shards,\n             redis_max_clients=self._ray_params.redis_max_clients,\n             redirect_worker_output=True,\n             password=self._ray_params.redis_password,\n             include_java=self._ray_params.include_java,\n             redis_max_memory=self._ray_params.redis_max_memory)\n        assert (\n            ray_constants.PROCESS_TYPE_REDIS_SERVER not in self.all_processes)\n        self.all_processes[ray_constants.PROCESS_TYPE_REDIS_SERVER] = (\n            process_infos)", "category": "Python"}, {"instruction": "def ssh_interface(vm_):\n    '''\n    Return the ssh_interface type to connect to. Either 'public_ips' (default)\n    or 'private_ips'.\n    '''\n", "input": "", "output": "    ret = config.get_cloud_config_value(\n        'ssh_interface', vm_, __opts__, default='public_ips',\n        search_global=False\n    )\n    if ret not in ('public_ips', 'private_ips'):\n        log.warning(\n            'Invalid ssh_interface: %s. '\n            'Allowed options are (\"public_ips\", \"private_ips\"). '\n            'Defaulting to \"public_ips\".', ret\n        )\n        ret = 'public_ips'\n    return ret", "category": "Python"}, {"instruction": "def get_or_create_config(path, config):\n    \"\"\"Using TOML format, load config from given path, or write out example based on defaults\"\"\"\n", "input": "", "output": "    if os.path.isfile(path):\n        with open(path) as fh:\n            _LOG.debug(\"loading config from %s\", os.path.abspath(path))\n            config._inflate(toml.load(fh))\n    else:\n        try:\n            os.makedirs(os.path.dirname(path))\n        except OSError:\n            pass\n        with open(path, \"w\") as fh:\n            toml.dump(config._deflate(), fh)", "category": "Python"}, {"instruction": "def _flatten_plus_safe(data_and_files):\n    \"\"\"Flatten names of files and create temporary file names.\n    \"\"\"\n", "input": "", "output": "    data, rollback_files = _normalize_args(data_and_files)\n    with tx_tmpdir(data) as tmpdir:\n        tx_files = [os.path.join(tmpdir, os.path.basename(f))\n                    for f in rollback_files]\n        yield tx_files, rollback_files", "category": "Python"}, {"instruction": "def unit_tangent(self, t=None):\n        \"\"\"returns the unit tangent of the segment at t.\"\"\"\n", "input": "", "output": "        assert self.end != self.start\n        dseg = self.end - self.start\n        return dseg/abs(dseg)", "category": "Python"}, {"instruction": "def number_of_records_per_hour(self, value=None):\n        \"\"\"Corresponds to IDD Field `number_of_records_per_hour`\n\n        Args:\n            value (int): value for IDD Field `number_of_records_per_hour`\n                if `value` is None it will not be checked against the\n                specification and is assumed to be a missing value\n\n        Raises:\n            ValueError: if `value` is not a valid value\n\n        \"\"\"\n", "input": "", "output": "        if value is not None:\n            try:\n                value = int(value)\n            except ValueError:\n                raise ValueError(\n                    'value {} need to be of type int '\n                    'for field `number_of_records_per_hour`'.format(value))\n\n        self._number_of_records_per_hour = value", "category": "Python"}, {"instruction": "def _get_key_file_path():\n        \"\"\"Return the key file path.\"\"\"\n", "input": "", "output": "        if os.getenv(USER_HOME) is not None and os.access(os.getenv(USER_HOME),\n                                                          os.W_OK):\n            return os.path.join(os.getenv(USER_HOME), KEY_FILE_NAME)\n\n        return os.path.join(os.getcwd(), KEY_FILE_NAME)", "category": "Python"}, {"instruction": "def run(self, background=False):\n        \"\"\"Runs `on_startup`, `main` and `on_shutdown`, blocking until finished, unless background is set.\"\"\"\n", "input": "", "output": "        if self.__bgthread:\n            raise Exception('run has already been called (since last stop)')\n        self.__shutdown.clear()\n        if background:\n            self.__bgthread = Thread(target=self.__run, name=('bg_' + self.__client.agent_id))\n            self.__bgthread.daemon = True\n            self.__bgthread.start()\n        else:\n            self.__run()", "category": "Python"}, {"instruction": "def spp_call_peaks(\n                self, treatment_bam, control_bam, treatment_name, control_name,\n                output_dir, broad, cpus, qvalue=None):\n        \"\"\"\n        Build command for R script to call peaks with SPP.\n\n        :param str treatment_bam: Path to file with data for treatment sample.\n        :param str control_bam: Path to file with data for control sample.\n        :param str treatment_name: Name for the treatment sample.\n        :param str control_name: Name for the control sample.\n        :param str output_dir: Path to folder for output.\n        :param str | bool broad: Whether to specify broad peak calling mode.\n        :param int cpus: Number of cores the script may use.\n        :param float qvalue: FDR, as decimal value\n        :return str: Command to run.\n        \"\"\"\n", "input": "", "output": "        broad = \"TRUE\" if broad else \"FALSE\"\n        cmd = self.tools.Rscript + \" `which spp_peak_calling.R` {0} {1} {2} {3} {4} {5} {6}\".format(\n            treatment_bam, control_bam, treatment_name, control_name, broad, cpus, output_dir\n        )\n        if qvalue is not None:\n            cmd += \" {}\".format(qvalue)\n        return cmd", "category": "Python"}, {"instruction": "def html(self) -> _BaseHTML:\n        \"\"\"Unicode representation of the HTML content\n        (`learn more <http://www.diveintopython3.net/strings.html>`_).\n        \"\"\"\n", "input": "", "output": "        if self._html:\n            return self.raw_html.decode(self.encoding, errors='replace')\n        else:\n            return etree.tostring(self.element, encoding='unicode').strip()", "category": "Python"}, {"instruction": "def delta_to_str(rd):\n    \"\"\" Convert a relativedelta to a human-readable string \"\"\"\n", "input": "", "output": "    parts = []\n    if rd.days > 0:\n        parts.append(\"%d day%s\" % (rd.days, plural(rd.days)))\n    clock_parts = []\n    if rd.hours > 0:\n        clock_parts.append(\"%02d\" % rd.hours)\n    if rd.minutes > 0 or rd.hours > 0:\n        clock_parts.append(\"%02d\" % rd.minutes)\n    if rd.seconds > 0 or rd.minutes > 0 or rd.hours > 0:\n        clock_parts.append(\"%02d\" % rd.seconds)\n    if clock_parts:\n        parts.append(\":\".join(clock_parts))\n    return \" \".join(parts)", "category": "Python"}, {"instruction": "def _closest_centroid(self, x):\n        \"\"\"Returns the index of the closest centroid to the sample\n        \"\"\"\n", "input": "", "output": "        closest_centroid = 0\n        distance = 10^9\n\n        for i in range(self.n_clusters):\n            current_distance = linalg.norm(x - self.centroids[i])\n            if current_distance < distance:\n                closest_centroid = i\n                distance = current_distance\n\n        return closest_centroid", "category": "Python"}, {"instruction": "def get_package_from_uri(self, manifest_uri: URI) -> Package:\n        \"\"\"\n        Returns a `Package <https://github.com/ethpm/py-ethpm/blob/master/ethpm/package.py>`__\n        instance built with the Manifest stored at the URI.\n        If you want to use a specific IPFS backend, set ``ETHPM_IPFS_BACKEND_CLASS``\n        to your desired backend. Defaults to Infura IPFS backend.\n\n        * Parameters:\n            * ``uri``: Must be a valid content-addressed URI\n        \"\"\"\n", "input": "", "output": "        return Package.from_uri(manifest_uri, self.web3)", "category": "Python"}, {"instruction": "def get_subperiods(self, unit):\n        \"\"\"\n            Return the list of all the periods of unit ``unit`` contained in self.\n\n            Examples:\n\n            >>> period('2017').get_subperiods(MONTH)\n            >>> [period('2017-01'), period('2017-02'), ... period('2017-12')]\n\n            >>> period('year:2014:2').get_subperiods(YEAR)\n            >>> [period('2014'), period('2015')]\n        \"\"\"\n", "input": "", "output": "        if unit_weight(self.unit) < unit_weight(unit):\n            raise ValueError('Cannot subdivide {0} into {1}'.format(self.unit, unit))\n\n        if unit == YEAR:\n            return [self.this_year.offset(i, YEAR) for i in range(self.size)]\n\n        if unit == MONTH:\n            return [self.first_month.offset(i, MONTH) for i in range(self.size_in_months)]\n\n        if unit == DAY:\n            return [self.first_day.offset(i, DAY) for i in range(self.size_in_days)]", "category": "Python"}, {"instruction": "def addDenylistAddress(self, *args, **kwargs):\n        \"\"\"\n        Denylist Given Address\n\n        Add the given address to the notification denylist. The address\n        can be of either of the three supported address type namely pulse, email\n        or IRC(user or channel). Addresses in the denylist will be ignored\n        by the notification service.\n\n        This method takes input: ``v1/notification-address.json#``\n\n        This method is ``experimental``\n        \"\"\"\n", "input": "", "output": "\n        return self._makeApiCall(self.funcinfo[\"addDenylistAddress\"], *args, **kwargs)", "category": "Python"}, {"instruction": "def morphs(self, name, index_name=None):\n        \"\"\"\n        Add the proper columns for a polymorphic table.\n\n        :type name: str\n\n        :type index_name: str\n        \"\"\"\n", "input": "", "output": "        self.unsigned_integer(\"%s_id\" % name)\n        self.string(\"%s_type\" % name)\n        self.index([\"%s_id\" % name, \"%s_type\" % name], index_name)", "category": "Python"}, {"instruction": "def visit_extslice(self, node, parent):\n        \"\"\"visit an ExtSlice node by returning a fresh instance of it\"\"\"\n", "input": "", "output": "        newnode = nodes.ExtSlice(parent=parent)\n        newnode.postinit([self.visit(dim, newnode) for dim in node.dims])\n        return newnode", "category": "Python"}, {"instruction": "def parse_date(value: Union[date, StrIntFloat]) -> date:\n    \"\"\"\n    Parse a date/int/float/string and return a datetime.date.\n\n    Raise ValueError if the input is well formatted but not a valid date.\n    Raise ValueError if the input isn't well formatted.\n    \"\"\"\n", "input": "", "output": "    if isinstance(value, date):\n        if isinstance(value, datetime):\n            return value.date()\n        else:\n            return value\n\n    number = get_numeric(value)\n    if number is not None:\n        return from_unix_seconds(number).date()\n\n    match = date_re.match(cast(str, value))\n    if not match:\n        raise errors.DateError()\n\n    kw = {k: int(v) for k, v in match.groupdict().items()}\n\n    with change_exception(errors.DateError, ValueError):\n        return date(**kw)", "category": "Python"}, {"instruction": "def is_possible_number_for_type(numobj, numtype):\n    \"\"\"Convenience wrapper around is_possible_number_for_type_with_reason.\n\n    Instead of returning the reason for failure, this method returns true if\n    the number is either a possible fully-qualified number (containing the area\n    code and country code), or if the number could be a possible local number\n    (with a country code, but missing an area code). Local numbers are\n    considered possible if they could be possibly dialled in this format: if\n    the area code is needed for a call to connect, the number is not considered\n    possible without it.\n\n    Arguments:\n    numobj -- the number object that needs to be checked\n    numtype -- the type we are interested in\n\n    Returns True if the number is possible\n\n    \"\"\"\n", "input": "", "output": "    result = is_possible_number_for_type_with_reason(numobj, numtype)\n    return (result == ValidationResult.IS_POSSIBLE or\n            result == ValidationResult.IS_POSSIBLE_LOCAL_ONLY)", "category": "Python"}, {"instruction": "def beginning_of_line(self):\n        r\"\"\"\n        Return true if the scan pointer is at the beginning of a line.\n\n            >>> s = Scanner(\"test\\ntest\\n\")\n            >>> s.beginning_of_line()\n            True\n            >>> s.skip(r'te')\n            2\n            >>> s.beginning_of_line()\n            False\n            >>> s.skip(r'st\\n')\n            3\n            >>> s.beginning_of_line()\n            True\n            >>> s.terminate()\n            >>> s.beginning_of_line()\n            True\n        \"\"\"\n", "input": "", "output": "        if self.pos > len(self.string):\n            return None\n        elif self.pos == 0:\n            return True\n        return self.string[self.pos - 1] == '\\n'", "category": "Python"}, {"instruction": "def activate_bootstrap(driver):\n    \"\"\" Allows you to use Bootstrap Tours with SeleniumBase\n        http://bootstraptour.com/\n    \"\"\"\n", "input": "", "output": "    bootstrap_tour_css = constants.BootstrapTour.MIN_CSS\n    bootstrap_tour_js = constants.BootstrapTour.MIN_JS\n\n    verify_script = (", "category": "Python"}, {"instruction": "def _extract_next_page_link(self):\n        \"\"\" Try to get next page link. \"\"\"\n", "input": "", "output": "\n        # HEADS UP: we do not abort if next_page_link is already set:\n        #           we try to find next (eg. find 3 if already at page 2).\n\n        for pattern in self.config.next_page_link:\n            items = self.parsed_tree.xpath(pattern)\n\n            if not items:\n                continue\n\n            if len(items) == 1:\n                item = items[0]\n\n                if 'href' in item.keys():\n                    self.next_page_link = item.get('href')\n\n                else:\n                    self.next_page_link = item.text.strip()\n\n                LOGGER.info(u'Found next page link: %s.',\n                            self.next_page_link)\n\n                # First found link is the good one.\n                break\n\n            else:\n                LOGGER.warning(u'%s items for next-page link %s',\n                               items, pattern,\n                               extra={'siteconfig': self.config.host})", "category": "Python"}, {"instruction": "def chunk(iterator, max_size):\n    \"\"\"Chunk a list/set/etc.\n\n    :param iter iterator: The iterable object to chunk.\n    :param int max_size: Max size of each chunk. Remainder chunk may be smaller.\n\n    :return: Yield list of items.\n    :rtype: iter\n    \"\"\"\n", "input": "", "output": "    gen = iter(iterator)\n    while True:\n        chunked = list()\n        for i, item in enumerate(gen):\n            chunked.append(item)\n            if i >= max_size - 1:\n                break\n        if not chunked:\n            return\n        yield chunked", "category": "Python"}, {"instruction": "def create_tc_entity(self, key, value):\n        \"\"\"Create method of CRUD operation for TC entity data.\n\n        Args:\n            key (string): The variable to write to the DB.\n            value (any): The data to write to the DB.\n\n        Returns:\n            (string): Result of DB write.\n        \"\"\"\n", "input": "", "output": "        data = None\n        if key is not None and value is not None:\n            data = self.db.create(key.strip(), json.dumps(value))\n        else:\n            self.tcex.log.warning(u'The key or value field was None.')\n        return data", "category": "Python"}, {"instruction": "def follow(*args):\n    \"\"\"Start following changes to one or more FILENAME(s).\"\"\"\n", "input": "", "output": "    parser = argparse.ArgumentParser(prog=\"%s %s\" % (__package__, follow.__name__), description=follow.__doc__)\n    parser.add_argument('FILENAME', help=\"files to follow\", nargs=\"+\")\n    args = parser.parse_args(args)\n\n    config = FragmentsConfig()\n    for s, filename in _iterate_over_files(args.FILENAME, config, statuses='?'):\n        fullpath = os.path.realpath(filename)\n        if fullpath.startswith(config.root):\n            key = os.path.relpath(fullpath, config.root)\n            if key in config['files']:\n                yield \"'%s' is already being followed\" % os.path.relpath(filename)\n                continue\n            if os.access(fullpath, os.W_OK|os.R_OK):\n                file_sha = _file_key(key)\n                config['files'][key] = file_sha\n                yield \"'%s' is now being followed (SHA-256: '%s')\" % (os.path.relpath(filename), file_sha)\n            else:\n                yield \"Could not access '%s' to follow it\" % os.path.relpath(filename)\n        else:\n            yield \"Could not follow '%s'; it is outside the repository\" % os.path.relpath(filename)\n    config.dump()", "category": "Python"}, {"instruction": "def undelete_dashboard(self, id, **kwargs):  # noqa: E501\n        \"\"\"Undelete a specific dashboard  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.undelete_dashboard(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :return: ResponseContainerDashboard\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.undelete_dashboard_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.undelete_dashboard_with_http_info(id, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def targetLow(self):\n        \"\"\"Return the low byte of the target address field.\n\n        Used in All-Link Cleanup messages.\n        \"\"\"\n", "input": "", "output": "        low_byte = None\n        if self.target is not None and self._messageFlags.isBroadcast:\n            low_byte = self.target.bytes[0]\n        return low_byte", "category": "Python"}, {"instruction": "def update_timer(self, timer, action, usecs):\n        \"\"\"\n        Called by libcouchbase to add/remove timers\n        \"\"\"\n", "input": "", "output": "        if action == PYCBC_EVACTION_WATCH:\n            timer.schedule(usecs, self.reactor)\n\n        elif action == PYCBC_EVACTION_UNWATCH:\n            timer.cancel()\n\n        elif action == PYCBC_EVACTION_CLEANUP:\n            timer.cleanup()", "category": "Python"}, {"instruction": "def calculate_oobatake_dG(seq, temp):\n    \"\"\"Get free energy of unfolding (dG) using Oobatake method in units cal/mol.\n\n    Args:\n        seq (str, Seq, SeqRecord): Amino acid sequence\n        temp (float): Temperature in degrees C\n\n    Returns:\n        float: Free energy of unfolding dG (J/mol)\n\n    \"\"\"\n", "input": "", "output": "\n    dH = calculate_oobatake_dH(seq, temp)\n    dS = calculate_oobatake_dS(seq, temp)\n    dG = dH - (temp + 273.15) * dS\n\n    # 563.552 - a correction for N- and C-terminal group (approximated from 7 examples in the paper)\n    return dG - 563.552", "category": "Python"}, {"instruction": "def shrink(self, src, width=0, max_value=0, filter_method=None,\n               path=None, flags=0):\n        \"\"\"Shrink sketch\n        Params:\n            <Sketch> src_sketch\n            <int> width\n            <int> max_value\n            <lambda> | <function> filter\n            <str> path\n            <int> flags\n        \"\"\"\n", "input": "", "output": "        if filter_method:\n            get_ = _madoka.Sketch_get__\n            set_ = _madoka.Sketch_set__\n            new_sketch = Sketch(width, max_value, path, flags, src.seed)\n            for table_id in range(SKETCH_DEPTH):\n                for offset in range(width, src.width, width):\n                    for cell_id in range(width):\n                        val = get_(src, table_id, offset + cell_id)\n                        val = filter_method(val)\n                        val = max_value if val > max_value else val\n                        if val > get_(new_sketch, table_id, cell_id):\n                            set_(new_sketch, table_id, cell_id, val)\n            self.swap(new_sketch)\n        else:\n            _madoka.Sketch_shrink(self, src, width, max_value, None, path, flags)", "category": "Python"}, {"instruction": "def ParseOptions(cls, options, output_module):\n    \"\"\"Parses and validates options.\n\n    Args:\n      options (argparse.Namespace): parser options.\n      output_module (OutputModule): output module to configure.\n\n    Raises:\n      BadConfigObject: when the output module object does not have the\n          SetCredentials or SetDatabaseName methods.\n    \"\"\"\n", "input": "", "output": "    if not hasattr(output_module, 'SetCredentials'):\n      raise errors.BadConfigObject('Unable to set username information.')\n\n    if not hasattr(output_module, 'SetDatabaseName'):\n      raise errors.BadConfigObject('Unable to set database information.')\n\n    username = cls._ParseStringOption(\n        options, 'username', default_value=cls._DEFAULT_USERNAME)\n    password = cls._ParseStringOption(\n        options, 'password', default_value=cls._DEFAULT_PASSWORD)\n    name = cls._ParseStringOption(\n        options, 'db_name', default_value=cls._DEFAULT_NAME)\n\n    output_module.SetCredentials(username=username, password=password)\n    output_module.SetDatabaseName(name)\n    server_config.ServerArgumentsHelper.ParseOptions(options, output_module)", "category": "Python"}, {"instruction": "def add_argument(self, *args, **kwargs):\n        \"\"\"Adds an argument to be parsed.\n\n        Accepts either a single instance of Argument or arguments to be passed\n        into :class:`Argument`'s constructor.\n\n        See :class:`Argument`'s constructor for documentation on the\n        available options.\n        \"\"\"\n", "input": "", "output": "\n        if len(args) == 1 and isinstance(args[0], self.argument_class):\n            self.args.append(args[0])\n        else:\n            self.args.append(self.argument_class(*args, **kwargs))\n\n        # Do not know what other argument classes are out there\n        if self.trim and self.argument_class is Argument:\n            # enable trim for appended element\n            self.args[-1].trim = kwargs.get('trim', self.trim)\n\n        return self", "category": "Python"}, {"instruction": "def pretty_str(self, indent=0):\n        \"\"\"Return a human-readable string representation of this object.\n\n        Kwargs:\n            indent (int): The amount of spaces to use as indentation.\n        \"\"\"\n", "input": "", "output": "        indent = ' ' * indent\n        values = '{{{}}}'.format(', '.join(map(pretty_str, self.value)))\n\n        if self.parenthesis:\n            return '{}({})'.format(indent, values)\n        return '{}{}'.format(indent, values)", "category": "Python"}, {"instruction": "def _create_variables(self, n_features):\n        \"\"\"Create the TensorFlow variables for the model.\n\n        :param n_features: number of features\n        :return: self\n        \"\"\"\n", "input": "", "output": "        w_name = 'weights'\n        self.W = tf.Variable(tf.truncated_normal(\n            shape=[n_features, self.num_hidden], stddev=0.1), name=w_name)\n        tf.summary.histogram(w_name, self.W)\n\n        bh_name = 'hidden-bias'\n        self.bh_ = tf.Variable(tf.constant(0.1, shape=[self.num_hidden]),\n                               name=bh_name)\n        tf.summary.histogram(bh_name, self.bh_)\n\n        bv_name = 'visible-bias'\n        self.bv_ = tf.Variable(tf.constant(0.1, shape=[n_features]),\n                               name=bv_name)\n        tf.summary.histogram(bv_name, self.bv_)", "category": "Python"}, {"instruction": "def archive_compile(filename, command=\"make\"):\n    \"\"\"\n    Returns if the given archive properly compile.\n    Extract it in a temporary directory, run the given command, and return True it's result is 0\n    \"\"\"\n", "input": "", "output": "    if not tarfile.is_tarfile(filename):\n        print(\"Cannot extract archive\")\n        return False\n    if command == \"\":\n        return True\n    with tempfile.TemporaryDirectory(suffix=\"prof\") as tmpdir:\n        with tarfile.open(filename) as tararchive:\n            tararchive.extractall(tmpdir)\n            cwd = os.getcwd()  # get current directory\n            try:\n                os.chdir(tmpdir)\n                print(\"Running {} in {} for file {}\".format(command, tmpdir, filename))\n                make = os.system(command)\n                if make == 0:\n                    print(\"Successfully compiled\")\n                    return True\n            finally:\n                os.chdir(cwd)\n    return False", "category": "Python"}, {"instruction": "def createGroupResponse(self, group, vendorSpecific=None):\n        \"\"\"CNIdentity.createGroup(session, groupName) \u2192 Subject\n        https://releases.dataone.org/online/api-\n        documentation-v2.0.1/apis/CN_APIs.html#CNIdentity.createGroup.\n\n        Args:\n          group:\n          vendorSpecific:\n\n        Returns:\n\n        \"\"\"\n", "input": "", "output": "        mmp_dict = {'group': ('group.xml', group.toxml('utf-8'))}\n        return self.POST('groups', fields=mmp_dict, headers=vendorSpecific)", "category": "Python"}, {"instruction": "def unpack(self, buff, offset=0):\n        \"\"\"Unpack a binary message into this object's attributes.\n\n        Unpack the binary value *buff* and update this object attributes based\n        on the results.\n\n        Args:\n            buff (bytes): Binary data package to be unpacked.\n            offset (int): Where to begin unpacking.\n\n        Raises:\n            Exception: If there is a struct unpacking error.\n\n        \"\"\"\n", "input": "", "output": "        def _int2hex(number):\n            return \"{0:0{1}x}\".format(number, 2)\n\n        try:\n            unpacked_data = struct.unpack('!6B', buff[offset:offset+6])\n        except struct.error as exception:\n            raise exceptions.UnpackException('%s; %s: %s' % (exception,\n                                                             offset, buff))\n\n        transformed_data = ':'.join([_int2hex(x) for x in unpacked_data])\n        self._value = transformed_data", "category": "Python"}, {"instruction": "def clean_series(series, *args, **kwargs):\n    \"\"\"Ensure all datetimes are valid Timestamp objects and dtype is np.datetime64[ns]\n    >>> from datetime import timedelta\n    >>> clean_series(pd.Series([datetime.datetime(1, 1, 1), 9, '1942', datetime.datetime(1970, 10, 23)]))\n    0    1677-09-22 00:12:44+00:00\n    1                            9\n    2                         1942\n    3    1970-10-23 00:00:00+00:00\n    dtype: object\n    >>> clean_series(pd.Series([datetime.datetime(1, 1, 1), datetime.datetime(3000, 10, 23)]))\n    0             1677-09-22 00:12:44+00:00\n    1   2262-04-11 23:47:16.854775+00:00\n    dtype: datetime64[ns, UTC]\n    \"\"\"\n", "input": "", "output": "    if not series.dtype == np.dtype('O'):\n        return series\n    if any_generated((isinstance(v, datetime.datetime) for v in series)):\n        series = series.apply(clip_datetime)\n    if any_generated((isinstance(v, basestring) for v in series)):\n        series = series.apply(encode)\n    series = series.apply(try_float_int)\n    return series", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"\n        Signals the worker threads to exit and waits on them.\n        \"\"\"\n", "input": "", "output": "        if not self.stopped():\n            self._stop.set()\n            for worker in self._workers:\n                worker.join()", "category": "Python"}, {"instruction": "def build_info(self):\n        \"\"\"Return the build's info\"\"\"\n", "input": "", "output": "        if 'build_info' not in self._memo:\n            self._memo['build_info'] = _get_url(self.artifact_url('json')).json()\n        return self._memo['build_info']", "category": "Python"}, {"instruction": "def p_Try(p):\n    '''\n    Try : TRY COLON Terminator Block Catch\n        | TRY COLON Terminator Block Catch FINALLY COLON Terminator Block\n    '''\n", "input": "", "output": "    if len(p) <= 6:\n        p[0] = Try(p[3], p[4], p[5], None, None)\n    else:\n        p[0] = Try(p[3], p[4], p[5], p[8], p[9])", "category": "Python"}, {"instruction": "def clip(self, lower=None, upper=None):\n    '''\n    Trim values at input thresholds using pandas function\n    '''\n", "input": "", "output": "    df = self.export_df()\n    df = df.clip(lower=lower, upper=upper)\n    self.load_df(df)", "category": "Python"}, {"instruction": "def get_command(domain_name, command_name):\n    \"\"\"Returns a closure function that dispatches message to the WebSocket.\"\"\"\n", "input": "", "output": "    def send_command(self, **kwargs):\n        return self.ws.send_message(\n            '{0}.{1}'.format(domain_name, command_name),\n            kwargs\n        )\n\n    return send_command", "category": "Python"}, {"instruction": "def py_to_glsl(root):\n    \"\"\"Translate Python AST into GLSL code.\n\n    root: an ast.FunctionDef object\n\n    Return a list of strings, where each string is a line of GLSL\n    code.\n    \"\"\"\n", "input": "", "output": "    atg = AstToGlsl()\n    code = atg.visit(root)\n    return code.lines", "category": "Python"}, {"instruction": "def split_log(logf):\n        \"\"\"split concat log into individual samples\"\"\"\n", "input": "", "output": "        flashpatt = re.compile(\n            r'\\[FLASH\\] Fast Length Adjustment of SHort reads\\n(.+?)\\[FLASH\\] FLASH', flags=re.DOTALL)\n        return flashpatt.findall(logf)", "category": "Python"}, {"instruction": "async def set_access_string(self, **params):\n\t\t\"\"\"Writes content access string to database \n\t\t\"\"\"\n", "input": "", "output": "\t\tif params.get(\"message\"):\n\t\t\tparams = json.loads(params.get(\"message\", \"{}\"))\n\n\t\tcid = int(params.get(\"cid\", \"0\"))\n\t\tseller_access_string = params.get(\"seller_access_string\")\n\t\tseller_pubkey = params.get(\"seller_pubkey\")\n\t\tcoinid = params.get(\"coinid\")\n\n\t\ttry:\n\t\t\tcoinid = coinid.replace(\"TEST\", \"\")\n\t\texcept:\n\t\t\tpass\n\n\t\tdatabase = client[coinid]\n\t\tcollection = database[settings.CONTENT]\n\t\tcontent = await collection.find_one({\"cid\":cid})\n\n\t\tif not content:\n\t\t\treturn {\"error\":404, \"reason\":\"Content not found\"}\n\n\t\tif not all([cid, seller_access_string, seller_pubkey]):\n\t\t\treturn {\"error\":400, \"reason\":\"Missed required fields\"}\n\n\t\tawait collection.find_one_and_update({\"cid\":cid},\n\t\t\t\t\t\t{\"$set\":{\"seller_access_string\":seller_access_string}})\n\n\t\tawait collection.find_one_and_update({\"cid\":cid},\n\t\t\t\t\t\t{\"$set\":{\"seller_pubkey\":seller_pubkey}})\n\n\t\tcontent = await collection.find_one({\"cid\":cid})\n\t\treturn {i:content[i] for i in content if i != \"_id\"}", "category": "Python"}, {"instruction": "def resource_set_create(self, token, name, **kwargs):\n        \"\"\"\n        Create a resource set.\n\n        https://docs.kantarainitiative.org/uma/rec-oauth-resource-reg-v1_0_1.html#rfc.section.2.2.1\n\n        :param str token: client access token\n        :param str id: Identifier of the resource set\n        :param str name:\n        :param str uri: (optional)\n        :param str type: (optional)\n        :param list scopes: (optional)\n        :param str icon_url: (optional)\n        :param str DisplayName: (optional)\n        :param boolean ownerManagedAccess: (optional)\n        :param str owner: (optional)\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        return self._realm.client.post(\n            self.well_known['resource_registration_endpoint'],\n            data=self._get_data(name=name, **kwargs),\n            headers=self.get_headers(token)\n        )", "category": "Python"}, {"instruction": "def horner_log(coeffs, log_coeff, x):\n    '''Technically possible to save one addition of the last term of \n    coeffs is removed but benchmarks said nothing was saved'''\n", "input": "", "output": "    tot = 0.0\n    for c in coeffs:\n        tot = tot*x + c\n    return tot + log_coeff*log(x)", "category": "Python"}, {"instruction": "def plot_joint_sfs_scaled(*args, **kwargs):\n    \"\"\"Plot a scaled joint site frequency spectrum.\n\n    Parameters\n    ----------\n    s : array_like, int, shape (n_chromosomes_pop1, n_chromosomes_pop2)\n        Joint site frequency spectrum.\n    ax : axes, optional\n        Axes on which to draw. If not provided, a new figure will be created.\n    imshow_kwargs : dict-like\n        Additional keyword arguments, passed through to ax.imshow().\n\n    Returns\n    -------\n    ax : axes\n        The axes on which the plot was drawn.\n\n    \"\"\"\n", "input": "", "output": "    imshow_kwargs = kwargs.get('imshow_kwargs', dict())\n    imshow_kwargs.setdefault('norm', None)\n    kwargs['imshow_kwargs'] = imshow_kwargs\n    ax = plot_joint_sfs(*args, **kwargs)\n    return ax", "category": "Python"}, {"instruction": "def get_config_load_path(conf_path=None):\n    \"\"\"\n    Return config file load path\n\n    Priority:\n        1. conf_path\n        2. current directory\n        3. home directory\n\n    Parameters\n    ----------\n    conf_path\n\n    Returns\n    -------\n\n    \"\"\"\n", "input": "", "output": "\n    if conf_path is None:\n        # test ./andes.conf\n        if os.path.isfile('andes.conf'):\n            conf_path = 'andes.conf'\n        # test ~/andes.conf\n        home_dir = os.path.expanduser('~')\n        if os.path.isfile(os.path.join(home_dir, '.andes', 'andes.conf')):\n            conf_path = os.path.join(home_dir, '.andes', 'andes.conf')\n\n    if conf_path is not None:\n        logger.debug('Found config file at {}.'.format(conf_path))\n\n    return conf_path", "category": "Python"}, {"instruction": "def update_from_tuple(self, the_tuple):\n        \"\"\"\n        Update task from tuple.\n        \"\"\"\n", "input": "", "output": "        if not the_tuple.rowcount:\n            raise Queue.ZeroTupleException(\"Error updating task\")\n\n        row = the_tuple[0]\n\n        if self.task_id != row[0]:\n            raise Queue.BadTupleException(\"Wrong task: id's are not match\")\n\n        self.state = row[1]\n        self.data = row[2]", "category": "Python"}, {"instruction": "def pdf(self):\n        r\"\"\"\n        Generate the vector of probabilities for the Beta-binomial\n        (n, a, b) distribution.\n\n        The Beta-binomial distribution takes the form\n\n        .. math::\n            p(k \\,|\\, n, a, b) =\n            {n \\choose k} \\frac{B(k + a, n - k + b)}{B(a, b)},\n            \\qquad k = 0, \\ldots, n,\n\n        where :math:`B` is the beta function.\n\n        Parameters\n        ----------\n        n : scalar(int)\n            First parameter to the Beta-binomial distribution\n        a : scalar(float)\n            Second parameter to the Beta-binomial distribution\n        b : scalar(float)\n            Third parameter to the Beta-binomial distribution\n\n        Returns\n        -------\n        probs: array_like(float)\n            Vector of probabilities over k\n\n        \"\"\"\n", "input": "", "output": "        n, a, b = self.n, self.a, self.b\n        k = np.arange(n + 1)\n        probs = binom(n, k) * beta(k + a, n - k + b) / beta(a, b)\n        return probs", "category": "Python"}, {"instruction": "def GetRadioButtonSelect(selectList, title=\"Select\", msg=\"\"):\n    \"\"\"\n    Create radio button window for option selection\n\n    title: Window name\n    mag: Label of the radio button\n\n    return (seldctedItem, selectedindex)\n    \"\"\"\n", "input": "", "output": "    root = tkinter.Tk()\n    root.title(title)\n    val = tkinter.IntVar()\n    val.set(0)\n\n    if msg != \"\":\n        tkinter.Label(root, text=msg).pack()\n\n    index = 0\n    for item in selectList:\n        tkinter.Radiobutton(root, text=item, variable=val,\n                            value=index).pack(anchor=tkinter.W)\n        index += 1\n\n    tkinter.Button(root, text=\"OK\", fg=\"black\", command=root.quit).pack()\n    root.mainloop()\n    root.destroy()\n\n    print(selectList[val.get()] + \" is selected\")\n    return (selectList[val.get()], val.get())", "category": "Python"}, {"instruction": "def _increment(self, n=1):\n        \"\"\"Move forward n tokens in the stream.\"\"\"\n", "input": "", "output": "        if self._cur_position >= self.num_tokens-1:\n            self._cur_positon = self.num_tokens - 1\n            self._finished = True\n        else:\n            self._cur_position += n", "category": "Python"}, {"instruction": "async def create(cls, fsm_context: FSMContext):\n        \"\"\"\n        :param fsm_context:\n        :return:\n        \"\"\"\n", "input": "", "output": "        proxy = cls(fsm_context)\n        await proxy.load()\n        return proxy", "category": "Python"}, {"instruction": "def mktime(self, **kwargs):\n        \"\"\" return the name of a selected events ft1file\n        \"\"\"\n", "input": "", "output": "        kwargs_copy = self.base_dict.copy()\n        kwargs_copy.update(**kwargs)\n        kwargs_copy['dataset'] = kwargs.get('dataset', self.dataset(**kwargs))\n        kwargs_copy['component'] = kwargs.get(\n            'component', self.component(**kwargs))\n        self._replace_none(kwargs_copy)        \n        localpath = NameFactory.mktime_format.format(**kwargs_copy)\n        if kwargs.get('fullpath', False):\n            return self.fullpath(localpath=localpath)\n        return localpath", "category": "Python"}, {"instruction": "def _generate_base_svm_regression_spec(model):\n    \"\"\"\n    Takes an SVM regression model  produces a starting spec using the parts.\n    that are shared between all SVMs.\n    \"\"\"\n", "input": "", "output": "    if not(_HAS_SKLEARN):\n        raise RuntimeError('scikit-learn not found. scikit-learn conversion API is disabled.')\n\n    spec = _Model_pb2.Model()\n    spec.specificationVersion = SPECIFICATION_VERSION\n    svm = spec.supportVectorRegressor\n\n    _set_kernel(model, svm)\n\n    svm.rho = -model.intercept_[0]\n    for i in range(len(model._dual_coef_)):\n        for cur_alpha in model._dual_coef_[i]:\n            svm.coefficients.alpha.append(cur_alpha)\n\n    for cur_src_vector in model.support_vectors_:\n        cur_dest_vector = svm.denseSupportVectors.vectors.add()\n        for i in cur_src_vector:\n            cur_dest_vector.values.append(i)\n    return spec", "category": "Python"}, {"instruction": "def get_log_hierarchy_id(self):\n        \"\"\"Gets the hierarchy ``Id`` associated with this session.\n\n        return: (osid.id.Id) - the hierarchy ``Id`` associated with this\n                session\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.BinHierarchySession.get_bin_hierarchy_id\n        if self._catalog_session is not None:\n            return self._catalog_session.get_catalog_hierarchy_id()\n        return self._hierarchy_session.get_hierarchy_id()", "category": "Python"}, {"instruction": "def updated_current_fields(\n            self,\n            update_fields):\n        \"\"\"updated_current_fields\n\n        :param update_fields: dict with values for\n                              updating fields_to_add\n        \"\"\"\n", "input": "", "output": "        self.fields_to_add = {}\n        for k in self.org_fields:\n            self.fields_to_add[k] = self.org_fields[k]\n        self.fields_to_add.update(update_fields)", "category": "Python"}, {"instruction": "def fetch_job(self, job_id, checkout=False):\n        \"\"\"\n        Fetch the current job reference (refs/aetros/job/<id>) from origin and (when checkout=True)read its tree to\n        the current git index and checkout into working director.\n        \"\"\"\n", "input": "", "output": "        self.job_id = job_id\n\n        self.logger.debug(\"Git fetch job reference %s\" % (self.ref_head, ))\n        out, code, err = self.command_exec(['ls-remote', 'origin', self.ref_head])\n\n        if code:\n            self.logger.error('Could not find the job ' + job_id + ' on the server. Are you online and does the job exist?')\n            sys.exit(1)\n\n        try:\n            self.command_exec(['fetch', '-f', '-n', 'origin', self.ref_head+':'+self.ref_head])\n        except Exception:\n            self.logger.error(\"Could not load job information for \" + job_id + '. You need to be online to start pre-configured jobs.')\n            raise\n\n        self.read_job(job_id, checkout)", "category": "Python"}, {"instruction": "def make_json_decoder_hook(str_decoders=STRING_DECODERS,\n                           extra_str_decoders=tuple(),\n                           converters=MappingProxyType(dict())) -> Callable:\n    \"\"\"Customize JSON string decoder hooks.\n\n    Object hook for typical deserialization scenarios.\n\n    Notes\n    -----\n    Specifying a field in converters will ensure custom decoding/passthrough.\n\n    Parameters\n    ----------\n    str_decoders: functions for decoding strings to objects.\n    extra_str_decoders: appends additional string decoders to str_decoders.\n    converters: field / parser function mapping.\n    \"\"\"\n", "input": "", "output": "\n    str_decoders = tuple(chain(str_decoders, extra_str_decoders))\n    object_hook = partial(json_decoder_hook, str_decoders=str_decoders,\n                          converters=converters)\n\n    return object_hook", "category": "Python"}, {"instruction": "def remove_object(self, instance, bucket_name, object_name):\n        \"\"\"\n        Remove an object from a bucket.\n\n        :param str instance: A Yamcs instance name.\n        :param str bucket_name: The name of the bucket.\n        :param str object_name: The object to remove.\n        \"\"\"\n", "input": "", "output": "        url = '/buckets/{}/{}/{}'.format(instance, bucket_name, object_name)\n        self._client.delete_proto(url)", "category": "Python"}, {"instruction": "def load(self, *args, **kwargs):\n        \"\"\"Load the required datasets from the multiple scenes.\"\"\"\n", "input": "", "output": "        self._generate_scene_func(self._scenes, 'load', False, *args, **kwargs)", "category": "Python"}, {"instruction": "def set_security_zones_activation(self, internal=True, external=True):\r\n        \"\"\" this function will set the alarm system to armed or disable it\r\n        \r\n        Args:\r\n          internal(bool): activates/deactivates the internal zone\r\n          external(bool): activates/deactivates the external zone\r\n        \r\n        Examples:\r\n          arming while being at home\r\n          \r\n          >>> home.set_security_zones_activation(False,True)\r\n          \r\n          arming without being at home\r\n          \r\n          >>> home.set_security_zones_activation(True,True)\r\n          \r\n          disarming the alarm system\r\n          \r\n          >>> home.set_security_zones_activation(False,False)\r\n        \"\"\"\n", "input": "", "output": "        data = {\"zonesActivation\": {\"EXTERNAL\": external, \"INTERNAL\": internal}}\r\n        return self._restCall(\"home/security/setZonesActivation\", json.dumps(data))", "category": "Python"}, {"instruction": "def space(self):\n        \"\"\"Get system disk space usage.\n\n        :return: :class:`system.Space <system.Space>` object\n        :rtype: system.Space\n        \"\"\"\n", "input": "", "output": "        schema = SpaceSchema()\n        resp = self.service.get(self.base+'space/')\n        return self.service.decode(schema, resp)", "category": "Python"}, {"instruction": "def longest_bar_prefix_value(self):\n        \"\"\"\n        Calculates the longest progress bar prefix in order to keep all progress bars left-aligned.\n        :return: Length of the longest task prefix in character unit.\n        \"\"\"\n", "input": "", "output": "        longest = 0\n        for key, t in self.tasks.items():\n            size = len(t.prefix)\n            if size > longest:\n                longest = size\n\n        return longest", "category": "Python"}, {"instruction": "def iter_segments(obj, neurite_filter=None, neurite_order=NeuriteIter.FileOrder):\n    '''Return an iterator to the segments in a collection of neurites\n\n    Parameters:\n        obj: neuron, population, neurite, section, or iterable containing neurite objects\n        neurite_filter: optional top level filter on properties of neurite neurite objects\n        neurite_order: order upon which neurite should be iterated. Values:\n            - NeuriteIter.FileOrder: order of appearance in the file\n            - NeuriteIter.NRN: NRN simulator order: soma -> axon -> basal -> apical\n\n    Note:\n        This is a convenience function provided for generic access to\n        neuron segments. It may have a performance overhead WRT custom-made\n        segment analysis functions that leverage numpy and section-wise iteration.\n    '''\n", "input": "", "output": "    sections = iter((obj,) if isinstance(obj, Section) else\n                    iter_sections(obj,\n                                  neurite_filter=neurite_filter,\n                                  neurite_order=neurite_order))\n\n    return chain.from_iterable(zip(sec.points[:-1], sec.points[1:])\n                               for sec in sections)", "category": "Python"}, {"instruction": "def to_api_repr(self):\n        \"\"\"Generate a resource for :meth:`_begin`.\"\"\"\n", "input": "", "output": "        configuration = self._configuration.to_api_repr()\n\n        resource = {\n            \"jobReference\": self._properties[\"jobReference\"],\n            \"configuration\": configuration,\n        }\n        configuration[\"query\"][\"query\"] = self.query\n\n        return resource", "category": "Python"}, {"instruction": "def is_alive(self):\n        \"\"\"\n        Will test whether the ACS service is up and alive.\n        \"\"\"\n", "input": "", "output": "        response = self.get_monitoring_heartbeat()\n        if response.status_code == 200 and response.content == 'alive':\n            return True\n\n        return False", "category": "Python"}, {"instruction": "def load_metadata_from_file(filename):\n    \"\"\" Load the plot related metadata saved in a file\n\n    Parameters\n    ----------\n    filename: str\n        Name of file load metadata from.\n\n    Returns\n    -------\n    cp: ConfigParser\n        A configparser object containing the metadata\n    \"\"\"\n", "input": "", "output": "    try:\n        extension = os.path.splitext(filename)[1]\n        return _metadata_loader[extension](filename)\n    except KeyError:\n        raise TypeError('Cannot read metadata from file %s, extension %s not '\n                        'supported at this time' % (filename, extension))", "category": "Python"}, {"instruction": "def foreach(self, f):\n        \"\"\"\n        Applies a function to all elements of this RDD.\n\n        >>> def f(x): print(x)\n        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n        \"\"\"\n", "input": "", "output": "        f = fail_on_stopiteration(f)\n\n        def processPartition(iterator):\n            for x in iterator:\n                f(x)\n            return iter([])\n        self.mapPartitions(processPartition).count()", "category": "Python"}, {"instruction": "def _get_subnet_explicit_route_table(subnet_id, vpc_id, conn=None, region=None, key=None, keyid=None, profile=None):\n    '''\n    helper function to find subnet explicit route table associations\n\n    .. versionadded:: 2016.11.0\n    '''\n", "input": "", "output": "    if not conn:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    if conn:\n        vpc_route_tables = conn.get_all_route_tables(filters={'vpc_id': vpc_id})\n        for vpc_route_table in vpc_route_tables:\n            for rt_association in vpc_route_table.associations:\n                if rt_association.subnet_id == subnet_id and not rt_association.main:\n                    return rt_association.id\n    return None", "category": "Python"}, {"instruction": "def AcosTriAngles(v, f, normalize):\n    \"\"\" Returns a Ch object whose only attribute \"v\" represents the flattened vertices.\"\"\"\n", "input": "", "output": "\n    if normalize:\n        nm = lambda x : NormalizedNx3(x)\n    else:\n        nm = lambda x : x\n\n    return Ch(lambda v :\n        Sum3xN(NormalizedNx3(TriEdges(f, 1, 0, nm(v))) * NormalizedNx3(TriEdges(f, 2, 0, nm(v)))) &\n        Sum3xN(NormalizedNx3(TriEdges(f, 2, 1, nm(v))) * NormalizedNx3(TriEdges(f, 0, 1, nm(v)))) &\n        Sum3xN(NormalizedNx3(TriEdges(f, 0, 2, nm(v))) * NormalizedNx3(TriEdges(f, 1, 2, nm(v)))))", "category": "Python"}, {"instruction": "def register(self, event_name):\n        \"\"\"\n        Decorator,\n\n        Registers the decorated function as a callback for the `event_name`\n        given.\n\n        :param event_name: The name of the event to register for.\n\n        Example:\n            >>> callbacks = Callbacks()\n            >>> @callbacks.register(\"my_event\")\n            ... def hello():\n            ...     print(\"Hello world\")\n            ...\n            >>> # Which then can be called\n            >>> callbacks.call(\"my_event\")\n            Hello world\n\n        \"\"\"\n", "input": "", "output": "        def registrar(func):\n            self.callbacks[event_name].append(func)\n\n            return func\n\n        return registrar", "category": "Python"}, {"instruction": "def ingest(cls, resource, element=None, xpath=\"ti:citation\"):\n        \"\"\" Ingest xml to create a citation\n\n        :param resource: XML on which to do xpath\n        :param element: Element where the citation should be stored\n        :param xpath: XPath to use to retrieve citation\n\n        :return: XmlCtsCitation\n        \"\"\"\n", "input": "", "output": "        # Reuse of of find citation\n        results = resource.xpath(xpath, namespaces=XPATH_NAMESPACES)\n        if len(results) > 0:\n            citation = cls(\n                name=results[0].get(\"label\"),\n                xpath=results[0].get(\"xpath\"),\n                scope=results[0].get(\"scope\")\n            )\n\n            if isinstance(element, cls):\n                element.child = citation\n                cls.ingest(\n                    resource=results[0],\n                    element=element.child\n                )\n            else:\n                element = citation\n                cls.ingest(\n                    resource=results[0],\n                    element=element\n                )\n\n            return citation\n\n        return None", "category": "Python"}, {"instruction": "def read_all(self):\n        \"\"\"Read all data until EOF; block until connection closed.\"\"\"\n", "input": "", "output": "        self.process_rawq()\n        while not self.eof:\n            self.fill_rawq()\n            self.process_rawq()\n        buf = self.cookedq.getvalue()\n        self.cookedq.seek(0)\n        self.cookedq.truncate()\n        return buf", "category": "Python"}, {"instruction": "def iter_by_year(self):\n        \"\"\"Split the return objects by year and iterate\"\"\"\n", "input": "", "output": "        for key, grp in self.rets.groupby(lambda x: x.year):\n            yield key, CumulativeRets(rets=grp)", "category": "Python"}, {"instruction": "def start_transaction_input_with_inactive(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        start_transaction = ET.Element(\"start_transaction\")\n        config = start_transaction\n        input = ET.SubElement(start_transaction, \"input\")\n        with_inactive = ET.SubElement(input, \"with-inactive\", xmlns=\"http://tail-f.com/ns/netconf/inactive/1.0\")\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def validate_json_file(file_list):\n    \"\"\" validate JSON testcase format\n    \"\"\"\n", "input": "", "output": "    for json_file in set(file_list):\n        if not json_file.endswith(\".json\"):\n            logger.log_warning(\"Only JSON file format can be validated, skip: {}\".format(json_file))\n            continue\n\n        logger.color_print(\"Start to validate JSON file: {}\".format(json_file), \"GREEN\")\n\n        with io.open(json_file) as stream:\n            try:\n                json.load(stream)\n            except ValueError as e:\n                raise SystemExit(e)\n\n        print(\"OK\")", "category": "Python"}, {"instruction": "def new_session(self):\n        \"\"\"Establish a new session.\"\"\"\n", "input": "", "output": "        body = yield from self._fetch_json(URL_LOGIN, self._new_session_data)\n        self.sma_sid = jmespath.search('result.sid', body)\n        if self.sma_sid:\n            return True\n\n        msg = 'Could not start session, %s, got {}'.format(body)\n\n        if body.get('err'):\n            if body.get('err') == 503:\n                _LOGGER.error(\"Max amount of sessions reached\")\n            else:\n                _LOGGER.error(msg, body.get('err'))\n        else:\n            _LOGGER.error(msg, \"Session ID expected [result.sid]\")\n        return False", "category": "Python"}, {"instruction": "def link(self, mu, dist):\n        \"\"\"\n        glm link function\n        this is useful for going from mu to the linear prediction\n\n        Parameters\n        ----------\n        mu : array-like of legth n\n        dist : Distribution instance\n\n        Returns\n        -------\n        lp : np.array of length n\n        \"\"\"\n", "input": "", "output": "        return np.log(mu) - np.log(dist.levels - mu)", "category": "Python"}, {"instruction": "def get(self, page_size, current_page, session):\n        '''taobao.crm.groups.get \u67e5\u8be2\u5356\u5bb6\u7684\u5206\u7ec4\n        \n        \u67e5\u8be2\u5356\u5bb6\u7684\u5206\u7ec4\uff0c\u8fd4\u56de\u67e5\u8be2\u5230\u7684\u5206\u7ec4\u5217\u8868\uff0c\u5206\u9875\u8fd4\u56de\u5206\u7ec4'''\n", "input": "", "output": "        request = TOPRequest('taobao.crm.groups.get')\n        request['page_size'] = page_size\n        request['current_page'] = current_page\n        self.create(self.execute(request, session))\n        return self.groups", "category": "Python"}, {"instruction": "def generate_page_toc(soup):\n  \"\"\"Return page-level (~list of headings) TOC template data for soup\"\"\"\n", "input": "", "output": "  # Maybe we don't want to show all the headings. E.g., it's common for a page\n  # to have just one H1, a title at the top. Our heuristic: if a page has just\n  # one heading of some outline level, don't show it.\n  found_depth_counts = collections.defaultdict(int)\n  for tag in soup.find_all(_heading_re):\n    if (tag.get('id') or tag.get('name')):\n      found_depth_counts[hdepth(tag)] += 1\n\n  depth_list = [i for i in range(100) if 1 < found_depth_counts[i]]\n  depth_list = depth_list[:4]\n  toc = []\n  for tag in soup.find_all(_heading_re):\n    depth = hdepth(tag)\n    if depth in depth_list:\n      toc.append(dict(depth=depth_list.index(depth) + 1,\n                      link=tag.get('id') or tag.get('name'),\n                      text=tag.text))\n  return toc", "category": "Python"}, {"instruction": "def clean_username(self):\n        \"\"\"\n        Validate that the username is unique and not listed \n        in ``defaults.ACCOUNTS_FORBIDDEN_USERNAMES`` list.\n        \n        \"\"\"\n", "input": "", "output": "        try: \n            user = get_user_model().objects.get(username=self.cleaned_data[\"username\"])\n        except get_user_model().DoesNotExist: \n            pass\n        else: \n            raise forms.ValidationError(\n                            self.error_messages['duplicate_username'])\n\n        if self.cleaned_data['username'].lower() \\\n            in defaults.ACCOUNTS_FORBIDDEN_USERNAMES:\n            raise forms.ValidationError(_(u'This username is not allowed.'))\n        return self.cleaned_data['username']", "category": "Python"}, {"instruction": "def queryset(self, request, queryset):\n        \"\"\"\n        Return the object's entries if a value is set.\n        \"\"\"\n", "input": "", "output": "        if self.value():\n            params = {self.lookup_key: self.value()}\n            return queryset.filter(**params)", "category": "Python"}, {"instruction": "def _run_server_ops(state, host, progress=None):\n    '''\n    Run all ops for a single server.\n    '''\n", "input": "", "output": "\n    logger.debug('Running all ops on {0}'.format(host))\n\n    for op_hash in state.get_op_order():\n        op_meta = state.op_meta[op_hash]\n\n        logger.info('--> {0} {1} on {2}'.format(\n            click.style('--> Starting operation:', 'blue'),\n            click.style(', '.join(op_meta['names']), bold=True),\n            click.style(host.name, bold=True),\n        ))\n\n        result = _run_server_op(state, host, op_hash)\n\n        # Trigger CLI progress if provided\n        if progress:\n            progress((host, op_hash))\n\n        if result is False:\n            raise PyinfraError('Error in operation {0} on {1}'.format(\n                ', '.join(op_meta['names']), host,\n            ))\n\n        if pyinfra.is_cli:\n            print()", "category": "Python"}, {"instruction": "def today(year=None):\n    \"\"\"this day, last year\"\"\"\n", "input": "", "output": "    return datetime.date(int(year), _date.month, _date.day) if year else _date", "category": "Python"}, {"instruction": "def _get_site_amplification_term(self, C, vs30):\n        \"\"\"\n        Returns the site amplification given Eurocode 8 site classification\n        \"\"\"\n", "input": "", "output": "        f_s = np.zeros_like(vs30)\n        # Site class B\n        idx = np.logical_and(vs30 < 800.0, vs30 >= 360.0)\n        f_s[idx] = C[\"eB\"]\n        # Site Class C\n        idx = np.logical_and(vs30 < 360.0, vs30 >= 180.0)\n        f_s[idx] = C[\"eC\"]\n        # Site Class D\n        idx = vs30 < 180.0\n        f_s[idx] = C[\"eD\"]\n        return f_s", "category": "Python"}, {"instruction": "def classproperty(func):\n  \"\"\"Use as a decorator on a method definition to make it a class-level attribute.\n\n  This decorator can be applied to a method, a classmethod, or a staticmethod. This decorator will\n  bind the first argument to the class object.\n\n  Usage:\n  >>> class Foo(object):\n  ...   @classproperty\n  ...   def name(cls):\n  ...     return cls.__name__\n  ...\n  >>> Foo.name\n  'Foo'\n\n  Setting or deleting the attribute of this name will overwrite this property.\n\n  The docstring of the classproperty `x` for a class `C` can be obtained by\n  `C.__dict__['x'].__doc__`.\n  \"\"\"\n", "input": "", "output": "  doc = func.__doc__\n\n  if not isinstance(func, (classmethod, staticmethod)):\n    func = classmethod(func)\n\n  return ClassPropertyDescriptor(func, doc)", "category": "Python"}, {"instruction": "def in_(self, *objs):\n        \"\"\"\n        Create a condition\n        \"\"\"\n", "input": "", "output": "        if not objs:\n            return self.table.c[self.fielda]!=self.table.c[self.fielda]\n        else:\n            keys = get_objs_columns(objs, self.reference_fieldname)\n            sub_query = select([self.table.c[self.fielda]], (self.table.c[self.fieldb] == self.reference_class.c[self.reference_fieldname]) & (self.table.c[self.fieldb].in_(keys)))\n            condition = self.model_class.c[self.reversed_fieldname].in_(sub_query)\n            return condition", "category": "Python"}, {"instruction": "def _retrieve(self, namespace, stream, start_id, end_time, order, limit,\n                configuration):\n    \"\"\"\n    Yield events from stream starting after the event with id `start_id` until\n    and including events with timestamp `end_time`.\n    \"\"\"\n", "input": "", "output": "    start_id_event = Event(start_id)\n    end_id_event = Event(uuid_from_kronos_time(end_time,\n                                               _type=UUIDType.HIGHEST))\n    stream_events = self.db[namespace][stream]\n\n    # Find the interval our events belong to.\n    lo = bisect.bisect_left(stream_events, start_id_event)\n    if lo + 1 > len(stream_events):\n      return\n    if stream_events[lo] == start_id_event:\n      lo += 1\n    hi = bisect.bisect_right(stream_events, end_id_event)\n\n    if order == ResultOrder.DESCENDING:\n      index_it = xrange(hi - 1, lo - 1, -1)\n    else:\n      index_it = xrange(lo, hi)\n\n    for i in index_it:\n      if limit <= 0:\n        break\n      limit -= 1\n      yield marshal.dumps(stream_events[i])", "category": "Python"}, {"instruction": "def maximumCanEqual(self):\n        \"\"\"Flag indicating if the minimum value is inclusive or exclusive.\"\"\"\n", "input": "", "output": "        if self.maximum is None:\n            raise SchemaError(\"maximumCanEqual requires presence of maximum\")\n        value = self._schema.get(\"maximumCanEqual\", True)\n        if value is not True and value is not False:\n            raise SchemaError(\n                \"maximumCanEqual value {0!r} is not a boolean\".format(\n                    value))\n        return value", "category": "Python"}, {"instruction": "def _join(self, identifier, instance=None, tags=None):\n        \"\"\"\n        Join the identifier tuple with periods \".\", combine the arbitrary tags\n        with the base tags and the identifier tag, convert tags to \"tag=value\"\n        format, and then join everything with \",\".\n        \"\"\"\n", "input": "", "output": "        tag_list = []\n        if tags is not None:\n            tag_list.extend(tags.items())\n        tag_list.extend(self._base_tags)\n        return \".\".join(identifier) + \",\" + \",\".join(\n            \"{}={}\".format(k, v)\n            for k, v in tag_list\n        )", "category": "Python"}, {"instruction": "def do_struct(self, subcmd, opts, message):\n        \"\"\"${cmd_name}: get the structure of the specified message\n\n        ${cmd_usage}\n        ${cmd_option_list}\n        \"\"\"\n", "input": "", "output": "        client = MdClient(self.maildir, filesystem=self.filesystem)\n        as_json = getattr(opts, \"json\", False)\n        client.getstruct(message, as_json=as_json, stream=self.stdout)", "category": "Python"}, {"instruction": "def cbpdnmd_relax(k):\n    \"\"\"Do relaxation for the cbpdn stage. The only parameter is the slice\n    index `k` and there are no return values; all inputs and outputs are\n    from and to global variables.\n    \"\"\"\n", "input": "", "output": "\n    mp_Z_X[k] = mp_xrlx * mp_Z_X[k] + (1 - mp_xrlx) * mp_Z_Y1[k]\n    mp_DX[k] = mp_xrlx * mp_DX[k] + (1 - mp_xrlx) * (mp_Z_Y0[k] + mp_S[k])", "category": "Python"}, {"instruction": "def add_firewalld_port(port, permanent=True):\n    \"\"\" adds a firewall rule \"\"\"\n", "input": "", "output": "\n    yum_install(packages=['firewalld'])\n\n    log_green('adding a new fw rule: %s' % port)\n    with settings(hide('warnings', 'running', 'stdout', 'stderr'),\n                  warn_only=True, capture=True):\n        p = ''\n        if permanent:\n            p = '--permanent'\n        sudo('firewall-cmd --add-port %s %s' % (port, p))\n        sudo('systemctl restart firewalld')", "category": "Python"}, {"instruction": "def __sub_make_request(self, foc, gpid, callback):\n        \"\"\"Make right subscription request depending on whether local or global - used by __sub*\"\"\"\n", "input": "", "output": "        # global\n        if isinstance(gpid, string_types):\n            gpid = uuid_to_hex(gpid)\n            ref = (foc, gpid)\n            with self.__sub_add_reference(ref):\n                req = self._client._request_sub_create(self.__lid, foc, gpid, callback=callback)\n        # local\n        elif isinstance(gpid, Sequence) and len(gpid) == 2:\n            ref = (foc, tuple(gpid))\n            with self.__sub_add_reference(ref):\n                req = self._client._request_sub_create_local(self.__lid, foc, *gpid, callback=callback)\n        else:\n            raise ValueError('gpid must be string or two-element tuple')\n\n        req._run_on_completion(self.__sub_del_reference, ref)\n        return req", "category": "Python"}, {"instruction": "def show_toolbars(self):\r\n        \"\"\"Show/Hides toolbars.\"\"\"\n", "input": "", "output": "        value = not self.toolbars_visible\r\n        CONF.set('main', 'toolbars_visible', value)\r\n        if value:\r\n            self.save_visible_toolbars()\r\n        else:\r\n            self.get_visible_toolbars()\r\n\r\n        for toolbar in self.visible_toolbars:\r\n            toolbar.toggleViewAction().setChecked(value)\r\n            toolbar.setVisible(value)\r\n\r\n        self.toolbars_visible = value\r\n        self._update_show_toolbars_action()", "category": "Python"}, {"instruction": "def to_dense(self):\n        \"\"\"\n        Convert to dense DataFrame\n\n        Returns\n        -------\n        df : DataFrame\n        \"\"\"\n", "input": "", "output": "        data = {k: v.to_dense() for k, v in self.items()}\n        return DataFrame(data, index=self.index, columns=self.columns)", "category": "Python"}, {"instruction": "def allKeys(self):\r\n        \"\"\"\r\n        Returns a list of all the keys for this settings instance.\r\n        \r\n        :return     [<str>, ..]\r\n        \"\"\"\n", "input": "", "output": "        if self._customFormat:\r\n            return self._customFormat.allKeys()\r\n        else:\r\n            return super(XSettings, self).allKeys()", "category": "Python"}, {"instruction": "def fetch(self, year, week, overwrite=False):\n        \"\"\"Fetch PageViews and Downloads from Elasticsearch.\"\"\"\n", "input": "", "output": "        self.config['overwrite_files'] = overwrite\n        time_start = time.time()\n        self._fetch_pageviews(self.storage, year, week, ip_users=False)\n        self._fetch_downloads(self.storage, year, week, ip_users=False)\n        # CDS has no user_agent before this date 1433400000:\n        self._fetch_pageviews(self.storage, year, week, ip_users=True)\n        self._fetch_downloads(self.storage, year, week, ip_users=True)\n        logger.info('Fetch %s-%s in %s seconds.', year, week,\n                    time.time() - time_start)", "category": "Python"}, {"instruction": "def get_first_of_element(element, sub, contype=None):\n    \"\"\"\u62bd\u53d6lxml.etree\u5e93\u4e2delem\u5bf9\u8c61\u4e2d\u6587\u5b57\n\n    Args:\n        element: lxml.etree.Element\n        sub: str\n\n    Returns:\n        elem\u4e2d\u6587\u5b57\n    \"\"\"\n", "input": "", "output": "    content = element.xpath(sub)\n    return list_or_empty(content, contype)", "category": "Python"}, {"instruction": "def format_code(source, preferred_quote=\"'\"):\n    \"\"\"Return source code with quotes unified.\"\"\"\n", "input": "", "output": "    try:\n        return _format_code(source, preferred_quote)\n    except (tokenize.TokenError, IndentationError):\n        return source", "category": "Python"}, {"instruction": "def name(self, name):\n        \"\"\"Set the member name.\n\n        Note that a member name cannot appear in other enums, or generally\n        anywhere else in the IDB.\n        \"\"\"\n", "input": "", "output": "        success = idaapi.set_enum_member_name(self.cid, name)\n        if not success:\n            raise exceptions.CantRenameEnumMember(\n                \"Failed renaming {!r} to {!r}. Does the name exist somewhere else?\".format(self.name, name))", "category": "Python"}, {"instruction": "def raise_figure_window(f=0):\n    \"\"\"\n    Raises the supplied figure number or figure window.\n    \"\"\"\n", "input": "", "output": "    if _fun.is_a_number(f): f = _pylab.figure(f)\n    f.canvas.manager.window.raise_()", "category": "Python"}, {"instruction": "def _vertex_list_to_dataframe(ls, id_column_name):\n    \"\"\"\n    Convert a list of vertices into dataframe.\n    \"\"\"\n", "input": "", "output": "    assert HAS_PANDAS, 'Cannot use dataframe because Pandas is not available or version is too low.'\n    cols = reduce(set.union, (set(v.attr.keys()) for v in ls))\n    df = pd.DataFrame({id_column_name: [v.vid for v in ls]})\n    for c in cols:\n        df[c] = [v.attr.get(c) for v in ls]\n    return df", "category": "Python"}, {"instruction": "def data_not_in(db_data, user_data):\n        \"\"\"Validate data not in user data.\n\n        Args:\n            db_data (str): The data store in Redis.\n            user_data (list): The user provided data.\n\n        Returns:\n            bool: True if the data passed validation.\n        \"\"\"\n", "input": "", "output": "        if isinstance(user_data, list):\n            if db_data not in user_data:\n                return True\n        return False", "category": "Python"}, {"instruction": "def fn(self, i, n=1, interval=0, pre_dl=None, post_dl=None):\n        \"\"\"Press Fn key n times.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u6309 Fn \u529f\u80fd\u952e n \u6b21\u3002\n        \"\"\"\n", "input": "", "output": "        self.delay(pre_dl)\n        self.k.tap_key(self.k.function_keys[i], n, interval)\n        self.delay(post_dl)", "category": "Python"}, {"instruction": "def validate(path, regex=None):\n    \"\"\"\n    Validate that all the keys in the given list of path components are valid, given that they do not contain the separator, and match any optional regex given.\n    \"\"\"\n", "input": "", "output": "    validated = []\n    for elem in path:\n        key = elem[0]\n        strkey = str(key)\n        if (regex and (not regex.findall(strkey))):\n            raise dpath.exceptions.InvalidKeyName(\"{} at {} does not match the expression {}\"\n                                                  \"\".format(strkey,\n                                                            validated,\n                                                            regex.pattern))\n        validated.append(strkey)", "category": "Python"}, {"instruction": "def time_to_seconds(x):\n    \"\"\"Convert a time in a seconds sum\"\"\"\n", "input": "", "output": "    if isinstance(x, time):\n        return ((((x.hour * 60) + x.minute) * 60 + x.second) * 10**6 +\n                x.microsecond) / 10**6\n\n    if is_str(x):\n        return x\n    # Clamp to valid time\n    return x and max(0, min(x, 24 * 3600 - 10**-6))", "category": "Python"}, {"instruction": "def _eliminate_leafs(self, graph):\n        \"\"\"\n        Eliminate leaf objects - that are objects not referencing any other\n        objects in the list `graph`. Returns the list of objects without the\n        objects identified as leafs.\n        \"\"\"\n", "input": "", "output": "        result = []\n        idset = set([id(x) for x in graph])\n        for n in graph:\n            refset = set([id(x) for x in get_referents(n)])\n            if refset.intersection(idset):\n                result.append(n)\n        return result", "category": "Python"}, {"instruction": "def consume(self, routingKey, msg):\n        \"\"\"\n        Consumer for this (CaptureData) class. Gets the data sent from yieldMetricsValue and\n        sends it to the storage backends.\n        \"\"\"\n", "input": "", "output": "        build_data = msg['build_data']\n        builder_info = yield self.master.data.get((\"builders\", build_data['builderid']))\n\n        if self._builder_name_matches(builder_info) and self._data_name == msg['data_name']:\n            try:\n                ret_val = self._callback(msg['post_data'])\n            except Exception as e:\n                raise CaptureCallbackError(\"CaptureData failed for build %s of builder %s.\"\n                                           \" Exception generated: %s with message %s\"\n                                           % (build_data['number'], builder_info['name'],\n                                              type(e).__name__, str(e)))\n            post_data = ret_val\n            series_name = '%s-%s' % (builder_info['name'], self._data_name)\n            context = self._defaultContext(build_data, builder_info['name'])\n            yield self._store(post_data, series_name, context)", "category": "Python"}, {"instruction": "def normalize_array(lst):\n    \"\"\"Normalizes list\n\n    :param lst: Array of floats\n    :return: Normalized (in [0, 1]) input array\n    \"\"\"\n", "input": "", "output": "    np_arr = np.array(lst)\n    x_normalized = np_arr / np_arr.max(axis=0)\n    return list(x_normalized)", "category": "Python"}, {"instruction": "def _deserializeNT(data, glob):\n    \"\"\"\n    Deserialize special kinds of dicts from _serializeNT().\n    \"\"\"\n", "input": "", "output": "    if isinstance(data, list):\n        return [_deserializeNT(item, glob) for item in data]\n\n    elif isinstance(data, tuple):\n        return tuple(_deserializeNT(item, glob) for item in data)\n\n    elif isinstance(data, dict) and \"__nt_name\" in data:  # is namedtuple\n        class_name = data[\"__nt_name\"]\n        del data[\"__nt_name\"]\n\n        # given globals\n        if class_name in glob:\n            return glob[class_name](\n                **dict(zip(data, _deserializeNT(data.values(), glob)))\n            )\n\n        # \"local\" (package) globals\n        return globals()[class_name](\n            **dict(zip(data, _deserializeNT(data.values(), glob)))\n        )\n\n    elif isinstance(data, dict):\n        return {\n            key: _deserializeNT(data[key], glob)\n            for key in data\n        }\n\n    elif isinstance(data, unicode):\n        return data.encode(\"utf-8\")\n\n    return data", "category": "Python"}, {"instruction": "def get_split_adjusted_asof_idx(self, dates):\n        \"\"\"\n        Compute the index in `dates` where the split-adjusted-asof-date\n        falls. This is the date up to which, and including which, we will\n        need to unapply all adjustments for and then re-apply them as they\n        come in. After this date, adjustments are applied as normal.\n\n        Parameters\n        ----------\n        dates : pd.DatetimeIndex\n            The calendar dates over which the Pipeline is being computed.\n\n        Returns\n        -------\n        split_adjusted_asof_idx : int\n            The index in `dates` at which the data should be split.\n        \"\"\"\n", "input": "", "output": "        split_adjusted_asof_idx = dates.searchsorted(\n            self._split_adjusted_asof\n        )\n        # The split-asof date is after the date index.\n        if split_adjusted_asof_idx == len(dates):\n            split_adjusted_asof_idx = len(dates) - 1\n        elif self._split_adjusted_asof < dates[0].tz_localize(None):\n            split_adjusted_asof_idx = -1\n        return split_adjusted_asof_idx", "category": "Python"}, {"instruction": "def add(self, name, path=None, **kwargs):\n        \"\"\"add new project with given name and path to database\n        if the path is not given, current working directory will be taken\n        ...as default\n        \"\"\"\n", "input": "", "output": "        path = path or kwargs.pop('default_path', None)\n\n        if not self._path_is_valid(path):\n            return\n\n        if not self._is_unique(name, path):\n            p = Project.select().where(\n                (Project.name == name) |\n                (Project.path == path)\n            )[0]\n            self._print(self._ERROR_PROJECT_EXISTS.format(name, p.path), 'red')\n            return\n\n        Project.create(name=name, path=path)\n        self._print(self._SUCCESS_PROJECT_ADDED.format(name), 'green')", "category": "Python"}, {"instruction": "def append(self, item):\n        \"\"\"\n        Try to add an item to this element.\n\n        If the item is of the wrong type, and if this element has a sub-type,\n        then try to create such a sub-type and insert the item into that, instead.\n        \n        This happens recursively, so (in python-markup):\n          L [ u'Foo' ]\n        actually creates:\n          L [ LE [ P [ T [ u'Foo' ] ] ] ]\n\n        If that doesn't work, raise a TypeError.\n        \"\"\"\n", "input": "", "output": "\n        okay = True\n        if not isinstance(item, self.contentType):\n            if hasattr(self.contentType, 'contentType'):\n                try:\n                    item = self.contentType(content=[item])\n                except TypeError:\n                    okay = False\n            else:\n                okay = False\n                \n        if not okay:\n            raise TypeError(\"Wrong content type for %s: %s (%s)\" % (\n                self.__class__.__name__, repr(type(item)), repr(item)))\n\n        self.content.append(item)", "category": "Python"}, {"instruction": "def zunionstore(self, dest, keys, aggregate=None):\n        \"\"\"\n        Union multiple sorted sets specified by ``keys`` into\n        a new sorted set, ``dest``. Scores in the destination will be\n        aggregated based on the ``aggregate``, or SUM if none is provided.\n        \"\"\"\n", "input": "", "output": "        return self._zaggregate('ZUNIONSTORE', dest, keys, aggregate)", "category": "Python"}, {"instruction": "def top(self, container, ps_args=None):\n        \"\"\"\n        Display the running processes of a container.\n\n        Args:\n            container (str): The container to inspect\n            ps_args (str): An optional arguments passed to ps (e.g. ``aux``)\n\n        Returns:\n            (str): The output of the top\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n", "input": "", "output": "        u = self._url(\"/containers/{0}/top\", container)\n        params = {}\n        if ps_args is not None:\n            params['ps_args'] = ps_args\n        return self._result(self._get(u, params=params), True)", "category": "Python"}, {"instruction": "def _add_meta_value_to_xml_doc(self, doc, parent, obj):\n        \"\"\"Values in the meta element dict are converted to a BadgerFish-style\n            encoding (see _convert_hbf_meta_val_for_xml), so regardless of input_format,\n            we treat them as if they were BadgerFish.\n        \"\"\"\n", "input": "", "output": "        return self._add_subtree_to_xml_doc(doc,\n                                            parent,\n                                            subtree=obj,\n                                            key='meta',\n                                            key_order=None)", "category": "Python"}, {"instruction": "def _validate_all_tags_are_used(metadata):\n    \"\"\"Ensure all tags are used in some filter.\"\"\"\n", "input": "", "output": "    tag_names = set([tag_name for tag_name, _ in metadata.tags])\n    filter_arg_names = set()\n    for location, _ in metadata.registered_locations:\n        for filter_info in metadata.get_filter_infos(location):\n            for filter_arg in filter_info.args:\n                if is_tag_argument(filter_arg):\n                    filter_arg_names.add(get_directive_argument_name(filter_arg))\n\n    unused_tags = tag_names - filter_arg_names\n    if unused_tags:\n        raise GraphQLCompilationError(u'This GraphQL query contains @tag directives whose values '\n                                      u'are not used: {}. This is not allowed. Please either use '\n                                      u'them in a filter or remove them entirely.'\n                                      .format(unused_tags))", "category": "Python"}, {"instruction": "def get_setting(name, app=None):\n    \"\"\"\n    Returns the value for `name` settings (looks into `app` config, and into\n    DEFAULT_SETTINGS). Returns None if not set.\n\n    :param name: (str) name of a setting (e.g. FLASKS3_URL_STYLE)\n\n    :param app: Flask app instance\n\n    :return: setting value or None\n    \"\"\"\n", "input": "", "output": "    default_value = DEFAULT_SETTINGS.get(name, None)\n    return app.config.get(name, default_value) if app else default_value", "category": "Python"}, {"instruction": "def worker_thread(self):\n        \"\"\"\n        The primary worker thread--this thread pulls from the monitor queue and\n        runs the monitor, submitting the results to the handler queue.\n\n        Calls a sub method based on type of monitor.\n        \"\"\"\n", "input": "", "output": "        self.thread_debug(\"Starting monitor thread\")\n        while not self.thread_stopper.is_set():\n            mon = self.workers_queue.get()\n            self.thread_debug(\"Processing {type} Monitor: {title}\".format(**mon))\n            result = getattr(self, \"_worker_\" + mon['type'])(mon)\n            self.workers_queue.task_done()\n            self.results_queue.put({'type':mon['type'], 'result':result})", "category": "Python"}, {"instruction": "def dataset_list_files(self, dataset):\n        \"\"\" list files for a dataset\n             Parameters\n            ==========\n            dataset: the string identified of the dataset\n                     should be in format [owner]/[dataset-name]\n        \"\"\"\n", "input": "", "output": "        if dataset is None:\n            raise ValueError('A dataset must be specified')\n        if '/' in dataset:\n            self.validate_dataset_string(dataset)\n            dataset_urls = dataset.split('/')\n            owner_slug = dataset_urls[0]\n            dataset_slug = dataset_urls[1]\n        else:\n            owner_slug = self.get_config_value(self.CONFIG_NAME_USER)\n            dataset_slug = dataset\n        dataset_list_files_result = self.process_response(\n            self.datasets_list_files_with_http_info(\n                owner_slug=owner_slug, dataset_slug=dataset_slug))\n        return ListFilesResult(dataset_list_files_result)", "category": "Python"}, {"instruction": "def translate_row_col_to_index(self, row, col):\n        \"\"\"\n        Given a (row, col) tuple, return the corresponding index.\n        (Row and col params are 0-based.)\n\n        Negative row/col values are turned into zero.\n        \"\"\"\n", "input": "", "output": "        try:\n            result = self._line_start_indexes[row]\n            line = self.lines[row]\n        except IndexError:\n            if row < 0:\n                result = self._line_start_indexes[0]\n                line = self.lines[0]\n            else:\n                result = self._line_start_indexes[-1]\n                line = self.lines[-1]\n\n        result += max(0, min(col, len(line)))\n\n        # Keep in range. (len(self.text) is included, because the cursor can be\n        # right after the end of the text as well.)\n        result = max(0, min(result, len(self.text)))\n        return result", "category": "Python"}, {"instruction": "def next(self, timeout=None):\n        \"\"\"Return the next result value in the sequence. Raise\n        StopIteration at the end. Can raise the exception raised by\n        the Job\"\"\"\n", "input": "", "output": "        try:\n            apply_result = self._collector._get_result(self._idx, timeout)\n        except IndexError:\n            # Reset for next time\n            self._idx = 0\n            raise StopIteration\n        except:\n            self._idx = 0\n            raise\n        self._idx += 1\n        assert apply_result.ready()\n        return apply_result.get(0)", "category": "Python"}, {"instruction": "def run_hooks(self, packet):\n        \"\"\"\n        Run any additional functions that want to process this type of packet.\n        These can be internal parser hooks, or external hooks that process\n        information\n        \"\"\"\n", "input": "", "output": "\n        if packet.__class__ in self.internal_hooks:\n            self.internal_hooks[packet.__class__](packet)\n\n        if packet.__class__ in self.hooks:\n            self.hooks[packet.__class__](packet)", "category": "Python"}, {"instruction": "def get_user(username, **kwargs):\n    '''\n    Get username line from switch.\n\n    .. code-block: bash\n\n        salt '*' nxos.cmd get_user username=admin\n    '''\n", "input": "", "output": "    command = 'show run | include \"^username {0} password 5 \"'.format(username)\n    info = ''\n    info = show(command, **kwargs)\n    if isinstance(info, list):\n        info = info[0]\n    return info", "category": "Python"}, {"instruction": "def load_all_distributions(self):\n        \"\"\"Replace the :attr:`distributions` attribute with all scipy distributions\"\"\"\n", "input": "", "output": "        distributions = []\n        for this in dir(scipy.stats):\n            if \"fit\" in eval(\"dir(scipy.stats.\" + this +\")\"):\n                distributions.append(this)\n        self.distributions = distributions[:]", "category": "Python"}, {"instruction": "def set_debug(self, new):\n        \"\"\"\n        Set debug mode.\n\n        @param new: new value\n        @type new: bool\n\n        @return: old value\n        @rtype: bool\n        \"\"\"\n", "input": "", "output": "        if type(new) is not bool:\n            raise pyhsm.exception.YHSM_WrongInputType(\n                'new', bool, type(new))\n        old = self.debug\n        self.debug = new\n        self.stick.set_debug(new)\n        return old", "category": "Python"}, {"instruction": "def get_serializer_info(self, serializer):\n        \"\"\"\n        Given an instance of a serializer, return a dictionary of metadata\n        about its fields.\n        \"\"\"\n", "input": "", "output": "        if hasattr(serializer, 'child'):\n            # If this is a `ListSerializer` then we want to examine the\n            # underlying child serializer instance instead.\n            serializer = serializer.child\n        return self.get_fields(serializer.fields)", "category": "Python"}, {"instruction": "def total_write_throughput(self):\n        \"\"\" Combined write throughput of table and global indexes \"\"\"\n", "input": "", "output": "        total = self.write_throughput\n        for index in itervalues(self.global_indexes):\n            total += index.write_throughput\n        return total", "category": "Python"}, {"instruction": "def find_videos_by_related(self, video_id, count=20):\n        \"\"\"doc: http://open.youku.com/docs/doc?id=52\n        \"\"\"\n", "input": "", "output": "        url = 'https://openapi.youku.com/v2/videos/by_related.json'\n        params = {\n            'client_id': self.client_id,\n            'video_id': video_id,\n            'count': count\n        }\n        r = requests.get(url, params=params)\n        check_error(r)\n        return r.json()", "category": "Python"}, {"instruction": "def update(self, name, **kwargs):\n        \"\"\"\n        Update existing role.\n\n        http://www.keycloak.org/docs-api/3.4/rest-api/index.html#_roles_resource\n\n        :param str name: Name for the role\n        :param str description: (optional)\n        :param str id: (optional)\n        :param bool client_role: (optional)\n        :param bool composite: (optional)\n        :param object composites: (optional)\n        :param str container_id: (optional)\n        :param bool scope_param_required: (optional)\n        \"\"\"\n", "input": "", "output": "        payload = OrderedDict(name=name)\n\n        for key in ROLE_KWARGS:\n            if key in kwargs:\n                payload[to_camel_case(key)] = kwargs[key]\n\n        return self._client.put(\n            url=self._client.get_full_url(\n                self.get_path('single',\n                              realm=self._realm_name,\n                              id=self._client_id,\n                              role_name=self._role_name)\n            ),\n            data=json.dumps(payload, sort_keys=True)\n        )", "category": "Python"}, {"instruction": "def _is_bugged_tarfile(self):\n        \"\"\"\n        Check for tar file that tarfile library mistakenly reports as invalid.\n        Happens with tar files created on FAT systems.  See:\n        http://stackoverflow.com/questions/25552162/tarfile-readerror-file-could-not-be-opened-successfully\n        \"\"\"\n", "input": "", "output": "        try:\n            output = subprocess.check_output(['file', '-z', self.destination]).decode('utf8')\n            return 'tar archive' in output and 'gzip compressed data' in output\n        except subprocess.CalledProcessError:\n            return False", "category": "Python"}, {"instruction": "def number_format(self):\n        \"\"\"\n        The formatting template string that determines how a number in this\n        series is formatted, both in the chart and in the Excel spreadsheet;\n        for example '#,##0.0'. If not specified for this series, it is\n        inherited from the parent chart data object.\n        \"\"\"\n", "input": "", "output": "        number_format = self._number_format\n        if number_format is None:\n            return self._chart_data.number_format\n        return number_format", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Close the connection.\"\"\"\n", "input": "", "output": "        if not self.lock_disable and self.session_config_lock:\n            self._unlock()\n        self.device.close()", "category": "Python"}, {"instruction": "def set_attributes(self, doc, data_type):\n        \"\"\"\n        :param Optional[str] doc: Documentation string of alias.\n        :param data_type: The source data type referenced by the alias.\n        \"\"\"\n", "input": "", "output": "        self.raw_doc = doc\n        self.doc = doc_unwrap(doc)\n        self.data_type = data_type\n\n        # Make sure we don't have a cyclic reference.\n        # Since attributes are set one data type at a time, only the last data\n        # type to be populated in a cycle will be able to detect the cycle.\n        # Before that, the cycle will be broken by an alias with no populated\n        # source.\n        cur_data_type = data_type\n        while is_alias(cur_data_type):\n            cur_data_type = cur_data_type.data_type\n            if cur_data_type == self:\n                raise InvalidSpec(\n                    \"Alias '%s' is part of a cycle.\" % self.name,\n                    self._ast_node.lineno, self._ast_node.path)", "category": "Python"}, {"instruction": "def GetVolumeByIndex(self, volume_index):\n    \"\"\"Retrieves a specific volume based on the index.\n\n    Args:\n      volume_index (int): index of the volume.\n\n    Returns:\n      Volume: a volume or None if not available.\n    \"\"\"\n", "input": "", "output": "    if not self._is_parsed:\n      self._Parse()\n      self._is_parsed = True\n\n    if volume_index < 0 or volume_index >= len(self._volume_identifiers):\n      return None\n\n    volume_identifier = self._volume_identifiers[volume_index]\n    return self._volumes[volume_identifier]", "category": "Python"}, {"instruction": "def filename_formatters(self, data, row):\n        \"\"\"\n        Returns a dict containing the various filename formatter values\n\n        Values are gotten from the vaping data message as well as the\n        currently processed row in the message\n\n        - `data`: vaping message\n        - `row`: vaping message data row\n        \"\"\"\n", "input": "", "output": "\n        r = {\n            \"source\" : data.get(\"source\"),\n            \"field\" : self.field,\n            \"type\" : data.get(\"type\")\n        }\n        r.update(**row)\n        return r", "category": "Python"}, {"instruction": "def refresh_ip(self, context, cancellation_context, ports):\n        \"\"\"\n        Refresh IP Command, will refresh the ip of the vm and will update it on the resource\n        :param ResourceRemoteCommandContext context: the context the command runs on\n        :param cancellation_context:\n        :param list[string] ports: the ports of the connection between the remote resource and the local resource, NOT IN USE!!!\n        \"\"\"\n", "input": "", "output": "        resource_details = self._parse_remote_model(context)\n        # execute command\n        res = self.command_wrapper.execute_command_with_connection(context,\n                                                                   self.refresh_ip_command.refresh_ip,\n                                                                   resource_details,\n                                                                   cancellation_context,\n                                                                   context.remote_endpoints[\n                                                                       0].app_context.app_request_json)\n        return set_command_result(result=res, unpicklable=False)", "category": "Python"}, {"instruction": "def simxSetUIButtonProperty(clientID, uiHandle, uiButtonID, prop, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n", "input": "", "output": "    \n    return c_SetUIButtonProperty(clientID, uiHandle, uiButtonID, prop, operationMode)", "category": "Python"}, {"instruction": "def read_welcome_message(self):\n        '''Read the welcome message.\n\n        Coroutine.\n        '''\n", "input": "", "output": "        reply = yield from self._control_stream.read_reply()\n\n        self.raise_if_not_match(\n            'Server ready', ReplyCodes.service_ready_for_new_user, reply)", "category": "Python"}, {"instruction": "def write_config_file(self, f, comments):\n        \"\"\"This method write a sample file, with attributes, descriptions,\n        sample values, required flags, using the configuration object\n        properties.\n        \"\"\"\n", "input": "", "output": "        if comments:\n            f.write(\"#####################################\\n\")\n            f.write(\"# Section : \")\n            f.write(\"#\".join(self.get_representation()) + \"\\n\")\n            f.write(\"#####################################\\n\")\n        f.write(\"[\" + self._name + \"]\\n\")\n        if self._desc and comments:\n            f.write(\"# Description : \")\n            for i in self._desc.split('\\n'):\n                f.write(\"# \")\n                f.write(i)\n                f.write(\"\\n\")\n            f.write(\"\\n\")", "category": "Python"}, {"instruction": "def retrieveVals(self):\n        \"\"\"Retrieve values for graphs.\"\"\"\n", "input": "", "output": "        lighttpdInfo = LighttpdInfo(self._host, self._port,\n                                self._user, self._password, \n                                self._statuspath, self._ssl)\n        stats = lighttpdInfo.getServerStats()\n        if self.hasGraph('lighttpd_access'):\n            self.setGraphVal('lighttpd_access', 'reqs', stats['Total Accesses'])\n        if self.hasGraph('lighttpd_bytes'):\n            self.setGraphVal('lighttpd_bytes', 'bytes', \n                             stats['Total kBytes'] * 1000)\n        if self.hasGraph('lighttpd_servers'):\n            self.setGraphVal('lighttpd_servers', 'busy', stats['BusyServers'])\n            self.setGraphVal('lighttpd_servers', 'idle', stats['IdleServers'])\n            self.setGraphVal('lighttpd_servers', 'max', stats['MaxServers'])", "category": "Python"}, {"instruction": "def ReadStoredProcedure(self, sproc_link, options=None):\n        \"\"\"Reads a stored procedure.\n\n        :param str sproc_link:\n            The link to the stored procedure.\n        :param dict options:\n            The request options for the request.\n\n        :return:\n            The read Stored Procedure.\n        :rtype:\n            dict\n\n        \"\"\"\n", "input": "", "output": "        if options is None:\n            options = {}\n\n        path = base.GetPathFromLink(sproc_link)\n        sproc_id = base.GetResourceIdOrFullNameFromLink(sproc_link)\n        return self.Read(path, 'sprocs', sproc_id, None, options)", "category": "Python"}, {"instruction": "def DocToHelp(doc):\n  \"\"\"Takes a __doc__ string and reformats it as help.\"\"\"\n", "input": "", "output": "\n  # Get rid of starting and ending white space. Using lstrip() or even\n  # strip() could drop more than maximum of first line and right space\n  # of last line.\n  doc = doc.strip()\n\n  # Get rid of all empty lines.\n  whitespace_only_line = re.compile('^[ \\t]+$', re.M)\n  doc = whitespace_only_line.sub('', doc)\n\n  # Cut out common space at line beginnings.\n  doc = pep257.trim(doc)\n\n  # Just like this module's comment, comments tend to be aligned somehow.\n  # In other words they all start with the same amount of white space.\n  # 1) keep double new lines;\n  # 2) keep ws after new lines if not empty line;\n  # 3) all other new lines shall be changed to a space;\n  # Solution: Match new lines between non white space and replace with space.\n  doc = re.sub(r'(?<=\\S)\\n(?=\\S)', ' ', doc, flags=re.M)\n\n  return doc", "category": "Python"}, {"instruction": "def _parse_lifecycle_config(self, (response, xml_bytes)):\n        \"\"\"Parse a C{LifecycleConfiguration} XML document.\"\"\"\n", "input": "", "output": "        root = XML(xml_bytes)\n        rules = []\n\n        for content_data in root.findall(\"Rule\"):\n            id = content_data.findtext(\"ID\")\n            prefix = content_data.findtext(\"Prefix\")\n            status = content_data.findtext(\"Status\")\n            expiration = int(content_data.findtext(\"Expiration/Days\"))\n            rules.append(\n                LifecycleConfigurationRule(id, prefix, status, expiration))\n\n        return LifecycleConfiguration(rules)", "category": "Python"}, {"instruction": "def convert(self, template_name, parameter_values):\n        \"\"\"\n        Converts the given template to IAM-ready policy statement by substituting template parameters with the given\n        values.\n\n        :param template_name: Name of the template\n        :param parameter_values: Values for all parameters of the template\n        :return dict: Dictionary containing policy statement\n        :raises ValueError: If the given inputs don't represent valid template\n        :raises InsufficientParameterValues: If the parameter values don't have values for all required parameters\n        \"\"\"\n", "input": "", "output": "\n        if not self.has(template_name):\n            raise TemplateNotFoundException(template_name)\n\n        template = self.get(template_name)\n        return template.to_statement(parameter_values)", "category": "Python"}, {"instruction": "def read_lsm_channelcolors(fh):\n    \"\"\"Read LSM ChannelColors structure from file and return as dict.\"\"\"\n", "input": "", "output": "    result = {'Mono': False, 'Colors': [], 'ColorNames': []}\n    pos = fh.tell()\n    (size, ncolors, nnames,\n     coffset, noffset, mono) = struct.unpack('<IIIIII', fh.read(24))\n    if ncolors != nnames:\n        log.warning(\n            'read_lsm_channelcolors: invalid LSM ChannelColors structure')\n        return result\n    result['Mono'] = bool(mono)\n    # Colors\n    fh.seek(pos + coffset)\n    colors = fh.read_array('uint8', count=ncolors*4).reshape((ncolors, 4))\n    result['Colors'] = colors.tolist()\n    # ColorNames\n    fh.seek(pos + noffset)\n    buffer = fh.read(size - noffset)\n    names = []\n    while len(buffer) > 4:\n        size = struct.unpack('<I', buffer[:4])[0]\n        names.append(bytes2str(buffer[4:3+size]))\n        buffer = buffer[4+size:]\n    result['ColorNames'] = names\n    return result", "category": "Python"}, {"instruction": "def get_by_uri(cls, uri):\n        \"\"\"Get a file instance by URI.\"\"\"\n", "input": "", "output": "        assert uri is not None\n        return cls.query.filter_by(uri=uri).one_or_none()", "category": "Python"}, {"instruction": "def close(self, connection, *, commit=True):\n    \"\"\"Close the connection using the closer method passed to the constructor.\"\"\"\n", "input": "", "output": "    if commit:\n      connection.commit()\n    else:\n      connection.rollback()\n    self.closer(connection)", "category": "Python"}, {"instruction": "def insert(self, table, value, ignore=False, commit=True):\n        \"\"\"\n        Insert a dict into db.\n        :type table: string\n        :type value: dict\n        :type ignore: bool\n        :type commit: bool\n        :return: int. The row id of the insert.\n        \"\"\"\n", "input": "", "output": "        value_q, _args = self._value_parser(value, columnname=False)\n        _sql = ''.join(['INSERT', ' IGNORE' if ignore else '', ' INTO ', self._backtick(table),\n                        ' (', self._backtick_columns(value), ') VALUES (', value_q, ');'])\n\n        if self.debug:\n            return self.cur.mogrify(_sql, _args)\n\n        self.cur.execute(_sql, _args)\n        if commit:\n            self.conn.commit()\n        return self.cur.lastrowid", "category": "Python"}, {"instruction": "def serialize(self, queryset, **options):\n        \"\"\"\n        Serialize a queryset.\n        \"\"\"\n", "input": "", "output": "        self.options = options\n\n        self.stream = options.get(\"stream\", StringIO())\n        self.primary_key = options.get(\"primary_key\", None)\n        self.properties = options.get(\"properties\")\n        self.geometry_field = options.get(\"geometry_field\", \"geom\")\n        self.use_natural_keys = options.get(\"use_natural_keys\", False)\n        self.bbox = options.get(\"bbox\", None)\n        self.bbox_auto = options.get(\"bbox_auto\", None)\n        self.srid = options.get(\"srid\", GEOJSON_DEFAULT_SRID)\n        self.crs = options.get(\"crs\", True)\n\n        self.start_serialization()\n\n        if ValuesQuerySet is not None and isinstance(queryset, ValuesQuerySet):\n            self.serialize_values_queryset(queryset)\n\n        elif isinstance(queryset, list):\n            self.serialize_object_list(queryset)\n\n        elif isinstance(queryset, QuerySet):\n            self.serialize_queryset(queryset)\n\n        self.end_serialization()\n        return self.getvalue()", "category": "Python"}, {"instruction": "def get_file_flags(flags):\n    \"\"\"Show flag names and handle dict size.\n    \"\"\"\n", "input": "", "output": "    res = render_flags(flags & ~rf.RAR_FILE_DICTMASK, file_bits)\n\n    xf = (flags & rf.RAR_FILE_DICTMASK) >> 5\n    res += \",\" + file_parms[xf]\n    return res", "category": "Python"}, {"instruction": "def gdal2np_dtype(b):\n    \"\"\"\n    Get NumPy datatype that corresponds with GDAL RasterBand datatype\n    Input can be filename, GDAL Dataset, GDAL RasterBand, or GDAL integer dtype\n    \"\"\"\n", "input": "", "output": "    dt_dict = gdal_array.codes\n    if isinstance(b, str):\n        b = gdal.Open(b)\n    if isinstance(b, gdal.Dataset):\n        b = b.GetRasterBand(1)\n    if isinstance(b, gdal.Band):\n        b = b.DataType\n    if isinstance(b, int):\n        np_dtype = dt_dict[b]\n    else:\n        np_dtype = None\n        print(\"Input must be GDAL Dataset or RasterBand object\")\n    return np_dtype", "category": "Python"}, {"instruction": "def _load_from_tar(self, dtype_out_time, dtype_out_vert=False):\n        \"\"\"Load data save in tarball form on the file system.\"\"\"\n", "input": "", "output": "        path = os.path.join(self.dir_tar_out, 'data.tar')\n        utils.io.dmget([path])\n        with tarfile.open(path, 'r') as data_tar:\n            ds = xr.open_dataset(\n                data_tar.extractfile(self.file_name[dtype_out_time])\n            )\n            return ds[self.name]", "category": "Python"}, {"instruction": "def set_config_modify(dn=None, inconfig=None, hierarchical=False):\n    '''\n    The configConfMo method configures the specified managed object in a single subtree (for example, DN).\n    '''\n", "input": "", "output": "    ret = {}\n    cookie = logon()\n\n    # Declare if the search contains hierarchical results.\n    h = \"false\"\n    if hierarchical is True:\n        h = \"true\"\n\n    payload = '<configConfMo cookie=\"{0}\" inHierarchical=\"{1}\" dn=\"{2}\">' \\\n              '<inConfig>{3}</inConfig></configConfMo>'.format(cookie, h, dn, inconfig)\n    r = __utils__['http.query'](DETAILS['url'],\n                                data=payload,\n                                method='POST',\n                                decode_type='plain',\n                                decode=True,\n                                verify_ssl=False,\n                                raise_error=True,\n                                status=True,\n                                headers=DETAILS['headers'])\n\n    _validate_response_code(r['status'], cookie)\n\n    answer = re.findall(r'(<[\\s\\S.]*>)', r['text'])[0]\n    items = ET.fromstring(answer)\n    logout(cookie)\n    for item in items:\n        ret[item.tag] = prepare_return(item)\n    return ret", "category": "Python"}, {"instruction": "def _nth_of_year(self, nth, day_of_week):\n        \"\"\"\n        Modify to the given occurrence of a given day of the week\n        in the current year. If the calculated occurrence is outside,\n        the scope of the current year, then return False and no\n        modifications are made. Use the supplied consts\n        to indicate the desired day_of_week, ex. DateTime.MONDAY.\n\n        :type nth: int\n\n        :type day_of_week: int or None\n\n        :rtype: DateTime\n        \"\"\"\n", "input": "", "output": "        if nth == 1:\n            return self.first_of(\"year\", day_of_week)\n\n        dt = self.first_of(\"year\")\n        year = dt.year\n        for i in range(nth - (1 if dt.day_of_week == day_of_week else 0)):\n            dt = dt.next(day_of_week)\n\n        if year != dt.year:\n            return False\n\n        return self.on(self.year, dt.month, dt.day).start_of(\"day\")", "category": "Python"}, {"instruction": "def apply_plugins(plugin_names):\n    \"\"\"\n    This function should be used by code in the SQUAD core to trigger\n    functionality from plugins.\n\n    The ``plugin_names`` argument is list of plugins names to be used. Most\n    probably, you will want to pass the list of plugins enabled for a given\n    project, e.g.  ``project.enabled_plugins``.\n\n    Example::\n\n        from squad.core.plugins import apply_plugins\n\n        # ...\n\n        for plugin in apply_plugins(project.enabled_plugins):\n            plugin.method(...)\n\n    \"\"\"\n", "input": "", "output": "    if plugin_names is None:\n        return\n\n    for p in plugin_names:\n        try:\n            plugin = get_plugin_instance(p)\n            yield(plugin)\n        except PluginNotFound:\n            pass", "category": "Python"}, {"instruction": "def _escape_token(token, alphabet):\n  \"\"\"Escape away underscores and OOV characters and append '_'.\n\n  This allows the token to be expressed as the concatenation of a list\n  of subtokens from the vocabulary. The underscore acts as a sentinel\n  which allows us to invertibly concatenate multiple such lists.\n\n  Args:\n    token: A unicode string to be escaped.\n    alphabet: A set of all characters in the vocabulary's alphabet.\n\n  Returns:\n    escaped_token: An escaped unicode string.\n\n  Raises:\n    ValueError: If the provided token is not unicode.\n  \"\"\"\n", "input": "", "output": "  if not isinstance(token, six.text_type):\n    raise ValueError(\"Expected string type for token, got %s\" % type(token))\n\n  token = token.replace(u\"\\\\\", u\"\\\\\\\\\").replace(u\"_\", u\"\\\\u\")\n  ret = [c if c in alphabet and c != u\"\\n\" else r\"\\%d;\" % ord(c) for c in token]\n  return u\"\".join(ret) + \"_\"", "category": "Python"}, {"instruction": "def call(self, action, payload):\n        \"\"\"\n        Make an XML-RPC call to the server. This method will automatically\n        authenticate the call with self.api_key, if that is set.\n\n        returns: deferred that when fired returns a dict with data from this\n                 XML-RPC call.\n        \"\"\"\n", "input": "", "output": "        if self.api_key:\n            payload['Bugzilla_api_key'] = self.api_key\n        d = self.proxy.callRemote(action, payload)\n        d.addErrback(self._parse_errback)\n        return d", "category": "Python"}, {"instruction": "def n_orifices_per_row_max(self):\n        \"\"\"A bound on the number of orifices allowed in each row.\n        The distance between consecutive orifices must be enough to retain\n        structural integrity of the pipe.\n        \"\"\"\n", "input": "", "output": "        c = math.pi * pipe.ID_SDR(self.nom_diam_pipe, self.sdr)\n        b = self.orifice_diameter + self.s_orifice\n\n        return math.floor(c/b)", "category": "Python"}, {"instruction": "def settable(self):\n        \"\"\"return the subset of those options that are settable at any\n        time.\n\n        Settable options are in `versatile_options()`, but the\n        list might be incomplete.\n\n        \"\"\"\n", "input": "", "output": "        return CMAOptions([i for i in list(self.items())\n                                if i[0] in CMAOptions.versatile_options()])", "category": "Python"}, {"instruction": "def friendly_format(self):\n        \"\"\"Serialize to a format more suitable for displaying to end users.\"\"\"\n", "input": "", "output": "        if self.description is not None:\n            msg = self.description\n        else:\n            msg = 'errorCode: {} / detailCode: {}'.format(\n                self.errorCode, self.detailCode\n            )\n        return self._fmt(self.name, msg)", "category": "Python"}, {"instruction": "def get_plugin_modules(plugins):\n    \"\"\"\n    Get plugin modules from input strings\n    :param tuple plugins: a tuple of plugin names in str\n    \"\"\"\n", "input": "", "output": "    if not plugins:\n        raise MissingPluginNames(\"input plugin names are required\")\n\n    modules = []\n\n    for plugin in plugins:\n        short_name = PLUGIN_MAPPING.get(plugin.lower(), plugin.lower())\n        full_path = '%s%s' % (module_prefix, short_name)\n        modules.append(importlib.import_module(full_path))\n\n    return tuple(modules)", "category": "Python"}, {"instruction": "def util_mic_len(pkt):\n    ''' Calculate the length of the attribute value field '''\n", "input": "", "output": "    if (pkt.nwk_seclevel == 0):  # no encryption, no mic\n        return 0\n    elif (pkt.nwk_seclevel == 1):  # MIC-32\n        return 4\n    elif (pkt.nwk_seclevel == 2):  # MIC-64\n        return 8\n    elif (pkt.nwk_seclevel == 3):  # MIC-128\n        return 16\n    elif (pkt.nwk_seclevel == 4):  # ENC\n        return 0\n    elif (pkt.nwk_seclevel == 5):  # ENC-MIC-32\n        return 4\n    elif (pkt.nwk_seclevel == 6):  # ENC-MIC-64\n        return 8\n    elif (pkt.nwk_seclevel == 7):  # ENC-MIC-128\n        return 16\n    else:\n        return 0", "category": "Python"}, {"instruction": "def pull(self, url):\n        \"\"\"\n        Tries to pull changes from external location.\n        \"\"\"\n", "input": "", "output": "        url = self._get_url(url)\n        cmd = ['pull']\n        cmd.append(\"--ff-only\")\n        cmd.append(url)\n        cmd = ' '.join(cmd)\n        # If error occurs run_git_command raises RepositoryError already\n        self.run_git_command(cmd)", "category": "Python"}, {"instruction": "def check_path_consistency(self, resource):\n        '''Path arguments must be consistent for all methods.'''\n", "input": "", "output": "        msg = ('Method \"{}\" path variables {}) do not conform with the '\n               'resource subpath declaration ({}).')\n        errors = []\n        # If subpath is not set, it will be detected by another checker\n        if resource.subpath is None:\n            return errors\n        declared = sorted(self.path_params_regex.findall(resource.subpath))\n        for callback in resource.callbacks:\n            actual = sorted(utils.filter_annotations_by_ptype(\n                callback, Ptypes.path))\n            if declared == actual:\n                continue\n            errors.append(msg.format(\n                '{}.{}'.format(resource.__name__, callback.__name__),\n                actual, resource.subpath))\n        return errors", "category": "Python"}, {"instruction": "def _sign_operation(op):\n    \"\"\"Obtains a signature for an operation in a ReportRequest.\n\n    Args:\n       op (:class:`endpoints_management.gen.servicecontrol_v1_messages.Operation`): an\n         operation used in a `ReportRequest`\n\n    Returns:\n       string: a unique signature for that operation\n    \"\"\"\n", "input": "", "output": "    md5 = hashlib.md5()\n    md5.update(op.consumerId.encode('utf-8'))\n    md5.update(b'\\x00')\n    md5.update(op.operationName.encode('utf-8'))\n    if op.labels:\n        signing.add_dict_to_hash(md5, encoding.MessageToPyValue(op.labels))\n    return md5.digest()", "category": "Python"}, {"instruction": "def set_checkbox_value(w, value):\r\n    \"\"\"\r\n    Sets a checkbox's \"checked\" property + signal blocking + value tolerance\r\n\r\n    Args:\r\n        w: QCheckBox instance\r\n        value: something that can be converted to a bool\r\n    \"\"\"\n", "input": "", "output": "    save = w.blockSignals(True)\r\n    try:\r\n        w.setChecked(bool(value))\r\n    finally:\r\n        w.blockSignals(save)", "category": "Python"}, {"instruction": "def to_help(self):\n        \"\"\"\n        Returns a string that contains the 'global_info' text and the options.\n\n        :return: the generated help string\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        result = []\n        result.append(self.classname)\n        result.append(\"=\" * len(self.classname))\n        result.append(\"\")\n        result.append(\"DESCRIPTION\")\n        result.append(\"\")\n        result.append(self.global_info())\n        result.append(\"\")\n        result.append(\"OPTIONS\")\n        result.append(\"\")\n        options = javabridge.call(self.jobject, \"listOptions\", \"()Ljava/util/Enumeration;\")\n        enum = javabridge.get_enumeration_wrapper(options)\n        while enum.hasMoreElements():\n            opt = Option(enum.nextElement())\n            result.append(opt.synopsis)\n            result.append(opt.description)\n            result.append(\"\")\n        return '\\n'.join(result)", "category": "Python"}, {"instruction": "def update_policy(self,cspDefaultHeaders):\n\t\t\"\"\" add items to existing csp policies \"\"\"\n", "input": "", "output": "\t\ttry:\n\t\t\tself.check_valid(cspDefaultHeaders)\n\t\t\tif self.inputs is not None:\n\t\t\t\tfor p,l in self.inputs.items():\n\t\t\t\t\tcspDefaultHeaders[p] = cspDefaultHeaders[p]+ list(set(self.inputs[p]) - set(cspDefaultHeaders[p]))\n\t\t\t\treturn cspDefaultHeaders\n\t\t\telse:\n\t\t\t\treturn self.inputs\n\t\texcept Exception, e:\n\t\t\traise", "category": "Python"}, {"instruction": "def search(search, **kwargs):\n    \"\"\"\n    Search for models whose names matches the given pattern. Print the\n    results to stdout.\n\n    .. deprecated :: 1.0.0\n        `search` will be moved to ``andeshelp`` in future versions.\n\n    Parameters\n    ----------\n    search : str\n        Partial or full name of the model to search for\n\n    kwargs : dict\n        Other keyword arguments.\n\n    Returns\n    -------\n    list\n        The list of model names that match the given pattern.\n    \"\"\"\n", "input": "", "output": "\n    from .models import all_models\n    out = []\n\n    if not search:\n        return out\n\n    keys = sorted(list(all_models.keys()))\n\n    for key in keys:\n        vals = all_models[key]\n        val = list(vals.keys())\n        val = sorted(val)\n\n        for item in val:\n            if search.lower() in item.lower():\n                out.append(key + '.' + item)\n\n    if out:\n        print('Search result: <file.model> containing <{}>'\n              .format(search))\n        print(' '.join(out))\n    else:\n        print('No model containing <{:s}> found'.format(search))\n\n    return out", "category": "Python"}, {"instruction": "def itersubdirs(self, pattern=None, abspath=False):\n        \"\"\" Generator for all subdirs (except excluded).\n\n        :type pattern: str\n        :param pattern: Unix style (glob like/gitignore like) pattern\n\n        \"\"\"\n", "input": "", "output": "        if pattern is not None:\n            globster = Globster([pattern])\n        for root, dirs, files in self.walk():\n            for d in dirs:\n                if pattern is None or (pattern is not None and globster.match(d)):\n                    if abspath:\n                        yield os.path.join(root, d)\n                    else:\n                        yield self.relpath(os.path.join(root, d))", "category": "Python"}, {"instruction": "def get_tok(self, source):\n        \"\"\"Get string token from object\n\n        Strings are assumed to already be a token; if source or entry, see\n        if it is a persisted thing (\"original_tok\" is in its metadata), else\n        generate its own token.\n        \"\"\"\n", "input": "", "output": "        if isinstance(source, str):\n            return source\n\n        if isinstance(source, CatalogEntry):\n            return source._metadata.get('original_tok', source._tok)\n\n        if isinstance(source, DataSource):\n            return source.metadata.get('original_tok', source._tok)\n        raise IndexError", "category": "Python"}, {"instruction": "def validate_integer(option, value):\n    \"\"\"Validates that 'value' is an integer (or basestring representation).\n    \"\"\"\n", "input": "", "output": "    if isinstance(value, integer_types):\n        return value\n    elif isinstance(value, string_type):\n        try:\n            return int(value)\n        except ValueError:\n            raise ValueError(\"The value of %s must be \"\n                             \"an integer\" % (option,))\n    raise TypeError(\"Wrong type for %s, value must be an integer\" % (option,))", "category": "Python"}, {"instruction": "def confidence_interval_dichotomous(\n        point_estimate,\n        sample_size,\n        confidence=.95,\n        bias=False,\n        percentage=True,\n        **kwargs\n):\n    \"\"\"Dichotomous confidence interval from sample size and maybe a bias\"\"\"\n", "input": "", "output": "    alpha = ppf((confidence + 1) / 2, sample_size - 1)\n    p = point_estimate\n    if percentage:\n        p /= 100\n\n    margin = sqrt(p * (1 - p) / sample_size)\n    if bias:\n        margin += .5 / sample_size\n    if percentage:\n        margin *= 100\n\n    return (point_estimate - alpha * margin, point_estimate + alpha * margin)", "category": "Python"}, {"instruction": "def from_file(filename, srcLang, destLang, serverEndpoint=ServerEndpoint):\n    '''\n    Traslates the content of source file to destination language\n    :param filename: file whose contents needs translation\n    :param srcLang: name of language of input file\n    :param destLang: name of language of desired language\n    :param serverEndpoint: Tika server end point (Optional)\n    :return: translated content\n    '''\n", "input": "", "output": "    jsonOutput = doTranslate1(srcLang+':'+destLang, filename, serverEndpoint)\n    return jsonOutput[1]", "category": "Python"}, {"instruction": "def terminal_path_lengths_per_neurite(neurites, neurite_type=NeuriteType.all):\n    '''Get the path lengths to each terminal point per neurite in a collection'''\n", "input": "", "output": "    return list(sectionfunc.section_path_length(s)\n                for n in iter_neurites(neurites, filt=is_type(neurite_type))\n                for s in iter_sections(n, iterator_type=Tree.ileaf))", "category": "Python"}, {"instruction": "def log(self, string):\n        \"\"\"\n        appends input string to log file and sends it to log function (self.log_function)\n        Returns:\n\n        \"\"\"\n", "input": "", "output": "\n        self.log_data.append(string)\n        if self.log_function is None:\n            print(string)\n        else:\n            self.log_function(string)", "category": "Python"}, {"instruction": "def verify(self, func=None, *args, **kwargs):\n        \"\"\"Subtract the rhs from the lhs of the equation\n\n        Before the substraction, each side is expanded and any scalars are\n        simplified. If given, `func` with the positional arguments `args` and\n        keyword-arguments `kwargs` is applied to the result before returning\n        it.\n\n        You may complete the verification by checking the :attr:`is_zero`\n        attribute of the returned expression.\n        \"\"\"\n", "input": "", "output": "        res = (\n            self.lhs.expand().simplify_scalar() -\n            self.rhs.expand().simplify_scalar())\n        if func is not None:\n            return func(res, *args, **kwargs)\n        else:\n            return res", "category": "Python"}, {"instruction": "def Defaults(self,key):\n\t\t\"\"\"Returns default configurations for resources deployed to this group.\n\n\t\tIf specified key is not defined returns None.\n\n\t\t# {\"cpu\":{\"inherited\":false},\"memoryGB\":{\"inherited\":false},\"networkId\":{\"inherited\":false},\n\t\t# \"primaryDns\":{\"value\":\"172.17.1.26\",\"inherited\":true},\"secondaryDns\":{\"value\":\"172.17.1.27\",\"inherited\":true},\n\t\t# \"templateName\":{\"value\":\"WIN2012DTC-64\",\"inherited\":false}}\n\t\t\"\"\"\n", "input": "", "output": "\n\t\tif not hasattr(self,'defaults'):\n\t\t\tself.defaults = clc.v2.API.Call('GET','groups/%s/%s/defaults' % (self.alias,self.id), session=self.session)\n\t\ttry:\n\t\t\treturn(self.defaults[key]['value'])\n\t\texcept:\n\t\t\treturn(None)", "category": "Python"}, {"instruction": "def truncate(self, path, size):\n        \"\"\"\n        Change the size of the file specified by ``path``.  This usually\n        extends or shrinks the size of the file, just like the `~file.truncate`\n        method on Python file objects.\n\n        :param str path: path of the file to modify\n        :param int size: the new size of the file\n        \"\"\"\n", "input": "", "output": "        path = self._adjust_cwd(path)\n        self._log(DEBUG, \"truncate({!r}, {!r})\".format(path, size))\n        attr = SFTPAttributes()\n        attr.st_size = size\n        self._request(CMD_SETSTAT, path, attr)", "category": "Python"}, {"instruction": "def complain_on_err(self):\n        \"\"\"Complain about any parsing-related errors raised inside.\"\"\"\n", "input": "", "output": "        try:\n            yield\n        except ParseBaseException as err:\n            complain(self.make_parse_err(err, reformat=False, include_ln=False))\n        except CoconutException as err:\n            complain(err)", "category": "Python"}, {"instruction": "def escape_path(path):\n    \"\"\"Escape any invalid characters in HTTP URL, and uppercase all escapes.\"\"\"\n", "input": "", "output": "    # There's no knowing what character encoding was used to create URLs\n    # containing %-escapes, but since we have to pick one to escape invalid\n    # path characters, we pick UTF-8, as recommended in the HTML 4.0\n    # specification:\n    # http://www.w3.org/TR/REC-html40/appendix/notes.html#h-B.2.1\n    # And here, kind of: draft-fielding-uri-rfc2396bis-03\n    # (And in draft IRI specification: draft-duerst-iri-05)\n    # (And here, for new URI schemes: RFC 2718)\n    path = quote(path, HTTP_PATH_SAFE)\n    path = ESCAPED_CHAR_RE.sub(uppercase_escaped_char, path)\n    return path", "category": "Python"}, {"instruction": "def underlying_likelihood(self, binary_outcomes, modelparams, expparams):\n        \"\"\"\n        Given outcomes hypothesized for the underlying model, returns the likelihood\n        which which those outcomes occur.\n        \"\"\"\n", "input": "", "output": "        original_mps = modelparams[..., self._orig_mps_slice]\n        return self.underlying_model.likelihood(binary_outcomes, original_mps, expparams)", "category": "Python"}, {"instruction": "def make_copy(self, copy_to):\n        \"\"\"\n        Copy self attributes to the new object.\n\n        :param CFGBase copy_to: The target to copy to.\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        for attr, value in self.__dict__.items():\n            if attr.startswith('__') and attr.endswith('__'):\n                continue\n            setattr(copy_to, attr, value)", "category": "Python"}, {"instruction": "def delete_feature_base(dbpath, set_object, name):\n    \"\"\"\n    Generic function which deletes a feature from a database\n\n    Parameters\n    ----------\n    dbpath : string, path to SQLite database file\n    set_object : object (either TestSet or TrainSet) which is stored in the database\n    name : string, name of the feature to be deleted\n\n    Returns\n    -------\n    None\n    \"\"\"\n", "input": "", "output": "    engine = create_engine('sqlite:////' + dbpath)\n    session_cl = sessionmaker(bind=engine)\n    session = session_cl()\n    tmp_object = session.query(set_object).get(1)\n    if tmp_object.features is not None and name in tmp_object.features:\n        for i in session.query(set_object).order_by(set_object.id):\n            del i.features[name]\n    session.commit()\n    session.close()\n    return None", "category": "Python"}, {"instruction": "def add_spin_by_element(self, spins):\n        \"\"\"\n        Add spin states to a structure.\n\n        Args:\n            spisn (dict): Dict of spins associated with\n            elements or species, e.g. {\"Ni\":+5} or {\"Ni2+\":5}\n        \"\"\"\n", "input": "", "output": "        for site in self.sites:\n            new_sp = {}\n            for sp, occu in site.species.items():\n                sym = sp.symbol\n                oxi_state = getattr(sp, \"oxi_state\", None)\n                new_sp[Specie(sym, oxidation_state=oxi_state,\n                              properties={'spin': spins.get(str(sp), spins.get(sym, None))})] = occu\n            site.species = new_sp", "category": "Python"}, {"instruction": "def get_surveys(self):\n        \"\"\"Gets all surveys in account\n        \n        Args:\n            None\n            \n        Returns:\n            list: a list of all surveys\n        \"\"\"\n", "input": "", "output": "        payload = { \n            'Request': 'getSurveys',\n            'Format': 'JSON'\n            }\n        r = self._session.get(QUALTRICS_URL, params=payload)\n        output = r.json()\n        return output['Result']['Surveys']", "category": "Python"}, {"instruction": "def safe_unicode_stdin(string):\n    \"\"\"\n    Safely convert the given string to a Unicode string,\n    decoding using ``sys.stdin.encoding`` if needed.\n\n    If running from a frozen binary, ``utf-8`` encoding is assumed.\n\n    :param variant string: the byte string or Unicode string to convert\n    :rtype: string\n    \"\"\"\n", "input": "", "output": "    if string is None:\n        return None\n    if is_bytes(string):\n        if FROZEN:\n            return string.decode(\"utf-8\")\n        try:\n            return string.decode(sys.stdin.encoding)\n        except UnicodeDecodeError:\n            return string.decode(sys.stdin.encoding, \"replace\")\n        except:\n            return string.decode(\"utf-8\")\n    return string", "category": "Python"}, {"instruction": "def configure(self, config):\n        \"\"\"\n        Configures component by passing configuration parameters.\n\n        :param config: configuration parameters to be set.\n        \"\"\"\n", "input": "", "output": "        connections = ConnectionParams.many_from_config(config)\n        for connection in connections:\n            self._connections.append(connection)", "category": "Python"}, {"instruction": "def cuts_outside(self):\n        '''Report whether the enzyme cuts outside its recognition site.\n        Cutting at the very end of the site returns True.\n\n        :returns: Whether the enzyme will cut outside its recognition site.\n        :rtype: bool\n\n        '''\n", "input": "", "output": "        for index in self.cut_site:\n            if index < 0 or index > len(self.recognition_site) + 1:\n                return True\n        return False", "category": "Python"}, {"instruction": "def pretty_print(d, ind='', verbosity=0):\n    \"\"\"Pretty print a data dictionary from the bridge client\n    \"\"\"\n", "input": "", "output": "    assert isinstance(d, dict)\n    for k, v in sorted(d.items()):\n        str_base = '{} - [{}] {}'.format(ind, type(v).__name__, k)\n\n        if isinstance(v, dict):\n            print(str_base.replace('-', '+', 1))\n            pretty_print(v, ind=ind+'  ', verbosity=verbosity)\n            continue\n        elif isinstance(v, np.ndarray):\n            node = '{}, {}, {}'.format(str_base, v.dtype, v.shape)\n            if verbosity >= 2:\n                node += '\\n{}'.format(v)\n        elif isinstance(v, Sequence):\n            if v and isinstance(v, (list, tuple)):\n                itemtype = ' of ' + type(v[0]).__name__\n                pos = str_base.find(']')\n                str_base = str_base[:pos] + itemtype + str_base[pos:]\n            node = '{}, {}'.format(str_base, v)\n            if verbosity < 1 and len(node) > 80:\n                node = node[:77] + '...'\n        else:\n            node = '{}, {}'.format(str_base, v)\n        print(node)", "category": "Python"}, {"instruction": "def _user_input() -> str:\n    \"\"\"\n    A helper function which waits for user multi-line input.\n\n    :return: A string input by user, separated by ``'\\\\n'``.\n    \"\"\"\n", "input": "", "output": "\n    lines = []\n    try:\n        while True:\n            line = input()\n            if line != '':\n                lines.append(line)\n            else:\n                break\n    except (EOFError, KeyboardInterrupt):\n        return '\\n'.join(lines)", "category": "Python"}, {"instruction": "def gbk2big5(chars):\n    \"\"\"\n    Convert from gbk format to big5 representation of chars.\n    \"\"\"\n", "input": "", "output": "    out = ''\n    for char in chars:\n        if char in _cd.GBK:\n            out += _cd.BIG5[_cd.GBK.index(char)]\n        else:\n            out += char\n    return out", "category": "Python"}, {"instruction": "def write_peps(self, peps, reverse_seqs):\n        \"\"\"Writes peps to db. We can reverse to be able to look up\n        peptides that have some amino acids missing at the N-terminal.\n        This way we can still use the index.\n        \"\"\"\n", "input": "", "output": "        if reverse_seqs:\n            peps = [(x[0][::-1],) for x in peps]\n        cursor = self.get_cursor()\n        cursor.executemany(\n            'INSERT INTO known_searchspace(seqs) VALUES (?)', peps)\n        self.conn.commit()", "category": "Python"}, {"instruction": "def copy(self, name=None):\n        \"\"\"Make a deep copy of self.\n\n        Parameters\n        ----------\n        name : str\n            Name of the copied mesh.\n        \"\"\"\n", "input": "", "output": "        return Struct.copy(self, deep=True, name=name)", "category": "Python"}, {"instruction": "def company(anon, obj, field, val):\n    \"\"\"\n    Generates a random company name\n    \"\"\"\n", "input": "", "output": "    return anon.faker.company(field=field)", "category": "Python"}, {"instruction": "def format_private_ip_address(result):\n    '''\n    Formats the PrivateIPAddress object removing arguments that are empty\n    '''\n", "input": "", "output": "    from collections import OrderedDict\n    # Only display parameters that have content\n    order_dict = OrderedDict()\n    if result.ip_address is not None:\n        order_dict['ipAddress'] = result.ip_address\n    if result.subnet_resource_id is not None:\n        order_dict['subnetResourceId'] = result.subnet_resource_id\n\n    return order_dict", "category": "Python"}, {"instruction": "def _read_channel(channel, stream, start, duration):\n    \"\"\" Get channel using lalframe \"\"\"\n", "input": "", "output": "    channel_type = lalframe.FrStreamGetTimeSeriesType(channel, stream)\n    read_func = _fr_type_map[channel_type][0]\n    d_type = _fr_type_map[channel_type][1]\n    data = read_func(stream, channel, start, duration, 0)\n    return TimeSeries(data.data.data, delta_t=data.deltaT, epoch=start,\n                      dtype=d_type)", "category": "Python"}, {"instruction": "def handleHeader(self, key, value):\n        \"\"\"Handle header values.\"\"\"\n", "input": "", "output": "\n        if key == 'CIMError':\n            self.CIMError = urllib.parse.unquote(value)\n        if key == 'PGErrorDetail':\n            self.PGErrorDetail = urllib.parse.unquote(value)", "category": "Python"}, {"instruction": "def map(self, func, *columns):\n        \"\"\"\n        Map a function to rows, or to given columns\n        \"\"\"\n", "input": "", "output": "        if not columns:\n            return map(func, self.rows)\n        else:\n            values = (self.values(column) for column in columns)\n            result = [map(func, v) for v in values]\n            if len(columns) == 1:\n                return result[0]\n            else:\n                return result", "category": "Python"}, {"instruction": "def cumulative_value(self, slip_moment, mmax, mag_value, bbar, dbar):\n        '''\n        Returns the rate of events with M > mag_value\n\n        :param float slip_moment:\n            Product of slip (cm/yr) * Area (cm ^ 2) * shear_modulus (dyne-cm)\n        :param float mmax:\n            Maximum magnitude\n        :param float mag_value:\n            Magnitude value\n        :param float bbar:\n            \\bar{b} parameter (effectively = b * log(10.))\n        :param float dbar:\n            \\bar{d} parameter\n        '''\n", "input": "", "output": "        delta_m = mmax - mag_value\n        a_2 = self._get_a2(bbar, dbar, slip_moment, mmax)\n        return a_2 * (np.exp(bbar * delta_m) - 1.) * (delta_m > 0.0)", "category": "Python"}, {"instruction": "def unique(self, sort=False):\n        \"\"\" Return unique set of values in image \"\"\"\n", "input": "", "output": "        unique_vals = np.unique(self.numpy())\n        if sort:\n            unique_vals = np.sort(unique_vals)\n        return unique_vals", "category": "Python"}, {"instruction": "def create_stream_subscription(self, stream, on_data, timeout=60):\n        \"\"\"\n        Create a new stream subscription.\n\n        :param str stream: The name of the stream.\n        :param on_data: Function that gets called with  :class:`.StreamData`\n                        updates.\n        :param float timeout: The amount of seconds to wait for the request\n                              to complete.\n        :return: Future that can be used to manage the background websocket\n                 subscription\n        :rtype: .WebSocketSubscriptionFuture\n        \"\"\"\n", "input": "", "output": "        options = rest_pb2.StreamSubscribeRequest()\n        options.stream = stream\n\n        manager = WebSocketSubscriptionManager(\n            self._client, resource='stream', options=options)\n\n        # Represent subscription as a future\n        subscription = WebSocketSubscriptionFuture(manager)\n\n        wrapped_callback = functools.partial(\n            _wrap_callback_parse_stream_data, subscription, on_data)\n\n        manager.open(wrapped_callback, instance=self._instance)\n\n        # Wait until a reply or exception is received\n        subscription.reply(timeout=timeout)\n\n        return subscription", "category": "Python"}, {"instruction": "def ninteraction(df, drop=False):\n    \"\"\"\n    Compute a unique numeric id for each unique row in\n    a data frame. The ids start at 1 -- in the spirit\n    of `plyr::id`\n\n    Parameters\n    ----------\n    df : dataframe\n        Rows\n    drop : bool\n        If true, drop unused categorical levels leaving no\n        gaps in the assignments.\n\n    Returns\n    -------\n    out : list\n        Row asssignments.\n\n    Notes\n    -----\n    So far there has been no need not to drop unused levels\n    of categorical variables.\n    \"\"\"\n", "input": "", "output": "    if len(df) == 0:\n        return []\n\n    # Special case for single variable\n    if len(df.columns) == 1:\n        return _id_var(df[df.columns[0]], drop)\n\n    # Calculate individual ids\n    ids = df.apply(_id_var, axis=0)\n    ids = ids.reindex(columns=reversed(ids.columns))\n\n    # Calculate dimensions\n    def len_unique(x):\n        return len(np.unique(x))\n    ndistinct = ids.apply(len_unique, axis=0).values\n\n    combs = np.array(\n        np.hstack([1, np.cumprod(ndistinct[:-1])]))\n    mat = np.array(ids)\n    res = (mat - 1) @ combs.T + 1\n    res = np.array(res).flatten().tolist()\n\n    if drop:\n        return _id_var(res, drop)\n    else:\n        return res", "category": "Python"}, {"instruction": "def get_config(item, default=None):\n    \"\"\"\n    Use this function to get values from the config object.\n    \"\"\"\n", "input": "", "output": "    import giotto\n    return getattr(giotto._config, item, default) or default", "category": "Python"}, {"instruction": "def finddirs(pattern, path='.', exclude=None, recursive=True):\n    \"\"\"Find directories that match *pattern* in *path*\"\"\"\n", "input": "", "output": "    import fnmatch\n    import os\n    if recursive:\n        for root, dirnames, filenames in os.walk(path):\n            for pat in _to_list(pattern):\n                for dirname in fnmatch.filter(dirnames, pat):\n                    dirpath = join(abspath(root), dirname)\n                    for excl in _to_list(exclude):\n                        if excl and fnmatch.fnmatch(dirpath, excl):\n                            break\n                    else:\n                        yield dirpath\n    else:\n        for pat in _to_list(pattern):\n            for dirname in fnmatch.filter(listdirs(path), pat):\n                dirpath = join(abspath(path), dirname)\n                for excl in _to_list(exclude):\n                    if excl and fnmatch.fnmatch(dirpath, excl):\n                        break\n                else:\n                    yield dirpath", "category": "Python"}, {"instruction": "def dispatch(self):\n        \"\"\"Dispatch http request to registerd commands.\n        Example::\n\n            slack = Slack(app)\n            app.add_url_rule('/', view_func=slack.dispatch)\n        \"\"\"\n", "input": "", "output": "        from flask import request\n\n        method = request.method\n\n        data = request.args\n        if method == 'POST':\n            data = request.form\n\n        token = data.get('token')\n        team_id = data.get('team_id')\n        command = data.get('command') or data.get('trigger_word')\n\n        if isinstance(command, string_types):\n            command = command.strip().lstrip('/')\n\n        try:\n            self.validate(command, token, team_id, method)\n        except SlackError as e:\n            return self.response(e.msg)\n\n        func, _, _, kwargs = self._commands[(team_id, command)]\n        kwargs.update(data.to_dict())\n\n        return func(**kwargs)", "category": "Python"}, {"instruction": "def _apply_rewrites(date_classes, rules):\n    \"\"\"\n    Return a list of date elements by applying rewrites to the initial date element list\n    \"\"\"\n", "input": "", "output": "    for rule in rules:\n        date_classes = rule.execute(date_classes)\n\n    return date_classes", "category": "Python"}, {"instruction": "def fetch_album_name(self):\n        \"\"\"\n        Get the name of the album from lastfm.\n        \"\"\"\n", "input": "", "output": "        response = get_lastfm('track.getInfo', artist=self.artist,\n                              track=self.title)\n        if response:\n            try:\n                self.album = response['track']['album']['title']\n                logger.debug('Found album %s from lastfm', self.album)\n            except Exception:\n                logger.warning('Could not fetch album name for %s', self)\n        else:\n            logger.warning('Could not fetch album name for %s', self)", "category": "Python"}, {"instruction": "def touch2screen(w, h, o, x, y):\r\n    '''convert touch position'''\n", "input": "", "output": "    if o == 0:\r\n        return x, y\r\n    elif o == 1: # landscape-right\r\n        return y, w-x\r\n    elif o == 2: # upsidedown\r\n        return w-x, h-y\r\n    elif o == 3: # landscape-left\r\n        return h-y, x\r\n    return x, y", "category": "Python"}, {"instruction": "def fill_attrs(self, attrs):\n        \"\"\"Update the 'attrs' dict with generated ImplementationProperty.\"\"\"\n", "input": "", "output": "        for trname, attrname in self.transitions_at.items():\n\n            implem = self.implementations[trname]\n\n            if attrname in attrs:\n                conflicting = attrs[attrname]\n                if not self._may_override(implem, conflicting):\n                    raise ValueError(\n                        \"Can't override transition implementation %s=%r with %r\" %\n                        (attrname, conflicting, implem))\n\n            attrs[attrname] = implem\n        return attrs", "category": "Python"}, {"instruction": "def score(args):\n    \"\"\"\n    %prog score blastfile query.fasta A.ids\n\n    Add up the scores for each query seq. Go through the lines and for each\n    query sequence, add up the scores when subject is in each pile by A.ids.\n    \"\"\"\n", "input": "", "output": "    from jcvi.formats.base import SetFile\n    from jcvi.formats.fasta import Fasta\n\n    p = OptionParser(score.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    blastfile, fastafile, idsfile = args\n    ids = SetFile(idsfile)\n\n    blast = Blast(blastfile)\n    scores = defaultdict(int)\n    for b in blast:\n        query = b.query\n        subject = b.subject\n        if subject not in ids:\n            continue\n        scores[query] += b.score\n\n    logging.debug(\"A total of {0} ids loaded.\".format(len(ids)))\n\n    f = Fasta(fastafile)\n    for s in f.iterkeys_ordered():\n        sc = scores.get(s, 0)\n        print(\"\\t\".join((s, str(sc))))", "category": "Python"}, {"instruction": "def check_types(srctype, sinktype, linkMerge, valueFrom):\n    # type: (Any, Any, Optional[Text], Optional[Text]) -> Text\n    \"\"\"Check if the source and sink types are \"pass\", \"warning\", or \"exception\".\n    \"\"\"\n", "input": "", "output": "\n    if valueFrom is not None:\n        return \"pass\"\n    if linkMerge is None:\n        if can_assign_src_to_sink(srctype, sinktype, strict=True):\n            return \"pass\"\n        if can_assign_src_to_sink(srctype, sinktype, strict=False):\n            return \"warning\"\n        return \"exception\"\n    if linkMerge == \"merge_nested\":\n        return check_types({\"items\": _get_type(srctype), \"type\": \"array\"},\n                           _get_type(sinktype), None, None)\n    if linkMerge == \"merge_flattened\":\n        return check_types(merge_flatten_type(_get_type(srctype)), _get_type(sinktype), None, None)\n    raise WorkflowException(u\"Unrecognized linkMerge enum '{}'\".format(linkMerge))", "category": "Python"}, {"instruction": "def _get_vlan_length(buff):\n        \"\"\"Return the total length of VLAN tags in a given Ethernet buffer.\"\"\"\n", "input": "", "output": "        length = 0\n        begin = 12\n\n        while(buff[begin:begin+2] in (EtherType.VLAN.to_bytes(2, 'big'),\n                                      EtherType.VLAN_QINQ.to_bytes(2, 'big'))):\n            length += 4\n            begin += 4\n\n        return length", "category": "Python"}, {"instruction": "def execution_duration(self):\n        \"\"\"\n        Returns total BMDS execution time, in seconds.\n        \"\"\"\n", "input": "", "output": "        duration = None\n        if self.execution_start and self.execution_end:\n            delta = self.execution_end - self.execution_start\n            duration = delta.total_seconds()\n        return duration", "category": "Python"}, {"instruction": "def length(value, min=None, max=None):\n    \"\"\"\n    Return whether or not the length of given string is within a specified\n    range.\n\n    Examples::\n\n        >>> length('something', min=2)\n        True\n\n        >>> length('something', min=9, max=9)\n        True\n\n        >>> length('something', max=5)\n        ValidationFailure(func=length, ...)\n\n    :param value:\n        The string to validate.\n    :param min:\n        The minimum required length of the string. If not provided, minimum\n        length will not be checked.\n    :param max:\n        The maximum length of the string. If not provided, maximum length\n        will not be checked.\n\n    .. versionadded:: 0.2\n    \"\"\"\n", "input": "", "output": "    if (min is not None and min < 0) or (max is not None and max < 0):\n        raise AssertionError(\n            '`min` and `max` need to be greater than zero.'\n        )\n    return between(len(value), min=min, max=max)", "category": "Python"}, {"instruction": "def _make_random_string(length):\n    \"\"\"Returns a random lowercase, uppercase, alphanumerical string.\n\n    :param int length: The length in bytes of the string to generate.\n    \"\"\"\n", "input": "", "output": "    chars = string.ascii_lowercase + string.ascii_uppercase + string.digits\n    return ''.join(random.choice(chars) for x in range(length))", "category": "Python"}, {"instruction": "def get_jsapi_signature(self, prepay_id, timestamp=None, nonce_str=None):\n        \"\"\"\n        \u83b7\u53d6 JSAPI \u7b7e\u540d\n\n        :param prepay_id: \u7edf\u4e00\u4e0b\u5355\u63a5\u53e3\u8fd4\u56de\u7684 prepay_id \u53c2\u6570\u503c\n        :param timestamp: \u53ef\u9009\uff0c\u65f6\u95f4\u6233\uff0c\u9ed8\u8ba4\u4e3a\u5f53\u524d\u65f6\u95f4\u6233\n        :param nonce_str: \u53ef\u9009\uff0c\u968f\u673a\u5b57\u7b26\u4e32\uff0c\u9ed8\u8ba4\u81ea\u52a8\u751f\u6210\n        :return: \u7b7e\u540d\n        \"\"\"\n", "input": "", "output": "        data = {\n            'appId': self.sub_appid or self.appid,\n            'timeStamp': timestamp or to_text(int(time.time())),\n            'nonceStr': nonce_str or random_string(32),\n            'signType': 'MD5',\n            'package': 'prepay_id={0}'.format(prepay_id),\n        }\n        return calculate_signature(\n            data,\n            self._client.api_key if not self._client.sandbox else self._client.sandbox_api_key\n        )", "category": "Python"}, {"instruction": "def submatrix(matrix,i1,i2,j1,j2):\n    \"\"\"\n    returns the submatrix defined by the index bounds i1-i2 and j1-j2\n\n    Endpoints included!\n    \"\"\"\n", "input": "", "output": "\n    new = []\n    for i in range(i1,i2+1):\n        new.append(matrix[i][j1:j2+1])\n    return _n.array(new)", "category": "Python"}, {"instruction": "def get_logger(name, level=None):\n    \"\"\" Return a setup logger for the given name\n\n    :param name: The name for the logger. It is advised to use __name__. The logger name will be prepended by \\\"jb.\\\".\n    :type name: str\n    :param level: the logging level, e.g. logging.DEBUG, logging.INFO etc\n    :type level: int\n    :returns: Logger\n    :rtype: logging.Logger\n    :raises: None\n\n    The logger default level is defined in the constants :data:`jukeboxcore.constants.DEFAULT_LOGGING_LEVEL` but can be overwritten by the environment variable \\\"JUKEBOX_LOG_LEVEL\\\"\n\n    \"\"\"\n", "input": "", "output": "    log = logging.getLogger(\"jb.%s\" % name)\n    if level is not None:\n        log.setLevel(level)\n    return log", "category": "Python"}, {"instruction": "def get_primary_keys(conn, table: str, schema='public'):\n    \"\"\"Returns primary key columns for a specific table.\"\"\"\n", "input": "", "output": "\n    query = ", "category": "Python"}, {"instruction": "def _write_config(self):\n        \"\"\"Write the value to the config register in the device \"\"\"\n", "input": "", "output": "        normal_flag = False\n        if self._mode == MODE_NORMAL:\n            #Writes to the config register may be ignored while in Normal mode\n            normal_flag = True\n            self.mode = MODE_SLEEP #So we switch to Sleep mode first\n        self._write_register_byte(_BME280_REGISTER_CONFIG, self._config)\n        if normal_flag:\n            self.mode = MODE_NORMAL", "category": "Python"}, {"instruction": "def capture(self):\n\t\t\"\"\"\n\t\tCapture the payment of an existing, uncaptured, charge.\n\t\tThis is the second half of the two-step payment flow, where first you\n\t\tcreated a charge with the capture option set to False.\n\n\t\tSee https://stripe.com/docs/api#capture_charge\n\t\t\"\"\"\n", "input": "", "output": "\n\t\tcaptured_charge = self.api_retrieve().capture()\n\t\treturn self.__class__.sync_from_stripe_data(captured_charge)", "category": "Python"}, {"instruction": "def convert_uv(pinyin):\n    \"\"\"\u00fc \u8f6c\u6362\uff0c\u8fd8\u539f\u539f\u59cb\u7684\u97f5\u6bcd\n\n    \u00fc\u884c\u7684\u97f5\u8ddf\u58f0\u6bcdj\uff0cq\uff0cx\u62fc\u7684\u65f6\u5019\uff0c\u5199\u6210ju(\u5c45)\uff0cqu(\u533a)\uff0cxu(\u865a)\uff0c\n    \u00fc\u4e0a\u4e24\u70b9\u4e5f\u7701\u7565\uff1b\u4f46\u662f\u8ddf\u58f0\u6bcdn\uff0cl\u62fc\u7684\u65f6\u5019\uff0c\u4ecd\u7136\u5199\u6210n\u00fc(\u5973)\uff0cl\u00fc(\u5415)\u3002\n    \"\"\"\n", "input": "", "output": "    return UV_RE.sub(\n        lambda m: ''.join((m.group(1), UV_MAP[m.group(2)], m.group(3))),\n        pinyin)", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\" Closes the lid\"\"\"\n", "input": "", "output": "        self._geometry.lid_status = self._module.close()\n        self._ctx.deck.recalculate_high_z()\n        return self._geometry.lid_status", "category": "Python"}, {"instruction": "def alert(text='', title='', button=OK_TEXT, root=None, timeout=None):\n    \"\"\"Displays a simple message box with text and a single OK button. Returns the text of the button clicked on.\"\"\"\n", "input": "", "output": "    assert TKINTER_IMPORT_SUCCEEDED, 'Tkinter is required for pymsgbox'\n    return _buttonbox(msg=text, title=title, choices=[str(button)], root=root, timeout=timeout)", "category": "Python"}, {"instruction": "def safe_quotes(text, escape_single_quotes=False):\n    \"\"\"htmlify string\"\"\"\n", "input": "", "output": "    if isinstance(text, str):\n        safe_text = text.replace('\"', \"&quot;\")\n        if escape_single_quotes:\n            safe_text = safe_text.replace(\"'\", \"&#92;'\")\n        return safe_text.replace('True', 'true')\n    return text", "category": "Python"}, {"instruction": "def success_count(self):\n        \"\"\"\n        Amount of passed test cases in this list.\n\n        :return: integer\n        \"\"\"\n", "input": "", "output": "        return len([i for i, result in enumerate(self.data) if result.success])", "category": "Python"}, {"instruction": "def add_auth (self, user=None, password=None, pattern=None):\n        \"\"\"Add given authentication data.\"\"\"\n", "input": "", "output": "        if not user or not pattern:\n            log.warn(LOG_CHECK,\n            _(\"missing user or URL pattern in authentication data.\"))\n            return\n        entry = dict(\n            user=user,\n            password=password,\n            pattern=re.compile(pattern),\n        )\n        self[\"authentication\"].append(entry)", "category": "Python"}, {"instruction": "def _tzsp_guess_next_tag(payload):\n    \"\"\"\n    :return: class representing the next tag, Raw on error, None on missing payload  # noqa: E501\n    \"\"\"\n", "input": "", "output": "\n    if not payload:\n        warning('missing payload')\n        return None\n\n    tag_type = orb(payload[0])\n\n    try:\n        tag_class_definition = _TZSP_TAG_CLASSES[tag_type]\n\n    except KeyError:\n\n        return _tzsp_handle_unknown_tag(payload, tag_type)\n\n    if type(tag_class_definition) is not dict:\n        return tag_class_definition\n\n    try:\n        length = orb(payload[1])\n    except IndexError:\n        length = None\n\n    if not length:\n        warning('no tag length given - packet to short')\n        return Raw\n\n    try:\n        return tag_class_definition[length]\n    except KeyError:\n        warning('invalid tag length {} for tag type {}'.format(length, tag_type))  # noqa: E501\n        return Raw", "category": "Python"}, {"instruction": "def pathparts(self):\n        \"\"\"A list of the parts of the path, with the root node returning\n        an empty list.\n        \"\"\"\n", "input": "", "output": "        try:\n            parts = self.parent.pathparts()\n            parts.append(self.name)\n            return parts\n        except AttributeError:\n            return []", "category": "Python"}, {"instruction": "def register_model_converter(model, app):\n    \"\"\"Add url converter for model\n\n    Example:\n        class Student(db.model):\n            id = Column(Integer, primary_key=True)\n            name =  Column(String(50))\n\n        register_model_converter(Student)\n\n        @route('/classmates/<Student:classmate>')\n        def get_classmate_info(classmate):\n            pass\n\n    This only support model's have single primary key.\n    You need call this function before create view function.\n    \"\"\"\n", "input": "", "output": "    if hasattr(model, 'id'):\n        class Converter(_ModelConverter):\n            _model = model\n        app.url_map.converters[model.__name__] = Converter", "category": "Python"}, {"instruction": "def install_local(self):\n        \"\"\"Make a symlink in install folder to a local NApp.\n\n        Raises:\n            FileNotFoundError: If NApp is not found.\n\n        \"\"\"\n", "input": "", "output": "        folder = self._get_local_folder()\n        installed = self.installed_dir()\n        self._check_module(installed.parent)\n        installed.symlink_to(folder.resolve())", "category": "Python"}, {"instruction": "def get_fallbackservers(self, orgid, page=None):\n        \"\"\"Get Fallback server\"\"\"\n", "input": "", "output": "        opts = {}\n        if page:\n            opts['page'] = page\n        return self.api_call(\n            ENDPOINTS['fallbackservers']['list'],\n            dict(orgid=orgid), **opts)", "category": "Python"}, {"instruction": "def _get_from_java_home(self):\n        \"\"\"\n        Retrieves the Java library path according to the JAVA_HOME environment\n        variable\n\n        :return: The path to the JVM library, or None\n        \"\"\"\n", "input": "", "output": "        # Get the environment variable\n        java_home = os.getenv(\"JAVA_HOME\")\n        if java_home and os.path.exists(java_home):\n            # Get the real installation path\n            java_home = os.path.realpath(java_home)\n\n            # Cygwin has a bug in realpath\n            if not os.path.exists(java_home):\n                java_home = os.getenv(\"JAVA_HOME\")\n\n            # Look for the library file\n            return self.find_libjvm(java_home)", "category": "Python"}, {"instruction": "def main():\n    \"\"\" Send data \"\"\"\n", "input": "", "output": "\n    try:\n        args = get_config()\n        result = webpush(\n            args.sub_info,\n            data=args.data,\n            vapid_private_key=args.key,\n            vapid_claims=args.claims,\n            curl=args.curl,\n            content_encoding=args.encoding)\n        print(result)\n    except Exception as ex:\n        print(\"ERROR: {}\".format(ex))", "category": "Python"}, {"instruction": "def add_client(self, user_id=None):\n        \"\"\"\n        Adds current instance to public or private channel.\n        If user_id is specified it will be added to the private channel,\n        If user_id is not specified it will be added to the public one instead.\n        \"\"\"\n", "input": "", "output": "        if user_id is None:\n            # generate a random uuid if it's an unauthenticated client\n            self.channel = 'public'\n            user_id = uuid.uuid1().hex\n        else:\n            self.channel = 'private'\n        \n        self.id = user_id\n        self.channels[self.channel][self.id] = self\n        print 'Client connected to the %s channel.' % self.channel", "category": "Python"}, {"instruction": "def encode_dataset_coordinates(dataset):\n    \"\"\"Encode coordinates on the given dataset object into variable specific\n    and global attributes.\n\n    When possible, this is done according to CF conventions.\n\n    Parameters\n    ----------\n    dataset : Dataset\n        Object to encode.\n\n    Returns\n    -------\n    variables : dict\n    attrs : dict\n    \"\"\"\n", "input": "", "output": "    non_dim_coord_names = set(dataset.coords) - set(dataset.dims)\n    return _encode_coordinates(dataset._variables, dataset.attrs,\n                               non_dim_coord_names=non_dim_coord_names)", "category": "Python"}, {"instruction": "def split_qname(qname):\n        \"\"\"Split `qname` into namespace URI and local name\n\n        Return namespace and local name as a tuple. This is a static\n        method.\"\"\"\n", "input": "", "output": "        res = qname.split(YinParser.ns_sep)\n        if len(res) == 1:       # no namespace\n            return None, res[0]\n        else:\n            return res", "category": "Python"}, {"instruction": "def get_fun(returner, fun):\n    '''\n    Return info about last time fun was called on each minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ret.get_fun mysql network.interfaces\n    '''\n", "input": "", "output": "    returners = salt.loader.returners(__opts__, __salt__)\n    return returners['{0}.get_fun'.format(returner)](fun)", "category": "Python"}, {"instruction": "def process_belrdf(rdf_str, print_output=True):\n    \"\"\"Return a BelRdfProcessor for a BEL/RDF string.\n\n    Parameters\n    ----------\n    rdf_str : str\n        A BEL/RDF string to be processed. This will usually come from reading\n        a .rdf file.\n\n    Returns\n    -------\n    bp : BelRdfProcessor\n        A BelRdfProcessor object which contains INDRA Statements in\n        bp.statements.\n\n    Notes\n    -----\n    This function calls all the specific get_type_of_mechanism()\n    functions of the newly constructed BelRdfProcessor to extract\n    INDRA Statements.\n    \"\"\"\n", "input": "", "output": "    g = rdflib.Graph()\n    try:\n        g.parse(data=rdf_str, format='nt')\n    except ParseError as e:\n        logger.error('Could not parse rdf: %s' % e)\n        return None\n    # Build INDRA statements from RDF\n    bp = BelRdfProcessor(g)\n    bp.get_complexes()\n    bp.get_activating_subs()\n    bp.get_modifications()\n    bp.get_activating_mods()\n    bp.get_transcription()\n    bp.get_activation()\n    bp.get_conversions()\n\n    # Print some output about the process\n    if print_output:\n        bp.print_statement_coverage()\n        bp.print_statements()\n    return bp", "category": "Python"}, {"instruction": "def remove_node(self, node):\n        \"\"\"\n            Removes node from circle and rebuild it.\n        \"\"\"\n", "input": "", "output": "        try:\n            self._nodes.remove(node)\n            del self._weights[node]\n        except (KeyError, ValueError):\n            pass\n        self._hashring = dict()\n        self._sorted_keys = []\n\n        self._build_circle()", "category": "Python"}, {"instruction": "def truncate_table(self, tablename):\n        \"\"\"\n        Use 'TRUNCATE TABLE' to truncate the given table\n        \"\"\"\n", "input": "", "output": "        self.cursor.execute('TRUNCATE TABLE %s' %tablename)\n        self.db.commit()", "category": "Python"}, {"instruction": "def check_webserver_running(url=\"http://localhost:8800\", max_retries=30):\n    \"\"\"\n    Returns True if a given URL is responding within a given timeout.\n    \"\"\"\n", "input": "", "output": "\n    retry = 0\n    response = ''\n    success = False\n\n    while response != requests.codes.ok and retry < max_retries:\n        try:\n            response = requests.head(url, allow_redirects=True).status_code\n            success = True\n        except:\n            sleep(1)\n\n        retry += 1\n\n    if not success:\n        logging.warning('Unable to connect to %s within %s retries'\n                     % (url, max_retries))\n    return success", "category": "Python"}, {"instruction": "def apartial(coro, *args, **kwargs):\n    '''\n    Wraps a coroutine function with pre-defined arguments (including keyword\n    arguments).  It is an asynchronous version of :func:`functools.partial`.\n    '''\n", "input": "", "output": "\n    @functools.wraps(coro)\n    async def wrapped(*cargs, **ckwargs):\n        return await coro(*args, *cargs, **kwargs, **ckwargs)\n\n    return wrapped", "category": "Python"}, {"instruction": "def put(self, key, value, ttl=0):\n        \"\"\"\n        Associates the specified value with the specified key in this map. If the map previously contained a mapping for\n        the key, the old value is replaced by the specified value. If ttl is provided, entry will expire and get evicted\n        after the ttl.\n\n        :param key: (object), the specified key.\n        :param value: (object), the value to associate with the key.\n        :param ttl: (int), maximum time in seconds for this entry to stay, if not provided, the value configured on\n            server side configuration will be used(optional).\n        :return: (object), previous value associated with key or None if there was no mapping for key.\n        \"\"\"\n", "input": "", "output": "        check_not_none(key, \"key can't be None\")\n        check_not_none(key, \"value can't be None\")\n        key_data = self._to_data(key)\n        value_data = self._to_data(value)\n        return self._encode_invoke_on_key(replicated_map_put_codec, key_data, key=key_data, value=value_data,\n                                          ttl=to_millis(ttl))", "category": "Python"}, {"instruction": "def deserialize_rfc(attr):\n        \"\"\"Deserialize RFC-1123 formatted string into Datetime object.\n\n        :param str attr: response string to be deserialized.\n        :rtype: Datetime\n        :raises: DeserializationError if string format invalid.\n        \"\"\"\n", "input": "", "output": "        if isinstance(attr, ET.Element):\n            attr = attr.text\n        try:\n            date_obj = datetime.datetime.strptime(\n                attr, \"%a, %d %b %Y %H:%M:%S %Z\")\n            if not date_obj.tzinfo:\n                date_obj = date_obj.replace(tzinfo=TZ_UTC)\n        except ValueError as err:\n            msg = \"Cannot deserialize to rfc datetime object.\"\n            raise_with_traceback(DeserializationError, msg, err)\n        else:\n            return date_obj", "category": "Python"}, {"instruction": "def predict_real(self, X):\n        r\"\"\"Predict the probability of being 1 for each label.\n\n        Parameters\n        ----------\n        X : array-like, shape=(n_samples, n_features)\n            Feature vector.\n\n        Returns\n        -------\n        pred : numpy array, shape=(n_samples, n_labels)\n            Predicted probability of each label.\n        \"\"\"\n", "input": "", "output": "        X = np.asarray(X)\n        if self.clfs_ is None:\n            raise ValueError(\"Train before prediction\")\n        if X.shape[1] != self.n_features_:\n            raise ValueError('given feature size does not match')\n\n        pred = np.zeros((X.shape[0], self.n_labels_))\n        for i in range(self.n_labels_):\n            pred[:, i] = self.clfs_[i].predict_real(X)[:, 1]\n        return pred", "category": "Python"}, {"instruction": "def _set(self, instance, value, **kw):\n        \"\"\"Set the value of the field\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"ATFieldManager::set: value=%r\" % value)\n\n        # check field permission\n        if not self.field.checkPermission(\"write\", instance):\n            raise Unauthorized(\"You are not allowed to write the field {}\"\n                               .format(self.name))\n\n        # check if field is writable\n        if not self.field.writeable(instance):\n            raise Unauthorized(\"Field {} is read only.\"\n                               .format(self.name))\n\n        # id fields take only strings\n        if self.name == \"id\":\n            value = str(value)\n\n        # get the field mutator\n        mutator = self.field.getMutator(instance)\n\n        # Inspect function and apply *args and **kwargs if possible.\n        mapply(mutator, value, **kw)\n\n        return True", "category": "Python"}, {"instruction": "def index_data(self, data, index_name, doc_type):\n        \"\"\"Index data in Stub Indexer.\"\"\"\n", "input": "", "output": "\n        print 'ELS Stub Indexer getting called...'\n        print '%s %s %s %s' % (self, data, index_name, doc_type)", "category": "Python"}, {"instruction": "def serve(self, host='', port=8000, no_documentation=False, display_intro=True):\n        \"\"\"Runs the basic hug development server against this API\"\"\"\n", "input": "", "output": "        if no_documentation:\n            api = self.server(None)\n        else:\n            api = self.server()\n\n        if display_intro:\n            print(INTRO)\n\n        httpd = make_server(host, port, api)\n        print(\"Serving on {0}:{1}...\".format(host, port))\n        httpd.serve_forever()", "category": "Python"}, {"instruction": "def runPlugins(self, category, func, protocol, *args):\n        \"\"\"\n        Run the specified set of plugins against a given protocol.\n        \"\"\"\n", "input": "", "output": "        # Plugins are already sorted by priority\n        for plugin in self.plugins:\n            # If a plugin throws an exception, we should catch it gracefully.\n            try:\n                event_listener = getattr(plugin, func)\n            except AttributeError:\n                # If the plugin doesn't implement the event, do nothing\n                pass\n            else:\n                try:\n                    stop = event_listener(protocol, *args)\n                    if stop:\n                        break\n                except Exception:\n                    # A plugin should not be able to crash the bot.\n                    # Catch and log all errors.\n                    traceback.print_exc()", "category": "Python"}, {"instruction": "def cmd_set_homepos(self, args):\n        '''called when user selects \"Set Home\" on map'''\n", "input": "", "output": "        (lat, lon) = (self.click_position[0], self.click_position[1])\n        print(\"Setting home to: \", lat, lon)\n        self.master.mav.command_int_send(\n            self.settings.target_system, self.settings.target_component,\n            mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT_INT,\n            mavutil.mavlink.MAV_CMD_DO_SET_HOME,\n            1, # current\n            0, # autocontinue\n            0, # param1\n            0, # param2\n            0, # param3\n            0, # param4\n            int(lat*1e7), # lat\n            int(lon*1e7), # lon\n            0)", "category": "Python"}, {"instruction": "def make_image(location, size, fmt):\n    '''\n    Create a blank virtual machine image file of the specified size in\n    megabytes. The image can be created in any format supported by qemu\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' qemu_img.make_image /tmp/image.qcow 2048 qcow2\n        salt '*' qemu_img.make_image /tmp/image.raw 10240 raw\n    '''\n", "input": "", "output": "    if not os.path.isabs(location):\n        return ''\n    if not os.path.isdir(os.path.dirname(location)):\n        return ''\n    if not __salt__['cmd.retcode'](\n            'qemu-img create -f {0} {1} {2}M'.format(\n                fmt,\n                location,\n                size),\n                python_shell=False):\n        return location\n    return ''", "category": "Python"}, {"instruction": "def set(self, key, value):\n        \"\"\"Sets a hyperparameter.  Can be used to set an array of hyperparameters.\"\"\"\n", "input": "", "output": "        self.store[key]=value\n        return self.store", "category": "Python"}, {"instruction": "def _ExtractRequestSummaryFields(self, request, error=None):\n    \"\"\"Extract fields used in the summary logs.\n\n    Args:\n      request:  a urllib2.Request instance configured to make the request.\n      [optional]\n      error: a urllib2.HttpError instance used to retrieve error details.\n\n    Returns:\n      A dict containing the fields to be output in the summary logs.\n    \"\"\"\n", "input": "", "output": "    headers = request.headers\n    summary_fields = {\n        'server': request.get_full_url(),\n        'contentRange': headers['Content-range'],\n        'contentLength': headers['Content-length']\n    }\n\n    if error:\n      summary_fields['isError'] = True\n      summary_fields['errorMessage'] = error.reason\n    else:\n      summary_fields['isError'] = False\n\n    return summary_fields", "category": "Python"}, {"instruction": "def add(self, host_value):\n        \"\"\"Add the given value to the collection.\n\n        :param host: an ip address or a hostname\n        :raises InvalidHostError: raised when the given value\n        is not a valid ip address nor a hostname\n        \"\"\"\n", "input": "", "output": "        host_obj = self._host_factory(host_value)\n        if self._get_match(host_obj) is not None:\n            return\n        self._add_new(host_obj)", "category": "Python"}, {"instruction": "def collection(self, collection_id):\n        \"\"\"Create a sub-collection underneath the current document.\n\n        Args:\n            collection_id (str): The sub-collection identifier (sometimes\n                referred to as the \"kind\").\n\n        Returns:\n            ~.firestore_v1beta1.collection.CollectionReference: The\n            child collection.\n        \"\"\"\n", "input": "", "output": "        child_path = self._path + (collection_id,)\n        return self._client.collection(*child_path)", "category": "Python"}, {"instruction": "def make_end_to_end_distance_plot(nb_segments, end_to_end_distance, neurite_type):\n    '''Plot end-to-end distance vs number of segments'''\n", "input": "", "output": "    plt.figure()\n    plt.plot(nb_segments, end_to_end_distance)\n    plt.title(neurite_type)\n    plt.xlabel('Number of segments')\n    plt.ylabel('End-to-end distance')\n    plt.show()", "category": "Python"}, {"instruction": "def MGMT_ANNOUNCE_BEGIN(self, sAddr, xCommissionerSessionId, listChannelMask, xCount, xPeriod):\n        \"\"\"send MGMT_ANNOUNCE_BEGIN message to a given destination\n\n        Returns:\n            True: successful to send MGMT_ANNOUNCE_BEGIN message.\n            False: fail to send MGMT_ANNOUNCE_BEGIN message.\n        \"\"\"\n", "input": "", "output": "        print '%s call MGMT_ANNOUNCE_BEGIN' % self.port\n        channelMask = ''\n        channelMask = self.__ChannelMaskListToStr(listChannelMask)\n        try:\n            cmd = WPANCTL_CMD + 'commissioner announce-begin %s %s %s %s' % (channelMask, xCount, xPeriod, sAddr)\n            print cmd\n            return self.__sendCommand(cmd) != 'Fail'\n        except Exception, e:\n            ModuleHelper.WriteIntoDebugLogger('MGMT_ANNOUNCE_BEGIN() error: ' + str(e))", "category": "Python"}, {"instruction": "def set_pointing_label(self):\n\t    \"\"\"Let the label of the current pointing to the value in the plabel box\"\"\"\n", "input": "", "output": "\n\t    self.pointings[self.current]['label']['text']=w.plabel.get()\n\t    self.reset()", "category": "Python"}, {"instruction": "async def set_max_relative_mod(self, max_mod,\n                                   timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Override the maximum relative modulation from the thermostat.\n        Valid values are 0 through 100. Clear the setting by specifying\n        a non-numeric value.\n        Return the newly accepted value, '-' if a previous value was\n        cleared, or None on failure.\n\n        This method is a coroutine\n        \"\"\"\n", "input": "", "output": "        if isinstance(max_mod, int) and not 0 <= max_mod <= 100:\n            return None\n        cmd = OTGW_CMD_MAX_MOD\n        status = {}\n        ret = await self._wait_for_cmd(cmd, max_mod, timeout)\n        if ret not in ['-', None]:\n            ret = int(ret)\n        if ret == '-':\n            status[DATA_SLAVE_MAX_RELATIVE_MOD] = None\n        else:\n            status[DATA_SLAVE_MAX_RELATIVE_MOD] = ret\n        self._update_status(status)\n        return ret", "category": "Python"}, {"instruction": "def update_cookies(self, cookies: Optional[LooseCookies]) -> None:\n        \"\"\"Update request cookies header.\"\"\"\n", "input": "", "output": "        if not cookies:\n            return\n\n        c = SimpleCookie()\n        if hdrs.COOKIE in self.headers:\n            c.load(self.headers.get(hdrs.COOKIE, ''))\n            del self.headers[hdrs.COOKIE]\n\n        if isinstance(cookies, Mapping):\n            iter_cookies = cookies.items()\n        else:\n            iter_cookies = cookies  # type: ignore\n        for name, value in iter_cookies:\n            if isinstance(value, Morsel):\n                # Preserve coded_value\n                mrsl_val = value.get(value.key, Morsel())\n                mrsl_val.set(value.key, value.value, value.coded_value)  # type: ignore  # noqa\n                c[name] = mrsl_val\n            else:\n                c[name] = value  # type: ignore\n\n        self.headers[hdrs.COOKIE] = c.output(header='', sep=';').strip()", "category": "Python"}, {"instruction": "def get_proteins_from_psm(line):\n    \"\"\"From a line, return list of proteins reported by Mzid2TSV. When unrolled\n    lines are given, this returns the single protein from the line.\"\"\"\n", "input": "", "output": "    proteins = line[mzidtsvdata.HEADER_PROTEIN].split(';')\n    outproteins = []\n    for protein in proteins:\n        prepost_protein = re.sub('\\(pre=.*post=.*\\)', '', protein).strip()\n        outproteins.append(prepost_protein)\n    return outproteins", "category": "Python"}, {"instruction": "def user_session_info_output_user_role(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        user_session_info = ET.Element(\"user_session_info\")\n        config = user_session_info\n        output = ET.SubElement(user_session_info, \"output\")\n        user_role = ET.SubElement(output, \"user-role\")\n        user_role.text = kwargs.pop('user_role')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def validNormalizeAttributeValue(self, elem, name, value):\n        \"\"\"Does the validation related extra step of the normalization\n          of attribute values:  If the declared value is not CDATA,\n          then the XML processor must further process the normalized\n          attribute value by discarding any leading and trailing\n          space (#x20) characters, and by replacing sequences of\n           space (#x20) characters by single space (#x20) character. \"\"\"\n", "input": "", "output": "        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidNormalizeAttributeValue(self._o, elem__o, name, value)\n        return ret", "category": "Python"}, {"instruction": "def clean_account(self):\n        \"\"\"Ensure this is an income account\"\"\"\n", "input": "", "output": "        account = self.cleaned_data['account']\n        if not account:\n            return\n\n        if account.type != Account.TYPES.income:\n            raise ValidationError('Account must be an income account')\n\n        try:\n            account.housemate\n        except Housemate.DoesNotExist:\n            pass\n        else:\n            raise ValidationError('Account already has a housemate')\n\n        return account", "category": "Python"}, {"instruction": "def set_mlimits(self, row, column, min=None, max=None):\n        \"\"\"Set limits for the point meta (colormap).\n\n        Point meta values outside this range will be clipped.\n\n        :param min: value for start of the colormap.\n        :param max: value for end of the colormap.\n\n        \"\"\"\n", "input": "", "output": "        subplot = self.get_subplot_at(row, column)\n        subplot.set_mlimits(min, max)", "category": "Python"}, {"instruction": "def make_middleware(app=None, *args, **kw):\n    \"\"\" Given an app, return that app wrapped in iWSGIMiddleware \"\"\"\n", "input": "", "output": "    app = iWSGIMiddleware(app, *args, **kw)\n    return app", "category": "Python"}, {"instruction": "def cast(cls, value_type, value, visitor=None, **kwargs):\n        \"\"\"Cast is for visitors where you are visiting some random data\n        structure (perhaps returned by a previous ``VisitorPattern.visit()``\n        operation), and you want to convert back to the value type.\n\n        This function also takes positional arguments:\n\n            ``value_type=``\\ *RecordType*\n                The type to cast to.\n\n            ``value=``\\ *object*\n\n            ``visitor=``\\ *Visitor.Options*\n                Specifies the visitor options, which customizes the descent\n                and reduction.\n        \"\"\"\n", "input": "", "output": "        if visitor is None:\n            visitor = cls.Visitor(\n                cls.grok, cls.reverse, cls.collect, cls.produce,\n                **kwargs)\n\n        return cls.map(visitor, value, value_type)", "category": "Python"}, {"instruction": "def get_branch():\n    \"\"\"\n    GET THE CURRENT GIT BRANCH\n    \"\"\"\n", "input": "", "output": "    proc = Process(\"git status\", [\"git\", \"status\"])\n\n    try:\n        while True:\n            raw_line = proc.stdout.pop()\n            line = raw_line.decode('utf8').strip()\n            if line.startswith(\"On branch \"):\n                return line[10:]\n    finally:\n        try:\n            proc.join()\n        except Exception:\n            pass", "category": "Python"}, {"instruction": "def highlight(parser, token):\n    \"\"\"\n    Tag to put a highlighted source code <pre> block in your code.\n    This takes two arguments, the language and a little explaination message\n    that will be generated before the code.  The second argument is optional.\n\n    Your code will be fed through pygments so you can use any language it\n    supports.\n\n    Usage::\n\n      {% load highlighting %}\n      {% highlight 'python' 'Excerpt: blah.py' %}\n      def need_food(self):\n          print(\"Love is colder than death\")\n      {% endhighlight %}\n\n    \"\"\"\n", "input": "", "output": "    if not HAS_PYGMENTS:  # pragma: no cover\n        raise ImportError(\"Please install 'pygments' library to use highlighting.\")\n    nodelist = parser.parse(('endhighlight',))\n    parser.delete_first_token()\n    bits = token.split_contents()[1:]\n    if len(bits) < 1:\n        raise TemplateSyntaxError(\"'highlight' statement requires an argument\")\n    return CodeNode(bits[0], nodelist, *bits[1:])", "category": "Python"}, {"instruction": "def ProcessResponse(self, client_id, response):\n    \"\"\"Actually processes the contents of the response.\"\"\"\n", "input": "", "output": "    precondition.AssertType(client_id, Text)\n    downsampled = rdf_client_stats.ClientStats.Downsampled(response)\n\n    if data_store.AFF4Enabled():\n      urn = rdf_client.ClientURN(client_id).Add(\"stats\")\n\n      with aff4.FACTORY.Create(\n          urn, aff4_stats.ClientStats, token=self.token, mode=\"w\") as stats_fd:\n        # Only keep the average of all values that fall within one minute.\n        stats_fd.AddAttribute(stats_fd.Schema.STATS, downsampled)\n\n    if data_store.RelationalDBEnabled():\n      data_store.REL_DB.WriteClientStats(client_id, downsampled)\n\n    return downsampled", "category": "Python"}, {"instruction": "def _update_config_file(username, password, email, url, config_path):\n        \"\"\"Update the config file with the authorization.\"\"\"\n", "input": "", "output": "        try:\n            # read the existing config\n            config = json.load(open(config_path, \"r\"))\n        except ValueError:\n            config = dict()\n\n        if not config.get('auths'):\n            config['auths'] = dict()\n\n        if not config['auths'].get(url):\n            config['auths'][url] = dict()\n        encoded_credentials = dict(\n            auth=base64.b64encode(username + b':' + password),\n            email=email\n        )\n        config['auths'][url] = encoded_credentials\n        try:\n            json.dump(config, open(config_path, \"w\"), indent=5, sort_keys=True)\n        except Exception as exc:\n            raise exceptions.AnsibleContainerConductorException(\n                u\"Failed to write registry config to {0} - {1}\".format(config_path, exc)\n            )", "category": "Python"}, {"instruction": "def highlight_differences(s1, s2, color):\n    \"\"\"Highlight the characters in s2 that differ from those in s1.\"\"\"\n", "input": "", "output": "    ls1, ls2 = len(s1), len(s2)\n\n    diff_indices = [i for i, (a, b) in enumerate(zip(s1, s2)) if a != b]\n\n    print(s1)\n\n    if ls2 > ls1:\n        colorise.cprint('_' * (ls2-ls1), fg=color)\n    else:\n        print()\n\n    colorise.highlight(s2, indices=diff_indices, fg=color, end='')\n\n    if ls1 > ls2:\n        colorise.cprint('_' * (ls1-ls2), fg=color)\n    else:\n        print()", "category": "Python"}, {"instruction": "def _make_sync_method(name):\n  \"\"\"Helper to synthesize a synchronous method from an async method name.\n\n  Used by the @add_sync_methods class decorator below.\n\n  Args:\n    name: The name of the synchronous method.\n\n  Returns:\n    A method (with first argument 'self') that retrieves and calls\n    self.<name>, passing its own arguments, expects it to return a\n    Future, and then waits for and returns that Future's result.\n  \"\"\"\n", "input": "", "output": "\n  def sync_wrapper(self, *args, **kwds):\n    method = getattr(self, name)\n    future = method(*args, **kwds)\n    return future.get_result()\n\n  return sync_wrapper", "category": "Python"}, {"instruction": "def is_website_affected(self, website):\n        \"\"\" Tell if the website is affected by the domain change \"\"\"\n", "input": "", "output": "        if self.domain is None:\n            return True\n        if not self.include_subdomains:\n            return self.domain in website['subdomains']\n        else:\n            dotted_domain = \".\" + self.domain\n            for subdomain in website['subdomains']:\n                if subdomain == self.domain or subdomain.endswith(dotted_domain):\n                    return True\n            return False", "category": "Python"}, {"instruction": "def _get_migration_files(self, path):\n        \"\"\"\n        Get all of the migration files in a given path.\n\n        :type path: str\n\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        files = glob.glob(os.path.join(path, \"[0-9]*_*.py\"))\n\n        if not files:\n            return []\n\n        files = list(map(lambda f: os.path.basename(f).replace(\".py\", \"\"), files))\n\n        files = sorted(files)\n\n        return files", "category": "Python"}, {"instruction": "def system_update_keyspace(self, ks_def):\n    \"\"\"\n    updates properties of a keyspace. returns the new schema id.\n\n    Parameters:\n     - ks_def\n    \"\"\"\n", "input": "", "output": "    self._seqid += 1\n    d = self._reqs[self._seqid] = defer.Deferred()\n    self.send_system_update_keyspace(ks_def)\n    return d", "category": "Python"}, {"instruction": "def list(self, **kwargs):\n        \"\"\" https://api.slack.com/methods/groups.list\n        \"\"\"\n", "input": "", "output": "        if kwargs:\n            self.params.update(kwargs)\n        return FromUrl('https://slack.com/api/groups.list', self._requests)(data=self.params).get()", "category": "Python"}, {"instruction": "def rl_dashes(x):\n    \"\"\"\n    Replace dash to long/medium dashes\n    \"\"\"\n", "input": "", "output": "    patterns = (\n        # \u0442\u0438\u0440\u0435\n        (re.compile(u'(^|(.\\\\s))\\\\-\\\\-?(([\\\\s\\u202f].)|$)', re.MULTILINE|re.UNICODE), u'\\\\1\\u2014\\\\3'),\n        # \u0434\u0438\u0430\u043f\u0430\u0437\u043e\u043d\u044b \u043c\u0435\u0436\u0434\u0443 \u0446\u0438\u0444\u0440\u0430\u043c\u0438 - en dash\n        (re.compile(u'(\\\\d[\\\\s\\u2009]*)\\\\-([\\\\s\\u2009]*\\d)', re.MULTILINE|re.UNICODE), u'\\\\1\\u2013\\\\2'),\n        # TODO: \u0430 \u0447\u0442\u043e \u0441 \u043c\u0438\u043d\u0443\u0441\u043e\u043c?\n    )\n    return _sub_patterns(patterns, x)", "category": "Python"}, {"instruction": "def clean_comment_body(body):\n    \"\"\"Returns given comment HTML as plaintext.\n\n    Converts all HTML tags and entities within 4chan comments\n    into human-readable text equivalents.\n    \"\"\"\n", "input": "", "output": "    body = _parser.unescape(body)\n    body = re.sub(r'<a [^>]+>(.+?)</a>', r'\\1', body)\n    body = body.replace('<br>', '\\n')\n    body = re.sub(r'<.+?>', '', body)\n    return body", "category": "Python"}, {"instruction": "def roundTime(dt=None, roundTo=1):\n    \"\"\"Round a datetime object to any time period (in seconds)\n    dt : datetime.datetime object, default now.\n    roundTo : Closest number of seconds to round to, default 1 second.\n    Author: Thierry Husson 2012 - Use it as you want but don't blame me.\n    http://stackoverflow.com/questions/3463930/how-to-round-the-minute-of-a-datetime-object-python/10854034#10854034\n    \"\"\"\n", "input": "", "output": "    if dt == None : dt = datetime.now()\n    seconds = total_seconds(dt - dt.min)\n    # // is a floor division, not a comment on following line:\n    rounding = (seconds+roundTo/2) // roundTo * roundTo\n    return dt + timedelta(0,rounding-seconds,-dt.microsecond)", "category": "Python"}, {"instruction": "def loadCats(self, ids=[]):\n        \"\"\"\n        Load cats with the specified ids.\n        :param ids (int array)       : integer ids specifying cats\n        :return: cats (object array) : loaded cat objects\n        \"\"\"\n", "input": "", "output": "        if _isArrayLike(ids):\n            return [self.cats[id] for id in ids]\n        elif type(ids) == int:\n            return [self.cats[ids]]", "category": "Python"}, {"instruction": "def update_position(self, newpos):\n        '''update object position'''\n", "input": "", "output": "        if getattr(self, 'trail', None) is not None:\n            self.trail.update_position(newpos)\n        self.latlon = newpos.latlon\n        if hasattr(self, 'rotation'):\n            self.rotation = newpos.rotation", "category": "Python"}, {"instruction": "def tokenize_by_number(s):\n    \"\"\" splits a string into a list of tokens\n        each is either a string containing no numbers\n        or a float \"\"\"\n", "input": "", "output": "\n    r = find_number(s)\n\n    if r == None:\n        return [ s ]\n    else:\n        tokens = []\n        if r[0] > 0:\n            tokens.append(s[0:r[0]])\n        tokens.append( float(s[r[0]:r[1]]) )\n        if r[1] < len(s):\n            tokens.extend(tokenize_by_number(s[r[1]:]))\n        return tokens\n    assert False", "category": "Python"}, {"instruction": "def advance(self):\n        \"\"\"Increments the cursor position.\"\"\"\n", "input": "", "output": "        self.cursor += 1\n        if self.cursor >= len(self.raw):\n            self.char = None\n        else:\n            self.char = self.raw[self.cursor]", "category": "Python"}, {"instruction": "def set_in_selected(self, key, value):\n        \"\"\"Set the (key, value) for the selected server in the list.\"\"\"\n", "input": "", "output": "        # Static list then dynamic one\n        if self.screen.active_server >= len(self.static_server.get_servers_list()):\n            self.autodiscover_server.set_server(\n                self.screen.active_server - len(self.static_server.get_servers_list()),\n                key, value)\n        else:\n            self.static_server.set_server(self.screen.active_server, key, value)", "category": "Python"}, {"instruction": "def check_path_exists_foreach(path_params, func):\n    \"\"\" check that paths exist (unless we are in a transaction) \"\"\"\n", "input": "", "output": "    @wraps(func)\n    def wrapper(*args):\n        self = args[0]\n        params = args[1]\n\n        if not self.in_transaction:\n            for name in path_params:\n                value = getattr(params, name)\n                paths = value if type(value) == list else [value]\n                resolved = []\n                for path in paths:\n                    path = self.resolve_path(path)\n                    if not self.client.exists(path):\n                        self.show_output(\"Path %s doesn't exist\", path)\n                        return False\n                    resolved.append(path)\n\n                if type(value) == list:\n                    setattr(params, name, resolved)\n                else:\n                    setattr(params, name, resolved[0])\n\n        return func(self, params)\n\n    return wrapper", "category": "Python"}, {"instruction": "def detach_subnets(self, subnets):\n        \"\"\"\n        Detaches load balancer from one or more subnets.\n\n        :type subnets: string or List of strings\n        :param subnets: The name of the subnet(s) to detach.\n\n        \"\"\"\n", "input": "", "output": "        if isinstance(subnets, str) or isinstance(subnets, unicode):\n            subnets = [subnets]\n        new_subnets = self.connection.detach_lb_to_subnets(self.name, subnets)\n        self.subnets = new_subnets", "category": "Python"}, {"instruction": "def count_star(self) -> int:\n        \"\"\"\n        Implements the ``COUNT(*)`` specialization.\n        \"\"\"\n", "input": "", "output": "        count_query = (self.statement.with_only_columns([func.count()])\n                       .order_by(None))\n        return self.session.execute(count_query).scalar()", "category": "Python"}, {"instruction": "def has_hash_of(self, destpath, code, package):\n        \"\"\"Determine if a file has the hash of the code.\"\"\"\n", "input": "", "output": "        if destpath is not None and os.path.isfile(destpath):\n            with openfile(destpath, \"r\") as opened:\n                compiled = readfile(opened)\n            hashash = gethash(compiled)\n            if hashash is not None and hashash == self.comp.genhash(package, code):\n                return compiled\n        return None", "category": "Python"}, {"instruction": "def _handle_bugged_tarfile(self, destination, skip_top_level):\n        \"\"\"\n        Handle tar file that tarfile library mistakenly reports as invalid.\n        Happens with tar files created on FAT systems.  See:\n        http://stackoverflow.com/questions/25552162/tarfile-readerror-file-could-not-be-opened-successfully\n        \"\"\"\n", "input": "", "output": "        args = ['tar', '-xzf', self.destination, '-C', destination]\n        if skip_top_level:\n            args.extend(['--strip-components', '1'])\n        subprocess.check_call(args)", "category": "Python"}, {"instruction": "def get_imgid(self, img):\n        \"\"\"Obtain a unique identifier of the image.\n\n        Parameters\n        ----------\n        img : astropy.io.fits.HDUList\n\n        Returns\n        -------\n        str:\n             Identification of the image\n\n        \"\"\"\n", "input": "", "output": "        imgid = img.filename()\n\n        # More heuristics here...\n        # get FILENAME keyword, CHECKSUM, for example...\n        hdr = self.get_header(img)\n        if 'checksum' in hdr:\n            return hdr['checksum']\n\n        if 'filename' in hdr:\n            return hdr['filename']\n\n        if not imgid:\n            imgid = repr(img)\n\n        return imgid", "category": "Python"}, {"instruction": "def wrap_lines(content, length=80):\n    \"\"\"Wraps long lines to a maximum length of 80.\n\n    :param content: the content to wrap.\n    :param legnth: the maximum length to wrap the content.\n\n    :type content: str\n    :type length: int\n\n    :returns: a string containing the wrapped content.\n    :rtype: str\n\n    \"\"\"\n", "input": "", "output": "    return \"\\n\".join(textwrap.wrap(content, length, break_long_words=False))", "category": "Python"}, {"instruction": "def defverb(self, s1, p1, s2, p2, s3, p3):\n        \"\"\"\n        Set the verb plurals for s1, s2 and s3 to p1, p2 and p3 respectively.\n\n        Where 1, 2 and 3 represent the 1st, 2nd and 3rd person forms of the verb.\n\n        \"\"\"\n", "input": "", "output": "        self.checkpat(s1)\n        self.checkpat(s2)\n        self.checkpat(s3)\n        self.checkpatplural(p1)\n        self.checkpatplural(p2)\n        self.checkpatplural(p3)\n        self.pl_v_user_defined.extend((s1, p1, s2, p2, s3, p3))\n        return 1", "category": "Python"}, {"instruction": "def DeleteInstance(self, context, ports):\n        \"\"\"\n        Destroy Vm Command, will only destroy the vm and will not remove the resource\n\n        :param models.QualiDriverModels.ResourceRemoteCommandContext context: the context the command runs on\n        :param list[string] ports: the ports of the connection between the remote resource and the local resource, NOT IN USE!!!\n        \"\"\"\n", "input": "", "output": "        resource_details = self._parse_remote_model(context)\n        # execute command\n        res = self.command_wrapper.execute_command_with_connection(\n            context,\n            self.destroy_virtual_machine_command.DeleteInstance,\n            resource_details.vm_uuid,\n            resource_details.fullname)\n        return set_command_result(result=res, unpicklable=False)", "category": "Python"}, {"instruction": "def _pdf(self, xloc, dist, base, cache):\n        \"\"\"Probability density function.\"\"\"\n", "input": "", "output": "        return evaluation.evaluate_density(\n            dist, base**xloc, cache=cache)*base**xloc*numpy.log(base)", "category": "Python"}, {"instruction": "def contains_one_of(*fields):\n    \"\"\"Enables ensuring that one of multiple optional fields is set\"\"\"\n", "input": "", "output": "    message = 'Must contain any one of the following fields: {0}'.format(', '.join(fields))\n\n    def check_contains(endpoint_fields):\n        for field in fields:\n            if field in endpoint_fields:\n                return\n\n        errors = {}\n        for field in fields:\n            errors[field] = 'one of these must have a value'\n        return errors\n    check_contains.__doc__ = message\n    return check_contains", "category": "Python"}, {"instruction": "def set_minmax(field, render_kw=None, force=False):\n    \"\"\"\n    Returns *render_kw* with *min* and *max* set if validators use them.\n\n    Sets *min* and / or *max* keys if a `Length` or `NumberRange` validator is\n    using them.\n\n    ..note::\n\n        This won't change keys already present unless *force* is used.\n\n    \"\"\"\n", "input": "", "output": "    if render_kw is None:\n        render_kw = {}\n    for validator in field.validators:\n        if isinstance(validator, MINMAX_VALIDATORS):\n            if 'min' not in render_kw or force:\n                v_min = getattr(validator, 'min', -1)\n                if v_min not in (-1, None):\n                    render_kw['min'] = v_min\n            if 'max' not in render_kw or force:\n                v_max = getattr(validator, 'max', -1)\n                if v_max not in (-1, None):\n                    render_kw['max'] = v_max\n    return render_kw", "category": "Python"}, {"instruction": "def save(self, **kwargs):\n        \"\"\"\n        Save and return a list of object instances.\n        \"\"\"\n", "input": "", "output": "        validated_data = [\n            dict(list(attrs.items()) + list(kwargs.items()))\n            for attrs in self.validated_data\n        ]\n\n        if \"id\" in validated_data:\n            ModelClass = self.Meta.model\n\n            try:\n                self.instance = ModelClass.objects.get(id=validated_data[\"id\"])\n            except ModelClass.DoesNotExist:\n                pass\n\n        return super(VideohubVideoSerializer, self).save(**kwargs)", "category": "Python"}, {"instruction": "def delete_task(self, id, client=None):\n        \"\"\"Deletes a task from the current task queue.\n\n        If the task isn't found (backend 404), raises a\n        :class:`gcloud.exceptions.NotFound`.\n\n        :type id: string\n        :param id: A task name to delete.\n\n        :type client: :class:`gcloud.taskqueue.client.Client` or ``NoneType``\n        :param client: Optional. The client to use.  If not passed, falls back\n                       to the ``client`` stored on the current taskqueue.\n\n        :raises: :class:`gcloud.exceptions.NotFound`\n        \"\"\"\n", "input": "", "output": "        client = self._require_client(client)\n        task = Task(taskqueue=self, id=id)\n\n        # We intentionally pass `_target_object=None` since a DELETE\n        # request has no response value (whether in a standard request or\n        # in a batch request).\n        client.connection.api_request(method='DELETE', path=task.path, _target_object=None)", "category": "Python"}, {"instruction": "def _softmax(x, dim):\n    \"\"\"Computes softmax along a specified dim. Keras currently lacks this feature.\n    \"\"\"\n", "input": "", "output": "\n    if K.backend() == 'tensorflow':\n        import tensorflow as tf\n        return tf.nn.softmax(x, dim)\n    elif K.backend() is 'cntk':\n        import cntk\n        return cntk.softmax(x, dim)\n    elif K.backend() == 'theano':\n        # Theano cannot softmax along an arbitrary dim.\n        # So, we will shuffle `dim` to -1 and un-shuffle after softmax.\n        perm = np.arange(K.ndim(x))\n        perm[dim], perm[-1] = perm[-1], perm[dim]\n        x_perm = K.permute_dimensions(x, perm)\n        output = K.softmax(x_perm)\n\n        # Permute back\n        perm[dim], perm[-1] = perm[-1], perm[dim]\n        output = K.permute_dimensions(x, output)\n        return output\n    else:\n        raise ValueError(\"Backend '{}' not supported\".format(K.backend()))", "category": "Python"}, {"instruction": "def add(class_, name, value, sep=';'):\n\t\t\"\"\"\n\t\tAdd a value to a delimited variable, but only when the value isn't\n\t\talready present.\n\t\t\"\"\"\n", "input": "", "output": "\t\tvalues = class_.get_values_list(name, sep)\n\t\tif value in values:\n\t\t\treturn\n\t\tnew_value = sep.join(values + [value])\n\t\twinreg.SetValueEx(\n\t\t\tclass_.key, name, 0, winreg.REG_EXPAND_SZ, new_value)\n\t\tclass_.notify()", "category": "Python"}, {"instruction": "def export_key(keyids=None, secret=False, user=None, gnupghome=None):\n    '''\n    Export a key from the GPG keychain\n\n    keyids\n        The key ID(s) of the key(s) to be exported. Can be specified as a comma\n        separated string or a list. Anything which GnuPG itself accepts to\n        identify a key - for example, the key ID or the fingerprint could be\n        used.\n\n    secret\n        Export the secret key identified by the ``keyids`` information passed.\n\n    user\n        Which user's keychain to access, defaults to user Salt is running as.\n        Passing the user as ``salt`` will set the GnuPG home directory to the\n        ``/etc/salt/gpgkeys``.\n\n    gnupghome\n        Specify the location where GPG keyring and related files are stored.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gpg.export_key keyids=3FAD9F1E\n\n        salt '*' gpg.export_key keyids=3FAD9F1E secret=True\n\n        salt '*' gpg.export_key keyids=\"['3FAD9F1E','3FBD8F1E']\" user=username\n\n    '''\n", "input": "", "output": "    gpg = _create_gpg(user, gnupghome)\n\n    if isinstance(keyids, six.string_types):\n        keyids = keyids.split(',')\n    return gpg.export_keys(keyids, secret)", "category": "Python"}, {"instruction": "def reset(self):\n        \"\"\"Resets references.\"\"\"\n", "input": "", "output": "        self.indchar = None\n        self.comments = {}\n        self.refs = []\n        self.set_skips([])\n        self.docstring = \"\"\n        self.ichain_count = 0\n        self.tre_store_count = 0\n        self.case_check_count = 0\n        self.stmt_lambdas = []\n        if self.strict:\n            self.unused_imports = set()\n        self.bind()", "category": "Python"}, {"instruction": "def pack_key(self, owner_id='', kid=''):\n        \"\"\"\n        Find a key to be used for signing the Json Web Token\n\n        :param owner_id: Owner of the keys to chose from\n        :param kid: Key ID\n        :return: One key\n        \"\"\"\n", "input": "", "output": "        keys = pick_key(self.my_keys(owner_id, 'sig'), 'sig', alg=self.alg,\n                        kid=kid)\n\n        if not keys:\n            raise NoSuitableSigningKeys('kid={}'.format(kid))\n\n        return keys[0]", "category": "Python"}, {"instruction": "def remove(self):\n        \"\"\"todo: Docstring for remove\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"\")\n\n        rd = self.repo_dir\n\n        logger.debug(\"pkg path %s\", rd)\n        if not rd:\n            print(\n                \"unable to find pkg '%s'. %s\" % (self.name, did_u_mean(self.name))\n            )\n            return\n\n        # Does the repo have any uncommitted changes?\n        # Is the repo out of sync(needs a push?)\n\n        # Are you sure?\n        resp = input(self.term.red(\"Are you sure you want to remove the '%s' pkg? [y|N] \" %\n                                   self.name))\n\n        if resp == 'y' or resp == 'yes':\n            self.pr_atten('removing {}...', self.name)\n            shutil.rmtree(rd)", "category": "Python"}, {"instruction": "def pack_remb_fci(bitrate, ssrcs):\n    \"\"\"\n    Pack the FCI for a Receiver Estimated Maximum Bitrate report.\n\n    https://tools.ietf.org/html/draft-alvestrand-rmcat-remb-03\n    \"\"\"\n", "input": "", "output": "    data = b'REMB'\n    exponent = 0\n    mantissa = bitrate\n    while mantissa > 0x3ffff:\n        mantissa >>= 1\n        exponent += 1\n    data += pack('!BBH',\n                 len(ssrcs),\n                 (exponent << 2) | (mantissa >> 16),\n                 (mantissa & 0xffff))\n    for ssrc in ssrcs:\n        data += pack('!L', ssrc)\n    return data", "category": "Python"}, {"instruction": "def tokenize_list(self, text):\n        \"\"\"\n        Split a text into separate words.\n        \"\"\"\n", "input": "", "output": "        return [self.get_record_token(record) for record in self.analyze(text)]", "category": "Python"}, {"instruction": "def delete(self, user):\n        \"\"\"Delete a resource\"\"\"\n", "input": "", "output": "        if user:\n            can_delete = yield self.can_delete(user)\n        else:\n            can_delete = False\n\n        if not can_delete:\n            raise exceptions.Unauthorized('User may not delete the resource')\n\n        doc = {\n            '_id': self.id,\n            '_deleted': True\n        }\n\n        try:\n            doc['_rev'] = self._rev\n        except AttributeError:\n            pass\n\n        db = self.db_client()\n        yield db.save_doc(doc)\n\n        self._resource = doc", "category": "Python"}, {"instruction": "def post(self, endpoint: str, **kwargs) -> dict:\n        \"\"\"HTTP POST operation to API endpoint.\"\"\"\n", "input": "", "output": "\n        return self._request('POST', endpoint, **kwargs)", "category": "Python"}, {"instruction": "def answer(self):\n        \"\"\" Return the answer for the question from the validator.\n\n            This will ultimately only be called on the first validator if\n            multiple validators have been added.\n        \"\"\"\n", "input": "", "output": "        if isinstance(self.validator, list):\n            return self.validator[0].choice()\n        return self.validator.choice()", "category": "Python"}, {"instruction": "def submit(xml_root, submit_config, session, dry_run=None, **kwargs):\n    \"\"\"Submits data to the Polarion Importer.\"\"\"\n", "input": "", "output": "    properties.xunit_fill_testrun_id(xml_root, kwargs.get(\"testrun_id\"))\n    if dry_run is not None:\n        properties.set_dry_run(xml_root, dry_run)\n    xml_input = utils.etree_to_string(xml_root)\n\n    logger.info(\"Submitting data to %s\", submit_config.submit_target)\n    files = {\"file\": (\"results.xml\", xml_input)}\n    try:\n        response = session.post(submit_config.submit_target, files=files)\n    # pylint: disable=broad-except\n    except Exception as err:\n        logger.error(err)\n        response = None\n\n    return SubmitResponse(response)", "category": "Python"}, {"instruction": "def offline(f):\n    \"\"\" This decorator allows you to access ``ctx.peerplays`` which is\n        an instance of PeerPlays with ``offline=True``.\n    \"\"\"\n", "input": "", "output": "\n    @click.pass_context\n    @verbose\n    def new_func(ctx, *args, **kwargs):\n        ctx.obj[\"offline\"] = True\n        ctx.peerplays = PeerPlays(**ctx.obj)\n        ctx.blockchain = ctx.peerplays\n        set_shared_peerplays_instance(ctx.peerplays)\n        return ctx.invoke(f, *args, **kwargs)\n\n    return update_wrapper(new_func, f)", "category": "Python"}, {"instruction": "def render_and_write(template_dir, path, context):\n    \"\"\"Renders the specified template into the file.\n\n    :param template_dir: the directory to load the template from\n    :param path: the path to write the templated contents to\n    :param context: the parameters to pass to the rendering engine\n    \"\"\"\n", "input": "", "output": "    env = Environment(loader=FileSystemLoader(template_dir))\n    template_file = os.path.basename(path)\n    template = env.get_template(template_file)\n    log('Rendering from template: %s' % template.name, level=DEBUG)\n    rendered_content = template.render(context)\n    if not rendered_content:\n        log(\"Render returned None - skipping '%s'\" % path,\n            level=WARNING)\n        return\n\n    write(path, rendered_content.encode('utf-8').strip())\n    log('Wrote template %s' % path, level=DEBUG)", "category": "Python"}, {"instruction": "def _create_api_uri(self, *parts):\n        \"\"\"Internal helper for creating fully qualified endpoint URIs.\"\"\"\n", "input": "", "output": "        return urljoin(self.BASE_API_URI, '/'.join(imap(quote, parts)))", "category": "Python"}, {"instruction": "def compile(self, source_code, post_treatment=''.join):\n        \"\"\"Compile given source code.\n        Return object code, modified by given post treatment.\n        \"\"\"\n", "input": "", "output": "        # read structure\n        structure = self._structure(source_code)\n        values    = self._struct_to_values(structure, source_code)\n        # create object code, translated in targeted language\n        obj_code = langspec.translated(\n            structure, values, \n            self.target_lang_spec\n        )\n        # apply post treatment and return\n        return obj_code if post_treatment is None else post_treatment(obj_code)", "category": "Python"}, {"instruction": "def get_option(option_name, section_name=\"main\", default=_sentinel, cfg_file=cfg_file):\n    \"\"\" Returns a specific option specific in a config file\n\n    Arguments:\n        option_name  -- Name of the option (example host_name)\n        section_name -- Which section of the config (default: name)\n\n    examples:\n    >>> get_option(\"some option\", default=\"default result\")\n    'default result'\n    \"\"\"\n", "input": "", "output": "    defaults = get_defaults()\n\n    # As a quality issue, we strictly disallow looking up an option that does not have a default\n    # value specified in the code\n    #if option_name not in defaults.get(section_name, {}) and default == _sentinel:\n    #    raise ValueError(\"There is no default value for Option %s in section %s\" % (option_name, section_name))\n\n    # If default argument was provided, we set variable my_defaults to that\n    # otherwise use the global nago defaults\n    if default != _sentinel:\n        my_defaults = {option_name: default}\n    else:\n        my_defaults = defaults.get('section_name', {})\n\n    # Lets parse our configuration file and see what we get\n    parser = get_parser(cfg_file)\n    return parser.get(section_name, option_name, vars=my_defaults)", "category": "Python"}, {"instruction": "def result(self):\n        \"\"\" Return the result of the AMP (as a string)\"\"\"\n", "input": "", "output": "        ret = self.get('result')\n        if ret is not None:\n            ret = u(ret)\n        return ret", "category": "Python"}, {"instruction": "def get(self, sid):\n        \"\"\"\n        Constructs a WebhookContext\n\n        :param sid: A 34 character string that uniquely identifies this resource.\n\n        :returns: twilio.rest.messaging.v1.session.webhook.WebhookContext\n        :rtype: twilio.rest.messaging.v1.session.webhook.WebhookContext\n        \"\"\"\n", "input": "", "output": "        return WebhookContext(self._version, session_sid=self._solution['session_sid'], sid=sid, )", "category": "Python"}, {"instruction": "def rm(name, recurse=False, profile=None, **kwargs):\n    '''\n    Deletes a key from etcd\n\n    name\n        The etcd key name to remove, for example ``/foo/bar/baz``.\n\n    recurse\n        Optional, defaults to ``False``. If ``True`` performs a recursive delete.\n\n    profile\n        Optional, defaults to ``None``. Sets the etcd profile to use which has\n        been defined in the Salt Master config.\n\n        .. code-block:: yaml\n\n            my_etd_config:\n              etcd.host: 127.0.0.1\n              etcd.port: 4001\n    '''\n", "input": "", "output": "\n    rtn = {\n        'name': name,\n        'result': True,\n        'changes': {}\n    }\n\n    if not __salt__['etcd.get'](name, profile=profile, **kwargs):\n        rtn['comment'] = 'Key does not exist'\n        return rtn\n\n    if __salt__['etcd.rm'](name, recurse=recurse, profile=profile, **kwargs):\n        rtn['comment'] = 'Key removed'\n        rtn['changes'] = {\n            name: 'Deleted'\n        }\n    else:\n        rtn['comment'] = 'Unable to remove key'\n\n    return rtn", "category": "Python"}, {"instruction": "def get_assessment_offered_admin_session(self, proxy):\n        \"\"\"Gets the ``OsidSession`` associated with the assessment offered administration service.\n\n        arg:    proxy (osid.proxy.Proxy): a proxy\n        return: (osid.assessment.AssessmentOfferedAdminSession) - an\n                ``AssessmentOfferedAdminSession``\n        raise:  NullArgument - ``proxy`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented - ``supports_assessment_offered_admin()``\n                is ``false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_assessment_offered_admin()`` is ``true``.*\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_assessment_offered_admin():\n            raise errors.Unimplemented()\n        # pylint: disable=no-member\n        return sessions.AssessmentOfferedAdminSession(proxy=proxy, runtime=self._runtime)", "category": "Python"}, {"instruction": "def unassign_assessment_part_from_bank(self, assessment_part_id, bank_id):\n        \"\"\"Removes an ``AssessmentPart`` from an ``Bank``.\n\n        arg:    assessment_part_id (osid.id.Id): the ``Id`` of the\n                ``AssessmentPart``\n        arg:    bank_id (osid.id.Id): the ``Id`` of the ``Bank``\n        raise:  NotFound - ``assessment_part_id`` or ``bank_id`` not\n                found or ``assessment_part_id`` not assigned to\n                ``bank_id``\n        raise:  NullArgument - ``assessment_part_id`` or ``bank_id`` is\n                ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        mgr = self._get_provider_manager('ASSESSMENT', local=True)\n        lookup_session = mgr.get_bank_lookup_session(proxy=self._proxy)\n        lookup_session.get_bank(bank_id)  # to raise NotFound\n        self._unassign_object_from_catalog(assessment_part_id, bank_id)", "category": "Python"}, {"instruction": "def most_common(self, n=None):\n        '''List the n most common elements and their counts from the most\n        common to the least.  If n is None, then list all element counts.\n\n        >>> Counter('abracadabra').most_common(3)\n        [('a', 5), ('r', 2), ('b', 2)]\n\n        '''\n", "input": "", "output": "        if n is None:\n            return sorted(iteritems(self), key=itemgetter(1), reverse=True)\n        return nlargest(n, iteritems(self), key=itemgetter(1))", "category": "Python"}, {"instruction": "def repair(self, x, copy_if_changed=True, copy_always=False):\n        \"\"\"projects infeasible values on the domain bound, might be\n        overwritten by derived class \"\"\"\n", "input": "", "output": "        if copy_always:\n            x = array(x, copy=True)\n            copy = False\n        else:\n            copy = copy_if_changed\n        if self.bounds is None:\n            return x\n        for ib in [0, 1]:\n            if self.bounds[ib] is None:\n                continue\n            for i in rglen(x):\n                idx = min([i, len(self.bounds[ib]) - 1])\n                if self.bounds[ib][idx] is not None and \\\n                        (-1)**ib * x[i] < (-1)**ib * self.bounds[ib][idx]:\n                    if copy:\n                        x = array(x, copy=True)\n                        copy = False\n                    x[i] = self.bounds[ib][idx]", "category": "Python"}, {"instruction": "def _set_advertising_data(self, packet_type, data):\n        \"\"\"Set the advertising data for advertisements sent out by this bled112\n\n        Args:\n            packet_type (int): 0 for advertisement, 1 for scan response\n            data (bytearray): the data to set\n        \"\"\"\n", "input": "", "output": "\n        payload = struct.pack(\"<BB%ss\" % (len(data)), packet_type, len(data), bytes(data))\n        response = self._send_command(6, 9, payload)\n\n        result, = unpack(\"<H\", response.payload)\n        if result != 0:\n            return False, {'reason': 'Error code from BLED112 setting advertising data', 'code': result}\n\n        return True, None", "category": "Python"}, {"instruction": "def XanyKX(self):\n        \"\"\"\n        compute cross covariance for any and rest\n        \"\"\"\n", "input": "", "output": "        result = np.empty((self.P,self.F_any.shape[1],self.dof), order='C')\n        #This is trivially parallelizable:\n        for p in range(self.P):\n            FanyD = self.Fstar_any * self.D[:,p:p+1]\n            start = 0\n            #This is trivially parallelizable:\n            for term in range(self.len):\n                stop = start + self.F[term].shape[1]*self.A[term].shape[0]\n                result[p,:,start:stop] = self.XanyKX2_single_p_single_term(p=p, F1=FanyD, F2=self.Fstar[term], A2=self.Astar[term])\n                start = stop\n        return result", "category": "Python"}, {"instruction": "def get_file_md5sum(path):\n    \"\"\"Calculate the MD5 hash for a file.\"\"\"\n", "input": "", "output": "    with open(path, 'rb') as fh:\n        h = str(hashlib.md5(fh.read()).hexdigest())\n    return h", "category": "Python"}, {"instruction": "def undefine(self):\n        \"\"\"Undefine the Method.\n\n        Python equivalent of the CLIPS undefmethod command.\n\n        The object becomes unusable after this method has been called.\n\n        \"\"\"\n", "input": "", "output": "        if lib.EnvUndefmethod(self._env, self._gnc, self._idx) != 1:\n            raise CLIPSError(self._env)\n\n        self._env = None", "category": "Python"}, {"instruction": "def removeReferenceSet(self, referenceSet):\n        \"\"\"\n        Removes the specified referenceSet from this repository. This performs\n        a cascading removal of all references within this referenceSet.\n        However, it does not remove any of the ReadGroupSets or items that\n        refer to this ReferenceSet. These must be deleted before the\n        referenceSet can be removed.\n        \"\"\"\n", "input": "", "output": "        try:\n            q = models.Reference.delete().where(\n                    models.Reference.referencesetid == referenceSet.getId())\n            q.execute()\n            q = models.Referenceset.delete().where(\n                    models.Referenceset.id == referenceSet.getId())\n            q.execute()\n        except Exception:\n            msg = (\"Unable to delete reference set.  \"\n                   \"There are objects currently in the registry which are \"\n                   \"aligned against it.  Remove these objects before removing \"\n                   \"the reference set.\")\n            raise exceptions.RepoManagerException(msg)", "category": "Python"}, {"instruction": "def parse(str_, lsep=\",\", avsep=\":\", vssep=\",\", avssep=\";\"):\n    \"\"\"Generic parser\"\"\"\n", "input": "", "output": "    if avsep in str_:\n        return parse_attrlist(str_, avsep, vssep, avssep)\n    if lsep in str_:\n        return parse_list(str_, lsep)\n\n    return parse_single(str_)", "category": "Python"}, {"instruction": "def Clear(self):\n    \"\"\"Clears the breakpoint and releases all breakpoint resources.\n\n    This function is assumed to be called by BreakpointsManager. Therefore we\n    don't call CompleteBreakpoint from here.\n    \"\"\"\n", "input": "", "output": "    self._RemoveImportHook()\n    if self._cookie is not None:\n      native.LogInfo('Clearing breakpoint %s' % self.GetBreakpointId())\n      native.ClearConditionalBreakpoint(self._cookie)\n      self._cookie = None\n\n    self._completed = True", "category": "Python"}, {"instruction": "def _auth(self):\n        \"\"\"\n        Creates hash from api keys and returns all required parametsrs\n        \n        :returns:  str -- URL encoded query parameters containing \"ts\", \"apikey\", and \"hash\"\n        \"\"\"\n", "input": "", "output": "        ts = datetime.datetime.now().strftime(\"%Y-%m-%d%H:%M:%S\")\n        hash_string = hashlib.md5(\"%s%s%s\" % (ts, self.private_key, self.public_key)).hexdigest()\n        return \"ts=%s&apikey=%s&hash=%s\" % (ts, self.public_key, hash_string)", "category": "Python"}, {"instruction": "def AQLQuery(self, query, batchSize = 100, rawResults = False, bindVars = {}, options = {}, count = False, fullCount = False,\n                 json_encoder = None, **moreArgs) :\n        \"\"\"Set rawResults = True if you want the query to return dictionnaries instead of Document objects.\n        You can use **moreArgs to pass more arguments supported by the api, such as ttl=60 (time to live)\"\"\"\n", "input": "", "output": "        return AQLQuery(self, query, rawResults = rawResults, batchSize = batchSize, bindVars  = bindVars, options = options, count = count, fullCount = fullCount,\n                        json_encoder = json_encoder, **moreArgs)", "category": "Python"}, {"instruction": "def _normalize(self, flags):\n        \"\"\"Take any format of flags and turn it into a hex string.\"\"\"\n", "input": "", "output": "        norm = None\n        if isinstance(flags, MessageFlags):\n            norm = flags.bytes\n        elif isinstance(flags, bytearray):\n            norm = binascii.hexlify(flags)\n        elif isinstance(flags, int):\n            norm = bytes([flags])\n        elif isinstance(flags, bytes):\n            norm = binascii.hexlify(flags)\n        elif isinstance(flags, str):\n            flags = flags[0:2]\n            norm = binascii.hexlify(binascii.unhexlify(flags.lower()))\n        elif flags is None:\n            norm = None\n        else:\n            _LOGGER.warning('MessageFlags with unknown type %s: %r',\n                            type(flags), flags)\n        return norm", "category": "Python"}, {"instruction": "def finalize(self):\n        \"\"\"\n        Finalize the visualization to create an \"origin grid\" feel instead of\n        the default matplotlib feel. Set the title, remove spines, and label\n        the grid with components. This function also adds a legend from the\n        sizes if required.\n        \"\"\"\n", "input": "", "output": "        # Set the default title if a user hasn't supplied one\n        self.set_title(\"{} Intercluster Distance Map (via {})\".format(\n            self.estimator.__class__.__name__, self.embedding.upper()\n        ))\n\n        # Create the origin grid and minimalist display\n        self.ax.set_xticks([0])\n        self.ax.set_yticks([0])\n        self.ax.set_xticklabels([])\n        self.ax.set_yticklabels([])\n        self.ax.set_xlabel(\"PC2\")\n        self.ax.set_ylabel(\"PC1\")\n\n        # Make the legend by creating an inset axes that shows relative sizing\n        # based on the scoring metric supplied by the user.\n        if self.legend:\n            self._make_size_legend()\n\n        return self.ax", "category": "Python"}, {"instruction": "def make_hasher(algorithm_id):\n    \"\"\"Create a hashing object for the given signing algorithm.\"\"\"\n", "input": "", "output": "    if algorithm_id == 1:\n        return hashes.Hash(hashes.SHA1(), default_backend())\n    elif algorithm_id == 2:\n        return hashes.Hash(hashes.SHA384(), default_backend())\n    else:\n        raise ValueError(\"Unsupported signing algorithm: %s\" % algorithm_id)", "category": "Python"}, {"instruction": "def is_connected(T, directed=True):\n    r\"\"\"Check connectivity of the transition matrix.\n\n    Return true, if the input matrix is completely connected,\n    effectively checking if the number of connected components equals one.\n\n    Parameters\n    ----------\n    T : scipy.sparse matrix\n        Transition matrix\n    directed : bool, optional\n       Whether to compute connected components for a directed  or\n       undirected graph. Default is True.\n\n    Returns\n    -------\n    connected : boolean, returning true only if T is connected.\n\n\n    \"\"\"\n", "input": "", "output": "    nc = connected_components(T, directed=directed, connection='strong', \\\n                              return_labels=False)\n    return nc == 1", "category": "Python"}, {"instruction": "def close(self):\n    \"\"\"Closes the stream to which we are writing.\"\"\"\n", "input": "", "output": "    self.acquire()\n    try:\n      self.flush()\n      try:\n        # Do not close the stream if it's sys.stderr|stdout. They may be\n        # redirected or overridden to files, which should be managed by users\n        # explicitly.\n        if self.stream not in (sys.stderr, sys.stdout) and (\n            not hasattr(self.stream, 'isatty') or not self.stream.isatty()):\n          self.stream.close()\n      except ValueError:\n        # A ValueError is thrown if we try to run isatty() on a closed file.\n        pass\n      super(PythonHandler, self).close()\n    finally:\n      self.release()", "category": "Python"}, {"instruction": "def url_param2value(param):\n    \"\"\"\n    CONVERT URL QUERY PARAMETERS INTO DICT\n    \"\"\"\n", "input": "", "output": "    if param == None:\n        return Null\n    if param == None:\n        return Null\n\n    def _decode(v):\n        output = []\n        i = 0\n        while i < len(v):\n            c = v[i]\n            if c == \"%\":\n                d = hex2chr(v[i + 1:i + 3])\n                output.append(d)\n                i += 3\n            else:\n                output.append(c)\n                i += 1\n\n        output = text_type(\"\".join(output))\n        try:\n            return json2value(output)\n        except Exception:\n            pass\n        return output\n\n    query = Data()\n    for p in param.split('&'):\n        if not p:\n            continue\n        if p.find(\"=\") == -1:\n            k = p\n            v = True\n        else:\n            k, v = p.split(\"=\")\n            v = _decode(v)\n\n        u = query.get(k)\n        if u is None:\n            query[k] = v\n        elif is_list(u):\n            u += [v]\n        else:\n            query[k] = [u, v]\n\n    return query", "category": "Python"}, {"instruction": "def _doc_parms(cls):\n    \"\"\"Return a tuple of the doc parms.\"\"\"\n", "input": "", "output": "    axis_descr = \"{%s}\" % ', '.join(\"{0} ({1})\".format(a, i)\n                                    for i, a in enumerate(cls._AXIS_ORDERS))\n    name = (cls._constructor_sliced.__name__\n            if cls._AXIS_LEN > 1 else 'scalar')\n    name2 = cls.__name__\n    return axis_descr, name, name2", "category": "Python"}, {"instruction": "def low(data, queue=False, **kwargs):\n    '''\n    Execute a single low data call\n\n    This function is mostly intended for testing the state system and is not\n    likely to be needed in everyday usage.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' state.low '{\"state\": \"pkg\", \"fun\": \"installed\", \"name\": \"vi\"}'\n    '''\n", "input": "", "output": "    conflict = _check_queue(queue, kwargs)\n    if conflict is not None:\n        return conflict\n    try:\n        st_ = salt.state.State(__opts__, proxy=__proxy__)\n    except NameError:\n        st_ = salt.state.State(__opts__)\n    err = st_.verify_data(data)\n    if err:\n        __context__['retcode'] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR\n        return err\n    ret = st_.call(data)\n    if isinstance(ret, list):\n        __context__['retcode'] = salt.defaults.exitcodes.EX_STATE_COMPILER_ERROR\n    if __utils__['state.check_result'](ret):\n        __context__['retcode'] = salt.defaults.exitcodes.EX_STATE_FAILURE\n    return ret", "category": "Python"}, {"instruction": "def _normalize_string(raw_str):\n  \"\"\"Normalizes the string using tokenizer.encode.\n\n  Args:\n    raw_str: the input string\n\n  Returns:\n   A string which is ready to be tokenized using split()\n  \"\"\"\n", "input": "", "output": "  return \" \".join(\n      token.strip()\n      for token in tokenizer.encode(text_encoder.native_to_unicode(raw_str)))", "category": "Python"}, {"instruction": "def on_task_init(self, task_id, task):\n        \"\"\"Called before every task.\"\"\"\n", "input": "", "output": "        try:\n            is_eager = task.request.is_eager\n        except AttributeError:\n            is_eager = False\n        if not is_eager:\n            self.close_database()", "category": "Python"}, {"instruction": "def _harvest_lost_resources(self):\n        \"\"\"Return lost resources to pool.\"\"\"\n", "input": "", "output": "        with self._lock:\n            for i in self._unavailable_range():\n                rtracker = self._reference_queue[i]\n                if rtracker is not None and rtracker.available():\n                    self.put_resource(rtracker.resource)", "category": "Python"}, {"instruction": "def createEditor( self, parent, option, index ):\r\n        \"\"\"\r\n        Creates a new editor for the given index parented to the inputed widget.\r\n        \r\n        :param      parent | <QWidget>\r\n                    option | <QStyleOption>\r\n                    index  | <QModelIndex>\r\n        \r\n        :return     <QWidget> || None\r\n        \"\"\"\n", "input": "", "output": "        # determine the editor to use for the inputed index based on the\r\n        # table column\r\n        item   = self.parent().itemFromIndex(index)\r\n        column = self.column(index)\r\n        \r\n        # edit based on column preferences\r\n        if column and \\\r\n           not column.isReadOnly() and \\\r\n           isinstance(item, XOrbRecordItem):\r\n            plugin = self.plugin(column)\r\n            if not plugin:\r\n                return None\r\n            \r\n            return plugin.createEditor(parent, item.record(), column)\r\n        \r\n        return super(XOrbTreeWidgetDelegate, self).createEditor(parent,\r\n                                                                option,\r\n                                                                index)", "category": "Python"}, {"instruction": "def have_thumbnail(self, fitsimage, image):\n        \"\"\"Returns True if we already have a thumbnail version of this image\n        cached, False otherwise.\n        \"\"\"\n", "input": "", "output": "        chname = self.fv.get_channel_name(fitsimage)\n\n        # Look up our version of the thumb\n        idx = image.get('idx', None)\n        path = image.get('path', None)\n        if path is not None:\n            path = os.path.abspath(path)\n            name = iohelper.name_image_from_path(path, idx=idx)\n        else:\n            name = 'NoName'\n\n        # get image name\n        name = image.get('name', name)\n\n        thumbkey = self.get_thumb_key(chname, name, path)\n        with self.thmblock:\n            return thumbkey in self.thumb_dict", "category": "Python"}, {"instruction": "def _lerp(x, x0, x1, y0, y1):\n  \"\"\"Affinely map from [x0, x1] onto [y0, y1].\"\"\"\n", "input": "", "output": "  return y0 + (x - x0) * float(y1 - y0) / (x1 - x0)", "category": "Python"}, {"instruction": "def update_image(self):\n        \"\"\"\n        Set's the image's cmap.\n        \"\"\"\n", "input": "", "output": "        if self._image:\n            self._image.set_cmap(self.get_cmap())\n            _pylab.draw()", "category": "Python"}, {"instruction": "def _prune_node(self, node):\n        \"\"\"\n        Prune the given node if context exits cleanly.\n        \"\"\"\n", "input": "", "output": "        if self.is_pruning:\n            # node is mutable, so capture the key for later pruning now\n            prune_key, node_body = self._node_to_db_mapping(node)\n            should_prune = (node_body is not None)\n        else:\n            should_prune = False\n\n        yield\n\n        # Prune only if no exception is raised\n        if should_prune:\n            del self.db[prune_key]", "category": "Python"}, {"instruction": "def iter_geno_marker(self, markers, return_index=False):\n        \"\"\"Iterates over genotypes for a list of markers.\n\n        Args:\n            markers (list): The list of markers to iterate onto.\n            return_index (bool): Wether to return the marker's index or not.\n\n        Returns:\n            tuple: The name of the marker as a string, and its genotypes as a\n            :py:class:`numpy.ndarray` (additive format).\n\n        \"\"\"\n", "input": "", "output": "        if self._mode != \"r\":\n            raise UnsupportedOperation(\"not available in 'w' mode\")\n\n        # If string, we change to list\n        if isinstance(markers, str):\n            markers = [markers]\n\n        # Iterating over all markers\n        if return_index:\n            for marker in markers:\n                geno, seek = self.get_geno_marker(marker, return_index=True)\n                yield marker, geno, seek\n        else:\n            for marker in markers:\n                yield marker, self.get_geno_marker(marker)", "category": "Python"}, {"instruction": "def diff(self, diff):\n        \"\"\" Serialize to a dictionary. \"\"\"\n", "input": "", "output": "        if diff is None:\n            return None\n        return dict(\n            toVol=diff.toUUID,\n            fromVol=diff.fromUUID,\n            size=diff.size,\n            sizeIsEstimated=diff.sizeIsEstimated,\n        )", "category": "Python"}, {"instruction": "def get_pins(self):\n        \"\"\" Get a list containing references to all 16 pins of the chip.\n\n        :Example:\n\n        >>> expander = MCP23017I2C(gw)\n        >>> pins = expander.get_pins()\n        >>> pprint.pprint(pins)\n        [<GPIOPin A0 on MCP23017I2C>,\n         <GPIOPin A1 on MCP23017I2C>,\n         <GPIOPin A2 on MCP23017I2C>,\n         <GPIOPin A3 on MCP23017I2C>,\n         <GPIOPin A4 on MCP23017I2C>,\n         <GPIOPin A5 on MCP23017I2C>,\n         <GPIOPin A6 on MCP23017I2C>,\n         <GPIOPin B0 on MCP23017I2C>,\n         <GPIOPin B1 on MCP23017I2C>,\n         <GPIOPin B2 on MCP23017I2C>,\n         <GPIOPin B3 on MCP23017I2C>,\n         <GPIOPin B4 on MCP23017I2C>,\n         <GPIOPin B5 on MCP23017I2C>,\n         <GPIOPin B6 on MCP23017I2C>]\n\n\n        \"\"\"\n", "input": "", "output": "        result = []\n        for a in range(0, 7):\n            result.append(GPIOPin(self, '_action', {'pin': 'A{}'.format(a)}, name='A{}'.format(a)))\n        for b in range(0, 7):\n            result.append(GPIOPin(self, '_action', {'pin': 'B{}'.format(b)}, name='B{}'.format(b)))\n        return result", "category": "Python"}, {"instruction": "def include_theme_files(self, fragment):\n        \"\"\"\n        Gets theme configuration and renders theme css into fragment\n        \"\"\"\n", "input": "", "output": "        theme = self.get_theme()\n        if not theme or 'package' not in theme:\n            return\n\n        theme_package, theme_files = theme.get('package', None), theme.get('locations', [])\n        resource_loader = ResourceLoader(theme_package)\n        for theme_file in theme_files:\n            fragment.add_css(resource_loader.load_unicode(theme_file))", "category": "Python"}, {"instruction": "def extract_largest(self, inplace=False):\n        \"\"\"\n        Extract largest connected set in mesh.\n\n        Can be used to reduce residues obtained when generating an isosurface.\n        Works only if residues are not connected (share at least one point with)\n        the main component of the image.\n\n        Parameters\n        ----------\n        inplace : bool, optional\n            Updates mesh in-place while returning nothing.\n\n        Returns\n        -------\n        mesh : vtki.PolyData\n            Largest connected set in mesh\n\n        \"\"\"\n", "input": "", "output": "        mesh =  self.connectivity(largest=True)\n        if inplace:\n            self.overwrite(mesh)\n        else:\n            return mesh", "category": "Python"}, {"instruction": "def handle_term(self, _, __, tokens: ParseResults) -> ParseResults:\n        \"\"\"Handle BEL terms (the subject and object of BEL relations).\"\"\"\n", "input": "", "output": "        self.ensure_node(tokens)\n        return tokens", "category": "Python"}, {"instruction": "def create_module_rst_file(module_name):\n    \"\"\"Function for creating content in each .rst file for a module.\n\n    :param module_name: name of the module.\n    :type module_name: str\n\n    :returns: A content for auto module.\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "\n    return_text = 'Module:  ' + module_name\n    dash = '=' * len(return_text)\n    return_text += '\\n' + dash + '\\n\\n'\n    return_text += '.. automodule:: ' + module_name + '\\n'\n    return_text += '   :members:\\n\\n'\n\n    return return_text", "category": "Python"}, {"instruction": "def _get_toplevel(path, user=None, password=None, output_encoding=None):\n    '''\n    Use git rev-parse to return the top level of a repo\n    '''\n", "input": "", "output": "    return _git_run(\n        ['git', 'rev-parse', '--show-toplevel'],\n        cwd=path,\n        user=user,\n        password=password,\n        output_encoding=output_encoding)['stdout']", "category": "Python"}, {"instruction": "def get_node_bundle(manager, handle_id=None, node=None):\n    \"\"\"\n    :param manager: Neo4jDBSessionManager\n    :param handle_id: Unique id\n    :type handle_id: str|unicode\n    :param node: Node object\n    :type node: neo4j.v1.types.Node\n    :return: dict\n    \"\"\"\n", "input": "", "output": "    if not node:\n        node = get_node(manager, handle_id=handle_id, legacy=False)\n    d = {\n        'data': node.properties\n    }\n    labels = list(node.labels)\n    labels.remove('Node')  # All nodes have this label for indexing\n    for label in labels:\n        if label in META_TYPES:\n            d['meta_type'] = label\n            labels.remove(label)\n    d['labels'] = labels\n    return d", "category": "Python"}, {"instruction": "def add(self, effect=None, act=None, obj=None,\n            policy=None, policies=None):\n        \"\"\"Insert an individual (effect, action, object) triple or all\n        triples for a policy or list of policies.\n\n        \"\"\"\n", "input": "", "output": "        if policies is not None:\n            for p in policies:\n                self.add(policy=p)\n        elif policy is not None:\n            for e, a, o in policy:\n                self.add(e, a, o)\n        else:\n            objc = obj.components if obj is not None else []\n            self.tree[act.components + objc] = effect", "category": "Python"}, {"instruction": "def commentmap(self, cache=True):\n        \"\"\"return the comments from the docx, keyed to string id.\"\"\"\n", "input": "", "output": "        if self.__commentmap is not None and cache==True:\n            return self.__commentmap\n        else:\n            x = self.xml(src='word/comments.xml')\n            d = Dict()\n            if x is None: return d\n            for comment in x.root.xpath(\"w:comment\", namespaces=self.NS):\n                id = comment.get(\"{%(w)s}id\" % self.NS)\n                typ = comment.get(\"{%(w)s}type\" % self.NS)\n                d[id] = Dict(id=id, type=typ, elem=comment)\n            if cache==True: self.__commentmap = d\n            return d", "category": "Python"}, {"instruction": "def veas2tas(eas, h):\n    \"\"\" Equivalent airspeed to true airspeed \"\"\"\n", "input": "", "output": "    rho = vdensity(h)\n    tas = eas * np.sqrt(rho0 / rho)\n    return tas", "category": "Python"}, {"instruction": "def _backcompatible_cache_file(query_flags, bed_file, target_name, data):\n    \"\"\"Back-compatible: retrieve cache file from previous location.\n    \"\"\"\n", "input": "", "output": "    cmd_id = \"num_\" + \" and \".join(query_flags).replace(\" \", \"_\")\n    if bed_file is not None:\n        target_name = target_name or os.path.basename(bed_file)\n        cmd_id += \"_on_\" + target_name\n    work_dir = os.path.join(dd.get_work_dir(data), \"coverage\", dd.get_sample_name(data), \"sambamba\")\n    output_file = os.path.join(work_dir, cmd_id)\n    if utils.file_exists(output_file):\n        return output_file", "category": "Python"}, {"instruction": "def get_twitter_id(self, cache=True):\n        \"\"\"Get the twitter id for this artist if it exists\n\n        Args:\n\n        Kwargs:\n\n        Returns:\n            A twitter ID string\n\n        Example:\n\n        >>> a = artist.Artist('big boi')\n        >>> a.get_twitter_id()\n        u'BigBoi'\n        >>>\n        \"\"\"\n", "input": "", "output": "        if not (cache and ('twitter' in self.cache)):\n            response = self.get_attribute('twitter')\n            self.cache['twitter'] = response['artist'].get('twitter')\n        return self.cache['twitter']", "category": "Python"}, {"instruction": "def create(self, unique_name=values.unset, friendly_name=values.unset,\n               identity=values.unset, deployment_sid=values.unset,\n               enabled=values.unset):\n        \"\"\"\n        Create a new DeviceInstance\n\n        :param unicode unique_name: A unique, addressable name of this Device.\n        :param unicode friendly_name: A human readable description for this Device.\n        :param unicode identity: An identifier of the Device user.\n        :param unicode deployment_sid: The unique SID of the Deployment group.\n        :param bool enabled: The enabled\n\n        :returns: Newly created DeviceInstance\n        :rtype: twilio.rest.preview.deployed_devices.fleet.device.DeviceInstance\n        \"\"\"\n", "input": "", "output": "        data = values.of({\n            'UniqueName': unique_name,\n            'FriendlyName': friendly_name,\n            'Identity': identity,\n            'DeploymentSid': deployment_sid,\n            'Enabled': enabled,\n        })\n\n        payload = self._version.create(\n            'POST',\n            self._uri,\n            data=data,\n        )\n\n        return DeviceInstance(self._version, payload, fleet_sid=self._solution['fleet_sid'], )", "category": "Python"}, {"instruction": "def _extract_authenticity_token(data):\n    \"\"\"Don't look, I'm hideous!\"\"\"\n", "input": "", "output": "    # Super-cheap Python3 hack.\n    if not isinstance(data, str):\n        data = str(data, 'utf-8')\n    pos = data.find(\"authenticity_token\")\n    # Super-gross.\n    authtok = str(data[pos + 41:pos + 41 + 88])\n    return authtok", "category": "Python"}, {"instruction": "def get_option_vip(self, id_environment_vip):\n        \"\"\"Get all Option VIP by Environment Vip.\n\n        :return: Dictionary with the following structure:\n\n        ::\n\n            {\u2018option_vip\u2019: [{\u2018id\u2019: < id >,\n            \u2018tipo_opcao\u2019: < tipo_opcao >,\n            \u2018nome_opcao_txt\u2019: < nome_opcao_txt >}, ... too option vips ...] }\n\n        :raise EnvironmentVipNotFoundError: Environment VIP not registered.\n        :raise DataBaseError: Can't connect to networkapi database.\n        :raise XMLError: Failed to generate the XML response.\n        \"\"\"\n", "input": "", "output": "\n        url = 'optionvip/environmentvip/' + str(id_environment_vip) + '/'\n\n        code, xml = self.submit(None, 'GET', url)\n\n        return self.response(code, xml, ['option_vip'])", "category": "Python"}, {"instruction": "def copy(self):\n        \"\"\"Returns a deep copy of the instance.\"\"\"\n", "input": "", "output": "        clone = self.__class__()\n        clone.fields = self.fields.copy()\n        for k in clone.fields:\n            clone.fields[k] = self.get_field(k).do_copy(clone.fields[k])\n        clone.default_fields = self.default_fields.copy()\n        clone.overloaded_fields = self.overloaded_fields.copy()\n        clone.overload_fields = self.overload_fields.copy()\n        clone.underlayer = self.underlayer\n        clone.explicit = self.explicit\n        clone.raw_packet_cache = self.raw_packet_cache\n        clone.post_transforms = self.post_transforms[:]\n        clone.__dict__[\"payload\"] = self.payload.copy()\n        clone.payload.add_underlayer(clone)\n        clone.time = self.time\n        clone.sent_time = self.sent_time\n        return clone", "category": "Python"}, {"instruction": "def on_pickle_dumps(self, pickle, config, dictionary, **kwargs):\n        \"\"\" The :mod:`pickle` dumps method.\n\n        :param module pickle: The ``pickle`` module\n        :param class config: The instance's config class\n        :param dict dictionary: The dictionary instance to serailize\n        :returns: The serialized content\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "\n        return pickle.dumps(dictionary, protocol=pickle.HIGHEST_PROTOCOL)", "category": "Python"}, {"instruction": "def all_record_sets(self):\n        \"\"\"Generator to loop through current record set.\n\n        Call next page if it exists.\n        \"\"\"\n", "input": "", "output": "        is_truncated = True\n        start_record_name = None\n        start_record_type = None\n        kwargs = self.get_base_kwargs()\n        while is_truncated:\n            if start_record_name is not None:\n                kwargs.update({\n                    'StartRecordName': start_record_name,\n                    'StartRecordType': start_record_type\n                })\n            result = self.get_record_sets(**kwargs)\n            for record_set in result.get('ResourceRecordSets', []):\n                yield record_set\n\n            is_truncated = result.get('IsTruncated', False)\n\n            start_record_name = result.get('NextRecordName', None)\n            start_record_type = result.get('NextRecordType', None)", "category": "Python"}, {"instruction": "def resizeColumnsToContents(self):\r\n        \"\"\"Resize the columns to its contents.\"\"\"\n", "input": "", "output": "        self._autosized_cols = set()\r\n        self._resizeColumnsToContents(self.table_level,\r\n                                      self.table_index, self._max_autosize_ms)\r\n        self._update_layout()", "category": "Python"}, {"instruction": "def split_conditional (property):\n    \"\"\" If 'property' is conditional property, returns\n        condition and the property, e.g\n        <variant>debug,<toolset>gcc:<inlining>full will become\n        <variant>debug,<toolset>gcc <inlining>full.\n        Otherwise, returns empty string.\n    \"\"\"\n", "input": "", "output": "    assert isinstance(property, basestring)\n    m = __re_split_conditional.match (property)\n\n    if m:\n        return (m.group (1), '<' + m.group (2))\n\n    return None", "category": "Python"}, {"instruction": "def from_path(filename):\n        \"\"\"Creates a sourcemap view from a file path.\"\"\"\n", "input": "", "output": "        filename = to_bytes(filename)\n        if NULL_BYTE in filename:\n            raise ValueError('null byte in path')\n        return ProguardView._from_ptr(rustcall(\n            _lib.lsm_proguard_mapping_from_path,\n            filename + b'\\x00'))", "category": "Python"}, {"instruction": "def plot_data(orig_data, data):\n    '''plot data in 3D'''\n", "input": "", "output": "    import numpy as np\n    from mpl_toolkits.mplot3d import Axes3D\n    import matplotlib.pyplot as plt\n\n    for dd, c in [(orig_data, 'r'), (data, 'b')]:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n\n        xs = [ d.x for d in dd ]\n        ys = [ d.y for d in dd ]\n        zs = [ d.z for d in dd ]\n        ax.scatter(xs, ys, zs, c=c, marker='o')\n\n        ax.set_xlabel('X Label')\n        ax.set_ylabel('Y Label')\n        ax.set_zlabel('Z Label')\n    plt.show()", "category": "Python"}, {"instruction": "def _add_ce_record(self, curr_dr_len, thislen):\n        # type: (int, int) -> int\n        '''\n        An internal method to add a new length to a Continuation Entry.  If the\n        Continuation Entry does not yet exist, this method creates it.\n\n        Parameters:\n         curr_dr_len - The current Directory Record length.\n         thislen - The new length to add to the Continuation Entry.\n        Returns:\n         An integer representing the current directory record length after\n         adding the Continuation Entry.\n        '''\n", "input": "", "output": "        if self.dr_entries.ce_record is None:\n            self.dr_entries.ce_record = RRCERecord()\n            self.dr_entries.ce_record.new()\n            curr_dr_len += RRCERecord.length()\n        self.dr_entries.ce_record.add_record(thislen)\n        return curr_dr_len", "category": "Python"}, {"instruction": "def mark_read(request, message_id, dispatch_id, hashed, redirect_to=None):\n    \"\"\"Handles mark message as read request.\n\n    :param Request request:\n    :param int message_id:\n    :param int dispatch_id:\n    :param str hashed:\n    :param str redirect_to:\n    :return:\n    \"\"\"\n", "input": "", "output": "    if redirect_to is None:\n        redirect_to = get_static_url('img/sitemessage/blank.png')\n\n    return _generic_view(\n        'handle_mark_read_request', sig_mark_read_failed,\n        request, message_id, dispatch_id, hashed, redirect_to=redirect_to\n    )", "category": "Python"}, {"instruction": "def make_fileitem_fileextension(extension, condition='is', negate=False, preserve_case=False):\n    \"\"\"\n    Create a node for FileItem/FileExtension\n    \n    :return: A IndicatorItem represented as an Element node\n    \"\"\"\n", "input": "", "output": "    document = 'FileItem'\n    search = 'FileItem/FileExtension'\n    content_type = 'string'\n    content = extension\n    ii_node = ioc_api.make_indicatoritem_node(condition, document, search, content_type, content,\n                                              negate=negate, preserve_case=preserve_case)\n    return ii_node", "category": "Python"}, {"instruction": "def _surfdens(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _surfdens\n        PURPOSE:\n           evaluate the surface density for this potential\n        INPUT:\n           R - Galactocentric cylindrical radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           the density\n        HISTORY:\n           2018-08-19 - Written - Bovy (UofT)\n        \"\"\"\n", "input": "", "output": "        Rb= R**2.+self._b2\n        return self._b2*z*(3.*Rb+2.*z**2.)/Rb**2.*(Rb+z**2.)**-1.5/2./nu.pi", "category": "Python"}, {"instruction": "def create_container_definition(container_name, image, port=80, cpu=1.0, memgb=1.5,\n                                environment=None):\n    '''Makes a python dictionary of container properties.\n\n    Args:\n        container_name: The name of the container.\n        image (str): Container image string. E.g. nginx.\n        port (int): TCP port number. E.g. 8080.\n        cpu (float): Amount of CPU to allocate to container. E.g. 1.0.\n        memgb (float): Memory in GB to allocate to container. E.g. 1.5.\n        environment (list): A list of [{'name':'envname', 'value':'envvalue'}].\n        Sets environment variables in the container.\n\n    Returns:\n        A Python dictionary of container properties, pass a list of these to\n        create_container_group().\n    '''\n", "input": "", "output": "    container = {'name': container_name}\n    container_properties = {'image': image}\n    container_properties['ports'] = [{'port': port}]\n    container_properties['resources'] = {\n        'requests': {'cpu': cpu, 'memoryInGB': memgb}}\n    container['properties'] = container_properties\n\n    if environment is not None:\n        container_properties['environmentVariables'] = environment\n    return container", "category": "Python"}, {"instruction": "def __token(self, client_id, redirect_uri, client_secret, code, **kwargs):\n        \"\"\"Call documentation: `/oauth2/token\n        <https://www.wepay.com/developer/reference/oauth2#token>`_, plus extra\n        keyword parameter:\n        \n        :keyword bool batch_mode: turn on/off the batch_mode, see \n           :class:`wepay.api.WePay`\n\n        :keyword str batch_reference_id: `reference_id` param for batch call,\n           see :class:`wepay.api.WePay`\n\n        :keyword str api_version: WePay API version, see\n           :class:`wepay.api.WePay`\n\n        \"\"\"\n", "input": "", "output": "        params = {\n            'client_id': client_id,\n            'redirect_uri': redirect_uri,\n            'client_secret': client_secret,\n            'code': code\n        }\n        return self.make_call(self.__token, params, kwargs)", "category": "Python"}, {"instruction": "def __add_namespaceinfo(self, ni):\n        \"\"\"Internal method to directly add a _NamespaceInfo object to this\n        set.  No sanity checks are done (e.g. checking for prefix conflicts),\n        so be sure to do it yourself before calling this.\"\"\"\n", "input": "", "output": "        self.__ns_uri_map[ni.uri] = ni\n        for prefix in ni.prefixes:\n            self.__prefix_map[prefix] = ni", "category": "Python"}, {"instruction": "def cos_c(self, N=None):  # percent=0.9,\n\t\t\"\"\"Return the squared cosines for each column.\"\"\"\n", "input": "", "output": "\n\t\tif not hasattr(self, 'G') or self.G.shape[1] < self.rank:\n\t\t\tself.fs_c(N=self.rank)  # generate\n\t\tself.dc = norm(self.G, axis=1)**2\n\t\t# cheaper than diag(self.G.dot(self.G.T))?\n\n\t\treturn apply_along_axis(lambda _: _/self.dc, 0, self.G[:, :N]**2)", "category": "Python"}, {"instruction": "def transform(self, X):\n        \"\"\"\n        Delete all features, which were not relevant in the fit phase.\n\n        :param X: data sample with all features, which will be reduced to only those that are relevant\n        :type X: pandas.DataSeries or numpy.array\n\n        :return: same data sample as X, but with only the relevant features\n        :rtype: pandas.DataFrame or numpy.array\n        \"\"\"\n", "input": "", "output": "        if self.relevant_features is None:\n            raise RuntimeError(\"You have to call fit before.\")\n\n        if isinstance(X, pd.DataFrame):\n            return X.copy().loc[:, self.relevant_features]\n        else:\n            return X[:, self.relevant_features]", "category": "Python"}, {"instruction": "def moist_amplification_factor(Tkelvin, relative_humidity=0.8):\n    '''Compute the moisture amplification factor for the moist diffusivity\n    given relative humidity and reference temperature profile.'''\n", "input": "", "output": "    deltaT = 0.01\n    #  slope of saturation specific humidity at 1000 hPa\n    dqsdTs = (qsat(Tkelvin+deltaT/2, 1000.) - qsat(Tkelvin-deltaT/2, 1000.)) / deltaT\n    return const.Lhvap / const.cp * relative_humidity * dqsdTs", "category": "Python"}, {"instruction": "def lease(self, items):\n        \"\"\"Add the given messages to lease management.\n\n        Args:\n            items(Sequence[LeaseRequest]): The items to lease.\n        \"\"\"\n", "input": "", "output": "        self._manager.leaser.add(items)\n        self._manager.maybe_pause_consumer()", "category": "Python"}, {"instruction": "def mode_ts(ec, mode=\"\", ts=None):\n    \"\"\"\n    Get string for the mode\n    :param str ec: extract or collapse\n    :param str mode: \"paleo\" or \"chron\" mode\n    :param list ts: Time series (for collapse)\n    :return str phrase: Phrase\n    \"\"\"\n", "input": "", "output": "    phrase = \"\"\n    if ec == \"extract\":\n        if mode==\"chron\":\n            phrase = \"extracting chronData...\"\n        else:\n            phrase = \"extracting paleoData...\"\n    elif ec == \"collapse\":\n        if ts[0][\"mode\"] == \"chronData\":\n            phrase = \"collapsing chronData\"\n        else:\n            phrase = \"collapsing paleoData...\"\n    return phrase", "category": "Python"}, {"instruction": "def plot(self, num_pts, color=None, alpha=None, ax=None):\n        \"\"\"Plot the current curve.\n\n        Args:\n            num_pts (int): Number of points to plot.\n            color (Optional[Tuple[float, float, float]]): Color as RGB profile.\n            alpha (Optional[float]): The alpha channel for the color.\n            ax (Optional[matplotlib.artist.Artist]): matplotlib axis object\n                to add plot to.\n\n        Returns:\n            matplotlib.artist.Artist: The axis containing the plot. This\n            may be a newly created axis.\n\n        Raises:\n            NotImplementedError: If the curve's dimension is not ``2``.\n        \"\"\"\n", "input": "", "output": "        if self._dimension != 2:\n            raise NotImplementedError(\n                \"2D is the only supported dimension\",\n                \"Current dimension\",\n                self._dimension,\n            )\n\n        s_vals = np.linspace(0.0, 1.0, num_pts)\n        points = self.evaluate_multi(s_vals)\n        if ax is None:\n            ax = _plot_helpers.new_axis()\n        ax.plot(points[0, :], points[1, :], color=color, alpha=alpha)\n        return ax", "category": "Python"}, {"instruction": "def _get_attr_by_tag(obj, tag, attr_name):\n    \"\"\"Get attribute from an object via a string tag.\n\n    Parameters\n    ----------\n    obj : object from which to get the attribute\n    attr_name : str\n        Unmodified name of the attribute to be found.  The actual attribute\n        that is returned may be modified be 'tag'.\n    tag : str\n        Tag specifying how to modify 'attr_name' by pre-pending it with 'tag'.\n        Must be a key of the _TAG_ATTR_MODIFIERS dict.\n\n    Returns\n    -------\n    the specified attribute of obj\n    \"\"\"\n", "input": "", "output": "    attr_name = _TAG_ATTR_MODIFIERS[tag] + attr_name\n    return getattr(obj, attr_name)", "category": "Python"}, {"instruction": "def _proxy(self):\n        \"\"\"\n        Generate an instance context for the instance, the context is capable of\n        performing various actions.  All instance actions are proxied to the context\n\n        :returns: QueueContext for this QueueInstance\n        :rtype: twilio.rest.api.v2010.account.queue.QueueContext\n        \"\"\"\n", "input": "", "output": "        if self._context is None:\n            self._context = QueueContext(\n                self._version,\n                account_sid=self._solution['account_sid'],\n                sid=self._solution['sid'],\n            )\n        return self._context", "category": "Python"}, {"instruction": "def get_class(name, config_key, module):\n    \"\"\"Get the class by its name as a string.\"\"\"\n", "input": "", "output": "    clsmembers = inspect.getmembers(module, inspect.isclass)\n    for string_name, act_class in clsmembers:\n        if string_name == name:\n            return act_class\n\n    # Check if the user has specified a plugin and if the class is in there\n    cfg = get_project_configuration()\n    if config_key in cfg:\n        modname = os.path.splitext(os.path.basename(cfg[config_key]))[0]\n        if os.path.isfile(cfg[config_key]):\n            usermodule = imp.load_source(modname, cfg[config_key])\n            clsmembers = inspect.getmembers(usermodule, inspect.isclass)\n            for string_name, act_class in clsmembers:\n                if string_name == name:\n                    return act_class\n        else:\n            logging.warning(\"File '%s' does not exist. Adjust ~/.hwrtrc.\",\n                            cfg['data_analyzation_plugins'])\n\n    logging.debug(\"Unknown class '%s'.\", name)\n    return None", "category": "Python"}, {"instruction": "def RANGE(start, end, step=None):\n    \"\"\"\n    Generates the sequence from the specified starting number by successively\n    incrementing the starting number by the specified step value up to but not including the end point.\n    See https://docs.mongodb.com/manual/reference/operator/aggregation/range/\n    for more details\n    :param start: An integer (or valid expression) that specifies the start of the sequence.\n    :param end: An integer (or valid expression) that specifies the exclusive upper limit of the sequence.\n    :param step: An integer (or valid expression) that specifies the increment value.\n    :return: Aggregation operator\n    \"\"\"\n", "input": "", "output": "    return {'$range': [start, end, step]} if step is not None else {'$range': [start, end]}", "category": "Python"}, {"instruction": "def get(self, request, bot_id, format=None):\n        \"\"\"\n        Get list of Kik bots\n        ---\n        serializer: KikBotSerializer\n        responseMessages:\n            - code: 401\n              message: Not authenticated\n        \"\"\"\n", "input": "", "output": "        return super(KikBotList, self).get(request, bot_id, format)", "category": "Python"}, {"instruction": "def validate(self, data):\n        \"\"\"\n        We need to set some timestamps according to the accounting packet type\n        * update_time: set everytime a Interim-Update / Stop packet is received\n        * stop_time: set everytime a Stop packet is received\n        * session_time: calculated if not present in the accounting packet\n        :param data: accounting packet\n        :return: Dict accounting packet\n        \"\"\"\n", "input": "", "output": "        time = timezone.now()\n        status_type = data.pop('status_type')\n        if status_type == 'Interim-Update':\n            data['update_time'] = time\n        if status_type == 'Stop':\n            data['update_time'] = time\n            data['stop_time'] = time\n        return data", "category": "Python"}, {"instruction": "def by_summoner_by_champion(self, region, encrypted_summoner_id, champion_id):\n        \"\"\"\n        Get a champion mastery by player ID and champion ID.\n\n        :param string                           region: the region to execute this request on\n        :param string encrypted_summoner_id:    Summoner ID associated with the player\n        :param long champion_id:                Champion ID to retrieve Champion Mastery for\n\n        :returns: ChampionMasteryDTO: This object contains single Champion Mastery information for\n                                      player and champion combination.\n        \"\"\"\n", "input": "", "output": "        url, query = ChampionMasteryApiV4Urls.by_summoner_by_champion(\n            region=region,\n            encrypted_summoner_id=encrypted_summoner_id,\n            champion_id=champion_id,\n        )\n        return self._raw_request(\n            self.by_summoner_by_champion.__name__, region, url, query\n        )", "category": "Python"}, {"instruction": "def import_headers(cls, http_code):\n\t\t\"\"\" Create WHTTPHeaders by the given code. If code has 'Set-Cookie' headers, that headers are\n\t\tparsed, data are stored in internal cookie jar. At the end of parsing 'Set-Cookie' headers are\n\t\tremoved from the result\n\n\t\t:param http_code: HTTP code to parse\n\t\t:return: WHTTPHeaders\n\t\t\"\"\"\n", "input": "", "output": "\t\theaders = WHTTPHeaders()\n\t\tmessage = email.message_from_file(StringIO(http_code))\n\t\tfor header_name, header_value in message.items():\n\t\t\theaders.add_headers(header_name, header_value)\n\n\t\tcookie_header = headers.get_headers('Set-Cookie')\n\t\tif cookie_header is not None:\n\t\t\tfor cookie_string in cookie_header:\n\t\t\t\tfor single_cookie in WHTTPCookieJar.import_header_text(cookie_string):\n\t\t\t\t\theaders.set_cookie_jar().add_cookie(single_cookie)\n\t\t\theaders.remove_headers('Set-Cookie')\n\n\t\treturn headers", "category": "Python"}, {"instruction": "def parametrize(params):\n    \"\"\"Return list of params as params.\n\n    >>> parametrize(['a'])\n    'a'\n    >>> parametrize(['a', 'b'])\n    'a[b]'\n    >>> parametrize(['a', 'b', 'c'])\n    'a[b][c]'\n\n    \"\"\"\n", "input": "", "output": "    returned = str(params[0])\n    returned += \"\".join(\"[\" + str(p) + \"]\" for p in params[1:])\n    return returned", "category": "Python"}, {"instruction": "def parse_instruction(string, location, tokens):\n    \"\"\"Parse an ARM instruction.\n    \"\"\"\n", "input": "", "output": "    mnemonic_str = tokens.get(\"mnemonic\")\n    operands = [op for op in tokens.get(\"operands\", [])]\n\n    instr = ArmInstruction(\n        string,\n        mnemonic_str[\"ins\"],\n        operands,\n        arch_info.architecture_mode\n    )\n\n    if \"cc\" in mnemonic_str:\n        instr.condition_code = cc_mapper[mnemonic_str[\"cc\"]]\n\n    if \"uf\" in mnemonic_str:\n        instr.update_flags = True\n\n    if \"ldm_stm_addr_mode\" in mnemonic_str:\n        instr.ldm_stm_addr_mode = ldm_stm_am_mapper[mnemonic_str[\"ldm_stm_addr_mode\"]]\n\n    return instr", "category": "Python"}, {"instruction": "def _paste_mask(box, mask, shape):\n    \"\"\"\n    Args:\n        box: 4 float\n        mask: MxM floats\n        shape: h,w\n    Returns:\n        A uint8 binary image of hxw.\n    \"\"\"\n", "input": "", "output": "    # int() is floor\n    # box fpcoor=0.0 -> intcoor=0.0\n    x0, y0 = list(map(int, box[:2] + 0.5))\n    # box fpcoor=h -> intcoor=h-1, inclusive\n    x1, y1 = list(map(int, box[2:] - 0.5))    # inclusive\n    x1 = max(x0, x1)    # require at least 1x1\n    y1 = max(y0, y1)\n\n    w = x1 + 1 - x0\n    h = y1 + 1 - y0\n\n    # rounding errors could happen here, because masks were not originally computed for this shape.\n    # but it's hard to do better, because the network does not know the \"original\" scale\n    mask = (cv2.resize(mask, (w, h)) > 0.5).astype('uint8')\n    ret = np.zeros(shape, dtype='uint8')\n    ret[y0:y1 + 1, x0:x1 + 1] = mask\n    return ret", "category": "Python"}, {"instruction": "def enable_servicegroup_host_notifications(self, servicegroup):\n        \"\"\"Enable host notifications for a servicegroup\n        Format of the line that triggers function call::\n\n        ENABLE_SERVICEGROUP_HOST_NOTIFICATIONS;<servicegroup_name>\n\n        :param servicegroup: servicegroup to enable\n        :type servicegroup: alignak.objects.servicegroup.Servicegroup\n        :return: None\n        \"\"\"\n", "input": "", "output": "        for service_id in servicegroup.get_services():\n            if service_id in self.daemon.services:\n                host_id = self.daemon.services[service_id].host\n                self.enable_host_notifications(self.daemon.hosts[host_id])", "category": "Python"}, {"instruction": "def from_dict(data, ctx):\n        \"\"\"\n        Instantiate a new PositionBookBucket from a dict (generally from\n        loading a JSON response). The data used to instantiate the\n        PositionBookBucket is a shallow copy of the dict passed in, with any\n        complex child types instantiated appropriately.\n        \"\"\"\n", "input": "", "output": "\n        data = data.copy()\n\n        if data.get('price') is not None:\n            data['price'] = ctx.convert_decimal_number(\n                data.get('price')\n            )\n\n        if data.get('longCountPercent') is not None:\n            data['longCountPercent'] = ctx.convert_decimal_number(\n                data.get('longCountPercent')\n            )\n\n        if data.get('shortCountPercent') is not None:\n            data['shortCountPercent'] = ctx.convert_decimal_number(\n                data.get('shortCountPercent')\n            )\n\n        return PositionBookBucket(**data)", "category": "Python"}, {"instruction": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n", "input": "", "output": "        config = super(PassengerCollector, self).get_default_config()\n        config.update({\n            \"path\":         \"passenger_stats\",\n            \"bin\":          \"/usr/lib/ruby-flo/bin/passenger-memory-stats\",\n            \"use_sudo\":     False,\n            \"sudo_cmd\":     \"/usr/bin/sudo\",\n            \"passenger_status_bin\": \"/usr/bin/passenger-status\",\n            \"passenger_memory_stats_bin\": \"/usr/bin/passenger-memory-stats\",\n        })\n        return config", "category": "Python"}, {"instruction": "def _parse_seed_tbl(self, var_tbl):\n        \"\"\"Parse a table of variable bindings (dictionary with key = variable name)\"\"\"\n", "input": "", "output": "\n        m: int = len(var_tbl)\n        # Find the length of each variable to infer T\n        T = self._check_forward_mode_input_dict(var_tbl)\n\n        # The shape of X based on T and m\n        shape = (T, m)\n\n        # Initialize X to zeros in the correct shape        \n        X = np.zeros(shape)\n        # loop through the bound variables\n        for j, var_name in enumerate(sorted(var_tbl)):\n            if var_name in self.var_names:\n                X[:,j] = var_tbl[var_name]\n        return X", "category": "Python"}, {"instruction": "def comments(self, extra_params=None):\n        \"\"\"\n        All Comments in this Ticket\n        \"\"\"\n", "input": "", "output": "\n        # Default params\n        params = {\n            'per_page': settings.MAX_PER_PAGE,\n        }\n\n        if extra_params:\n            params.update(extra_params)\n\n        return self.api._get_json(\n            TicketComment,\n            space=self,\n            rel_path=self.space._build_rel_path(\n                'tickets/%s/ticket_comments' % self['number']\n            ),\n            extra_params=params,\n            get_all=True,  # Retrieve all comments in the ticket\n        )", "category": "Python"}, {"instruction": "def _set_property_dict_item(obj, prop, key, value):\n    ''' Sets the dict item key of the attr from obj.\n\n        Basicaly it does getattr(obj, prop)[key] = value.\n\n\n        For the disk device we added some checks to make\n        device changes on the CLI saver.\n    '''\n", "input": "", "output": "    attr = getattr(obj, prop)\n    if prop == 'devices':\n        device_type = value['type']\n\n        if device_type == 'disk':\n\n            if 'path' not in value:\n                raise SaltInvocationError(\n                    \"path must be given as parameter\"\n                )\n\n            if value['path'] != '/' and 'source' not in value:\n                raise SaltInvocationError(\n                    \"source must be given as parameter\"\n                )\n\n        for k in value.keys():\n            if k.startswith('__'):\n                del value[k]\n\n        attr[key] = value\n\n    else:  # config\n        attr[key] = six.text_type(value)\n\n    pylxd_save_object(obj)\n\n    return _pylxd_model_to_dict(obj)", "category": "Python"}, {"instruction": "def next(self):\n        \"\"\"Get the next processable item of the queue.\n\n        A processable item is supposed to have the status `queued`.\n\n        Returns:\n            None : If no key is found.\n            Int: If a valid entry is found.\n\n        \"\"\"\n", "input": "", "output": "        smallest = None\n        for key in self.queue.keys():\n            if self.queue[key]['status'] == 'queued':\n                if smallest is None or key < smallest:\n                    smallest = key\n        return smallest", "category": "Python"}, {"instruction": "def clear(self, asset_manager_id):\n        \"\"\" This method deletes all the data for an asset_manager_id.\n            It should be used with extreme caution.  In production it\n            is almost always better to Inactivate rather than delete. \"\"\"\n", "input": "", "output": "        self.logger.info('Clear Market Data - Asset Manager: %s', asset_manager_id)\n        url = '%s/clear/%s' % (self.endpoint, asset_manager_id)\n        response = self.session.delete(url)\n        if response.ok:\n            eod_price_count = response.json().get('eod_price_count', 'Unknown')\n            self.logger.info('Deleted %s EOD Prices.', eod_price_count)\n            fx_rate_count = response.json().get('fx_rate_count', 'Unknown')\n            self.logger.info('Deleted %s FX Rates.', fx_rate_count)\n            return response.json()\n        else:\n            self.logger.error(response.text)\n            response.raise_for_status()", "category": "Python"}, {"instruction": "def set_cursor(self, row, col):\n        \"\"\"Set cursor position to given row and column in the current window.\n\n        Operation is not added to the jump list.\n        \"\"\"\n", "input": "", "output": "        self._vim.current.window.cursor = (row, col)", "category": "Python"}, {"instruction": "def _parse_datetime(val, fmt=None):\n  \"\"\"Returns a datetime object parsed from :val:. The timezone, if any, on the\n  string must be in the format that python can parse.\n\n  :param val: a string to be parsed as a date\n  :param fmt: a format string, a tuple of format strings, or None. If None a built in list of format\n              strings will be used to try to parse the datetime.\n  \"\"\"\n", "input": "", "output": "  if fmt is None:\n    fmt = _DEFAULT_FORMATS\n\n  if isinstance(fmt, str):\n    return datetime.strptime(val, fmt)\n  else:\n    for opt in fmt:\n      try:\n        return _parse_datetime(val, opt)\n      except ValueError:\n        continue\n    raise ValueError(\"Unable to parse string '{}' as a timestamp\".format(val))", "category": "Python"}, {"instruction": "def _send_api_message(self, message):\n        \"\"\"Send a Slack message via the chat.postMessage api.\n\n        :param message: a dict of kwargs to be passed to slacker.\n        \"\"\"\n", "input": "", "output": "\n        self.slack.chat.post_message(**message)\n        self.log.debug(\"sent api message %r\", message)", "category": "Python"}, {"instruction": "def join(*args, trailing_slash=False):\n    \"\"\"\n    Return a url path joined from the arguments. It correctly handles blank/None\n    arguments, and removes back-to-back slashes, eg::\n\n        assert join('/', 'foo', None, 'bar', '', 'baz') == '/foo/bar/baz'\n        assert join('/', '/foo', '/', '/bar/') == '/foo/bar'\n\n    Note that it removes trailing slashes by default, so if you want to keep those,\n    then you need to pass the ``trailing_slash`` keyword argument::\n\n        assert join('/foo', 'baz', None, trailing_slash=True) == '/foo/baz/'\n    \"\"\"\n", "input": "", "output": "    dirty_path = '/'.join(map(lambda x: x and x or '', args))\n    path = re.sub(r'/+', '/', dirty_path)\n    if path in {'', '/'}:\n        return '/'\n    path = path.rstrip('/')\n    return path if not trailing_slash else path + '/'", "category": "Python"}, {"instruction": "def get_info(self, code):\n        \"\"\"Return a dict of information about the currency\"\"\"\n", "input": "", "output": "        currency = self.get_currency(code)\n        info = {}\n\n        users = list(filter(None, currency['users'].split(',')))\n        if users:\n            info['Users'] = users\n\n        alt = list(filter(None, currency['alternatives'].split(',')))\n        if alt:\n            info['Alternatives'] = alt\n\n        if self.modified:\n            info['YFUpdate'] = self.modified.isoformat()\n\n        return info", "category": "Python"}, {"instruction": "def un(byts):\n    '''\n    Use msgpack to de-serialize a python object.\n\n    Args:\n        byts (bytes): The bytes to de-serialize\n\n    Notes:\n        String objects are decoded using utf8 encoding.  In order to handle\n        potentially malformed input, ``unicode_errors='surrogatepass'`` is set\n        to allow decoding bad input strings.\n\n    Returns:\n        obj: The de-serialized object\n    '''\n", "input": "", "output": "    # This uses a subset of unpacker_kwargs\n    return msgpack.loads(byts, use_list=False, raw=False, unicode_errors='surrogatepass')", "category": "Python"}, {"instruction": "def _ref_covered_by_at_least_one_full_length_contig(nucmer_hits, percent_threshold, max_nt_extend):\n        '''Returns true iff there exists a contig that completely\n           covers the reference sequence\n           nucmer_hits = hits made by self._parse_nucmer_coords_file.'''\n", "input": "", "output": "        for l in nucmer_hits.values():\n            for hit in l:\n                if ( (2 * max_nt_extend) + len(hit.ref_coords()) ) / hit.ref_length >= percent_threshold:\n                    return True\n        return False", "category": "Python"}, {"instruction": "def move_to(self, folder_id):\n    \"\"\"\n    :param str folder_id: The Calendar ID to where you want to move the event to.\n    Moves an event to a different folder (calendar).  ::\n\n      event = service.calendar().get_event(id='KEY HERE')\n      event.move_to(folder_id='NEW CALENDAR KEY HERE')\n    \"\"\"\n", "input": "", "output": "    if not folder_id:\n      raise TypeError(u\"You can't move an event to a non-existant folder\")\n\n    if not isinstance(folder_id, BASESTRING_TYPES):\n      raise TypeError(u\"folder_id must be a string\")\n\n    if not self.id:\n      raise TypeError(u\"You can't move an event that hasn't been created yet.\")\n\n    self.refresh_change_key()\n    response_xml = self.service.send(soap_request.move_event(self, folder_id))\n    new_id, new_change_key = self._parse_id_and_change_key_from_response(response_xml)\n    if not new_id:\n      raise ValueError(u\"MoveItem returned success but requested item not moved\")\n\n    self._id = new_id\n    self._change_key = new_change_key\n    self.calendar_id = folder_id\n    return self", "category": "Python"}, {"instruction": "def place_data_on_block_device(blk_device, data_src_dst):\n    \"\"\"Migrate data in data_src_dst to blk_device and then remount.\"\"\"\n", "input": "", "output": "    # mount block device into /mnt\n    mount(blk_device, '/mnt')\n    # copy data to /mnt\n    copy_files(data_src_dst, '/mnt')\n    # umount block device\n    umount('/mnt')\n    # Grab user/group ID's from original source\n    _dir = os.stat(data_src_dst)\n    uid = _dir.st_uid\n    gid = _dir.st_gid\n    # re-mount where the data should originally be\n    # TODO: persist is currently a NO-OP in core.host\n    mount(blk_device, data_src_dst, persist=True)\n    # ensure original ownership of new mount.\n    os.chown(data_src_dst, uid, gid)", "category": "Python"}, {"instruction": "def main():\n    \"\"\"The main entry point for the command line interface.\"\"\"\n", "input": "", "output": "    # get arguments from the command line\n    args = _get_args()\n    # create the environment\n    env = NESEnv(args.rom)\n    # play the environment with the given mode\n    if args.mode == 'human':\n        play_human(env)\n    else:\n        play_random(env, args.steps)", "category": "Python"}, {"instruction": "def verify_password(self, password):\n        \"\"\" Verify a given string for being valid password \"\"\"\n", "input": "", "output": "        if self.password is None:\n            return False\n\n        from boiler.user.util.passlib import passlib_context\n        return passlib_context.verify(str(password), self.password)", "category": "Python"}, {"instruction": "def StreamInChunks(self, callback=None, finish_callback=None,\n                       additional_headers=None):\n        \"\"\"Send this (resumable) upload in chunks.\"\"\"\n", "input": "", "output": "        return self.__StreamMedia(\n            callback=callback, finish_callback=finish_callback,\n            additional_headers=additional_headers)", "category": "Python"}, {"instruction": "def threshold_image(img, bkground_thresh, bkground_value=0.0):\n    \"\"\"\n    Thresholds a given image at a value or percentile.\n\n    Replacement value can be specified too.\n\n\n    Parameters\n    -----------\n    image_in : ndarray\n        Input image\n\n    bkground_thresh : float\n        a threshold value to identify the background\n\n    bkground_value : float\n        a value to fill the background elements with. Default 0.\n\n    Returns\n    -------\n\n    thresholded_image : ndarray\n        thresholded and/or filled image\n\n    \"\"\"\n", "input": "", "output": "\n    if bkground_thresh is None:\n        return img\n\n    if isinstance(bkground_thresh, str):\n        try:\n            thresh_perc = float(bkground_thresh.replace('%', ''))\n        except:\n            raise ValueError(\n                'percentile specified could not be parsed correctly '\n                ' - must be a string of the form \"5%\", \"10%\" etc')\n        else:\n            thresh_value = np.percentile(img, thresh_perc)\n    elif isinstance(bkground_thresh, (float, int)):\n        thresh_value = bkground_thresh\n    else:\n        raise ValueError('Invalid specification for background threshold.')\n\n    img[img < thresh_value] = bkground_value\n\n    return img", "category": "Python"}, {"instruction": "def _yield_abbreviations_for_parameter(param, kwargs):\n    \"\"\"Get an abbreviation for a function parameter.\"\"\"\n", "input": "", "output": "    name = param.name\n    kind = param.kind\n    ann = param.annotation\n    default = param.default\n    not_found = (name, empty, empty)\n    if kind in (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY):\n        if name in kwargs:\n            value = kwargs.pop(name)\n        elif ann is not empty:\n            warn(\"Using function annotations to implicitly specify interactive controls is deprecated. Use an explicit keyword argument for the parameter instead.\", DeprecationWarning)\n            value = ann\n        elif default is not empty:\n            value = default\n        else:\n            yield not_found\n        yield (name, value, default)\n    elif kind == Parameter.VAR_KEYWORD:\n        # In this case name=kwargs and we yield the items in kwargs with their keys.\n        for k, v in kwargs.copy().items():\n            kwargs.pop(k)\n            yield k, v, empty", "category": "Python"}, {"instruction": "def delete_columns(mat, columns_to_delete):\n\t'''\n\t>>> a = csr_matrix(np.array([[0, 1, 3, 0, 1, 0],\n\t\t                           [0, 0, 1, 0, 1, 1]])\n\t>>> delete_columns(a, [1,2]).todense()\n\tmatrix([[0, 0, 1, 0],\n          [0, 0, 1, 1]])\n\n\tParameters\n\t----------\n\tmat : csr_matrix\n\tcolumns_to_delete : list[int]\n\n\tReturns\n\t-------\n\tcsr_matrix that is stripped of columns indices columns_to_delete\n\t'''\n", "input": "", "output": "\tcolumn_mask = np.ones(mat.shape[1], dtype=bool)\n\tcolumn_mask[columns_to_delete] = 0\n\treturn mat.tocsc()[:, column_mask].tocsr()", "category": "Python"}, {"instruction": "def sessioninfo(self):\n        ''' session info '''\n", "input": "", "output": "        response, status_code = self.__pod__.Session.get_v2_sessioninfo(\n            sessionToken=self.__session__\n        ).result()\n        self.logger.debug('%s: %s' % (status_code, response))\n        return status_code, response", "category": "Python"}, {"instruction": "def data(self, column, role):\n        \"\"\"Return the data for the specified column and role\n\n        The column addresses one attribute of the data.\n\n        :param column: the data column\n        :type column: int\n        :param role: the data role\n        :type role: QtCore.Qt.ItemDataRole\n        :returns: data depending on the role\n        :rtype:\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        return self.columns[column](self._note, role)", "category": "Python"}, {"instruction": "def autocorr_coeff(x, t, tau1, tau2):\r\n    \"\"\"Calculate the autocorrelation coefficient.\"\"\"\n", "input": "", "output": "    return corr_coeff(x, x, t, tau1, tau2)", "category": "Python"}, {"instruction": "def _try_to_get_extension(obj):\n    \"\"\"\n    Try to get file extension from given path or file object.\n\n    :param obj: a file, file-like object or something\n    :return: File extension or None\n\n    >>> _try_to_get_extension(\"a.py\")\n    'py'\n    \"\"\"\n", "input": "", "output": "    if is_path(obj):\n        path = obj\n\n    elif is_path_obj(obj):\n        return obj.suffix[1:]\n\n    elif is_file_stream(obj):\n        try:\n            path = get_path_from_stream(obj)\n        except ValueError:\n            return None\n\n    elif is_ioinfo(obj):\n        path = obj.path\n\n    else:\n        return None\n\n    if path:\n        return get_file_extension(path)\n\n    return None", "category": "Python"}, {"instruction": "def run_LDA(df):\n    \"\"\"\n    Run LinearDiscriminantAnalysis on input dataframe (df) and return\n    transformed data, scalings and explained variance by discriminants.\n    \"\"\"\n", "input": "", "output": "    # Prep variables for sklearn LDA\n    X = df.iloc[:, 1:df.shape[1]].values     # input data matrix\n    y = df[\"Condition\"].values               # data categories list\n\n    # Calculate LDA\n    sklearn_lda = LDA()\n    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n    try:\n        exp_var = sklearn_lda.explained_variance_ratio_\n    except AttributeError as ae:\n        print(\"\\n{}: explained variance cannot be computed.\\nPlease check this GitHub PR:\"\n              \" https://github.com/scikit-learn/scikit-learn/pull/6027\".format(ae))\n        return X_lda_sklearn, y, \"NA\"\n    return X_lda_sklearn, y, exp_var", "category": "Python"}, {"instruction": "def _read_from_sql(self, request, db_name):\n        \"\"\"\n        Using the contextlib, I hope to close the connection to database when\n        not in use\n        \"\"\"\n", "input": "", "output": "        with contextlib.closing(sqlite3.connect(\"{}.db\".format(db_name))) as con:\n            return sql.read_sql(sql=request, con=con)", "category": "Python"}, {"instruction": "def get(self, name=\"\"):\n        \"\"\"Get the address(es).\n        \"\"\"\n", "input": "", "output": "        addrs = []\n\n        with self._address_lock:\n            for metadata in self._addresses.values():\n                if (name == \"\" or\n                        (name and name in metadata[\"service\"])):\n                    mda = copy.copy(metadata)\n                    mda[\"receive_time\"] = mda[\"receive_time\"].isoformat()\n                    addrs.append(mda)\n        LOGGER.debug('return address %s', str(addrs))\n        return addrs", "category": "Python"}, {"instruction": "def retry_on_exception(num_retries, base_delay=0, exc_type=Exception):\n    \"\"\"If the decorated function raises exception exc_type, allow num_retries\n    retry attempts before raise the exception.\n    \"\"\"\n", "input": "", "output": "    def _retry_on_exception_inner_1(f):\n        def _retry_on_exception_inner_2(*args, **kwargs):\n            retries = num_retries\n            multiplier = 1\n            while True:\n                try:\n                    return f(*args, **kwargs)\n                except exc_type:\n                    if not retries:\n                        raise\n\n                delay = base_delay * multiplier\n                multiplier += 1\n                log(\"Retrying '%s' %d more times (delay=%s)\" %\n                    (f.__name__, retries, delay), level=INFO)\n                retries -= 1\n                if delay:\n                    time.sleep(delay)\n\n        return _retry_on_exception_inner_2\n\n    return _retry_on_exception_inner_1", "category": "Python"}, {"instruction": "def defineGeometry(self):\n        \"\"\"Set the 3D geometry of the cell.\"\"\"\n", "input": "", "output": "        self.soma.L = 18.8\n        self.soma.diam = 18.8\n        self.soma.Ra = 123.0\n\n        self.dend.L = 200.0\n        self.dend.diam = 1.0\n        self.dend.Ra = 100.0", "category": "Python"}, {"instruction": "def commit_index(self, message):\n        \"\"\"\n        Commit the current index.\n        :param message: str\n        :return: str the generated commit sha\n        \"\"\"\n", "input": "", "output": "        tree_id = self.write_tree()\n\n        args = ['commit-tree', tree_id, '-p', self.ref_head]\n\n        # todo, this can end in a race-condition with other processes adding commits\n        commit = self.command_exec(args, message)[0].decode('utf-8').strip()\n        self.command_exec(['update-ref', self.ref_head, commit])\n\n        return commit", "category": "Python"}, {"instruction": "def _assert_git_repo(target):\n        \"\"\" Asserts that a given target directory is a git repository \"\"\"\n", "input": "", "output": "        hooks_dir = os.path.abspath(os.path.join(target, HOOKS_DIR_PATH))\n        if not os.path.isdir(hooks_dir):\n            raise GitHookInstallerError(u\"{0} is not a git repository.\".format(target))", "category": "Python"}, {"instruction": "def get_robot_variables():\n    \"\"\"Return list of Robot Framework -compatible cli-variables parsed\n    from ROBOT_-prefixed environment variable\n\n    \"\"\"\n", "input": "", "output": "    prefix = 'ROBOT_'\n    variables = []\n\n    def safe_str(s):\n        if isinstance(s, six.text_type):\n            return s\n        else:\n            return six.text_type(s, 'utf-8', 'ignore')\n    for key in os.environ:\n        if key.startswith(prefix) and len(key) > len(prefix):\n            variables.append(safe_str(\n                '%s:%s' % (key[len(prefix):], os.environ[key]),\n            ))\n    return variables", "category": "Python"}, {"instruction": "def uinit(self, ushape):\n        \"\"\"Return initialiser for working variable U\"\"\"\n", "input": "", "output": "\n        if self.opt['Y0'] is None:\n            return np.zeros(ushape, dtype=self.dtype)\n        else:\n            # If initial Y is non-zero, initial U is chosen so that\n            # the relevant dual optimality criterion (see (3.10) in\n            # boyd-2010-distributed) is satisfied.\n            return self.Y", "category": "Python"}, {"instruction": "def for_model(self, model):\n        \"\"\"\n        QuerySet for all comments for a particular model (either an instance or\n        a class).\n        \"\"\"\n", "input": "", "output": "        ct = ContentType.objects.get_for_model(model)\n        qs = self.get_queryset().filter(content_type=ct)\n        if isinstance(model, models.Model):\n            qs = qs.filter(object_pk=force_text(model._get_pk_val()))\n        return qs", "category": "Python"}, {"instruction": "def load_diagonal(cov, load=None):\n        \"\"\"Return the given square matrix with a small amount added to the diagonal\n        to make it positive semi-definite.\n        \"\"\"\n", "input": "", "output": "        n, m = cov.shape\n        assert n == m, \"matrix must be square, but found shape {}\".format((n, m))\n\n        if load is None:\n            load = np.sqrt(np.finfo(np.float64).eps) # machine epsilon\n        return cov + np.eye(n) * load", "category": "Python"}, {"instruction": "def setup_environment(config: Dict[str, Any], environment_type: Environment) -> None:\n    \"\"\"Sets the config depending on the environment type\"\"\"\n", "input": "", "output": "    # interpret the provided string argument\n    if environment_type == Environment.PRODUCTION:\n        # Safe configuration: restrictions for mainnet apply and matrix rooms have to be private\n        config['transport']['matrix']['private_rooms'] = True\n\n    config['environment_type'] = environment_type\n\n    print(f'Raiden is running in {environment_type.value.lower()} mode')", "category": "Python"}, {"instruction": "def _maybe_make_unit_desc(self, unit_desc):\n    \"\"\"Return the UnitDescriptor or convert a string to one.\"\"\"\n", "input": "", "output": "    if isinstance(unit_desc, str) or unit_desc is None:\n      unit_desc = units.Unit(unit_desc)\n    if not isinstance(unit_desc, units.UnitDescriptor):\n      raise TypeError('Invalid units for measurement %s: %s' % (self.name,\n                                                                unit_desc))\n    return unit_desc", "category": "Python"}, {"instruction": "def depth(self):\n        \"\"\"\n        The number of hierarchy levels in this category graph. Returns 0 if\n        it contains no categories.\n        \"\"\"\n", "input": "", "output": "        categories = self._categories\n        if not categories:\n            return 0\n        first_depth = categories[0].depth\n        for category in categories[1:]:\n            if category.depth != first_depth:\n                raise ValueError('category depth not uniform')\n        return first_depth", "category": "Python"}, {"instruction": "def merge_user_commits(cls, commits):\n        \"\"\"Merge all the commits for the user.\n\n        Aggregate line counts, and use the most recent commit (by date/time)\n        as the representative commit for the user.\n        \"\"\"\n", "input": "", "output": "        user = None\n        for commit in commits:\n            if not user:\n                user = commit\n            else:\n                if commit.committer_time > user.committer_time:\n                    commit.line_count += user.line_count\n                    user = commit\n                else:\n                    user.line_count += commit.line_count\n        return user", "category": "Python"}, {"instruction": "def Trim(self, flags):\n        \"\"\"\n        Trim the nodes from the tree keeping only the root hash.\n\n        Args:\n            flags: \"0000\" for trimming, any other value for keeping the nodes.\n        \"\"\"\n", "input": "", "output": "        logger.info(\"Trimming!\")\n        flags = bytearray(flags)\n        length = 1 << self.Depth - 1\n        while len(flags) < length:\n            flags.append(0)\n\n        MerkleTree._TrimNode(self.Root, 0, self.Depth, flags)", "category": "Python"}, {"instruction": "def edge(self, tail_name, head_name, label=None, _attributes=None, **attrs):\n        \"\"\"Create an edge between two nodes.\n\n        Args:\n            tail_name: Start node identifier.\n            head_name: End node identifier.\n            label: Caption to be displayed near the edge.\n            attrs: Any additional edge attributes (must be strings).\n        \"\"\"\n", "input": "", "output": "        tail_name = self._quote_edge(tail_name)\n        head_name = self._quote_edge(head_name)\n        attr_list = self._attr_list(label, attrs, _attributes)\n        line = self._edge % (tail_name, head_name, attr_list)\n        self.body.append(line)", "category": "Python"}, {"instruction": "def is_relation_made(relation, keys='private-address'):\n    '''\n    Determine whether a relation is established by checking for\n    presence of key(s).  If a list of keys is provided, they\n    must all be present for the relation to be identified as made\n    '''\n", "input": "", "output": "    if isinstance(keys, str):\n        keys = [keys]\n    for r_id in relation_ids(relation):\n        for unit in related_units(r_id):\n            context = {}\n            for k in keys:\n                context[k] = relation_get(k, rid=r_id,\n                                          unit=unit)\n            if None not in context.values():\n                return True\n    return False", "category": "Python"}, {"instruction": "def enable_scanners_by_group(self, group):\n        \"\"\"\n        Enables the scanners in the group if it matches one in the scanner_group_map.\n        \"\"\"\n", "input": "", "output": "        if group == 'all':\n            self.logger.debug('Enabling all scanners')\n            return self.zap.ascan.enable_all_scanners()\n\n        try:\n            scanner_list = self.scanner_group_map[group]\n        except KeyError:\n            raise ZAPError(\n                'Invalid group \"{0}\" provided. Valid groups are: {1}'.format(\n                    group, ', '.join(self.scanner_groups)\n                )\n            )\n\n        self.logger.debug('Enabling scanner group {0}'.format(group))\n        return self.enable_scanners_by_ids(scanner_list)", "category": "Python"}, {"instruction": "def listen(self, callback=None):\n        \"\"\"Start the &listen long poll and return immediately.\"\"\"\n", "input": "", "output": "        self._running = True\n        self.loop.create_task(self._async_listen(callback))", "category": "Python"}, {"instruction": "def default_diff(latest_config, current_config):\n    \"\"\"Determine if two revisions have actually changed.\"\"\"\n", "input": "", "output": "    # Pop off the fields we don't care about:\n    pop_no_diff_fields(latest_config, current_config)\n\n    diff = DeepDiff(\n        latest_config,\n        current_config,\n        ignore_order=True\n    )\n    return diff", "category": "Python"}, {"instruction": "def text_width(string, font_name, font_size):\n    \"\"\"Determine with width in pixels of string.\"\"\"\n", "input": "", "output": "    return stringWidth(string, fontName=font_name, fontSize=font_size)", "category": "Python"}, {"instruction": "def state_entry(self, args=None, **kwargs): # pylint: disable=arguments-differ\n        \"\"\"\n        Create an entry state.\n\n        :param args: List of SootArgument values (optional).\n        \"\"\"\n", "input": "", "output": "        state = self.state_blank(**kwargs)\n        # for the Java main method `public static main(String[] args)`,\n        # we add symbolic cmdline arguments\n        if not args and state.addr.method.name == 'main' and \\\n                        state.addr.method.params[0] == 'java.lang.String[]':\n            cmd_line_args = SimSootExpr_NewArray.new_array(state, \"java.lang.String\", BVS('argc', 32))\n            cmd_line_args.add_default_value_generator(self.generate_symbolic_cmd_line_arg)\n            args = [SootArgument(cmd_line_args, \"java.lang.String[]\")]\n            # for referencing the Java array, we need to know the array reference\n            # => saves it in the globals dict\n            state.globals['cmd_line_args'] = cmd_line_args\n        # setup arguments\n        SimEngineSoot.setup_arguments(state, args)\n        return state", "category": "Python"}, {"instruction": "def set_user(self, user):\n        \"\"\"\n        Writes user data to session.\n\n        Args:\n            user: User object\n\n        \"\"\"\n", "input": "", "output": "        self.session['user_id'] = user.key\n        self.session['user_data'] = user.clean_value()\n        role = self.get_role()\n        # TODO: this should be remembered from previous login\n        # self.session['role_data'] = default_role.clean_value()\n        self.session['role_id'] = role.key\n        self.current.role_id = role.key\n        self.current.user_id = user.key\n        # self.perm_cache = PermissionCache(role.key)\n        self.session['permissions'] = role.get_permissions()", "category": "Python"}, {"instruction": "def GetScriptHashesForVerifying(self):\n        \"\"\"\n        Get a list of script hashes for verifying transactions.\n\n        Raises:\n            Exception: if there are no valid transactions to claim from.\n\n        Returns:\n            list: of UInt160 type script hashes.\n        \"\"\"\n", "input": "", "output": "        hashes = super(ClaimTransaction, self).GetScriptHashesForVerifying()\n\n        for hash, group in groupby(self.Claims, lambda x: x.PrevHash):\n            tx, height = Blockchain.Default().GetTransaction(hash)\n\n            if tx is None:\n                raise Exception(\"Invalid Claim Operation\")\n\n            for claim in group:\n                if len(tx.outputs) <= claim.PrevIndex:\n                    raise Exception(\"Invalid Claim Operation\")\n\n                script_hash = tx.outputs[claim.PrevIndex].ScriptHash\n\n                if script_hash not in hashes:\n                    hashes.append(script_hash)\n\n        hashes.sort()\n\n        return hashes", "category": "Python"}, {"instruction": "def main():\n    \"\"\"\n    NAME \n        dipole_plat.py\n\n    DESCRIPTION\t\n        gives paleolatitude from given inclination, assuming GAD field\n\n    SYNTAX\n        dipole_plat.py [command line options]<filename\n\n    OPTIONS\n        -h prints help message and quits\n        -i allows interactive entry of latitude\n        -f file, specifies file name on command line\n    \"\"\"\n", "input": "", "output": "    if '-h' in sys.argv:\n        print(main.__doc__)\n        sys.exit()\n    elif '-f' in sys.argv:\n       ind=sys.argv.index('-f')\n       file=sys.argv[ind+1]\n       f=open(file,'r')\n       data=f.readlines()\n    elif '-i' not in sys.argv:\n       data=sys.stdin.readlines()\n    if '-i' not in sys.argv:\n        for line in data:\n            rec=line.split()\n            print('%7.1f'%(pmag.plat(float(rec[0]))))\n    else: \n       while 1:\n           try:\n               inc=input(\"Inclination for converting to paleolatitude: <cntl-D> to quit \")\n               print('%7.1f'%(pmag.plat(float(inc))))\n           except:\n               print('\\n Good-bye \\n')\n               sys.exit()", "category": "Python"}, {"instruction": "def python_parser(self, obj, *args):\n        \"\"\"operate a python obj\"\"\"\n", "input": "", "output": "        attr, args = args[0], args[1:]\n        item = getattr(obj, attr)\n        if callable(item):\n            item = item(*args)\n        return [item]", "category": "Python"}, {"instruction": "async def get_webhook_info(self) -> types.WebhookInfo:\n        \"\"\"\n        Use this method to get current webhook status. Requires no parameters.\n\n        If the bot is using getUpdates, will return an object with the url field empty.\n\n        Source: https://core.telegram.org/bots/api#getwebhookinfo\n\n        :return: On success, returns a WebhookInfo object\n        :rtype: :obj:`types.WebhookInfo`\n        \"\"\"\n", "input": "", "output": "        payload = generate_payload(**locals())\n\n        result = await self.request(api.Methods.GET_WEBHOOK_INFO, payload)\n        return types.WebhookInfo(**result)", "category": "Python"}, {"instruction": "def connect():\n    \"\"\"Connect controller to handle token exchange and query Uber API.\"\"\"\n", "input": "", "output": "\n    # Exchange authorization code for acceess token and create session\n    session = auth_flow.get_session(request.url)\n    client = UberRidesClient(session)\n\n    # Fetch profile for driver\n    profile = client.get_driver_profile().json\n\n    # Fetch last 50 trips and payments for driver\n    trips = client.get_driver_trips(0, 50).json\n    payments = client.get_driver_payments(0, 50).json\n\n    return render_template('driver_dashboard.html',\n                           profile=profile,\n                           trips=trips['trips'],\n                           payments=payments['payments']\n                           )", "category": "Python"}, {"instruction": "def lambda_tilde(mass1, mass2, lambda1, lambda2):\n    \"\"\" The effective lambda parameter\n\n    The mass-weighted dominant effective lambda parameter defined in\n    https://journals.aps.org/prd/pdf/10.1103/PhysRevD.91.043002\n    \"\"\"\n", "input": "", "output": "    m1, m2, lambda1, lambda2, input_is_array = ensurearray(\n        mass1, mass2, lambda1, lambda2)\n    lsum = lambda1 + lambda2\n    ldiff, _ = ensurearray(lambda1 - lambda2)\n    mask = m1 < m2\n    ldiff[mask] = -ldiff[mask]\n    eta = eta_from_mass1_mass2(m1, m2)\n    p1 = (lsum) * (1 + 7. * eta - 31 * eta ** 2.0)\n    p2 = (1 - 4 * eta)**0.5 * (1 + 9 * eta - 11 * eta ** 2.0) * (ldiff)\n    return formatreturn(8.0 / 13.0 * (p1 + p2), input_is_array)", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"\n        Closes the lvm and vg_t handle. Usually you would never need to use this method\n        unless you are doing operations using the ctypes function wrappers in conversion.py\n\n        *Raises:*\n\n        *       HandleError\n        \"\"\"\n", "input": "", "output": "        if self.handle:\n            cl = lvm_vg_close(self.handle)\n            if cl != 0:\n                raise HandleError(\"Failed to close VG handle after init check.\")\n            self.__vgh = None\n            self.lvm.close()", "category": "Python"}, {"instruction": "def isatty_from_env(cls, env):\n    \"\"\"Determine whether remote file descriptors are tty capable using std nailgunned env variables.\n\n    :param dict env: A dictionary representing the environment.\n    :returns: A tuple of boolean values indicating istty or not for (stdin, stdout, stderr).\n    \"\"\"\n", "input": "", "output": "    def str_int_bool(i):\n      return i.isdigit() and bool(int(i))  # Environment variable values should always be strings.\n\n    return tuple(\n      str_int_bool(env.get(cls.TTY_ENV_TMPL.format(fd_id), '0')) for fd_id in STDIO_DESCRIPTORS\n    )", "category": "Python"}, {"instruction": "def geo_search(self, radius, node=None, center=False, **kwargs):\n        '''Get a list of nodes whose coordinates are closer than *radius* to *node*.'''\n", "input": "", "output": "        node = as_node(node if node is not None else self)\n\n        G = self.subgraph(**kwargs)\n\n        pos = nx.get_node_attributes(G, 'pos')\n        if not pos:\n            return []\n        nodes, coords = list(zip(*pos.items()))\n        kdtree = KDTree(coords)  # Cannot provide generator.\n        indices = kdtree.query_ball_point(pos[node], radius)\n        return [nodes[i] for i in indices if center or (nodes[i] != node)]", "category": "Python"}, {"instruction": "def create(version):\n        \"\"\"Return a new instance for *version*, which can be either `'1.0'`\n        or `'2.0'`.\"\"\"\n", "input": "", "output": "        clsname = 'JsonRpcV{}'.format(version.rstrip('.0'))\n        cls = globals()[clsname]\n        return cls(version)", "category": "Python"}, {"instruction": "def handle_uploaded_file(file, folder=None, is_public=True):\n    '''handle uploaded file to folder\n    match first media type and create media object and returns it\n\n    file: File object\n    folder: str or Folder isinstance\n    is_public: boolean\n    '''\n", "input": "", "output": "    _folder = None\n\n    if folder and isinstance(folder, Folder):\n        _folder = folder\n    elif folder:\n        _folder, folder_created = Folder.objects.get_or_create(\n            name=folder)\n\n    for cls in MEDIA_MODELS:\n        if cls.matches_file_type(file.name):\n\n            obj, created = cls.objects.get_or_create(\n                original_filename=file.name,\n                file=file,\n                folder=_folder,\n                is_public=is_public)\n\n            if created:\n                return obj\n\n    return None", "category": "Python"}, {"instruction": "def set_filters(query, base_filters):\n    \"\"\"Put together all filters we have and set them as 'and' filter\n    within filtered query.\n\n    :param query: elastic query being constructed\n    :param base_filters: all filters set outside of query (eg. resource config, sub_resource_lookup)\n    \"\"\"\n", "input": "", "output": "    filters = [f for f in base_filters if f is not None]\n    query_filter = query['query']['filtered'].get('filter', None)\n    if query_filter is not None:\n        if 'and' in query_filter:\n            filters.extend(query_filter['and'])\n        else:\n            filters.append(query_filter)\n    if filters:\n        query['query']['filtered']['filter'] = {'and': filters}", "category": "Python"}, {"instruction": "def get_climate(self, device_label):\n        \"\"\" Get climate history\n        Args:\n            device_label: device label of climate device\n        \"\"\"\n", "input": "", "output": "        response = None\n        try:\n            response = requests.get(\n                urls.climate(self._giid),\n                headers={\n                    'Accept': 'application/json, text/javascript, */*; q=0.01',\n                    'Cookie': 'vid={}'.format(self._vid)},\n                params={\n                    \"deviceLabel\": device_label})\n        except requests.exceptions.RequestException as ex:\n            raise RequestError(ex)\n        _validate_response(response)\n        return json.loads(response.text)", "category": "Python"}, {"instruction": "def command_umount(self, system_id, *system_ids):\n        \"\"\"Unmounts the specified sftp system.\n        Usage: sftpman umount {id}..\n        \"\"\"\n", "input": "", "output": "        system_ids = (system_id,) + system_ids\n        has_failed = False\n        for system_id in system_ids:\n            try:\n                system = SystemModel.create_by_id(system_id, self.environment)\n                controller = SystemControllerModel(system, self.environment)\n                controller.unmount()\n            except SftpConfigException as e:\n                sys.stderr.write('Cannot unmount %s: %s\\n\\n' % (system_id, str(e)))\n                has_failed = True\n        if has_failed:\n            sys.exit(1)", "category": "Python"}, {"instruction": "def parse_single_report(f):\n    \"\"\" Parse a samtools idxstats idxstats \"\"\"\n", "input": "", "output": "\n    parsed_data = OrderedDict()\n    for l in f.splitlines():\n        s = l.split(\"\\t\")\n        try:\n            parsed_data[s[0]] = int(s[2])\n        except (IndexError, ValueError):\n            pass\n    return parsed_data", "category": "Python"}, {"instruction": "def _get_assessment_part_lookup_session(self):\n        \"\"\"need to account for magic parts\"\"\"\n", "input": "", "output": "        section = getattr(self, '_assessment_section', None)\n        session = get_assessment_part_lookup_session(self._runtime,\n                                                     self._proxy,\n                                                     section)\n        session.use_unsequestered_assessment_part_view()\n        session.use_federated_bank_view()\n        return session", "category": "Python"}, {"instruction": "def get_user(self, username):\n        \"\"\"Get user information.\n\n        :param str username: User to get info on.\n        \"\"\"\n", "input": "", "output": "\n        r = self._query_('/users/%s' % username, 'GET')\n\n        result = User(r.json())\n        return result", "category": "Python"}, {"instruction": "def _showxy(self, viewer, data_x, data_y):\n        \"\"\"Update the info from the last position recorded under the cursor.\n        \"\"\"\n", "input": "", "output": "        self._cursor_last_update = time.time()\n        try:\n            image = viewer.get_image()\n            if (image is None) or not isinstance(image, BaseImage.BaseImage):\n                # No compatible image loaded for this channel\n                return\n\n            settings = viewer.get_settings()\n            info = image.info_xy(data_x, data_y, settings)\n\n            # Are we reporting in data or FITS coordinates?\n            off = self.settings.get('pixel_coords_offset', 0.0)\n            info.x += off\n            info.y += off\n\n        except Exception as e:\n            self.logger.warning(\n                \"Can't get info under the cursor: %s\" % (str(e)))\n            return\n\n        # TODO: can this be made more efficient?\n        chname = self.get_channel_name(viewer)\n        channel = self.get_channel(chname)\n\n        self.make_callback('field-info', channel, info)\n\n        self.update_pending()\n        return True", "category": "Python"}, {"instruction": "def do_stopInstance(self,args):\n        \"\"\"Stop specified instance\"\"\"\n", "input": "", "output": "        parser = CommandArgumentParser(\"stopInstance\")\n        parser.add_argument(dest='instance',help='instance index or name');\n        parser.add_argument('-f','--force',action='store_true',dest='force',help='instance index or name');\n        args = vars(parser.parse_args(args))\n\n        instanceId = args['instance']\n        force = args['force']\n        try:\n            index = int(instanceId)\n            instances = self.scalingGroupDescription['AutoScalingGroups'][0]['Instances']\n            instanceId = instances[index]\n        except ValueError:\n            pass\n\n        client = AwsConnectionFactory.getEc2Client()\n        client.stop_instances(InstanceIds=[instanceId['InstanceId']],Force=force)", "category": "Python"}, {"instruction": "def get_selected_thread(self):\n        \"\"\"returns currently selected :class:`~alot.db.Thread`\"\"\"\n", "input": "", "output": "        threadlinewidget = self.get_selected_threadline()\n        thread = None\n        if threadlinewidget:\n            thread = threadlinewidget.get_thread()\n        return thread", "category": "Python"}, {"instruction": "def addDerivesFrom(self, child_id, parent_id):\n        \"\"\"\n        We add a derives_from relationship between the child and parent id.\n        Examples of uses include between:\n        an allele and a construct or strain here,\n        a cell line and it's parent genotype.  Adding the parent and child to\n        the graph should happen outside of this function call to ensure graph\n        integrity.\n        :param child_id:\n        :param parent_id:\n        :return:\n\n        \"\"\"\n", "input": "", "output": "\n        self.graph.addTriple(\n            child_id, self.globaltt['derives_from'], parent_id)\n\n        return", "category": "Python"}, {"instruction": "def handleDetailDblClick( self, item ):\r\n        \"\"\"\r\n        Handles when a detail item is double clicked on.\r\n        \r\n        :param      item | <QTreeWidgetItem>\r\n        \"\"\"\n", "input": "", "output": "        if ( isinstance(item, XOrbRecordItem) ):\r\n            self.emitRecordDoubleClicked(item.record())", "category": "Python"}, {"instruction": "def process_exception(self, request, exception):\n        \"\"\"\n        Add user details.\n        \"\"\"\n", "input": "", "output": "        if request.user and hasattr(request.user, 'email'):\n            request.META['USER'] = request.user.email", "category": "Python"}, {"instruction": "def token(cls: Type[CSVType], time: int) -> CSVType:\n        \"\"\"\n        Return CSV instance from time\n\n        :param time: Timestamp\n        :return:\n        \"\"\"\n", "input": "", "output": "        csv = cls()\n        csv.time = str(time)\n        return csv", "category": "Python"}, {"instruction": "def convert_language_code(django_lang):\n    \"\"\"\n    Converts Django language codes \"ll-cc\" into ISO codes \"ll_CC\" or \"ll\"\n\n    :param django_lang: Django language code as ll-cc\n    :type django_lang: str\n    :return: ISO language code as ll_CC\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    lang_and_country = django_lang.split('-')\n    try:\n        return '_'.join((lang_and_country[0], lang_and_country[1].upper()))\n    except IndexError:\n        return lang_and_country[0]", "category": "Python"}, {"instruction": "def start_class(self, class_):\n        \"\"\"\n        Start all services of a given class. If this manager doesn't already\n        have a service of that class, it constructs one and starts it.\n        \"\"\"\n", "input": "", "output": "        matches = filter(lambda svc: isinstance(svc, class_), self)\n        if not matches:\n            svc = class_()\n            self.register(svc)\n            matches = [svc]\n        map(self.start, matches)\n        return matches", "category": "Python"}, {"instruction": "def p_program_tokenstring(p):\n    \"\"\" program : defs NEWLINE\n    \"\"\"\n", "input": "", "output": "    try:\n        tmp = [str(x()) if isinstance(x, MacroCall) else x for x in p[1]]\n    except PreprocError as v:\n        error(v.lineno, v.message)\n\n    tmp.append(p[2])\n    p[0] = tmp", "category": "Python"}, {"instruction": "def at_index(self, pos, predicate=None, index=None):\n        \"\"\"\n        Retrieves a list of matches from given position\n        \"\"\"\n", "input": "", "output": "        return filter_index(self._index_dict[pos], predicate, index)", "category": "Python"}, {"instruction": "def getUserAgent():\n\t'''\n\tGenerate a randomized user agent by permuting a large set of possible values.\n\tThe returned user agent should look like a valid, in-use brower, with a specified preferred language of english.\n\n\tReturn value is a list of tuples, where each tuple is one of the user-agent headers.\n\n\tCurrently can provide approximately 147 * 17 * 5 * 5 * 2 * 3 * 2 values, or ~749K possible\n\tunique user-agents.\n\t'''\n", "input": "", "output": "\n\tcoding = random.choice(ENCODINGS)\n\trandom.shuffle(coding)\n\tcoding = random.choice((\", \", \",\")).join(coding)\n\n\taccept_list = [tmp for tmp in random.choice(ACCEPT)]\n\taccept_list.append(random.choice(ACCEPT_POSTFIX))\n\taccept_str = random.choice((\", \", \",\")).join(accept_list)\n\n\tassert accept_str.count(\"*.*\") <= 1\n\n\tuser_agent = [\n\t\t\t\t('User-Agent'\t\t,\trandom.choice(USER_AGENTS)),\n\t\t\t\t('Accept-Language'\t,\trandom.choice(ACCEPT_LANGUAGE)),\n\t\t\t\t('Accept'\t\t\t,\taccept_str),\n\t\t\t\t('Accept-Encoding'\t,\tcoding)\n\t\t\t\t]\n\treturn user_agent", "category": "Python"}, {"instruction": "def _hex_to_rgba(hexs):\n    \"\"\"Convert hex to rgba, permitting alpha values in hex\"\"\"\n", "input": "", "output": "    hexs = np.atleast_1d(np.array(hexs, '|U9'))\n    out = np.ones((len(hexs), 4), np.float32)\n    for hi, h in enumerate(hexs):\n        assert isinstance(h, string_types)\n        off = 1 if h[0] == '#' else 0\n        assert len(h) in (6+off, 8+off)\n        e = (len(h)-off) // 2\n        out[hi, :e] = [int(h[i:i+2], 16) / 255.\n                       for i in range(off, len(h), 2)]\n    return out", "category": "Python"}, {"instruction": "def box_cox(table):\n    \"\"\"\n    box-cox transform table\n    \"\"\"\n", "input": "", "output": "    from scipy.stats import boxcox as bc\n    t = []\n    for i in table:\n        if min(i) == 0:\n            scale = min([j for j in i if j != 0]) * 10e-10\n        else:\n            scale = 0\n        t.append(np.ndarray.tolist(bc(np.array([j + scale for j in i]))[0]))\n    return t", "category": "Python"}, {"instruction": "def file_cmd(context, tags, archive, bundle_name, path):\n    \"\"\"Add a file to a bundle.\"\"\"\n", "input": "", "output": "    bundle_obj = context.obj['db'].bundle(bundle_name)\n    if bundle_obj is None:\n        click.echo(click.style(f\"unknown bundle: {bundle_name}\", fg='red'))\n        context.abort()\n    version_obj = bundle_obj.versions[0]\n    new_file = context.obj['db'].new_file(\n        path=str(Path(path).absolute()),\n        to_archive=archive,\n        tags=[context.obj['db'].tag(tag_name) if context.obj['db'].tag(tag_name) else\n              context.obj['db'].new_tag(tag_name) for tag_name in tags]\n    )\n    new_file.version = version_obj\n    context.obj['db'].add_commit(new_file)\n    click.echo(click.style(f\"new file added: {new_file.path} ({new_file.id})\", fg='green'))", "category": "Python"}, {"instruction": "def current_hold_price(self):\n        \"\"\"\u8ba1\u7b97\u76ee\u524d\u6301\u4ed3\u7684\u6210\u672c  \u7528\u4e8e\u6a21\u62df\u76d8\u548c\u5b9e\u76d8\u67e5\u8be2\n\n        Returns:\n            [type] -- [description]\n        \"\"\"\n", "input": "", "output": "        \n        def weights(x):\n            n=len(x)\n            res=1\n            while res>0 or res<0:\n                res=sum(x[:n]['amount'])\n                n=n-1\n            \n            x=x[n+1:]     \n            \n            if sum(x['amount']) != 0:\n                return np.average(\n                    x['price'],\n                    weights=x['amount'],\n                    returned=True\n                )\n            else:\n                return np.nan\n        return self.history_table.set_index(\n            'datetime',\n            drop=False\n        ).sort_index().groupby('code').apply(weights).dropna()", "category": "Python"}, {"instruction": "def buildType(valtype, extra=[]):\n        \"\"\"Build a Type\n\n        :param str valtype: A type code to be used with the 'value' field.  Must be an array\n        :param list extra: A list of tuples describing additional non-standard fields\n        :returns: A :py:class:`Type`\n        \"\"\"\n", "input": "", "output": "        assert valtype[:1] == 'a', 'valtype must be an array'\n        return Type(id=\"epics:nt/NTMultiChannel:1.0\",\n                    spec=[\n                    ('value', valtype),\n                    ('channelName', 'as'),\n                    ('descriptor', 's'),\n                    ('alarm', alarm),\n                    ('timeStamp', timeStamp),\n                    ('severity', 'ai'),\n                    ('status', 'ai'),\n                    ('message', 'as'),\n                    ('secondsPastEpoch', 'al'),\n                    ('nanoseconds', 'ai'),\n                    ('userTag', 'ai'),\n                    ('isConnected', 'a?'),\n                    ] + extra)", "category": "Python"}, {"instruction": "def _check_row_size(self, array):\n        \"\"\"Check that the specified array fits the previous rows size\n        \"\"\"\n", "input": "", "output": "\n        if not self._row_size:\n            self._row_size = len(array)\n        elif self._row_size != len(array):\n            raise ArraySizeError, \"array should contain %d elements\" \\\n                % self._row_size", "category": "Python"}, {"instruction": "def _maybe_apply_time_shift(da, time_offset=None, **DataAttrs):\n        \"\"\"Correct off-by-one error in GFDL instantaneous model data.\n\n        Instantaneous data that is outputted by GFDL models is generally off by\n        one timestep.  For example, a netCDF file that is supposed to\n        correspond to 6 hourly data for the month of January, will have its\n        last time value be in February.\n        \"\"\"\n", "input": "", "output": "        if time_offset is not None:\n            time = times.apply_time_offset(da[TIME_STR], **time_offset)\n            da[TIME_STR] = time\n        else:\n            if DataAttrs['dtype_in_time'] == 'inst':\n                if DataAttrs['intvl_in'].endswith('hr'):\n                    offset = -1 * int(DataAttrs['intvl_in'][0])\n                else:\n                    offset = 0\n                time = times.apply_time_offset(da[TIME_STR], hours=offset)\n                da[TIME_STR] = time\n        return da", "category": "Python"}, {"instruction": "def start_handling_includes(self, t=None):\n        \"\"\"\n        Causes the PreProcessor object to start processing #import,\n        #include and #include_next lines.\n\n        This method will be called when a #if, #ifdef, #ifndef or #elif\n        evaluates True, or when we reach the #else in a #if, #ifdef,\n        #ifndef or #elif block where a condition already evaluated\n        False.\n\n        \"\"\"\n", "input": "", "output": "        d = self.dispatch_table\n        p = self.stack[-1] if self.stack else self.default_table\n\n        for k in ('import', 'include', 'include_next'):\n            d[k] = p[k]", "category": "Python"}, {"instruction": "def _get_formatoptions(cls, include_bases=True):\n        \"\"\"\n        Iterator over formatoptions\n\n        This class method returns an iterator that contains all the\n        formatoptions descriptors that are in this class and that are defined\n        in the base classes\n\n        Notes\n        -----\n        There is absolutely no need to call this method besides the plotter\n        initialization, since all formatoptions are in the plotter itself.\n        Just type::\n\n        >>> list(plotter)\n\n        to get the formatoptions.\n\n        See Also\n        --------\n        _format_keys\"\"\"\n", "input": "", "output": "        def base_fmtos(base):\n            return filter(\n                lambda key: isinstance(getattr(cls, key), Formatoption),\n                getattr(base, '_get_formatoptions', empty)(False))\n\n        def empty(*args, **kwargs):\n            return list()\n        fmtos = (attr for attr, obj in six.iteritems(cls.__dict__)\n                 if isinstance(obj, Formatoption))\n        if not include_bases:\n            return fmtos\n        return unique_everseen(chain(fmtos, *map(base_fmtos, cls.__mro__)))", "category": "Python"}, {"instruction": "def arm(self, wait=True, timeout=None):\n        '''Arm the vehicle.\n\n        If wait is True, wait for arm operation to complete before\n        returning.  If timeout is nonzero, raise a TimeouTerror if the\n        vehicle has not armed after timeout seconds.\n        '''\n", "input": "", "output": "\n        self.armed = True\n\n        if wait:\n            self.wait_for(lambda: self.armed, timeout=timeout,\n                          errmsg='failed to arm vehicle')", "category": "Python"}, {"instruction": "def add_user(config, group, username):\n        \"\"\"Add specified user to specified group.\"\"\"\n", "input": "", "output": "        client = Client()\n        client.prepare_connection()\n        group_api = API(client)\n        try:\n            group_api.add_user(group, username)\n        except ldap_tools.exceptions.NoGroupsFound:  # pragma: no cover\n            print(\"Group ({}) not found\".format(group))\n        except ldap_tools.exceptions.TooManyResults:  # pragma: no cover\n            print(\"Query for group ({}) returned multiple results.\".format(\n                group))\n        except ldap3.TYPE_OR_VALUE_EXISTS:  # pragma: no cover\n            print(\"{} already exists in {}\".format(username, group))", "category": "Python"}, {"instruction": "def more_mem_per_proc(self, factor=1):\n        \"\"\"\n        Method to increase the amount of memory asked for, by factor.\n        Return: new memory if success, 0 if memory cannot be increased.\n        \"\"\"\n", "input": "", "output": "        base_increase = 2000\n        old_mem = self.mem_per_proc\n        new_mem = old_mem + factor*base_increase\n\n        if new_mem < self.hw.mem_per_node:\n            self.set_mem_per_proc(new_mem)\n            return new_mem\n\n        raise self.Error('could not increase mem_per_proc further')", "category": "Python"}, {"instruction": "def list_farms():\n    \"\"\"\n    List all farms you can manage. If you have selected a farm already, the\n    name of that farm will be prefixed with an asterisk in the returned list.\n    \"\"\"\n", "input": "", "output": "    utils.check_for_cloud_server()\n    utils.check_for_cloud_user()\n    server = Server(config[\"cloud_server\"][\"url\"])\n    server.log_in(\n        config[\"cloud_server\"][\"username\"], config[\"cloud_server\"][\"password\"]\n    )\n    farms_list = server.get_user_info().get(\"farms\", [])\n    if not len(farms_list):\n        raise click.ClickException(\n            \"No farms exist. Run `openag cloud create_farm` to create one\"\n        )\n    active_farm_name = config[\"cloud_server\"][\"farm_name\"]\n    for farm_name in farms_list:\n        if farm_name == active_farm_name:\n            click.echo(\"*\"+farm_name)\n        else:\n            click.echo(farm_name)", "category": "Python"}, {"instruction": "def get(self, table_name):\n        \"\"\"Load table class by name, class not yet initialized\"\"\"\n", "input": "", "output": "        assert table_name in self.tabs, \\\n            \"Table not avaiable. Avaiable tables: {}\".format(\n                \", \".join(self.tabs.keys())\n            )\n        return self.tabs[table_name]", "category": "Python"}, {"instruction": "def cli(ctx, editor):\n    \"\"\"Edit saved commands.\"\"\"\n", "input": "", "output": "\n    commands = utils.read_commands()\n    if commands is []:\n        click.echo(\"No commands to edit, Add one by 'keep new'. \")\n    else:\n        edit_header = \"# Unchanged file will abort the operation\\n\"\n        new_commands = utils.edit_commands(commands, editor, edit_header)\n        if new_commands and new_commands != commands:\n            click.echo(\"Replace:\\n\")\n            click.secho(\"\\t{}\".format('\\n\\t'.join(utils.format_commands(commands))),\n                        fg=\"green\")\n            click.echo(\"With:\\n\\t\")\n            click.secho(\"\\t{}\".format('\\n\\t'.join(utils.format_commands(new_commands))),\n                        fg=\"green\")\n            if click.confirm(\"\", default=False):\n                utils.write_commands(new_commands)", "category": "Python"}, {"instruction": "def simulationStep(self, step=0):\n        \"\"\"\n        Make a simulation step and simulate up to the given millisecond in sim time.\n        If the given value is 0 or absent, exactly one step is performed.\n        Values smaller than or equal to the current sim time result in no action.\n        \"\"\"\n", "input": "", "output": "        self._queue.append(tc.CMD_SIMSTEP)\n        self._string += struct.pack(\"!BBi\", 1 +\n                                    1 + 4, tc.CMD_SIMSTEP, step)\n        result = self._sendExact()\n        for subscriptionResults in self._subscriptionMapping.values():\n            subscriptionResults.reset()\n        numSubs = result.readInt()\n        responses = []\n        while numSubs > 0:\n            responses.append(self._readSubscription(result))\n            numSubs -= 1\n        return responses", "category": "Python"}, {"instruction": "def connect(self):\n        \"\"\"\u4e0e\u8fdc\u7aef\u5efa\u7acb\u8fde\u63a5,\u5e76\u8fdb\u884c\u9a8c\u8bc1\u8eab\u4efd.\"\"\"\n", "input": "", "output": "        self._client = Telnet(self.hostname, self.port)\n        self.closed = False\n        query = {\n            \"MPRPC\": self.VERSION,\n            \"AUTH\": {\n                \"USERNAME\": self.username,\n                \"PASSWORD\": self.password\n            }\n        }\n        queryb = self.encoder(query)\n        if self.debug is True:\n            print(\"send auth {}\".format(queryb))\n        self._client.write(queryb)\n        self.remote_info = self._responsehandler()", "category": "Python"}, {"instruction": "def _serialize_default(cls, obj):\n        \"\"\"\n        :type obj: int|str|bool|float|bytes|unicode|list|dict|object\n\n        :rtype: int|str|bool|list|dict\n        \"\"\"\n", "input": "", "output": "\n        if obj is None or cls._is_primitive(obj):\n            return obj\n        elif cls._is_bytes(obj):\n            return obj.decode()\n        elif type(obj) == list:\n            return cls._serialize_list(obj)\n        else:\n            dict_ = cls._get_obj_raw(obj)\n\n            return cls._serialize_dict(dict_)", "category": "Python"}, {"instruction": "def _onEncoding(self, encString, line, pos, absPosition):\n        \"\"\"Memorizes module encoding\"\"\"\n", "input": "", "output": "        self.encoding = Encoding(encString, line, pos, absPosition)", "category": "Python"}, {"instruction": "def _remove_tags(conn, load_balancer_names, tags):\n    '''\n    Delete metadata tags for the specified resource ids.\n\n    :type load_balancer_names: list\n    :param load_balancer_names: A list of load balancer names.\n\n    :type tags: list\n    :param tags: A list containing just tag names for the tags to be\n                 deleted.\n    '''\n", "input": "", "output": "    params = {}\n    conn.build_list_params(params, load_balancer_names,\n                           'LoadBalancerNames.member.%d')\n    conn.build_list_params(params, tags,\n                           'Tags.member.%d.Key')\n    return conn.get_status('RemoveTags', params, verb='POST')", "category": "Python"}, {"instruction": "def transaction_abort(self, transaction_id, **kwargs):\n        \"\"\"Abort a transaction and roll back all operations.\n        :param transaction_id: ID of transaction to be aborted.\n        :param **kwargs: Further parameters for the transport layer.\n        \"\"\"\n", "input": "", "output": "        if transaction_id not in self.__transactions:\n            raise workflows.Error(\"Attempting to abort unknown transaction\")\n        self.log.debug(\"Aborting transaction %s\", transaction_id)\n        self.__transactions.remove(transaction_id)\n        self._transaction_abort(transaction_id, **kwargs)", "category": "Python"}, {"instruction": "def create(self, project, subject, **attrs):\n        \"\"\"\n        Create a new :class:`UserStory`.\n\n        :param project: :class:`Project` id\n        :param subject: subject of the :class:`UserStory`\n        :param attrs: optional attributes of the :class:`UserStory`\n        \"\"\"\n", "input": "", "output": "        attrs.update({'project': project, 'subject': subject})\n        return self._new_resource(payload=attrs)", "category": "Python"}, {"instruction": "def unlock(arguments):\n    \"\"\"Unlock the database.\"\"\"\n", "input": "", "output": "    import redis\n    u = coil.utils.ask(\"Redis URL\", \"redis://localhost:6379/0\")\n    db = redis.StrictRedis.from_url(u)\n    db.set('site:lock', 0)\n    print(\"Database unlocked.\")\n    return 0", "category": "Python"}, {"instruction": "def normalize(seq):\n    \"\"\"\n    Scales each number in the sequence so that the sum of all numbers equals 1.\n    \"\"\"\n", "input": "", "output": "    s = float(sum(seq))\n    return [v/s for v in seq]", "category": "Python"}, {"instruction": "def _pad_string(self, str, colwidth):\n        \"\"\"Center-pads a string to the given column width using spaces.\"\"\"\n", "input": "", "output": "        width = self._disp_width(str)\n        prefix = (colwidth - 1 - width) // 2\n        suffix = colwidth - prefix - width\n        return ' ' * prefix + str + ' ' * suffix", "category": "Python"}, {"instruction": "def decode_produce_response(cls, data):\n        \"\"\"\n        Decode bytes to a ProduceResponse\n\n        :param bytes data: bytes to decode\n        :returns: iterable of `afkak.common.ProduceResponse`\n        \"\"\"\n", "input": "", "output": "        ((correlation_id, num_topics), cur) = relative_unpack('>ii', data, 0)\n\n        for _i in range(num_topics):\n            topic, cur = read_short_ascii(data, cur)\n            ((num_partitions,), cur) = relative_unpack('>i', data, cur)\n            for _i in range(num_partitions):\n                ((partition, error, offset), cur) = relative_unpack('>ihq', data, cur)\n\n                yield ProduceResponse(topic, partition, error, offset)", "category": "Python"}, {"instruction": "def add_journal(self, data):\n        \"\"\"\n        This method include new journals to the ArticleMeta.\n\n        data: legacy SciELO Documents JSON Type 3.\n        \"\"\"\n", "input": "", "output": "\n        journal = self.dispatcher(\n            'add_journal',\n            data,\n            self._admintoken\n        )\n\n        return json.loads(journal)", "category": "Python"}, {"instruction": "def get_template(self, template_name, **parameters):\n        \"\"\"Pull templates from the AWS templates folder\"\"\"\n", "input": "", "output": "        template_path = pathlib.Path(self.template_dir).joinpath(template_name)\n        return get_template(template_path, **parameters)", "category": "Python"}, {"instruction": "def send_photo(self, peer: Peer, photo: str, caption: str=None, reply: int=None, on_success: callable=None,\n                   reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send photo to peer.\n        :param peer: Peer to send message to.\n        :param photo: File path to photo to send.\n        :param caption: Caption for photo\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n", "input": "", "output": "        if isinstance(reply, Message):\n            reply = reply.id\n\n        photo = botapi.InputFile('photo', botapi.InputFileInfo(photo, open(photo, 'rb'), get_mimetype(photo)))\n\n        botapi.send_photo(chat_id=peer.id, photo=photo, caption=caption, reply_to_message_id=reply, on_success=on_success,\n                          reply_markup=reply_markup, **self.request_args).run()", "category": "Python"}, {"instruction": "def _server_connect(self, s):\n        \"\"\"\n        Sets up a TCP connection to the server.\n        \"\"\"\n", "input": "", "output": "        self._socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self._socket.setblocking(0)\n        self._socket.settimeout(1.0)\n\n        if self.options[\"tcp_nodelay\"]:\n            self._socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n\n        self.io = tornado.iostream.IOStream(self._socket,\n            max_buffer_size=self._max_read_buffer_size,\n            max_write_buffer_size=self._max_write_buffer_size,\n            read_chunk_size=self._read_chunk_size)\n\n        # Connect to server with a deadline\n        future = self.io.connect((s.uri.hostname, s.uri.port))\n        yield tornado.gen.with_timeout(\n            timedelta(seconds=self.options[\"connect_timeout\"]), future)\n\n        # Called whenever disconnected from the server.\n        self.io.set_close_callback(self._process_op_err)", "category": "Python"}, {"instruction": "def remove_path(target_path):\n    \"\"\" Delete the target path \"\"\"\n", "input": "", "output": "    if os.path.isdir(target_path):\n        shutil.rmtree(target_path)\n    else:\n        os.unlink(target_path)", "category": "Python"}, {"instruction": "def use(self, url, name='mytable'):\n        '''Changes the data provider\n        >>> yql.use('http://myserver.com/mytables.xml')\n        '''\n", "input": "", "output": "        self.yql_table_url = url\n        self.yql_table_name = name\n        return {'table url': url, 'table name': name}", "category": "Python"}, {"instruction": "def _init_map(self):\n        \"\"\"call these all manually because non-cooperative\"\"\"\n", "input": "", "output": "        DecimalAnswerFormRecord._init_map(self)\n        DecimalValuesFormRecord._init_map(self)\n        TextAnswerFormRecord._init_map(self)\n        TextsFormRecord._init_map(self)\n        super(edXNumericResponseAnswerFormRecord, self)._init_map()", "category": "Python"}, {"instruction": "def import_status(handler, host=None, core_name=None, verbose=False):\n    '''\n    Submits an import command to the specified handler using specified options.\n    This command can only be run if the minion is configured with\n    solr.type: 'master'\n\n    handler : str\n        The name of the data import handler.\n    host : str (None)\n        The solr host to query. __opts__['host'] is default.\n    core : str (None)\n        The core the handler belongs to.\n    verbose : boolean (False)\n        Specifies verbose output\n\n    Return : dict<str,obj>::\n\n        {'success':boolean, 'data':dict, 'errors':list, 'warnings':list}\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' solr.import_status dataimport None music False\n    '''\n", "input": "", "output": "    if not _is_master() and _get_none_or_value(host) is None:\n        errors = ['solr.import_status can only be called by \"master\" minions']\n        return _get_return_dict(False, errors=errors)\n\n    extra = [\"command=status\"]\n    if verbose:\n        extra.append(\"verbose=true\")\n    url = _format_url(handler, host=host, core_name=core_name, extra=extra)\n    return _http_request(url)", "category": "Python"}, {"instruction": "def get_keyword(self, collection_id, name):\n        \"\"\"Get a specific keyword from a library\"\"\"\n", "input": "", "output": "        sql = ", "category": "Python"}, {"instruction": "def setOffset(self, value):\n        \"\"\"\n        Sets the offset of the L{WriteData} stream object in wich the data is written.\n        \n        @type value: int\n        @param value: Integer value that represent the offset we want to start writing in the L{WriteData} stream.\n            \n        @raise WrongOffsetValueException: The value is beyond the total length of the data. \n        \"\"\"\n", "input": "", "output": "        if value >= len(self.data.getvalue()):\n            raise excep.WrongOffsetValueException(\"Wrong offset value. Must be less than %d\" % len(self.data))\n        self.data.seek(value)", "category": "Python"}, {"instruction": "def from_urdf_file(cls, urdf_file, base_elements=None, last_link_vector=None, base_element_type=\"link\", active_links_mask=None, name=\"chain\"):\n        \"\"\"Creates a chain from an URDF file\n\n        Parameters\n        ----------\n        urdf_file: str\n            The path of the URDF file\n        base_elements: list of strings\n            List of the links beginning the chain\n        last_link_vector: numpy.array\n            Optional : The translation vector of the tip.\n        name: str\n            The name of the Chain\n        base_element_type: str\n        active_links_mask: list[bool]\n        \"\"\"\n", "input": "", "output": "        if base_elements is None:\n            base_elements = [\"base_link\"]\n\n        links = URDF_utils.get_urdf_parameters(urdf_file, base_elements=base_elements, last_link_vector=last_link_vector, base_element_type=base_element_type)\n        # Add an origin link at the beginning\n        return cls([link_lib.OriginLink()] + links, active_links_mask=active_links_mask, name=name)", "category": "Python"}, {"instruction": "def clear_text(self):\n        \"\"\"Clears the text.\n\n        raise:  NoAccess - ``Metadata.isRequired()`` or\n                ``Metadata.isReadOnly()`` is ``true``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for osid.repository.AssetForm.clear_title_template\n        if (self.get_text_metadata().is_read_only() or\n                self.get_text_metadata().is_required()):\n            raise errors.NoAccess()\n        self._my_map['text'] = dict(self._text_default)", "category": "Python"}, {"instruction": "def translate_sites(self, indices=None, vector=None):\n        \"\"\"\n        Translate specific sites by some vector, keeping the sites within the\n        unit cell.\n\n        Args:\n            indices (list): List of site indices on which to perform the\n                translation.\n            vector (3x1 array): Translation vector for sites.\n        \"\"\"\n", "input": "", "output": "        if indices is None:\n            indices = range(len(self))\n        if vector is None:\n            vector == [0, 0, 0]\n        for i in indices:\n            site = self._sites[i]\n            new_site = Site(site.species, site.coords + vector,\n                            properties=site.properties)\n            self._sites[i] = new_site", "category": "Python"}, {"instruction": "def validate_obj(keys, obj):\n    \"\"\"Super simple \"object\" validation.\"\"\"\n", "input": "", "output": "    msg = ''\n    for k in keys:\n        if isinstance(k, str):\n            if k not in obj or (not isinstance(obj[k], list) and not obj[k]):\n                if msg:\n                    msg = \"%s,\" % msg\n\n                msg = \"%s%s\" % (msg, k)\n        elif isinstance(k, list):\n            found = False\n            for k_a in k:\n                if k_a in obj:\n                    found = True\n\n            if not found:\n                if msg:\n                    msg = \"%s,\" % msg\n\n                msg = \"%s(%s\" % (msg, ','.join(k))\n\n    if msg:\n        msg = \"%s missing\" % msg\n\n    return msg", "category": "Python"}, {"instruction": "def read_simulation_temps(pathname,NumTemps):\n    \"\"\"Reads in the various temperatures from each TEMP#/simul.output file by knowing\n        beforehand the total number of temperatures (parameter at top)\n    \"\"\"\n", "input": "", "output": "\n    print(\"--Reading temperatures from %s/...\" % pathname)\n\n    # Initialize return variable\n    temps_from_file = numpy.zeros(NumTemps, numpy.float64)\n\n    for k in range(NumTemps):\n        infile = open(os.path.join(pathname,'TEMP'+ str(k), 'simul'+str(k)+'.output'), 'r')\n        lines = infile.readlines()\n        infile.close()\n\n        for line in lines:\n            if (line[0:11] == 'Temperature'):\n                vals = line.split(':')\n                break\n        temps_from_file[k] = float(vals[1])\n\n    return temps_from_file", "category": "Python"}, {"instruction": "def safe_mkstemp(suffix, prefix='filedownloadutils_'):\n    \"\"\"Create a temporary filename that don't have any '.' inside a part\n    from the suffix.\"\"\"\n", "input": "", "output": "    tmpfd, tmppath = tempfile.mkstemp(\n        suffix=suffix,\n        prefix=prefix,\n        dir=current_app.config['CFG_TMPSHAREDDIR']\n    )\n    # Close the file and leave the responsability to the client code to\n    # correctly open/close it.\n    os.close(tmpfd)\n\n    if '.' not in suffix:\n        # Just in case format is empty\n        return tmppath\n    while '.' in os.path.basename(tmppath)[:-len(suffix)]:\n        os.remove(tmppath)\n        tmpfd, tmppath = tempfile.mkstemp(\n            suffix=suffix,\n            prefix=prefix,\n            dir=current_app.config['CFG_TMPSHAREDDIR']\n        )\n        os.close(tmpfd)\n    return tmppath", "category": "Python"}, {"instruction": "def get_cmd_epilog(self):\n        \"\"\"\n        Get the trailing, multi-line description of this command.\n\n        :returns:\n            ``self.epilog``, if defined\n        :returns:\n            A substring of the class docstring between the string ``@EPILOG``\n            and the end of the docstring, if defined\n        :returns:\n            None, otherwise\n\n        The epilog is similar to the description string but it is instead\n        printed after the section containing detailed descriptions of all of\n        the command line arguments.\n\n        Please consider following good practice by providing additional details\n        about how the command can be used, perhaps an example or a reference to\n        means of finding additional documentation.\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.source.epilog\n        except AttributeError:\n            pass\n        try:\n            return '\\n'.join(\n                get_localized_docstring(\n                    self, self.get_gettext_domain()\n                ).splitlines()[1:]\n            ).split('@EPILOG@', 1)[1].strip()\n        except (AttributeError, IndexError, ValueError):\n            pass", "category": "Python"}, {"instruction": "def message(self, subject, text):\n        \"\"\"Compose a message to this user.  Calls :meth:`narwal.Reddit.compose`.\n        \n        :param subject: subject of message\n        :param text: body of message\n        \"\"\"\n", "input": "", "output": "        return self._reddit.compose(self.name, subject, text)", "category": "Python"}, {"instruction": "def erase_end_of_line (self): # <ESC>[0K -or- <ESC>[K\n        '''Erases from the current cursor position to the end of the current\n        line.'''\n", "input": "", "output": "\n        self.fill_region (self.cur_r, self.cur_c, self.cur_r, self.cols)", "category": "Python"}, {"instruction": "async def dump_tuple(self, elem, elem_type, params=None):\n        \"\"\"\n        Dumps tuple of elements to the writer.\n\n        :param elem:\n        :param elem_type:\n        :param params:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if len(elem) != len(elem_type.f_specs()):\n            raise ValueError('Fixed size tuple has not defined size: %s' % len(elem_type.f_specs()))\n\n        elem_fields = params[0] if params else None\n        if elem_fields is None:\n            elem_fields = elem_type.f_specs()\n        for idx, elem in enumerate(elem):\n            try:\n                self.tracker.push_index(idx)\n                await self._dump_field(elem, elem_fields[idx], params[1:] if params else None)\n                self.tracker.pop()\n\n            except Exception as e:\n                raise helpers.ArchiveException(e, tracker=self.tracker) from e", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"\n        run method\n        \"\"\"\n", "input": "", "output": "        if self.pre_message is None:\n            try:\n                input()  # pre message of None causes the input to be silent\n            except EOFError:\n                pass\n        else:\n            try:\n                input(self.pre_message)\n            except EOFError:\n                pass\n        if self.post_message:\n            print(self.post_message)\n        self.exit_time = time.time()", "category": "Python"}, {"instruction": "def fitting_rmsd(w_fit, C_fit, r_fit, Xs):\n    '''Calculate the RMSD of fitting.'''\n", "input": "", "output": "    return np.sqrt(sum((geometry.point_line_distance(p, C_fit, w_fit) - r_fit) ** 2\n                    for p in Xs) / len(Xs))", "category": "Python"}, {"instruction": "def filter(self, msg_type=None, maxsize=0):\n        \"\"\"\n        Get a filtered iterator of messages for synchronous, blocking use in\n        another thread.\n        \"\"\"\n", "input": "", "output": "        if self._dead:\n            return iter(())\n        iterator = Handler._SBPQueueIterator(maxsize)\n        # We use a weakref so that the iterator may be garbage collected if it's\n        # consumer no longer has a reference.\n        ref = weakref.ref(iterator)\n        self._sinks.append(ref)\n\n        def feediter(msg, **metadata):\n            i = ref()\n            if i is not None:\n                i(msg, **metadata)\n            else:\n                raise Handler._DeadCallbackException\n\n        self.add_callback(feediter, msg_type)\n        return iterator", "category": "Python"}, {"instruction": "def get_real_related(self, id_equip):\n        \"\"\"\n        Find reals related with equipment\n\n        :param id_equip: Identifier of equipment\n\n        :return: Following dictionary:\n\n        ::\n\n            {'vips': [{'port_real': < port_real >,\n            'server_pool_member_id': < server_pool_member_id >,\n            'ip': < ip >,\n            'port_vip': < port_vip >,\n            'host_name': < host_name >,\n            'id_vip': < id_vip >, ...],\n            'equip_name': < equip_name > }}\n\n        :raise EquipamentoNaoExisteError: Equipment not registered.\n        :raise InvalidParameterError: Some parameter was invalid.\n        :raise DataBaseError: Networkapi failed to access the database.\n        :raise XMLError: Networkapi failed to generate the XML response.\n        \"\"\"\n", "input": "", "output": "        url = 'equipamento/get_real_related/' + str(id_equip) + '/'\n\n        code, xml = self.submit(None, 'GET', url)\n\n        data = self.response(code, xml)\n        return data", "category": "Python"}, {"instruction": "def rollout(self, timesteps=None, **kwargs):\n        \"\"\"Generate a system trial, no feedback is incorporated.\"\"\"\n", "input": "", "output": "\n        self.reset_state()\n\n        if timesteps is None:\n            if kwargs.has_key('tau'):\n                timesteps = int(self.timesteps / kwargs['tau'])\n            else: \n                timesteps = self.timesteps\n\n        # set up tracking vectors\n        y_track = np.zeros((timesteps, self.dmps)) \n        dy_track = np.zeros((timesteps, self.dmps))\n        ddy_track = np.zeros((timesteps, self.dmps))\n    \n        for t in range(timesteps):\n        \n            y, dy, ddy = self.step(**kwargs)\n\n            # record timestep\n            y_track[t] = y\n            dy_track[t] = dy\n            ddy_track[t] = ddy\n\n        return y_track, dy_track, ddy_track", "category": "Python"}, {"instruction": "def set_collection(self, service_name, collection_name, to_cache):\n        \"\"\"\n        Sets a collection class within the cache.\n\n        :param service_name: The service a given ``Collection`` talks to. Ex.\n            ``sqs``, ``sns``, ``dynamodb``, etc.\n        :type service_name: string\n\n        :param collection_name: The name of the ``Collection``. Ex.\n            ``QueueCollection``, ``NotificationCollection``,\n            ``TableCollection``, etc.\n        :type collection_name: string\n\n        :param to_cache: The class to be cached for the service.\n        :type to_cache: class\n        \"\"\"\n", "input": "", "output": "        self.services.setdefault(service_name, {})\n        self.services[service_name].setdefault('collections', {})\n        self.services[service_name]['collections'].setdefault(collection_name, {})\n        options = self.services[service_name]['collections'][collection_name]\n        classpath = self.build_classpath(to_cache.__bases__[0])\n\n        if classpath == 'kotocore.collections.Collection':\n            classpath = 'default'\n\n        options[classpath] = to_cache", "category": "Python"}, {"instruction": "def process(self):\n    \"\"\"Collect the artifacts.\n\n    Raises:\n      DFTimewolfError: if no artifacts specified nor resolved by platform.\n    \"\"\"\n", "input": "", "output": "    threads = []\n    for client in self.find_clients(self.hostnames):\n      print(client)\n      thread = threading.Thread(target=self._process_thread, args=(client, ))\n      threads.append(thread)\n      thread.start()\n\n    for thread in threads:\n      thread.join()", "category": "Python"}, {"instruction": "def remove(attributes, properties):\n    \"\"\"Returns a property sets which include all the elements\n    in 'properties' that do not have attributes listed in 'attributes'.\"\"\"\n", "input": "", "output": "    if isinstance(attributes, basestring):\n        attributes = [attributes]\n    assert is_iterable_typed(attributes, basestring)\n    assert is_iterable_typed(properties, basestring)\n    result = []\n    for e in properties:\n        attributes_new = feature.attributes(get_grist(e))\n        has_common_features = 0\n        for a in attributes_new:\n            if a in attributes:\n                has_common_features = 1\n                break\n\n        if not has_common_features:\n            result += e\n\n    return result", "category": "Python"}, {"instruction": "def contents(self):\n        \"\"\" Returns the item in the container, if there is one.\n        This will be a standard item object. \"\"\"\n", "input": "", "output": "        rawitem = self._item.get(\"contained_item\")\n        if rawitem:\n            return self.__class__(rawitem, self._schema)", "category": "Python"}, {"instruction": "def set_meta(self, instance):\n        \"\"\"\n        Set django-meta stuff from LandingPageModel instance.\n        \"\"\"\n", "input": "", "output": "        self.use_title_tag = True\n        self.title = instance.title", "category": "Python"}, {"instruction": "def preview(src_path):\n  ''' Generates a preview of src_path in the requested format.\n  :returns: A list of preview paths, one for each page.\n  '''\n", "input": "", "output": "  previews = []\n\n  if sketch.is_sketchfile(src_path):\n    previews = sketch.preview(src_path)\n\n  if not previews:\n    previews = quicklook.preview(src_path)\n\n  previews = [safely_decode(preview) for preview in previews]\n\n  return previews", "category": "Python"}, {"instruction": "def parse_args(args):\n    \"\"\"\n    This parses the arguments and returns a tuple containing:\n\n    (args, command, command_args)\n\n    For example, \"--config=bar start --with=baz\" would return:\n\n    (['--config=bar'], 'start', ['--with=baz'])\n    \"\"\"\n", "input": "", "output": "    index = None\n    for arg_i, arg in enumerate(args):\n        if not arg.startswith('-'):\n            index = arg_i\n            break\n\n    # Unable to parse any arguments\n    if index is None:\n        return (args, None, [])\n\n    return (args[:index], args[index], args[(index + 1):])", "category": "Python"}, {"instruction": "def _get_biodata(base_file, args):\n    \"\"\"Retrieve biodata genome targets customized by install parameters.\n    \"\"\"\n", "input": "", "output": "    with open(base_file) as in_handle:\n        config = yaml.safe_load(in_handle)\n    config[\"install_liftover\"] = False\n    config[\"genome_indexes\"] = args.aligners\n    ann_groups = config.pop(\"annotation_groups\", {})\n    config[\"genomes\"] = [_setup_genome_annotations(g, args, ann_groups)\n                         for g in config[\"genomes\"] if g[\"dbkey\"] in args.genomes]\n    return config", "category": "Python"}, {"instruction": "def uninvert_unique_two_lists(flat_list, reconstruct_tup):\n    \"\"\"\n    flat_list = thumb_list\n    \"\"\"\n", "input": "", "output": "    import utool as ut\n    (inverse3, cumsum, inverse2, inverse1) = reconstruct_tup\n    flat_stacked_ = ut.take(flat_list, inverse3)\n    unique_list1_, unique_list2_ = ut.unflatten2(flat_stacked_, cumsum)\n    res_list1_ = ut.take(unique_list1_, inverse1)\n    res_list2_ = ut.take(unique_list2_, inverse2)\n    return res_list1_, res_list2_", "category": "Python"}, {"instruction": "def install(self):\n        \"\"\"\n        Called when installed on the user store. Installs my powerups.\n        \"\"\"\n", "input": "", "output": "        items = []\n        for typeName in self.types:\n            it = self.store.findOrCreate(namedAny(typeName))\n            installOn(it, self.store)\n            items.append(str(it.storeID).decode('ascii'))\n        self._items = items", "category": "Python"}, {"instruction": "def update_existing_pivot(self, id, attributes, touch=True):\n        \"\"\"\n        Update an existing pivot record on the table.\n        \"\"\"\n", "input": "", "output": "        if self.updated_at() in self._pivot_columns:\n            attributes = self.set_timestamps_on_attach(attributes, True)\n\n        updated = self._new_picot_statement_for_id(id).update(attributes)\n\n        if touch:\n            self.touch_if_touching()\n\n        return updated", "category": "Python"}, {"instruction": "def restart(self, container, **kwargs):\n        \"\"\"\n        Identical to :meth:`docker.api.container.ContainerApiMixin.restart` with additional logging.\n        \"\"\"\n", "input": "", "output": "        self.push_log(\"Restarting container '{0}'.\".format(container))\n        super(DockerFabricClient, self).restart(container, **kwargs)", "category": "Python"}, {"instruction": "def validator(flag_name, message='Flag validation failed',\n              flag_values=_flagvalues.FLAGS):\n  \"\"\"A function decorator for defining a flag validator.\n\n  Registers the decorated function as a validator for flag_name, e.g.\n\n  @flags.validator('foo')\n  def _CheckFoo(foo):\n    ...\n\n  See register_validator() for the specification of checker function.\n\n  Args:\n    flag_name: str, name of the flag to be checked.\n    message: str, error text to be shown to the user if checker returns False.\n        If checker raises flags.ValidationError, message from the raised\n        error will be shown.\n    flag_values: flags.FlagValues, optional FlagValues instance to validate\n        against.\n  Returns:\n    A function decorator that registers its function argument as a validator.\n  Raises:\n    AttributeError: Raised when flag_name is not registered as a valid flag\n        name.\n  \"\"\"\n", "input": "", "output": "\n  def decorate(function):\n    register_validator(flag_name, function,\n                       message=message,\n                       flag_values=flag_values)\n    return function\n  return decorate", "category": "Python"}, {"instruction": "def _GenDiscoveryDoc(service_class_names,\n                     output_path, hostname=None,\n                     application_path=None):\n  \"\"\"Write discovery documents generated from the service classes to file.\n\n  Args:\n    service_class_names: A list of fully qualified ProtoRPC service names.\n    output_path: The directory to output the discovery docs to.\n    hostname: A string hostname which will be used as the default version\n      hostname. If no hostname is specificied in the @endpoints.api decorator,\n      this value is the fallback. Defaults to None.\n    application_path: A string containing the path to the AppEngine app.\n\n  Returns:\n    A list of discovery doc filenames.\n  \"\"\"\n", "input": "", "output": "  output_files = []\n  service_configs = GenApiConfig(\n      service_class_names, hostname=hostname,\n      config_string_generator=discovery_generator.DiscoveryGenerator(),\n      application_path=application_path)\n  for api_name_version, config in service_configs.iteritems():\n    discovery_name = api_name_version + '.discovery'\n    output_files.append(_WriteFile(output_path, discovery_name, config))\n\n  return output_files", "category": "Python"}, {"instruction": "def getEthernetLinkStatus(self, wanInterfaceId=1, timeout=1):\n        \"\"\"Execute GetEthernetLinkStatus action to get the status of the ethernet link.\n\n        :param int wanInterfaceId: the id of the WAN device\n        :param float timeout: the timeout to wait for the action to be executed\n        :return: status of the ethernet link\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        namespace = Wan.getServiceType(\"getEthernetLinkStatus\") + str(wanInterfaceId)\n        uri = self.getControlURL(namespace)\n\n        results = self.execute(uri, namespace, \"GetEthernetLinkStatus\", timeout=timeout)\n\n        return results[\"NewEthernetLinkStatus\"]", "category": "Python"}, {"instruction": "def create_signed_entities_descriptor(entity_descriptors, security_context, valid_for=None):\n    \"\"\"\n    :param entity_descriptors: the entity descriptors to put in in an EntitiesDescriptor tag and sign\n    :param security_context: security context for the signature\n    :param valid_for: number of hours the metadata should be valid\n    :return: the signed XML document\n\n    :type entity_descriptors: Sequence[saml2.md.EntityDescriptor]]\n    :type security_context: saml2.sigver.SecurityContext\n    :type valid_for: Optional[int]\n    \"\"\"\n", "input": "", "output": "    entities_desc, xmldoc = entities_descriptor(entity_descriptors, valid_for=valid_for, name=None, ident=None,\n                                                sign=True, secc=security_context)\n    if not valid_instance(entities_desc):\n        raise ValueError(\"Could not construct valid EntitiesDescriptor tag\")\n\n    return xmldoc", "category": "Python"}, {"instruction": "def get_success_url(self):\n        \"\"\"\n        Returns the supplied success URL.\n        \"\"\"\n", "input": "", "output": "        if self.success_url:\n            # Forcing possible reverse_lazy evaluation\n            url = force_text(self.success_url)\n        else:\n            raise ImproperlyConfigured(\n                \"No URL to redirect to. Provide a success_url.\")\n        return url", "category": "Python"}, {"instruction": "def clear_last_check(self):\n        \"\"\"Clear the checksum of the file.\"\"\"\n", "input": "", "output": "        with db.session.begin_nested():\n            self.last_check = None\n            self.last_check_at = datetime.utcnow()\n        return self", "category": "Python"}, {"instruction": "def login_redirect(redirect=None, user=None):\n    \"\"\"\n    Redirect user after successful sign in.\n\n    First looks for a ``requested_redirect``. If not supplied will fall-back \n    to the user specific account page. If all fails, will fall-back to the \n    standard Django ``LOGIN_REDIRECT_URL`` setting. Returns a string defining \n    the URI to go next.\n\n    :param redirect:\n        A value normally supplied by ``next`` form field. Gets preference\n        before the default view which requires the user.\n\n    :param user:\n        A ``User`` object specifying the user who has just logged in.\n\n    :return: String containing the URI to redirect to.\n\n    \"\"\"\n", "input": "", "output": "    if redirect: return redirect\n    elif user is not None:\n        return defaults.ACCOUNTS_LOGIN_REDIRECT_URL % \\\n                {'username': user.username}\n    else: return settings.LOGIN_REDIRECT_URL", "category": "Python"}, {"instruction": "def _filter_startswith(self, term, field_name, field_type, is_not):\n        \"\"\"\n        Returns a startswith query on the un-stemmed term.\n\n        Assumes term is not a list.\n        \"\"\"\n", "input": "", "output": "        if field_type == 'text':\n            if len(term.split()) == 1:\n                term = '^ %s*' % term\n                query = self.backend.parse_query(term)\n            else:\n                term = '^ %s' % term\n                query = self._phrase_query(term.split(), field_name, field_type)\n        else:\n            term = '^%s*' % term\n            query = self.backend.parse_query(term)\n\n        if is_not:\n            return xapian.Query(xapian.Query.OP_AND_NOT, self._all_query(), query)\n        return query", "category": "Python"}, {"instruction": "def normal_fields(self):\n        \"\"\"fields that aren't magic (eg, aren't _id, _created, _updated)\"\"\"\n", "input": "", "output": "        return {f:v for f, v in self.fields.items() if not f.startswith('_')}", "category": "Python"}, {"instruction": "def nickmask(prefix: str, kwargs: Dict[str, Any]) -> None:\n    \"\"\" store nick, user, host in kwargs if prefix is correct format \"\"\"\n", "input": "", "output": "    if \"!\" in prefix and \"@\" in prefix:\n        # From a user\n        kwargs[\"nick\"], remainder = prefix.split(\"!\", 1)\n        kwargs[\"user\"], kwargs[\"host\"] = remainder.split(\"@\", 1)\n    else:\n        # From a server, probably the host\n        kwargs[\"host\"] = prefix", "category": "Python"}, {"instruction": "def highlight(self, *args):\n        \"\"\" Highlights the region with a colored frame. Accepts the following parameters:\n\n        highlight([toEnable], [seconds], [color])\n        \n        * toEnable (boolean): Enables or disables the overlay\n        * seconds (number): Seconds to show overlay\n        * color (string): Hex code (\"#XXXXXX\") or color name (\"black\")\n        \"\"\"\n", "input": "", "output": "        toEnable = (self._highlighter is None)\n        seconds = 3\n        color = \"red\"\n        if len(args) > 3:\n            raise TypeError(\"Unrecognized argument(s) for highlight()\")\n        for arg in args:\n            if type(arg) == bool:\n                toEnable = arg\n            elif isinstance(arg, Number):\n                seconds = arg\n            elif isinstance(arg, basestring):\n                color = arg\n        if self._highlighter is not None:\n                self._highlighter.close()\n        if toEnable:\n            self._highlighter = PlatformManager.highlight((self.getX(), self.getY(), self.getW(), self.getH()), color, seconds)", "category": "Python"}, {"instruction": "def lose():\n    \"\"\"Enables access to websites that are defined as 'distractors'\"\"\"\n", "input": "", "output": "    changed = False\n    with open(settings.HOSTS_FILE, \"r\") as hosts_file:\n        new_file = []\n        in_block = False\n        for line in hosts_file:\n            if in_block:\n                if line.strip() == settings.END_TOKEN:\n                    in_block = False\n                    changed = True\n            elif line.strip() == settings.START_TOKEN:\n                in_block = True\n            else:\n                new_file.append(line)\n    if changed:\n        with open(settings.HOSTS_FILE, \"w\") as hosts_file:\n            hosts_file.write(\"\".join(new_file))\n\n    reset_network(\"Concentration is now lost :(.\")", "category": "Python"}, {"instruction": "def g_(self, X):\n        \"\"\"\n        computes h()\n\n        :param X:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if self._interpol:\n            if not hasattr(self, '_g_interp'):\n\n                if self._lookup:\n                    x = self._x_lookup\n                    g_x = self._g_lookup\n                else:\n                    x = np.linspace(0, self._max_interp_X, self._num_interp_X)\n                    g_x = self._g(x)\n                self._g_interp = interp.interp1d(x, g_x, kind='linear', axis=-1, copy=False, bounds_error=False,\n                                                 fill_value=0, assume_sorted=True)\n            return self._g_interp(X)\n        else:\n            return self._g(X)", "category": "Python"}, {"instruction": "def import_numpy():\n    \"\"\"\n    Returns the numpy module if it exists on the system,\n    otherwise returns None.\n    \"\"\"\n", "input": "", "output": "    try:\n        imp.find_module('numpy')\n        numpy_exists = True\n    except ImportError:\n        numpy_exists = False\n\n    if numpy_exists:\n        # We do this outside of try/except block in case numpy exists\n        # but is not installed correctly. We do not want to catch an\n        # incorrect installation which would manifest as an\n        # ImportError.\n        import numpy as np\n    else:\n        np = None\n\n    return np", "category": "Python"}, {"instruction": "def read_env(src, expr):\n    r\"\"\"Read the environment from buffer.\n\n    Advances the buffer until right after the end of the environment. Adds\n    parsed content to the expression automatically.\n\n    :param Buffer src: a buffer of tokens\n    :param TexExpr expr: expression for the environment\n    :rtype: TexExpr\n    \"\"\"\n", "input": "", "output": "    contents = []\n    if expr.name in SKIP_ENVS:\n        contents = [src.forward_until(lambda s: s == '\\\\end')]\n    while src.hasNext() and not src.startswith('\\\\end{%s}' % expr.name):\n        contents.append(read_tex(src))\n    if not src.startswith('\\\\end{%s}' % expr.name):\n        end = src.peek((0, 5))\n        explanation = 'Instead got %s' % end if end else 'Reached end of file.'\n        raise EOFError('Expecting \\\\end{%s}. %s' % (expr.name, explanation))\n    else:\n        src.forward(4)\n    expr.append(*contents)\n    return expr", "category": "Python"}, {"instruction": "def fsdecode(path, os_name=os.name, fs_encoding=FS_ENCODING, errors=None):\n    '''\n    Decode given path.\n\n    :param path: path will be decoded if using bytes\n    :type path: bytes or str\n    :param os_name: operative system name, defaults to os.name\n    :type os_name: str\n    :param fs_encoding: current filesystem encoding, defaults to autodetected\n    :type fs_encoding: str\n    :return: decoded path\n    :rtype: str\n    '''\n", "input": "", "output": "    if not isinstance(path, bytes):\n        return path\n    if not errors:\n        use_strict = PY_LEGACY or os_name == 'nt'\n        errors = 'strict' if use_strict else 'surrogateescape'\n    return path.decode(fs_encoding, errors=errors)", "category": "Python"}, {"instruction": "def validateDocument(self, doc):\n        \"\"\"Try to validate the document instance  basically it does\n          the all the checks described by the XML Rec i.e. validates\n          the internal and external subset (if present) and validate\n           the document tree. \"\"\"\n", "input": "", "output": "        if doc is None: doc__o = None\n        else: doc__o = doc._o\n        ret = libxml2mod.xmlValidateDocument(self._o, doc__o)\n        return ret", "category": "Python"}, {"instruction": "def trace_min_buffer_capacity(self):\n        \"\"\"Retrieves the minimum capacity the trace buffer can be configured with.\n\n        Args:\n          self (JLink): the ``JLink`` instance.\n\n        Returns:\n          The minimum configurable capacity for the trace buffer.\n        \"\"\"\n", "input": "", "output": "        cmd = enums.JLinkTraceCommand.GET_MIN_CAPACITY\n        data = ctypes.c_uint32(0)\n        res = self._dll.JLINKARM_TRACE_Control(cmd, ctypes.byref(data))\n        if (res == 1):\n            raise errors.JLinkException('Failed to get min trace buffer size.')\n        return data.value", "category": "Python"}, {"instruction": "def _stamped_deps(stamp_directory, func, dependencies, *args, **kwargs):\n    \"\"\"Run func, assumed to have dependencies as its first argument.\"\"\"\n", "input": "", "output": "    if not isinstance(dependencies, list):\n        jobstamps_dependencies = [dependencies]\n    else:\n        jobstamps_dependencies = dependencies\n\n    kwargs.update({\n        \"jobstamps_cache_output_directory\": stamp_directory,\n        \"jobstamps_dependencies\": jobstamps_dependencies\n    })\n    return jobstamp.run(func, dependencies, *args, **kwargs)", "category": "Python"}, {"instruction": "def script_request_send(self, target_system, target_component, seq, force_mavlink1=False):\n                '''\n                Request script item with the sequence number seq. The response of the\n                system to this message should be a SCRIPT_ITEM\n                message.\n\n                target_system             : System ID (uint8_t)\n                target_component          : Component ID (uint8_t)\n                seq                       : Sequence (uint16_t)\n\n                '''\n", "input": "", "output": "                return self.send(self.script_request_encode(target_system, target_component, seq), force_mavlink1=force_mavlink1)", "category": "Python"}, {"instruction": "def create_primes(threshold):\n    \"\"\"\n    Generate prime values using sieve of Eratosthenes method.\n\n    Args:\n        threshold (int):\n            The upper bound for the size of the prime values.\n\n    Returns (List[int]):\n        All primes from 2 and up to ``threshold``.\n    \"\"\"\n", "input": "", "output": "    if threshold == 2:\n        return [2]\n\n    elif threshold < 2:\n        return []\n\n    numbers = list(range(3, threshold+1, 2))\n    root_of_threshold = threshold ** 0.5\n    half = int((threshold+1)/2-1)\n    idx = 0\n    counter = 3\n    while counter <= root_of_threshold:\n        if numbers[idx]:\n            idy = int((counter*counter-3)/2)\n            numbers[idy] = 0\n            while idy < half:\n                numbers[idy] = 0\n                idy += counter\n        idx += 1\n        counter = 2*idx+3\n    return [2] + [number for number in numbers if number]", "category": "Python"}, {"instruction": "def is_continuous(docgraph, dominating_node):\n    \"\"\"return True, if the tokens dominated by the given node are all adjacent\"\"\"\n", "input": "", "output": "    first_onset, last_offset = get_span_offsets(docgraph, dominating_node)\n    span_range = xrange(first_onset, last_offset+1)\n\n    token_offsets = (docgraph.get_offsets(tok)\n                     for tok in get_span(docgraph, dominating_node))\n    char_positions = set(itertools.chain.from_iterable(xrange(on, off+1)\n                         for on, off in token_offsets))\n    for item in span_range:\n        if item not in char_positions:\n            return False\n    return True", "category": "Python"}, {"instruction": "def _ParseInsserv(self, data):\n    \"\"\"/etc/insserv.conf* entries define system facilities.\n\n    Full format details are in man 8 insserv, but the basic structure is:\n      $variable          facility1 facility2\n      $second_variable   facility3 $variable\n\n    Any init script that specifies Required-Start: $second_variable needs to be\n    expanded to facility1 facility2 facility3.\n\n    Args:\n      data: A string of insserv definitions.\n    \"\"\"\n", "input": "", "output": "    p = config_file.FieldParser()\n    entries = p.ParseEntries(data)\n    raw = {e[0]: e[1:] for e in entries}\n    # Now expand out the facilities to services.\n    facilities = {}\n    for k, v in iteritems(raw):\n      # Remove interactive tags.\n      k = k.replace(\"<\", \"\").replace(\">\", \"\")\n      facilities[k] = v\n    for k, vals in iteritems(facilities):\n      self.insserv[k] = []\n      for v in vals:\n        self.insserv[k].extend(self._InsservExpander(facilities, v))", "category": "Python"}, {"instruction": "def _single_store(self, addr, offset, size, data):\n        \"\"\"\n        Performs a single store.\n        \"\"\"\n", "input": "", "output": "\n        if offset == 0 and size == self.width:\n            self._contents[addr] = data\n        elif offset == 0:\n            cur = self._single_load(addr, size, self.width - size)\n            self._contents[addr] = data.concat(cur)\n        elif offset + size == self.width:\n            cur = self._single_load(addr, 0, offset)\n            self._contents[addr] = cur.concat(data)\n        else:\n            cur = self._single_load(addr, 0, self.width)\n            start = cur.get_bytes(0, offset)\n            end = cur.get_bytes(offset+size, self.width-offset-size)\n            self._contents[addr] = start.concat(data, end)", "category": "Python"}, {"instruction": "def get_label(self, lang='en'):\n        \"\"\"\n        Returns the label for a certain language\n        :param lang:\n        :type lang: str\n        :return: returns the label in the specified language, an empty string if the label does not exist\n        \"\"\"\n", "input": "", "output": "        if self.fast_run:\n            return list(self.fast_run_container.get_language_data(self.wd_item_id, lang, 'label'))[0]\n        try:\n            return self.wd_json_representation['labels'][lang]['value']\n        except KeyError:\n            return ''", "category": "Python"}, {"instruction": "def _get_substitute_element(head, elt, ps):\n    '''if elt matches a member of the head substitutionGroup, return \n    the GED typecode.\n\n    head -- ElementDeclaration typecode, \n    elt -- the DOM element being parsed\n    ps -- ParsedSoap Instance\n    '''\n", "input": "", "output": "    if not isinstance(head, ElementDeclaration):\n        return None\n\n    return ElementDeclaration.getSubstitutionElement(head, elt, ps)", "category": "Python"}, {"instruction": "def step(self, observations):\n        \"\"\" Sample action from an action space for given state \"\"\"\n", "input": "", "output": "        q_values = self(observations)\n\n        return {\n            'actions': self.q_head.sample(q_values),\n            'q': q_values\n        }", "category": "Python"}, {"instruction": "def get_stored_variation(self, experiment, user_profile):\n    \"\"\" Determine if the user has a stored variation available for the given experiment and return that.\n\n    Args:\n      experiment: Object representing the experiment for which user is to be bucketed.\n      user_profile: UserProfile object representing the user's profile.\n\n    Returns:\n      Variation if available. None otherwise.\n    \"\"\"\n", "input": "", "output": "\n    user_id = user_profile.user_id\n    variation_id = user_profile.get_variation_for_experiment(experiment.id)\n\n    if variation_id:\n      variation = self.config.get_variation_from_id(experiment.key, variation_id)\n      if variation:\n        self.logger.info('Found a stored decision. User \"%s\" is in variation \"%s\" of experiment \"%s\".' % (\n          user_id,\n          variation.key,\n          experiment.key\n        ))\n        return variation\n\n    return None", "category": "Python"}, {"instruction": "def hue(stream, **kwargs):\n    \"\"\"Modify the hue and/or the saturation of the input.\n\n    Args:\n        h: Specify the hue angle as a number of degrees. It accepts an expression, and defaults to \"0\".\n        s: Specify the saturation in the [-10,10] range. It accepts an expression and defaults to \"1\".\n        H: Specify the hue angle as a number of radians. It accepts an expression, and defaults to \"0\".\n        b: Specify the brightness in the [-10,10] range. It accepts an expression and defaults to \"0\".\n\n    Official documentation: `hue <https://ffmpeg.org/ffmpeg-filters.html#hue>`__\n    \"\"\"\n", "input": "", "output": "    return FilterNode(stream, hue.__name__, kwargs=kwargs).stream()", "category": "Python"}, {"instruction": "def _ancestors(collection):\n    \"\"\"Get the ancestors of the collection.\"\"\"\n", "input": "", "output": "    for index, c in enumerate(collection.path_to_root()):\n        if index > 0 and c.dbquery is not None:\n            raise StopIteration\n        yield c.name\n    raise StopIteration", "category": "Python"}, {"instruction": "def vswitch_set_vlan_id_for_user(self, vswitch_name, userid, vlan_id):\n        \"\"\"Set vlan id for user when connecting to the vswitch\n\n        :param str vswitch_name: the name of the vswitch\n        :param str userid: the user id of the vm\n        :param int vlan_id: the VLAN id\n        \"\"\"\n", "input": "", "output": "        self._networkops.set_vswitch_port_vlan_id(vswitch_name,\n                                                  userid, vlan_id)", "category": "Python"}, {"instruction": "def parse_or_die(self, args=None):\n        \"\"\"Like :meth:`ParseKeywords.parse`, but calls :func:`pkwit.cli.die` if a\n        :exc:`KwargvError` is raised, printing the exception text. Returns\n        *self* for convenience.\n\n        \"\"\"\n", "input": "", "output": "        from .cli import die\n\n        try:\n            return self.parse(args)\n        except KwargvError as e:\n            die(e)", "category": "Python"}, {"instruction": "def post(self, request, *args, **kwargs):\n        \"\"\" Handles POST requests. \"\"\"\n", "input": "", "output": "        return self.approve(request, *args, **kwargs)", "category": "Python"}, {"instruction": "def getKwConfig(self, kw):\n        \"\"\" return the configuration of kw, dict\n\n        USAGE: rdict = getKwConfig(kw)\n        \"\"\"\n", "input": "", "output": "        confd = self.getKwAsDict(kw).values()[0].values()[0]\n        return {k.lower(): v for k, v in confd.items()}", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"\n        Start collecting trace information.\n        \"\"\"\n", "input": "", "output": "        origin = inspect.stack()[1][0]\n\n        self.reset()\n\n        # Install the tracer on this thread.\n        self._start_tracer(origin)", "category": "Python"}, {"instruction": "def bintoihex(buf, spos=0x0000):\n    \"\"\"Convert binary buffer to ihex and return as string.\"\"\"\n", "input": "", "output": "    c = 0\n    olen = len(buf)\n    ret = \"\"\n    # 16 byte lines\n    while (c+0x10) <= olen:\n        adr = c + spos\n        l = ':10{0:04X}00'.format(adr)\n        sum = 0x10+((adr>>8)&M8)+(adr&M8)\n        for j in range(0,0x10):\n            nb = buf[c+j]\n            l += '{0:02X}'.format(nb)\n            sum = (sum + nb)&M8\n        l += '{0:02X}'.format((~sum+1)&M8)\n        ret += l + '\\n'\n        c += 0x10\n    # remainder\n    if c < olen:\n        rem = olen-c\n        sum = rem\n        adr = c + spos\n        l = ':{0:02X}{1:04X}00'.format(rem,adr)   # rem < 0x10\n        sum += ((adr>>8)&M8)+(adr&M8)\n        for j in range(0,rem):\n            nb = buf[c+j]\n            l += '{0:02X}'.format(nb)\n            sum = (sum + nb)&M8\n        l += '{0:02X}'.format((~sum+1)&M8)\n        ret += l + '\\n'\n    ret += ':00000001FF\\n'        # EOF\n    return ret", "category": "Python"}, {"instruction": "def uninstall(cert_name,\n              keychain=\"/Library/Keychains/System.keychain\",\n              keychain_password=None):\n    '''\n    Uninstall a certificate from a keychain\n\n    cert_name\n        The name of the certificate to remove\n\n    keychain\n        The keychain to install the certificate to, this defaults to\n        /Library/Keychains/System.keychain\n\n    keychain_password\n        If your keychain is likely to be locked pass the password and it will be unlocked\n        before running the import\n\n        Note: The password given here will show up as plaintext in the returned job\n        info.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keychain.install test.p12 test123\n    '''\n", "input": "", "output": "    if keychain_password is not None:\n        unlock_keychain(keychain, keychain_password)\n\n    cmd = 'security delete-certificate -c \"{0}\" {1}'.format(cert_name, keychain)\n    return __salt__['cmd.run'](cmd)", "category": "Python"}, {"instruction": "def _handle_tag_removeobject2(self):\n        \"\"\"Handle the RemoveObject2 tag.\"\"\"\n", "input": "", "output": "        obj = _make_object(\"RemoveObject2\")\n        obj.Depth = unpack_ui16(self._src)\n        return obj", "category": "Python"}, {"instruction": "def _Close(self):\n    \"\"\"Closes the file-like object.\n\n    If the file-like object was passed in the init function\n    the encrypted stream file-like object does not control\n    the file-like object and should not actually close it.\n    \"\"\"\n", "input": "", "output": "    if not self._file_object_set_in_init:\n      self._file_object.close()\n      self._file_object = None\n\n    self._decrypter = None\n    self._decrypted_data = b''\n    self._encrypted_data = b''", "category": "Python"}, {"instruction": "def check_palette(palette):\n    \"\"\"\n    Check a palette argument (to the :class:`Writer` class) for validity.\n    Returns the palette as a list if okay;\n    raises an exception otherwise.\n    \"\"\"\n", "input": "", "output": "\n    # None is the default and is allowed.\n    if palette is None:\n        return None\n\n    p = list(palette)\n    if not (0 < len(p) <= 256):\n        raise ProtocolError(\n            \"a palette must have between 1 and 256 entries,\"\n            \" see https://www.w3.org/TR/PNG/#11PLTE\")\n    seen_triple = False\n    for i, t in enumerate(p):\n        if len(t) not in (3, 4):\n            raise ProtocolError(\n                \"palette entry %d: entries must be 3- or 4-tuples.\" % i)\n        if len(t) == 3:\n            seen_triple = True\n        if seen_triple and len(t) == 4:\n            raise ProtocolError(\n                \"palette entry %d: all 4-tuples must precede all 3-tuples\" % i)\n        for x in t:\n            if int(x) != x or not(0 <= x <= 255):\n                raise ProtocolError(\n                    \"palette entry %d: \"\n                    \"values must be integer: 0 <= x <= 255\" % i)\n    return p", "category": "Python"}, {"instruction": "def generate_headline_from_description(sender, instance, *args, **kwargs):\n    '''\n    Auto generate the headline of the node from the first lines of the description.\n    '''\n", "input": "", "output": "    lines = instance.description.split('\\n')\n    headline = truncatewords(lines[0], 20)\n    if headline[:-3] == '...':\n        headline = truncatechars(headline.replace(' ...', ''), 250)  # Just in case the words exceed char limit.\n    else:\n        headline = truncatechars(headline, 250)\n    instance.headline = headline", "category": "Python"}, {"instruction": "def lemma(self):\n        \"\"\"\n        Lazy-loads the lemma for this word\n\n        :getter: Returns the plain string value of the word lemma\n        :type: str\n\n        \"\"\"\n", "input": "", "output": "        if self._lemma is None:\n            lemmata = self._element.xpath('lemma/text()')\n            if len(lemmata) > 0:\n                self._lemma = lemmata[0]\n        return self._lemma", "category": "Python"}, {"instruction": "def get_vocabularies(self):\n        \"\"\"Get the vocabularies to pull the qualifiers from.\"\"\"\n", "input": "", "output": "        # Timeout in seconds.\n        timeout = 15\n        socket.setdefaulttimeout(timeout)\n        # Create the ordered vocabulary URL.\n        vocab_url = VOCABULARIES_URL.replace('all', 'all-verbose')\n        # Request the vocabularies dictionary.\n        try:\n            vocab_dict = eval(urllib2.urlopen(vocab_url).read())\n        except:\n            raise UNTLStructureException('Could not retrieve the vocabularies')\n        return vocab_dict", "category": "Python"}, {"instruction": "def qos_map_cos_traffic_class_cos7(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        qos = ET.SubElement(config, \"qos\", xmlns=\"urn:brocade.com:mgmt:brocade-qos\")\n        map = ET.SubElement(qos, \"map\")\n        cos_traffic_class = ET.SubElement(map, \"cos-traffic-class\")\n        name_key = ET.SubElement(cos_traffic_class, \"name\")\n        name_key.text = kwargs.pop('name')\n        cos7 = ET.SubElement(cos_traffic_class, \"cos7\")\n        cos7.text = kwargs.pop('cos7')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def is_prime(n, rnd=default_pseudo_random, k=DEFAULT_ITERATION,\n             algorithm=None):\n    '''Test if n is a prime number\n\n       m - the integer to test\n       rnd - the random number generator to use for the probalistic primality\n       algorithms,\n       k - the number of iterations to use for the probabilistic primality\n       algorithms,\n       algorithm - the primality algorithm to use, default is Miller-Rabin. The\n       gmpy implementation is used if gmpy is installed.\n\n       Return value: True is n seems prime, False otherwise.\n    '''\n", "input": "", "output": "\n    if algorithm is None:\n        algorithm = PRIME_ALGO\n    if algorithm == 'gmpy-miller-rabin':\n        if not gmpy:\n            raise NotImplementedError\n        return gmpy.is_prime(n, k)\n    elif algorithm == 'miller-rabin':\n        # miller rabin probability of primality is 1/4**k\n        return miller_rabin(n, k, rnd=rnd)\n    elif algorithm == 'solovay-strassen':\n        # for jacobi it's 1/2**k\n        return randomized_primality_testing(n, rnd=rnd, k=k*2)\n    else:\n        raise NotImplementedError", "category": "Python"}, {"instruction": "def blocks(self):\n        \"\"\"\n        The RDD of sub-matrix blocks\n        ((blockRowIndex, blockColIndex), sub-matrix) that form this\n        distributed matrix.\n\n        >>> mat = BlockMatrix(\n        ...     sc.parallelize([((0, 0), Matrices.dense(3, 2, [1, 2, 3, 4, 5, 6])),\n        ...                     ((1, 0), Matrices.dense(3, 2, [7, 8, 9, 10, 11, 12]))]), 3, 2)\n        >>> blocks = mat.blocks\n        >>> blocks.first()\n        ((0, 0), DenseMatrix(3, 2, [1.0, 2.0, 3.0, 4.0, 5.0, 6.0], 0))\n\n        \"\"\"\n", "input": "", "output": "        # We use DataFrames for serialization of sub-matrix blocks\n        # from Java, so we first convert the RDD of blocks to a\n        # DataFrame on the Scala/Java side. Then we map each Row in\n        # the DataFrame back to a sub-matrix block on this side.\n        blocks_df = callMLlibFunc(\"getMatrixBlocks\", self._java_matrix_wrapper._java_model)\n        blocks = blocks_df.rdd.map(lambda row: ((row[0][0], row[0][1]), row[1]))\n        return blocks", "category": "Python"}, {"instruction": "def _indiameter_from_fibers(self):\n        r\"\"\"\n        Calculate an indiameter by distance transforming sections of the\n        fiber image. By definition the maximum value will be the largest radius\n        of an inscribed sphere inside the fibrous hull\n        \"\"\"\n", "input": "", "output": "        Np = self.num_pores()\n        indiam = np.zeros(Np, dtype=float)\n        incen = np.zeros([Np, 3], dtype=float)\n        hull_pores = np.unique(self._hull_image)\n        (Lx, Ly, Lz) = np.shape(self._hull_image)\n        (indx, indy, indz) = np.indices([Lx, Ly, Lz])\n        indx = indx.flatten()\n        indy = indy.flatten()\n        indz = indz.flatten()\n        for i, pore in enumerate(hull_pores):\n            logger.info(\"Processing pore: \"+str(i)+\" of \"+str(len(hull_pores)))\n            dt_pore = self._dt_image*(self._hull_image == pore)\n            indiam[pore] = dt_pore.max()*2\n            max_ind = np.argmax(dt_pore)\n            incen[pore, 0] = indx[max_ind]\n            incen[pore, 1] = indy[max_ind]\n            incen[pore, 2] = indz[max_ind]\n        indiam *= self.network.resolution\n        incen *= self.network.resolution\n        return (indiam, incen)", "category": "Python"}, {"instruction": "def get(search=\"unsigned\"):\n    \"\"\" List all available plugins\"\"\"\n", "input": "", "output": "    plugins = []\n    for i in os.walk('/usr/lib/nagios/plugins'):\n        for f in i[2]:\n            plugins.append(f)\n    return plugins", "category": "Python"}, {"instruction": "def compare_fetch_with_fs(self):\n        \"\"\"Compares the fetch entries with the files actually\n           in the payload, and returns a list of all the files\n           that still need to be fetched.\n        \"\"\"\n", "input": "", "output": "\n        files_on_fs = set(self.payload_files())\n        files_in_fetch = set(self.files_to_be_fetched())\n\n        return list(files_in_fetch - files_on_fs)", "category": "Python"}, {"instruction": "def put_many(self, items: Iterable[T], context: PipelineContext = None) -> None:\n        \"\"\"Puts multiple objects of the same type into the data sink. The objects may be transformed into a new type for insertion if necessary.\n\n        Args:\n            items: An iterable (e.g. list) of objects to be inserted into the data sink.\n            context: The context of the insertions (mutable).\n        \"\"\"\n", "input": "", "output": "        LOGGER.info(\"Creating transform generator for items \\\"{items}\\\" for sink \\\"{sink}\\\"\".format(items=items, sink=self._sink))\n        transform_generator = (self._transform(data=item, context=context) for item in items)\n        LOGGER.info(\"Putting transform generator for items \\\"{items}\\\" into sink \\\"{sink}\\\"\".format(items=items, sink=self._sink))\n        self._sink.put_many(self._store_type, transform_generator, context)", "category": "Python"}, {"instruction": "def run_in_window(target, on_destroy=gtk.main_quit):\n    \"\"\"Run a widget, or a delegate in a Window\n    \"\"\"\n", "input": "", "output": "    w = _get_in_window(target)\n    if on_destroy:\n        w.connect('destroy', on_destroy)\n    w.resize(500, 400)\n    w.move(100, 100)\n    w.show_all()\n    gtk.main()", "category": "Python"}, {"instruction": "def _on_timeout():\n    \"\"\"Invoked periodically to ensure that metrics that have been collected\n    are submitted to InfluxDB.\n\n    :rtype: tornado.concurrent.Future or None\n\n    \"\"\"\n", "input": "", "output": "    global _buffer_size\n\n    LOGGER.debug('No metrics submitted in the last %.2f seconds',\n                 _timeout_interval / 1000.0)\n    _buffer_size = _pending_measurements()\n    if _buffer_size:\n        return _trigger_batch_write()\n    _start_timeout()", "category": "Python"}, {"instruction": "def remove_short_sci_segs(self, minSegLength):\n        \"\"\"\n        Function to remove all science segments\n        shorter than a specific length. Also updates the file on disk to remove\n        these segments.\n\n        Parameters\n        -----------\n        minSegLength : int\n            Maximum length of science segments. Segments shorter than this will\n            be removed.\n        \"\"\"\n", "input": "", "output": "        newsegment_list = segments.segmentlist()\n        for key, seglist in self.segment_dict.items():\n            newsegment_list = segments.segmentlist()\n            for seg in seglist:\n                if abs(seg) > minSegLength:\n                    newsegment_list.append(seg)\n            newsegment_list.coalesce()\n            self.segment_dict[key] = newsegment_list\n        self.to_segment_xml(override_file_if_exists=True)", "category": "Python"}, {"instruction": "def mysql_aes_encrypt(val, key):\n    \"\"\"Mysql AES encrypt value with secret key.\n\n    :param val: Plain text value.\n    :param key: The AES key.\n    :returns: The encrypted AES value.\n    \"\"\"\n", "input": "", "output": "    assert isinstance(val, binary_type) or isinstance(val, text_type)\n    assert isinstance(key, binary_type) or isinstance(key, text_type)\n    k = _mysql_aes_key(_to_binary(key))\n    v = _mysql_aes_pad(_to_binary(val))\n    e = _mysql_aes_engine(k).encryptor()\n\n    return e.update(v) + e.finalize()", "category": "Python"}, {"instruction": "def can_handle_suffix(self, suffix):\n        \"\"\"Tell if one of the managed indexes  can be used for the given filter prefix\n\n        For parameters, see BaseIndex.can_handle_suffix\n\n        \"\"\"\n", "input": "", "output": "        for index in self._indexes:\n            if index.can_handle_suffix(suffix):\n                return True\n\n        return False", "category": "Python"}, {"instruction": "def build_payload(self, payload):\n        \"\"\"Build payload of all parts and write them into the payload buffer\"\"\"\n", "input": "", "output": "        remaining_size = self.MAX_SEGMENT_PAYLOAD_SIZE\n\n        for part in self.parts:\n            part_payload = part.pack(remaining_size)\n            payload.write(part_payload)\n            remaining_size -= len(part_payload)", "category": "Python"}, {"instruction": "def top_eigenvector(A,niter=1000,force_iteration=False):\n    '''\n    assuming the LEFT invariant subspace of A corresponding to the LEFT\n    eigenvalue of largest modulus has geometric multiplicity of 1 (trivial\n    Jordan block), returns the vector at the intersection of that eigenspace and\n    the simplex\n\n    A should probably be a ROW-stochastic matrix\n\n    probably uses power iteration\n    '''\n", "input": "", "output": "    n = A.shape[0]\n    np.seterr(invalid='raise',divide='raise')\n    if n <= 25 and not force_iteration:\n        x = np.repeat(1./n,n)\n        x = np.linalg.matrix_power(A.T,niter).dot(x)\n        x /= x.sum()\n        return x\n    else:\n        x1 = np.repeat(1./n,n)\n        x2 = x1.copy()\n        for itr in range(niter):\n            np.dot(A.T,x1,out=x2)\n            x2 /= x2.sum()\n            x1,x2 = x2,x1\n            if np.linalg.norm(x1-x2) < 1e-8:\n                break\n        return x1", "category": "Python"}, {"instruction": "def map(self, fn):\n        \"\"\"Run a map function across all y points in the series\"\"\"\n", "input": "", "output": "        return TimeSeries([(x, fn(y)) for x, y in self.points])", "category": "Python"}, {"instruction": "def unwrap(self, value, session=None):\n        ''' Unwraps the elements of ``value`` using ``ListField.item_type`` and\n            returns them in a list'''\n", "input": "", "output": "        kwargs = {}\n        if self.has_autoload:\n            kwargs['session'] = session\n        self.validate_unwrap(value, **kwargs)\n        return [ self.item_type.unwrap(v, **kwargs) for v in value]", "category": "Python"}, {"instruction": "def _parse_by_pattern(self, lines, pattern):\n        \"\"\"Match pattern line by line and return Results.\n\n        Use ``_create_output_from_match`` to convert pattern match groups to\n        Result instances.\n\n        Args:\n            lines (iterable): Output lines to be parsed.\n            pattern: Compiled pattern to match against lines.\n            result_fn (function): Receive results of one match and return a\n                Result.\n\n        Return:\n            generator: Result instances.\n        \"\"\"\n", "input": "", "output": "        for line in lines:\n            match = pattern.match(line)\n            if match:\n                params = match.groupdict()\n                if not params:\n                    params = match.groups()\n                yield self._create_output_from_match(params)", "category": "Python"}, {"instruction": "def install_locale(cls, locale_code, locale_type):\n        \"\"\"Install the locale specified by `language_code`, for localizations of type `locale_type`.\n\n        If we can't perform localized formatting for the specified locale,\n        then the default localization format will be used.\n\n        If the locale specified is already installed for the selected type, then this is a no-op.\n        \"\"\"\n", "input": "", "output": "\n        # Skip if the locale is already installed\n        if locale_code == getattr(cls, locale_type):\n            return\n        try:\n            # We create a Locale instance to see if the locale code is supported\n            locale = Locale(locale_code)\n            log.debug('Installed locale %s', locale_code)\n        except UnknownLocaleError:\n            default = settings.DEFAULT_LOCALIZATION_FORMAT\n            log.warning('Unknown locale %s, falling back to %s', locale_code, default)\n            locale = Locale(default)\n        setattr(cls, locale_type, locale.language)", "category": "Python"}, {"instruction": "def element_id_by_label(browser, label):\n    \"\"\"Return the id of a label's for attribute\"\"\"\n", "input": "", "output": "    label = XPathSelector(browser,\n                          unicode('//label[contains(., \"%s\")]' % label))\n    if not label:\n        return False\n    return label.get_attribute('for')", "category": "Python"}, {"instruction": "def round_to_sigfigs(num, sigfigs):\n    \"\"\"\n    Rounds a number rounded to a specific number of significant\n    figures instead of to a specific precision.\n    \"\"\"\n", "input": "", "output": "    if type(sigfigs) != int:\n        raise TypeError(\"Number of significant figures must be integer.\")\n    elif sigfigs < 1:\n        raise ValueError(\"Number of significant figures \"\n                         \"must be larger than zero.\")\n    elif num == 0:\n        return num\n    else:\n        prec = int(sigfigs - np.ceil(np.log10(np.absolute(num))))\n        return round(num, prec)", "category": "Python"}, {"instruction": "def from_string(cls, string, *, default_func=None):\n        '''Construct a Service from a string.\n\n        If default_func is provided and any ServicePart is missing, it is called with\n        default_func(protocol, part) to obtain the missing part.\n        '''\n", "input": "", "output": "        if not isinstance(string, str):\n            raise TypeError(f'service must be a string: {string}')\n\n        parts = string.split('://', 1)\n        if len(parts) == 2:\n            protocol, address = parts\n        else:\n            item, = parts\n            protocol = None\n            if default_func:\n                if default_func(item, ServicePart.HOST) and default_func(item, ServicePart.PORT):\n                    protocol, address = item, ''\n                else:\n                    protocol, address = default_func(None, ServicePart.PROTOCOL), item\n            if not protocol:\n                raise ValueError(f'invalid service string: {string}')\n\n        if default_func:\n            default_func = partial(default_func, protocol.lower())\n        address = NetAddress.from_string(address, default_func=default_func)\n        return cls(protocol, address)", "category": "Python"}, {"instruction": "def showPerformance(self):\n        \"\"\"\n        SRN.showPerformance()\n        Clears the context layer(s) and then repeatedly cycles through\n        training patterns until the user decides to quit.\n        \"\"\"\n", "input": "", "output": "        if len(self.inputs) == 0:\n            print('no patterns to test')\n            return\n        self.setContext()\n        while True:\n            BackpropNetwork.showPerformance(self)\n            if self.quitFromSweep:\n                return", "category": "Python"}, {"instruction": "def is_assignment_allowed(self):\n        \"\"\"Check if analyst assignment is allowed\n        \"\"\"\n", "input": "", "output": "        if not self.is_manage_allowed():\n            return False\n        review_state = api.get_workflow_status_of(self.context)\n        edit_states = [\"open\", \"attachment_due\", \"to_be_verified\"]\n        return review_state in edit_states", "category": "Python"}, {"instruction": "def _sighash_prep(self, index, script):\n        '''\n        SproutTx, int, byte-like -> SproutTx\n        Sighashes suck\n        Performs the sighash setup described here:\n        https://en.bitcoin.it/wiki/OP_CHECKSIG#How_it_works\n        https://bitcoin.stackexchange.com/questions/3374/how-to-redeem-a-basic-tx\n        We save on complexity by refusing to support OP_CODESEPARATOR\n        '''\n", "input": "", "output": "\n        if len(self.tx_ins) == 0:\n            return self.copy(joinsplit_sig=b'')\n        # 0 out scripts in tx_ins\n        copy_tx_ins = [tx_in.copy(stack_script=b'', redeem_script=b'')\n                       for tx_in in self.tx_ins]\n\n        # NB: The script for the current transaction input in txCopy is set to\n        #     subScript (lead in by its length as a var-integer encoded!)\n        copy_tx_ins[index] = \\\n            copy_tx_ins[index].copy(stack_script=b'', redeem_script=script)\n\n        return self.copy(tx_ins=copy_tx_ins, joinsplit_sig=b'')", "category": "Python"}, {"instruction": "def normalize(text: str) -> str:\n    \"\"\"\n    Thai text normalize\n\n    :param str text: thai text\n    :return: thai text\n    **Example**::\n     >>> print(normalize(\"\u0e40\u0e40\u0e1b\u0e25\u0e01\")==\"\u0e41\u0e1b\u0e25\u0e01\") # \u0e40 \u0e40 \u0e1b \u0e25 \u0e01 \u0e01\u0e31\u0e1a \u0e41\u0e1b\u0e25\u0e01\n     True\n    \"\"\"\n", "input": "", "output": "    for data in _NORMALIZE_RULE2:\n        text = re.sub(data[0].replace(\"t\", \"[\u0e48\u0e49\u0e4a\u0e4b]\"), data[1], text)\n    for data in list(zip(_NORMALIZE_RULE1, _NORMALIZE_RULE1)):\n        text = re.sub(data[0].replace(\"t\", \"[\u0e48\u0e49\u0e4a\u0e4b]\") + \"+\", data[1], text)\n    return text", "category": "Python"}, {"instruction": "def _handle_response(self, response):\n        \"\"\"\n        Handles the response received from Scrapyd.\n        \"\"\"\n", "input": "", "output": "        if not response.ok:\n            raise ScrapydResponseError(\n                \"Scrapyd returned a {0} error: {1}\".format(\n                    response.status_code,\n                    response.text))\n\n        try:\n            json = response.json()\n        except ValueError:\n            raise ScrapydResponseError(\"Scrapyd returned an invalid JSON \"\n                                       \"response: {0}\".format(response.text))\n        if json['status'] == 'ok':\n            json.pop('status')\n            return json\n        elif json['status'] == 'error':\n            raise ScrapydResponseError(json['message'])", "category": "Python"}, {"instruction": "def reconnect_to_broker(self):\n        \"\"\"Connect or reconnect to broker\"\"\"\n", "input": "", "output": "        #print \"CONNECT !\"\n        if self.client:\n            self.poller.unregister(self.client)\n            self.client.close()\n        self.client = self.ctx.socket(zmq.DEALER)\n        self.client.linger = 0\n        self.client.connect(self.broker)\n        self.poller.register(self.client, zmq.POLLIN)\n        if self.verbose:\n            logging.info(\"I: connecting to broker at %s...\", self.broker)", "category": "Python"}, {"instruction": "def argresample(a,**kwargs):\r\n    '''\r\n    This function returns the positions needed for resampling a \"gappy\" vector \r\n    \r\n    :keyword dt: Force sampling at a given resolution. Otherwise uses main data sampling (computed by getting the median value of the derivative)\r\n    '''\n", "input": "", "output": "    \r\n    dt=kwargs.get('dt',np.median(deriv(a)))\r\n    \r\n    h,_=np.histogram(a, bins=(a.max()-a.min())/dt + 1, range=[a.min()-dt/2.,a.max()+dt/2.])\r\n    \r\n    return h.astype(bool)", "category": "Python"}, {"instruction": "def skip_if_empty(func):\n    \"\"\"\n    Decorator for validation functions which makes them pass if the value\n    passed in is the EMPTY sentinal value.\n    \"\"\"\n", "input": "", "output": "    @partial_safe_wraps(func)\n    def inner(value, *args, **kwargs):\n        if value is EMPTY:\n            return\n        else:\n            return func(value, *args, **kwargs)\n    return inner", "category": "Python"}, {"instruction": "def cmd_loadfile(args):\n    '''callback from menu to load a log file'''\n", "input": "", "output": "    if len(args) != 1:\n        fileargs = \" \".join(args)\n    else:\n        fileargs = args[0]\n    if not os.path.exists(fileargs):\n        print(\"Error loading file \", fileargs);\n        return\n    if os.name == 'nt':\n        #convert slashes in Windows\n        fileargs = fileargs.replace(\"\\\\\", \"/\")\n    loadfile(fileargs.strip('\"'))", "category": "Python"}, {"instruction": "def get_location_id(self, location):\n        \"\"\"Finds the location ID of a given datacenter\n\n        This is mostly used so either a dc name, or regions keyname can be used when ordering\n        :param str location: Region Keyname (DALLAS13) or datacenter name (dal13)\n        :returns: integer id of the datacenter\n        \"\"\"\n", "input": "", "output": "\n        if isinstance(location, int):\n            return location\n        mask = \"mask[id,name,regions[keyname]]\"\n        if match(r'[a-zA-Z]{3}[0-9]{2}', location) is not None:\n            search = {'name': {'operation': location}}\n        else:\n            search = {'regions': {'keyname': {'operation': location}}}\n        datacenter = self.client.call('SoftLayer_Location', 'getDatacenters', mask=mask, filter=search)\n        if len(datacenter) != 1:\n            raise exceptions.SoftLayerError(\"Unable to find location: %s\" % location)\n        return datacenter[0]['id']", "category": "Python"}, {"instruction": "def unique_otuids(groups):\n    \"\"\"\n    Get unique OTUIDs of each category.\n\n    :type groups: Dict\n    :param groups: {Category name: OTUIDs in category}\n\n    :return type: dict\n    :return: Dict keyed on category name and unique OTUIDs as values.\n    \"\"\"\n", "input": "", "output": "    uniques = {key: set() for key in groups}\n    for i, group in enumerate(groups):\n        to_combine = groups.values()[:i]+groups.values()[i+1:]\n        combined = combine_sets(*to_combine)\n        uniques[group] = groups[group].difference(combined)\n    return uniques", "category": "Python"}, {"instruction": "def _do_merge(orig_files, out_file, config, region):\n    \"\"\"Do the actual work of merging with bcftools merge.\n    \"\"\"\n", "input": "", "output": "    if not utils.file_exists(out_file):\n        with file_transaction(config, out_file) as tx_out_file:\n            _check_samples_nodups(orig_files)\n            prep_files = run_multicore(p_bgzip_and_index, [[x, config] for x in orig_files], config)\n            input_vcf_file = \"%s-files.txt\" % utils.splitext_plus(out_file)[0]\n            with open(input_vcf_file, \"w\") as out_handle:\n                for fname in prep_files:\n                    out_handle.write(fname + \"\\n\")\n            bcftools = config_utils.get_program(\"bcftools\", config)\n            output_type = \"z\" if out_file.endswith(\".gz\") else \"v\"\n            region_str = \"-r {}\".format(region) if region else \"\"\n            cmd = \"{bcftools} merge -O {output_type} {region_str} `cat {input_vcf_file}` > {tx_out_file}\"\n            do.run(cmd.format(**locals()), \"Merge variants\")\n    if out_file.endswith(\".gz\"):\n        bgzip_and_index(out_file, config)\n    return out_file", "category": "Python"}, {"instruction": "def copy(self, target, timeout=500):\n        \"\"\"Copy or download this file to a new local file\"\"\"\n", "input": "", "output": "\n        if self.metadata and 'encoding' in self.metadata:\n            with io.open(target,'w', encoding=self.metadata['encoding']) as f:\n                for line in self:\n                    f.write(line)\n        else:\n            with io.open(target,'wb') as f:\n                for line in self:\n                    if sys.version < '3' and isinstance(line,unicode): #pylint: disable=undefined-variable\n                        f.write(line.encode('utf-8'))\n                    elif sys.version >= '3' and isinstance(line,str):\n                        f.write(line.encode('utf-8'))\n                    else:\n                        f.write(line)", "category": "Python"}, {"instruction": "def image_to_osd(image,\n                 lang='osd',\n                 config='',\n                 nice=0,\n                 output_type=Output.STRING):\n    '''\n    Returns string containing the orientation and script detection (OSD)\n    '''\n", "input": "", "output": "    config = '{}-psm 0 {}'.format(\n        '' if get_tesseract_version() < '3.05' else '-',\n        config.strip()\n    ).strip()\n    args = [image, 'osd', lang, config, nice]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DICT: lambda: osd_to_dict(run_and_get_output(*args)),\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()", "category": "Python"}, {"instruction": "def removeNotice(self, data):\n        \"\"\"\n        Add custom notice to front-end for this NodeServers\n\n        :param data: Index of notices list to remove.\n        \"\"\"\n", "input": "", "output": "        LOGGER.info('Sending removenotice to Polyglot for index {}'.format(data))\n        message = { 'removenotice': data }\n        self.send(message)", "category": "Python"}, {"instruction": "def with_added_dimensions(self, n):\n        \"\"\"\n        Adds n dimensions and returns the Rect.  If n < 0, removes\n        dimensions.\n        \"\"\"\n", "input": "", "output": "        if n > 0:\n            return Rect(np.pad(self.data, ((0, 0), (0, n)), 'constant'))\n        return Rect(self.data[:, :self.dimensions + n])", "category": "Python"}, {"instruction": "def to_python(self, value):\n        \"\"\"Convert the value to the appropriate timezone.\"\"\"\n", "input": "", "output": "        # pylint: disable=newstyle\n        value = super(LinkedTZDateTimeField, self).to_python(value)\n\n        if not value:\n            return value\n\n        return value.astimezone(self.timezone)", "category": "Python"}, {"instruction": "def filter(stream_spec, filter_name, *args, **kwargs):\n    \"\"\"Apply custom filter.\n\n    ``filter_`` is normally used by higher-level filter functions such as ``hflip``, but if a filter implementation\n    is missing from ``fmpeg-python``, you can call ``filter_`` directly to have ``fmpeg-python`` pass the filter name\n    and arguments to ffmpeg verbatim.\n\n    Args:\n        stream_spec: a Stream, list of Streams, or label-to-Stream dictionary mapping\n        filter_name: ffmpeg filter name, e.g. `colorchannelmixer`\n        *args: list of args to pass to ffmpeg verbatim\n        **kwargs: list of keyword-args to pass to ffmpeg verbatim\n\n    The function name is suffixed with ``_`` in order avoid confusion with the standard python ``filter`` function.\n\n    Example:\n\n        ``ffmpeg.input('in.mp4').filter('hflip').output('out.mp4').run()``\n    \"\"\"\n", "input": "", "output": "    return filter_multi_output(stream_spec, filter_name, *args, **kwargs).stream()", "category": "Python"}, {"instruction": "def noncentral_t_like(x, mu, lam, nu):\n    R\"\"\"\n    Non-central Student's T log-likelihood.\n\n    Describes a normal variable whose precision is gamma distributed.\n\n    .. math::\n        f(x|\\mu,\\lambda,\\nu) = \\frac{\\Gamma(\\frac{\\nu +\n        1}{2})}{\\Gamma(\\frac{\\nu}{2})}\n        \\left(\\frac{\\lambda}{\\pi\\nu}\\right)^{\\frac{1}{2}}\n        \\left[1+\\frac{\\lambda(x-\\mu)^2}{\\nu}\\right]^{-\\frac{\\nu+1}{2}}\n\n    :Parameters:\n      - `x` : Input data.\n      - `mu` : Location parameter.\n      - `lambda` : Scale parameter.\n      - `nu` : Degrees of freedom.\n\n    \"\"\"\n", "input": "", "output": "    mu = np.asarray(mu)\n    lam = np.asarray(lam)\n    nu = np.asarray(nu)\n    return flib.nct(x, mu, lam, nu)", "category": "Python"}, {"instruction": "def p_additional_catches(p):\n    '''additional_catches : additional_catches CATCH LPAREN fully_qualified_class_name VARIABLE RPAREN LBRACE inner_statement_list RBRACE\n                          | empty'''\n", "input": "", "output": "    if len(p) == 10:\n        p[0] = p[1] + [ast.Catch(p[4], ast.Variable(p[5], lineno=p.lineno(5)),\n                                 p[8], lineno=p.lineno(2))]\n    else:\n        p[0] = []", "category": "Python"}, {"instruction": "def ComplementEquivalence(*args, **kwargs):\n    \"\"\"Change x != y to not(x == y).\"\"\"\n", "input": "", "output": "    return ast.Complement(\n        ast.Equivalence(*args, **kwargs), **kwargs)", "category": "Python"}, {"instruction": "def _addIndex(catalog, index, indextype):\n    \"\"\"\n    This function indexes the index element into the catalog if it isn't yet.\n    :catalog: a catalog object\n    :index: an index id as string\n    :indextype: the type of the index as string\n    :returns: a boolean as True if the element has been indexed and it returns\n    False otherwise.\n    \"\"\"\n", "input": "", "output": "    if index not in catalog.indexes():\n        try:\n            if indextype == 'ZCTextIndex':\n                addZCTextIndex(catalog, index)\n            else:\n                catalog.addIndex(index, indextype)\n            logger.info('Catalog index %s added to %s.' % (index, catalog.id))\n            return True\n        except:\n            logger.error(\n                'Catalog index %s error while adding to %s.'\n                % (index, catalog.id))\n    return False", "category": "Python"}, {"instruction": "def get_rate_source():\n    \"\"\"Get the default Rate Source and return it.\"\"\"\n", "input": "", "output": "    backend = money_rates_settings.DEFAULT_BACKEND()\n    try:\n        return RateSource.objects.get(name=backend.get_source_name())\n    except RateSource.DoesNotExist:\n        raise CurrencyConversionException(\n            \"Rate for %s source do not exists. \"\n            \"Please run python manage.py update_rates\" % backend.get_source_name())", "category": "Python"}, {"instruction": "def intersects(self, x):\n        \"\"\"Check if intersection with rectangle x is not empty.\"\"\"\n", "input": "", "output": "        r1 = Rect(x)\n        if self.isEmpty or self.isInfinite or r1.isEmpty:\n            return False\n        r = Rect(self)\n        if r.intersect(r1).isEmpty:\n            return False\n        return True", "category": "Python"}, {"instruction": "def diff_args(subparsers):\n    \"\"\"Add command line options for the diff operation\"\"\"\n", "input": "", "output": "    diff_parser = subparsers.add_parser('diff')\n    secretfile_args(diff_parser)\n    vars_args(diff_parser)\n    base_args(diff_parser)\n    thaw_from_args(diff_parser)", "category": "Python"}, {"instruction": "def intersection(self, other):\n        \"\"\"AND together version ranges.\n\n        Calculates the intersection of this range with one or more other ranges.\n\n        Args:\n            other: VersionRange object (or list of) to AND with.\n\n        Returns:\n            New VersionRange object representing the intersection, or None if\n            no ranges intersect.\n        \"\"\"\n", "input": "", "output": "        if not hasattr(other, \"__iter__\"):\n            other = [other]\n\n        bounds = self.bounds\n        for range in other:\n            bounds = self._intersection(bounds, range.bounds)\n            if not bounds:\n                return None\n\n        range = VersionRange(None)\n        range.bounds = bounds\n        return range", "category": "Python"}, {"instruction": "def to_configs(self):\n        \"\"\"Return a config object that contains the measurement configurations\n        (a,b,m,n) from the data\n\n        Returns\n        -------\n        config_obj : reda.ConfigManager\n        \"\"\"\n", "input": "", "output": "        config_obj = reda.configs.configManager.ConfigManager()\n        config_obj.add_to_configs(self.data[['a', 'b', 'm', 'n']].values)\n        return config_obj", "category": "Python"}, {"instruction": "def eject_medium(self, attachment):\n        \"\"\"Tells VBoxSVC that the guest has ejected the medium associated with\n        the medium attachment.\n\n        in attachment of type :class:`IMediumAttachment`\n            The medium attachment where the eject happened.\n\n        return new_attachment of type :class:`IMediumAttachment`\n            A new reference to the medium attachment, as the config change can\n            result in the creation of a new instance.\n\n        \"\"\"\n", "input": "", "output": "        if not isinstance(attachment, IMediumAttachment):\n            raise TypeError(\"attachment can only be an instance of type IMediumAttachment\")\n        new_attachment = self._call(\"ejectMedium\",\n                     in_p=[attachment])\n        new_attachment = IMediumAttachment(new_attachment)\n        return new_attachment", "category": "Python"}, {"instruction": "def superclasses(self, inherited=False):\n        \"\"\"Iterate over the superclasses of the class.\n\n        This function is the Python equivalent\n        of the CLIPS class-superclasses command.\n\n        \"\"\"\n", "input": "", "output": "        data = clips.data.DataObject(self._env)\n\n        lib.EnvClassSuperclasses(\n            self._env, self._cls, data.byref, int(inherited))\n\n        for klass in classes(self._env, data.value):\n            yield klass", "category": "Python"}, {"instruction": "def window(ible, length):\n    \"\"\"Split `ible` into multiple lists of length `length`.\n\n    >>> list(window(range(5), 2))\n    [[0, 1], [2, 3], [4]]\n    \"\"\"\n", "input": "", "output": "    if length <= 0:  # pragma: NO COVER\n        raise ValueError\n    ible = iter(ible)\n    while True:\n        elts = [xx for ii, xx in zip(range(length), ible)]\n        if elts:\n            yield elts\n        else:\n            break", "category": "Python"}, {"instruction": "def _get_upserts_distinct(queryset, model_objs_updated, model_objs_created, unique_fields):\n    \"\"\"\n    Given a list of model objects that were updated and model objects that were created,\n    fetch the pks of the newly created models and return the two lists in a tuple\n    \"\"\"\n", "input": "", "output": "\n    # Keep track of the created models\n    created_models = []\n\n    # If we created new models query for them\n    if model_objs_created:\n        created_models.extend(\n            queryset.extra(\n                where=['({unique_fields_sql}) in %s'.format(\n                    unique_fields_sql=', '.join(unique_fields)\n                )],\n                params=[\n                    tuple([\n                        tuple([\n                            getattr(model_obj, field)\n                            for field in unique_fields\n                        ])\n                        for model_obj in model_objs_created\n                    ])\n                ]\n            )\n        )\n\n    # Return the models\n    return model_objs_updated, created_models", "category": "Python"}, {"instruction": "def solve_spectral(prob, *args, **kwargs):\n    \"\"\"Solve the spectral relaxation with lambda = 1.\n    \"\"\"\n", "input": "", "output": "\n    # TODO: do this efficiently without SDP lifting\n\n    # lifted variables and semidefinite constraint\n    X = cvx.Semidef(prob.n + 1)\n\n    W = prob.f0.homogeneous_form()\n    rel_obj = cvx.Minimize(cvx.sum_entries(cvx.mul_elemwise(W, X)))\n\n    W1 = sum([f.homogeneous_form() for f in prob.fs if f.relop == '<='])\n    W2 = sum([f.homogeneous_form() for f in prob.fs if f.relop == '=='])\n\n    rel_prob = cvx.Problem(\n        rel_obj,\n        [\n            cvx.sum_entries(cvx.mul_elemwise(W1, X)) <= 0,\n            cvx.sum_entries(cvx.mul_elemwise(W2, X)) == 0,\n            X[-1, -1] == 1\n        ]\n    )\n    rel_prob.solve(*args, **kwargs)\n\n    if rel_prob.status not in [cvx.OPTIMAL, cvx.OPTIMAL_INACCURATE]:\n        raise Exception(\"Relaxation problem status: %s\" % rel_prob.status)\n\n    (w, v) = LA.eig(X.value)\n    return np.sqrt(np.max(w))*np.asarray(v[:-1, np.argmax(w)]).flatten(), rel_prob.value", "category": "Python"}, {"instruction": "def keyPressEvent( self, event ):\r\n        \"\"\"\r\n        Looks for the Esc key to close the popup.\r\n        \r\n        :param      event | <QKeyEvent>\r\n        \"\"\"\n", "input": "", "output": "        if ( event.key() == Qt.Key_Escape ):\r\n            self.reject()\r\n            event.accept()\r\n            return\r\n        \r\n        elif ( event.key() in (Qt.Key_Return, Qt.Key_Enter) ):\r\n            if self._autoDefault:\r\n                self.accept()\r\n                event.accept()\r\n            return\r\n        \r\n        super(XPopupWidget, self).keyPressEvent(event)", "category": "Python"}, {"instruction": "def handle(self):\n        \"\"\"\n        Handle a message\n        :return: True if success, False otherwise\n        \"\"\"\n", "input": "", "output": "\n        if self.component_type == StreamComponent.SOURCE:\n            msg = self.handler_function()\n            return self.__send(msg)\n\n        logger = self.logger\n        data = self.__receive()\n        if data is None:\n            return False\n        else:\n            logger.debug(\"Calling %s \" % self.handler_function)\n            result = self.handler_function(data.decode(self.char_encoding))\n            if self.component_type == StreamComponent.PROCESSOR:\n                logger.debug(\"Sending p3:%s %s %s\" % (PYTHON3, result, str(type(result))))\n                if not self.__send(result):\n                    return False\n        return True", "category": "Python"}, {"instruction": "def delete_search_document(self, *, index):\n        \"\"\"Delete document from named index.\"\"\"\n", "input": "", "output": "        cache.delete(self.search_document_cache_key)\n        get_client().delete(index=index, doc_type=self.search_doc_type, id=self.pk)", "category": "Python"}, {"instruction": "def _register_server(self, server, timeout=30):\n        '''Register a new SiriDB Server.\n\n        This method is used by the SiriDB manage tool and should not be used\n        otherwise. Full access rights are required for this request.\n        '''\n", "input": "", "output": "        result = self._loop.run_until_complete(\n            self._protocol.send_package(CPROTO_REQ_REGISTER_SERVER,\n                                        data=server,\n                                        timeout=timeout))\n        return result", "category": "Python"}, {"instruction": "def xor(key, data):\n    \"\"\"\n    Perform cyclical exclusive or operations on ``data``.\n\n    The ``key`` can be a an integer *(0 <= key < 256)* or a byte sequence. If\n    the key is smaller than the provided ``data``, the ``key`` will be\n    repeated.\n\n    Args:\n        key(int or bytes): The key to xor ``data`` with.\n        data(bytes): The data to perform the xor operation on.\n\n    Returns:\n        bytes: The result of the exclusive or operation.\n\n    Examples:\n        >>> from pwny import *\n        >>> xor(5, b'ABCD')\n        b'DGFA'\n        >>> xor(5, b'DGFA')\n        b'ABCD'\n        >>> xor(b'pwny', b'ABCDEFGHIJKLMNOPQRSTUVWXYZ')\n        b'15-=51)19=%5=9!)!%=-%!9!)-'\n        >>> xor(b'pwny', b'15-=51)19=%5=9!)!%=-%!9!)-')\n        b'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    \"\"\"\n", "input": "", "output": "\n    if type(key) is int:\n        key = six.int2byte(key)\n    key_len = len(key)\n\n    return b''.join(\n        six.int2byte(c ^ six.indexbytes(key, i % key_len))\n        for i, c in enumerate(six.iterbytes(data))\n    )", "category": "Python"}, {"instruction": "def get_source_files(target, build_context) -> list:\n    \"\"\"Return list of source files for `target`.\"\"\"\n", "input": "", "output": "    all_sources = list(target.props.sources)\n    for proto_dep_name in target.props.protos:\n        proto_dep = build_context.targets[proto_dep_name]\n        all_sources.extend(proto_dep.artifacts.get(AT.gen_cc).keys())\n    return all_sources", "category": "Python"}, {"instruction": "def _send_periodic_internal(self, msg, period, duration=None):\n        \"\"\"Send a message using built-in cyclic transmit list functionality.\"\"\"\n", "input": "", "output": "        if self._scheduler is None:\n            self._scheduler = HANDLE()\n            _canlib.canSchedulerOpen(self._device_handle, self.channel,\n                                     self._scheduler)\n            caps = structures.CANCAPABILITIES()\n            _canlib.canSchedulerGetCaps(self._scheduler, caps)\n            self._scheduler_resolution = float(caps.dwClockFreq) / caps.dwCmsDivisor\n            _canlib.canSchedulerActivate(self._scheduler, constants.TRUE)\n        return CyclicSendTask(self._scheduler, msg, period, duration,\n                              self._scheduler_resolution)", "category": "Python"}, {"instruction": "def cmd_full_return(\n            self,\n            tgt,\n            fun,\n            arg=(),\n            timeout=None,\n            tgt_type='glob',\n            ret='',\n            verbose=False,\n            kwarg=None,\n            **kwargs):\n        '''\n        Execute a salt command and return\n        '''\n", "input": "", "output": "        was_listening = self.event.cpub\n\n        try:\n            pub_data = self.run_job(\n                tgt,\n                fun,\n                arg,\n                tgt_type,\n                ret,\n                timeout,\n                kwarg=kwarg,\n                listen=True,\n                **kwargs)\n\n            if not pub_data:\n                return pub_data\n\n            return (self.get_cli_static_event_returns(pub_data['jid'],\n                                                      pub_data['minions'],\n                                                      timeout,\n                                                      tgt,\n                                                      tgt_type,\n                                                      verbose))\n        finally:\n            if not was_listening:\n                self.event.close_pub()", "category": "Python"}, {"instruction": "def encode_sentence(obj):\r\n    \"\"\"Encode a single sentence.\"\"\"\n", "input": "", "output": "    warnings.warn(\"deprecated. Please use bioc.biocxml.encoder.encode_sentence\", DeprecationWarning)\r\n    return bioc.biocxml.encoder.encode_sentence(obj)", "category": "Python"}, {"instruction": "def get_rule(self, field_id):\n        \"\"\"\n        Returns the rule for the field identified by the id.\n\n        If it is set as not being compulsory, the rule will be adapted to\n        accept string composed only of white characters.\n\n        :param field_id: unique id in the system for the field\n        :return: the rule of a field\n        \"\"\"\n", "input": "", "output": "\n        if field_id in self._fields:\n            # Field already exists\n            field = self._fields[field_id]\n        else:\n            # Field does not exist\n            # It is created\n            field = self._create_field(field_id)\n\n            # Field is saved\n            self._fields[field_id] = field\n\n        return field", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"\n        Close (destroy) this USB context, and all related instances.\n\n        When this method has been called, methods on its instance will\n        become mosty no-ops, returning None until explicitly re-opened\n        (by calling open() or __enter__()).\n\n        Note: \"exit\" is a deprecated alias of \"close\".\n        \"\"\"\n", "input": "", "output": "        self.__auto_open = False\n        self.__context_cond.acquire()\n        try:\n            while self.__context_refcount and self.__context_p:\n                self.__context_cond.wait()\n            self._exit()\n        finally:\n            self.__context_cond.notifyAll()\n            self.__context_cond.release()", "category": "Python"}, {"instruction": "def sync(self):\r\n        \"\"\"\r\n        Syncs the information for this settings out to the file system.\r\n        \"\"\"\n", "input": "", "output": "        if self._customFormat:\r\n            self._customFormat.save(self.fileName())\r\n        else:\r\n            super(XSettings, self).sync()", "category": "Python"}, {"instruction": "def _plot_ulims(\n    ax, x, y, xerr, color, capsize=5, height_fraction=0.25, elinewidth=2\n):\n    \"\"\"\n    Plot upper limits as arrows with cap at value of upper limit.\n\n    uplim behaviour has been fixed in matplotlib 1.4\n    \"\"\"\n", "input": "", "output": "    ax.errorbar(\n        x, y, xerr=xerr, ls=\"\", color=color, elinewidth=elinewidth, capsize=0\n    )\n\n    from distutils.version import LooseVersion\n    import matplotlib\n\n    mpl_version = LooseVersion(matplotlib.__version__)\n    if mpl_version >= LooseVersion(\"1.4.0\"):\n        ax.errorbar(\n            x,\n            y,\n            yerr=height_fraction * y,\n            ls=\"\",\n            uplims=True,\n            color=color,\n            elinewidth=elinewidth,\n            capsize=capsize,\n            zorder=10,\n        )\n    else:\n        ax.errorbar(\n            x,\n            (1 - height_fraction) * y,\n            yerr=height_fraction * y,\n            ls=\"\",\n            lolims=True,\n            color=color,\n            elinewidth=elinewidth,\n            capsize=capsize,\n            zorder=10,\n        )", "category": "Python"}, {"instruction": "def dictify(r, root=True):\n    \"\"\"Convert an ElementTree into a dict.\"\"\"\n", "input": "", "output": "    if root:\n        return {r.tag: dictify(r, False)}\n    d = {}\n    if r.text and r.text.strip():\n        try:\n            return int(r.text)\n        except ValueError:\n            try:\n                return float(r.text)\n            except ValueError:\n                return r.text\n    for x in r.findall(\"./*\"):\n        if x.tag in d and not isinstance(d[x.tag], list):\n            d[x.tag] = [d[x.tag]]\n            d[x.tag].append(dictify(x, False))\n        else:\n            d[x.tag] = dictify(x, False)\n    return d", "category": "Python"}, {"instruction": "def trim_columns(self, columns_to_trim):\n        \"\"\"\n        remove column in design matrix\n        \"\"\"\n", "input": "", "output": "        # TODO check if trimmed column is actually one of the columns\n        if len(self._trimmed_columns) == 0:\n            self._trimmed_columns.append(columns_to_trim)\n        else:\n            self._trimmed_columns.extend(columns_to_trim)\n        self._trimmed_columns = self._trimmed_columns[0]\n        self.encoder['trimmed_columns'] = self._trimmed_columns", "category": "Python"}, {"instruction": "def suggest(self, name, term, **kwargs):\n        \"\"\"Set suggestion options.\n\n        :arg name: The name to use for the suggestions.\n        :arg term: The term to suggest similar looking terms for.\n\n        Additional keyword options:\n\n        * ``field`` -- The field to base suggestions upon, defaults to _all\n\n        Results will have a ``_suggestions`` property containing the\n        suggestions for all terms.\n\n        .. Note::\n\n           Suggestions are only supported since Elasticsearch 0.90.\n\n           Calling this multiple times will add multiple suggest clauses to\n           the query.\n        \"\"\"\n", "input": "", "output": "        return self._clone(next_step=('suggest', (name, term, kwargs)))", "category": "Python"}, {"instruction": "def get_sdb_by_id(self, sdb_id):\n        \"\"\" Return the details for the given safe deposit box id\n\n        Keyword arguments:\n        sdb_id -- this is the id of the safe deposit box, not the path.\n        \"\"\"\n", "input": "", "output": "        sdb_resp = get_with_retry(self.cerberus_url + '/v2/safe-deposit-box/' + sdb_id,\n                                headers=self.HEADERS)\n\n        throw_if_bad_response(sdb_resp)\n\n        return sdb_resp.json()", "category": "Python"}, {"instruction": "def _handle_401(self, data):\n        \"\"\"Handle Lain being helpful\"\"\"\n", "input": "", "output": "\n        ex = ConnectionError(\n            \"Can't login to a protected room without a proper password\", data\n        )\n        self.conn.reraise(ex)", "category": "Python"}, {"instruction": "def list_settings(self):\n        \"\"\"\n        Get list of all appropriate settings and their default values.\n        \"\"\"\n", "input": "", "output": "        result = super().list_settings()\n        result.append((self.SETTING_TEXT_HIGHLIGHT, None))\n        return result", "category": "Python"}, {"instruction": "def fit_plots(ax,xfit,xraw,yfit,yraw):\n    '''\n    #====================================================================\n    plot the fitted results for data fit and refit\n    #====================================================================\n    '''\n", "input": "", "output": "    global _yfits_\n    _yfits_ = yfit\n    ax.plot(xfit, yfit)\n    ax.plot(xfit, np.sum(yfit,axis=1))\n    ax.scatter(xraw, yraw)\n    ax.set_xlabel('Field (log10(mT))')\n    ax.set_ylabel('IRM normalization')", "category": "Python"}, {"instruction": "def update_vm(vm_ref, vm_config_spec):\n    '''\n    Updates the virtual machine configuration with the given object\n\n    vm_ref\n        Virtual machine managed object reference\n\n    vm_config_spec\n        Virtual machine config spec object to update\n    '''\n", "input": "", "output": "    vm_name = get_managed_object_name(vm_ref)\n    log.trace('Updating vm \\'%s\\'', vm_name)\n    try:\n        task = vm_ref.ReconfigVM_Task(vm_config_spec)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)\n    vm_ref = wait_for_task(task, vm_name, 'ReconfigureVM Task')\n    return vm_ref", "category": "Python"}, {"instruction": "def massage_keys(keys):\n    \"\"\"Goes through list of GPG/Keybase keys. For the keybase keys\n    it will attempt to look up the GPG key\"\"\"\n", "input": "", "output": "    m_keys = []\n    for key in keys:\n        if key.startswith('keybase:'):\n            m_keys.append(cryptorito.key_from_keybase(key[8:])['fingerprint'])\n        else:\n            m_keys.append(key)\n\n    return m_keys", "category": "Python"}, {"instruction": "def _calc_var_theta(self):\n        \"\"\"Calculate an estimate of the var(theta)\"\"\"\n", "input": "", "output": "        if self.decaying_prior:\n            n_sampled = np.clip(self.alpha_ + self.beta_, 1, np.inf)\n            prior_weight = 1/n_sampled\n            alpha = self.alpha_ + prior_weight * self.alpha_0\n            beta = self.beta_ + prior_weight * self.beta_0\n        else:\n            alpha = self.alpha_ + self.alpha_0\n            beta = self.beta_ + self.beta_0\n        # Variance of Beta-distributed rv\n        self.var_theta_ = ( alpha * beta /\n                            ((alpha + beta)**2 * (alpha + beta + 1)) )", "category": "Python"}, {"instruction": "def resume(self, vehID):\n        \"\"\"resume(string) -> None\n\n        Resumes the vehicle from the current stop (throws an error if the vehicle is not stopped).\n        \"\"\"\n", "input": "", "output": "        self._connection._beginMessage(\n            tc.CMD_SET_VEHICLE_VARIABLE, tc.CMD_RESUME, vehID, 1 + 4)\n        self._connection._string += struct.pack(\"!Bi\", tc.TYPE_COMPOUND, 0)\n        self._connection._sendExact()", "category": "Python"}, {"instruction": "def cmd(\n        name,\n        fun=None,\n        arg=(),\n        **kwargs):\n    '''\n    Execute a runner asynchronous:\n\n    USAGE:\n\n    .. code-block:: yaml\n\n        run_cloud:\n          wheel.cmd:\n            - fun: key.delete\n            - match: minion_id\n    '''\n", "input": "", "output": "    ret = {'name': name,\n           'changes': {},\n           'comment': '',\n           'result': True}\n    if fun is None:\n        fun = name\n    client = salt.wheel.WheelClient(__opts__)\n    low = {'fun': fun,\n            'arg': arg,\n            'kwargs': kwargs}\n    client.cmd_async(low)\n    return ret", "category": "Python"}, {"instruction": "def file_handler(name, logname, filename, mode='a', encoding=None,\n                 delay=False):\n    \"\"\"\n    A Bark logging handler logging output to a named file.\n\n    Similar to logging.FileHandler.\n    \"\"\"\n", "input": "", "output": "\n    return wrap_log_handler(logging.FileHandler(\n        filename, mode=mode, encoding=encoding, delay=delay))", "category": "Python"}, {"instruction": "def create_arguments(primary, pyfunction, call_node, scope):\n    \"\"\"A factory for creating `Arguments`\"\"\"\n", "input": "", "output": "    args = list(call_node.args)\n    args.extend(call_node.keywords)\n    called = call_node.func\n    # XXX: Handle constructors\n    if _is_method_call(primary, pyfunction) and \\\n       isinstance(called, ast.Attribute):\n        args.insert(0, called.value)\n    return Arguments(args, scope)", "category": "Python"}, {"instruction": "def send_registration_mail(email, *, request, **kwargs):\n    \"\"\"send_registration_mail(email, *, request, **kwargs)\n    Sends the registration mail\n\n    * ``email``: The email address where the registration link should be\n      sent to.\n    * ``request``: A HTTP request instance, used to construct the complete\n      URL (including protocol and domain) for the registration link.\n    * Additional keyword arguments for ``get_confirmation_url`` respectively\n      ``get_confirmation_code``.\n\n    The mail is rendered using the following two templates:\n\n    * ``registration/email_registration_email.txt``: The first line of this\n      template will be the subject, the third to the last line the body of the\n      email.\n    * ``registration/email_registration_email.html``: The body of the HTML\n      version of the mail. This template is **NOT** available by default and\n      is not required either.\n    \"\"\"\n", "input": "", "output": "\n    render_to_mail(\n        \"registration/email_registration_email\",\n        {\"url\": get_confirmation_url(email, request, **kwargs)},\n        to=[email],\n    ).send()", "category": "Python"}, {"instruction": "def stan_rdump(data, filename):\n    \"\"\"\n    Dump a dictionary with model data into a file using the R dump format that\n    Stan supports.\n\n    Parameters\n    ----------\n    data : dict\n    filename : str\n\n    \"\"\"\n", "input": "", "output": "    for name in data:\n        if not is_legal_stan_vname(name):\n            raise ValueError(\"Variable name {} is not allowed in Stan\".format(name))\n    with open(filename, 'w') as f:\n        f.write(_dict_to_rdump(data))", "category": "Python"}, {"instruction": "def encode(self):\n        \"\"\"The bytes representation of this :class:`HttpRequest`.\n\n        Called by :class:`HttpResponse` when it needs to encode this\n        :class:`HttpRequest` before sending it to the HTTP resource.\n        \"\"\"\n", "input": "", "output": "        # Call body before fist_line in case the query is changes.\n        first_line = self.first_line()\n        if self.body and self.wait_continue:\n            self.headers['expect'] = '100-continue'\n        headers = self.headers\n        if self.unredirected_headers:\n            headers = self.unredirected_headers.copy()\n            headers.update(self.headers)\n        buffer = [first_line.encode('ascii'), b'\\r\\n']\n        buffer.extend((('%s: %s\\r\\n' % (name, value)).encode(CHARSET)\n                      for name, value in headers.items()))\n        buffer.append(b'\\r\\n')\n        return b''.join(buffer)", "category": "Python"}, {"instruction": "def bin_upgrade(self):\n        \"\"\"Install-upgrade Slackware binary packages\n        \"\"\"\n", "input": "", "output": "        packages = self.args[1:]\n        options = [\n            \"-u\",\n            \"--upgradepkg\"\n        ]\n        flag = \"\"\n        flags = [\n            \"--dry-run\",\n            \"--install-new\",\n            \"--reinstall\",\n            \"--verbose\"\n        ]\n        if len(self.args) > 1 and self.args[0] in options:\n            if self.args[1] in flags:\n                flag = self.args[1]\n                packages = self.args[2:]\n            PackageManager(packages).upgrade(flag)\n        else:\n            usage(\"\")", "category": "Python"}, {"instruction": "def pod_absent(name, namespace='default', **kwargs):\n    '''\n    Ensures that the named pod is absent from the given namespace.\n\n    name\n        The name of the pod\n\n    namespace\n        The name of the namespace\n    '''\n", "input": "", "output": "\n    ret = {'name': name,\n           'changes': {},\n           'result': False,\n           'comment': ''}\n\n    pod = __salt__['kubernetes.show_pod'](name, namespace, **kwargs)\n\n    if pod is None:\n        ret['result'] = True if not __opts__['test'] else None\n        ret['comment'] = 'The pod does not exist'\n        return ret\n\n    if __opts__['test']:\n        ret['comment'] = 'The pod is going to be deleted'\n        ret['result'] = None\n        return ret\n\n    res = __salt__['kubernetes.delete_pod'](name, namespace, **kwargs)\n    if res['code'] == 200 or res['code'] is None:\n        ret['result'] = True\n        ret['changes'] = {\n            'kubernetes.pod': {\n                'new': 'absent', 'old': 'present'}}\n        if res['code'] is None:\n            ret['comment'] = 'In progress'\n        else:\n            ret['comment'] = res['message']\n    else:\n        ret['comment'] = 'Something went wrong, response: {0}'.format(res)\n\n    return ret", "category": "Python"}, {"instruction": "def pair_hmm_align_unaligned_seqs(seqs, moltype=DNA_cogent, params={}):\n    \"\"\"\n        Checks parameters for pairwise alignment, returns alignment.\n\n        Code from Greg Caporaso.\n    \"\"\"\n", "input": "", "output": "\n    seqs = LoadSeqs(data=seqs, moltype=moltype, aligned=False)\n    try:\n        s1, s2 = seqs.values()\n    except ValueError:\n        raise ValueError(\n            \"Pairwise aligning of seqs requires exactly two seqs.\")\n\n    try:\n        gap_open = params['gap_open']\n    except KeyError:\n        gap_open = 5\n    try:\n        gap_extend = params['gap_extend']\n    except KeyError:\n        gap_extend = 2\n    try:\n        score_matrix = params['score_matrix']\n    except KeyError:\n        score_matrix = make_dna_scoring_dict(\n            match=1, transition=-1, transversion=-1)\n\n    return local_pairwise(s1, s2, score_matrix, gap_open, gap_extend)", "category": "Python"}, {"instruction": "def parse_cmd_string(text, off, trim=True):\n    '''\n    Parse in a command line string which may be quoted.\n    '''\n", "input": "", "output": "    if trim:\n        _, off = nom(text, off, whites)\n\n    if isquote(text, off):\n        return parse_string(text, off, trim=trim)\n\n    if nextchar(text, off, '('):\n        return parse_list(text, off)\n\n    return meh(text, off, whites)", "category": "Python"}, {"instruction": "def set_func(self, func, pnames, args=()):\n        \"\"\"Set the model function to use an efficient but tedious calling convention.\n\n        The function should obey the following convention::\n\n            def func(param_vec, *args):\n                modeled_data = { do something using param_vec }\n                return modeled_data\n\n        This function creates the :class:`pwkit.lmmin.Problem` so that the\n        caller can futz with it before calling :meth:`solve`, if so desired.\n\n        Returns *self*.\n\n        \"\"\"\n", "input": "", "output": "        from .lmmin import Problem\n\n        self.func = func\n        self._args = args\n        self.pnames = list(pnames)\n        self.lm_prob = Problem(len(self.pnames))\n        return self", "category": "Python"}, {"instruction": "def _find_title(html_file):\n    \"\"\"Finds the <title> for the given HTML file, or (Unknown).\"\"\"\n", "input": "", "output": "    # TODO Is it necessary to read files like this?\n    with html_file.open() as f:\n        for line in f:\n            if '<title>' in line:\n                # + 7 to skip len('<title>')\n                return line[line.index('<title>') + 7:line.index('</title>')]\n\n    return '(Unknown)'", "category": "Python"}, {"instruction": "def delete_prefix(self, key_prefix):\n        \"\"\"Delete a range of keys with a prefix in etcd.\"\"\"\n", "input": "", "output": "        return self.delete(\n            key_prefix, range_end=_encode(_increment_last_byte(key_prefix)))", "category": "Python"}, {"instruction": "async def step(self):\n        \"\"\" EXPERIMENTAL: Change self._client.game_step during the step function to increase or decrease steps per second \"\"\"\n", "input": "", "output": "        result = await self._execute(step=sc_pb.RequestStep(count=self.game_step))\n        return result", "category": "Python"}, {"instruction": "def to_dict(self):\n        \"\"\"Dump season to a dictionary.\n\n        :return: Season dictionary\n        :rtype: :class:`~python:dict`\n        \"\"\"\n", "input": "", "output": "\n        result = self.to_identifier()\n\n        result.update({\n            'ids': dict([\n                (key, value) for (key, value) in self.keys[1:]  # NOTE: keys[0] is the season identifier\n            ])\n        })\n\n        if self.rating:\n            result['rating'] = self.rating.value\n            result['rated_at'] = to_iso8601_datetime(self.rating.timestamp)\n\n        result['in_watchlist'] = self.in_watchlist if self.in_watchlist is not None else 0\n\n        # Extended Info\n        if self.first_aired:\n            result['first_aired'] = to_iso8601_datetime(self.first_aired)\n\n        if self.episode_count:\n            result['episode_count'] = self.episode_count\n\n        if self.aired_episodes:\n            result['aired_episodes'] = self.aired_episodes\n\n        return result", "category": "Python"}, {"instruction": "def predict(self, X):\n        \"\"\"\n        Predict values using the model\n\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape [n_samples, n_features]\n\n        Returns\n        -------\n        C : numpy array of shape [n_samples, n_outputs]\n            Predicted values.\n        \"\"\"\n", "input": "", "output": "        if self._genelm_regressor is None:\n            raise ValueError(\"SimpleELMRegressor not fitted\")\n\n        return self._genelm_regressor.predict(X)", "category": "Python"}, {"instruction": "def delimiter_encodeseq(delimiter, encodeseq, charset):\n    '''Coerce delimiter and encodeseq to unicode and verify that they are not\n       the same'''\n", "input": "", "output": "    delimiter = coerce_unicode(delimiter, charset)\n    encodeseq = coerce_unicode(encodeseq, charset)\n    if 1 != len(encodeseq):\n        raise FSQEncodeError(errno.EINVAL, u'encode sequence must be 1'\\\n                             u' character, not {0}'.format(len(encodeseq)))\n    if 1 != len(delimiter):\n        raise FSQEncodeError(errno.EINVAL, u'delimiter must be 1 character,'\\\n                             u' not {0}'.format(len(delimiter)))\n    if delimiter == encodeseq:\n        raise FSQEncodeError(errno.EINVAL, u'delimiter and encoding may not'\\\n                             u' be the same: both: {0}'.format(encodeseq))\n    try:\n        delimiter.encode('ascii')\n    except UnicodeEncodeError:\n        raise FSQEncodeError(errno.EINVAL, u'delimiter must be ascii')\n    try:\n        encodeseq.encode('ascii')\n    except UnicodeEncodeError:\n        raise FSQEncodeError(errno.EINVAL, u'encodeseq must be ascii')\n\n    return delimiter, encodeseq", "category": "Python"}, {"instruction": "def contains(self, chrom, start, end, overlap=True):\n        \"\"\"This returns a list of VCFEntry objects which cover a specified location.\n\n        :param chrom: The landmark identifier (usually a chromosome)\n        :param start: The 1-based position of the start of the range we are querying\n        :param end: The 1-based position of the end of the range we are querying\n        :param overlap: A boolean value, if true we allow features to overlap the query range.\n        For instance, overlap=True with the range (5,10), will return a VCFEntry object\n        spanning from (8,15). overlap=False will only return objects fully containing the range.\n        :return: A list of VCFEntry objects\n        \"\"\"\n", "input": "", "output": "        d = self.positions.get(chrom,[])\n        if overlap:\n            return [vcf_entry for vcf_start, vcf_end in d\n                    for vcf_entry in d[(vcf_start, vcf_end)]\n                    if not (end < vcf_start or start > vcf_end)]\n        else:\n            return [vcf_entry for vcf_start, vcf_end in d\n                    for vcf_entry in d[(vcf_start, vcf_end)]\n                    if (vcf_start <= start and vcf_end >= end)]", "category": "Python"}, {"instruction": "def mobile(self):\n        \"\"\"\n        Access the mobile\n\n        :returns: twilio.rest.api.v2010.account.incoming_phone_number.mobile.MobileList\n        :rtype: twilio.rest.api.v2010.account.incoming_phone_number.mobile.MobileList\n        \"\"\"\n", "input": "", "output": "        if self._mobile is None:\n            self._mobile = MobileList(self._version, account_sid=self._solution['account_sid'], )\n        return self._mobile", "category": "Python"}, {"instruction": "def previous_page_url(self):\n        \"\"\"\n        :return str: Returns a link to the previous_page_url or None if doesn't exist.\n        \"\"\"\n", "input": "", "output": "        if 'meta' in self._payload and 'previous_page_url' in self._payload['meta']:\n            return self._payload['meta']['previous_page_url']\n        elif 'previous_page_uri' in self._payload and self._payload['previous_page_uri']:\n            return self._version.domain.absolute_url(self._payload['previous_page_uri'])\n\n        return None", "category": "Python"}, {"instruction": "def scan_in_memory(node, env, path=()):\n    \"\"\"\n    \"Scans\" a Node.FS.Dir for its in-memory entries.\n    \"\"\"\n", "input": "", "output": "    try:\n        entries = node.entries\n    except AttributeError:\n        # It's not a Node.FS.Dir (or doesn't look enough like one for\n        # our purposes), which can happen if a target list containing\n        # mixed Node types (Dirs and Files, for example) has a Dir as\n        # the first entry.\n        return []\n    entry_list = sorted(filter(do_not_scan, list(entries.keys())))\n    return [entries[n] for n in entry_list]", "category": "Python"}, {"instruction": "def delete_network_subname(self, sub_name):\n        \"\"\"Delete the network by part of its name, use with caution. \"\"\"\n", "input": "", "output": "        try:\n            body = {}\n            net_list = self.neutronclient.list_networks(body=body)\n            for net in net_list:\n                if net.get('name').find(sub_name) != -1:\n                    self.delete_network_all_subnets(net.get('net_id'))\n        except Exception as exc:\n            LOG.error(\"Failed to get network by subname %(name)s, \"\n                      \"Exc %(exc)s\",\n                      {'name': sub_name, 'exc': str(exc)})", "category": "Python"}, {"instruction": "def _matches_filters(self, obj, filter_args):\n        \"\"\"\n        Return a boolean indicating whether a resource object matches a set\n        of filter arguments.\n        This is used for client-side filtering.\n\n        Depending on the properties specified in the filter arguments, this\n        method retrieves the resource properties from the HMC.\n\n        Parameters:\n\n          obj (BaseResource):\n            Resource object.\n\n          filter_args (dict):\n            Filter arguments. For details, see :ref:`Filtering`.\n            `None` causes the resource to always match.\n\n        Returns:\n\n          bool: Boolean indicating whether the resource object matches the\n            filter arguments.\n        \"\"\"\n", "input": "", "output": "        if filter_args is not None:\n            for prop_name in filter_args:\n                prop_match = filter_args[prop_name]\n                if not self._matches_prop(obj, prop_name, prop_match):\n                    return False\n        return True", "category": "Python"}, {"instruction": "def TSKVolumeGetBytesPerSector(tsk_volume):\n  \"\"\"Retrieves the number of bytes per sector from a TSK volume object.\n\n  Args:\n    tsk_volume (pytsk3.Volume_Info): TSK volume information.\n\n  Returns:\n    int: number of bytes per sector or 512 by default.\n  \"\"\"\n", "input": "", "output": "  # Note that because pytsk3.Volume_Info does not explicitly defines info\n  # we need to check if the attribute exists and has a value other\n  # than None. Default to 512 otherwise.\n  if hasattr(tsk_volume, 'info') and tsk_volume.info is not None:\n    block_size = getattr(tsk_volume.info, 'block_size', 512)\n  else:\n    block_size = 512\n\n  return block_size", "category": "Python"}, {"instruction": "def l2_batch_normalize(x, epsilon=1e-12, scope=None):\n  \"\"\"\n  Helper function to normalize a batch of vectors.\n  :param x: the input placeholder\n  :param epsilon: stabilizes division\n  :return: the batch of l2 normalized vector\n  \"\"\"\n", "input": "", "output": "  with tf.name_scope(scope, \"l2_batch_normalize\") as name_scope:\n    x_shape = tf.shape(x)\n    x = tf.contrib.layers.flatten(x)\n    x /= (epsilon + reduce_max(tf.abs(x), 1, keepdims=True))\n    square_sum = reduce_sum(tf.square(x), 1, keepdims=True)\n    x_inv_norm = tf.rsqrt(np.sqrt(epsilon) + square_sum)\n    x_norm = tf.multiply(x, x_inv_norm)\n    return tf.reshape(x_norm, x_shape, name_scope)", "category": "Python"}]