[{"instruction": "def abort_now ():\n    \"\"\"Force exit of current process without cleanup.\"\"\"\n", "input": "", "output": "    if os.name == 'posix':\n        # Unix systems can use signals\n        import signal\n        os.kill(os.getpid(), signal.SIGTERM)\n        time.sleep(1)\n        os.kill(os.getpid(), signal.SIGKILL)\n    elif os.name == 'nt':\n        # NT has os.abort()\n        os.abort()\n    else:\n        # All other systems have os._exit() as best shot.\n        os._exit(3)", "category": "Python"}, {"instruction": "def transform(self, X, y=None):\n        \"\"\"Use the model to transform matrix to Shared Response space\n\n        Parameters\n        ----------\n        X : list of 2D arrays, element i has shape=[voxels_i, samples_i]\n            Each element in the list contains the fMRI data of one subject\n            note that number of voxels and samples can vary across subjects\n        y : not used (as it is unsupervised learning)\n\n\n        Returns\n        -------\n        s : list of 2D arrays, element i has shape=[features_i, samples_i]\n            Shared responses from input data (X)\n        \"\"\"\n", "input": "", "output": "\n        # Check if the model exist\n        if hasattr(self, 'w_') is False:\n            raise NotFittedError(\"The model fit has not been run yet.\")\n\n        # Check the number of subjects\n        if len(X) != len(self.w_):\n            raise ValueError(\"The number of subjects does not match the one\"\n                             \" in the model.\")\n\n        s = [None] * len(X)\n        for subject in range(len(X)):\n            if X[subject] is not None:\n                s[subject] = self.w_[subject].T.dot(X[subject])\n\n        return s", "category": "Python"}, {"instruction": "def marketplace(self):\n        \"\"\"\n        :returns: Version marketplace of preview\n        :rtype: twilio.rest.preview.marketplace.Marketplace\n        \"\"\"\n", "input": "", "output": "        if self._marketplace is None:\n            self._marketplace = Marketplace(self)\n        return self._marketplace", "category": "Python"}, {"instruction": "def check_user_can_attach_file_to_comments(user_info, recid):\n    \"\"\"Check if the user is authorized to attach a file to comments\n    for given recid. This function does not check that user can view\n    the comments or send comments.\n\n    Returns the same type as acc_authorize_action\n    \"\"\"\n", "input": "", "output": "    # First can we find an authorization for this case action, for\n    # this collection?\n    record_primary_collection = guess_primary_collection_of_a_record(recid)\n    return acc_authorize_action(\n        user_info,\n        'attachcommentfile',\n        authorized_if_no_roles=False,\n        collection=record_primary_collection)", "category": "Python"}, {"instruction": "def get_pkglist():\n    \"\"\"\n    Return list of all installed packages\n\n    Note: It returns one project name per pkg no matter how many versions\n    of a particular package is installed\n\n    @returns: list of project name strings for every installed pkg\n\n    \"\"\"\n", "input": "", "output": "\n    dists = Distributions()\n    projects = []\n    for (dist, _active) in dists.get_distributions(\"all\"):\n        if dist.project_name not in projects:\n            projects.append(dist.project_name)\n    return projects", "category": "Python"}, {"instruction": "def fix_argument_only(self):\r\n        '''\r\n        fix_argument_only() -> Either or Unit(Argument)\r\n        `<arg> | ARG | <arg3>` ->\r\n            `Required(Argument('<arg>', 'ARG', '<arg3>'))`\r\n        `[<arg>] | [ARG] | [<arg3>]` ->\r\n            `Optional(Argument('<arg>', 'ARG', '<arg3>'))`\r\n        `(<arg>) | [ARG]` -> not change, return self\r\n        `-a | --better` -> not change\r\n        '''\n", "input": "", "output": "        # for idx, branch in enumerate(self):\r\n        #     if isinstance(branch[0], Either):\r\n        #         self[idx] = branch.fix()\r\n        first_type = type(self[0])\r\n        if first_type not in (Required, Optional):\r\n            return self\r\n        for branch in self:\r\n            if not (len(branch) == 1 and\r\n                    isinstance(branch, first_type) and\r\n                    isinstance(branch[0], Argument)):\r\n                logger.debug('fix %r not change', self)\r\n                return self\r\n        else:\r\n            first = self[0][0]\r\n            for each in self:\r\n                first.names.update(each[0].names)\r\n            result = first_type(first)\r\n            logger.debug('fix %r -> %r', self, result)\r\n            return result", "category": "Python"}, {"instruction": "def getModPath(self, *paths):\n        '''\n        Construct a path relative to this module's working directory.\n\n        Args:\n            *paths: A list of path strings\n\n        Notes:\n            This creates the module specific directory if it does not exist.\n\n        Returns:\n            (str): The full path (or None if no cortex dir is configured).\n        '''\n", "input": "", "output": "        dirn = self.getModDir()\n        return s_common.genpath(dirn, *paths)", "category": "Python"}, {"instruction": "def register_textx_subcommands():\n    \"\"\"\n    Find and use all textx sub-commands registered through extension points.\n    Extension points for CLI extension are:\n    - `textx_commands` - for registering top-level commands.\n    - `textx_command_groups` - for registering command groups.\n    \"\"\"\n", "input": "", "output": "    # Register direct sub-commands\n    global textx\n    for subcommand in pkg_resources.iter_entry_points(group='textx_commands'):\n        textx.command()(subcommand.load())\n\n    # Register sub-command groups\n    for subgroup in pkg_resources.iter_entry_points(\n            group='textx_command_groups'):\n        subgroup.load()(textx)", "category": "Python"}, {"instruction": "def get(self, key, alt=None):\n        \"\"\"If dictionary contains _key_ return the associated value,\n        otherwise return _alt_.\n        \"\"\"\n", "input": "", "output": "        with self.lock:\n            if key in self:\n                return self.getitem(key)\n\n            else:\n                return alt", "category": "Python"}, {"instruction": "def isotime(at=None, subsecond=False):\n    \"\"\"Stringify time in ISO 8601 format.\"\"\"\n", "input": "", "output": "    if not at:\n        at = utcnow()\n    st = at.strftime(_ISO8601_TIME_FORMAT\n                     if not subsecond\n                     else _ISO8601_TIME_FORMAT_SUBSECOND)\n    tz = at.tzinfo.tzname(None) if at.tzinfo else 'UTC'\n    st += ('Z' if tz == 'UTC' else tz)\n    return st", "category": "Python"}, {"instruction": "def parse_statement(self, stream):\n        \"\"\"\n        Statement ::= AggrGroup\n                    | AggrObject\n                    | AssignmentStmt\n        \"\"\"\n", "input": "", "output": "\n        if self.has_group(stream):\n            return self.parse_group(stream)\n\n        if self.has_object(stream):\n            return self.parse_object(stream)\n\n        if self.has_assignment(stream):\n            return self.parse_assignment(stream)\n\n        if self.has_assignment_symbol(stream):\n            return self.broken_assignment(stream.lineno - 1)\n\n        self.raise_unexpected(stream)", "category": "Python"}, {"instruction": "def ARPLimitExceeded_originator_switch_info_switchIdentifier(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        ARPLimitExceeded = ET.SubElement(config, \"ARPLimitExceeded\", xmlns=\"http://brocade.com/ns/brocade-notification-stream\")\n        originator_switch_info = ET.SubElement(ARPLimitExceeded, \"originator-switch-info\")\n        switchIdentifier = ET.SubElement(originator_switch_info, \"switchIdentifier\")\n        switchIdentifier.text = kwargs.pop('switchIdentifier')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def Network_setBlockedURLs(self, urls):\n\t\t\"\"\"\n\t\tFunction path: Network.setBlockedURLs\n\t\t\tDomain: Network\n\t\t\tMethod name: setBlockedURLs\n\t\t\n\t\t\tWARNING: This function is marked 'Experimental'!\n\t\t\n\t\t\tParameters:\n\t\t\t\tRequired arguments:\n\t\t\t\t\t'urls' (type: array) -> URL patterns to block. Wildcards ('*') are allowed.\n\t\t\tNo return value.\n\t\t\n\t\t\tDescription: Blocks URLs from loading.\n\t\t\"\"\"\n", "input": "", "output": "\t\tassert isinstance(urls, (list, tuple)\n\t\t    ), \"Argument 'urls' must be of type '['list', 'tuple']'. Received type: '%s'\" % type(\n\t\t    urls)\n\t\tsubdom_funcs = self.synchronous_command('Network.setBlockedURLs', urls=urls)\n\t\treturn subdom_funcs", "category": "Python"}, {"instruction": "def live_scores(self, live_scores):\n        \"\"\"Prints the live scores in a pretty format\"\"\"\n", "input": "", "output": "        scores = sorted(live_scores, key=lambda x: x[\"league\"])\n        for league, games in groupby(scores, key=lambda x: x[\"league\"]):\n            self.league_header(league)\n            for game in games:\n                self.scores(self.parse_result(game), add_new_line=False)\n                click.secho('   %s' % Stdout.utc_to_local(game[\"time\"],\n                                                          use_12_hour_format=False),\n                            fg=self.colors.TIME)\n                click.echo()", "category": "Python"}, {"instruction": "def kubectl(*args, input=None, **flags):\n    \"\"\"Simple wrapper to kubectl.\"\"\"\n", "input": "", "output": "    # Build command line call.\n    line = ['kubectl'] + list(args)\n    line = line + get_flag_args(**flags)\n    if input is not None:\n        line = line + ['-f', '-']\n    # Run subprocess\n    output = subprocess.run(\n        line,\n        input=input,\n        capture_output=True,\n        text=True\n    )\n    return output", "category": "Python"}, {"instruction": "def derivatives(self, x, y, Rs, theta_Rs, r_core, center_x=0, center_y=0):\n        \"\"\"\n        deflection angles\n        :param x: x coordinate\n        :param y: y coordinate\n        :param Rs: scale radius\n        :param rho0: central core density\n        :param r_core: core radius\n        :param center_x:\n        :param center_y:\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        rho0 = self._alpha2rho0(theta_Rs=theta_Rs, Rs=Rs, r_core=r_core)\n\n        if Rs < 0.0000001:\n            Rs = 0.0000001\n        x_ = x - center_x\n        y_ = y - center_y\n        R = np.sqrt(x_ ** 2 + y_ ** 2)\n\n        dx, dy = self.coreBurkAlpha(R, Rs, rho0, r_core, x_, y_)\n\n        return dx, dy", "category": "Python"}, {"instruction": "def main():\n    \"\"\"\n    Install a package from the local folder\n    :return:\n    \"\"\"\n", "input": "", "output": "    common.setup_main()\n    config = common.ConfigData()\n    dist_folder = 'dist'\n\n    if os.path.isdir(dist_folder):\n        shutil.rmtree(dist_folder)\n    args = [\n        '{}'.format(config.python),\n        'setup.py',\n        'sdist',\n    ]\n    if config.setup_quiet:\n        args.extend([\n            '--quiet',\n        ])\n    common.check_call_no_output(args)\n    files = list(os.listdir(dist_folder))\n    assert len(files) == 1, \"too many files in {}\".format(dist_folder)\n    new_file = os.path.join(dist_folder, files[0])\n    args = []\n    if config.use_sudo:\n        args.extend([\n            'sudo',\n            '-H',\n        ])\n    args.extend([\n        '{}'.format(config.pip),\n        'install',\n        '--quiet',\n        '--upgrade',\n        new_file,\n    ])\n    if config.pip_quiet:\n        args.extend([\n            '--quiet',\n        ])\n    if config.install_in_user_folder:\n        args.extend([\n            '--user',\n        ])\n    common.check_call_no_output(args)", "category": "Python"}, {"instruction": "def _resolve(self, path, migration_file):\n        \"\"\"\n        Resolve a migration instance from a file.\n\n        :param migration_file: The migration file\n        :type migration_file: str\n\n        :rtype: eloquent.migrations.migration.Migration\n        \"\"\"\n", "input": "", "output": "        variables = {}\n\n        name = '_'.join(migration_file.split('_')[4:])\n        migration_file = os.path.join(path, '%s.py' % migration_file)\n\n        with open(migration_file) as fh:\n            exec(fh.read(), {}, variables)\n\n        klass = variables[inflection.camelize(name)]\n\n        instance = klass()\n        instance.set_schema_builder(self.get_repository().get_connection().get_schema_builder())\n\n        return instance", "category": "Python"}, {"instruction": "def get_block_info(self):\n        \"\"\"\n        Get the retrieved block information.\n        Return [(height, [txs])] on success, ordered on height\n        Raise if not finished downloading\n        \"\"\"\n", "input": "", "output": "        if not self.finished:\n            raise Exception(\"Not finished downloading\")\n\n        ret = []\n        for (block_hash, block_data) in self.block_info.items():\n            ret.append( (block_data['height'], block_data['txns']) )\n\n        return ret", "category": "Python"}, {"instruction": "def mime_type(filename):\n\t\"\"\" Guess mime type for the given file name\n\n\tNote: this implementation uses python_magic package which is not thread-safe, as a workaround global lock is\n\tused for the ability to work in threaded environment\n\n\t:param filename: file name to guess\n\t:return: str\n\t\"\"\"\n", "input": "", "output": "\t# TODO: write lock-free mime_type function\n\ttry:\n\t\t__mime_lock.acquire()\n\n\t\textension = filename.split(\".\")\n\t\textension = extension[len(extension) - 1]\n\n\t\tif extension == \"woff2\":\n\t\t\treturn \"application/font-woff2\"\n\t\tif extension == \"css\":\n\t\t\treturn \"text/css\"\n\n\t\tm = magic.from_file(filename, mime=True)\n\t\tm = m.decode() if isinstance(m, bytes) else m  # compatibility fix, some versions return bytes some - str\n\t\tif m == \"text/plain\":\n\t\t\tguessed_type = mimetypes.guess_type(filename)[0]  # for js-detection\n\t\t\tif guessed_type:\n\t\t\t\treturn guessed_type\n\t\treturn m\n\tfinally:\n\t\t__mime_lock.release()", "category": "Python"}, {"instruction": "def get_memory_annotations(cls, exclude=None):\n        \"\"\"Get annotations in memory which inherits from cls.\n\n        :param tuple/type exclude: annotation type(s) to exclude from search.\n        :return: found annotations which inherits from cls.\n        :rtype: set\n        \"\"\"\n", "input": "", "output": "\n        result = set()\n\n        # get global dictionary\n        annotations_in_memory = Annotation.__ANNOTATIONS_IN_MEMORY__\n\n        exclude = () if exclude is None else exclude\n\n        # iterate on annotation classes\n        for annotation_cls in annotations_in_memory:\n\n            # if annotation class is excluded, continue\n            if issubclass(annotation_cls, exclude):\n                continue\n\n            # if annotation class inherits from self, add it in the result\n            if issubclass(annotation_cls, cls):\n                result |= annotations_in_memory[annotation_cls]\n\n        return result", "category": "Python"}, {"instruction": "def addComponentToPathway(self, component_id, pathway_id):\n        \"\"\"\n        This can be used directly when the component is directly involved in\n        the pathway.  If a transforming event is performed on the component\n        first, then the addGeneToPathway should be used instead.\n\n        :param pathway_id:\n        :param component_id:\n        :return:\n        \"\"\"\n", "input": "", "output": "        self.graph.addTriple(component_id, self.globaltt['involved in'], pathway_id)\n\n        return", "category": "Python"}, {"instruction": "def mark_all_as_unread(self, recipient=None):\n        \"\"\"Mark as unread any read messages in the current queryset.\n\n        Optionally, filter these by recipient first.\n        \"\"\"\n", "input": "", "output": "        qset = self.read(True)\n\n        if recipient:\n            qset = qset.filter(recipient=recipient)\n\n        return qset.update(unread=True)", "category": "Python"}, {"instruction": "def peek_32(library, session, address):\n    \"\"\"Read an 32-bit value from the specified address.\n\n    Corresponds to viPeek32 function of the VISA library.\n\n    :param library: the visa library wrapped by ctypes.\n    :param session: Unique logical identifier to a session.\n    :param address: Source address to read the value.\n    :return: Data read from bus, return value of the library call.\n    :rtype: bytes, :class:`pyvisa.constants.StatusCode`\n    \"\"\"\n", "input": "", "output": "    value_32 = ViUInt32()\n    ret = library.viPeek32(session, address, byref(value_32))\n    return value_32.value, ret", "category": "Python"}, {"instruction": "def multipart_delete(self, multipart):\n        \"\"\"Abort a multipart upload.\n\n        :param multipart: A :class:`invenio_files_rest.models.MultipartObject`\n            instance.\n        :returns: A Flask response.\n        \"\"\"\n", "input": "", "output": "        multipart.delete()\n        db.session.commit()\n        if multipart.file_id:\n            remove_file_data.delay(str(multipart.file_id))\n        return self.make_response('', 204)", "category": "Python"}, {"instruction": "def worker_thread(context):\n    \"\"\"The worker thread routines.\"\"\"\n", "input": "", "output": "    queue = context.task_queue\n    parameters = context.worker_parameters\n\n    if parameters.initializer is not None:\n        if not run_initializer(parameters.initializer, parameters.initargs):\n            context.state = ERROR\n            return\n\n    for task in get_next_task(context, parameters.max_tasks):\n        execute_next_task(task)\n        queue.task_done()", "category": "Python"}, {"instruction": "async def run(self):\n        \"\"\"\n        Runs the agent. Answer to the requests made by the Backend.\n        May raise an asyncio.CancelledError, in which case the agent should clean itself and restart completely.\n        \"\"\"\n", "input": "", "output": "        self._logger.info(\"Agent started\")\n        self.__backend_socket.connect(self.__backend_addr)\n\n        # Tell the backend we are up and have `concurrency` threads available\n        self._logger.info(\"Saying hello to the backend\")\n        await ZMQUtils.send(self.__backend_socket, AgentHello(self.__friendly_name, self.__concurrency, self.environments))\n        self.__last_ping = time.time()\n\n        run_listen = self._loop.create_task(self.__run_listen())\n\n        self._loop.call_later(1, self._create_safe_task, self.__check_last_ping(run_listen))\n\n        await run_listen", "category": "Python"}, {"instruction": "def _GetDateValuesWithEpoch(self, number_of_days, date_time_epoch):\n    \"\"\"Determines date values.\n\n    Args:\n      number_of_days (int): number of days since epoch.\n      date_time_epoch (DateTimeEpoch): date and time of the epoch.\n\n    Returns:\n       tuple[int, int, int]: year, month, day of month.\n    \"\"\"\n", "input": "", "output": "    return self._GetDateValues(\n        number_of_days, date_time_epoch.year, date_time_epoch.month,\n        date_time_epoch.day_of_month)", "category": "Python"}, {"instruction": "def resample_returns(\n        returns,\n        func,\n        seed=0,\n        num_trials=100\n):\n    \"\"\"\n    Resample the returns and calculate any statistic on every new sample.\n\n    https://en.wikipedia.org/wiki/Resampling_(statistics)\n\n    :param returns (Series, DataFrame): Returns\n    :param func: Given the resampled returns calculate a statistic\n    :param seed: Seed for random number generator\n    :param num_trials: Number of times to resample and run the experiment\n    :return: Series of resampled statistics\n    \"\"\"\n", "input": "", "output": "\n    # stats = []\n    if type(returns) is pd.Series:\n        stats = pd.Series(index=range(num_trials))\n    elif type(returns) is pd.DataFrame:\n        stats = pd.DataFrame(\n            index=range(num_trials),\n            columns=returns.columns\n        )\n    else:\n        raise(TypeError(\"returns needs to be a Series or DataFrame!\"))\n\n    n = returns.shape[0]\n    for i in range(num_trials):\n        random_indices = resample(returns.index, n_samples=n, random_state=seed + i)\n        stats.loc[i] = func(returns.loc[random_indices])\n\n    return stats", "category": "Python"}, {"instruction": "def is_multivalued(value):\n    \"\"\"\n    Determine whether the given value should be treated as a sequence\n    of multiple values when used as a request parameter.\n\n    In general anything that is iterable is multivalued.  For example,\n    `list` and `tuple` instances are multivalued.  Generators are\n    multivalued, as are the iterable objects returned by `zip`,\n    `itertools.chain`, etc.  However, a simple `int` is single-valued.\n    `str` and `bytes` are special cases: although these are iterable,\n    we treat each as a single value rather than as a sequence of\n    isolated characters or bytes.\n    \"\"\"\n", "input": "", "output": "\n    # special cases: iterable, but not multivalued\n    if isinstance(value, (string_types, binary_type)):\n        return False\n\n    # general rule: multivalued if iterable\n    return isinstance(value, Iterable)", "category": "Python"}, {"instruction": "def parsedate_tz(data):\n    \"\"\"Convert a date string to a time tuple.\n\n    Accounts for military timezones.\n    \"\"\"\n", "input": "", "output": "    res = _parsedate_tz(data)\n    if not res:\n        return\n    if res[9] is None:\n        res[9] = 0\n    return tuple(res)", "category": "Python"}, {"instruction": "def append_data(dset, data):\n    \"\"\"Append data to dset along axis 0. Data must be a single element or\n    a 1D array of the same type as the dataset (including compound datatypes).\"\"\"\n", "input": "", "output": "    N = data.shape[0] if hasattr(data, 'shape') else 1\n    if N == 0:\n        return\n    oldlen = dset.shape[0]\n    newlen = oldlen + N\n    dset.resize(newlen, axis=0)\n    dset[oldlen:] = data", "category": "Python"}, {"instruction": "def apply_color(self, arr, state):\n        \"\"\"Apply color formula to an array.\"\"\"\n", "input": "", "output": "        ops = self.cmd(state)\n        for func in parse_operations(ops):\n            arr = func(arr)\n        return arr", "category": "Python"}, {"instruction": "def get_fields(model_class):\n    \"\"\"\n    Pass in a mongo model class and extract all the attributes which\n    are mongoengine fields\n\n    Returns:\n        list of strings of field attributes\n    \"\"\"\n", "input": "", "output": "    return [\n        attr for attr, value in model_class.__dict__.items()\n        if issubclass(type(value), (mongo.base.BaseField, mongo.EmbeddedDocumentField))  # noqa\n    ]", "category": "Python"}, {"instruction": "def pip_report(self):\n        \"\"\"\n        Show editable pip-requirements line necessary to clone this repository\n\n        Yields:\n            str: pip-requirements line necessary to clone this repository\n        \"\"\"\n", "input": "", "output": "        comment = '#' if not self.remote_url else ''\n        if os.path.exists(os.path.join(self.fpath, 'setup.py')):\n            yield u\"%s-e %s+%s@%s#egg=%s\" % (\n                comment,\n                self.label,\n                self.to_normal_url(self.remote_url),\n                self.current_id,\n                self.eggname)\n        return", "category": "Python"}, {"instruction": "def list_scanners(zap_helper, scanners):\n    \"\"\"Get a list of scanners and whether or not they are enabled.\"\"\"\n", "input": "", "output": "    scanner_list = zap_helper.zap.ascan.scanners()\n\n    if scanners is not None and 'all' not in scanners:\n        scanner_list = filter_by_ids(scanner_list, scanners)\n\n    click.echo(tabulate([[s['id'], s['name'], s['policyId'], s['enabled'], s['attackStrength'], s['alertThreshold']]\n                         for s in scanner_list],\n                        headers=['ID', 'Name', 'Policy ID', 'Enabled', 'Strength', 'Threshold'],\n                        tablefmt='grid'))", "category": "Python"}, {"instruction": "def node_branch(self, al_keys, al_values):\n            \"\"\"\n            For each node in <al_values>, add to internal contents\n            dictionary using key from <al_keys>.\n            \"\"\"\n", "input": "", "output": "            if len(al_keys) != len(al_values):\n                self.error_exit(\"adding branch nodes\", \"#keys != #values\", 1)\n            ldict = dict(zip(al_keys, al_values))\n            self.node_dictBranch(ldict)", "category": "Python"}, {"instruction": "def parse_ents(doc, options={}):\n    \"\"\"Generate named entities in [{start: i, end: i, label: 'label'}] format.\n\n    doc (Doc): Document do parse.\n    RETURNS (dict): Generated entities keyed by text (original text) and ents.\n    \"\"\"\n", "input": "", "output": "    ents = [\n        {\"start\": ent.start_char, \"end\": ent.end_char, \"label\": ent.label_}\n        for ent in doc.ents\n    ]\n    if not ents:\n        user_warning(Warnings.W006)\n    title = doc.user_data.get(\"title\", None) if hasattr(doc, \"user_data\") else None\n    settings = get_doc_settings(doc)\n    return {\"text\": doc.text, \"ents\": ents, \"title\": title, \"settings\": settings}", "category": "Python"}, {"instruction": "def merge_setting(request_setting, session_setting, dict_class=OrderedDict):\n    \"\"\"\n    Determines appropriate setting for a given request, taking into account the\n    explicit setting on that request, and the setting in the session. If a\n    setting is a dictionary, they will be merged together using `dict_class`\n    \"\"\"\n", "input": "", "output": "\n    if session_setting is None:\n        return request_setting\n\n    if request_setting is None:\n        return session_setting\n\n    # Bypass if not a dictionary (e.g. verify)\n    if not (\n            isinstance(session_setting, Mapping) and\n            isinstance(request_setting, Mapping)\n    ):\n        return request_setting\n\n    merged_setting = dict_class(to_key_val_list(session_setting))\n    merged_setting.update(to_key_val_list(request_setting))\n\n    # Remove keys that are set to None.\n    for (k, v) in request_setting.items():\n        if v is None:\n            del merged_setting[k]\n\n    merged_setting = dict((k, v) for (k, v) in merged_setting.items() if v is not None)\n\n    return merged_setting", "category": "Python"}, {"instruction": "def members(self, uid=\"*\", objects=False):\n        \"\"\" members() issues an ldap query for all users, and returns a dict\n            for each matching entry. This can be quite slow, and takes roughly\n            3s to complete. You may optionally restrict the scope by specifying\n            a uid, which is roughly equivalent to a search(uid='foo')\n        \"\"\"\n", "input": "", "output": "        entries = self.search(uid='*')\n        if objects:\n            return self.memberObjects(entries)\n        result = []\n        for entry in entries:\n            result.append(entry[1])\n        return result", "category": "Python"}, {"instruction": "def periodic_callback(self):\n        \"\"\"Periodically help maintain adapter internal state\n        \"\"\"\n", "input": "", "output": "\n        while True:\n            try:\n                action = self._deferred.get(False)\n                action()\n            except queue.Empty:\n                break\n            except Exception:\n                self._logger.exception('Exception in periodic callback')", "category": "Python"}, {"instruction": "def sub(self, inplace=False, **kwargs):\n        \"\"\"Return a entity where the conditions are met\"\"\"\n", "input": "", "output": "        filter_ = self.where(**kwargs)\n        return self.subindex(filter_, inplace)", "category": "Python"}, {"instruction": "def confd_state_internal_cdb_client_lock(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        confd_state = ET.SubElement(config, \"confd-state\", xmlns=\"http://tail-f.com/yang/confd-monitoring\")\n        internal = ET.SubElement(confd_state, \"internal\")\n        cdb = ET.SubElement(internal, \"cdb\")\n        client = ET.SubElement(cdb, \"client\")\n        lock = ET.SubElement(client, \"lock\")\n        lock.text = kwargs.pop('lock')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def urijoin(base, ref, strict=False):\n    \"\"\"Convert a URI reference relative to a base URI to its target URI\n    string.\n\n    \"\"\"\n", "input": "", "output": "    if isinstance(base, type(ref)):\n        return urisplit(base).transform(ref, strict).geturi()\n    elif isinstance(base, bytes):\n        return urisplit(base.decode()).transform(ref, strict).geturi()\n    else:\n        return urisplit(base).transform(ref.decode(), strict).geturi()", "category": "Python"}, {"instruction": "def _redis_connection_settings(self):\n        \"\"\"Return a dictionary of redis connection settings.\n\n        \"\"\"\n", "input": "", "output": "        return {config.HOST: self.settings.get(config.HOST, self._REDIS_HOST),\n                config.PORT: self.settings.get(config.PORT, self._REDIS_PORT),\n                'selected_db': self.settings.get(config.DB, self._REDIS_DB)}", "category": "Python"}, {"instruction": "def port(self, name):\n        \"\"\"\n        Locate a port by name.\n\n        @param name: A port name.\n        @type name: str\n        @return: The port object.\n        @rtype: L{Port}\n\n        \"\"\"\n", "input": "", "output": "        for p in self.ports:\n            if p.name == name:\n                return p", "category": "Python"}, {"instruction": "def close (self):\n        \"\"\"Get results and close clamd daemon sockets.\"\"\"\n", "input": "", "output": "        self.wsock.close()\n        data = self.sock.recv(self.sock_rcvbuf)\n        while data:\n            if \"FOUND\\n\" in data:\n                self.infected.append(data)\n            if \"ERROR\\n\" in data:\n                self.errors.append(data)\n            data = self.sock.recv(self.sock_rcvbuf)\n        self.sock.close()", "category": "Python"}, {"instruction": "def mt_interact(self):\n        \"\"\"Multithreaded version of interact().\"\"\"\n", "input": "", "output": "        import _thread\n        _thread.start_new_thread(self.listener, ())\n        while 1:\n            line = sys.stdin.readline()\n            if not line:\n                break\n            self.write(line)", "category": "Python"}, {"instruction": "def _proxy(self):\n        \"\"\"\n        Generate an instance context for the instance, the context is capable of\n        performing various actions.  All instance actions are proxied to the context\n\n        :returns: TranscriptionContext for this TranscriptionInstance\n        :rtype: twilio.rest.api.v2010.account.recording.transcription.TranscriptionContext\n        \"\"\"\n", "input": "", "output": "        if self._context is None:\n            self._context = TranscriptionContext(\n                self._version,\n                account_sid=self._solution['account_sid'],\n                recording_sid=self._solution['recording_sid'],\n                sid=self._solution['sid'],\n            )\n        return self._context", "category": "Python"}, {"instruction": "def cancel_order(self, order_id, stock):\n        \"\"\"Cancel An Order\n\n        https://starfighter.readme.io/docs/cancel-an-order\n        \"\"\"\n", "input": "", "output": "        url_fragment = 'venues/{venue}/stocks/{stock}/orders/{order_id}'.format(\n            venue=self.venue,\n            stock=stock,\n            order_id=order_id,\n        )\n        url = urljoin(self.base_url, url_fragment)\n        return self.session.delete(url).json()", "category": "Python"}, {"instruction": "def transitingPlanets(self):\n        \"\"\" Returns a list of transiting planet objects\n        \"\"\"\n", "input": "", "output": "\n        transitingPlanets = []\n\n        for planet in self.planets:\n            try:\n                if planet.isTransiting:\n                    transitingPlanets.append(planet)\n            except KeyError:  # No 'discoverymethod' tag - this also filters Solar System planets\n                pass\n\n        return transitingPlanets", "category": "Python"}, {"instruction": "def getEngineVersion(self, outputFormat = 'full'):\n\t\t\"\"\"\n\t\tReturns the version number of the latest installed version of UE4\n\t\t\"\"\"\n", "input": "", "output": "\t\tversion = self._getEngineVersionDetails()\n\t\tformats = {\n\t\t\t'major': version['MajorVersion'],\n\t\t\t'minor': version['MinorVersion'],\n\t\t\t'patch': version['PatchVersion'],\n\t\t\t'full': '{}.{}.{}'.format(version['MajorVersion'], version['MinorVersion'], version['PatchVersion']),\n\t\t\t'short': '{}.{}'.format(version['MajorVersion'], version['MinorVersion'])\n\t\t}\n\t\t\n\t\t# Verify that the requested output format is valid\n\t\tif outputFormat not in formats:\n\t\t\traise Exception('unreconised version output format \"{}\"'.format(outputFormat))\n\t\t\n\t\treturn formats[outputFormat]", "category": "Python"}, {"instruction": "def get_usb_controller_by_name(self, name):\n        \"\"\"Returns a USB controller with the given type.\n\n        in name of type str\n\n        return controller of type :class:`IUSBController`\n\n        raises :class:`VBoxErrorObjectNotFound`\n            A USB controller with given name doesn't exist.\n        \n        \"\"\"\n", "input": "", "output": "        if not isinstance(name, basestring):\n            raise TypeError(\"name can only be an instance of type basestring\")\n        controller = self._call(\"getUSBControllerByName\",\n                     in_p=[name])\n        controller = IUSBController(controller)\n        return controller", "category": "Python"}, {"instruction": "def generate_new_meta(name: str, description: str, vendor: str, license: str) -> dict:\n    \"\"\"\n    Create the metadata tree for the given model name and the list of dependencies.\n\n    :param name: Name of the model.\n    :param description: Description of the model.\n    :param vendor: Name of the party which is responsible for support of the model.\n    :param license: License identifier.\n    :return: dict with the metadata.\n    \"\"\"\n", "input": "", "output": "    check_license(license)\n    return {\n        \"code\": None,\n        \"created_at\": get_datetime_now(),\n        \"datasets\": [],\n        \"dependencies\": [],\n        \"description\": description,\n        \"vendor\": vendor,\n        \"environment\": collect_environment_without_packages(),\n        \"extra\": None,\n        \"license\": license,\n        \"metrics\": {},\n        \"model\": name,\n        \"parent\": None,\n        \"references\": [],\n        \"series\": None,\n        \"tags\": [],\n        \"uuid\": str(uuid.uuid4()),\n        \"version\": [1, 0, 0],\n    }", "category": "Python"}, {"instruction": "def get_file(self, filename, scope='all'):\n        ''' Returns the BIDSFile object with the specified path.\n\n        Args:\n            filename (str): The path of the file to retrieve. Must be either\n                an absolute path, or relative to the root of this BIDSLayout.\n            scope (str, list): Scope of the search space. If passed, only\n                BIDSLayouts that match the specified scope will be\n                searched. See BIDSLayout docstring for valid values.\n\n        Returns: A BIDSFile, or None if no match was found.\n        '''\n", "input": "", "output": "        filename = os.path.abspath(os.path.join(self.root, filename))\n        layouts = self._get_layouts_in_scope(scope)\n        for ly in layouts:\n            if filename in ly.files:\n                return ly.files[filename]\n        return None", "category": "Python"}, {"instruction": "def search(self, query):\n        \"\"\"\n        :param str query: Search query. May contain boolean/set operations\n            and parentheses.\n        :returns: a list of document hashes corresponding to matching\n            documents.\n\n        Search the index. The return value is a list of dictionaries\n        corresponding to the documents that matched. These dictionaries contain\n        a ``content`` key with the original indexed content, along with any\n        additional metadata that was specified.\n        \"\"\"\n", "input": "", "output": "        return [self.get_document(key) for key, _ in self._search(query)]", "category": "Python"}, {"instruction": "def put(self, key, value):\n    '''Stores the object in all underlying datastores.'''\n", "input": "", "output": "    for store in self._stores:\n      store.put(key, value)", "category": "Python"}, {"instruction": "def worker_complete():\n    \"\"\"Complete worker.\"\"\"\n", "input": "", "output": "    participant_id = request.args.get(\"participant_id\")\n    if not participant_id:\n        return error_response(\n            error_type=\"bad request\", error_text=\"participantId parameter is required\"\n        )\n\n    try:\n        _worker_complete(participant_id)\n    except KeyError:\n        return error_response(\n            error_type=\"ParticipantId not found: {}\".format(participant_id)\n        )\n\n    return success_response(status=\"success\")", "category": "Python"}, {"instruction": "def ngrams(path, elem, ignore_hash=True):\n    \"\"\"\n    Yields N-grams from a JSTOR DfR dataset.\n\n    Parameters\n    ----------\n    path : string\n        Path to unzipped JSTOR DfR folder containing N-grams.\n    elem : string\n        Name of subdirectory containing N-grams. (e.g. 'bigrams').\n    ignore_hash : bool\n        If True, will exclude all N-grams that contain the hash '#' character.\n\n    Returns\n    -------\n    ngrams : :class:`.FeatureSet`\n\n    \"\"\"\n", "input": "", "output": "\n    grams = GramGenerator(path, elem, ignore_hash=ignore_hash)\n    return FeatureSet({k: Feature(f) for k, f in grams})", "category": "Python"}, {"instruction": "def atmos_worker(srcs, window, ij, args):\n    \"\"\"A simple atmospheric correction user function.\"\"\"\n", "input": "", "output": "    src = srcs[0]\n    rgb = src.read(window=window)\n    rgb = to_math_type(rgb)\n\n    atmos = simple_atmo(rgb, args[\"atmo\"], args[\"contrast\"], args[\"bias\"])\n\n    # should be scaled 0 to 1, scale to outtype\n    return scale_dtype(atmos, args[\"out_dtype\"])", "category": "Python"}, {"instruction": "def scroll_page_down(event):\n    \"\"\"\n    Scroll page down. (Prefer the cursor at the top of the page, after scrolling.)\n    \"\"\"\n", "input": "", "output": "    w = _current_window_for_event(event)\n    b = event.cli.current_buffer\n\n    if w and w.render_info:\n        # Scroll down one page.\n        line_index = max(w.render_info.last_visible_line(), w.vertical_scroll + 1)\n        w.vertical_scroll = line_index\n\n        b.cursor_position = b.document.translate_row_col_to_index(line_index, 0)\n        b.cursor_position += b.document.get_start_of_line_position(after_whitespace=True)", "category": "Python"}, {"instruction": "def induce_subgraph(G, node_subset):\n    \"\"\"\n    Induce a subgraph of G.\n\n    Parameters\n    ----------\n    G : networkx multidigraph\n    node_subset : list-like\n        the subset of nodes to induce a subgraph of G\n\n    Returns\n    -------\n    G2 : networkx multidigraph\n        the subgraph of G induced by node_subset\n    \"\"\"\n", "input": "", "output": "\n    node_subset = set(node_subset)\n\n    # copy nodes into new graph\n    G2 = G.__class__()\n    G2.add_nodes_from((n, G.nodes[n]) for n in node_subset)\n    \n    # copy edges to new graph, including parallel edges\n    if G2.is_multigraph:\n        G2.add_edges_from((n, nbr, key, d)\n            for n, nbrs in G.adj.items() if n in node_subset\n            for nbr, keydict in nbrs.items() if nbr in node_subset\n            for key, d in keydict.items())\n    else:\n        G2.add_edges_from((n, nbr, d)\n            for n, nbrs in G.adj.items() if n in node_subset\n            for nbr, d in nbrs.items() if nbr in node_subset)\n    \n    # update graph attribute dict, and return graph\n    G2.graph.update(G.graph)\n    return G2", "category": "Python"}, {"instruction": "def submit_job(self, halt_on_error=True):\n        \"\"\"Submit Batch request to ThreatConnect API.\"\"\"\n", "input": "", "output": "        # check global setting for override\n        if self.halt_on_batch_error is not None:\n            halt_on_error = self.halt_on_batch_error\n\n        try:\n            r = self.tcex.session.post('/v2/batch', json=self.settings)\n        except Exception as e:\n            self.tcex.handle_error(10505, [e], halt_on_error)\n        if not r.ok or 'application/json' not in r.headers.get('content-type', ''):\n            self.tcex.handle_error(10510, [r.status_code, r.text], halt_on_error)\n        data = r.json()\n        if data.get('status') != 'Success':\n            self.tcex.handle_error(10510, [r.status_code, r.text], halt_on_error)\n        self.tcex.log.debug('Batch Submit Data: {}'.format(data))\n        return data.get('data', {}).get('batchId')", "category": "Python"}, {"instruction": "def _checkServer(self, address, port):\n        \"\"\"\n        *Check that the TCP Port we've decided to use for tunnelling is available*\n        \"\"\"\n", "input": "", "output": "        # CREATE A TCP SOCKET\n        import socket\n        s = socket.socket()\n\n        try:\n            s.connect((address, port))\n            return True\n        except socket.error, e:\n            self.log.warning(\n                ", "category": "Python"}, {"instruction": "def _init_glyph(self, plot, mapping, properties):\n        \"\"\"\n        Returns a Bokeh glyph object and optionally creates a colorbar.\n        \"\"\"\n", "input": "", "output": "        ret = super(ColorbarPlot, self)._init_glyph(plot, mapping, properties)\n        if self.colorbar:\n            for k, v in list(self.handles.items()):\n                if not k.endswith('color_mapper'):\n                    continue\n                self._draw_colorbar(plot, v, k[:-12])\n        return ret", "category": "Python"}, {"instruction": "def after_run(self, remote_file_data):\n        \"\"\"\n        Save uuid of file to our LocalFile\n        :param remote_file_data: dict: DukeDS file data\n        \"\"\"\n", "input": "", "output": "        if self.file_upload_post_processor:\n            self.file_upload_post_processor.run(self.settings.data_service, remote_file_data)\n        remote_file_id = remote_file_data['id']\n        self.settings.watcher.transferring_item(self.local_file)\n        self.local_file.set_remote_id_after_send(remote_file_id)", "category": "Python"}, {"instruction": "def unicode_dict(_dict):\n    \"\"\"\n    Make sure keys and values of dict is unicode.\n    \"\"\"\n", "input": "", "output": "    r = {}\n    for k, v in iteritems(_dict):\n        r[unicode_obj(k)] = unicode_obj(v)\n    return r", "category": "Python"}, {"instruction": "def route(self, path, routinemethod, container = None, host = None, vhost = None, method = [b'GET', b'HEAD']):\n        '''\n        Route specified path to a WSGI-styled routine factory\n        \n        :param path: path to match, can be a regular expression\n         \n        :param routinemethod: factory function routinemethod(env), env is an Environment object\n                see also utils.http.Environment\n        \n        :param container: routine container\n        \n        :param host: if specified, only response to request to specified host\n        \n        :param vhost: if specified, only response to request to specified vhost.\n                      If not specified, response to dispatcher default vhost.\n        \n        :param method: if specified, response to specified methods\n        '''\n", "input": "", "output": "        self.routeevent(path, statichttp(container)(routinemethod), container, host, vhost, method)", "category": "Python"}, {"instruction": "def w_diffuser_inner(sed_inputs=sed_dict):\n    \"\"\"Return the inner width of each diffuser in the sedimentation tank.\n    Parameters\n    ----------\n    sed_inputs : dict\n        A dictionary of all of the constant inputs needed for sedimentation tank\n        calculations can be found in sed.yaml\n    Returns\n    -------\n    float\n        Inner width of each diffuser in the sedimentation tank\n    Examples\n    --------\n    >>> from aide_design.play import*\n    >>>\n    \"\"\"\n", "input": "", "output": "    return ut.ceil_nearest(w_diffuser_inner_min(sed_inputs).magnitude,\n                           (np.arange(1/16,1/4,1/16)*u.inch).magnitude)", "category": "Python"}, {"instruction": "def masked_flip(padded_sequence: torch.Tensor,\n                sequence_lengths: List[int]) -> torch.Tensor:\n    \"\"\"\n        Flips a padded tensor along the time dimension without affecting masked entries.\n\n        Parameters\n        ----------\n        padded_sequence : ``torch.Tensor``\n            The tensor to flip along the time dimension.\n            Assumed to be of dimensions (batch size, num timesteps, ...)\n        sequence_lengths : ``torch.Tensor``\n            A list containing the lengths of each unpadded sequence in the batch.\n\n        Returns\n        -------\n        A ``torch.Tensor`` of the same shape as padded_sequence.\n        \"\"\"\n", "input": "", "output": "    assert padded_sequence.size(0) == len(sequence_lengths), \\\n        f'sequence_lengths length ${len(sequence_lengths)} does not match batch size ${padded_sequence.size(0)}'\n    num_timesteps = padded_sequence.size(1)\n    flipped_padded_sequence = torch.flip(padded_sequence, [1])\n    sequences = [flipped_padded_sequence[i, num_timesteps - length:] for i, length in enumerate(sequence_lengths)]\n    return torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True)", "category": "Python"}, {"instruction": "def _systemctl_wait_until_finish(self, machine, unit):\n        \"\"\"\n        Internal method\n        workaround for systemd-run without --wait option\n        see  _run_systemdrun_decide method\n\n        :param machine:\n        :param unit:\n        :return:\n        \"\"\"\n", "input": "", "output": "        while True:\n            metadata = convert_kv_to_dict(\n                run_cmd(\n                    [\"systemctl\", \"--no-pager\", \"show\", \"-M\", machine, unit],\n                    return_output=True))\n            if not metadata[\"SubState\"] in [\"exited\", \"failed\"]:\n                time.sleep(0.1)\n            else:\n                break\n        run_cmd([\"systemctl\", \"--no-pager\", \"-M\", machine, \"stop\", unit], ignore_status=True)\n        return metadata[\"ExecMainStatus\"]", "category": "Python"}, {"instruction": "def _listen(self, protocols, From, description):\n        \"\"\"\n        Implementation of L{Listen}.\n        \"\"\"\n", "input": "", "output": "        # The peer is coming from a client-side representation of the user\n        # described by 'From', and talking *to* a server-side representation of\n        # the user described by 'From'.\n        self.verifyCertificateAllowed(From, From)\n        theirCert = Certificate.peerFromTransport(self.transport)\n        for protocolName in protocols:\n            if protocolName.startswith('.'):\n                raise VerifyError(\n                    \"Internal protocols are for server-server use _only_: %r\" %\n                    protocolName)\n\n            key = (From, protocolName)\n            value = (self, theirCert, description)\n            log.msg(\"%r listening for %r\" % key)\n            self.listeningClient.append((key, value))\n            self.service.listeningClients.setdefault(key, []).append(value)\n        return {}", "category": "Python"}, {"instruction": "def allButDOI(self):\n        \"\"\"\n        Returns a string of the normalized values from the Citation excluding the DOI number. Equivalent to getting the ID with [ID()](#metaknowledge.citation.Citation.ID) then appending the extra values from [Extra()](#metaknowledge.citation.Citation.Extra) and then removing the substring containing the DOI number.\n\n        # Returns\n\n        `str`\n\n        > A string containing the data of the Citation.\n        \"\"\"\n", "input": "", "output": "        extraTags = ['extraAuthors', 'V', 'issue', 'P', 'misc']\n        s = self.ID()\n        extras = []\n        for tag in extraTags:\n            if getattr(self, tag, False):\n                extras.append(str(getattr(self, tag)))\n        if len(extras) > 0:\n            return \"{0}, {1}\".format(s, ', '.join(extras))\n        else:\n            return s", "category": "Python"}, {"instruction": "def with_validator(self, validator):\n    \"\"\"Add a validator callback to this Measurement, chainable.\"\"\"\n", "input": "", "output": "    if not callable(validator):\n      raise ValueError('Validator must be callable', validator)\n    self.validators.append(validator)\n    self._cached = None\n    return self", "category": "Python"}, {"instruction": "def DictReader(file_obj, columns=None):  # pylint: disable=invalid-name\n    \"\"\"\n    Reader for a parquet file object.\n\n    This function is a generator returning an OrderedDict for each row\n    of data in the parquet file. Nested values will be flattend into the\n    top-level dict and can be referenced with '.' notation (e.g. 'foo' -> 'bar'\n    is referenced as 'foo.bar')\n\n    :param file_obj: the file containing parquet data\n    :param columns: the columns to include. If None (default), all columns\n                    are included. Nested values are referenced with \".\" notation\n    \"\"\"\n", "input": "", "output": "    footer = _read_footer(file_obj)\n    keys = columns if columns else [s.name for s in\n                                    footer.schema if s.type]\n\n    for row in reader(file_obj, columns):\n        yield OrderedDict(zip(keys, row))", "category": "Python"}, {"instruction": "def clean(self, value):\n        \"\"\"Cleans and returns the given value, or raises a ParameterNotValidError exception\"\"\"\n", "input": "", "output": "\n        if isinstance(value, numpy.ndarray):\n            return value\n        elif isinstance(value, (list, tuple)):\n            return numpy.array(value)\n\n        raise ParameterNotValidError", "category": "Python"}, {"instruction": "def route_has_dead_links(root, machine):\n    \"\"\"Quickly determine if a route uses any dead links.\n\n    Parameters\n    ----------\n    root : :py:class:`~rig.place_and_route.routing_tree.RoutingTree`\n        The root of the RoutingTree which contains nothing but RoutingTrees\n        (i.e. no vertices and links).\n    machine : :py:class:`~rig.place_and_route.Machine`\n        The machine in which the routes exist.\n\n    Returns\n    -------\n    bool\n        True if the route uses any dead/missing links, False otherwise.\n    \"\"\"\n", "input": "", "output": "    for direction, (x, y), routes in root.traverse():\n        for route in routes:\n            if (x, y, route) not in machine:\n                return True\n    return False", "category": "Python"}, {"instruction": "def stop(name, call=None):\n    '''\n    Stop a VM in DimensionData.\n\n    name:\n        The name of the VM to stop.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a stop vm_name\n    '''\n", "input": "", "output": "    conn = get_conn()\n    node = get_node(conn, name)  # pylint: disable=not-callable\n    log.debug('Node of Cloud VM: %s', node)\n\n    status = conn.ex_shutdown_graceful(node)\n    log.debug('Status of Cloud VM: %s', status)\n\n    return status", "category": "Python"}, {"instruction": "def _detect(self, score_only):\n        \"\"\"\n        Detect anomaly periods.\n        :param bool score_only: if true, only anomaly scores are computed.\n        \"\"\"\n", "input": "", "output": "        try:\n            algorithm = self.algorithm(**self.algorithm_params)\n            self.anom_scores = algorithm.run()\n        except exceptions.NotEnoughDataPoints:\n            algorithm = anomaly_detector_algorithms['default_detector'](self.time_series)\n            self.threshold = self.threshold or ANOMALY_THRESHOLD['default_detector']\n            self.anom_scores = algorithm.run()\n        if not score_only:\n            self._detect_anomalies()", "category": "Python"}, {"instruction": "def Get(self):\n    \"\"\"Fetch and return a proper ClientApproval object.\"\"\"\n", "input": "", "output": "\n    args = user_pb2.ApiGetClientApprovalArgs(\n        client_id=self.client_id,\n        approval_id=self.approval_id,\n        username=self.username)\n    result = self._context.SendRequest(\"GetClientApproval\", args)\n    return ClientApproval(\n        data=result, username=self._context.username, context=self._context)", "category": "Python"}, {"instruction": "def master_compile(master_opts, minion_opts, grains, id_, saltenv):\n    '''\n    Compile the master side low state data, and build the hidden state file\n    '''\n", "input": "", "output": "    st_ = MasterHighState(master_opts, minion_opts, grains, id_, saltenv)\n    return st_.compile_highstate()", "category": "Python"}, {"instruction": "def insert_multiple_records(self,\n                                table: str,\n                                fields: Sequence[str],\n                                records: Sequence[Sequence[Any]]) -> int:\n        \"\"\"Inserts a record into database, table \"table\", using the list of\n        fieldnames and the list of records (each a list of values).\n        Returns number of rows affected.\"\"\"\n", "input": "", "output": "        self.ensure_db_open()\n        sql = self.localize_sql(get_sql_insert(table, fields,\n                                               self.get_delims()))\n        log.debug(\"About to insert multiple records with SQL template: \" + sql)\n        try:\n            cursor = self.db.cursor()\n            debug_sql(sql, records)\n            cursor.executemany(sql, records)\n            # ... binds the placeholders (?, %s) to values in the process\n            # http://www.python.org/dev/peps/pep-0249/\n            log.debug(\"Records inserted.\")\n            return cursor.rowcount\n        except:  # nopep8\n            log.exception(\"insert_multiple_records: Failed to insert records.\")\n            raise", "category": "Python"}, {"instruction": "def __sort_draw(self):\n        \"\"\"Sort the drawable objects according to ascending order\"\"\"\n", "input": "", "output": "        if self.__do_need_sort_draw:\n            self.__draw_objects.sort(key=cmp_to_key(self.__draw_cmp))\n            self.__do_need_sort_draw = False", "category": "Python"}, {"instruction": "def rule_command_cmdlist_interface_q_interface_ve_leaf_interface_ve_leaf(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        rule = ET.SubElement(config, \"rule\", xmlns=\"urn:brocade.com:mgmt:brocade-aaa\")\n        index_key = ET.SubElement(rule, \"index\")\n        index_key.text = kwargs.pop('index')\n        command = ET.SubElement(rule, \"command\")\n        cmdlist = ET.SubElement(command, \"cmdlist\")\n        interface_q = ET.SubElement(cmdlist, \"interface-q\")\n        interface_ve_leaf = ET.SubElement(interface_q, \"interface-ve-leaf\")\n        interface = ET.SubElement(interface_ve_leaf, \"interface\")\n        ve_leaf = ET.SubElement(interface, \"ve-leaf\")\n        ve_leaf.text = kwargs.pop('ve_leaf')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def ip_acl_ip_access_list_standard_hide_ip_acl_std_seq_action(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        ip_acl = ET.SubElement(config, \"ip-acl\", xmlns=\"urn:brocade.com:mgmt:brocade-ip-access-list\")\n        ip = ET.SubElement(ip_acl, \"ip\")\n        access_list = ET.SubElement(ip, \"access-list\")\n        standard = ET.SubElement(access_list, \"standard\")\n        name_key = ET.SubElement(standard, \"name\")\n        name_key.text = kwargs.pop('name')\n        hide_ip_acl_std = ET.SubElement(standard, \"hide-ip-acl-std\")\n        seq = ET.SubElement(hide_ip_acl_std, \"seq\")\n        seq_id_key = ET.SubElement(seq, \"seq-id\")\n        seq_id_key.text = kwargs.pop('seq_id')\n        action = ET.SubElement(seq, \"action\")\n        action.text = kwargs.pop('action')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def disconnect_layer_listener(self):\n        \"\"\"Destroy the signal/slot to listen for layers loaded in QGIS.\n\n        ..seealso:: connect_layer_listener\n        \"\"\"\n", "input": "", "output": "        project = QgsProject.instance()\n        project.layersWillBeRemoved.disconnect(self.get_layers)\n        project.layersAdded.disconnect(self.get_layers)\n        project.layersRemoved.disconnect(self.get_layers)\n\n        self.iface.mapCanvas().layersChanged.disconnect(self.get_layers)\n        self.iface.currentLayerChanged.disconnect(self.layer_changed)", "category": "Python"}, {"instruction": "def attach_mock(self, mock, attribute):\n        \"\"\"\n        Attach a mock as an attribute of this one, replacing its name and\n        parent. Calls to the attached mock will be recorded in the\n        `method_calls` and `mock_calls` attributes of this one.\"\"\"\n", "input": "", "output": "        mock._mock_parent = None\n        mock._mock_new_parent = None\n        mock._mock_name = ''\n        mock._mock_new_name = None\n\n        setattr(self, attribute, mock)", "category": "Python"}, {"instruction": "def list_zones(profile):\n    '''\n    List zones for the given profile\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_dns.list_zones profile1\n    '''\n", "input": "", "output": "    conn = _get_driver(profile=profile)\n    return [_simple_zone(zone) for zone in conn.list_zones()]", "category": "Python"}, {"instruction": "def cachedstr(self):\n        \"\"\"Returns the full string of the file contents from the cache for\n        the file that we are currently providing intellisense for.\"\"\"\n", "input": "", "output": "        if self._cachedstr is None:\n            if self.module is not None:\n                refstring = self.module.refstring\n                self._cachedstr = refstring.splitlines()\n            else:\n                self._cachedstr = []\n\n        return self._cachedstr", "category": "Python"}, {"instruction": "def isPositiveStrand(self):\n    \"\"\"\n    Check if this genomic region is on the positive strand.\n\n    :return: True if this element is on the positive strand\n    \"\"\"\n", "input": "", "output": "    if self.strand is None and self.DEFAULT_STRAND == self.POSITIVE_STRAND:\n      return True\n    return self.strand == self.POSITIVE_STRAND", "category": "Python"}, {"instruction": "def remove_completer(self):\n        \"\"\"\n        Removes current completer.\n\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        if self.__completer:\n            LOGGER.debug(\"> Removing '{0}' completer.\".format(self.__completer))\n            # Signals / Slots.\n            self.__completer.activated.disconnect(self.__insert_completion)\n\n            self.__completer.deleteLater()\n            self.__completer = None\n        return True", "category": "Python"}, {"instruction": "def _cho_solve(c_and_lower, b, check_finite=True):\n    \"\"\"Implementaton of :func:`scipy.linalg.cho_solve` using\n    a function supported in cupy.\"\"\"\n", "input": "", "output": "\n    L = c_and_lower[0]\n    y = cpxl.solve_triangular(L, b, trans=0, lower=True,\n                              check_finite=check_finite)\n    return cpxl.solve_triangular(L, y, trans=1, lower=True,\n                                 check_finite=check_finite)", "category": "Python"}, {"instruction": "def elasprepFD(self):\r\n    \"\"\"\r\n    dx4, D = elasprepFD(dx,Te,E=1E11,nu=0.25)\r\n    \r\n    Defines the variables (except for the subset flexural rigidity) that are\r\n    needed to run \"coeff_matrix_1d\"\r\n    \"\"\"\n", "input": "", "output": "    self.dx4 = self.dx**4\r\n    self.D = self.E*self.Te**3/(12*(1-self.nu**2))", "category": "Python"}, {"instruction": "def _get_v23_frame(self, **kwargs):\n        \"\"\"Returns a frame copy which is suitable for writing into a v2.3 tag.\n\n        kwargs get passed to the specs.\n        \"\"\"\n", "input": "", "output": "\n        new_kwargs = {}\n        for checker in self._framespec:\n            name = checker.name\n            value = getattr(self, name)\n            new_kwargs[name] = checker._validate23(self, value, **kwargs)\n\n        for checker in self._optionalspec:\n            name = checker.name\n            if hasattr(self, name):\n                value = getattr(self, name)\n                new_kwargs[name] = checker._validate23(self, value, **kwargs)\n\n        return type(self)(**new_kwargs)", "category": "Python"}, {"instruction": "def _SkipLengthDelimited(buffer, pos, end):\n  \"\"\"Skip a length-delimited value.  Returns the new position.\"\"\"\n", "input": "", "output": "\n  (size, pos) = _DecodeVarint(buffer, pos)\n  pos += size\n  if pos > end:\n    raise _DecodeError('Truncated message.')\n  return pos", "category": "Python"}, {"instruction": "def from_chord_shorthand(self, shorthand):\n        \"\"\"Empty the container and add the notes in the shorthand.\n\n        See mingus.core.chords.from_shorthand for an up to date list of\n        recognized format.\n\n        Example:\n        >>> NoteContainer().from_chord_shorthand('Am')\n        ['A-4', 'C-5', 'E-5']\n        \"\"\"\n", "input": "", "output": "        self.empty()\n        self.add_notes(chords.from_shorthand(shorthand))\n        return self", "category": "Python"}, {"instruction": "def connect(self, interface, event, object_path, handler):\n        \"\"\"\n        Connect to a DBus signal. If ``object_path`` is None, subscribe for\n        all objects and invoke the callback with the object_path as its first\n        argument.\n        \"\"\"\n", "input": "", "output": "        if object_path:\n            def callback(connection, sender_name, object_path,\n                         interface_name, signal_name, parameters):\n                return handler(*unpack_variant(parameters))\n        else:\n            def callback(connection, sender_name, object_path,\n                         interface_name, signal_name, parameters):\n                return handler(object_path, *unpack_variant(parameters))\n        return self.connection.signal_subscribe(\n            self.bus_name,\n            interface,\n            event,\n            object_path,\n            None,\n            Gio.DBusSignalFlags.NONE,\n            callback,\n        )", "category": "Python"}, {"instruction": "def bam2fastq(bamfile, univ_options, picard_options):\n    \"\"\"\n    Split an input bam to paired fastqs.\n\n    :param str bamfile: Path to a bam file\n    :param dict univ_options: Dict of universal options used by almost all tools\n    :param dict picard_options: Dict of options specific to Picard\n    :return: Path to the _1.fastq file\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    work_dir = os.path.split(bamfile)[0]\n    base_name = os.path.split(os.path.splitext(bamfile)[0])[1]\n    parameters = ['SamToFastq',\n                  ''.join(['I=', docker_path(bamfile)]),\n                  ''.join(['F=/data/', base_name, '_1.fastq']),\n                  ''.join(['F2=/data/', base_name, '_2.fastq']),\n                  ''.join(['FU=/data/', base_name, '_UP.fastq'])]\n    docker_call(tool='picard', tool_parameters=parameters, work_dir=work_dir,\n                dockerhub=univ_options['dockerhub'], java_xmx=univ_options['java_Xmx'],\n                tool_version=picard_options['version'])\n    first_fastq = ''.join([work_dir, '/', base_name, '_1.fastq'])\n    assert os.path.exists(first_fastq)\n    return first_fastq", "category": "Python"}, {"instruction": "def has_no_jumps(neuron, max_distance=30.0, axis='z'):\n    '''Check if there are jumps (large movements in the `axis`)\n\n    Arguments:\n        neuron(Neuron): The neuron object to test\n        max_distance(float): value above which consecutive z-values are\n        considered a jump\n        axis(str): one of x/y/z, which axis to check for jumps\n\n    Returns:\n        CheckResult with result list of ids of bad sections\n    '''\n", "input": "", "output": "    bad_ids = []\n    axis = {'x': COLS.X, 'y': COLS.Y, 'z': COLS.Z, }[axis.lower()]\n    for neurite in iter_neurites(neuron):\n        section_segment = ((sec, seg) for sec in iter_sections(neurite)\n                           for seg in iter_segments(sec))\n        for sec, (p0, p1) in islice(section_segment, 1, None):  # Skip neurite root segment\n            if max_distance < abs(p0[axis] - p1[axis]):\n                bad_ids.append((sec.id, [p0, p1]))\n    return CheckResult(len(bad_ids) == 0, bad_ids)", "category": "Python"}, {"instruction": "def remove_line(self, section, line):\n        \"\"\"Remove all instances of a line.\n\n        Returns:\n            int: the number of lines removed\n        \"\"\"\n", "input": "", "output": "        try:\n            s = self._get_section(section, create=False)\n        except KeyError:\n            # No such section, skip.\n            return 0\n\n        return s.remove(line)", "category": "Python"}, {"instruction": "def _data_update(subjects, queue, run_flag):\n        \"\"\"\n        Get data from backgound process and notify all subscribed observers with the new data\n        \"\"\"\n", "input": "", "output": "        while run_flag.running:\n            while not queue.empty():\n                data = queue.get()\n                for subject in [s for s in subjects if not s.is_disposed]:\n                    subject.on_next(data)\n            time.sleep(0.1)", "category": "Python"}, {"instruction": "def extract(self, destination):\n        \"\"\"Extracts the contents of the archive to the specifed directory.\n\n        Args:\n            destination (str):\n                Path to an empty directory to extract the files to.\n        \"\"\"\n", "input": "", "output": "\n        if os.path.exists(destination):\n            raise OSError(20, 'Destination exists', destination)\n\n        self.__extract_directory(\n            '.',\n            self.files['files'],\n            destination\n        )", "category": "Python"}, {"instruction": "def write_gct(df, file_path):\n    \"\"\"\n    Writes the provided DataFrame to a GCT file.\n\n    Assumes that the DataFrame matches the structure of those produced\n    by the GCT() function in this library\n\n    :param df:\n    :param file_path:\n    :return:\n    \"\"\"\n", "input": "", "output": "    with open(file_path, 'w') as file:\n        file.write('#1.2\\n' + str(len(df.index)) + '\\t' + str(len(df.columns)) + '\\n')\n        df.to_csv(file, sep='\\t', mode='w+')", "category": "Python"}, {"instruction": "def zlexcount(self, name, min, max):\n        \"\"\"\n        Return the number of items in the sorted set ``name`` between the\n        lexicographical range ``min`` and ``max``.\n        \"\"\"\n", "input": "", "output": "        return self.execute_command('ZLEXCOUNT', name, min, max)", "category": "Python"}, {"instruction": "def xtime(b, n):\n    \"\"\"Repeated polynomial multiplication in GF(2^8).\"\"\"\n", "input": "", "output": "    b = b.reshape(8)\n    for _ in range(n):\n        b = exprzeros(1) + b[:7] ^ uint2exprs(0x1b, 8) & b[7]*8\n    return b", "category": "Python"}, {"instruction": "def getNumberOfRequiredVerifications(self):\n        \"\"\"Returns the number of required verifications a test for this\n        analysis requires before being transitioned to 'verified' state\n        :returns: number of required verifications\n        \"\"\"\n", "input": "", "output": "        num = self.getField('NumberOfRequiredVerifications').get(self)\n        if num < 1:\n            return self.bika_setup.getNumberOfRequiredVerifications()\n        return num", "category": "Python"}, {"instruction": "def GE(classical_reg1, classical_reg2, classical_reg3):\n    \"\"\"\n    Produce an GE instruction.\n\n    :param classical_reg1: Memory address to which to store the comparison result.\n    :param classical_reg2: Left comparison operand.\n    :param classical_reg3: Right comparison operand.\n    :return: A ClassicalGreaterEqual instance.\n    \"\"\"\n", "input": "", "output": "    classical_reg1, classical_reg2, classical_reg3 = prepare_ternary_operands(classical_reg1,\n                                                                              classical_reg2,\n                                                                              classical_reg3)\n    return ClassicalGreaterEqual(classical_reg1, classical_reg2, classical_reg3)", "category": "Python"}, {"instruction": "def cli(env, columns):\n    \"\"\"List Users.\"\"\"\n", "input": "", "output": "\n    mgr = SoftLayer.UserManager(env.client)\n    users = mgr.list_users()\n\n    table = formatting.Table(columns.columns)\n    for user in users:\n        table.add_row([value or formatting.blank()\n                       for value in columns.row(user)])\n\n    env.fout(table)", "category": "Python"}, {"instruction": "def reset_network(message):\n    \"\"\"Resets the users network to make changes take effect\"\"\"\n", "input": "", "output": "    for command in settings.RESTART_NETWORK:\n        try:\n            subprocess.check_call(command)\n        except:\n            pass\n    print(message)", "category": "Python"}, {"instruction": "def event(self, event):\r\n        \"\"\"\r\n        Qt override.\r\n\r\n        This is needed to be able to intercept the Tab key press event.\r\n        \"\"\"\n", "input": "", "output": "        if event.type() == QEvent.KeyPress:\r\n            if (event.key() == Qt.Key_Tab or event.key() == Qt.Key_Space):\r\n                text = self.text()\r\n                cursor = self.cursorPosition()\r\n                # fix to include in \"undo/redo\" history\r\n                if cursor != 0 and text[cursor-1] == ' ':\r\n                    text = text[:cursor-1] + ROW_SEPARATOR + ' ' +\\\r\n                        text[cursor:]\r\n                else:\r\n                    text = text[:cursor] + ' ' + text[cursor:]\r\n                self.setCursorPosition(cursor)\r\n                self.setText(text)\r\n                self.setCursorPosition(cursor + 1)\r\n                return False\r\n        return QWidget.event(self, event)", "category": "Python"}, {"instruction": "def rename(client, old, new, force):\n    \"\"\"Rename the workflow named <old> to <new>.\"\"\"\n", "input": "", "output": "    from renku.models.refs import LinkReference\n    LinkReference(client=client, name=_ref(old)).rename(_ref(new), force=force)", "category": "Python"}, {"instruction": "def VarintEncode(value):\n  \"\"\"Convert an integer to a varint and write it using the write function.\"\"\"\n", "input": "", "output": "  result = b\"\"\n  if value < 0:\n    raise ValueError(\"Varint can not encode a negative number.\")\n\n  bits = value & 0x7f\n  value >>= 7\n\n  while value:\n    result += HIGH_CHR_MAP[bits]\n    bits = value & 0x7f\n    value >>= 7\n\n  result += CHR_MAP[bits]\n\n  return result", "category": "Python"}, {"instruction": "def get_properties(properties_file='raw.properties.json', env=None, region=None):\n    \"\"\"Get contents of _properties_file_ for the _env_.\n\n    Args:\n        properties_file (str): File name of `create-configs` JSON output.\n        env (str): Environment to read optionally.\n        region (str): Region to get specific configs for.\n\n    Returns:\n        dict: JSON loaded Application properties for _env_.\n        None: Given _env_ was not found in `create-configs` JSON output.\n\n    \"\"\"\n", "input": "", "output": "    with open(properties_file, 'rt') as file_handle:\n        properties = json.load(file_handle)\n\n    env_properties = properties.get(env, properties)\n    contents = env_properties.get(region, env_properties)\n    LOG.debug('Found properties for %s:\\n%s', env, contents)\n    return contents", "category": "Python"}, {"instruction": "def _HandleMetadataUpdate(\n      self, metadata_key='', recursive=True, wait=True, timeout=None,\n      retry=True):\n    \"\"\"Wait for a successful metadata response.\n\n    Args:\n      metadata_key: string, the metadata key to watch for changes.\n      recursive: bool, True if we should recursively watch for metadata changes.\n      wait: bool, True if we should wait for a metadata change.\n      timeout: int, timeout in seconds for returning metadata output.\n      retry: bool, True if we should retry on failure.\n\n    Returns:\n      json, the deserialized contents of the metadata server.\n    \"\"\"\n", "input": "", "output": "    exception = None\n    while True:\n      try:\n        return self._GetMetadataUpdate(\n            metadata_key=metadata_key, recursive=recursive, wait=wait,\n            timeout=timeout)\n      except (httpclient.HTTPException, socket.error, urlerror.URLError) as e:\n        if not isinstance(e, type(exception)):\n          exception = e\n          self.logger.error('GET request error retrieving metadata. %s.', e)\n        if retry:\n          continue\n        else:\n          break", "category": "Python"}, {"instruction": "def _parse(self):\n        \"\"\"\n        The function for parsing the JSON response to the vars dictionary.\n        \"\"\"\n", "input": "", "output": "\n        try:\n\n            self.vars['status'] = self.json['status']\n\n        except (KeyError, ValueError, TypeError):\n\n            pass\n\n        for v in ['remarks', 'notices']:\n\n            try:\n\n                self.vars[v] = self.summarize_notices(self.json[v])\n\n            except (KeyError, ValueError, TypeError):\n\n                pass\n\n        try:\n\n            self.vars['links'] = self.summarize_links(self.json['links'])\n\n        except (KeyError, ValueError, TypeError):\n\n            pass\n\n        try:\n\n            self.vars['events'] = self.summarize_events(self.json['events'])\n\n        except (KeyError, ValueError, TypeError):\n\n            pass", "category": "Python"}, {"instruction": "def downstream(self, node):\n        \"\"\" Returns a list of all nodes this node has edges towards.\n\n        Args:\n            node (str): The node whose downstream nodes you want to find.\n\n        Returns:\n            list: A list of nodes that are immediately downstream from the\n                  node.\n        \"\"\"\n", "input": "", "output": "        graph = self.graph\n        if node not in graph:\n            raise KeyError('node %s is not in graph' % node)\n        return list(graph[node])", "category": "Python"}, {"instruction": "def init_app(self, app,\n                 entry_point_group_mappings='invenio_search.mappings',\n                 entry_point_group_templates='invenio_search.templates',\n                 **kwargs):\n        \"\"\"Flask application initialization.\n\n        :param app: An instance of :class:`~flask.app.Flask`.\n        \"\"\"\n", "input": "", "output": "        self.init_config(app)\n\n        app.cli.add_command(index_cmd)\n\n        state = _SearchState(\n            app,\n            entry_point_group_mappings=entry_point_group_mappings,\n            entry_point_group_templates=entry_point_group_templates,\n            **kwargs\n        )\n        self._state = app.extensions['invenio-search'] = state", "category": "Python"}, {"instruction": "def fit_cluster(data, assignments, k, P_init, R_init, means):\n    \"\"\"\n    Fits NB/poisson params to a cluster.\n    \"\"\"\n", "input": "", "output": "    for c in range(k):\n        if data[:,assignments==c].shape[1] == 0:\n            _, assignments = kmeans_pp(data, k)\n    genes, cells = data.shape\n    nb_gene_indices = np.array([True for i in range(genes)])\n    for c in range(k):\n        c_data = data[:,assignments==c]\n        nb_gene_indices = nb_gene_indices & find_nb_genes(c_data)\n    for c in range(k):\n        c_data = data[:,assignments==c]\n        nb_genes = c_data[nb_gene_indices,:]\n        poisson_genes = c_data[~nb_gene_indices, :]\n        P_init[nb_gene_indices, c], R_init[nb_gene_indices, c] = nb_fit(nb_genes)\n        means[~nb_gene_indices, c] = poisson_genes.mean(1)\n    return nb_gene_indices", "category": "Python"}, {"instruction": "def _execute_callback(self, callback, data):\n        \"\"\"Execute the callback in another thread. Wait for and return the results.\"\"\"\n", "input": "", "output": "        web_client = WebClient(\n            token=self.token, base_url=self.base_url, ssl=self.ssl, proxy=self.proxy\n        )\n        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n            # Execute the callback on a separate thread,\n            future = executor.submit(\n                callback, rtm_client=self, web_client=web_client, data=data\n            )\n\n            while future.running():\n                pass\n\n            future.result()", "category": "Python"}, {"instruction": "def union(self, other):\n        \"\"\"produce a 'union' of this dict and another (at the key level).\n\n        values in the second dict take precedence over that of the first\"\"\"\n", "input": "", "output": "        x = SetLikeDict(**self)\n        x.update(other)\n        return x", "category": "Python"}, {"instruction": "def is_correct(self):\n        \"\"\"Check if this object configuration is correct ::\n\n        * Check if dateranges of timeperiod are valid\n        * Call our parent class is_correct checker\n\n        :return: True if the configuration is correct, otherwise False if at least one daterange\n        is not correct\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        state = True\n        for daterange in self.dateranges:\n            good = daterange.is_correct()\n            if not good:\n                self.add_error(\"[timeperiod::%s] invalid daterange '%s'\"\n                               % (self.get_name(), daterange))\n            state &= good\n\n        # Warn about non correct entries\n        for entry in self.invalid_entries:\n            self.add_error(\"[timeperiod::%s] invalid entry '%s'\" % (self.get_name(), entry))\n\n        return super(Timeperiod, self).is_correct() and state", "category": "Python"}, {"instruction": "def compute_distances_dict(egg):\n    \"\"\" Creates a nested dict of distances \"\"\"\n", "input": "", "output": "    pres, rec, features, dist_funcs = parse_egg(egg)\n    pres_list = list(pres)\n    features_list = list(features)\n\n    # initialize dist dict\n    distances = {}\n\n    # for each word in the list\n    for idx1, item1 in enumerate(pres_list):\n\n        distances[item1]={}\n\n        # for each word in the list\n        for idx2, item2 in enumerate(pres_list):\n\n            distances[item1][item2]={}\n\n            # for each feature in dist_funcs\n            for feature in dist_funcs:\n                distances[item1][item2][feature] = builtin_dist_funcs[dist_funcs[feature]](features_list[idx1][feature],features_list[idx2][feature])\n\n    return distances", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"close\"\"\"\n", "input": "", "output": "        self.debug_log(\n            'close - start shutdown')\n        self.shutdown()\n        logging.Handler.close(self)\n        self.debug_log(\n            'close - handler join on processes={}'.format(\n                len(self.processes)))\n        self.debug_log(\n            'close - done')", "category": "Python"}, {"instruction": "def assert_fingerprint(cert, fingerprint):\n    \"\"\"\n    Checks if given fingerprint matches the supplied certificate.\n\n    :param cert:\n        Certificate as bytes object.\n    :param fingerprint:\n        Fingerprint as string of hexdigits, can be interspersed by colons.\n    \"\"\"\n", "input": "", "output": "\n    fingerprint = fingerprint.replace(':', '').lower()\n    digest_length = len(fingerprint)\n    hashfunc = HASHFUNC_MAP.get(digest_length)\n    if not hashfunc:\n        raise SSLError(\n            'Fingerprint of invalid length: {0}'.format(fingerprint))\n\n    # We need encode() here for py32; works on py2 and p33.\n    fingerprint_bytes = unhexlify(fingerprint.encode())\n\n    cert_digest = hashfunc(cert).digest()\n\n    if not _const_compare_digest(cert_digest, fingerprint_bytes):\n        raise SSLError('Fingerprints did not match. Expected \"{0}\", got \"{1}\".'\n                       .format(fingerprint, hexlify(cert_digest)))", "category": "Python"}, {"instruction": "def hostname(self, value):\n        \"\"\"\n        The hostname where the log message was created.\n\n        Should be the first part of the hostname, or\n        an IP address. Should NOT be set to a fully\n        qualified domain name.\n\n        \"\"\"\n", "input": "", "output": "        if value is None:\n            value = socket.gethostname()\n        self._hostname = value", "category": "Python"}, {"instruction": "def uniquify(seq):\n    \"\"\"Return unique values in a list in the original order. See:  \\\n        http://www.peterbe.com/plog/uniqifiers-benchmark\n\n    Args:\n      seq (list): original list.\n\n    Returns:\n      list: list without duplicates preserving original order.\n\n    \"\"\"\n", "input": "", "output": "\n    seen = set()\n    seen_add = seen.add\n    return [x for x in seq if x not in seen and not seen_add(x)]", "category": "Python"}, {"instruction": "def logout(self):\n        \"\"\"\n            Logout from the remote server.\n        \"\"\"\n", "input": "", "output": "        self.client.write('exit\\r\\n')\n        self.client.read_all()\n        self.client.close()", "category": "Python"}, {"instruction": "def scatterplot_charges(self, title, csv_filename, analysis_set = ''):\n        '''Scatterplot by residue charge.'''\n", "input": "", "output": "\n        # Create CSV input\n        new_dataframe = self.dataframe.copy()\n        experimental_field = BenchmarkRun.get_analysis_set_fieldname('Experimental', analysis_set)\n        new_dataframe = new_dataframe[[experimental_field, 'Predicted', 'ResidueCharges']]\n        new_dataframe['Opacity'] = 0.4\n        new_dataframe = new_dataframe.dropna(subset = [experimental_field, 'Predicted'])\n        new_dataframe.to_csv(csv_filename, sep = ',', header = True)\n\n        plot_scale = ", "category": "Python"}, {"instruction": "def _handle_sigusr1(signum: int, frame: Any) -> None:\n        \"\"\"Print stacktrace.\"\"\"\n", "input": "", "output": "        print('=' * 70)\n        print(''.join(traceback.format_stack()))\n        print('-' * 70)", "category": "Python"}, {"instruction": "def get_random_number_generator_and_set_seed(seed=None):\n  \"\"\"Get a JAX random number generator and set random seed everywhere.\"\"\"\n", "input": "", "output": "  random.seed(seed)\n  # While python random accepts None as seed and uses time/os seed then,\n  # some other functions expect integers so we create one here.\n  if seed is None:\n    seed = random.randint(0, 2**31 - 1)\n  tf.set_random_seed(seed)\n  numpy.random.seed(seed)\n  return jax_random.get_prng(seed)", "category": "Python"}, {"instruction": "def _del_all_subscriptions(self, session):\n        \"\"\"\n        Delete all topic subscriptions for a given session\n        :param session:\n        :return:\n        \"\"\"\n", "input": "", "output": "        filter_queue = deque()\n        for topic in self._subscriptions:\n            if self._del_subscription(topic, session):\n                filter_queue.append(topic)\n        for topic in filter_queue:\n            if not self._subscriptions[topic]:\n                del self._subscriptions[topic]", "category": "Python"}, {"instruction": "def get_command(self, ctx, cmd_name):\n        \"\"\"Override to handle command aliases\"\"\"\n", "input": "", "output": "        rv = click.Group.get_command(self, ctx, cmd_name)\n        if rv is not None:\n            return rv\n        cmd_name = self.command_aliases.get(cmd_name, \"\")\n        return click.Group.get_command(self, ctx, cmd_name)", "category": "Python"}, {"instruction": "def globals_changed(self):\n        \"\"\"True if any Global has changed.\"\"\"\n", "input": "", "output": "        value = bool(lib.EnvGetGlobalsChanged(self._env))\n        lib.EnvSetGlobalsChanged(self._env, int(False))\n\n        return value", "category": "Python"}, {"instruction": "def convert_values(args_list):\n    \"\"\"convert_value in bulk.\n\n    :param args_list: list of value, source, target currency pairs\n\n    :return: map of converted values\n    \"\"\"\n", "input": "", "output": "    rate_map = get_rates(map(itemgetter(1, 2), args_list))\n    value_map = {}\n    for value, source, target in args_list:\n        args = (value, source, target)\n        if source == target:\n            value_map[args] = value\n        else:\n            value_map[args] = value * rate_map[(source, target)]\n\n    return value_map", "category": "Python"}, {"instruction": "def p_type_def_2(t):\n    \"\"\"type_def : ENUM ID enum_body SEMI\"\"\"\n", "input": "", "output": "    id = t[2]\n    body = t[3]\n    lineno = t.lineno(1)\n    sortno = t.lineno(4) + 0.5\n    if id_unique(id, 'enum', lineno):\n        name_dict[id] = enum_info(id, body, lineno, sortno)", "category": "Python"}, {"instruction": "def serialize(self):\n        \"\"\" Get the serialized Dynamo format for the update \"\"\"\n", "input": "", "output": "        if self.action == 'Create':\n            payload = self.extra['index'].schema()\n        else:\n            payload = {\n                'IndexName': self.index_name,\n            }\n            if self.action == 'Update':\n                payload['ProvisionedThroughput'] = \\\n                    self.extra['throughput'].schema()\n        return {\n            self.action: payload\n        }", "category": "Python"}, {"instruction": "def download_software_version(version=None, synch=False):\n    '''\n    Download software packages by version number.\n\n    Args:\n        version(str): The version of the PANOS file to download.\n\n        synch (bool): If true then the file will synch to the peer unit.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' panos.download_software_version 8.0.0\n        salt '*' panos.download_software_version 8.0.0 True\n\n    '''\n", "input": "", "output": "    if not version:\n        raise CommandExecutionError(\"Version option must not be none.\")\n\n    if not isinstance(synch, bool):\n        raise CommandExecutionError(\"Synch option must be boolean..\")\n\n    if synch is True:\n        query = {'type': 'op',\n                 'cmd': '<request><system><software><download>'\n                        '<version>{0}</version></download></software></system></request>'.format(version)}\n    else:\n        query = {'type': 'op',\n                 'cmd': '<request><system><software><download><sync-to-peer>yes</sync-to-peer>'\n                        '<version>{0}</version></download></software></system></request>'.format(version)}\n\n    return _get_job_results(query)", "category": "Python"}, {"instruction": "def remove(self, priority, observer, callble):\n        \"\"\"\n        Remove one observer, which had priority and callble.\n        \"\"\"\n", "input": "", "output": "        self.flush()\n        for i in range(len(self) - 1, -1, -1):\n            p,o,c = self[i]\n            if priority==p and observer==o and callble==c:\n                del self._poc[i]", "category": "Python"}, {"instruction": "def reply(self, reply_comment):\n        \"\"\"Reply to the Message.\n\n        Notes:\n            HTML can be inserted in the string and will be interpreted properly by Outlook.\n\n        Args:\n            reply_comment: String message to send with email.\n\n        \"\"\"\n", "input": "", "output": "        payload = '{ \"Comment\": \"' + reply_comment + '\"}'\n        endpoint = 'https://outlook.office.com/api/v2.0/me/messages/' + self.message_id + '/reply'\n\n        self._make_api_call('post', endpoint, data=payload)", "category": "Python"}, {"instruction": "def type(self):\n        \"\"\"\n        Read-only. A member of :ref:`MsoColorType`, one of RGB, THEME, or\n        AUTO, corresponding to the way this color is defined. Its value is\n        |None| if no color is applied at this level, which causes the\n        effective color to be inherited from the style hierarchy.\n        \"\"\"\n", "input": "", "output": "        color = self._color\n        if color is None:\n            return None\n        if color.themeColor is not None:\n            return MSO_COLOR_TYPE.THEME\n        if color.val == ST_HexColorAuto.AUTO:\n            return MSO_COLOR_TYPE.AUTO\n        return MSO_COLOR_TYPE.RGB", "category": "Python"}, {"instruction": "def clean_entry(entry):\n    \"\"\"Consolidate some entry attributes and rename a few others.\n\n    Consolidate address attributes into a single field and replace some field\n    names with others as described in FIELD_TRANSFORM_INDEX.\n\n    @param entry: The entry attributes to update.\n    @type entry: dict\n    @return: Entry copy after running clean operations\n    @rtype: dict\n    \"\"\"\n", "input": "", "output": "    newEntry = {}\n\n    if 'Address1' in entry:\n        address = str(entry['Address1'])\n        if entry['Address2'] != '':\n            address = address + ' ' + str(entry['Address2'])\n        newEntry['address'] = address\n        del entry['Address1']\n        del entry['Address2']\n\n    for key in entry.keys():\n\n        if key != None:\n        \n            if key in FIELD_TRANSFORM_INDEX:\n                newKey = FIELD_TRANSFORM_INDEX[key]\n            else:\n                newKey = key\n            \n            newKey = newKey[:1].lower() + newKey[1:]\n            newEntry[newKey] = entry[key]\n\n    return newEntry", "category": "Python"}, {"instruction": "def has_sample(self, md5):\n        \"\"\"Checks if data store has this sample.\n\n        Args:\n            md5: The md5 digest of the required sample.\n\n        Returns:\n            True if sample with this md5 is present, else False.\n        \"\"\"\n", "input": "", "output": "\n        # The easiest thing is to simply get the sample and if that\n        # succeeds than return True, else return False\n        sample = self.get_sample(md5)\n        return True if sample else False", "category": "Python"}, {"instruction": "def endpoint(url):\n    \"\"\"Display / set / update the ECS endpoint URL.\"\"\"\n", "input": "", "output": "    if not url:\n        click.secho(dtool_config.utils.get_ecs_endpoint(CONFIG_PATH))\n    else:\n        click.secho(dtool_config.utils.set_ecs_endpoint(CONFIG_PATH, url))", "category": "Python"}, {"instruction": "def update(self):\n        \"\"\"Update processes stats using the input method.\"\"\"\n", "input": "", "output": "        # Init new stats\n        stats = self.get_init_value()\n\n        if self.input_method == 'local':\n            # Update stats using the standard system lib\n            # Note: Update is done in the processcount plugin\n            # Just return the processes list\n            stats = glances_processes.getlist()\n\n        elif self.input_method == 'snmp':\n            # No SNMP grab for processes\n            pass\n\n        # Update the stats\n        self.stats = stats\n\n        # Get the max values (dict)\n        # Use Deep copy to avoid change between update and display\n        self.max_values = copy.deepcopy(glances_processes.max_values())\n\n        return self.stats", "category": "Python"}, {"instruction": "def revert_cnf(node):\n    \"\"\"Reverts a parse tree (RuleNode) to its original non-CNF form (Node).\"\"\"\n", "input": "", "output": "    if isinstance(node, T):\n        return node\n    # Reverts TERM rule.\n    if node.rule.lhs.name.startswith('__T_'):\n        return node.children[0]\n    else:\n        children = []\n        for child in map(revert_cnf, node.children):\n            # Reverts BIN rule.\n            if isinstance(child, RuleNode) and child.rule.lhs.name.startswith('__SP_'):\n                children += child.children\n            else:\n                children.append(child)\n        # Reverts UNIT rule.\n        if isinstance(node.rule, UnitSkipRule):\n            return unroll_unit_skiprule(node.rule.lhs, node.rule.rhs,\n                                    node.rule.skipped_rules, children,\n                                    node.rule.weight, node.rule.alias)\n        else:\n            return RuleNode(node.rule, children)", "category": "Python"}, {"instruction": "def recursive_delete(self, path, **kwargs):\n        \"\"\"Recursively delete path.\"\"\"\n", "input": "", "output": "\n        while True:\n            try:\n                yield self.delete(path, **kwargs)\n            except NoNodeException:\n                break\n            except NotEmptyException:\n                children = yield self.get_children(path)\n                for name in children:\n                    yield self.recursive_delete(path + \"/\" + name)\n            else:\n                break", "category": "Python"}, {"instruction": "def delete(self, instance, session):\r\n        '''delete an *instance*'''\n", "input": "", "output": "        if instance._meta.type == 'structure':\r\n            return self._delete_structure(instance)\r\n        inst = self.pop(instance)\r\n        instance = inst if inst is not None else instance\r\n        if instance is not None:\r\n            state = instance.get_state()\r\n            if state.persistent:\r\n                state.deleted = True\r\n                self._deleted[state.iid] = instance\r\n                instance.session = session\r\n            else:\r\n                instance.session = None\r\n            return instance", "category": "Python"}, {"instruction": "def filter(self, query: str) -> Optional['CompatNodeIterator']:\n        \"\"\"\n        Further filter the results using this iterator as base.\n        \"\"\"\n", "input": "", "output": "        if not self._last_node:\n            return None\n\n        return filter(self._last_node, query)", "category": "Python"}, {"instruction": "def _allocate_ids_async(cls, size=None, max=None, parent=None,\n                          **ctx_options):\n    \"\"\"Allocates a range of key IDs for this model class.\n\n    This is the asynchronous version of Model._allocate_ids().\n    \"\"\"\n", "input": "", "output": "    from . import tasklets\n    ctx = tasklets.get_context()\n    cls._pre_allocate_ids_hook(size, max, parent)\n    key = Key(cls._get_kind(), None, parent=parent)\n    fut = ctx.allocate_ids(key, size=size, max=max, **ctx_options)\n    post_hook = cls._post_allocate_ids_hook\n    if not cls._is_default_hook(Model._default_post_allocate_ids_hook,\n                                post_hook):\n      fut.add_immediate_callback(post_hook, size, max, parent, fut)\n    return fut", "category": "Python"}, {"instruction": "def pop_events(self):\n    '''\n    Pop all events and return a `collections.deque` object. The\n    returned container can be empty. This method is preferred over\n    `pop_event()` as it is much faster as the lock has to be acquired\n    only once and also avoids running into an infinite loop during\n    event processing.\n    '''\n", "input": "", "output": "\n    with self.lock:\n      events = self.events\n      self.events = collections.deque()\n      return events", "category": "Python"}, {"instruction": "def on_response(self, request, response, **kwargs):\n        # type: (Request, Response, Any) -> None\n        \"\"\"Extract data from the body of a REST response object.\n\n        This will load the entire payload in memory.\n\n        Will follow Content-Type to parse.\n        We assume everything is UTF8 (BOM acceptable).\n\n        :param raw_data: Data to be processed.\n        :param content_type: How to parse if raw_data is a string/bytes.\n        :raises JSONDecodeError: If JSON is requested and parsing is impossible.\n        :raises UnicodeDecodeError: If bytes is not UTF8\n        :raises xml.etree.ElementTree.ParseError: If bytes is not valid XML\n        \"\"\"\n", "input": "", "output": "        # If response was asked as stream, do NOT read anything and quit now\n        if kwargs.get(\"stream\", True):\n            return\n\n        http_response = response.http_response\n\n        response.context[self.CONTEXT_NAME] = self.deserialize_from_http_generics(\n            http_response.text(),\n            http_response.headers\n        )", "category": "Python"}, {"instruction": "def dag_longest_path(graph, source, target):\n    \"\"\"\n    Finds the longest path in a dag between two nodes\n    \"\"\"\n", "input": "", "output": "    if source == target:\n        return [source]\n    allpaths = nx.all_simple_paths(graph, source, target)\n    longest_path = []\n    for l in allpaths:\n        if len(l) > len(longest_path):\n            longest_path = l\n    return longest_path", "category": "Python"}, {"instruction": "def _unsetLearningMode(self):\n    \"\"\"\n    Unsets the learning mode, to start inference.\n    \"\"\"\n", "input": "", "output": "    for region in self.TMRegions:\n      region.setParameter(\"learn\", False)\n    super(L4TMExperiment, self)._unsetLearningMode()", "category": "Python"}, {"instruction": "def get(self, name_or_klass):\n        \"\"\"\n        Gets a mode by name (or class)\n\n        :param name_or_klass: The name or the class of the mode to get\n        :type name_or_klass: str or type\n        :rtype: pyqode.core.api.Mode\n        \"\"\"\n", "input": "", "output": "        if not isinstance(name_or_klass, str):\n            name_or_klass = name_or_klass.__name__\n        return self._modes[name_or_klass]", "category": "Python"}, {"instruction": "def answered_by(self, rec):\n        \"\"\"Returns true if the question is answered by the record\"\"\"\n", "input": "", "output": "        return self.clazz == rec.clazz and \\\n                (self.type == rec.type or self.type == _TYPE_ANY) and \\\n                self.name == rec.name", "category": "Python"}, {"instruction": "def deactivate(self):\n        \"\"\"Deactivates the plugin.\n\n        The plugin may override the method to do specific\n        plugin deactivation handling.\n        Note: if overriden do not forget to call the\n              base class deactivate()\n        \"\"\"\n", "input": "", "output": "        self.ide.project.sigProjectChanged.disconnect(self.__collectGarbage)\n        self.ide.editorsManager.sigTabClosed.disconnect(self.__collectGarbage)\n\n        WizardInterface.deactivate(self)", "category": "Python"}, {"instruction": "def resources(self,\n                  start=1,\n                  num=10):\n        \"\"\"\n        Resources lists all file resources for the organization. The start\n        and num paging parameters are supported.\n\n        Inputs:\n           start - the number of the first entry in the result set response\n                   The index number is 1-based and the default is 1\n           num - the maximum number of results to be returned as a whole #\n        \"\"\"\n", "input": "", "output": "        url = self._url + \"/resources\"\n        params = {\n            \"f\" : \"json\",\n            \"start\" : start,\n            \"num\" : num\n        }\n        return self._get(url=url,\n                            param_dict=params,\n                            securityHandler=self._securityHandler,\n                            proxy_url=self._proxy_url,\n                            proxy_port=self._proxy_port)", "category": "Python"}, {"instruction": "def get_http_header(self) -> Response:\n        '''Return the HTTP header.\n\n        It only attempts to read the first 4 KiB of the payload.\n\n        Returns:\n            Response, None: Returns an instance of\n            :class:`.http.request.Response` or None.\n        '''\n", "input": "", "output": "        with wpull.util.reset_file_offset(self.block_file):\n            data = self.block_file.read(4096)\n\n        match = re.match(br'(.*?\\r?\\n\\r?\\n)', data)\n\n        if not match:\n            return\n\n        status_line, dummy, field_str = match.group(1).partition(b'\\n')\n\n        try:\n            version, code, reason = Response.parse_status_line(status_line)\n        except ValueError:\n            return\n\n        response = Response(status_code=code, reason=reason, version=version)\n\n        try:\n            response.fields.parse(field_str, strict=False)\n        except ValueError:\n            return\n\n        return response", "category": "Python"}, {"instruction": "def delete(self, redis):\n        ''' Deletes this field's value from the databse. Should be implemented\n        in special cases '''\n", "input": "", "output": "        value = getattr(self.obj, self.name)\n        redis.srem(self.key() + ':' + value, self.obj.id)", "category": "Python"}, {"instruction": "def _convert_todo(p_todo):\n    \"\"\" Converts a Todo instance to a dictionary. \"\"\"\n", "input": "", "output": "    creation_date = p_todo.creation_date()\n    completion_date = p_todo.completion_date()\n\n    result = {\n        'source': p_todo.source(),\n        'text': p_todo.text(),\n        'priority': p_todo.priority(),\n        'completed': p_todo.is_completed(),\n        'tags': p_todo.tags(),\n        'projects': list(p_todo.projects()),\n        'contexts': list(p_todo.contexts()),\n        'creation_date':\n            creation_date.isoformat() if creation_date else None,\n        'completion_date':\n            completion_date.isoformat() if completion_date else None\n    }\n\n    return result", "category": "Python"}, {"instruction": "def getsavefilename(parent=None, caption='', basedir='', filters='',\r\n                    selectedfilter='', options=None):\r\n    \"\"\"Wrapper around QtGui.QFileDialog.getSaveFileName static method\r\n    Returns a tuple (filename, selectedfilter) -- when dialog box is canceled,\r\n    returns a tuple of empty strings\r\n    Compatible with PyQt >=v4.4 (API #1 and #2) and PySide >=v1.0\"\"\"\n", "input": "", "output": "    return _qfiledialog_wrapper('getSaveFileName', parent=parent,\r\n                                caption=caption, basedir=basedir,\r\n                                filters=filters, selectedfilter=selectedfilter,\r\n                                options=options)", "category": "Python"}, {"instruction": "def name(self):\n        '''\n        Since other types of events (PublicEvents, class Series, etc.) are subclasses\n        of this class, it is a good idea to override this method for those subclasses,\n        to provide a more intuitive name.  However, defining this property at the\n        event level ensures that <object>.name can always be used to access a readable\n        name for describing the event.\n        '''\n", "input": "", "output": "        if self.startTime:\n            return _('Event, begins %s' % (self.startTime.strftime('%a., %B %d, %Y, %I:%M %p')))\n        else:\n            return _('Event #%s' % (self.id))", "category": "Python"}, {"instruction": "def forwards(apps, schema_editor):\n    \"\"\"\n    Create initial recurrence rules.\n    \"\"\"\n", "input": "", "output": "    RecurrenceRule = apps.get_model('icekit_events', 'RecurrenceRule')\n    for description, recurrence_rule in RULES:\n        RecurrenceRule.objects.get_or_create(\n            description=description,\n            defaults=dict(recurrence_rule=recurrence_rule),\n        )", "category": "Python"}, {"instruction": "def get_xy_1D(ds, stride=1, getval=False):\n    \"\"\"Return 1D arrays of x and y map coordinates for input GDAL Dataset \n    \"\"\"\n", "input": "", "output": "    gt = ds.GetGeoTransform()\n    #stride = stride_m/gt[1]\n    pX = np.arange(0, ds.RasterXSize, stride)\n    pY = np.arange(0, ds.RasterYSize, stride)\n    mX, dummy = pixelToMap(pX, pY[0], gt)\n    dummy, mY = pixelToMap(pX[0], pY, gt)\n    return mX, mY", "category": "Python"}, {"instruction": "def common(self, other):\n\t\t'''\n\t\t\tFind the shared part of two multipliers. This is the largest multiplier\n\t\t\twhich can be safely subtracted from both the originals. This may\n\t\t\treturn the \"zero\" multiplier.\n\t\t'''\n", "input": "", "output": "\t\tmandatory = min(self.mandatory, other.mandatory)\n\t\toptional = min(self.optional, other.optional)\n\t\treturn multiplier(mandatory, mandatory + optional)", "category": "Python"}, {"instruction": "def cmd_cameraview(self, args):\n        '''camera view commands'''\n", "input": "", "output": "        state = self\n        if args and args[0] == 'set':\n            if len(args) < 3:\n                state.view_settings.show_all()\n            else:\n                state.view_settings.set(args[1], args[2])\n                state.update_col()\n        else:\n            print('usage: cameraview set')", "category": "Python"}, {"instruction": "def name_to_system_object(self, name):\n        \"\"\"\n            Give SystemObject instance corresponding to the name\n        \"\"\"\n", "input": "", "output": "        if isinstance(name, str):\n            if self.allow_name_referencing:\n                name = name\n            else:\n                raise NameError('System.allow_name_referencing is set to False, cannot convert string to name')\n        elif isinstance(name, Object):\n            name = str(name)\n        return self.namespace.get(name, None)", "category": "Python"}, {"instruction": "def getHouses(date, pos, hsys):\n    \"\"\" Returns the lists of houses and angles.\n    \n    Since houses and angles are computed at the\n    same time, this function should be fast.\n    \n    \"\"\"\n", "input": "", "output": "    houses, angles = eph.getHouses(date.jd, pos.lat, pos.lon, hsys)\n    hList = [House.fromDict(house) for house in houses]\n    aList = [GenericObject.fromDict(angle) for angle in angles]\n    return (HouseList(hList), GenericList(aList))", "category": "Python"}, {"instruction": "def definition_rst(self, definition, spec_path=None):\n        '''\n        Prepare and write information about definition\n        :param definition: --name of definition that would be prepared for render\n        :type definition: str, unicode\n        :param spec_path: --path to definitions\n        :type spec_path: str, unicode\n        :return:\n        '''\n", "input": "", "output": "        spec_path = spec_path or self.models_path\n        definitions = self.spec[spec_path]\n        definition_property = definitions[definition]['properties'].copy()\n        if not definition_property:\n            self.write('{}', self.indent_depth)\n            return\n        self.indent_depth += 1\n        definition_property = self.find_nested_models(definition_property, definitions)\n        json_str = json.dumps(definition_property, indent=4)\n        for line in json_str.split('\\n'):\n            self.write(line, self.indent_depth)\n        self.indent_depth -= 1", "category": "Python"}, {"instruction": "def contains_all(self, other):\n        \"\"\"Return ``True`` if ``other`` is a sequence of integers.\"\"\"\n", "input": "", "output": "        dtype = getattr(other, 'dtype', None)\n        if dtype is None:\n            dtype = np.result_type(*other)\n        return is_int_dtype(dtype)", "category": "Python"}, {"instruction": "def periodic_ping(self) -> None:\n        \"\"\"Send a ping to keep the websocket alive\n\n        Called periodically if the websocket_ping_interval is set and non-zero.\n        \"\"\"\n", "input": "", "output": "        if self.is_closing() and self.ping_callback is not None:\n            self.ping_callback.stop()\n            return\n\n        # Check for timeout on pong. Make sure that we really have\n        # sent a recent ping in case the machine with both server and\n        # client has been suspended since the last ping.\n        now = IOLoop.current().time()\n        since_last_pong = now - self.last_pong\n        since_last_ping = now - self.last_ping\n        assert self.ping_interval is not None\n        assert self.ping_timeout is not None\n        if (\n            since_last_ping < 2 * self.ping_interval\n            and since_last_pong > self.ping_timeout\n        ):\n            self.close()\n            return\n\n        self.write_ping(b\"\")\n        self.last_ping = now", "category": "Python"}, {"instruction": "def filter_trends(self, pattern=''):\r\n        \"\"\"\r\n        Filter available trends\r\n        \"\"\"\n", "input": "", "output": "        filtered_trends = {}\r\n        with open(self.abspath) as fobj:\r\n            for idx, line in enumerate(fobj):\r\n                variable_idx = idx-self._attributes['CATALOG']-1\r\n                if 'TIME SERIES' in line:\r\n                    break\r\n                if pattern in line and variable_idx > 0:\r\n                    filtered_trends[variable_idx] = line\r\n        return filtered_trends", "category": "Python"}, {"instruction": "def _exception_raise(self):\n        \"\"\"\n        Raises a pending exception that was recorded while getting a\n        Task ready for execution.\n        \"\"\"\n", "input": "", "output": "        exc = self.exc_info()[:]\n        try:\n            exc_type, exc_value, exc_traceback = exc\n        except ValueError:\n            exc_type, exc_value = exc\n            exc_traceback = None\n\n        # raise exc_type(exc_value).with_traceback(exc_traceback)\n        if sys.version_info[0] == 2:\n            exec(\"raise exc_type, exc_value, exc_traceback\")\n        else: #  sys.version_info[0] == 3:\n            if isinstance(exc_value, Exception): #hasattr(exc_value, 'with_traceback'):\n                # If exc_value is an exception, then just reraise\n                exec(\"raise exc_value.with_traceback(exc_traceback)\")\n            else:\n                # else we'll create an exception using the value and raise that\n                exec(\"raise exc_type(exc_value).with_traceback(exc_traceback)\")", "category": "Python"}, {"instruction": "def p2wsh_input_and_witness(outpoint, stack, witness_script, sequence=None):\n    '''\n    Outpoint, str, str, int -> (TxIn, InputWitness)\n    Create a signed witness TxIn and InputWitness from a p2wsh prevout\n    '''\n", "input": "", "output": "    if sequence is None:\n        sequence = guess_sequence(witness_script)\n    stack = list(map(\n        lambda x: b'' if x == 'NONE' else bytes.fromhex(x), stack.split()))\n    stack.append(script_ser.serialize(witness_script))\n    return tb.make_witness_input_and_witness(outpoint, sequence, stack)", "category": "Python"}, {"instruction": "def identify_request(request: RequestType):\n    \"\"\"Try to identify whether this is a Diaspora request.\n\n    Try first public message. Then private message. The check if this is a legacy payload.\n    \"\"\"\n", "input": "", "output": "    # Private encrypted JSON payload\n    try:\n        data = json.loads(decode_if_bytes(request.body))\n        if \"encrypted_magic_envelope\" in data:\n            return True\n    except Exception:\n        pass\n    # Public XML payload\n    try:\n        xml = etree.fromstring(encode_if_text(request.body))\n        if xml.tag == MAGIC_ENV_TAG:\n            return True\n    except Exception:\n        pass\n    return False", "category": "Python"}, {"instruction": "def get_experiments(redis, active=True):\n    \"\"\"Gets the full list of experiments\"\"\"\n", "input": "", "output": "\n    key = ACTIVE_EXPERIMENTS_REDIS_KEY if active else ARCHIVED_EXPERIMENTS_REDIS_KEY\n    return [Experiment(redis, escape.to_unicode(name)) for name in redis.smembers(key)]", "category": "Python"}, {"instruction": "def _get_flowcell_id(in_file, require_single=True):\n    \"\"\"Retrieve the unique flowcell id represented in the SampleSheet.\n    \"\"\"\n", "input": "", "output": "    fc_ids = set([x[0] for x in _read_input_csv(in_file)])\n    if require_single and len(fc_ids) > 1:\n        raise ValueError(\"There are several FCIDs in the same samplesheet file: %s\" % in_file)\n    else:\n        return fc_ids", "category": "Python"}, {"instruction": "def pad_chunk_columns(chunk):\n    \"\"\"Given a set of items to be inserted, make sure they all have the\n    same columns by padding columns with None if they are missing.\"\"\"\n", "input": "", "output": "    columns = set()\n    for record in chunk:\n        columns.update(record.keys())\n    for record in chunk:\n        for column in columns:\n            record.setdefault(column, None)\n    return chunk", "category": "Python"}, {"instruction": "def cancel(self):\n        \"\"\"Cancel an EighthScheduledActivity.\n\n        This does nothing besides set the cancelled flag and save the\n        object.\n\n        \"\"\"\n", "input": "", "output": "        # super(EighthScheduledActivity, self).save(*args, **kwargs)\n\n        logger.debug(\"Running cancel hooks: {}\".format(self))\n\n        if not self.cancelled:\n            logger.debug(\"Cancelling {}\".format(self))\n            self.cancelled = True\n        self.save()\n        # NOT USED. Was broken anyway.\n        ", "category": "Python"}, {"instruction": "def numRegisteredForRoleName(event, roleName):\r\n    '''\r\n    This tag allows one to access the number of registrations\r\n    for any dance role using only the role's name.\r\n    '''\n", "input": "", "output": "    if not isinstance(event, Event):\r\n        return None\r\n\r\n    try:\r\n        role = DanceRole.objects.get(name=roleName)\r\n    except ObjectDoesNotExist:\r\n        return None\r\n\r\n    return event.numRegisteredForRole(role)", "category": "Python"}, {"instruction": "def draw(self, layout, coord):\n        \"\"\"\n        Draw geom\n\n        Parameters\n        ----------\n        layout : Layout\n            Layout object created when the plot is getting\n            built\n        coord : coord\n            Type of coordinate axes\n        \"\"\"\n", "input": "", "output": "        params = copy(self.geom.params)\n        params.update(self.stat.params)\n        params['zorder'] = self.zorder\n        self.data = self.geom.handle_na(self.data)\n        # At this point each layer must have the data\n        # that is created by the plot build process\n        self.geom.draw_layer(self.data, layout, coord, **params)", "category": "Python"}, {"instruction": "def allowed_values(self):\n        \"\"\"A tuple containing the allowed values for this Slot.\n\n        The Python equivalent of the CLIPS slot-allowed-values function.\n\n        \"\"\"\n", "input": "", "output": "        data = clips.data.DataObject(self._env)\n\n        lib.EnvDeftemplateSlotAllowedValues(\n            self._env, self._tpl, self._name, data.byref)\n\n        return tuple(data.value) if isinstance(data.value, list) else ()", "category": "Python"}, {"instruction": "def mode(self, values, weights=None):\n        \"\"\"compute the mode within each group.\n\n        Parameters\n        ----------\n        values : array_like, [keys, ...]\n            values to compute the mode of per group\n        weights : array_like, [keys], float, optional\n            optional weight associated with each entry in values\n\n        Returns\n        -------\n        unique: ndarray, [groups]\n            unique keys\n        reduced : ndarray, [groups, ...]\n            value array, reduced over groups\n        \"\"\"\n", "input": "", "output": "        if weights is None:\n            unique, weights = npi.count((self.index.sorted_group_rank_per_key, values))\n        else:\n            unique, weights = npi.group_by((self.index.sorted_group_rank_per_key, values)).sum(weights)\n\n        x, bin = npi.group_by(unique[0]).argmax(weights)\n        return x, unique[1][bin]", "category": "Python"}, {"instruction": "def maybe_stream(s):\n    \"\"\"Ensure that the given argument is a stream.\"\"\"\n", "input": "", "output": "    if isinstance(s, Stream):\n        return s\n\n    if s is None:\n        stream = InMemStream()\n        stream.close()  # we don't intend to write anything\n        return stream\n\n    if isinstance(s, unicode):\n        s = s.encode('utf-8')\n    if isinstance(s, bytearray):\n        s = bytes(s)\n\n    if isinstance(s, bytes):\n        stream = InMemStream(s)\n        stream.close()  # we don't intend to write anything\n        return stream\n\n    # s may still conform to the Stream interface. Yay duck typing.\n    return s", "category": "Python"}, {"instruction": "def note2snd(pitch, quarters):\n  \"\"\"\n  Creates an audio Stream object for a single note.\n\n  Parameters\n  ----------\n  pitch :\n    Pitch note like ``\"A4\"``, as a string, or ``None`` for a rest.\n  quarters :\n    Duration in quarters (see ``quarter_dur``).\n  \"\"\"\n", "input": "", "output": "  dur = quarters * quarter_dur\n  if pitch is None:\n    return zeros(dur)\n  freq = str2freq(pitch) * Hz\n  return synth(freq, dur)", "category": "Python"}, {"instruction": "def create(self, data):\n        \"\"\"\n        Create a new SyncListItemInstance\n\n        :param dict data: The data\n\n        :returns: Newly created SyncListItemInstance\n        :rtype: twilio.rest.preview.sync.service.sync_list.sync_list_item.SyncListItemInstance\n        \"\"\"\n", "input": "", "output": "        data = values.of({'Data': serialize.object(data), })\n\n        payload = self._version.create(\n            'POST',\n            self._uri,\n            data=data,\n        )\n\n        return SyncListItemInstance(\n            self._version,\n            payload,\n            service_sid=self._solution['service_sid'],\n            list_sid=self._solution['list_sid'],\n        )", "category": "Python"}, {"instruction": "def to_bool(text):\n    '''\n    Convert the string name of a boolean to that boolean value.\n    '''\n", "input": "", "output": "    downcased_text = six.text_type(text).strip().lower()\n\n    if downcased_text == 'false':\n        return False\n    elif downcased_text == 'true':\n        return True\n    return text", "category": "Python"}, {"instruction": "def compare_hexdigests( digest1, digest2 ):\n    \"\"\"Compute difference in bits between digest1 and digest2\n       returns -127 to 128; 128 is the same, -127 is different\"\"\"\n", "input": "", "output": "    # convert to 32-tuple of unsighed two-byte INTs\n    digest1 = tuple([int(digest1[i:i+2],16) for i in range(0,63,2)])\n    digest2 = tuple([int(digest2[i:i+2],16) for i in range(0,63,2)])\n    bits = 0\n    for i in range(32):\n        bits += POPC[255 & digest1[i] ^ digest2[i]]\n    return 128 - bits", "category": "Python"}, {"instruction": "def get_text(item, tag, default=False):\n        \"\"\" returns text content of an xml tag \"\"\"\n", "input": "", "output": "        try:\n            xmlnode = item.getElementsByTagName(tag)[0].firstChild\n        except IndexError as e:\n            if default is not False:\n                return default\n            else:\n                raise IndexError(e)\n\n        if xmlnode is not None:\n            return unicode(xmlnode.nodeValue)\n        # empty tag\n        else:\n            return ''", "category": "Python"}, {"instruction": "def get_edges_with_citations(self, citations: Iterable[Citation]) -> List[Edge]:\n        \"\"\"Get edges with one of the given citations.\"\"\"\n", "input": "", "output": "        return self.session.query(Edge).join(Evidence).filter(Evidence.citation.in_(citations)).all()", "category": "Python"}, {"instruction": "def add_section(self, section_name: str) -> None:\n        \"\"\"Add a section to the :class:`SampleSheet`.\"\"\"\n", "input": "", "output": "        section_name = self._whitespace_re.sub('_', section_name)\n        self._sections.append(section_name)\n        setattr(self, section_name, Section())", "category": "Python"}, {"instruction": "def abort(self):\n        \"\"\"Abort the SBI (and associated PBs).\"\"\"\n", "input": "", "output": "        self.set_status('aborted')\n        DB.remove_from_list('{}:active'.format(self._type), self._id)\n        DB.append_to_list('{}:aborted'.format(self._type), self._id)\n        sbi_pb_ids = ast.literal_eval(\n            DB.get_hash_value(self._key, 'processing_block_ids'))\n\n        for pb_id in sbi_pb_ids:\n            pb = ProcessingBlock(pb_id)\n            pb.abort()", "category": "Python"}, {"instruction": "def _autobounds(self):\n        \"\"\"Simple calculation for bounds.\"\"\"\n", "input": "", "output": "        bounds = {}\n\n        def check(prop, compare, extreme, val):\n            opp = min if compare is max else max\n            bounds.setdefault(prop, val)\n            bounds[prop] = opp(compare(bounds[prop], val), extreme)\n\n        def bound_check(lat_lon):\n            lat, lon = lat_lon\n            check('max_lat', max, 90, lat)\n            check('min_lat', min, -90, lat)\n            check('max_lon', max, 180, lon)\n            check('min_lon', min, -180, lon)\n\n        lat_lons = [lat_lon for feature in self._features.values() for\n                    lat_lon in feature.lat_lons]\n        if not lat_lons:\n            lat_lons.append(self._default_lat_lon)\n        for lat_lon in lat_lons:\n            bound_check(lat_lon)\n\n        return bounds", "category": "Python"}, {"instruction": "def _get_longest_hit_at_qry_start(self, nucmer_hits):\n        '''Input: list of nucmer hits to the same query. Returns the longest hit to the start of the query, or None if there is no such hit'''\n", "input": "", "output": "        hits_at_start = [hit for hit in nucmer_hits if self._is_at_qry_start(hit)]\n        return self._get_longest_hit_by_ref_length(hits_at_start)", "category": "Python"}, {"instruction": "def pretty_format_row(self, row, filler=\" \", splitter=\"|\"):\n        \"\"\"Gets pretty-formatted row\n\n        :param row: List of data\n        :param filler: Fill empty columns with this char\n        :param splitter: Separate columns with this char\n        :return: Pretty formatted row\n        \"\"\"\n", "input": "", "output": "        return self.get_pretty_row(\n            row,\n            filler,\n            splitter\n        )", "category": "Python"}, {"instruction": "def _startMqtt(self):\n        \"\"\"\n        The client start method. Starts the thread for the MQTT Client\n        and publishes the connected message.\n        \"\"\"\n", "input": "", "output": "        LOGGER.info('Connecting to MQTT... {}:{}'.format(self._server, self._port))\n        try:\n            # self._mqttc.connect_async(str(self._server), int(self._port), 10)\n            self._mqttc.connect_async('{}'.format(self._server), int(self._port), 10)\n            self._mqttc.loop_forever()\n        except Exception as ex:\n            template = \"An exception of type {0} occurred. Arguments:\\n{1!r}\"\n            message = template.format(type(ex).__name__, ex.args)\n            LOGGER.error(\"MQTT Connection error: {}\".format(message), exc_info=True)", "category": "Python"}, {"instruction": "def _to_r(o, as_data=False, level=0):\n    \"\"\"Helper function to convert python data structures to R equivalents\n\n    TODO: a single model for transforming to r to handle\n    * function args\n    * lists as function args\n    \"\"\"\n", "input": "", "output": "    if o is None:\n        return \"NA\"\n    if isinstance(o, basestring):\n        return o\n    if hasattr(o, \"r\"):\n        # bridge to @property r on GGStatement(s)\n        return o.r\n    elif isinstance(o, bool):\n        return \"TRUE\" if o else \"FALSE\"\n    elif isinstance(o, (list, tuple)):\n        inner = \",\".join([_to_r(x, True, level+1) for x in o])\n        return \"c({})\".format(inner) if as_data else inner\n    elif isinstance(o, dict):\n        inner = \",\".join([\"{}={}\".format(k, _to_r(v, True, level+1))\n                         for k, v in sorted(o.iteritems(), key=lambda x: x[0])])\n        return \"list({})\".format(inner) if as_data else inner\n    return str(o)", "category": "Python"}, {"instruction": "def precompute(config, soups):\n  \"\"\"Return info we want to compute (and preserve) before we mutate things.\"\"\"\n", "input": "", "output": "  show_toc = config.get('show_toc', {})\n  page = {}\n  pantsrefs = precompute_pantsrefs(soups)\n  for p, soup in soups.items():\n    title = get_title(soup) or p\n    page[p] = PrecomputedPageInfo(title=title, show_toc=show_toc.get(p, True))\n  return Precomputed(page=page, pantsref=pantsrefs)", "category": "Python"}, {"instruction": "def print_summary_stats(counter):\n    \"\"\"\n    Prints summary statistics about which writer strategies were used,\n    and how much they were used.\n\n    :param counter: A list of lists. The first entry on the inner list is a\n        number count of how many times a WriterStrategy was used, and the\n        second entry is a string label describing the WriterStrategy\n    \"\"\"\n", "input": "", "output": "    sum = 0     # Sum of labeled WriterStrategy modifications\n    u_sum = 0   # Sum of unlabeled WriterStrategy modifications\n    for c in counter:\n        if c[1] is not None and c[0] > 0:\n            print(\"Total \" + c[1] + \" modified:\\t\" + str(c[0]))\n            sum += c[0]\n        else:\n            u_sum += c[0]\n    if u_sum > 0:\n        print(\"Unlabeled modifications: \\t\" + str(u_sum))\n    print(\"Total modifications: \\t\\t\" + str(sum + u_sum))", "category": "Python"}, {"instruction": "def f_cell_temp(poa_global, wind_speed, air_temp):\n    \"\"\"\n    Calculate cell temperature.\n\n    :param poa_global: plane of array global irradiance [W/m**2]\n    :param wind_speed: wind speed [m/s]\n    :param air_temp: ambient dry bulb air temperature [degC]\n    :return: cell temperature [degC]\n    \"\"\"\n", "input": "", "output": "    temps = pvlib.pvsystem.sapm_celltemp(poa_global, wind_speed, air_temp)\n    return temps['temp_cell'].values, temps['temp_module'].values", "category": "Python"}, {"instruction": "def connection_exists(self, from_obj, to_obj):\n        \"\"\"\n        Returns ``True`` if a connection between the given objects exists,\n        else ``False``.\n        \"\"\"\n", "input": "", "output": "        self._validate_ctypes(from_obj, to_obj)\n        return self.connections.filter(from_pk=from_obj.pk, to_pk=to_obj.pk).exists()", "category": "Python"}, {"instruction": "def optimize(self, angles0, target):\n        \"\"\"Calculate an optimum argument of an objective function.\"\"\"\n", "input": "", "output": "        def new_objective(angles):\n            a = angles - angles0\n            if isinstance(self.smooth_factor, (np.ndarray, list)):\n                if len(a) == len(self.smooth_factor):\n                    return (self.f(angles, target) +\n                            np.sum(self.smooth_factor * np.power(a, 2)))\n                else:\n                    raise ValueError('len(smooth_factor) != number of joints')\n            else:\n                return (self.f(angles, target) +\n                        self.smooth_factor * np.sum(np.power(a, 2)))\n\n        return scipy.optimize.minimize(\n            new_objective,\n            angles0,\n            **self.optimizer_opt).x", "category": "Python"}, {"instruction": "def members(self):\n        \"\"\"Returns a list of :class:`Member` that are currently inside this voice channel.\"\"\"\n", "input": "", "output": "        ret = []\n        for user_id, state in self.guild._voice_states.items():\n            if state.channel.id == self.id:\n                member = self.guild.get_member(user_id)\n                if member is not None:\n                    ret.append(member)\n        return ret", "category": "Python"}, {"instruction": "def _make_eval_func(self, tensors, session, feed_dict, fetches,\n                      callback=None):\n    \"\"\"Construct a function that evaluates a `Tensor` or list of `Tensor`s.\"\"\"\n", "input": "", "output": "    if not isinstance(tensors, list):\n      tensors = [tensors]\n    num_tensors = len(tensors)\n\n    def eval_func(x):\n      ", "category": "Python"}, {"instruction": "def add_user_to_group(self, username, group_name):\n        \"\"\"\n        Add given user to a group\n\n        :param username: str\n        :param group_name: str\n        :return: Current state of the group\n        \"\"\"\n", "input": "", "output": "        url = 'rest/api/2/group/user'\n        params = {'groupname': group_name}\n        data = {'name': username}\n\n        return self.post(url, params=params, data=data)", "category": "Python"}, {"instruction": "def _Load(self,location):\n\t\t\"\"\"Load all networks associated with the given location.\n\n\t\thttps://www.centurylinkcloud.com/api-docs/v2/#get-network-list#request\n\t\t\"\"\"\n", "input": "", "output": "\n\t\t# https://api.ctl.io/v2-experimental/networks/ALIAS/WA1\n\t\tfor network in clc.v2.API.Call('GET','/v2-experimental/networks/%s/%s' % (self.alias,location),{},session=self.session):\n\t\t\tself.networks.append(Network(id=network['id'],alias=self.alias,network_obj=network,session=self.session))", "category": "Python"}, {"instruction": "def nvalues(self):\n        \"\"\"Returns the number of values recorded on this single line. If the\n        number is variable, it returns -1.\"\"\"\n", "input": "", "output": "        if self._nvalues is None:\n            self._nvalues = 0\n            for val in self.values:\n                if type(val) == type(int):\n                    self._nvalues += val\n                else:\n                    self._nvalues = -1\n                    break\n\n        return self._nvalues", "category": "Python"}, {"instruction": "def cumulative_max(self):\n        \"\"\"\n        Return the cumulative maximum value of the elements in the SArray.\n\n        Returns an SArray where each element in the output corresponds to the\n        maximum value of all the elements preceding and including it. The\n        SArray is expected to be of numeric type (int, float).\n\n        Returns\n        -------\n        out : SArray[int, float]\n\n        Notes\n        -----\n         - Missing values are ignored while performing the cumulative\n           aggregate operation.\n\n        Examples\n        --------\n        >>> sa = SArray([1, 0, 3, 4, 2])\n        >>> sa.cumulative_max()\n        dtype: int\n        rows: 3\n        [1, 1, 3, 4, 4]\n        \"\"\"\n", "input": "", "output": "        from .. import extensions\n        agg_op = \"__builtin__cum_max__\"\n        return SArray(_proxy = self.__proxy__.builtin_cumulative_aggregate(agg_op))", "category": "Python"}, {"instruction": "def _add_slash(self, string=None):\n        \"\"\" if a string doesn't end in a '/' add one \"\"\"\n", "input": "", "output": "        if(not str.endswith(string, '/')):\n            return str.join('', [string, '/'])\n        return str(string)", "category": "Python"}, {"instruction": "def _parse_args(args: List[str]) -> ProjectRunConfig:\n    \"\"\"\n    Parses the given CLI arguments to get a run configuration.\n    :param args: CLI arguments\n    :return: run configuration derived from the given CLI arguments\n    \"\"\"\n", "input": "", "output": "    parser = argparse.ArgumentParser(prog=\"gitlab-get-variables\", description=\"Tool for getting a GitLab project's \"\n                                                                              \"build variables\")\n    add_common_arguments(parser, project=True)\n    arguments = parser.parse_args(args)\n    return ProjectRunConfig(project=arguments.project, url=arguments.url, token=arguments.token, debug=arguments.debug)", "category": "Python"}, {"instruction": "def read(self, offset, length):\n        \"\"\"\n        Return *length* bytes from this stream starting at *offset*.\n        \"\"\"\n", "input": "", "output": "        self._file.seek(offset)\n        return self._file.read(length)", "category": "Python"}, {"instruction": "def recCopyElement(oldelement):\n    \"\"\"Generates a copy of an xml element and recursively of all\n    child elements.\n\n    :param oldelement: an instance of lxml.etree._Element\n\n    :returns: a copy of the \"oldelement\"\n\n    .. warning::\n        doesn't copy ``.text`` or ``.tail`` of xml elements\n    \"\"\"\n", "input": "", "output": "    newelement = ETREE.Element(oldelement.tag, oldelement.attrib)\n    if len(oldelement.getchildren()) > 0:\n        for childelement in oldelement.getchildren():\n            newelement.append(recCopyElement(childelement))\n    return newelement", "category": "Python"}, {"instruction": "def add_missing_children(required_children, element_children):\n    \"\"\"Determine if there are elements not in the children\n    that need to be included as blank elements in the form.\n    \"\"\"\n", "input": "", "output": "    element_tags = [element.tag for element in element_children]\n    # Loop through the elements that should be in the form.\n    for contained_element in required_children:\n        # If the element doesn't exist in the form,\n        # add the element to the children.\n        if contained_element not in element_tags:\n            try:\n                added_child = PYUNTL_DISPATCH[contained_element](content='')\n            except:\n                added_child = PYUNTL_DISPATCH[contained_element]()\n            element_children.append(added_child)\n    return element_children", "category": "Python"}, {"instruction": "def receive_promise(self, msg):\n        '''\n        Returns an Accept messages if a quorum of Promise messages is achieved\n        '''\n", "input": "", "output": "        self.observe_proposal(msg.proposal_id)\n\n        if not self.leader and msg.proposal_id == self.proposal_id and msg.from_uid not in self.promises_received:\n\n            self.promises_received.add(msg.from_uid)\n\n            if self.highest_accepted_id is None or msg.last_accepted_id > self.highest_accepted_id:\n                self.highest_accepted_id = msg.last_accepted_id\n                if msg.last_accepted_value is not None:\n                    self.proposed_value = msg.last_accepted_value\n\n            if len(self.promises_received) == self.quorum_size:\n                self.leader = True\n\n                if self.proposed_value is not None:\n                    self.current_accept_msg = Accept(self.network_uid, self.proposal_id, self.proposed_value)\n                    return self.current_accept_msg", "category": "Python"}, {"instruction": "def is_prime(number):\n    \"\"\"\n    Function to test primality of a number. Function lifted from online\n    resource:\n        http://www.codeproject.com/Articles/691200/Primality-test-algorithms-Prime-test-The-fastest-w\n\n    This function is distributed under a separate licence:\n        This article, along with any associated source code and files, is \\\n        licensed under The Code Project Open License (CPOL)\n\n    :type number: int\n    :param number: Integer to test for primality\n\n    :returns: bool\n\n    >>> is_prime(4)\n    False\n    >>> is_prime(3)\n    True\n    \"\"\"\n", "input": "", "output": "    ''' if number != 1 '''\n    if number > 1:\n        ''' repeat the test few times '''\n        for time in range(3):\n            ''' Draw a RANDOM number in range of number ( Z_number )  '''\n            randomNumber = random.randint(2, number - 1)\n            ''' Test if a^(n-1) = 1 mod n '''\n            if pow(randomNumber, number - 1, number) != 1:\n                return False\n        return True\n    else:\n        ''' case number == 1 '''\n        return False", "category": "Python"}, {"instruction": "def _glsl_mix(controls=None):\n    \"\"\"Generate a GLSL template function from a given interpolation patterns\n    and control points.\"\"\"\n", "input": "", "output": "    assert (controls[0], controls[-1]) == (0., 1.)\n    ncolors = len(controls)\n    assert ncolors >= 2\n    if ncolors == 2:\n        s = \"    return mix($color_0, $color_1, t);\\n\"\n    else:\n        s = \"\"\n        for i in range(ncolors-1):\n            if i == 0:\n                ifs = 'if (t < %.6f)' % (controls[i+1])\n            elif i == (ncolors-2):\n                ifs = 'else'\n            else:\n                ifs = 'else if (t < %.6f)' % (controls[i+1])\n            adj_t = '(t - %s) / %s' % (controls[i],\n                                       controls[i+1] - controls[i])\n            s += (\"%s {\\n    return mix($color_%d, $color_%d, %s);\\n} \" %\n                  (ifs, i, i+1, adj_t))\n    return \"vec4 colormap(float t) {\\n%s\\n}\" % s", "category": "Python"}, {"instruction": "def GetRowCache(self, query):\n    \"\"\"Retrieves the row cache for a specific query.\n\n    The row cache is a set that contains hashes of values in a row. The row\n    cache is used to find duplicate row when a database and a database with\n    a WAL file is parsed.\n\n    Args:\n      query (str): query.\n\n    Returns:\n      set: hashes of the rows that have been parsed.\n    \"\"\"\n", "input": "", "output": "    query_hash = hash(query)\n    if query_hash not in self._row_caches:\n      self._row_caches[query_hash] = set()\n    return self._row_caches[query_hash]", "category": "Python"}, {"instruction": "def _mod_aggregate(self, low, running, chunks):\n        '''\n        Execute the aggregation systems to runtime modify the low chunk\n        '''\n", "input": "", "output": "        agg_opt = self.functions['config.option']('state_aggregate')\n        if 'aggregate' in low:\n            agg_opt = low['aggregate']\n        if agg_opt is True:\n            agg_opt = [low['state']]\n        elif not isinstance(agg_opt, list):\n            return low\n        if low['state'] in agg_opt and not low.get('__agg__'):\n            agg_fun = '{0}.mod_aggregate'.format(low['state'])\n            if agg_fun in self.states:\n                try:\n                    low = self.states[agg_fun](low, chunks, running)\n                    low['__agg__'] = True\n                except TypeError:\n                    log.error('Failed to execute aggregate for state %s', low['state'])\n        return low", "category": "Python"}, {"instruction": "def _pillar(self, load):\n        '''\n        Return the pillar data for the minion\n        '''\n", "input": "", "output": "        if any(key not in load for key in ('id', 'grains')):\n            return False\n#        pillar = salt.pillar.Pillar(\n        log.debug('Master _pillar using ext: %s', load.get('ext'))\n        pillar = salt.pillar.get_pillar(\n                self.opts,\n                load['grains'],\n                load['id'],\n                load.get('saltenv', load.get('env')),\n                load.get('ext'),\n                self.mminion.functions,\n                pillar_override=load.get('pillar_override', {}))\n        data = pillar.compile_pillar()\n        if self.opts.get('minion_data_cache', False):\n            self.cache.store('minions/{0}'.format(load['id']),\n                             'data',\n                             {'grains': load['grains'], 'pillar': data})\n            if self.opts.get('minion_data_cache_events') is True:\n                self.event.fire_event({'comment': 'Minion data cache refresh'}, salt.utils.event.tagify(load['id'], 'refresh', 'minion'))\n        return data", "category": "Python"}, {"instruction": "def load_configuration(yaml: yaml.ruamel.yaml.YAML, filename: str) -> DictLike:\n    \"\"\" Load an analysis configuration from a file.\n\n    Args:\n        yaml: YAML object to use in loading the configuration.\n        filename: Filename of the YAML configuration file.\n    Returns:\n        dict-like object containing the loaded configuration\n    \"\"\"\n", "input": "", "output": "    with open(filename, \"r\") as f:\n        config = yaml.load(f)\n\n    return config", "category": "Python"}, {"instruction": "def get_class_list(cls) -> DOMTokenList:\n        \"\"\"Get class-level class list, including all super class's.\"\"\"\n", "input": "", "output": "        cl = []\n        cl.append(DOMTokenList(cls, cls.class_))\n        if cls.inherit_class:\n            for base_cls in cls.__bases__:\n                if issubclass(base_cls, WdomElement):\n                    cl.append(base_cls.get_class_list())\n        # Reverse order so that parent's class comes to front  <- why?\n        cl.reverse()\n        return DOMTokenList(cls, *cl)", "category": "Python"}, {"instruction": "def add_node(self, node):\n        \"\"\"Link to the agent from a parent based on the parent's fitness\"\"\"\n", "input": "", "output": "        num_agents = len(self.nodes(type=Agent))\n        curr_generation = int((num_agents - 1) / float(self.generation_size))\n        node.generation = curr_generation\n\n        if curr_generation == 0 and self.initial_source:\n            parent = self._select_oldest_source()\n        else:\n            parent = self._select_fit_node_from_generation(\n                node_type=type(node), generation=curr_generation - 1\n            )\n\n        if parent is not None:\n            parent.connect(whom=node)\n            parent.transmit(to_whom=node)", "category": "Python"}, {"instruction": "def all(self, page=1, per_page=10, order_by=\"latest\"):\n        \"\"\"\n        Get a single page from the list of all photos.\n\n        :param page [integer]: Page number to retrieve. (Optional; default: 1)\n        :param per_page [integer]: Number of items per page. (Optional; default: 10)\n        :param order_by [string]: How to sort the photos. Optional.\n        (Valid values: latest, oldest, popular; default: latest)\n        :return: [Array]: A single page of the Photo list.\n        \"\"\"\n", "input": "", "output": "        return self._all(\"/photos\", page=page, per_page=per_page, order_by=order_by)", "category": "Python"}, {"instruction": "def dateheure(objet):\n        \"\"\" abstractRender d'une date-heure datetime.datetime au format JJ/MM/AAAA\u00e0HH:mm \"\"\"\n", "input": "", "output": "        if objet:\n            return \"{}/{}/{} \u00e0 {:02}:{:02}\".format(objet.day, objet.month, objet.year, objet.hour, objet.minute)\n        return \"\"", "category": "Python"}, {"instruction": "def class_register(cls):\n    \"\"\"Class decorator that allows to map LSP method names to class methods.\"\"\"\n", "input": "", "output": "    cls.handler_registry = {}\n    for method_name in dir(cls):\n        method = getattr(cls, method_name)\n        if hasattr(method, '_handle'):\n            cls.handler_registry.update({method._handle: method_name})\n    return cls", "category": "Python"}, {"instruction": "def debug(self):\n        \"\"\"Retrieve the debug information from the identity manager.\"\"\"\n", "input": "", "output": "        url = '{}debug/status'.format(self.url)\n        try:\n            return make_request(url, timeout=self.timeout)\n        except ServerError as err:\n            return {\"error\": str(err)}", "category": "Python"}, {"instruction": "def input(self):\n        \"\"\"\n        Return a list of all the aesthetics covered by\n        the scales.\n        \"\"\"\n", "input": "", "output": "        lst = [s.aesthetics for s in self]\n        return list(itertools.chain(*lst))", "category": "Python"}, {"instruction": "def flash(self, flash=True):\n        \"\"\"\n        Turn on or off flashing of the device's LED for physical\n        identification purposes.\n        \"\"\"\n", "input": "", "output": "        if flash:\n            action = canstat.kvLED_ACTION_ALL_LEDS_ON\n        else:\n            action = canstat.kvLED_ACTION_ALL_LEDS_OFF\n\n        try:\n            kvFlashLeds(self._read_handle, action, 30000)\n        except (CANLIBError, NotImplementedError) as e:\n            log.error('Could not flash LEDs (%s)', e)", "category": "Python"}, {"instruction": "def obj(extract=None, child_transform=None, transform=None):\n  \"\"\"Returns a partial of get_obj that only needs a source argument.\"\"\"\n", "input": "", "output": "  return lambda source: get_obj(source, extract, child_transform, transform)", "category": "Python"}, {"instruction": "def setContext(self, value = .5):\n        \"\"\"\n        Clears the context layer by setting context layer to (default) value 0.5. \n        \"\"\"\n", "input": "", "output": "        for context in list(self.contextLayers.values()):\n            context.resetFlags() # hidden activations have already been copied in\n            context.setActivations(value)", "category": "Python"}, {"instruction": "def spatial_slice_zeros(x):\n  \"\"\"Experimental summary that shows how many planes are unused for a batch.\"\"\"\n", "input": "", "output": "  return tf.cast(tf.reduce_all(tf.less_equal(x, 0.0), [0, 1, 2]),\n                 tf.float32)", "category": "Python"}, {"instruction": "def cart2sph(x, y, z):\n    \"\"\"\n    Converts cartesian coordinates `x`, `y`, `z` into a longitude and latitude.\n    x=0, y=0, z=0 is assumed to correspond to the center of the globe.\n    Returns lon and lat in radians.\n\n    Parameters\n    ----------\n    `x`, `y`, `z` : Arrays of cartesian coordinates\n\n    Returns\n    -------\n    lon : Longitude in radians\n    lat : Latitude in radians\n    \"\"\"\n", "input": "", "output": "    r = np.sqrt(x**2 + y**2 + z**2)\n    lat = np.arcsin(z/r)\n    lon = np.arctan2(y, x)\n    return lon, lat", "category": "Python"}, {"instruction": "def _assign_uid(self, sid):\n        \"\"\"\n        Purpose: Assign a uid to the current object based on the sid passed. Pass the current uid to children of\n        current object\n        \"\"\"\n", "input": "", "output": "        self._uid = ru.generate_id(\n            'pipeline.%(item_counter)04d', ru.ID_CUSTOM, namespace=sid)\n        for stage in self._stages:\n            stage._assign_uid(sid)\n\n        self._pass_uid()", "category": "Python"}, {"instruction": "def view(self, vleaf, fpath=None, cleanup=True, format=None):\n        \"\"\"View the graph.\n\n        Args:\n          vleaf (`nnabla.Variable`): End variable. All variables and functions which can be traversed from this variable are shown in the reuslt.\n          fpath (`str`): The file path used to save. \n          cleanup (`bool`): Clean up the source file after rendering. Default is True.\n          format (str):\n              Force overwrite ``format`` (``'pdf', 'png', ...)``) configuration.\n\n        \"\"\"\n", "input": "", "output": "        graph = self.create_graphviz_digraph(vleaf, format=format)\n        graph.view(fpath, cleanup=cleanup)", "category": "Python"}, {"instruction": "def send_exit_status(self, status):\n        \"\"\"\n        Send the exit status of an executed command to the client.  (This\n        really only makes sense in server mode.)  Many clients expect to\n        get some sort of status code back from an executed command after\n        it completes.\n        \n        @param status: the exit code of the process\n        @type status: int\n        \n        @since: 1.2\n        \"\"\"\n", "input": "", "output": "        # in many cases, the channel will not still be open here.\n        # that's fine.\n        m = Message()\n        m.add_byte(chr(MSG_CHANNEL_REQUEST))\n        m.add_int(self.remote_chanid)\n        m.add_string('exit-status')\n        m.add_boolean(False)\n        m.add_int(status)\n        self.transport._send_user_message(m)", "category": "Python"}, {"instruction": "def assets(self, asset_type=None):\n        \"\"\"\n        Retrieves all of the assets of a given asset_type\n\n        Args:\n            asset_type: (str) Either None, PHONE, HANDLER, or URL\n\n        Returns:\n\n        \"\"\"\n", "input": "", "output": "        if not self.can_update():\n            self._tcex.handle_error(910, [self.type])\n\n        if not asset_type:\n            return self.tc_requests.adversary_assets(\n                self.api_type, self.api_sub_type, self.unique_id\n            )\n        if asset_type == 'PHONE':\n            return self.tc_requests.adversary_phone_assets(\n                self.api_type, self.api_sub_type, self.unique_id\n            )\n        if asset_type == 'HANDLER':\n            return self.tc_requests.adversary_handle_assets(\n                self.api_type, self.api_sub_type, self.unique_id\n            )\n        if asset_type == 'URL':\n            return self.tc_requests.adversary_url_assets(\n                self.api_type, self.api_sub_type, self.unique_id\n            )\n\n        self._tcex.handle_error(\n            925, ['asset_type', 'assets', 'asset_type', 'asset_type', asset_type]\n        )\n        return None", "category": "Python"}, {"instruction": "def configure(self, transport, auth, address, port):\n        \"\"\"\n        Connect paramiko transport\n\n        :type auth: :py:class`margaritashotgun.auth.AuthMethods`\n        :param auth: authentication object\n        :type address: str\n        :param address: remote server ip or hostname\n        :type port: int\n        :param port: remote server port\n        :type hostkey: :py:class:`paramiko.key.HostKey`\n        :param hostkey: remote host ssh server key\n        \"\"\"\n", "input": "", "output": "\n        self.transport = transport\n        self.username = auth.username\n        self.address = address\n        self.port = port", "category": "Python"}, {"instruction": "def deploy(self, unique_id, configs=None):\n    \"\"\"Deploys the service to the host.  This should at least perform the same actions as install and start\n    but may perform additional tasks as needed.\n\n    :Parameter unique_id: the name of the process\n    :Parameter configs: a mao of configs the deployer may use to modify the deployment\n    \"\"\"\n", "input": "", "output": "    self.install(unique_id, configs)\n    self.start(unique_id, configs)", "category": "Python"}, {"instruction": "def assignIPAddresses(self, prefix=None):\n        '''\n        Assign IP addresses to all interfaces on hosts and routers in the\n        network.  \n        \n        NB: this method assumes that all interfaces are assigned\n        addresses on the same subnet.  If you don't want that behavior,\n        the setInterfaceAddresses method must be used.\n        '''\n", "input": "", "output": "        if not prefix:\n            subnet = ipaddress.IPv4Network('10.0.0.0/8')\n        else:\n            subnet = ipaddress.IPv4Network(str(prefix),strict=False)\n\n        ipgenerator = subnet.hosts()\n\n        # collect all links; figure out which ones need to be numbered (i.e., \n        # only interfaces connected to hosts and routers)\n        nodes_to_number = self.hosts + self.routers\n\n        for u,v in sorted(self.links):\n            linkdata = self.getLink(u,v)\n            for node in [u,v]:\n                if node in nodes_to_number:\n                    ifname = linkdata[node]\n                    intf = self.getNode(node)['nodeobj'].getInterface(ifname)\n                    intf.ipaddr = next(ipgenerator)\n                    intf.netmask = subnet.netmask", "category": "Python"}, {"instruction": "def Override(self, overrides):\n        \"\"\"\n        Produce a modified environment whose variables are overridden by\n        the overrides dictionaries.  \"overrides\" is a dictionary that\n        will override the variables of this environment.\n\n        This function is much more efficient than Clone() or creating\n        a new Environment because it doesn't copy the construction\n        environment dictionary, it just wraps the underlying construction\n        environment, and doesn't even create a wrapper object if there\n        are no overrides.\n        \"\"\"\n", "input": "", "output": "        if not overrides: return self\n        o = copy_non_reserved_keywords(overrides)\n        if not o: return self\n        overrides = {}\n        merges = None\n        for key, value in o.items():\n            if key == 'parse_flags':\n                merges = value\n            else:\n                overrides[key] = SCons.Subst.scons_subst_once(value, self, key)\n        env = OverrideEnvironment(self, overrides)\n        if merges: env.MergeFlags(merges)\n        return env", "category": "Python"}, {"instruction": "def add_applicator(self, table, cols, function):\n        \"\"\"\n        Add an applicator. When reading *table*, rows in *table* will be\n        modified by apply_rows().\n\n        Args:\n            table: The table to apply the function to.\n            cols: The columns in *table* to apply the function on.\n            function: The applicator function.\n        \"\"\"\n", "input": "", "output": "\n        if table not in self.relations:\n            raise ItsdbError('Cannot add applicator; table \"{}\" is not '\n                             'defined by the relations file.'\n                             .format(table))\n        if cols is None:\n            raise ItsdbError('Cannot add applicator; columns not specified.')\n        fields = set(f.name for f in self.relations[table])\n        for col in cols:\n            if col not in fields:\n                raise ItsdbError('Cannot add applicator; column \"{}\" not '\n                                 'defined by the relations file.'\n                                 .format(col))\n        self.applicators[table].append((cols, function))", "category": "Python"}, {"instruction": "def dirsplit(path):\n    r\"\"\"\n    Args:\n        path (str):\n\n    Returns:\n        list: components of the path\n\n    CommandLine:\n        python -m utool.util_path --exec-dirsplit\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> paths = []\n        >>> paths.append('E:/window file/foo')\n        >>> paths.append('/normal/foo')\n        >>> paths.append('~/relative/path')\n        >>> results = [dirsplit(path) for path in paths]\n        >>> import re\n        >>> results2 = [re.split('\\\\/', path) for path in paths]\n        >>> print(results2)\n        >>> result = ut.repr2(results)\n        >>> print(result)\n    \"\"\"\n", "input": "", "output": "    #return path.split(os.sep)\n    parts = []\n    remain = path\n    part = True\n    #while True:\n    while part != '' and remain != '':\n        remain, part = split(remain)\n        parts.append(part)\n    parts = [p for p in parts if p != '']\n    if remain != '':\n        parts.append(remain)\n    parts = parts[::-1]\n    return parts", "category": "Python"}, {"instruction": "def add_alert_tag(self, id, tag_value, **kwargs):  # noqa: E501\n        \"\"\"Add a tag to a specific alert  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.add_alert_tag(id, tag_value, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :param str tag_value: (required)\n        :return: ResponseContainer\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.add_alert_tag_with_http_info(id, tag_value, **kwargs)  # noqa: E501\n        else:\n            (data) = self.add_alert_tag_with_http_info(id, tag_value, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def uploads(self):\n        \"\"\"returns the class to perform the upload function.  it will\n        only return the uploads class if syncEnabled is True.\n        \"\"\"\n", "input": "", "output": "        if self.syncEnabled == True:\n            return Uploads(url=self._url + \"/uploads\",\n                           securityHandler=self._securityHandler,\n                           proxy_url=self._proxy_url,\n                           proxy_port=self._proxy_port)\n        return None", "category": "Python"}, {"instruction": "def relabel(self, label_map):\n        \"\"\"Returns a copy of the catalogue with the labels remapped according\n        to `label_map`.\n\n        `label_map` is a dictionary mapping existing labels to new\n        labels. Any existing label that is not given a mapping is\n        deleted from the resulting catalogue.\n\n        :param label_map: mapping of labels to new labels\n        :type label_map: `dict`\n        :rtype: `tacl.Catalogue`\n\n        \"\"\"\n", "input": "", "output": "        catalogue = copy.deepcopy(self)\n        to_delete = set()\n        for work, old_label in catalogue.items():\n            if old_label in label_map:\n                catalogue[work] = label_map[old_label]\n            else:\n                to_delete.add(catalogue[work])\n        for label in to_delete:\n            catalogue.remove_label(label)\n        return catalogue", "category": "Python"}, {"instruction": "def _get_horoscope(self, day='today'):\n        \"\"\"gets a horoscope from site html\n\n        :param day: day for which to get horoscope. Default is 'today'\n\n        :returns: dictionary of horoscope details\n        \"\"\"\n", "input": "", "output": "        if not is_valid_day(day):\n            raise HoroscopeException(\"Invalid day. Allowed days: [today|yesterday|tomorrow]\" )\n\n        horoscope = ''.join([str(s).strip() for s in self.tree.xpath('//*[@id=\"%s\"]/p/text()' % day)])\n\n        if day is 'yesterday':\n            date = self.date_today - timedelta(days=1)\n        elif day is 'today':\n            date = self.date_today\n        elif day is 'tomorrow':\n            date = self.date_today + timedelta(days=1)\n\n        return {\n            'date': date.strftime(\"%Y-%m-%d\"),\n            'sunsign': self.sunsign.capitalize(),\n            'horoscope': horoscope + \"(c) Kelli Fox, The Astrologer, http://new.theastrologer.com\",\n            'meta': self._get_horoscope_meta(day),\n            'credit': '(c) Kelli Fox, The Astrologer, http://new.theastrologer.com'\n        }", "category": "Python"}, {"instruction": "def send_to_delivery_stream(events, stream_name):\n    \"\"\"Sends a list of events to a Firehose delivery stream.\"\"\"\n", "input": "", "output": "    if not events:\n        logger.info(\"No events provided: nothing delivered to Firehose\")\n        return\n\n    records = []\n    for event in events:\n        if not isinstance(event, str):\n            # csv events already have a newline\n            event = json.dumps(event) + \"\\n\"\n        records.append({\"Data\": event})\n    firehose = boto3.client(\"firehose\")\n    logger.info(\"Delivering %s records to Firehose stream '%s'\",\n                len(records), stream_name)\n    resp = firehose.put_record_batch(\n        DeliveryStreamName=stream_name,\n        Records=records)\n    return resp", "category": "Python"}, {"instruction": "def _recv_sf(self, data):\n        \"\"\"Process a received 'Single Frame' frame\"\"\"\n", "input": "", "output": "        self.rx_timer.cancel()\n        if self.rx_state != ISOTP_IDLE:\n            warning(\"RX state was reset because single frame was received\")\n            self.rx_state = ISOTP_IDLE\n\n        length = six.indexbytes(data, 0) & 0xf\n        if len(data) - 1 < length:\n            return 1\n\n        msg = data[1:1 + length]\n        self.rx_queue.put(msg)\n        for cb in self.rx_callbacks:\n            cb(msg)\n        self.call_release()\n        return 0", "category": "Python"}, {"instruction": "def index_of_coincidence(*texts):\n    \"\"\"Calculate the index of coincidence for one or more ``texts``.\n    The results are averaged over multiple texts to return the delta index of coincidence.\n\n    Examples:\n        >>> index_of_coincidence(\"aabbc\")\n        0.2\n\n        >>> index_of_coincidence(\"aabbc\", \"abbcc\")\n        0.2\n\n    Args:\n        *texts (variable length argument list): The texts to analyze\n\n    Returns:\n        Decimal value of the index of coincidence\n\n    Raises:\n        ValueError: If texts is empty\n        ValueError: If any text is less that 2 character long\n    \"\"\"\n", "input": "", "output": "    if not texts:\n        raise ValueError(\"texts must not be empty\")\n\n    return statistics.mean(_calculate_index_of_coincidence(frequency_analyze(text), len(text)) for text in texts)", "category": "Python"}, {"instruction": "def _get_partial_string_timestamp_match_key(self, key, labels):\n        \"\"\"Translate any partial string timestamp matches in key, returning the\n        new key (GH 10331)\"\"\"\n", "input": "", "output": "        if isinstance(labels, MultiIndex):\n            if (isinstance(key, str) and labels.levels[0].is_all_dates):\n                # Convert key '2016-01-01' to\n                # ('2016-01-01'[, slice(None, None, None)]+)\n                key = tuple([key] + [slice(None)] * (len(labels.levels) - 1))\n\n            if isinstance(key, tuple):\n                # Convert (..., '2016-01-01', ...) in tuple to\n                # (..., slice('2016-01-01', '2016-01-01', None), ...)\n                new_key = []\n                for i, component in enumerate(key):\n                    if (isinstance(component, str) and\n                            labels.levels[i].is_all_dates):\n                        new_key.append(slice(component, component, None))\n                    else:\n                        new_key.append(component)\n                key = tuple(new_key)\n\n        return key", "category": "Python"}, {"instruction": "def get_crypt_class(self):\n        \"\"\"\n        Get the Keyczar class to use.\n\n        The class can be customized with the ENCRYPTED_FIELD_MODE setting. By default,\n        this setting is DECRYPT_AND_ENCRYPT. Set this to ENCRYPT to disable decryption.\n        This is necessary if you are only providing public keys to Keyczar.\n\n        Returns:\n            keyczar.Encrypter if ENCRYPTED_FIELD_MODE is ENCRYPT.\n            keyczar.Crypter if ENCRYPTED_FIELD_MODE is DECRYPT_AND_ENCRYPT.\n\n        Override this method to customize the type of Keyczar class returned.\n        \"\"\"\n", "input": "", "output": "        crypt_type = getattr(settings, 'ENCRYPTED_FIELD_MODE', 'DECRYPT_AND_ENCRYPT')\n        if crypt_type == 'ENCRYPT':\n            crypt_class_name = 'Encrypter'\n        elif crypt_type == 'DECRYPT_AND_ENCRYPT':\n            crypt_class_name = 'Crypter'\n        else:\n            raise ImproperlyConfigured(\n                'ENCRYPTED_FIELD_MODE must be either DECRYPT_AND_ENCRYPT '\n                'or ENCRYPT, not %s.' % crypt_type)\n        return getattr(keyczar, crypt_class_name)", "category": "Python"}, {"instruction": "def _(text, *args, **kwargs):\n    \"\"\"Translate and then and format the text with ``str.format``.\"\"\"\n", "input": "", "output": "    msg = _t.gettext(text)\n    if args or kwargs:\n        return msg.format(*args, **kwargs)\n    else:\n        return msg", "category": "Python"}, {"instruction": "def drive(self, event, *args):\n        \"\"\"\n        Used to dispatch events.\n        \"\"\"\n", "input": "", "output": "\n        maps = self.base.get(event, self.step)\n        for handle, data in maps[:]:\n            params = args + data\n            try:\n                handle(self, *params)\n            except Stop:\n                break\n            except StopIteration:\n                pass\n            except Kill as Root:\n                raise\n            except Erase:\n                maps.remove((handle, data))\n            except Exception as e:\n                debug(event, params)\n\n        for handle in self.pool:\n            handle(self, event, args)", "category": "Python"}, {"instruction": "def start():\r\n  r\"\"\"Starts ec.\r\n  \"\"\"\n", "input": "", "output": "  processPendingModules()\r\n\r\n  if not state.main_module_name in ModuleMembers: # don't start the core when main is not Ec-ed\r\n    return\r\n\r\n  MainModule = sys.modules[state.main_module_name]\r\n\r\n  if not MainModule.__ec_member__.Members: # there was some error while loading script(s)\r\n    return\r\n\r\n  global BaseGroup\r\n  BaseGroup = MainModule.__ec_member__\r\n\r\n  Argv = sys.argv[1:]\r\n  global mode\r\n  mode = 'd' if Argv else 's' # dispatch / shell mode\r\n\r\n  if mode == 's':\r\n    import shell\r\n    shell.init()\r\n\r\n  else:\r\n    import dispatch\r\n    dispatch.init(Argv)\r\n\r\n  processExitHooks()", "category": "Python"}, {"instruction": "def active_path(context, pattern, css=None):\n    \"\"\"\n    Highlight menu item based on path.\n    \n    Returns a css class if ``request.path`` is in given ``pattern``.\n    \n    :param pattern:\n        Regex url pattern.\n\n    :param css:\n        Css class to be returned for highlighting. Return active if none set.\n\n    \"\"\"\n", "input": "", "output": "    request = context['request']\n    #pattern = \"^\" + pattern + \"$\"\n    if re.search(pattern, request.path):\n        return css if css else 'active'\n    return ''", "category": "Python"}, {"instruction": "def apply_link_ref(offset: int, length: int, value: bytes, bytecode: bytes) -> bytes:\n    \"\"\"\n    Returns the new bytecode with `value` put into the location indicated by `offset` and `length`.\n    \"\"\"\n", "input": "", "output": "    try:\n        validate_empty_bytes(offset, length, bytecode)\n    except ValidationError:\n        raise BytecodeLinkingError(\"Link references cannot be applied to bytecode\")\n\n    new_bytes = (\n        # Ignore linting error b/c conflict b/w black & flake8\n        bytecode[:offset]\n        + value\n        + bytecode[offset + length :]  # noqa: E201, E203\n    )\n    return new_bytes", "category": "Python"}, {"instruction": "def rebuildtable(cls):\n        \"\"\"Regenerate the entire closuretree.\"\"\"\n", "input": "", "output": "        cls._closure_model.objects.all().delete()\n        cls._closure_model.objects.bulk_create([cls._closure_model(\n            parent_id=x['pk'],\n            child_id=x['pk'],\n            depth=0\n        ) for x in cls.objects.values(\"pk\")])\n        for node in cls.objects.all():\n            node._closure_createlink()", "category": "Python"}, {"instruction": "def send(self):\n        \"\"\"Send the response header, including the status line and all the\n        HTTP headers.\n        \"\"\"\n", "input": "", "output": "\n        if hasattr(self, 'request'):\n            self.request.responded = True\n        self.connection.writer.write(str(self).encode())\n        yield from self.connection.writer.drain()", "category": "Python"}, {"instruction": "def vm_detach(name, kwargs=None, call=None):\n    '''\n    Detaches a disk from a virtual machine.\n\n    .. versionadded:: 2016.3.0\n\n    name\n        The name of the VM from which to detach the disk.\n\n    disk_id\n        The ID of the disk to detach.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a vm_detach my-vm disk_id=1\n    '''\n", "input": "", "output": "    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The vm_detach action must be called with -a or --action.'\n        )\n\n    if kwargs is None:\n        kwargs = {}\n\n    disk_id = kwargs.get('disk_id', None)\n    if disk_id is None:\n        raise SaltCloudSystemExit(\n            'The vm_detach function requires a \\'disk_id\\' to be provided.'\n        )\n\n    server, user, password = _get_xml_rpc()\n    auth = ':'.join([user, password])\n    vm_id = int(get_vm_id(kwargs={'name': name}))\n    response = server.one.vm.detach(auth, vm_id, int(disk_id))\n\n    data = {\n        'action': 'vm.detach',\n        'detached': response[0],\n        'vm_id': response[1],\n        'error_code': response[2],\n    }\n\n    return data", "category": "Python"}, {"instruction": "def is_subtype (type, base):\n    \"\"\" Same as is_derived. Should be removed.\n    \"\"\"\n", "input": "", "output": "    assert isinstance(type, basestring)\n    assert isinstance(base, basestring)\n    # TODO: remove this method\n    return is_derived (type, base)", "category": "Python"}, {"instruction": "def fit(self, X, y, **fit_params):\n        \"\"\"Find the best parameters for a particular model.\n\n        Parameters\n        ----------\n        X, y : array-like\n        **fit_params\n            Additional partial fit keyword arguments for the estimator.\n        \"\"\"\n", "input": "", "output": "        return default_client().sync(self._fit, X, y, **fit_params)", "category": "Python"}, {"instruction": "def get_localizer(self):\n        \"\"\" get an empty/generic localizer matrix that can be filled\n\n        Returns\n        -------\n            localizer : pyemu.Matrix\n                matrix with nnz obs names for rows and adj par names for columns\n\n        \"\"\"\n", "input": "", "output": "        onames = self.pst.nnz_obs_names\n        pnames = self.pst.adj_par_names\n        localizer = Matrix(x=np.ones((len(onames),len(pnames))),row_names=onames,col_names=pnames)\n        return localizer", "category": "Python"}, {"instruction": "def list_vmss_vm_instance_view_pg(access_token, subscription_id, resource_group, vmss_name,\n                                  link=None):\n    '''Gets one page of a paginated list of scale set VM instance views.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n        resource_group (str): Azure resource group name.\n        vmss_name (str): Name of the virtual machine scale set.\n        link (str): Optional link to URI to get list (as part of a paginated API query).\n\n    Returns:\n        HTTP response. JSON body of list of VM instance views.\n    '''\n", "input": "", "output": "    if link is None:\n        endpoint = ''.join([get_rm_endpoint(),\n                            '/subscriptions/', subscription_id,\n                            '/resourceGroups/', resource_group,\n                            '/providers/Microsoft.Compute/virtualMachineScaleSets/', vmss_name,\n                            '/virtualMachines?$expand=instanceView&$select=instanceView',\n                            '&api-version=', COMP_API])\n    else:\n        endpoint = link\n    return do_get(endpoint, access_token)", "category": "Python"}, {"instruction": "def process_key_dict(self, key, d, level):\n        \"\"\"\n        Process key value dicts e.g. METADATA \"key\" \"value\"\n        \"\"\"\n", "input": "", "output": "\n        # add any composite level comments\n        comments = d.get(\"__comments__\", {})\n        lines = []\n        self._add_type_comment(level, comments, lines)\n\n        lines += [self.add_start_line(key, level)]\n        lines += self.process_dict(d, level, comments)\n        lines.append(self.add_end_line(level, 1, key))\n\n        return lines", "category": "Python"}, {"instruction": "def include_context(predicate, num, iterative):\n    \"\"\"\n    Return elements in `iterative` including `num` before and after elements.\n\n    >>> ''.join(include_context(lambda x: x == '!', 2, 'bb!aa__bb!aa'))\n    'bb!aabb!aa'\n\n    \"\"\"\n", "input": "", "output": "    (it0, it1, it2) = itertools.tee(iterative, 3)\n    psf = _forward_shifted_predicate(predicate, num, it1)\n    psb = _backward_shifted_predicate(predicate, num, it2)\n    return (e for (e, pf, pb) in zip(it0, psf, psb) if pf or pb)", "category": "Python"}, {"instruction": "def GetZipInfo(self):\n    \"\"\"Retrieves the ZIP info object.\n\n    Returns:\n      zipfile.ZipInfo: a ZIP info object or None if not available.\n\n    Raises:\n      PathSpecError: if the path specification is incorrect.\n    \"\"\"\n", "input": "", "output": "    if not self._zip_info:\n      location = getattr(self.path_spec, 'location', None)\n      if location is None:\n        raise errors.PathSpecError('Path specification missing location.')\n\n      if not location.startswith(self._file_system.LOCATION_ROOT):\n        raise errors.PathSpecError('Invalid location in path specification.')\n\n      if len(location) == 1:\n        return None\n\n      zip_file = self._file_system.GetZipFile()\n      try:\n        self._zip_info = zip_file.getinfo(location[1:])\n      except KeyError:\n        pass\n\n    return self._zip_info", "category": "Python"}, {"instruction": "async def log(\n            self,\n            date: datetime.date = None,\n            days: int = None,\n            details: bool = False) -> list:\n        \"\"\"Get watering information for X days from Y date.\"\"\"\n", "input": "", "output": "        endpoint = 'watering/log'\n        if details:\n            endpoint += '/details'\n\n        if date and days:\n            endpoint = '{0}/{1}/{2}'.format(\n                endpoint, date.strftime('%Y-%m-%d'), days)\n\n        data = await self._request('get', endpoint)\n        return data['waterLog']['days']", "category": "Python"}, {"instruction": "def codemirror_field_css_bundle(field):\n    \"\"\"\n    Filter to get CodeMirror CSS bundle name needed for a single field.\n\n    Example:\n        ::\n\n        {% load djangocodemirror_tags %}\n        {{ form.myfield|codemirror_field_css_bundle }}\n\n    Arguments:\n        field (djangocodemirror.fields.CodeMirrorField): A form field.\n\n    Raises:\n        CodeMirrorFieldBundleError: Raised if Codemirror configuration from\n        field does not have a bundle name.\n\n    Returns:\n        string: Bundle name to load with webassets.\n    \"\"\"\n", "input": "", "output": "    manifesto = CodemirrorAssetTagRender()\n    manifesto.register_from_fields(field)\n\n    try:\n        bundle_name = manifesto.css_bundle_names()[0]\n    except IndexError:\n        msg = (\"Given field with configuration name '{}' does not have a \"\n               \"Javascript bundle name\")\n        raise CodeMirrorFieldBundleError(msg.format(field.config_name))\n\n    return bundle_name", "category": "Python"}, {"instruction": "def _make_passage_kwargs(urn, reference):\n    \"\"\" Little helper used by CapitainsCtsPassage here to comply with parents args\n\n    :param urn: URN String\n    :param reference: Reference String\n    :return: Dictionary of arguments with URN based on identifier and reference\n    \"\"\"\n", "input": "", "output": "    kwargs = {}\n    if urn is not None:\n        if reference is not None:\n            kwargs[\"urn\"] = URN(\"{}:{}\".format(urn.upTo(URN.VERSION), reference))\n        else:\n            kwargs[\"urn\"] = urn\n    return kwargs", "category": "Python"}, {"instruction": "def to_dict(self, delimiter=DEFAULT_DELIMITER, dict_type=collections.OrderedDict):\n        \"\"\" Get the dictionary representation of the current parser.\n\n        :param str delimiter: The delimiter used for nested dictionaries,\n            defaults to \":\", optional\n        :param class dict_type: The dictionary type to use for building the dictionary\n            reperesentation, defaults to collections.OrderedDict, optional\n        :return: The dictionary representation of the parser instance\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "\n        root_key = self.sections()[0]\n        return self._build_dict(\n            self._sections, delimiter=delimiter, dict_type=dict_type\n        ).get(root_key, {})", "category": "Python"}, {"instruction": "def update_data(self, data):\n        \"\"\"Change the data set in the Somoclu object. It is useful when the\n        data is updated and the training should continue on the new data.\n\n        :param data: The training data.\n        :type data: 2D numpy.array of float32.\n        \"\"\"\n", "input": "", "output": "        oldn_dim = self.n_dim\n        if data.dtype != np.float32:\n            print(\"Warning: data was not float32. A 32-bit copy was made\")\n            self._data = np.float32(data)\n        else:\n            self._data = data\n        self.n_vectors, self.n_dim = data.shape\n        if self.n_dim != oldn_dim and oldn_dim != 0:\n            raise Exception(\"The dimension of the new data does not match!\")\n        self.bmus = np.zeros(self.n_vectors * 2, dtype=np.intc)", "category": "Python"}, {"instruction": "def discriminator1(ndf, no_bias=True, fix_gamma=True, eps=1e-5 + 1e-12):\n    '''First part of the discriminator which takes a 32x32 image as input\n    and output a convolutional feature map, this is required to calculate\n    the layer loss'''\n", "input": "", "output": "    BatchNorm = mx.sym.BatchNorm\n\n    data = mx.sym.Variable('data')\n\n    d1 = mx.sym.Convolution(data, name='d1', kernel=(5,5), stride=(2,2), pad=(2,2), num_filter=ndf, no_bias=no_bias)\n    dact1 = mx.sym.LeakyReLU(d1, name='dact1', act_type='leaky', slope=0.2)\n\n    d2 = mx.sym.Convolution(dact1, name='d2', kernel=(5,5), stride=(2,2), pad=(2,2), num_filter=ndf*2, no_bias=no_bias)\n    dbn2 = BatchNorm(d2, name='dbn2', fix_gamma=fix_gamma, eps=eps)\n    dact2 = mx.sym.LeakyReLU(dbn2, name='dact2', act_type='leaky', slope=0.2)\n\n    d3 = mx.sym.Convolution(dact2, name='d3', kernel=(5,5), stride=(2,2), pad=(2,2), num_filter=ndf*4, no_bias=no_bias)\n    dbn3 = BatchNorm(d3, name='dbn3', fix_gamma=fix_gamma, eps=eps)\n    dact3 = mx.sym.LeakyReLU(dbn3, name='dact3', act_type='leaky', slope=0.2)\n\n    return dact3", "category": "Python"}, {"instruction": "def wrap_exception(exception, cause):\n        \"\"\"\n        Wraps another exception into specified application exception object.\n\n        If original exception is of ApplicationException type it is returned without changes.\n        Otherwise the original error is set as a cause to specified ApplicationException object.\n\n        :param exception: an ApplicationException object to wrap the cause\n\n        :param cause: an original error object\n\n        :return: an original or newly created ApplicationException\n        \"\"\"\n", "input": "", "output": "        if isinstance(cause, ApplicationException):\n            return cause\n        \n        exception.with_cause(cause)\n        return exception", "category": "Python"}, {"instruction": "def send_mail_worker(config, mail, event):\n    \"\"\"Worker task to send out an email, which is a blocking process unless it is threaded\"\"\"\n", "input": "", "output": "    log = \"\"\n\n    try:\n        if config.get('ssl', True):\n            server = SMTP_SSL(config['server'], port=config['port'], timeout=30)\n        else:\n            server = SMTP(config['server'], port=config['port'], timeout=30)\n\n        if config['tls']:\n            log += 'Starting TLS\\n'\n            server.starttls()\n\n        if config['username'] != '':\n            log += 'Logging in with ' + str(config['username']) + \"\\n\"\n            server.login(config['username'], config['password'])\n        else:\n            log += 'No username, trying anonymous access\\n'\n\n        log += 'Sending Mail\\n'\n        response_send = server.send_message(mail)\n        server.quit()\n\n    except timeout as e:\n        log += 'Could not send email: ' + str(e) + \"\\n\"\n        return False, log, event\n\n    log += 'Server response:' + str(response_send)\n    return True, log, event", "category": "Python"}, {"instruction": "def read_csv_as_integer(csv_name, integer_columns, usecols=None):\n    \"\"\"Returns a DataFrame from a .csv file stored in /data/raw/.\n    Converts columns specified by 'integer_columns' to integer.\n    \"\"\"\n", "input": "", "output": "    csv_path = os.path.join(DATA_FOLDER, csv_name)\n    csv = pd.read_csv(csv_path, low_memory=False, usecols=usecols)\n    for column in integer_columns:\n        csv = csv[pd.to_numeric(csv[column], errors=\"coerce\").notnull()]\n    csv[integer_columns] = csv[integer_columns].apply(pd.to_numeric)\n    return csv", "category": "Python"}, {"instruction": "def _save_artifact(build, data, content_type):\n    \"\"\"Saves an artifact to the DB and returns it.\"\"\"\n", "input": "", "output": "    sha1sum = hashlib.sha1(data).hexdigest()\n    artifact = models.Artifact.query.filter_by(id=sha1sum).first()\n\n    if artifact:\n      logging.debug('Upload already exists: artifact_id=%r', sha1sum)\n    else:\n      logging.info('Upload received: artifact_id=%r, content_type=%r',\n                   sha1sum, content_type)\n      artifact = models.Artifact(\n          id=sha1sum,\n          content_type=content_type,\n          data=data)\n      _artifact_created(artifact)\n\n    artifact.owners.append(build)\n    return artifact", "category": "Python"}, {"instruction": "def _repr_html_(self):\n        \"\"\"\n        Return a html representation for a particular DataFrame.\n\n        Mainly for IPython notebook.\n        \"\"\"\n", "input": "", "output": "        if self._info_repr():\n            buf = StringIO(\"\")\n            self.info(buf=buf)\n            # need to escape the <class>, should be the first line.\n            val = buf.getvalue().replace('<', r'&lt;', 1)\n            val = val.replace('>', r'&gt;', 1)\n            return '<pre>' + val + '</pre>'\n\n        if get_option(\"display.notebook_repr_html\"):\n            max_rows = get_option(\"display.max_rows\")\n            max_cols = get_option(\"display.max_columns\")\n            show_dimensions = get_option(\"display.show_dimensions\")\n\n            return self.to_html(max_rows=max_rows, max_cols=max_cols,\n                                show_dimensions=show_dimensions, notebook=True)\n        else:\n            return None", "category": "Python"}, {"instruction": "def save_codeobject(self, obj):\n        \"\"\"\n        Save a code object\n        \"\"\"\n", "input": "", "output": "        if PY3:  # pragma: no branch\n            args = (\n                obj.co_argcount, obj.co_kwonlyargcount, obj.co_nlocals, obj.co_stacksize,\n                obj.co_flags, obj.co_code, obj.co_consts, obj.co_names, obj.co_varnames,\n                obj.co_filename, obj.co_name, obj.co_firstlineno, obj.co_lnotab, obj.co_freevars,\n                obj.co_cellvars\n            )\n        else:\n            args = (\n                obj.co_argcount, obj.co_nlocals, obj.co_stacksize, obj.co_flags, obj.co_code,\n                obj.co_consts, obj.co_names, obj.co_varnames, obj.co_filename, obj.co_name,\n                obj.co_firstlineno, obj.co_lnotab, obj.co_freevars, obj.co_cellvars\n            )\n        self.save_reduce(types.CodeType, args, obj=obj)", "category": "Python"}, {"instruction": "def flatatt(self, **attr):\n        '''Return a string with attributes to add to the tag'''\n", "input": "", "output": "        cs = ''\n        attr = self._attr\n        classes = self._classes\n        data = self._data\n        css = self._css\n        attr = attr.copy() if attr else {}\n        if classes:\n            cs = ' '.join(classes)\n            attr['class'] = cs\n        if css:\n            attr['style'] = ' '.join(('%s:%s;' % (k, v) for\n                                      k, v in css.items()))\n        if data:\n            for k, v in data.items():\n                attr['data-%s' % k] = dump_data_value(v)\n        if attr:\n            return ''.join(attr_iter(attr))\n        else:\n            return ''", "category": "Python"}, {"instruction": "def startup_config_path(self):\n        \"\"\"\n        :returns: Path of the startup config\n        \"\"\"\n", "input": "", "output": "        return os.path.join(self._working_directory, \"configs\", \"i{}_startup-config.cfg\".format(self._dynamips_id))", "category": "Python"}, {"instruction": "def nii_ugzip(imfile, outpath=''):\n    '''Uncompress *.gz file'''\n", "input": "", "output": "    import gzip\n    with gzip.open(imfile, 'rb') as f:\n        s = f.read()\n    # Now store the uncompressed data\n    if outpath=='':\n        fout = imfile[:-3]\n    else:\n        fout = os.path.join(outpath, os.path.basename(imfile)[:-3])\n    # store uncompressed file data from 's' variable\n    with open(fout, 'wb') as f:\n        f.write(s)\n    return fout", "category": "Python"}, {"instruction": "def generate_policy_statements(self):\n        \"\"\"Generates the policy statements for the role used by the function.\n\n        To add additional statements you can either override the\n        `extended_policy_statements` method to return a list of Statements\n        to be added to the policy, or override this method itself if you\n        need more control.\n\n        Returns:\n            list: A list of :class:`awacs.aws.Statement` objects.\n        \"\"\"\n", "input": "", "output": "        statements = self._policy_statements\n        statements.extend(\n            lambda_basic_execution_statements(\n                self.function.Ref()\n            )\n        )\n        extended_statements = self.extended_policy_statements()\n        if extended_statements:\n            statements.extend(extended_statements)\n        return statements", "category": "Python"}, {"instruction": "def create_resource_object(self, type, id):\n        \"\"\"Create a resource object of type for the integer id. type\n        should be one of the following strings:\n\n        resource\n        drawable\n        window\n        pixmap\n        fontable\n        font\n        gc\n        colormap\n        cursor\n\n        This function can be used when a resource ID has been fetched\n        e.g. from an resource or a command line argument. Resource\n        objects should never be created by instantiating the appropriate\n        class directly, since any X extensions dynamically added by the\n        library will not be available.\n        \"\"\"\n", "input": "", "output": "        return self.display.resource_classes[type](self.display, id)", "category": "Python"}, {"instruction": "def dependencies(source):\n    \"\"\"Output a list of all classes referenced by the given source.\"\"\"\n", "input": "", "output": "    loader = ClassLoader(source, max_cache=-1)\n    all_dependencies = set()\n    for klass in loader.classes:\n        new_dependencies = loader.dependencies(klass) - all_dependencies\n        all_dependencies.update(new_dependencies)\n        for new_dep in new_dependencies:\n            click.echo(new_dep)", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Flush data, write 28 bytes BGZF EOF marker, and close BGZF file.\n        samtools will look for a magic EOF marker, just a 28 byte empty BGZF\n        block, and if it is missing warns the BAM file may be truncated. In\n        addition to samtools writing this block, so too does bgzip - so this\n        implementation does too.\n        \"\"\"\n", "input": "", "output": "        if self._buffer:\n            self.flush()\n        self._handle.write(_bgzf_eof)\n        self._handle.flush()\n        self._handle.close()", "category": "Python"}, {"instruction": "def get_facet_values_as_list(self, field):\n        '''\n        :param str field: Name of facet field to retrieve values from.\n\n        Returns facet values as list for a given field. Example::\n\n            >>> res = solr.query('SolrClient_unittest',{\n                'q':'*:*',\n                'facet':'true',\n                'facet.field':'facet_test',\n            })\n            >>> res.get_facet_values_as_list('facet_test')\n            [9, 6, 14, 10, 11]\n            >>> res.get_facets()\n            {'facet_test': {'Lorem': 9, 'ipsum': 6, 'amet,': 14, 'dolor': 10, 'sit': 11}}\n\n        '''\n", "input": "", "output": "        facets = self.get_facets()\n        out = []\n        if field in facets.keys():\n            for facetfield in facets[field]:\n                out.append(facets[field][facetfield])\n            return out\n        else:\n            raise SolrResponseError(\"No field in facet output\")", "category": "Python"}, {"instruction": "def to_json(self) -> Dict:\n        \"\"\"Return this query as a JSON object.\"\"\"\n", "input": "", "output": "        rv = {\n            'network_ids': self.network_ids,\n        }\n\n        if self.seeding:\n            rv['seeding'] = self.seeding.to_json()\n\n        if self.pipeline:\n            rv['pipeline'] = self.pipeline.to_json()\n\n        return rv", "category": "Python"}, {"instruction": "def delete_issue_link(self, id):\n        \"\"\"Delete a link between two issues.\n\n        :param id: ID of the issue link to delete\n        \"\"\"\n", "input": "", "output": "        url = self._get_url('issueLink') + \"/\" + id\n        return self._session.delete(url)", "category": "Python"}, {"instruction": "def ReadHuntOutputPluginsStates(self, hunt_id, cursor=None):\n    \"\"\"Reads all hunt output plugins states of a given hunt.\"\"\"\n", "input": "", "output": "\n    columns = \", \".join(_HUNT_OUTPUT_PLUGINS_STATES_COLUMNS)\n\n    query = (\"SELECT {columns} FROM hunt_output_plugins_states \"\n             \"WHERE hunt_id = %s\".format(columns=columns))\n    rows_returned = cursor.execute(query, [db_utils.HuntIDToInt(hunt_id)])\n    if rows_returned > 0:\n      states = []\n      for row in cursor.fetchall():\n        states.append(self._HuntOutputPluginStateFromRow(row))\n      return states\n\n    query = \"SELECT hunt_id FROM hunts WHERE hunt_id = %s\"\n    rows_returned = cursor.execute(query, [db_utils.HuntIDToInt(hunt_id)])\n    if rows_returned == 0:\n      raise db.UnknownHuntError(hunt_id)\n\n    return []", "category": "Python"}, {"instruction": "def main(argv=None):\n    \"\"\"Run when invoked from the operating system shell\"\"\"\n", "input": "", "output": "\n    parser = argparse.ArgumentParser(\n        description='Commands as arguments'\n    )\n    command_help = 'optional command to run, if no command given, enter an interactive shell'\n    parser.add_argument('command', nargs='?',\n                        help=command_help)\n    arg_help = 'optional arguments for command'\n    parser.add_argument('command_args', nargs=argparse.REMAINDER,\n                        help=arg_help)\n\n    args = parser.parse_args(argv)\n\n    c = CmdLineApp()\n\n    if args.command:\n        # we have a command, run it and then exit\n        c.onecmd_plus_hooks('{} {}'.format(args.command, ' '.join(args.command_args)))\n    else:\n        # we have no command, drop into interactive mode\n        c.cmdloop()", "category": "Python"}, {"instruction": "def write(self, symbol, data, prune_previous_version=True, metadata=None, **kwargs):\n        \"\"\"\n        Records a write request to be actioned on context exit. Takes exactly the same parameters as the regular\n        library write call.\n        \"\"\"\n", "input": "", "output": "        if data is not None:\n            # We only write data if existing data is None or the Timeseries data has changed or metadata has changed\n            if self.base_ts.data is None or not are_equals(data, self.base_ts.data) or metadata != self.base_ts.metadata:\n                self._do_write = True\n        self._write = partial(self._version_store.write, symbol, data, prune_previous_version=prune_previous_version,\n                              metadata=metadata, **kwargs)", "category": "Python"}, {"instruction": "def strain_in_plane(self, **kwargs):\n        '''\n        Returns the in-plane strain assuming no lattice relaxation, which\n        is positive for tensile strain and negative for compressive strain.\n        '''\n", "input": "", "output": "        if self._strain_out_of_plane is not None:\n            return ((self._strain_out_of_plane / -2.) *\n                    (self.unstrained.c11(**kwargs) /\n                     self.unstrained.c12(**kwargs)  )  )\n        else:\n            return 1 - self.unstrained.a(**kwargs) / self.substrate.a(**kwargs)", "category": "Python"}, {"instruction": "def refresh(self):\n        \"\"\"\n        Refreshes the internal lookup table if necessary.\n        \"\"\"\n", "input": "", "output": "        try:\n            self._private_to_public = self.cloud_discovery.discover_nodes()\n        except Exception as ex:\n            self.logger.warning(\"Failed to load addresses from Hazelcast.cloud: {}\".format(ex.args[0]),\n                                extra=self._logger_extras)", "category": "Python"}, {"instruction": "def requested(self, user, include=None):\n        \"\"\"\n        Retrieve the requested tickets for this user.\n\n        :param include: list of objects to sideload. `Side-loading API Docs \n            <https://developer.zendesk.com/rest_api/docs/core/side_loading>`__.\n        :param user: User object or id\n        \"\"\"\n", "input": "", "output": "        return self._query_zendesk(self.endpoint.requested, 'ticket', id=user, include=include)", "category": "Python"}, {"instruction": "def has_path(nodes, A, B):\n    r\"\"\"Test if nodes from a breadth_first_order search lead from A to\n    B.\n\n    Parameters\n    ----------\n    nodes : array_like\n        Nodes from breadth_first_oder_seatch\n    A : array_like\n        The set of educt states\n    B : array_like\n        The set of product states\n\n    Returns\n    -------\n    has_path : boolean\n        True if there exists a path, else False\n\n    \"\"\"\n", "input": "", "output": "    x1 = np.intersect1d(nodes, A).size > 0\n    x2 = np.intersect1d(nodes, B).size > 0\n    return x1 and x2", "category": "Python"}, {"instruction": "def fpath(self, version=None, tags=None, ext=None):\n        \"\"\"Returns the filepath appropriate for an instance of this dataset.\n\n        Parameters\n        ----------\n        version: str, optional\n            The version of the instance of this dataset.\n        tags : list of str, optional\n            The tags associated with the given instance of this dataset.\n        ext : str, optional\n            The file extension to use. If not given, the default extension is\n            used.\n\n        Returns\n        -------\n        str\n            The appropariate filepath.\n        \"\"\"\n", "input": "", "output": "        if self.singleton:\n            return dataset_filepath(\n                filename=self.fname(version=version, tags=tags, ext=ext),\n                task=self.task,\n                **self.kwargs,\n            )\n        return dataset_filepath(\n            filename=self.fname(version=version, tags=tags, ext=ext),\n            dataset_name=self.name,\n            task=self.task,\n            **self.kwargs,\n        )", "category": "Python"}, {"instruction": "def allowed_values(self):\n        \"\"\"Return a list of allowed values.\"\"\"\n", "input": "", "output": "        if self._allowed_values is None:\n            self._allowed_values = ValueList()\n            for val in self.scraper._fetch_allowed_values(self):\n                if isinstance(val, DimensionValue):\n                    self._allowed_values.append(val)\n                else:\n                    self._allowed_values.append(DimensionValue(val,\n                                                               Dimension()))\n        return self._allowed_values", "category": "Python"}, {"instruction": "def validatePushElement(self, ctxt, elem, qname):\n        \"\"\"Push a new element start on the validation stack. \"\"\"\n", "input": "", "output": "        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        if elem is None: elem__o = None\n        else: elem__o = elem._o\n        ret = libxml2mod.xmlValidatePushElement(ctxt__o, self._o, elem__o, qname)\n        return ret", "category": "Python"}, {"instruction": "def _load_image(cls, rkey):\n        \"\"\"Load image from file or return the cached instance.\"\"\"\n", "input": "", "output": "\n        v = cls._stock[rkey]\n        img = None\n        itype = v['type']\n        if itype in ('stock', 'data'):\n            img = tk.PhotoImage(format=v['format'], data=v['data'])\n        elif itype == 'created':\n            img = v['image']\n        else:\n            img = tk.PhotoImage(file=v['filename'])\n        cls._cached[rkey] = img\n        logger.info('Loaded resource %s.' % rkey)\n        return img", "category": "Python"}, {"instruction": "def last_room_temp(self):\n        \"\"\"Return avg room temperature for last session.\"\"\"\n", "input": "", "output": "        try:\n            rmtemps = self.intervals[1]['timeseries']['tempRoomC']\n        except KeyError:\n            return None\n        tmp = 0\n        num_temps = len(rmtemps)\n\n        if num_temps == 0:\n            return None\n\n        for temp in rmtemps:\n            tmp += temp[1]\n        rmtemp = tmp/num_temps\n        return rmtemp", "category": "Python"}, {"instruction": "def frequency_bins(header):\n  '''\n  Returnes the frequency-axis lower bin edge values for the spectrogram. \n  '''\n", "input": "", "output": "\n  center_frequency = 1.0e6*header['rf_center_frequency']\n  if header[\"number_of_subbands\"] > 1:\n    center_frequency += header[\"subband_spacing_hz\"]*(header[\"number_of_subbands\"]/2.0 - 0.5)\n\n  return np.fft.fftshift(\\\n    np.fft.fftfreq( int(header[\"number_of_subbands\"] * constants.bins_per_half_frame*(1.0 - header['over_sampling'])), \\\n      1.0/(header[\"number_of_subbands\"]*header[\"subband_spacing_hz\"])) + center_frequency\n    )", "category": "Python"}, {"instruction": "def _print_results(file, status):\n        \"\"\"Print the download results.\n\n        Args:\n            file (str): The filename.\n            status (str): The file download status.\n        \"\"\"\n", "input": "", "output": "\n        file_color = c.Fore.GREEN\n        status_color = c.Fore.RED\n        if status == 'Success':\n            status_color = c.Fore.GREEN\n        elif status == 'Skipped':\n            status_color = c.Fore.YELLOW\n        print(\n            '{}{!s:<13}{}{!s:<35}{}{!s:<8}{}{}'.format(\n                c.Fore.CYAN,\n                'Downloading:',\n                file_color,\n                file,\n                c.Fore.CYAN,\n                'Status:',\n                status_color,\n                status,\n            )\n        )", "category": "Python"}, {"instruction": "def Elasticsearch(*args, **kwargs):\n    \"\"\"Elasticsearch wrapper function\n\n    Wrapper function around the official Elasticsearch class that adds\n    a simple version check upon initialization.\n    In particular it checks if the major version of the library in use\n    match the one of the cluster that we are tring to interact with.\n    The check can be skipped by setting to false the check_version parameter.\n\n    #note: Boyska didn't like subclassing :)\n    \"\"\"\n", "input": "", "output": "    check_version = kwargs.pop('check_version', True)\n    es = Elasticsearch_official(*args, **kwargs)\n    if check_version:\n        es_version = es.info()['version']['number'].split('.')\n        if(int(es_version[0]) != int(es_pylib_version[0])):\n            raise RuntimeError(\"The Elasticsearch python library version does not match the one of the running cluster: {} != {}. Please install the correct elasticsearch-py version\".format(es_pylib_version[0], es_version[0]))\n    return es", "category": "Python"}, {"instruction": "def simxGetCollectionHandle(clientID, collectionName, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n", "input": "", "output": "\n    handle = ct.c_int()\n    if (sys.version_info[0] == 3) and (type(collectionName) is str):\n        collectionName=collectionName.encode('utf-8')\n    return c_GetCollectionHandle(clientID, collectionName, ct.byref(handle), operationMode), handle.value", "category": "Python"}, {"instruction": "def global_start_index(self, value):\n        \"\"\"Set the global start index.\"\"\"\n", "input": "", "output": "        if not isinstance(value, int) and value is not None:\n            raise TypeError('global_start_index attribute must be of int '\n                            'type.')\n        self._global_start_index = value", "category": "Python"}, {"instruction": "def _get_from_send_queue(self):\n        ''' Get message from send queue, if one exists '''\n", "input": "", "output": "        try:\n            packet = self.transmit.get(block=False)\n            self.logger.info('Sending packet')\n            self.logger.debug(packet)\n            return packet\n        except queue.Empty:\n            pass\n        return None", "category": "Python"}, {"instruction": "def get_json(self):\n        \"\"\"Create JSON data for slot.\n\n        :returns: JSON data for slot as follows:\n\n            {\n                \"@SlotIdx\":0,\n                \"OnboardControllers\":{\n                    \"OnboardController\": [\n                    ]\n                },\n                \"AddOnCards\":{\n                    \"AddOnCard\": [\n                    ]\n                }\n            }\n        \"\"\"\n", "input": "", "output": "        json = self.get_basic_json()\n        if self.onboard_cards:\n            json['OnboardControllers'] = {\n                'OnboardController':\n                    [c.get_json() for c in self.onboard_cards.values()]\n            }\n        if self.addon_cards:\n            json['AddOnCards'] = {\n                'AddOnCard': [c.get_json() for c in self.addon_cards.values()]\n            }\n        return json", "category": "Python"}, {"instruction": "def qsize(self):\n        \"\"\"\n        Returns the number of items currently in the queue\n\n        :return: Integer containing size of the queue\n        :exception: ConnectionError if queue is not connected\n        \"\"\"\n", "input": "", "output": "        if not self.connected:\n            raise QueueNotConnectedError(\"Queue is not Connected\")\n\n        try:\n            size = self.__db.llen(self._key)\n        except redis.ConnectionError as e:\n            raise redis.ConnectionError(repr(e))\n        return size", "category": "Python"}, {"instruction": "def run(self, cmd):\n        \"\"\"Execute git command in bash\"\"\"\n", "input": "", "output": "        cmd = ['git', '--git-dir=%s' % self.path] + cmd\n        print(\"cmd list\", cmd)\n        print(\"cmd\", ' '.join(cmd))\n        res = None\n        try:\n            res = subprocess.check_output(cmd)\n        except BaseException:\n            pass\n        if res:\n            try:\n                res = res.decode()\n            except UnicodeDecodeError:\n                res = res.decode('utf-8')\n        return res", "category": "Python"}, {"instruction": "def get_context_data(self, **kwargs):\n        \"\"\"Update context data with list of RPC methods of the current entry point.\n        Will be used to display methods documentation page\"\"\"\n", "input": "", "output": "        kwargs.update({\n            'methods': registry.get_all_methods(self.entry_point, sort_methods=True),\n        })\n        return super(RPCEntryPoint, self).get_context_data(**kwargs)", "category": "Python"}, {"instruction": "def convert(self, value, param, ctx):  # pylint: disable=inconsistent-return-statements\n        \"\"\"Validate memory argument. Returns the memory value in megabytes.\"\"\"\n", "input": "", "output": "        matches = MEMORY_RE.match(value.lower())\n        if matches is None:\n            self.fail('%s is not a valid value for memory amount' % value, param, ctx)\n        amount_str, unit = matches.groups()\n        amount = int(amount_str)\n        if unit in [None, 'm', 'mb']:\n            # Assume the user intends gigabytes if they specify a number < 1024\n            if amount < 1024:\n                return amount * 1024\n            else:\n                if amount % 1024 != 0:\n                    self.fail('%s is not an integer that is divisable by 1024' % value, param, ctx)\n                return amount\n        elif unit in ['g', 'gb']:\n            return amount * 1024", "category": "Python"}, {"instruction": "def _parse_local_interface(self, config):\n        \"\"\"Scans the config block and parses the local-interface value\n\n        Args:\n            config (str): The config block to scan\n\n        Returns:\n            dict: A dict object that is intended to be merged into the\n                resource dict\n        \"\"\"\n", "input": "", "output": "        match = re.search(r'local-interface (\\w+)', config)\n        value = match.group(1) if match else None\n        return dict(local_interface=value)", "category": "Python"}, {"instruction": "def data_storage_shape(self):\n        \"\"\"Shape tuple of the data as stored in the file.\n\n        If no header is available (i.e., before it has been initialized),\n        or any of the header entries ``'nx', 'ny', 'nz'`` is missing,\n        -1 is returned, which makes reshaping a no-op.\n        Otherwise, the returned shape is a permutation of `data_shape`,\n        i.e., ``(nx, ny, nz)``, according to `data_axis_order` in the\n        following way::\n\n            data_shape[i] == data_storage_shape[data_axis_order[i]]\n\n        See Also\n        --------\n        data_shape\n        data_axis_order\n        \"\"\"\n", "input": "", "output": "        if self.data_shape == -1:\n            return -1\n        else:\n            return tuple(self.data_shape[ax]\n                         for ax in np.argsort(self.data_axis_order))", "category": "Python"}, {"instruction": "def tpr(y_true, y_pred, round=True):\n    \"\"\"True positive rate `tp / (tp + fn)`\n    \"\"\"\n", "input": "", "output": "    y_true, y_pred = _mask_value_nan(y_true, y_pred)\n    if round:\n        y_true = np.round(y_true)\n        y_pred = np.round(y_pred)\n    return skm.recall_score(y_true, y_pred)", "category": "Python"}, {"instruction": "async def home_z(self, mount: top_types.Mount = None):\n        \"\"\" Home the two z-axes \"\"\"\n", "input": "", "output": "        if not mount:\n            axes = [Axis.Z, Axis.A]\n        else:\n            axes = [Axis.by_mount(mount)]\n        await self.home(axes)", "category": "Python"}, {"instruction": "def effective_patterns(self, context=None):\n        \"\"\"\n        Get effective patterns for this rebulk object and its children.\n        :param context:\n        :type context:\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        patterns = list(self._patterns)\n        for rebulk in self._rebulks:\n            if not rebulk.disabled(context):\n                extend_safe(patterns, rebulk._patterns)\n        return patterns", "category": "Python"}, {"instruction": "def list_reced_topics(self, user_alias=None, start=0):\n        \"\"\"\n        \u63a8\u8350\u7684\u8bdd\u9898\u5217\u8868\n        \n        :param user_alias: \u6307\u5b9a\u7528\u6237\uff0c\u9ed8\u8ba4\u5f53\u524d\n        :param start: \u7ffb\u9875\n        :return: \u5e26\u4e0b\u4e00\u9875\u7684\u5217\u8868\n        \"\"\"\n", "input": "", "output": "        user_alias = user_alias or self.api.user_alias\n        xml = self.api.xml(API_GROUP_LIST_USER_RECED_TOPICS % user_alias, params={'start': start})\n        return build_list_result(self._parse_topic_table(xml, 'title,comment,time,group,rec'), xml)", "category": "Python"}, {"instruction": "def check_who_read(self, messages):\n        \"\"\" Check who read each message. \"\"\"\n", "input": "", "output": "        # we get the corresponding Participation objects\n        for m in messages:\n            readers = []\n            for p in m.thread.participation_set.all():\n                if p.date_last_check is None:\n                    pass\n                elif p.date_last_check > m.sent_at:\n                    # the message has been read\n                    readers.append(p.participant.id)\n            setattr(m, \"readers\", readers)\n\n        return messages", "category": "Python"}, {"instruction": "def tx_for_tx_hash(self, tx_hash):\n        \"\"\"\n        returns the pycoin.tx object for tx_hash\n        \"\"\"\n", "input": "", "output": "        try:\n            url_append = \"?token=%s&includeHex=true\" % self.api_key\n            url = self.base_url(\"txs/%s%s\" % (b2h_rev(tx_hash), url_append))\n            result = json.loads(urlopen(url).read().decode(\"utf8\"))\n            tx = Tx.parse(io.BytesIO(h2b(result.get(\"hex\"))))\n            return tx\n        except Exception:\n            raise Exception", "category": "Python"}, {"instruction": "def _up_to_date(md5, fn):\n    \"\"\"\n    Make sure md5sum(fn) == md5, and if not, delete `fn`.\n    \"\"\"\n", "input": "", "output": "    if os.path.exists(fn):\n        if hashlib.md5(open(fn).read()).hexdigest() == md5:\n            logger.info('md5sum match for %s' % fn)\n            return True\n        else:\n            logger.info('wrong md5sum for %s' % fn)\n            os.unlink(fn)", "category": "Python"}, {"instruction": "def put(self, local_path, destination_s3_path, **kwargs):\n        \"\"\"\n        Put an object stored locally to an S3 path.\n        :param local_path: Path to source local file\n        :param destination_s3_path: URL for target S3 location\n        :param kwargs: Keyword arguments are passed to the boto function `put_object`\n        \"\"\"\n", "input": "", "output": "        self._check_deprecated_argument(**kwargs)\n\n        # put the file\n        self.put_multipart(local_path, destination_s3_path, **kwargs)", "category": "Python"}, {"instruction": "def deploy(self):\n        '''\n            Creates a link at the original path of this target\n        '''\n", "input": "", "output": "        if not os.path.exists(self.path):\n            makedirs(self.path)\n\n        link(self.vault_path, self.real_path)", "category": "Python"}, {"instruction": "def cite(self, max_authors=5):\n        \"\"\"\n        Return string with a citation for the record, formatted as:\n        '{authors} ({year}). {title} {journal} {volume}({issue}): {pages}.'\n        \"\"\"\n", "input": "", "output": "        citation_data = {\n            'title': self.title,\n            'authors': self.authors_et_al(max_authors),\n            'year': self.year,\n            'journal': self.journal,\n            'volume': self.volume,\n            'issue': self.issue,\n            'pages': self.pages,\n        }\n        citation = \"{authors} ({year}). {title} {journal}\".format(\n            **citation_data)\n\n        if self.volume and self.issue and self.pages:\n            citation += \" {volume}({issue}): {pages}.\".format(**citation_data)\n        elif self.volume and self.issue:\n            citation += \" {volume}({issue}).\".format(**citation_data)\n        elif self.volume and self.pages:\n            citation += \" {volume}: {pages}.\".format(**citation_data)\n        elif self.volume:\n            citation += \" {volume}.\".format(**citation_data)\n        elif self.pages:\n            citation += \" {pages}.\".format(**citation_data)\n        else:\n            citation += \".\"\n\n        return citation", "category": "Python"}, {"instruction": "def load(self, text,\n         lineterminator='\\r\\n',\n         quotechar='\"',\n         delimiter=\",\",\n         escapechar=escapechar,\n         quoting=csv.QUOTE_MINIMAL):\n    \"\"\"Item from CSV representation.\"\"\"\n", "input": "", "output": "\n    f = io.StringIO(text)\n\n    if not quotechar:\n        quoting = csv.QUOTE_NONE\n\n    reader = csv.DictReader(\n        f,\n        delimiter=delimiter,\n        quotechar=quotechar,\n        quoting=quoting,\n        lineterminator=lineterminator)\n\n    if reader.fieldnames:\n        reader.fieldnames = [field.strip() for field in reader.fieldnames]\n\n    try:\n        self.primitive = next(reader)\n    except StopIteration:\n        self.primitive = {}", "category": "Python"}, {"instruction": "def contents(self):\n        \"\"\"\n        Contents returns a list of the files in the data directory.\n        \"\"\"\n", "input": "", "output": "        data = find_dataset_path(\n            self.name, data_home=self.data_home, ext=None\n        )\n        return os.listdir(data)", "category": "Python"}, {"instruction": "def dt_comp(self, sampled_topics):\n\n        \"\"\"\n        Compute document-topic matrix from sampled_topics.\n        \"\"\"\n", "input": "", "output": "\n        samples = sampled_topics.shape[0]\n        dt = np.zeros((self.D, self.K, samples))\n\n        for s in range(samples):            \n                dt[:, :, s] = \\\n                    samplers_lda.dt_comp(self.docid, sampled_topics[s, :],\n                                         self.N, self.K, self.D, self.alpha)\n\n        return dt", "category": "Python"}, {"instruction": "def mousePressEvent(self, event):\n        \"\"\"Override Qt method\n\n        Select line, and starts selection\n        \"\"\"\n", "input": "", "output": "        line_number = self.editor.get_linenumber_from_mouse_event(event)\n        self._pressed = line_number\n        self._released = line_number\n        self.editor.select_lines(self._pressed,\n                                 self._released)", "category": "Python"}, {"instruction": "def underscore_to_camelcase(value, first_upper=True):\n    \"\"\"Transform string from underscore_string to camelCase.\n\n    :param value: string with underscores\n    :param first_upper: the result will have its first character in upper case\n    :type value: str\n    :return: string in CamelCase or camelCase according to the first_upper\n    :rtype: str\n\n    :Example:\n        >>> underscore_to_camelcase('camel_case')\n        'CamelCase'\n        >>> underscore_to_camelcase('camel_case', False)\n        'camelCase'\n    \"\"\"\n", "input": "", "output": "    value = str(value)\n    camelized = \"\".join(x.title() if x else '_' for x in value.split(\"_\"))\n    if not first_upper:\n        camelized = camelized[0].lower() + camelized[1:]\n    return camelized", "category": "Python"}, {"instruction": "def normalizeGlyphUnicode(value):\n    \"\"\"\n    Normalizes glyph unicode.\n\n    * **value** must be an int or hex (represented as a string).\n    * **value** must be in a unicode range.\n    * Returned value will be an ``int``.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(value, (int, basestring)) or isinstance(value, bool):\n        raise TypeError(\"Glyph unicode must be a int or hex string, not %s.\"\n                        % type(value).__name__)\n    if isinstance(value, basestring):\n        try:\n            value = int(value, 16)\n        except ValueError:\n            raise ValueError(\"Glyph unicode hex must be a valid hex string.\")\n    if value < 0 or value > 1114111:\n        raise ValueError(\"Glyph unicode must be in the Unicode range.\")\n    return value", "category": "Python"}, {"instruction": "def graft(coll, branch, index):\n    '''Graft list branch into coll at index\n    '''\n", "input": "", "output": "    pre = coll[:index]\n    post = coll[index:]\n    ret = pre + branch + post\n    return ret", "category": "Python"}, {"instruction": "def simple_periodic_send(bus):\n    \"\"\"\n    Sends a message every 20ms with no explicit timeout\n    Sleeps for 2 seconds then stops the task.\n    \"\"\"\n", "input": "", "output": "    print(\"Starting to send a message every 200ms for 2s\")\n    msg = can.Message(arbitration_id=0x123, data=[1, 2, 3, 4, 5, 6], is_extended_id=False)\n    task = bus.send_periodic(msg, 0.20)\n    assert isinstance(task, can.CyclicSendTaskABC)\n    time.sleep(2)\n    task.stop()\n    print(\"stopped cyclic send\")", "category": "Python"}, {"instruction": "def _columns_sql(self, with_namespace=False, **kwargs):\n        \"\"\"\n        SQL for Columns clause for INSERT queries\n        :param with_namespace:\n            Remove from kwargs, never format the column terms with namespaces since only one table can be inserted into\n        \"\"\"\n", "input": "", "output": "        return ' ({columns})'.format(\n              columns=','.join(term.get_sql(with_namespace=False, **kwargs)\n                               for term in self._columns)\n        )", "category": "Python"}, {"instruction": "def _convert(self, value):\n        \"\"\"Returns a PasswordHash from the given string.\n\n        PasswordHash instances or None values will return unchanged.\n        Strings will be hashed and the resulting PasswordHash returned.\n        Any other input will result in a TypeError.\n        \"\"\"\n", "input": "", "output": "        if isinstance(value, PasswordHash):\n            return value\n        elif isinstance(value, str):\n            value = value.encode('utf-8')\n            return PasswordHash.new(value, self.rounds)\n        elif value is not None:\n            raise TypeError(\n                'Cannot convert {} to a PasswordHash'.format(type(value)))", "category": "Python"}, {"instruction": "def register_validator(validator):\n        \"\"\"\n        Register a Validator class for file verification.\n\n        :param validator:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if hasattr(validator, \"EXTS\") and hasattr(validator, \"run\"):\n            ValidatorFactory.PLUGINS.append(validator)\n        else:\n            raise ValidatorException(\"Validator does not have 'run' method or EXTS variable!\")", "category": "Python"}, {"instruction": "def arcsIn(G: Graph, n: Node) -> RDFGraph:\n    \"\"\" arcsIn(G, n) is the set of triples in a graph G with object n. \"\"\"\n", "input": "", "output": "    return RDFGraph(G.triples((None, None, n)))", "category": "Python"}, {"instruction": "def json(self, start=None):\n        \"\"\"\n        :param start: start key in dot notation\n        returns the dict in json format\n        :return: json string version\n        :rtype: string\n        \"\"\"\n", "input": "", "output": "        if start is not None:\n            data = self.data[start]\n        return json.dumps(self.data, indent=4)", "category": "Python"}, {"instruction": "def compounding(start, stop, compound, t=0.0):\n    \"\"\"Yield an infinite series of compounding values. Each time the\n    generator is called, a value is produced by multiplying the previous\n    value by the compound rate.\n\n    EXAMPLE:\n      >>> sizes = compounding(1., 10., 1.5)\n      >>> assert next(sizes) == 1.\n      >>> assert next(sizes) == 1 * 1.5\n      >>> assert next(sizes) == 1.5 * 1.5\n    \"\"\"\n", "input": "", "output": "    curr = float(start)\n    while True:\n        yield _clip(curr, start, stop)\n        curr *= compound", "category": "Python"}, {"instruction": "def env_int(name, required=False, default=empty):\n    \"\"\"Pulls an environment variable out of the environment and casts it to an\n    integer. If the name is not present in the environment and no default is\n    specified then a ``ValueError`` will be raised. Similarly, if the\n    environment value is not castable to an integer, a ``ValueError`` will be\n    raised.\n\n    :param name: The name of the environment variable be pulled\n    :type name: str\n\n    :param required: Whether the environment variable is required. If ``True``\n    and the variable is not present, a ``KeyError`` is raised.\n    :type required: bool\n\n    :param default: The value to return if the environment variable is not\n    present. (Providing a default alongside setting ``required=True`` will raise\n    a ``ValueError``)\n    :type default: bool\n    \"\"\"\n", "input": "", "output": "    value = get_env_value(name, required=required, default=default)\n    if value is empty:\n        raise ValueError(\n            \"`env_int` requires either a default value to be specified, or for \"\n            \"the variable to be present in the environment\"\n        )\n    return int(value)", "category": "Python"}, {"instruction": "def sis_sigma_v2theta_E(self, v_sigma):\n        \"\"\"\n        converts the velocity dispersion into an Einstein radius for a SIS profile\n        :param v_sigma: velocity dispersion (km/s)\n        :return: theta_E (arcsec)\n        \"\"\"\n", "input": "", "output": "        theta_E = 4 * np.pi * (v_sigma * 1000./const.c)**2 * self.D_ds / self.D_s / const.arcsec\n        return theta_E", "category": "Python"}, {"instruction": "def get_endpoint_by_endpoint_id(self, endpoint_id):\n        \"\"\"\n        Get an endpoint by endpoint id\n        \"\"\"\n", "input": "", "output": "        self._validate_uuid(endpoint_id)\n\n        url = \"/notification/v1/endpoint/{}\".format(endpoint_id)\n\n        response = NWS_DAO().getURL(url, self._read_headers)\n        if response.status != 200:\n            raise DataFailureException(url, response.status, response.data)\n\n        data = json.loads(response.data)\n        return self._endpoint_from_json(data.get(\"Endpoint\"))", "category": "Python"}, {"instruction": "def download(cls, filename, input_dir, dl_dir=None):\n        \"\"\"Download the resource from the storage.\"\"\"\n", "input": "", "output": "        file_info = cls.parse_remote(filename)\n        if not dl_dir:\n            dl_dir = os.path.join(input_dir, file_info.container,\n                                  os.path.dirname(file_info.blob))\n            utils.safe_makedir(dl_dir)\n\n        out_file = os.path.join(dl_dir, os.path.basename(file_info.blob))\n\n        if not utils.file_exists(out_file):\n            with file_transaction({}, out_file) as tx_out_file:\n                blob_service = cls.connect(filename)\n                blob_service.get_blob_to_path(\n                    container_name=file_info.container,\n                    blob_name=file_info.blob,\n                    file_path=tx_out_file)\n        return out_file", "category": "Python"}, {"instruction": "def _check_lib(self, remake, compiler, debug, profile):\n        \"\"\"Makes sure that the linked library with the original code exists. If it doesn't\n        the library is compiled from scratch.\n        \"\"\"\n", "input": "", "output": "        from os import path\n        if self.link is None or not path.isfile(self.link):\n            self.makelib(remake, True, compiler, debug, profile)", "category": "Python"}, {"instruction": "def log_all(self, file):\n        \"\"\"Log all data received from RFLink to file.\"\"\"\n", "input": "", "output": "        global rflink_log\n        if file == None:\n            rflink_log = None\n        else:\n            log.debug('logging to: %s', file)\n            rflink_log = open(file, 'a')", "category": "Python"}, {"instruction": "def _get_req_fp(self, op):\n\t\t'''Decisions on what verb to use and content headers happen here\n\t\tArgs:\n\t\t\top \t\t\ta string specifying a http verb'''\n", "input": "", "output": "\t\tif(op):\n\t\t\top = op.lower()\n\t\t\tif op == 'get':\n\t\t\t\treturn requests.get, None\n\t\t\tif op == 'put':\n\t\t\t\treturn requests.put, {'Content-Type': 'application/x-www-form-urlencoded'}\n\t\t\tif op == 'post':\n\t\t\t\treturn requests.post, {'Content-Type': 'application/json'}\n\t\t\tif op == 'delete':\n\t\t\t\treturn requests.delete, None\n\t\telse:\n\t\t\traise NotImplementedError('Operation {} is not supported!'.format(op))", "category": "Python"}, {"instruction": "def _DecodeKey(self, key):\n        \"\"\"Turn a key into a string if possible\"\"\"\n", "input": "", "output": "\n        if self.dict.attrindex.HasBackward(key):\n            return self.dict.attrindex.GetBackward(key)\n        return key", "category": "Python"}, {"instruction": "def is_text_extractor_available(extension: str) -> bool:\n    \"\"\"\n    Is a text extractor available for the specified extension?\n    \"\"\"\n", "input": "", "output": "    if extension is not None:\n        extension = extension.lower()\n    info = ext_map.get(extension)\n    if info is None:\n        return False\n    availability = info[AVAILABILITY]\n    if type(availability) == bool:\n        return availability\n    elif callable(availability):\n        return availability()\n    else:\n        raise ValueError(\n            \"Bad information object for extension: {}\".format(extension))", "category": "Python"}, {"instruction": "def delete_library_value(self, key: str) -> None:\n        \"\"\"Delete the library value for the given key.\n\n        Please consult the developer documentation for a list of valid keys.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n", "input": "", "output": "        desc = Metadata.session_key_map.get(key)\n        if desc is not None:\n            field_id = desc['path'][-1]\n            setattr(ApplicationData.get_session_metadata_model(), field_id, None)\n            return\n        raise KeyError()", "category": "Python"}, {"instruction": "def _file_loc(self):\n        \"\"\"_file_loc helper returns a possible config filename.\n        EG /tmp/stuff/fish.py -> /tmp/stuff/fish.ini\n        \"\"\"\n", "input": "", "output": "        if self.__fname is None:\n            f = os.path.splitext(os.path.basename(argv[0]))[0] + '.ini'\n            cwd = os.getcwd()\n            # todo: prefer script path or current path ??\n            # print(os.path.realpath(sys.argv[0]))\n            # todo: if os.path.exists(os.path.join(cwd, main.__file__)):\n            return os.path.join(cwd, f)\n        return self.__fname", "category": "Python"}, {"instruction": "def addr(self,address):\n        '''returns dictionary with frame information for given address (a tuple of two hex numbers)'''\n", "input": "", "output": "        if isinstance(address,basestring):\n            # If you just gave me a single string, assume its \"XXXX XXXX\"\n            addr = address.split()\n        else:\n            addr = list(address)\n        # Convert to actual hex if you give me strings\n        for i in xrange(len(addr)):\n            if isinstance(addr[i],basestring):\n                addr[i] = int(addr[i],16)\n        for frame in self.raw_frames:\n            if frame['addr']==address:\n                return frame", "category": "Python"}, {"instruction": "def _init_pval_name(self, **kws):\n        \"\"\"Initialize pvalue attribute name.\"\"\"\n", "input": "", "output": "        if 'pval_name' in kws:\n            return kws['pval_name']\n        # If go2res contains GO Terms\n        if self.is_goterm:\n            return \"p_{M}\".format(M=next(iter(self.go2res.values())).get_method_name())\n        # If go2res contains GO namedtuples\n        for fld in next(iter(self.go2res.values()))._fields:\n            if fld[:2] == 'p_' and fld != 'p_uncorrected':\n                return fld", "category": "Python"}, {"instruction": "def make_1D_gauss(n, m, s):\n    \"\"\"return a 1D histogram for a gaussian distribution (n bins, mean m and std s)\n\n    Parameters\n    ----------\n\n    n : int\n        number of bins in the histogram\n    m : float\n        mean value of the gaussian distribution\n    s : float\n        standard deviaton of the gaussian distribution\n\n\n    Returns\n    -------\n    h : np.array (n,)\n          1D histogram for a gaussian distribution\n\n    \"\"\"\n", "input": "", "output": "    x = np.arange(n, dtype=np.float64)\n    h = np.exp(-(x - m)**2 / (2 * s**2))\n    return h / h.sum()", "category": "Python"}, {"instruction": "def _display_error(normalized_data, stream):\n    \"\"\"\n    print error message from docker-py stream and raise Exception.\n    {\n        u'message': u\"Error getting container 981c3e17bfc6138d1985c0c8f5616e9064e56559e817646ad38211a456d6bcf2 from driver\n        devicemapper: Error mounting '/dev/mapper/docker-8:3-34393696-981c3e17bfc6138d1985c0c8f5616e9064e56559e817646ad38211a456d6bcf2'\n        on '/data/docker/devicemapper/mnt/981c3e17bfc6138d1985c0c8f5616e9064e56559e817646ad38211a456d6bcf2': no such file\n        or directory\"\n    }\n\n    \"\"\"\n", "input": "", "output": "    # TODO: need to revisit this later.\n    error = normalized_data['error']\n    if 'error_detail' in normalized_data:\n        stream.write(\"exit code: {0}\\n\".format(normalized_data['error_detail'].get('code'),\n                                               'There was no exit code provided'))\n\n        stream.write(normalized_data['error_detail'].get('message', 'There were no message details provided.'))\n\n    raise DockerStreamException(error)", "category": "Python"}, {"instruction": "def register_comet_object(self, *args, **kwargs):\n        \"\"\"Registers all functions from the object as Comet functions\n        (see :ref:`comet-plugin`).\n\n        This makes mass registration of functions a lot easier.\n\n        Refer to :func:`sijax.plugin.comet.register_comet_object`\n        for more details -ts signature differs slightly.\n\n        This method's signature is the same, except that the first\n        argument that :func:`sijax.plugin.comet.register_comet_object`\n        expects is the Sijax instance, and this method\n        does that automatically, so you don't have to do it.\n        \"\"\"\n", "input": "", "output": "        sijax.plugin.comet.register_comet_object(self._sijax, *args, **kwargs)", "category": "Python"}, {"instruction": "def build_url(self, *args, **kwargs):\n        \"\"\"Builds a new API url from scratch.\"\"\"\n", "input": "", "output": "        parts = [kwargs.get('base_url') or self.base_url]\n        parts.extend(args)\n        parts = [str(p) for p in parts]\n        key = tuple(parts)\n        __logs__.info('Building a url from %s', key)\n        if not key in __url_cache__:\n            __logs__.info('Missed the cache building the url')\n            __url_cache__[key] = '/'.join(parts)\n        return __url_cache__[key]", "category": "Python"}, {"instruction": "def get_ids_g_goids(self, goids):\n        \"\"\"Get database IDs (DB_IDs), given a set of GO IDs.\"\"\"\n", "input": "", "output": "        return set(nt.DB_ID for nt in self.associations if nt.GO_ID in goids)", "category": "Python"}, {"instruction": "def determine_chan_detect_threshold(kal_out):\n    \"\"\"Return channel detect threshold from kal output.\"\"\"\n", "input": "", "output": "    channel_detect_threshold = \"\"\n    while channel_detect_threshold == \"\":\n        for line in kal_out.splitlines():\n            if \"channel detect threshold: \" in line:\n                channel_detect_threshold = str(line.split()[-1])\n        if channel_detect_threshold == \"\":\n            print(\"Unable to parse sample rate\")\n            channel_detect_threshold = None\n    return channel_detect_threshold", "category": "Python"}, {"instruction": "def _config_to_text(self):\n        \"\"\"Render the configuration as text.\"\"\"\n", "input": "", "output": "        r = u'' # unicode('',\"UTF-8\")\n        for k in self._config:\n            # if k == 'creation_date':\n            #     r +=  k + \": \" + self._config[k][0] + '\\n'\n            # else:\n            #uk = unicode(k,\"UTF-8\")\n            cosa = '\\n        '.join(self._config[k]) + '\\n'\n            r += k + \": \" + cosa\n            #r += k + \": \" + '\\n        '.join(self._config[k]) + '\\n'\n        r += '\\n'\n        return r", "category": "Python"}, {"instruction": "def iter_regions(self):\n        \"\"\"\n        Return an iterable list of all region files. Use this function if you only\n        want to loop through each region files once, and do not want to cache the results.\n        \"\"\"\n", "input": "", "output": "        # TODO: Implement BoundingBox\n        # TODO: Implement sort order\n        for x,z in self.regionfiles.keys():\n            close_after_use = False\n            if (x,z) in self.regions:\n                regionfile = self.regions[(x,z)]\n            else:\n                # It is not yet cached.\n                # Get file, but do not cache later.\n                regionfile = region.RegionFile(self.regionfiles[(x,z)], chunkclass = self.chunkclass)\n                regionfile.loc = Location(x=x,z=z)\n                close_after_use = True\n            try:\n                yield regionfile\n            finally:\n                if close_after_use:\n                    regionfile.close()", "category": "Python"}, {"instruction": "def parse(readDataInstance):\n        \"\"\"\n        Returns a new L{ImageDebugDirectory} object.\n        \n        @type readDataInstance: L{ReadData}\n        @param readDataInstance: A new L{ReadData} object with data to be parsed as a L{ImageDebugDirectory} object.\n        \n        @rtype: L{ImageDebugDirectory}\n        @return: A new L{ImageDebugDirectory} object.\n        \"\"\"\n", "input": "", "output": "        dbgDir = ImageDebugDirectory()\n\n        dbgDir.characteristics.value = readDataInstance.readDword()\n        dbgDir.timeDateStamp.value = readDataInstance.readDword()\n        dbgDir.majorVersion.value = readDataInstance.readWord()\n        dbgDir.minorVersion.value = readDataInstance.readWord()\n        dbgDir.type.value = readDataInstance.readDword()\n        dbgDir.sizeOfData.value = readDataInstance.readDword()\n        dbgDir.addressOfData.value = readDataInstance.readDword()\n        dbgDir.pointerToRawData.value = readDataInstance.readDword()\n        \n        return dbgDir", "category": "Python"}, {"instruction": "def _compute_document_meta(self):\n        \"\"\"\n        Return documents meta information that can\n        be used for fast document lookups. Meta information\n        consists of documents titles, categories and positions\n        in file.\n        \"\"\"\n", "input": "", "output": "        meta = OrderedDict()\n\n        bounds_iter = xml_utils.bounds(self.filename,\n                            start_re=r'<text id=\"(\\d+)\"[^>]*name=\"([^\"]*)\"',\n                            end_re=r'</text>')\n\n        for match, bounds in bounds_iter:\n            doc_id, title = str(match.group(1)), match.group(2)\n            title = xml_utils.unescape_attribute(title)\n\n            # cache categories\n            xml_data = xml_utils.load_chunk(self.filename, bounds)\n            doc = Document(compat.ElementTree.XML(xml_data.encode('utf8')))\n\n            meta[doc_id] = _DocumentMeta(title, bounds, doc.categories())\n        return meta", "category": "Python"}, {"instruction": "async def handle_exception(self, error: Exception) -> Response:\n        \"\"\"Handle an uncaught exception.\n\n        By default this switches the error response to a 500 internal\n        server error.\n        \"\"\"\n", "input": "", "output": "        await got_request_exception.send(self, exception=error)\n\n        self.log_exception(sys.exc_info())\n\n        if self.propagate_exceptions:\n            return await traceback_response()\n\n        internal_server_error = all_http_exceptions[500]()\n        handler = self._find_exception_handler(internal_server_error)\n\n        if handler is None:\n            return internal_server_error.get_response()\n        else:\n            return await self.finalize_request(await handler(error), from_error_handler=True)", "category": "Python"}, {"instruction": "def _bytes_to_str(lines):\n    \"\"\"\n    Convert all lines from byte string to unicode string, if necessary\n    \"\"\"\n", "input": "", "output": "    if len(lines) >= 1 and hasattr(lines[0], 'decode'):\n        return [line.decode('utf-8') for line in lines]\n    else:\n        return lines", "category": "Python"}, {"instruction": "def get_new_index(self):\n        \"\"\"\n        Gets the next available index and marks it as in use.\n        :return:  An index that is not currently in use\n        \"\"\"\n", "input": "", "output": "        free_index = next(filterfalse(self.numbers.__contains__, count(0, 1)))\n        self.numbers.add(free_index)\n        return free_index", "category": "Python"}, {"instruction": "def get_string_from_data(self, offset, data):\n        \"\"\"Get an ASCII string from within the data.\"\"\"\n", "input": "", "output": "        \n        # OC Patch\n        b = None\n        \n        try:\n            b = data[offset]\n        except IndexError:\n            return ''\n        \n        s = ''\n        while ord(b):\n            s += b\n            offset += 1\n            try:\n                b = data[offset]\n            except IndexError:\n                break\n        \n        return s", "category": "Python"}, {"instruction": "def prox_soft(X, step, thresh=0):\n    \"\"\"Soft thresholding proximal operator\n    \"\"\"\n", "input": "", "output": "    thresh_ = _step_gamma(step, thresh)\n    return np.sign(X)*prox_plus(np.abs(X) - thresh_, step)", "category": "Python"}, {"instruction": "def add_event(cls, event, event_name=None):\n        \"\"\"Add events\"\"\"\n", "input": "", "output": "        # setattr(cls, event_name, event)\n        event_name = event_name or event.__name__\n        setattr(cls, event_name, types.MethodType(event, cls))", "category": "Python"}, {"instruction": "def unique_ordered(list_):\n    \"\"\"\n    Returns unique items in ``list_`` in the order they were seen.\n\n    Args:\n        list_ (list):\n\n    Returns:\n        list: unique_list - unique list which maintains order\n\n    CommandLine:\n        python -m utool.util_list --exec-unique_ordered\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_list import *  # NOQA\n        >>> list_ = [4, 6, 6, 0, 6, 1, 0, 2, 2, 1]\n        >>> unique_list = unique_ordered(list_)\n        >>> result = ('unique_list = %s' % (str(unique_list),))\n        >>> print(result)\n        unique_list = [4, 6, 0, 1, 2]\n    \"\"\"\n", "input": "", "output": "    list_ = list(list_)\n    flag_list = flag_unique_items(list_)\n    unique_list = compress(list_, flag_list)\n    return unique_list", "category": "Python"}, {"instruction": "def remove_draft_child(self):\n        \"\"\"Remove the draft child from versioning.\"\"\"\n", "input": "", "output": "        if self.draft_child:\n            with db.session.begin_nested():\n                super(PIDNodeVersioning, self).remove_child(self.draft_child,\n                                                            reorder=True)", "category": "Python"}, {"instruction": "def zero_pad(seq, left=0, right=0, zero=0.):\n  \"\"\"\n  Zero padding sample generator (not a Stream!).\n\n  Parameters\n  ----------\n  seq :\n    Sequence to be padded.\n  left :\n    Integer with the number of elements to be padded at left (before).\n    Defaults to zero.\n  right :\n    Integer with the number of elements to be padded at right (after).\n    Defaults to zero.\n  zero :\n    Element to be padded. Defaults to a float zero (0.0).\n\n  Returns\n  -------\n  A generator that pads the given ``seq`` with samples equals to ``zero``,\n  ``left`` times before and ``right`` times after it.\n\n  \"\"\"\n", "input": "", "output": "  for unused in xrange(left):\n    yield zero\n  for item in seq:\n    yield item\n  for unused in xrange(right):\n    yield zero", "category": "Python"}, {"instruction": "def build_assert(cls: Type[_Block], nodes: List[ast.stmt], min_line_number: int) -> _Block:\n        \"\"\"\n        Assert block is all nodes that are after the Act node.\n\n        Note:\n            The filtering is *still* running off the line number of the Act\n            node, when instead it should be using the last line of the Act\n            block.\n        \"\"\"\n", "input": "", "output": "        return cls(filter_assert_nodes(nodes, min_line_number), LineType._assert)", "category": "Python"}, {"instruction": "def is_gesture(self):\n\t\t\"\"\"Macro to check if this event is\n\t\ta :class:`~libinput.event.GestureEvent`.\n\t\t\"\"\"\n", "input": "", "output": "\n\t\tif self in {type(self).GESTURE_SWIPE_BEGIN, type(self).GESTURE_SWIPE_END,\n\t\t\ttype(self).GESTURE_SWIPE_UPDATE, type(self).GESTURE_PINCH_BEGIN,\n\t\t\ttype(self).GESTURE_PINCH_UPDATE, type(self).GESTURE_PINCH_END}:\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False", "category": "Python"}, {"instruction": "def register_flags(self, flags):\n        \"\"\"\n        Register flags as being relevant to this handler.\n\n        Relevant flags will be used to determine if the handler should\n        be re-invoked due to changes in the set of active flags.  If this\n        handler has already been invoked during this :func:`dispatch` run\n        and none of its relevant flags have been set or removed since then,\n        then the handler will be skipped.\n\n        This is also used for linting and composition purposes, to determine\n        if a layer has unhandled flags.\n        \"\"\"\n", "input": "", "output": "        self._CONSUMED_FLAGS.update(flags)\n        self._flags.update(flags)", "category": "Python"}, {"instruction": "def light_curve(self, t, texp=0.0, tol=1e-8, maxdepth=4):\n        \"\"\"\n        Get the light curve evaluated at a list of times using the current\n        model.\n\n        :param t:\n            The times where the light curve should be evaluated (in days).\n\n        :param tol:\n            The stopping criterion for the exposure time integration.\n\n        :param maxdepth:\n            The maximum recursion depth of the exposure time integrator.\n\n        \"\"\"\n", "input": "", "output": "        t = np.atleast_1d(t)\n        return CythonSolver().simple_light_curve(self._get_params(),\n                                                 t, texp, tol, maxdepth)", "category": "Python"}, {"instruction": "def virtual_interface_list(self, name):\n        '''\n        Get virtual interfaces on slice\n        '''\n", "input": "", "output": "        nt_ks = self.compute_conn\n        nets = nt_ks.virtual_interfaces.list(self._server_uuid_from_name(name))\n        return [network.__dict__ for network in nets]", "category": "Python"}, {"instruction": "def run_dot(dot):\n    \"\"\"Converts a graph in DOT format into an IPython displayable object.\"\"\"\n", "input": "", "output": "    global impl\n    if impl is None:\n        impl = guess_impl()\n    if impl == \"dot\":\n        return run_dot_dot(dot)\n    elif impl == \"js\":\n        return run_dot_js(dot)\n    else:\n        raise ValueError(\"unknown implementation {}\".format(impl))", "category": "Python"}, {"instruction": "def apply_procs(self, procs, kwargs, inputstring, log=True):\n        \"\"\"Apply processors to inputstring.\"\"\"\n", "input": "", "output": "        for get_proc in procs:\n            proc = get_proc(self)\n            inputstring = proc(inputstring, **kwargs)\n            if log:\n                logger.log_tag(proc.__name__, inputstring, multiline=True)\n        return inputstring", "category": "Python"}, {"instruction": "def _escape_jid(jid):\n    '''\n    Do proper formatting of the jid\n    '''\n", "input": "", "output": "    jid = six.text_type(jid)\n    jid = re.sub(r\"'*\", \"\", jid)\n    return jid", "category": "Python"}, {"instruction": "def _get_child_relation(self, child_pid):\n        \"\"\"Retrieve the relation between this node and a child PID.\"\"\"\n", "input": "", "output": "        return PIDRelation.query.filter_by(\n            parent=self._resolved_pid,\n            child=child_pid,\n            relation_type=self.relation_type.id).one()", "category": "Python"}, {"instruction": "def _validate_config(self, config):\n        \"\"\"\n        Validates that all necessary config parameters are specified.\n        :type config: dict[str, dict[str, Any] | str]\n        :param config: the module config\n        \"\"\"\n", "input": "", "output": "        if config is None:\n            raise ValueError(\"OIDCFrontend conf can't be 'None'.\")\n\n        for k in {\"signing_key_path\", \"provider\"}:\n            if k not in config:\n                raise ValueError(\"Missing configuration parameter '{}' for OpenID Connect frontend.\".format(k))", "category": "Python"}, {"instruction": "def get_inline_views_from_fieldsets(fieldsets):\n    \"\"\"Returns a list of field names from an admin fieldsets structure.\"\"\"\n", "input": "", "output": "    inline_views = []\n    for _, opts in fieldsets or ():\n        if 'fieldsets' in opts:\n            inline_views += get_inline_views_from_fieldsets(opts.get('fieldsets'))\n        elif 'inline_view' in opts:\n            inline_views.append(opts.get('inline_view'))\n    return inline_views", "category": "Python"}, {"instruction": "def create_boilerplate():\n    \"\"\"\n    Create kibitzr.yml and kibitzr-creds.yml in current directory\n    if they do not exist.\n    \"\"\"\n", "input": "", "output": "    if not os.path.exists('kibitzr.yml'):\n        with open('kibitzr.yml', 'wt') as fp:\n            logger.info(\"Saving sample check in kibitzr.yml\")\n            fp.write(KIBITZR_YML)\n    else:\n        logger.info(\"kibitzr.yml already exists. Skipping\")\n    if not os.path.exists('kibitzr-creds.yml'):\n        with open('kibitzr-creds.yml', 'wt') as fp:\n            logger.info(\"Creating kibitzr-creds.yml\")\n            fp.write(KIBITZR_CREDS_YML)\n        os.chmod('kibitzr-creds.yml', stat.S_IRUSR | stat.S_IWUSR)\n    else:\n        logger.info(\"kibitzr-creds.yml already exists. Skipping\")", "category": "Python"}, {"instruction": "def findLowest(self, symorders):\n        \"\"\"Find the position of the first lowest tie in a\n        symorder or -1 if there are no ties\"\"\"\n", "input": "", "output": "        _range = range(len(symorders))\n        stableSymorders = map(None, symorders, _range)\n\n        # XXX FIX ME\n        # Do I need to sort?\n        stableSymorders.sort()\n        lowest = None\n        for index in _range:\n            if stableSymorders[index][0] == lowest:\n                return stableSymorders[index-1][1]\n            lowest = stableSymorders[index][0]\n        return -1", "category": "Python"}, {"instruction": "def create_datafeed(self):\n        ''' create datafeed '''\n", "input": "", "output": "        response, status_code = self.__agent__.Datafeed.post_v4_datafeed_create(\n            sessionToken=self.__session__,\n            keyManagerToken=self.__keymngr__\n        ).result()\n        # return the token\n        self.logger.debug('%s: %s' % (status_code, response))\n        return status_code, response['id']", "category": "Python"}, {"instruction": "def reserve_job(self, timeout=5):\n        \"\"\"Reserve a job for this connection. Blocks for TIMEOUT secionds and raises TIMED_OUT if no job was available\n\n        :param timeout: Time to wait for a job, in seconds.\n        :type timeout: int\n        \"\"\"\n", "input": "", "output": "        timeout = int(timeout)\n        if self.socket_timeout is not None:\n            if timeout >= self.socket_timeout:\n                raise ValueError('reserve_job timeout must be < socket timeout')\n        if not self._watchlist:\n            raise ValueError('Select a tube or two before reserving a job')\n        with self._sock_ctx() as socket:\n            self._send_message('reserve-with-timeout {0}'.format(timeout), socket)\n            job_id, job_data = self._receive_id_and_data_with_prefix(b'RESERVED', socket)\n            return Job(job_id, job_data)", "category": "Python"}, {"instruction": "def show_image(kwargs, call=None):\n    '''\n    Show the details from QingCloud concerning an image.\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt-cloud -f show_image my-qingcloud image=trustysrvx64c\n        salt-cloud -f show_image my-qingcloud image=trustysrvx64c,coreos4\n        salt-cloud -f show_image my-qingcloud image=trustysrvx64c zone=ap1\n    '''\n", "input": "", "output": "    if call != 'function':\n        raise SaltCloudSystemExit(\n            'The show_images function must be called with '\n            '-f or --function'\n        )\n\n    if not isinstance(kwargs, dict):\n        kwargs = {}\n\n    images = kwargs['image']\n    images = images.split(',')\n\n    params = {\n        'action': 'DescribeImages',\n        'images': images,\n        'zone': _get_specified_zone(kwargs, get_configured_provider()),\n    }\n\n    items = query(params=params)\n\n    if not items['image_set']:\n        raise SaltCloudNotFound('The specified image could not be found.')\n\n    result = {}\n    for image in items['image_set']:\n        result[image['image_id']] = {}\n        for key in image:\n            result[image['image_id']][key] = image[key]\n\n    return result", "category": "Python"}, {"instruction": "def setup(self):\n        \"\"\"Setup.\"\"\"\n", "input": "", "output": "\n        self.blocks = self.config['block_comments']\n        self.lines = self.config['line_comments']\n        self.group_comments = self.config['group_comments']\n        self.prefix = self.config['prefix']\n        self.generic_mode = self.config['generic_mode']\n        self.strings = self.config['strings']\n        self.trigraphs = self.config['trigraphs']\n        self.decode_escapes = self.config['decode_escapes']\n        self.charset_size = self.config['charset_size']\n        self.wide_charset_size = self.config['wide_charset_size']\n        self.exec_charset = self.get_encoding_name(self.config['exec_charset'])\n        self.wide_exec_charset = self.get_encoding_name(self.config['wide_exec_charset'])\n        self.string_types, self.wild_string_types = self.eval_string_type(self.config['string_types'])\n        if not self.generic_mode:\n            self.pattern = RE_CPP", "category": "Python"}, {"instruction": "def _CreateOutputFileHandles(self, output_type):\n    \"\"\"Creates a new gzipped output tempfile for the output type.\n\n    We write to JSON data to gzip_filehandle to get compressed data. We hold a\n    reference to the original filehandle (gzip_filehandle_parent) so we can pass\n    the gzip data to bigquery.\n\n    Args:\n      output_type: string of export type to be used in filename. e.g.\n        ExportedFile\n\n    Returns:\n      A TempOutputTracker object\n    \"\"\"\n", "input": "", "output": "    gzip_filehandle_parent = tempfile.NamedTemporaryFile(suffix=output_type)\n    gzip_filehandle = gzip.GzipFile(gzip_filehandle_parent.name, \"wb\",\n                                    self.GZIP_COMPRESSION_LEVEL,\n                                    gzip_filehandle_parent)\n    self.temp_output_trackers[output_type] = TempOutputTracker(\n        output_type=output_type,\n        gzip_filehandle=gzip_filehandle,\n        gzip_filehandle_parent=gzip_filehandle_parent)\n    return self.temp_output_trackers[output_type]", "category": "Python"}, {"instruction": "def cleanup(self):\n        \"\"\"Do the final clean up before shutting down.\"\"\"\n", "input": "", "output": "        size = settings.get('history_size')\n        self._save_history_to_file(self.commandprompthistory,\n                                   self._cmd_hist_file, size=size)\n        self._save_history_to_file(self.senderhistory, self._sender_hist_file,\n                                   size=size)\n        self._save_history_to_file(self.recipienthistory,\n                                   self._recipients_hist_file, size=size)", "category": "Python"}, {"instruction": "def get_xauth_access_token(self, username, password):\n        \"\"\"\n        Get an access token from an username and password combination.\n        In order to get this working you need to create an app at\n        http://twitter.com/apps, after that send a mail to api@twitter.com\n        and request activation of xAuth for it.\n        \"\"\"\n", "input": "", "output": "        try:\n            url = self._get_oauth_url('access_token')\n            oauth = OAuth1(self.consumer_key,\n                           client_secret=self.consumer_secret)\n            r = requests.post(url=url,\n                              auth=oauth,\n                              headers={'x_auth_mode': 'client_auth',\n                                       'x_auth_username': username,\n                                       'x_auth_password': password})\n\n            credentials = parse_qs(r.content)\n            return credentials.get('oauth_token')[0], credentials.get('oauth_token_secret')[0]\n        except Exception as e:\n            raise TweepError(e)", "category": "Python"}, {"instruction": "def keep(self, *cols):\n        \"\"\"\n        Limit the dataframe to some columns\n\n        :param cols: names of the columns\n        :type cols: str\n\n        :example: ``ds.keep(\"Col 1\", \"Col 2\")``\n        \"\"\"\n", "input": "", "output": "        try:\n            self.df = self.df[list(cols)]\n        except Exception as e:\n            self.err(e, \"Can not remove colums\")\n            return\n        self.ok(\"Setting dataframe to columns\", \" \".join(cols))", "category": "Python"}, {"instruction": "def upload(self, photo_file, **kwds):\n        \"\"\"\n        Endpoint: /photo/upload.json\n\n        Uploads the specified photo filename.\n        \"\"\"\n", "input": "", "output": "        with open(photo_file, 'rb') as in_file:\n            result = self._client.post(\"/photo/upload.json\",\n                                       files={'photo': in_file},\n                                       **kwds)[\"result\"]\n        return Photo(self._client, result)", "category": "Python"}, {"instruction": "def add_mapped_chain_ids(self, mapped_chains):\n        \"\"\"Add chains by ID into the mapped_chains attribute\n\n        Args:\n            mapped_chains (str, list): Chain ID or list of IDs\n\n        \"\"\"\n", "input": "", "output": "        mapped_chains = ssbio.utils.force_list(mapped_chains)\n\n        for c in mapped_chains:\n            if c not in self.mapped_chains:\n                self.mapped_chains.append(c)\n                log.debug('{}: added to list of mapped chains'.format(c))\n            else:\n                log.debug('{}: chain already in list of mapped chains, not adding'.format(c))", "category": "Python"}, {"instruction": "def getproducts(self, force_refresh=False, **kwargs):\n        \"\"\"\n        Query all products and return the raw dict info. Takes all the\n        same arguments as product_get.\n\n        On first invocation this will contact bugzilla and internally\n        cache the results. Subsequent getproducts calls or accesses to\n        self.products will return this cached data only.\n\n        :param force_refresh: force refreshing via refresh_products()\n        \"\"\"\n", "input": "", "output": "        if force_refresh or not self._cache.products:\n            self.refresh_products(**kwargs)\n        return self._cache.products", "category": "Python"}, {"instruction": "def routing_feature(app):\n    \"\"\"\n    Add routing feature\n    Allows to define application routes un urls.py file and use lazy views.\n    Additionally enables regular exceptions in route definitions\n    \"\"\"\n", "input": "", "output": "    # enable regex routes\n    app.url_map.converters['regex'] = RegexConverter\n\n    urls = app.name.rsplit('.', 1)[0] + '.urls.urls'\n\n    # important issue ahead\n    # see: https://github.com/projectshift/shift-boiler/issues/11\n    try:\n        urls = import_string(urls)\n    except ImportError as e:\n        err = 'Failed to import {}. If it exists, check that it does not '\n        err += 'import something non-existent itself! '\n        err += 'Try to manually import it to debug.'\n        raise ImportError(err.format(urls))\n\n    # add routes now\n    for route in urls.keys():\n        route_options = urls[route]\n        route_options['rule'] = route\n        app.add_url_rule(**route_options)", "category": "Python"}, {"instruction": "async def gather(self, *cmds: str) -> Tuple[int]:\n        \"\"\"Coroutine to spawn subprocesses and block until completion.\n\n        Note:\n            The same `max_concurrency` restriction that applies to `spawn`\n            also applies here.\n\n        Returns:\n            The exit codes of the spawned subprocesses, in the order they were\n            passed.\n\n        \"\"\"\n", "input": "", "output": "        subprocs = self.spawn(*cmds)\n        subproc_wait_coros = [subproc.wait_done() for subproc in subprocs]\n        return await asyncio.gather(*subproc_wait_coros)", "category": "Python"}, {"instruction": "def logout_service_description(self):\n        \"\"\"Logout service description.\"\"\"\n", "input": "", "output": "        label = 'Logout from ' + self.name\n        if (self.auth_type):\n            label = label + ' (' + self.auth_type + ')'\n        return({\"@id\": self.logout_uri,\n                \"profile\": self.profile_base + 'logout',\n                \"label\": label})", "category": "Python"}, {"instruction": "def fi_business_id(business_id):\n    \"\"\"\n    Validate a Finnish Business ID.\n\n    Each company in Finland has a distinct business id. For more\n    information see `Finnish Trade Register`_\n\n    .. _Finnish Trade Register:\n        http://en.wikipedia.org/wiki/Finnish_Trade_Register\n\n    Examples::\n\n        >>> fi_business_id('0112038-9')  # Fast Monkeys Ltd\n        True\n\n        >>> fi_business_id('1234567-8')  # Bogus ID\n        ValidationFailure(func=fi_business_id, ...)\n\n    .. versionadded:: 0.4\n    .. versionchanged:: 0.5\n        Method renamed from ``finnish_business_id`` to ``fi_business_id``\n\n    :param business_id: business_id to validate\n    \"\"\"\n", "input": "", "output": "    if not business_id or not re.match(business_id_pattern, business_id):\n        return False\n    factors = [7, 9, 10, 5, 8, 4, 2]\n    numbers = map(int, business_id[:7])\n    checksum = int(business_id[8])\n    sum_ = sum(f * n for f, n in zip(factors, numbers))\n    modulo = sum_ % 11\n    return (11 - modulo == checksum) or (modulo == 0 and checksum == 0)", "category": "Python"}, {"instruction": "def _router_address(self, data):\n        \"\"\"only for IPv6 addresses\"\"\"\n", "input": "", "output": "        args = data.split()[1:]\n        try:\n            self._relay_attrs['ip_v6'].extend(args)\n        except KeyError:\n            self._relay_attrs['ip_v6'] = list(args)", "category": "Python"}, {"instruction": "def _endian(self) -> str:\n        '''\n        Determines endianess of the CDF file\n        Only used in __init__\n        '''\n", "input": "", "output": "        if (self._encoding == 1 or self._encoding == 2 or self._encoding == 5 or\n            self._encoding == 7 or self._encoding == 9 or self._encoding == 11 or\n                self._encoding == 12):\n            return 'big-endian'\n        else:\n            return 'little-endian'", "category": "Python"}, {"instruction": "def _parameters(cls, tokens):\n        \"\"\"\n        Extract all of the parameters from the tokens.\n\n        :param tokens: The tokens to extract the parameters from\n        :type tokens: list\n\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        arguments = []\n        options = []\n\n        for token in tokens:\n            if not token.startswith(\"--\"):\n                arguments.append(cls._parse_argument(token))\n            else:\n                options.append(cls._parse_option(token))\n\n        return {\"arguments\": arguments, \"options\": options}", "category": "Python"}, {"instruction": "def markdown_single_text(self, catalog, cdli_number):\n        \"\"\"\n        Prints single text in file in markdown.\n       :param catalog: text ingested by cdli_corpus\n       :param cdli_number: text you wish to print\n       :return: output in filename.md\n       \"\"\"\n", "input": "", "output": "        if cdli_number in catalog:\n            pnum = catalog[cdli_number]['pnum']\n            edition = catalog[cdli_number]['edition']\n            metadata = '\\n\\t'.join(catalog[cdli_number]['metadata'])\n            transliteration = '\\n\\t'.join(catalog[cdli_number]['transliteration'])\n            normalization = '\\n\\t'.join(catalog[cdli_number]['normalization'])\n            translation = '\\n\\t'.join(catalog[cdli_number]['translation'])\n            m_d = ", "category": "Python"}, {"instruction": "def get(key, default=-1):\n        \"\"\"Backport support for original codes.\"\"\"\n", "input": "", "output": "        if isinstance(key, int):\n            return OptionClass(key)\n        if key not in OptionClass._member_map_:\n            extend_enum(OptionClass, key, default)\n        return OptionClass[key]", "category": "Python"}, {"instruction": "def insert_attribute(self, att, index):\n        \"\"\"\n        Inserts the attribute at the specified location.\n\n        :param att: the attribute to insert\n        :type att: Attribute\n        :param index: the index to insert the attribute at\n        :type index: int\n        \"\"\"\n", "input": "", "output": "        javabridge.call(self.jobject, \"insertAttributeAt\", \"(Lweka/core/Attribute;I)V\", att.jobject, index)", "category": "Python"}, {"instruction": "def run(vrn_info, cnvs_by_name, somatic_info):\n    \"\"\"Run THetA analysis given output from CNV caller on a tumor/normal pair.\n    \"\"\"\n", "input": "", "output": "    cmd = _get_cmd(\"RunTHeTA.py\")\n    if not cmd:\n        logger.info(\"THetA scripts not found in current PATH. Skipping.\")\n    else:\n        from bcbio.structural import cnvkit\n        work_dir = _sv_workdir(somatic_info.tumor_data)\n        assert \"cnvkit\" in cnvs_by_name, \"THetA requires CNVkit calls\"\n        cnv_info = cnvkit.export_theta(cnvs_by_name[\"cnvkit\"], somatic_info.tumor_data)\n        cnv_info[\"theta_input\"] = subset_by_supported(cnv_info[\"theta_input\"], _theta_to_coords,\n                                                      cnvs_by_name, work_dir, somatic_info.tumor_data)\n        return _run_theta(cnv_info, somatic_info.tumor_data, work_dir, run_n3=False)", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self,\n                   'matching_results') and self.matching_results is not None:\n            _dict['matching_results'] = self.matching_results\n        if hasattr(self, 'results') and self.results is not None:\n            _dict['results'] = [x._to_dict() for x in self.results]\n        if hasattr(self, 'aggregations') and self.aggregations is not None:\n            _dict['aggregations'] = [x._to_dict() for x in self.aggregations]\n        if hasattr(self, 'passages') and self.passages is not None:\n            _dict['passages'] = [x._to_dict() for x in self.passages]\n        if hasattr(\n                self,\n                'duplicates_removed') and self.duplicates_removed is not None:\n            _dict['duplicates_removed'] = self.duplicates_removed\n        return _dict", "category": "Python"}, {"instruction": "def datacenter(self, name):\n        \"\"\"\n        :param name: location key\n        :type name: :py:class:`basestring`\n        \n        :Returns: a new DataCenter object\n        \n        This method treats the 'name' argument as a location key (on the \n        `known_locations` attribute dict) or FQDN, and keeps existing \n        authentication and other configuration from this object.\n        \"\"\"\n", "input": "", "output": "        # The base form of this, as below, simply sets up a redirect. \n        # j, _ = self.request('GET', 'datacenters/' + str(name))\n        if name not in self.known_locations and '.' not in name:\n            self.datacenters()\n        dc = DataCenter(location=name, headers=self.default_headers, \n                login=self.login, verbose=self.verbose, \n                verify=self.verify, known_locations=self.known_locations)\n        dc.auth = self.auth\n        return dc", "category": "Python"}, {"instruction": "def server_enabled(s_name, **connection_args):\n    '''\n    Check if a server is enabled globally\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' netscaler.server_enabled 'serverName'\n    '''\n", "input": "", "output": "    server = _server_get(s_name, **connection_args)\n    return server is not None and server.get_state() == 'ENABLED'", "category": "Python"}, {"instruction": "def retrieve(self):\n        \"\"\"\n        Retrieve contact information on ourselves.\n        \"\"\"\n", "input": "", "output": "        response = self.request(E.retrieveResellerRequest())\n        return Reseller(response.data)", "category": "Python"}, {"instruction": "def annotation_has_expired(event, key, timeout):\n    \"\"\"Check if an event error has expired.\"\"\"\n", "input": "", "output": "    anns = get_annotations(event, key)\n    if anns:\n        return (time.time() - anns[0][\"ts\"]) > timeout\n    else:\n        return False", "category": "Python"}, {"instruction": "def csrf_token():\n    \"\"\"\n    Generate a token string from bytes arrays. The token in the session is user\n    specific.\n    \"\"\"\n", "input": "", "output": "    if \"_csrf_token\" not in session:\n        session[\"_csrf_token\"] = os.urandom(128)\n    return hmac.new(app.secret_key, session[\"_csrf_token\"],\n            digestmod=sha1).hexdigest()", "category": "Python"}, {"instruction": "def write_quick(self, i2c_addr, force=None):\n        \"\"\"\n        Perform quick transaction. Throws IOError if unsuccessful.\n        :param i2c_addr: i2c address\n        :type i2c_addr: int\n        :param force:\n        :type force: Boolean\n        \"\"\"\n", "input": "", "output": "        self._set_address(i2c_addr, force=force)\n        msg = i2c_smbus_ioctl_data.create(\n            read_write=I2C_SMBUS_WRITE, command=0, size=I2C_SMBUS_QUICK)\n        ioctl(self.fd, I2C_SMBUS, msg)", "category": "Python"}, {"instruction": "def get_trade_fee(self, **params):\n        \"\"\"Get trade fee.\n\n        https://github.com/binance-exchange/binance-official-api-docs/blob/master/wapi-api.md#trade-fee-user_data\n\n        :param symbol: optional\n        :type symbol: str\n        :param recvWindow: the number of milliseconds the request is valid for\n        :type recvWindow: int\n\n        :returns: API response\n\n        .. code-block:: python\n\n            {\n                \"tradeFee\": [\n                    {\n                        \"symbol\": \"ADABNB\",\n                        \"maker\": 0.9000,\n                        \"taker\": 1.0000\n                    }, {\n                        \"symbol\": \"BNBBTC\",\n                        \"maker\": 0.3000,\n                        \"taker\": 0.3000\n                    }\n                ],\n                \"success\": true\n            }\n\n        :raises: BinanceWithdrawException\n\n        \"\"\"\n", "input": "", "output": "        res = self._request_withdraw_api('get', 'tradeFee.html', True, data=params)\n        if not res['success']:\n            raise BinanceWithdrawException(res['msg'])\n        return res", "category": "Python"}, {"instruction": "def alwaysThrew(self, error_type=None): #pylint: disable=invalid-name\n        \"\"\"\n        Determining whether the specified exception is the ONLY thrown exception\n        Args:\n            error_type:\n                None: checking without specified exception\n                Specified Exception\n        Return: Boolean\n        \"\"\"\n", "input": "", "output": "        if self.callCount == 0:\n            return False\n        if not error_type:\n            return True if len(self.exceptions) == self.callCount else False\n        else:\n            return uch.obj_in_list_always(self.exceptions, error_type)", "category": "Python"}, {"instruction": "def _validate_interface_option(attr, value, addrfam='inet'):\n    '''lookup the validation function for a [addrfam][attr] and\n    return the results\n\n    :param attr: attribute name\n    :param value: raw setting value\n    :param addrfam: address family (inet, inet6,\n    '''\n", "input": "", "output": "    valid, _value, errmsg = False, value, 'Unknown validator'\n    attrmaps = ATTRMAPS.get(addrfam, [])\n    for attrmap in attrmaps:\n        if attr in attrmap:\n            validate_func = attrmap[attr]\n            (valid, _value, errmsg) = validate_func(value)\n            break\n    return (valid, _value, errmsg)", "category": "Python"}, {"instruction": "def tg_mean(tas, freq='YS'):\n    r\"\"\"Mean of daily average temperature.\n\n    Resample the original daily mean temperature series by taking the mean over each period.\n\n    Parameters\n    ----------\n    tas : xarray.DataArray\n      Mean daily temperature [\u2103] or [K]\n    freq : str, optional\n      Resampling frequency\n\n    Returns\n    -------\n    xarray.DataArray\n      The mean daily temperature at the given time frequency\n\n    Notes\n    -----\n    Let :math:`TN_i` be the mean daily temperature of day :math:`i`, then for a period :math:`p` starting at\n    day :math:`a` and finishing on day :math:`b`:\n\n    .. math::\n\n       TG_p = \\frac{\\sum_{i=a}^{b} TN_i}{b - a + 1}\n\n\n    Examples\n    --------\n    The following would compute for each grid cell of file `tas.day.nc` the mean temperature\n    at the seasonal frequency, ie DJF, MAM, JJA, SON, DJF, etc.:\n\n    >>> t = xr.open_dataset('tas.day.nc')\n    >>> tg = tm_mean(t, freq=\"QS-DEC\")\n    \"\"\"\n", "input": "", "output": "\n    arr = tas.resample(time=freq) if freq else tas\n    return arr.mean(dim='time', keep_attrs=True)", "category": "Python"}, {"instruction": "def Registry(address='https://index.docker.io', **kwargs):\n    \"\"\"\n    :return:\n    \"\"\"\n", "input": "", "output": "    registry = None\n    try:\n        try:\n            registry = V1(address, **kwargs)\n            registry.ping()\n        except RegistryException:\n            registry = V2(address, **kwargs)\n            registry.ping()\n    except OSError:\n        logger.warning(\n            'Was unable to verify certs for a registry @ {0}. '\n            'Will not be able to interact with it for any operations until the certs can be validated.'.format(address)\n        )\n\n    return registry", "category": "Python"}, {"instruction": "def tiles_from_bounds(self, bounds, zoom):\n        \"\"\"\n        Return all tiles intersecting with bounds.\n\n        Bounds values will be cleaned if they cross the antimeridian or are\n        outside of the Northern or Southern tile pyramid bounds.\n\n        - bounds: tuple of (left, bottom, right, top) bounding values in tile\n            pyramid CRS\n        - zoom: zoom level\n        \"\"\"\n", "input": "", "output": "        validate_zoom(zoom)\n        if not isinstance(bounds, tuple) or len(bounds) != 4:\n            raise ValueError(\"bounds must be a tuple of left, bottom, right, top values\")\n        if not isinstance(bounds, Bounds):\n            bounds = Bounds(*bounds)\n        if self.is_global:\n            for tile in _global_tiles_from_bounds(self, bounds, zoom):\n                yield tile\n        else:\n            for tile in _tiles_from_cleaned_bounds(self, bounds, zoom):\n                yield tile", "category": "Python"}, {"instruction": "def assertion(func):\n    \"\"\"Extend sure with a custom assertion method.\"\"\"\n", "input": "", "output": "    func = assertionmethod(func)\n    setattr(AssertionBuilder, func.__name__, func)\n    return func", "category": "Python"}, {"instruction": "def _normalizeImpurityMatrix(matrix):\n    \"\"\"Normalize each row of the matrix that the sum of the row equals 1.\n\n    :params matrix: a matrix (2d nested list) containing numbers, each isobaric\n        channel must be present as a row.\n    :returns: a matrix containing normalized values\n    \"\"\"\n", "input": "", "output": "    newMatrix = list()\n    for line in matrix:\n        total = sum(line)\n        if total != 0:\n            newMatrix.append([i / total for i in line])\n        else:\n            newMatrix.append(line)\n    return newMatrix", "category": "Python"}, {"instruction": "def L_channel(Q_plant, sed_inputs=sed_dict):\n    \"\"\"Return the length of the inlet and exit channels for the sedimentation tank.\n    Parameters\n    ----------\n    Q_plant : float\n        Total plant flow rate\n    sed_inputs : dict\n        A dictionary of all of the constant inputs needed for sedimentation tank\n        calculations can be found in sed.yaml\n    Returns\n    -------\n    float\n        Length of the inlet and exit channels for the sedimentation tank.\n    Examples\n    --------\n    >>> from aide_design.play import*\n    >>>\n    \"\"\"\n", "input": "", "output": "    n_tanks = n_tanks(Q_plant, sed_inputs)\n    return ((n_tanks * sed_inputs['tank']['W']) + sed_inputs['thickness_wall'] +\n            ((n_tanks-1) * sed_inputs['thickness_wall']))", "category": "Python"}, {"instruction": "def get_parameter_dict(self, include_frozen=False):\n        \"\"\"\n        Get an ordered dictionary of the parameters\n\n        Args:\n            include_frozen (Optional[bool]): Should the frozen parameters be\n                included in the returned value? (default: ``False``)\n\n        \"\"\"\n", "input": "", "output": "        return OrderedDict(zip(\n            self.get_parameter_names(include_frozen=include_frozen),\n            self.get_parameter_vector(include_frozen=include_frozen),\n        ))", "category": "Python"}, {"instruction": "def distance_to(self, other_catchment):\n        \"\"\"\n        Returns the distance between the centroids of two catchments in kilometers.\n\n        :param other_catchment: Catchment to calculate distance to\n        :type other_catchment: :class:`.Catchment`\n        :return: Distance between the catchments in km.\n        :rtype: float\n        \"\"\"\n", "input": "", "output": "        try:\n            if self.country == other_catchment.country:\n                try:\n                    return 0.001 * hypot(self.descriptors.centroid_ngr.x - other_catchment.descriptors.centroid_ngr.x,\n                                         self.descriptors.centroid_ngr.y - other_catchment.descriptors.centroid_ngr.y)\n                except TypeError:\n                    # In case no centroid available, just return infinity which is helpful in most cases\n                    return float('+inf')\n            else:\n                # If the catchments are in a different country (e.g. `ni` versus `gb`) then set distance to infinity.\n                return float('+inf')\n        except (TypeError, KeyError):\n            raise InsufficientDataError(\"Catchment `descriptors` attribute must be set first.\")", "category": "Python"}, {"instruction": "def create(self, msgtype, *args, **kwargs):\n        ''' Create a new Message instance for the given type.\n\n        Args:\n            msgtype (str) :\n\n        '''\n", "input": "", "output": "        if msgtype not in self._messages:\n            raise ProtocolError(\"Unknown message type %r for protocol version %s\" % (msgtype, self._version))\n        return self._messages[msgtype].create(*args, **kwargs)", "category": "Python"}, {"instruction": "def add(self, items):\n        '''add a submenu'''\n", "input": "", "output": "        if not isinstance(items, list):\n            items = [items]\n        for m in items:\n            updated = False\n            for i in range(len(self.items)):\n                if self.items[i].name == m.name:\n                    self.items[i] = m\n                    updated = True\n            if not updated:\n                self.items.append(m)", "category": "Python"}, {"instruction": "def get_aliases(self):\n        \"\"\"\n        RETURN LIST OF {\"alias\":a, \"index\":i} PAIRS\n        ALL INDEXES INCLUDED, EVEN IF NO ALIAS {\"alias\":Null}\n        \"\"\"\n", "input": "", "output": "        for index, desc in self.get_metadata().indices.items():\n            if not desc[\"aliases\"]:\n                yield wrap({\"index\": index})\n            elif desc['aliases'][0] == index:\n                Log.error(\"should not happen\")\n            else:\n                for a in desc[\"aliases\"]:\n                    yield wrap({\"index\": index, \"alias\": a})", "category": "Python"}, {"instruction": "def quat_to_rotation_matrix(q):\n    \"\"\"Convert unit quaternion to rotation matrix\n    \n    Parameters\n    -------------\n    q : (4,) ndarray\n            Unit quaternion, scalar as first element\n\n    Returns\n    ----------------\n    R : (3,3) ndarray\n            Rotation matrix\n    \n    \"\"\"\n", "input": "", "output": "    q = q.flatten()\n    assert q.size == 4\n    assert_almost_equal(np.linalg.norm(q), 1.0, err_msg=\"Not a unit quaternion!\")\n    qq = q ** 2\n    R = np.array([[qq[0] + qq[1] - qq[2] - qq[3], 2*q[1]*q[2] -\n2*q[0]*q[3], 2*q[1]*q[3] + 2*q[0]*q[2]],\n                [2*q[1]*q[2] + 2*q[0]*q[3], qq[0] - qq[1] + qq[2] -\nqq[3], 2*q[2]*q[3] - 2*q[0]*q[1]],\n                [2*q[1]*q[3] - 2*q[0]*q[2], 2*q[2]*q[3] + 2*q[0]*q[1],\nqq[0] - qq[1] - qq[2] + qq[3]]])\n    return R", "category": "Python"}, {"instruction": "def file():\n        \"\"\" Grammar for files found in the overall input files.\t\"\"\"\n", "input": "", "output": "        return (\n            Optional(Word(alphanums).setResultsName('alias') +\n                Suppress(Literal('.'))) + Suppress(White()) +\n            Word(approved_printables).setResultsName('filename')\n            )", "category": "Python"}, {"instruction": "def add_dr( self, dr ):\n        \"\"\"\n        Add an observed interatomic distance to the g(r) data at dr.\n\n        Args:\n            dr (Float): the interatomic distance, dr.\n\n        Returns:\n            None\n        \"\"\"\n", "input": "", "output": "        this_bin = int( dr / self.dr ) \n        if this_bin > self.number_of_bins:\n            raise IndexError( 'dr is larger than rdf max_r' )\n        self.data[ this_bin ] += 1", "category": "Python"}, {"instruction": "def show(self, args: argparse.Namespace, parameter: str = '') -> None:\n        \"\"\"Shows current settings of parameters.\n\n        :param args: argparse parsed arguments from the set command\n        :param parameter: optional search parameter\n        \"\"\"\n", "input": "", "output": "        param = utils.norm_fold(parameter.strip())\n        result = {}\n        maxlen = 0\n\n        for p in self.settable:\n            if (not param) or p.startswith(param):\n                result[p] = '{}: {}'.format(p, str(getattr(self, p)))\n                maxlen = max(maxlen, len(result[p]))\n\n        if result:\n            for p in sorted(result):\n                if args.long:\n                    self.poutput('{} # {}'.format(result[p].ljust(maxlen), self.settable[p]))\n                else:\n                    self.poutput(result[p])\n\n            # If user has requested to see all settings, also show read-only settings\n            if args.all:\n                self.poutput('\\nRead only settings:{}'.format(self.cmdenvironment()))\n        else:\n            self.perror(\"Parameter '{}' not supported (type 'set' for list of parameters).\".format(param),\n                        traceback_war=False)", "category": "Python"}, {"instruction": "def _get_rule_port_ranges(rule):\n        \"\"\" Extracts ports ranges from the NSG rule object\n            Returns an array of PortsRange tuples\n        \"\"\"\n", "input": "", "output": "        properties = rule['properties']\n        if 'destinationPortRange' in properties:\n            return [PortsRangeHelper._get_port_range(properties['destinationPortRange'])]\n        else:\n            return [PortsRangeHelper._get_port_range(r)\n                    for r in properties['destinationPortRanges']]", "category": "Python"}, {"instruction": "def check_diag(self, jac, name):\n        \"\"\"\n        Check matrix ``jac`` for diagonal elements that equals 0\n        \"\"\"\n", "input": "", "output": "        system = self.system\n        pos = []\n        names = []\n        pairs = ''\n        size = jac.size\n        diag = jac[0:size[0] ** 2:size[0] + 1]\n\n        for idx in range(size[0]):\n            if abs(diag[idx]) <= 1e-8:\n                pos.append(idx)\n\n        for idx in pos:\n            names.append(system.varname.__dict__[name][idx])\n\n        if len(names) > 0:\n            for i, j in zip(pos, names):\n                pairs += '{0}: {1}\\n'.format(i, j)\n            logger.debug('Jacobian diagonal check:')\n            logger.debug(pairs)", "category": "Python"}, {"instruction": "def independent_get_coefficients(coef, rhouv, s, i, j, k, u, v,\n                                 unfolding, matrix_form):\n    r\"\"\"Get the indices mu, nu, and term coefficients for linear terms.\n\n    >>> from fast.symbolic import define_density_matrix\n    >>> Ne = 2\n    >>> coef = 1+2j\n    >>> rhouv = define_density_matrix(Ne)[1, 1]\n    >>> s, i, j, k, u, v = (1, 1, 0, 1, 1, 1)\n    >>> unfolding = Unfolding(Ne, real=True, normalized=True)\n\n    >>> independent_get_coefficients(coef, rhouv, s, i, j, k, u, v,\n    ...                              unfolding, False)\n    [[1, None, -2.00000000000000, False, False]]\n\n    \"\"\"\n", "input": "", "output": "    if matrix_form:\n        coef = -coef\n    Mu = unfolding.Mu\n    mu = Mu(s, i, j)\n    rhouv_isconjugated = False\n    if s == 1:\n        coef_list = [[mu, None, -im(coef), matrix_form, rhouv_isconjugated]]\n    elif s == -1:\n        coef_list = [[mu, None, re(coef), matrix_form, rhouv_isconjugated]]\n    else:\n        coef_list = [[mu, None, coef, matrix_form, rhouv_isconjugated]]\n    return coef_list", "category": "Python"}, {"instruction": "def __prepare_histogram(h1, h2):\n    \"\"\"Convert the histograms to scipy.ndarrays if required.\"\"\"\n", "input": "", "output": "    h1 = h1 if scipy.ndarray == type(h1) else scipy.asarray(h1)\n    h2 = h2 if scipy.ndarray == type(h2) else scipy.asarray(h2)\n    if h1.shape != h2.shape or h1.size != h2.size:\n        raise ValueError('h1 and h2 must be of same shape and size')\n    return h1, h2", "category": "Python"}, {"instruction": "def set_environment(self, environment):\n        \"\"\"Set the environment for all nodes.\"\"\"\n", "input": "", "output": "        todo = deque([self])\n        while todo:\n            node = todo.popleft()\n            node.environment = environment\n            todo.extend(node.iter_child_nodes())\n        return self", "category": "Python"}, {"instruction": "def channel_names(self) -> tuple:\n        \"\"\"Channel names.\"\"\"\n", "input": "", "output": "        if \"channel_names\" not in self.attrs.keys():\n            self.attrs[\"channel_names\"] = np.array([], dtype=\"S\")\n        return tuple(s.decode() for s in self.attrs[\"channel_names\"])", "category": "Python"}, {"instruction": "def addPixmap(self, pixmap):\r\n        \"\"\"\r\n        Adds the pixmap to the list for this slider.\r\n        \r\n        :param      pixmap | <QPixmap> || <str>\r\n        \"\"\"\n", "input": "", "output": "        scene = self.scene()\r\n        scene.addItem(XImageItem(pixmap))\r\n        self.recalculate()", "category": "Python"}, {"instruction": "def encodeMsg(self, mesg):\n        '''Get byts for a message'''\n", "input": "", "output": "\n        fmt = self.locs.get('log:fmt')\n        if fmt == 'jsonl':\n            s = json.dumps(mesg, sort_keys=True) + '\\n'\n            buf = s.encode()\n            return buf\n\n        elif fmt == 'mpk':\n            buf = s_msgpack.en(mesg)\n            return buf\n\n        mesg = f'Unknown encoding format: {fmt}'\n        raise s_exc.SynErr(mesg=mesg)", "category": "Python"}, {"instruction": "def neg(x, context=None):\n    \"\"\"\n    Return -x.\n\n    \"\"\"\n", "input": "", "output": "    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_neg,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "category": "Python"}, {"instruction": "def initialize_private_canvas(self, private_canvas):\n        \"\"\"Initialize the private canvas used by this instance.\n        \"\"\"\n", "input": "", "output": "        if self.t_.get('show_pan_position', False):\n            self.show_pan_mark(True)\n\n        if self.t_.get('show_focus_indicator', False):\n            self.show_focus_indicator(True)", "category": "Python"}, {"instruction": "def set_tag(self, namespace, repository, tag, image_id):\n        \"\"\"PUT /v1/repositories/(namespace)/(repository)/tags/(tag*)\"\"\"\n", "input": "", "output": "        return self._http_call(self.TAGS + '/' + tag, put, data=image_id,\n                               namespace=namespace, repository=repository)", "category": "Python"}, {"instruction": "def officers(self, num, **kwargs):\n        \"\"\"Search for a company's registered officers by company number.\n\n        Args:\n          num (str): Company number to search on.\n          kwargs (dict): additional keywords passed into\n            requests.session.get *params* keyword.\n        \"\"\"\n", "input": "", "output": "        baseuri = self._BASE_URI + \"company/{}/officers\".format(num)\n        res = self.session.get(baseuri, params=kwargs)\n        self.handle_http_error(res)\n        return res", "category": "Python"}, {"instruction": "def get_settings():\n    '''\n    Get all currently loaded settings.\n    '''\n", "input": "", "output": "    settings = {}\n    for config_file in config_files():\n        config_contents = load_config(config_file)\n        if config_contents is not None:\n            settings = deep_merge(settings, config_contents)\n    return settings", "category": "Python"}, {"instruction": "def get_random_connection(self):\n        \"\"\"\n        Open new connection to random redis server.\n        \"\"\"\n", "input": "", "output": "        if self._available_connections:\n            node_name = random.choice(list(self._available_connections.keys()))\n            conn_list = self._available_connections[node_name]\n            # check it in case of empty connection list\n            if conn_list:\n                return conn_list.pop()\n        for node in self.nodes.random_startup_node_iter():\n            connection = self.get_connection_by_node(node)\n\n            if connection:\n                return connection\n\n        raise Exception(\"Cant reach a single startup node.\")", "category": "Python"}, {"instruction": "def decode_kempressed(bytestring):\n  \"\"\"subvol not bytestring since numpy conversion is done inside fpzip extension.\"\"\"\n", "input": "", "output": "  subvol = fpzip.decompress(bytestring, order='F')\n  return np.swapaxes(subvol, 3,2) - 2.0", "category": "Python"}, {"instruction": "def get_groups(self, limit=100, offset=0):\n        \"\"\"\n        Get all groups from your current team\n        \"\"\"\n", "input": "", "output": "        url = self.TEAM_GROUPS_URL + \"?limit=%s&offset=%s\" % (limit, offset)\n\n        connection = Connection(self.token)\n        connection.set_url(self.production, url)\n\n        return connection.get_request()", "category": "Python"}, {"instruction": "def _verify_cartesian(s, t):\n        \"\"\"Verifies that a point is in the reference triangle.\n\n        I.e., checks that they sum to <= one and are each non-negative.\n\n        Args:\n            s (float): Parameter along the reference triangle.\n            t (float): Parameter along the reference triangle.\n\n        Raises:\n            ValueError: If the point lies outside the reference triangle.\n        \"\"\"\n", "input": "", "output": "        if s < 0.0 or t < 0.0 or s + t > 1.0:\n            raise ValueError(\"Point lies outside reference triangle\", s, t)", "category": "Python"}, {"instruction": "def get(cls, filter=None, **kwargs):\n        \"\"\"\n        Returns a Document if any document is filtered, returns None otherwise\n        \"\"\"\n", "input": "", "output": "        document = cls(cls.find_one(filter, **kwargs))\n        return document if document.document else None", "category": "Python"}, {"instruction": "def get_parameter_tbl(self, parameter):\n        \"\"\"\n        This method returns parameters as list of dict in case of table type\n        parameter\n        \"\"\"\n", "input": "", "output": "        par = []\n        for entry in parameter.findall('Entry'):\n            instance = defaultdict(list)\n            instance['Instance'] = entry.find('Instance').text.split()\n            if entry.find('ProbTable') is None:\n                instance['ValueTable'] = entry.find('ValueTable').text.split()\n            else:\n                instance['ProbTable'] = entry.find('ProbTable').text.split()\n            par.append(instance)\n        return par", "category": "Python"}, {"instruction": "def _update_limits_from_api(self):\n        \"\"\"\n        Query Lambda's DescribeLimits API action, and update limits\n        with the quotas returned. Updates ``self.limits``.\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"Updating limits for Lambda from the AWS API\")\n        if len(self.limits) == 2:\n            return\n        self.connect()\n        lims = self.conn.get_account_settings()['AccountLimit']\n        self.limits['Total Code Size (MiB)']._set_api_limit(\n            (lims['TotalCodeSize']/1048576))\n        self.limits['Code Size Unzipped (MiB) per Function']._set_api_limit(\n            (lims['CodeSizeUnzipped']/1048576))\n        self.limits['Unreserved Concurrent Executions']._set_api_limit(\n            lims['UnreservedConcurrentExecutions'])\n        self.limits['Concurrent Executions']._set_api_limit(\n            lims['ConcurrentExecutions'])\n        self.limits['Code Size Zipped (MiB) per Function']._set_api_limit(\n            (lims['CodeSizeZipped']/1048576))", "category": "Python"}, {"instruction": "def _serie_format(self, serie, value):\n        \"\"\"Format an independent value for the serie\"\"\"\n", "input": "", "output": "\n        kwargs = {'chart': self, 'serie': serie, 'index': None}\n        formatter = (serie.formatter or self.formatter or self._value_format)\n        kwargs = filter_kwargs(formatter, kwargs)\n        return formatter(value, **kwargs)", "category": "Python"}, {"instruction": "def get_saml_provider(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    Get SAML provider document.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.get_saml_provider arn\n    '''\n", "input": "", "output": "    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        provider = conn.get_saml_provider(name)\n        return provider['get_saml_provider_response']['get_saml_provider_result']['saml_metadata_document']\n    except boto.exception.BotoServerError as e:\n        log.debug(__utils__['boto.get_error'](e))\n        log.error('Failed to get SAML provider document %s.', name)\n        return False", "category": "Python"}, {"instruction": "def flipped(self, x=False, y=True):\n        \"\"\"Return a Rect with the same bounds but with axes inverted\n\n        Parameters\n        ----------\n        x : bool\n            Flip the X axis.\n        y : bool\n            Flip the Y axis.\n\n        Returns\n        -------\n        rect : instance of Rect\n            The flipped rectangle.\n        \"\"\"\n", "input": "", "output": "        pos = list(self.pos)\n        size = list(self.size)\n        for i, flip in enumerate((x, y)):\n            if flip:\n                pos[i] += size[i]\n                size[i] *= -1\n        return Rect(pos, size)", "category": "Python"}, {"instruction": "def remove_BC(self, pores=None):\n        r\"\"\"\n        Removes all boundary conditions from the specified pores\n\n        Parameters\n        ----------\n        pores : array_like\n            The pores from which boundary conditions are to be removed.  If no\n            pores are specified, then BCs are removed from all pores. No error\n            is thrown if the provided pores do not have any BCs assigned.\n        \"\"\"\n", "input": "", "output": "        if pores is None:\n            pores = self.Ps\n        if 'pore.bc_value' in self.keys():\n            self['pore.bc_value'][pores] = np.nan\n        if 'pore.bc_rate' in self.keys():\n            self['pore.bc_rate'][pores] = np.nan", "category": "Python"}, {"instruction": "def reset(self):\n        \"\"\"Reset coincidence counter\"\"\"\n", "input": "", "output": "        self.counts = defaultdict(partial(np.zeros, (465, self.tmax * 2 + 1)))\n        self.n_timeslices = defaultdict(int)", "category": "Python"}, {"instruction": "def _smart_round(figure, width=10, max_decimal=4):\n    \"\"\"Round large numbers as integers, smaller numbers as decimals.\"\"\"\n", "input": "", "output": "    n_digits = len(str(int(figure)))\n    n_decimal = width - (n_digits + 1)\n    if n_decimal <= 1:\n        return str(int(figure))\n    else:\n        n_decimal = min(n_decimal, max_decimal)\n        format_str = \"%.\" + str(n_decimal) + \"f\"\n        return format_str % figure", "category": "Python"}, {"instruction": "def outbox(self):\n        \"\"\" :class:`Outbox feed <pypump.models.feed.Outbox>` with all\n        :class:`activities <pypump.models.activity.Activity>` sent by the person.\n\n        Example:\n            >>> for activity in pump.me.outbox[:2]:\n            ...     print(activity)\n            ...\n            pypumptest2 unliked a comment in reply to a note\n            pypumptest2 deleted a note\n        \"\"\"\n", "input": "", "output": "        if self._outbox is None:\n            self._outbox = Outbox(self.links['activity-outbox'], pypump=self._pump)\n        return self._outbox", "category": "Python"}, {"instruction": "def edit_profile():\n    \"\"\"Updates a profile\"\"\"\n", "input": "", "output": "    if g.user is None:\n        abort(401)\n    form = dict(name=g.user.name, email=g.user.email)\n    if request.method == 'POST':\n        if 'delete' in request.form:\n            db_session.delete(g.user)\n            db_session.commit()\n            session['openid'] = None\n            flash(u'Profile deleted')\n            return redirect(url_for('index'))\n        form['name'] = request.form['name']\n        form['email'] = request.form['email']\n        if not form['name']:\n            flash(u'Error: you have to provide a name')\n        elif '@' not in form['email']:\n            flash(u'Error: you have to enter a valid email address')\n        else:\n            flash(u'Profile successfully created')\n            g.user.name = form['name']\n            g.user.email = form['email']\n            db_session.commit()\n            return redirect(url_for('edit_profile'))\n    return render_template('edit_profile.html', form=form)", "category": "Python"}, {"instruction": "def fixup_version(destination, version):\n    \"\"\"Newer releases of the SDK do not have the version number set correctly\n    in the VERSION file. Fix it up.\"\"\"\n", "input": "", "output": "    version_path = os.path.join(\n        destination, 'google_appengine', 'VERSION')\n\n    with open(version_path, 'r') as f:\n        version_data = f.read()\n\n    version_data = version_data.replace(\n        'release: \"0.0.0\"',\n        'release: \"{}\"'.format('.'.join(str(x) for x in version)))\n\n    with open(version_path, 'w') as f:\n        f.write(version_data)", "category": "Python"}, {"instruction": "def patch_discriminator(x, filters=64, filter_size=5, n=4,\n                        name=\"patch_discrim\"):\n  \"\"\"Patch descriminator.\"\"\"\n", "input": "", "output": "  with tf.variable_scope(name):\n    x_shape = shape_list(x)\n    spatial_dims = [x_shape[1] // 4, x_shape[2] // 4]\n    x = tf.random_crop(x, [x_shape[0]] + spatial_dims + [x_shape[3]])\n    for i in range(n):\n      x = general_conv(\n          x=x,\n          num_filters=filters * 2**i,\n          filter_size=filter_size,\n          stride=2 if i != n - 1 else 1,\n          stddev=0.02,\n          padding=\"SAME\",\n          name=\"c%d\" % i,\n          do_norm=\"instance\" if i != 0 else False,\n          do_relu=i != n - 1,\n          relufactor=0.2)\n    x = tf.reduce_mean(x, [1, 2])\n    return x", "category": "Python"}, {"instruction": "def make_list_threads_message(self, py_db, seq):\n        \"\"\" returns thread listing as XML \"\"\"\n", "input": "", "output": "        try:\n            threads = get_non_pydevd_threads()\n            cmd_text = [\"<xml>\"]\n            append = cmd_text.append\n            for thread in threads:\n                if is_thread_alive(thread):\n                    append(self._thread_to_xml(thread))\n            append(\"</xml>\")\n            return NetCommand(CMD_RETURN, seq, ''.join(cmd_text))\n        except:\n            return self.make_error_message(seq, get_exception_traceback_str())", "category": "Python"}, {"instruction": "def toRoman(num):\n    \"\"\"convert integer to Roman numeral\"\"\"\n", "input": "", "output": "    if not 0 < num < 5000:\n        raise ValueError(\"number %n out of range (must be 1..4999)\", num)\n    if int(num) != num:\n        raise TypeError(\"decimals %n can not be converted\", num)\n\n    result = \"\"\n    for numeral, integer in romanNumeralMap:\n        while num >= integer:\n            result += numeral\n            num -= integer\n    return result", "category": "Python"}, {"instruction": "def install_global_objects(self):\n        \"\"\"\n        Process [GLOBAL_OBJECTS], and inject all object to uliweb module, so\n        user can import from uliweb\n        \"\"\"\n", "input": "", "output": "        import uliweb\n        for k, v in settings.GLOBAL_OBJECTS.items():\n            setattr(uliweb, k, import_attr(v))", "category": "Python"}, {"instruction": "def on_security_data_node(self, node):\n        \"\"\"process a securityData node - FIXME: currently not handling relateDate node \"\"\"\n", "input": "", "output": "        sid = XmlHelper.get_child_value(node, 'security')\n        farr = node.getElement('fieldData')\n        dmap = defaultdict(list)\n        for i in range(farr.numValues()):\n            pt = farr.getValue(i)\n            [dmap[f].append(XmlHelper.get_child_value(pt, f, allow_missing=1)) for f in ['date'] + self.fields]\n\n        if not dmap:\n            frame = pd.DataFrame(columns=self.fields)\n        else:\n            idx = dmap.pop('date')\n            frame = pd.DataFrame(dmap, columns=self.fields, index=idx)\n            frame.index.name = 'date'\n        self.response.on_security_complete(sid, frame)", "category": "Python"}, {"instruction": "def sample(self, iter, length=None, verbose=0):\n        \"\"\"\n        Draws iter samples from the posterior.\n        \"\"\"\n", "input": "", "output": "        self._cur_trace_index = 0\n        self.max_trace_length = iter\n        self._iter = iter\n        self.verbose = verbose or 0\n        self.seed()\n\n        # Assign Trace instances to tallyable objects.\n        self.db.connect_model(self)\n\n        # Initialize database -> initialize traces.\n        if length is None:\n            length = iter\n        self.db._initialize(self._funs_to_tally, length)\n\n        # Put traces on objects\n        for v in self._variables_to_tally:\n            v.trace = self.db._traces[v.__name__]\n\n        # Loop\n        self._current_iter = 0\n        self._loop()\n        self._finalize()", "category": "Python"}, {"instruction": "def get_queryset(self):\n        \"\"\"\n        Fixes get_query_set vs get_queryset for Django <1.6\n        \"\"\"\n", "input": "", "output": "        try:\n            qs = super(UserManager, self).get_queryset()\n        except AttributeError:  # pragma: no cover\n            qs = super(UserManager, self).get_query_set()\n        return qs", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Close subscription.\n        \"\"\"\n", "input": "", "output": "        if self._S is not None:\n            # after .close() self._event should never be called\n            self._S.close()\n            # wait for Cancelled to be delivered\n            self._evt.wait()\n            self._S = None", "category": "Python"}, {"instruction": "def convert(self, format=\"png\", **kwargs):\n        \"\"\"\n        png, ps, pdf, gif, jpg, svg\n        returns image in format as bytes\n        \"\"\"\n", "input": "", "output": "        if format.upper() in cairosvg.SURFACES:\n            surface = cairosvg.SURFACES[format.upper()]\n        else:\n            raise Exception(\"'%s' image format unavailable: use one of %s\" %\n                            (format.upper(), list(cairosvg.SURFACES.keys())))\n        return surface.convert(bytestring=str(self), **kwargs)", "category": "Python"}, {"instruction": "def _get_magnitude_term(self, C, mag):\n        \"\"\"\n        Returns the magnitude scaling term - equation 3\n        \"\"\"\n", "input": "", "output": "        if mag >= self.CONSTS[\"Mh\"]:\n            return C[\"e1\"] + C[\"b3\"] * (mag - self.CONSTS[\"Mh\"])\n        else:\n            return C[\"e1\"] + (C[\"b1\"] * (mag - self.CONSTS[\"Mh\"])) +\\\n                (C[\"b2\"] * (mag - self.CONSTS[\"Mh\"]) ** 2.)", "category": "Python"}, {"instruction": "def read_saved_screenshot_to_array(self, screen_id, bitmap_format):\n        \"\"\"Screenshot in requested format is retrieved to an array of bytes.\n\n        in screen_id of type int\n            Saved guest screen to read from.\n\n        in bitmap_format of type :class:`BitmapFormat`\n            The requested format.\n\n        out width of type int\n            Image width.\n\n        out height of type int\n            Image height.\n\n        return data of type str\n            Array with resulting image data.\n\n        \"\"\"\n", "input": "", "output": "        if not isinstance(screen_id, baseinteger):\n            raise TypeError(\"screen_id can only be an instance of type baseinteger\")\n        if not isinstance(bitmap_format, BitmapFormat):\n            raise TypeError(\"bitmap_format can only be an instance of type BitmapFormat\")\n        (data, width, height) = self._call(\"readSavedScreenshotToArray\",\n                     in_p=[screen_id, bitmap_format])\n        return (data, width, height)", "category": "Python"}, {"instruction": "def should_skip_middleware(self, middleware, matching, rest) -> bool:\n        \"\"\"\n        Returns True (i.e. should skip) if request does not match the\n        entire middleware path.\n        This is a simple check if 'rest' is truthy or not.\n        \"\"\"\n", "input": "", "output": "        return bool(not matching) or bool(rest)", "category": "Python"}, {"instruction": "def create_osd_keyring(conn, cluster, key):\n    \"\"\"\n    Run on osd node, writes the bootstrap key if not there yet.\n    \"\"\"\n", "input": "", "output": "    logger = conn.logger\n    path = '/var/lib/ceph/bootstrap-osd/{cluster}.keyring'.format(\n        cluster=cluster,\n    )\n    if not conn.remote_module.path_exists(path):\n        logger.warning('osd keyring does not exist yet, creating one')\n        conn.remote_module.write_keyring(path, key)", "category": "Python"}, {"instruction": "def get_selected_state(self):\n        \"\"\"Returns the current selected state\n        \"\"\"\n", "input": "", "output": "        form_key = \"{}_review_state\".format(self.form_id)\n        return self.request.get(form_key, \"default\")", "category": "Python"}, {"instruction": "def _get_image_numpy_dtype(self):\n        \"\"\"\n        Get the numpy dtype for the image\n        \"\"\"\n", "input": "", "output": "        try:\n            ftype = self._info['img_equiv_type']\n            npy_type = _image_bitpix2npy[ftype]\n        except KeyError:\n            raise KeyError(\"unsupported fits data type: %d\" % ftype)\n\n        return npy_type", "category": "Python"}, {"instruction": "def GetNotation(self, id, type):\n        '''\n        method which searches for notation from <type> list at position <id>\n        :param id: the number to look for - i.e if you're looking for the first one in wrap notation, id will be 0\n        :param type: post, pre or wrap\n        :return: the notation class searched for or none\n        '''\n", "input": "", "output": "        if type == \"post\":\n            if (id == -\n                1 and len(self.postnotation) > 0) or (id != -\n                                                      1 and len(self.postnotation) > id):\n                return self.postnotation[id]\n        if type == \"pre\":\n            if (id == -\n                1 and len(self.prenotation) > 0) or (id != -\n                                                     1 and len(self.postnotation) > id):\n                return self.prenotation[id]\n        if type == \"wrap\":\n            if (id == -\n                1 and len(self.wrap_notation) > 0) or (id != -\n                                                       1 and len(self.postnotation) > id):\n                return self.wrap_notation[id]", "category": "Python"}, {"instruction": "def delete_db_instance(self, dbid):\n        ''' Delete DB '''\n", "input": "", "output": "        if not self.connect_to_aws_rds():\n            return False\n        try:\n            database = self.rdsc.delete_dbinstance(dbid,\n                                                   skip_final_snapshot=True)\n            print database\n        except:\n            return False\n        else:\n            return True", "category": "Python"}, {"instruction": "def _handle_response(self, response):\n        \"\"\"\n        Handle the HTTP response by memoing the headers and then delivering\n        bytes.\n        \"\"\"\n", "input": "", "output": "        self.client.status = response.code\n        self.response_headers = response.headers\n        # XXX This workaround (which needs to be improved at that) for possible\n        # bug in Twisted with new client:\n        # http://twistedmatrix.com/trac/ticket/5476\n        if self._method.upper() == 'HEAD' or response.code == NO_CONTENT:\n            return succeed('')\n        receiver = self.receiver_factory()\n        receiver.finished = d = Deferred()\n        receiver.content_length = response.length\n        response.deliverBody(receiver)\n        if response.code >= 400:\n            d.addCallback(self._fail_response, response)\n        return d", "category": "Python"}, {"instruction": "def _call_command_in_repo(comm, repo, log, fail=False, log_flag=True):\n    \"\"\"Use `subprocess` to call a command in a certain (repo) directory.\n\n    Logs the output (both `stderr` and `stdout`) to the log, and checks the\n    return codes to make sure they're valid.  Raises error if not.\n\n    Raises\n    ------\n    exception `subprocess.CalledProcessError`: if the command fails\n\n    \"\"\"\n", "input": "", "output": "    if log_flag:\n        log.debug(\"Running '{}'.\".format(\" \".join(comm)))\n    process = subprocess.Popen(\n        comm, cwd=repo, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    (stdout, stderr) = process.communicate()\n    if stderr is not None:\n        err_msg = stderr.decode('ascii').strip().splitlines()\n        for em in err_msg:\n            log.error(em)\n    if stdout is not None:\n        out_msg = stdout.decode('ascii').strip().splitlines()\n        for om in out_msg:\n            log.warning(om)\n    # Raises an error if the command failed.\n    if fail:\n        if process.returncode:\n            raise subprocess.CalledProcessError\n    return", "category": "Python"}, {"instruction": "def Save(self):\n        \"\"\"Saves the current system\"\"\"\n", "input": "", "output": "        # This method is intercepted to allow ui_sync\n        if self._file_to_save_on_Save:\n            self._iopticalsystem.SaveAs(self._file_to_save_on_Save)\n        else:\n            self._iopticalsystem.Save()", "category": "Python"}, {"instruction": "def _compute_vline_scores(self):\n        \"\"\"Does the hard work to prepare ``vline_score``.\n        \"\"\"\n", "input": "", "output": "        M, N, L = self.M, self.N, self.L\n        vline_score = {}\n        for x in range(M):\n            laststart = [0 if (x, 0, 1, k) in self else None for k in range(L)]\n            for y in range(N):\n                block = [0] * (y + 1)\n                for k in range(L):\n                    if (x, y, 1, k) not in self:\n                        laststart[k] = None\n                    elif laststart[k] is None:\n                        laststart[k] = y\n                        block[y] += 1\n                    elif y and (x, y, 1, k) not in self[x, y - 1, 1, k]:\n                        laststart[k] = y\n                    else:\n                        for y1 in range(laststart[k], y + 1):\n                            block[y1] += 1\n                for y1 in range(y + 1):\n                    vline_score[x, y1, y] = block[y1]\n        self._vline_score = vline_score", "category": "Python"}, {"instruction": "def stream_destroy(stream):\n    \"\"\"Wraps openjp2 library function opj_stream_destroy.\n\n    Destroys the stream created by create_stream.\n\n    Parameters\n    ----------\n    stream : STREAM_TYPE_P\n        The file stream.\n    \"\"\"\n", "input": "", "output": "    OPENJP2.opj_stream_destroy.argtypes = [STREAM_TYPE_P]\n    OPENJP2.opj_stream_destroy.restype = ctypes.c_void_p\n    OPENJP2.opj_stream_destroy(stream)", "category": "Python"}, {"instruction": "def triangulate(self):\n        \"\"\"\n        Triangulates the set of vertices and stores the triangles in faces and\n        the convex hull in convex_hull.\n        \"\"\"\n", "input": "", "output": "        npts = self._vertices.shape[0]\n        if np.any(self._vertices[0] != self._vertices[1]):\n            # start != end, so edges must wrap around to beginning.\n            edges = np.empty((npts, 2), dtype=np.uint32)\n            edges[:, 0] = np.arange(npts)\n            edges[:, 1] = edges[:, 0] + 1\n            edges[-1, 1] = 0\n        else:\n            # start == end; no wrapping required.\n            edges = np.empty((npts-1, 2), dtype=np.uint32)\n            edges[:, 0] = np.arange(npts)\n            edges[:, 1] = edges[:, 0] + 1\n\n        tri = Triangulation(self._vertices, edges)\n        tri.triangulate()\n        return tri.pts, tri.tris", "category": "Python"}, {"instruction": "def bootstrap_ts(y, func, B=1000, b=3):\n    \"\"\" Bootstrap a timeseries using a window size:b. \"\"\"\n", "input": "", "output": "    beta_star = np.empty(B)\n    z = y\n    z_star = np.empty(len(z))\n    for boot_i in range(B):\n        for block_i, start in enumerate(np.random.randint(len(z) - b + 1, size=len(z) / b)):\n            z_star[block_i * b:(block_i + 1) * b] = z[start:start + b]\n            beta_star[boot_i] = func(z_star)\n    return beta_star", "category": "Python"}, {"instruction": "def dict(self):\n        \"\"\"\n        calls the overridden method and adds provenance and summary data\n\n        :return: dictionary representation of the metadata\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        metadata = super(ImpactLayerMetadata, self).dict\n\n        metadata['provenance'] = self.provenance\n        metadata['summary_data'] = self.summary_data\n\n        return metadata", "category": "Python"}, {"instruction": "def show_halogen(self):\n        \"\"\"Visualize halogen bonds.\"\"\"\n", "input": "", "output": "        halogen = self.plcomplex.halogen_bonds\n        all_don_x, all_acc_o = [], []\n        for h in halogen:\n            all_don_x.append(h.don_id)\n            all_acc_o.append(h.acc_id)\n            cmd.select('tmp_bs', 'id %i & %s' % (h.acc_id, self.protname))\n            cmd.select('tmp_lig', 'id %i & %s' % (h.don_id, self.ligname))\n\n            cmd.distance('HalogenBonds', 'tmp_bs', 'tmp_lig')\n        if not len(all_acc_o) == 0:\n            self.select_by_ids('HalogenAccept', all_acc_o, restrict=self.protname)\n            self.select_by_ids('HalogenDonor', all_don_x, restrict=self.ligname)\n        if self.object_exists('HalogenBonds'):\n            cmd.set('dash_color', 'greencyan', 'HalogenBonds')", "category": "Python"}, {"instruction": "def on_commit(self, changes):\n        \"\"\"Method that gets called when a model is changed. This serves\n        to do the actual index writing.\n        \"\"\"\n", "input": "", "output": "        if _get_config(self)['enable_indexing'] is False:\n            return None\n\n        for wh in self.whoosheers:\n            if not wh.auto_update:\n                continue\n            writer = None\n            for change in changes:\n                if change[0].__class__ in wh.models:\n                    method_name = '{0}_{1}'.format(change[1], change[0].__class__.__name__.lower())\n                    method = getattr(wh, method_name, None)\n                    if method:\n                        if not writer:\n                            writer = type(self).get_or_create_index(_get_app(self), wh).\\\n                                writer(timeout=_get_config(self)['writer_timeout'])\n                        method(writer, change[0])\n            if writer:\n                writer.commit()", "category": "Python"}, {"instruction": "def productForm(self, request, tag):\n        \"\"\"\n        Render a L{liveform.LiveForm} -- the main purpose of this fragment --\n        which will allow the administrator to endow or deprive existing users\n        using Products.\n        \"\"\"\n", "input": "", "output": "\n        def makeRemover(i):\n            def remover(s3lected):\n                if s3lected:\n                    return self.products[i]\n                return None\n            return remover\n\n        f = liveform.LiveForm(\n            self._endow,\n            [liveform.Parameter(\n                    'products' + str(i),\n                    liveform.FORM_INPUT,\n                    liveform.LiveForm(\n                        makeRemover(i),\n                        [liveform.Parameter(\n                                's3lected',\n                                liveform.RADIO_INPUT,\n                                bool,\n                                repr(p),\n                                )],\n                        '',\n                        ),\n                    )\n             for (i, p)\n             in enumerate(self.products)],\n            self.which.capitalize() + u' ' + self.username)\n        f.setFragmentParent(self)\n        return f", "category": "Python"}, {"instruction": "def read_pseudo_zval(self):\n        \"\"\"\n        Create pseudopotential ZVAL dictionary.\n        \"\"\"\n", "input": "", "output": "        try:\n            def poscar_line(results, match):\n                poscar_line = match.group(1)\n                results.poscar_line = re.findall(r'[A-Z][a-z]?', poscar_line)\n\n            def zvals(results, match):\n                zvals = match.group(1)\n                results.zvals = map(float, re.findall(r'-?\\d+\\.\\d*', zvals))\n\n            search = []\n            search.append([r'^.*POSCAR.*=(.*)', None, poscar_line])\n            search.append([r'^\\s+ZVAL.*=(.*)', None, zvals])\n\n            micro_pyawk(self.filename, search, self)\n\n            zval_dict = {}\n            for x, y in zip(self.poscar_line, self.zvals):\n                zval_dict.update({x: y})\n            self.zval_dict = zval_dict\n\n            # Clean-up\n            del (self.poscar_line)\n            del (self.zvals)\n        except:\n            raise Exception(\"ZVAL dict could not be parsed.\")", "category": "Python"}, {"instruction": "def p_return_expr(p):\n    \"\"\" statement : RETURN expr\n    \"\"\"\n", "input": "", "output": "    if not FUNCTION_LEVEL:  # At less one level\n        syntax_error(p.lineno(1), 'Syntax Error: Returning value out of FUNCTION')\n        p[0] = None\n        return\n\n    if FUNCTION_LEVEL[-1].kind is None:  # This function was not correctly declared.\n        p[0] = None\n        return\n\n    if FUNCTION_LEVEL[-1].kind != KIND.function:\n        syntax_error(p.lineno(1), 'Syntax Error: SUBs cannot return a value')\n        p[0] = None\n        return\n\n    if is_numeric(p[2]) and FUNCTION_LEVEL[-1].type_ == TYPE.string:\n        syntax_error(p.lineno(2), 'Type Error: Function must return a string, not a numeric value')\n        p[0] = None\n        return\n\n    if not is_numeric(p[2]) and FUNCTION_LEVEL[-1].type_ != TYPE.string:\n        syntax_error(p.lineno(2), 'Type Error: Function must return a numeric value, not a string')\n        p[0] = None\n        return\n\n    p[0] = make_sentence('RETURN', FUNCTION_LEVEL[-1],\n                         make_typecast(FUNCTION_LEVEL[-1].type_, p[2],\n                                       p.lineno(1)))", "category": "Python"}, {"instruction": "def cancel(self):\n        \"\"\"Cancel add-on install.\"\"\"\n", "input": "", "output": "        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            self.find_secondary_button().click()", "category": "Python"}, {"instruction": "def _SGraphFromJsonTree(json_str):\n    \"\"\"\n    Convert the Json Tree to SGraph\n    \"\"\"\n", "input": "", "output": "    g = json.loads(json_str)\n    vertices = [_Vertex(x['id'],\n                dict([(str(k), v) for k, v in _six.iteritems(x) if k != 'id']))\n                                                      for x in g['vertices']]\n    edges = [_Edge(x['src'], x['dst'],\n             dict([(str(k), v) for k, v in _six.iteritems(x) if k != 'src' and k != 'dst']))\n                                                      for x in g['edges']]\n    sg = _SGraph().add_vertices(vertices)\n    if len(edges) > 0:\n        sg = sg.add_edges(edges)\n    return sg", "category": "Python"}, {"instruction": "def get_targets(conn=None):\n    \"\"\"\n    get_brain_targets function from Brain.Targets table.\n\n    :return: <generator> yields dict objects\n    \"\"\"\n", "input": "", "output": "    targets = RBT\n    results = targets.run(conn)\n    for item in results:\n        yield item", "category": "Python"}, {"instruction": "def execOnSubArrays(arrs, fn, splitX, splitY):\n    \"\"\"\n    execute a function(on or multiple arrays)\n    only on sub sections\n    works only on 2d arrays at the moment\n\n    >>> a1 = np.ones((1000,1000))\n    >>> a2 = np.ones((1000,1000))\n    >>> out = execOnSubArrays((a1,a2), lambda sa1,sa2: sa1+as2, splitX=10, splitY=10)\n\n    \"\"\"\n", "input": "", "output": "    if type(arrs) not in (tuple, list):\n        arrs = (arrs,)\n    s0, s1 = arrs[0].shape\n    ss0 = s0 // splitX\n    ss1 = s1 // splitY\n    px, py = 0, 0\n    out = None\n    for ix in range(splitX):\n        if ix == splitX - 1:\n            ss0 = s0 - px\n\n        for iy in range(splitY):\n            if iy == splitY - 1:\n                ss1 = s1 - py\n            # current sub arrays:\n            sarrs = [a[px:px + ss0, py:py + ss1] for a in arrs]\n            sub = fn(*tuple(sarrs))\n\n            if out is None:\n                out = np.empty(shape=(s0, s1), dtype=sub.dtype)\n\n            out[px:px + ss0, py:py + ss1] = sub\n\n            py += ss1\n        py = 0\n        px += ss0\n    return out", "category": "Python"}, {"instruction": "def get_watermark_for_regex(\n    kafka_client,\n    topic_regex,\n):\n    \"\"\"This method:\n        * refreshes metadata for the kafka client\n        * fetches watermarks\n\n    :param kafka_client: KafkaToolClient instance\n    :param topic: the topic regex\n    :returns: dict <topic>: [ConsumerPartitionOffsets]\n    \"\"\"\n", "input": "", "output": "    # Refresh client metadata. We do not use the topic list, because we\n    # don't want to accidentally create the topic if it does not exist.\n    # If Kafka is unavailable, let's retry loading client metadata\n    try:\n        kafka_client.load_metadata_for_topics()\n    except KafkaUnavailableError:\n        kafka_client.load_metadata_for_topics()\n\n    topics_to_be_considered = []\n\n    for topic in kafka_client.topic_partitions:\n        if re.search(topic_regex, topic):\n            topics_to_be_considered.append(topic)\n\n    watermarks = get_topics_watermarks(\n        kafka_client, topics_to_be_considered\n    )\n    return watermarks", "category": "Python"}, {"instruction": "def covariance(self,pt0,pt1):\n        \"\"\" get the covarince between two points implied by Vario2d\n\n        Parameters\n        ----------\n        pt0 : (iterable of len 2)\n            first point x and y\n        pt1 : (iterable of len 2)\n            second point x and y\n\n        Returns\n        -------\n        cov : float\n            covariance between pt0 and pt1\n\n        \"\"\"\n", "input": "", "output": "\n        x = np.array([pt0[0],pt1[0]])\n        y = np.array([pt0[1],pt1[1]])\n        names = [\"n1\",\"n2\"]\n        return self.covariance_matrix(x,y,names=names).x[0,1]", "category": "Python"}, {"instruction": "def visible_fields(self):\n        \"\"\"\n        Returns the reduced set of visible fields to output from the form.\n\n        This method respects the provided ``fields`` configuration _and_ exlcudes\n        all fields from the ``exclude`` configuration.\n\n        If no ``fields`` where provided when configuring this fieldset, all visible\n        fields minus the excluded fields will be returned.\n\n        :return: List of bound field instances or empty tuple.\n        \"\"\"\n", "input": "", "output": "\n        form_visible_fields = self.form.visible_fields()\n\n        if self.render_fields:\n            fields = self.render_fields\n        else:\n            fields = [field.name for field in form_visible_fields]\n\n        filtered_fields = [field for field in fields if field not in self.exclude_fields]\n        return [field for field in form_visible_fields if field.name in filtered_fields]", "category": "Python"}, {"instruction": "def dict_diff(d1, d2, no_key='<KEYNOTFOUND>'):\n    # type: (DictUpperBound, DictUpperBound, str) -> Dict\n    \"\"\"Compares two dictionaries\n\n    Args:\n        d1 (DictUpperBound): First dictionary to compare\n        d2 (DictUpperBound): Second dictionary to compare\n        no_key (str): What value to use if key is not found Defaults to '<KEYNOTFOUND>'.\n\n    Returns:\n        Dict: Comparison dictionary\n\n    \"\"\"\n", "input": "", "output": "    d1keys = set(d1.keys())\n    d2keys = set(d2.keys())\n    both = d1keys & d2keys\n    diff = {k: (d1[k], d2[k]) for k in both if d1[k] != d2[k]}\n    diff.update({k: (d1[k], no_key) for k in d1keys - both})\n    diff.update({k: (no_key, d2[k]) for k in d2keys - both})\n    return diff", "category": "Python"}, {"instruction": "def move_entry(self, entry = None, group = None):\n        \"\"\"Move an entry to another group.\n\n        A v1Group group and a v1Entry entry are needed.\n\n        \"\"\"\n", "input": "", "output": "\n        if entry is None or group is None or type(entry) is not v1Entry or \\\n            type(group) is not v1Group:\n            raise KPError(\"Need an entry and a group.\")\n        elif entry not in self.entries:\n            raise KPError(\"No entry found.\")\n        elif group in self.groups:\n            entry.group.entries.remove(entry)\n            group.entries.append(entry)\n            entry.group_id = group.id_\n            entry.group = group\n            return True\n        else:\n            raise KPError(\"No group found.\")", "category": "Python"}, {"instruction": "def get_ancestor_processes():\n    \"\"\"Get a list of the executables of all ancestor processes.\"\"\"\n", "input": "", "output": "    if not _ANCESTOR_PROCESSES and psutil is not None:\n        proc = psutil.Process(os.getpid())\n        while proc.parent() is not None:\n            try:\n                _ANCESTOR_PROCESSES.append(proc.parent().exe())\n                proc = proc.parent()\n            except psutil.Error:\n                break\n    return _ANCESTOR_PROCESSES", "category": "Python"}, {"instruction": "def sign(payload, keypair):\n    \"\"\"Return a JWS-JS format signature given a JSON-serializable payload and\n    an Ed25519 keypair.\"\"\"\n", "input": "", "output": "    get_ed25519ll()\n    #\n    header = {\n                \"alg\": ALG,\n                \"jwk\": {\n                    \"kty\": ALG,  # alg -> kty in jwk-08.\n                    \"vk\": native(urlsafe_b64encode(keypair.vk))\n                }\n             }\n\n    encoded_header = urlsafe_b64encode(binary(json.dumps(header, sort_keys=True)))\n    encoded_payload = urlsafe_b64encode(binary(json.dumps(payload, sort_keys=True)))\n    secured_input = b\".\".join((encoded_header, encoded_payload))\n    sig_msg = ed25519ll.crypto_sign(secured_input, keypair.sk)\n    signature = sig_msg[:ed25519ll.SIGNATUREBYTES]\n    encoded_signature = urlsafe_b64encode(signature)\n\n    return {\"recipients\":\n            [{\"header\": native(encoded_header),\n              \"signature\": native(encoded_signature)}],\n            \"payload\": native(encoded_payload)}", "category": "Python"}, {"instruction": "def reset(target, settings):\n    \"\"\"\n    \u91cd\u7f6e\u8bbe\u7f6e\n    :param target:\n    :param settings:\n    :return:\n    \"\"\"\n", "input": "", "output": "    target_settings = _import_module(target)\n    for k, v in settings.items():\n        if hasattr(target_settings, k):\n            setattr(target_settings, k, _get_value(getattr(target_settings, k), v))\n        else:\n            logger.debug('AttributeError {target} has no attribute {k}'.format(target=target, k=k))\n    return target_settings", "category": "Python"}, {"instruction": "def dump_queue(queue):\n        \"\"\"\n        Empties all pending items in a queue and returns them in a list.\n        \"\"\"\n", "input": "", "output": "        result = []\n\n        try:\n            while True:\n                item = queue.get_nowait()\n                result.append(item)\n        except: Empty\n\n        return result", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Closes out the stream.\"\"\"\n", "input": "", "output": "        _LOGGER.debug(\"Closing stream\")\n        if not hasattr(self, \"footer\"):\n            raise SerializationError(\"Footer not read\")\n        super(StreamDecryptor, self).close()", "category": "Python"}, {"instruction": "def send_message(message, params, site, logger):\n    \"\"\"Send a message to the Sentry server\"\"\"\n", "input": "", "output": "    client.capture(\n        'Message',\n        message=message,\n        params=tuple(params),\n        data={\n            'site': site,\n            'logger': logger,\n        },\n    )", "category": "Python"}, {"instruction": "def create_seq(self, ):\n        \"\"\"Create a sequence and store it in the self.sequence\n\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        name = self.name_le.text()\n        desc = self.desc_pte.toPlainText()\n        try:\n            seq = djadapter.models.Sequence(name=name, project=self._project, description=desc)\n            seq.save()\n            self.sequence = seq\n            self.accept()\n        except:\n            log.exception(\"Could not create new sequence\")", "category": "Python"}, {"instruction": "def loadf(path, encoding=None, model=None, parser=None):\n    \"\"\"Deserialize path (.arpa, .gz) to a Python object.\"\"\"\n", "input": "", "output": "    path = str(path)\n    if path.endswith('.gz'):\n        with gzip.open(path, mode='rt', encoding=encoding) as f:\n            return load(f, model=model, parser=parser)\n    else:\n        with open(path, mode='rt', encoding=encoding) as f:\n            return load(f, model=model, parser=parser)", "category": "Python"}, {"instruction": "def get_free_shipping_promotion_by_id(cls, free_shipping_promotion_id, **kwargs):\n        \"\"\"Find FreeShippingPromotion\n\n        Return single instance of FreeShippingPromotion by its ID.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.get_free_shipping_promotion_by_id(free_shipping_promotion_id, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str free_shipping_promotion_id: ID of freeShippingPromotion to return (required)\n        :return: FreeShippingPromotion\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._get_free_shipping_promotion_by_id_with_http_info(free_shipping_promotion_id, **kwargs)\n        else:\n            (data) = cls._get_free_shipping_promotion_by_id_with_http_info(free_shipping_promotion_id, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def watch_log_for_alive(self, nodes, from_mark=None, timeout=720, filename='system.log'):\n        \"\"\"\n        Watch the log of this node until it detects that the provided other\n        nodes are marked UP. This method works similarly to watch_log_for_death.\n\n        We want to provide a higher default timeout when this is called on DSE.\n        \"\"\"\n", "input": "", "output": "        super(DseNode, self).watch_log_for_alive(nodes, from_mark=from_mark, timeout=timeout, filename=filename)", "category": "Python"}, {"instruction": "def filter(array, predicates, ty=None):\n    \"\"\"\n    Returns a new array, with each element in the original array satisfying the\n    passed-in predicate set to `new_value`\n\n    Args:\n        array (WeldObject / Numpy.ndarray): Input array\n        predicates (WeldObject / Numpy.ndarray<bool>): Predicate set\n        ty (WeldType): Type of each element in the input array\n\n    Returns:\n        A WeldObject representing this computation\n    \"\"\"\n", "input": "", "output": "    weld_obj = WeldObject(encoder_, decoder_)\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    predicates_var = weld_obj.update(predicates)\n    if isinstance(predicates, WeldObject):\n        predicates_var = predicates.obj_id\n        weld_obj.dependencies[predicates_var] = predicates\n\n    weld_template = ", "category": "Python"}, {"instruction": "def get_polyline(self, id_num, style='google'):\n        \"\"\"Return the polyline of the activity with the given id.\n\n        :param style: The type of polyline to return. May be one of\n                      'google', 'svg', or 'geojson'.\n\n        \"\"\"\n", "input": "", "output": "        parts = ['my', 'activities', id_num, 'polyline']\n        if style != 'google':\n            parts.append(style)\n        url = self._build_url(*parts)\n\n        return self._json(url)", "category": "Python"}, {"instruction": "def filter(self, **kwargs):\n        \"\"\"\n        Returns a new QuerySet instance with the args ANDed to the existing\n        set.\n        \"\"\"\n", "input": "", "output": "        if kwargs:\n            assert self.query.can_filter(), \"Cannot filter a query once a slice has been taken.\"\n\n        clone = self._clone()\n        clone.query.add_filters(**kwargs)\n\n        return clone", "category": "Python"}, {"instruction": "def bootstrap(self, arg=None):\n        '''\n        This only takes args so it can be used as a callback. Don't\n        pass an arg, it is ignored.\n        '''\n", "input": "", "output": "        try:\n            d = self.protocol.add_event_listener(\n                'CONF_CHANGED', self._conf_changed)\n        except RuntimeError:\n            # for Tor versions which don't understand CONF_CHANGED\n            # there's nothing we can really do.\n            log.msg(\n                \"Can't listen for CONF_CHANGED event; won't stay up-to-date \"\n                \"with other clients.\")\n            d = defer.succeed(None)\n        d.addCallback(lambda _: self.protocol.get_info_raw(\"config/names\"))\n        d.addCallback(self._do_setup)\n        d.addCallback(self.do_post_bootstrap)\n        d.addErrback(self.do_post_errback)", "category": "Python"}, {"instruction": "def draw_help(self, surf):\n    \"\"\"Draw the help dialog.\"\"\"\n", "input": "", "output": "    if not self._help:\n      return\n\n    def write(loc, text):\n      surf.write_screen(self._font_large, colors.black, loc, text)\n\n    surf.surf.fill(colors.white * 0.8)\n    write((1, 1), \"Shortcuts:\")\n\n    max_len = max(len(s) for s, _ in self.shortcuts)\n    for i, (hotkey, description) in enumerate(self.shortcuts, start=2):\n      write((2, i), hotkey)\n      write((3 + max_len * 0.7, i), description)", "category": "Python"}, {"instruction": "def read_json_file(fpath):\n    \"\"\"\n    Read a JSON file from ``fpath``; raise an exception if it doesn't exist.\n\n    :param fpath: path to file to read\n    :type fpath: str\n    :return: deserialized JSON\n    :rtype: dict\n    \"\"\"\n", "input": "", "output": "    if not os.path.exists(fpath):\n        raise Exception('ERROR: file %s does not exist.' % fpath)\n    with open(fpath, 'r') as fh:\n        raw = fh.read()\n    res = json.loads(raw)\n    return res", "category": "Python"}, {"instruction": "def _get_gz_ymd(self):\n        \"\"\"\n        (sy, sm, sd) -> term / gz_year / gz_month / gz_day\n        \"\"\"\n", "input": "", "output": "        solar_date = MIN_SOLAR_DATE + datetime.timedelta(days=self._offset)\n        sy, sm, sd = solar_date.year, solar_date.month, solar_date.day\n        s_offset = (datetime.date(sy, sm, sd) - MIN_SOLAR_DATE).days\n        gz_year = TextUtils.STEMS[(self.year - 4) % 10] + TextUtils.BRANCHES[(self.year - 4) % 12]\n        gz_day = TextUtils.get_gz_cn((s_offset + 40) % 60)\n        term_name, next_gz_month = TermUtils.get_term_info(sy, sm, sd)\n        if next_gz_month:\n            gz_month = TextUtils.get_gz_cn((sy - 1900) * 12 + sm + 12)\n        else:\n            gz_month = TextUtils.get_gz_cn((sy - 1900) * 12 + sm + 11)\n        return gz_year, gz_month, gz_day, term_name", "category": "Python"}, {"instruction": "def press(button=LEFT):\n    \"\"\" Sends a down event for the specified button, using the provided constants \"\"\"\n", "input": "", "output": "    location = get_position()\n    button_code, button_down, _, _ = _button_mapping[button]\n    e = Quartz.CGEventCreateMouseEvent(\n        None,\n        button_down,\n        location,\n        button_code)\n\n    # Check if this is a double-click (same location within the last 300ms)\n    if _last_click[\"time\"] is not None and datetime.datetime.now() - _last_click[\"time\"] < datetime.timedelta(seconds=0.3) and _last_click[\"button\"] == button and _last_click[\"position\"] == location:\n        # Repeated Click\n        _last_click[\"click_count\"] = min(3, _last_click[\"click_count\"]+1)\n    else:\n        # Not a double-click - Reset last click\n        _last_click[\"click_count\"] = 1\n    Quartz.CGEventSetIntegerValueField(\n        e,\n        Quartz.kCGMouseEventClickState,\n        _last_click[\"click_count\"])\n    Quartz.CGEventPost(Quartz.kCGHIDEventTap, e)\n    _button_state[button] = True\n    _last_click[\"time\"] = datetime.datetime.now()\n    _last_click[\"button\"] = button\n    _last_click[\"position\"] = location", "category": "Python"}, {"instruction": "def netloc(self):\n        \"\"\"\n        Return the netloc\n        \"\"\"\n", "input": "", "output": "        url = self._tuple\n        if url.username and url.password:\n            netloc = '%s:%s@%s' % (url.username, url.password, url.host)\n        elif url.username and not url.password:\n            netloc = '%s@%s' % (url.username, url.host)\n        else:\n            netloc = url.host\n        if url.port:\n            netloc = '%s:%s' % (netloc, url.port)\n        return netloc", "category": "Python"}, {"instruction": "def convert_surfaces(self, parent, alpha=False):\n        \"\"\" Convert all images in the data to match the parent\n\n        :param parent: pygame.Surface\n        :param alpha: preserve alpha channel or not\n        :return: None\n        \"\"\"\n", "input": "", "output": "        images = list()\n        for i in self.tmx.images:\n            try:\n                if alpha:\n                    images.append(i.convert_alpha(parent))\n                else:\n                    images.append(i.convert(parent))\n            except AttributeError:\n                images.append(None)\n        self.tmx.images = images", "category": "Python"}, {"instruction": "def to_bb(YY, y=\"deprecated\"):\n    \"\"\"Convert mask YY to a bounding box, assumes 0 as background nonzero object\"\"\"\n", "input": "", "output": "    cols,rows = np.nonzero(YY)\n    if len(cols)==0: return np.zeros(4, dtype=np.float32)\n    top_row = np.min(rows)\n    left_col = np.min(cols)\n    bottom_row = np.max(rows)\n    right_col = np.max(cols)\n    return np.array([left_col, top_row, right_col, bottom_row], dtype=np.float32)", "category": "Python"}, {"instruction": "def get_channel_info(self):\n        \"\"\"Get the current channel info.\"\"\"\n", "input": "", "output": "        self.request(EP_GET_CHANNEL_INFO)\n        return {} if self.last_response is None else self.last_response.get('payload')", "category": "Python"}, {"instruction": "def to_skycoord(self, unit=None):\n        \"\"\"Convert this distance to an AstroPy ``SkyCoord`` object.\"\"\"\n", "input": "", "output": "        from astropy.coordinates import SkyCoord\n        from astropy.units import au\n        x, y, z = self.position.au\n        return SkyCoord(representation='cartesian', x=x, y=y, z=z, unit=au)", "category": "Python"}, {"instruction": "def _update_mean_in_window(self):\n        \"\"\"\n        Compute mean in window the slow way. useful for first step.\n\n        Considers all values in window\n\n        See Also\n        --------\n        _add_observation_to_means : fast update of mean for single observation addition\n        _remove_observation_from_means : fast update of mean for single observation removal\n\n        \"\"\"\n", "input": "", "output": "        self._mean_x_in_window = numpy.mean(self._x_in_window)\n        self._mean_y_in_window = numpy.mean(self._y_in_window)", "category": "Python"}, {"instruction": "def showDiffResults(self):\n        \"\"\"\n        Show results when diff option on.\n        \"\"\"\n", "input": "", "output": "        try:\n            oldWarnings = self.parseWarnings(self._readDiffFile())\n        except:\n            sys.stderr.write(self.errorResultRead % self.diffOption)\n            return 1\n\n        newWarnings = self.parseWarnings(self.streamForDiff.getvalue())\n\n        diffWarnings = self.generateDiff(oldWarnings, newWarnings)\n\n        if diffWarnings:\n            diffResult = self.formatWarnings(diffWarnings)\n            self.outputStream.write(diffResult + \"\\n\")\n            return len(diffWarnings)\n        else:\n            return 0", "category": "Python"}, {"instruction": "def temperature_data_to_csv(temperature_data, path_or_buf):\n    \"\"\" Write temperature data to CSV. See also :any:`pandas.DataFrame.to_csv`.\n\n    Parameters\n    ----------\n    temperature_data : :any:`pandas.Series`\n        Temperature data series with :any:`pandas.DatetimeIndex`.\n    path_or_buf : :any:`str` or file handle, default None\n        File path or object, if None is provided the result is returned as a string.\n    \"\"\"\n", "input": "", "output": "    if temperature_data.index.name is None:\n        temperature_data.index.name = \"dt\"\n    if temperature_data.name is None:\n        temperature_data.name = \"temperature\"\n    return temperature_data.to_frame().to_csv(path_or_buf, index=True)", "category": "Python"}, {"instruction": "def validate_statusline(self, valid_statusline):\n        \"\"\"\n        Check that the statusline is valid, eg. starts with a numeric\n        code. If not, replace with passed in valid_statusline\n        \"\"\"\n", "input": "", "output": "        code = self.get_statuscode()\n        try:\n            code = int(code)\n            assert(code > 0)\n            return True\n        except(ValueError, AssertionError):\n            self.statusline = valid_statusline\n            return False", "category": "Python"}, {"instruction": "def handle_connection(self, client, ssl):\n        \"\"\"Handle a new connection with handle *client*.\n\n        This method exists so that it can be overridden in subclass. It is not\n        intended to be called directly.\n        \"\"\"\n", "input": "", "output": "        if ssl:\n            context = ssl if hasattr(ssl, 'set_ciphers') \\\n                            else ssl(client) if callable(ssl) \\\n                            else create_default_context(True)\n            transport = SslTransport(client, context, True)\n        else:\n            transport = Transport(client)\n        transport._log = self._log\n        transport._server = self\n        if DEBUG:\n            self._log.debug('new connection on {}', saddr(client.getsockname()))\n            if hasattr(client, 'getpeername'):\n                self._log.debug('remote peer is {}', saddr(client.getpeername()))\n        protocol = self._protocol_factory()\n        protocol._log = self._log\n        protocol._timeout = self._timeout\n        self._connections[transport] = protocol\n        self.connection_made(transport, protocol)\n        transport.start(protocol)", "category": "Python"}, {"instruction": "def path_to_slug(path):\n    \"\"\"\n    Removes everything from the given URL path, including\n    language code and ``PAGES_SLUG`` if any is set, returning\n    a slug that would match a ``Page`` instance's slug.\n    \"\"\"\n", "input": "", "output": "    from yacms.urls import PAGES_SLUG\n    lang_code = translation.get_language_from_path(path)\n    for prefix in (lang_code, settings.SITE_PREFIX, PAGES_SLUG):\n        if prefix:\n            path = path.replace(prefix, \"\", 1)\n    return clean_slashes(path) or \"/\"", "category": "Python"}, {"instruction": "def send_dir(self, local_path, remote_path, user='root'):\n        \"\"\"Upload a directory on the remote host.\n        \"\"\"\n", "input": "", "output": "        self.enable_user(user)\n        return self.ssh_pool.send_dir(user, local_path, remote_path)", "category": "Python"}, {"instruction": "def addr2line(self, addrq):\n        '''\n        Get the line number for a given bytecode offset\n\n        Analogous to PyCode_Addr2Line; translated from pseudocode in\n        Objects/lnotab_notes.txt\n        '''\n", "input": "", "output": "        co_lnotab = self.pyop_field('co_lnotab').proxyval(set())\n\n        # Initialize lineno to co_firstlineno as per PyCode_Addr2Line\n        # not 0, as lnotab_notes.txt has it:\n        lineno = int_from_int(self.field('co_firstlineno'))\n\n        addr = 0\n        for addr_incr, line_incr in zip(co_lnotab[::2], co_lnotab[1::2]):\n            addr += ord(addr_incr)\n            if addr > addrq:\n                return lineno\n            lineno += ord(line_incr)\n        return lineno", "category": "Python"}, {"instruction": "def denormalize_volume(volume):\n        '''convert volume metadata from archivant to es format'''\n", "input": "", "output": "        id = volume.get('id', None)\n        res = dict()\n        res.update(volume['metadata'])\n        denorm_attachments = list()\n        for a in volume['attachments']:\n            denorm_attachments.append(Archivant.denormalize_attachment(a))\n        res['_attachments'] = denorm_attachments\n        return id, res", "category": "Python"}, {"instruction": "def _after_request(self, response):\n        \"\"\"\n        The signal handler for the request_finished signal.\n        \"\"\"\n", "input": "", "output": "        if not getattr(g, '_has_exception', False):\n            extra = self.summary_extra()\n            self.summary_logger.info('', extra=extra)\n        return response", "category": "Python"}, {"instruction": "def remove_duplicates(lst):\n    \"\"\"\n    Emulate what a Python ``set()`` does, but keeping the element's order.\n    \"\"\"\n", "input": "", "output": "    dset = set()\n    return [l for l in lst if l not in dset and not dset.add(l)]", "category": "Python"}, {"instruction": "def new_binary_container(self, name):\n        \"\"\"Defines a new binary container to template.\n\n        Binary container can only contain binary fields defined with `Bin`\n        keyword.\n\n        Examples:\n        | New binary container | flags |\n        | bin | 2 | foo |\n        | bin | 6 | bar |\n        | End binary container |\n        \"\"\"\n", "input": "", "output": "        self._message_stack.append(BinaryContainerTemplate(name, self._current_container))", "category": "Python"}, {"instruction": "def get_pixel_skydirs(self):\n        \"\"\"Get a list of sky coordinates for the centers of every pixel.\n\n        \"\"\"\n", "input": "", "output": "\n        xpix = np.linspace(0, self.npix[0] - 1., self.npix[0])\n        ypix = np.linspace(0, self.npix[1] - 1., self.npix[1])\n        xypix = np.meshgrid(xpix, ypix, indexing='ij')\n        return SkyCoord.from_pixel(np.ravel(xypix[0]),\n                                   np.ravel(xypix[1]), self.wcs)", "category": "Python"}, {"instruction": "def lin_sim(goid1, goid2, godag, termcnts):\n    '''\n        Computes Lin's similarity measure.\n    '''\n", "input": "", "output": "    sim_r = resnik_sim(goid1, goid2, godag, termcnts)\n    return lin_sim_calc(goid1, goid2, sim_r, termcnts)", "category": "Python"}, {"instruction": "def make_output_layers(self):\n        \"\"\"\n        Extract the ordering of output layers.\n        \"\"\"\n", "input": "", "output": "        # TODO\n        # use successors == 0 as the criteria for output layer\n        # will fail when some intermediate layers also generate output.\n        # However, because the possibility of having inserted layers,\n        # it's more difficult to tell which layer is the output layer.\n        # Once possible way is to keep track of newly added layers...\n        self.output_layers = []\n        for layer in self.layer_list:\n            if len(self.get_successors(layer)) == 0:\n                self.output_layers.append(layer)", "category": "Python"}, {"instruction": "def requiredGPU_MB(self, n):\n        \"\"\"Required GPU memory in MBytes\n        \"\"\"\n", "input": "", "output": "        from darknet.core import darknet_with_cuda\n        if (darknet_with_cuda()): # its using cuda\n            free = getFreeGPU_MB()\n            print(\"Yolo: requiredGPU_MB: required, free\", n, free)\n            if (free == -1): # could not detect ..\n                return True\n            return (free>=n)\n        else:\n            return True", "category": "Python"}, {"instruction": "def as_dict(self):\n        \"\"\"\n        A JSON serializable dict representation of self.\n        \"\"\"\n", "input": "", "output": "        return {\"@module\": self.__class__.__module__,\n                \"@class\": self.__class__.__name__,\n                \"operation\": self.operation, \"title\": self.title,\n                \"xc\": self.xc.as_dict(), \"basis_set\": self.basis_set.as_dict(),\n                \"units\": self.units.as_dict(), \"scf\": self.scf.as_dict(),\n                \"geo\": self.geo.as_dict(),\n                \"others\": [k.as_dict() for k in self.other_directives]}", "category": "Python"}, {"instruction": "def _fingerprint_dict_with_files(self, option_val):\n    \"\"\"Returns a fingerprint of the given dictionary containing file paths.\n\n    Any value which is a file path which exists on disk will be fingerprinted by that file's\n    contents rather than by its path.\n\n    This assumes the files are small enough to be read into memory.\n\n    NB: The keys of the dict are assumed to be strings -- if they are not, the dict should be\n    converted to encode its keys with `stable_option_fingerprint()`, as is done in the `fingerprint()`\n    method.\n    \"\"\"\n", "input": "", "output": "    return stable_option_fingerprint({\n      k: self._expand_possible_file_value(v) for k, v in option_val.items()\n    })", "category": "Python"}, {"instruction": "def set_user_licenses(self, user, add=None, remove=None):\n        \"\"\"Implements: assignLicense\n        https://msdn.microsoft.com/library/azure/ad/graph/api/functions-and-actions#assignLicense\n\n        \"add\" is a dictionary of licence sku id's that reference an\n        array of disabled plan id's\n             add = { '<license-sku-id>': ['<disabled-plan-id'>, ...]\n        \"remove\" is an array of license sku id's\n             remove = ['<license-sku-id'>, ...]\n\n        \"\"\"\n", "input": "", "output": "        url = '/users/%s/assignLicense' % (user)\n        add_licenses = []\n        if add:\n            for l in add:\n                add_licenses.append({\n                    'skuId': l,\n                    'disabledPlans': add[l]\n                })\n\n        body = {\n            'addLicenses': add_licenses,\n            'removeLicenses': remove if remove else []\n        }\n\n        data = self.post_resource(url, json=body)\n        return data", "category": "Python"}, {"instruction": "def df(self, src):\n        '''Perform ``df`` on a path'''\n", "input": "", "output": "        return self._getStdOutCmd([self._hadoop_cmd, 'fs', '-df', self._full_hdfs_path(src)], True)", "category": "Python"}, {"instruction": "def range(self, dim, data_range=True, dimension_range=True):\n        \"\"\"Return the lower and upper bounds of values along dimension.\n\n        Args:\n            dimension: The dimension to compute the range on.\n            data_range (bool): Compute range from data values\n            dimension_range (bool): Include Dimension ranges\n                Whether to include Dimension range and soft_range\n                in range calculation\n\n        Returns:\n            Tuple containing the lower and upper bound\n        \"\"\"\n", "input": "", "output": "        dim = self.get_dimension(dim)\n\n        if dim is None or (not data_range and not dimension_range):\n            return (None, None)\n        elif all(util.isfinite(v) for v in dim.range) and dimension_range:\n            return dim.range\n        elif dim in self.dimensions() and data_range and bool(self):\n            lower, upper = self.interface.range(self, dim)\n        else:\n            lower, upper = (np.NaN, np.NaN)\n        if not dimension_range:\n            return lower, upper\n        return util.dimension_range(lower, upper, dim.range, dim.soft_range)", "category": "Python"}, {"instruction": "def is_local(self):\n        \"\"\"Whether this is a local variable.\n\n            In general, a variable is *local* if its containing scope is a\n            statement (e.g. a block), or a function, given that the variable\n            is not one of the function's parameters.\n        \"\"\"\n", "input": "", "output": "        return (isinstance(self.scope, CodeStatement)\n                or (isinstance(self.scope, CodeFunction)\n                    and self not in self.scope.parameters))", "category": "Python"}, {"instruction": "def create_rank_dicts(self, alt_scores):\r\n        \"\"\"\r\n        Description:\r\n            Takes in the scores of the alternatives in the form alt:score and\r\n            generates the dictionaries mapping alternatives to rankings and\r\n            rankings to alternatives.\r\n        Parameters:\r\n            alt_scores: dictionary of the scores of every alternative\r\n        \"\"\"\n", "input": "", "output": "        self.alts_to_ranks = dict()\r\n        cur_score = max(alt_scores.values())\r\n        cur_rank = 0\r\n        self.ranks_to_alts = {cur_rank:[]}\r\n        for i in sorted(alt_scores.keys(), key=lambda x: -alt_scores[x]):\r\n            if alt_scores[i] == cur_score:\r\n                self.ranks_to_alts[cur_rank].append(i)\r\n            elif alt_scores[i] < cur_score:\r\n                cur_rank += 1\r\n                cur_score = alt_scores[i]\r\n                self.ranks_to_alts[cur_rank] = [i]\r\n            self.alts_to_ranks[i] = cur_rank", "category": "Python"}, {"instruction": "def on_about_to_process(self, plugin, instance):\n        \"\"\"Reflect currently running pair in GUI\"\"\"\n", "input": "", "output": "\n        if instance is not None:\n            instance_model = self.data[\"models\"][\"instances\"]\n            index = instance_model.items.index(instance)\n            index = instance_model.createIndex(index, 0)\n            instance_model.setData(index, True, model.IsProcessing)\n\n        plugin_model = self.data[\"models\"][\"plugins\"]\n        index = plugin_model.items.index(plugin)\n        index = plugin_model.createIndex(index, 0)\n        plugin_model.setData(index, True, model.IsProcessing)\n        self.info(\"%s %s\" % (self.tr(\"Processing\"), index.data(model.Label)))", "category": "Python"}, {"instruction": "def local_2d_halo_exchange(k, v, num_h_blocks, h_dim,\n                           num_w_blocks, w_dim, mask_right):\n  \"\"\"Halo exchange for keys and values for Local 2D attention.\"\"\"\n", "input": "", "output": "  for blocks_dim, block_size_dim, halo_size in [\n      (num_h_blocks, h_dim, h_dim.size),\n      (num_w_blocks, w_dim, w_dim.size)]:\n    # shape of k is [num_h_blocks, num_w_blocks, h_dim, w_dim, kv_channels]\n    if halo_size > 0:\n      if blocks_dim is not None:\n        if mask_right:\n          k = mtf.left_halo_exchange(k, blocks_dim, block_size_dim, halo_size)\n          v = mtf.left_halo_exchange(v, blocks_dim, block_size_dim, halo_size)\n        else:\n          k = mtf.halo_exchange(k, blocks_dim, block_size_dim, halo_size)\n          v = mtf.halo_exchange(v, blocks_dim, block_size_dim, halo_size)\n      else:\n        if mask_right:\n          k = mtf.pad(k, [halo_size, None], block_size_dim.name)\n          v = mtf.pad(v, [halo_size, None], block_size_dim.name)\n        else:\n          k = mtf.pad(k, [halo_size, halo_size], block_size_dim.name)\n          v = mtf.pad(v, [halo_size, halo_size], block_size_dim.name)\n  return k, v", "category": "Python"}, {"instruction": "def debug_out(i):\n    \"\"\"\n    Input:  i - dictionary\n\n    Output: return = 0\n    \"\"\"\n", "input": "", "output": "\n    import copy\n    import json\n\n    ii={}\n\n    # Check main unprintable keys\n    for k in i:\n        try:\n           s=json.dumps(i[k])\n        except Exception as e: \n           pass\n        else:\n           ii[k]=i[k]\n\n    # Dump\n    out(json.dumps(ii, indent=2))\n\n    return {'return':0}", "category": "Python"}, {"instruction": "def _load_data(self):\n        \"\"\"\n        Load data from raw_data or file_path\n        \"\"\"\n", "input": "", "output": "        if self.raw_data is None and self.data_format is not FormatType.PYTHON:\n            if self.file_path is None:\n                raise ArgumentInvalid('One of \"raw_data\" or \"file_path\" should be set!')\n            if not os.path.isfile(self.file_path) or not os.access(self.file_path, os.R_OK):\n                raise ArgumentInvalid('\"file_path\" should be a valid path to an exist file with read permission!')\n            with open(self.file_path) as f:\n                self.raw_data = f.read()", "category": "Python"}, {"instruction": "def _setter(self, attr, value, bottom, top, to_step):\n        \"\"\" Set a value.\n\n        :param attr: Attribute to set.\n        :param value: Value to use.\n        :param bottom: Get to bottom value.\n        :param top: Get to top value.\n        :param to_step: Get to intermediary value.\n        \"\"\"\n", "input": "", "output": "        if value < 0 or value > 1:\n            raise ValueError(\"out of range\")\n        if value == 0.0:\n            bottom()\n        elif value == 1.0:\n            top()\n        else:\n            to_step(value)\n        setattr(self, attr, value)", "category": "Python"}, {"instruction": "def create_web_element(self, element_id):\n        \"\"\"Creates a web element with the specified `element_id`.\"\"\"\n", "input": "", "output": "        return self._web_element_cls(self, element_id, w3c=self.w3c)", "category": "Python"}, {"instruction": "def get_messages_from_stream(data):\n    \"\"\"Extract complete messages from stream and cut out them from stream.\n\n    @data - stream binary data\n    @return - [list of messages, choped stream data]\n\n    \"\"\"\n", "input": "", "output": "    messages = []\n    iterator = HEADER_RE.finditer(data)\n    last_pos = 0\n    for match in iterator:\n        pos = match.span()[0]\n        header = read_machine_header(data[pos:])\n        h_len = __get_machine_header_length(header)\n        cur_last_pos = pos + h_len + header['meta_len'] + header['data_len']\n\n        if cur_last_pos > len(data):\n            break\n\n        header, meta, bin_data = parse_message(data[pos:])\n        messages.append({'header': header, 'meta': meta, 'data': bin_data})\n\n        last_pos = cur_last_pos\n\n    data = data[last_pos:]\n    return messages, data", "category": "Python"}, {"instruction": "def norm(x, mu, sigma=1.0):\n    \"\"\" Scipy norm function \"\"\"\n", "input": "", "output": "    return stats.norm(loc=mu, scale=sigma).pdf(x)", "category": "Python"}, {"instruction": "def _objectmethods(self, obj: str, *args) -> list:\n        \"\"\"\n        This method parses the SAS log for artifacts (tables and graphics) that were created\n        from the procedure method call\n\n        :param obj: str -- proc object\n        :param args: list likely none\n        :return: list -- the tables and graphs available for tab complete\n        \"\"\"\n", "input": "", "output": "        code = \"%listdata(\"\n        code += obj\n        code += \");\"\n        self.logger.debug(\"Object Method macro call: \" + str(code))\n        res = self.sas.submit(code, \"text\")\n        meth = res['LOG'].splitlines()\n        for i in range(len(meth)):\n            meth[i] = meth[i].lstrip().rstrip()\n        self.logger.debug('SAS Log: ' + res['LOG'])\n        objlist = meth[meth.index('startparse9878') + 1:meth.index('endparse9878')]\n        self.logger.debug(\"PROC attr list: \" + str(objlist))\n        return objlist", "category": "Python"}, {"instruction": "def setPermissions(self, dbName, access) :\n        \"\"\"Grant revoke rights on a database, 'access' is supposed to be boolean. ArangoDB grants/revokes both read and write rights at the same time\"\"\"\n", "input": "", "output": "        import json\n\n        if not self.URL :\n            raise CreationError(\"Please save user first\", None, None)\n\n        rights = []\n        if access :\n            rights.append(\"rw\")\n\n        rights = ''.join(rights)\n\n        if not self.connection.hasDatabase(dbName) :\n            raise KeyError(\"Unknown database: %s\" % dbName)\n\n        url = \"%s/database/%s\" % (self.URL, dbName)\n        r = self.connection.session.put(url, data = json.dumps({\"grant\": rights}, default=str))\n        if r.status_code < 200 or r.status_code > 202 :\n            raise CreationError(\"Unable to grant rights\", r.content)", "category": "Python"}, {"instruction": "def post_copy(self, cursor):\n        \"\"\"\n        Performs post-copy sql - such as cleansing data, inserting into production table (if copied to temp table), etc.\n        \"\"\"\n", "input": "", "output": "        logger.info('Executing post copy queries')\n        for query in self.queries:\n            cursor.execute(query)", "category": "Python"}, {"instruction": "def metadata(abbr, __metadata=__metadata):\n    \"\"\"\n    Grab the metadata for the given two-letter abbreviation.\n    \"\"\"\n", "input": "", "output": "    # This data should change very rarely and is queried very often so\n    # cache it here\n    abbr = abbr.lower()\n    if abbr in __metadata:\n        return __metadata[abbr]\n    rv = db.metadata.find_one({'_id': abbr})\n\n    __metadata[abbr] = rv\n    return rv", "category": "Python"}, {"instruction": "def ensure_pool(func):\n    \"\"\"Decorator that ensures a pool is available for a stepper.\"\"\"\n", "input": "", "output": "    def func_wrapper(*args, **kwargs):\n        if len(args) == 0 or not isinstance(args[0], Stepper):\n            raise Exception('@ensure_pool can only be used on Stepper methods.')\n        if args[0]._pool is None:\n            with args[0]:\n                return func(*args, **kwargs)\n        else:\n            return func(*args, **kwargs)\n    return func_wrapper", "category": "Python"}, {"instruction": "def restore_event(self, requestId):\n        \"\"\"restore an event based on the requestId.\n\n        For example if the user app had to shutdown with pending requests.\n        The user can rebuild the Events they were waiting for based on the requestId(s).\n        \"\"\"\n", "input": "", "output": "        with self.__requests:\n            if requestId not in self.__requests:\n                self.__requests[requestId] = RequestEvent(requestId)\n                return True\n        return False", "category": "Python"}, {"instruction": "def store(self, key, value):\n        \"\"\"Store the key, value pair in our redis server\"\"\"\n", "input": "", "output": "        # Prepend tweepy to our key,\n        # this makes it easier to identify tweepy keys in our redis server\n        key = self.pre_identifier + key\n        # Get a pipe (to execute several redis commands in one step)\n        pipe = self.client.pipeline()\n        # Set our values in a redis hash (similar to python dict)\n        pipe.set(key, pickle.dumps((time.time(), value)))\n        # Set the expiration\n        pipe.expire(key, self.timeout)\n        # Add the key to a set containing all the keys\n        pipe.sadd(self.keys_container, key)\n        # Execute the instructions in the redis server\n        pipe.execute()", "category": "Python"}, {"instruction": "def lranksums(x,y):\n    \"\"\"\nCalculates the rank sums statistic on the provided scores and\nreturns the result.  Use only when the n in each condition is > 20 and you\nhave 2 independent samples of ranks.\n\nUsage:   lranksums(x,y)\nReturns: a z-statistic, two-tailed p-value\n\"\"\"\n", "input": "", "output": "    n1 = len(x)\n    n2 = len(y)\n    alldata = x+y\n    ranked = rankdata(alldata)\n    x = ranked[:n1]\n    y = ranked[n1:]\n    s = sum(x)\n    expected = n1*(n1+n2+1) / 2.0\n    z = (s - expected) / math.sqrt(n1*n2*(n1+n2+1)/12.0)\n    prob = 2*(1.0 -zprob(abs(z)))\n    return z, prob", "category": "Python"}, {"instruction": "def get_config(config_spec):\n        \"\"\"Like get_json_config but does not parse result as JSON\"\"\"\n", "input": "", "output": "        config_file = None\n        if config_spec.startswith(\"http\"):\n            # URL: fetch it\n            config_file = urllib.urlopen(config_spec)\n        else:\n            # string: open file with that name\n            config_file = open(config_spec)\n        config = json.load(config_file)\n        # Close any open files\n        try:\n            config_file.close()\n        except:\n            pass\n        return config", "category": "Python"}, {"instruction": "def _orientation(self, edge, point):\n        \"\"\" Returns +1 if edge[0]->point is clockwise from edge[0]->edge[1], \n        -1 if counterclockwise, and 0 if parallel.\n        \"\"\"\n", "input": "", "output": "        v1 = self.pts[point] - self.pts[edge[0]]\n        v2 = self.pts[edge[1]] - self.pts[edge[0]]\n        c = np.cross(v1, v2)  # positive if v1 is CW from v2\n        return 1 if c > 0 else (-1 if c < 0 else 0)", "category": "Python"}, {"instruction": "def segments(self, index=None, params=None):\n        \"\"\"\n        The segments command is the detailed view of Lucene segments per index.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-segments.html>`_\n\n        :arg index: A comma-separated list of index names to limit the returned\n            information\n        :arg bytes: The unit in which to display byte values, valid choices are:\n            'b', 'k', 'kb', 'm', 'mb', 'g', 'gb', 't', 'tb', 'p', 'pb'\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg v: Verbose mode. Display column headers, default False\n        \"\"\"\n", "input": "", "output": "        return self.transport.perform_request('GET', _make_path('_cat',\n            'segments', index), params=params)", "category": "Python"}, {"instruction": "def get_attributes(self):\n        \"\"\"\n        Returns the Node attributes.\n\n        Usage::\n\n            >>>\tnode_a = AbstractNode(\"MyNodeA\", attributeA=Attribute(value=\"A\"), attributeB=Attribute(value=\"B\"))\n            >>> node_a.get_attributes()\n            [<Attribute object at 0x7fa471d3b5e0>, <Attribute object at 0x101e6c4a0>]\n\n        :return: Attributes.\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "\n        return [attribute for attribute in self.itervalues() if issubclass(attribute.__class__, Attribute)]", "category": "Python"}, {"instruction": "def marshal_bson(\n    obj,\n    types=BSON_TYPES,\n    fields=None,\n):\n    \"\"\" Recursively marshal a Python object to a BSON-compatible dict\n        that can be passed to PyMongo, Motor, etc...\n\n    Args:\n        obj:    object, It's members can be nested Python\n                objects which will be converted to dictionaries\n        types:  tuple-of-types, The BSON primitive types, typically\n                you would not change this\n        fields: None-list-of-str, Explicitly marshal only these fields\n    Returns:\n        dict\n    \"\"\"\n", "input": "", "output": "    return marshal_dict(\n        obj,\n        types,\n        fields=fields,\n    )", "category": "Python"}, {"instruction": "def remove_text_structure(self, text_structure): # TODO: delete also TextElements one by one\n        \"\"\"\n        Remove any citable text structure to the work.\n        \"\"\"\n", "input": "", "output": "        idx = self.hucit_has_structure.index(text_structure)\n        ts = self.hucit_has_structure.pop(idx)\n        ts.remove()\n        self.update()\n        return", "category": "Python"}, {"instruction": "def str2et(time):\n    \"\"\"\n    Convert a string representing an epoch to a double precision\n    value representing the number of TDB seconds past the J2000\n    epoch corresponding to the input epoch.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/str2et_c.html\n\n    :param time: A string representing an epoch.\n    :type time: str\n    :return: The equivalent value in seconds past J2000, TDB.\n    :rtype: float\n    \"\"\"\n", "input": "", "output": "    if isinstance(time, list):\n        return numpy.array([str2et(t) for t in time])\n    time = stypes.stringToCharP(time)\n    et = ctypes.c_double()\n    libspice.str2et_c(time, ctypes.byref(et))\n    return et.value", "category": "Python"}, {"instruction": "def copy_children(self, foreign_id, existing_node):\n        '''\n        Initiates copying of tree, with existing_node acting as root\n        '''\n", "input": "", "output": "        url = \"{}/api/v2/pages/{}/\".format(self.base_url, foreign_id)\n        self.log(\n            ACTION,\n            \"Copying Children\",\n            {\"existing node type\": str(type(existing_node))})\n\n        # TODO: create a robust wrapper around this functionality\n        try:\n            self.log(ACTION, \"Requesting Data\", {\"url\": url})\n            response = requests.get(url)\n            content = json.loads(response.content)\n            self.log(SUCCESS, \"Data Fetched Successfully\", {\"url\": url})\n\n            main_language_child_ids = content[\"meta\"][\"main_language_children\"]\n            if main_language_child_ids:\n                for main_language_child_id in main_language_child_ids:\n                    self.copy_page_and_children(\n                        foreign_id=main_language_child_id,\n                        parent_id=existing_node.id, depth=1)\n            else:\n                self.log(SUCCESS, \"No children to copy\")\n        except Exception as e:\n            self.log(ERROR, \"Copying Children\", {\"url\": url, \"exception\": e})", "category": "Python"}, {"instruction": "def compile_excludes(self):\n        \"\"\"Compile a set of regexps for files to be exlcuded from scans.\"\"\"\n", "input": "", "output": "        self.compiled_exclude_files = []\n        for pattern in self.exclude_files:\n            try:\n                self.compiled_exclude_files.append(re.compile(pattern))\n            except re.error as e:\n                raise ValueError(\n                    \"Bad python regex in exclude '%s': %s\" % (pattern, str(e)))", "category": "Python"}, {"instruction": "def p_propertyDeclaration_6(p):\n    # pylint: disable=line-too-long\n    \"\"\"propertyDeclaration_6 : qualifierList dataType propertyName defaultValue ';'\"\"\"\n", "input": "", "output": "    quals = OrderedDict([(x.name, x) for x in p[1]])\n    p[0] = CIMProperty(p[3], cimvalue(p[4], p[2]),\n                       type=p[2], qualifiers=quals)", "category": "Python"}, {"instruction": "def new(self, *args, **kwargs) -> str:\n        \"\"\"\n        Generate callback data\n\n        :param args:\n        :param kwargs:\n        :return:\n        \"\"\"\n", "input": "", "output": "        args = list(args)\n\n        data = [self.prefix]\n\n        for part in self._part_names:\n            value = kwargs.pop(part, None)\n            if value is None:\n                if args:\n                    value = args.pop(0)\n                else:\n                    raise ValueError(f\"Value for '{part}' is not passed!\")\n\n            if value is not None and not isinstance(value, str):\n                value = str(value)\n\n            if not value:\n                raise ValueError(f\"Value for part {part} can't be empty!'\")\n            elif self.sep in value:\n                raise ValueError(f\"Symbol defined as separator can't be used in values of parts\")\n\n            data.append(value)\n\n        if args or kwargs:\n            raise TypeError('Too many arguments is passed!')\n\n        callback_data = self.sep.join(data)\n        if len(callback_data) > 64:\n            raise ValueError('Resulted callback data is too long!')\n\n        return callback_data", "category": "Python"}, {"instruction": "def doCommit(self, p: Prepare):\n        \"\"\"\n        Create a commit message from the given Prepare message and trigger the\n        commit phase\n        :param p: the prepare message\n        \"\"\"\n", "input": "", "output": "        key_3pc = (p.viewNo, p.ppSeqNo)\n        self.logger.debug(\"{} Sending COMMIT{} at {}\".format(self, key_3pc, self.get_current_time()))\n\n        params = [\n            self.instId, p.viewNo, p.ppSeqNo\n        ]\n\n        pre_prepare = self.getPrePrepare(*key_3pc)\n\n        # BLS multi-sig:\n        if p.stateRootHash is not None:\n            pre_prepare = self.getPrePrepare(*key_3pc)\n            params = self._bls_bft_replica.update_commit(params, pre_prepare)\n\n        commit = Commit(*params)\n        if self.isMaster:\n            rv = self.execute_hook(ReplicaHooks.CREATE_CM, commit)\n            commit = rv if rv is not None else commit\n\n        self.send(commit, TPCStat.CommitSent)\n        self.addToCommits(commit, self.name)", "category": "Python"}, {"instruction": "def walk(self):\n        \"\"\"\n        A generator that walking through all sub packages and sub modules.\n\n        1. current package object (\u5305\u5bf9\u8c61)\n        2. current package's parent (\u5f53\u524d\u5305\u5bf9\u8c61\u7684\u6bcd\u5305)\n        3. list of sub packages (\u6240\u6709\u5b50\u5305)\n        4. list of sub modules (\u6240\u6709\u6a21\u5757)\n        \"\"\"\n", "input": "", "output": "        yield (\n            self,\n            self.parent,\n            list(self.sub_packages.values()),\n            list(self.sub_modules.values()),\n        )\n\n        for pkg in self.sub_packages.values():\n            for things in pkg.walk():\n                yield things", "category": "Python"}, {"instruction": "def add_user(cls, username, email):\n        \"\"\" Add user to service \"\"\"\n", "input": "", "output": "        sanitized = str(cls.__sanitize_username(username))\n        logger.debug(\"Adding user to SeAT with username %s\" % sanitized)\n        password = cls.__generate_random_pass()\n        ret = cls.exec_request('user', 'post', username=sanitized, email=str(email), password=password)\n        logger.debug(ret)\n        if cls._response_ok(ret):\n            logger.info(\"Added SeAT user with username %s\" % sanitized)\n            return sanitized, password\n        logger.info(\"Failed to add SeAT user with username %s\" % sanitized)\n        return None, None", "category": "Python"}, {"instruction": "def _pid_is_alive(cls, pid, timeout):\n        \"\"\"Check if a PID is alive with a timeout.\"\"\"\n", "input": "", "output": "        try:\n            proc = psutil.Process(pid)\n        except psutil.NoSuchProcess:\n            return False\n\n        try:\n            proc.wait(timeout=timeout)\n        except psutil.TimeoutExpired:\n            return True\n\n        return False", "category": "Python"}, {"instruction": "def _quasi_topological_sort(self):\n        \"\"\"\n        Perform a quasi-topological sort on an already constructed CFG graph (a networkx DiGraph)\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        # Clear the existing sorting result\n        self._quasi_topological_order = {}\n\n        ctr = self._graph.number_of_nodes()\n\n        for ep in self._entry_points:\n            # FIXME: This is not always correct. We'd better store CFGNodes in self._entry_points\n            ep_node = self.get_any_node(ep)\n\n            if not ep_node:\n                continue\n\n            for n in networkx.dfs_postorder_nodes(self._graph, source=ep_node):\n                if n not in self._quasi_topological_order:\n                    self._quasi_topological_order[n] = ctr\n                    ctr -= 1", "category": "Python"}, {"instruction": "def find_by_b64ids(self, _ids, **kwargs):\n        \"\"\"\n        Pass me a list of base64-encoded ObjectId\n        \"\"\"\n", "input": "", "output": "\n        return self.find_by_ids([ObjectId(base64.b64decode(_id)) for _id in _ids], **kwargs)", "category": "Python"}, {"instruction": "def kernel_status(self, user_name, kernel_slug, **kwargs):  # noqa: E501\n        \"\"\"Get the status of the latest kernel version  # noqa: E501\n\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.kernel_status(user_name, kernel_slug, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str user_name: Kernel owner (required)\n        :param str kernel_slug: Kernel name (required)\n        :return: Result\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.kernel_status_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n        else:\n            (data) = self.kernel_status_with_http_info(user_name, kernel_slug, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def _example_rt_data(quote_ctx):\n    \"\"\"\n    \u83b7\u53d6\u5206\u65f6\u6570\u636e\uff0c\u8f93\u51fa \u65f6\u95f4\uff0c\u6570\u636e\u72b6\u6001\uff0c\u5f00\u76d8\u591a\u5c11\u5206\u949f\uff0c\u76ee\u524d\u4ef7\uff0c\u6628\u6536\u4ef7\uff0c\u5e73\u5747\u4ef7\uff0c\u6210\u4ea4\u91cf\uff0c\u6210\u4ea4\u989d\n    \"\"\"\n", "input": "", "output": "    stock_code_list = [\"US.AAPL\", \"HK.00700\"]\n\n    ret_status, ret_data = quote_ctx.subscribe(stock_code_list, ft.SubType.RT_DATA)\n    if ret_status != ft.RET_OK:\n        print(ret_data)\n        exit()\n\n    for stk_code in stock_code_list:\n        ret_status, ret_data = quote_ctx.get_rt_data(stk_code)\n        if ret_status != ft.RET_OK:\n            print(stk_code, ret_data)\n            exit()\n\n        print(\"%s RT_DATA\" % stk_code)\n        print(ret_data)\n        print(\"\\n\\n\")", "category": "Python"}, {"instruction": "def normalize_mode(mode):\n    \"\"\"\\\n    Returns a (Micro) QR Code mode constant which is equivalent to the\n    provided `mode`.\n\n    In case the provided `mode` is ``None``, this function returns ``None``.\n    Otherwise a mode constant is returned unless the provided parameter cannot\n    be mapped to a valid mode. In the latter case, a ModeError is raised.\n\n    :param mode: An integer or string or ``None``.\n    :raises: ModeError: In case the provided `mode` does not represent a valid\n             QR Code mode.\n    :rtype: int or None\n    \"\"\"\n", "input": "", "output": "    if mode is None or (isinstance(mode, int)\n                        and mode in consts.MODE_MAPPING.values()):\n        return mode\n    try:\n        return consts.MODE_MAPPING[mode.lower()]\n    except:  # KeyError or mode.lower() fails\n        raise ModeError('Illegal mode \"{0}\". Supported values: {1}'\n                        .format(mode, ', '.join(sorted(consts.MODE_MAPPING.keys()))))", "category": "Python"}, {"instruction": "def add_binary_content_type(application, content_type, pack, unpack):\n    \"\"\"\n    Add handler for a binary content type.\n\n    :param tornado.web.Application application: the application to modify\n    :param str content_type: the content type to add\n    :param pack: function that packs a dictionary to a byte string.\n        ``pack(dict) -> bytes``\n    :param unpack: function that takes a byte string and returns a\n        dictionary.  ``unpack(bytes) -> dict``\n\n    \"\"\"\n", "input": "", "output": "    add_transcoder(application,\n                   handlers.BinaryContentHandler(content_type, pack, unpack))", "category": "Python"}, {"instruction": "def _to_ctfile_bond_block(self, key):\n        \"\"\"Create bond block in `CTfile` format.\n\n        :param str key: Ctab atom block key. \n        :return: Ctab bond block.\n        :rtype: :py:class:`str`\n        \"\"\"\n", "input": "", "output": "        counter = OrderedCounter(Bond.bond_block_format)\n        ctab_bond_block = '\\n'.join([''.join([str(value).rjust(spacing) for value, spacing\n                                              in zip(bond._ctab_data.values(), counter.values())])\n                                     for bond in self[key]])\n        return '{}\\n'.format(ctab_bond_block)", "category": "Python"}, {"instruction": "def finditer(self, expr):\n        \"\"\"Return an iterator over all matches in `expr`\n\n        Iterate over all :class:`MatchDict` results of matches for any\n        matching (sub-)expressions in `expr`. The order of the matches conforms\n        to the equivalent matched expressions returned by :meth:`findall`.\n        \"\"\"\n", "input": "", "output": "        try:\n            for arg in expr.args:\n                for m in self.finditer(arg):\n                    yield m\n            for arg in expr.kwargs.values():\n                for m in self.finditer(arg):\n                    yield m\n        except AttributeError:\n            pass\n        m = self.match(expr)\n        if m:\n            yield m", "category": "Python"}, {"instruction": "def allow_ast_comparison():\n    \"\"\"This ugly little monkey-patcher adds in a helper class\n    to all the AST node types.  This helper class allows\n    eq/ne comparisons to work, so that entire trees can\n    be easily compared by Python's comparison machinery.\n    Used by the anti8 functions to compare old and new ASTs.\n    Could also be used by the test library.\n\n\n    \"\"\"\n", "input": "", "output": "\n    class CompareHelper(object):\n        def __eq__(self, other):\n            return type(self) == type(other) and vars(self) == vars(other)\n\n        def __ne__(self, other):\n            return type(self) != type(other) or vars(self) != vars(other)\n\n    for item in vars(ast).values():\n        if type(item) != type:\n            continue\n        if issubclass(item, ast.AST):\n            try:\n                item.__bases__ = tuple(list(item.__bases__) + [CompareHelper])\n            except TypeError:\n                pass", "category": "Python"}, {"instruction": "def __parse_drac(output):\n    '''\n    Parse Dell DRAC output\n    '''\n", "input": "", "output": "    drac = {}\n    section = ''\n\n    for i in output.splitlines():\n        if i.strip().endswith(':') and '=' not in i:\n            section = i[0:-1]\n            drac[section] = {}\n        if i.rstrip() and '=' in i:\n            if section in drac:\n                drac[section].update(dict(\n                    [[prop.strip() for prop in i.split('=')]]\n                ))\n            else:\n                section = i.strip()\n                if section not in drac and section:\n                    drac[section] = {}\n\n    return drac", "category": "Python"}, {"instruction": "def if_range(self) -> Optional[datetime.datetime]:\n        \"\"\"The value of If-Range HTTP header, or None.\n\n        This header is represented as a `datetime` object.\n        \"\"\"\n", "input": "", "output": "        return self._http_date(self.headers.get(hdrs.IF_RANGE))", "category": "Python"}, {"instruction": "def to_file(self, fp, format_, fps=None, **kwargs):\n        \"\"\"\n        Write subtitle file to file object.\n\n        See :meth:`SSAFile.save()` for full description.\n\n        Note:\n            This is a low-level method. Usually, one of :meth:`SSAFile.save()`\n            or :meth:`SSAFile.to_string()` is preferable.\n\n        Arguments:\n            fp (file object): A file object, ie. :class:`io.TextIOBase` instance.\n                Note that the file must be opened in text mode (as opposed to binary).\n\n        \"\"\"\n", "input": "", "output": "        impl = get_format_class(format_)\n        impl.to_file(self, fp, format_, fps=fps, **kwargs)", "category": "Python"}, {"instruction": "def do_prune(self):\n        \"\"\"\n        Return True if prune_table, prune_column, and prune_date are implemented.\n        If only a subset of prune variables are override, an exception is raised to remind the user to implement all or none.\n        Prune (data newer than prune_date deleted) before copying new data in.\n        \"\"\"\n", "input": "", "output": "        if self.prune_table and self.prune_column and self.prune_date:\n            return True\n        elif self.prune_table or self.prune_column or self.prune_date:\n            raise Exception('override zero or all prune variables')\n        else:\n            return False", "category": "Python"}, {"instruction": "def get_cartesian(self):\n        \"\"\"Return a :class:`~Cartesian` where all\n        members of a symmetry equivalence class are inserted back in.\n\n        Args:\n            None\n\n        Returns:\n            Cartesian: A new cartesian instance.\n        \"\"\"\n", "input": "", "output": "        coords = ['x', 'y', 'z']\n        eq_sets = self._metadata['eq']['eq_sets']\n        sym_ops = self._metadata['eq']['sym_ops']\n        frame = pd.DataFrame(index=[i for v in eq_sets.values() for i in v],\n                             columns=['atom', 'x', 'y', 'z'], dtype='f8')\n        frame['atom'] = pd.Series(\n            {i: self.loc[k, 'atom'] for k, v in eq_sets.items() for i in v})\n        frame.loc[self.index, coords] = self.loc[:, coords]\n        for i in eq_sets:\n            for j in eq_sets[i]:\n                frame.loc[j, coords] = np.dot(sym_ops[i][j],\n                                              frame.loc[i, coords])\n        return Cartesian(frame)", "category": "Python"}, {"instruction": "def write (self, s):\n        \"\"\"Process text, writing it to the virtual screen while handling\n        ANSI escape codes.\n        \"\"\"\n", "input": "", "output": "        if isinstance(s, bytes):\n            s = self._decode(s)\n        for c in s:\n            self.process(c)", "category": "Python"}, {"instruction": "def _encode_item(self, item: str) -> str:\n        \"\"\"\n        If anonymization is on, an item gets salted and hashed here.\n\n        :param str item:\n        :return: Hashed item, if anonymization is on; the unmodified item otherwise\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        assert item is not None\n        if not self.__redis_conf['anonymization']:\n            return item\n        connection = self.__get_connection()\n        salt = connection.get(self.__redis_conf['salt_key'])\n        if salt is None:\n            salt = create_salt()\n            connection.set(self.__redis_conf['salt_key'], salt)\n        BlackRed.__release_connection(connection)\n        return sha512(salt + item.encode()).hexdigest()", "category": "Python"}, {"instruction": "def _start_lock_renewer(self):\n        \"\"\"\n        Starts the lock refresher thread.\n        \"\"\"\n", "input": "", "output": "        if self._lock_renewal_thread is not None:\n            raise AlreadyStarted(\"Lock refresh thread already started\")\n\n        logger.debug(\n            \"Starting thread to refresh lock every %s seconds\",\n            self._lock_renewal_interval\n        )\n        self._lock_renewal_stop = threading.Event()\n        self._lock_renewal_thread = threading.Thread(\n            group=None,\n            target=self._lock_renewer,\n            kwargs={'lockref': weakref.ref(self),\n                    'interval': self._lock_renewal_interval,\n                    'stop': self._lock_renewal_stop}\n        )\n        self._lock_renewal_thread.setDaemon(True)\n        self._lock_renewal_thread.start()", "category": "Python"}, {"instruction": "def getOrderedLinks(self, session):\n        \"\"\"\n        Retrieve the links in the order of the link number.\n\n        Args:\n            session (:mod:`sqlalchemy.orm.session.Session`): SQLAlchemy session object bound to PostGIS enabled database.\n\n        Returns:\n            list: A list of :class:`.StreamLink` objects.\n        \"\"\"\n", "input": "", "output": "        streamLinks = session.query(StreamLink).\\\n                            filter(StreamLink.channelInputFile == self).\\\n                            order_by(StreamLink.linkNumber).\\\n                            all()\n\n        return streamLinks", "category": "Python"}, {"instruction": "def set(self, paths: Union[str, Iterable[str]], metadata: Union[IrodsMetadata, List[IrodsMetadata]]):\n        \"\"\"\n        Sets the given metadata on the iRODS entities at the given path or paths. Similar to `add` excpet pre-existing\n        metadata with matching keys will be overwritten.\n\n        If a single metadata collection is given, that metadata is set for all paths. If a list of metadata are given,\n        each collection is set for the path with the corresponding index.\n\n        A `ValueError` will be raised will be raised if the path does not correspond to a valid entity.\n        :param path: the path of the entity to set the metadata for\n        :param metadata: the metadata to set\n        \"\"\"\n", "input": "", "output": "", "category": "Python"}, {"instruction": "def schoice(self, seq: str, end: int = 10) -> str:\n        \"\"\"Choice function which returns string created from sequence.\n\n        :param seq: Sequence of letters or digits.\n        :type seq: tuple or list\n        :param end: Max value.\n        :return: Single string.\n        \"\"\"\n", "input": "", "output": "        return ''.join(self.choice(list(seq))\n                       for _ in range(end))", "category": "Python"}, {"instruction": "def get_peak_mem():\n    '''\n    this returns peak memory use since process starts till the moment its called\n    '''\n", "input": "", "output": "    import resource\n    rusage_denom = 1024.\n    if sys.platform == 'darwin':\n        # ... it seems that in OSX the output is different units ...\n        rusage_denom = rusage_denom * rusage_denom\n    mem = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss / rusage_denom\n    return mem", "category": "Python"}, {"instruction": "def get_dimension_index(self, dimension):\n        \"\"\"Get the index of the requested dimension.\n\n        Args:\n            dimension: Dimension to look up by name or by index\n\n        Returns:\n            Integer index of the requested dimension\n        \"\"\"\n", "input": "", "output": "        if isinstance(dimension, int):\n            if (dimension < (self.ndims + len(self.vdims)) or\n                dimension < len(self.dimensions())):\n                return dimension\n            else:\n                return IndexError('Dimension index out of bounds')\n        dim = dimension_name(dimension)\n        try:\n            dimensions = self.kdims+self.vdims\n            return [i for i, d in enumerate(dimensions) if d == dim][0]\n        except IndexError:\n            raise Exception(\"Dimension %s not found in %s.\" %\n                            (dim, self.__class__.__name__))", "category": "Python"}, {"instruction": "def convert_the_getters(getters):\n    \"\"\"\n    A function used to prepare the arguments of calculator and atoms getter methods\n    \"\"\"\n", "input": "", "output": "    return_list = []\n    for getter in getters:\n        \n        if isinstance(getter,basestring):\n            out_args = \"\"\n            method_name = getter\n            \n        else:\n            method_name, a = getter\n            \n            out_args = convert_the_args(a)\n            \n        return_list.append( (method_name, out_args) )\n    return return_list", "category": "Python"}, {"instruction": "def write_uint8(self, value, little_endian=True):\n        \"\"\"\n        Pack the value as an unsigned byte and write 1 byte to the stream.\n\n        Args:\n            value:\n            little_endian (bool): specify the endianness. (Default) Little endian.\n\n        Returns:\n            int: the number of bytes written.\n        \"\"\"\n", "input": "", "output": "        if little_endian:\n            endian = \"<\"\n        else:\n            endian = \">\"\n        return self.pack('%sB' % endian, value)", "category": "Python"}, {"instruction": "def blackbody_wn_rad2temp(wavenumber, radiance):\n    \"\"\"Derive brightness temperatures from radiance using the Planck \n    function. Wavenumber space\"\"\"\n", "input": "", "output": "\n    if np.isscalar(radiance):\n        rad = np.array([radiance, ], dtype='float64')\n    else:\n        rad = np.array(radiance, dtype='float64')\n    if np.isscalar(wavenumber):\n        wavnum = np.array([wavenumber, ], dtype='float64')\n    else:\n        wavnum = np.array(wavenumber, dtype='float64')\n\n    const1 = H_PLANCK * C_SPEED / K_BOLTZMANN\n    const2 = 2 * H_PLANCK * C_SPEED**2\n    res = const1 * wavnum / np.log(np.divide(const2 * wavnum**3, rad) + 1.0)\n\n    shape = rad.shape\n    resshape = res.shape\n\n    if wavnum.shape[0] == 1:\n        if rad.shape[0] == 1:\n            return res[0]\n        else:\n            return res[::].reshape(shape)\n    else:\n        if rad.shape[0] == 1:\n            return res[0, :]\n        else:\n            if len(shape) == 1:\n                return np.reshape(res, (shape[0], resshape[1]))\n            else:\n                return np.reshape(res, (shape[0], shape[1], resshape[1]))", "category": "Python"}, {"instruction": "def get(protocol, subset, classes=CLASSES, variables=VARIABLES):\n  '''Returns the data subset given a particular protocol\n\n\n  Parameters\n\n    protocol (string): one of the valid protocols supported by this interface\n\n    subset (string): one of 'train' or 'test'\n\n    classes (list of string): a list of strings containing the names of the\n      classes from which you want to have the data from\n\n    variables (list of strings): a list of strings containg the names of the\n      variables (features) you want to have data from\n\n\n  Returns:\n\n    data (numpy.ndarray): The data for all the classes and variables nicely\n      packed into one numpy 3D array. One depth represents the data for one\n      class, one row is one example, one column a given feature.\n\n  '''\n", "input": "", "output": "\n  retval = split_data(bob.db.iris.data(), subset, PROTOCOLS[protocol])\n\n  # filter variables (features)\n  varindex = [VARIABLES.index(k) for k in variables]\n\n  # filter class names and variable indexes at the same time\n  retval = dict([(k, retval[k][:,varindex]) for k in classes])\n\n  # squash the data\n  return numpy.array([retval[k] for k in classes])", "category": "Python"}, {"instruction": "def request_set_sensor_inactive(self, req, sensor_name):\n        \"\"\"Set sensor status to inactive\"\"\"\n", "input": "", "output": "        sensor = self.get_sensor(sensor_name)\n        ts, status, value = sensor.read()\n        sensor.set_value(value, sensor.INACTIVE, ts)\n        return('ok',)", "category": "Python"}, {"instruction": "def convert_pointSource(self, node):\n        \"\"\"\n        Convert the given node into a point source object.\n\n        :param node: a node with tag pointGeometry\n        :returns: a :class:`openquake.hazardlib.source.PointSource` instance\n        \"\"\"\n", "input": "", "output": "        geom = node.pointGeometry\n        lon_lat = ~geom.Point.pos\n        msr = valid.SCALEREL[~node.magScaleRel]()\n        return source.PointSource(\n            source_id=node['id'],\n            name=node['name'],\n            tectonic_region_type=node.attrib.get('tectonicRegion'),\n            mfd=self.convert_mfdist(node),\n            rupture_mesh_spacing=self.rupture_mesh_spacing,\n            magnitude_scaling_relationship=msr,\n            rupture_aspect_ratio=~node.ruptAspectRatio,\n            upper_seismogenic_depth=~geom.upperSeismoDepth,\n            lower_seismogenic_depth=~geom.lowerSeismoDepth,\n            location=geo.Point(*lon_lat),\n            nodal_plane_distribution=self.convert_npdist(node),\n            hypocenter_distribution=self.convert_hpdist(node),\n            temporal_occurrence_model=self.get_tom(node))", "category": "Python"}, {"instruction": "def __add_symbols(self, cmd):\n        \"\"\"\n        Add all additional defined and undefined symbols.\n\n        \"\"\"\n", "input": "", "output": "\n        if self.__config.define_symbols:\n            symbols = self.__config.define_symbols\n            cmd.append(''.join(\n                [' -D\"%s\"' % def_symbol for def_symbol in symbols]))\n\n        if self.__config.undefine_symbols:\n            un_symbols = self.__config.undefine_symbols\n            cmd.append(''.join(\n                [' -U\"%s\"' % undef_symbol for undef_symbol in un_symbols]))\n\n        return cmd", "category": "Python"}, {"instruction": "def get_certificate(self, **kwargs):\n        \"\"\"GetCertificate.\n        [Preview API]\n        :rtype: object\n        \"\"\"\n", "input": "", "output": "        response = self._send(http_method='GET',\n                              location_id='2e0dbce7-a327-4bc0-a291-056139393f6d',\n                              version='5.0-preview.1',\n                              accept_media_type='application/octet-stream')\n        if \"callback\" in kwargs:\n            callback = kwargs[\"callback\"]\n        else:\n            callback = None\n        return self._client.stream_download(response, callback=callback)", "category": "Python"}, {"instruction": "def create_sslcert(self, name, common_name, pri, ca):\n        \"\"\"\n        \u4fee\u6539\u8bc1\u4e66\uff0c\u6587\u6863 https://developer.qiniu.com/fusion/api/4246/the-domain-name#11\n\n        Args:\n           name:        \u8bc1\u4e66\u540d\u79f0\n           common_name: \u76f8\u5173\u57df\u540d\n           pri:         \u8bc1\u4e66\u79c1\u94a5\n           ca:          \u8bc1\u4e66\u5185\u5bb9\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56dedict{certID: <CertID>}\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n\n\n        \"\"\"\n", "input": "", "output": "        req = {}\n        req.update({\"name\": name})\n        req.update({\"common_name\": common_name})\n        req.update({\"pri\": pri})\n        req.update({\"ca\": ca})\n\n        body = json.dumps(req)\n        url = '{0}/sslcert'.format(self.server)\n        return self.__post(url, body)", "category": "Python"}, {"instruction": "def os_path_relpath(path, start=os.path.curdir):\n    \"\"\"Return a relative version of a path\"\"\"\n", "input": "", "output": "\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    start_list = [x for x in os.path.abspath(start).split(os.path.sep) if x]\n    path_list = [x for x in os.path.abspath(path).split(os.path.sep) if x]\n\n    # Work out how much of the filepath is shared by start and path.\n    i = len(os.path.commonprefix([start_list, path_list]))\n\n    rel_list = [os.path.pardir] * (len(start_list)-i) + path_list[i:]\n    if not rel_list:\n        return os.path.curdir\n    return os.path.join(*rel_list)", "category": "Python"}, {"instruction": "def author_view(self, context):\n        \"\"\"\n        Display a the studio editor when the user has clicked \"View\" to see the container view,\n        otherwise just show the normal 'author_preview_view' or 'student_view' preview.\n        \"\"\"\n", "input": "", "output": "        root_xblock = context.get('root_xblock')\n\n        if root_xblock and root_xblock.location == self.location:\n            # User has clicked the \"View\" link. Show an editable preview of this block's children\n            return self.author_edit_view(context)\n        return self.author_preview_view(context)", "category": "Python"}, {"instruction": "def fetch_all(self, org):\n        \"\"\"\n        Fetches all local objects\n        :param org: the org\n        :return: the queryset\n        \"\"\"\n", "input": "", "output": "        qs = self.model.objects.filter(org=org)\n        if self.local_backend_attr is not None:\n            qs = qs.filter(**{self.local_backend_attr: self.backend})\n        return qs", "category": "Python"}, {"instruction": "def get_channel(self, name):\n        \"\"\"\n        Details about an individual channel.\n\n        :param name: The channel name\n        :type name: str\n        \"\"\"\n", "input": "", "output": "        return self._api_get('/api/channels/{0}'.format(\n            urllib.parse.quote_plus(name)\n        ))", "category": "Python"}, {"instruction": "def save(self):\n        \"\"\"\n        IPAddress can only change its PTR record. Saves the current state, PUT /ip_address/uuid.\n        \"\"\"\n", "input": "", "output": "        body = {'ip_address': {'ptr_record': self.ptr_record}}\n        data = self.cloud_manager.request('PUT', '/ip_address/' + self.address, body)\n        self._reset(**data['ip_address'])", "category": "Python"}, {"instruction": "def get_checkpoint_path(model_path):\n    \"\"\"\n    Work around TF problems in checkpoint path handling.\n\n    Args:\n        model_path: a user-input path\n    Returns:\n        str: the argument that can be passed to NewCheckpointReader\n    \"\"\"\n", "input": "", "output": "    if os.path.basename(model_path) == model_path:\n        model_path = os.path.join('.', model_path)  # avoid #4921 and #6142\n    if os.path.basename(model_path) == 'checkpoint':\n        assert tfv1.gfile.Exists(model_path), model_path\n        model_path = tf.train.latest_checkpoint(os.path.dirname(model_path))\n        # to be consistent with either v1 or v2\n\n    # fix paths if provided a wrong one\n    new_path = model_path\n    if '00000-of-00001' in model_path:\n        new_path = model_path.split('.data')[0]\n    elif model_path.endswith('.index'):\n        new_path = model_path.split('.index')[0]\n    if new_path != model_path:\n        logger.info(\n            \"Checkpoint path {} is auto-corrected to {}.\".format(model_path, new_path))\n        model_path = new_path\n    assert tfv1.gfile.Exists(model_path) or tfv1.gfile.Exists(model_path + '.index'), model_path\n    return model_path", "category": "Python"}, {"instruction": "def get_groups(self):\n        '''Retrieve groups listed in ChemInventory'''\n", "input": "", "output": "        resp = self._post('general-retrievelocations', 'locations')\n        final_resp = []\n        if resp['groupinfo']:\n            for group in resp['groupinfo']:\n                final_resp.append(Group(\n                    name=group.get('name'),\n                    inventory_id=group.get('id')\n                ))\n        return final_resp", "category": "Python"}, {"instruction": "def format(self, record):\n    \"\"\"Format the record as tersely as possible but preserve info.\"\"\"\n", "input": "", "output": "    super(CliFormatter, self).format(record)\n    localized_time = datetime.datetime.fromtimestamp(record.created)\n    terse_time = localized_time.strftime(u'%H:%M:%S')\n    terse_level = record.levelname[0]\n    terse_name = record.name.split('.')[-1]\n    match = RECORD_LOGGER_RE.match(record.name)\n    if match:\n      # Figure out which OpenHTF subsystem the record came from.\n      subsys_match = SUBSYSTEM_LOGGER_RE.match(record.name)\n      if subsys_match:\n        terse_name = '<{subsys}: {id}>'.format(\n            subsys=subsys_match.group('subsys'),\n            id=subsys_match.group('id'))\n      else:\n        # Fall back to using the last five characters of the test UUID.\n        terse_name = '<test %s>' % match.group('test_uid')[-5:]\n    return '{lvl} {time} {logger} - {msg}'.format(lvl=terse_level,\n                                                  time=terse_time,\n                                                  logger=terse_name,\n                                                  msg=record.message)", "category": "Python"}, {"instruction": "def awsRetry(f):\n    \"\"\"\n    This decorator retries the wrapped function if aws throws unexpected errors\n    errors.\n    It should wrap any function that makes use of boto\n    \"\"\"\n", "input": "", "output": "    @wraps(f)\n    def wrapper(*args, **kwargs):\n        for attempt in retry(delays=truncExpBackoff(),\n                             timeout=300,\n                             predicate=awsRetryPredicate):\n            with attempt:\n                return f(*args, **kwargs)\n    return wrapper", "category": "Python"}, {"instruction": "def _get_merge_diff(self):\n        \"\"\"\n        The merge diff is not necessarily what needs to be loaded\n        for example under NTP, even though the 'ntp commit' command might be\n        alread configured, it is mandatory to be sent\n        otherwise it won't take the new configuration - see:\n        https://github.com/napalm-automation/napalm-nxos/issues/59\n        therefore this method will return the real diff (but not necessarily what is\n        being sent by the merge_load_config()\n        \"\"\"\n", "input": "", "output": "        diff = []\n        running_config = self.get_config(retrieve=\"running\")[\"running\"]\n        running_lines = running_config.splitlines()\n        for line in self.merge_candidate.splitlines():\n            if line not in running_lines and line:\n                if line[0].strip() != \"!\":\n                    diff.append(line)\n        return \"\\n\".join(diff)", "category": "Python"}, {"instruction": "def _basic_cancel(self, frame_in):\n        \"\"\"Handle a Basic Cancel frame.\n\n        :param specification.Basic.Cancel frame_in: Amqp frame.\n\n        :return:\n        \"\"\"\n", "input": "", "output": "        LOGGER.warning(\n            'Received Basic.Cancel on consumer_tag: %s',\n            try_utf8_decode(frame_in.consumer_tag)\n        )\n        self.remove_consumer_tag(frame_in.consumer_tag)", "category": "Python"}, {"instruction": "def compute_taxes(self, precision=None):\n        '''\n        Returns the total amount of taxes of this group.\n        @param precision:int Total amount of discounts\n        @return: Decimal\n        '''\n", "input": "", "output": "        return sum([line.compute_taxes(precision) for line in self.__lines])", "category": "Python"}, {"instruction": "def fmod(x, y, context=None):\n    \"\"\"\n    Return ``x`` reduced modulo ``y``.\n\n    Returns the value of x - n * y, where n is the integer quotient of x\n    divided by y, rounded toward zero.\n\n    Special values are handled as described in Section F.9.7.1 of the ISO C99\n    standard: If x is infinite or y is zero, the result is NaN. If y is\n    infinite and x is finite, the result is x rounded to the current context.\n    If the result is zero, it has the sign of x.\n\n    \"\"\"\n", "input": "", "output": "    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_fmod,\n        (\n            BigFloat._implicit_convert(x),\n            BigFloat._implicit_convert(y),\n        ),\n        context,\n    )", "category": "Python"}, {"instruction": "def reverse_dictionary(d):\n    \"\"\" Reverses the key value pairs for a given dictionary.\n\n    Parameters\n    ----------\n    d : :obj:`dict`\n        dictionary to reverse\n\n    Returns\n    -------\n    :obj:`dict`\n        dictionary with keys and values swapped\n    \"\"\"\n", "input": "", "output": "    rev_d = {}\n    [rev_d.update({v:k}) for k, v in d.items()]\n    return rev_d", "category": "Python"}, {"instruction": "def persons_significant_control(self, num, statements=False, **kwargs):\n        \"\"\"Search for a list of persons with significant control.\n\n        Searches for persons of significant control based on company number for\n        a specified company. Specify statements=True to only search for\n        officers with statements.\n\n        Args:\n            num (str, int): Company number to search on.\n            statements (Optional[bool]): Search only for persons with\n                statements. Default is False.\n            kwargs (dict): additional keywords passed into requests.session.get\n            *params* keyword.\n        \"\"\"\n", "input": "", "output": "        baseuri = (self._BASE_URI +\n                   'company/{}/persons-with-significant-control'.format(num))\n\n        # Only append statements to the URL if statements is True\n        if statements is True:\n            baseuri += '-statements'\n\n        res = self.session.get(baseuri, params=kwargs)\n        self.handle_http_error(res)\n        return res", "category": "Python"}, {"instruction": "def get_monomials(variables, degree):\n    \"\"\"Generates all noncommutative monomials up to a degree\n\n    :param variables: The noncommutative variables to generate monomials from\n    :type variables: list of :class:`sympy.physics.quantum.operator.Operator`\n                     or\n                     :class:`sympy.physics.quantum.operator.HermitianOperator`.\n    :param degree: The maximum degree.\n    :type degree: int.\n\n    :returns: list of monomials.\n    \"\"\"\n", "input": "", "output": "    if degree == -1:\n        return []\n    if not variables:\n        return [S.One]\n    else:\n        _variables = variables[:]\n        _variables.insert(0, 1)\n        ncmonomials = [S.One]\n        ncmonomials.extend(var for var in variables)\n        for var in variables:\n            if not is_hermitian(var):\n                ncmonomials.append(var.adjoint())\n        for _ in range(1, degree):\n            temp = []\n            for var in _variables:\n                for new_var in ncmonomials:\n                    temp.append(var * new_var)\n                    if var != 1 and not is_hermitian(var):\n                        temp.append(var.adjoint() * new_var)\n            ncmonomials = unique(temp[:])\n        return ncmonomials", "category": "Python"}, {"instruction": "def find_roots( disconnected, index, shared ):\n    \"\"\"Find appropriate \"root\" objects from which to recurse the hierarchies\n    \n    Will generate a synthetic root for anything which doesn't have any parents...\n    \"\"\"\n", "input": "", "output": "    log.warn( '%s disconnected objects in %s total objects', len(disconnected), len(index))\n    natural_roots = [x for x in disconnected if x.get('refs') and not x.get('parents')]\n    log.warn( '%s objects with no parents at all' ,len(natural_roots))\n    for natural_root in natural_roots:\n        recurse_module(\n            natural_root, index, shared\n        )\n        yield natural_root\n    rest = [x for x in disconnected if x.get( 'totsize' ) is None]\n    un_found = {\n        'type': 'module',\n        'name': '<disconnected objects>',\n        'children': rest,\n        'parents': [ ],\n        'size': 0,\n        'totsize': sum([x['size'] for x in rest],0),\n        'address': new_address( index ),\n    }\n    index[un_found['address']] = un_found\n    yield un_found", "category": "Python"}, {"instruction": "def node_stats(self, nodes=None):\n        \"\"\"\n        The cluster :ref:`nodes info <es-guide-reference-api-admin-cluster-nodes-stats>` API allows to retrieve one or more (or all) of\n        the cluster nodes information.\n        \"\"\"\n", "input": "", "output": "        parts = [\"_cluster\", \"nodes\", \"stats\"]\n        if nodes:\n            parts = [\"_cluster\", \"nodes\", \",\".join(nodes), \"stats\"]\n\n        path = make_path(*parts)\n        return self.conn._send_request('GET', path)", "category": "Python"}, {"instruction": "def get(self, id):\n        \"\"\"\n        Return an instance based on the given primary key identifier,\n        or None if not found.\n\n        This returns a record whether active or not.\n        \"\"\"\n", "input": "", "output": "        ctx = self.context.copy()\n        ctx['active_test'] = False\n        results = self.rpc_model.search_read(\n            [('id', '=', id)],\n            None, None, None, self.fields,\n            context=ctx\n        )\n        return results and results[0] or None", "category": "Python"}, {"instruction": "def get_song_lyric(self, song_id):\n        \"\"\"Get a song's lyric.\n\n        warning: use old api.\n        :params song_id: song id.\n        :return: a song's lyric.\n        \"\"\"\n", "input": "", "output": "\n        url = 'http://music.163.com/api/song/lyric?os=osx&id={}&lv=-1&kv=-1&tv=-1'.format(  # NOQA\n            song_id)\n        result = self.get_request(url)\n        if 'lrc' in result and result['lrc']['lyric'] is not None:\n            lyric_info = result['lrc']['lyric']\n        else:\n            lyric_info = 'Lyric not found.'\n        return lyric_info", "category": "Python"}, {"instruction": "def maxTreeDepthDivide(rootValue, currentDepth=0, parallelLevel=2):\n    \"\"\"Finds a tree node that represents rootValue and computes the max depth\n       of this tree branch.\n       This function will emit new futures until currentDepth=parallelLevel\"\"\"\n", "input": "", "output": "    thisRoot = shared.getConst('myTree').search(rootValue)\n    if currentDepth >= parallelLevel:\n        return thisRoot.maxDepth(currentDepth)\n    else:\n        # Base case\n        if not any([thisRoot.left, thisRoot.right]):\n            return currentDepth\n        if not all([thisRoot.left, thisRoot.right]):\n            return thisRoot.maxDepth(currentDepth)\n\n        # Parallel recursion\n        return max(\n            futures.map(\n                maxTreeDepthDivide,\n                [\n                    thisRoot.left.payload,\n                    thisRoot.right.payload,\n                ],\n                cycle([currentDepth + 1]),\n                cycle([parallelLevel]),\n            )\n        )", "category": "Python"}, {"instruction": "def _check_checksum(msg):\n    \"\"\"Ensure checksum in message is good.\"\"\"\n", "input": "", "output": "    checksum = int(msg[-2:], 16)\n    for char in msg[:-2]:\n        checksum += ord(char)\n    if (checksum % 256) != 0:\n        raise ValueError(\"Elk message checksum invalid\")", "category": "Python"}, {"instruction": "def get_protein_targets_only(target_chembl_ids):\n    \"\"\"Given list of ChEMBL target ids, return dict of SINGLE PROTEIN targets\n\n    Parameters\n    ----------\n    target_chembl_ids : list\n        list of chembl_ids as strings\n\n    Returns\n    -------\n    protein_targets : dict\n        dictionary keyed to ChEMBL target ids with lists of activity ids\n    \"\"\"\n", "input": "", "output": "    protein_targets = {}\n    for target_chembl_id in target_chembl_ids:\n        target = query_target(target_chembl_id)\n        if 'SINGLE PROTEIN' in target['target_type']:\n            protein_targets[target_chembl_id] = target\n    return protein_targets", "category": "Python"}, {"instruction": "def update(self, incr=1, force=False):\n        \"\"\"\n        Args:\n            incr(int): Amount to increment ``count`` (Default: 1)\n            force(bool): Force refresh even if ``min_delta`` has not been reached\n\n        Increment progress bar and redraw\n\n        Both this counter and the parent are incremented.\n\n        Progress bar is only redrawn if min_delta seconds past since the last update on the parent.\n        \"\"\"\n", "input": "", "output": "\n        self.count += incr\n        self.parent.update(incr, force)", "category": "Python"}, {"instruction": "def get_handlers(self, kind=None):\n        \"\"\"\n        Retrieves the handlers of the given kind. If kind is None, all handlers\n        are returned.\n\n        :param kind: The kind of the handlers to return\n        :return: A list of handlers, or an empty list\n        \"\"\"\n", "input": "", "output": "        with self._lock:\n            if kind is not None:\n                try:\n                    return self._handlers[kind][:]\n                except KeyError:\n                    return []\n\n            return self.__all_handlers.copy()", "category": "Python"}, {"instruction": "def get_monophyletic(self, values, target_attr):\n        \"\"\"\n        Returns a list of nodes matching the provided monophyly\n        criteria. For a node to be considered a match, all\n        `target_attr` values within and node, and exclusively them,\n        should be grouped.\n\n        :param values: a set of values for which monophyly is\n            expected.\n\n        :param target_attr: node attribute being used to check\n            monophyly (i.e. species for species trees, names for gene\n            family trees).\n        \"\"\"\n", "input": "", "output": "        if type(values) != set:\n            values = set(values)\n\n        n2values = self.get_cached_content(store_attr=target_attr)\n\n        is_monophyletic = lambda node: n2values[node] == values\n        for match in self.iter_leaves(is_leaf_fn=is_monophyletic):\n            if is_monophyletic(match):\n                yield match", "category": "Python"}, {"instruction": "def k_subsets(set_, k):\n    \"\"\"Return subsets of given set with given cardinality.\n    :param k: Cardinality of subsets to return\n    :return: Iterable containing all ``k``-subsets of given set\n    \"\"\"\n", "input": "", "output": "    ensure_countable(set_)\n\n    if not isinstance(k, Integral):\n        raise TypeError(\"subset cardinality must be a number\")\n    if not (k >= 0):\n        raise ValueError(\"subset cardinality must be positive\")\n    if not (k <= len(set_)):\n        raise ValueError(\"subset cardinality must not exceed set cardinality\")\n\n    result = combinations(set_, k)\n    return _harmonize_subset_types(set_, result)", "category": "Python"}, {"instruction": "def plot(self, n=500, eigenvalues=None, sum=None, title=None,\n             ax=None, **kwargs):\n        r\"\"\"Docstring overloaded at import time.\"\"\"\n", "input": "", "output": "        from pygsp.plotting import _plot_filter\n        return _plot_filter(self, n=n, eigenvalues=eigenvalues, sum=sum,\n                            title=title, ax=ax, **kwargs)", "category": "Python"}, {"instruction": "def add_eager_constraints(self, models):\n        \"\"\"\n        Set the constraints for an eager load of the relation.\n\n        :type models: list\n        \"\"\"\n", "input": "", "output": "        return self._query.where_in(\n            self._foreign_key, self.get_keys(models, self._local_key)\n        )", "category": "Python"}, {"instruction": "def _get_cache_key(self, **kwargs):\n        \"\"\"\n        Take this task's configured ``significant_kwargs`` and build a hash\n        that all equivalent task calls will match.\n\n        Takes in kwargs and returns a string.\n\n        To change the way the cache key is generated or do more in-depth\n        processing, override this method.\n        \"\"\"\n", "input": "", "output": "        m = md5()\n        for significant_kwarg in self.significant_kwargs:\n            key, to_str = significant_kwarg\n            try:\n                m.update(to_str(kwargs[key]))\n            except (TypeError, UnicodeEncodeError):\n                # Python 3.x strings aren't accepted by hash.update().\n                # String should be byte-encoded first.\n                m.update(to_str(kwargs[key]).encode('utf-8'))\n\n        if hasattr(self, 'cache_prefix'):\n            cache_prefix = self.cache_prefix\n        else:\n            cache_prefix = '%s.%s' % (self.__module__, self.__name__)\n        return '%s:%s' % (cache_prefix, m.hexdigest())", "category": "Python"}, {"instruction": "def _get_timezone(self, root):\n        \"\"\"Find timezone informatation on bottom of the page.\"\"\"\n", "input": "", "output": "        tz_str = root.xpath('//div[@class=\"smallfont\" and @align=\"center\"]')[0].text\n        hours = int(self._tz_re.search(tz_str).group(1))\n        return tzoffset(tz_str, hours * 60)", "category": "Python"}, {"instruction": "def get_options():\n    \"\"\"Return a list of acceptable QT APIs, in decreasing order of\n    preference\n    \"\"\"\n", "input": "", "output": "    #already imported Qt somewhere. Use that\n    loaded = loaded_api()\n    if loaded is not None:\n        return [loaded]\n\n    mpl = sys.modules.get('matplotlib', None)\n\n    if mpl is not None and not check_version(mpl.__version__, '1.0.2'):\n        #1.0.1 only supports PyQt4 v1\n        return [QT_API_PYQT_DEFAULT]\n\n    if os.environ.get('QT_API', None) is None:\n        #no ETS variable. Ask mpl, then use either\n        return matplotlib_options(mpl) or [QT_API_PYQT_DEFAULT, QT_API_PYSIDE, QT_API_PYQT5]\n\n    #ETS variable present. Will fallback to external.qt\n    return None", "category": "Python"}, {"instruction": "def calculate_size(name, interceptor):\n    \"\"\" Calculates the request payload size\"\"\"\n", "input": "", "output": "    data_size = 0\n    data_size += calculate_size_str(name)\n    data_size += calculate_size_data(interceptor)\n    return data_size", "category": "Python"}, {"instruction": "def init_app(self, app):\n        \"\"\"Configures the specified Flask app to enforce SSL.\"\"\"\n", "input": "", "output": "        app.config.setdefault('SSLIFY_AGE', self.defaults['age'])\n        app.config.setdefault('SSLIFY_SUBDOMAINS', self.defaults['subdomains'])\n        app.config.setdefault('SSLIFY_PERMANENT', self.defaults['permanent'])\n        app.config.setdefault('SSLIFY_SKIPS', self.defaults['skips'])\n\n        app.before_request(self.redirect_to_ssl)\n        app.after_request(self.set_hsts_header)", "category": "Python"}, {"instruction": "def get_sql(self):\n        \"\"\"\n        Builds and returns the WHERE portion of the sql\n\n        :return: the WHERE portion of the sql\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        # reset arg index and args\n        self.arg_index = 0\n        self.args = {}\n\n        # build the WHERE sql portion if needed\n        if len(self.wheres):\n            where = self.build_where_part(self.wheres)\n            return 'WHERE {0} '.format(where)\n        return ''", "category": "Python"}, {"instruction": "def command(self):\n        \"\"\"\n        Returns a string representing the command you have to type to\n        obtain the same packet\n        \"\"\"\n", "input": "", "output": "        f = []\n        for fn, fv in six.iteritems(self.fields):\n            fld = self.get_field(fn)\n            if isinstance(fv, (list, dict, set)) and len(fv) == 0:\n                continue\n            if isinstance(fv, Packet):\n                fv = fv.command()\n            elif fld.islist and fld.holds_packets and isinstance(fv, list):\n                fv = \"[%s]\" % \",\".join(map(Packet.command, fv))\n            elif isinstance(fld, FlagsField):\n                fv = int(fv)\n            else:\n                fv = repr(fv)\n            f.append(\"%s=%s\" % (fn, fv))\n        c = \"%s(%s)\" % (self.__class__.__name__, \", \".join(f))\n        pc = self.payload.command()\n        if pc:\n            c += \"/\" + pc\n        return c", "category": "Python"}, {"instruction": "def _get_svc_list(service_status):\n    '''\n    Returns all service statuses\n    '''\n", "input": "", "output": "    prefix = '/etc/rc.d/'\n    ret = set()\n    lines = glob.glob('{0}*'.format(prefix))\n    for line in lines:\n        svc = _get_svc(line, service_status)\n        if svc is not None:\n            ret.add(svc)\n\n    return sorted(ret)", "category": "Python"}, {"instruction": "def _data_frame(content):\n    \"\"\"\n    Helper funcation that converts text-based get response\n    to a pandas dataframe for additional manipulation.\n    \"\"\"\n", "input": "", "output": "    response = loads(content)\n    key = [x for x in response.keys() if x in c.response_data][0]\n    frame = DataFrame(response[key])\n    final_frame = _convert(frame)\n    return final_frame", "category": "Python"}, {"instruction": "def theta_join(self, node):\n        \"\"\"\n        Translate a join node into latex qtree node.\n        :param node: a treebrd node\n        :return: a qtree subtree rooted at the node\n        \"\"\"\n", "input": "", "output": "        return '[.${op}_{{{conditions}}}$ {left} {right} ]'\\\n            .format(op=latex_operator[node.operator],\n                    conditions=node.conditions,\n                    left=self.translate(node.left),\n                    right=self.translate(node.right))", "category": "Python"}, {"instruction": "def add_user(self, team, params={}, **options): \n        \"\"\"The user making this call must be a member of the team in order to add others.\n        The user to add must exist in the same organization as the team in order to be added.\n        The user to add can be referenced by their globally unique user ID or their email address.\n        Returns the full user record for the added user.\n\n        Parameters\n        ----------\n        team : {Id} Globally unique identifier for the team.\n        [data] : {Object} Data for the request\n          - user : {String} An identifier for the user. Can be one of an email address,\n          the globally unique identifier for the user, or the keyword `me`\n          to indicate the current user making the request.\n        \"\"\"\n", "input": "", "output": "        path = \"/teams/%s/addUser\" % (team)\n        return self.client.post(path, params, **options)", "category": "Python"}, {"instruction": "def get_country_long(self, ip):\n        ''' Get country_long '''\n", "input": "", "output": "        rec = self.get_all(ip)\n        return rec and rec.country_long", "category": "Python"}, {"instruction": "def get_string(self, prompt, default_str=None) -> str:\n        \"\"\"Return a string value that the user enters. Raises exception for cancel.\"\"\"\n", "input": "", "output": "        accept_event = threading.Event()\n        value_ref = [None]\n\n        def perform():\n            def accepted(text):\n                value_ref[0] = text\n                accept_event.set()\n\n            def rejected():\n                accept_event.set()\n\n            self.__message_column.remove_all()\n            pose_get_string_message_box(self.ui, self.__message_column, prompt, str(default_str), accepted, rejected)\n            #self.__message_column.add(self.__make_cancel_row())\n\n        with self.__lock:\n            self.__q.append(perform)\n            self.document_controller.add_task(\"ui_\" + str(id(self)), self.__handle_output_and_q)\n        accept_event.wait()\n        def update_message_column():\n            self.__message_column.remove_all()\n            self.__message_column.add(self.__make_cancel_row())\n        self.document_controller.add_task(\"ui_\" + str(id(self)), update_message_column)\n        if value_ref[0] is None:\n            raise Exception(\"Cancel\")\n        return value_ref[0]", "category": "Python"}, {"instruction": "def get(self, q, limit=None):\n        \"\"\"\n        Performs a search against the predict endpoint\n\n        :param q: query to be searched for [STRING]\n        :return: { score: [0|1] }\n        \"\"\"\n", "input": "", "output": "        uri = '{}/predict?q={}'.format(self.client.remote, q)\n        self.logger.debug(uri)\n\n        body = self.client.get(uri)\n        return body['score']", "category": "Python"}, {"instruction": "def before_update(mapper, conn, target):\n        \"\"\"Set the column id number based on the table number and the sequence\n        id for the column.\"\"\"\n", "input": "", "output": "\n        assert target.datatype or target.valuetype\n\n        target.name = Column.mangle_name(target.name)\n\n        Column.update_number(target)", "category": "Python"}, {"instruction": "def get_access_token(self, code):\n        \"\"\"Get new access token.\"\"\"\n", "input": "", "output": "        try:\n            self._token = super().fetch_token(\n                MINUT_TOKEN_URL,\n                client_id=self._client_id,\n                client_secret=self._client_secret,\n                code=code,\n            )\n        # except Exception as e:\n        except MissingTokenError as error:\n            _LOGGER.debug(\"Token issues: %s\", error)\n        return self._token", "category": "Python"}, {"instruction": "def _reset_sig_handler(self):\n        \"\"\"Restore the signal handlers to their previous state (before the\n         call to _setup_sig_handler().\"\"\"\n", "input": "", "output": "\n        signal.signal(signal.SIGINT, self.old_sigint)\n        signal.signal(signal.SIGTERM, self.old_sigterm)\n        try:\n            signal.signal(signal.SIGHUP, self.old_sighup)\n        except AttributeError:\n            pass", "category": "Python"}, {"instruction": "def remove_pipe(self, name):\n        \"\"\"Remove a component from the pipeline.\n\n        name (unicode): Name of the component to remove.\n        RETURNS (tuple): A `(name, component)` tuple of the removed component.\n\n        DOCS: https://spacy.io/api/language#remove_pipe\n        \"\"\"\n", "input": "", "output": "        if name not in self.pipe_names:\n            raise ValueError(Errors.E001.format(name=name, opts=self.pipe_names))\n        return self.pipeline.pop(self.pipe_names.index(name))", "category": "Python"}, {"instruction": "def buckets(self):\n        \"\"\"https://github.com/frictionlessdata/tableschema-bigquery-py#storage\n        \"\"\"\n", "input": "", "output": "\n        # No cached value\n        if self.__buckets is None:\n\n            # Get response\n            response = self.__service.tables().list(\n                projectId=self.__project,\n                datasetId=self.__dataset).execute()\n\n            # Extract buckets\n            self.__buckets = []\n            for table in response.get('tables', []):\n                table_name = table['tableReference']['tableId']\n                bucket = self.__mapper.restore_bucket(table_name)\n                if bucket is not None:\n                    self.__buckets.append(bucket)\n\n        return self.__buckets", "category": "Python"}, {"instruction": "def stream_url(self):\n        \"\"\"Build url for stream.\"\"\"\n", "input": "", "output": "        rtsp_url = RTSP_URL.format(\n            host=self.config.host, video=self.video_query,\n            audio=self.audio_query, event=self.event_query)\n        _LOGGER.debug(rtsp_url)\n        return rtsp_url", "category": "Python"}, {"instruction": "def set_default_proxy(proxy_type=None, addr=None, port=None, rdns=True, username=None, password=None):\n    \"\"\"\n    set_default_proxy(proxy_type, addr[, port[, rdns[, username, password]]])\n\n    Sets a default proxy which all further socksocket objects will use,\n    unless explicitly changed. All parameters are as for socket.set_proxy().\n    \"\"\"\n", "input": "", "output": "    socksocket.default_proxy = (proxy_type, addr, port, rdns,\n                                username.encode() if username else None,\n                                password.encode() if password else None)", "category": "Python"}, {"instruction": "def get_metadata(item):\n    \"\"\"Get metadata information from the distribution.\n\n    Depending on the package this may either be in METADATA or PKG-INFO\n\n    :param item: pkg_resources WorkingSet item\n    :returns: metadata resource as list of non-blank non-comment lines\n    \"\"\"\n", "input": "", "output": "    for metadata_key in ('METADATA', 'PKG-INFO'):\n        try:\n            metadata_lines = item.get_metadata_lines(metadata_key)\n            break\n        except (KeyError, IOError):\n            # The package will be shown in the report without any license information\n            # if a metadata key is not found.\n            metadata_lines = []\n\n    return metadata_lines", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"Start the connection to a transport.\"\"\"\n", "input": "", "output": "        connect_thread = threading.Thread(target=self._connect)\n        connect_thread.start()", "category": "Python"}, {"instruction": "def RegisterLateBindingCallback(target_name, callback, **kwargs):\n  \"\"\"Registers a callback to be invoked when the RDFValue named is declared.\"\"\"\n", "input": "", "output": "  _LATE_BINDING_STORE.setdefault(target_name, []).append((callback, kwargs))", "category": "Python"}, {"instruction": "def get_id(self):\n        \"\"\"\n        get unique identifier of this container\n\n        :return: str\n        \"\"\"\n", "input": "", "output": "        if self._id is None:\n            # FIXME: provide a better error message when key is not defined\n            self._id = self.inspect(refresh=False)[\"Id\"]\n        return self._id", "category": "Python"}, {"instruction": "def used_in_func(statement: str, filename: str = '<string>',\n                 mode: str = 'exec'):\n    '''Parse a Python statement and analyze the symbols used. The result\n    will be used to determine what variables a step depends upon.'''\n", "input": "", "output": "    try:\n        return get_used_in_func(ast.parse(statement, filename, mode))\n    except Exception as e:\n        raise RuntimeError(f'Failed to parse statement: {statement} {e}')", "category": "Python"}, {"instruction": "def _setup_logging():\n    \"\"\"Setup logging to log to nowhere by default.\n\n    For details, see:\n    http://docs.python.org/3/howto/logging.html#library-config\n\n    Internal function.\n    \"\"\"\n", "input": "", "output": "    import logging\n\n    logger = logging.getLogger('spotify-connect')\n    handler = logging.NullHandler()\n    logger.addHandler(handler)", "category": "Python"}, {"instruction": "def p_expr_minus_expr(p):\n    \"\"\" expr : expr MINUS expr\n    \"\"\"\n", "input": "", "output": "    p[0] = make_binary(p.lineno(2), 'MINUS', p[1], p[3], lambda x, y: x - y)", "category": "Python"}, {"instruction": "def _get_content_type_queryset(models_list):\n    \"\"\" Get list of services content types \"\"\"\n", "input": "", "output": "    content_type_ids = {c.id for c in ContentType.objects.get_for_models(*models_list).values()}\n    return ContentType.objects.filter(id__in=content_type_ids)", "category": "Python"}, {"instruction": "def permission_required_with_403(perm, login_url=None):\n    \"\"\"\n    Decorator for views that checks whether a user has a particular permission\n    enabled, redirecting to the login page or rendering a 403 as necessary.\n\n    See :meth:`django.contrib.auth.decorators.permission_required`.\n    \"\"\"\n", "input": "", "output": "    return user_passes_test_with_403(lambda u: u.has_perm(perm), login_url=login_url)", "category": "Python"}, {"instruction": "def contains(self, x, y):\n        \"\"\"\n        Returns true if the given point is contained within the\n        bounding box, where all boundaries of the box are\n        considered to be inclusive.\n        \"\"\"\n", "input": "", "output": "        left, bottom, right, top = self.aarect().lbrt()\n        return (left <= x <= right) and (bottom <= y <= top)", "category": "Python"}, {"instruction": "async def send_message(user_id: int, text: str, disable_notification: bool = False) -> bool:\n    \"\"\"\n    Safe messages sender\n\n    :param user_id:\n    :param text:\n    :param disable_notification:\n    :return:\n    \"\"\"\n", "input": "", "output": "    try:\n        await bot.send_message(user_id, text, disable_notification=disable_notification)\n    except exceptions.BotBlocked:\n        log.error(f\"Target [ID:{user_id}]: blocked by user\")\n    except exceptions.ChatNotFound:\n        log.error(f\"Target [ID:{user_id}]: invalid user ID\")\n    except exceptions.RetryAfter as e:\n        log.error(f\"Target [ID:{user_id}]: Flood limit is exceeded. Sleep {e.timeout} seconds.\")\n        await asyncio.sleep(e.timeout)\n        return await send_message(user_id, text)  # Recursive call\n    except exceptions.UserDeactivated:\n        log.error(f\"Target [ID:{user_id}]: user is deactivated\")\n    except exceptions.TelegramAPIError:\n        log.exception(f\"Target [ID:{user_id}]: failed\")\n    else:\n        log.info(f\"Target [ID:{user_id}]: success\")\n        return True\n    return False", "category": "Python"}, {"instruction": "def age(*paths):\n\t'''Return the minimum age of a set of files.\n\tReturns 0 if no paths are given.\n\tReturns time.time() if a path does not exist.'''\n", "input": "", "output": "\tif not paths:\n\t\treturn 0\n\n\tfor path in paths:\n\t\tif not os.path.exists(path):\n\t\t\treturn time.time()\n\n\treturn min([(time.time() - os.path.getmtime(path)) for path in paths])", "category": "Python"}, {"instruction": "def fasta_files_equal(seq_file1, seq_file2):\n    \"\"\"Check equality of a FASTA file to another FASTA file\n\n    Args:\n        seq_file1: Path to a FASTA file\n        seq_file2: Path to another FASTA file\n\n    Returns:\n        bool: If the sequences are the same\n\n    \"\"\"\n", "input": "", "output": "\n    # Load already set representative sequence\n    seq1 = SeqIO.read(open(seq_file1), 'fasta')\n\n    # Load kegg sequence\n    seq2 = SeqIO.read(open(seq_file2), 'fasta')\n\n    # Test equality\n    if str(seq1.seq) == str(seq2.seq):\n        return True\n    else:\n        return False", "category": "Python"}, {"instruction": "def utf8_normalize(input_filename, comment_char='#', to_upper=False):\n    \"\"\"Normalize UTF-8 characters of a file\n    \"\"\"\n", "input": "", "output": "    # Prepare the input dictionary file in UTF-8 and NFC\n    temp_dict_fd, output_filename = tempfile.mkstemp()\n\n    logging.debug('to_nfc from file {} to file {}'.format(input_filename, output_filename))\n\n    with codecs.open(output_filename, 'w', 'utf-8') as f:\n        with codecs.open(input_filename, 'r', 'utf-8') as input_f:\n            lines = sorted([unicodedata.normalize(UTF8_NORMALIZATION, l)\n                            for l in input_f.readlines()\n                            if not l.strip().startswith(comment_char)])\n            if to_upper:\n                lines = [l.upper() for l in lines]\n\n            f.writelines(lines)\n\n    return output_filename", "category": "Python"}, {"instruction": "def select_directory(self):\r\n        \"\"\"Select directory\"\"\"\n", "input": "", "output": "        basedir = to_text_string(self.wd_edit.text())\r\n        if not osp.isdir(basedir):\r\n            basedir = getcwd_or_home()\r\n        directory = getexistingdirectory(self, _(\"Select directory\"), basedir)\r\n        if directory:\r\n            self.wd_edit.setText(directory)\r\n            self.dir = directory", "category": "Python"}, {"instruction": "def recent(self, include=None):\n        \"\"\"\n        Retrieve the most recent tickets\n        \"\"\"\n", "input": "", "output": "        return self._query_zendesk(self.endpoint.recent, 'ticket', id=None, include=include)", "category": "Python"}, {"instruction": "def inverse_prediction(self, s_g):\n        \"\"\" Compute a motor command to reach the sensory goal s_g. It is a shortcut for self.infer(self.conf.s_dims, self.conf.m_dims, s_g)\n        \"\"\"\n", "input": "", "output": "        return self.infer(self.conf.s_dims, self.conf.m_dims, s_g)", "category": "Python"}, {"instruction": "def iter_block_items(self):\n        \"\"\"\n        Generate a reference to each of the block-level content elements in\n        this cell, in the order they appear.\n        \"\"\"\n", "input": "", "output": "        block_item_tags = (qn('w:p'), qn('w:tbl'), qn('w:sdt'))\n        for child in self:\n            if child.tag in block_item_tags:\n                yield child", "category": "Python"}, {"instruction": "def _make_annulus_path(patch_inner, patch_outer):\n        \"\"\"\n        Defines a matplotlib annulus path from two patches.\n\n        This preserves the cubic Bezier curves (CURVE4) of the aperture\n        paths.\n\n        # This is borrowed from photutils aperture.\n        \"\"\"\n", "input": "", "output": "\n        import matplotlib.path as mpath\n\n        path_inner = patch_inner.get_path()\n        transform_inner = patch_inner.get_transform()\n        path_inner = transform_inner.transform_path(path_inner)\n\n        path_outer = patch_outer.get_path()\n        transform_outer = patch_outer.get_transform()\n        path_outer = transform_outer.transform_path(path_outer)\n\n        verts_inner = path_inner.vertices[:-1][::-1]\n        verts_inner = np.concatenate((verts_inner, [verts_inner[-1]]))\n\n        verts = np.vstack((path_outer.vertices, verts_inner))\n        codes = np.hstack((path_outer.codes, path_inner.codes))\n\n        return mpath.Path(verts, codes)", "category": "Python"}, {"instruction": "async def list(self) -> List[str]:\n        \"\"\"\n        Return list of pool names configured, empty list for none.\n\n        :return: list of pool names.\n        \"\"\"\n", "input": "", "output": "\n        LOGGER.debug('NodePoolManager.list >>>')\n\n        rv = [p['pool'] for p in await pool.list_pools()]\n\n        LOGGER.debug('NodePoolManager.list <<< %s', rv)\n        return rv", "category": "Python"}, {"instruction": "def GetParent(self):\n    \"\"\"Constructs a path info corresponding to the parent of current path.\n\n    The root path (represented by an empty list of components, corresponds to\n    `/` on Unix-like systems) does not have a parent.\n\n    Returns:\n      Instance of `rdf_objects.PathInfo` or `None` if parent does not exist.\n    \"\"\"\n", "input": "", "output": "    if self.root:\n      return None\n\n    return PathInfo(\n        components=self.components[:-1],\n        path_type=self.path_type,\n        directory=True)", "category": "Python"}, {"instruction": "def check_basic_auth(user, passwd):\n    \"\"\"Checks user authentication using HTTP Basic Auth.\"\"\"\n", "input": "", "output": "\n    auth = request.authorization\n    return auth and auth.username == user and auth.password == passwd", "category": "Python"}, {"instruction": "def _start_path(self, sp):\n        \"\"\"Return a newly created `a:path` element added to *sp*.\n\n        The returned `a:path` element has an `a:moveTo` element representing\n        the shape starting point as its only child.\n        \"\"\"\n", "input": "", "output": "        path = sp.add_path(w=self._dx, h=self._dy)\n        path.add_moveTo(\n            *self._local_to_shape(\n                self._start_x, self._start_y\n            )\n        )\n        return path", "category": "Python"}, {"instruction": "def listify(func=None, wrapper=list):\n    \"\"\"\n    A decorator which wraps a function's return value in ``list(...)``.\n    Useful when an algorithm can be expressed more cleanly as a generator but\n    the function should return an list.\n    Example::\n        >>> @listify\n        ... def get_lengths(iterable):\n        ...     for i in iterable:\n        ...         yield len(i)\n        >>> get_lengths([\"spam\", \"eggs\"])\n        [4, 4]\n        >>>\n        >>> @listify(wrapper=tuple)\n        ... def get_lengths_tuple(iterable):\n        ...     for i in iterable:\n        ...         yield len(i)\n        >>> get_lengths_tuple([\"foo\", \"bar\"])\n        (3, 3)\n    \"\"\"\n", "input": "", "output": "\n    def _listify_return(func):\n        @functools.wraps(func)\n        def _listify_helper(*args, **kw):\n            return wrapper(func(*args, **kw))\n\n        return _listify_helper\n\n    if func is None:\n        return _listify_return\n    return _listify_return(func)", "category": "Python"}, {"instruction": "def _on_move(self, event):\n        \"\"\"Make the cross follow the cursor.\"\"\"\n", "input": "", "output": "        w = self.winfo_width()\n        h = self.winfo_height()\n        x = min(max(event.x, 0), w)\n        y = min(max(event.y, 0), h)\n        self.coords('cross_h', 0, y, w, y)\n        self.coords('cross_v', x, 0, x, h)\n        self.event_generate(\"<<ColorChanged>>\")", "category": "Python"}, {"instruction": "def exec_command(command, **kwargs):\n    \"\"\"\n    Executes the given command and send the output to the console\n\n    :param str|list command:\n\n    :kwargs:\n        * `shell`  (``bool`` = False) --\n        * `stdin`  (``*`` = None)     --\n        * `stdout` (``*`` = None)     --\n        * `stderr` (``*`` = None)     --\n\n    :return: CommandReturnValue\n    \"\"\"\n", "input": "", "output": "\n    shell = kwargs.get('shell', False)\n    stdin = kwargs.get('stdin', None)\n    stdout = kwargs.get('stdout', None)\n    stderr = kwargs.get('stderr', None)\n\n    kwargs.update(shell=shell)\n    kwargs.update(stdin=stdin)\n    kwargs.update(stdout=stdout)\n    kwargs.update(stderr=stderr)\n\n    if not isinstance(command, list):\n        command = shlex.split(command)\n\n    return_value = subprocess.call(command, **kwargs)\n\n    return CommandReturnValue(return_value=return_value,\n                              stdin=stdin,\n                              stdout=stdout,\n                              stderr=stderr)", "category": "Python"}, {"instruction": "def rgb2gray(rgb):\n    \"\"\"Convert an RGB image (or images) to grayscale.\n\n    Parameters\n    ----------\n    rgb : ndarray\n      RGB image as Nr x Nc x 3 or Nr x Nc x 3 x K array\n\n    Returns\n    -------\n    gry : ndarray\n      Grayscale image as Nr x Nc or Nr x Nc x K array\n    \"\"\"\n", "input": "", "output": "\n    w = sla.atleast_nd(rgb.ndim, np.array([0.299, 0.587, 0.144],\n                                          dtype=rgb.dtype, ndmin=3))\n    return np.sum(w * rgb, axis=2)", "category": "Python"}, {"instruction": "def _datapaths(self):\n        \"\"\"Returns a simple key-value map for easy access to data paths\"\"\"\n", "input": "", "output": "        paths = { }\n        try:\n            data = self._config['data']\n            for k in data:\n                paths[k] = data[k]['path']\n        except KeyError as e:\n            raise AitConfigMissing(e.message)\n        except Exception as e:\n            raise AitConfigError('Error reading data paths: %s' % e)\n\n        return paths", "category": "Python"}, {"instruction": "def read_file(filename):\n    \"\"\"\n    Reads in the file I created manually (by recompiling and adding timers)\n    \n    Not used any more but left for historical reasons (the first 'speedup'\n    plots were generated with this function)\n    \"\"\"\n", "input": "", "output": "    dtype = np.dtype([('same_cell', np.int32),\n                      ('N1', np.int),\n                      ('N2', np.int),\n                      ('time', np.float)\n                      ])\n    if pd is not None:\n        timings = pd.read_csv(filename, header=None,\n                              engine=\"c\",\n                              dtype={'same_cell': np.int32,\n                                     'N1': np.int,\n                                     'N2': np.int,\n                                     'time': np.float},\n                              index_col=None,\n                              names=['same_cell', 'N1', 'N2', 'time'],\n                              delim_whitespace=True)\n    else:\n        timings = np.loadtxt(filename, dtype=dtype)\n    return timings", "category": "Python"}, {"instruction": "def parse_column_key(setting_string):\n        \"\"\"\n        Parses 'setting_string' as str formatted in <column>[:<key>]\n        and returns str type 'column' and 'key'\n        \"\"\"\n", "input": "", "output": "        if ':' in setting_string:\n            # splits <column>:<key> into <column> and <key>\n            column, key = setting_string.split(':', 1)\n        else:\n            # stores <column> and <value>=None\n            column = setting_string\n            key = None\n\n        return column, key", "category": "Python"}, {"instruction": "def unique_field_data_types(self):\n        \"\"\"\n        Checks if all variants have different data types.\n\n        If so, the selected variant can be determined just by the data type of\n        the value without needing a field name / tag. In some languages, this\n        lets us make a shortcut\n        \"\"\"\n", "input": "", "output": "        data_type_names = set()\n        for field in self.fields:\n            if not is_void_type(field.data_type):\n                if field.data_type.name in data_type_names:\n                    return False\n                else:\n                    data_type_names.add(field.data_type.name)\n        else:\n            return True", "category": "Python"}, {"instruction": "def check_output(self, cmd):\n        \"\"\"\n        Execute a shell command remotely and return the output.\n\n        Simplified version of Popen when you only want the output as a string and detect any errors.\n        \"\"\"\n", "input": "", "output": "        p = self.Popen(cmd, stdout=subprocess.PIPE)\n        output, _ = p.communicate()\n        if p.returncode != 0:\n            raise RemoteCalledProcessError(p.returncode, cmd, self.host, output=output)\n        return output", "category": "Python"}, {"instruction": "def get_event(self, listener, timeout):\n        \"\"\"Get events from this peer's event queue (for passive mode). Calling this method\n        regularly is required for passive event listeners to avoid system overload;\n        see :py:func:`IEventSource.register_listener`  for details.\n\n        in listener of type :class:`IEventListener`\n            Which listener to get data for.\n\n        in timeout of type int\n            Maximum time to wait for events, in ms;\n            0 = no wait, -1 = indefinite wait.\n\n        return event of type :class:`IEvent`\n            Event retrieved, or null if none available.\n\n        raises :class:`VBoxErrorObjectNotFound`\n            Listener is not registered, or autounregistered.\n        \n        \"\"\"\n", "input": "", "output": "        if not isinstance(listener, IEventListener):\n            raise TypeError(\"listener can only be an instance of type IEventListener\")\n        if not isinstance(timeout, baseinteger):\n            raise TypeError(\"timeout can only be an instance of type baseinteger\")\n        event = self._call(\"getEvent\",\n                     in_p=[listener, timeout])\n        event = IEvent(event)\n        return event", "category": "Python"}, {"instruction": "def kurtosis(x):\n    \"\"\"\n    Returns the kurtosis of x (calculated with the adjusted Fisher-Pearson standardized\n    moment coefficient G2).\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n", "input": "", "output": "    if not isinstance(x, pd.Series):\n        x = pd.Series(x)\n    return pd.Series.kurtosis(x)", "category": "Python"}, {"instruction": "def md_to_pdf(input_name, output_name):\n    \"\"\"\n    Converts an input MarkDown file to a PDF of the given output name.\n\n    Parameters\n    ==========\n    input_name : String\n    Relative file location of the input file to where this function is being called.\n\n    output_name : String\n    Relative file location of the output file to where this function is being called. Note that .pdf can be omitted.\n\n    Examples\n    ========\n    Suppose we have a directory as follows:\n    data/\n        doc.md\n    \n    To convert the document:\n    >>> from aide_document import convert\n    >>> convert.md_to_pdf('data/doc.md', 'data/doc.pdf')\n\n    .pdf can also be omitted from the second argument.\n    \"\"\"\n", "input": "", "output": "\n    if output_name[-4:] == '.pdf':\n        os.system(\"pandoc \" + input_name + \" -o \" + output_name)\n    else:\n        os.system(\"pandoc \" + input_name + \" -o \" + output_name + \".pdf\" )", "category": "Python"}, {"instruction": "def get_SCAT(points, low_bound, high_bound, x_max, y_max):\n    \"\"\"\n    runs SCAT test and returns boolean\n    \"\"\"\n", "input": "", "output": "    # iterate through all relevant points and see if any of them fall outside of your SCAT box\n    SCAT = True\n    for point in points:\n        result = in_SCAT_box(point[0], point[1], low_bound, high_bound, x_max, y_max)\n        if result == False:\n           # print \"SCAT TEST FAILED\"\n            SCAT = False\n    return SCAT", "category": "Python"}, {"instruction": "def expand_query(config, kwds):\n    \"\"\"\n    Expand `kwds` based on `config.search.query_expander`.\n\n    :type config: .config.Configuration\n    :type kwds: dict\n    :rtype: dict\n    :return: Return `kwds`, modified in place.\n\n    \"\"\"\n", "input": "", "output": "    pattern = []\n    for query in kwds.pop('pattern', []):\n        expansion = config.search.alias.get(query)\n        if expansion is None:\n            pattern.append(query)\n        else:\n            parser = SafeArgumentParser()\n            search_add_arguments(parser)\n            ns = parser.parse_args(expansion)\n            for (key, value) in vars(ns).items():\n                if isinstance(value, (list, tuple)):\n                    if not kwds.get(key):\n                        kwds[key] = value\n                    else:\n                        kwds[key].extend(value)\n                else:\n                    kwds[key] = value\n    kwds['pattern'] = pattern\n    return config.search.kwds_adapter(kwds)", "category": "Python"}, {"instruction": "def count(self, Class, set=None, recursive=True,ignore=True):\n        \"\"\"See :meth:`AbstractElement.count`\"\"\"\n", "input": "", "output": "        if self.mode == Mode.MEMORY:\n            s = 0\n            for t in self.data:\n                s +=  sum( 1 for e in t.select(Class,recursive,True ) )\n            return s", "category": "Python"}, {"instruction": "def set_angle_limit(self, limit_for_id, **kwargs):\n        \"\"\" Sets the angle limit to the specified motors. \"\"\"\n", "input": "", "output": "        convert = kwargs['convert'] if 'convert' in kwargs else self._convert\n\n        if 'wheel' in self.get_control_mode(limit_for_id.keys()):\n            raise ValueError('can not change the angle limit of a motor in wheel mode')\n\n        if (0, 0) in limit_for_id.values():\n            raise ValueError('can not set limit to (0, 0)')\n\n        self._set_angle_limit(limit_for_id, convert=convert)", "category": "Python"}, {"instruction": "def move(self, queue, delay=0, depends=None):\n        '''Move this job out of its existing state and into another queue. If\n        a worker has been given this job, then that worker's attempts to\n        heartbeat that job will fail. Like ``Queue.put``, this accepts a\n        delay, and dependencies'''\n", "input": "", "output": "        logger.info('Moving %s to %s from %s',\n            self.jid, queue, self.queue_name)\n        return self.client('put', self.worker_name,\n            queue, self.jid, self.klass_name,\n            json.dumps(self.data), delay, 'depends', json.dumps(depends or [])\n        )", "category": "Python"}, {"instruction": "def paragraphs(self, index = None):\n        \"\"\"Returns a generator of Paragraph elements found (recursively) under this element.\n\n        Arguments:\n            index (int or None): If set to an integer, will retrieve and return the n'th element (starting at 0) instead of returning the generator of all\n        \"\"\"\n", "input": "", "output": "        if index is None:\n            return self.select(Paragraph,None,True,default_ignore_structure)\n        else:\n            if index < 0:\n                index = self.count(Paragraph,None,True,default_ignore_structure) + index\n            for i,e in enumerate(self.select(Paragraph,None,True,default_ignore_structure)):\n                if i == index:\n                    return e\n            raise IndexError", "category": "Python"}, {"instruction": "def app(config, src, dst, features, reload, force):\n    \"\"\"Takes several files or directories as src and generates the code\n    in the given dst directory.\"\"\"\n", "input": "", "output": "    config = Path(config)\n    if reload:\n        argv = sys.argv.copy()\n        argv.remove('--reload')\n        monitor(config.dirname(), src, dst, argv)\n    else:\n        run(config, src, dst, force)", "category": "Python"}, {"instruction": "def render(self, *args, **kwargs):\n        \"\"\"Render widget and register it in Django's cache.\"\"\"\n", "input": "", "output": "        output = super(HeavySelect2Mixin, self).render(*args, **kwargs)\n        self.set_to_cache()\n        return output", "category": "Python"}, {"instruction": "def _get_binding_info(hostheader='', ipaddress='*', port=80):\n    '''\n    Combine the host header, IP address, and TCP port into bindingInformation format.\n    '''\n", "input": "", "output": "    ret = r'{0}:{1}:{2}'.format(ipaddress, port, hostheader.replace(' ', ''))\n\n    return ret", "category": "Python"}, {"instruction": "def NRMSE_sliding(data, pred, windowSize):\n  \"\"\"\n  Computing NRMSE in a sliding window\n  :param data:\n  :param pred:\n  :param windowSize:\n  :return: (window_center, NRMSE)\n  \"\"\"\n", "input": "", "output": "\n  halfWindowSize = int(round(float(windowSize)/2))\n  window_center = range(halfWindowSize, len(data)-halfWindowSize, int(round(float(halfWindowSize)/5.0)))\n  nrmse = []\n  for wc in window_center:\n    nrmse.append(NRMSE(data[wc-halfWindowSize:wc+halfWindowSize],\n                       pred[wc-halfWindowSize:wc+halfWindowSize]))\n\n  return (window_center, nrmse)", "category": "Python"}, {"instruction": "def get(cls, resource_id):\n        \"\"\"Returns the class object identified by `resource_id`\n\n        Args:\n            resource_id (str): Unique EC2 Instance ID to load from database\n\n        Returns:\n            EC2 Instance object if found, else None\n        \"\"\"\n", "input": "", "output": "        res = Resource.get(resource_id)\n        return cls(res) if res else None", "category": "Python"}, {"instruction": "def get(self, request, *args, **kwargs):\n        \"\"\"Wraps super().get(...) in order to return 404 status code if\n        the page parameter is invalid\n        \"\"\"\n", "input": "", "output": "        response = super().get(request, args, kwargs)\n        try:\n            response.render()\n        except Http404:\n            request.GET = request.GET.copy()\n            request.GET['page'] = '1'\n            response = super().get(request, args, kwargs)\n            response.status_code = 404\n\n        return response", "category": "Python"}, {"instruction": "def recv_all(self, timeout='default'):\n        \"\"\"\n        Return all data recieved until connection closes.\n\n        Aliases: read_all, readall, recvall\n        \"\"\"\n", "input": "", "output": "\n        self._print_recv_header('======== Receiving until close{timeout_text} ========', timeout)\n\n        return self._recv_predicate(lambda s: 0, timeout, raise_eof=False)", "category": "Python"}, {"instruction": "def convert_descriptor(self, descriptor):\n        \"\"\"Convert descriptor to BigQuery\n        \"\"\"\n", "input": "", "output": "\n        # Fields\n        fields = []\n        fallbacks = []\n        schema = tableschema.Schema(descriptor)\n        for index, field in enumerate(schema.fields):\n            converted_type = self.convert_type(field.type)\n            if not converted_type:\n                converted_type = 'STRING'\n                fallbacks.append(index)\n            mode = 'NULLABLE'\n            if field.required:\n                mode = 'REQUIRED'\n            fields.append({\n                'name': _slugify_field_name(field.name),\n                'type': converted_type,\n                'mode': mode,\n            })\n\n        # Descriptor\n        converted_descriptor = {\n            'fields': fields,\n        }\n\n        return (converted_descriptor, fallbacks)", "category": "Python"}, {"instruction": "def ping_entry(self, entry):\n        \"\"\"\n        Ping an entry to a directory.\n        \"\"\"\n", "input": "", "output": "        entry_url = '%s%s' % (self.ressources.site_url,\n                              entry.get_absolute_url())\n        categories = '|'.join([c.title for c in entry.categories.all()])\n\n        try:\n            reply = self.server.weblogUpdates.extendedPing(\n                self.ressources.current_site.name,\n                self.ressources.blog_url, entry_url,\n                self.ressources.blog_feed, categories)\n        except Exception:\n            try:\n                reply = self.server.weblogUpdates.ping(\n                    self.ressources.current_site.name,\n                    self.ressources.blog_url, entry_url,\n                    categories)\n            except Exception:\n                reply = {'message': '%s is an invalid directory.' %\n                         self.server_name,\n                         'flerror': True}\n        return reply", "category": "Python"}, {"instruction": "def do_round(value, precision=0, method='common'):\n    \"\"\"Round the number to a given precision. The first\n    parameter specifies the precision (default is ``0``), the\n    second the rounding method:\n\n    - ``'common'`` rounds either up or down\n    - ``'ceil'`` always rounds up\n    - ``'floor'`` always rounds down\n\n    If you don't specify a method ``'common'`` is used.\n\n    .. sourcecode:: jinja\n\n        {{ 42.55|round }}\n            -> 43.0\n        {{ 42.55|round(1, 'floor') }}\n            -> 42.5\n\n    Note that even if rounded to 0 precision, a float is returned.  If\n    you need a real integer, pipe it through `int`:\n\n    .. sourcecode:: jinja\n\n        {{ 42.55|round|int }}\n            -> 43\n    \"\"\"\n", "input": "", "output": "    if not method in ('common', 'ceil', 'floor'):\n        raise FilterArgumentError('method must be common, ceil or floor')\n    if method == 'common':\n        return round(value, precision)\n    func = getattr(math, method)\n    return func(value * (10 ** precision)) / (10 ** precision)", "category": "Python"}, {"instruction": "def round_float(f, digits, rounding=ROUND_HALF_UP):\n    \"\"\"\n    Accurate float rounding from http://stackoverflow.com/a/15398691.\n    \"\"\"\n", "input": "", "output": "    return Decimal(str(f)).quantize(Decimal(10) ** (-1 * digits),\n                                    rounding=rounding)", "category": "Python"}, {"instruction": "def invocation():\n    \"\"\"reconstructs the invocation for this python program\"\"\"\n", "input": "", "output": "    cmdargs = [sys.executable] + sys.argv[:]\n    invocation = \" \".join(shlex.quote(s) for s in cmdargs)\n    return invocation", "category": "Python"}, {"instruction": "def sync_s3(self):\n        \"\"\"Walk the media/static directories and syncs files to S3\"\"\"\n", "input": "", "output": "        bucket, key = self.open_s3()\n        for directory in self.DIRECTORIES:\n            for root, dirs, files in os.walk(directory):\n                self.upload_s3((bucket, key, self.AWS_BUCKET_NAME, directory), root, files, dirs)", "category": "Python"}, {"instruction": "def raise_if_error(frame):\n    \"\"\"\n    Checks a frame and raises the relevant exception if required.\n    \"\"\"\n", "input": "", "output": "    if \"status\" not in frame or frame[\"status\"] == b\"\\x00\":\n        return\n    codes_and_exceptions = {\n        b\"\\x01\": exceptions.ZigBeeUnknownError,\n        b\"\\x02\": exceptions.ZigBeeInvalidCommand,\n        b\"\\x03\": exceptions.ZigBeeInvalidParameter,\n        b\"\\x04\": exceptions.ZigBeeTxFailure\n    }\n    if frame[\"status\"] in codes_and_exceptions:\n        raise codes_and_exceptions[frame[\"status\"]]()\n    raise exceptions.ZigBeeUnknownStatus()", "category": "Python"}, {"instruction": "def update_webhook_metadata(self, policy, webhook, metadata):\n        \"\"\"\n        Adds the given metadata dict to the existing metadata for the specified\n        webhook.\n        \"\"\"\n", "input": "", "output": "        return self.manager.update_webhook_metadata(self, policy, webhook,\n                metadata)", "category": "Python"}, {"instruction": "def ParseFileObject(self, parser_mediator, file_object):\n    \"\"\"Parses a Windows EventLog (EVT) file-like object.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      file_object (dfvfs.FileIO): a file-like object.\n    \"\"\"\n", "input": "", "output": "    evt_file = pyevt.file()\n    evt_file.set_ascii_codepage(parser_mediator.codepage)\n\n    try:\n      evt_file.open_file_object(file_object)\n    except IOError as exception:\n      parser_mediator.ProduceExtractionWarning(\n          'unable to open file with error: {0!s}'.format(exception))\n      return\n\n    try:\n      self._ParseRecords(parser_mediator, evt_file)\n    finally:\n      evt_file.close()", "category": "Python"}, {"instruction": "def get_sum_fluxes(self):\n        \"\"\"Get the sum of the fluxes calculated so far.\n\n        >>> from hydpy.models.test_v1 import *\n        >>> parameterstep()\n        >>> fluxes.q = 0.0\n        >>> fluxes.fastaccess._q_sum = 1.0\n        >>> model.get_sum_fluxes()\n        >>> fluxes.q\n        q(1.0)\n        \"\"\"\n", "input": "", "output": "        fluxes = self.sequences.fluxes\n        for flux in fluxes.numerics:\n            flux(getattr(fluxes.fastaccess, '_%s_sum' % flux.name))", "category": "Python"}, {"instruction": "def chdir(self, directory, browsing_history=False,\r\n              refresh_explorer=True, refresh_console=True):\r\n        \"\"\"Set directory as working directory\"\"\"\n", "input": "", "output": "        if directory:\r\n            directory = osp.abspath(to_text_string(directory))\r\n\r\n        # Working directory history management\r\n        if browsing_history:\r\n            directory = self.history[self.histindex]\r\n        elif directory in self.history:\r\n            self.histindex = self.history.index(directory)\r\n        else:\r\n            if self.histindex is None:\r\n                self.history = []\r\n            else:\r\n                self.history = self.history[:self.histindex+1]\r\n            self.history.append(directory)\r\n            self.histindex = len(self.history)-1\r\n        \r\n        # Changing working directory\r\n        try:\r\n            os.chdir(directory)\r\n            if refresh_explorer:\r\n                self.set_explorer_cwd.emit(directory)\r\n            if refresh_console:\r\n                self.set_current_console_wd.emit(directory)\r\n            self.refresh_findinfiles.emit()\r\n        except OSError:\r\n            self.history.pop(self.histindex)\r\n        self.refresh_plugin()", "category": "Python"}, {"instruction": "def find_organization(session, name):\n    \"\"\"Find an organization.\n\n    Find an organization by its `name` using the given `session`.\n    When the organization does not exist the function will\n    return `None`.\n\n    :param session: database session\n    :param name: name of the organization to find\n\n    :returns: an organization object; `None` when the organization\n        does not exist\n    \"\"\"\n", "input": "", "output": "    organization = session.query(Organization). \\\n        filter(Organization.name == name).first()\n\n    return organization", "category": "Python"}, {"instruction": "def expiration_extractor(widget, data):\n    \"\"\"Extract expiration information.\n\n    - If active flag not set, Account is disabled (value 0).\n    - If active flag set and value is UNSET, account never expires.\n    - If active flag set and datetime choosen, account expires at given\n      datetime.\n    - Timestamp in seconds since epoch is returned.\n    \"\"\"\n", "input": "", "output": "    active = int(data.request.get('%s.active' % widget.name, '0'))\n    if not active:\n        return 0\n    expires = data.extracted\n    if expires:\n        return time.mktime(expires.utctimetuple())\n    return UNSET", "category": "Python"}, {"instruction": "def escape(self, value):\n        \"\"\"Escape given value.\"\"\"\n", "input": "", "output": "        value = soft_unicode(value)\n\n        if self._engine._escape is None:  # pylint: disable=protected-access\n            return value\n\n        return self._engine._escape(value)", "category": "Python"}, {"instruction": "def GetNewSessionID(self, **_):\n    \"\"\"Returns a random integer session ID for this hunt.\n\n    All hunts are created under the aff4:/hunts namespace.\n\n    Returns:\n      a formatted session id string.\n    \"\"\"\n", "input": "", "output": "    return rdfvalue.SessionID(base=\"aff4:/hunts\", queue=self.runner_args.queue)", "category": "Python"}, {"instruction": "def lookup(self, tmp):\n        \"\"\"\n        Return the type of temporary variable `tmp` as an enum string\n        \"\"\"\n", "input": "", "output": "        if tmp < 0 or tmp > self.types_used:\n            l.debug(\"Invalid temporary number %d\", tmp)\n            raise IndexError(tmp)\n        return self.types[tmp]", "category": "Python"}, {"instruction": "def check_house(self, complex: str, house: str) -> bool:\n        \"\"\"\n        Check if given house exists in the rumetr database\n        \"\"\"\n", "input": "", "output": "        self.check_complex(complex)\n        if '%s__%s' % (complex, house) in self._checked_houses:\n            return True\n\n        try:\n            self.get('developers/{developer}/complexes/{complex}/houses/{house}/'.format(\n                developer=self.developer,\n                complex=complex,\n                house=house,\n            ))\n        except exceptions.Rumetr404Exception:\n            raise exceptions.RumetrHouseNotFound('Unknown house (complex is known) \u2014\u00a0may be you should create one?')\n\n        self._checked_houses.add('%s__%s' % (complex, house))\n        return True", "category": "Python"}, {"instruction": "def search_dashboard_deleted_for_facets(self, **kwargs):  # noqa: E501\n        \"\"\"Lists the values of one or more facets over the customer's deleted dashboards  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.search_dashboard_deleted_for_facets(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param FacetsSearchRequestContainer body:\n        :return: ResponseContainerFacetsResponseContainer\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.search_dashboard_deleted_for_facets_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.search_dashboard_deleted_for_facets_with_http_info(**kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def send_fax(self, payload_outbox, **kwargs):  # noqa: E501\n        \"\"\"Send a fax  # noqa: E501\n\n        With this API call you will be able to send a fax (one or more files) to one or more destinations. If you are a corporate member and you don't have a fax number set your **from** parameter to **NO_NUMBER**  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.send_fax(payload_outbox, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param PayloadOutbox payload_outbox: (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return self.send_fax_with_http_info(payload_outbox, **kwargs)  # noqa: E501\n        else:\n            (data) = self.send_fax_with_http_info(payload_outbox, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def parse_request(self):\n        \"\"\"Override parse_request method to enrich basic functionality of `BaseHTTPRequestHandler` class\n\n        Original class can only invoke do_GET, do_POST, do_PUT, etc method implementations if they are defined.\n        But we would like to have at least some simple routing mechanism, i.e.:\n        GET /uri1/part2 request should invoke `do_GET_uri1()`\n        POST /other should invoke `do_POST_other()`\n\n        If the `do_<REQUEST_METHOD>_<first_part_url>` method does not exists we'll fallback to original behavior.\"\"\"\n", "input": "", "output": "\n        ret = BaseHTTPRequestHandler.parse_request(self)\n        if ret:\n            mname = self.path.lstrip('/').split('/')[0]\n            mname = self.command + ('_' + mname if mname else '')\n            if hasattr(self, 'do_' + mname):\n                self.command = mname\n        return ret", "category": "Python"}, {"instruction": "def zbar_function(fname, restype, *args):\n    \"\"\"Returns a foreign function exported by `zbar`.\n\n    Args:\n        fname (:obj:`str`): Name of the exported function as string.\n        restype (:obj:): Return type - one of the `ctypes` primitive C data\n        types.\n        *args: Arguments - a sequence of `ctypes` primitive C data types.\n\n    Returns:\n        cddl.CFunctionType: A wrapper around the function.\n    \"\"\"\n", "input": "", "output": "    prototype = CFUNCTYPE(restype, *args)\n    return prototype((fname, load_libzbar()))", "category": "Python"}, {"instruction": "def nacm_cmd_exec_default(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        nacm = ET.SubElement(config, \"nacm\", xmlns=\"urn:ietf:params:xml:ns:yang:ietf-netconf-acm\")\n        cmd_exec_default = ET.SubElement(nacm, \"cmd-exec-default\", xmlns=\"http://tail-f.com/yang/acm\")\n        cmd_exec_default.text = kwargs.pop('cmd_exec_default')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def sections(self):\n        \"\"\" Returns a list of all media sections in this library. Library sections may be any of\n            :class:`~plexapi.library.MovieSection`, :class:`~plexapi.library.ShowSection`,\n            :class:`~plexapi.library.MusicSection`, :class:`~plexapi.library.PhotoSection`.\n        \"\"\"\n", "input": "", "output": "        key = '/library/sections'\n        sections = []\n        for elem in self._server.query(key):\n            for cls in (MovieSection, ShowSection, MusicSection, PhotoSection):\n                if elem.attrib.get('type') == cls.TYPE:\n                    section = cls(self._server, elem, key)\n                    self._sectionsByID[section.key] = section\n                    sections.append(section)\n        return sections", "category": "Python"}, {"instruction": "def check_validity_for_dict(keys, dict):\n    \"\"\"\n    >>> dict = {'a': 0, 'b': 1, 'c': 2}\n    >>> keys = ['a', 'd', 'e']\n    >>> check_validity_for_dict(keys, dict) == False\n    True\n    >>> keys = ['a', 'b', 'c']\n    >>> check_validity_for_dict(keys, dict) == False\n    False\n    \"\"\"\n", "input": "", "output": "    for key in keys:\n        if key not in dict or dict[key] is '' or dict[key] is None:\n            return False\n    return True", "category": "Python"}, {"instruction": "def load_rank(self, region, season=-1):\n        \"\"\"|coro|\n        Loads the players rank for this region and season\n\n        Parameters\n        ----------\n        region : str\n            the name of the region you want to get the rank for\n        season : Optional[int]\n            the season you want to get the rank for (defaults to -1, latest season)\n\n        Returns\n        -------\n        :class:`Rank`\n            the players rank for this region and season\"\"\"\n", "input": "", "output": "        data = yield from self.auth.get(\"https://public-ubiservices.ubi.com/v1/spaces/%s/sandboxes/%s/r6karma/players?board_id=pvp_ranked&profile_ids=%s&region_id=%s&season_id=%s\" % (self.spaceid, self.platform_url, self.id, region, season))\n\n        if \"players\" in data and self.id in data[\"players\"]:\n            regionkey = \"%s:%s\" % (region, season)\n            self.ranks[regionkey] = Rank(data[\"players\"][self.id])\n            return self.ranks[regionkey]\n        else:\n            raise InvalidRequest(\"Missing players key in returned JSON object %s\" % str(data))", "category": "Python"}, {"instruction": "async def unset_lock(self, resource, lock_identifier):\n        \"\"\"\n        Tries to unset the lock to all the redis instances\n\n        :param resource: The resource string name to lock\n        :param lock_identifier: The id of the lock. A unique string\n        :return float: The elapsed time that took to lock the instances in iseconds\n        :raises: LockError if the lock has not matching identifier in more then\n            (N/2 - 1) instances\n        \"\"\"\n", "input": "", "output": "        start_time = time.time()\n\n        successes = await asyncio.gather(*[\n            i.unset_lock(resource, lock_identifier) for\n            i in self.instances\n        ], return_exceptions=True)\n        successful_remvoes = sum(s is None for s in successes)\n\n        elapsed_time = time.time() - start_time\n        unlocked = True if successful_remvoes >= int(len(self.instances) / 2) + 1 else False\n\n        self.log.debug('Lock \"%s\" is unset on %d/%d instances in %s seconds',\n                       resource, successful_remvoes, len(self.instances), elapsed_time)\n\n        if not unlocked:\n            raise LockError('Can not release the lock')\n\n        return elapsed_time", "category": "Python"}, {"instruction": "def _log_tcex_version(self):\n        \"\"\"Log the current TcEx version number.\"\"\"\n", "input": "", "output": "        self.log.info(u'TcEx Version: {}'.format(__import__(__name__).__version__))", "category": "Python"}, {"instruction": "def simple_response_str(command, status, status_text, content=\"\"):\n    \"\"\" Creates an OSP response XML string.\n\n    Arguments:\n        command (str): OSP Command to respond to.\n        status (int): Status of the response.\n        status_text (str): Status text of the response.\n        content (str): Text part of the response XML element.\n\n    Return:\n        String of response in xml format.\n    \"\"\"\n", "input": "", "output": "    response = Element('%s_response' % command)\n    for name, value in [('status', str(status)), ('status_text', status_text)]:\n        response.set(name, str(value))\n    if isinstance(content, list):\n        for elem in content:\n            response.append(elem)\n    elif isinstance(content, Element):\n        response.append(content)\n    else:\n        response.text = content\n    return tostring(response)", "category": "Python"}, {"instruction": "def exclusions(self):\n        \"\"\"Return list of browser exclusion rules defined in the Configuration.\n        \"\"\"\n", "input": "", "output": "        exclusion_rules = [\n            r.strip()\n            for r in self.config.get(\"browser_exclude_rule\", \"\").split(\",\")\n            if r.strip()\n        ]\n        return exclusion_rules", "category": "Python"}, {"instruction": "def handle_error(err, halt=True):\n        \"\"\"Print errors message and optionally exit.\n\n        Args:\n            err (str): The error message to print.\n            halt (bool, optional): Defaults to True. If True the script will exit.\n        \"\"\"\n", "input": "", "output": "        print('{}{}{}'.format(c.Style.BRIGHT, c.Fore.RED, err))\n        if halt:\n            sys.exit(1)", "category": "Python"}, {"instruction": "def next(self):\n        \"\"\" Where to redirect after authorization \"\"\"\n", "input": "", "output": "        next = request.args.get('next')\n        if next is None:\n            params = self.default_redirect_params\n            next = url_for(self.default_redirect_endpoint, **params)\n        return next", "category": "Python"}, {"instruction": "def tenses(self, verb, parse=True):\n        \"\"\" Returns a list of possible tenses for the given inflected verb.\n        \"\"\"\n", "input": "", "output": "        verb = verb.lower()\n        a = set()\n        b = self.lemma(verb, parse=parse)\n        v = []\n        if b in self:\n            v = self[b]\n        elif parse is True: # rule-based\n            v = self.find_lexeme(b)\n        # For each tense in the verb lexeme that matches the given tense,\n        # 1) retrieve the tense tuple,\n        # 2) retrieve the tense tuples for which that tense is a default.\n        for i, tense in enumerate(v):\n            if tense == verb:\n                for id, index in self._format.items():\n                    if i == index:\n                        a.add(id)\n                for id1, id2 in self._default.items():\n                    if id2 in a:\n                        a.add(id1)\n                for id1, id2 in self._default.items():\n                    if id2 in a:\n                        a.add(id1)\n        a = (TENSES[id][:-2] for id in a)\n        a = Tenses(sorted(a))\n        return a", "category": "Python"}, {"instruction": "def set_speech_ssml(self, ssml):\n        \"\"\"Set response output speech as SSML type.\n\n        Args:\n            ssml: str. Response speech used when type is 'SSML', should be formatted\n                with Speech Synthesis Markup Language. Cannot exceed 8,000\n                characters.\n        \"\"\"\n", "input": "", "output": "        self.response.outputSpeech.type = 'SSML'\n        self.response.outputSpeech.ssml = ssml", "category": "Python"}, {"instruction": "def _moveBy(self, value, **kwargs):\n        \"\"\"\n        This is the environment implementation of\n        :meth:`BaseObject.moveBy`.\n\n        **value** will be an iterable containing two\n        :ref:`type-int-float` values defining the x and y\n        values to move the object by. It will have been\n        normalized with :func:`normalizers.normalizeTransformationOffset`.\n\n        Subclasses may override this method.\n        \"\"\"\n", "input": "", "output": "        x, y = value\n        t = transform.Offset(x, y)\n        self.transformBy(tuple(t), **kwargs)", "category": "Python"}, {"instruction": "def read_group(self, group_id, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Query the group by its identifier.\n\n        Supported methods:\n            GET: /{mount_point}/group/id/{id}. Produces: 200 application/json\n\n        :param group_id: Identifier of the group.\n        :type group_id: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n", "input": "", "output": "        api_path = '/v1/{mount_point}/group/id/{id}'.format(\n            mount_point=mount_point,\n            id=group_id,\n        )\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "category": "Python"}, {"instruction": "def porosity_profile(im, axis):\n    r\"\"\"\n    Returns a porosity profile along the specified axis\n\n    Parameters\n    ----------\n    im : ND-array\n        The volumetric image for which to calculate the porosity profile\n    axis : int\n        The axis (0, 1, or 2) along which to calculate the profile.  For\n        instance, if `axis` is 0, then the porosity in each YZ plane is\n        calculated and returned as 1D array with 1 value for each X position.\n\n    Returns\n    -------\n    result : 1D-array\n        A 1D-array of porosity along the specified axis\n    \"\"\"\n", "input": "", "output": "    if axis >= im.ndim:\n        raise Exception('axis out of range')\n    im = np.atleast_3d(im)\n    a = set(range(im.ndim)).difference(set([axis]))\n    a1, a2 = a\n    prof = np.sum(np.sum(im, axis=a2), axis=a1)/(im.shape[a2]*im.shape[a1])\n    return prof*100", "category": "Python"}, {"instruction": "def fontFamilyIndex(qFont, families):\n    \"\"\" Searches the index of qFont.family in the families list.\n        If qFont.family() is not in the list, the index of qFont.defaultFamily() is returned.\n        If that is also not present an error is raised.\n    \"\"\"\n", "input": "", "output": "    try:\n        return families.index(qFont.family())\n    except ValueError:\n        if False and DEBUGGING:\n            raise\n        else:\n            logger.warn(\"{} not found in font families, using default.\".format(qFont.family()))\n            return families.index(qFont.defaultFamily())", "category": "Python"}, {"instruction": "def get_packet(self, generation_time, sequence_number):\n        \"\"\"\n        Gets a single packet by its identifying key (gentime, seqNum).\n\n        :param ~datetime.datetime generation_time: When the packet was generated (packet time)\n        :param int sequence_number: Sequence number of the packet\n        :rtype: .Packet\n        \"\"\"\n", "input": "", "output": "        url = '/archive/{}/packets/{}/{}'.format(\n            self._instance, to_isostring(generation_time), sequence_number)\n        response = self._client.get_proto(url)\n        message = yamcs_pb2.TmPacketData()\n        message.ParseFromString(response.content)\n        return Packet(message)", "category": "Python"}, {"instruction": "def get_queryset(self):\n        \"\"\"\n        Returns only objects which are accessible to the current user.\n        If user is not authenticated all public objects will be returned.\n\n        Model must implement AccessLevelManager!\n        \"\"\"\n", "input": "", "output": "        return self.queryset.all().accessible_to(user=self.request.user)", "category": "Python"}, {"instruction": "def mix(color1, color2, pos=0.5):\n    \"\"\"\n    Return the mix of two colors at a state of :pos:\n\n    Retruns color1 * pos + color2 * (1 - pos)\n    \"\"\"\n", "input": "", "output": "    opp_pos = 1 - pos\n\n    red = color1[0] * pos + color2[0] * opp_pos\n    green = color1[1] * pos + color2[1] * opp_pos\n    blue = color1[2] * pos + color2[2] * opp_pos\n    return int(red), int(green), int(blue)", "category": "Python"}, {"instruction": "def enable_rights(self):\n        \"\"\"\n        Enables rights management provided by :class:`fatbotslim.handlers.RightsHandler`.\n        \"\"\"\n", "input": "", "output": "        if self.rights is None:\n            handler_instance = RightsHandler(self)\n            self.handlers.insert(len(self.default_handlers), handler_instance)", "category": "Python"}, {"instruction": "def load_with_fn(load_fn, content_or_strm, container, allow_primitives=False,\n                 **options):\n    \"\"\"\n    Load data from given string or stream 'content_or_strm'.\n\n    :param load_fn: Callable to load data\n    :param content_or_strm: data content or stream provides it\n    :param container: callble to make a container object\n    :param allow_primitives:\n        True if the parser.load* may return objects of primitive data types\n        other than mapping types such like JSON parser\n    :param options: keyword options passed to 'load_fn'\n\n    :return: container object holding data\n    \"\"\"\n", "input": "", "output": "    ret = load_fn(content_or_strm, **options)\n    if anyconfig.utils.is_dict_like(ret):\n        return container() if (ret is None or not ret) else container(ret)\n\n    return ret if allow_primitives else container(ret)", "category": "Python"}, {"instruction": "async def handle_user_exception(self, error: Exception) -> Response:\n        \"\"\"Handle an exception that has been raised.\n\n        This should forward :class:`~quart.exception.HTTPException` to\n        :meth:`handle_http_exception`, then attempt to handle the\n        error. If it cannot it should reraise the error.\n        \"\"\"\n", "input": "", "output": "        if isinstance(error, HTTPException) and not self.trap_http_exception(error):\n            return await self.handle_http_exception(error)\n\n        handler = self._find_exception_handler(error)\n        if handler is None:\n            raise error\n        return await handler(error)", "category": "Python"}, {"instruction": "def _int_to_binary(integer, size=None):\n    \"\"\"Return bit list representation of integer.\n\n    If size is given, binary string is padded with 0s, or clipped to the size.\n    \"\"\"\n", "input": "", "output": "    binary_list = map(int, format(integer, 'b'))\n\n    if size is None:\n        return binary_list\n    else:\n        if len(binary_list) > size:\n            # Too long, take only last n\n            return binary_list[len(binary_list)-size:]\n        elif size > len(binary_list):\n            # Too short, pad\n            return [0]*(size-len(binary_list)) + binary_list\n        else:\n            # Just right\n            return binary_list", "category": "Python"}, {"instruction": "def set_rectangle(self, rectangle):\n        \"\"\"\n        Set the rectangle for the selection.\n        :param rectangle:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if rectangle == self.rectangle():\n            return False\n\n        if rectangle is None:\n            self.reset()\n        else:\n            self.start_point = QgsPointXY(\n                rectangle.xMinimum(), rectangle.yMinimum())\n            self.end_point = QgsPointXY(\n                rectangle.xMaximum(), rectangle.yMaximum())\n            self.show_rectangle(self.start_point, self.end_point)\n        return True", "category": "Python"}, {"instruction": "def valid_paths(self, *args):\n        \"\"\"\n        Validate that given paths are not the same.\n\n        Args:\n            (string): Path to validate.\n\n        Raises:\n            boussole.exceptions.SettingsInvalidError: If there is more than one\n                occurence of the same path.\n\n        Returns:\n            bool: ``True`` if paths are validated.\n        \"\"\"\n", "input": "", "output": "        for i, path in enumerate(args, start=0):\n            cp = list(args)\n            current = cp.pop(i)\n            if current in cp:\n                raise SettingsInvalidError(\"Multiple occurences finded for \"\n                                           \"path: {}\".format(current))\n\n        return True", "category": "Python"}, {"instruction": "def delete_many(cls, documents):\n        \"\"\"Delete multiple documents\"\"\"\n", "input": "", "output": "\n        # Ensure all documents have been converted to frames\n        frames = cls._ensure_frames(documents)\n\n        all_count = len(documents)\n        assert len([f for f in frames if '_id' in f._document]) == all_count, \\\n                \"Can't delete documents without `_id`s\"\n\n        # Send delete signal\n        signal('delete').send(cls, frames=frames)\n\n        # Prepare the documents to be deleted\n        ids = [f._id for f in frames]\n\n        # Delete the documents\n        cls.get_collection().delete_many({'_id': {'$in': ids}})\n\n        # Send deleted signal\n        signal('deleted').send(cls, frames=frames)", "category": "Python"}, {"instruction": "def create_parser(arg_parser: ArgumentParser = None) -> ArgumentParser:\n    \"\"\"\n    Creates an argument parser populated with the arg formats for the server\n    command.\n    \"\"\"\n", "input": "", "output": "\n    parser = arg_parser or ArgumentParser()\n    parser.description = 'Cauldron kernel server'\n\n    parser.add_argument(\n        '-p', '--port',\n        dest='port',\n        type=int,\n        default=5010\n    )\n\n    parser.add_argument(\n        '-d', '--debug',\n        dest='debug',\n        default=False,\n        action='store_true'\n    )\n\n    parser.add_argument(\n        '-v', '--version',\n        dest='version',\n        default=False,\n        action='store_true'\n    )\n\n    parser.add_argument(\n        '-c', '--code',\n        dest='authentication_code',\n        type=str,\n        default=''\n    )\n\n    parser.add_argument(\n        '-n', '--name',\n        dest='host',\n        type=str,\n        default=None\n    )\n\n    return parser", "category": "Python"}, {"instruction": "def get_scheduler_from_hostname(self, host_name):\n        \"\"\"Get scheduler linked to the given host_name\n\n        :param host_name: host_name we want the scheduler from\n        :type host_name: str\n        :return: scheduler with id corresponding to the mapping table\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        scheduler_uuid = self.hosts_schedulers.get(host_name, None)\n        return self.schedulers.get(scheduler_uuid, None)", "category": "Python"}, {"instruction": "def to_gremlin(self):\n        \"\"\"Return a unicode object with the Gremlin representation of this block.\"\"\"\n", "input": "", "output": "        self.validate()\n        template_data = {\n            'direction': self.direction,\n            'edge_name': self.edge_name,\n            'inverse_direction': 'in' if self.direction == 'out' else 'out'\n        }\n        return (u'collectMany{{entry -> entry.{direction}_{edge_name}'\n                u'.collect{{edge -> edge.{inverse_direction}V.next()}}}}'\n                .format(**template_data))", "category": "Python"}, {"instruction": "def fillzip(*l):\n    \"\"\"like zip (for things that have a length), but repeats the last element of all shorter lists such that the result is as long as the longest.\"\"\"\n", "input": "", "output": "    maximum = max(len(el) for el in l)\n    return zip(*[el + [el[-1]]*(maximum-len(el)) for el in l])", "category": "Python"}, {"instruction": "def flush_body(self) -> bool:\n        \"\"\"\n        \u53d1\u9001\u5185\u5bb9\u4f53\n        \"\"\"\n", "input": "", "output": "        if self._body is None:\n            return False\n        elif isinstance(self._body, bytes):\n            self.write(self._body)\n            return True\n        return False", "category": "Python"}, {"instruction": "def add_new_devices_callback(self, callback):\n        \"\"\"Register as callback for when new devices are added. \"\"\"\n", "input": "", "output": "        self._new_devices_callbacks.append(callback)\n        _LOGGER.debug('Added new devices callback to %s', callback)", "category": "Python"}, {"instruction": "def p_ParamList(p):\n    '''\n    ParamList :\n              | Param\n              | ParamList COMMA Param\n    '''\n", "input": "", "output": "    if len(p) == 1:\n        p[0] = ParamList(None, None)\n    elif len(p) == 2:\n        p[0] = ParamList(None, p[1])\n    else:\n        p[0] = ParamList(p[1], p[3])", "category": "Python"}, {"instruction": "def load_single_dict(pinyin_dict, style='default'):\n    \"\"\"\u8f7d\u5165\u7528\u6237\u81ea\u5b9a\u4e49\u7684\u5355\u5b57\u62fc\u97f3\u5e93\n\n    :param pinyin_dict: \u5355\u5b57\u62fc\u97f3\u5e93\u3002\u6bd4\u5982\uff1a ``{0x963F: u\"\u0101,\u0113\"}``\n    :param style: pinyin_dict \u53c2\u6570\u503c\u7684\u62fc\u97f3\u5e93\u98ce\u683c. \u652f\u6301 'default', 'tone2'\n    :type pinyin_dict: dict\n    \"\"\"\n", "input": "", "output": "    if style == 'tone2':\n        for k, v in pinyin_dict.items():\n            v = _replace_tone2_style_dict_to_default(v)\n            PINYIN_DICT[k] = v\n    else:\n        PINYIN_DICT.update(pinyin_dict)\n\n    mmseg.retrain(mmseg.seg)", "category": "Python"}, {"instruction": "def police_priority_map_conform_map_pri4_conform(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        police_priority_map = ET.SubElement(config, \"police-priority-map\", xmlns=\"urn:brocade.com:mgmt:brocade-policer\")\n        name_key = ET.SubElement(police_priority_map, \"name\")\n        name_key.text = kwargs.pop('name')\n        conform = ET.SubElement(police_priority_map, \"conform\")\n        map_pri4_conform = ET.SubElement(conform, \"map-pri4-conform\")\n        map_pri4_conform.text = kwargs.pop('map_pri4_conform')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def jquery_click(self, selector, by=By.CSS_SELECTOR):\n        \"\"\" Clicks an element using jQuery. Different from using pure JS. \"\"\"\n", "input": "", "output": "        selector, by = self.__recalculate_selector(selector, by)\n        self.wait_for_element_present(\n            selector, by=by, timeout=settings.SMALL_TIMEOUT)\n        if self.is_element_visible(selector, by=by):\n            self.__demo_mode_highlight_if_active(selector, by)\n        selector = self.convert_to_css_selector(selector, by=by)\n        selector = self.__make_css_match_first_element_only(selector)\n        click_script = ", "category": "Python"}, {"instruction": "def _get_horoscope_meta(self, day='today'):\n        \"\"\"gets a horoscope meta from site html\n\n        :param day: day for which to get horoscope meta. Default is 'today'\n\n        :returns: dictionary of horoscope mood details\n        \"\"\"\n", "input": "", "output": "        if not is_valid_day(day):\n            raise HoroscopeException(\"Invalid day. Allowed days: [today|yesterday|tomorrow]\" )\n\n        return {\n            'intensity': str(self.tree.xpath('//*[@id=\"%s\"]/div[3]/div[1]/p[1]/text()' % day)[0]).replace(\": \", \"\"),\n            'mood': str(self.tree.xpath('//*[@id=\"%s\"]/div[3]/div[1]/p[2]/text()' % day)[0]).replace(\": \", \"\"),\n            'keywords': str(self.tree.xpath('//*[@id=\"%s\"]/div[3]/div[2]/p[1]/text()' % day)[0]).replace(\": \", \"\"),\n        }", "category": "Python"}, {"instruction": "def filter_silenced(self):\n        '''\n        Determine whether a check is silenced and shouldn't handle.\n        '''\n", "input": "", "output": "        stashes = [\n            ('client', '/silence/{}'.format(self.event['client']['name'])),\n            ('check', '/silence/{}/{}'.format(\n                self.event['client']['name'],\n                self.event['check']['name'])),\n            ('check', '/silence/all/{}'.format(self.event['check']['name']))\n        ]\n        for scope, path in stashes:\n            if self.stash_exists(path):\n                self.bail(scope + ' alerts silenced')", "category": "Python"}, {"instruction": "def set_global_tracer(value):\n    \"\"\"Sets the global tracer.\n    It is an error to pass ``None``.\n\n    :param value: the :class:`Tracer` used as global instance.\n    :type value: :class:`Tracer`\n    \"\"\"\n", "input": "", "output": "    if value is None:\n        raise ValueError('The global Tracer tracer cannot be None')\n\n    global tracer, is_tracer_registered\n    tracer = value\n    is_tracer_registered = True", "category": "Python"}, {"instruction": "def index_search_document(self, *, index):\n        \"\"\"\n        Create or replace search document in named index.\n\n        Checks the local cache to see if the document has changed,\n        and if not aborts the update, else pushes to ES, and then\n        resets the local cache. Cache timeout is set as \"cache_expiry\"\n        in the settings, and defaults to 60s.\n\n        \"\"\"\n", "input": "", "output": "        cache_key = self.search_document_cache_key\n        new_doc = self.as_search_document(index=index)\n        cached_doc = cache.get(cache_key)\n        if new_doc == cached_doc:\n            logger.debug(\"Search document for %r is unchanged, ignoring update.\", self)\n            return []\n        cache.set(cache_key, new_doc, timeout=get_setting(\"cache_expiry\", 60))\n        get_client().index(\n            index=index, doc_type=self.search_doc_type, body=new_doc, id=self.pk\n        )", "category": "Python"}, {"instruction": "def basic_dependencies(self):\n        \"\"\"\n        Accesses basic dependencies from the XML output\n\n        :getter: Returns the dependency graph for basic dependencies\n        :type: corenlp_xml.dependencies.DependencyGraph\n\n        \"\"\"\n", "input": "", "output": "        if self._basic_dependencies is None:\n            deps = self._element.xpath('dependencies[@type=\"basic-dependencies\"]')\n            if len(deps) > 0:\n                self._basic_dependencies = DependencyGraph(deps[0])\n        return self._basic_dependencies", "category": "Python"}, {"instruction": "def tcpPorts(self, req, tag):\n        \"\"\"\n        Create and return a L{PortScrollingFragment} for the L{TCPPort} items\n        in site store.\n        \"\"\"\n", "input": "", "output": "        f = PortScrollingFragment(\n            self.store,\n            TCPPort,\n            (TCPPort.portNumber,\n             TCPPort.interface,\n             FactoryColumn(TCPPort.factory)))\n        f.setFragmentParent(self)\n        f.docFactory = webtheme.getLoader(f.fragmentName)\n        return tag[f]", "category": "Python"}, {"instruction": "def scheduled():\n    '''\n    List scheduled jobs.\n    '''\n", "input": "", "output": "    for job in sorted(schedulables(), key=lambda s: s.name):\n        for task in PeriodicTask.objects(task=job.name):\n            label = job_label(task.task, task.args, task.kwargs)\n            echo(SCHEDULE_LINE.format(\n                name=white(task.name.encode('utf8')),\n                label=label,\n                schedule=task.schedule_display\n            ).encode('utf8'))", "category": "Python"}, {"instruction": "def migration(resource, version, previous_version=''):\n    \"\"\"Register a migration function\"\"\"\n", "input": "", "output": "    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            migrated = func(*args, **kwargs)\n\n            return migrated\n\n        m = Migration(wrapper, resource, version, previous_version)\n        m.register()\n\n        return m\n\n    return decorator", "category": "Python"}, {"instruction": "def run_qaml(self):\n        \"\"\"\n        Create and run the GenomeQAML system call\n        \"\"\"\n", "input": "", "output": "        logging.info('Running GenomeQAML quality assessment')\n        qaml_call = 'classify.py -t {tf} -r {rf}'\\\n            .format(tf=self.qaml_path,\n                    rf=self.qaml_report)\n        make_path(self.reportpath)\n        # Only attempt to assess assemblies if the report doesn't already exist\n        if not os.path.isfile(self.qaml_report):\n            # Run the system calls\n            out, err = run_subprocess(qaml_call)\n            # Acquire thread lock, and write the logs to file\n            self.threadlock.acquire()\n            write_to_logfile(qaml_call, qaml_call, self.logfile)\n            write_to_logfile(out, err, self.logfile)\n            self.threadlock.release()", "category": "Python"}, {"instruction": "def astar(problem, graph_search=False, viewer=None):\n    '''\n    A* search.\n\n    If graph_search=True, will avoid exploring repeated states.\n    Requires: SearchProblem.actions, SearchProblem.result,\n    SearchProblem.is_goal, SearchProblem.cost, and SearchProblem.heuristic.\n    '''\n", "input": "", "output": "    return _search(problem,\n                   BoundedPriorityQueue(),\n                   graph_search=graph_search,\n                   node_factory=SearchNodeStarOrdered,\n                   graph_replace_when_better=True,\n                   viewer=viewer)", "category": "Python"}, {"instruction": "def _find_playlist(self):\n        \"\"\"\n        Internal method to populate the object given the ``id`` or\n        ``reference_id`` that has been set in the constructor.\n        \"\"\"\n", "input": "", "output": "        data = None\n        if self.id:\n            data = self.connection.get_item(\n                'find_playlist_by_id', playlist_id=self.id)\n        elif self.reference_id:\n            data = self.connection.get_item(\n                'find_playlist_by_reference_id',\n                reference_id=self.reference_id)\n\n        if data:\n            self._load(data)", "category": "Python"}, {"instruction": "def _get_next_obj(self, length):\n        \"\"\"Assumes we've already read the object length\"\"\"\n", "input": "", "output": "        data = b''\n        while len(data) < length:\n            data += self.sock.recv(length - len(data))\n\n        return pickle.loads(data)", "category": "Python"}, {"instruction": "def numberwang(random=random, *args, **kwargs):\n    \"\"\"\n    Return a number that is spelled out.\n\n    >>> numberwang(random=mock_random)\n    'two'\n    >>> numberwang(random=mock_random, capitalize=True)\n    'Two'\n    >>> numberwang(random=mock_random, slugify=True)\n    'two'\n\n    \"\"\"\n", "input": "", "output": "    n = random.randint(2, 150)\n    return inflectify.number_to_words(n)", "category": "Python"}, {"instruction": "def _mean_image_subtraction(image, means, num_channels):\n  \"\"\"Subtracts the given means from each image channel.\n\n  For example:\n    means = [123.68, 116.779, 103.939]\n    image = _mean_image_subtraction(image, means)\n\n  Note that the rank of `image` must be known.\n\n  Args:\n    image: a tensor of size [height, width, C].\n    means: a C-vector of values to subtract from each channel.\n    num_channels: number of color channels in the image that will be distorted.\n\n  Returns:\n    the centered image.\n\n  Raises:\n    ValueError: If the rank of `image` is unknown, if `image` has a rank other\n      than three or if the number of channels in `image` doesn't match the\n      number of values in `means`.\n  \"\"\"\n", "input": "", "output": "  if image.get_shape().ndims != 3:\n    raise ValueError('Input must be of size [height, width, C>0]')\n\n  if len(means) != num_channels:\n    raise ValueError('len(means) must match the number of channels')\n\n  mlperf_log.resnet_print(key=mlperf_log.INPUT_MEAN_SUBTRACTION,\n                          value=means)\n\n  # We have a 1-D tensor of means; convert to 3-D.\n  means = tf.expand_dims(tf.expand_dims(means, 0), 0)\n\n  return image - means", "category": "Python"}, {"instruction": "def file_hash(load, fnd):\n    '''\n    Return a file hash, the hash type is set in the master config file\n    '''\n", "input": "", "output": "    if 'env' in load:\n        # \"env\" is not supported; Use \"saltenv\".\n        load.pop('env')\n\n    if not all(x in load for x in ('path', 'saltenv')):\n        return ''\n    ret = {'hash_type': __opts__['hash_type']}\n    relpath = fnd['rel']\n    path = fnd['path']\n    hashdest = os.path.join(__opts__['cachedir'],\n                            'hgfs/hash',\n                            load['saltenv'],\n                            '{0}.hash.{1}'.format(relpath,\n                                                  __opts__['hash_type']))\n    if not os.path.isfile(hashdest):\n        ret['hsum'] = salt.utils.hashutils.get_hash(path, __opts__['hash_type'])\n        with salt.utils.files.fopen(hashdest, 'w+') as fp_:\n            fp_.write(ret['hsum'])\n        return ret\n    else:\n        with salt.utils.files.fopen(hashdest, 'rb') as fp_:\n            ret['hsum'] = salt.utils.stringutils.to_unicode(fp_.read())\n        return ret", "category": "Python"}, {"instruction": "def mark_as_inactive(self, kid):\n        \"\"\"\n        Mark a specific key as inactive based on the keys KeyID\n        \n        :param kid: The Key Identifier \n        \"\"\"\n", "input": "", "output": "        k = self.get_key_with_kid(kid)\n        k.inactive_since = time.time()", "category": "Python"}, {"instruction": "def summary(self, name=None):\n        \"\"\"\n        Return a summarized representation.\n\n        .. deprecated:: 0.23.0\n        \"\"\"\n", "input": "", "output": "        warnings.warn(\"'summary' is deprecated and will be removed in a \"\n                      \"future version.\", FutureWarning, stacklevel=2)\n        return self._summary(name)", "category": "Python"}, {"instruction": "def _get_package_status(package):\n    \"\"\"Get the status for a package.\"\"\"\n", "input": "", "output": "    status = package[\"status_str\"] or \"Unknown\"\n    stage = package[\"stage_str\"] or \"Unknown\"\n    if stage == \"Fully Synchronised\":\n        return status\n    return \"%(status)s / %(stage)s\" % {\"status\": status, \"stage\": stage}", "category": "Python"}, {"instruction": "def _sign(self,params):\n        '''\n        Generate API sign code\n        '''\n", "input": "", "output": "        for k, v in params.iteritems():\n            if type(v) == int: v = str(v)\n            elif type(v) == float: v = '%.2f'%v\n            elif type(v) in (list, set): \n                v = ','.join([str(i) for i in v])\n            elif type(v) == bool: v = 'true' if v else 'false'\n            elif type(v) == datetime.datetime: v = v.strftime('%Y-%m-%d %X')\n            if type(v) == unicode:\n                params[k] = v.encode('utf-8')\n            else:\n                params[k] = v\n        src = self.APP_SECRET + ''.join([\"%s%s\" % (k, v) for k, v in sorted(params.iteritems())])\n        return md5(src).hexdigest().upper()", "category": "Python"}, {"instruction": "def get_config_map(self, name):\n        \"\"\"\n        Get a ConfigMap object from the server\n\n        Raises exception on error\n\n        :param name: str, name of configMap to get from the server\n        :returns: ConfigMapResponse containing the ConfigMap with the requested name\n        \"\"\"\n", "input": "", "output": "        response = self.os.get_config_map(name)\n        config_map_response = ConfigMapResponse(response.json())\n        return config_map_response", "category": "Python"}, {"instruction": "def login(config, api_key=\"\"):\n    \"\"\"Store your Bugzilla API Key\"\"\"\n", "input": "", "output": "    if not api_key:\n        info_out(\n            \"If you don't have an API Key, go to:\\n\"\n            \"https://bugzilla.mozilla.org/userprefs.cgi?tab=apikey\\n\"\n        )\n        api_key = getpass.getpass(\"API Key: \")\n\n    # Before we store it, let's test it.\n    url = urllib.parse.urljoin(config.bugzilla_url, \"/rest/whoami\")\n    assert url.startswith(\"https://\"), url\n    response = requests.get(url, params={\"api_key\": api_key})\n    if response.status_code == 200:\n        if response.json().get(\"error\"):\n            error_out(\"Failed - {}\".format(response.json()))\n        else:\n            update(\n                config.configfile,\n                {\n                    \"BUGZILLA\": {\n                        \"bugzilla_url\": config.bugzilla_url,\n                        \"api_key\": api_key,\n                        # \"login\": login,\n                    }\n                },\n            )\n            success_out(\"Yay! It worked!\")\n    else:\n        error_out(\"Failed - {} ({})\".format(response.status_code, response.json()))", "category": "Python"}, {"instruction": "def validate_field_list(fields, allow_fmt_specs=False, name_filter=None):\n    \"\"\" Make sure the fields in the given list exist.\n\n        @param fields: List of fields (comma-/space-separated if a string).\n        @type fields: list or str\n        @return: validated field names.\n        @rtype: list\n    \"\"\"\n", "input": "", "output": "    formats = [i[4:] for i in globals() if i.startswith(\"fmt_\")]\n\n    try:\n        fields = [i.strip() for i in fields.replace(',', ' ').split()]\n    except AttributeError:\n        # Not a string, expecting an iterable\n        pass\n\n    if name_filter:\n        fields = [name_filter(name) for name in fields]\n\n    for name in fields:\n        if allow_fmt_specs and '.' in name:\n            fullname = name\n            name, fmtspecs = name.split('.', 1)\n            for fmtspec in fmtspecs.split('.'):\n                if fmtspec not in formats and fmtspec != \"raw\":\n                    raise error.UserError(\"Unknown format specification %r in %r\" % (fmtspec, fullname))\n\n        if name not in engine.FieldDefinition.FIELDS and not engine.TorrentProxy.add_manifold_attribute(name):\n            raise error.UserError(\"Unknown field name %r\" % (name,))\n\n    return fields", "category": "Python"}, {"instruction": "def p_Dictionary(p):\n  \"\"\"Dictionary : dictionary IDENTIFIER Inheritance \"{\" DictionaryMembers \"}\" \";\"\n  \"\"\"\n", "input": "", "output": "  p[0] = model.Dictionary(name=p[2], parent=p[3], members=p[5])", "category": "Python"}, {"instruction": "def gevent_spawn(self):\n        \"\"\" Spawn worker threads (using gevent) \"\"\"\n", "input": "", "output": "        monkey.patch_all(thread=False)\n        joinall([spawn(self.gevent_worker) for x in range(self.queue_worker_amount)])", "category": "Python"}, {"instruction": "def _get_env_list(obj, env):\n    \"\"\"Creates the list of environments to read\n\n    :param obj: the settings instance\n    :param env: settings env default='DYNACONF'\n    :return: a list of working environments\n    \"\"\"\n", "input": "", "output": "    # add the [default] env\n    env_list = [obj.get(\"DEFAULT_ENV_FOR_DYNACONF\")]\n    # compatibility with older versions that still uses [dynaconf] as\n    # [default] env\n    global_env = obj.get(\"ENVVAR_PREFIX_FOR_DYNACONF\") or \"DYNACONF\"\n    if global_env not in env_list:\n        env_list.append(global_env)\n    # add the current env\n    if obj.current_env and obj.current_env not in env_list:\n        env_list.append(obj.current_env)\n    # add a manually set env\n    if env and env not in env_list:\n        env_list.append(env)\n    # add the [global] env\n    env_list.append(\"GLOBAL\")\n    return [env.lower() for env in env_list]", "category": "Python"}, {"instruction": "def set_phases(self, literals=[]):\n        \"\"\"\n            Sets polarities of a given list of variables.\n        \"\"\"\n", "input": "", "output": "\n        if self.minisat:\n            pysolvers.minisatgh_setphases(self.minisat, literals)", "category": "Python"}, {"instruction": "def mean(x):\n    \"\"\"\n    Return a numpy array of column mean.\n    It does not affect if the array is one dimension\n\n    Parameters\n    ----------\n    x : ndarray\n        A numpy array instance\n\n    Returns\n    -------\n    ndarray\n        A 1 x n numpy array instance of column mean\n\n    Examples\n    --------\n    >>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n    >>> np.array_equal(mean(a), [2, 5, 8])\n    True\n    >>> a = np.array([1, 2, 3])\n    >>> np.array_equal(mean(a), [1, 2, 3])\n    True\n\n    \"\"\"\n", "input": "", "output": "    if x.ndim > 1 and len(x[0]) > 1:\n        return np.mean(x, axis=1)\n    return x", "category": "Python"}, {"instruction": "def merge_hash(a, b):\n    ''' merges hash b into a\n    this means that if b has key k, the resulting has will have a key k\n    which value comes from b\n    said differently, all key/value combination from b will override a's '''\n", "input": "", "output": "\n    # and iterate over b keys\n    for k, v in b.iteritems():\n        if k in a and isinstance(a[k], dict):\n            # if this key is a hash and exists in a\n            # we recursively call ourselves with \n            # the key value of b\n            a[k] = merge_hash(a[k], v)\n        else:\n            # k is not in a, no need to merge b, we just deecopy\n            # or k is not a dictionnary, no need to merge b either, we just deecopy it\n            a[k] = v\n    # finally, return the resulting hash when we're done iterating keys\n    return a", "category": "Python"}, {"instruction": "def get_widget_label_for(self, fieldname, default=None):\n        \"\"\"Lookup the widget of the field and return the label\n        \"\"\"\n", "input": "", "output": "        widget = self.get_widget_for(fieldname)\n        if widget is None:\n            return default\n        return widget.label", "category": "Python"}, {"instruction": "def __make_innermsg(resource, rtype, ref, action=None, payload=None, limit=None):\n        \"\"\"return innermsg chunk (dict)\n        \"\"\"\n", "input": "", "output": "        if action is not None and not isinstance(action, (tuple, list)):\n            raise TypeError('action must be None/tuple/list')\n        p = {M_RESOURCE: resource,\n             M_TYPE: int(rtype),\n             M_CLIENTREF: ref,\n             # Ensure action path consists only of strings\n             M_ACTION: tuple(u(element) for element in action) if action else None,\n             M_PAYLOAD: payload}\n        if limit is not None:  # Note: fmtted like \"0/15\" where 0 = offset, 15 = limit\n            p[M_RANGE] = limit\n        return p", "category": "Python"}, {"instruction": "def parse_tx_op_return(tx):\n    \"\"\"\n    Given a transaction, locate its OP_RETURN and parse\n    out its opcode and payload.\n    Return (opcode, payload) on success\n    Return (None, None) if there is no OP_RETURN, or if it's not a blockchain ID operation.\n    \"\"\"\n", "input": "", "output": "\n    # find OP_RETURN output\n    op_return = None\n    outputs = tx['vout']\n    for out in outputs:\n        script_key = out['scriptPubKey']['hex']\n        if int(script_key[0:2], 16) == virtualchain.OPCODE_VALUES['OP_RETURN']:\n            op_return = script_key.decode('hex')\n            break\n\n    if op_return is None:\n        msg = 'transaction has no OP_RETURN output'\n        log.error(msg)\n        log.debug('{}:\\n{}'.format(msg, simplejson.dumps(tx)))\n        return None, None\n\n    # [0] is OP_RETURN, [1] is the length; [2:4] are 'id', [4] is opcode\n    magic = op_return[2:4]\n\n    if magic != blockstack_magic_bytes():\n        # not a blockchain ID operation\n        msg = 'OP_RETURN output does not encode a blockchain ID operation'\n        log.error(msg)\n        return None, None\n\n    opcode, payload = op_return[4], op_return[5:]\n\n    return (opcode, payload)", "category": "Python"}, {"instruction": "def push(self, item):\n        '''Push the value item onto the heap, maintaining the heap invariant.\n        If the item is not hashable, a TypeError is raised.\n        '''\n", "input": "", "output": "        hash(item)\n        heapq.heappush(self._items, item)", "category": "Python"}, {"instruction": "def exit_full_screen(self):\n        \"\"\"Change from full screen to windowed mode and remove key binding\"\"\"\n", "input": "", "output": "        self.tk.attributes(\"-fullscreen\", False)\n        self._full_screen = False\n        self.events.remove_event(\"<FullScreen.Escape>\")", "category": "Python"}, {"instruction": "def query(self, query):\n        \"\"\"\n        Query bugzilla and return a list of matching bugs.\n        query must be a dict with fields like those in in querydata['fields'].\n        Returns a list of Bug objects.\n        Also see the _query() method for details about the underlying\n        implementation.\n        \"\"\"\n", "input": "", "output": "        try:\n            r = self._proxy.Bug.search(query)\n        except Fault as e:\n\n            # Try to give a hint in the error message if url_to_query\n            # isn't supported by this bugzilla instance\n            if (\"query_format\" not in str(e) or\n                \"RHBugzilla\" in str(e.__class__) or\n                self._check_version(5, 0)):\n                raise\n            raise BugzillaError(\"%s\\nYour bugzilla instance does not \"\n                \"appear to support API queries derived from bugzilla \"\n                \"web URL queries.\" % e)\n\n        log.debug(\"Query returned %s bugs\", len(r['bugs']))\n        return [Bug(self, dict=b,\n                autorefresh=self.bug_autorefresh) for b in r['bugs']]", "category": "Python"}, {"instruction": "def getR(self, i=5, j=6):\n        \"\"\" return transport matrix element, indexed by i, j,\n        be default, return dispersion value, i.e. getR(5,6) in [m]\n\n        :param i: row index, with initial index of 1\n        :param j: col indx, with initial index of 1\n        :return: transport matrix element\n        \"\"\"\n", "input": "", "output": "        if self.refresh is True:\n            self.getMatrix()\n        return self.transM[i - 1, j - 1]", "category": "Python"}, {"instruction": "def import_code(mod_code, mod_name):\n    \"\"\"Create a module object by code.\n    @param mod_code: the code that the module contains.\n\n    @param mod_name: module name.\n    \"\"\"\n", "input": "", "output": "    mod_obj = imp.new_module(mod_name)\n\n    mod_obj.__file__ = None\n\n    exec_(mod_code, mod_obj.__dict__, mod_obj.__dict__)\n\n    add_to_sys_modules(mod_name=mod_name, mod_obj=mod_obj)\n\n    return mod_obj", "category": "Python"}, {"instruction": "def save_callback(operation, graphdef):\n    '''callback from save thread'''\n", "input": "", "output": "    if operation == 'test':\n        for e in graphdef.expressions:\n            if expression_ok(e):\n                graphdef.expression = e\n                display_graph(graphdef)\n                return\n        mestate.console.writeln('Invalid graph expressions', fg='red')\n        return\n    if operation == 'save':\n        save_graph(graphdef)", "category": "Python"}, {"instruction": "def deserialize(self, data):\n        '''\n        Deserialize the data based on request content type headers\n        '''\n", "input": "", "output": "        ct_in_map = {\n            'application/x-www-form-urlencoded': self._form_loader,\n            'application/json': salt.utils.json.loads,\n            'application/x-yaml': salt.utils.yaml.safe_load,\n            'text/yaml': salt.utils.yaml.safe_load,\n            # because people are terrible and don't mean what they say\n            'text/plain': salt.utils.json.loads\n        }\n\n        try:\n            # Use cgi.parse_header to correctly separate parameters from value\n            value, parameters = cgi.parse_header(self.request.headers['Content-Type'])\n            return ct_in_map[value](tornado.escape.native_str(data))\n        except KeyError:\n            self.send_error(406)\n        except ValueError:\n            self.send_error(400)", "category": "Python"}, {"instruction": "def set_fft_params(func):\n    \"\"\"Decorate a method to automatically convert quantities to samples\n    \"\"\"\n", "input": "", "output": "    @wraps(func)\n    def wrapped_func(series, method_func, *args, **kwargs):\n        ", "category": "Python"}, {"instruction": "def get(self, key):\n        \"\"\"\n        Fetches the value with a given keypath from the given node.\n\n        Key will be encoded into binary array format first.\n        \"\"\"\n", "input": "", "output": "        validate_is_bytes(key)\n\n        return self._get(self.root_hash, encode_to_bin(key))", "category": "Python"}, {"instruction": "def child(self, fragment):\n        \"\"\"\n        Returns a path of a child item represented by `fragment`.\n        \"\"\"\n", "input": "", "output": "        return os.path.join(self.path, FS(fragment).path)", "category": "Python"}, {"instruction": "def _writeFault(self, fail, request):\n        \"\"\"\n        fail -- failure\n        request -- request message\n        ex -- Exception \n        \"\"\"\n", "input": "", "output": "        response = fault.FaultFromException(fail.value, False, fail.tb).AsSOAP() \n        self._writeResponse(response, request, status=500)", "category": "Python"}, {"instruction": "def update_paired_notebooks(self, path, fmt, formats):\n        \"\"\"Update the list of paired notebooks to include/update the current pair\"\"\"\n", "input": "", "output": "        if not formats:\n            self.drop_paired_notebook(path)\n            return\n\n        new_paired_paths = paired_paths(path, fmt, formats)\n        for alt_path, _ in new_paired_paths:\n            self.drop_paired_notebook(alt_path)\n\n        long_formats = long_form_multiple_formats(formats)\n        if len(long_formats) == 1 and set(long_formats[0]) <= {'extension'}:\n            return\n\n        short_formats = short_form_multiple_formats(formats)\n        for alt_path, alt_fmt in new_paired_paths:\n            self.paired_notebooks[alt_path] = short_form_one_format(alt_fmt), short_formats", "category": "Python"}, {"instruction": "def _vmomentsurfaceIntegrand(vz,vR,vT,R,z,df,sigmaR1,gamma,sigmaz1,n,m,o): #pragma: no cover because this is too slow; a warning is shown\n    \"\"\"Internal function that is the integrand for the vmomentsurface mass integration\"\"\"\n", "input": "", "output": "    return vR**n*vT**m*vz**o*df(R,vR*sigmaR1,vT*sigmaR1*gamma,z,vz*sigmaz1,\n                                use_physical=False)", "category": "Python"}, {"instruction": "def create_recipe_file_logger(logger, logfile, logformat):\n    \"\"\"Redirect Recipe log messages to a file.\"\"\"\n", "input": "", "output": "    recipe_formatter = logging.Formatter(logformat)\n    fh = logging.FileHandler(logfile, mode='w')\n    fh.setLevel(logging.DEBUG)\n    fh.setFormatter(recipe_formatter)\n    return fh", "category": "Python"}, {"instruction": "def get_parser():\n    \"\"\" Parses the arguments if script is run directly via console \"\"\"\n", "input": "", "output": "    parser = argparse.ArgumentParser(description='Converts HTML from file or url to a clean text version')\n    parser.add_argument('input', nargs='?', default=None, help='Html input either from a file or an url (default:stdin)')\n    parser.add_argument('-o', '--output', type=str, help='Output file (default:stdout).')\n    parser.add_argument('-e', '--encoding', type=str, help='Content encoding for files (default:utf-8)', default='utf-8')\n    parser.add_argument('-i', '--display-image-captions', action='store_true', default=False, help='Display image captions (default:false).')\n    parser.add_argument('-l', '--display-link-targets', action='store_true', default=False, help='Display link targets (default:false).')\n    parser.add_argument('-d', '--deduplicate-image-captions', action='store_true', default=False, help='Deduplicate image captions (default:false).')\n    return parser", "category": "Python"}, {"instruction": "def _path_is_executable_others(path):\n    '''\n    Check every part of path for executable permission\n    '''\n", "input": "", "output": "    prevpath = None\n    while path and path != prevpath:\n        try:\n            if not os.stat(path).st_mode & stat.S_IXOTH:\n                return False\n        except OSError:\n            return False\n        prevpath = path\n        path, _ = os.path.split(path)\n    return True", "category": "Python"}, {"instruction": "def find_gui_and_backend():\n    \"\"\"Return the gui and mpl backend.\"\"\"\n", "input": "", "output": "    matplotlib = sys.modules['matplotlib']\n    # WARNING: this assumes matplotlib 1.1 or newer!!\n    backend = matplotlib.rcParams['backend']\n    # In this case, we need to find what the appropriate gui selection call\n    # should be for IPython, so we can activate inputhook accordingly\n    gui = backend2gui.get(backend, None)\n    return gui, backend", "category": "Python"}, {"instruction": "def handleFailure(self, test, err):\n        \"\"\"\n        Baseclass override. Called when a test fails.\n\n        If the test isn't going to be rerun again, then report the failure\n        to the nose test result.\n\n        :param test:\n            The test that has raised an error\n        :type test:\n            :class:`nose.case.Test`\n        :param err:\n            Information about the test failure (from sys.exc_info())\n        :type err:\n            `tuple` of `class`, :class:`Exception`, `traceback`\n        :return:\n            True, if the test will be rerun; False, if nose should handle it.\n        :rtype:\n            `bool`\n        \"\"\"\n", "input": "", "output": "        # pylint:disable=invalid-name\n        want_failure = self._handle_test_error_or_failure(test, err)\n        if not want_failure and id(test) in self._tests_that_reran:\n            self._nose_result.addFailure(test, err)\n        return want_failure or None", "category": "Python"}, {"instruction": "def status_to_string(dbus_status):\n    '''\n    Converts a numeric dbus snapper status into a string\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' snapper.status_to_string <dbus_status>\n    '''\n", "input": "", "output": "    status_tuple = (\n        dbus_status & 0b000000001, dbus_status & 0b000000010, dbus_status & 0b000000100,\n        dbus_status & 0b000001000, dbus_status & 0b000010000, dbus_status & 0b000100000,\n        dbus_status & 0b001000000, dbus_status & 0b010000000, dbus_status & 0b100000000\n    )\n\n    return [DBUS_STATUS_MAP[status] for status in status_tuple if status]", "category": "Python"}, {"instruction": "def remove(cls, resource_id, parent_id=None, grandparent_id=None,\n               wait=True, timeout=None):\n        \"\"\"Delete the required resource.\"\"\"\n", "input": "", "output": "        raise exception.NotSupported(feature=\"DELETE\",\n                                     context=\"VirtualSwitchManager\")", "category": "Python"}, {"instruction": "def make(cls, poetry, env, io):\n        \"\"\"Build a wheel in the dist/ directory, and optionally upload it.\"\"\"\n", "input": "", "output": "        cls.make_in(poetry, env, io)", "category": "Python"}, {"instruction": "def get_message(self, offset, size):\n        '''Get an individual :class:`MacMailMessage` within a Mail\n        data file, based on size and offset information from the\n        corresponding :class:`MacIndexMessage`.\n\n        :param offset: offset within the Mail file where the desired\n            message begins, i.e. :attr:`MacMailMessage.offset`\n        :param size: size of the message,\n            i.e. :attr:`MacMailMessage.size`\n        '''\n", "input": "", "output": "        return MacMailMessage(size=size, mm=self.mmap, offset=offset)", "category": "Python"}, {"instruction": "def exists(self, relpath, rsc=None, useFilepath=None):\r\n        \"\"\"\r\n        Checks to see if the inputed path represents an existing file or directory.\r\n\r\n        :param      relpath     | <str>\r\n                    rsc         | <str>\r\n                    useFilepath | <bool> or None\r\n        \"\"\"\n", "input": "", "output": "        path = self.find(relpath, rsc, useFilepath)\r\n        if path.startswith(':'):\r\n            return QtCore.QResource(path).isValid()\r\n        else:\r\n            return os.path.exists(path)", "category": "Python"}, {"instruction": "def get_assessment_part_id(self):\n        \"\"\"Gets the assessment part ``Id`` to which this rule belongs.\n\n        return: (osid.id.Id) - ``Id`` of an assessment part\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for osid.learning.Activity.get_objective_id\n        if not bool(self._my_map['assessmentPartId']):\n            raise errors.IllegalState('assessment_part empty')\n        return Id(self._my_map['assessmentPartId'])", "category": "Python"}, {"instruction": "def map_copy(source: tcod.map.Map, dest: tcod.map.Map) -> None:\n    \"\"\"Copy map data from `source` to `dest`.\n\n    .. deprecated:: 4.5\n        Use Python's copy module, or see :any:`tcod.map.Map` and assign between\n        array attributes manually.\n    \"\"\"\n", "input": "", "output": "    if source.width != dest.width or source.height != dest.height:\n        dest.__init__(  # type: ignore\n            source.width, source.height, source._order\n        )\n    dest._Map__buffer[:] = source._Map__buffer[:]", "category": "Python"}, {"instruction": "def _write_init_models(self, filenames):\n        \"\"\" Write init file\n\n            Args:\n                filenames (dict): dict of filename and classes\n\n        \"\"\"\n", "input": "", "output": "\n        self.write(destination=self.output_directory, filename=\"__init__.py\", template_name=\"__init_model__.py.tpl\",\n                   filenames=self._prepare_filenames(filenames),\n                   class_prefix=self._class_prefix,\n                   product_accronym=self._product_accronym,\n                   header=self.header_content)", "category": "Python"}, {"instruction": "def set_stripe_api_version(version=None, validate=True):\n\t\"\"\"\n\tSet the desired API version to use for Stripe requests.\n\n\t:param version: The version to set for the Stripe API.\n\t:type version: ``str``\n\t:param validate: If True validate the value for the specified version).\n\t:type validate: ``bool``\n\t\"\"\"\n", "input": "", "output": "\tversion = version or get_stripe_api_version()\n\n\tif validate:\n\t\tvalid = validate_stripe_api_version(version)\n\t\tif not valid:\n\t\t\traise ValueError(\"Bad stripe API version: {}\".format(version))\n\n\tstripe.api_version = version", "category": "Python"}, {"instruction": "def _represent_match_traversal(match_traversal):\n    \"\"\"Emit MATCH query code for an entire MATCH traversal sequence.\"\"\"\n", "input": "", "output": "    output = []\n\n    output.append(_first_step_to_match(match_traversal[0]))\n    for step in match_traversal[1:]:\n        output.append(_subsequent_step_to_match(step))\n\n    return u''.join(output)", "category": "Python"}, {"instruction": "def force_unicode(value):\n    \"\"\"\n    Forces a bytestring to become a Unicode string.\n    \"\"\"\n", "input": "", "output": "    if IS_PY3:\n        # Python 3.X\n        if isinstance(value, bytes):\n            value = value.decode('utf-8', errors='replace')\n        elif not isinstance(value, str):\n            value = str(value)\n    else:\n        # Python 2.X\n        if isinstance(value, str):\n            value = value.decode('utf-8', 'replace')\n        elif not isinstance(value, basestring):  # NOQA: F821\n            value = unicode(value)  # NOQA: F821\n\n    return value", "category": "Python"}, {"instruction": "def untrace_property(cls, accessor):\n    \"\"\"\n    Untraces given class property.\n\n    :param cls: Class of the property.\n    :type cls: object\n    :param accessor: Property to untrace.\n    :type accessor: property\n    :return: Definition success.\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "\n    if not is_traced(accessor.fget) or not is_traced(accessor.fset) or not is_traced(accessor.fdel):\n        return False\n\n    name = get_method_name(accessor)\n    setattr(cls, name, property(untracer(accessor.fget),\n                                untracer(accessor.fset),\n                                untracer(accessor.fdel)))\n    return True", "category": "Python"}, {"instruction": "def valid(cls, token, **kwargs):\n        \"\"\"\n        Check if a token exists and has not expired\n\n        :param token: the token\n        :return: bool\n        \"\"\"\n", "input": "", "output": "        try:\n            token = yield cls.get(token)\n        except couch.NotFound:\n            raise Return(False)\n\n        raise Return(token.ttl >= datetime.utcnow())", "category": "Python"}, {"instruction": "def send(self, load, tries=3, timeout=60, raw=False):\n        '''\n        Send a request, return a future which will complete when we send the message\n        '''\n", "input": "", "output": "        if self.crypt == 'clear':\n            ret = yield self._uncrypted_transfer(load, tries=tries, timeout=timeout)\n        else:\n            ret = yield self._crypted_transfer(load, tries=tries, timeout=timeout, raw=raw)\n        raise tornado.gen.Return(ret)", "category": "Python"}, {"instruction": "def update_group(self, ID, data):\n        \"\"\"Update a Group.\"\"\"\n", "input": "", "output": "        # http://teampasswordmanager.com/docs/api-groups/#update_group\n        log.info('Update group %s with %s' % (ID, data))\n        self.put('groups/%s.json' % ID, data)", "category": "Python"}, {"instruction": "def spectrogram(self, ref=None, segmentLengthMultiplier=1, window='hann'):\n        \"\"\"\n        analyses the source to generate a spectrogram\n        :param ref: the reference value for dB purposes.\n        :param segmentLengthMultiplier: allow for increased resolution.\n        :return:\n            t : ndarray\n            Array of time slices.\n            f : ndarray\n            Array of sample frequencies.\n            Pxx : ndarray\n            linear spectrum values.\n        \"\"\"\n", "input": "", "output": "        t, f, Sxx = signal.spectrogram(self.samples,\n                                       self.fs,\n                                       window=window,\n                                       nperseg=self.getSegmentLength() * segmentLengthMultiplier,\n                                       detrend=False,\n                                       scaling='spectrum')\n        Sxx = np.sqrt(Sxx)\n        if ref is not None:\n            Sxx = librosa.amplitude_to_db(Sxx, ref)\n        return t, f, Sxx", "category": "Python"}, {"instruction": "def prove(self, file, challenge, tag):\n        \"\"\"Returns a proof of ownership of the given file based on the\n        challenge.  The proof consists of a hash of the specified file chunk\n        and the complete merkle branch.\n\n        :param file: a file that supports `read()`, `seek()` and `tell()`\n        :param challenge: the challenge to use for generating this proof\n        :param tag: the file tag as provided from the client\n        :param filesz: optional filesz parameter.  if not specified, the\n            filesz will be detected by seeking to the end of the stream\n        \"\"\"\n", "input": "", "output": "        leaf = MerkleLeaf(challenge.index,\n                          MerkleHelper.get_chunk_hash(file,\n                                                      challenge.seed,\n                                                      filesz=tag.filesz,\n                                                      chunksz=tag.chunksz))\n        return Proof(leaf, tag.tree.get_branch(challenge.index))", "category": "Python"}, {"instruction": "def create_user(self, claims):\n        \"\"\"\n        Create the user if it doesn't exist yet\n\n        Args:\n            claims (dict): claims from the access token\n\n        Returns:\n            django.contrib.auth.models.User: A Django user\n        \"\"\"\n", "input": "", "output": "        # Create the user\n        username_claim = settings.USERNAME_CLAIM\n        usermodel = get_user_model()\n        user, created = usermodel.objects.get_or_create(**{\n            usermodel.USERNAME_FIELD: claims[username_claim]\n        })\n        if created or not user.password:\n            user.set_unusable_password()\n            logger.debug(\"User '{}' has been created.\".format(claims[username_claim]))\n\n        return user", "category": "Python"}, {"instruction": "def get_nonce(block_representation, coin_symbol='btc', api_key=None):\n    '''\n    Takes a block_representation and returns the nonce\n    '''\n", "input": "", "output": "    return get_block_overview(block_representation=block_representation,\n            coin_symbol=coin_symbol, txn_limit=1, api_key=api_key)['bits']", "category": "Python"}, {"instruction": "def set_flag(flag, value=None):\n    \"\"\"set_flag(flag)\n    Set the given flag as active.\n\n    :param str flag: Name of flag to set.\n\n    .. note:: **Changes to flags are reset when a handler crashes.** Changes to\n       flags happen immediately, but they are only persisted at the end of a\n       complete and successful run of the reactive framework. All unpersisted\n       changes are discarded when a hook crashes.\n    \"\"\"\n", "input": "", "output": "    old_flags = get_flags()\n    unitdata.kv().update({flag: value}, prefix='reactive.states.')\n    if flag not in old_flags:\n        tracer().set_flag(flag)\n        FlagWatch.change(flag)\n        trigger = _get_trigger(flag, None)\n        for flag_name in trigger['set_flag']:\n            set_flag(flag_name)\n        for flag_name in trigger['clear_flag']:\n            clear_flag(flag_name)", "category": "Python"}, {"instruction": "def _compute_acq_withGradients(self, x):\n        \"\"\"\n        Integrated GP-Lower Confidence Bound and its derivative\n        \"\"\"\n", "input": "", "output": "        means, stds, dmdxs, dsdxs = self.model.predict_withGradients(x)\n        f_acqu = None\n        df_acqu = None\n        for m, s, dmdx, dsdx in zip(means, stds, dmdxs, dsdxs):  \n            f = -m + self.exploration_weight * s        \n            df = -dmdx + self.exploration_weight * dsdx\n            if f_acqu is None:\n                f_acqu = f        \n                df_acqu = df\n            else:\n                f_acqu += f\n                df_acqu += df\n        return f_acqu/(len(means)), df_acqu/(len(means))", "category": "Python"}, {"instruction": "def get_max_op_version():\n    '''\n    .. versionadded:: 2019.2.0\n\n    Returns the glusterfs volume's max op-version value\n    Requires Glusterfs version > 3.9\n\n    CLI Example:\n    .. code-block:: bash\n\n        salt '*' glusterfs.get_max_op_version\n    '''\n", "input": "", "output": "    if _get_version() < (3, 10,):\n        return False, 'Glusterfs version must be 3.10+.  Your version is {0}.'.format(str('.'.join(str(i) for i in _get_version())))\n\n    cmd = 'volume get all cluster.max-op-version'\n    root = _gluster_xml(cmd)\n\n    if not _gluster_ok(root):\n        return False, root.find('opErrstr').text\n\n    result = {}\n    for max_op_version in _iter(root, 'volGetopts'):\n        for item in max_op_version:\n            if item.tag == 'Value':\n                result = item.text\n            elif item.tag == 'Opt':\n                for child in item:\n                    if child.tag == 'Value':\n                        result = child.text\n\n    return result", "category": "Python"}, {"instruction": "def _parse_fields_http(self, *args, **kwargs):\n        \"\"\"\n        Deprecated. This will be removed in a future release.\n        \"\"\"\n", "input": "", "output": "\n        from warnings import warn\n        warn('IPASN._parse_fields_http() has been deprecated and will be '\n             'removed. You should now use IPASN.parse_fields_http().')\n        return self.parse_fields_http(*args, **kwargs)", "category": "Python"}, {"instruction": "def saltmem(human_readable=False):\n    '''\n    .. versionadded:: 2015.8.0\n\n    Returns the amount of memory that salt is using\n\n    human_readable : False\n        return the value in a nicely formatted number\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' status.saltmem\n        salt '*' status.saltmem human_readable=True\n    '''\n", "input": "", "output": "    # psutil.Process defaults to current process (`os.getpid()`)\n    p = psutil.Process()\n\n    # Use oneshot to get a snapshot\n    with p.oneshot():\n        mem = p.memory_info().rss\n\n    if human_readable:\n        return _byte_calc(mem)\n\n    return mem", "category": "Python"}, {"instruction": "def ProcessRequest(self, method_config, request):\n        \"\"\"Hook for pre-processing of requests.\"\"\"\n", "input": "", "output": "        if self.log_request:\n            logging.info(\n                'Calling method %s with %s: %s', method_config.method_id,\n                method_config.request_type_name, request)\n        return request", "category": "Python"}, {"instruction": "def get_view_model(cls):\n    \"\"\"\n    Get the model to use in the filter_class by inspecting\n    the queryset or by using a declared auto_filters_model\n    \"\"\"\n", "input": "", "output": "    msg = 'When using get_queryset you must set a auto_filters_model field in the viewset'\n    if cls.queryset is not None:\n        return cls.queryset.model\n    else:\n        assert hasattr(cls, 'auto_filters_model'), msg\n        return cls.auto_filters_model", "category": "Python"}, {"instruction": "def get_metrics(self):\n        \"\"\"Get a metric for each gauge in the registry at the current time.\n\n        :rtype: set(:class:`opencensus.metrics.export.metric.Metric`)\n        :return: A set of `Metric`s, one for each registered gauge.\n        \"\"\"\n", "input": "", "output": "        now = datetime.utcnow()\n        metrics = set()\n        for gauge in self.gauges.values():\n            metrics.add(gauge.get_metric(now))\n        return metrics", "category": "Python"}, {"instruction": "def _set_account_policy(name, policy):\n    '''\n    Set a value in the user accountPolicy. For use by this module only\n\n    :param str name: The user name\n    :param str policy: The policy to apply\n\n    :return: True if success, otherwise False\n    :rtype: bool\n\n    :raises: CommandExecutionError on user not found or any other unknown error\n    '''\n", "input": "", "output": "    cmd = 'pwpolicy -u {0} -setpolicy \"{1}\"'.format(name, policy)\n\n    try:\n        return salt.utils.mac_utils.execute_return_success(cmd)\n    except CommandExecutionError as exc:\n        if 'Error: user <{0}> not found'.format(name) in exc.strerror:\n            raise CommandExecutionError('User not found: {0}'.format(name))\n        raise CommandExecutionError('Unknown error: {0}'.format(exc.strerror))", "category": "Python"}, {"instruction": "def process_fig_and_ax_argument(fig, ax, default_figsize=None):\n    \"\"\"Process 'fig' and 'ax' arguments.\n\n    'fig' is of type: 'matplotlib.figure.Figure' (or its child object)\n    'ax' is of type: 'matplotlib.axes._base._AxesBase' (or its child object)\n\n    'fig' and 'ax' should be simultaneously None or of respective proper type.\n    \"\"\"\n", "input": "", "output": "    if default_figsize is not None:\n        assert type(default_figsize) in [tuple, list]\n        assert len(default_figsize) == 2\n\n    if (fig is None) and (ax is None):\n        fig, ax = plt.subplots(figsize=default_figsize)\n    else:\n        assert (is_figure(fig)) and (is_axes(ax))\n    return fig, ax", "category": "Python"}, {"instruction": "def _construct_register(reg, default_reg):\n    \"\"\"Constructs a register dict.\"\"\"\n", "input": "", "output": "    if reg:\n        x = dict((k, reg.get(k, d)) for k, d in default_reg.items())\n    else:\n        x = dict(default_reg)\n    return x", "category": "Python"}, {"instruction": "def parse(f, encoding='utf-8'):\n    \"\"\"\n    Parse the TDL file *f* and yield the interpreted contents.\n\n    If *f* is a filename, the file is opened and closed when the\n    generator has finished, otherwise *f* is an open file object and\n    will not be closed when the generator has finished.\n\n    Args:\n        f (str, file): a filename or open file object\n        encoding (str): the encoding of the file (default: `\"utf-8\"`;\n            ignored if *f* is an open file)\n    \"\"\"\n", "input": "", "output": "    if hasattr(f, 'read'):\n        for event in _parse(f):\n            yield event\n    else:\n        with io.open(f, encoding=encoding) as fh:\n            for event in _parse(fh):\n                yield event", "category": "Python"}, {"instruction": "def reverse(self, matching_name, **kwargs):\n        \"\"\" Getting a matching name and URL args and return a corresponded URL\n        \"\"\"\n", "input": "", "output": "        for record in self.matching_records:\n            if record.name == matching_name:\n                path_template = record.path_template\n                break\n        else:\n            raise NotReversed\n\n        if path_template.wildcard_name:\n            l = kwargs.get(path_template.wildcard_name)\n            if not l:\n                raise NotReversed\n            additional_path = '/'.join(l)\n            extra_path_elements = path_template.pattern.split('/')[:-1]\n            pattern = join_paths('/'.join(extra_path_elements), additional_path)\n        else:\n            pattern = path_template.pattern\n\n        try:\n            url = pattern.format(**kwargs)\n        except KeyError:\n            raise NotReversed\n        return url", "category": "Python"}, {"instruction": "def render_tag(self, context, caller, **kwargs):\n        '''render content with \"active\" urls logic'''\n", "input": "", "output": "        # load configuration from passed options\n        self.load_configuration(**kwargs)\n\n        # get request from context\n        request = context['request']\n\n        # get full path from request\n        self.full_path = request.get_full_path()\n\n        # render content of extension\n        content = caller()\n\n        # check content for \"active\" urls\n        content = render_content(\n            content,\n            full_path=self.full_path,\n            parent_tag=self.parent_tag,\n            css_class=self.css_class,\n            menu=self.menu,\n            ignore_params=self.ignore_params,\n        )\n\n        return content", "category": "Python"}, {"instruction": "def _get_recipients(self, array):\n        \"\"\"Returns an iterator of objects\n           in the form [\"Name <address@example.com\", ...]\n           from the array [[\"address@example.com\", \"Name\"]]\n        \"\"\"\n", "input": "", "output": "        for address, name in array:\n            if not name:\n                yield address\n            else:\n                yield \"\\\"%s\\\" <%s>\" % (name, address)", "category": "Python"}, {"instruction": "def cast_item(cls, item):\r\n        \"\"\"Cast list item to the appropriate tag type.\"\"\"\n", "input": "", "output": "        if not isinstance(item, cls.subtype):\r\n            incompatible = isinstance(item, Base) and not any(\r\n                issubclass(cls.subtype, tag_type) and isinstance(item, tag_type)\r\n                for tag_type in cls.all_tags.values()\r\n            )\r\n            if incompatible:\r\n                raise IncompatibleItemType(item, cls.subtype)\r\n\r\n            try:\r\n                return cls.subtype(item)\r\n            except EndInstantiation:\r\n                raise ValueError('List tags without an explicit subtype must '\r\n                                 'either be empty or instantiated with '\r\n                                 'elements from which a subtype can be '\r\n                                 'inferred') from None\r\n            except (IncompatibleItemType, CastError):\r\n                raise\r\n            except Exception as exc:\r\n                raise CastError(item, cls.subtype) from exc\r\n        return item", "category": "Python"}, {"instruction": "def read_checksum_digest(path, checksum_cls=hashlib.sha256):\n  \"\"\"Given a hash constructor, returns checksum digest and size of file.\"\"\"\n", "input": "", "output": "  checksum = checksum_cls()\n  size = 0\n  with tf.io.gfile.GFile(path, \"rb\") as f:\n    while True:\n      block = f.read(io.DEFAULT_BUFFER_SIZE)\n      size += len(block)\n      if not block:\n        break\n      checksum.update(block)\n  return checksum.hexdigest(), size", "category": "Python"}, {"instruction": "def _server_rollback():\n    \"\"\"Removes script database and archive files to rollback the CI server\n    installation.\n    \"\"\"\n", "input": "", "output": "    #Remove the data and archive files specified in settings. The cron\n    #gets remove by the _setup_server() script if -rollback is specified.\n    from os import path, remove\n    archpath = path.abspath(path.expanduser(settings.archfile))\n    if path.isfile(archpath) and not args[\"nolive\"]:\n        vms(\"Removing archive JSON file at {}.\".format(archpath))\n        remove(archpath)\n    datapath = path.abspath(path.expanduser(settings.datafile))\n    if path.isfile(datapath) and not args[\"nolive\"]:\n        vms(\"Removing script database JSON file at {}\".format(datapath))\n        remove(datapath)", "category": "Python"}, {"instruction": "def setStaticNodes(self, nodes, preemptable):\n        \"\"\"\n        Used to track statically provisioned nodes. This method must be called\n        before any auto-scaled nodes are provisioned.\n\n        These nodes are treated differently than auto-scaled nodes in that they should\n        not be automatically terminated.\n\n        :param nodes: list of Node objects\n        \"\"\"\n", "input": "", "output": "        prefix = 'non-' if not preemptable else ''\n        logger.debug(\"Adding %s to %spreemptable static nodes\", nodes, prefix)\n        if nodes is not None:\n            self.static[preemptable] = {node.privateIP : node for node in nodes}", "category": "Python"}, {"instruction": "def hex_decode(input, errors='strict'):\n    \"\"\" Decodes the object input and returns a tuple (output\n        object, length consumed).\n\n        input must be an object which provides the bf_getreadbuf\n        buffer slot. Python strings, buffer objects and memory\n        mapped files are examples of objects providing this slot.\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n", "input": "", "output": "    assert errors == 'strict'\n    output = binascii.a2b_hex(\"\".join(char for char in input if char in hexdigits))\n    return (output, len(input))", "category": "Python"}, {"instruction": "def is_interactive(self):\n        \"\"\" Determine if the user requested interactive mode.\n        \"\"\"\n", "input": "", "output": "        # The Python interpreter sets sys.flags correctly, so use them!\n        if sys.flags.interactive:\n            return True\n\n        # IPython does not set sys.flags when -i is specified, so first\n        # check it if it is already imported.\n        if '__IPYTHON__' not in dir(six.moves.builtins):\n            return False\n\n        # Then we check the application singleton and determine based on\n        # a variable it sets.\n        try:\n            from IPython.config.application import Application as App\n            return App.initialized() and App.instance().interact\n        except (ImportError, AttributeError):\n            return False", "category": "Python"}, {"instruction": "def get_config_value(key, config_path=None, default=None):\n    \"\"\"Get a configuration value.\n\n    Preference:\n    1. From environment\n    2. From JSON configuration file supplied in ``config_path`` argument\n    3. The default supplied to the function\n\n    :param key: name of lookup value\n    :param config_path: path to JSON configuration file\n    :param default: default fall back value\n    :returns: value associated with the key\n    \"\"\"\n", "input": "", "output": "    if config_path is None:\n        config_path = DEFAULT_CONFIG_PATH\n\n    # Start by setting default value\n    value = default\n\n    # Update from config file\n    value = get_config_value_from_file(\n        key=key,\n        config_path=config_path,\n        default=value\n    )\n\n    # Update from environment variable\n    value = os.environ.get(key, value)\n    return value", "category": "Python"}, {"instruction": "def padded_accuracy_topk(predictions,\n                         labels,\n                         k,\n                         weights_fn=common_layers.weights_nonzero):\n  \"\"\"Percentage of times that top-k predictions matches labels on non-0s.\"\"\"\n", "input": "", "output": "  with tf.variable_scope(\"padded_accuracy_topk\", values=[predictions, labels]):\n    padded_predictions, padded_labels = common_layers.pad_with_zeros(\n        predictions, labels)\n    weights = weights_fn(padded_labels)\n    effective_k = tf.minimum(k,\n                             common_layers.shape_list(padded_predictions)[-1])\n    _, outputs = tf.nn.top_k(padded_predictions, k=effective_k)\n    outputs = tf.to_int32(outputs)\n    padded_labels = tf.to_int32(padded_labels)\n    padded_labels = tf.expand_dims(padded_labels, axis=-1)\n    padded_labels += tf.zeros_like(outputs)  # Pad to same shape.\n    same = tf.to_float(tf.equal(outputs, padded_labels))\n    same_topk = tf.reduce_sum(same, axis=-1)\n    return same_topk, weights", "category": "Python"}, {"instruction": "def K(self, X, X2, target):\n        \"\"\"Return covariance between X and X2.\"\"\"\n", "input": "", "output": "        self._K_computations(X, X2)\n        target += self.variance*self._K_dvar", "category": "Python"}, {"instruction": "def _isNonAxi(Pot):\n    \"\"\"\n    NAME:\n\n       _isNonAxi\n\n    PURPOSE:\n\n       Determine whether this potential is non-axisymmetric\n\n    INPUT:\n\n       Pot - Potential instance or list of such instances\n\n    OUTPUT:\n\n       True or False depending on whether the potential is non-axisymmetric (note that some potentials might return True, even though for some parameter values they are axisymmetric)\n\n    HISTORY:\n\n       2016-06-16 - Written - Bovy (UofT)\n\n    \"\"\"\n", "input": "", "output": "    isList= isinstance(Pot,list)\n    if isList:\n        isAxis= [not _isNonAxi(p) for p in Pot]\n        nonAxi= not nu.prod(nu.array(isAxis))\n    else:\n        nonAxi= Pot.isNonAxi\n    return nonAxi", "category": "Python"}, {"instruction": "def task_id_str(task_family, params):\n    \"\"\"\n    Returns a canonical string used to identify a particular task\n\n    :param task_family: The task family (class name) of the task\n    :param params: a dict mapping parameter names to their serialized values\n    :return: A unique, shortened identifier corresponding to the family and params\n    \"\"\"\n", "input": "", "output": "    # task_id is a concatenation of task family, the first values of the first 3 parameters\n    # sorted by parameter name and a md5hash of the family/parameters as a cananocalised json.\n    param_str = json.dumps(params, separators=(',', ':'), sort_keys=True)\n    param_hash = hashlib.md5(param_str.encode('utf-8')).hexdigest()\n\n    param_summary = '_'.join(p[:TASK_ID_TRUNCATE_PARAMS]\n                             for p in (params[p] for p in sorted(params)[:TASK_ID_INCLUDE_PARAMS]))\n    param_summary = TASK_ID_INVALID_CHAR_REGEX.sub('_', param_summary)\n\n    return '{}_{}_{}'.format(task_family, param_summary, param_hash[:TASK_ID_TRUNCATE_HASH])", "category": "Python"}, {"instruction": "def devno_free_if_allocated(self, devno):\n        \"\"\"\n        Free a device number allocated with :meth:`devno_alloc`.\n\n        If the device number is not currently allocated or not in the pool\n        range, nothing happens.\n\n        Parameters:\n          devno (string): The device number as four hexadecimal digits.\n        \"\"\"\n", "input": "", "output": "        devno_int = int(devno, 16)\n        self._devno_pool.free_if_allocated(devno_int)", "category": "Python"}, {"instruction": "def _Cfunction(name, flags, errcheck, *types):\n    \"\"\"(INTERNAL) New ctypes function binding.\n    \"\"\"\n", "input": "", "output": "    if hasattr(dll, name) and name in _Globals:\n        p = ctypes.CFUNCTYPE(*types)\n        f = p((name, dll), flags)\n        if errcheck is not None:\n            f.errcheck = errcheck\n        # replace the Python function\n        # in this module, but only when\n        # running as python -O or -OO\n        if __debug__:\n            _Cfunctions[name] = f\n        else:\n            _Globals[name] = f\n        return f\n    raise NameError('no function %r' % (name,))", "category": "Python"}, {"instruction": "def set_content_model(self):\n        \"\"\"\n        Set content_model to the child class's related name, or None if this is\n        the base class.\n        \"\"\"\n", "input": "", "output": "        if not self.content_model:\n            is_base_class = (\n                base_concrete_model(ContentTyped, self) == self.__class__)\n            self.content_model = (\n                None if is_base_class else self.get_content_model_name())", "category": "Python"}, {"instruction": "def get_list_url(self, kind_slug=None):\n        \"\"\"\n        Get the list URL for this Work.\n        You can also pass a kind_slug in (e.g. 'movies') and it will use that\n        instead of the Work's kind_slug. (Why? Useful in views. Or tests of\n        views, at least.)\n        \"\"\"\n", "input": "", "output": "        if kind_slug is None:\n            kind_slug = self.KIND_SLUGS[self.kind]\n        return reverse('spectator:events:work_list',\n                                            kwargs={'kind_slug': kind_slug})", "category": "Python"}, {"instruction": "def _index(self, refresh_time=None):\n        \"\"\"Bottle callback for index.html (/) file.\"\"\"\n", "input": "", "output": "\n        if refresh_time is None or refresh_time < 1:\n            refresh_time = self.args.time\n\n        # Update the stat\n        self.__update__()\n\n        # Display\n        return template(\"index.html\", refresh_time=refresh_time)", "category": "Python"}, {"instruction": "def is_complete(self, all_domain_residue_ids):\n        '''Check that all ligands (specified via the set or list all_domain_residue_ids containing columns 21:27 of the\n           HETATM records) in the source PDB file are considered in the mapping.'''\n", "input": "", "output": "        mapped_domain_residues = sorted([v.from_pdb_residue_id for k, v in self.mapping.iteritems()])\n        assert(len(all_domain_residue_ids) == len(set(all_domain_residue_ids)))\n        return mapped_domain_residues == sorted(all_domain_residue_ids)", "category": "Python"}, {"instruction": "def get(self, treeiter, *columns):\n        \"\"\"\n        :param treeiter: the :obj:`Gtk.TreeIter`\n        :type treeiter: :obj:`Gtk.TreeIter`\n\n        :param \\\\*columns: a list of column indices to fetch\n        :type columns: (:obj:`int`)\n\n        Returns a tuple of all values specified by their indices in `columns`\n        in the order the indices are contained in `columns`\n\n        Also see :obj:`Gtk.TreeStore.get_value`\\\\()\n        \"\"\"\n", "input": "", "output": "\n        n_columns = self.get_n_columns()\n\n        values = []\n        for col in columns:\n            if not isinstance(col, int):\n                raise TypeError(\"column numbers must be ints\")\n\n            if col < 0 or col >= n_columns:\n                raise ValueError(\"column number is out of range\")\n\n            values.append(self.get_value(treeiter, col))\n\n        return tuple(values)", "category": "Python"}, {"instruction": "def masked_array(data, data_units=None, **kwargs):\n    \"\"\"Create a :class:`numpy.ma.MaskedArray` with units attached.\n\n    This is a thin wrapper around :func:`numpy.ma.masked_array` that ensures that\n    units are properly attached to the result (otherwise units are silently lost). Units\n    are taken from the ``units`` argument, or if this is ``None``, the units on ``data``\n    are used.\n\n    Parameters\n    ----------\n    data : array_like\n        The source data. If ``units`` is `None`, this should be a `pint.Quantity` with\n        the desired units.\n    data_units : str or `pint.Unit`\n        The units for the resulting `pint.Quantity`\n    **kwargs : Arbitrary keyword arguments passed to `numpy.ma.masked_array`\n\n    Returns\n    -------\n    `pint.Quantity`\n\n    \"\"\"\n", "input": "", "output": "    if data_units is None:\n        data_units = data.units\n    return units.Quantity(np.ma.masked_array(data, **kwargs), data_units)", "category": "Python"}, {"instruction": "def SecurityCheck(func):\n  \"\"\"A decorator applied to protected web handlers.\"\"\"\n", "input": "", "output": "\n  def Wrapper(request, *args, **kwargs):\n    ", "category": "Python"}, {"instruction": "def compute_metric(self, components):\n        \"\"\"Compute precision from `components`\"\"\"\n", "input": "", "output": "        numerator = components[PRECISION_RELEVANT_RETRIEVED]\n        denominator = components[PRECISION_RETRIEVED]\n        if denominator == 0.:\n            if numerator == 0:\n                return 1.\n            else:\n                raise ValueError('')\n        else:\n            return numerator/denominator", "category": "Python"}, {"instruction": "def all(self, paths, access=None, recursion=False):\n        \"\"\"\n        Iterates over `paths` (which may consist of files and/or directories).\n        Removes duplicates and returns list of valid paths meeting access\n        criteria.\n        \"\"\"\n", "input": "", "output": "        self.__init__()\n        self.access = access\n        self.filetype = 'all'\n        self.paths = paths\n        self.recursion = recursion\n\n        return _sorter(self._generator_other())", "category": "Python"}, {"instruction": "async def digital_read(self, command):\n        \"\"\"\n        This method reads and returns the last reported value for a digital pin.\n        Normally not used since digital pin updates will be provided automatically\n        as they occur with the digital_message_reply being sent to the client after set_pin_mode is called..\n        (see enable_digital_reporting for message format)\n\n        :param command: {\"method\": \"digital_read\", \"params\": [PIN]}\n        :returns: {\"method\": \"digital_read_reply\", \"params\": [PIN, DIGITAL_DATA_VALUE]}\n        \"\"\"\n", "input": "", "output": "        pin = int(command[0])\n        data_val = await self.core.digital_read(pin)\n        reply = json.dumps({\"method\": \"digital_read_reply\", \"params\": [pin, data_val]})\n        await self.websocket.send(reply)", "category": "Python"}, {"instruction": "def selected_canvas_agglayer(self):\n        \"\"\"Obtain the canvas aggregation layer selected by user.\n\n        :returns: The currently selected map layer in the list.\n        :rtype: QgsMapLayer\n        \"\"\"\n", "input": "", "output": "        if self.lstCanvasAggLayers.selectedItems():\n            item = self.lstCanvasAggLayers.currentItem()\n        else:\n            return None\n        try:\n            layer_id = item.data(QtCore.Qt.UserRole)\n        except (AttributeError, NameError):\n            layer_id = None\n\n        layer = QgsProject.instance().mapLayer(layer_id)\n        return layer", "category": "Python"}, {"instruction": "def _get_md_files(self):\n        \"\"\"Get all markdown files.\"\"\"\n", "input": "", "output": "        all_f = _all_files_matching_ext(os.getcwd(), \"md\")\n        exclusions = [\n            \"*.egg/*\",\n            \"*.eggs/*\",\n            \"*build/*\"\n        ] + self.exclusions\n        return sorted([f for f in all_f if not _is_excluded(f, exclusions)])", "category": "Python"}, {"instruction": "def parse(self, data):\n        \"\"\"\n        populate instance with values and sub-sections\n        :param data: UTF-8 encoded manifest\n        :type data: bytes\n        \"\"\"\n", "input": "", "output": "\n        data = data.decode('utf-8')\n        self.linesep = detect_linesep(data)\n\n        # the first section is the main one for the manifest. It's\n        # also where we will check for our newline separator\n        sections = parse_sections(data)\n        self.load(next(sections))\n\n        # and all following sections are considered sub-sections\n        for section in sections:\n            next_section = ManifestSection(None)\n            next_section.load(section)\n            self.sub_sections[next_section.primary()] = next_section", "category": "Python"}, {"instruction": "def bulk_get(cls, imports, api=None):\n        \"\"\"\n        Retrieve imports in bulk\n        :param imports: Imports to be retrieved.\n        :param api: Api instance.\n        :return: List of ImportBulkRecord objects.\n        \"\"\"\n", "input": "", "output": "        api = api or cls._API\n        import_ids = [Transform.to_import(import_) for import_ in imports]\n        data = {'import_ids': import_ids}\n\n        response = api.post(url=cls._URL['bulk_get'], data=data)\n        return ImportBulkRecord.parse_records(response=response, api=api)", "category": "Python"}, {"instruction": "def missing_any(da, freq, **kwds):\n    r\"\"\"Return a boolean DataArray indicating whether there are missing days in the resampled array.\n\n    Parameters\n    ----------\n    da : DataArray\n      Input array at daily frequency.\n    freq : str\n      Resampling frequency.\n\n    Returns\n    -------\n    out : DataArray\n      A boolean array set to True if any month or year has missing values.\n    \"\"\"\n", "input": "", "output": "    c = da.notnull().resample(time=freq).sum(dim='time')\n\n    if '-' in freq:\n        pfreq, anchor = freq.split('-')\n    else:\n        pfreq = freq\n\n    if pfreq.endswith('S'):\n        start_time = c.indexes['time']\n        end_time = start_time.shift(1, freq=freq)\n    else:\n        end_time = c.indexes['time']\n        start_time = end_time.shift(-1, freq=freq)\n\n    n = (end_time - start_time).days\n    nda = xr.DataArray(n.values, coords={'time': c.time}, dims='time')\n    return c != nda", "category": "Python"}, {"instruction": "def dropEvent(self, ev):\n        \"\"\"Process drop event.\"\"\"\n", "input": "", "output": "        # use function above to accept event if it contains a file URL\n        files = self._checkDragDropEvent(ev)\n        if files:\n            pos = ev.pos()\n            dropitem = self.itemAt(pos)\n            dprint(1, \"dropped on\", pos.x(), pos.y(), dropitem and str(dropitem.text(1)))\n            # if event originated with ourselves, reorder items\n            if ev.source() is self:\n                self.reorderItems(dropitem, *files)\n            # else event is from someone else, accept the dropped files\n            else:\n                self._dropped_on = dropitem\n                self.emit(SIGNAL(\"droppedFiles\"), *files)\n                # if event originated with another DPTreeWidget, emit a draggedAwayFiles() signal on its behalf\n                if isinstance(ev.source(), DPTreeWidget):\n                    ev.source().emit(SIGNAL(\"draggedAwayFiles\"), *files)", "category": "Python"}, {"instruction": "def iter_documents(self, fileids=None, categories=None, _destroy=False):\n        \"\"\" Return an iterator over corpus documents. \"\"\"\n", "input": "", "output": "        doc_ids = self._filter_ids(fileids, categories)\n        for doc in imap(self.get_document, doc_ids):\n            yield doc\n            if _destroy:\n                doc.destroy()", "category": "Python"}, {"instruction": "def find_pair(self, crypto=\"\", fiat=\"\", verbose=False):\n        \"\"\"\n        This utility is used to find an exchange that supports a given exchange pair.\n        \"\"\"\n", "input": "", "output": "        self.fetch_pairs()\n        if not crypto and not fiat:\n            raise Exception(\"Fiat or Crypto required\")\n\n        def is_matched(crypto, fiat, pair):\n            if crypto and not fiat:\n                return pair.startswith(\"%s-\" % crypto)\n            if crypto and fiat:\n                return pair == \"%s-%s\" % (crypto, fiat)\n            if not crypto:\n                return pair.endswith(\"-%s\" % fiat)\n\n        matched_pairs = {}\n        for Service, pairs in self._all_pairs.items():\n            matched = [p for p in pairs if is_matched(crypto, fiat, p)]\n            if matched:\n                matched_pairs[Service] = matched\n\n        return matched_pairs", "category": "Python"}, {"instruction": "def predict(self, X, break_ties=\"random\", return_probs=False, **kwargs):\n        \"\"\"Predicts (int) labels for an input X on all tasks\n\n        Args:\n            X: The input for the predict_proba method\n            break_ties: A tie-breaking policy (see Classifier._break_ties())\n            return_probs: Return the predicted probabilities as well\n\n        Returns:\n            Y_p: An n-dim np.ndarray of predictions in {1,...k}\n            [Optionally: Y_s: An [n, k] np.ndarray of predicted probabilities]\n        \"\"\"\n", "input": "", "output": "        Y_s = self._to_numpy(self.predict_proba(X, **kwargs))\n        Y_p = self._break_ties(Y_s, break_ties).astype(np.int)\n        if return_probs:\n            return Y_p, Y_s\n        else:\n            return Y_p", "category": "Python"}, {"instruction": "def pbc_diff(fcoords1, fcoords2):\n    \"\"\"\n    Returns the 'fractional distance' between two coordinates taking into\n    account periodic boundary conditions.\n\n    Args:\n        fcoords1: First set of fractional coordinates. e.g., [0.5, 0.6,\n            0.7] or [[1.1, 1.2, 4.3], [0.5, 0.6, 0.7]]. It can be a single\n            coord or any array of coords.\n        fcoords2: Second set of fractional coordinates.\n\n    Returns:\n        Fractional distance. Each coordinate must have the property that\n        abs(a) <= 0.5. Examples:\n        pbc_diff([0.1, 0.1, 0.1], [0.3, 0.5, 0.9]) = [-0.2, -0.4, 0.2]\n        pbc_diff([0.9, 0.1, 1.01], [0.3, 0.5, 0.9]) = [-0.4, -0.4, 0.11]\n    \"\"\"\n", "input": "", "output": "    fdist = np.subtract(fcoords1, fcoords2)\n    return fdist - np.round(fdist)", "category": "Python"}, {"instruction": "def resize_image(self, data):\n        '''Resize image if height over 50 pixels and convert to JPEG.\n\n        Given a ByteIO or StringIO data input, this method ensures that the\n        image is not over 50 pixels high.  If it is over 50 pixels high, the\n        image is resized to precisely 50 pixels in height and the width is\n        adjusted accordingly in keeping with the width/height ratio.  The image\n        is always converted to JPEG to minimize any potentials issues while\n        embedding the image in the Excel workbook.\n\n        :param data: ByteIO or StringIO stream containing image data\n        :returns Reference to a BytesIO instance containing resized image data.\n\n        '''\n", "input": "", "output": "        image = Image.open(data)\n        stream_out = BytesIO()\n        width, height = image.size[:]\n        if height > 50:\n            width = int(width * 50 / height)\n            height = 50\n            image = image.resize((width, 50))\n\n        image.save(stream_out, format=\"JPEG\", quality=100)\n\n        stream_out.seek(0)\n        return stream_out", "category": "Python"}, {"instruction": "def fix_filename(path):\n    r\"\"\"Fix filenames for use in LaTeX.\n\n    Latex has problems if there are one or more points in the filename, thus\n    'abc.def.jpg' will be changed to '{abc.def}.jpg'\n\n    Args\n    ----\n    filename : str\n        The filen name to be changed.\n\n    Returns\n    -------\n    str\n        The new filename.\n\n    Examples\n    --------\n    >>> fix_filename(\"foo.bar.pdf\")\n    '{foo.bar}.pdf'\n    >>> fix_filename(\"/etc/local/foo.bar.pdf\")\n    '/etc/local/{foo.bar}.pdf'\n    >>> fix_filename(\"/etc/local/foo.bar.baz/document.pdf\")\n    '/etc/local/foo.bar.baz/document.pdf'\n    >>> fix_filename(\"/etc/local/foo.bar.baz/foo~1/document.pdf\")\n    '\\detokenize{/etc/local/foo.bar.baz/foo~1/document.pdf}'\n    \"\"\"\n", "input": "", "output": "\n    path_parts = path.split('/' if os.name == 'posix' else '\\\\')\n    dir_parts = path_parts[:-1]\n\n    filename = path_parts[-1]\n    file_parts = filename.split('.')\n\n    if len(file_parts) > 2:\n        filename = '{' + '.'.join(file_parts[0:-1]) + '}.' + file_parts[-1]\n\n    dir_parts.append(filename)\n    fixed_path = '/'.join(dir_parts)\n\n    if '~' in fixed_path:\n        fixed_path = r'\\detokenize{' + fixed_path + '}'\n\n    return fixed_path", "category": "Python"}, {"instruction": "def get_lb_pkgs(self):\n        \"\"\"Retrieves the local load balancer packages.\n\n        :returns: A dictionary containing the load balancer packages\n        \"\"\"\n", "input": "", "output": "\n        _filter = {'items': {'description':\n                             utils.query_filter('*Load Balancer*')}}\n\n        packages = self.prod_pkg.getItems(id=0, filter=_filter)\n        pkgs = []\n        for package in packages:\n            if not package['description'].startswith('Global'):\n                pkgs.append(package)\n        return pkgs", "category": "Python"}, {"instruction": "def upload_files(self, abspaths, relpaths, remote_objects):\n        \"\"\"\n        Determines files to be uploaded and call ``upload_file`` on each.\n        \"\"\"\n", "input": "", "output": "        for relpath in relpaths:\n            abspath = [p for p in abspaths if p[len(self.file_root):] == relpath][0]\n            cloud_datetime = remote_objects[relpath] if relpath in remote_objects else None\n            local_datetime = datetime.datetime.utcfromtimestamp(os.stat(abspath).st_mtime)\n\n            if cloud_datetime and local_datetime < cloud_datetime:\n                self.skip_count += 1\n                if not self.quiet:\n                    print(\"Skipped {0}: not modified.\".format(relpath))\n                continue\n            if relpath in remote_objects:\n                self.update_count += 1\n            else:\n                self.create_count += 1\n            self.upload_file(abspath, relpath)", "category": "Python"}, {"instruction": "def gen_chunks(self, gen):\n        \"\"\"Generates byte chunks of a given size.\n\n        Takes a bytes generator and yields chunks of a maximum of\n        ``chunk_size`` bytes.\n\n        Parameters\n        ----------\n        gen : generator\n            The bytes generator that produces the bytes\n        \"\"\"\n", "input": "", "output": "        for data in gen:\n            size = len(data)\n            if size < self.chunk_size:\n                yield data\n            else:\n                mv = buffer(data)\n                offset = 0\n                while offset < size:\n                    nb = min(self.chunk_size, size - offset)\n                    yield mv[offset:offset + nb]\n                    offset += nb", "category": "Python"}, {"instruction": "def all_groupings(partition):\n    \"\"\"Return all possible groupings of states for a particular coarse graining\n    (partition) of a network.\n\n    Args:\n        partition (tuple[tuple]): A partition of micro-elements into macro\n            elements.\n\n    Yields:\n        tuple[tuple[tuple]]: A grouping of micro-states into macro states of\n        system.\n\n    TODO: document exactly how to interpret the grouping.\n    \"\"\"\n", "input": "", "output": "    if not all(partition):\n        raise ValueError('Each part of the partition must have at least one '\n                         'element.')\n\n    micro_groupings = [_partitions_list(len(part) + 1) if len(part) > 1\n                       else [[[0], [1]]] for part in partition]\n\n    for grouping in itertools.product(*micro_groupings):\n        if all(len(element) < 3 for element in grouping):\n            yield tuple(tuple(tuple(tuple(state) for state in states)\n                              for states in grouping))", "category": "Python"}, {"instruction": "def _setup_root_filesystem(self, root_dir):\n        \"\"\"Setup the filesystem layout in the given root directory.\n        Create a copy of the existing proc- and dev-mountpoints in the specified root\n        directory. Afterwards we chroot into it.\n\n        @param root_dir: The path of the root directory that is used to execute the process.\n        \"\"\"\n", "input": "", "output": "        root_dir = root_dir.encode()\n\n        # Create an empty proc folder into the root dir. The grandchild still needs a view of\n        # the old /proc, therefore we do not mount a fresh /proc here.\n        proc_base = os.path.join(root_dir, b\"proc\")\n        util.makedirs(proc_base, exist_ok=True)\n\n        dev_base = os.path.join(root_dir, b\"dev\")\n        util.makedirs(dev_base, exist_ok=True)\n\n        # Create a copy of the host's dev- and proc-mountpoints.\n        # They are marked as private in order to not being changed\n        # by existing mounts during run execution.\n        container.make_bind_mount(b\"/dev/\", dev_base, recursive=True, private=True)\n        container.make_bind_mount(b\"/proc/\", proc_base, recursive=True, private=True)\n\n        os.chroot(root_dir)", "category": "Python"}, {"instruction": "def process_data(self, fname, tokenizer, max_size):\n        \"\"\"\n        Loads data from the input file.\n\n        :param fname: input file name\n        :param tokenizer: tokenizer\n        :param max_size: loads at most 'max_size' samples from the input file,\n            if None loads the entire dataset\n        \"\"\"\n", "input": "", "output": "        logging.info(f'Processing data from {fname}')\n        data = []\n        with open(fname) as dfile:\n            for idx, line in enumerate(dfile):\n                if max_size and idx == max_size:\n                    break\n                entry = tokenizer.segment(line)\n                entry = torch.tensor(entry)\n                data.append(entry)\n        return data", "category": "Python"}, {"instruction": "def intersection(self, key, *others):\n        \"\"\"Return a new set with elements common to the set and all others.\"\"\"\n", "input": "", "output": "        if not isinstance(key, str):\n            raise ValueError(\"String expected.\")\n        self.db.sinterstore(key, [self.key] + [o.key for o in others])\n        return Set(key)", "category": "Python"}, {"instruction": "def find(self, target, relation):\n        ''' returns back all elements the target has a relation to '''\n", "input": "", "output": "        query = 'select ob1.code from objects as ob1, objects as ob2, relations where relations.dst=ob1.id and relations.name=? and relations.src=ob2.id and ob2.code=?' # src is id not source :/\n        for i in self._execute(query, (relation, self.serialize(target))):\n            yield self.deserialize(i[0])", "category": "Python"}, {"instruction": "def scoreTagToString(self, scoreTag):\n        \"\"\"\n        :param scoreTag:\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        scoreTagToString = \"\"\n        if scoreTag == \"P+\":\n            scoreTagToString = 'strong positive'\n        elif scoreTag == \"P\":\n            scoreTagToString = 'positive'\n        elif scoreTag == \"NEU\":\n            scoreTagToString = 'neutral'\n        elif scoreTag == \"N\":\n            scoreTagToString = 'negative'\n        elif scoreTag == \"N+\":\n            scoreTagToString = 'strong negative'\n        elif scoreTag == \"NONE\":\n            scoreTagToString = 'no sentiment'\n\n        return scoreTagToString", "category": "Python"}, {"instruction": "def device_event_retrieve(self, device_event_id, **kwargs):  # noqa: E501\n        \"\"\"Retrieve a device event.  # noqa: E501\n\n        Retrieve a specific device event.  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass asynchronous=True\n        >>> thread = api.device_event_retrieve(device_event_id, asynchronous=True)\n        >>> result = thread.get()\n\n        :param asynchronous bool\n        :param str device_event_id: (required)\n        :return: DeviceEventData\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('asynchronous'):\n            return self.device_event_retrieve_with_http_info(device_event_id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.device_event_retrieve_with_http_info(device_event_id, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def next(self):\n        \"\"\"\n        fetch the chart identified by this chart's next_id attribute\n        if the next_id is either null or not present for this chart return None\n        returns the new chart instance on sucess\"\"\"\n", "input": "", "output": "        try:\n            if self.next_id:\n                return Chart(self.next_id)\n            else:\n                log.debug('attempted to get next chart, but none was found')\n                return\n        except AttributeError:\n            #chart does not implement next pointer\n            log.debug('attempted to get next chart from a chart without a next attribute')\n            return None", "category": "Python"}, {"instruction": "def cleaned_up_slab(self):\n        \"\"\"\n        Returns a slab with the adsorbates removed\n        \"\"\"\n", "input": "", "output": "        ads_strs = list(self.ads_entries_dict.keys())\n        cleaned = self.structure.copy()\n        cleaned.remove_species(ads_strs)\n        return cleaned", "category": "Python"}, {"instruction": "def new(cls, package, slide_part):\n        \"\"\"\n        Create and return a new notes slide part based on the notes master\n        and related to both the notes master part and *slide_part*. If no\n        notes master is present, create one based on the default template.\n        \"\"\"\n", "input": "", "output": "        notes_master_part = package.presentation_part.notes_master_part\n        notes_slide_part = cls._add_notes_slide_part(\n            package, slide_part, notes_master_part\n        )\n        notes_slide = notes_slide_part.notes_slide\n        notes_slide.clone_master_placeholders(notes_master_part.notes_master)\n        return notes_slide_part", "category": "Python"}, {"instruction": "def delete_discount_promotion_by_id(cls, discount_promotion_id, **kwargs):\n        \"\"\"Delete DiscountPromotion\n\n        Delete an instance of DiscountPromotion by its ID.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.delete_discount_promotion_by_id(discount_promotion_id, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str discount_promotion_id: ID of discountPromotion to delete. (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._delete_discount_promotion_by_id_with_http_info(discount_promotion_id, **kwargs)\n        else:\n            (data) = cls._delete_discount_promotion_by_id_with_http_info(discount_promotion_id, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def assume_role(role_arn, session_name=None, duration_seconds=None,\n                region='us-east-1', env_vars=None):\n    \"\"\"Assume IAM role.\"\"\"\n", "input": "", "output": "    if session_name is None:\n        session_name = 'runway'\n    assume_role_opts = {'RoleArn': role_arn,\n                        'RoleSessionName': session_name}\n    if duration_seconds:\n        assume_role_opts['DurationSeconds'] = int(duration_seconds)\n    boto_args = {}\n    if env_vars:\n        for i in ['aws_access_key_id', 'aws_secret_access_key',\n                  'aws_session_token']:\n            if env_vars.get(i.upper()):\n                boto_args[i] = env_vars[i.upper()]\n\n    sts_client = boto3.client('sts', region_name=region, **boto_args)\n    LOGGER.info(\"Assuming role %s...\", role_arn)\n    response = sts_client.assume_role(**assume_role_opts)\n    return {'AWS_ACCESS_KEY_ID': response['Credentials']['AccessKeyId'],\n            'AWS_SECRET_ACCESS_KEY': response['Credentials']['SecretAccessKey'],  # noqa\n            'AWS_SESSION_TOKEN': response['Credentials']['SessionToken']}", "category": "Python"}, {"instruction": "def geometrize_stops(stops: List[str], *, use_utm: bool = False) -> DataFrame:\n    \"\"\"\n    Given a stops DataFrame, convert it to a GeoPandas GeoDataFrame\n    and return the result.\n\n    Parameters\n    ----------\n    stops : DataFrame\n        A GTFS stops table\n    use_utm : boolean\n        If ``True``, then convert the output to local UTM coordinates;\n        otherwise use WGS84 coordinates\n\n    Returns\n    -------\n    GeoPandas GeoDataFrame\n        Looks like the given stops DataFrame, but has a ``'geometry'``\n        column of Shapely Point objects that replaces\n        the ``'stop_lon'`` and ``'stop_lat'`` columns.\n\n    Notes\n    -----\n    Requires GeoPandas.\n\n    \"\"\"\n", "input": "", "output": "    import geopandas as gpd\n\n    g = (\n        stops.assign(\n            geometry=lambda x: [\n                sg.Point(p) for p in x[[\"stop_lon\", \"stop_lat\"]].values\n            ]\n        )\n        .drop([\"stop_lon\", \"stop_lat\"], axis=1)\n        .pipe(lambda x: gpd.GeoDataFrame(x, crs=cs.WGS84))\n    )\n\n    if use_utm:\n        lat, lon = stops.loc[0, [\"stop_lat\", \"stop_lon\"]].values\n        crs = hp.get_utm_crs(lat, lon)\n        g = g.to_crs(crs)\n\n    return g", "category": "Python"}, {"instruction": "def get_error_messages(self):\n        \"\"\"\n        Method returning error messages. Should return list of messages.\n\n        By default it find element with class ``error`` and theirs value in\n        attribute ``error`` or text if that attribute is missing. You can change\n        this method accordingly to your app.\n\n        Error messages returned from this method are used in decorators\n        :py:func:`.expected_error_messages` and :py:func:`.allowed_error_messages`.\n        \"\"\"\n", "input": "", "output": "        try:\n            error_elms = self.get_elms(class_name='error')\n        except NoSuchElementException:\n            return []\n        else:\n            try:\n                error_values = [error_elm.get_attribute('error') for error_elm in error_elms]\n            except Exception:\n                error_values = [error_elm.text for error_elm in error_elms]\n            finally:\n                return error_values", "category": "Python"}, {"instruction": "def make_ring_filelist(self, sourcekeys, rings, galprop_run):\n        \"\"\" Make a list of all the template files for a merged component\n\n        Parameters\n        ----------\n\n        sourcekeys : list-like of str\n            The names of the componenents to merge\n        rings : list-like of int\n            The indices of the rings to merge\n        galprop_run : str\n            String identifying the galprop parameters\n        \"\"\"\n", "input": "", "output": "        flist = []\n        for sourcekey in sourcekeys:\n            for ring in rings:\n                flist += [self.make_ring_filename(sourcekey,\n                                                  ring, galprop_run)]\n        return flist", "category": "Python"}, {"instruction": "def walk_tree(start, attr):\n    \"\"\"\n    Recursively walk through a tree relationship. This iterates a tree in a top-down approach,\n    fully reaching the end of a lineage before moving onto the next sibling of that generation.\n    \"\"\"\n", "input": "", "output": "    path = [start]\n    for child in path:\n        yield child\n        idx = path.index(child)\n        for grandchild in reversed(getattr(child, attr)):\n            path.insert(idx + 1, grandchild)", "category": "Python"}, {"instruction": "def parse_args(argv=None):\n    \"\"\"Parse command line options.\"\"\"\n", "input": "", "output": "    parser = ArgumentParser()\n    parser.add_argument('--password', dest=\"password\", type=str)\n    parser.add_argument('--user', dest=\"user\", type=str)\n    parser.add_argument('--config-file', dest=\"config_file\", type=str)\n\n    options = parser.parse_args(argv)\n    return options", "category": "Python"}, {"instruction": "def checksum(value):\n    \"\"\"\n    Calculates the checksum char used for the 16th char.\n    Author: Vincenzo Palazzo\n    \"\"\"\n", "input": "", "output": "    return chr(65 + sum(CHECKSUM_TABLE[index % 2][ALPHANUMERICS_DICT[char]]\n                        for index, char in enumerate(value)) % 26)", "category": "Python"}, {"instruction": "def set_dhcp_only_all(interface):\n    '''\n    Configure specified adapter to use DHCP only\n\n    Change adapter mode to TCP/IP. If previous adapter mode was EtherCAT, the target will need reboot.\n\n    :param str interface: interface label\n    :return: True if the settings were applied, otherwise an exception will be thrown.\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.dhcp_only_all interface-label\n    '''\n", "input": "", "output": "    if not __grains__['lsb_distrib_id'] == 'nilrt':\n        raise salt.exceptions.CommandExecutionError('Not supported in this version')\n    initial_mode = _get_adapter_mode_info(interface)\n    _save_config(interface, 'Mode', 'TCPIP')\n    _save_config(interface, 'dhcpenabled', '1')\n    _save_config(interface, 'linklocalenabled', '0')\n    if initial_mode == 'ethercat':\n        __salt__['system.set_reboot_required_witnessed']()\n    else:\n        _restart(interface)\n    return True", "category": "Python"}, {"instruction": "def publish(idx=None):\n    \"\"\"Publish packaged distributions to pypi index\"\"\"\n", "input": "", "output": "    if idx is None:\n        idx = ''\n    else:\n        idx = '-r ' + idx\n    run('python setup.py register {}'.format(idx))\n    run('twine upload {} dist/*.whl dist/*.egg dist/*.tar.gz'.format(idx))", "category": "Python"}, {"instruction": "def normalise_correlation(image_tile_dict, transformed_array, template, normed_tolerance=1):\n    \"\"\"Calculates the normalisation coefficients of potential match positions\n       Then normalises the correlation at these positions, and returns them\n       if they do indeed constitute a match\n    \"\"\"\n", "input": "", "output": "    template_norm = np.linalg.norm(template)\n    image_norms = {(x,y):np.linalg.norm(image_tile_dict[(x,y)])*template_norm for (x,y) in image_tile_dict.keys()}\n    match_points = image_tile_dict.keys()\n    # for correlation, then need to transofrm back to get correct value for division\n    h, w = template.shape\n    #points_from_transformed_array = [(match[0] + h - 1, match[1] + w - 1) for match in match_points]\n    image_matches_normalised = {match_points[i]:transformed_array[match_points[i][0], match_points[i][1]]/image_norms[match_points[i]] for i in range(len(match_points))}\n    result = {key:value for key, value in image_matches_normalised.items() if np.round(value, decimals=3) >= normed_tolerance}\n    return result.keys()", "category": "Python"}, {"instruction": "def _login_request(self, url_login):\n        \"\"\"Internal function to send login request. \"\"\"\n", "input": "", "output": "\n        expiration_time = self._exp_time\n        payload = {'expirationTime': expiration_time}\n        # TODO(padkrish), after testing with certificates, make the\n        # verify option configurable.\n        res = requests.post(url_login,\n                            data=jsonutils.dumps(payload),\n                            headers=self._req_headers,\n                            auth=(self._user, self._pwd),\n                            timeout=self.timeout_resp, verify=False)\n        session_id = ''\n        if res and res.status_code in self._resp_ok:\n            session_id = res.json().get('Dcnm-Token')\n        self._req_headers.update({'Dcnm-Token': session_id})", "category": "Python"}, {"instruction": "def update_move_counts(self, start_game, end_game, interval=1000):\n        \"\"\"Used to update the move_count cell for older games.\n\n        Should not be needed except for backfill or repair.\n\n        move_count cells will be updated in both g_<game_id>_m_000 rows\n        and ct_<game_id>_<move_count> rows.\n        \"\"\"\n", "input": "", "output": "        for g in range(start_game, end_game, interval):\n            with tf.Session() as sess:\n                start_row = ROW_PREFIX.format(g)\n                end_row = ROW_PREFIX.format(g + interval)\n                print('Range:', start_row, end_row)\n                start_time = time.time()\n                ds = self.tf_table.keys_by_range_dataset(start_row, end_row)\n                h = _histogram_move_keys_by_game(sess, ds)\n                self._write_move_counts(sess, h)\n                end_time = time.time()\n                elapsed = end_time - start_time\n                print('  games/sec:', len(h)/elapsed)", "category": "Python"}, {"instruction": "def get_nb_strings(self):\n        \"\"\"\n        Return the total number of strings in all Analysis objects\n        \"\"\"\n", "input": "", "output": "        nb = 0\n        seen = []\n        for digest, dx in self.analyzed_vms.items():\n            if dx in seen:\n                continue\n            seen.append(dx)\n            nb += len(dx.get_strings_analysis())\n        return nb", "category": "Python"}, {"instruction": "def proximal(self):\n        \"\"\"Return the `proximal factory` of the functional.\"\"\"\n", "input": "", "output": "\n        domain = self.domain\n        diameter = self.diameter\n\n        class ProximalSimplex(Operator):\n            ", "category": "Python"}, {"instruction": "def sum(self, vector):\n        \"\"\"Return a Vector instance as the vector sum of two vectors.\"\"\"\n", "input": "", "output": "        return self.from_list(\n            [x + vector.vector[i] for i, x in self.to_list()]\n        )", "category": "Python"}, {"instruction": "def getCanonicalRep(record_cluster):\n    \"\"\"\n    Given a list of records within a duplicate cluster, constructs a\n    canonical representation of the cluster by finding canonical\n    values for each field\n\n    \"\"\"\n", "input": "", "output": "    canonical_rep = {}\n\n    keys = record_cluster[0].keys()\n\n    for key in keys:\n        key_values = []\n        for record in record_cluster:\n            # assume non-empty values always better than empty value\n            # for canonical record\n            if record[key]:\n                key_values.append(record[key])\n        if key_values:\n            canonical_rep[key] = getCentroid(key_values, comparator)\n        else:\n            canonical_rep[key] = ''\n\n    return canonical_rep", "category": "Python"}, {"instruction": "def return_ok(self, cookie, request):\n        \"\"\"\n        If you override .return_ok(), be sure to call this method.  If it\n        returns false, so should your subclass (assuming your subclass wants to\n        be more strict about which cookies to return).\n\n        \"\"\"\n", "input": "", "output": "        # Path has already been checked by .path_return_ok(), and domain\n        # blocking done by .domain_return_ok().\n        _debug(\" - checking cookie %s=%s\", cookie.name, cookie.value)\n\n        for n in \"version\", \"verifiability\", \"secure\", \"expires\", \"port\", \"domain\":\n            fn_name = \"return_ok_\"+n\n            fn = getattr(self, fn_name)\n            if not fn(cookie, request):\n                return False\n        return True", "category": "Python"}, {"instruction": "def subsample(self, down_to=1, new_path=None):\n        \"\"\"Pick a number of sequences from the file pseudo-randomly.\"\"\"\n", "input": "", "output": "        # Auto path #\n        if new_path is None: subsampled = self.__class__(new_temp_path())\n        elif isinstance(new_path, FASTA): subsampled = new_path\n        else:                subsampled = self.__class__(new_path)\n        # Check size #\n        if down_to > len(self):\n            message = \"Can't subsample %s down to %i. Only down to %i.\"\n            print Color.ylw + message % (self, down_to, len(self)) + Color.end\n            self.copy(new_path)\n            return\n        # Do it #\n        subsampled.create()\n        for seq in isubsample(self, down_to): subsampled.add_seq(seq)\n        subsampled.close()\n        # Did it work #\n        assert len(subsampled) == down_to\n        return subsampled", "category": "Python"}, {"instruction": "def generated_tag_data(tags):\n    \"\"\"Convert :obj:`dict` to S3 Tag list.\n\n    Args:\n        tags (dict): Dictonary of tag key and tag value passed.\n\n    Returns:\n        list: List of dictionaries.\n\n    \"\"\"\n", "input": "", "output": "    generated_tags = []\n    for key, value in tags.items():\n        generated_tags.append({\n            'Key': key,\n            'Value': value,\n        })\n    return generated_tags", "category": "Python"}, {"instruction": "def get_gutter_client(\n        alias='default',\n        cache=CLIENT_CACHE,\n        **kwargs\n):\n    \"\"\"\n    Creates gutter clients and memoizes them in a registry for future quick access.\n\n    Args:\n        alias (str or None): Name of the client. Used for caching.\n            If name is falsy then do not use the cache.\n        cache (dict): cache to store gutter managers in.\n        **kwargs: kwargs to be passed the Manger class.\n\n    Returns (Manager):\n        A gutter client.\n\n    \"\"\"\n", "input": "", "output": "    from gutter.client.models import Manager\n\n    if not alias:\n        return Manager(**kwargs)\n    elif alias not in cache:\n        cache[alias] = Manager(**kwargs)\n\n    return cache[alias]", "category": "Python"}, {"instruction": "def combineByKey(self, createCombiner, mergeValue, mergeCombiners,\n                     numPartitions=None):\n        \"\"\"\n        Return a new DStream by applying combineByKey to each RDD.\n        \"\"\"\n", "input": "", "output": "        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n\n        def func(rdd):\n            return rdd.combineByKey(createCombiner, mergeValue, mergeCombiners, numPartitions)\n        return self.transform(func)", "category": "Python"}, {"instruction": "def download_package_to_sandbox(sandbox, package_url):\n    \"\"\"\n    Downloads an unzips a package to the sandbox\n    :param sandbox: temporary directory name\n    :param package_url: link to package download\n    :returns: name of unzipped package directory\n    \"\"\"\n", "input": "", "output": "\n    response = requests.get(package_url)\n\n    package_tar = os.path.join(sandbox, 'package.tar.gz')\n\n    with open(package_tar, 'w') as f:\n        f.write(response.content)\n\n    os.chdir(sandbox)\n\n    with tarfile.open('package.tar.gz', 'r:gz') as tf:\n        tf.extractall()\n\n    directory = [d for d in os.listdir(sandbox) if os.path.isdir(d)][0]\n\n    return os.path.join(sandbox, directory)", "category": "Python"}, {"instruction": "def unused(self):\n        \"\"\"\n        Return unused user-created elements.\n        \n        :rtype: list(Element)\n        \"\"\"\n", "input": "", "output": "        self._params.update(\n            href=self._resource.get('search_unused'))\n        return self", "category": "Python"}, {"instruction": "def san_managers(self):\n        \"\"\"\n        Gets the SanManagers API client.\n\n        Returns:\n            SanManagers:\n        \"\"\"\n", "input": "", "output": "        if not self.__san_managers:\n            self.__san_managers = SanManagers(self.__connection)\n        return self.__san_managers", "category": "Python"}, {"instruction": "def get_image_file_hash(image_path):\n    '''get_image_hash will return an md5 hash of the file based on a criteria level.\n    :param level: one of LOW, MEDIUM, HIGH\n    :param image_path: full path to the singularity image\n    '''\n", "input": "", "output": "    hasher = hashlib.md5()\n    with open(image_path, \"rb\") as f:\n        for chunk in iter(lambda: f.read(4096), b\"\"):\n            hasher.update(chunk)\n    return hasher.hexdigest()", "category": "Python"}, {"instruction": "def whofaved_deviation(self, deviationid, offset=0, limit=10):\n\n        \"\"\"Fetch a list of users who faved the deviation\n\n        :param deviationid: The deviationid you want to fetch\n        :param offset: the pagination offset\n        :param limit: the pagination limit\n        \"\"\"\n", "input": "", "output": "\n        response = self._req('/deviation/whofaved', get_data={\n            'deviationid' : deviationid,\n            'offset' : offset,\n            'limit' : limit\n        })\n\n        users = []\n\n        for item in response['results']:\n            u = {}\n            u['user'] = User()\n            u['user'].from_dict(item['user'])\n            u['time'] = item['time']\n\n            users.append(u)\n\n        return {\n            \"results\" : users,\n            \"has_more\" : response['has_more'],\n            \"next_offset\" : response['next_offset']\n        }", "category": "Python"}, {"instruction": "def shakeshake2_indiv_grad(x1, x2, dy):\n  \"\"\"Overriding gradient for shake-shake of 2 tensors.\"\"\"\n", "input": "", "output": "  y = shakeshake2_py(x1, x2, individual=True)\n  dx = tf.gradients(ys=[y], xs=[x1, x2], grad_ys=[dy])\n  return dx", "category": "Python"}, {"instruction": "def pushd(cls, new_dir):\n        \"\"\"Change directory, and back to previous directory.\n\n        It behaves like \"pushd directory; something; popd\".\n        \"\"\"\n", "input": "", "output": "        previous_dir = os.getcwd()\n        try:\n            new_ab_dir = None\n            if os.path.isabs(new_dir):\n                new_ab_dir = new_dir\n            else:\n                new_ab_dir = os.path.join(previous_dir, new_dir)\n            # Use absolute path to show it on FileNotFoundError message.\n            cls.cd(new_ab_dir)\n            yield\n        finally:\n            cls.cd(previous_dir)", "category": "Python"}, {"instruction": "def get_mods(package):\n    \"\"\" List all loadable python modules in a directory\n\n    This function looks inside the specified directory for all files that look\n    like Python modules with a numeric prefix and returns them. It will omit\n    any duplicates and return file names without extension.\n\n    :param package: package object\n    :returns:       list of tuples containing filename without extension,\n                    major_version and minor_version\n    \"\"\"\n", "input": "", "output": "    pkgdir = package.__path__[0]\n    matches = filter(None, [PYMOD_RE.match(f) for f in os.listdir(pkgdir)])\n    parse_match = lambda groups: (groups[0], int(groups[1]), int(groups[2]))\n    return sorted(list(set([parse_match(m.groups()) for m in matches])),\n                  key=lambda x: (x[1], x[2]))", "category": "Python"}, {"instruction": "def fetch(self, order_id, data={}, **kwargs):\n        \"\"\"\"\n        Fetch Order for given Id\n\n        Args:\n            order_id : Id for which order object has to be retrieved\n\n        Returns:\n            Order dict for given order Id\n        \"\"\"\n", "input": "", "output": "        return super(Order, self).fetch(order_id, data, **kwargs)", "category": "Python"}, {"instruction": "def _VarUInt64ByteSizeNoTag(uint64):\n  \"\"\"Returns the number of bytes required to serialize a single varint\n  using boundary value comparisons. (unrolled loop optimization -WPierce)\n  uint64 must be unsigned.\n  \"\"\"\n", "input": "", "output": "  if uint64 <= 0x7f: return 1\n  if uint64 <= 0x3fff: return 2\n  if uint64 <= 0x1fffff: return 3\n  if uint64 <= 0xfffffff: return 4\n  if uint64 <= 0x7ffffffff: return 5\n  if uint64 <= 0x3ffffffffff: return 6\n  if uint64 <= 0x1ffffffffffff: return 7\n  if uint64 <= 0xffffffffffffff: return 8\n  if uint64 <= 0x7fffffffffffffff: return 9\n  if uint64 > UINT64_MAX:\n    raise message.EncodeError('Value out of range: %d' % uint64)\n  return 10", "category": "Python"}, {"instruction": "def query_random(**kwargs):\n        '''\n        Return the random records of centain kind.\n        '''\n", "input": "", "output": "\n        if 'limit' in kwargs:\n            limit = kwargs['limit']\n        elif 'num' in kwargs:\n            limit = kwargs['num']\n        else:\n            limit = 10\n\n        kind = kwargs.get('kind', None)\n\n        if kind:\n            rand_recs = TabPost.select().where(\n                (TabPost.kind == kind) &\n                (TabPost.valid == 1)\n            ).order_by(\n                peewee.fn.Random()\n            ).limit(limit)\n        else:\n            rand_recs = TabPost.select().where(\n                TabPost.valid == 1\n            ).order_by(\n                peewee.fn.Random()\n            ).limit(limit)\n        return rand_recs", "category": "Python"}, {"instruction": "def read_corpus(file_name):\n    \"\"\"\n    Read and return the data from a corpus json file.\n    \"\"\"\n", "input": "", "output": "    with io.open(file_name, encoding='utf-8') as data_file:\n        return yaml.load(data_file)", "category": "Python"}, {"instruction": "def state_not_literal(self, value):\n        \"\"\"Parse not literal.\"\"\"\n", "input": "", "output": "        value = negate = chr(value)\n        while value == negate:\n            value = choice(self.literals)\n        yield value", "category": "Python"}, {"instruction": "def __create(self, short_description, period, **kwargs):\n        \"\"\"Call documentation: `/preapproval/create\n        <https://www.wepay.com/developer/reference/preapproval#create>`_, plus\n        extra keyword parameters:\n\n        :keyword str access_token: will be used instead of instance's\n           ``access_token``, with ``batch_mode=True`` will set `authorization`\n           param to it's value.\n\n        :keyword bool batch_mode: turn on/off the batch_mode, see\n           :class:`wepay.api.WePay`\n\n        :keyword str batch_reference_id: `reference_id` param for batch call,\n           see :class:`wepay.api.WePay`\n\n        :keyword str api_version: WePay API version, see\n           :class:`wepay.api.WePay`\n\n        \"\"\"\n", "input": "", "output": "        params = {\n            'short_description': short_description,\n            'period': period\n        }\n        return self.make_call(self.__create, params, kwargs)", "category": "Python"}, {"instruction": "def abort_submission(namespace, workspace, submission_id):\n    \"\"\"Abort running job in a workspace.\n\n    Args:\n        namespace (str): project to which workspace belongs\n        workspace (str): Workspace name\n        submission_id (str): Submission's unique identifier\n\n    Swagger:\n        https://api.firecloud.org/#!/Submissions/deleteSubmission\n    \"\"\"\n", "input": "", "output": "    uri = \"workspaces/{0}/{1}/submissions/{2}\".format(namespace,\n                                        workspace, submission_id)\n    return __delete(uri)", "category": "Python"}, {"instruction": "def add_sam2rnf_parser(subparsers, subcommand, help, description, simulator_name=None):\n    \"\"\"Add another parser for a SAM2RNF-like command.\n\n\tArgs:\n\t\tsubparsers (subparsers): File name of the genome from which read tuples are created (FASTA file).\n\t\tsimulator_name (str): Name of the simulator used in comments.\n\t\"\"\"\n", "input": "", "output": "\n    parser_sam2rnf = subparsers.add_parser(subcommand, help=help, description=description)\n\n    parser_sam2rnf.set_defaults(func=sam2rnf)\n\n    parser_sam2rnf.add_argument(\n        '-s', '--sam', type=str, metavar='file', dest='sam_fn', required=True,\n        help='Input SAM/BAM with true (expected) alignments of the reads  (- for standard input).'\n    )\n\n    _add_shared_params(parser_sam2rnf, unmapped_switcher=True)\n\n    parser_sam2rnf.add_argument(\n        '-n',\n        '--simulator-name',\n        type=str,\n        metavar='str',\n        dest='simulator_name',\n        default=simulator_name,\n        help='Name of the simulator (for RNF).' if simulator_name is not None else argparse.SUPPRESS,\n    )", "category": "Python"}, {"instruction": "def assign_qualification(self, qualification_id, worker_id, score, notify=False):\n        \"\"\"Score a worker for a specific qualification\"\"\"\n", "input": "", "output": "        return self._is_ok(\n            self.mturk.associate_qualification_with_worker(\n                QualificationTypeId=qualification_id,\n                WorkerId=worker_id,\n                IntegerValue=score,\n                SendNotification=notify,\n            )\n        )", "category": "Python"}, {"instruction": "def StopHuntIfCrashLimitExceeded(hunt_id):\n  \"\"\"Stops the hunt if number of crashes exceeds the limit.\"\"\"\n", "input": "", "output": "  hunt_obj = data_store.REL_DB.ReadHuntObject(hunt_id)\n\n  # Do nothing if the hunt is already stopped.\n  if hunt_obj.hunt_state == rdf_hunt_objects.Hunt.HuntState.STOPPED:\n    return hunt_obj\n\n  if hunt_obj.crash_limit:\n    hunt_counters = data_store.REL_DB.ReadHuntCounters(hunt_id)\n    if hunt_counters.num_crashed_clients >= hunt_obj.crash_limit:\n      # Remove our rules from the forman and cancel all the started flows.\n      # Hunt will be hard-stopped and it will be impossible to restart it.\n      reason = (\"Hunt %s reached the crashes limit of %d \"\n                \"and was stopped.\") % (hunt_obj.hunt_id, hunt_obj.crash_limit)\n      StopHunt(hunt_obj.hunt_id, reason=reason)\n\n  return hunt_obj", "category": "Python"}, {"instruction": "def get_operator_index(self, name):\n        \"\"\"|coro|\n\n        Gets the operators index from the operator definitions dict\n\n        Returns\n        -------\n        str\n            the operator index\"\"\"\n", "input": "", "output": "        opdefs = yield from self.get_operator_definitions()\n\n        name = name.lower()\n        if name not in opdefs:\n            return None\n\n        return opdefs[name][\"index\"]", "category": "Python"}, {"instruction": "def getPaths(urlOrPaths):\n    '''\n    Determines if the given URL in urlOrPaths is a URL or a file or directory. If it's\n    a directory, it walks the directory and then finds all file paths in it, and ads them\n    too. If it's a file, it adds it to the paths. If it's a URL it just adds it to the path.\n    :param urlOrPaths: the url or path to be scanned\n    :return: ``list`` of paths\n    '''\n", "input": "", "output": "    if isinstance(urlOrPaths, basestring):\n        #FIXME: basestring is undefined\n        urlOrPaths = [urlOrPaths]  # do not recursively walk over letters of a single path which can include \"/\"\n    paths = []\n    for eachUrlOrPaths in urlOrPaths:\n        if os.path.isdir(eachUrlOrPaths):\n            for root, directories, filenames in walk(eachUrlOrPaths):\n                for filename in filenames:\n                    paths.append(os.path.join(root,filename))\n        else:\n            paths.append(eachUrlOrPaths)\n    return paths", "category": "Python"}, {"instruction": "def _flushBuffer(self, request):\n        '''Flush any pending data from the buffer to the request'''\n", "input": "", "output": "        assert request is self.requestSession.request\n        self.requestSession.writeData(self.buffer)\n        self.buffer = []", "category": "Python"}, {"instruction": "def cmdline(argv=sys.argv[1:]):\n    \"\"\"\n    Script for merging different collections of stop words.\n    \"\"\"\n", "input": "", "output": "    parser = ArgumentParser(\n        description='Create and merge collections of stop words')\n    parser.add_argument(\n        'language', help='The language used in the collection')\n    parser.add_argument('sources', metavar='FILE', nargs='+',\n                        help='Source files to parse')\n    options = parser.parse_args(argv)\n\n    factory = StopWordFactory()\n    language = options.language\n    stop_words = factory.get_stop_words(language, fail_safe=True)\n\n    for filename in options.sources:\n        stop_words += StopWord(language, factory.read_collection(filename))\n\n    filename = factory.get_collection_filename(stop_words.language)\n    factory.write_collection(filename, stop_words.collection)", "category": "Python"}, {"instruction": "def get_folding_model_data(\n        nstep, rvec0=np.zeros((5)), nskip=1, dt=0.01, kT=10.0, mass=1.0, damping=1.0, rcut=3.0):\n    r\"\"\"wrapper for the folding model generator\"\"\"\n", "input": "", "output": "    fm = FoldingModel(dt, kT, mass=mass, damping=damping, rcut=rcut)\n    return fm.sample(rvec0, nstep, nskip=nskip)", "category": "Python"}, {"instruction": "def get_role_model():\n    \"\"\"\n    Returns the Role model that is active in this project.\n    \"\"\"\n", "input": "", "output": "    app_model = getattr(settings, \"ARCTIC_ROLE_MODEL\", \"arctic.Role\")\n\n    try:\n        return django_apps.get_model(app_model)\n    except ValueError:\n        raise ImproperlyConfigured(\n            \"ARCTIC_ROLE_MODEL must be of the \" \"form 'app_label.model_name'\"\n        )\n    except LookupError:\n        raise ImproperlyConfigured(\n            \"ARCTIC_ROLE_MODEL refers to model '%s' that has not been \"\n            \"installed\" % settings.ARCTIC_ROLE_MODEL\n        )", "category": "Python"}, {"instruction": "def functions(self):\n        \"\"\"\n        A list of functions declared or defined in this module.\n        \"\"\"\n", "input": "", "output": "        return [v for v in self.globals.values()\n                if isinstance(v, values.Function)]", "category": "Python"}, {"instruction": "def banner(cls, content_=\"Well Come\"):\n        \"\"\"\u751f\u6210\u53603\u884c\u7684\u5b57\u7b26\u4e32\"\"\"\n", "input": "", "output": "        # char def\n        sp_char = \"#\"\n        # length calc\n        itsays = content_.strip()\n        effective_length = int(len(itsays))\n        # gen contents\n        side_space = ' ' * int(effective_length * ((1 - 0.618) / 0.618) / 2)\n        content_line = sp_char + side_space + itsays + side_space + sp_char\n        content_line_length = len(content_line)\n        banner_border = sp_char * content_line_length\n        return banner_border + '\\n' + content_line + '\\n' + banner_border", "category": "Python"}, {"instruction": "def verify_security_data(security):\n    \"\"\"Verify an untrusted security token.\n\n    :param security: security token\n    :type security: dict\n    :return: True if valid\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "    random_token = security[TOKEN_RANDOM]\n    hashed_token = security[TOKEN_HASHED]\n    return str(hashed_token) == str(compute_token(random_token))", "category": "Python"}, {"instruction": "def normpdf(x, mu, sigma):\n    \"\"\"\n    Describes the relative likelihood that a real-valued random variable X will\n    take on a given value.\n    \n    http://en.wikipedia.org/wiki/Probability_density_function\n    \"\"\"\n", "input": "", "output": "    u = (x-mu)/abs(sigma)\n    y = (1/(math.sqrt(2*pi)*abs(sigma)))*math.exp(-u*u/2)\n    return y", "category": "Python"}, {"instruction": "def legal_date(year, month, day):\n    '''Check if this is a legal date in the Julian calendar'''\n", "input": "", "output": "    daysinmonth = month_length(year, month)\n\n    if not (0 < day <= daysinmonth):\n        raise ValueError(\"Month {} doesn't have a day {}\".format(month, day))\n\n    return True", "category": "Python"}, {"instruction": "def load_model_by_id(self, model_id):\n        \"\"\"Get the model by model_id\n\n        Parameters\n        ----------\n        model_id : int\n            model index\n        \n        Returns\n        -------\n        load_model : Graph\n            the model graph representation\n        \"\"\"\n", "input": "", "output": "\n        with open(os.path.join(self.path, str(model_id) + \".json\")) as fin:\n            json_str = fin.read().replace(\"\\n\", \"\")\n\n        load_model = json_to_graph(json_str)\n        return load_model", "category": "Python"}, {"instruction": "def bytes_dict_cast(dict_, include_keys=True, include_vals=True, **kwargs):\n    \"\"\"\n    Converts any string-like items in input dict to bytes-like values, with\n    respect to python version\n\n    Parameters\n    ----------\n    dict_ : dict\n        any string-like objects contained in the dict will be converted to bytes\n    include_keys : bool, default=True\n        if True, cast keys to bytes, else ignore\n    include_values : bool, default=True\n        if True, cast values to bytes, else ignore\n    kwargs:\n        encoding: str, default: 'utf-8'\n            encoding to be used when encoding string\n    \"\"\"\n", "input": "", "output": "    new_keys = bytes_list_cast(dict_.keys(), **kwargs) if include_keys else dict_.keys()\n    new_vals = bytes_list_cast(dict_.values(), **kwargs) if include_vals else dict_.values()\n    new_dict = dict(zip_(new_keys, new_vals))\n    return new_dict", "category": "Python"}, {"instruction": "def raw(cls, exp, files=None):\n        \"\"\"\n        :param str|unicode exp: Haskell expression to evaluate.\n        :param dict[str|unicode, str|unicode] files: Dictionary of file names->contents\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        payload = {'exp': exp}\n        if files:\n            # TODO: Implement stdin.\n            stdin = []\n            payload['args'] = json.dumps([stdin, files])\n        try:\n            return requests.get('http://tryhaskell.org/eval', params=payload).json()\n        except ValueError as e:\n            raise cls.Error(e)", "category": "Python"}, {"instruction": "def _realPath(self, newPathName: str = None) -> str:\n        \"\"\" Private Real Path\n\n        Get path name.\n\n        @param newPathName: variable for new path name if passed argument.\n        @type newPathName: String\n        @return: Path Name as string.\n        \"\"\"\n", "input": "", "output": "\n        directory = self._directory()\n        assert directory\n        return os.path.join(directory.path,\n                            newPathName if newPathName else self._pathName)", "category": "Python"}, {"instruction": "def destroy_vm_by_name(self, si, vm_name, vm_path, logger):\n        \"\"\" \n        destroy the given vm  \n        :param si:      pyvmomi 'ServiceInstance'\n        :param vm_name: str name of the vm to destroyed\n        :param vm_path: str path to the vm that will be destroyed\n        :param logger:\n        \"\"\"\n", "input": "", "output": "        if vm_name is not None:\n            vm = self.find_vm_by_name(si, vm_path, vm_name)\n            if vm:\n                return self.destroy_vm(vm, logger)\n        raise ValueError('vm not found')", "category": "Python"}, {"instruction": "def nacm_log_if_default_permit(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        nacm = ET.SubElement(config, \"nacm\", xmlns=\"urn:ietf:params:xml:ns:yang:ietf-netconf-acm\")\n        log_if_default_permit = ET.SubElement(nacm, \"log-if-default-permit\", xmlns=\"http://tail-f.com/yang/acm\")\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def by_owner(cls, session, owner_name):\n        \"\"\"\n        Get packages from a given owner username.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param owner_name: owner username\n        :type owner_name: unicode\n\n        :return: package instances\n        :rtype: generator of :class:`pyshop.models.Package`\n        \"\"\"\n", "input": "", "output": "        return cls.find(session,\n                        join=(cls.owners),\n                        where=(User.login == owner_name,),\n                        order_by=cls.name)", "category": "Python"}, {"instruction": "def get(self, filetype, **kwargs):\n        \"\"\"Returns file name, downloading if remote access configured.\n\n        Parameters\n        ----------\n        filetype : str\n            type of file\n\n        keyword arguments :\n            keywords to fully specify path\n\n        Notes\n        -----\n        Path templates are defined in $DIMAGE_DIR/data/dimage_paths.ini\n        \"\"\"\n", "input": "", "output": "\n        path = self.full(filetype, **kwargs)\n\n        if path:\n            if self._remote:\n                self.download_url_to_path(self.url(filetype, **kwargs), path)\n        else:\n            print(\"There is no file with filetype=%r to access in the tree module loaded\" % filetype)", "category": "Python"}, {"instruction": "def create_mysql_pymysql(self, **kwargs):\n        \"\"\"\n        :rtype: Engine\n        \"\"\"\n", "input": "", "output": "        return self._ce(\n            self._ccs(self.DialectAndDriver.mysql_pymysql), **kwargs\n        )", "category": "Python"}, {"instruction": "def reindex_to(self, x, attribute=\"Name\"):\n        \"\"\"\n        Returns a copy that only has rows corresponding to feature names in x.\n\n        Parameters\n        ----------\n        x : str or pybedtools.BedTool\n            BED, GFF, GTF, or VCF where the \"Name\" field (that is, the value\n            returned by feature['Name']) or any arbitrary attribute\n\n        attribute : str\n            Attribute containing the name of the feature to use as the index.\n        \"\"\"\n", "input": "", "output": "        names = [i[attribute] for i in x]\n        new = self.copy()\n        new.data = new.data.reindex(names)\n        return new", "category": "Python"}, {"instruction": "def task(name, deps = None, fn = None):\n\t\"\"\"Define a new task.\"\"\"\n", "input": "", "output": "\tif callable(deps):\n\t\tfn = deps\n\t\tdeps = None\n\n\tif not deps and not fn:\n\t\tlogger.log(logger.red(\"The task '%s' is empty\" % name))\n\telse:\n\t\ttasks[name] = [fn, deps]", "category": "Python"}, {"instruction": "def verify(self, chksum):\n        \"\"\"\n        verify: 1 byte -> boolean\n\n        verify checksums the frame, adds the expected checksum, and\n        determines whether the result is correct. The result should\n        be 0xFF.\n        \"\"\"\n", "input": "", "output": "        total = 0\n\n        # Add together all bytes\n        for byte in self.data:\n            total += byteToInt(byte)\n\n        # Add checksum too\n        total += byteToInt(chksum)\n\n        # Only keep low bits\n        total &= 0xFF\n\n        # Check result\n        return total == 0xFF", "category": "Python"}, {"instruction": "def configure_network(ip, netmask, gateway):\n    '''\n    Configure Network Interface\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ilo.configure_network [IP ADDRESS] [NETMASK] [GATEWAY]\n    '''\n    current = network()\n\n    # Check to see if the network is already configured\n    if (ip in current['Network Settings']['IP_ADDRESS']['VALUE'] and\n            netmask in current['Network Settings']['SUBNET_MASK']['VALUE'] and\n            gateway in current['Network Settings']['GATEWAY_IP_ADDRESS']['VALUE']):\n        return True\n\n    _xml = \"\"\"<RIBCL VERSION=\"2.0\">\n                <LOGIN USER_LOGIN=\"adminname\" PASSWORD=\"password\">\n                  <RIB_INFO MODE=\"write\">\n                    <MOD_NETWORK_SETTINGS>\n                      <IP_ADDRESS value=\"{0}\"/>\n                      <SUBNET_MASK value=\"{1}\"/>\n                      <GATEWAY_IP_ADDRESS value=\"{2}\"/>\n                    </MOD_NETWORK_SETTINGS>\n                  </RIB_INFO>\n                </LOGIN>\n              </RIBCL> \"\"\"\n", "input": "", "output": "\n    return __execute_cmd('Configure_Network', _xml)", "category": "Python"}, {"instruction": "def _create_deserializer(self) -> JsonObjectDeserializer:\n        \"\"\"\n        Creates a deserializer that is to be used by this decoder.\n        :return: the deserializer\n        \"\"\"\n", "input": "", "output": "        if self._deserializer_cache is None:\n            deserializer_cls = type(\n                \"%sInternalDeserializer\" % type(self),\n                (JsonObjectDeserializer,),\n                {\n                    \"_JSON_ENCODER_ARGS\": self._args,\n                    \"_JSON_ENCODER_KWARGS\": self._kwargs\n                }\n            )\n            self._deserializer_cache = deserializer_cls(self._get_property_mappings(), self._get_deserializable_cls())\n        return self._deserializer_cache", "category": "Python"}, {"instruction": "def get_internal_queue(self, thread_id):\n        \"\"\" returns internal command queue for a given thread.\n        if new queue is created, notify the RDB about it \"\"\"\n", "input": "", "output": "        if thread_id.startswith('__frame__'):\n            thread_id = thread_id[thread_id.rfind('|') + 1:]\n        return self._cmd_queue[thread_id]", "category": "Python"}, {"instruction": "def connect(signal, receiver):\n    \"\"\"Register `receiver` method/function as a receiver for the `signal`.\n\n    When the signal is emitted, this receiver will be invoked along with\n    all other associated signals.\n\n    Args:\n        signal: A signal identifier (e.g., a signal name)\n        receiver: A callable object to connect to the signal.\n    \"\"\"\n", "input": "", "output": "    __check_receiver(receiver)\n\n    if __is_bound_method(receiver):\n        ref = WeakMethod\n    else:\n        ref = weakref.ref\n\n    with __lock:\n        __purge()\n        __receivers[signal].append(ref(receiver))", "category": "Python"}, {"instruction": "def p(value, bits=None, endian=None, target=None):\n    \"\"\"\n    Pack a signed pointer for a given target.\n\n    Args:\n        value(int): The value to pack.\n        bits(:class:`pwnypack.target.Target.Bits`): Override the default\n            word size. If ``None`` it will look at the word size of\n            ``target``.\n        endian(:class:`~pwnypack.target.Target.Endian`): Override the default\n            byte order. If ``None``, it will look at the byte order of\n            the ``target`` argument.\n        target(:class:`~pwnypack.target.Target`): Override the default byte\n            order. If ``None``, it will look at the byte order of\n            the global :data:`~pwnypack.target.target`.\n    \"\"\"\n", "input": "", "output": "\n    return globals()['p%d' % _get_bits(bits, target)](value, endian=endian, target=target)", "category": "Python"}, {"instruction": "def findMax(arr):\n    \"\"\"\n    in comparison to argrelmax() more simple and  reliable peak finder\n    \"\"\"\n", "input": "", "output": "    out = np.zeros(shape=arr.shape, dtype=bool)\n    _calcMax(arr, out)\n    return out", "category": "Python"}, {"instruction": "def distinct(self, key=lambda x: x):\n        \"\"\"\n        Returns enumerable containing elements that are distinct based on\n        given key selector\n        :param key: key selector as lambda expression\n        :return: new Enumerable object\n        \"\"\"\n", "input": "", "output": "        return Enumerable(self.group_by(key=key).select(lambda g: g.first()).to_list())", "category": "Python"}, {"instruction": "def find_trivial_constructor(type_):\n    \"\"\"\n    Returns reference to trivial constructor.\n\n    Args:\n        type_ (declarations.class_t): the class to be searched.\n\n    Returns:\n        declarations.constructor_t: the trivial constructor\n\n    \"\"\"\n", "input": "", "output": "    assert isinstance(type_, class_declaration.class_t)\n\n    trivial = type_.constructors(\n        lambda x: is_trivial_constructor(x),\n        recursive=False,\n        allow_empty=True)\n    if trivial:\n        return trivial[0]\n\n    return None", "category": "Python"}, {"instruction": "def publish_and_get_event(self, resource):\n        \"\"\"Publish and get the event from base station.\"\"\"\n", "input": "", "output": "        l_subscribed = False\n        this_event = None\n\n        if not self.__subscribed:\n            self._get_event_stream()\n            self._subscribe_myself()\n            l_subscribed = True\n\n        status = self.publish(\n            action='get',\n            resource=resource,\n            mode=None,\n            publish_response=False)\n\n        if status == 'success':\n            i = 0\n            while not this_event and i < 2:\n                self.__event_handle.wait(5.0)\n                self.__event_handle.clear()\n                _LOGGER.debug(\"Instance %s resource: %s\", str(i), resource)\n                for event in self.__events:\n                    if event['resource'] == resource:\n                        this_event = event\n                        self.__events.remove(event)\n                        break\n                i = i + 1\n\n        if l_subscribed:\n            self._unsubscribe_myself()\n            self._close_event_stream()\n            l_subscribed = False\n\n        return this_event", "category": "Python"}, {"instruction": "def urban_lookup(word):\n\t'''\n\tReturn a Urban Dictionary definition for a word or None if no result was\n\tfound.\n\t'''\n", "input": "", "output": "\turl = \"http://api.urbandictionary.com/v0/define\"\n\tparams = dict(term=word)\n\tresp = requests.get(url, params=params)\n\tresp.raise_for_status()\n\tres = resp.json()\n\tif not res['list']:\n\t\treturn\n\treturn res['list'][0]['definition']", "category": "Python"}, {"instruction": "def run(): # pragma: no cover\n    \"\"\"Defines how to start the CLI for the DomainTools API\"\"\"\n", "input": "", "output": "    out_file, out_format, arguments = parse()\n    user, key = arguments.pop('user', None), arguments.pop('key', None)\n    if not user or not key:\n        sys.stderr.write('Credentials are required to perform API calls.\\n')\n        sys.exit(1)\n\n    api = API(user, key, https=arguments.pop('https'), verify_ssl=arguments.pop('verify_ssl'),\n              rate_limit=arguments.pop('rate_limit'))\n    response = getattr(api, arguments.pop('api_call'))(**arguments)\n    output = str(getattr(response, out_format) if out_format != 'list' else response.as_list())\n    out_file.write(output if output.endswith('\\n') else output + '\\n')", "category": "Python"}, {"instruction": "def get_property(self, name):\n        \"\"\"Return a named property for a resource, if available. Will raise an `AttributeError` if the property\n        does not exist\n\n        Args:\n            name (str): Name of the property to return\n\n        Returns:\n            `ResourceProperty`\n        \"\"\"\n", "input": "", "output": "        for prop in self.resource.properties:\n            if prop.name == name:\n                return prop\n\n        raise AttributeError(name)", "category": "Python"}, {"instruction": "def get_events_in_both_arrays(events_one, events_two):\n    \"\"\"\n    Calculates the events that exist in both arrays.\n\n    \"\"\"\n", "input": "", "output": "    events_one = np.ascontiguousarray(events_one)  # change memory alignement for c++ library\n    events_two = np.ascontiguousarray(events_two)  # change memory alignement for c++ library\n    event_result = np.empty_like(events_one)\n    count = analysis_functions.get_events_in_both_arrays(events_one, events_two, event_result)\n    return event_result[:count]", "category": "Python"}, {"instruction": "def Y_wider(self):\n        \"\"\"Increase the distance of the lines.\"\"\"\n", "input": "", "output": "        self.parent.value('y_distance', self.parent.value('y_distance') * 1.4)\n        self.parent.traces.display()", "category": "Python"}, {"instruction": "def drdlat(r, lon, lat):\n    \"\"\"\n    Compute the Jacobian of the transformation from latitudinal to\n    rectangular coordinates.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/drdlat_c.html\n\n    :param r: Distance of a point from the origin.\n    :type r: float\n    :param lon: Angle of the point from the XZ plane in radians.\n    :type lon: float\n    :param lat: Angle of the point from the XY plane in radians.\n    :type lat: float\n    :return: Matrix of partial derivatives.\n    :rtype: 3x3-Element Array of floats\n    \"\"\"\n", "input": "", "output": "    r = ctypes.c_double(r)\n    lon = ctypes.c_double(lon)\n    lat = ctypes.c_double(lat)\n    jacobi = stypes.emptyDoubleMatrix()\n    libspice.drdlat_c(r, lon, lat, jacobi)\n    return stypes.cMatrixToNumpy(jacobi)", "category": "Python"}, {"instruction": "def run(command, cwd=None, shell=False, raiseOnError=False):\n\t\t\"\"\"\n\t\tExecutes a child process and waits for it to complete\n\t\t\"\"\"\n", "input": "", "output": "\t\treturncode = subprocess.call(command, cwd=cwd, shell=shell)\n\t\tif raiseOnError == True and returncode != 0:\n\t\t\traise Exception('child process ' + str(command) + ' failed with exit code ' + str(returncode))\n\t\treturn returncode", "category": "Python"}, {"instruction": "def refactor_froms_to_imports(self, offset):\n        \"\"\"Converting imports of the form \"from ...\" to \"import ...\".\"\"\"\n", "input": "", "output": "        refactor = ImportOrganizer(self.project)\n        changes = refactor.froms_to_imports(self.resource, offset)\n        return translate_changes(changes)", "category": "Python"}, {"instruction": "def max_langevin_fixed_point(x, r, m):\n    \"\"\"\n    Largest fixed point of dynamics  :math:argmax_x {h(x)=0}` estimated from polynomial :math:`h(x)`,\n    which has been fitted to the deterministic dynamics of Langevin model\n\n    .. math::\n        \\dot(x)(t) = h(x(t)) + R \\mathcal(N)(0,1)\n\n    as described by\n\n        Friedrich et al. (2000): Physics Letters A 271, p. 217-222\n        *Extracting model equations from experimental data*\n\n    For short time-series this method is highly dependent on the parameters.\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :param m: order of polynom to fit for estimating fixed points of dynamics\n    :type m: int\n    :param r: number of quantils to use for averaging\n    :type r: float\n\n    :return: Largest fixed point of deterministic dynamics\n    :return type: float\n    \"\"\"\n", "input": "", "output": "\n    coeff = _estimate_friedrich_coefficients(x, m, r)\n\n    try:\n        max_fixed_point = np.max(np.real(np.roots(coeff)))\n    except (np.linalg.LinAlgError, ValueError):\n        return np.nan\n\n    return max_fixed_point", "category": "Python"}, {"instruction": "def parse(self):\n        \"\"\" parse data \"\"\"\n", "input": "", "output": "        super(GeoRss, self).parse()\n\n        # support RSS and ATOM\n        tag_name = 'item' if '<item>' in self.data else 'entry'\n\n        self.parsed_data = self.parsed_data.getElementsByTagName(tag_name)", "category": "Python"}, {"instruction": "def _read_legacy_10x_mtx(path, var_names='gene_symbols', make_unique=True, cache=False):\n    \"\"\"\n    Read mex from output from Cell Ranger v2 or earlier versions\n    \"\"\"\n", "input": "", "output": "    path = Path(path)\n    adata = read(path / 'matrix.mtx', cache=cache).T  # transpose the data\n    genes = pd.read_csv(path / 'genes.tsv', header=None, sep='\\t')\n    if var_names == 'gene_symbols':\n        var_names = genes[1]\n        if make_unique:\n            var_names = anndata.utils.make_index_unique(pd.Index(var_names))\n        adata.var_names = var_names\n        adata.var['gene_ids'] = genes[0].values\n    elif var_names == 'gene_ids':\n        adata.var_names = genes[0]\n        adata.var['gene_symbols'] = genes[1].values\n    else:\n        raise ValueError('`var_names` needs to be \\'gene_symbols\\' or \\'gene_ids\\'')\n    adata.obs_names = pd.read_csv(path / 'barcodes.tsv', header=None)[0]\n    return adata", "category": "Python"}, {"instruction": "def get_userinfo(self):\r\n        \"\"\"Method to get current user's name, mobile, email and position.\"\"\"\n", "input": "", "output": "        wanted_fields = [\"name\", \"mobile\", \"orgEmail\", \"position\", \"avatar\"]\r\n        userinfo = {k: self.json_response.get(k, None) for k in wanted_fields}\r\n        return userinfo", "category": "Python"}, {"instruction": "def sun_zenith_angle(utc_time, lon, lat):\n    \"\"\"Sun-zenith angle for *lon*, *lat* at *utc_time*.\n    lon,lat in degrees.\n    The angle returned is given in degrees\n    \"\"\"\n", "input": "", "output": "    return np.rad2deg(np.arccos(cos_zen(utc_time, lon, lat)))", "category": "Python"}, {"instruction": "def get_deployment_id(deployment_name,\n                      token_manager=None,\n                      app_url=defaults.APP_URL):\n    \"\"\"\n    return the deployment id for the deployment with the specified name\n\n    \"\"\"\n", "input": "", "output": "\n    headers = token_manager.get_access_token_headers()\n    deployment_url = environment.get_deployment_url(app_url=app_url)\n    response = requests.get('%s/api/v1/deployments' % deployment_url,\n                            headers=headers)\n\n    if response.status_code == 200:\n        deployments = response.json()\n\n        for deployment in deployments:\n            if deployment['name'] == deployment_name:\n                return deployment['deployment_id']\n\n        raise JutException('Unable to find deployment with name %s' % deployment_name)\n    else:\n        raise JutException('Error %s: %s' % (response.status_code, response.text))", "category": "Python"}, {"instruction": "def get_probs(data, prob_column=None):\n    \"\"\"\n    Checks for presence of a probability column and returns the result\n    as a numpy array. If the probabilities are weights (i.e. they don't\n    sum to 1), then this will be recalculated.\n\n    Parameters\n    ----------\n    data: pandas.DataFrame\n        Table to sample from.\n    prob_column: string, optional, default None\n        Name of the column in the data to provide probabilities or weights.\n\n    Returns\n    -------\n    numpy.array\n\n    \"\"\"\n", "input": "", "output": "    if prob_column is None:\n        p = None\n    else:\n        p = data[prob_column].fillna(0).values\n        if p.sum() == 0:\n            p = np.ones(len(p))\n        if abs(p.sum() - 1.0) > 1e-8:\n            p = p / (1.0 * p.sum())\n    return p", "category": "Python"}, {"instruction": "def _astype(self, dtype):\n        \"\"\"Internal helper for `astype`.\n\n        Subclasses with differing init parameters should overload this\n        method.\n        \"\"\"\n", "input": "", "output": "        kwargs = {}\n        if is_floating_dtype(dtype):\n            # Use weighting only for floating-point types, otherwise, e.g.,\n            # `space.astype(bool)` would fail\n            weighting = getattr(self, 'weighting', None)\n            if weighting is not None:\n                kwargs['weighting'] = weighting\n\n        return type(self)(self.shape, dtype=dtype, **kwargs)", "category": "Python"}, {"instruction": "def get_runtime_config():\n    \"\"\"\n    Returns all the Turi Create configuration variables that can be set\n    at runtime. See :py:func:`turicreate.config.set_runtime_config()` to set these\n    values and for documentation on the effect of each variable.\n\n    Returns\n    -------\n    Returns a dictionary of {key:value,..}\n\n    See Also\n    --------\n    set_runtime_config\n    \"\"\"\n", "input": "", "output": "    from .._connect import main as _glconnect\n    unity = _glconnect.get_unity()\n    return unity.list_globals(True)", "category": "Python"}, {"instruction": "def get_labels(cls, path=None):\n        \"\"\"Get all server configuration labels.\n\n        :param path: A string. The configuration file to be manipulated.\n            Defaults to what is returned by\n            :func:`nailgun.config._get_config_file_path`.\n        :returns: Server configuration labels, where each label is a string.\n\n        \"\"\"\n", "input": "", "output": "        if path is None:\n            path = _get_config_file_path(\n                cls._xdg_config_dir,\n                cls._xdg_config_file\n            )\n        with open(path) as config_file:\n            # keys() returns a list in Python 2 and a view in Python 3.\n            return tuple(json.load(config_file).keys())", "category": "Python"}, {"instruction": "def unit_code_msg(housecode, unitcode):\n        \"\"\"Create an X10 message to send the house code and unit code.\"\"\"\n", "input": "", "output": "        house_byte = 0\n        unit_byte = 0\n        if isinstance(housecode, str):\n            house_byte = insteonplm.utils.housecode_to_byte(housecode) << 4\n            unit_byte = insteonplm.utils.unitcode_to_byte(unitcode)\n        elif isinstance(housecode, int) and housecode < 16:\n            house_byte = housecode << 4\n            unit_byte = unitcode\n        else:\n            house_byte = housecode\n            unit_byte = unitcode\n        return X10Received(house_byte + unit_byte, 0x00)", "category": "Python"}, {"instruction": "def set_result(self, result, separator=''):\n        \"\"\"Store the result (string) into the result key of the AMP\n        if one_line is true then replace \\n by separator\n        \"\"\"\n", "input": "", "output": "        if self.one_line():\n            self.configs['result'] = str(result).replace('\\n', separator)\n        else:\n            self.configs['result'] = str(result)", "category": "Python"}, {"instruction": "def setup_pod(build_file_path, manage_dir=None, local_requirements=None):\n    \"\"\"\n    This must be called by the project's build.py for pyntofdjango to function.\n\n\n    You can specify it directly with the optional manage_dir kwarg.\n\n    :param build_file_path: E.g. os.path.abspath(__file__)\n    :param manage_dir: Optional, Searched for if None.\n    :param local_requirements: Optional, pip requirements file for local development. Searched for if None.\n    :return:\n    \"\"\"\n", "input": "", "output": "    paths.setup(build_file_path, manage_dir=manage_dir, local_requirements=local_requirements)", "category": "Python"}, {"instruction": "def get_elevation(self, latitude, longitude, approximate=None):\n        \"\"\"\n        If approximate is True then only the points from SRTM grid will be\n        used, otherwise a basic aproximation of nearby points will be calculated.\n        \"\"\"\n", "input": "", "output": "        if not (self.latitude - self.resolution <= latitude < self.latitude + 1):\n            raise Exception('Invalid latitude %s for file %s' % (latitude, self.file_name))\n        if not (self.longitude <= longitude < self.longitude + 1 + self.resolution):\n            raise Exception('Invalid longitude %s for file %s' % (longitude, self.file_name))\n\n        row, column = self.get_row_and_column(latitude, longitude)\n\n        if approximate:\n            return self.approximation(latitude, longitude)\n        else:\n            return self.get_elevation_from_row_and_column(int(row), int(column))", "category": "Python"}, {"instruction": "def count_below_mean(x):\n    \"\"\"\n    Returns the number of values in x that are lower than the mean of x\n\n    :param x: the time series to calculate the feature of\n    :type x: numpy.ndarray\n    :return: the value of this feature\n    :return type: float\n    \"\"\"\n", "input": "", "output": "    m = np.mean(x)\n    return np.where(x < m)[0].size", "category": "Python"}, {"instruction": "def clients(self):\n        \"\"\" Returns list of all :class:`~plexapi.client.PlexClient` objects connected to server. \"\"\"\n", "input": "", "output": "        items = []\n        ports = None\n        for elem in self.query('/clients'):\n            port = elem.attrib.get('port')\n            if not port:\n                log.warning('%s did not advertise a port, checking plex.tv.', elem.attrib.get('name'))\n                ports = self._myPlexClientPorts() if ports is None else ports\n                port = ports.get(elem.attrib.get('machineIdentifier'))\n            baseurl = 'http://%s:%s' % (elem.attrib['host'], port)\n            items.append(PlexClient(baseurl=baseurl, server=self,\n                                    token=self._token, data=elem, connect=False))\n\n        return items", "category": "Python"}, {"instruction": "def g_voigt(self):\n        \"\"\"\n        returns the G_v shear modulus\n        \"\"\"\n", "input": "", "output": "        return (2. * self.voigt[:3, :3].trace() -\n                np.triu(self.voigt[:3, :3]).sum() +\n                3 * self.voigt[3:, 3:].trace()) / 15.", "category": "Python"}, {"instruction": "def get_category_by_id(cls, category_id, **kwargs):\n        \"\"\"Find Category\n\n        Return single instance of Category by its ID.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.get_category_by_id(category_id, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str category_id: ID of category to return (required)\n        :return: Category\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._get_category_by_id_with_http_info(category_id, **kwargs)\n        else:\n            (data) = cls._get_category_by_id_with_http_info(category_id, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def preview(file):\n    \"\"\"Render appropiate template with embed flag.\"\"\"\n", "input": "", "output": "    return render_template(\n        'invenio_previewer/xml_prismjs.html',\n        file=file,\n        content=render(file),\n        js_bundles=['previewer_prism_js'],\n        css_bundles=['previewer_prism_css'],\n    )", "category": "Python"}, {"instruction": "def setValue(self, key, value, channel=1):\n        \"\"\"\n        Some devices allow to directly set values to perform a specific task.\n        \"\"\"\n", "input": "", "output": "        if channel in self.CHANNELS:\n            return self.CHANNELS[channel].setValue(key, value)\n\n        LOG.error(\"HMDevice.setValue: channel not found %i!\" % channel)", "category": "Python"}, {"instruction": "def satisfy_custom_matcher(self, args, kwargs):\n        \"\"\"Returns a boolean indicating whether or not the mock will accept the provided arguments.\n\n        :param tuple args: A tuple of position args\n        :param dict kwargs: A dictionary of keyword args\n        :return: Whether or not the mock accepts the provided arguments.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        is_match = super(Expectation, self).satisfy_custom_matcher(args, kwargs)\n\n        if is_match:\n            self._satisfy()\n\n        return is_match", "category": "Python"}]