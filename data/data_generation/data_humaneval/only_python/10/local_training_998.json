[{"instruction": "def poke_64(library, session, address, data):\n    \"\"\"Write an 64-bit value from the specified address.\n\n    Corresponds to viPoke64 function of the VISA library.\n\n    :param library: the visa library wrapped by ctypes.\n    :param session: Unique logical identifier to a session.\n    :param address: Source address to read the value.\n    :param data: value to be written to the bus.\n    :return: return value of the library call.\n    :rtype: :class:`pyvisa.constants.StatusCode`\n    \"\"\"\n", "input": "", "output": "    return library.viPoke64(session, address, data)", "category": "Python"}, {"instruction": "def _populate_relationships(self, rec_curr):\n        \"\"\"Convert GO IDs in relationships to GO Term record objects. Populate children.\"\"\"\n", "input": "", "output": "        for relationship_type, goids in rec_curr.relationship.items():\n            parent_recs = set([self[goid] for goid in goids])\n            rec_curr.relationship[relationship_type] = parent_recs\n            for parent_rec in parent_recs:\n                if relationship_type not in parent_rec.relationship_rev:\n                    parent_rec.relationship_rev[relationship_type] = set([rec_curr])\n                else:\n                    parent_rec.relationship_rev[relationship_type].add(rec_curr)", "category": "Python"}, {"instruction": "def overlapping_points(coords_a, coords_b, cutoff, periodic=False):\n    '''Return the indices of *coords_b* points that overlap with\n    *coords_a* points. The overlap is calculated based on *cutoff*.\n\n    **Parameters**\n    \n    coords_a: np.ndarray((NA, 3))\n    \n    coords_b: np.ndarray((NB, 3))\n    \n    cutoff: float\n       Distance within two points are considered overlapping.\n    \n    periodic: False or np.ndarray(3)\n       Periodicity in x, y, z dimensions\n    \n    '''\n", "input": "", "output": "    \n    res = distance_matrix(coords_a, coords_b, periodic=periodic, \n                          cutoff=cutoff, method=\"cell-lists\")\n    overlaps = res.nonzero()[1]\n    return np.unique(overlaps)", "category": "Python"}, {"instruction": "def cache_key_name(cls, *args):\n        \"\"\"Return the name of the key to use to cache the current configuration\"\"\"\n", "input": "", "output": "        if cls.KEY_FIELDS != ():\n            if len(args) != len(cls.KEY_FIELDS):\n                raise TypeError(\n                    \"cache_key_name() takes exactly {} arguments ({} given)\".format(len(cls.KEY_FIELDS), len(args))\n                )\n            # pylint: disable=unicode-builtin\n            return 'configuration/{}/current/{}'.format(cls.__name__, ','.join(six.text_type(arg) for arg in args))\n        else:\n            return 'configuration/{}/current'.format(cls.__name__)", "category": "Python"}, {"instruction": "def set_vm_status(self, device='FLOPPY',\n                      boot_option='BOOT_ONCE', write_protect='YES'):\n        \"\"\"Sets the Virtual Media drive status\n\n        It also allows the boot options for booting from the virtual media.\n        \"\"\"\n", "input": "", "output": "        dic = {'DEVICE': device.upper()}\n        xml = self._create_dynamic_xml(\n            'SET_VM_STATUS', 'RIB_INFO', 'write', dic)\n\n        if six.PY2:\n            child_iterator = xml.getiterator()\n        else:\n            child_iterator = xml.iter()\n\n        for child in child_iterator:\n            if child.tag == 'SET_VM_STATUS':\n                etree.SubElement(child, 'VM_BOOT_OPTION',\n                                 VALUE=boot_option.upper())\n                etree.SubElement(child, 'VM_WRITE_PROTECT',\n                                 VALUE=write_protect.upper())\n\n        d = self._request_ilo(xml)\n        data = self._parse_output(d)\n        return data", "category": "Python"}, {"instruction": "def list(self, page: int = None, pageSize: int = None, search: str = None, workspaceId: str = None) -> dict:\n        \"\"\"\n        Retrieves a list of JSON descriptions for all forms in your Typeform account (public and private).\n        Forms are listed in reverse-chronological order based on the last date they were modified.\n        \"\"\"\n", "input": "", "output": "        return self.__client.request('get', '/forms', params={\n            'page': page,\n            'page_size': pageSize,\n            'search': search,\n            'workspace_id': workspaceId\n        })", "category": "Python"}, {"instruction": "def breadth(dirs):\n    \"\"\"\n    Crawl through directories like os.walk, but use a 'breadth first' approach\n    (os.walk uses 'depth first')\n    \"\"\"\n", "input": "", "output": "    while dirs:\n        next_dirs = []\n        print(\"Dirs: '{}'\".format(dirs))\n        for d in dirs:\n            next_dirs = []\n            try:\n                for name in os.listdir(d):\n                    p = os.path.join(d, name)\n                    if os.path.isdir(p):\n                        print(p)\n                        next_dirs.append(p)\n            except PermissionError as nallowed:\n                print(nallowed)\n        dirs = next_dirs\n        if dirs:\n            yield dirs", "category": "Python"}, {"instruction": "def page_from_file(input_file):\n    \"\"\"\n    Create a new PAGE-XML from a METS file representing a PAGE-XML or an image.\n\n    Arguments:\n        * input_file (OcrdFile):\n    \"\"\"\n", "input": "", "output": "    #  print(\"PARSING PARSING '%s'\" % input_file)\n    if input_file.mimetype.startswith('image'):\n        return page_from_image(input_file)\n    if input_file.mimetype == MIMETYPE_PAGE:\n        return parse(input_file.local_filename, silence=True)\n    raise Exception(\"Unsupported mimetype '%s'\" % input_file.mimetype)", "category": "Python"}, {"instruction": "def subtractprivkeys(p1,p2):\n    '''\n    Input must be 64-char hex string\n    '''\n", "input": "", "output": "\n    return dechex(((int(p1,16) + (N - int(p2,16))) % N),32)", "category": "Python"}, {"instruction": "def _perturbation(self):\n        \"\"\"\n        Internal function for parameter initialization\n        Returns Gaussian perturbation\n        \"\"\"\n", "input": "", "output": "        if self.P>1:\n            scales = []\n            for term_i in range(self.n_randEffs):\n                _scales = sp.randn(self.diag[term_i].shape[0])\n                if self.jitter[term_i]>0:\n                    _scales  = sp.concatenate((_scales,sp.zeros(1)))\n                scales.append(_scales)\n            scales = sp.concatenate(scales)\n        else:\n            scales = sp.randn(self.vd.getNumberScales())\n        return scales", "category": "Python"}, {"instruction": "def __get_logged_in_id(self):\n        \"\"\"\n        Fetch the logged in users ID, with caching. ID is reset on calls to log_in.\n        \"\"\"\n", "input": "", "output": "        if self.__logged_in_id == None:\n            self.__logged_in_id = self.account_verify_credentials().id\n        return self.__logged_in_id", "category": "Python"}, {"instruction": "def ProcessMessages(self, msgs=None, token=None):\n    \"\"\"Process the new file and add to the file store.\"\"\"\n", "input": "", "output": "    if not data_store.AFF4Enabled():\n      return\n\n    filestore_fd = aff4.FACTORY.Create(\n        legacy_filestore.FileStore.PATH,\n        legacy_filestore.FileStore,\n        mode=\"w\",\n        token=token)\n\n    for vfs_urn in msgs:\n      with aff4.FACTORY.Open(vfs_urn, mode=\"rw\", token=token) as vfs_fd:\n        try:\n          filestore_fd.AddFile(vfs_fd)\n        except Exception as e:  # pylint: disable=broad-except\n          logging.exception(\"Exception while adding file to filestore: %s\", e)", "category": "Python"}, {"instruction": "def is_builtin_module(module_name):\n    \"\"\"Test if a module is a builtin module (numpy, math, ...).\"\"\"\n", "input": "", "output": "    module_name = module_name.split(\".\")[0]\n    return (module_name in MODULES or\n            (module_name in cxx_keywords and module_name + \"_\" in MODULES))", "category": "Python"}, {"instruction": "def getSheet(book=None,sheet=None):\n    \"\"\"returns the pyorigin object for a sheet.\"\"\"\n", "input": "", "output": "\n    # figure out what book to use\n    if book and not book.lower() in [x.lower() for x in bookNames()]:\n        print(\"book %s doesn't exist\"%book)\n        return\n    if book is None:\n        book=activeBook().lower()\n    if book is None:\n        print(\"no book given or selected\")\n        return\n\n    # figure out what sheet to use\n    if sheet and not sheet.lower() in [x.lower() for x in sheetNames(book)]:\n        print(\"sheet %s doesn't exist\"%sheet)\n        return\n    if sheet is None:\n        sheet=activeSheet().lower()\n    if sheet is None:\n        return(\"no sheet given or selected\")\n        print\n\n    # by now, we know the book/sheet exists and can be found\n    for poSheet in PyOrigin.WorksheetPages(book).Layers():\n        if poSheet.GetName().lower()==sheet.lower():\n            return poSheet\n    return False", "category": "Python"}, {"instruction": "def metadata(\n        self, output_format=None, feature_name_callback=None, **kwargs\n    ):\n        \"\"\"\n        Gets SensorML objects for all procedures in your filtered features.\n\n        You should override the default output_format for servers that do not\n        respond properly.\n        \"\"\"\n", "input": "", "output": "        callback = feature_name_callback or str\n        if output_format is None:\n            output_format = (\n                'text/xml; subtype=\"sensorML/1.0.1/profiles/ioos_sos/1.0\"'\n            )\n\n        responses = []\n        if self.features is not None:\n            for feature in self.features:\n                ds_kwargs = kwargs.copy()\n                ds_kwargs.update(\n                    {\n                        \"outputFormat\": output_format,\n                        \"procedure\": callback(feature),\n                    }\n                )\n\n                responses.append(\n                    SensorML(self.server.describe_sensor(**ds_kwargs))\n                )\n\n        return responses", "category": "Python"}, {"instruction": "def _property_parse_cmd(cmd, alias=None):\n    '''\n    Parse output of zpool/zfs get command\n    '''\n", "input": "", "output": "    if not alias:\n        alias = {}\n    properties = {}\n\n    # NOTE: append get to command\n    if cmd[-3:] != 'get':\n        cmd += ' get'\n\n    # NOTE: parse output\n    prop_hdr = []\n    for prop_data in _exec(cmd=cmd)['stderr'].split('\\n'):\n        # NOTE: make the line data more managable\n        prop_data = prop_data.lower().split()\n\n        # NOTE: skip empty lines\n        if not prop_data:\n            continue\n        # NOTE: parse header\n        elif prop_data[0] == 'property':\n            prop_hdr = prop_data\n            continue\n        # NOTE: skip lines after data\n        elif not prop_hdr or prop_data[1] not in ['no', 'yes']:\n            continue\n\n        # NOTE: create property dict\n        prop = _property_create_dict(prop_hdr, prop_data)\n\n        # NOTE: add property to dict\n        properties[prop['name']] = prop\n        if prop['name'] in alias:\n            properties[alias[prop['name']]] = prop\n\n        # NOTE: cleanup some duplicate data\n        del prop['name']\n    return properties", "category": "Python"}, {"instruction": "def _objective_bestscore(self, old, new):\n        \"\"\"An objective function that returns True if new has a better score\n        than old, and ``False`` otherwise.\n\n        INPUTS:\n            old (tuple): a tuple (score, embedding)\n\n            new (tuple): a tuple (score, embedding)\n\n        \"\"\"\n", "input": "", "output": "        (oldscore, oldthing) = old\n        (newscore, newthing) = new\n        if oldscore is None:\n            return True\n        if newscore is None:\n            return False\n        return oldscore < newscore", "category": "Python"}, {"instruction": "def should_be_excluded(name, exclude_patterns):\n    \"\"\"Check if a name should be excluded.\n\n    Returns True if name matches at least one of the exclude patterns in\n    the exclude_patterns list.\n\n    \"\"\"\n", "input": "", "output": "    for pattern in exclude_patterns:\n        if fnmatch.fnmatch(name, pattern):\n            return True\n    return False", "category": "Python"}, {"instruction": "def task_id(self):\n        \"\"\"\n        Hack to return a task ID for a build, including container CG builds.\n\n        We have something for this in Brewweb, but not yet for upstream Koji:\n        https://pagure.io/koji/issue/215\n        \"\"\"\n", "input": "", "output": "        if self['task_id']:\n            return self['task_id']\n        if self.extra and 'container_koji_task_id' in self.extra:\n            return self.extra['container_koji_task_id']", "category": "Python"}, {"instruction": "def btc_make_data_script( data, **ignored ):\n    \"\"\"\n    Make a data-bearing transaction output.\n    Data must be a hex string\n    Returns a hex string.\n    \"\"\"\n", "input": "", "output": "    if len(data) >= MAX_DATA_LEN * 2:\n        raise ValueError(\"Data hex string is too long\")     # note: data is a hex string\n\n    if len(data) % 2 != 0:\n        raise ValueError(\"Data hex string is not even length\")\n\n    return \"6a{:02x}{}\".format(len(data)/2, data)", "category": "Python"}, {"instruction": "def fraction(self, value: float) -> 'Size':\n        \"\"\"Set the fraction of free space to use.\"\"\"\n", "input": "", "output": "        raise_not_number(value)\n        self.maximum = '{}fr'.format(value)\n        return self", "category": "Python"}, {"instruction": "def ConvCnstrMODOptionsDefaults(method='fista'):\n    \"\"\"Get defaults dict for the ConvCnstrMOD class specified by the\n    ``method`` parameter.\n    \"\"\"\n", "input": "", "output": "\n    dflt = copy.deepcopy(ccmod_class_label_lookup(method).Options.defaults)\n    if method == 'fista':\n        dflt.update({'MaxMainIter': 1, 'BackTrack':\n                     {'gamma_u': 1.2, 'MaxIter': 50}})\n    else:\n        dflt.update({'MaxMainIter': 1, 'AutoRho':\n                     {'Period': 10, 'AutoScaling': False,\n                      'RsdlRatio': 10.0, 'Scaling': 2.0,\n                      'RsdlTarget': 1.0}})\n    return dflt", "category": "Python"}, {"instruction": "def parse_metrics(self, f):\n        \"\"\"\n        Parse the metrics.tsv file from RNA-SeQC\n        \"\"\"\n", "input": "", "output": "        headers = None\n        for l in f['f'].splitlines():\n            s = l.strip().split(\"\\t\")\n            if headers is None:\n                headers = s\n            else:\n                s_name = s[ headers.index('Sample') ]\n                data = dict()\n                for idx, h in enumerate(headers):\n                    try:\n                        data[h] = float(s[idx])\n                    except ValueError:\n                        data[h] = s[idx]\n                self.rna_seqc_metrics[s_name] = data", "category": "Python"}, {"instruction": "def body_lines(self):\n        \"\"\" Return a normalized list of lines from message's body. \"\"\"\n", "input": "", "output": "        if not self.message.is_multipart():\n            body = self.message.get_payload(None, decode=True)\n        else:\n            _, _, body = self.message.as_string().partition(\"\\n\\n\")\n        if isinstance(body, bytes):\n            for enc in ['ascii', 'utf-8']:\n                try:\n                    body = body.decode(enc)\n                    break\n                except UnicodeDecodeError:\n                    continue\n            else:\n                body = self.message.get_payload(None, decode=False)\n        return body.splitlines(True)", "category": "Python"}, {"instruction": "def show_input(self, template_helper, language, seed):\n        \"\"\" Show InputBox \"\"\"\n", "input": "", "output": "        header = ParsableText(self.gettext(language, self._header), \"rst\",\n                              translation=self._translations.get(language, gettext.NullTranslations()))\n        return str(DisplayableCodeSingleLineProblem.get_renderer(template_helper)\n                   .tasks.single_line_code(self.get_id(), header, \"text\", 0, self._optional, self._default))", "category": "Python"}, {"instruction": "def connectivity_map(dset,prefix,x,y,z,radius=2):\n    '''Will perform connectivity analysis on ``dset`` using seed point ``(x,y,z)`` (in RAI order) with a sphere of radius ``radius``.\n    Does not perform any preprocessing of ``dset``. This should be already motion corrected, noise-regressed, residualized, etc.'''\n", "input": "", "output": "    seed_series = nl.sphere_average(dset,x,y,z,radius)\n    with tempfile.NamedTemporaryFile(delete=False) as temp:\n        temp.write('\\n'.join([str(x) for x in seed_series]))\n    decon = nl.Decon()\n    decon.input_dsets = dset\n    decon.stim_files = {'seed':temp.name}\n    decon.prefix = prefix\n    decon.run()\n    try:\n        os.remove(temp.name)\n    except:\n        pass", "category": "Python"}, {"instruction": "def convert_to_culled_timestep(self, timestep=1):\n        \"\"\"Convert this collection to one that only has datetimes that fit a timestep.\"\"\"\n", "input": "", "output": "        valid_s = self.header.analysis_period.VALIDTIMESTEPS.keys()\n        assert timestep in valid_s, \\\n            'timestep {} is not valid. Choose from: {}'.format(timestep, valid_s)\n\n        new_ap, new_values, new_datetimes = self._timestep_cull(timestep)\n        self.header._analysis_period = new_ap\n        self._values = new_values\n        self._datetimes = new_datetimes", "category": "Python"}, {"instruction": "def update_reminder(self, reminder):\n\t\t'''Creates a reminder with the provided attributes.\n\t\tArgs:\n\t\t\treminder\t\tupdated reminder of StreakReminder type\n\t\t\treturn\t\t\t(status code, reminder dict)\n\t\t'''\n", "input": "", "output": "\t\turi = '/'.join([self.api_uri,\n\t\t\t\t\t\tself.reminders_suffix,\n\t\t\t\t\t\t])\n\t\t#req sanity check\n\t\tpayload = None\n\t\tif  type(reminder) is not StreakReminder:\n\t\t\treturn requests.codes.bad_request, None\n\n\t\tpayload = reminder.to_dict(rw = True)\n\t\n\t\ttry:\n\t\t\turi = '/'.join([uri, reminder.attributes['key']])\n\t\texcept KeyError:\n\t\t\treturn requests.codes.bad_request, None\n\t\n\t\tcode, data = self._req('post', uri , json.dumps(payload))\n\t\t\n\t\treturn code, data", "category": "Python"}, {"instruction": "def radius(self):\n        '''\n        Radius of the ellipse, Point class.\n        '''\n", "input": "", "output": "        try:\n            return self._radius\n        except AttributeError:\n            pass\n        self._radius = Point(1, 1, 0)\n        return self._radius", "category": "Python"}, {"instruction": "def uxor(self):\n        \"\"\"Unary XOR reduction operator\"\"\"\n", "input": "", "output": "        return reduce(operator.xor, self._items, self.ftype.box(0))", "category": "Python"}, {"instruction": "def run_syncdb():\n    \"\"\"\n    Runs `./manage.py syncdb --migrate` on the given server.\n\n    Usage::\n\n        fab <server> run_syncdb\n\n    \"\"\"\n", "input": "", "output": "    with cd(settings.FAB_SETTING('SERVER_PROJECT_ROOT')):\n        if StrictVersion(django.get_version()) < StrictVersion('1.7'):\n            run_workon('python{} manage.py syncdb --migrate --noinput'.format(\n                PYTHON_VERSION))\n        else:\n            run_workon('python{} manage.py migrate'.format(PYTHON_VERSION))", "category": "Python"}, {"instruction": "def gradF_t(X, Y, S, M_E, E, m0, rho):\r\n    ''' Compute the gradient.\r\n    '''\n", "input": "", "output": "    n, r = X.shape\r\n    m, r = Y.shape\r\n\r\n    XS = np.dot(X, S)\r\n    YS = np.dot(Y, S.T)\r\n    XSY = np.dot(XS, Y.T)\r\n\r\n    Qx = np.dot(np.dot(X.T, ((M_E - XSY) * E)), YS) / n\r\n    Qy = np.dot(np.dot(Y.T, ((M_E - XSY) * E).T), XS) / m\r\n\r\n    W = np.dot((XSY - M_E) * E, YS) + np.dot(X, Qx) + rho * Gp(X, m0, r)\r\n    Z = np.dot(((XSY - M_E) * E).T, XS) + np.dot(Y, Qy) + rho * Gp(Y, m0, r)\r\n\r\n    return W, Z", "category": "Python"}, {"instruction": "def postActivate_(self):\n        \"\"\"Whatever you need to do after creating the shmem client\n        \"\"\"\n", "input": "", "output": "        if (self.requiredGPU_MB(self.required_mb)):\n            self.analyzer = YoloV3TinyAnalyzer(verbose = self.verbose)\n        else:\n            self.warning_message = \"WARNING: not enough GPU memory!\"\n            self.analyzer = None", "category": "Python"}, {"instruction": "def remove_record(self, record):\n        \"\"\"Remove an already accepted record from the community.\n\n        :param record: Record object.\n        :type record: `invenio_records.api.Record`\n        \"\"\"\n", "input": "", "output": "        if not self.has_record(record):\n            current_app.logger.warning(\n                'Community removal: record {uuid} was not in community '\n                '\"{comm}\"'.format(uuid=record.id, comm=self.id))\n        else:\n            key = current_app.config['COMMUNITIES_RECORD_KEY']\n            record[key] = [c for c in record[key] if c != self.id]\n\n        if current_app.config['COMMUNITIES_OAI_ENABLED']:\n            if self.oaiset.has_record(record):\n                self.oaiset.remove_record(record)", "category": "Python"}, {"instruction": "def http_range(self) -> slice:\n        \"\"\"The content of Range HTTP header.\n\n        Return a slice instance.\n\n        \"\"\"\n", "input": "", "output": "        rng = self._headers.get(hdrs.RANGE)\n        start, end = None, None\n        if rng is not None:\n            try:\n                pattern = r'^bytes=(\\d*)-(\\d*)$'\n                start, end = re.findall(pattern, rng)[0]\n            except IndexError:  # pattern was not found in header\n                raise ValueError(\"range not in acceptable format\")\n\n            end = int(end) if end else None\n            start = int(start) if start else None\n\n            if start is None and end is not None:\n                # end with no start is to return tail of content\n                start = -end\n                end = None\n\n            if start is not None and end is not None:\n                # end is inclusive in range header, exclusive for slice\n                end += 1\n\n                if start >= end:\n                    raise ValueError('start cannot be after end')\n\n            if start is end is None:  # No valid range supplied\n                raise ValueError('No start or end of range specified')\n\n        return slice(start, end, 1)", "category": "Python"}, {"instruction": "def to_best_int_float(val):\n    \"\"\"Get best type for value between int and float\n\n    :param val: value\n    :type val:\n    :return: int(float(val)) if int(float(val)) == float(val), else float(val)\n    :rtype: int | float\n\n    >>> to_best_int_float(\"20.1\")\n    20.1\n\n    >>> to_best_int_float(\"20.0\")\n    20\n\n    >>> to_best_int_float(\"20\")\n    20\n    \"\"\"\n", "input": "", "output": "    integer = int(float(val))\n    flt = float(val)\n    # If the f is a .0 value,\n    # best match is int\n    if integer == flt:\n        return integer\n    return flt", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"Implements main entry point for plugin execution.\"\"\"\n", "input": "", "output": "        if len(self._argv) > 1 and len(self._argv[1]) > 0:\n            oper = self._argv[1]\n        else:\n            oper = 'fetch'\n        if oper == 'fetch':\n            ret = self.fetch()\n        elif oper == 'config':\n            ret = self.config()\n            if ret and self._dirtyConfig:\n                ret = self.fetch()\n        elif oper == 'autoconf':\n            ret = self.autoconf()\n            if ret:\n                print \"yes\"\n            else:\n                print \"no\"\n            ret = True\n        elif oper == 'suggest':\n            ret = self.suggest()\n        else:\n            raise AttributeError(\"Invalid command argument: %s\" % oper)\n        return ret", "category": "Python"}, {"instruction": "def DbGetServerInfo(self, argin):\n        \"\"\" Get info about host, mode and level for specified server\n\n        :param argin: server name\n        :type: tango.DevString\n        :return: server info\n        :rtype: tango.DevVarStringArray \"\"\"\n", "input": "", "output": "        self._log.debug(\"In DbGetServerInfo()\")\n        return self.db.get_server_info(argin)", "category": "Python"}, {"instruction": "def _load_commands(root):\n    \"\"\"Returns {name: Command}\"\"\"\n", "input": "", "output": "    def proto_text(t):\n        if t.tag == 'name':\n            return '{name}'\n        elif t.tag == 'ptype':\n            return '{type}'\n        out = []\n        if t.text:\n            out.append(_escape_tpl_str(t.text))\n        for x in t:\n            out.append(proto_text(x))\n            if x.tail:\n                out.append(_escape_tpl_str(x.tail))\n        return ''.join(out)\n    out = collections.OrderedDict()\n    for elem in root.findall('commands/command'):\n        type_elem = elem.find('proto/ptype')\n        name = elem.get('name') or elem.find('proto/name').text\n        type = type_elem.text if type_elem is not None else None\n        proto_template = proto_text(elem.find('proto'))\n        params = [_load_param(x) for x in elem.findall('param')]\n        comment = elem.get('comment')\n        out[name] = Command(name, type, proto_template, params, comment)\n    return out", "category": "Python"}, {"instruction": "def gdaldem_mem_ma(ma, ds=None, res=None, extent=None, srs=None, processing='hillshade', returnma=False, computeEdges=False):\n    \"\"\"\n    Wrapper to allow gdaldem calculations for arbitrary NumPy masked array input\n    Untested, work in progress placeholder\n    Should only need to specify res, can caluclate local gt, cartesian srs\n    \"\"\"\n", "input": "", "output": "    if ds is None:\n        ds = mem_ds(res, extent, srs=None, dtype=gdal.GDT_Float32)\n    else:\n        ds = mem_ds_copy(ds)\n    b = ds.GetRasterBand(1)\n    b.WriteArray(ma)\n    out = gdaldem_mem_ds(ds, processing=processing, returnma=returnma)\n    return out", "category": "Python"}, {"instruction": "def _deserialize(self, value, attr, data):\n        \"\"\"Deserialize string by sanitizing HTML.\"\"\"\n", "input": "", "output": "        value = super(SanitizedHTML, self)._deserialize(value, attr, data)\n        return bleach.clean(\n            value,\n            tags=self.tags,\n            attributes=self.attrs,\n            strip=True,\n        ).strip()", "category": "Python"}, {"instruction": "def changeThreadColor(self, color, thread_id=None):\n        \"\"\"\n        Changes thread color\n\n        :param color: New thread color\n        :param thread_id: User/Group ID to change color of. See :ref:`intro_threads`\n        :type color: models.ThreadColor\n        :raises: FBchatException if request failed\n        \"\"\"\n", "input": "", "output": "        thread_id, thread_type = self._getThread(thread_id, None)\n\n        data = {\n            \"color_choice\": color.value if color != ThreadColor.MESSENGER_BLUE else \"\",\n            \"thread_or_other_fbid\": thread_id,\n        }\n        j = self._post(self.req_url.THREAD_COLOR, data, fix_request=True, as_json=True)", "category": "Python"}, {"instruction": "def add_dbxref(self, feature_id, db, accession, organism=None, sequence=None):\n        \"\"\"\n        Add a dbxref to a feature\n\n        :type feature_id: str\n        :param feature_id: Feature UUID\n\n        :type db: str\n        :param db: DB Name (e.g. PMID)\n\n        :type accession: str\n        :param accession: Accession Value\n\n        :type organism: str\n        :param organism: Organism Common Name\n\n        :type sequence: str\n        :param sequence: Sequence Name\n\n        This seems to show two attributes being added, but it behaves like those two are one.\n\n        :rtype: dict\n        :return: A standard apollo feature dictionary ({\"features\": [{...}]})\n        \"\"\"\n", "input": "", "output": "        data = {\n            'features': [\n                {\n                    'uniquename': feature_id,\n                    'dbxrefs': [\n                        {\n                            'db': db,\n                            'accession': accession,\n                        }\n                    ]\n                }\n            ]\n        }\n        data = self._update_data(data, organism, sequence)\n        return self.post('addDbxref', data)", "category": "Python"}, {"instruction": "def _init_forms(self):\n        \"\"\"\n            Init forms for Add and Edit\n        \"\"\"\n", "input": "", "output": "        super(BaseCRUDView, self)._init_forms()\n        conv = GeneralModelConverter(self.datamodel)\n        if not self.add_form:\n            self.add_form = conv.create_form(\n                self.label_columns,\n                self.add_columns,\n                self.description_columns,\n                self.validators_columns,\n                self.add_form_extra_fields,\n                self.add_form_query_rel_fields,\n            )\n        if not self.edit_form:\n            self.edit_form = conv.create_form(\n                self.label_columns,\n                self.edit_columns,\n                self.description_columns,\n                self.validators_columns,\n                self.edit_form_extra_fields,\n                self.edit_form_query_rel_fields,\n            )", "category": "Python"}, {"instruction": "def list_subscribers(self, list_id):\n        \"\"\"\n        List subscribers of a list\n\n        :param list_id: list ID number\n        :return: :class:`~responsebot.models.User` object\n        \"\"\"\n", "input": "", "output": "        return [User(user._json) for user in self._client.list_subscribers(list_id=list_id)]", "category": "Python"}, {"instruction": "def _key_tab(self):\r\n        \"\"\"Action for TAB key\"\"\"\n", "input": "", "output": "        if self.is_cursor_on_last_line():\r\n            empty_line = not self.get_current_line_to_cursor().strip()\r\n            if empty_line:\r\n                self.stdkey_tab()\r\n            else:\r\n                self.show_code_completion()", "category": "Python"}, {"instruction": "def export_cfg(obj, file_name):\n    \"\"\" Exports curves and surfaces in libconfig format.\n\n    .. note::\n\n        Requires `libconf <https://pypi.org/project/libconf/>`_ package.\n\n    Libconfig format is also used by the `geomdl command-line application <https://github.com/orbingol/geomdl-cli>`_\n    as a way to input shape data from the command line.\n\n    :param obj: input geometry\n    :type obj: abstract.SplineGeometry, multi.AbstractContainer\n    :param file_name: name of the output file\n    :type file_name: str\n    :raises GeomdlException: an error occurred writing the file\n    \"\"\"\n", "input": "", "output": "    def callback(data):\n        return libconf.dumps(data)\n\n    # Check if it is possible to import 'libconf'\n    try:\n        import libconf\n    except ImportError:\n        raise exch.GeomdlException(\"Please install 'libconf' package to use libconfig format: pip install libconf\")\n\n    # Export data\n    exported_data = exch.export_dict_str(obj=obj, callback=callback)\n\n    # Write to file\n    return exch.write_file(file_name, exported_data)", "category": "Python"}, {"instruction": "def convert_multiPlanesRupture(self, node):\n        \"\"\"\n        Convert a multiPlanesRupture node.\n\n        :param node: the rupture node\n        \"\"\"\n", "input": "", "output": "        mag, rake, hypocenter = self.get_mag_rake_hypo(node)\n        with context(self.fname, node):\n            surfaces = list(node.getnodes('planarSurface'))\n        rupt = source.rupture.BaseRupture(\n            mag=mag, rake=rake,\n            tectonic_region_type=None,\n            hypocenter=hypocenter,\n            surface=self.convert_surfaces(surfaces))\n        return rupt", "category": "Python"}, {"instruction": "def sql(self, query):\n        \"\"\"\n        Convert a SQL query to an Ibis table expression\n\n        Parameters\n        ----------\n\n        Returns\n        -------\n        table : TableExpr\n        \"\"\"\n", "input": "", "output": "        # Get the schema by adding a LIMIT 0 on to the end of the query. If\n        # there is already a limit in the query, we find and remove it\n        limited_query = 'SELECT * FROM ({}) t0 LIMIT 0'.format(query)\n        schema = self._get_schema_using_query(limited_query)\n        return ops.SQLQueryResult(query, schema, self).to_expr()", "category": "Python"}, {"instruction": "def wait(self, timeout=None):\n        \"\"\"Wait for the request to finish, optionally timing out. Returns True if the request has finished or False if\n        it is still pending.\n\n        Raises [LinkException](AmqpLink.m.html#IoticAgent.Core.AmqpLink.LinkException) if the request failed due to a\n        network related problem.\n        \"\"\"\n", "input": "", "output": "        if self.__event.wait(timeout):\n            if self.exception is not None:\n                # todo better way to raise errors on behalf of other Threads?\n                raise self.exception  # pylint: disable=raising-bad-type\n            return True\n        return False", "category": "Python"}, {"instruction": "def potential_energy(self, potential=None):\n        r\"\"\"\n        The potential energy *per unit mass*:\n\n        .. math::\n\n            E_\\Phi = \\Phi(\\boldsymbol{q})\n\n        Returns\n        -------\n        E : :class:`~astropy.units.Quantity`\n            The potential energy.\n        \"\"\"\n", "input": "", "output": "        if self.hamiltonian is None and potential is None:\n            raise ValueError(\"To compute the potential energy, a potential\"\n                             \" object must be provided!\")\n        if potential is None:\n            potential = self.hamiltonian.potential\n\n        return super(Orbit,self).potential_energy(potential)", "category": "Python"}, {"instruction": "def parseAttributes(self, attributes=None, **kwAttributes):\n        \"\"\"Parses CSS attribute source strings, and return as an inline stylesheet.\n        Use to parse a tag's highly CSS-based attributes like 'font'.\n\n        See also: parseSingleAttr\n        \"\"\"\n", "input": "", "output": "        attributes = attributes if attributes is not None else {}\n        if attributes:\n            kwAttributes.update(attributes)\n\n        self.cssBuilder.beginInline()\n        try:\n            properties = []\n            try:\n                for propertyName, src in six.iteritems(kwAttributes):\n                    src, property = self._parseDeclarationProperty(src.strip(), propertyName)\n                    properties.append(property)\n\n            except self.ParseError as err:\n                err.setFullCSSSource(src, inline=True)\n                raise\n\n            result = self.cssBuilder.inline(properties)\n        finally:\n            self.cssBuilder.endInline()\n        return result", "category": "Python"}, {"instruction": "def exec_workflow(self, model, record_id, signal):\n        \"\"\"Execute the workflow `signal` on\n        the instance having the ID `record_id` of `model`.\n\n        *Python 2:*\n\n        :raise: :class:`odoorpc.error.RPCError`\n        :raise: :class:`odoorpc.error.InternalError` (if not logged)\n        :raise: `urllib2.URLError` (connection error)\n\n        *Python 3:*\n\n        :raise: :class:`odoorpc.error.RPCError`\n        :raise: :class:`odoorpc.error.InternalError` (if not logged)\n        :raise: `urllib.error.URLError` (connection error)\n        \"\"\"\n", "input": "", "output": "        if tools.v(self.version)[0] >= 11:\n            raise DeprecationWarning(\n                u\"Workflows have been removed in Odoo >= 11.0\")\n        self._check_logged_user()\n        # Execute the workflow query\n        args_to_send = [self.env.db, self.env.uid, self._password,\n                        model, signal, record_id]\n        data = self.json(\n            '/jsonrpc',\n            {'service': 'object',\n             'method': 'exec_workflow',\n             'args': args_to_send})\n        return data.get('result')", "category": "Python"}, {"instruction": "def AddLogFileOptions(self, argument_group):\n    \"\"\"Adds the log file option to the argument group.\n\n    Args:\n      argument_group (argparse._ArgumentGroup): argparse argument group.\n    \"\"\"\n", "input": "", "output": "    argument_group.add_argument(\n        '--logfile', '--log_file', '--log-file', action='store',\n        metavar='FILENAME', dest='log_file', type=str, default='', help=(\n            'Path of the file in which to store log messages, by default '\n            'this file will be named: \"{0:s}-YYYYMMDDThhmmss.log.gz\". Note '\n            'that the file will be gzip compressed if the extension is '\n            '\".gz\".').format(self.NAME))", "category": "Python"}, {"instruction": "def _arrayize_vectorized_indexer(indexer, shape):\n    \"\"\" Return an identical vindex but slices are replaced by arrays \"\"\"\n", "input": "", "output": "    slices = [v for v in indexer.tuple if isinstance(v, slice)]\n    if len(slices) == 0:\n        return indexer\n\n    arrays = [v for v in indexer.tuple if isinstance(v, np.ndarray)]\n    n_dim = arrays[0].ndim if len(arrays) > 0 else 0\n    i_dim = 0\n    new_key = []\n    for v, size in zip(indexer.tuple, shape):\n        if isinstance(v, np.ndarray):\n            new_key.append(np.reshape(v, v.shape + (1, ) * len(slices)))\n        else:  # slice\n            shape = ((1,) * (n_dim + i_dim) + (-1,) +\n                     (1,) * (len(slices) - i_dim - 1))\n            new_key.append(np.arange(*v.indices(size)).reshape(shape))\n            i_dim += 1\n    return VectorizedIndexer(tuple(new_key))", "category": "Python"}, {"instruction": "def reset(self):\n        \"\"\"Reset the display.\"\"\"\n", "input": "", "output": "        if self._rst is None:\n            return\n        # Set reset high for a millisecond.\n        self._rst.value = True\n        time.sleep(0.001)\n        # Set reset low for 10 milliseconds.\n        self._rst.value = False\n        time.sleep(0.010)\n        # Set reset high again.\n        self._rst.value = False", "category": "Python"}, {"instruction": "def find_objects(uid=None):\n    \"\"\"Find the object by its UID\n\n    1. get the object from the given uid\n    2. fetch objects specified in the request parameters\n    3. fetch objects located in the request body\n\n    :param uid: The UID of the object to find\n    :type uid: string\n    :returns: List of found objects\n    :rtype: list\n    \"\"\"\n", "input": "", "output": "    # The objects to cut\n    objects = []\n\n    # get the object by the given uid or try to find it by the request\n    # parameters\n    obj = get_object_by_uid(uid) or get_object_by_request()\n\n    if obj:\n        objects.append(obj)\n    else:\n        # no uid -> go through the record items\n        records = req.get_request_data()\n        for record in records:\n            # try to get the object by the given record\n            obj = get_object_by_record(record)\n\n            # no object found for this record\n            if obj is None:\n                continue\n            objects.append(obj)\n\n    return objects", "category": "Python"}, {"instruction": "def list_sas(self, filters=None):\n        \"\"\"Retrieve active IKE_SAs and associated CHILD_SAs.\n\n        :param filters: retrieve only matching IKE_SAs (optional)\n        :type filters: dict\n        :return: list of active IKE_SAs and associated CHILD_SAs\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        _, sa_list = self.handler.streamed_request(\"list-sas\",\n                                                   \"list-sa\", filters)\n        return sa_list", "category": "Python"}, {"instruction": "def update_event(self, hint, obj):\n        \"\"\"Updates tree colums when itemdata is changed.\"\"\"\n", "input": "", "output": "\n        tree = self.treeview\n        data = obj\n        item = self.get_item_by_data(obj)\n        if item:\n            if data.get_id() != tree.item(item, 'text'):\n                tree.item(item, text=data.get_id())\n            # if tree.parent(item) != '' and 'layout' in data:\n            if tree.parent(item) != '':\n                row = data.get_layout_property('row')\n                col = data.get_layout_property('column')\n                values = tree.item(item, 'values')\n                if (row != values[1] or col != values[2]):\n                    values = (data.get_class(), row, col)\n                tree.item(item, values=values)\n            self.draw_widget(item)\n            self.app.set_changed()", "category": "Python"}, {"instruction": "def log_wrap(origfunc):\n    \"\"\"\n    DRY: Use magic instead of code to get a string for the correct log\n    level when calling ``print_log_msg``. Because writing the same\n    boilerplate code in each log_XXX def was too painful to commit.\n    \"\"\"\n", "input": "", "output": "    def orig_func_wraper(msg, *args):\n        # Take the callers name and snap it in two, result is log\n        # level, e.g.: log_debug is DEBUG level.\n        log_level = origfunc.__name__.split(\"_\")[1]\n\n        import Log\n        if getattr(Log, \"LOG_%s\" % log_level.upper()) <= \\\n                Log.LOG_LEVEL_CURRENT:\n            # flatten and stringify the positional params so we don't\n            # tuple() a tuple or an array and end up with\n            # weirdness.\n            a = map(str, juicer.utils.flatten(args))\n            print_log_msg(log_level, str(msg) % tuple(a))\n    return orig_func_wraper", "category": "Python"}, {"instruction": "def language_to_locale(language):\n    \"\"\"\n    Converts django's `LANGUAGE_CODE` settings to a proper locale code.\n    \"\"\"\n", "input": "", "output": "    tokens = language.split('-')\n    if len(tokens) == 1:\n        return tokens[0]\n    return \"%s_%s\" % (tokens[0], tokens[1].upper())", "category": "Python"}, {"instruction": "def action(callback=None, name=None, path=None, methods=Method.GET, resource=None, tags=None,\n           summary=None, middleware=None):\n    # type: (Callable, Path, Path, Methods, Type[Resource], Tags, str, List[Any]) -> Operation\n    \"\"\"\n    Decorator to apply an action to a resource. An action is applied to a `detail` operation.\n    \"\"\"\n", "input": "", "output": "    # Generate action path\n    path = path or '{key_field}'\n    if name:\n        path += name\n\n    def inner(c):\n        return Operation(c, path, methods, resource, tags, summary, middleware)\n    return inner(callback) if callback else inner", "category": "Python"}, {"instruction": "def add(self, chrom, element):\n        \"\"\"insert an element. use this method as the IntervalTree one.\n        this will simply call the IntervalTree.add method on the right tree\n\n        :param chrom: chromosome\n        :param element: the argument of IntervalTree.insert_interval\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        self._trees.setdefault(chrom, IntervalTree()).insert_interval( element )", "category": "Python"}, {"instruction": "def _authenticate_user(self, user, password):\n        \"\"\"\n        Returns the response of authenticating with the given\n        user and password.\n        \"\"\"\n", "input": "", "output": "        headers = self._get_headers()\n        params = {\n                'username': user,\n                'password': password,\n                'grant_type': 'password',\n                }\n        uri = self.uri + '/oauth/token'\n\n        logging.debug(\"URI=\" + str(uri))\n        logging.debug(\"HEADERS=\" + str(headers))\n        logging.debug(\"BODY=\" + str(params))\n\n        response = requests.post(uri, headers=headers, params=params)\n        if response.status_code == 200:\n            logging.debug(\"RESPONSE=\" + str(response.json()))\n            return response.json()\n        else:\n            logging.warning(\"Failed to authenticate %s\" % (user))\n            response.raise_for_status()", "category": "Python"}, {"instruction": "def make_node(self, x):\n        \"\"\"Create a node for the computation graph.\n\n        Parameters\n        ----------\n        x : `theano.tensor.var.TensorVariable`\n            Input to the node.\n\n        Returns\n        -------\n        node : `theano.gof.graph.Apply`\n            Node for the Theano expression graph. Its only input is ``x``,\n            and the output is of the same type.\n        \"\"\"\n", "input": "", "output": "        x = theano.tensor.as_tensor_variable(x)\n\n        # Create tensor type with correct dtype.\n        # The second argument specifies the number of dimensions of the output.\n        # False means that we do not support broadcasting.\n        if isinstance(self.operator, Functional):\n            # Make scalar out type\n            out_type = theano.tensor.TensorVariable(\n                theano.tensor.TensorType(self.operator.domain.dtype, ()))\n        else:\n            out_type = theano.tensor.TensorVariable(\n                theano.tensor.TensorType(\n                    self.operator.range.dtype,\n                    [False] * len(self.operator.range.shape)))\n\n        return theano.Apply(self, [x], [out_type.type()])", "category": "Python"}, {"instruction": "def fix_filename(filename, suffix=''):\n    \"\"\"\n    e.g.\n        fix_filename('icon.png', '_40x40')\n        \n        return\n    \n            icon_40x40.png\n    \"\"\"\n", "input": "", "output": "    if suffix:\n        f, ext = os.path.splitext(filename)\n        return f+suffix+ext\n    else:\n        return filename", "category": "Python"}, {"instruction": "def repository(self):\n        \"\"\"Repository.\"\"\"\n", "input": "", "output": "        m = re.match(\"(.+)(_\\d{4}_\\d{2}_\\d{2}_)(.+)\", self.__module__)\n        if m:\n            return m.group(1)\n        m = re.match(\"(.+)(_release_)(.+)\", self.__module__)\n        if m:\n            return m.group(1)", "category": "Python"}, {"instruction": "def recv_raw(self, x=MTU):\n        \"\"\"Receives a packet, then returns a tuple containing (cls, pkt_data, time)\"\"\"\n", "input": "", "output": "        pkt, sa_ll = self.ins.recvfrom(x)\n        if self.outs and sa_ll[2] == socket.PACKET_OUTGOING:\n            return None, None, None\n        ts = get_last_packet_timestamp(self.ins)\n        return self.LL, pkt, ts", "category": "Python"}, {"instruction": "def update_rpndict(self, rpndict):\n        \"\"\" update rpndict, try to solve rpn expressions as many as possible,\n            leave unsolvable unchanged.\n\n            return new dict\n        \"\"\"\n", "input": "", "output": "        tmpdict = {k: v for k, v in rpndict.items()}\n        for k, v in rpndict.items():\n            v_str = str(v)\n            if rpn.Rpn.solve_rpn(v_str) is None:\n                tmpdict[k] = self.rinse_rpnexp(v_str, tmpdict)\n            else:\n                tmpdict[k] = rpn.Rpn.solve_rpn(v_str)\n        return tmpdict", "category": "Python"}, {"instruction": "def _replace_zeros(arr, default_min_value):\n    \"\"\"Substitute 0s in the list with a near-zero value.\n\n    Parameters\n    -----------\n    arr : numpy.array(float)\n    default_min_value : float\n        If the smallest non-zero element in `arr` is greater than the default,\n        use the default instead.\n\n    Returns\n    -----------\n    numpy.array(float)\n    \"\"\"\n", "input": "", "output": "    min_nonzero_value = min(default_min_value, np.min(arr[arr > 0]))\n    closest_to_zero = np.nextafter(min_nonzero_value, min_nonzero_value - 1)\n    arr[arr == 0] = closest_to_zero\n    return arr", "category": "Python"}, {"instruction": "def _validate_install(self):\r\n\r\n        ''' a method to validate heroku is installed '''\n", "input": "", "output": "\r\n        self.printer('Checking heroku installation ... ', flush=True)\r\n    \r\n    # import dependencies\r\n        from os import devnull\r\n        from subprocess import call, check_output\r\n        \r\n    # validate cli installation\r\n        sys_command = 'heroku --version'\r\n        try:\r\n            call(sys_command, shell=True, stdout=open(devnull, 'wb'))\r\n        except Exception as err:\r\n            self.printer('ERROR')\r\n            raise Exception('\"heroku cli\" not installed. GoTo: https://devcenter.heroku.com/articles/heroku-cli')\r\n    \r\n    # print response and return\r\n        self.printer('done.')\r\n        \r\n        return True", "category": "Python"}, {"instruction": "def init(self, value):\n        ''' hash passwords given in the constructor '''\n", "input": "", "output": "        value = self.value_or_default(value)\n\n        if value is None: return None\n\n        if is_hashed(value):\n            return value\n\n        return make_password(value)", "category": "Python"}, {"instruction": "def closest(self, ps: Union[\"Units\", List[\"Point2\"], Set[\"Point2\"]]) -> Union[\"Unit\", \"Point2\"]:\n        \"\"\" This function assumes the 2d distance is meant \"\"\"\n", "input": "", "output": "        assert ps\n        if len(ps) == 1:\n            return ps[0]\n        closest_distance_squared = math.inf\n        for p2 in ps:\n            p2pos = p2\n            if not isinstance(p2pos, Point2):\n                p2pos = p2.position\n            distance = (self[0] - p2pos[0]) ** 2 + (self[1] - p2pos[1]) ** 2\n            if distance < closest_distance_squared:\n                closest_distance_squared = distance\n                closest_element = p2\n        return closest_element", "category": "Python"}, {"instruction": "def _GetComparable(self, sub_comparable_string=''):\n    \"\"\"Retrieves the comparable representation.\n\n    This is a convenience function for constructing comparables.\n\n    Args:\n      sub_comparable_string (str): sub comparable string.\n\n    Returns:\n      str: comparable representation of the path specification.\n    \"\"\"\n", "input": "", "output": "    string_parts = []\n\n    string_parts.append(getattr(self.parent, 'comparable', ''))\n    string_parts.append('type: {0:s}'.format(self.type_indicator))\n\n    if sub_comparable_string:\n      string_parts.append(', {0:s}'.format(sub_comparable_string))\n    string_parts.append('\\n')\n\n    return ''.join(string_parts)", "category": "Python"}, {"instruction": "def random(magnitude=1):\n        \"\"\" Create a unit vector pointing in a random direction. \"\"\"\n", "input": "", "output": "        theta = random.uniform(0, 2 * math.pi)\n        return magnitude * Vector(math.cos(theta), math.sin(theta))", "category": "Python"}, {"instruction": "def from_nibabel(nib_image):\n    \"\"\"\n    Convert a nibabel image to an ANTsImage\n    \"\"\"\n", "input": "", "output": "    tmpfile = mktemp(suffix='.nii.gz')\n    nib_image.to_filename(tmpfile)\n    new_img = iio2.image_read(tmpfile)\n    os.remove(tmpfile)\n    return new_img", "category": "Python"}, {"instruction": "def diff(self, n=1, axis=-1):\n        \"\"\"Calculate the n-th order discrete difference along given axis.\n\n        The first order difference is given by ``out[n] = a[n+1] - a[n]`` along\n        the given axis, higher order differences are calculated by using `diff`\n        recursively.\n\n        Parameters\n        ----------\n        n : int, optional\n            The number of times values are differenced.\n        axis : int, optional\n            The axis along which the difference is taken, default is the\n            last axis.\n\n        Returns\n        -------\n        diff : `Series`\n            The `n` order differences. The shape of the output is the same\n            as the input, except along `axis` where the dimension is\n            smaller by `n`.\n\n        See Also\n        --------\n        numpy.diff\n            for documentation on the underlying method\n        \"\"\"\n", "input": "", "output": "        out = super(Array, self).diff(n=n, axis=axis)\n        try:\n            out.x0 = self.x0 + self.dx * n\n        except AttributeError:  # irregular xindex\n            out.x0 = self.xindex[n]\n        return out", "category": "Python"}, {"instruction": "def save_pubsec_file(self, path: str) -> None:\n        \"\"\"\n        Save a Duniter PubSec file (PubSec) v1\n\n        :param path: Path to file\n        \"\"\"\n", "input": "", "output": "        # version\n        version = 1\n\n        # base58 encode keys\n        base58_signing_key = Base58Encoder.encode(self.sk)\n        base58_public_key = self.pubkey\n\n        # save file\n        with open(path, 'w') as fh:\n            fh.write(\n                ", "category": "Python"}, {"instruction": "def sbesselj_sum(z, N):\r\n    \"\"\"Tests the Spherical Bessel function jn using the sum:\r\n\r\n        Inf\r\n        sum  (2*n+1) * jn(z)**2 = 1\r\n        n=0\r\n\r\n\r\n        z:  The argument.\r\n        N:  Large N value that the sum runs too.\r\n\r\n    Note that the sum only converges to 1 for large N value (i.e. N >> z).\r\n\r\n    The routine returns the relative error of the assumption.\r\n    \"\"\"\n", "input": "", "output": "\r\n    b = sbesselj(z, N)\r\n    vvv = 2.0 * np.array(range(0, N), dtype=np.float64) + 1.0\r\n    sm = np.sum(np.sort(vvv * (b ** 2)))\r\n    return np.abs((sm - 1.0) / sm) + np.spacing(1)", "category": "Python"}, {"instruction": "def git_tag(repo_dir, tagname, message=None, force=True):\n    \"\"\"Create an annotated tag at the current head.\"\"\"\n", "input": "", "output": "    message = message or \"%s\" % tagname\n    command = ['git', 'tag', '--annotate', '--message', message]\n    if force:\n        command.append('--force')\n    # append the tag as the final arg\n    command.append(tagname)\n    return execute_git_command(command, repo_dir=repo_dir)", "category": "Python"}, {"instruction": "def edit(self, name, permission=''):\n        \"\"\"Edit this team.\n\n        :param str name: (required)\n        :param str permission: (optional), ('pull', 'push', 'admin')\n        :returns: bool\n        \"\"\"\n", "input": "", "output": "        if name:\n            data = {'name': name, 'permission': permission}\n            json = self._json(self._patch(self._api, data=dumps(data)), 200)\n            if json:\n                self._update_(json)\n                return True\n        return False", "category": "Python"}, {"instruction": "def execute(sql, args=None, key='default'):\n    \"\"\"It is used for update, delete records.\n\n    :param sql string: the sql stamtement like 'select * from %s'\n    :param args  list: Wen set None, will use dbi execute(sql), else\n            dbi execute(sql, args), the args keep the original rules, it shuld be tuple or list of list\n    :param key: a key for your dabtabase you wanna use\n\n    eg::\n\n        execute('insert into users values(%s, %s)', [(1L, 'blablabla'), (2L, 'animer')])\n        execute('delete from users')\n    \"\"\"\n", "input": "", "output": "    database = __db[key]\n    return database.execute(sql, args)", "category": "Python"}, {"instruction": "def _cached_search_compile(pattern, re_verbose, re_version, pattern_type):\n    \"\"\"Cached search compile.\"\"\"\n", "input": "", "output": "\n    return _bre_parse._SearchParser(pattern, re_verbose, re_version).parse()", "category": "Python"}, {"instruction": "def load_stream(handle, delimiter=None):\n    \"\"\"\n    Creates a string generator from a stream (file handle) containing data \n    delimited by the delimiter strings. This is a stand-alone function and \n    should be used to feed external data into a pipeline.\n\n    Arguments:\n\n      - hande(``file``) A file handle open for reading.\n      - delimiter(``str``) [default: ``None``] The default means that items will\n        be separated by two new-line characters i.e.: ``\"\\\\n\\\\n\"``.\n    \n    \"\"\"\n", "input": "", "output": "    delimiter = (delimiter or \"\") + \"\\n\"\n    while True:\n        item = []\n        while True:\n            line = handle.readline()\n            if line == \"\":\n                raise StopIteration\n            elif line == delimiter:\n                if item:\n                    break\n            elif line != '\\n':\n                item.append(line)\n\n        yield \"\".join(item)", "category": "Python"}, {"instruction": "def requestTreeMenu(self, point):\r\n        \"\"\"\r\n        Emits the itemMenuRequested and treeMenuRequested signals\r\n        for the given item.\r\n        \r\n        :param      point | <QPoint>\r\n        \"\"\"\n", "input": "", "output": "        item = self.uiGanttTREE.itemAt(point)\r\n        if item:\r\n            glbl_pos = self.uiGanttTREE.viewport().mapToGlobal(point)\r\n            self.treeMenuRequested.emit(item, glbl_pos)\r\n            self.itemMenuRequested.emit(item, glbl_pos)", "category": "Python"}, {"instruction": "def json(self):\n        \"\"\"Returns the search results as a list of JSON objects.\"\"\"\n", "input": "", "output": "        if self.search_results is None:\n            return None\n\n        result = []\n        for row in self.search_results['rows']:\n            obj = {}\n            for index in range(0, len(self.search_results['fields'])):\n                obj[self.search_results['fields'][index]] = row[index]\n            result.append(obj)\n\n        return result", "category": "Python"}, {"instruction": "def encode2(self):\n        \"\"\"Return the base64 encoding of the fig attribute and insert in html image tag.\"\"\"\n", "input": "", "output": "        buf = BytesIO()\n        self.fig.savefig(buf, format='png', bbox_inches='tight', dpi=100)\n        buf.seek(0)\n        string = b64encode(buf.read())\n        return '<img src=\"data:image/png;base64,{0}\">'.format(urlquote(string))", "category": "Python"}, {"instruction": "def clear(self):\n        \"\"\"Deletes all activations in the agenda.\"\"\"\n", "input": "", "output": "        if lib.EnvDeleteActivation(self._env, ffi.NULL) != 1:\n            raise CLIPSError(self._env)", "category": "Python"}, {"instruction": "def dense_to_one_hot(labels_dense, num_classes):\n        \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n", "input": "", "output": "        num_labels = labels_dense.shape[0]\n        index_offset = np.arange(num_labels) * num_classes\n        labels_one_hot = np.zeros((num_labels, num_classes))\n        labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n        return labels_one_hot", "category": "Python"}, {"instruction": "def query_source(self, source):\n        \"\"\"\n        Query by source\n        \"\"\"\n", "input": "", "output": "        return self._get_repo_filter(Layer.objects).filter(url=source)", "category": "Python"}, {"instruction": "def last_git_release_tag(git_tags: str) -> str:\n    \"\"\"\n    :git_tags: chronos.helpers.git_tags() function output.\n\n    Returns the latest Git tag ending with a SemVer as a string.\n    \"\"\"\n", "input": "", "output": "    semver_re = re.compile(r'[0-9]+\\.[0-9]+\\.[0-9]+$')\n    str_ver = []\n    for i in git_tags.split():\n        if semver_re.search(i):\n            str_ver.append(i)\n\n    try:\n        return str_ver[0]\n    except IndexError:\n        raise NoGitTagsException", "category": "Python"}, {"instruction": "def read_srml_month_from_solardat(station, year, month, filetype='PO'):\n    \"\"\"Request a month of SRML[1] data from solardat and read it into\n    a Dataframe.\n\n    Parameters\n    ----------\n    station: str\n        The name of the SRML station to request.\n    year: int\n        Year to request data for\n    month: int\n        Month to request data for.\n    filetype: string\n        SRML file type to gather. 'RO' and 'PO' are the\n        only minute resolution files.\n\n    Returns\n    -------\n    data: pd.DataFrame\n        One month of data from SRML.\n\n    References\n    ----------\n    [1] University of Oregon Solar Radiation Measurement Laboratory\n        `http://solardat.uoregon.edu/ <http://solardat.uoregon.edu/>`_\n    \"\"\"\n", "input": "", "output": "    file_name = \"{station}{filetype}{year:02d}{month:02d}.txt\".format(\n        station=station,\n        filetype=filetype,\n        year=year % 100,\n        month=month)\n    url = \"http://solardat.uoregon.edu/download/Archive/\"\n    data = read_srml(url + file_name)\n    return data", "category": "Python"}, {"instruction": "def handle(self, **options):\n        \"\"\"\n        Override handle to supress summary output\n        \"\"\"\n", "input": "", "output": "        super(Command, self).handle(**options)\n        return \"{} static file{} copied.\".format(\n            self.num_copied_files,\n            '' if self.num_copied_files == 1 else 's')", "category": "Python"}, {"instruction": "def put(self, json=None):\n        \"\"\"Send a PUT request and return the JSON decoded result.\n\n        Args:\n            json (dict, optional): Object to encode and send in request.\n\n        Returns:\n            mixed: JSON decoded response data.\n        \"\"\"\n", "input": "", "output": "        return self._call('put', url=self.endpoint, json=json)", "category": "Python"}, {"instruction": "def resize(self, size, interp='bilinear'):\n        \"\"\"Resize the image.\n\n        Parameters\n        ----------\n        size : int, float, or tuple\n            * int   - Percentage of current size.\n            * float - Fraction of current size.\n            * tuple - Size of the output image.\n\n        interp : :obj:`str`, optional\n            Interpolation to use for re-sizing ('nearest', 'lanczos', 'bilinear',\n            'bicubic', or 'cubic')\n        \"\"\"\n", "input": "", "output": "        # resize channels separately\n        color_im_resized = self.color.resize(size, interp)\n        depth_im_resized = self.depth.resize(size, interp)\n\n        # return combination of resized data\n        return RgbdImage.from_color_and_depth(\n            color_im_resized, depth_im_resized)", "category": "Python"}, {"instruction": "def write_bit(self, b):\n        \"\"\"\n        Write a boolean value.\n\n        \"\"\"\n", "input": "", "output": "        if b:\n            b = 1\n        else:\n            b = 0\n        shift = self.bitcount % 8\n        if shift == 0:\n            self.bits.append(0)\n        self.bits[-1] |= (b << shift)\n        self.bitcount += 1", "category": "Python"}, {"instruction": "def create_table_from_dataframe(\n        self,\n        dataframe,\n        table_name=\"\",\n        primary_key=None,\n        add_primary_key_column=False,\n        index_attrs=None,\n    ):\n        \"\"\"\n        Create a table from a pandas.DataFrame instance.\n\n        :param pandas.DataFrame dataframe: DataFrame instance to convert.\n        :param str table_name: Table name to create.\n        :param str primary_key: |primary_key|\n        :param tuple index_attrs: |index_attrs|\n\n        :Examples:\n            :ref:`example-create-table-from-df`\n        \"\"\"\n", "input": "", "output": "\n        self.__create_table_from_tabledata(\n            TableData.from_dataframe(dataframe=dataframe, table_name=table_name),\n            primary_key,\n            add_primary_key_column,\n            index_attrs,\n        )", "category": "Python"}, {"instruction": "def _frame_generator(self, frame_duration_ms, audio, sample_rate):\n        \"\"\"Generates audio frames from PCM audio data.\n        Takes the desired frame duration in milliseconds, the PCM data, and\n        the sample rate.\n        Yields Frames of the requested duration.\n        \"\"\"\n", "input": "", "output": "        n = int(sample_rate * (frame_duration_ms / 1000.0) * 2)\n        offset = 0\n        timestamp = 0.0\n        duration = (float(n) / sample_rate) / 2.0\n        while offset + n < len(audio):\n            yield self.Frame(audio[offset:offset + n], timestamp, duration)\n            timestamp += duration\n            offset += n", "category": "Python"}, {"instruction": "def get_cms_model_order(model_name):\n    \"\"\"\n    Return a numeric ordering for a model name.\n    \"\"\"\n", "input": "", "output": "    for (name, order) in iteritems(appsettings.FLUENT_DASHBOARD_CMS_MODEL_ORDER):\n        if name in model_name:\n            return order\n    return 999", "category": "Python"}, {"instruction": "def read_file(file_name, encoding='utf-8'):\n    \"\"\"\n    \u8bfb\u6587\u672c\u6587\u4ef6\n    :param encoding:\n    :param file_name:\n    :return:\n    \"\"\"\n", "input": "", "output": "    with open(file_name, 'rb') as f:\n        data = f.read()\n\n    if encoding is not None:\n        data = data.decode(encoding)\n\n    return data", "category": "Python"}, {"instruction": "def get_project(self) -> str:\n        \"\"\" Get the ihc project and make sure controller is ready before\"\"\"\n", "input": "", "output": "        with IHCController._mutex:\n            if self._project is None:\n                if self.client.get_state() != IHCSTATE_READY:\n                    ready = self.client.wait_for_state_change(IHCSTATE_READY,\n                                                              10)\n                    if ready != IHCSTATE_READY:\n                        return None\n                self._project = self.client.get_project()\n        return self._project", "category": "Python"}, {"instruction": "def load_basic_system_bindings():\n    \"\"\"\n    Basic system bindings (For both Emacs and Vi mode.)\n    \"\"\"\n", "input": "", "output": "    registry = Registry()\n\n    suspend_supported = Condition(\n        lambda cli: suspend_to_background_supported())\n\n    @registry.add_binding(Keys.ControlZ, filter=suspend_supported)\n    def _(event):\n        ", "category": "Python"}, {"instruction": "def setBendLength(self, x):\n        \"\"\" set bend length\n\n        :param x: new bend length to be assigned, [m]\n        :return: None\n        \"\"\"\n", "input": "", "output": "        if x != self.bend_length:\n            self.bend_length = x\n            self.refresh = True", "category": "Python"}, {"instruction": "def start_proc_mask_signal(proc):\n    \"\"\"\n    Start process(es) with SIGINT ignored.\n\n    Args:\n        proc: (mp.Process or list)\n\n    Note:\n        The signal mask is only applied when called from main thread.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(proc, list):\n        proc = [proc]\n\n    with mask_sigint():\n        for p in proc:\n            if isinstance(p, mp.Process):\n                if sys.version_info < (3, 4) or mp.get_start_method() == 'fork':\n                    log_once(\n\"Starting a process with 'fork' method is not safe and may consume unnecessary extra memory.\"\n\" Use 'forkserver' method (available after Py3.4) instead if you run into any issues. \"\n\"See https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods\",\n'warn')  # noqa\n            p.start()", "category": "Python"}, {"instruction": "def createDataChannel(self, label, maxPacketLifeTime=None, maxRetransmits=None,\n                          ordered=True, protocol='', negotiated=False, id=None):\n        \"\"\"\n        Create a data channel with the given label.\n\n        :rtype: :class:`RTCDataChannel`\n        \"\"\"\n", "input": "", "output": "        if maxPacketLifeTime is not None and maxRetransmits is not None:\n            raise ValueError('Cannot specify both maxPacketLifeTime and maxRetransmits')\n\n        if not self.__sctp:\n            self.__createSctpTransport()\n\n        parameters = RTCDataChannelParameters(\n            id=id,\n            label=label,\n            maxPacketLifeTime=maxPacketLifeTime,\n            maxRetransmits=maxRetransmits,\n            negotiated=negotiated,\n            ordered=ordered,\n            protocol=protocol)\n        return RTCDataChannel(self.__sctp, parameters)", "category": "Python"}, {"instruction": "def _values(self):\n        \"\"\"Getter for series values (flattened)\"\"\"\n", "input": "", "output": "        return [\n            val[1] for serie in self.series for val in\n            (serie.interpolated if self.interpolate else serie.points)\n            if val[1] is not None and (not self.logarithmic or val[1] > 0)\n        ]", "category": "Python"}, {"instruction": "def get_resource_retriever(url):\n    \"\"\"\n    Get the appropriate retriever object for the specified url based on url scheme.\n    Makes assumption that HTTP urls do not require any special authorization.\n\n    For HTTP urls: returns HTTPResourceRetriever\n    For s3:// urls returns S3ResourceRetriever\n\n    :param url: url of the resource to be retrieved\n    :return: ResourceRetriever object\n    \"\"\"\n", "input": "", "output": "\n    if url.startswith('http://') or url.startswith('https://'):\n        return HttpResourceRetriever(url)\n    else:\n        raise ValueError('Unsupported scheme in url: %s' % url)", "category": "Python"}, {"instruction": "def pair_strings_sum_formatter(a, b):\n  \"\"\"\n  Formats the sum of a and b.\n\n  Note\n  ----\n  Both inputs are numbers already converted to strings.\n\n  \"\"\"\n", "input": "", "output": "  if b[:1] == \"-\":\n    return \"{0} - {1}\".format(a, b[1:])\n  return \"{0} + {1}\".format(a, b)", "category": "Python"}, {"instruction": "def checkout(self, _hash: str) -> None:\n        \"\"\"\n        Checkout the repo at the speficied commit.\n        BE CAREFUL: this will change the state of the repo, hence it should\n        *not* be used with more than 1 thread.\n\n        :param _hash: commit hash to checkout\n        \"\"\"\n", "input": "", "output": "        with self.lock:\n            self._delete_tmp_branch()\n            self.git.checkout('-f', _hash, b='_PD')", "category": "Python"}, {"instruction": "def combine_lists_reducer(\n        key: str,\n        merged_list: list,\n        component: COMPONENT\n) -> list:\n    \"\"\"\n    Reducer function to combine the lists for the specified key into a\n    single, flat list\n\n    :param key:\n        The key on the COMPONENT instances to operate upon\n    :param merged_list:\n        The accumulated list of values populated by previous calls to this\n        reducer function\n    :param component:\n        The COMPONENT instance from which to append values to the\n        merged_list\n    :return:\n        The updated merged_list with the values for the COMPONENT added\n        onto it\n    \"\"\"\n", "input": "", "output": "\n    merged_list.extend(getattr(component, key))\n    return merged_list", "category": "Python"}, {"instruction": "def crc16_nojit(s, crc=0):\n  \"\"\"CRC16 implementation acording to CCITT standards.\"\"\"\n", "input": "", "output": "  for ch in bytearray(s):  # bytearray's elements are integers in both python 2 and 3\n    crc = ((crc << 8) & 0xFFFF) ^ _crc16_tab[((crc >> 8) & 0xFF) ^ (ch & 0xFF)]\n    crc &= 0xFFFF\n  return crc", "category": "Python"}, {"instruction": "def wait(self, timeout=None):\n        \"\"\"\n        Waits for the client to stop its loop\n        \"\"\"\n", "input": "", "output": "        self.__stopped.wait(timeout)\n        return self.__stopped.is_set()", "category": "Python"}, {"instruction": "def _spec_fft(self, complex_data):\n    '''\n    Calculates the DFT of the complex_data along axis = 1.  This assumes complex_data is a 2D array.\n\n    This uses numpy and the code is straight forward\n    np.fft.fftshift( np.fft.fft(complex_data), 1)\n\n    Note that we automatically shift the FFT frequency bins so that along the frequency axis, \n    \"negative\" frequencies are first, then the central frequency, followed by \"positive\" frequencies.\n    '''\n", "input": "", "output": "    return np.fft.fftshift( np.fft.fft(complex_data), 1)", "category": "Python"}, {"instruction": "def offset(self):\n        \"\"\"\n        The offset of the event data object.\n\n        :rtype: ~azure.eventhub.common.Offset\n        \"\"\"\n", "input": "", "output": "        try:\n            return Offset(self._annotations[EventData.PROP_OFFSET].decode('UTF-8'))\n        except (KeyError, AttributeError):\n            return None", "category": "Python"}, {"instruction": "def matches(self, filter_props):\n        \"\"\"Check if the filter matches the supplied properties.\"\"\"\n", "input": "", "output": "        if filter_props is None:\n            return False\n\n        found_one = False\n        for key, value in filter_props.items():\n            if key in self.properties and value != self.properties[key]:\n                return False\n            elif key in self.properties and value == self.properties[key]:\n                found_one = True\n\n        return found_one", "category": "Python"}, {"instruction": "def plot_confusion_matrix(cm, title=\"Confusion Matrix\"):\n    \"\"\"Plots a confusion matrix for each subject\n    \"\"\"\n", "input": "", "output": "    import matplotlib.pyplot as plt\n    import math\n    plt.figure()\n    subjects = len(cm)\n    root_subjects = math.sqrt(subjects)\n    cols = math.ceil(root_subjects)\n    rows = math.ceil(subjects/cols)\n    classes = cm[0].shape[0]\n    for subject in range(subjects):\n        plt.subplot(rows, cols, subject+1)\n        plt.imshow(cm[subject], interpolation='nearest', cmap=plt.cm.bone)\n        plt.xticks(np.arange(classes), range(1, classes+1))\n        plt.yticks(np.arange(classes), range(1, classes+1))\n        cbar = plt.colorbar(ticks=[0.0, 1.0], shrink=0.6)\n        cbar.set_clim(0.0, 1.0)\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"True label\")\n        plt.title(\"{0:d}\".format(subject + 1))\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()", "category": "Python"}, {"instruction": "def cli(obj):\n    \"\"\"List API keys.\"\"\"\n", "input": "", "output": "    client = obj['client']\n\n    if obj['output'] == 'json':\n        r = client.http.get('/keys')\n        click.echo(json.dumps(r['keys'], sort_keys=True, indent=4, ensure_ascii=False))\n    else:\n        timezone = obj['timezone']\n        headers = {\n            'id': 'ID', 'key': 'API KEY', 'user': 'USER', 'scopes': 'SCOPES', 'text': 'TEXT',\n            'expireTime': 'EXPIRES', 'count': 'COUNT', 'lastUsedTime': 'LAST USED', 'customer': 'CUSTOMER'\n        }\n        click.echo(tabulate([k.tabular(timezone) for k in client.get_keys()], headers=headers, tablefmt=obj['output']))", "category": "Python"}, {"instruction": "def presigned_put_object(self, bucket_name, object_name,\n                             expires=timedelta(days=7)):\n        \"\"\"\n        Presigns a put object request and provides a url\n\n        Example:\n            from datetime import timedelta\n\n            presignedURL = presigned_put_object('bucket_name',\n                                                'object_name',\n                                                timedelta(days=7))\n            print(presignedURL)\n\n        :param bucket_name: Bucket for the presigned url.\n        :param object_name: Object for which presigned url is generated.\n        :param expires: optional expires argument to specify timedelta.\n           Defaults to 7days.\n        :return: Presigned put object url.\n        \"\"\"\n", "input": "", "output": "\n        return self.presigned_url('PUT',\n                                  bucket_name,\n                                  object_name,\n                                  expires)", "category": "Python"}, {"instruction": "def info(self):\n        \"\"\"Retrieve info about the attribute : name, data type and\n        number of values.\n\n        Args::\n\n          no argument\n\n        Returns::\n\n          3-element tuple holding:\n\n          - attribute name\n          - attribute data type (see constants SDC.xxx)\n          - number of values in the attribute; for a string-valued\n            attribute (data type SDC.CHAR8), the number of values\n            corresponds to the string length\n\n\n        C library equivalent : SDattrinfo\n                                                       \"\"\"\n", "input": "", "output": "        if self._index is None:\n            try:\n                self._index = self._obj.findattr(self._name)\n            except HDF4Error:\n                raise HDF4Error(\"info: cannot convert name to index\")\n        status, self._name, data_type, n_values = \\\n                              _C.SDattrinfo(self._obj._id, self._index)\n        _checkErr('info', status, 'illegal attribute index')\n        return self._name, data_type, n_values", "category": "Python"}, {"instruction": "def show_intro(self):\r\n        \"\"\"Show intro to IPython help\"\"\"\n", "input": "", "output": "        from IPython.core.usage import interactive_usage\r\n        self.main.help.show_rich_text(interactive_usage)", "category": "Python"}, {"instruction": "def expire_key(self, key):\n        '''\n        Expire the key, delete the value, and call the callback function\n        if one is specified.\n\n        Args:\n            key: The ``TimedDict`` key\n        '''\n", "input": "", "output": "        value = self.base_dict[key]\n        del self[key]\n        if self.callback is not None:\n            self.callback(\n                key, value, *self.callback_args, **self.callback_kwargs)", "category": "Python"}, {"instruction": "def execute(self, query, params=None, cursor=None):\n        \"\"\"Execute query in pool.\n\n        Returns future yielding closed cursor.\n        You can get rows, lastrowid, etc from the cursor.\n        :param cursor: cursor class(Cursor, DictCursor. etc.)\n\n        :return: Future of cursor\n        :rtype: Future\n        \"\"\"\n", "input": "", "output": "        conn = yield self._get_conn()\n        try:\n            cur = conn.cursor(cursor)\n            yield cur.execute(query, params)\n            yield cur.close()\n        except:\n            self._close_conn(conn)\n            raise\n        else:\n            self._put_conn(conn)\n        raise Return(cur)", "category": "Python"}, {"instruction": "def computePointing(self, ra_deg, dec_deg, roll_deg, cartesian=False):\n        \"\"\"Compute a pointing model without changing the internal object pointing\"\"\"\n", "input": "", "output": "        # Roll FOV\n        Rrotate = r.rotateInXMat(roll_deg)  # Roll\n\n        # Slew from ra/dec of zero\n        Ra = r.rightAscensionRotationMatrix(ra_deg)\n        Rd = r.declinationRotationMatrix(dec_deg)\n        Rslew = np.dot(Ra, Rd)\n\n        R = np.dot(Rslew, Rrotate)\n\n        slew = self.origin*1\n        for i, row in enumerate(self.origin):\n            slew[i, 3:6] = np.dot(R, row[3:6])\n\n        if cartesian is False:\n            slew = self.getRaDecs(slew)\n        return slew", "category": "Python"}, {"instruction": "def scroll_constrain (self):\n        '''This keeps the scroll region within the screen region.'''\n", "input": "", "output": "\n        if self.scroll_row_start <= 0:\n            self.scroll_row_start = 1\n        if self.scroll_row_end > self.rows:\n            self.scroll_row_end = self.rows", "category": "Python"}, {"instruction": "def snapshots(self):\n        \"\"\"\n        Provides access to snapshot management methods for the given content type.\n\n        API reference: https://www.contentful.com/developers/docs/references/content-management-api/#/reference/snapshots/content-type-snapshots-collection\n\n        :return: :class:`ContentTypeSnapshotsProxy <contentful_management.content_type_snapshots_proxy.ContentTypeSnapshotsProxy>` object.\n        :rtype: contentful.content_type_snapshots_proxy.ContentTypeSnapshotsProxy\n\n        Usage:\n\n            >>> content_type_snapshots_proxy = content_type.entries()\n            <ContentTypeSnapshotsProxy space_id=\"cfexampleapi\" environment_id=\"master\" content_type_id=\"cat\">\n        \"\"\"\n", "input": "", "output": "        return ContentTypeSnapshotsProxy(self._client, self.space.id, self._environment_id, self.id)", "category": "Python"}, {"instruction": "def _download_files(self, client, flow_id):\n    \"\"\"Download files from the specified flow.\n\n    Args:\n      client: GRR Client object to which to download flow data from.\n      flow_id: GRR flow ID.\n\n    Returns:\n      str: path of downloaded files.\n    \"\"\"\n", "input": "", "output": "    output_file_path = os.path.join(\n        self.output_path, '.'.join((flow_id, 'zip')))\n\n    if os.path.exists(output_file_path):\n      print('{0:s} already exists: Skipping'.format(output_file_path))\n      return None\n\n    flow = client.Flow(flow_id)\n    file_archive = flow.GetFilesArchive()\n    file_archive.WriteToFile(output_file_path)\n\n    # Unzip archive for processing and remove redundant zip\n    fqdn = client.data.os_info.fqdn.lower()\n    client_output_file = os.path.join(self.output_path, fqdn)\n    if not os.path.isdir(client_output_file):\n      os.makedirs(client_output_file)\n\n    with zipfile.ZipFile(output_file_path) as archive:\n      archive.extractall(path=client_output_file)\n    os.remove(output_file_path)\n\n    return client_output_file", "category": "Python"}, {"instruction": "def find_related_modules(package, related_name_re='.+',\n                         ignore_exceptions=False):\n    \"\"\"Find matching modules using a package and a module name pattern.\"\"\"\n", "input": "", "output": "    warnings.warn('find_related_modules has been deprecated.',\n                  DeprecationWarning)\n    package_elements = package.rsplit(\".\", 1)\n    try:\n        if len(package_elements) == 2:\n            pkg = __import__(package_elements[0], globals(), locals(), [\n                             package_elements[1]])\n            pkg = getattr(pkg, package_elements[1])\n        else:\n            pkg = __import__(package_elements[0], globals(), locals(), [])\n        pkg_path = pkg.__path__\n    except AttributeError:\n        return []\n\n    # Find all modules named according to related_name\n    p = re.compile(related_name_re)\n    modules = []\n\n    for name in find_modules(package, include_packages=True):\n        if p.match(name.split('.')[-1]):\n            try:\n                modules.append(import_string(name, silent=ignore_exceptions))\n            except Exception as e:\n                if not ignore_exceptions:\n                    raise e\n\n    return modules", "category": "Python"}, {"instruction": "def label(self, name):\n        \"\"\"Get the label specified by ``name``\n\n        :param str name: (required), name of the label\n        :returns: :class:`Label <github3.issues.label.Label>` if successful,\n            else None\n        \"\"\"\n", "input": "", "output": "        json = None\n        if name:\n            url = self._build_url('labels', name, base_url=self._api)\n            json = self._json(self._get(url), 200)\n        return Label(json, self) if json else None", "category": "Python"}, {"instruction": "def file_mtime(file_path):\n    \"\"\"\n    Returns the file modified time. This is with regards to the last\n    modification the file has had in the droopescan repo, rather than actual\n    file modification time in the filesystem.\n    @param file_path: file path relative to the executable.\n    @return datetime.datetime object.\n    \"\"\"\n", "input": "", "output": "    if not os.path.isfile(file_path):\n        raise IOError('File \"%s\" does not exist.' % file_path)\n\n    ut = subprocess.check_output(['git', 'log', '-1', '--format=%ct',\n        file_path]).strip()\n\n    return datetime.fromtimestamp(int(ut))", "category": "Python"}, {"instruction": "def dr( self, cell_lengths ):\n        \"\"\"\n        Particle displacement vector for this jump\n\n        Args:\n            cell_lengths (np.array(x,y,z)): Cell lengths for the orthogonal simulation cell.\n\n        Returns\n            (np.array(x,y,z)): dr\n        \"\"\"\n", "input": "", "output": "        half_cell_lengths = cell_lengths / 2.0\n        this_dr = self.final_site.r - self.initial_site.r\n        for i in range( 3 ):\n            if this_dr[ i ] > half_cell_lengths[ i ]:\n                this_dr[ i ] -= cell_lengths[ i ]\n            if this_dr[ i ] < -half_cell_lengths[ i ]:\n                this_dr[ i ] += cell_lengths[ i ]\n        return this_dr", "category": "Python"}, {"instruction": "def delete_async(self, **ctx_options):\n    \"\"\"Schedule deletion of the entity for this Key.\n\n    This returns a Future, whose result becomes available once the\n    deletion is complete.  If no such entity exists, a Future is still\n    returned.  In all cases the Future's result is None (i.e. there is\n    no way to tell whether the entity existed or not).\n    \"\"\"\n", "input": "", "output": "    from . import tasklets, model\n    ctx = tasklets.get_context()\n    cls = model.Model._kind_map.get(self.kind())\n    if cls:\n      cls._pre_delete_hook(self)\n    fut = ctx.delete(self, **ctx_options)\n    if cls:\n      post_hook = cls._post_delete_hook\n      if not cls._is_default_hook(model.Model._default_post_delete_hook,\n                                  post_hook):\n        fut.add_immediate_callback(post_hook, self, fut)\n    return fut", "category": "Python"}, {"instruction": "def lookup(self, module_name):\n        \"\"\"Searches for a file providing given module.\n\n        Returns the normalized module id and path of the file.\n        \"\"\"\n", "input": "", "output": "        for search_path in self._paths:\n            module_path = os.path.join(search_path, module_name)\n            new_module_name, module_file = self._lookup(module_path, module_name)\n            if module_file:\n                return new_module_name, module_file\n        return None, None", "category": "Python"}, {"instruction": "def order_search(self, search):\n        \"\"\"Order given search by the ordering parameter given in request.\n\n        :param search: ElasticSearch query object\n\n        \"\"\"\n", "input": "", "output": "        ordering = self.get_query_param('ordering', self.ordering)\n        if not ordering:\n            return search\n\n        sort_fields = []\n        for raw_ordering in ordering.split(','):\n            ordering_field = raw_ordering.lstrip('-')\n            if ordering_field not in self.ordering_fields:\n                raise ParseError('Ordering by `{}` is not supported.'.format(ordering_field))\n\n            ordering_field = self.ordering_map.get(ordering_field, ordering_field)\n            direction = '-' if raw_ordering[0] == '-' else ''\n            sort_fields.append('{}{}'.format(direction, ordering_field))\n\n        return search.sort(*sort_fields)", "category": "Python"}, {"instruction": "def remove_by_name(self, name, index=0):\n        \"\"\"\n        Remove the child having the given name at the given position\n\n        :type name: ``str``\n        :param name: child name (e.g. PID)\n\n        :type index: ``int``\n        :param index: child index\n\n        :return: an instance of :class:`Element <hl7apy.core.Element>` subclass\n        \"\"\"\n", "input": "", "output": "        child = self.child_at_index(name, index)\n        self.remove(child)\n        return child", "category": "Python"}, {"instruction": "def forward(self, x: torch.Tensor, sublayer: Callable[[torch.Tensor], torch.Tensor]) -> torch.Tensor:\n        \"\"\"Apply residual connection to any sublayer with the same size.\"\"\"\n", "input": "", "output": "        return x + self.dropout(sublayer(self.norm(x)))", "category": "Python"}, {"instruction": "def cmd_map(args):\n    '''map command'''\n", "input": "", "output": "    import mavflightview\n    #mestate.mlog.reduce_by_flightmodes(mestate.flightmode_selections)\n    #setup and process the map\n    options = mavflightview.mavflightview_options()\n    options.condition = mestate.settings.condition\n    options._flightmodes = mestate.mlog._flightmodes\n    options.show_flightmode_legend = mestate.settings.show_flightmode\n    if len(args) > 0:\n        options.types = ','.join(args)\n    [path, wp, fen, used_flightmodes, mav_type] = mavflightview.mavflightview_mav(mestate.mlog, options, mestate.flightmode_selections)\n    child = multiproc.Process(target=mavflightview.mavflightview_show, args=[path, wp, fen, used_flightmodes, mav_type, options])\n    child.start()\n    mestate.mlog.rewind()", "category": "Python"}, {"instruction": "def cache_page(page_cache, page_hash, cache_size):\n    \"\"\"Add a page to the page cache.\"\"\"\n", "input": "", "output": "    page_cache.append(page_hash)\n    if len(page_cache) > cache_size:\n        page_cache.pop(0)", "category": "Python"}, {"instruction": "def cli(ctx, config, quiet):\n    \"\"\"AWS ECS Docker Deployment Tool\"\"\"\n", "input": "", "output": "    ctx.obj = {}\n    ctx.obj['config'] = load_config(config.read())  # yaml.load(config.read())\n    ctx.obj['quiet'] = quiet\n    log(ctx, ' * ' + rnd_scotty_quote() + ' * ')", "category": "Python"}, {"instruction": "def _quotient_exponent(x, y):\n    \"\"\"\n    Given two positive finite MPFR instances x and y,\n    find the exponent of x / y; that is, the unique\n    integer e such that 2**(e-1) <= x / y < 2**e.\n\n    \"\"\"\n", "input": "", "output": "    assert mpfr.mpfr_regular_p(x)\n    assert mpfr.mpfr_regular_p(y)\n\n    # Make copy of x with the exponent of y.\n    x2 = mpfr.Mpfr_t()\n    mpfr.mpfr_init2(x2, mpfr.mpfr_get_prec(x))\n    mpfr.mpfr_set(x2, x, mpfr.MPFR_RNDN)\n    mpfr.mpfr_set_exp(x2, mpfr.mpfr_get_exp(y))\n\n    # Compare x2 and y, disregarding the sign.\n    extra = mpfr.mpfr_cmpabs(x2, y) >= 0\n    return extra + mpfr.mpfr_get_exp(x) - mpfr.mpfr_get_exp(y)", "category": "Python"}, {"instruction": "def dateTimeToString(dateTime, convertToUTC=False):\n    \"\"\"\n    Ignore tzinfo unless convertToUTC.  Output string.\n    \"\"\"\n", "input": "", "output": "    if dateTime.tzinfo and convertToUTC:\n        dateTime = dateTime.astimezone(utc)\n\n    datestr = \"{}{}{}T{}{}{}\".format(\n        numToDigits(dateTime.year, 4),\n        numToDigits(dateTime.month, 2),\n        numToDigits(dateTime.day, 2),\n        numToDigits(dateTime.hour, 2),\n        numToDigits(dateTime.minute, 2),\n        numToDigits(dateTime.second, 2),\n    )\n    if tzinfo_eq(dateTime.tzinfo, utc):\n        datestr += \"Z\"\n    return datestr", "category": "Python"}, {"instruction": "def get_files(self, path):\n        \"\"\"\n        Yields full path and filename for each file that exists in the given directory. Will\n        ignore hidden files (that start with a \".\") and directories\n        :param path: Directory or filename\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        if os.path.isfile(path):\n            self.logger.debug('Called with single file as target: {0}'.format(path))\n            yield path\n            return\n\n        self.logger.debug('Getting list of files in {0}'.format(path))\n\n        try:\n            for f in os.listdir(path):\n                cur_file = os.path.join(path, f)\n                if f[0] != '.' and os.path.isfile(cur_file):\n                    yield cur_file\n        except OSError:\n            self.logger.error('Could not read files from {0}'.format(path))\n            raise", "category": "Python"}, {"instruction": "def getTransactionByBlock(self, block_identifier, transaction_index):\n        \"\"\"\n        `eth_getTransactionByBlockHashAndIndex`\n        `eth_getTransactionByBlockNumberAndIndex`\n        \"\"\"\n", "input": "", "output": "        method = select_method_for_block_identifier(\n            block_identifier,\n            if_predefined='eth_getTransactionByBlockNumberAndIndex',\n            if_hash='eth_getTransactionByBlockHashAndIndex',\n            if_number='eth_getTransactionByBlockNumberAndIndex',\n        )\n        result = self.web3.manager.request_blocking(\n            method,\n            [block_identifier, transaction_index],\n        )\n        if result is None:\n            raise TransactionNotFound(\n                f\"Transaction index: {transaction_index} \"\n                f\"on block id: {block_identifier} not found.\"\n            )\n        return result", "category": "Python"}, {"instruction": "def store(self, value, ptr, align=None):\n        \"\"\"\n        Store value to pointer, with optional guaranteed alignment:\n            *ptr = name\n        \"\"\"\n", "input": "", "output": "        if not isinstance(ptr.type, types.PointerType):\n            raise TypeError(\"cannot store to value of type %s (%r): not a pointer\"\n                            % (ptr.type, str(ptr)))\n        if ptr.type.pointee != value.type:\n            raise TypeError(\"cannot store %s to %s: mismatching types\"\n                            % (value.type, ptr.type))\n        st = instructions.StoreInstr(self.block, value, ptr)\n        st.align = align\n        self._insert(st)\n        return st", "category": "Python"}, {"instruction": "def key_to_path(self, key):\n        \"\"\"Return the fullpath to the file with sha1sum key.\"\"\"\n", "input": "", "output": "        return os.path.join(self.cache_dir, key[:2], key[2:4],\n                            key[4:] + '.pkl')", "category": "Python"}, {"instruction": "def get_attachment_data(self, attachment):\n        \"\"\"Attachments data\n        \"\"\"\n", "input": "", "output": "        f = attachment.getAttachmentFile()\n        attachment_type = attachment.getAttachmentType()\n        attachment_keys = attachment.getAttachmentKeys()\n        filename = f.filename\n        filesize = self.get_filesize(f)\n        mimetype = f.getContentType()\n        report_option = attachment.getReportOption()\n\n        return {\n            \"obj\": attachment,\n            \"attachment_type\": attachment_type,\n            \"attachment_keys\": attachment_keys,\n            \"file\": f,\n            \"uid\": api.get_uid(attachment),\n            \"filesize\": filesize,\n            \"filename\": filename,\n            \"mimetype\": mimetype,\n            \"report_option\": report_option,\n        }", "category": "Python"}, {"instruction": "def train(X_train, X_test, y_train, y_test, **kwargs):\n    '''\n    >>> corpus = CorpusReader('annot.opcorpora.xml')\n    >>> X_train, x_test, y_train, y_test = get_train_data(corpus, test_size=0.33, random_state=42)\n    >>> crf = train(X_train, X_test, y_train, y_test)\n    '''\n", "input": "", "output": "    crf = Trainer()\n    crf.set_params({\n        'c1': 1.0,\n        'c2': 0.001,\n        'max_iterations': 200,\n    })\n\n    for xseq, yseq in zip(X_train, y_train):\n        crf.append(xseq, yseq)\n    crf.train(model_name)\n    return crf", "category": "Python"}, {"instruction": "def set_owner_process(uid, gid):\n    \"\"\" set user and group of workers processes \"\"\"\n", "input": "", "output": "    if gid:\n        try:\n            os.setgid(gid)\n        except OverflowError:\n            # versions of python < 2.6.2 don't manage unsigned int for\n            # groups like on osx or fedora\n            os.setgid(-ctypes.c_int(-gid).value)\n    if uid:\n        os.setuid(uid)", "category": "Python"}, {"instruction": "def on_put(self, request, response, txn_id=None):\n        \"\"\"Responds to PUT request containing events.\"\"\"\n", "input": "", "output": "        response.body = \"{}\"\n\n        # Check whether repeat txn_id\n        if not self._is_new(txn_id):\n            response.status = falcon.HTTP_200\n            return\n\n        request.context[\"body\"] = request.stream.read()\n        try:\n            events = json.loads(request.context[\"body\"].decode(\"utf-8\"))[\"events\"]\n        except(KeyError, ValueError, UnicodeDecodeError):\n            response.status = falcon.HTTP_400\n            response.body = \"Malformed request body\"\n            return\n\n        if self.handler(EventStream(events, self.Api)):\n            response.status = falcon.HTTP_200\n        else:\n            response.status = falcon.HTTP_400", "category": "Python"}, {"instruction": "def _macaroons_for_domain(cookies, domain):\n    '''Return any macaroons from the given cookie jar that\n    apply to the given domain name.'''\n", "input": "", "output": "    req = urllib.request.Request('https://' + domain + '/')\n    cookies.add_cookie_header(req)\n    return httpbakery.extract_macaroons(req)", "category": "Python"}, {"instruction": "def update_datafeed(self, datafeed_id, body, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/ml-update-datafeed.html>`_\n\n        :arg datafeed_id: The ID of the datafeed to update\n        :arg body: The datafeed update settings\n        \"\"\"\n", "input": "", "output": "        for param in (datafeed_id, body):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"POST\",\n            _make_path(\"_ml\", \"datafeeds\", datafeed_id, \"_update\"),\n            params=params,\n            body=body,\n        )", "category": "Python"}, {"instruction": "def _unassembled_reads1_out_file_name(self):\n        \"\"\"Checks file name is set for reads1 output.\n           Returns absolute path.\"\"\"\n", "input": "", "output": "        if self.Parameters['-1'].isOn():\n            unassembled_reads1 = self._absolute(\n                str(self.Parameters['-1'].Value))\n        else:\n            raise ValueError(\"No reads1 (flag: -1) output path specified\")\n        return unassembled_reads1", "category": "Python"}, {"instruction": "def represented_args(args, separator=\" \"):\n    \"\"\"\n    Args:\n        args (list | tuple | None): Arguments to represent\n        separator (str | unicode): Separator to use\n\n    Returns:\n        (str): Quoted as needed textual representation\n    \"\"\"\n", "input": "", "output": "    result = []\n    if args:\n        for text in args:\n            result.append(quoted(short(text)))\n    return separator.join(result)", "category": "Python"}, {"instruction": "def convert_pdf_to_txt(pdf, startpage=None):\n    \"\"\"Convert a pdf file to text and return the text.\n\n    This method requires pdftotext to be installed.\n\n    Parameters\n    ----------\n    pdf : str\n        path to pdf file\n    startpage : int, optional\n        the first page we try to convert\n\n    Returns\n    -------\n    str\n        the converted text\n\n    \"\"\"\n", "input": "", "output": "    if startpage is not None:\n        startpageargs = ['-f', str(startpage)]\n    else:\n        startpageargs = []\n    stdout = subprocess.Popen([\"pdftotext\", \"-q\"] + startpageargs + [pdf, \"-\"],\n                              stdout=subprocess.PIPE).communicate()[0]\n    # python2 and 3\n    if not isinstance(stdout, str):\n        stdout = stdout.decode()\n    return stdout", "category": "Python"}, {"instruction": "def clear(self):\n        \"\"\"Clear ErrorMessage.\n        \"\"\"\n", "input": "", "output": "        self.problems = []\n        self.details = []\n        self.suggestions = []\n        self.tracebacks = []", "category": "Python"}, {"instruction": "def remove_entry(self, offset, length):\n        # type: (int, int) -> None\n        '''\n        Given an offset and length, find and remove the entry in this block\n        that corresponds.\n\n        Parameters:\n         offset - The offset of the entry to look for.\n         length - The length of the entry to look for.\n        Returns:\n         Nothing.\n        '''\n", "input": "", "output": "        for index, entry in enumerate(self._entries):\n            if entry.offset == offset and entry.length == length:\n                del self._entries[index]\n                break\n        else:\n            raise pycdlibexception.PyCdlibInternalError('Could not find an entry for the RR CE entry in the CE block!')", "category": "Python"}, {"instruction": "def load_remote_project_archive(self, project_zip_path):\n        \"\"\"\n        Puts the project files from S3 in /tmp and adds to path\n        \"\"\"\n", "input": "", "output": "        project_folder = '/tmp/{0!s}'.format(self.settings.PROJECT_NAME)\n        if not os.path.isdir(project_folder):\n            # The project folder doesn't exist in this cold lambda, get it from S3\n            if not self.session:\n                boto_session = boto3.Session()\n            else:\n                boto_session = self.session\n\n            # Download zip file from S3\n            remote_bucket, remote_file = parse_s3_url(project_zip_path)\n            s3 = boto_session.resource('s3')\n            archive_on_s3 = s3.Object(remote_bucket, remote_file).get()\n\n            with tarfile.open(fileobj=archive_on_s3['Body'], mode=\"r|gz\") as t:\n                t.extractall(project_folder)\n\n        # Add to project path\n        sys.path.insert(0, project_folder)\n\n        # Change working directory to project folder\n        # Related: https://github.com/Miserlou/Zappa/issues/702\n        os.chdir(project_folder)\n        return True", "category": "Python"}, {"instruction": "def set_stroke(self, width=1, color='black'):\r\n        \"\"\"Sets the stroke properties.\r\n\r\n        Args:\r\n            width (int): stroke width\r\n            color (str): stroke color\r\n        \"\"\"\n", "input": "", "output": "        self.attributes['stroke'] = color\r\n        self.attributes['stroke-width'] = str(width)", "category": "Python"}, {"instruction": "def db_uri(self):\n        \"\"\"The connection URL for the remote database. For example:\n        postgres://some-long-uid@ec2-52-7-232-59.compute-1.amazonaws.com:5432/d5fou154it1nvt\n        \"\"\"\n", "input": "", "output": "        output = self.get(\"DATABASE\", subcommand=\"pg:credentials:url\")\n        match = re.search(\"(postgres://.*)$\", output)\n        if match is None:\n            raise NameError(\n                \"Could not retrieve the DB URI. Check for error output from \"\n                \"heroku above the stack trace.\"\n            )\n        return match.group(1)", "category": "Python"}, {"instruction": "def _fully_connected(self, x, out_dim):\n    \"\"\"FullyConnected layer for final output.\"\"\"\n", "input": "", "output": "    if self.init_layers:\n      fc = LinearnGPU(out_dim, w_name='DW')\n      fc.name = 'logits'\n      self.layers += [fc]\n    else:\n      fc = self.layers[self.layer_idx]\n      self.layer_idx += 1\n    fc.device_name = self.device_name\n    fc.set_training(self.training)\n    return fc.fprop(x)", "category": "Python"}, {"instruction": "def _rd_fld_vals(name, val, set_list_ft=True, qty_min=0, qty_max=None):\n        \"\"\"Further split a GPAD value within a single field.\"\"\"\n", "input": "", "output": "        if not val and qty_min == 0:\n            return [] if set_list_ft else set()\n        vals = val.split('|') # Use a pipe to separate entries\n        num_vals = len(vals)\n        assert num_vals >= qty_min, \\\n            \"FLD({F}): MIN QUANTITY({Q}) WASN'T MET: {V}\".format(\n                F=name, Q=qty_min, V=vals)\n        if qty_max is not None:\n            assert num_vals <= qty_max, \\\n                \"FLD({F}): MAX QUANTITY({Q}) EXCEEDED: {V}\".format(\n                    F=name, Q=qty_max, V=vals)\n        return vals if set_list_ft else set(vals)", "category": "Python"}, {"instruction": "def _raveled_index_for_transformed(self, param):\n        \"\"\"\n        get the raveled index for a param for the transformed parameter array\n        (optimizer array).\n\n        that is an int array, containing the indexes for the flattened\n        param inside this parameterized logic.\n\n        !Warning! be sure to call this method on the highest parent of a hierarchy,\n        as it uses the fixes to do its work. If you do not know\n        what you are doing, do not use this method, it will have\n        unexpected returns!\n        \"\"\"\n", "input": "", "output": "        ravi = self._raveled_index_for(param)\n        if self._has_fixes():\n            fixes = self._fixes_\n            ### Transformed indices, handling the offsets of previous fixes\n            transformed = (np.r_[:self.size] - (~fixes).cumsum())\n            return transformed[ravi[fixes[ravi]]]\n        else:\n            return ravi", "category": "Python"}, {"instruction": "def gopro_set_response_send(self, cmd_id, status, force_mavlink1=False):\n                '''\n                Response from a GOPRO_COMMAND set request\n\n                cmd_id                    : Command ID (uint8_t)\n                status                    : Status (uint8_t)\n\n                '''\n", "input": "", "output": "                return self.send(self.gopro_set_response_encode(cmd_id, status), force_mavlink1=force_mavlink1)", "category": "Python"}, {"instruction": "def write(self, location=None):\n        \"\"\"\n        Write file to I/O backend.\n        \"\"\"\n", "input": "", "output": "        # Take location and expand tilde.\n        if location is not None:\n            self.location = location\n        assert self.location\n\n        # Find I/O backend that handles this location.\n        for io in self.editor.io_backends:\n            if io.can_open_location(self.location):\n                break\n        else:\n            self.editor.show_message('Unknown location: %r' % location)\n\n        # Write it.\n        try:\n            io.write(self.location, self.buffer.text + '\\n', self.encoding)\n            self.is_new = False\n        except Exception as e:\n            # E.g. \"No such file or directory.\"\n            self.editor.show_message('%s' % e)\n        else:\n            # When the save succeeds: update: _file_content.\n            self._file_content = self.buffer.text", "category": "Python"}, {"instruction": "def method(func):\n    \"\"\"Wrap a function as a method.\"\"\"\n", "input": "", "output": "    attr = abc.abstractmethod(func)\n    attr.__imethod__ = True\n    return attr", "category": "Python"}, {"instruction": "def send_ether_over_wpa(self, pkt, **kwargs):\n        \"\"\"Send an Ethernet packet using the WPA channel\n        Extra arguments will be ignored, and are just left for compatibility\n        \"\"\"\n", "input": "", "output": "\n        payload = LLC() / SNAP() / pkt[Ether].payload\n        dest = pkt.dst\n        if dest == \"ff:ff:ff:ff:ff:ff\":\n            self.send_wpa_to_group(payload, dest)\n        else:\n            assert dest == self.client\n            self.send_wpa_to_client(payload)", "category": "Python"}, {"instruction": "def preprocess(train_dataset, output_dir, eval_dataset=None, checkpoint=None, cloud=None):\n  \"\"\"Blocking version of preprocess_async(). The only difference is that it blocks the caller\n     until the job finishes, and it does not have a return value.\n  \"\"\"\n", "input": "", "output": "  with warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    job = preprocess_async(train_dataset, output_dir, eval_dataset, checkpoint, cloud)\n    job.wait()\n    print(job.state)", "category": "Python"}, {"instruction": "def edge_to_bel(u: BaseEntity, v: BaseEntity, data: EdgeData, sep: Optional[str] = None) -> str:\n    \"\"\"Take two nodes and gives back a BEL string representing the statement.\n\n    :param u: The edge's source's PyBEL node data dictionary\n    :param v: The edge's target's PyBEL node data dictionary\n    :param data: The edge's data dictionary\n    :param sep: The separator between the source, relation, and target. Defaults to ' '\n    \"\"\"\n", "input": "", "output": "    sep = sep or ' '\n    u_str = _decanonicalize_edge_node(u, data, node_position=SUBJECT)\n    v_str = _decanonicalize_edge_node(v, data, node_position=OBJECT)\n\n    return sep.join((u_str, data[RELATION], v_str))", "category": "Python"}, {"instruction": "def lowerbound(self, axis=0):\n        \"\"\"\n        Get the lower bound of the binning along an axis\n        \"\"\"\n", "input": "", "output": "        if not 0 <= axis < self.GetDimension():\n            raise ValueError(\n                \"axis must be a non-negative integer less than \"\n                \"the dimensionality of the histogram\")\n        if axis == 0:\n            return self.xedges(1)\n        if axis == 1:\n            return self.yedges(1)\n        if axis == 2:\n            return self.zedges(1)\n        raise TypeError(\"axis must be an integer\")", "category": "Python"}, {"instruction": "def add_update_resources(self, resources, ignore_datasetid=False):\n        # type: (List[Union[hdx.data.resource.Resource,Dict,str]], bool) -> None\n        \"\"\"Add new or update existing resources with new metadata to the dataset\n\n        Args:\n            resources (List[Union[hdx.data.resource.Resource,Dict,str]]): A list of either resource ids or resources metadata from either Resource objects or dictionaries\n            ignore_datasetid (bool): Whether to ignore dataset id in the resource. Defaults to False.\n\n        Returns:\n            None\n        \"\"\"\n", "input": "", "output": "        if not isinstance(resources, list):\n            raise HDXError('Resources should be a list!')\n        for resource in resources:\n            self.add_update_resource(resource, ignore_datasetid)", "category": "Python"}, {"instruction": "def tree(X, n, alpha=0, symbolic=False):\n    \"\"\"Recurrence coefficients for generalized Laguerre polynomials. Set\n    alpha=0 (default) to get classical Laguerre.\n    \"\"\"\n", "input": "", "output": "    args = recurrence_coefficients(n, alpha=alpha, symbolic=symbolic)\n    return line_tree(X, *args)", "category": "Python"}, {"instruction": "def last(self):\n        \"\"\"\n        Returns another NdLayout constituted of the last views of the\n        individual elements (if they are maps).\n        \"\"\"\n", "input": "", "output": "        last_items = []\n        for (k, v) in self.items():\n            if isinstance(v, NdMapping):\n                item = (k, v.clone((v.last_key, v.last)))\n            elif isinstance(v, AdjointLayout):\n                item = (k, v.last)\n            else:\n                item = (k, v)\n            last_items.append(item)\n        return self.clone(last_items)", "category": "Python"}, {"instruction": "def set_output_port(self, new_value, old_value=0):\n        \"\"\"Sets the output port value to new_value, defaults to old_value.\"\"\"\n", "input": "", "output": "        print(\"Setting output port to {}.\".format(new_value))\n        port_value = old_value\n        try:\n            port_value = int(new_value)  # dec\n        except ValueError:\n            port_value = int(new_value, 16)  # hex\n        finally:\n            self.pifacedigital.output_port.value = port_value\n            return port_value", "category": "Python"}, {"instruction": "def set_property(self, prop, value):\n        \"\"\"Change value of a DAAP property, e.g. volume or media position.\"\"\"\n", "input": "", "output": "        cmd_url = 'ctrl-int/1/setproperty?{}={}&[AUTH]'.format(\n            prop, value)\n        return self.daap.post(cmd_url)", "category": "Python"}, {"instruction": "def delete(\n            self,\n            alias,\n            uri,\n            data=(),\n            headers=None,\n            allow_redirects=None,\n            timeout=None):\n        \"\"\" * * *   Deprecated- See Delete Request now   * * *\n\n        Send a DELETE request on the session object found using the\n        given `alias`\n\n        ``alias`` that will be used to identify the Session object in the cache\n\n        ``uri`` to send the DELETE request to\n\n        ``headers`` a dictionary of headers to use with the request\n\n        ``allow_redirects`` Boolean. Set to True if POST/PUT/DELETE redirect following is allowed.\n\n        ``timeout`` connection timeout\n        \"\"\"\n", "input": "", "output": "        logger.warn(\"Deprecation Warning: Use Delete Request in the future\")\n        session = self._cache.switch(alias)\n        data = self._utf8_urlencode(data)\n        redir = True if allow_redirects is None else allow_redirects\n\n        response = self._delete_request(\n            session, uri, data, json, None, headers, redir, timeout)\n\n        return response", "category": "Python"}, {"instruction": "def pexpire(self, key, milliseconds):\n        \"\"\"Emulate pexpire\"\"\"\n", "input": "", "output": "        return self._expire(self._encode(key), timedelta(milliseconds=milliseconds))", "category": "Python"}, {"instruction": "def get_float(self, key, default=None):\n        \"\"\"\n        Same as :meth:`dict.get`, but the value is converted to a float.\n        \"\"\"\n", "input": "", "output": "        value = self.get(key, default)\n        return None if value is None else float(value)", "category": "Python"}, {"instruction": "def subdivide(self):\n        \"\"\"Split the curve into a left and right half.\n\n        See :meth:`.Curve.subdivide` for more information.\n\n        Returns:\n            Tuple[SubdividedCurve, SubdividedCurve]: The left and right\n            sub-curves.\n        \"\"\"\n", "input": "", "output": "        left_nodes, right_nodes = _curve_helpers.subdivide_nodes(self.nodes)\n        midpoint = 0.5 * (self.start + self.end)\n        left = SubdividedCurve(\n            left_nodes, self.original_nodes, start=self.start, end=midpoint\n        )\n        right = SubdividedCurve(\n            right_nodes, self.original_nodes, start=midpoint, end=self.end\n        )\n        return left, right", "category": "Python"}, {"instruction": "def store_edges(self, edges):\n        \"\"\"Store the temporary network edges input file with re-mapped Ids.\"\"\"\n", "input": "", "output": "        with open(self.get_path(OslomRunner.TMP_EDGES_FILE), \"w\") as writer:\n            for edge in edges:\n                writer.write(\"{}\\t{}\\t{}\\n\".format(\n                    self.id_remapper.get_int_id(edge[0]),\n                    self.id_remapper.get_int_id(edge[1]),\n                    edge[2]))", "category": "Python"}, {"instruction": "def _parse(batch_cmd):\n        \"\"\"\n        :rtype:   (sh_cmd, batch_to_file_s, batch_from_file)\n        :returns: parsed result like below:\n\n        .. code-block:: python\n\n            # when parsing 'diff IN_BATCH0 IN_BATCH1 > OUT_BATCH'\n            (\n                'diff /tmp/relshell-AbCDeF /tmp/relshell-uVwXyz',\n                ( <instance of BatchToFile>, <instance of BatchToFile> )    # (IN_BATCH0, IN_BATCH1)\n                'STDOUT',\n            )\n        \"\"\"\n", "input": "", "output": "        cmd_array                    = shlex.split(batch_cmd)\n        (cmd_array, batch_to_file_s) = BatchCommand._parse_in_batches(cmd_array)\n        (cmd_array, batch_from_file) = BatchCommand._parse_out_batch(cmd_array)\n        return (list2cmdline(cmd_array), batch_to_file_s, batch_from_file)", "category": "Python"}, {"instruction": "def instance_cache(cls, func):\n        \"\"\" Save the cache to `self`\n\n        This decorator take it for granted that the decorated function\n        is a method.  The first argument of the function is `self`.\n\n        :param func: function to decorate\n        :return: the decorator\n        \"\"\"\n", "input": "", "output": "\n        @functools.wraps(func)\n        def func_wrapper(*args, **kwargs):\n            if not args:\n                raise ValueError('`self` is not available.')\n            else:\n                the_self = args[0]\n            func_key = cls.get_key(func)\n            val_cache = cls.get_self_cache(the_self, func_key)\n            lock = cls.get_self_cache_lock(the_self, func_key)\n\n            return cls._get_value_from_cache(\n                func, val_cache, lock, *args, **kwargs)\n\n        return func_wrapper", "category": "Python"}, {"instruction": "def start_instance(self, instance):\n        \"\"\"\n        Starts a single instance.\n\n        :param str instance: A Yamcs instance name.\n        \"\"\"\n", "input": "", "output": "        params = {'state': 'running'}\n        url = '/instances/{}'.format(instance)\n        self.patch_proto(url, params=params)", "category": "Python"}, {"instruction": "def as_flat_dict(self):\n        \"\"\"\n        Returns the parameters of a flat dictionary from keys to values.\n        Nested structure is collapsed with periods.\n        \"\"\"\n", "input": "", "output": "        flat_params = {}\n        def recurse(parameters, path):\n            for key, value in parameters.items():\n                newpath = path + [key]\n                if isinstance(value, dict):\n                    recurse(value, newpath)\n                else:\n                    flat_params['.'.join(newpath)] = value\n\n        recurse(self.params, [])\n        return flat_params", "category": "Python"}, {"instruction": "def truncate_weights(weights, min_weight=0.01, rescale=True):\n    \"\"\"\n    Truncates small weight vectors, i.e. sets weights below a treshold to zero.\n    This can be helpful to remove portfolio weights, which are negligibly small.\n    \n    Parameters\n    ----------\n    weights: pandas.Series\n        Optimal asset weights.\n    min_weight: float, optional\n        All weights, for which the absolute value is smaller\n        than this parameter will be set to zero.\n    rescale: boolean, optional\n        If 'True', rescale weights so that weights.sum() == 1.\n        If 'False', do not rescale.\n\n    Returns\n    -------\n    adj_weights: pandas.Series\n        Adjusted weights.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(weights, pd.Series):\n        raise ValueError(\"Weight vector is not a Series\")\n\n    adj_weights = weights[:]\n    adj_weights[adj_weights.abs() < min_weight] = 0.0\n\n    if rescale:\n        if not adj_weights.sum():\n            raise ValueError(\"Cannot rescale weight vector as sum is not finite\")\n        \n        adj_weights /= adj_weights.sum()\n\n    return adj_weights", "category": "Python"}, {"instruction": "def get_timeseries(self, *args, **kwargs):\n        \"\"\"\n        Returns an instance of the Time Series Service.\n        \"\"\"\n", "input": "", "output": "        import predix.data.timeseries\n        ts = predix.data.timeseries.TimeSeries(*args, **kwargs)\n        return ts", "category": "Python"}, {"instruction": "def result(self, s, a):\n        '''Result of applying an action to a state.'''\n", "input": "", "output": "        # result: boat on opposite side, and numbers of missioners and\n        # cannibals updated according to the move\n        if s[2] == 0:\n            return (s[0] - a[1][0], s[1] - a[1][1], 1)\n        else:\n            return (s[0] + a[1][0], s[1] + a[1][1], 0)", "category": "Python"}, {"instruction": "def get_relationship_form(self, *args, **kwargs):\n        \"\"\"Pass through to provider RelationshipAdminSession.get_relationship_form_for_update\"\"\"\n", "input": "", "output": "        # Implemented from kitosid template for -\n        # osid.resource.ResourceAdminSession.get_resource_form_for_update\n        # This method might be a bit sketchy. Time will tell.\n        if isinstance(args[-1], list) or 'relationship_record_types' in kwargs:\n            return self.get_relationship_form_for_create(*args, **kwargs)\n        else:\n            return self.get_relationship_form_for_update(*args, **kwargs)", "category": "Python"}, {"instruction": "def set_centralized_assembled_rows_cols(self, irn, jcn):\n        \"\"\"Set assembled matrix indices on processor 0.\n\n        The row and column indices (irn & jcn) should be one based.\n        \"\"\"\n", "input": "", "output": "        if self.myid != 0:\n            return\n        assert irn.size == jcn.size\n        self._refs.update(irn=irn, jcn=jcn)\n        self.id.nz = irn.size\n        self.id.irn = self.cast_array(irn)\n        self.id.jcn = self.cast_array(jcn)", "category": "Python"}, {"instruction": "def set_item_class_name(cls_obj):\n    \"\"\"\n    Return the first part of the class name of this custom generator.\n    This will be used for the class name of the items produced by this\n    generator.\n\n    Examples:\n        FoobarGenerator -> Foobar\n        QuuxGenerator   -> Quux\n    \"\"\"\n", "input": "", "output": "    if '__tohu__items__name__' in cls_obj.__dict__:\n        logger.debug(f\"Using item class name '{cls_obj.__tohu_items_name__}' (derived from attribute '__tohu_items_name__')\")\n    else:\n        m = re.match('^(.*)Generator$', cls_obj.__name__)\n        if m is not None:\n            cls_obj.__tohu_items_name__ = m.group(1)\n            logger.debug(f\"Using item class name '{cls_obj.__tohu_items_name__}' (derived from custom generator name)\")\n        else:\n            raise ValueError(\"Cannot derive class name for items to be produced by custom generator. \"\n                             \"Please set '__tohu_items_name__' at the top of the custom generator's \"\n                             \"definition or change its name so that it ends in '...Generator'\")", "category": "Python"}, {"instruction": "def snapshot(self):\n        \"\"\"\n        This is used mostly as a way to keep the cipher state and the seq_num.\n        \"\"\"\n", "input": "", "output": "        snap = connState(connection_end=self.connection_end,\n                         read_or_write=self.row,\n                         seq_num=self.seq_num,\n                         compression_alg=type(self.compression),\n                         ciphersuite=type(self.ciphersuite),\n                         tls_version=self.tls_version)\n        snap.cipher = self.cipher.snapshot()\n        if self.hmac:\n            snap.hmac.key = self.hmac.key\n        return snap", "category": "Python"}, {"instruction": "def decode(self, encoded):\n        \"\"\" Decodes a tensor into a sequence.\n\n        Args:\n            encoded (torch.Tensor): Encoded sequence.\n\n        Returns:\n            str: Sequence decoded from ``encoded``.\n        \"\"\"\n", "input": "", "output": "        encoded = super().decode(encoded)\n        return self.tokenizer.decode([self.itos[index] for index in encoded])", "category": "Python"}, {"instruction": "def parse_operation_type_definition(lexer: Lexer) -> OperationTypeDefinitionNode:\n    \"\"\"OperationTypeDefinition: OperationType : NamedType\"\"\"\n", "input": "", "output": "    start = lexer.token\n    operation = parse_operation_type(lexer)\n    expect_token(lexer, TokenKind.COLON)\n    type_ = parse_named_type(lexer)\n    return OperationTypeDefinitionNode(\n        operation=operation, type=type_, loc=loc(lexer, start)\n    )", "category": "Python"}, {"instruction": "def groups_set_topic(self, room_id, topic, **kwargs):\n        \"\"\"Sets the topic for the private group.\"\"\"\n", "input": "", "output": "        return self.__call_api_post('groups.setTopic', roomId=room_id, topic=topic, kwargs=kwargs)", "category": "Python"}, {"instruction": "def queryTypesDescriptions(self, types):\n        \"\"\"\n        Given a list of types, construct a dictionary such that\n        each key is a type, and each value is the corresponding sObject\n        for that type.\n        \"\"\"\n", "input": "", "output": "        types = list(types)\n        if types:\n            types_descs = self.describeSObjects(types)\n        else:\n            types_descs = []\n        return dict(map(lambda t, d: (t, d), types, types_descs))", "category": "Python"}, {"instruction": "def getAnalysisServiceSettings(self, uid):\n        \"\"\"Returns a dictionary with the settings for the analysis service that\n           match with the uid provided.\n\n        If there are no settings for the analysis service and template, returns\n        a dictionary with the key 'uid'\n        \"\"\"\n", "input": "", "output": "        settings = self.getAnalysisServicesSettings()\n        sets = [s for s in settings if s.get(\"uid\", \"\") == uid]\n        return sets[0] if sets else {\"uid\": uid}", "category": "Python"}, {"instruction": "def send_immediately(self, message, fail_silently=False):\n        \"\"\"Send a message immediately, outside the transaction manager.\n\n        If there is a connection error to the mail server this will have to\n        be handled manually. However if you pass ``fail_silently`` the error\n        will be swallowed.\n\n        :versionadded: 0.3\n\n        :param message: a 'Message' instance.\n\n        :param fail_silently: silently handle connection errors.\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.smtp_mailer.send(*self._message_args(message))\n        except smtplib.socket.error:\n            if not fail_silently:\n                raise", "category": "Python"}, {"instruction": "def help_version(self):\n        \"\"\"Help and version info\n        \"\"\"\n", "input": "", "output": "        if (len(self.args) == 1 and self.args[0] in [\"-h\", \"--help\"] and\n                self.args[1:] == []):\n            options()\n        elif (len(self.args) == 1 and self.args[0] in [\"-v\", \"--version\"] and\n                self.args[1:] == []):\n            prog_version()\n        else:\n            usage(\"\")", "category": "Python"}, {"instruction": "def strip_tweet(text, remove_url=True):\n    \"\"\"Strip tweet message.\n\n    This method removes mentions strings and urls(optional).\n\n    :param text: tweet message\n    :type text: :class:`str`\n\n    :param remove_url: Remove urls. default :const:`True`.\n    :type remove_url: :class:`boolean`\n\n    :returns: Striped tweet message\n    :rtype: :class:`str`\n\n    \"\"\"\n", "input": "", "output": "    if remove_url:\n        text = url_pattern.sub('', text)\n    else:\n        text = expand_url(text)\n    text = mention_pattern.sub('', text)\n    text = html_parser.unescape(text)\n    text = text.strip()\n    return text", "category": "Python"}, {"instruction": "def _format_bounding_box(bbox, output_format=\"%(lat1)s,%(lon1)s,%(lat2)s,%(lon2)s\"):\n        \"\"\"\n        Transform bounding box boundaries to a string matching\n        `output_format` from the following formats:\n\n            - [Point(lat1, lon1), Point(lat2, lon2)]\n            - [[lat1, lon1], [lat2, lon2]]\n            - [\"lat1,lon1\", \"lat2,lon2\"]\n\n        It is guaranteed that lat1 <= lat2 and lon1 <= lon2.\n        \"\"\"\n", "input": "", "output": "        if len(bbox) != 2:\n            raise GeocoderQueryError(\"Unsupported format for a bounding box\")\n        p1, p2 = bbox\n        p1, p2 = Point(p1), Point(p2)\n        return output_format % dict(lat1=min(p1.latitude, p2.latitude),\n                                    lon1=min(p1.longitude, p2.longitude),\n                                    lat2=max(p1.latitude, p2.latitude),\n                                    lon2=max(p1.longitude, p2.longitude))", "category": "Python"}, {"instruction": "def _combine_sets(self, sets, final_set):\n        \"\"\"\n        Given a list of set, combine them to create the final set that will be\n        used to make the final redis call.\n        \"\"\"\n", "input": "", "output": "        self.cls.get_connection().sinterstore(final_set, list(sets))\n        return final_set", "category": "Python"}, {"instruction": "def continuous_subpaths(self):\n        \"\"\"Breaks self into its continuous components, returning a list of\n        continuous subpaths.\n        I.e.\n        (all(subpath.iscontinuous() for subpath in self.continuous_subpaths())\n         and self == concatpaths(self.continuous_subpaths()))\n        )\n        \"\"\"\n", "input": "", "output": "        subpaths = []\n        subpath_start = 0\n        for i in range(len(self) - 1):\n            if self[i].end != self[(i+1) % len(self)].start:\n                subpaths.append(Path(*self[subpath_start: i+1]))\n                subpath_start = i+1\n        subpaths.append(Path(*self[subpath_start: len(self)]))\n        return subpaths", "category": "Python"}, {"instruction": "def from_file(cls, filename):\n        \"\"\"\n        Reads configuration from given path to a file in local file system and\n        returns parsed version of it.\n\n        :param filename: Path to the YAML file in local file system where the\n                         configuration will be read from.\n        :type filename: str\n\n        :return: Configuration instance parsed from given configuration file.\n        :rtype: Configuration\n        \"\"\"\n", "input": "", "output": "        instance = cls()\n\n        with open(filename, \"rb\") as file_stream:\n            config_data = yaml.load(file_stream)\n\n        instance.load(config_data)\n\n        return instance", "category": "Python"}, {"instruction": "def from_tuples(*tuples):\n        \"\"\"\n        Creates a new DependencyResolver from a list of key-value pairs called tuples\n        where key is dependency name and value the depedency locator (descriptor).\n\n        :param tuples: a list of values where odd elements are dependency name\n        and the following even elements are dependency locator (descriptor)\n\n        :return: a newly created DependencyResolver.\n        \"\"\"\n", "input": "", "output": "        result = DependencyResolver()\n        if tuples == None or len(tuples) == 0:\n            return result\n        \n        index = 0\n        while index < len(tuples):\n            if index + 1 >= len(tuples):\n                break\n\n            name = StringConverter.to_string(tuples[index])\n            locator = tuples[index + 1]\n\n            result.put(name, locator)\n            index = index + 2\n        \n        return result", "category": "Python"}, {"instruction": "def _shutdown(self, cancel, cancel_msg, exc_type=CancelledError):\n        ''' Internal shutdown used by 'shutdown' method  above '''\n", "input": "", "output": "        if cancel:\n            # Cancel all in-flight transfers if requested, before waiting\n            # for them to complete.\n            self._coordinator_controller.cancel(cancel_msg, exc_type)\n        try:\n            # Wait until there are no more in-progress transfers. This is\n            # wrapped in a try statement because this can be interrupted\n            # with a KeyboardInterrupt that needs to be caught.\n            self._coordinator_controller.wait()\n        except KeyboardInterrupt:\n            # If not errors were raised in the try block, the cancel should\n            # have no coordinators it needs to run cancel on. If there was\n            # an error raised in the try statement we want to cancel all of\n            # the inflight transfers before shutting down to speed that\n            # process up.\n            self._coordinator_controller.cancel('KeyboardInterrupt()')\n            raise\n        finally:\n            self._coordinator_controller.cleanup()", "category": "Python"}, {"instruction": "def profile_update(self, profile):\n        \"\"\"Update an existing profile with new parameters or remove deprecated parameters.\n\n        Args:\n            profile (dict): The dictionary containting the profile settings.\n        \"\"\"\n", "input": "", "output": "        # warn about missing install_json parameter\n        if profile.get('install_json') is None:\n            print(\n                '{}{}Missing install_json parameter for profile {}.'.format(\n                    c.Style.BRIGHT, c.Fore.YELLOW, profile.get('profile_name')\n                )\n            )\n\n        # update args section to v2 schema\n        self.profile_update_args_v2(profile)\n\n        # update args section to v3 schema\n        self.profile_update_args_v3(profile)\n\n        # remove legacy script field\n        self.profile_update_schema(profile)", "category": "Python"}, {"instruction": "def add(self, intervention, name=None):\n        \"\"\"\n        Add an intervention to vectorPop section.\n        intervention is either ElementTree or xml snippet\n        \"\"\"\n", "input": "", "output": "        if self.et is None:\n            return\n\n        assert isinstance(intervention, six.string_types)\n        et = ElementTree.fromstring(intervention)\n        vector_pop = VectorPopIntervention(et)\n\n        assert isinstance(vector_pop.name, six.string_types)\n\n        if name is not None:\n            assert isinstance(name, six.string_types)\n            et.attrib[\"name\"] = name\n\n        index = len(self.et.findall(\"intervention\"))\n        self.et.insert(index, et)", "category": "Python"}, {"instruction": "def eigen_table(self):\r\n        \"\"\"Eigenvalues, expl. variance, and cumulative expl. variance.\"\"\"\n", "input": "", "output": "        idx = [\"Eigenvalue\", \"Variability (%)\", \"Cumulative (%)\"]\r\n        table = pd.DataFrame(\r\n            np.array(\r\n                [self.eigenvalues, self.inertia, self.cumulative_inertia]\r\n            ),\r\n            columns=[\"F%s\" % i for i in range(1, self.keep + 1)],\r\n            index=idx,\r\n        )\r\n\r\n        return table", "category": "Python"}, {"instruction": "def _set_location(instance, location):\n    \"\"\"Sets a ``Location`` response header. If the location does not start with\n    a slash, the path of the current request is prepended.\n\n    :param instance: Resource instance (used to access the request and\n                     response)\n    :type instance: :class:`webob.resource.Resource`\n    \"\"\"\n", "input": "", "output": "    location = str(location)\n    if not location.startswith('/'):\n        location = urljoin(instance.request_path.rstrip('/') + '/', location)\n    instance.response.location = location", "category": "Python"}, {"instruction": "def handle_challenge(self,data):\n        \"\"\" Executed when the server requests additional\n            authentication\n        \"\"\"\n", "input": "", "output": "        # Send challenge response\n        self.send_message(AUTHENTICATE(\n            signature = self.password,\n            extra = {}\n        ))", "category": "Python"}, {"instruction": "def get_range_vector(size: int, device: int) -> torch.Tensor:\n    \"\"\"\n    Returns a range vector with the desired size, starting at 0. The CUDA implementation\n    is meant to avoid copy data from CPU to GPU.\n    \"\"\"\n", "input": "", "output": "    if device > -1:\n        return torch.cuda.LongTensor(size, device=device).fill_(1).cumsum(0) - 1\n    else:\n        return torch.arange(0, size, dtype=torch.long)", "category": "Python"}, {"instruction": "def tolocal(self):\n        \"\"\"\n        Convert to local mode.\n        \"\"\"\n", "input": "", "output": "        from thunder.images.readers import fromarray\n\n        if self.mode == 'local':\n            logging.getLogger('thunder').warn('images already in local mode')\n            pass\n\n        return fromarray(self.toarray())", "category": "Python"}, {"instruction": "def get_one(self, fields=list()):\n        \"\"\"Convenience function for queries returning only one result. Validates response before returning.\n\n        :param fields: List of fields to return in the result\n        :raise:\n            :MultipleResults: if more than one match is found\n        :return:\n            - Record content\n        \"\"\"\n", "input": "", "output": "        response = self.session.get(self._get_table_url(),\n                                    params=self._get_formatted_query(fields, limit=None, order_by=list(), offset=None))\n\n        content = self._get_content(response)\n        l = len(content)\n        if l > 1:\n            raise MultipleResults('Multiple results for get_one()')\n\n        if len(content) == 0:\n            return {}\n\n        return content[0]", "category": "Python"}, {"instruction": "def create(self, create_missing=None):\n        \"\"\"Do extra work to fetch a complete set of attributes for this entity.\n\n        For more information, see `Bugzilla #1223540\n        <https://bugzilla.redhat.com/show_bug.cgi?id=1223540>`_.\n\n        \"\"\"\n", "input": "", "output": "        return DockerComputeResource(\n            self._server_config,\n            id=self.create_json(create_missing)['id'],\n        ).read()", "category": "Python"}, {"instruction": "def _combine(self, x, y):\n    \"\"\"Combines two constraints, raising an error if they are not compatible.\"\"\"\n", "input": "", "output": "    if x is None or y is None:\n      return x or y\n    if x != y:\n      raise ValueError('Incompatible set of constraints provided.')\n    return x", "category": "Python"}, {"instruction": "def load(self, steps_dir=None, step_file=None, step_list=None):\n        \"\"\"Load CWL steps into the WorkflowGenerator's steps library.\n\n        Adds steps (command line tools and workflows) to the\n        ``WorkflowGenerator``'s steps library. These steps can be used to\n        create workflows.\n\n        Args:\n            steps_dir (str): path to directory containing CWL files. All CWL in\n                the directory are loaded.\n            step_file (str): path to a file containing a CWL step that will be\n                added to the steps library.\n        \"\"\"\n", "input": "", "output": "        self._closed()\n\n        self.steps_library.load(steps_dir=steps_dir, step_file=step_file,\n                                step_list=step_list)", "category": "Python"}, {"instruction": "def _flatten(self, iterator, **filter_options):\n        '''\n        iterator here gives as lists of tuples. Method flattens the structure\n        to a single list of tuples.\n        '''\n", "input": "", "output": "        resp = list()\n        for entry in iterator:\n            for tup in entry:\n                if self._matches_filter(tup, **filter_options):\n                    resp.append(tup)\n        return resp", "category": "Python"}, {"instruction": "def write_src(hdf5_out, gctoo_object, out_file_name):\n    \"\"\"\n\tWrites src as attribute of gctx out file. \n\n\tInput:\n\t\t- hdf5_out (h5py): hdf5 file to write to \n\t\t- gctoo_object (GCToo): GCToo instance to be written to .gctx\n\t\t- out_file_name (str): name of hdf5 out file. \n\t\"\"\"\n", "input": "", "output": "    if gctoo_object.src == None:\n        hdf5_out.attrs[src_attr] = out_file_name\n    else:\n        hdf5_out.attrs[src_attr] = gctoo_object.src", "category": "Python"}, {"instruction": "def is_lambda(fun):\n    \"\"\"\n    Check whether the given function is a lambda function.\n\n    .. testsetup::\n\n        from proso.func import is_lambda\n\n    .. testcode::\n\n        def not_lambda_fun():\n            return 1\n\n        lambda_fun = lambda: 1\n\n        print(\n            is_lambda(not_lambda_fun),\n            is_lambda(lambda_fun)\n        )\n    .. testoutput::\n\n        False True\n\n    Args:\n        fun (function)\n\n    Returns:\n        bool: True if the given function is a lambda function, False otherwise\n    \"\"\"\n", "input": "", "output": "    return isinstance(fun, type(LAMBDA)) and fun.__name__ == LAMBDA.__name__", "category": "Python"}, {"instruction": "def read (self, files=None):\n        \"\"\"\n        Read settings from given config files.\n\n        @raises: LinkCheckerError on syntax errors in the config file(s)\n        \"\"\"\n", "input": "", "output": "        if files is None:\n            cfiles = []\n        else:\n            cfiles = files[:]\n        if not cfiles:\n            userconf = get_user_config()\n            if os.path.isfile(userconf):\n                cfiles.append(userconf)\n        # filter invalid files\n        filtered_cfiles = []\n        for cfile in cfiles:\n            if not os.path.isfile(cfile):\n                log.warn(LOG_CHECK, _(\"Configuration file %r does not exist.\"), cfile)\n            elif not fileutil.is_readable(cfile):\n                log.warn(LOG_CHECK, _(\"Configuration file %r is not readable.\"), cfile)\n            else:\n                filtered_cfiles.append(cfile)\n        log.debug(LOG_CHECK, \"reading configuration from %s\", filtered_cfiles)\n        confparse.LCConfigParser(self).read(filtered_cfiles)", "category": "Python"}, {"instruction": "def get_jagged_stats(arr_list, **kwargs):\n    r\"\"\"\n    Args:\n        arr_list (list):\n\n    Returns:\n        dict: stats_dict\n\n    CommandLine:\n        python -m utool.util_dev --test-get_jagged_stats\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_dev import *  # NOQA\n        >>> import utool as ut\n        >>> kwargs = dict(use_nan=True)\n        >>> arr_list = [[1, 2, 3, 4], [3, 10], [np.nan, 3, 3, 3]]\n        >>> stats_dict = get_jagged_stats(arr_list, **kwargs)\n        >>> result = ut.align(str(ut.repr4(stats_dict)), ':')\n        >>> print(result)\n        {\n            'mean'   : [2.5, 6.5, 3.0],\n            'std'    : [1.118034, 3.5, 0.0],\n            'max'    : [4.0, 10.0, 3.0],\n            'min'    : [1.0, 3.0, 3.0],\n            'nMin'   : [1, 1, 3],\n            'nMax'   : [1, 1, 3],\n            'shape'  : ['(4,)', '(2,)', '(4,)'],\n            'num_nan': [0, 0, 1],\n        }\n\n    \"\"\"\n", "input": "", "output": "    import functools\n    stats_dict_list = list(map(functools.partial(get_stats, **kwargs), arr_list))\n    stats_dict = util_dict.dict_stack(stats_dict_list)\n    # Fix order\n    stats_dict = util_dict.order_dict_by(stats_dict, STAT_KEY_ORDER)\n    return stats_dict", "category": "Python"}, {"instruction": "def getauthority(self, default=None, encoding='utf-8', errors='strict'):\n        \"\"\"Return the decoded userinfo, host and port subcomponents of the URI\n        authority as a three-item tuple.\n\n        \"\"\"\n", "input": "", "output": "        # TBD: (userinfo, host, port) kwargs, default string?\n        if default is None:\n            default = (None, None, None)\n        elif not isinstance(default, collections.Iterable):\n            raise TypeError('Invalid default type')\n        elif len(default) != 3:\n            raise ValueError('Invalid default length')\n        # TODO: this could be much more efficient by using a dedicated regex\n        return (\n            self.getuserinfo(default[0], encoding, errors),\n            self.gethost(default[1], errors),\n            self.getport(default[2])\n        )", "category": "Python"}, {"instruction": "def _GetMetric(self, metric_name):\n    \"\"\"Fetches the metric object corresponding to the given name.\"\"\"\n", "input": "", "output": "    if metric_name in self._counter_metrics:\n      return self._counter_metrics[metric_name]\n    elif metric_name in self._event_metrics:\n      return self._event_metrics[metric_name]\n    elif metric_name in self._gauge_metrics:\n      return self._gauge_metrics[metric_name]\n    else:\n      raise ValueError(\"Metric %s is not registered.\" % metric_name)", "category": "Python"}, {"instruction": "def BA(self):\n        '''\n        Vertices B and A, list.\n\n        '''\n", "input": "", "output": "        try:\n            return self._BA\n        except AttributeError:\n            pass\n        self._BA = [self.B, self.A]\n        return self._BA", "category": "Python"}, {"instruction": "def resolve(self, component_type, **kwargs):\n        \"\"\"\n        Resolves an instance of the component type.\n        :param component_type: The type of the component (e.g. a class).\n        :param kwargs: Overriding arguments to use (by name) instead of resolving them.\n        :return: An instance of the component.\n        \"\"\"\n", "input": "", "output": "        with self._resolve_lock:\n            context = _ComponentContext(self)\n            return context.resolve(component_type, **kwargs)", "category": "Python"}, {"instruction": "def html_body(input_string, source_path=None, destination_path=None,\n              input_encoding='unicode', doctitle=1, initial_header_level=1):\n    \"\"\"\n    Given an input string, returns an HTML fragment as a string.\n\n    The return value is the contents of the <body> element.\n\n    Parameters (see `html_parts()` for the remainder):\n\n    - `output_encoding`: The desired encoding of the output.  If a Unicode\n      string is desired, use the default value of \"unicode\" .\n    \"\"\"\n", "input": "", "output": "    parts = html_parts(\n        input_string=input_string, source_path=source_path,\n        destination_path=destination_path,\n        input_encoding=input_encoding, doctitle=doctitle,\n        initial_header_level=initial_header_level)\n    fragment = parts['html_body']\n    return fragment", "category": "Python"}, {"instruction": "def value_from_datadict(self, data, files, name):\n        \"\"\"Ensure the payload is a list of values. In the case of a sub\n        form, we need to ensure the data is returned as a list and not a\n        dictionary.\n\n        When a dict is found in the given data, we need to ensure the data\n        is converted to a list perseving the field order.\n\n        \"\"\"\n", "input": "", "output": "        if name in data:\n            payload = data.get(name)\n            if isinstance(payload, (dict,)):\n                # Make sure we get the data in the correct roder\n                return [payload.get(f.name) for f in self.fields]\n            return payload\n        return super(FormFieldWidget, self).value_from_datadict(data, files, name)", "category": "Python"}, {"instruction": "def setup(cmd_args, suppress_output=False):\n    \"\"\" Call a setup.py command or list of commands\n\n    >>> result = setup('--name', suppress_output=True)\n    >>> result.exitval\n    0\n    >>> result = setup('notreal')\n    >>> result.exitval\n    1\n    \"\"\"\n", "input": "", "output": "    if not funcy.is_list(cmd_args) and not funcy.is_tuple(cmd_args):\n        cmd_args = shlex.split(cmd_args)\n    cmd_args = [sys.executable, 'setup.py'] + [x for x in cmd_args]\n    return call(cmd_args, suppress_output=suppress_output)", "category": "Python"}, {"instruction": "def add(self, *args):\n        \"\"\"\n        Add buttons\n\n        :param args:\n        :return: self\n        :rtype: :obj:`types.InlineKeyboardMarkup`\n        \"\"\"\n", "input": "", "output": "        row = []\n        for index, button in enumerate(args, start=1):\n            row.append(button)\n            if index % self.row_width == 0:\n                self.inline_keyboard.append(row)\n                row = []\n        if len(row) > 0:\n            self.inline_keyboard.append(row)\n        return self", "category": "Python"}, {"instruction": "def executePlateBiasAnalysis(options):\n    \"\"\"Execute the plate bias analysis with Plink.\n\n    :param options: the options.\n\n    :type options: argparse.Namespace\n\n    \"\"\"\n", "input": "", "output": "    # The plink command\n    plinkCommand = [\"plink\", \"--noweb\", \"--bfile\", options.bfile,\n                    \"--loop-assoc\", options.loop_assoc, \"--fisher\",\n                    \"--pfilter\", str(options.pfilter), \"--out\", options.out]\n    runCommand(plinkCommand)", "category": "Python"}, {"instruction": "def _entry_must_not_exist(df, k1, k2):\n    \"\"\"Evaluate key-subkey non-existence.\n\n    Checks that the key-subkey combo does not exists in the\n    configuration options.\n    \"\"\"\n", "input": "", "output": "    count = df[(df['k1'] == k1) &\n               (df['k2'] == k2)].shape[0]\n    if count > 0:\n        raise AlreadyRegisteredError(\n            \"Option {0}.{1} already registered\".format(k1, k2))", "category": "Python"}, {"instruction": "def on_message(self, client_conn, msg):\n        \"\"\"Handle message.\n\n        Returns\n        -------\n        ready : Future\n            A future that will resolve once we're ready, else None.\n\n        Notes\n        -----\n        *on_message* should not be called again until *ready* has resolved.\n\n        \"\"\"\n", "input": "", "output": "        MAX_QUEUE_SIZE = 30\n        if len(self._msg_queue) >= MAX_QUEUE_SIZE:\n            # This should never happen if callers to handle_message wait\n            # for its futures to resolve before sending another message.\n            # NM 2014-10-06: Except when there are multiple clients. Oops.\n            raise RuntimeError('MessageHandlerThread unhandled '\n                               'message queue full, not handling message')\n        ready_future = Future()\n        self._msg_queue.append((ready_future, client_conn, msg))\n        self._wake.set()\n        return ready_future", "category": "Python"}, {"instruction": "def calibrate_pressure(self):\n        '''calibrate pressure'''\n", "input": "", "output": "        if self.mavlink10():\n            self.mav.command_long_send(self.target_system, self.target_component,\n                                       mavlink.MAV_CMD_PREFLIGHT_CALIBRATION, 0,\n                                       0, 0, 1, 0, 0, 0, 0)\n        else:\n            MAV_ACTION_CALIBRATE_PRESSURE = 20\n            self.mav.action_send(self.target_system, self.target_component, MAV_ACTION_CALIBRATE_PRESSURE)", "category": "Python"}, {"instruction": "def image(self, raw_url, title='', alt=''):\n        \"\"\"\n        Filters the ``src`` attribute of an image.\n\n        Note that filtering the source URL of an ``<img>`` tag is only a very\n        basic protection, and it's mostly useless in modern browsers (they block\n        JavaScript in there by default). An example of attack that filtering\n        does not thwart is phishing based on HTTP Auth, see `this issue\n        <https://github.com/liberapay/liberapay.com/issues/504>`_ for details.\n\n        To mitigate this issue you should only allow images from trusted services,\n        for example your own image store, or a proxy (see :meth:`rewrite_url`).\n        \"\"\"\n", "input": "", "output": "        if self.check_url(raw_url, is_image_src=True):\n            url = self.rewrite_url(raw_url, is_image_src=True)\n            maybe_alt = ' alt=\"%s\"' % escape_html(alt) if alt else ''\n            maybe_title = ' title=\"%s\"' % escape_html(title) if title else ''\n            url = escape_html(url)\n            return '<img src=\"%s\"%s%s />' % (url, maybe_alt, maybe_title)\n        else:\n            return escape_html(\"![%s](%s)\" % (alt, raw_url))", "category": "Python"}, {"instruction": "def create(cls, record, bucket):\n        \"\"\"Create a new RecordsBuckets and adds it to the session.\n\n        :param record: Record used to relate with the ``Bucket``.\n        :param bucket: Bucket used to relate with the ``Record``.\n        :returns: The :class:`~invenio_records_files.models.RecordsBuckets`\n            object created.\n        \"\"\"\n", "input": "", "output": "        rb = cls(record=record, bucket=bucket)\n        db.session.add(rb)\n        return rb", "category": "Python"}, {"instruction": "def download_directory(self, remote_path, local_path, progress=None):\n        \"\"\"Downloads directory and downloads all nested files and directories from remote WebDAV to local.\n        If there is something on local path it deletes directories and files then creates new.\n\n        :param remote_path: the path to directory for downloading form WebDAV server.\n        :param local_path: the path to local directory for saving downloaded files and directories.\n        :param progress: Progress function. Not supported now.\n        \"\"\"\n", "input": "", "output": "        urn = Urn(remote_path, directory=True)\n        if not self.is_dir(urn.path()):\n            raise OptionNotValid(name='remote_path', value=remote_path)\n\n        if os.path.exists(local_path):\n            shutil.rmtree(local_path)\n\n        os.makedirs(local_path)\n\n        for resource_name in self.list(urn.path()):\n            _remote_path = f'{urn.path()}{resource_name}'\n            _local_path = os.path.join(local_path, resource_name)\n            self.download(local_path=_local_path, remote_path=_remote_path, progress=progress)", "category": "Python"}, {"instruction": "def skus_get(self, product_id, session):\n        '''taobao.fenxiao.product.skus.get SKU\u67e5\u8be2\u63a5\u53e3\n        \n        \u4ea7\u54c1sku\u67e5\u8be2'''\n", "input": "", "output": "        request = TOPRequest('taobao.fenxiao.product.skus.get')\n        request['product_id'] = product_id\n        self.create(self.execute(request, session), fields=['skus','total_results'], models={'skus':FenxiaoSku})\n        return self.skus", "category": "Python"}, {"instruction": "def baseOn(self, *args):\n        \"\"\"\n        Sets the given parameters in this grid to fixed values.\n        Accepts either a parameter dictionary or a list of (parameter, value) pairs.\n        \"\"\"\n", "input": "", "output": "        if isinstance(args[0], dict):\n            self.baseOn(*args[0].items())\n        else:\n            for (param, value) in args:\n                self.addGrid(param, [value])\n\n        return self", "category": "Python"}, {"instruction": "def mouseReleaseEvent( self, event ):\n        \"\"\"\n        Overloads the base QGraphicsScene method to reset the selection \\\n        signals and finish the connection.\n                    \n        :param      event   <QMouseReleaseEvent>\n        \"\"\"\n", "input": "", "output": "        if event.button() == Qt.MidButton:\n            self.setViewMode(False)\n            event.accept()\n            return\n        \n        super(XNodeScene, self).mouseReleaseEvent(event)\n        \n        # reset the selection blocked signals\n        self.blockSelectionSignals(False)\n        \n        # finish the connection\n        if ( self.isConnecting() ):\n            self.finishConnection()\n        \n        # emit a menu request signal if necessary\n        elif ( not event.isAccepted() and event.button() == Qt.RightButton ):\n            item = self.itemAt(event.scenePos())\n            if isinstance(item, XNode):\n                self.emitNodeMenuRequested(item)\n            else:\n                self.emitMenuRequested()\n            event.accept()", "category": "Python"}, {"instruction": "def delete(self, url, data=None, params=None):\n        \"\"\"\n        Low-level DELETE request interface to mite. Takes a URL to request\n        (relative), and optionally data to add to the request. Either returns\n        the JSON body of the request or raises a HttpException.\n\n        \"\"\"\n", "input": "", "output": "        return self.request(\"delete\", url, data, params)", "category": "Python"}, {"instruction": "def bipartition(seq):\n    \"\"\"Return a list of bipartitions for a sequence.\n\n    Args:\n        a (Iterable): The sequence to partition.\n\n    Returns:\n        list[tuple[tuple]]: A list of tuples containing each of the two\n        partitions.\n\n    Example:\n        >>> bipartition((1,2,3))\n        [((), (1, 2, 3)), ((1,), (2, 3)), ((2,), (1, 3)), ((1, 2), (3,))]\n    \"\"\"\n", "input": "", "output": "    return [(tuple(seq[i] for i in part0_idx),\n             tuple(seq[j] for j in part1_idx))\n            for part0_idx, part1_idx in bipartition_indices(len(seq))]", "category": "Python"}, {"instruction": "def update_member_details(self, member_id, payload_member_detail, **kwargs):  # noqa: E501\n        \"\"\"Modify member details  # noqa: E501\n\n        One of the paramters below is needed to modify member information  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.update_member_details(member_id, payload_member_detail, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str member_id: (required)\n        :param MemberDetail payload_member_detail: (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return self.update_member_details_with_http_info(member_id, payload_member_detail, **kwargs)  # noqa: E501\n        else:\n            (data) = self.update_member_details_with_http_info(member_id, payload_member_detail, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def _save_url(self, url_data, content, url_text, url_pos):\n        \"\"\"Saves url. Converts url to 1-line text and url position as offset from the file beginning to (line, column).\n\n        :param url_data: object for url storing\n        :param content: file content\n        :param url_text: url text\n        :param url_pos: url position from the beginning\n        \"\"\"\n", "input": "", "output": "        line = content.count('\\n', 0, url_pos) + 1\n        column = url_pos - content.rfind('\\n', 0, url_pos)\n        url_data.add_url(url_text.translate(None, '\\n '), line=line, column=column)", "category": "Python"}, {"instruction": "def add(self, child):\n        \"\"\"\n        Adds a typed child object to the component.\n\n        @param child: Child object to be added.\n        \"\"\"\n", "input": "", "output": "\n        if isinstance(child, Component):\n            self.add_child(child)\n        else:\n            raise ModelError('Unsupported child element')", "category": "Python"}, {"instruction": "def find_trees(self, query_dict=None, exact=False, verbose=False, wrap_response=False, **kwargs):\n        \"\"\"Query on tree properties. See documentation for _OTIWrapper class.\"\"\"\n", "input": "", "output": "        if self.use_v1:\n            uri = '{p}/singlePropertySearchForTrees'.format(p=self.query_prefix)\n        else:\n            uri = '{p}/find_trees'.format(p=self.query_prefix)\n        resp = self._do_query(uri,\n                              query_dict=query_dict,\n                              exact=exact,\n                              verbose=verbose,\n                              valid_keys=self.tree_search_term_set,\n                              kwargs=kwargs)\n        if wrap_response:\n            return TreeRefList(resp)\n        return resp", "category": "Python"}, {"instruction": "def metar(self) -> str:\n        \"\"\"\n        Builds a METAR string from a MIZ file\n\n        A lots of information is inferred from what information we have available in DCS. There constraints in the way\n        MIZ files are built, with precipitations for example.\n\n        Returns: METAR string\n        \"\"\"\n", "input": "", "output": "        metar = f'{self._icao} ' \\\n                f'{self._time} ' \\\n                f'{self._wind} ' \\\n                f'{self._visibility} ' \\\n                f'{self._precipitations} ' \\\n                f'{self._clouds} ' \\\n                f'{self._temperature} ' \\\n                f'{self._pressure} ' \\\n                f'{self._qualifier}'\n        return re.sub(' +', ' ', metar)", "category": "Python"}, {"instruction": "def validate(self, value):\n        \"\"\"\n        From a value available on the remote server, the method returns the\n        complete item matching the value.\n        If case the value is not available on the server side or filtered\n        through :meth:`item`, the class:`agnocomplete.exceptions.ItemNotFound`\n        is raised.\n        \"\"\"\n", "input": "", "output": "\n        url = self.get_item_url(value)\n        try:\n            data = self.http_call(url=url)\n        except requests.HTTPError:\n            raise ItemNotFound()\n\n        data = self.get_http_result(data)\n\n        try:\n            self.item(data)\n        except SkipItem:\n            raise ItemNotFound()\n\n        return value", "category": "Python"}, {"instruction": "def get(self, block=True, timeout=None):\n        \"\"\"get.\"\"\"\n", "input": "", "output": "        try:\n            item = super().get(block, timeout)\n            self._getsocket.recv(1)\n            return item\n        except queue.Empty:\n            raise queue.Empty", "category": "Python"}, {"instruction": "def read_out(self, block_address):\n        \"\"\"\n        Prints sector/block number and contents of block. Tag and auth must be set - does auth.\n        \"\"\"\n", "input": "", "output": "        if not self.is_tag_set_auth():\n            return True\n\n        error = self.do_auth(block_address)\n        if not error:\n            (error, data) = self.rfid.read(block_address)\n            print(self.sector_string(block_address) + \": \" + str(data))\n        else:\n            print(\"Error on \" + self.sector_string(block_address))", "category": "Python"}, {"instruction": "def update_shot_browser(self, project, releasetype):\n        \"\"\"Update the shot browser to the given project\n\n        :param releasetype: the releasetype for the model\n        :type releasetype: :data:`djadapter.RELEASETYPES`\n        :param project: the project of the shots\n        :type project: :class:`djadapter.models.Project`\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        if project is None:\n            self.shotbrws.set_model(None)\n            return\n        shotmodel = self.create_shot_model(project, releasetype)\n        self.shotbrws.set_model(shotmodel)", "category": "Python"}, {"instruction": "def concatenate_and_rewrite(self, paths, output_filename, variant=None):\n        \"\"\"Concatenate together files and rewrite urls\"\"\"\n", "input": "", "output": "        stylesheets = []\n        for path in paths:\n            def reconstruct(match):\n                quote = match.group(1) or ''\n                asset_path = match.group(2)\n                if NON_REWRITABLE_URL.match(asset_path):\n                    return \"url(%s%s%s)\" % (quote, asset_path, quote)\n                asset_url = self.construct_asset_path(asset_path, path,\n                                                      output_filename, variant)\n                return \"url(%s)\" % asset_url\n            content = self.read_text(path)\n            # content needs to be unicode to avoid explosions with non-ascii chars\n            content = re.sub(URL_DETECTOR, reconstruct, content)\n            stylesheets.append(content)\n        return '\\n'.join(stylesheets)", "category": "Python"}, {"instruction": "def setDoc(self, doc):\n        \"\"\"Presents the documentation\n\n        :param doc: documentation for StimulusModel. i.e. returned from \n        :meth:`componentDoc<sparkle.stim.stimulus_model.StimulusModel.componentDoc>`\n        or :meth:`templateDoc<sparkle.stim.stimulus_model.StimulusModel.templateDoc>`\n        \"\"\"\n", "input": "", "output": "        self.ui.overAtten.setNum(doc['overloaded_attenuation'])\n        # also set composite stim type\n        # self.ui.traceType.setText(doc['testtype'])\n\n        self.ui.componentDetails.clearDoc()\n        self.ui.componentDetails.setDoc(doc['components'])", "category": "Python"}, {"instruction": "def create_xml_file_from_string(self, content, destination=None):\n        \"\"\"\n        Creates XML file from text.\n\n        :param content: C++ source code\n        :type content: str\n\n        :param destination: file name for xml file\n        :type destination: str\n\n        :rtype: returns file name of xml file\n        \"\"\"\n", "input": "", "output": "        header_file = utils.create_temp_file_name(suffix='.h')\n\n        try:\n            with open(header_file, \"w+\") as header:\n                header.write(content)\n            xml_file = self.create_xml_file(header_file, destination)\n        finally:\n            utils.remove_file_no_raise(header_file, self.__config)\n        return xml_file", "category": "Python"}, {"instruction": "def get_file_descriptor(self):\n        \"\"\"Return the file descriptor for the given websocket\"\"\"\n", "input": "", "output": "        try:\n            return uwsgi.connection_fd()\n        except IOError as e:\n            self.close()\n            raise WebSocketError(e)", "category": "Python"}, {"instruction": "def url_to_host(url):\n    \"\"\"convert a url to a host (ip or domain)\n\n    :param url: url string\n    :returns: host: domain name or ipv4/v6 address\n    :rtype: str\n    :raises: ValueError: given an illegal url that without a ip or domain name\n    \"\"\"\n", "input": "", "output": "\n    regex_url = r\"([a-z][a-z0-9+\\-.]*://)?\" + \\\n                r\"([a-z0-9\\-._~%!$&'()*+,;=]+@)?\" + \\\n                r\"([a-z0-9\\-._~%]+\" + \\\n                r\"|\\[[a-z0-9\\-._~%!$&'()*+,;=:]+\\])?\" + \\\n                r\"(:(?P<port>[0-9]+))?\"\n\n    m = re.match(regex_url, url, re.IGNORECASE)\n    if m and m.group(3):\n        return url[m.start(3): m.end(3)]\n    else:\n        raise ValueError(\"URL without a valid host or ip\")", "category": "Python"}, {"instruction": "def read_file(self, id_file):\n        \"\"\"\n        Read and parse id data from id_file which must be an iterable\n        yielding Unicode strings formatted as in '/etc/subuid' or\n        '/etc/subgid'.\n        \"\"\"\n", "input": "", "output": "\n        lineno = 0\n        for line in id_file:\n            lineno += 1\n            id_data = line.split(':')\n\n            if len(id_data) != 3:\n                raise BadIdFile(\n                        id_file.name, lineno,\n                        'incorrect number of fields'\n                        )\n\n            name = id_data[0]\n            try:\n                first, count = int(id_data[1]), int(id_data[2])\n            except ValueError:\n                raise BadIdFile(\n                        id_file.name, lineno,\n                        'cannot get the id range'\n                        )\n\n            # Append the new range\n            if not name in self.__map:\n                self.__map[name] = IdRangeSet()\n            self.__map[name].append(first, count)", "category": "Python"}, {"instruction": "def merge(args):\n    \"\"\"\n    %prog merge output/*.csv > ahrd.csv\n\n    Merge AHRD results, remove redundant headers, empty lines, etc. If there are\n    multiple lines containing the same ID (first column). Then whatever comes\n    the first will get retained.\n    \"\"\"\n", "input": "", "output": "    p = OptionParser(merge.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    csvfiles = args\n    cf = csvfiles[0]\n    fp = open(cf)\n    for row in fp:\n        if row.startswith(\"Protein\"):\n            break\n    header = row.rstrip()\n    print(header)\n\n    seen = set()\n    for cf in csvfiles:\n        fp = open(cf)\n        for row in fp:\n            if row[0] == '#':\n                continue\n            if row.strip() == \"\":\n                continue\n            if row.strip() == header:\n                continue\n\n            atoms = row.rstrip().split(\"\\t\")\n            id = atoms[0]\n            if id in seen:\n                logging.error(\"ID `{0}` ignored.\".format(id))\n                continue\n\n            seen.add(id)\n            print(row.strip())", "category": "Python"}, {"instruction": "def will_not_clone(self, request, *args, **kwargs):\n        \"\"\"\n        Add save but not clone capability in the changeview\n        \"\"\"\n", "input": "", "output": "        paths = request.path_info.split('/')\n        index_of_object_id = paths.index(\"will_not_clone\") - 1\n        object_id = paths[index_of_object_id]\n        self.change_view(request, object_id)\n\n        admin_wordInUrl = index_of_object_id - 3\n        # This gets the adminsite for the app, and the model name and joins\n        # together with /\n        path = '/' + '/'.join(paths[admin_wordInUrl:index_of_object_id])\n        return HttpResponseRedirect(path)", "category": "Python"}, {"instruction": "def _from_stream(cls, stream, blob, filename=None):\n        \"\"\"\n        Return an instance of the |Image| subclass corresponding to the\n        format of the image in *stream*.\n        \"\"\"\n", "input": "", "output": "        image_header = _ImageHeaderFactory(stream)\n        if filename is None:\n            filename = 'image.%s' % image_header.default_ext\n        return cls(blob, filename, image_header)", "category": "Python"}, {"instruction": "def _flush_stack(self):\n        '''\n        Returns the final output and resets the machine's state.\n        '''\n", "input": "", "output": "        output = self._postprocess_output(''.join(self.stack))\n        self._clear_char()\n        self._empty_stack()\n\n        if not PYTHON_2:\n            return output\n        else:\n            return unicode(output)", "category": "Python"}, {"instruction": "def find_undeclared_variables(ast):\n    \"\"\"Returns a set of all variables in the AST that will be looked up from\n    the context at runtime.  Because at compile time it's not known which\n    variables will be used depending on the path the execution takes at\n    runtime, all variables are returned.\n\n    >>> from jinja2 import Environment, meta\n    >>> env = Environment()\n    >>> ast = env.parse('{% set foo = 42 %}{{ bar + foo }}')\n    >>> meta.find_undeclared_variables(ast) == set(['bar'])\n    True\n\n    .. admonition:: Implementation\n\n       Internally the code generator is used for finding undeclared variables.\n       This is good to know because the code generator might raise a\n       :exc:`TemplateAssertionError` during compilation and as a matter of\n       fact this function can currently raise that exception as well.\n    \"\"\"\n", "input": "", "output": "    codegen = TrackingCodeGenerator(ast.environment)\n    codegen.visit(ast)\n    return codegen.undeclared_identifiers", "category": "Python"}, {"instruction": "def yield_json(stream):\n    \"\"\"Uses array and object delimiter counts for balancing.\n    \"\"\"\n", "input": "", "output": "    buff = u\"\"\n    arr_count = 0\n    obj_count = 0\n    while True:\n        buff += read_chunk(stream)\n\n        # If we finish parsing all objs or arrays, yield a finished JSON\n        # entity.\n        if buff.endswith('{'):\n            obj_count += 1\n        if buff.endswith('['):\n            arr_count += 1\n        if buff.endswith(']'):\n            arr_count -= 1\n            if obj_count == arr_count == 0:\n                json_item = copy(buff)\n                buff = u\"\"\n                yield json_item\n        if buff.endswith('}'):\n            obj_count -= 1\n            if obj_count == arr_count == 0:\n                json_item = copy(buff)\n                buff = u\"\"\n                yield json_item", "category": "Python"}, {"instruction": "def collect_files(dirpath, cond=lambda fullname: True):\n    \"\"\"\n    Recursively collect the files contained inside dirpath.\n\n    :param dirpath: path to a readable directory\n    :param cond: condition on the path to collect the file\n    \"\"\"\n", "input": "", "output": "    files = []\n    for fname in os.listdir(dirpath):\n        fullname = os.path.join(dirpath, fname)\n        if os.path.isdir(fullname):  # navigate inside\n            files.extend(collect_files(fullname))\n        else:  # collect files\n            if cond(fullname):\n                files.append(fullname)\n    return files", "category": "Python"}, {"instruction": "def info(gandi, resource):\n    \"\"\" Display information about a snapshot profile.\n\n    Resource can be a profile name or ID\n    \"\"\"\n", "input": "", "output": "    output_keys = ['id', 'name', 'kept_total', 'target', 'quota_factor',\n                   'schedules']\n\n    result = gandi.snapshotprofile.info(resource)\n    output_snapshot_profile(gandi, result, output_keys)\n    return result", "category": "Python"}, {"instruction": "def set_current_filename(self, filename, editorwindow=None, focus=True):\r\n        \"\"\"Set focus to *filename* if this file has been opened.\r\n\r\n        Return the editor instance associated to *filename*.\r\n        \"\"\"\n", "input": "", "output": "        editorstack = self.get_current_editorstack(editorwindow)\r\n        return editorstack.set_current_filename(filename, focus)", "category": "Python"}, {"instruction": "def on_extslice(self, node):    # ():('dims',)\n        \"\"\"Extended slice.\"\"\"\n", "input": "", "output": "        return tuple([self.run(tnode) for tnode in node.dims])", "category": "Python"}, {"instruction": "def mousePressEvent(self, event):\r\n        \"\"\"Override Qt method\"\"\"\n", "input": "", "output": "        if event.button() == Qt.MidButton:\r\n            index = self.tabBar().tabAt(event.pos())\r\n            if index >= 0:\r\n                self.sig_close_tab.emit(index)\r\n                event.accept()\r\n                return\r\n        QTabWidget.mousePressEvent(self, event)", "category": "Python"}, {"instruction": "def _get_current_minute(self):\n        \"\"\"\n        Internal utility method to get the current simulation time.\n\n        Possible answers are:\n        - whatever the algorithm's get_datetime() method returns (this is what\n            `self.simulation_dt_func()` points to)\n        - sometimes we're knowingly not in a market minute, like if we're in\n            before_trading_start.  In that case, `self._adjust_minutes` is\n            True, and we get the previous market minute.\n        - if we're in daily mode, get the session label for this minute.\n        \"\"\"\n", "input": "", "output": "\n        dt = self.datetime\n\n        if self._adjust_minutes:\n            dt = \\\n                self.data_portal.trading_calendar.previous_minute(dt)\n\n        if self._daily_mode:\n            # if we're in daily mode, take the given dt (which is the last\n            # minute of the session) and get the session label for it.\n            dt = self.data_portal.trading_calendar.minute_to_session_label(dt)\n\n        return dt", "category": "Python"}, {"instruction": "def _set_fqdn(self):\n        \"\"\"Get FQDN from LDAP\"\"\"\n", "input": "", "output": "        results = self._search(\n            'cn=config',\n            '(objectClass=*)',\n            ['nsslapd-localhost'],\n            scope=ldap.SCOPE_BASE\n        )\n        if not results and type(results) is not list:\n            r = None\n        else:\n            dn, attrs = results[0]\n            r = attrs['nsslapd-localhost'][0].decode('utf-8')\n        self._fqdn = r\n        log.debug('FQDN: %s' % self._fqdn)", "category": "Python"}, {"instruction": "def X(self, i, j=slice(None, None, None)):\n        '''\n        Computes the design matrix at the given *PLD* order and the given\n        indices. The columns are the *PLD* vectors for the target at the\n        corresponding order, computed as the product of the fractional pixel\n        flux of all sets of :py:obj:`n` pixels, where :py:obj:`n` is the *PLD*\n        order.\n\n        '''\n", "input": "", "output": "\n        X1 = self.fpix[j] / self.norm[j].reshape(-1, 1)\n        X = np.product(list(multichoose(X1.T, i + 1)), axis=1).T\n        if self.X1N is not None:\n            return np.hstack([X, self.X1N[j] ** (i + 1)])\n        else:\n            return X", "category": "Python"}, {"instruction": "def string(self, units: typing.Optional[str] = None) -> str:\n        \"\"\"Return a string representation of the pressure, using the given units.\"\"\"\n", "input": "", "output": "        if not units:\n            _units: str = self._units\n        else:\n            if not units.upper() in CustomPressure.legal_units:\n                raise UnitsError(\"unrecognized pressure unit: '\" + units + \"'\")\n            _units = units.upper()\n        val = self.value(units)\n        if _units == \"MB\":\n            return \"%.0f mb\" % val\n        if _units == \"HPA\":\n            return \"%.0f hPa\" % val\n        if _units == \"IN\":\n            return \"%.2f inches\" % val\n        if _units == \"MM\":\n            return \"%.0f mmHg\" % val\n\n        raise ValueError(_units)", "category": "Python"}, {"instruction": "def extend_partial(self, times, obs_times, obs_losses, config=None):\n        \"\"\"\n            extends a partially observed curve\n\n            Parameters:\n            -----------\n\n            times: numpy array\n                times where to predict the loss\n            obs_times: numpy array\n                times where the curve has already been observed\n            obs_losses: numpy array\n                corresponding observed losses\n            config: numpy array\n                numerical reperesentation of the config; None if no config\n                information is available\n                \n            Returns:\n            --------\n            \n            mean and variance prediction at input times\n                \n                \n        \"\"\"\n", "input": "", "output": "        return self.predict_unseen(times, config)", "category": "Python"}, {"instruction": "def get_onsets(token_tuples):\n    \"\"\"\n    Parameters\n    ----------\n    token_tuples : list of (str, unicode)\n        a list/generator of (token ID, token string) tuples\n\n    Returns\n    -------\n    onset_tuples : generator of (str, int, int)\n        A list/generator of (token ID, onset, token length) tuples.\n        Note that PAULA starts counting string onsets with 1!\n    \"\"\"\n", "input": "", "output": "    onset = 1\n    for (token_id, token) in token_tuples:\n        yield (token_id, onset, len(token))\n        onset += (len(token) + 1)", "category": "Python"}, {"instruction": "def _extract_annotations_from_span(span):\n    \"\"\"Extract and convert time event annotations to zipkin annotations\"\"\"\n", "input": "", "output": "    if span.time_events is None:\n        return []\n\n    annotations = []\n    for time_event in span.time_events:\n        annotation = time_event.annotation\n        if not annotation:\n            continue\n\n        event_timestamp_mus = timestamp_to_microseconds(time_event.timestamp)\n        annotations.append({'timestamp': int(round(event_timestamp_mus)),\n                            'value': annotation.description})\n\n    return annotations", "category": "Python"}, {"instruction": "def do_int(value, default=0, base=10):\n    \"\"\"Convert the value into an integer. If the\n    conversion doesn't work it will return ``0``. You can\n    override this default using the first parameter. You\n    can also override the default base (10) in the second\n    parameter, which handles input with prefixes such as\n    0b, 0o and 0x for bases 2, 8 and 16 respectively.\n    The base is ignored for decimal numbers and non-string values.\n    \"\"\"\n", "input": "", "output": "    try:\n        if isinstance(value, string_types):\n            return int(value, base)\n        return int(value)\n    except (TypeError, ValueError):\n        # this quirk is necessary so that \"42.23\"|int gives 42.\n        try:\n            return int(float(value))\n        except (TypeError, ValueError):\n            return default", "category": "Python"}, {"instruction": "def enqueue(self, item, queue=None):\n        \"\"\"\n        Enqueue items.\n        If you define \"self.filter\" (sequence),\n        this method put the item to queue after filtering.\n        \"self.filter\" operates as blacklist.\n\n        This method expects that\n        \"item\" argument has dict type \"data\" attribute.\n        \"\"\"\n", "input": "", "output": "        if queue is None:\n            queue = self.queue\n        is_enqueue_item = True\n\n        if self.invalid_key_list is not None:\n            for entry in self.invalid_key_list:\n                if entry in item.data['key']:\n                    is_enqueue_item = False\n                    log_message = (\n                        '{key} is filtered by \"invalid_key_list\".'\n                        ''.format(key=item.data['key'],\n                                  plugin=__name__)\n                    )\n                    self.logger.debug(log_message)\n                    break\n\n        if is_enqueue_item:\n            try:\n                queue.put(item, block=False)\n                return True\n            except Full:\n                self.logger.error('Blackbird item Queue is Full!!!')\n                return False\n\n        else:\n            return False", "category": "Python"}, {"instruction": "def resp_set_power(self, resp, power_level=None):\n        \"\"\"Default callback for get_power/set_power\n        \"\"\"\n", "input": "", "output": "        if power_level is not None:\n            self.power_level=power_level\n        elif resp:\n            self.power_level=resp.power_level", "category": "Python"}, {"instruction": "def enriched(self, thresh=0.05, idx=True):\n        \"\"\"\n        Enriched features.\n\n        {threshdoc}\n        \"\"\"\n", "input": "", "output": "        return self.upregulated(thresh=thresh, idx=idx)", "category": "Python"}, {"instruction": "def sql(self, sql, params=()):\n        ''' execute sql request and return items\n        '''\n", "input": "", "output": "        def _items(items):\n            for item in items:\n                yield self._item_class(item)\n\n        sql = sql.strip()\n        try:\n            self._cursor.execute(sql, params)\n        except sqlite3.OperationalError, err:\n            raise SQLError('%s, SQL: %s, params: %s' % (err, sql, params) )\n        except sqlite3.IntegrityError:\n            raise DuplicateItem('Duplicate item, %s' % item)\n\n        if sql.upper().startswith('SELECT'):\n            return _items(self._cursor.fetchall())\n        else:\n            return None", "category": "Python"}, {"instruction": "def get_url(\n    width, height=None, background_color=\"cccccc\",\n    text_color=\"969696\", text=None, random_background_color=False\n):\n    \"\"\"\n    Craft the URL for a placeholder image.\n\n    You can customize the background color, text color and text using\n    the optional keyword arguments\n\n    If you want to use a random color pass in random_background_color as True.\n    \"\"\"\n", "input": "", "output": "    if random_background_color:\n        background_color = _get_random_color()\n\n    # If height is not provided, presume it is will be a square\n    if not height:\n        height = width\n    d = dict(\n        width=width,\n        height=height,\n        bcolor=background_color,\n        tcolor=text_color\n    )\n    url = URL % d\n    if text:\n        text = text.replace(\" \", \"+\")\n        url = url + \"?text=\" + text\n    return url", "category": "Python"}, {"instruction": "def post_replicate(request):\n    \"\"\"MNReplication.replicate(session, sysmeta, sourceNode) \u2192 boolean.\"\"\"\n", "input": "", "output": "    d1_gmn.app.views.assert_db.post_has_mime_parts(\n        request, (('field', 'sourceNode'), ('file', 'sysmeta'))\n    )\n    sysmeta_pyxb = d1_gmn.app.sysmeta.deserialize(request.FILES['sysmeta'])\n    d1_gmn.app.local_replica.assert_request_complies_with_replication_policy(\n        sysmeta_pyxb\n    )\n    pid = d1_common.xml.get_req_val(sysmeta_pyxb.identifier)\n    d1_gmn.app.views.assert_db.is_valid_pid_for_create(pid)\n    d1_gmn.app.local_replica.add_to_replication_queue(\n        request.POST['sourceNode'], sysmeta_pyxb\n    )\n    return d1_gmn.app.views.util.http_response_with_boolean_true_type()", "category": "Python"}, {"instruction": "def call(node: Node, key: str, value: Any):\n    \"\"\"Calls node or node instance method\"\"\"\n", "input": "", "output": "    value = _to_list(value)\n    if not value or not isinstance(value[-1], dict):\n        value.append({})\n    args = value[0:-1]\n    kwargs = value[-1]\n    node.__dict__[key](*args, **kwargs)", "category": "Python"}, {"instruction": "def log_train(batch_id, batch_num, metric, step_loss, log_interval, epoch_id, learning_rate):\n    \"\"\"Generate and print out the log message for training.\n    \"\"\"\n", "input": "", "output": "    metric_nm, metric_val = metric.get()\n    if not isinstance(metric_nm, list):\n        metric_nm = [metric_nm]\n        metric_val = [metric_val]\n\n    train_str = '[Epoch %d Batch %d/%d] loss=%.4f, lr=%.7f, metrics:' + \\\n                ','.join([i + ':%.4f' for i in metric_nm])\n    logging.info(train_str, epoch_id + 1, batch_id + 1, batch_num, \\\n                 step_loss / log_interval, \\\n                 learning_rate, \\\n                 *metric_val)", "category": "Python"}, {"instruction": "def transaction(self, callback):\n        \"\"\"Executes a function in a transaction.\n\n        The function gets passed this Connection instance as an (optional) parameter.\n\n        If an exception occurs during execution of the function or transaction commit,\n        the transaction is rolled back and the exception re-thrown.\n\n        :param callback: the function to execute in a transaction\n        :return: the value returned by the `callback`\n        :raise: Exception\n        \"\"\"\n", "input": "", "output": "        self.begin_transaction()\n        try:\n            result = callback(self)\n            self.commit()\n            return result\n        except:\n            self.rollback()\n            raise", "category": "Python"}, {"instruction": "def raise_status(response):\n    \"\"\"Raise an exception if the request did not return a status code of 200.\n\n    :param response: Request response body\n    \"\"\"\n", "input": "", "output": "    if response.status != 200:\n        if response.status == 401:\n            raise StrawPollException('Unauthorized', response)\n        elif response.status == 403:\n            raise StrawPollException('Forbidden', response)\n        elif response.status == 404:\n            raise StrawPollException('Not Found', response)\n        else:\n            response.raise_for_status()", "category": "Python"}, {"instruction": "def view_pdf(name=None):\n    \"\"\"Render a pdf file based on the given page.\n\n    .. note:: this is a bottle view\n\n    Keyword Arguments:\n        :name: (str) -- name of the rest file (without the .rst extension)\n                        MANDATORY\n    \"\"\"\n", "input": "", "output": "    if name is None:\n        return view_meta_index()\n\n    files = glob.glob(\"{0}.rst\".format(name))\n    if len(files) > 0:\n        file_handle = open(files[0], 'r')\n        dest_filename = name + '.pdf'\n        doctree = publish_doctree(file_handle.read())\n        try:\n            produce_pdf(doctree_content=doctree,\n                        filename=dest_filename)\n        except:\n            raise\n        else:\n            return static_file(dest_filename,\n                               root='',\n                               download=True)\n    else:\n        return abort(404)", "category": "Python"}, {"instruction": "def escape_windows_cmd_string(s):\n    \"\"\"Returns a string that is usable by the Windows cmd.exe.\n    The escaping is based on details here and emperical testing:\n    http://www.robvanderwoude.com/escapechars.php\n    \"\"\"\n", "input": "", "output": "    for c in '()%!^<>&|\"':\n        s = s.replace(c, '^' + c)\n    s = s.replace('/?', '/.')\n    return s", "category": "Python"}, {"instruction": "def _add(self, name, *args, **kw):\n        \"\"\"\n        Add an argument to the underlying parser and grow the list\n        .all_arguments and the set .names\n        \"\"\"\n", "input": "", "output": "        argname = list(self.argdict)[self._argno]\n        if argname != name:\n            raise NameError(\n                'Setting argument %s, but it should be %s' % (name, argname))\n        self._group.add_argument(*args, **kw)\n        self.all_arguments.append((args, kw))\n        self.names.append(name)\n        self._argno += 1", "category": "Python"}, {"instruction": "def _search(self, search_terms, begins_with=None):\n        \"\"\"\n        Returns a list of Archive id's in the table on Dynamo\n\n        \"\"\"\n", "input": "", "output": "\n        kwargs = dict(\n            ProjectionExpression='#id',\n            ExpressionAttributeNames={\"#id\": \"_id\"})\n\n        if len(search_terms) > 0:\n            kwargs['FilterExpression'] = reduce(\n                lambda x, y: x & y,\n                [Attr('tags').contains(arg) for arg in search_terms])\n\n        if begins_with:\n            if 'FilterExpression' in kwargs:\n                kwargs['FilterExpression'] = kwargs[\n                    'FilterExpression'] & Key('_id').begins_with(begins_with)\n\n            else:\n                kwargs['FilterExpression'] = Key(\n                    '_id').begins_with(begins_with)\n\n        while True:\n            res = self._table.scan(**kwargs)\n            for r in res['Items']:\n                yield r['_id']\n            if 'LastEvaluatedKey' in res:\n                kwargs['ExclusiveStartKey'] = res['LastEvaluatedKey']\n            else:\n                break", "category": "Python"}, {"instruction": "def register_postparsing_hook(self, func: Callable[[plugin.PostparsingData], plugin.PostparsingData]) -> None:\n        \"\"\"Register a function to be called after parsing user input but before running the command\"\"\"\n", "input": "", "output": "        self._validate_postparsing_callable(func)\n        self._postparsing_hooks.append(func)", "category": "Python"}, {"instruction": "def sense(self):\n        \"\"\"Return a situation, encoded as a bit string, which represents\n        the observable state of the environment.\n\n        Usage:\n            situation = scenario.sense()\n            assert isinstance(situation, BitString)\n\n        Arguments: None\n        Return:\n            The current situation.\n        \"\"\"\n", "input": "", "output": "        haystack = bitstrings.BitString.random(self.input_size)\n        self.needle_value = haystack[self.needle_index]\n        return haystack", "category": "Python"}, {"instruction": "def _not(cls, operation):\n        \"\"\"not operation\"\"\"\n", "input": "", "output": "\n        def _wrap(*args, **kwargs):\n            return not operation(*args, **kwargs)\n\n        return _wrap", "category": "Python"}, {"instruction": "def setup(cli):\n    \"\"\"Everything to make skypipe ready to use\"\"\"\n", "input": "", "output": "    if not cli.global_config.loaded:\n        setup_dotcloud_account(cli)\n    discover_satellite(cli)\n    cli.success(\"Skypipe is ready for action\")", "category": "Python"}, {"instruction": "def name_for_scalar_relationship(base, local_cls, referred_cls, constraint):\n    \"\"\" Overriding naming schemes. \"\"\"\n", "input": "", "output": "    name = referred_cls.__name__.lower() + \"_ref\"\n    return name", "category": "Python"}, {"instruction": "def _add_edge(self, layer, input_id, output_id):\n        \"\"\"Add a new layer to the graph. The nodes should be created in advance.\"\"\"\n", "input": "", "output": "\n        if layer in self.layer_to_id:\n            layer_id = self.layer_to_id[layer]\n            if input_id not in self.layer_id_to_input_node_ids[layer_id]:\n                self.layer_id_to_input_node_ids[layer_id].append(input_id)\n            if output_id not in self.layer_id_to_output_node_ids[layer_id]:\n                self.layer_id_to_output_node_ids[layer_id].append(output_id)\n        else:\n            layer_id = len(self.layer_list)\n            self.layer_list.append(layer)\n            self.layer_to_id[layer] = layer_id\n            self.layer_id_to_input_node_ids[layer_id] = [input_id]\n            self.layer_id_to_output_node_ids[layer_id] = [output_id]\n\n        self.adj_list[input_id].append((output_id, layer_id))\n        self.reverse_adj_list[output_id].append((input_id, layer_id))", "category": "Python"}, {"instruction": "def is_secret_registered(\n            self,\n            secrethash: SecretHash,\n            block_identifier: BlockSpecification,\n    ) -> bool:\n        \"\"\"True if the secret for `secrethash` is registered at `block_identifier`.\n\n        Throws NoStateForBlockIdentifier if the given block_identifier\n        is older than the pruning limit\n        \"\"\"\n", "input": "", "output": "        if not self.client.can_query_state_for_block(block_identifier):\n            raise NoStateForBlockIdentifier()\n\n        block = self.get_secret_registration_block_by_secrethash(\n            secrethash=secrethash,\n            block_identifier=block_identifier,\n        )\n        return block is not None", "category": "Python"}, {"instruction": "def get_size_for_density(self, size, target_density):\n        \"\"\"\n        Return the new image size for the target density\n        \"\"\"\n", "input": "", "output": "        current_size = size\n        current_density = DENSITY_MAP[self.source_density]\n        target_density = DENSITY_MAP[target_density]\n\n        return int(current_size * (target_density / current_density))", "category": "Python"}, {"instruction": "def setup_requests_logging(level):\r\n\t\"\"\"\r\n\tSetup logging for 'requests' such that it logs details about the\r\n\tconnection, headers, etc.\r\n\t\"\"\"\n", "input": "", "output": "\trequests_log = logging.getLogger(\"requests.packages.urllib3\")\r\n\trequests_log.setLevel(level)\r\n\trequests_log.propagate = True\r\n\r\n\t# enable debugging at httplib level\r\n\thttp_client.HTTPConnection.debuglevel = level <= logging.DEBUG", "category": "Python"}, {"instruction": "def reboot_node(node_id, profile, **libcloud_kwargs):\n    '''\n    Reboot a node in the cloud\n\n    :param node_id: Unique ID of the node to reboot\n    :type  node_id: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's reboot_node method\n    :type  libcloud_kwargs: ``dict``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_compute.reboot_node as-2346 profile1\n    '''\n", "input": "", "output": "    conn = _get_driver(profile=profile)\n    node = _get_by_id(conn.list_nodes(**libcloud_kwargs), node_id)\n    return conn.reboot_node(node, **libcloud_kwargs)", "category": "Python"}, {"instruction": "def read(self, handle):\n        '''Read binary data for this parameter from a file handle.\n\n        This reads exactly enough data from the current position in the file to\n        initialize the parameter.\n        '''\n", "input": "", "output": "        self.bytes_per_element, = struct.unpack('b', handle.read(1))\n        dims, = struct.unpack('B', handle.read(1))\n        self.dimensions = [struct.unpack('B', handle.read(1))[0] for _ in range(dims)]\n        self.bytes = b''\n        if self.total_bytes:\n            self.bytes = handle.read(self.total_bytes)\n        size, = struct.unpack('B', handle.read(1))\n        self.desc = size and handle.read(size).decode('utf-8') or ''", "category": "Python"}, {"instruction": "def distance_centimeters_ping(self):\n        \"\"\"\n        Measurement of the distance detected by the sensor,\n        in centimeters.\n\n        The sensor will take a single measurement then stop\n        broadcasting.\n\n        If you use this property too frequently (e.g. every\n        100msec), the sensor will sometimes lock up and writing\n        to the mode attribute will return an error. A delay of\n        250msec between each usage seems sufficient to keep the\n        sensor from locking up.\n        \"\"\"\n", "input": "", "output": "        # This mode is special; setting the mode causes the sensor to send out\n        # a \"ping\", but the mode isn't actually changed.\n        self.mode = self.MODE_US_SI_CM\n        return self.value(0) * self._scale('US_DIST_CM')", "category": "Python"}, {"instruction": "def get_module_class(self):\n        \"\"\"Return the module and class as a tuple of the given class in the\n        initializer.\n\n        :param reload: if ``True`` then reload the module before returning the\n        class\n\n        \"\"\"\n", "input": "", "output": "        pkg, cname = self.parse_module_class()\n        logger.debug(f'pkg: {pkg}, class: {cname}')\n        pkg = pkg.split('.')\n        mod = reduce(lambda m, n: getattr(m, n), pkg[1:], __import__(pkg[0]))\n        logger.debug(f'mod: {mod}')\n        if self.reload:\n            importlib.reload(mod)\n        cls = getattr(mod, cname)\n        logger.debug(f'class: {cls}')\n        return mod, cls", "category": "Python"}, {"instruction": "def _sync_last_sale_prices(self, dt=None):\n        \"\"\"Sync the last sale prices on the metrics tracker to a given\n        datetime.\n\n        Parameters\n        ----------\n        dt : datetime\n            The time to sync the prices to.\n\n        Notes\n        -----\n        This call is cached by the datetime. Repeated calls in the same bar\n        are cheap.\n        \"\"\"\n", "input": "", "output": "        if dt is None:\n            dt = self.datetime\n\n        if dt != self._last_sync_time:\n            self.metrics_tracker.sync_last_sale_prices(\n                dt,\n                self.data_portal,\n            )\n            self._last_sync_time = dt", "category": "Python"}, {"instruction": "def Msg(validator, message):\n    \"\"\"\n    Wraps the given validator callable, replacing any error messages raised.\n    \"\"\"\n", "input": "", "output": "    @wraps(Msg)\n    def built(value):\n        try:\n            return validator(value)\n        except Error as e:\n            e.message = message\n            raise e\n    return built", "category": "Python"}, {"instruction": "def _set_spyder_breakpoints(self, breakpoints):\n        \"\"\"Set all Spyder breakpoints in an active pdb session\"\"\"\n", "input": "", "output": "        if not self._pdb_obj:\n            return\n\n        # Breakpoints come serialized from Spyder. We send them\n        # in a list of one element to be able to send them at all\n        # in Python 2\n        serialized_breakpoints = breakpoints[0]\n        breakpoints = pickle.loads(serialized_breakpoints)\n\n        self._pdb_obj.set_spyder_breakpoints(breakpoints)", "category": "Python"}, {"instruction": "def get_graph_token_from_msi():\n    '''get a Microsoft Graph access token using Azure Cloud Shell's MSI_ENDPOINT.\n\n        Notes: \n        The auth token returned by this function is not an Azure auth token. Use it for querying\n        the Microsoft Graph API.\n        This function only works in an Azure cloud shell or virtual machine.\n\n    Returns:\n        A Microsoft Graph authentication token string.\n    '''\n", "input": "", "output": "    if 'ACC_CLOUD' in os.environ and 'MSI_ENDPOINT' in os.environ:\n        endpoint = os.environ['MSI_ENDPOINT']\n    else:\n        return None\n    \n    headers = {'Metadata': 'true'}\n    body = {\"resource\": 'https://' + GRAPH_RESOURCE_HOST + '/'}\n    ret = requests.post(endpoint, headers=headers, data=body)\n    return ret.json()['access_token']", "category": "Python"}, {"instruction": "def history(self):\n        \"\"\"Returns a list of search jobs corresponding to this saved search.\n\n        :return: A list of :class:`Job` objects.\n        \"\"\"\n", "input": "", "output": "        response = self.get(\"history\")\n        entries = _load_atom_entries(response)\n        if entries is None: return []\n        jobs = []\n        for entry in entries:\n            job = Job(self.service, entry.title)\n            jobs.append(job)\n        return jobs", "category": "Python"}, {"instruction": "def setup_fs_model(self):\r\n        \"\"\"Setup filesystem model\"\"\"\n", "input": "", "output": "        filters = QDir.AllDirs | QDir.Files | QDir.Drives | QDir.NoDotAndDotDot\r\n        self.fsmodel = QFileSystemModel(self)\r\n        self.fsmodel.setFilter(filters)\r\n        self.fsmodel.setNameFilterDisables(False)", "category": "Python"}, {"instruction": "def _get_hosted_zones(self):\n        \"\"\"\n        Return all available hosted zones\n\n        :returns: dict of hosted zones\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        results = paginate_dict(\n            self.conn.list_hosted_zones,\n            alc_marker_path=['NextMarker'],\n            alc_data_path=['HostedZones'],\n            alc_marker_param='Marker'\n        )\n\n        return results[\"HostedZones\"]", "category": "Python"}, {"instruction": "def count_protein_group_hits(lineproteins, groups):\n    \"\"\"Takes a list of protein accessions and a list of protein groups\n    content from DB. Counts for each group in list how many proteins\n    are found in lineproteins. Returns list of str amounts.\n    \"\"\"\n", "input": "", "output": "    hits = []\n    for group in groups:\n        hits.append(0)\n        for protein in lineproteins:\n            if protein in group:\n                hits[-1] += 1\n    return [str(x) for x in hits]", "category": "Python"}, {"instruction": "def detect_framebuffer(self, glo=None) -> 'Framebuffer':\n        '''\n            Detect framebuffer.\n\n            Args:\n                glo (int): Framebuffer object.\n\n            Returns:\n                :py:class:`Framebuffer` object\n        '''\n", "input": "", "output": "\n        res = Framebuffer.__new__(Framebuffer)\n        res.mglo, res._size, res._samples, res._glo = self.mglo.detect_framebuffer(glo)\n        res._color_attachments = None\n        res._depth_attachment = None\n        res.ctx = self\n        res.extra = None\n        return res", "category": "Python"}, {"instruction": "def on_episode_begin(self, episode, logs={}):\n        \"\"\" Called at beginning of each episode for each callback in callbackList\"\"\"\n", "input": "", "output": "        for callback in self.callbacks:\n            # Check if callback supports the more appropriate `on_episode_begin` callback.\n            # If not, fall back to `on_epoch_begin` to be compatible with built-in Keras callbacks.\n            if callable(getattr(callback, 'on_episode_begin', None)):\n                callback.on_episode_begin(episode, logs=logs)\n            else:\n                callback.on_epoch_begin(episode, logs=logs)", "category": "Python"}, {"instruction": "def get_policy_type(self, project, type_id):\n        \"\"\"GetPolicyType.\n        Retrieve a specific policy type by ID.\n        :param str project: Project ID or project name\n        :param str type_id: The policy ID.\n        :rtype: :class:`<PolicyType> <azure.devops.v5_0.policy.models.PolicyType>`\n        \"\"\"\n", "input": "", "output": "        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if type_id is not None:\n            route_values['typeId'] = self._serialize.url('type_id', type_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='44096322-2d3d-466a-bb30-d1b7de69f61f',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('PolicyType', response)", "category": "Python"}, {"instruction": "def attach(self, canvas):\n        \"\"\"Attach this tranform to a canvas\n\n        Parameters\n        ----------\n        canvas : instance of Canvas\n            The canvas.\n        \"\"\"\n", "input": "", "output": "        self._canvas = canvas\n        canvas.events.resize.connect(self.on_resize)\n        canvas.events.mouse_wheel.connect(self.on_mouse_wheel)\n        canvas.events.mouse_move.connect(self.on_mouse_move)", "category": "Python"}, {"instruction": "def encode(self,\n               data: mx.sym.Symbol,\n               data_length: Optional[mx.sym.Symbol],\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Encoded versions of input data (data, data_length, seq_len).\n        \"\"\"\n", "input": "", "output": "        with mx.AttrScope(__layout__=self.target_layout):\n            return mx.sym.swapaxes(data=data, dim1=0, dim2=1), data_length, seq_len", "category": "Python"}, {"instruction": "def G(self, ID, lat, lon):\n        \"\"\" Creates a generic entry for an object. \"\"\"\n", "input": "", "output": "        \n        # Equatorial coordinates\n        eqM = utils.eqCoords(lon, lat)\n        eqZ = eqM\n        if lat != 0:\n            eqZ = utils.eqCoords(lon, 0)\n        \n        return {\n            'id': ID,\n            'lat': lat,\n            'lon': lon,\n            'ra': eqM[0],\n            'decl': eqM[1],\n            'raZ': eqZ[0],\n            'declZ': eqZ[1],\n        }", "category": "Python"}, {"instruction": "def updateObj(self,event):\n        \"\"\"Put this object in the search box\"\"\"\n", "input": "", "output": "        \n        name=w.objList.get(\"active\")\n        w.SearchVar.set(name)\n        w.ObjInfo.set(objInfoDict[name])\n        return", "category": "Python"}, {"instruction": "def _add_variants(self, key, value, schema):\n        ''' also possible to define some function that takes\n            current value and creates a new value from it\n        '''\n", "input": "", "output": "        variants = schema.get('variants')\n        obj = {}\n        if variants:\n            for _key, func in variants.iteritems():\n                _value = func(value, self.store)\n                obj.update({_key: _value})\n        return obj", "category": "Python"}, {"instruction": "def stop(host=None, port=None):\r\n    \"\"\"stop of web server\"\"\"\n", "input": "", "output": "    app.config['HOST'] = first_value(host, app.config.get('HOST',None), '0.0.0.0')\r\n    app.config['PORT'] = int(first_value(port, app.config.get('PORT',None), 5001))\r\n    if app.config['HOST'] == \"0.0.0.0\": \r\n        host=\"127.0.0.1\"\r\n    else:\r\n        host = app.config['HOST']\r\n    port = app.config['PORT']    \r\n    try:\r\n        if requests.get('http://%s:%s/api/status' % (host, port)).status_code == 200:\r\n            requests.get('http://%s:%s/api/stop' % (host,port))\r\n            print('web server is stopped', file=sys.stdinfo)\r\n        else:\r\n            print('web server is not flaskserver', file=sys.stderr)\r\n    except:\r\n        print('web server is not flaskserver or not start', file=sys.stderr)", "category": "Python"}, {"instruction": "def update_hpolist(self, case_obj):\n        \"\"\"Update the HPO gene list for a case based on current terms.\"\"\"\n", "input": "", "output": "        hpo_list = self.case_genelist(case_obj)\n        hpo_results = hpo_genes(case_obj.phenotype_ids(),\n                                *self.phenomizer_auth)\n\n        if hpo_results is None:\n            pass\n            # Why raise here?\n            # raise RuntimeError(\"couldn't link to genes, try again\")\n        else:\n            gene_ids = [result['gene_id'] for result in hpo_results\n                        if result['gene_id']]\n            hpo_list.gene_ids = gene_ids\n            self.save()", "category": "Python"}, {"instruction": "async def publish(self, message):\r\n        \"\"\"Push a new message to the client. The data will be\r\n        available as a JSON object with the key ``data``.\r\n\r\n        \"\"\"\n", "input": "", "output": "        try:\r\n            self.write_message(dict(data=message))\r\n        except WebSocketClosedError:\r\n            self._close()", "category": "Python"}, {"instruction": "def set_text(self, text=None):\n        \"\"\"stub\"\"\"\n", "input": "", "output": "        if text is None:\n            raise NullArgument()\n        if self.get_text_metadata().is_read_only():\n            raise NoAccess()\n        if not self.my_osid_object_form._is_valid_string(\n                text,\n                self.get_text_metadata()):\n            raise InvalidArgument()\n        self.my_osid_object_form._my_map['text']['text'] = text", "category": "Python"}, {"instruction": "def get_psf_sky(self, ra, dec):\n        \"\"\"\n        Determine the local psf at a given sky location.\n        The psf is returned in degrees.\n\n\n        Parameters\n        ----------\n        ra, dec : float\n            The sky position (degrees).\n\n        Returns\n        -------\n        a, b, pa : float\n            The psf semi-major axis, semi-minor axis, and position angle in (degrees).\n            If a psf is defined then it is the psf that is returned, otherwise the image\n            restoring beam is returned.\n        \"\"\"\n", "input": "", "output": "        # If we don't have a psf map then we just fall back to using the beam\n        # from the fits header (including ZA scaling)\n        if self.data is None:\n            beam = self.wcshelper.get_beam(ra, dec)\n            return beam.a, beam.b, beam.pa\n\n        x, y = self.sky2pix([ra, dec])\n        # We leave the interpolation in the hands of whoever is making these images\n        # clamping the x,y coords at the image boundaries just makes sense\n        x = int(np.clip(x, 0, self.data.shape[1] - 1))\n        y = int(np.clip(y, 0, self.data.shape[2] - 1))\n        psf_sky = self.data[:, x, y]\n        return psf_sky", "category": "Python"}, {"instruction": "def run_until_shutdown(self):\n        ''' Run the Bokeh Server until shutdown is requested by the user,\n        either via a Keyboard interrupt (Ctrl-C) or SIGTERM.\n\n        Calling this method will start the Tornado ``IOLoop`` and block\n        all execution in the calling process.\n\n        Returns:\n            None\n\n        '''\n", "input": "", "output": "        if not self._started:\n            self.start()\n        # Install shutdown hooks\n        atexit.register(self._atexit)\n        signal.signal(signal.SIGTERM, self._sigterm)\n        try:\n            self._loop.start()\n        except KeyboardInterrupt:\n            print(\"\\nInterrupted, shutting down\")\n        self.stop()", "category": "Python"}, {"instruction": "def filter_unbound_ports(query):\n    \"\"\"Filter ports not bound to a host or network\"\"\"\n", "input": "", "output": "    # hack for pep8 E711: comparison to None should be\n    # 'if cond is not None'\n    none = None\n    port_model = models_v2.Port\n    binding_level_model = ml2_models.PortBindingLevel\n    query = (query\n             .join_if_necessary(port_model)\n             .join_if_necessary(binding_level_model)\n             .filter(\n                 binding_level_model.host != '',\n                 port_model.device_id != none,\n                 port_model.network_id != none))\n    return query", "category": "Python"}, {"instruction": "def set_name(self,name=None):\n        \"\"\"\n        Set the name of the object. If no name is given, the\n        name is extracted via the extract_name method.\n        \"\"\"\n", "input": "", "output": "        if name:\n            self.name = name[:254]\n        else:\n            self.name = self.extract_name()[:254]\n\n        self.save()\n\n        return self.name", "category": "Python"}, {"instruction": "def argument(self) -> bool:\n        \"\"\"Parse statement argument.\n\n        Return ``True`` if the argument is followed by block of substatements.\n        \"\"\"\n", "input": "", "output": "        next = self.peek()\n        if next == \"'\":\n            quoted = True\n            self.sq_argument()\n        elif next == '\"':\n            quoted = True\n            self.dq_argument()\n        elif self._arg == \"\":\n            quoted = False\n            self.unq_argument()\n        else:\n            raise UnexpectedInput(self, \"single or double quote\")\n        self.opt_separator()\n        next = self.peek()\n        if next == \";\":\n            return False\n        if next == \"{\":\n            return True\n        elif quoted and next == \"+\":\n            self.offset += 1\n            self.opt_separator()\n            return self.argument()\n        else:\n            raise UnexpectedInput(self, \"';', '{'\" +\n                                  (\" or '+'\" if quoted else \"\"))", "category": "Python"}, {"instruction": "def _huber_loss(x, delta=1.0):\n    \"\"\"Reference: https://en.wikipedia.org/wiki/Huber_loss\"\"\"\n", "input": "", "output": "    return tf.where(\n        tf.abs(x) < delta,\n        tf.square(x) * 0.5, delta * (tf.abs(x) - 0.5 * delta))", "category": "Python"}, {"instruction": "def OnCellText(self, event):\n        \"\"\"Text entry event handler\"\"\"\n", "input": "", "output": "\n        row, col, _ = self.grid.actions.cursor\n        self.grid.GetTable().SetValue(row, col, event.code)\n\n        event.Skip()", "category": "Python"}, {"instruction": "def chunk_cache(method):\n    \"\"\"\n    Caches chunks of default data.\n\n    This decorator caches generated default data so as to\n    avoid recomputing it on a subsequent queries to the\n    provider.\n    \"\"\"\n", "input": "", "output": "\n    @functools.wraps(method)\n    def wrapper(self, context):\n        # Defer to the method if no caching is enabled\n        if not self._is_cached:\n            return method(self, context)\n\n        # Construct the key for the given index\n        name = context.name\n        idx = context.array_extents(name)\n        key = tuple(i for t in idx for i in t)\n        # Access the sub-cache for this array\n        array_cache = self._chunk_cache[name]\n\n        # Cache miss, call the function\n        if key not in array_cache:\n            array_cache[key] = method(self, context)\n\n        return array_cache[key]\n\n    f = wrapper\n    f.__decorator__ = chunk_cache.__name__\n    return f", "category": "Python"}, {"instruction": "def getString(self, config, relation=0):\n        \"\"\"\n        Return a representation of a Radix according to config.\n\n        :param DisplayConfig config: configuration\n        :param int relation: the relation of this value to actual value\n        \"\"\"\n", "input": "", "output": "        return String(config, self.base).xform(self, relation)", "category": "Python"}, {"instruction": "def mac(raw):\n    \"\"\"\n    Converts a raw string to a standardised MAC Address EUI Format.\n\n    :param raw: the raw string containing the value of the MAC Address\n    :return: a string with the MAC Address in EUI format\n\n    Example:\n\n    .. code-block:: python\n\n        >>> mac('0123.4567.89ab')\n        u'01:23:45:67:89:AB'\n\n    Some vendors like Cisco return MAC addresses like a9:c5:2e:7b:6: which is not entirely valid\n    (with respect to EUI48 or EUI64 standards). Therefore we need to stuff with trailing zeros\n\n    Example\n    >>> mac('a9:c5:2e:7b:6:')\n    u'A9:C5:2E:7B:60:00'\n\n    If Cisco or other obscure vendors use their own standards, will throw an error and we can fix\n    later, however, still works with weird formats like:\n\n    >>> mac('123.4567.89ab')\n    u'01:23:45:67:89:AB'\n    >>> mac('23.4567.89ab')\n    u'00:23:45:67:89:AB'\n    \"\"\"\n", "input": "", "output": "    if raw.endswith(':'):\n        flat_raw = raw.replace(':', '')\n        raw = '{flat_raw}{zeros_stuffed}'.format(\n            flat_raw=flat_raw,\n            zeros_stuffed='0'*(12-len(flat_raw))\n        )\n    return py23_compat.text_type(EUI(raw, dialect=_MACFormat))", "category": "Python"}, {"instruction": "def update_launch_metadata(self, scaling_group, metadata):\n        \"\"\"\n        Adds the given metadata dict to the existing metadata for the scaling\n        group's launch configuration.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(scaling_group, ScalingGroup):\n            scaling_group = self.get(scaling_group)\n        curr_meta = scaling_group.launchConfiguration.get(\"args\", {}).get(\n                \"server\", {}).get(\"metadata\", {})\n        curr_meta.update(metadata)\n        return self.update_launch_config(scaling_group, metadata=curr_meta)", "category": "Python"}, {"instruction": "def _construct_single(self, basedir, port, name=None, extra=''):\n        \"\"\"\n        Construct command line strings for a single node.\n\n        Handles shards and stand-alones.\n        \"\"\"\n", "input": "", "output": "        datapath = self._create_paths(basedir, name)\n        self._construct_mongod(os.path.join(datapath, 'db'),\n                               os.path.join(datapath, 'mongod.log'), port,\n                               replset=None, extra=extra)\n\n        host = '%s:%i' % (self.args['hostname'], port)\n\n        return host", "category": "Python"}, {"instruction": "def __safe_callback(self, room_data):\n        \"\"\"\n        Safe use of the callback method, to avoid errors propagation\n\n        :param room_data: A RoomData object\n        \"\"\"\n", "input": "", "output": "        method = room_data.callback\n        if method is not None:\n            try:\n                method(room_data.room, room_data.nick)\n            except Exception as ex:\n                self.__logger.exception(\"Error calling back room creator: %s\",\n                                        ex)", "category": "Python"}, {"instruction": "def _preprocess_data(self, data):\n        \"\"\"\n        At this point, the data either has a `read` attribute (e.g. a file\n        object or a StringIO) or is a string that is a JSON document.\n\n        If self.chunksize, we prepare the data for the `__next__` method.\n        Otherwise, we read it into memory for the `read` method.\n        \"\"\"\n", "input": "", "output": "        if hasattr(data, 'read') and not self.chunksize:\n            data = data.read()\n        if not hasattr(data, 'read') and self.chunksize:\n            data = StringIO(data)\n\n        return data", "category": "Python"}, {"instruction": "def set_ground_width(self, ground_width):\n        '''set ground width of view'''\n", "input": "", "output": "        state = self.state\n        state.ground_width = ground_width\n        state.panel.re_center(state.width/2, state.height/2, state.lat, state.lon)", "category": "Python"}, {"instruction": "def david_go(refseq_list, annot=('SP_PIR_KEYWORDS', 'GOTERM_BP_FAT',\n                                        'GOTERM_CC_FAT', 'GOTERM_MF_FAT')):\n\n        \"\"\"\n        open a web-browser to the DAVID online enrichment tool\n\n        Parameters\n        ----------\n\n        refseq_list : list\n           list of refseq names to check for enrichment\n\n        annot : list\n           iterable of DAVID annotations to check for enrichment\n        \"\"\"\n", "input": "", "output": "        URL = \"http://david.abcc.ncifcrf.gov/api.jsp?type=REFSEQ_MRNA&ids=%s&tool=term2term&annot=\"\n        import webbrowser\n        webbrowser.open(URL % \",\".join(set(refseq_list)) + \",\".join(annot))", "category": "Python"}, {"instruction": "def price_ticks_away(price, n_ticks):\n    \"\"\"Returns an exact, valid Betfair price that is n_ticks \"ticks\" away from\n    the given price. n_ticks may positive, negative or zero (in which case the\n    same price is returned) but if there is no price n_ticks away from the\n    given price then an exception will be thrown.\n\n    :param float price: An exact, valid Betfair price\n    :param float n_ticks: The number of ticks away from price the new price is\n    :returns: An exact, valid Betfair price\n    :rtype: float\n    \"\"\"\n", "input": "", "output": "    price_index = PRICES.index(as_dec(price))\n    return float(PRICES[price_index + n_ticks])", "category": "Python"}, {"instruction": "def get_applications():\n        \"\"\"\n        :return: all knows applications\n        \"\"\"\n", "input": "", "output": "        LOGGER.debug(\"ApplicationService.get_applications\")\n        args = {'http_operation': 'GET', 'operation_path': ''}\n        response = ApplicationService.requester.call(args)\n        ret = None\n        if response.rc == 0:\n            ret = []\n            for application in response.response_content['applications']:\n                ret.append(Application.json_2_application(application))\n        elif response.rc != 404:\n            err_msg = 'ApplicationService.get_applications - Problem while getting applications. ' \\\n                      'Reason: ' + str(response.response_content) + '-' + str(response.error_message) + \\\n                      \" (\" + str(response.rc) + \")\"\n            LOGGER.warning(err_msg)\n        return ret", "category": "Python"}, {"instruction": "def find_trace_file(mname):\n        \"\"\"Tries to find the traces tdms file name\n\n        Returns None if no trace file is found.\n        \"\"\"\n", "input": "", "output": "        mname = pathlib.Path(mname)\n        tname = None\n\n        if mname.exists():\n            cand = mname.with_name(mname.name[:-5] + \"_traces.tdms\")\n            if cand.exists():\n                tname = cand\n\n        return tname", "category": "Python"}, {"instruction": "def remove(self, block_id):\n        \"\"\"Remove a Processing Block from the queue.\n\n        Args:\n            block_id (str):\n        \"\"\"\n", "input": "", "output": "        with self._mutex:\n            entry = self._block_map[block_id]\n            self._queue.remove(entry)", "category": "Python"}, {"instruction": "async def block(client: Client, number: int = 0, block_raw: str = None, signature: str = None) -> Union[dict,\n                                                                                                        ClientResponse]:\n    \"\"\"\n    GET/POST a block from/to the blockchain\n\n    :param client: Client to connect to the api\n    :param number: Block number to get\n    :param block_raw: Block document to post\n    :param signature: Signature of the block document issuer\n    :return:\n    \"\"\"\n", "input": "", "output": "    # POST block\n    if block_raw is not None and signature is not None:\n        return await client.post(MODULE + '/block', {'block': block_raw, 'signature': signature},\n                                 rtype=RESPONSE_AIOHTTP)\n    # GET block\n    return await client.get(MODULE + '/block/%d' % number, schema=BLOCK_SCHEMA)", "category": "Python"}, {"instruction": "def show_subnet(self, subnet, **_params):\n        \"\"\"Fetches information of a certain subnet.\"\"\"\n", "input": "", "output": "        return self.get(self.subnet_path % (subnet), params=_params)", "category": "Python"}, {"instruction": "def fetchall(self):\n        \"\"\"Fetch all rows.\"\"\"\n", "input": "", "output": "        result = self.query.result()\n        return [row.values() for row in result]", "category": "Python"}, {"instruction": "def call_sync(func):\n    \"\"\"Decorates a function to be called sync on the loop thread\"\"\"\n", "input": "", "output": "\n    @wraps(func)\n    def wrapper(self, *args, **kw):\n        ", "category": "Python"}, {"instruction": "def entryCheck(self, event = None, repair = True):\n        \"\"\" Ensure any INDEF entry is uppercase, before base class behavior \"\"\"\n", "input": "", "output": "        valupr = self.choice.get().upper()\n        if valupr.strip() == 'INDEF':\n            self.choice.set(valupr)\n        return EparOption.entryCheck(self, event, repair = repair)", "category": "Python"}, {"instruction": "def transpose_ntpl_list(lst):\n    \"\"\"Transpose a list of named tuple objects (of the same type) into a\n    named tuple of lists.\n\n    Parameters\n    ----------\n    lst : list of collections.namedtuple object\n      List of named tuple objects of the same type\n\n    Returns\n    -------\n    ntpl : collections.namedtuple object\n      Named tuple object with each entry consisting of a list of the\n      corresponding fields of the named tuple objects in list ``lst``\n    \"\"\"\n", "input": "", "output": "\n    if not lst:\n        return None\n    else:\n        cls = collections.namedtuple(lst[0].__class__.__name__,\n                                     lst[0]._fields)\n        return cls(*[[lst[k][l] for k in range(len(lst))]\n                     for l in range(len(lst[0]))])", "category": "Python"}, {"instruction": "def tree(self):\n        \"\"\"\n        :rtype: cmdtree.tree.CmdTree\n        \"\"\"\n", "input": "", "output": "        from cmdtree.tree import CmdTree\n        if self._tree is None:\n            self._tree = CmdTree()\n        return self._tree", "category": "Python"}, {"instruction": "def project(self, projection):\n        '''\n        Return coordinates transformed to a given projection\n        Projection should be a basemap or pyproj projection object or similar\n        '''\n", "input": "", "output": "        x, y = projection(self.lon.decimal_degree, self.lat.decimal_degree)\n        return (x, y)", "category": "Python"}, {"instruction": "def finish():\n    # type: () -> None\n    \"\"\" Merge current feature branch into develop. \"\"\"\n", "input": "", "output": "    pretend = context.get('pretend', False)\n\n    if not pretend and (git.staged() or git.unstaged()):\n        log.err(\n            \"You have uncommitted changes in your repo!\\n\"\n            \"You need to stash them before you merge the hotfix branch\"\n        )\n        sys.exit(1)\n\n    branch = git.current_branch(refresh=True)\n    base = common.get_base_branch()\n\n    prompt = \"<32>Merge <33>{}<32> into <33>{}<0>?\".format(branch.name, base)\n    if not click.confirm(shell.fmt(prompt)):\n        log.info(\"Cancelled\")\n        return\n\n    common.assert_branch_type('task')\n\n    # Merge task into it's base feature branch\n    common.git_checkout(base)\n    common.git_pull(base)\n    common.git_merge(base, branch.name)\n\n    # Cleanup\n    common.git_branch_delete(branch.name)\n    common.git_prune()\n\n    common.git_checkout(base)", "category": "Python"}, {"instruction": "def _size_fmt(num):\n    '''\n    Format bytes as human-readable file sizes\n    '''\n", "input": "", "output": "    try:\n        num = int(num)\n        if num < 1024:\n            return '{0} bytes'.format(num)\n        num /= 1024.0\n        for unit in ('KiB', 'MiB', 'GiB', 'TiB', 'PiB'):\n            if num < 1024.0:\n                return '{0:3.1f} {1}'.format(num, unit)\n            num /= 1024.0\n    except Exception:\n        log.error('Unable to format file size for \\'%s\\'', num)\n        return 'unknown'", "category": "Python"}, {"instruction": "def set_device(self, device_name):\n    \"\"\"\n    Set the device before the next fprop to create a new graph on the\n    specified device.\n    \"\"\"\n", "input": "", "output": "    device_name = unify_device_name(device_name)\n    self.device_name = device_name\n    for layer in self.layers:\n      layer.device_name = device_name", "category": "Python"}, {"instruction": "def _del_lock(end_state: NettingChannelEndState, secrethash: SecretHash) -> None:\n    \"\"\"Removes the lock from the indexing structures.\n\n    Note:\n        This won't change the merkletree!\n    \"\"\"\n", "input": "", "output": "    assert is_lock_pending(end_state, secrethash)\n\n    _del_unclaimed_lock(end_state, secrethash)\n\n    if secrethash in end_state.secrethashes_to_onchain_unlockedlocks:\n        del end_state.secrethashes_to_onchain_unlockedlocks[secrethash]", "category": "Python"}, {"instruction": "def get_mime_data(self, mime_type):\n        \"\"\"Return mime data previously attached to surface\n        using the specified mime type.\n\n        :param mime_type: The MIME type of the image data.\n        :type mime_type: ASCII string\n        :returns:\n            A CFFI buffer object, or :obj:`None`\n            if no data has been attached with the given mime type.\n\n        *New in cairo 1.10.*\n\n        \"\"\"\n", "input": "", "output": "        buffer_address = ffi.new('unsigned char **')\n        buffer_length = ffi.new('unsigned long *')\n        mime_type = ffi.new('char[]', mime_type.encode('utf8'))\n        cairo.cairo_surface_get_mime_data(\n            self._pointer, mime_type, buffer_address, buffer_length)\n        return (ffi.buffer(buffer_address[0], buffer_length[0])\n                if buffer_address[0] != ffi.NULL else None)", "category": "Python"}, {"instruction": "def remove_handler_if_not_configured(self, dict_config, requested_handlers, handler_name, check_key) -> None:\n        \"\"\"\n        Remove ``handler_name`` from ``dict_config`` and ``requested_handlers`` if ``check_key`` is empty.\n        \"\"\"\n", "input": "", "output": "        try:\n            if not dict_config[\"handlers\"][handler_name][check_key]:\n                dict_config[\"handlers\"].pop(handler_name)\n                if handler_name in requested_handlers:\n                    requested_handlers.remove(handler_name)\n        except KeyError:\n            # Ignore key errors\n            pass", "category": "Python"}, {"instruction": "def _get_dependency_order(g, node_list):\n    \"\"\"Return list of nodes as close as possible to the ordering in node_list,\n    but with child nodes earlier in the list than parents.\"\"\"\n", "input": "", "output": "    access_ = accessibility(g)\n    deps = dict((k, set(v) - set([k])) for k, v in access_.iteritems())\n    nodes = node_list + list(set(g.nodes()) - set(node_list))\n    ordered_nodes = []\n\n    while nodes:\n        n_ = nodes[0]\n        n_deps = deps.get(n_)\n        if (n_ in ordered_nodes) or (n_deps is None):\n            nodes = nodes[1:]\n            continue\n\n        moved = False\n        for i, n in enumerate(nodes[1:]):\n            if n in n_deps:\n                nodes = [nodes[i + 1]] + nodes[:i + 1] + nodes[i + 2:]\n                moved = True\n                break\n\n        if not moved:\n            ordered_nodes.append(n_)\n            nodes = nodes[1:]\n\n    return ordered_nodes", "category": "Python"}, {"instruction": "def is_blacklisted_import(importer, fullname):\n    \"\"\"\n    Return :data:`True` if `fullname` is part of a blacklisted package, or if\n    any packages have been whitelisted and `fullname` is not part of one.\n\n    NB:\n      - If a package is on both lists, then it is treated as blacklisted.\n      - If any package is whitelisted, then all non-whitelisted packages are\n        treated as blacklisted.\n    \"\"\"\n", "input": "", "output": "    return ((not any(fullname.startswith(s) for s in importer.whitelist)) or\n                (any(fullname.startswith(s) for s in importer.blacklist)))", "category": "Python"}, {"instruction": "def get_smart_contract_event_by_height(self, height: int, is_full: bool = False) -> List[dict]:\n        \"\"\"\n        This interface is used to get the corresponding smart contract event based on the height of block.\n\n        :param height: a decimal height value.\n        :param is_full:\n        :return: the information of smart contract event in dictionary form.\n        \"\"\"\n", "input": "", "output": "        payload = self.generate_json_rpc_payload(RpcMethod.GET_SMART_CONTRACT_EVENT, [height, 1])\n        response = self.__post(self.__url, payload)\n        if is_full:\n            return response\n        event_list = response['result']\n        if event_list is None:\n            event_list = list()\n        return event_list", "category": "Python"}, {"instruction": "def queue(self, blogname, **kwargs):\n        \"\"\"\n        Gets posts that are currently in the blog's queue\n\n        :param limit: an int, the number of posts you want returned\n        :param offset: an int, the post you want to start at, for pagination.\n        :param filter: the post format that you want returned: HTML, text, raw.\n\n        :returns: a dict created from the JSON response\n        \"\"\"\n", "input": "", "output": "        url = \"/v2/blog/{}/posts/queue\".format(blogname)\n        return self.send_api_request(\"get\", url, kwargs, ['limit', 'offset', 'filter'])", "category": "Python"}, {"instruction": "def ensure_crossplat_path(path, winroot='C:'):\n    r\"\"\"\n    ensure_crossplat_path\n\n    Args:\n        path (str):\n\n    Returns:\n        str: crossplat_path\n\n    Example(DOCTEST):\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_path import *  # NOQA\n        >>> path = r'C:\\somedir'\n        >>> cplat_path = ensure_crossplat_path(path)\n        >>> result = cplat_path\n        >>> print(result)\n        C:/somedir\n    \"\"\"\n", "input": "", "output": "    cplat_path = path.replace('\\\\', '/')\n    if cplat_path == winroot:\n        cplat_path += '/'\n    return cplat_path", "category": "Python"}, {"instruction": "def convert_scalar_multiply(net, node, model, builder):\n    \"\"\"Convert a scalar multiply layer from mxnet to coreml.\n\n    Parameters\n    ----------\n    net: network\n        A mxnet network object.\n\n    node: layer\n        Node to convert.\n\n    model: model\n        An model for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"\n", "input": "", "output": "    import numpy as _np\n    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    param = _get_attr(node)\n    alpha = _np.array([float(param['scalar'])])\n    builder.add_scale(name = name, input_name = input_name,\n            output_name = output_name, W = alpha, has_bias=False, b=None)", "category": "Python"}, {"instruction": "def remove(self, addon, dev=False):\n        \"\"\"Remove a dependency and uninstall it.\"\"\"\n", "input": "", "output": "        dependencies = self.get_dependency_manager(dev=dev)\n        other_dependencies = self.get_dependency_manager(dev=not dev)\n        self.stdout.write(style.format_command('Removing', addon))\n        removed = dependencies.remove(addon, warn=False)\n        if not removed:\n            removed = other_dependencies.remove(addon, warn=False)\n\n        if removed:\n            self.build()\n        else:\n            exception = '%s is not installed.' % Dependency(addon).to_stdout()\n            self.stdout.write(style.red(exception))", "category": "Python"}, {"instruction": "def find_inherited_key_completions(rootpath, root_env):\n  \"\"\"Return completion keys from INHERITED tuples.\n\n  Easiest way to get those is to evaluate the tuple, check if it is a CompositeTuple,\n  then enumerate the keys that are NOT in the rightmost tuple.\n  \"\"\"\n", "input": "", "output": "  tup = inflate_context_tuple(rootpath, root_env)\n  if isinstance(tup, runtime.CompositeTuple):\n    keys = set(k for t in tup.tuples[:-1] for k in t.keys())\n    return {n: get_completion(tup, n) for n in keys}\n  return {}", "category": "Python"}, {"instruction": "def handle(self, **options):\n        \"\"\"Collect tools.\"\"\"\n", "input": "", "output": "        self.set_options(**options)\n\n        os.makedirs(self.destination_path, exist_ok=True)\n\n        if self.interactive and any(os.listdir(self.destination_path)):\n            self.get_confirmation()\n\n        if self.clear:\n            self.clear_dir()\n\n        self.collect()", "category": "Python"}, {"instruction": "def generate(self, chars, format='png'):\n        \"\"\"Generate an Image Captcha of the given characters.\n\n        :param chars: text to be generated.\n        :param format: image file format\n        \"\"\"\n", "input": "", "output": "        im = self.generate_image(chars)\n        out = BytesIO()\n        im.save(out, format=format)\n        out.seek(0)\n        return out", "category": "Python"}, {"instruction": "def get_zeta_i_j(self, X):\n\t\t'''\n\t\tParameters\n\t\t----------\n\t\tX : np.array\n\t\t\tArray of word counts, shape (N, 2) where N is the vocab size.  X[:,0] is the\n\t\t\tpositive class, while X[:,1] is the negative class. None by default\n\n\t\tReturns\n\t\t-------\n\t\tnp.array of z-scores\n\t\t'''\n", "input": "", "output": "\t\ty_i, y_j = X.T[0], X.T[1]\n\t\treturn self.get_zeta_i_j_given_separate_counts(y_i, y_j)", "category": "Python"}, {"instruction": "def pix2canvas(self, pt):\n        \"\"\"Takes a 2-tuple of (x, y) in window coordinates and gives\n        the (cx, cy, cz) coordinates on the canvas.\n        \"\"\"\n", "input": "", "output": "        x, y = pt[:2]\n        #print('p2c in', x, y)\n        mm = gl.glGetDoublev(gl.GL_MODELVIEW_MATRIX)\n        pm = gl.glGetDoublev(gl.GL_PROJECTION_MATRIX)\n        vp = gl.glGetIntegerv(gl.GL_VIEWPORT)\n\n        win_x, win_y = float(x), float(vp[3] - y)\n        win_z = gl.glReadPixels(int(win_x), int(win_y), 1, 1,\n                                gl.GL_DEPTH_COMPONENT, gl.GL_FLOAT)\n        pos = glu.gluUnProject(win_x, win_y, win_z, mm, pm, vp)\n        #print('out', pos)\n        return pos", "category": "Python"}, {"instruction": "def smse(y_true, y_pred):\n    \"\"\"\n    Standardised mean squared error.\n\n    Parameters\n    ----------\n    y_true: ndarray\n        vector of true targets\n    y_pred: ndarray\n        vector of predicted targets\n\n    Returns\n    -------\n    float:\n        SMSE of predictions vs truth (scalar)\n\n    Example\n    -------\n    >>> y_true = np.random.randn(100)\n    >>> smse(y_true, y_true)\n    0.0\n    >>> smse(y_true, np.random.randn(100)) >= 1.0\n    True\n    \"\"\"\n", "input": "", "output": "\n    N = y_true.shape[0]\n    return ((y_true - y_pred)**2).sum() / (N * y_true.var())", "category": "Python"}, {"instruction": "def set_mode_auto(self):\n        '''enter auto mode'''\n", "input": "", "output": "        if self.mavlink10():\n            self.mav.command_long_send(self.target_system, self.target_component,\n                                       mavlink.MAV_CMD_MISSION_START, 0, 0, 0, 0, 0, 0, 0, 0)\n        else:\n            MAV_ACTION_SET_AUTO = 13\n            self.mav.action_send(self.target_system, self.target_component, MAV_ACTION_SET_AUTO)", "category": "Python"}, {"instruction": "def download_and_compile_igraph(self):\n        \"\"\"Downloads and compiles the C core of igraph.\"\"\"\n", "input": "", "output": "        print(\"We will now try to download and compile the C core from scratch.\")\n        print(\"Version number of the C core: %s\" % self.c_core_versions[0])\n        if len(self.c_core_versions) > 1:\n            print(\"We will also try: %s\" % \", \".join(self.c_core_versions[1:]))\n        print(\"\")\n\n        igraph_builder = IgraphCCoreBuilder(self.c_core_versions, self.c_core_url,\n                show_progress_bar=self.show_progress_bar)\n        if not igraph_builder.run():\n            print(\"Could not download and compile the C core of igraph.\")\n            print(\"\")\n            return False\n        else:\n            return True", "category": "Python"}, {"instruction": "def workflow_list_projects(object_id, input_params={}, always_retry=True, **kwargs):\n    \"\"\"\n    Invokes the /workflow-xxxx/listProjects API method.\n\n    For more info, see: https://wiki.dnanexus.com/API-Specification-v1.0.0/Cloning#API-method%3A-%2Fclass-xxxx%2FlistProjects\n    \"\"\"\n", "input": "", "output": "    return DXHTTPRequest('/%s/listProjects' % object_id, input_params, always_retry=always_retry, **kwargs)", "category": "Python"}, {"instruction": "def unpackb(packed, **kwargs):\n    '''\n    .. versionadded:: 2018.3.4\n\n    Wraps msgpack.unpack.\n\n    By default, this function uses the msgpack module and falls back to\n    msgpack_pure, if the msgpack is not available. You can pass an alternate\n    msgpack module using the _msgpack_module argument.\n    '''\n", "input": "", "output": "    msgpack_module = kwargs.pop('_msgpack_module', msgpack)\n    return msgpack_module.unpackb(packed, **kwargs)", "category": "Python"}, {"instruction": "def create_branch(version):\n    \"\"\"Create a fresh branch from upstream/master\"\"\"\n", "input": "", "output": "    repo = Repo.init(\".\")\n    if repo.is_dirty(untracked_files=True):\n        raise RuntimeError(f\"Repository is dirty, please commit/stash your changes.\")\n\n    branch_name = f\"release-{version}\"\n    print(f\"{Fore.CYAN}Create {branch_name} branch from upstream master\")\n    upstream = get_upstream(repo)\n    upstream.fetch()\n    release_branch = repo.create_head(branch_name, upstream.refs.master, force=True)\n    release_branch.checkout()\n    return repo", "category": "Python"}, {"instruction": "def set_proto_message_event(\n        pb_message_event,\n        span_data_message_event):\n    \"\"\"Sets properties on the protobuf message event.\n\n    :type pb_message_event:\n        :class: `~opencensus.proto.trace.Span.TimeEvent.MessageEvent`\n    :param pb_message_event: protobuf message event\n\n    :type span_data_message_event:\n        :class: `~opencensus.trace.time_event.MessageEvent`\n    :param span_data_message_event: opencensus message event\n    \"\"\"\n", "input": "", "output": "\n    pb_message_event.type = span_data_message_event.type\n    pb_message_event.id = span_data_message_event.id\n    pb_message_event.uncompressed_size = \\\n        span_data_message_event.uncompressed_size_bytes\n    pb_message_event.compressed_size = \\\n        span_data_message_event.compressed_size_bytes", "category": "Python"}, {"instruction": "def get_vehicle_health_report(session, vehicle_index):\n    \"\"\"Get complete vehicle health report.\"\"\"\n", "input": "", "output": "    profile = get_profile(session)\n    _validate_vehicle(vehicle_index, profile)\n    return session.get(VHR_URL, params={\n        'uuid': profile['vehicles'][vehicle_index]['uuid']\n    }).json()", "category": "Python"}, {"instruction": "def get_user_enclaves(self):\n        \"\"\"\n        Gets the list of enclaves that the user has access to.\n\n        :return: A list of |EnclavePermissions| objects, each representing an enclave and whether the requesting user\n            has read, create, and update access to it.\n        \"\"\"\n", "input": "", "output": "\n        resp = self._client.get(\"enclaves\")\n        return [EnclavePermissions.from_dict(enclave) for enclave in resp.json()]", "category": "Python"}, {"instruction": "def QueryItemsChangeFeed(self, collection_link, options=None):\n        \"\"\"Queries documents change feed in a collection.\n\n        :param str collection_link:\n            The link to the document collection.\n        :param dict options:\n            The request options for the request.\n            options may also specify partition key range id.\n\n        :return:\n            Query Iterable of Documents.\n        :rtype:\n            query_iterable.QueryIterable\n\n        \"\"\"\n", "input": "", "output": "\n        partition_key_range_id = None\n        if options is not None and 'partitionKeyRangeId' in options:\n            partition_key_range_id = options['partitionKeyRangeId']\n\n        return self._QueryChangeFeed(collection_link, \"Documents\" , options, partition_key_range_id)", "category": "Python"}, {"instruction": "def process_chunk(self, lenient=False):\n        \"\"\"\n        Process the next chunk and its data.\n        This only processes the following chunk types:\n        ``IHDR``, ``PLTE``, ``bKGD``, ``tRNS``, ``gAMA``, ``sBIT``, ``pHYs``.\n        All other chunk types are ignored.\n\n        If the optional `lenient` argument evaluates to `True`,\n        checksum failures will raise warnings rather than exceptions.\n        \"\"\"\n", "input": "", "output": "\n        type, data = self.chunk(lenient=lenient)\n        method = '_process_' + type.decode('ascii')\n        m = getattr(self, method, None)\n        if m:\n            m(data)", "category": "Python"}, {"instruction": "def generate(env):\n    \"\"\"Add Builders and construction variables for midl to an Environment.\"\"\"\n", "input": "", "output": "\n    env['MIDL']          = 'MIDL.EXE'\n    env['MIDLFLAGS']     = SCons.Util.CLVar('/nologo')\n    env['MIDLCOM']       = '$MIDL $MIDLFLAGS /tlb ${TARGETS[0]} /h ${TARGETS[1]} /iid ${TARGETS[2]} /proxy ${TARGETS[3]} /dlldata ${TARGETS[4]} $SOURCE 2> NUL'\n    env['BUILDERS']['TypeLibrary'] = midl_builder", "category": "Python"}, {"instruction": "def parse_log_path(args, trial_content):\n    '''parse log path'''\n", "input": "", "output": "    path_list = []\n    host_list = []\n    for trial in trial_content:\n        if args.trial_id and args.trial_id != 'all' and trial.get('id') != args.trial_id:\n            continue\n        pattern = r'(?P<head>.+)://(?P<host>.+):(?P<path>.*)'\n        match = re.search(pattern,trial['logPath'])\n        if match:\n            path_list.append(match.group('path'))\n            host_list.append(match.group('host'))\n    if not path_list:\n        print_error('Trial id %s error!' % args.trial_id)\n        exit(1)\n    return path_list, host_list", "category": "Python"}, {"instruction": "def get_int_from_user(self, title=\"Enter integer value\",\n                          cond_func=lambda i: i is not None):\n        \"\"\"Opens an integer entry dialog and returns integer\n\n        Parameters\n        ----------\n        title: String\n        \\tDialog title\n        cond_func: Function\n        \\tIf cond_func of int(<entry_value> then result is returned.\n        \\tOtherwise the dialog pops up again.\n\n        \"\"\"\n", "input": "", "output": "\n        is_integer = False\n\n        while not is_integer:\n            dlg = wx.TextEntryDialog(None, title, title)\n\n            if dlg.ShowModal() == wx.ID_OK:\n                result = dlg.GetValue()\n            else:\n                return None\n\n            dlg.Destroy()\n\n            try:\n                integer = int(result)\n\n                if cond_func(integer):\n                    is_integer = True\n\n            except ValueError:\n                pass\n\n        return integer", "category": "Python"}, {"instruction": "def K(self, parm):\n        \"\"\" Returns the Gram Matrix\n\n        Parameters\n        ----------\n        parm : np.ndarray\n            Parameters for the Gram Matrix\n\n        Returns\n        ----------\n        - Gram Matrix (np.ndarray)\n        \"\"\"\n", "input": "", "output": "        return ARD_K_matrix(self.X, parm) + np.identity(self.X.shape[0])*(10**-10)", "category": "Python"}, {"instruction": "def limits_hydrate(db, lims):\n    \"\"\"\n    Helper function to hydrate a list of limits.\n\n    :param db: A database handle.\n    :param lims: A list of limit strings, as retrieved from the\n                 database.\n    \"\"\"\n", "input": "", "output": "\n    return [limits.Limit.hydrate(db, lim) for lim in lims]", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"Stop the sensor.\n        \"\"\"\n", "input": "", "output": "        # check that everything is running\n        if not self._running:\n            logging.warning('Realsense not running. Aborting stop.')\n            return False\n\n        self._pipe.stop()\n        self._running = False\n        return True", "category": "Python"}, {"instruction": "def air_dps(self) -> Union[int, float]:\n        \"\"\" Does not include upgrades \"\"\"\n", "input": "", "output": "        if self._weapons:\n            weapon = next(\n                (weapon for weapon in self._weapons if weapon.type in {TargetType.Air.value, TargetType.Any.value}),\n                None,\n            )\n            if weapon:\n                return (weapon.damage * weapon.attacks) / weapon.speed\n        return 0", "category": "Python"}, {"instruction": "def random_line_data(chars_per_line=80):\n    \"\"\"\n    Function to create a line of a random string\n    Args:\n        chars_per_line: An integer that says how many characters to return\n\n    Returns:\n        A String\n\n    \"\"\"\n", "input": "", "output": "    return ''.join(__random.choice(__string.ascii_letters) for x in range(chars_per_line))", "category": "Python"}, {"instruction": "def _extract_text_and_child_element_list(minidom_node):\n    \"\"\"Returns a pair of the \"child\" content of minidom_node:\n        the first element of the pair is a concatenation of the text content\n        the second element is a list of non-text nodes.\n\n    The string concatenation strips leading and trailing whitespace from each\n    bit of text found and joins the fragments (with no separator between them).\n    \"\"\"\n", "input": "", "output": "    tl = []\n    ntl = []\n    for c in minidom_node.childNodes:\n        if c.nodeType == xml.dom.minidom.Node.TEXT_NODE:\n            tl.append(c)\n        else:\n            ntl.append(c)\n    try:\n        tl = [i.data.strip() for i in tl]\n        text_content = ''.join(tl)\n    except:\n        text_content = ''\n    return text_content, ntl", "category": "Python"}, {"instruction": "def hl_canvas2table_box(self, canvas, tag):\n        \"\"\"Highlight all markings inside user drawn box on table.\"\"\"\n", "input": "", "output": "        self.treeview.clear_selection()\n\n        # Remove existing box\n        cobj = canvas.get_object_by_tag(tag)\n        if cobj.kind != 'rectangle':\n            return\n        canvas.delete_object_by_tag(tag, redraw=False)\n\n        # Remove existing highlight\n        if self.markhltag:\n            try:\n                canvas.delete_object_by_tag(self.markhltag, redraw=True)\n            except Exception:\n                pass\n\n        # Nothing to do if no markings are displayed\n        try:\n            obj = canvas.get_object_by_tag(self.marktag)\n        except Exception:\n            return\n\n        if obj.kind != 'compound':\n            return\n\n        # Nothing to do if table has no data\n        if (len(self._xarr) == 0 or len(self._yarr) == 0 or\n                len(self._treepaths) == 0):\n            return\n\n        # Find markings inside box\n        mask = cobj.contains_arr(self._xarr, self._yarr)\n\n        for hlpath in self._treepaths[mask]:\n            self._highlight_path(hlpath)", "category": "Python"}, {"instruction": "def encode_csv(data_dict, column_names):\n  \"\"\"Builds a csv string.\n\n  Args:\n    data_dict: dict of {column_name: 1 value}\n    column_names: list of column names\n\n  Returns:\n    A csv string version of data_dict\n  \"\"\"\n", "input": "", "output": "  import csv\n  import six\n  values = [str(data_dict[x]) for x in column_names]\n  str_buff = six.StringIO()\n  writer = csv.writer(str_buff, lineterminator='')\n  writer.writerow(values)\n  return str_buff.getvalue()", "category": "Python"}, {"instruction": "def generate_eq(*members):\n    \"\"\"\n    Decorator that generates equality and inequality operators for the\n    decorated class. The given members as well as the type of self and other\n    will be taken into account.\n\n    Note that this decorator modifies the given class in place!\n\n    :param members: A list of members to compare for equality.\n    \"\"\"\n", "input": "", "output": "    def decorator(cls):\n        def eq(self, other):\n            if not isinstance(other, cls):\n                return False\n\n            return all(getattr(self, member) == getattr(other, member)\n                       for member in members)\n\n        def ne(self, other):\n            return not eq(self, other)\n\n        cls.__eq__ = eq\n        cls.__ne__ = ne\n        return cls\n\n    return decorator", "category": "Python"}, {"instruction": "def _on_select(self, *args):\n        \"\"\"\n        Function bound to event of selection in the Combobox, calls callback if callable\n        \n        :param args: Tkinter event\n        \"\"\"\n", "input": "", "output": "        if callable(self.__callback):\n            self.__callback(self.selection)", "category": "Python"}, {"instruction": "def update_record_field(table, sys_id, field, value):\n    '''\n    Update the value of a record's field in a servicenow table\n\n    :param table: The table name, e.g. sys_user\n    :type  table: ``str``\n\n    :param sys_id: The unique ID of the record\n    :type  sys_id: ``str``\n\n    :param field: The new value\n    :type  field: ``str``\n\n    :param value: The new value\n    :type  value: ``str``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion servicenow.update_record_field sys_user 2348234 first_name jimmy\n    '''\n", "input": "", "output": "    client = _get_client()\n    client.table = table\n    response = client.update({field: value}, sys_id)\n    return response", "category": "Python"}, {"instruction": "def ensure_is_date_object(x):\n    \"\"\"\n    Ensure input represents a valid date and return the corresponding `datetime.date` object.\n\n    Valid inputs:\n\n      - string of the form \"YYYY-MM-DD\"\n      - dt.date object\n      - pd.Timestamp of the form \"YYYY-MM-DD 00:00:00\" with freq='D' (as is generated by pd.date_range())\n    \"\"\"\n", "input": "", "output": "    error_msg = f\"Cannot convert input to date object: {x} (type: {type(x)})\"\n\n    if isinstance(x, dt.date):\n        if isinstance(x, pd.Timestamp):\n            if x.freq != 'D':\n                raise TohuDateError(\"Pandas Timestamp must have freq='D' set. Got: freq={x.freq!r}\")\n            elif pd.Timestamp(x.date()) == x:\n                return x.date()\n            else:\n                raise TohuDateError(error_msg)\n        elif isinstance(x, dt.datetime):\n            raise TohuDateError(error_msg)\n        else:\n            return x\n    elif isinstance(x, str):\n        return parse_date_string(x)\n    else:\n        raise TohuDateError(error_msg)", "category": "Python"}, {"instruction": "def to_cnf(self):\n        \"\"\"Return an equivalent expression in conjunctive normal form.\"\"\"\n", "input": "", "output": "        node = self.node.to_cnf()\n        if node is self.node:\n            return self\n        else:\n            return _expr(node)", "category": "Python"}, {"instruction": "def add_items_to_message(msg, log_dict):\n    \"\"\"Utility function to add dictionary items to a log message.\"\"\"\n", "input": "", "output": "    out = msg\n    for key, value in log_dict.items():\n        out += \" {}={}\".format(key, value)\n    return out", "category": "Python"}, {"instruction": "def yield_string_argument(node, pos):\n    \"\"\"\n    Yield just a string argument from position of the function call.\n    \"\"\"\n", "input": "", "output": "\n    if not isinstance(node.args.items[pos], asttypes.String):\n        return\n    yield to_str(node.args.items[pos])", "category": "Python"}, {"instruction": "def pset(self, n):\n        \"\"\"\n        Convert the nodes nsprefixes into a set.\n        @param n: A node.\n        @type n: L{Element}\n        @return: A set of namespaces.\n        @rtype: set\n        \"\"\"\n", "input": "", "output": "        s = set()\n        for ns in n.nsprefixes.items():\n            if self.permit(ns):\n                s.add(ns[1])\n        return s", "category": "Python"}, {"instruction": "def hook(self, name):\n        \"\"\" Return a decorator that attaches a callback to a hook. \"\"\"\n", "input": "", "output": "        def wrapper(func):\n            self.hooks.add(name, func)\n            return func\n        return wrapper", "category": "Python"}, {"instruction": "def _strip_line_sep(self, s):\n        \"\"\"Strip trailing line separators from s, but no other whitespaces.\"\"\"\n", "input": "", "output": "        if s[-2:] == b'\\r\\n':\n            return s[:-2]\n        elif s[-1:] == b'\\n':\n            return s[:-1]\n        else:\n            return s", "category": "Python"}, {"instruction": "def render_reverse(self, inst=None, context=None):\n        \"\"\"Renders the reverse URL for this path.\"\"\"\n", "input": "", "output": "        rendered = self.render(inst=inst, context=context)\n        parts = rendered.split('/')\n        # we only prettify URLs for these files\n        if parts[-1] in ['index.html', 'index.htm']:\n            return ('/'.join(parts[:-1])) + '/'\n        return rendered", "category": "Python"}, {"instruction": "def compound_id(obj):\n    \"\"\"Generate a hierarchical compound ID, separated by dots.\"\"\"\n", "input": "", "output": "    if isinstance(obj, (Category, Session)):\n        raise TypeError('Compound IDs are not supported for this entry type')\n    elif isinstance(obj, Event):\n        return unicode(obj.id)\n    elif isinstance(obj, Contribution):\n        return '{}.{}'.format(obj.event_id, obj.id)\n    elif isinstance(obj, SubContribution):\n        return '{}.{}.{}'.format(obj.contribution.event_id, obj.contribution_id, obj.id)", "category": "Python"}, {"instruction": "def generate_shapes(shapes):\n        \"\"\"\n        Generate the shapes for the topology\n\n        :param dict shapes: A dict of converted shapes from the old topology\n        :return: dict containing two lists (ellipse, rectangle)\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        new_shapes = {'ellipse': [], 'rectangle': []}\n\n        for shape in shapes:\n            tmp_shape = {}\n            for shape_item in shapes[shape]:\n                if shape_item != 'type':\n                    tmp_shape[shape_item] = shapes[shape][shape_item]\n\n            new_shapes[shapes[shape]['type']].append(tmp_shape)\n\n        return new_shapes", "category": "Python"}, {"instruction": "async def _set_wallets(an_data: dict) -> dict:\n    \"\"\"\n    Set wallets as configured for setnym operation.\n\n    :param an_data: dict mapping profiles to anchor data\n    :return: dict mapping anchor names to wallet objects\n    \"\"\"\n", "input": "", "output": "\n    w_mgr = WalletManager()\n    rv = {}\n    for profile in an_data:\n        w_cfg = {'id': an_data[profile].name}\n        if an_data[profile].wallet_type:\n            w_cfg['storage_type'] = an_data[profile].wallet_type\n        if an_data[profile].seed:\n            w_cfg['seed'] = an_data[profile].seed\n        if an_data[profile].did:\n            w_cfg['did'] = an_data[profile].did\n        if an_data[profile].wallet_create:\n            try:\n                await w_mgr.create(w_cfg, access=an_data[profile].wallet_access)\n            except ExtantWallet:\n                pass\n        rv[profile] = w_mgr.get(w_cfg, access=an_data[profile].wallet_access)\n\n    return rv", "category": "Python"}, {"instruction": "def setup_logging(self):\n        \"\"\"Set up self.logger\n\n        This function is called after load_configuration() and after changing\n        to new user/group IDs (if configured). Logging to syslog using the\n        root logger is configured by default, you can override this method if\n        you want something else.\n        \"\"\"\n", "input": "", "output": "        self.logger = logging.getLogger()\n\n        if os.path.exists('/dev/log'):\n            handler = SysLogHandler('/dev/log')\n        else:\n            handler = SysLogHandler()\n        self.logger.addHandler(handler)", "category": "Python"}, {"instruction": "def _build_query(self):\n        \"\"\"\n        Build the base query dictionary\n        \"\"\"\n", "input": "", "output": "        if isinstance(self._query_string, QueryString):\n            self._query_dsl = self._query_string\n        elif isinstance(self._query_string, string_types):\n            self._query_dsl = QueryString(self._query_string)\n        else:\n            self._query_dsl = MatchAll()", "category": "Python"}, {"instruction": "def accel_fl(q: np.ndarray):\n    \"\"\"Accelaration in the earth-sun system using Fluxion potential energy\"\"\"\n", "input": "", "output": "    # Infer number of dimensions from q\n    dims: int = len(q)\n    # Number of celestial bodies\n    B: int = dims // 3\n\n    # The force given the positions q of the bodies\n    f = force(q)\n\n    # The accelerations from this force\n    a = np.zeros(dims)\n    for i in range(B):\n        a[slices[i]] = f[slices[i]] / mass[i]\n    \n    return a", "category": "Python"}, {"instruction": "def _get_date_time_format(dt_string):\n    '''\n    Function that detects the date/time format for the string passed.\n\n    :param str dt_string:\n        A date/time string\n\n    :return: The format of the passed dt_string\n    :rtype: str\n\n    :raises: SaltInvocationError on Invalid Date/Time string\n    '''\n", "input": "", "output": "    valid_formats = [\n        '%H:%M',\n        '%H:%M:%S',\n        '%m:%d:%y',\n        '%m:%d:%Y',\n        '%m/%d/%y',\n        '%m/%d/%Y'\n    ]\n    for dt_format in valid_formats:\n        try:\n            datetime.strptime(dt_string, dt_format)\n            return dt_format\n        except ValueError:\n            continue\n    msg = 'Invalid Date/Time Format: {0}'.format(dt_string)\n    raise SaltInvocationError(msg)", "category": "Python"}, {"instruction": "def fetchall(self):\n        \"\"\" returns the remainder of records from the query. This is \n\t guaranteed by locking, so that no other thread can grab a few records \n\t while the set is fetched. This has the side effect that other threads \n\t may have to wait for an arbitrary long time until this query is done \n\t before they can return (obviously with None).\n        \"\"\"\n", "input": "", "output": "        self._cursorLock.acquire()\n\n        recs = []\n        while True:\n            rec = self.fetchone()\n            if rec is None:\n               break\n            recs.append(rec)\n\n        self._cursorLock.release()\n        return recs", "category": "Python"}, {"instruction": "def getAllText(cls, where=None, SEPERATOR=' ', orderBy=None):\n        \"\"\"Retrieve a list of of all possible instances of this class.\n\n        The list is composed of tuples in the format (id, description) -\n        where description is a string composed by the fields from\n        cls._shortView, joint with SEPERATOR.\n        \"\"\"\n", "input": "", "output": "        (sql, fields) = cls._prepareSQL(\"SELECTALL\", where, orderBy=orderBy)\n        curs = cls.cursor()\n        curs.execute(sql)\n        # We might start eating memory at this point\n        rows = curs.fetchall()\n        curs.close()\n        result = []\n        idPositions = [fields.index(key) for key in cls._sqlPrimary]\n        shortPos = [fields.index(short) for short in cls._shortView]\n        for row in rows:\n            ids = [row[pos] for pos in idPositions]\n            if len(idPositions) > 1:\n                ids = tuple(ids)\n            else:\n                ids = ids[0]\n            text = SEPERATOR.join([str(row[pos]) for pos in shortPos])\n            result.append((ids, text))\n        return result", "category": "Python"}, {"instruction": "def get_admins(self):\n        \"\"\"Check verification for all admins.\"\"\"\n", "input": "", "output": "        # no nickserv support, assume people are who they say they are.\n        if not self.config['feature'].getboolean('nickserv'):\n            return\n        with self.db.session_scope() as session:\n            for a in session.query(orm.Permissions).all():\n                if not a.registered:\n                    self.update_authstatus(a.nick)", "category": "Python"}, {"instruction": "def get_cpu_state(self):\n        \"\"\"\n        Retrieves CPU state from client\n        \"\"\"\n", "input": "", "output": "        state = c_int(0)\n        self.library.Cli_GetPlcStatus(self.pointer,byref(state))\n        \n        try:\n            status_string = cpu_statuses[state.value]\n        except KeyError:\n            status_string = None\n        \n        if not status_string:\n            raise Snap7Exception(\"The cpu state (%s) is invalid\" % state.value)\n        \n        logger.debug(\"CPU state is %s\" % status_string)\n        return status_string", "category": "Python"}, {"instruction": "def get_facts_by_name(api_url=None, fact_name=None, verify=False, cert=list()):\n    \"\"\"\n    Returns facts by name\n\n    :param api_url: Base PuppetDB API url\n    :param fact_name: Name of fact\n\n    \"\"\"\n", "input": "", "output": "    return utils._make_api_request(api_url, '/facts/{0}'.format(fact_name), verify, cert)", "category": "Python"}, {"instruction": "def get_plugin_class(self, plugin_name):\n        \"\"\"Returns the class registered under the given plugin name.\"\"\"\n", "input": "", "output": "        try:\n            return self.plugin_classes[plugin_name]\n        except KeyError:\n            raise RezPluginError(\"Unrecognised %s plugin: '%s'\"\n                                 % (self.pretty_type_name, plugin_name))", "category": "Python"}, {"instruction": "def _append_request_ids(self, resp):\n        \"\"\"Add request_ids as an attribute to the object\n\n        :param resp: Response object or list of Response objects\n        \"\"\"\n", "input": "", "output": "        if isinstance(resp, list):\n            # Add list of request_ids if response is of type list.\n            for resp_obj in resp:\n                self._append_request_id(resp_obj)\n        elif resp is not None:\n            # Add request_ids if response contains single object.\n            self._append_request_id(resp)", "category": "Python"}, {"instruction": "def currentSelected(self):\n        \"\"\"Returns the currently selected delimiter character.\n\n        Returns:\n            str: One of `,`, `;`, `\\t`, `*other*`.\n\n        \"\"\"\n", "input": "", "output": "        if self.commaRadioButton.isChecked():\n            return ','\n        elif self.semicolonRadioButton.isChecked():\n            return ';'\n        elif self.tabRadioButton.isChecked():\n            return '\\t'\n        elif self.otherRadioButton.isChecked():\n            return self.otherSeparatorLineEdit.text()\n        return", "category": "Python"}, {"instruction": "def head(self, url: StrOrURL, *, allow_redirects: bool=False,\n             **kwargs: Any) -> '_RequestContextManager':\n        \"\"\"Perform HTTP HEAD request.\"\"\"\n", "input": "", "output": "        return _RequestContextManager(\n            self._request(hdrs.METH_HEAD, url,\n                          allow_redirects=allow_redirects,\n                          **kwargs))", "category": "Python"}, {"instruction": "def run(analysis, path=None, name=None, info=None, **kwargs):\n    \"\"\"Run a single analysis.\n\n    :param Analysis analysis: Analysis class to run.\n    :param str path: Path of analysis. Can be `__file__`.\n    :param str name: Name of the analysis.\n    :param dict info: Optional entries are ``version``, ``title``,\n        ``readme``, ...\n    :param dict static: Map[url regex, root-folder] to serve static content.\n    \"\"\"\n", "input": "", "output": "    kwargs.update({\n        'analysis': analysis,\n        'path': path,\n        'name': name,\n        'info': info,\n    })\n    main(**kwargs)", "category": "Python"}, {"instruction": "def use_active_sequence_rule_view(self):\n        \"\"\"Pass through to provider SequenceRuleLookupSession.use_active_sequence_rule_view\"\"\"\n", "input": "", "output": "        self._operable_views['sequence_rule'] = ACTIVE\n        # self._get_provider_session('sequence_rule_lookup_session')  # To make sure the session is tracked\n        for session in self._get_provider_sessions():\n            try:\n                session.use_active_sequence_rule_view()\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def _write_wrapped(self, line, sep=\" \", indent=\"\", width=78):\n        \"\"\"Word-wrap a line of RiveScript code for being written to a file.\n\n        :param str line: The original line of text to word-wrap.\n        :param str sep: The word separator.\n        :param str indent: The indentation to use (as a set of spaces).\n        :param int width: The character width to constrain each line to.\n\n        :return str: The reformatted line(s).\"\"\"\n", "input": "", "output": "\n        words = line.split(sep)\n        lines = []\n        line  = \"\"\n        buf   = []\n\n        while len(words):\n            buf.append(words.pop(0))\n            line = sep.join(buf)\n            if len(line) > width:\n                # Need to word wrap!\n                words.insert(0, buf.pop())  # Undo\n                lines.append(sep.join(buf))\n                buf = []\n                line = \"\"\n\n        # Straggler?\n        if line:\n            lines.append(line)\n\n        # Returned output\n        result = lines.pop(0)\n        if len(lines):\n            eol = \"\"\n            if sep == \" \":\n                eol = \"\\s\"\n            for item in lines:\n                result += eol + \"\\n\" + indent + \"^ \" + item\n\n        return result", "category": "Python"}, {"instruction": "def get_rank_value(cls, name):\n    \"\"\"Returns the integer constant value for the given rank name.\n\n    :param string rank: the string rank name (E.g., 'HARDCODED').\n    :returns: the integer constant value of the rank.\n    :rtype: int\n    \"\"\"\n", "input": "", "output": "    if name in cls._RANK_NAMES.values():\n      return getattr(cls, name, None)\n    return None", "category": "Python"}, {"instruction": "def system_switch_attributes_rbridge_id_host_name(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        system = ET.SubElement(config, \"system\", xmlns=\"urn:brocade.com:mgmt:brocade-ras\")\n        switch_attributes = ET.SubElement(system, \"switch-attributes\")\n        rbridge_id = ET.SubElement(switch_attributes, \"rbridge-id\")\n        rbridge_id_key = ET.SubElement(rbridge_id, \"rbridge-id\")\n        rbridge_id_key.text = kwargs.pop('rbridge_id')\n        host_name = ET.SubElement(rbridge_id, \"host-name\")\n        host_name.text = kwargs.pop('host_name')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def correct_bad_footnote_urls(book_dir=BOOK_PATH, dest=None, include_tags=None,\n                              ext='.nlpiabak', skip_untitled=True):\n    \"\"\" DEPRECATED (see translate_line_footnotes)\n\n    Find bad footnotes (only urls), visit the page, add the title to the footnote \n\n    >>> len(correct_bad_footnote_urls(book_dir=BOOK_PATH, dest='cleaned_footnotes'))\n    1\n    >>> rm_r(os.path.join(BOOK_PATH, 'cleaned_footnotes'))\n    \"\"\"\n", "input": "", "output": "    # bad_url_lines = find_all_bad_footnote_urls(book_dir=book_dir)\n    # file_line_maps = []\n    return translate_book(translators=translate_line_footnotes, book_dir=book_dir, dest=dest, include_tags=include_tags,\n                          ext=ext, skip_untitled=skip_untitled)", "category": "Python"}, {"instruction": "def generateLowerBoundList(confidence, numUniqueFeatures, numLocationsPerObject,\n                           maxNumObjects):\n  \"\"\"\n  Metric: How unique is each object's most unique feature? Calculate the\n  probabilistic lower bound for the number of occurrences of an object's most\n  unique feature. For example, if confidence is 0.8, the tick \"3\" will be placed\n  at the point where 80% of objects are completely composed of features with 3\n  or more total occurrences, and 20% of objects have at least one feature that\n  has 2 or fewer total occurrences.\n  \"\"\"\n", "input": "", "output": "  # We're choosing a location, checking its feature, and checking how many\n  # *other* occurrences there are of this feature. So we check n - 1 locations.\n  maxNumOtherLocations = maxNumObjects*10 - 1\n\n  results = zip(itertools.count(1),\n                findBinomialNsWithLowerBoundSampleMinimum(\n                  confidence,\n                  itertools.count(1), 1./numUniqueFeatures, numLocationsPerObject,\n                  maxNumOtherLocations))\n\n  finalResults = [(numOtherLocations, interpolatedN / numLocationsPerObject)\n                  for numOtherLocations, (interpolatedN, _, _) in results]\n\n  return finalResults", "category": "Python"}, {"instruction": "def get_item_abspath(self, identifier):\n        \"\"\"Return absolute path at which item content can be accessed.\n\n        :param identifier: item identifier\n        :returns: absolute path from which the item content can be accessed\n        \"\"\"\n", "input": "", "output": "        admin_metadata = self.get_admin_metadata()\n        uuid = admin_metadata[\"uuid\"]\n        # Create directory for the specific dataset.\n        dataset_cache_abspath = os.path.join(self._irods_cache_abspath, uuid)\n        mkdir_parents(dataset_cache_abspath)\n\n        # Get the file extension from the  relpath from the handle metadata.\n        irods_item_path = os.path.join(self._data_abspath, identifier)\n        relpath = self._get_metadata_with_cache(irods_item_path, \"handle\")\n        _, ext = os.path.splitext(relpath)\n\n        local_item_abspath = os.path.join(\n            dataset_cache_abspath,\n            identifier + ext)\n\n        if not os.path.isfile(local_item_abspath):\n            tmp_local_item_abspath = local_item_abspath + \".tmp\"\n            _get_file_forcefully(irods_item_path, tmp_local_item_abspath)\n            os.rename(tmp_local_item_abspath, local_item_abspath)\n\n        return local_item_abspath", "category": "Python"}, {"instruction": "def prepare(self):\n\t\t\"\"\"Prepare the ordered list of transformers and reset context state to initial.\"\"\"\n", "input": "", "output": "\t\tself.scope = 0\n\t\tself.mapping = deque([0])\n\t\tself._handler = [i() for i in sorted(self.handlers, key=lambda handler: handler.priority)]", "category": "Python"}, {"instruction": "def get_homepage(self, shop_id):\n        \"\"\"\n        \u67e5\u8be2\u5546\u5bb6\u4e3b\u9875\n\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        http://mp.weixin.qq.com/wiki/6/2732f3cf83947e0e4971aa8797ee9d6a.html\n\n        :param shop_id: \u95e8\u5e97 ID\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n", "input": "", "output": "        res = self._post(\n            'homepage/get',\n            data={'shop_id': shop_id},\n            result_processor=lambda x: x['data']\n        )\n        return res", "category": "Python"}, {"instruction": "def to_displacements(self):\n        \"\"\"\n        Converts position coordinates of trajectory into displacements between consecutive frames\n        \"\"\"\n", "input": "", "output": "        if not self.coords_are_displacement:\n            displacements = np.subtract(self.frac_coords, np.roll(self.frac_coords, 1, axis=0))\n            displacements[0] = np.zeros(np.shape(self.frac_coords[0]))\n            # Deal with PBC\n            displacements = [np.subtract(item, np.round(item)) for item in displacements]\n\n            self.frac_coords = displacements\n            self.coords_are_displacement = True\n        return", "category": "Python"}, {"instruction": "def from_backend(self, dagobah_id):\n        \"\"\" Reconstruct this Dagobah instance from the backend. \"\"\"\n", "input": "", "output": "        logger.debug('Reconstructing Dagobah instance from backend with ID {0}'.format(dagobah_id))\n        rec = self.backend.get_dagobah_json(dagobah_id)\n        if not rec:\n            raise DagobahError('dagobah with id %s does not exist '\n                               'in backend' % dagobah_id)\n        self._construct_from_json(rec)", "category": "Python"}, {"instruction": "def regions_coverage(bed_file, target_name, data):\n    \"\"\"Generate coverage over regions of interest using mosdepth.\n    \"\"\"\n", "input": "", "output": "    ready_bed = tz.get_in([\"depth\", target_name, \"regions\"], data)\n    if ready_bed:\n        return ready_bed\n    else:\n        return run_mosdepth(data, target_name, bed_file).regions", "category": "Python"}, {"instruction": "def _get_config(self, unit, filename):\n        \"\"\"Get a ConfigParser object for parsing a unit's config file.\"\"\"\n", "input": "", "output": "        file_contents = unit.file_contents(filename)\n\n        # NOTE(beisner):  by default, ConfigParser does not handle options\n        # with no value, such as the flags used in the mysql my.cnf file.\n        # https://bugs.python.org/issue7005\n        config = configparser.ConfigParser(allow_no_value=True)\n        config.readfp(io.StringIO(file_contents))\n        return config", "category": "Python"}, {"instruction": "def exportCertificate(self, certificate, folder):\n        \"\"\"gets the SSL Certificates for a given machine\"\"\"\n", "input": "", "output": "        url = self._url + \"/sslcertificates/%s/export\" % certificate\n        params = {\n            \"f\" : \"json\",\n        }\n        return self._get(url=url,\n                         param_dict=params,\n                        out_folder=folder)", "category": "Python"}, {"instruction": "def getnoisefile(d, segment=-1):\n    \"\"\" Return name of noisefile for a given dictionary. Must have d['segment'] defined.\n    \"\"\"\n", "input": "", "output": "    if d.has_key('segment'):\n        return os.path.join(d['workdir'], 'noise_' + d['fileroot'] + '_sc' + str(d['scan']) + 'seg' + str(d['segment']) + '.pkl')\n    elif segment >= 0:\n        return os.path.join(d['workdir'], 'noise_' + d['fileroot'] + '_sc' + str(d['scan']) + 'seg' + str(segment) + '.pkl')\n    else:\n        return ''", "category": "Python"}, {"instruction": "def extract_version():\n    \"\"\"Return ggplot.__version__ without importing ggplot.\n    \n    Extracts version from ggplot/__init__.py\n    without importing ggplot, which requires dependencies to be installed.\n    \"\"\"\n", "input": "", "output": "    with open('ggplot/__init__.py') as fd:\n        ns = {}\n        for line in fd:\n            if line.startswith('__version__'):\n                exec(line.strip(), ns)\n                return ns['__version__']", "category": "Python"}, {"instruction": "def getFaxStatsCounters(self):\n        \"\"\"Query Asterisk Manager Interface for Fax Stats.\n        \n        CLI Command - fax show stats\n        \n        @return: Dictionary of fax stats.\n        \n        \"\"\"\n", "input": "", "output": "        if not self.hasFax():\n            return None\n        info_dict = {}\n        cmdresp = self.executeCommand('fax show stats')\n        ctxt = 'general'\n        for section in cmdresp.strip().split('\\n\\n')[1:]:\n            i = 0\n            for line in section.splitlines():\n                mobj = re.match('(\\S.*\\S)\\s*:\\s*(\\d+)\\s*$', line)\n                if mobj:\n                    if not info_dict.has_key(ctxt):\n                        info_dict[ctxt] = {}\n                    info_dict[ctxt][mobj.group(1).lower()] = int(mobj.group(2).lower())\n                elif i == 0:\n                    ctxt = line.strip().lower()\n                i += 1    \n        return info_dict", "category": "Python"}, {"instruction": "def fetch_and_create_image(self, url, image_title):\n        '''\n        fetches, creates image object\n\n        returns tuple with Image object and context dictionary containing\n        request URL\n        '''\n", "input": "", "output": "\n        context = {\n            \"file_url\": url,\n            \"foreign_title\": image_title,\n        }\n        try:\n            image_file = requests.get(url)\n            local_image = Image(\n                title=image_title,\n                file=ImageFile(\n                    BytesIO(image_file.content),\n                    name=image_title\n                )\n            )\n            local_image.save()\n            return (local_image, context)\n        except Exception as e:\n            context.update({\n                \"exception\": e,\n            })\n            raise ImageCreationFailed(context, None)", "category": "Python"}, {"instruction": "def listarraytranspose(L):\n    '''\n    Tranposes the simple array presentation of a list of lists (of equal length).\n    Argument:\n        L = [row1, row2, ...., rowN]\n        where the rowi are python lists of equal length.\n    Returns:\n        LT, a list of python lists such that LT[j][i] = L[i][j].\n    '''\n", "input": "", "output": "    return [[row[i] for row in L] for i in range(len(L[0]))]", "category": "Python"}, {"instruction": "def create_time_step(cls,\n                       observation=None,\n                       done=False,\n                       raw_reward=None,\n                       processed_reward=None,\n                       action=None):\n    \"\"\"Creates a TimeStep with both rewards and actions as optional.\"\"\"\n", "input": "", "output": "\n    return cls(observation, done, raw_reward, processed_reward, action)", "category": "Python"}, {"instruction": "def remove_error_class(klass):\n    \"\"\"\n    Removes a class from the L{ERROR_CLASS_MAP}.\n\n    An example::\n\n       >>> class AuthenticationError(Exception):\n       ...     pass\n       ...\n       >>> pyamf.add_error_class(AuthenticationError, 'Auth.Failed')\n       >>> pyamf.remove_error_class(AuthenticationError)\n\n    @see: L{add_error_class}\n    \"\"\"\n", "input": "", "output": "    if isinstance(klass, python.str_types):\n        if klass not in ERROR_CLASS_MAP:\n            raise ValueError('Code %s is not registered' % (klass,))\n    elif isinstance(klass, python.class_types):\n        classes = ERROR_CLASS_MAP.values()\n        if klass not in classes:\n            raise ValueError('Class %s is not registered' % (klass,))\n\n        klass = ERROR_CLASS_MAP.keys()[classes.index(klass)]\n    else:\n        raise TypeError(\"Invalid type, expected class or string\")\n\n    del ERROR_CLASS_MAP[klass]", "category": "Python"}, {"instruction": "def save_image(figure, filename):\n    \"\"\"Save an image to the docs images directory.\n\n    Args:\n        filename (str): The name of the file (not containing\n            directory info).\n    \"\"\"\n", "input": "", "output": "    path = os.path.join(IMAGES_DIR, filename)\n    figure.savefig(path, bbox_inches=\"tight\")\n    plt.close(figure)", "category": "Python"}, {"instruction": "def dumps(ms, single=False,\n          properties=False, pretty_print=True,\n          show_status=False, predicate_modifiers=False, **kwargs):\n    \"\"\"\n    Serialize an Xmrs object to a Eds representation\n\n    Args:\n        ms: an iterator of :class:`~delphin.mrs.xmrs.Xmrs` objects to\n            serialize (unless the *single* option is `True`)\n        single (bool): if `True`, treat *ms* as a single\n            :class:`~delphin.mrs.xmrs.Xmrs` object instead of as an\n            iterator\n        properties (bool): if `False`, suppress variable properties\n        pretty_print (bool): if `True`, add newlines and indentation\n        show_status (bool): if `True`, annotate disconnected graphs and\n            nodes\n    Returns:\n        an :class:`Eds` string representation of a corpus of Xmrs\n    \"\"\"\n", "input": "", "output": "    if not pretty_print and kwargs.get('indent'):\n        pretty_print = True\n    if single:\n        ms = [ms]\n    return serialize(\n        ms,\n        properties=properties,\n        pretty_print=pretty_print,\n        show_status=show_status,\n        predicate_modifiers=predicate_modifiers,\n        **kwargs\n    )", "category": "Python"}, {"instruction": "def _intersperse_insertion_rows_and_columns(self, pairwise_pvals):\n        \"\"\"Return pvals matrix with inserted NaN rows and columns, as numpy.ndarray.\n\n        Each insertion (a header or a subtotal) creates an offset in the calculated\n        pvals. These need to be taken into account when converting each pval to a\n        corresponding column letter. For this reason, we need to insert an all-NaN\n        row and a column at the right indices. These are the inserted indices of each\n        insertion, along respective dimensions.\n        \"\"\"\n", "input": "", "output": "        for i in self._insertion_indices:\n            pairwise_pvals = np.insert(pairwise_pvals, i, np.nan, axis=0)\n            pairwise_pvals = np.insert(pairwise_pvals, i, np.nan, axis=1)\n        return pairwise_pvals", "category": "Python"}, {"instruction": "def add_includes(self, includes):\n        # type: (_BaseSourcePaths, list) -> None\n        \"\"\"Add a list of includes\n        :param _BaseSourcePaths self: this\n        :param list includes: list of includes\n        \"\"\"\n", "input": "", "output": "        if not isinstance(includes, list):\n            if isinstance(includes, tuple):\n                includes = list(includes)\n            else:\n                includes = [includes]\n        # remove any starting rglob spec\n        incl = []\n        for inc in includes:\n            tmp = pathlib.Path(inc).parts\n            if tmp[0] == '**':\n                if len(tmp) == 1:\n                    continue\n                else:\n                    incl.append(str(pathlib.Path(*tmp[1:])))\n            else:\n                incl.append(inc)\n        # check for any remaining rglob specs\n        if any(['**' in x for x in incl]):\n            raise ValueError('invalid include specification containing \"**\"')\n        if self._include is None:\n            self._include = incl\n        else:\n            self._include.extend(incl)", "category": "Python"}, {"instruction": "async def clone(self, token):\n        \"\"\"Creates a new token by cloning an existing token\n\n        Parameters:\n            token (ObjectID): Token ID\n        Returns:\n            ObjectMeta: where value is token ID\n\n        This allows a token to serve as a template for others, making it\n        simple to generate new tokens without complex rule management.\n\n        As with create, a successful response body will return the ID of the\n        newly created ACL, like so::\n\n            {\n                \"ID\": \"adf4238a-882b-9ddc-4a9d-5b6758e4159e\"\n            }\n        \"\"\"\n", "input": "", "output": "        token_id = extract_attr(token, keys=[\"ID\"])\n        response = await self._api.put(\"/v1/acl/clone\", token_id)\n        return consul(response)", "category": "Python"}, {"instruction": "def protege_data(datas_str, sens):\n    \"\"\"\n    Used to crypt/decrypt data before saving locally.\n    Override if securit is needed.\n    bytes -> str when decrypting\n    str -> bytes when crypting\n\n    :param datas_str: When crypting, str. when decrypting bytes\n    :param sens: True to crypt, False to decrypt\n    \"\"\"\n", "input": "", "output": "    return bytes(datas_str, encoding=\"utf8\") if sens else str(datas_str, encoding=\"utf8\")", "category": "Python"}, {"instruction": "def _model_sanity(model):\n        \"\"\"\n        Perform some basic sanity checking on the model to warn users when they\n        might be trying something ill advised.\n\n        :param model: model instance.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(model, ODEModel) and not isinstance(model, BaseNumericalModel):\n            # Such a model should probably not contain derivatives\n            for var, expr in model.items():\n                if isinstance(var, sympy.Derivative) or expr.has(sympy.Derivative):\n                    warnings.warn(RuntimeWarning(\n                        'The model contains derivatives in its definition. '\n                        'Are you sure you don\\'t mean to use `symfit.ODEModel`?'\n                    ))", "category": "Python"}, {"instruction": "async def resume(self):\n        \"\"\"Sends the RESUME packet.\"\"\"\n", "input": "", "output": "        payload = {\n            'op': self.RESUME,\n            'd': {\n                'seq': self.sequence,\n                'session_id': self.session_id,\n                'token': self.token\n            }\n        }\n\n        await self.send_as_json(payload)\n        log.info('Shard ID %s has sent the RESUME payload.', self.shard_id)", "category": "Python"}, {"instruction": "def iter_target_siblings_and_ancestors(self, target):\n    \"\"\"Produces an iterator over a target's siblings and ancestor lineage.\n\n    :returns: A target iterator yielding the target and its siblings and then it ancestors from\n              nearest to furthest removed.\n    \"\"\"\n", "input": "", "output": "    def iter_targets_in_spec_path(spec_path):\n      try:\n        siblings = SiblingAddresses(spec_path)\n        for address in self._build_graph.inject_specs_closure([siblings]):\n          yield self._build_graph.get_target(address)\n      except AddressLookupError:\n        # A spec path may not have any addresses registered under it and that's ok.\n        # For example:\n        #  a:a\n        #  a/b/c:c\n        #\n        # Here a/b contains no addresses.\n        pass\n\n    def iter_siblings_and_ancestors(spec_path):\n      for sibling in iter_targets_in_spec_path(spec_path):\n        yield sibling\n      parent_spec_path = os.path.dirname(spec_path)\n      if parent_spec_path != spec_path:\n        for parent in iter_siblings_and_ancestors(parent_spec_path):\n          yield parent\n\n    for target in iter_siblings_and_ancestors(target.address.spec_path):\n      yield target", "category": "Python"}, {"instruction": "def remove_duplicates(self, configs=None):\n        \"\"\"remove duplicate entries from 4-point configurations. If no\n        configurations are provided, then use self.configs. Unique\n        configurations are only returned if configs is not None.\n\n        Parameters\n        ----------\n        configs: Nx4 numpy.ndarray, optional\n            remove duplicates from these configurations instead from\n            self.configs.\n\n        Returns\n        -------\n        configs_unique: Kx4 numpy.ndarray\n            unique configurations. Only returned if configs is not None\n\n        \"\"\"\n", "input": "", "output": "        if configs is None:\n            c = self.configs\n        else:\n            c = configs\n        struct = c.view(c.dtype.descr * 4)\n        configs_unique = np.unique(struct).view(c.dtype).reshape(-1, 4)\n        if configs is None:\n            self.configs = configs_unique\n        else:\n            return configs_unique", "category": "Python"}, {"instruction": "def get_instance(self, payload):\n        \"\"\"\n        Build an instance of EntityInstance\n\n        :param dict payload: Payload response from the API\n\n        :returns: twilio.rest.authy.v1.service.entity.EntityInstance\n        :rtype: twilio.rest.authy.v1.service.entity.EntityInstance\n        \"\"\"\n", "input": "", "output": "        return EntityInstance(self._version, payload, service_sid=self._solution['service_sid'], )", "category": "Python"}, {"instruction": "def run_ut_python3_qemu_internal():\n    \"\"\"this runs inside the vm\"\"\"\n", "input": "", "output": "    pkg = glob.glob('mxnet_dist/*.whl')[0]\n    logging.info(\"=== NOW Running inside QEMU ===\")\n    logging.info(\"PIP Installing %s\", pkg)\n    check_call(['sudo', 'pip3', 'install', pkg])\n    logging.info(\"PIP Installing mxnet/test_requirements.txt\") \n    check_call(['sudo', 'pip3', 'install', '-r', 'mxnet/test_requirements.txt'])\n    logging.info(\"Running tests in mxnet/tests/python/unittest/\")\n    check_call(['nosetests', '--with-timer', '--with-xunit', '--xunit-file', 'nosetests_unittest.xml', '--verbose', 'mxnet/tests/python/unittest/test_engine.py'])", "category": "Python"}, {"instruction": "def sorted_by_distance_to(self, position: Union[Unit, Point2], reverse: bool = False) -> \"Units\":\n        \"\"\" This function should be a bit faster than using units.sorted(keyfn=lambda u: u.distance_to(position)) \"\"\"\n", "input": "", "output": "        if len(self) in [0, 1]:\n            return self\n        position = position.position\n        return self.sorted(keyfn=lambda unit: unit.position._distance_squared(position), reverse=reverse)", "category": "Python"}, {"instruction": "def search_raw(self, query, indices=None, doc_types=None, headers=None, **query_params):\n        \"\"\"Execute a search against one or more indices to get the search hits.\n\n        `query` must be a Search object, a Query object, or a custom\n        dictionary of search parameters using the query DSL to be passed\n        directly.\n        \"\"\"\n", "input": "", "output": "        from .query import Search, Query\n\n        if isinstance(query, Query):\n            query = query.search()\n        if isinstance(query, Search):\n            query = query.serialize()\n        body = self._encode_query(query)\n        path = self._make_path(indices, doc_types, \"_search\")\n        return self._send_request('GET', path, body, params=query_params, headers=headers)", "category": "Python"}, {"instruction": "def clean(self):\n        \"\"\"Check that at least one service has been entered.\"\"\"\n", "input": "", "output": "        super(AtLeastOneRequiredInlineFormSet, self).clean()\n        if any(self.errors):\n            return\n        if not any(cleaned_data and not cleaned_data.get('DELETE', False) for cleaned_data in self.cleaned_data):\n            raise forms.ValidationError('\u041f\u0440\u0438\u043a\u0440\u0435\u043f\u0438\u0442\u0435 \u043d\u0435 \u043c\u0435\u043d\u0435\u0435 \u043e\u0434\u043d\u043e\u0433\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430')", "category": "Python"}, {"instruction": "def get_real_rating(self):\n        \"\"\"get_rating()\n        \n        Returns the unmodified average rating.\"\"\"\n", "input": "", "output": "        if not (self.votes and self.score):\n            return 0\n        return float(self.score)/self.votes", "category": "Python"}, {"instruction": "def notify(request, info):\n    '''The actor notify itself with a dictionary of information.\n\n    The command perform the following actions:\n\n    * Update the mailbox to the current consumer of the actor connection\n    * Update the info dictionary\n    * Returns the time of the update\n    '''\n", "input": "", "output": "    t = time()\n    actor = request.actor\n    remote_actor = request.caller\n    if isinstance(remote_actor, ActorProxyMonitor):\n        remote_actor.mailbox = request.connection\n        info['last_notified'] = t\n        remote_actor.info = info\n        callback = remote_actor.callback\n        # if a callback is still available, this is the first\n        # time we got notified\n        if callback:\n            remote_actor.callback = None\n            callback.set_result(remote_actor)\n            if actor.cfg.debug:\n                actor.logger.debug('Got first notification from %s',\n                                   remote_actor)\n        elif actor.cfg.debug:\n            actor.logger.debug('Got notification from %s', remote_actor)\n    else:\n        actor.logger.warning('notify got a bad actor')\n    return t", "category": "Python"}, {"instruction": "def element(cls, name, parent=None, interleave=None, occur=0):\n        \"\"\"Create an element node.\"\"\"\n", "input": "", "output": "        node = cls(\"element\", parent, interleave=interleave)\n        node.attr[\"name\"] = name\n        node.occur = occur\n        return node", "category": "Python"}, {"instruction": "def remove_package(self, team, user, package):\n        \"\"\"\n        Removes a package (all instances) from this store.\n        \"\"\"\n", "input": "", "output": "        self.check_name(team, user, package)\n\n        path = self.package_path(team, user, package)\n        remove_objs = set()\n        # TODO: do we really want to delete invisible dirs?\n        if os.path.isdir(path):\n            # Collect objects from all instances for potential cleanup\n            contents_path = os.path.join(path, PackageStore.CONTENTS_DIR)\n            for instance in os.listdir(contents_path):\n                pkg = self.get_package(team, user, package, pkghash=instance)\n                remove_objs.update(find_object_hashes(pkg))\n            # Remove package manifests\n            rmtree(path)\n\n        return self.prune(remove_objs)", "category": "Python"}, {"instruction": "def install_python(python, runas=None):\n    '''\n    Install a python implementation.\n\n    python\n        The version of python to install, should match one of the\n        versions listed by pyenv.list\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pyenv.install_python 2.0.0-p0\n    '''\n", "input": "", "output": "    python = re.sub(r'^python-', '', python)\n\n    env = None\n    env_list = []\n\n    if __grains__['os'] in ('FreeBSD', 'NetBSD', 'OpenBSD'):\n        env_list.append('MAKE=gmake')\n\n    if __salt__['config.option']('pyenv.build_env'):\n        env_list.append(__salt__['config.option']('pyenv.build_env'))\n\n    if env_list:\n        env = ' '.join(env_list)\n\n    ret = {}\n    ret = _pyenv_exec('install', python, env=env, runas=runas, ret=ret)\n    if ret['retcode'] == 0:\n        rehash(runas=runas)\n        return ret['stderr']\n    else:\n        # Cleanup the failed installation so it doesn't list as installed\n        uninstall_python(python, runas=runas)\n        return False", "category": "Python"}, {"instruction": "def _traceback_to_alignment(tb, a, b):\n    \"\"\"Convert a traceback (i.e. as returned by `tracebacks()`) into an alignment\n    (i.e. as returned by `align`).\n\n    Arguments:\n      tb: A traceback.\n      a: the sequence defining the rows in the traceback matrix.\n      b: the sequence defining the columns in the traceback matrix.\n\n    Returns: An iterable of (index, index) tupless where ether (but not both)\n      tuples can be `None`.\n    \"\"\"\n", "input": "", "output": "    # We subtract 1 from the indices here because we're translating from the\n    # alignment matrix space (which has one extra row and column) to the space\n    # of the input sequences.\n    for idx, direction in tb:\n        if direction == Direction.DIAG:\n            yield (idx[0] - 1, idx[1] - 1)\n        elif direction == Direction.UP:\n            yield (idx[0] - 1, None)\n        elif direction == Direction.LEFT:\n            yield (None, idx[1] - 1)", "category": "Python"}, {"instruction": "def StartElement(self, name, attributes):\n        '''\n        Expat start element event handler\n        '''\n", "input": "", "output": "        if name == 'hierarchy':\n            pass\n        elif name == 'node':\n            # Instantiate an Element object\n            attributes['uniqueId'] = 'id/no_id/%d' % self.idCount\n            bounds = re.split('[\\][,]', attributes['bounds'])\n            attributes['bounds'] = ((int(bounds[1]), int(bounds[2])), (int(bounds[4]), int(bounds[5])))\n            if DEBUG_BOUNDS:\n                print >> sys.stderr, \"bounds=\", attributes['bounds']\n            self.idCount += 1\n            child = View.factory(attributes, self.device, version=self.version, uiAutomatorHelper=self.uiAutomatorHelper)\n            self.views.append(child)\n            # Push element onto the stack and make it a child of parent\n            if not self.nodeStack:\n                self.root = child\n            else:\n                self.parent = self.nodeStack[-1]\n                self.parent.add(child)\n            self.nodeStack.append(child)", "category": "Python"}, {"instruction": "def create_next_tag():\n    \"\"\" creates a tag based on the date and previous tags \"\"\"\n", "input": "", "output": "    date = datetime.utcnow()\n    date_tag = '{}.{}.{}'.format(date.year, date.month, date.day)\n    if date_tag in latest_tag(): # if there was an update already today\n        latest = latest_tag().split('.') # split by spaces\n        if len(latest) == 4: # if it is not the first revision of the day\n            latest[-1]= str(int(latest[-1])+1)\n        else: # if it is the first revision of the day\n            latest+=['1']\n        date_tag = '.'.join(latest)\n    return date_tag", "category": "Python"}, {"instruction": "def temporary_instance_cache(*classes):\n    \"\"\"Use a temporary cache for instances in :meth:`~.Expression.create`\n\n    The instance cache used by :meth:`~.Expression.create` for any of the given\n    `classes` will be cleared upon entering the managed context, and restored\n    on leaving it.  That is, no cached instances from outside of the managed\n    context will be used within the managed context, and vice versa\n    \"\"\"\n", "input": "", "output": "    orig_instances = []\n    for cls in classes:\n        orig_instances.append(cls._instances)\n        cls._instances = {}\n    try:\n        yield\n    finally:\n        for i, cls in enumerate(classes):\n            cls._instances = orig_instances[i]", "category": "Python"}, {"instruction": "def _get_hosts_from_names(self, names):\n        \"\"\" validate hostnames from a list of names\n        \"\"\"\n", "input": "", "output": "        result = set()\n        hosts = map(lambda x: x.strip(), names.split(','))\n        for h in hosts:\n            if valid_hostname(h.split(':')[0]):\n                result.add(h if ':' in h else '%s:%d' % (h, self.PORT))\n            else:\n                raise conferr('Invalid hostname: %s' % h.split(':')[0])\n        return list(result)", "category": "Python"}, {"instruction": "async def inspect(self, *, node_id: str) -> Mapping[str, Any]:\n        \"\"\"\n        Inspect a node\n\n        Args:\n            node_id: The ID or name of the node\n        \"\"\"\n", "input": "", "output": "\n        response = await self.docker._query_json(\n            \"nodes/{node_id}\".format(node_id=node_id), method=\"GET\"\n        )\n        return response", "category": "Python"}, {"instruction": "def validate_equal(value):\n    \"\"\"\n    Validate the field value is equal to the given value.\n    Should work with anything that supports '==' operator.\n\n    :param value: Value to compare.\n    :raises: ``ValidationError('equal')``\n    \"\"\"\n", "input": "", "output": "    def equal_validator(field, data):\n        if field.value is None:\n            return\n        if not (field.value == value):\n            raise ValidationError('equal', other=value)\n    return equal_validator", "category": "Python"}, {"instruction": "def NetworkFee(self):\n        \"\"\"\n        Get the network fee.\n\n        Returns:\n            Fixed8:\n        \"\"\"\n", "input": "", "output": "        if self._network_fee is None:\n\n            input = Fixed8(0)\n\n            for coin_ref in self.References.values():\n                if coin_ref.AssetId == GetBlockchain().SystemCoin().Hash:\n                    input = input + coin_ref.Value\n\n            output = Fixed8(0)\n\n            for tx_output in self.outputs:\n                if tx_output.AssetId == GetBlockchain().SystemCoin().Hash:\n                    output = output + tx_output.Value\n\n            self._network_fee = input - output - self.SystemFee()\n\n        #            logger.info(\"Determined network fee to be %s \" % (self.__network_fee.value))\n\n        return self._network_fee", "category": "Python"}, {"instruction": "def getAppInfo(self, appId):\n        \"\"\"\n        Every application registered with Portal for ArcGIS has a unique\n        client ID and a list of redirect URIs that are used for OAuth. This\n        operation returns these OAuth-specific properties of an application.\n        You can use this information to update the redirect URIs by using\n        the Update App Info operation.\n\n        Input:\n           appId - unique id of the application to get the information\n            about.\n        \"\"\"\n", "input": "", "output": "        params = {\n            \"f\" : \"json\",\n            \"appID\" : appId\n        }\n        url = self._url + \"/oauth/getAppInfo\"\n        return self._get(url=url, param_dict=params,\n                         proxy_url=self._proxy_url,\n                         proxy_port=self._proxy_port)", "category": "Python"}, {"instruction": "def _perform_basic_permission_check(self, forum, user, permission):\n        \"\"\" Given a forum and a user, checks whether the latter has the passed permission.\n\n        The workflow is:\n\n            1. The permission is granted if the user is a superuser\n            2. If not, a check is performed with the given permission\n\n        \"\"\"\n", "input": "", "output": "        checker = self._get_checker(user)\n\n        # The action is granted if...\n        #     the user is the superuser\n        #     the user has the permission to do so\n        check = (user.is_superuser or checker.has_perm(permission, forum))\n        return check", "category": "Python"}, {"instruction": "def disassemble_current(self, dwThreadId):\n        \"\"\"\n        Disassemble the instruction at the program counter of the given thread.\n\n        @type  dwThreadId: int\n        @param dwThreadId: Global thread ID.\n            The program counter for this thread will be used as the disassembly\n            address.\n\n        @rtype:  tuple( long, int, str, str )\n        @return: The tuple represents an assembly instruction\n            and contains:\n             - Memory address of instruction.\n             - Size of instruction in bytes.\n             - Disassembly line of instruction.\n             - Hexadecimal dump of instruction.\n        \"\"\"\n", "input": "", "output": "        aThread = self.get_thread(dwThreadId)\n        return self.disassemble_instruction(aThread.get_pc())", "category": "Python"}, {"instruction": "def copy_group(from_file, to_file, key):\n    \"\"\"Recursively copy all groups/datasets/attributes from from_file[key] to\n    to_file. Datasets are not overwritten, attributes are.\n    \"\"\"\n", "input": "", "output": "    if not key in to_file:\n        from_file.copy(key, to_file, key)\n    else:\n        # also make sure any additional attributes are copied\n        for attr in from_file[key].attrs:\n            to_file.attrs[attr] = from_file[key].attrs[attr]\n            \n        if hasattr(from_file[key], 'keys'):\n            for subkey in from_file[key].keys():\n                copy_group(from_file, to_file, '/'.join([key,subkey]))", "category": "Python"}, {"instruction": "def unit(u):\n    '''\n    unit(u) yields the pimms-library unit object for the given unit object u (which may be from a\n      separate pint.UnitRegistry instance).\n    unit(uname) yields the unit object for the given unit name uname.\n    unit(None) yields None.\n    unit(q) yields the unit of the given quantity q.\n    '''\n", "input": "", "output": "    if u is None:    return None\n    elif is_unit(u): return getattr(units, str(u))\n    elif is_quantity(u):\n        if isinstance(u, tuple): return getattr(units, str(u[1]))\n        else: return getattr(units, str(u.u))\n    else:\n        raise ValueError('unrecotnized unit argument')", "category": "Python"}, {"instruction": "def _is_present(val):\n    \"\"\"Returns True if the value is not None, and if it is either not a string, or a string with\n    length > 0.\n    \"\"\"\n", "input": "", "output": "    if val is None:\n      return False\n    if isinstance(val, str):\n      return len(val) > 0\n    return True", "category": "Python"}, {"instruction": "def alter_table_with_cascade(self, dbname, tbl_name, new_tbl, cascade):\n    \"\"\"\n    Parameters:\n     - dbname\n     - tbl_name\n     - new_tbl\n     - cascade\n    \"\"\"\n", "input": "", "output": "    self.send_alter_table_with_cascade(dbname, tbl_name, new_tbl, cascade)\n    self.recv_alter_table_with_cascade()", "category": "Python"}, {"instruction": "def copy(self) -> \"ConsoleBuffer\":\n        \"\"\"Returns a copy of this ConsoleBuffer.\n\n        Returns:\n            ConsoleBuffer: A new ConsoleBuffer copy.\n        \"\"\"\n", "input": "", "output": "        other = ConsoleBuffer(0, 0)  # type: \"ConsoleBuffer\"\n        other.width = self.width\n        other.height = self.height\n        other.back_r = list(self.back_r)  # make explicit copies of all lists\n        other.back_g = list(self.back_g)\n        other.back_b = list(self.back_b)\n        other.fore_r = list(self.fore_r)\n        other.fore_g = list(self.fore_g)\n        other.fore_b = list(self.fore_b)\n        other.char = list(self.char)\n        return other", "category": "Python"}, {"instruction": "def parse_from_args(synonyms):\n    '''\n    Parse an array of string from argparser\n    to SynonymSet\n    '''\n", "input": "", "output": "\n    syns_str = ''.join(synonyms)\n\n    syns_str = syns_str.replace(' ', '')\n    \n    syn_set = SynonymSet()\n\n    # to check if we are parsing inside the parenthesis\n    inside_set = False\n    current_syn = ''\n    current_syn_set = set()\n\n    for char in syns_str:\n        if char == '{':\n            inside_set = True\n            current_syn_set = set()\n            continue\n\n        if char == '}':\n            inside_set = False\n            current_syn_set.add(current_syn)\n\n            syn_set.add_set(current_syn_set)\n\n            current_syn = ''\n\n            continue\n\n        if not inside_set:\n            raise Exception(\"Incorrect synonyms {}\".format(syns_str))\n\n        if char == ',':\n            if current_syn == '':\n                raise Exception(\"Incorrect synonyms {}\".format(syns_str))\n\n            current_syn_set.add(current_syn)\n            current_syn = ''\n\n            continue\n\n        current_syn += char\n\n    return syn_set", "category": "Python"}, {"instruction": "def sql_like_fragments(self) -> List[str]:\n        \"\"\"\n        Returns all the string literals to which a database column should be\n        compared using the SQL ``LIKE`` operator, to match this drug.\n\n        This isn't as accurate as the regex, but ``LIKE`` can do less.\n\n        ``LIKE`` uses the wildcards ``?`` and ``%``.\n        \"\"\"\n", "input": "", "output": "        if self._sql_like_fragments is None:\n            self._sql_like_fragments = []\n            for p in list(set(self.all_generics + self.alternatives)):\n                self._sql_like_fragments.extend(self.regex_to_sql_like(p))\n        return self._sql_like_fragments", "category": "Python"}, {"instruction": "def cleanup_outdir (outdir, archive):\n    \"\"\"Cleanup outdir after extraction and return target file name and\n    result string.\"\"\"\n", "input": "", "output": "    make_user_readable(outdir)\n    # move single directory or file in outdir\n    (success, msg) = move_outdir_orphan(outdir)\n    if success:\n        # msg is a single directory or filename\n        return msg, \"`%s'\" % msg\n    # outdir remains unchanged\n    # rename it to something more user-friendly (basically the archive\n    # name without extension)\n    outdir2 = util.get_single_outfile(\"\", archive)\n    os.rename(outdir, outdir2)\n    return outdir2, \"`%s' (%s)\" % (outdir2, msg)", "category": "Python"}, {"instruction": "def from_text(text):\n    \"\"\"Convert text into an opcode.\n\n    @param text: the textual opcode\n    @type text: string\n    @raises UnknownOpcode: the opcode is unknown\n    @rtype: int\n    \"\"\"\n", "input": "", "output": "\n    if text.isdigit():\n        value = int(text)\n        if value >= 0 and value <= 15:\n            return value\n    value = _by_text.get(text.upper())\n    if value is None:\n        raise UnknownOpcode\n    return value", "category": "Python"}, {"instruction": "def checksum(fpath, hasher=None, asbytes=False):\n    \"\"\"Returns the checksum of the file at the given path as a hex string\n    (default) or as a bytes literal. Uses MD5 by default.\n\n    **Attribution**:\n    Based on code from\n    `Stack Overflow <https://stackoverflow.com/a/3431835/789078>`_.\"\"\"\n", "input": "", "output": "    def blockiter(fpath, blocksize=0x1000):\n        with open(fpath, \"rb\") as afile:\n            block = afile.read(blocksize)\n            while len(block) > 0:\n                yield block\n                block = afile.read(blocksize)\n    hasher = hasher or hashlib.md5()\n    for block in blockiter(fpath):\n        hasher.update(block)\n    return (hasher.digest() if asbytes else hasher.hexdigest())", "category": "Python"}, {"instruction": "def nfa_complementation(nfa: dict) -> dict:\n    \"\"\" Returns a DFA reading the complemented language read by\n    input NFA.\n\n    Complement a nondeterministic automaton is possible\n    complementing the determinization of it.\n    The construction is effective, but it involves an exponential\n    blow-up, since determinization involves an unavoidable\n    exponential blow-up (i.e., if NFA has n states,\n    then the DFA has :math:`2^n` states).\n\n    :param dict nfa: input NFA.\n    :return: *(dict)* representing a completed DFA.\n    \"\"\"\n", "input": "", "output": "    determinized_nfa = nfa_determinization(nfa)\n    return DFA.dfa_complementation(determinized_nfa)", "category": "Python"}, {"instruction": "def SetColor(self, color):\n        \"\"\"\n        *color* may be any color understood by ROOT or matplotlib.\n\n        Set all color attributes with one method call.\n\n        For full documentation of accepted *color* arguments, see\n        :class:`rootpy.plotting.style.Color`.\n        \"\"\"\n", "input": "", "output": "        self.SetFillColor(color)\n        self.SetLineColor(color)\n        self.SetMarkerColor(color)", "category": "Python"}, {"instruction": "def get_file_paths(self, id_list):\n        \"\"\"Get a list of file paths based of a set of ids\n\n        Parameters\n        ----------\n\n        id_list : list\n            List of integer file keys\n\n        Returns list of file paths\n        \"\"\"\n", "input": "", "output": "        if id_list is None:\n            return []\n        try:\n            path_array = self._table[id_list - 1]['path']\n        except IndexError:\n            print(\"IndexError \", len(self._table), id_list)\n            path_array = []\n        return [path for path in path_array]", "category": "Python"}, {"instruction": "def to_bool(x):\n    \"\"\"\"Converts to a boolean in a semantically meaningful way.\"\"\"\n", "input": "", "output": "    if isinstance(x, bool):\n        return x\n    elif isinstance(x, str):\n        return False if x.lower() in _FALSES else True\n    else:\n        return bool(x)", "category": "Python"}, {"instruction": "def createURL(self, word, mode=\"phonefy\"):\n        \"\"\"\n        Method to create the URL replacing the word in the appropriate URL.\n\n        Args:\n        -----\n            word: Word to be searched.\n            mode: Mode to be executed.\n\n        Return:\n        -------\n            The URL to be queried.\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.modes[mode][\"url\"].format(placeholder=urllib.pathname2url(word))\n        except:\n            if mode == \"base\":\n                if word[0] == \"/\":\n                    return self.baseURL+word[1:], word\n                else:\n                    return self.baseURL+word\n            else:\n                try:\n                    return self.url[mode].replace(\"<\"+mode+\">\", urllib.pathname2url(word))\n                except:\n                    pass\n        return None", "category": "Python"}, {"instruction": "def import_reference(self, ):\n        \"\"\"Import the currently loaded reference\n\n        This will also update the status to :data:`Reftrack.IMPORTED`.\n\n        :returns: None\n        :rtype: None\n        :raises: :class:`ReftrackIntegrityError`\n        \"\"\"\n", "input": "", "output": "        assert self.status() in (self.LOADED, self.UNLOADED),\\\n            \"There is no reference for this entity.\"\n        refobjinter = self.get_refobjinter()\n        refobjinter.import_reference(self.get_refobj())\n        self.set_status(self.IMPORTED)\n        self.update_restrictions()\n        for c in self.get_all_children():\n            c.update_restrictions()\n        self.emit_data_changed()", "category": "Python"}, {"instruction": "def fcoe_get_login_output_fcoe_login_list_interface_type(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        fcoe_get_login = ET.Element(\"fcoe_get_login\")\n        config = fcoe_get_login\n        output = ET.SubElement(fcoe_get_login, \"output\")\n        fcoe_login_list = ET.SubElement(output, \"fcoe-login-list\")\n        fcoe_login_session_mac_key = ET.SubElement(fcoe_login_list, \"fcoe-login-session-mac\")\n        fcoe_login_session_mac_key.text = kwargs.pop('fcoe_login_session_mac')\n        interface_type = ET.SubElement(fcoe_login_list, \"interface-type\")\n        interface_type.text = kwargs.pop('interface_type')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def setDisabledColor(self, color):\n        \"\"\"\n        Sets the disabled color used when drawing this node as disabled.\n        \n        :param      color | <QColor>\n        \"\"\"\n", "input": "", "output": "        color = QColor(color)\n        if self._palette is None:\n            self._palette = XNodePalette(self._scenePalette)\n        \n        self._palette.setColor(self._palette.Disabled,\n                               self._palette.NodeBackground,\n                               color)\n        self._palette.setColor(self._palette.Disabled,\n                               self._palette.NodeAlternateBackground,\n                               color.darker(105))\n        self.setDirty()", "category": "Python"}, {"instruction": "def _get_nets_arin(self, *args, **kwargs):\n        \"\"\"\n        Deprecated. This will be removed in a future release.\n        \"\"\"\n", "input": "", "output": "\n        from warnings import warn\n        warn('Whois._get_nets_arin() has been deprecated and will be '\n             'removed. You should now use Whois.get_nets_arin().')\n        return self.get_nets_arin(*args, **kwargs)", "category": "Python"}, {"instruction": "def do_echo(self, line):\n        \"\"\"echo TEXT...\n\n           Display a line of text.\n        \"\"\"\n", "input": "", "output": "        args = self.line_to_args(line)\n        self.print(*args)", "category": "Python"}, {"instruction": "def measure(note1, note2):\n    \"\"\"Return an integer in the range of 0-11, determining the half note steps\n    between note1 and note2.\n\n    Examples:\n    >>> measure('C', 'D')\n    2\n    >>> measure('D', 'C')\n    10\n    \"\"\"\n", "input": "", "output": "    res = notes.note_to_int(note2) - notes.note_to_int(note1)\n    if res < 0:\n        return 12 - res * -1\n    else:\n        return res", "category": "Python"}, {"instruction": "def pic_loggedrequiredremoterelease_v1(self):\n    \"\"\"Update the receiver link sequence.\"\"\"\n", "input": "", "output": "    log = self.sequences.logs.fastaccess\n    rec = self.sequences.receivers.fastaccess\n    log.loggedrequiredremoterelease[0] = rec.d[0]", "category": "Python"}, {"instruction": "def _CheckIsPipe(self, file_entry):\n    \"\"\"Checks the is_pipe find specification.\n\n    Args:\n      file_entry (FileEntry): file entry.\n\n    Returns:\n      bool: True if the file entry matches the find specification, False if not.\n    \"\"\"\n", "input": "", "output": "    if definitions.FILE_ENTRY_TYPE_PIPE not in self._file_entry_types:\n      return False\n    return file_entry.IsPipe()", "category": "Python"}, {"instruction": "def blend_rect(self, x0, y0, x1, y1, dx, dy, destination, alpha=0xff):\n        \"\"\"Blend a rectangle onto the image\"\"\"\n", "input": "", "output": "        x0, y0, x1, y1 = self.rect_helper(x0, y0, x1, y1)\n        for x in range(x0, x1 + 1):\n            for y in range(y0, y1 + 1):\n                o = self._offset(x, y)\n                rgba = self.canvas[o:o + 4]\n                rgba[3] = alpha\n                destination.point(dx + x - x0, dy + y - y0, rgba)", "category": "Python"}, {"instruction": "def get_local_file(file):\n    \"\"\"\n    Get a local version of the file, downloading it from the remote storage if\n    required. The returned value should be used as a context manager to\n    ensure any temporary files are cleaned up afterwards.\n    \"\"\"\n", "input": "", "output": "    try:\n        with open(file.path):\n            yield file.path\n    except NotImplementedError:\n        _, ext = os.path.splitext(file.name)\n        with NamedTemporaryFile(prefix='wagtailvideo-', suffix=ext) as tmp:\n            try:\n                file.open('rb')\n                for chunk in file.chunks():\n                    tmp.write(chunk)\n            finally:\n                file.close()\n            tmp.flush()\n            yield tmp.name", "category": "Python"}, {"instruction": "def distinguish(self, id_, how=True):\n        \"\"\"Login required.  Sends POST to distinguish a submission or comment.  Returns :class:`things.Link` or :class:`things.Comment`, or raises :class:`exceptions.UnexpectedResponse` otherwise.\n        \n        URL: ``http://www.reddit.com/api/distinguish/``\n        \n        :param id\\_: full id of object to distinguish\n        :param how: either True, False, or 'admin'\n        \"\"\"\n", "input": "", "output": "        if how == True:\n            h = 'yes'\n        elif how == False:\n            h = 'no'\n        elif how == 'admin':\n            h = 'admin'\n        else:\n            raise ValueError(\"how must be either True, False, or 'admin'\") \n        data = dict(id=id_)\n        j = self.post('api', 'distinguish', h, data=data)\n        try:\n            return self._thingify(j['json']['data']['things'][0])\n        except Exception:\n            raise UnexpectedResponse(j)", "category": "Python"}, {"instruction": "def _tie_correct(sample):\n    \"\"\"\n    Returns the tie correction value for U.\n\n    See: https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.tiecorrect.html\n\n    \"\"\"\n", "input": "", "output": "    tc = 0\n    n = sum(sample.values())\n\n    if n < 2:\n        return 1.0  # Avoid a ``ZeroDivisionError``.\n\n    for k in sorted(sample.keys()):\n        tc += math.pow(sample[k], 3) - sample[k]\n    tc = 1 - tc / (math.pow(n, 3) - n)\n\n    return tc", "category": "Python"}, {"instruction": "def _inverse_permutation_indices(positions):\n    \"\"\"Like inverse_permutation, but also handles slices.\n\n    Parameters\n    ----------\n    positions : list of np.ndarray or slice objects.\n        If slice objects, all are assumed to be slices.\n\n    Returns\n    -------\n    np.ndarray of indices or None, if no permutation is necessary.\n    \"\"\"\n", "input": "", "output": "    if not positions:\n        return None\n\n    if isinstance(positions[0], slice):\n        positions = _consolidate_slices(positions)\n        if positions == slice(None):\n            return None\n        positions = [np.arange(sl.start, sl.stop, sl.step) for sl in positions]\n\n    indices = nputils.inverse_permutation(np.concatenate(positions))\n    return indices", "category": "Python"}, {"instruction": "def _stop(self):\n        ''' calculate the duration we actually paused for and then\n        finish building the task result string '''\n", "input": "", "output": "        duration = time.time() - self.start\n        self.result['stop'] = str(datetime.datetime.now())\n        self.result['delta'] = int(duration)\n\n        if self.duration_unit == 'minutes':\n            duration = round(duration / 60.0, 2)\n        else:\n            duration = round(duration, 2)\n\n        self.result['stdout'] = \"Paused for %s %s\" % (duration, self.duration_unit)", "category": "Python"}, {"instruction": "def export_figure(dpi=200, figure=\"gcf\", path=None):\n    \"\"\"\n    Saves the actual postscript data for the figure.\n    \"\"\"\n", "input": "", "output": "    if figure==\"gcf\": figure = _pylab.gcf()\n\n    if path==None: path = _s.dialogs.Save(\"*.*\", default_directory=\"save_plot_default_directory\")\n\n    if path==\"\":\n        print(\"aborted.\")\n        return\n\n    figure.savefig(path, dpi=dpi)", "category": "Python"}, {"instruction": "def _activate(self, icon):\n        \"\"\"Handle a left click event (show the menu).\"\"\"\n", "input": "", "output": "        self._popup_menu(icon, button=0, time=Gtk.get_current_event_time(),\n                         extended=False)", "category": "Python"}, {"instruction": "def _get_commit_msg(self, repo, sha):\n        '''\n        :param repo: the repo full name, ``{owner}/{project}``.\n            e.g. ``buildbot/buildbot``\n        '''\n", "input": "", "output": "        headers = {\n            'User-Agent': 'Buildbot'\n        }\n        if self._token:\n            headers['Authorization'] = 'token ' + self._token\n\n        url = '/repos/{}/commits/{}'.format(repo, sha)\n        http = yield httpclientservice.HTTPClientService.getService(\n            self.master, self.github_api_endpoint, headers=headers,\n            debug=self.debug, verify=self.verify)\n        res = yield http.get(url)\n        data = yield res.json()\n        msg = data.get('commit', {'message': 'No message field'})['message']\n        return msg", "category": "Python"}, {"instruction": "def from_dict(cls, data):\n        \"\"\"\n        :type data: dict[str, str]\n        :rtype: satosa.internal.AuthenticationInformation\n        :param data: A dict representation of an AuthenticationInformation object\n        :return: An AuthenticationInformation object\n        \"\"\"\n", "input": "", "output": "        return cls(\n            auth_class_ref=data.get(\"auth_class_ref\"),\n            timestamp=data.get(\"timestamp\"),\n            issuer=data.get(\"issuer\"),\n        )", "category": "Python"}, {"instruction": "def apply(self, q, bindings, drilldowns):\n        \"\"\" Apply a set of grouping criteria and project them. \"\"\"\n", "input": "", "output": "        info = []\n        for drilldown in self.parse(drilldowns):\n            for attribute in self.cube.model.match(drilldown):\n                info.append(attribute.ref)\n                table, column = attribute.bind(self.cube)\n                bindings.append(Binding(table, attribute.ref))\n                q = q.column(column)\n                q = q.group_by(column)\n        return info, q, bindings", "category": "Python"}, {"instruction": "def get_translated_file(fapi, file_uri, locale, retrieval_type, include_original_strings, use_cache, cache_dir=None):\n    \"\"\" Returns a translated file from smartling\n    \"\"\"\n", "input": "", "output": "    file_data = None\n    cache_name = str(file_uri)+\".\"+str(locale)+\".\"+str(retrieval_type)+\".\"+str(include_original_strings)\n    cache_file = os.path.join(cache_dir, sha1(cache_name)) if cache_dir else None\n\n    if use_cache and os.path.exists(cache_file):\n        print(\"Using cache file %s for %s translation file: %s\" % (cache_file, locale, file_uri))\n        file_data = read_from_file(cache_file)\n    elif not use_cache:\n        (file_data, code) = fapi.get(file_uri, locale,\n            retrievalType=retrieval_type,\n            includeOriginalStrings=include_original_strings)\n        file_data = str(file_data).strip()\n        if cache_file and code == 200 and len(file_data)>0:\n            print(\"Chaching to %s for %s translation file: %s\" % (cache_file, locale, file_uri))\n            write_to_file(cache_file, file_data)\n    if not file_data or len(file_data)==0:\n        print(\"%s translation not found for %s\" % (locale, file_uri))\n        return None\n    return file_data", "category": "Python"}, {"instruction": "def by_filter(cls, session, opts, **kwargs):\n        \"\"\"\n        Get packages from given filters.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param opts: filtering options\n        :type opts: `dict\n\n        :return: package instances\n        :rtype: generator of :class:`pyshop.models.Package`\n        \"\"\"\n", "input": "", "output": "        where = []\n\n        if opts.get('local_only'):\n            where.append(cls.local == True)\n\n        if opts.get('names'):\n            where.append(cls.name.in_(opts['names']))\n\n        if opts.get('classifiers'):\n            ids = [c.id for c in opts.get('classifiers')]\n            cls_pkg = classifier__package\n            qry = session.query(cls_pkg.c.package_id,\n                                func.count('*'))\n            qry = qry.filter(cls_pkg.c.classifier_id.in_(ids))\n            qry = qry.group_by(cls_pkg.c.package_id)\n            qry = qry.having(func.count('*') >= len(ids))\n            where.append(cls.id.in_([r[0] for r in qry.all()]))\n\n        return cls.find(session, where=where, **kwargs)", "category": "Python"}, {"instruction": "def visit_GeneratorExp(self, node: AST, dfltChaining: bool = True) -> str:\n        \"\"\"Return `node`s representation as generator expression.\"\"\"\n", "input": "", "output": "        return f\"({self.visit(node.elt)} \" \\\n               f\"{' '.join(self.visit(gen) for gen in node.generators)})\"", "category": "Python"}, {"instruction": "def render_string(self, path: str, **kwargs: Any) -> bytes:\n        \"\"\"Renders a template and returns it as a string.\"\"\"\n", "input": "", "output": "        return self.handler.render_string(path, **kwargs)", "category": "Python"}, {"instruction": "def check_dynamic_route_exists(pattern, routes_to_check, parameters):\n        \"\"\"\n        Check if a URL pattern exists in a list of routes provided based on\n        the comparison of URL pattern and the parameters.\n\n        :param pattern: URL parameter pattern\n        :param routes_to_check: list of dynamic routes either hashable or\n            unhashable routes.\n        :param parameters: List of :class:`Parameter` items\n        :return: Tuple of index and route if matching route exists else\n            -1 for index and None for route\n        \"\"\"\n", "input": "", "output": "        for ndx, route in enumerate(routes_to_check):\n            if route.pattern == pattern and route.parameters == parameters:\n                return ndx, route\n        else:\n            return -1, None", "category": "Python"}, {"instruction": "def set_connection(self, url):\n        \"\"\"\n        Sets the connection URL to the address a Neo4j server is set up at\n        \"\"\"\n", "input": "", "output": "        u = urlparse(url)\n\n        if u.netloc.find('@') > -1 and (u.scheme == 'bolt' or u.scheme == 'bolt+routing'):\n            credentials, hostname = u.netloc.rsplit('@', 1)\n            username, password, = credentials.split(':')\n        else:\n            raise ValueError(\"Expecting url format: bolt://user:password@localhost:7687\"\n                             \" got {0}\".format(url))\n\n        self.driver = GraphDatabase.driver(u.scheme + '://' + hostname,\n                                           auth=basic_auth(username, password),\n                                           encrypted=config.ENCRYPTED_CONNECTION,\n                                           max_pool_size=config.MAX_POOL_SIZE)\n        self.url = url\n        self._pid = os.getpid()\n        self._active_transaction = None", "category": "Python"}, {"instruction": "def apply_fit(xy,coeffs):\n    \"\"\" Apply the coefficients from a linear fit to\n        an array of x,y positions.\n\n        The coeffs come from the 'coeffs' member of the\n        'fit_arrays()' output.\n    \"\"\"\n", "input": "", "output": "    x_new = coeffs[0][2] + coeffs[0][0]*xy[:,0] + coeffs[0][1]*xy[:,1]\n    y_new = coeffs[1][2] + coeffs[1][0]*xy[:,0] + coeffs[1][1]*xy[:,1]\n\n    return x_new,y_new", "category": "Python"}, {"instruction": "def _fetch_AlignmentMapper(self, tx_ac, alt_ac=None, alt_aln_method=None):\n        \"\"\"convenience version of VariantMapper._fetch_AlignmentMapper that\n        derives alt_ac from transcript, assembly, and alt_aln_method\n        used to instantiate the AssemblyMapper instance\n\n        \"\"\"\n", "input": "", "output": "\n        if alt_ac is None:\n            alt_ac = self._alt_ac_for_tx_ac(tx_ac)\n        if alt_aln_method is None:\n            alt_aln_method = self.alt_aln_method\n        return super(AssemblyMapper, self)._fetch_AlignmentMapper(tx_ac, alt_ac, alt_aln_method)", "category": "Python"}, {"instruction": "def GET(self, mid=None):\n        '''\n        A convenience URL for getting lists of minions or getting minion\n        details\n\n        .. http:get:: /minions/(mid)\n\n            :reqheader X-Auth-Token: |req_token|\n            :reqheader Accept: |req_accept|\n\n            :status 200: |200|\n            :status 401: |401|\n            :status 406: |406|\n\n        **Example request:**\n\n        .. code-block:: bash\n\n            curl -i localhost:8000/minions/ms-3\n\n        .. code-block:: text\n\n            GET /minions/ms-3 HTTP/1.1\n            Host: localhost:8000\n            Accept: application/x-yaml\n\n        **Example response:**\n\n        .. code-block:: text\n\n            HTTP/1.1 200 OK\n            Content-Length: 129005\n            Content-Type: application/x-yaml\n\n            return:\n            - ms-3:\n                grains.items:\n                    ...\n        '''\n", "input": "", "output": "        cherrypy.request.lowstate = [{\n            'client': 'local', 'tgt': mid or '*', 'fun': 'grains.items',\n        }]\n        return {\n            'return': list(self.exec_lowstate(\n                token=cherrypy.session.get('token'))),\n        }", "category": "Python"}, {"instruction": "def fcoe_get_login_input_fcoe_login_vlan(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        fcoe_get_login = ET.Element(\"fcoe_get_login\")\n        config = fcoe_get_login\n        input = ET.SubElement(fcoe_get_login, \"input\")\n        fcoe_login_vlan = ET.SubElement(input, \"fcoe-login-vlan\")\n        fcoe_login_vlan.text = kwargs.pop('fcoe_login_vlan')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def _update(self):\n        \"\"\"Reload Taskwarrior files if the mtime is newer\"\"\"\n", "input": "", "output": "        update = False\n\n        with self._lock:\n            for fname in ['pending.data', 'completed.data']:\n                data_file = join(self._data_location, fname)\n                if exists(data_file):\n                    mtime = getmtime(data_file)\n                    if mtime > self._mtime:\n                        self._mtime = mtime\n                        update = True\n\n            if update:\n                self._tasks = {}\n                tasklist = loads(run(['task', 'rc.verbose=nothing', 'rc.hooks=off', 'rc.data.location={self._data_location}'.format(**locals()), 'export'], stdout=PIPE).stdout.decode('utf-8'))\n                for task in tasklist:\n                    project = task['project'] if 'project' in task else 'unaffiliated'\n                    if project not in self._tasks:\n                        self._tasks[project] = {}\n                    self._tasks[project][task['uuid']] = task", "category": "Python"}, {"instruction": "def dict_match(d, key, default=None):\n    \"\"\"Like __getitem__ but works as if the keys() are all filename patterns.\n    Returns the value of any dict key that matches the passed key.\n\n    Args:\n        d (dict): A dict with filename patterns as keys\n        key (str): A key potentially matching any of the keys\n        default (object): The object to return if no pattern matched the\n            passed in key\n    Returns:\n        object: The dict value where the dict key matched the passed in key.\n            Or default if there was no match.\n    \"\"\"\n", "input": "", "output": "\n    if key in d and \"[\" not in key:\n        return d[key]\n    else:\n        for pattern, value in iteritems(d):\n            if fnmatchcase(key, pattern):\n                return value\n    return default", "category": "Python"}, {"instruction": "def get_default_gateway():\n    \"\"\"\n    Attempts to read /proc/self/net/route to determine the default gateway in use.\n\n    :return: String - the ip address of the default gateway or None if not found/possible/non-existant\n    \"\"\"\n", "input": "", "output": "    try:\n        # The first line is the header line\n        # We look for the line where the Destination is 00000000 - that is the default route\n        # The Gateway IP is encoded backwards in hex.\n        with open(\"/proc/self/net/route\") as routes:\n            for line in routes:\n                parts = line.split('\\t')\n                if '00000000' == parts[1]:\n                    hip = parts[2]\n\n        if hip is not None and len(hip) is 8:\n            # Reverse order, convert hex to int\n            return \"%i.%i.%i.%i\" % (int(hip[6:8], 16), int(hip[4:6], 16), int(hip[2:4], 16), int(hip[0:2], 16))\n\n    except:\n        logger.warn(\"get_default_gateway: \", exc_info=True)", "category": "Python"}, {"instruction": "def make_data(self, message):\n\n        \"\"\"\n        make data string from message according to transport_content_type\n\n        Returns:\n\n            str: message data\n        \"\"\"\n", "input": "", "output": "\n        if not isinstance(message, Message):\n            return message\n        return message.export(self.transport_content_type)", "category": "Python"}, {"instruction": "def universal_anisotropy(self):\n        \"\"\"\n        returns the universal anisotropy value\n        \"\"\"\n", "input": "", "output": "        return 5. * self.g_voigt / self.g_reuss + \\\n            self.k_voigt / self.k_reuss - 6.", "category": "Python"}, {"instruction": "def parse_manifest(self, fpath):\n        \"\"\"Parse manifest file to build up the collection of images.\n        \n        :param fpath: path to the manifest file\n        \"\"\"\n", "input": "", "output": "        with open(fpath, 'r') as fh:\n            for entry in json.load(fh):\n\n                # Every entry of a manifest file needs to have a \"filename\"\n                # attribute. It is the only requirement so we check for it in a\n                # strict fashion.\n                if \"filename\" not in entry:\n                    raise(RuntimeError(\n                        'Entries in {} need to have \"filename\"'.format(fpath)))\n\n                filename = entry.pop(\"filename\")\n\n                proxy_image = None\n                if isinstance(self, MicroscopyCollection):\n                    proxy_image = MicroscopyImage(filename, entry)\n                else:\n                    proxy_image = ProxyImage(filename, entry)\n                self.append(proxy_image)", "category": "Python"}, {"instruction": "def restore(self):\n        \"\"\"\n        Restore the main dataframe\n        \"\"\"\n", "input": "", "output": "        if self.backup_df is None:\n            self.warning(\"No dataframe is backed up: nothing restore\")\n            return\n        self.df = self.backup_df\n        self.ok(\"Dataframe is restored\")", "category": "Python"}, {"instruction": "def query_param(name, field, required=False):\n    \"\"\"\n    Build a query parameter definition.\n\n    \"\"\"\n", "input": "", "output": "    parameter = build_parameter(field)\n    parameter[\"name\"] = name\n    parameter[\"in\"] = \"query\"\n    parameter[\"required\"] = False\n\n    return swagger.QueryParameterSubSchema(**parameter)", "category": "Python"}, {"instruction": "def real(self):\n        ''' Get realtime data\n\n            :rtype: dict\n            :returns: \u4ee3\u78bc\u53ef\u4ee5\u53c3\u8003\uff1ahttp://goristock.appspot.com/API#apiweight\n        '''\n", "input": "", "output": "        result = self.__raw['1'].copy()\n        result['c'] = self.__raw['1']['value']\n        result['value'] = self.__raw['200']['v2']\n        result['date'] = self.__raw['0']['time']\n        return result", "category": "Python"}, {"instruction": "def get_complete_version(version=None):\n    \"\"\"Returns a tuple of the promise version. If version argument is non-empty,\n    then checks for correctness of the tuple provided.\n    \"\"\"\n", "input": "", "output": "    if version is None:\n        from promise import VERSION\n\n        return VERSION\n    else:\n        assert len(version) == 5\n        assert version[3] in (\"alpha\", \"beta\", \"rc\", \"final\")\n\n    return version", "category": "Python"}, {"instruction": "def render(self, obj, opt=None):\n        \"\"\" render a Schema/Parameter\n\n        :param obj Schema/Parameter: the swagger spec object\n        :param opt dict: render option\n        :return: values that can be passed to Operation.__call__\n        :rtype: depends on type of 'obj'\n        \"\"\"\n", "input": "", "output": "        opt = self.default() if opt == None else opt\n        if not isinstance(opt, dict):\n            raise ValueError('Not a dict: {0}'.format(opt))\n\n        if isinstance(obj, Parameter):\n            if getattr(obj, 'in', None) == 'body':\n                return self._generate(obj.schema, opt)\n            return self._generate(obj, opt=opt)\n        elif isinstance(obj, Schema):\n            return self._generate(obj, opt)\n        else:\n            raise ValueError('Not a Schema/Parameter: {0}'.format(obj))", "category": "Python"}, {"instruction": "def parse(cls, detail_string):\n        \"\"\"Parse a string represention of Credentials.\"\"\"\n", "input": "", "output": "        split = detail_string.split(':')\n        if len(split) != 4:\n            raise Exception('invalid credentials')  # TODO: other exception\n\n        ltpk = binascii.unhexlify(split[0])\n        ltsk = binascii.unhexlify(split[1])\n        atv_id = binascii.unhexlify(split[2])\n        client_id = binascii.unhexlify(split[3])\n        return Credentials(ltpk, ltsk, atv_id, client_id)", "category": "Python"}, {"instruction": "def _is_exception_rule(self, element):\n        \"\"\" Check for \"exception rule\".\n\n        Address elements will be appended onto a new line on the lable except\n        for when the penultimate lable line fulfils certain criteria, in which\n        case the element will be concatenated onto the penultimate line. This\n        method checks for those criteria.\n\n        i) First and last characters of the Building Name are numeric\n          (eg '1to1' or '100:1')\n        ii) First and penultimate characters are numeric, last character is\n          alphabetic (eg '12A')\n        iii) Building Name has only one character (eg 'A')\n        \"\"\"\n", "input": "", "output": "        if element[0].isdigit() and element[-1].isdigit():\n            return True\n        if len(element) > 1 and element[0].isdigit() and element[-2].isdigit() and element[-1].isalpha():\n            return True\n        if len(element) == 1 and element.isalpha():\n            return True\n        return False", "category": "Python"}, {"instruction": "def img(self):\n        '''return a wx image'''\n", "input": "", "output": "        import wx\n        with warnings.catch_warnings():\n            warnings.simplefilter('ignore')\n            img = wx.EmptyImage(self.width, self.height)\n            img.SetData(self.imgstr)\n        return img", "category": "Python"}, {"instruction": "def document_path_path(cls, project, database, document_path):\n        \"\"\"Return a fully-qualified document_path string.\"\"\"\n", "input": "", "output": "        return google.api_core.path_template.expand(\n            \"projects/{project}/databases/{database}/documents/{document_path=**}\",\n            project=project,\n            database=database,\n            document_path=document_path,\n        )", "category": "Python"}, {"instruction": "def get_javascript_error(self, return_type='string'):\n        \"\"\"Return the gathered javascript error\n\n        Args:\n            return_type: 'string' | 'list'; default: 'string'\n        \"\"\"\n", "input": "", "output": "\n        if BROME_CONFIG['proxy_driver']['intercept_javascript_error']:\n            js_errors = self._driver.execute_script(\n                'return window.jsErrors; window.jsErrors = [];'\n            )\n\n            if not js_errors:\n                js_errors = []\n\n            if return_type == 'list':\n                if len(js_errors):\n                    return js_errors\n                else:\n                    return []\n            else:\n                if len(js_errors):\n                    return os.linesep.join(js_errors)\n                else:\n                    return self.no_javascript_error_string\n        else:\n            if return_type == 'list':\n                return []\n            else:\n                return self.no_javascript_error_string", "category": "Python"}, {"instruction": "def Stream(self, reader, amount=None):\n    \"\"\"Streams chunks of a given file starting at given offset.\n\n    Args:\n      reader: A `Reader` instance.\n      amount: An upper bound on number of bytes to read.\n\n    Yields:\n      `Chunk` instances.\n    \"\"\"\n", "input": "", "output": "    if amount is None:\n      amount = float(\"inf\")\n\n    data = reader.Read(min(self.chunk_size, amount))\n    if not data:\n      return\n\n    amount -= len(data)\n    offset = reader.offset - len(data)\n    yield Chunk(offset=offset, data=data)\n\n    while amount > 0:\n      # We need `len(data)` here because overlap size can be 0.\n      overlap = data[len(data) - self.overlap_size:]\n\n      new = reader.Read(min(self.chunk_size - self.overlap_size, amount))\n      if not new:\n        return\n\n      data = overlap + new\n\n      amount -= len(new)\n      offset = reader.offset - len(data)\n      yield Chunk(offset=offset, data=data, overlap=len(overlap))", "category": "Python"}, {"instruction": "def bytes_to_int(b, order='big'):  # pragma: no cover\n        \"\"\"convert bytes to integer\"\"\"\n", "input": "", "output": "        if six.PY2:\n            # save the original type of b without having to copy any data\n            _b = b.__class__()\n            if order != 'little':\n                b = reversed(b)\n\n            if not isinstance(_b, bytearray):\n                b = six.iterbytes(b)\n\n            return sum(c << (i * 8) for i, c in enumerate(b))\n\n        return int.from_bytes(b, order)", "category": "Python"}, {"instruction": "def tags_to_versions(tags, config=None):\n    \"\"\"\n    take tags that might be prefixed with a keyword and return only the version part\n    :param tags: an iterable of tags\n    :param config: optional configuration object\n    \"\"\"\n", "input": "", "output": "    result = []\n    for tag in tags:\n        tag = tag_to_version(tag, config=config)\n        if tag:\n            result.append(tag)\n    return result", "category": "Python"}, {"instruction": "def set_log_level(debug, verbose):\n    \"\"\"\n    Function for setting the logging level.\n\n    :param debug: This boolean field is the logging level.\n    :param verbose: This boolean field is the logging level.\n    \"\"\"\n", "input": "", "output": "    if debug:\n        logging.basicConfig(level=logging.DEBUG)\n    elif verbose:\n        logging.basicConfig(level=logging.INFO)", "category": "Python"}, {"instruction": "def all_downstreams(self, node, graph=None):\n        \"\"\"Returns a list of all nodes ultimately downstream\n        of the given node in the dependency graph, in\n        topological order.\"\"\"\n", "input": "", "output": "        if graph is None:\n            graph = self.graph\n        nodes = [node]\n        nodes_seen = set()\n        i = 0\n        while i < len(nodes):\n            downstreams = self.downstream(nodes[i], graph)\n            for downstream_node in downstreams:\n                if downstream_node not in nodes_seen:\n                    nodes_seen.add(downstream_node)\n                    nodes.append(downstream_node)\n            i += 1\n        return list(\n            filter(\n                lambda node: node in nodes_seen,\n                self.topological_sort(graph=graph)\n            )\n        )", "category": "Python"}, {"instruction": "def unlock_file(filename):\n    '''\n    Unlock a locked file\n\n    Note that these locks are only recognized by Salt Cloud, and not other\n    programs or platforms.\n    '''\n", "input": "", "output": "    log.trace('Removing lock for %s', filename)\n    lock = filename + '.lock'\n    try:\n        os.remove(lock)\n    except OSError as exc:\n        log.trace('Unable to remove lock for %s: %s', filename, exc)", "category": "Python"}, {"instruction": "def _filter_kwargs(self, keep_list, **kwargs):\n        ''' Filters the dict of *kwargs*, keeping only arguments \n            whose keys are in *keep_list* and discarding all other\n            arguments.\n            \n            Based on the filtring, constructs and returns a new \n            dict.\n        '''\n", "input": "", "output": "        new_kwargs = {}\n        for argName, argVal in kwargs.items():\n            if argName.lower() in keep_list:\n                new_kwargs[argName.lower()] = argVal\n        return new_kwargs", "category": "Python"}, {"instruction": "def get_sessions(self, app_path=None):\n        ''' Gets all currently active sessions for applications.\n\n        Args:\n            app_path (str, optional) :\n                The configured application path for the application to return\n                sessions for. If None, return active sessions for all\n                applications. (default: None)\n\n        Returns:\n            list[ServerSession]\n\n        '''\n", "input": "", "output": "        if app_path is not None:\n            return self._tornado.get_sessions(app_path)\n        all_sessions = []\n        for path in self._tornado.app_paths:\n            all_sessions += self._tornado.get_sessions(path)\n        return all_sessions", "category": "Python"}, {"instruction": "def validate(self, data):\n        \"\"\"\n        Validate data using sub defined schema/expressions ensuring at least\n        one value is valid.\n        :param data: data to be validated by provided schema.\n        :return: return validated data if not validation\n        \"\"\"\n", "input": "", "output": "        autos, errors = [], []\n        for s in [self._schema(s, error=self._error, ignore_extra_keys=self._ignore_extra_keys) for s in self._args]:\n            try:\n                validation = s.validate(data)\n                self.match_count += 1\n                if self.match_count > 1 and self.only_one:\n                    break\n                return validation\n            except SchemaError as _x:\n                autos, errors = _x.autos, _x.errors\n        raise SchemaError(\n            [\"%r did not validate %r\" % (self, data)] + autos,\n            [self._error.format(data) if self._error else None] + errors,\n        )", "category": "Python"}, {"instruction": "def rbac_policy_list(request, **kwargs):\n    \"\"\"List of RBAC Policies.\"\"\"\n", "input": "", "output": "    policies = neutronclient(request).list_rbac_policies(\n        **kwargs).get('rbac_policies')\n    return [RBACPolicy(p) for p in policies]", "category": "Python"}, {"instruction": "def run(dataset):\n    \"\"\"Run brain locally\"\"\"\n", "input": "", "output": "    config = _get_config()\n\n    if dataset:\n        _print(\"getting dataset from brains...\")\n        cprint(\"done\", 'green')\n\n        # check dataset cache for dataset\n        # if not exists\n        # r = requests.get('https://api.github.com/events', stream=True)\n        # with open(filename, 'wb') as fd:\n        #     for chunk in r.iter_content(chunk_size):\n        #         fd.write(chunk)\n\n    cprint('Running \"%s\"' % config[\"run\"], 'green', attrs=(\"underline\",))\n    call(config[\"run\"].split())", "category": "Python"}, {"instruction": "def wrapped_spawn(self, cmdElements, tag):\n    '''\n    wrap spawn with unique-ish travis fold prints\n    '''\n", "input": "", "output": "    import uuid\n    a = uuid.uuid1()\n    print(\"travis_fold:start:%s-%s\" % (tag, a))\n    try:\n        spawn0(self, cmdElements)\n    finally:\n        print(\"travis_fold:end:%s-%s\" % (tag, a))", "category": "Python"}, {"instruction": "def active_iterations(self):\n\t\t\"\"\"\n\t\tfunction to find active (not marked as finished) iterations \n\n\t\tReturns\n\t\t-------\n\t\t\tlist: all active iteration objects (empty if there are none)\n\t\t\"\"\"\n", "input": "", "output": "\n\t\tl = list(filter(lambda idx: not self.iterations[idx].is_finished, range(len(self.iterations))))\n\t\treturn(l)", "category": "Python"}, {"instruction": "def print_status(self):\n        \"\"\"Provide a snapshot of the current status.\"\"\"\n", "input": "", "output": "        s = []\n        s.append('=== State Machines: ===\\n')\n        for stm_id in Driver._stms_by_id:\n            stm = Driver._stms_by_id[stm_id]\n            s.append('    - {} in state {}\\n'.format(stm.id, stm.state))\n        s.append('=== Events in Queue: ===\\n')\n        for event in self._event_queue.queue:\n            if event is not None:\n                s.append('    - {} for {} with args:{} kwargs:{}\\n'.format(\n                    event['id'], event['stm'].id,\n                    event['args'], event['kwargs']))\n        s.append('=== Active Timers: {} ===\\n'.format(len(self._timer_queue)))\n        for timer in self._timer_queue:\n            s.append('    - {} for {} with timeout {}\\n'.format(\n                timer['id'], timer['stm'].id, timer['timeout']))\n        s.append('=== ================ ===\\n')\n        return ''.join(s)", "category": "Python"}, {"instruction": "def is_valid(self):\n        \"\"\"\n        Tests if the dependency is in a valid state\n        \"\"\"\n", "input": "", "output": "        return (\n            self.requirement is not None and self.requirement.optional\n        ) or bool(self._future_value)", "category": "Python"}, {"instruction": "def create_floatingip(self, floating_network, port=None):\n        '''\n        Creates a new floatingip\n        '''\n", "input": "", "output": "        net_id = self._find_network_id(floating_network)\n        body = {'floating_network_id': net_id}\n        if port:\n            port_id = self._find_port_id(port)\n            body['port_id'] = port_id\n\n        return self.network_conn.create_floatingip(body={'floatingip': body})", "category": "Python"}, {"instruction": "def get_auto_login():\n    '''\n    .. versionadded:: 2016.3.0\n\n    Gets the current setting for Auto Login\n\n    :return: If enabled, returns the user name, otherwise returns False\n    :rtype: str, bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' user.get_auto_login\n    '''\n", "input": "", "output": "    cmd = ['defaults',\n           'read',\n           '/Library/Preferences/com.apple.loginwindow.plist',\n           'autoLoginUser']\n    ret = __salt__['cmd.run_all'](cmd, ignore_retcode=True)\n    return False if ret['retcode'] else ret['stdout']", "category": "Python"}, {"instruction": "def remove_listener(self, registration_id):\n        \"\"\"\n        Removes the specified membership listener.\n\n        :param registration_id: (str), registration id of the listener to be deleted.\n        :return: (bool), if the registration is removed, ``false`` otherwise.\n        \"\"\"\n", "input": "", "output": "        try:\n            self.listeners.pop(registration_id)\n            return True\n        except KeyError:\n            return False", "category": "Python"}, {"instruction": "def notification_selected_sm_changed(self, model, prop_name, info):\n        \"\"\"If a new state machine is selected, make sure expansion state is stored and tree updated\"\"\"\n", "input": "", "output": "        selected_state_machine_id = self.model.selected_state_machine_id\n        if selected_state_machine_id is None:\n            return\n        self.update()", "category": "Python"}, {"instruction": "def _checkIdEquality(self, requestedEffect, effect):\n        \"\"\"\n        Tests whether a requested effect and an effect\n        present in an annotation are equal.\n        \"\"\"\n", "input": "", "output": "        return self._idPresent(requestedEffect) and (\n            effect.term_id == requestedEffect.term_id)", "category": "Python"}, {"instruction": "def make_map():\r\n    \"\"\"Create, configure and return the routes Mapper\"\"\"\n", "input": "", "output": "    map = Mapper(directory=config['pylons.paths']['controllers'],\r\n                 always_scan=config['debug'])\r\n\r\n    # The ErrorController route (handles 404/500 error pages); it should\r\n    # likely stay at the top, ensuring it can always be resolved\r\n    map.connect('error/:action/:id', controller='error')\r\n\r\n    # CUSTOM ROUTES HERE\r\n\r\n    map.connect('connect/:server', controller='irc', action='connect')\r\n    map.connect('pull/:id', controller='irc', action='pull')\r\n    map.connect('push/:id', controller='irc', action='push')\r\n\r\n    map.connect(':controller/:action/:id')\r\n\r\n    return map", "category": "Python"}, {"instruction": "def get_period(date_from: date, date_to: date) -> str:\n    \"\"\" Returns the period string from the given dates \"\"\"\n", "input": "", "output": "    assert isinstance(date_from, date)\n    assert isinstance(date_to, date)\n\n    str_from: str = date_from.isoformat()\n    str_to: str = date_to.isoformat()\n\n    return str_from + \" - \" + str_to", "category": "Python"}, {"instruction": "def flash(self, flash):\n        \"\"\"\n        Turn on or off flashing of the device's LED for physical\n        identification purposes.\n        \"\"\"\n", "input": "", "output": "        self.m_objPCANBasic.SetValue(self.m_PcanHandle, PCAN_CHANNEL_IDENTIFYING, bool(flash))", "category": "Python"}, {"instruction": "def pin_assets(self, file_or_dir_path: Path) -> List[Dict[str, str]]:\n        \"\"\"\n        Return a dict containing the IPFS hash, file name, and size of a file.\n        \"\"\"\n", "input": "", "output": "        if file_or_dir_path.is_dir():\n            asset_data = [dummy_ipfs_pin(path) for path in file_or_dir_path.glob(\"*\")]\n        elif file_or_dir_path.is_file():\n            asset_data = [dummy_ipfs_pin(file_or_dir_path)]\n        else:\n            raise FileNotFoundError(\n                f\"{file_or_dir_path} is not a valid file or directory path.\"\n            )\n        return asset_data", "category": "Python"}, {"instruction": "def pprint_arg(vnames, value):\n    \"\"\"\n    pretty print argument\n    :param vnames:\n    :param value:\n    :return:\n    \"\"\"\n", "input": "", "output": "    ret = ''\n    for name, v in zip(vnames, value):\n        ret += '%s=%s;' % (name, str(v))\n    return ret;", "category": "Python"}, {"instruction": "def blob_info(self):\n    \"\"\"Returns the BlobInfo for this file.\"\"\"\n", "input": "", "output": "    if not self.__blob_info:\n      self.__blob_info = BlobInfo.get(self.__blob_key)\n    return self.__blob_info", "category": "Python"}, {"instruction": "def getCallSetId(self, sampleName):\n        \"\"\"\n        Returns the callSetId for the specified sampleName in this\n        VariantSet.\n        \"\"\"\n", "input": "", "output": "        compoundId = datamodel.CallSetCompoundId(\n            self.getCompoundId(), sampleName)\n        return str(compoundId)", "category": "Python"}, {"instruction": "def filter_queryset(self, request, queryset, view):\n        \"\"\"Filter the queryset.\n\n        This is the main entry-point to this class, and\n        is called by DRF's list handler.\n        \"\"\"\n", "input": "", "output": "        self.request = request\n        self.view = view\n\n        # enable addition of extra filters (i.e., a Q())\n        # so custom filters can be added to the queryset without\n        # running into https://code.djangoproject.com/ticket/18437\n        # which, without this, would mean that filters added to the queryset\n        # after this is called may not behave as expected\n        extra_filters = self.view.get_extra_filters(request)\n\n        disable_prefetches = self.view.is_update()\n\n        self.DEBUG = settings.DEBUG\n\n        return self._build_queryset(\n            queryset=queryset,\n            extra_filters=extra_filters,\n            disable_prefetches=disable_prefetches,\n        )", "category": "Python"}, {"instruction": "def add_timex(self, timex_obj):\n        \"\"\"\n        Adds a timex object to the layer.\n        @type timex_obj: L{Ctime}\n        @param timex_obj: the timex object\n        \"\"\"\n", "input": "", "output": "        timex_id = timex_obj.get_id()\n        #check if id is not already present\n        if not timex_id in self.idx:\n            \n            timex_node = timex_obj.get_node()\n            self.node.append(timex_node)\n            self.idx[timex_id] = timex_node\n        else:\n            #FIXME: what we want is that the element receives a new identifier that\n            #is not present in current element yet\n            print('Error: trying to add new element with existing identifier')", "category": "Python"}, {"instruction": "def run_simulation(c1, c2):\n    \"\"\"\n    using character and planet, run the simulation\n    \"\"\"\n", "input": "", "output": "    print('running simulation...')\n    traits = character.CharacterCollection(character.fldr)\n    c1 = traits.generate_random_character()\n    c2 = traits.generate_random_character()\n    print(c1)\n    print(c2)\n    rules = battle.BattleRules(battle.rules_file)\n    b = battle.Battle(c1, c2, traits, rules, print_console='Yes')\n    print(b.status)", "category": "Python"}, {"instruction": "def configure_node(self, node):\n        \"\"\"Slaves need to know if they are collocated and what files have moved.\"\"\"\n", "input": "", "output": "\n        node.slaveinput['cov_master_host'] = socket.gethostname()\n        node.slaveinput['cov_master_topdir'] = self.topdir\n        node.slaveinput['cov_master_rsync_roots'] = [str(root) for root in node.nodemanager.roots]", "category": "Python"}, {"instruction": "def get_ligand_ring_selection(self,ring):\n        \"\"\"MDAnalysis atom selections of aromatic rings present in the ligand molecule.\n        Takes:\n            * ring * - index in self.ligrings dictionary\n        Output:\n            * ring_selection * - MDAnalysis Atom group\"\"\"\n", "input": "", "output": "        ring_names = \"\"\n        for atom in self.ligrings[ring]:\n            ring_names = ring_names+\" \"+str(atom)\n        ring_selection = self.topology_data.universe.ligand.select_atoms(\"name \"+ring_names)\n        return ring_selection", "category": "Python"}, {"instruction": "def load_ui_from_file(name: str):\n    \"\"\"\n    Returns a tuple from uic.loadUiType(), loading the ui file with the given name.\n    :param name:\n    :return:\n    \"\"\"\n", "input": "", "output": "    ui_file = _get_ui_qfile(name)\n    try:\n        base_type = uic.loadUiType(ui_file, from_imports=True)\n    finally:\n        ui_file.close()\n    return base_type", "category": "Python"}, {"instruction": "def make_token(cls, ephemeral_token: 'RedisEphemeralTokens') -> str:\n        \"\"\"\n        Returns a token to be used x number of times to allow a user account to access\n        certain resource.\n        \"\"\"\n", "input": "", "output": "        value = ephemeral_token.key\n        if ephemeral_token.scope:\n            value += ''.join(ephemeral_token.scope)\n\n        return get_hmac(cls.KEY_SALT + ephemeral_token.salt, value)[::2]", "category": "Python"}, {"instruction": "def visit_if(self, node, parent):\n        \"\"\"visit an If node by returning a fresh instance of it\"\"\"\n", "input": "", "output": "        newnode = nodes.If(node.lineno, node.col_offset, parent)\n        newnode.postinit(\n            self.visit(node.test, newnode),\n            [self.visit(child, newnode) for child in node.body],\n            [self.visit(child, newnode) for child in node.orelse],\n        )\n        return newnode", "category": "Python"}, {"instruction": "def get_take(self, max_take):\n        \"\"\"\n        Gets the number of items to return in a page.\n\n        :param max_take: the maximum number of items to return.\n\n        :return: the number of items to return.\n        \"\"\"\n", "input": "", "output": "        if self.take == None:\n            return max_take\n        if self.take < 0:\n            return 0\n        if self.take > max_take:\n            return max_take\n        return self.take", "category": "Python"}, {"instruction": "def uninstall(cls):\n        \"\"\"Remove the package manager from the system.\"\"\"\n", "input": "", "output": "        if os.path.exists(cls.home):\n            shutil.rmtree(cls.home)", "category": "Python"}, {"instruction": "def atlasdb_get_zonefile_by_txid( txid, con=None, path=None ):\n    \"\"\"\n    Look up a zonefile by txid\n    Returns {'zonefile_hash': ..., 'name': ..., etc.}\n    Returns None if not found\n    \"\"\"\n", "input": "", "output": "    ret = None\n    with AtlasDBOpen(con=con, path=path) as dbcon:\n\n        sql = \"SELECT * FROM zonefiles WHERE txid = ?;\"\n        args = (txid,)\n\n        cur = dbcon.cursor()\n        res = atlasdb_query_execute( cur, sql, args )\n\n        for zfinfo in res:\n            ret = {}\n            ret.update(zfinfo)\n            break\n\n    return ret", "category": "Python"}, {"instruction": "def estimate_size_in_bytes(cls, magic, compression_type, key, value):\n        \"\"\" Upper bound estimate of record size.\n        \"\"\"\n", "input": "", "output": "        assert magic in [0, 1], \"Not supported magic\"\n        # In case of compression we may need another overhead for inner msg\n        if compression_type:\n            return (\n                cls.LOG_OVERHEAD + cls.record_overhead(magic) +\n                cls.record_size(magic, key, value)\n            )\n        return cls.LOG_OVERHEAD + cls.record_size(magic, key, value)", "category": "Python"}, {"instruction": "def readList(self):\n        \"\"\"\n        Read a C{list} from the data stream.\n        \"\"\"\n", "input": "", "output": "        obj = []\n        self.context.addObject(obj)\n        l = self.stream.read_ulong()\n\n        for i in xrange(l):\n            obj.append(self.readElement())\n\n        return obj", "category": "Python"}, {"instruction": "def get_sar_info(self):\n        \"\"\"\n        Returns parsed sar info\n            :return: ``Dictionary``-style list of SAR data\n        \"\"\"\n", "input": "", "output": "\n        try:\n            test = self._sarinfo[\"CPU\"]\n            del test\n\n        except KeyError:\n            file_parsed = self.load_file()\n            if file_parsed:\n                return self._sarinfo\n            else:\n                return False\n\n        except:\n            ### DEBUG\n            traceback.print_exc()\n            return False\n\n        return self._sarinfo", "category": "Python"}, {"instruction": "def get_body_arguments(self, name: str, strip: bool = True) -> List[str]:\n        \"\"\"Returns a list of the body arguments with the given name.\n\n        If the argument is not present, returns an empty list.\n\n        .. versionadded:: 3.2\n        \"\"\"\n", "input": "", "output": "        return self._get_arguments(name, self.request.body_arguments, strip)", "category": "Python"}, {"instruction": "def get_xpath_frequencydistribution(paths):\r\n    \"\"\" Build and return a frequency distribution over xpath occurrences.\"\"\"\n", "input": "", "output": "    # \"html/body/div/div/text\" -> [ \"html\", \"body\", \"div\", \"div\", \"text\" ]\r\n    splitpaths = [p.split('/') for p in paths]\r\n\r\n    # get list of \"parentpaths\" by right-stripping off the last xpath-node,\r\n    # effectively getting the parent path\r\n    parentpaths = ['/'.join(p[:-1]) for p in splitpaths]\r\n\r\n    # build frequency distribution\r\n    parentpaths_counter = Counter(parentpaths)\r\n    return parentpaths_counter.most_common()", "category": "Python"}, {"instruction": "def top_k(x, reduced_dim, new_dim, dtype=tf.int32, name=None):\n  \"\"\"Like tf.top_k.\n\n  This operation returns two tensors with the same shape.  The output shape\n  is identical to the shape of x, except that reduced_dim is replaced by\n  new_dim.\n\n  Args:\n    x: a Tensor\n    reduced_dim: a Dimension in x.shape.dims.\n    new_dim: a Dimension.  The size determines k.\n    dtype: optional dtype for indices.\n    name: optional string.\n  Returns:\n    indices: a Tensor with given dtype.\n    values: a Tensor with same type as x.\n  \"\"\"\n", "input": "", "output": "  reduced_dim = convert_to_dimension(reduced_dim)\n  new_dim = convert_to_dimension(new_dim)\n  indices = []\n  values = []\n  k = new_dim.size\n  with tf.name_scope(name, default_name=\"top_k\"):\n    for i in xrange(k):\n      max_index, max_val = top_1(x, reduced_dim, dtype)\n      indices.append(max_index)\n      values.append(max_val)\n      if i + 1 < k:\n        x += one_hot(max_index, reduced_dim, on_value=-1e9, dtype=x.dtype)\n  axis = x.shape.dims.index(reduced_dim)\n  return stack(indices, new_dim.name, axis), stack(values, new_dim.name, axis)", "category": "Python"}, {"instruction": "def getInfos(self, CorpNum, MgtKeyList):\r\n        \"\"\" \uc0c1\ud0dc\uc815\ubcf4 \ub2e4\ub7c9 \ud655\uc778, \ucd5c\ub300 1000\uac74\r\n            args\r\n                CorpNum : \ud68c\uc6d0 \uc0ac\uc5c5\uc790 \ubc88\ud638\r\n                MgtKeyList : \ubb38\uc11c\uad00\ub9ac\ubc88\ud638 \ubaa9\ub85d\r\n            return\r\n                \uc0c1\ud0dc\uc815\ubcf4 \ubaa9\ub85d as List\r\n            raise\r\n                PopbillException\r\n        \"\"\"\n", "input": "", "output": "        if MgtKeyList == None or len(MgtKeyList) < 1:\r\n            raise PopbillException(-99999999, \"\uad00\ub9ac\ubc88\ud638\uac00 \uc785\ub825\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\")\r\n\r\n        postData = self._stringtify(MgtKeyList)\r\n\r\n        return self._httppost('/Cashbill/States', postData, CorpNum)", "category": "Python"}, {"instruction": "def _add_extra_attributes(attrs, extra_attr_map):\n    \"\"\"\n    :param attrs:  Current Attribute list\n    :param extraAttrMap:  Additional attributes to be added\n    :return: new_attr\n    \"\"\"\n", "input": "", "output": "    for attr in extra_attr_map:\n        if attr not in attrs:\n            attrs[attr] = extra_attr_map[attr]\n    return attrs", "category": "Python"}, {"instruction": "def dict_minus(d, *keys):\n    \"\"\"Delete key(s) from dict if exists, returning resulting dict\"\"\"\n", "input": "", "output": "    # make shallow copy\n    d = dict(d)\n    for key in keys:\n        try:\n            del d[key]\n        except:\n            pass\n    return d", "category": "Python"}, {"instruction": "def add_vbar_widget(self, ref, x=1, y=1, length=10):\n        \"\"\" Add Vertical Bar Widget \"\"\"\n", "input": "", "output": "\n        if ref not in self.widgets:\n            widget = widgets.VBarWidget(screen=self, ref=ref, x=x, y=y, length=length)\n            self.widgets[ref] = widget\n            return self.widgets[ref]", "category": "Python"}, {"instruction": "def members(self, as_set=False):\n        \"\"\"Return the set members tuple/frozenset.\"\"\"\n", "input": "", "output": "        if as_set:\n            return frozenset(map(self._members.__getitem__, self._indexes()))\n        return tuple(map(self._members.__getitem__, self._indexes()))", "category": "Python"}, {"instruction": "def explain_weights(self, **kwargs):\n        \"\"\"\n        Call :func:`eli5.show_weights` for the locally-fit\n        classification pipeline. Keyword arguments are passed\n        to :func:`eli5.show_weights`.\n\n        :func:`fit` must be called before using this method.\n        \"\"\"\n", "input": "", "output": "        self._fix_target_names(kwargs)\n        return eli5.explain_weights(self.clf_, vec=self.vec_, **kwargs)", "category": "Python"}, {"instruction": "def make_admin(user):\n    '''\n    Makes the given user an admin.\n    '''\n", "input": "", "output": "    tutor_group, owner_group = _get_user_groups()\n    user.is_staff = True\n    user.is_superuser = True\n    user.save()\n    owner_group.user_set.add(user)\n    owner_group.save()\n    tutor_group.user_set.add(user)\n    tutor_group.save()", "category": "Python"}, {"instruction": "def via(self, i: int=None) -> str:\n        \"\"\"\n        Returns an via message\n        \"\"\"\n", "input": "", "output": "        head = \"[\" + colors.green(\"via\") + \"]\"\n        if i is not None:\n            head = str(i) + \" \" + head\n        return head", "category": "Python"}, {"instruction": "def isasteroid(self):\n        \"\"\"`True` if `targetname` appears to be an asteroid.\"\"\"\n", "input": "", "output": "        if self.asteroid is not None:\n            return self.asteroid\n        elif self.comet is not None:\n            return not self.comet\n        else:\n            return any(self.parse_asteroid()) is not None", "category": "Python"}, {"instruction": "def ignore_whitespace_text_nodes(cls, wrapped_node):\n        \"\"\"\n        Find and delete any text nodes containing nothing but whitespace in\n        in the given node and its descendents.\n\n        This is useful for cleaning up excess low-value text nodes in a\n        document DOM after parsing a pretty-printed XML document.\n        \"\"\"\n", "input": "", "output": "        for child in wrapped_node.children:\n            if child.is_text and child.value.strip() == '':\n                child.delete()\n            else:\n                cls.ignore_whitespace_text_nodes(child)", "category": "Python"}, {"instruction": "def convert_to_american_phonetic_alphabet(self, arpabet):\n        '''\n        \u8f6c\u6362\u6210\u7f8e\u97f3\n        :param arpabet:\n        :return:\n        '''\n", "input": "", "output": "\n        word = self._convert_to_word(arpabet=arpabet)\n\n        if not word:\n            return None\n\n        return word.translate_to_american_phonetic_alphabet()", "category": "Python"}, {"instruction": "def set(cls, var_name: str, value: Any) -> 'Configuration':\n        \"\"\"\n        Set the variable\n\n        :param var_name: Variable name\n        :param value: Value of variable\n        :return: :class:`~haps.config.Configuration` instance for easy\\\n                  chaining\n        \"\"\"\n", "input": "", "output": "        with cls._lock:\n            if var_name not in cls().cache:\n                cls().cache[var_name] = value\n            else:\n                raise ConfigurationError(\n                    f'Value for {var_name} already set')\n        return cls()", "category": "Python"}, {"instruction": "def display_missing(self, data, return_bool=\"any\"):\n        \"\"\" ???\n        \n        Parameters\n        ----------\n        data            : pd.DataFrame()\n            Input dataframe.\n        return_bool     : bool\n            ???\n\n        Returns\n        -------\n        pd.DataFrame()\n            ???\n\n        \"\"\"\n", "input": "", "output": "\n        if return_bool == \"any\":\n            bool_sel = self._find_missing(data, return_bool=\"any\")\n\n        elif return_bool == \"all\":\n            bool_sel = self._find_missing(data, return_bool=\"all\")\n\n        return data[bool_sel]", "category": "Python"}, {"instruction": "def _pprint(dic):\n    \"\"\"Prints a dictionary with one indentation level\"\"\"\n", "input": "", "output": "    for key, value in dic.items():\n        print(\"        {0}: {1}\".format(key, value))", "category": "Python"}, {"instruction": "def _find_by_nsp(self, browser, criteria, tag, constraints):\r\n        \"\"\"Find element matches by  iOSNsPredicateString.\"\"\"\n", "input": "", "output": "        return self._filter_elements(\r\n            browser.find_elements_by_ios_predicate(criteria),\r\n            tag, constraints)", "category": "Python"}, {"instruction": "def get_instance(self, payload):\n        \"\"\"\n        Build an instance of NewSigningKeyInstance\n\n        :param dict payload: Payload response from the API\n\n        :returns: twilio.rest.api.v2010.account.new_signing_key.NewSigningKeyInstance\n        :rtype: twilio.rest.api.v2010.account.new_signing_key.NewSigningKeyInstance\n        \"\"\"\n", "input": "", "output": "        return NewSigningKeyInstance(self._version, payload, account_sid=self._solution['account_sid'], )", "category": "Python"}, {"instruction": "def start_ebrisk(rupgetter, srcfilter, param, monitor):\n    \"\"\"\n    Launcher for ebrisk tasks\n    \"\"\"\n", "input": "", "output": "    with monitor('weighting ruptures'):\n        rupgetter.set_weights(srcfilter, param['num_taxonomies'])\n    if rupgetter.weights.sum() <= param['maxweight']:\n        yield ebrisk(rupgetter, srcfilter, param, monitor)\n    else:\n        for rgetter in rupgetter.split(param['maxweight']):\n            yield ebrisk, rgetter, srcfilter, param", "category": "Python"}, {"instruction": "def print_start_command(self, command):\n        '''Set print command\n        \n        Args:\n            command: the type of command you desire.\n        Returns:\n            None\n        Raises:\n            RuntimeError: Command too long.\n        '''\n", "input": "", "output": "        size = len(command)\n        if size > 20:\n            raise RuntimeError('Command too long')\n        n1 = size/10\n        n2 = size%10\n        self.send('^PS'+chr(n1)+chr(n2)+command)", "category": "Python"}, {"instruction": "def clear_incident(self, id, **kwargs):\n        \"\"\"Clear an incident.\n        \"\"\"\n", "input": "", "output": "        resp = self._put(\n            self._u(self._INCIDENT_ENDPOINT_SUFFIX, id, 'clear'),\n            None,\n            **kwargs\n        )\n        resp.raise_for_status()\n        return resp", "category": "Python"}, {"instruction": "def fermi_fourier_trans_inverse_conjugate_4(qubits):\n    \"\"\"We will need to map the momentum states in the reversed order for\n    spin-down states to the position picture. This transformation can be\n    simply implemented the complex conjugate of the former one. We only\n    need to change the S gate to S* = S ** 3.\n\n    Args:\n        qubits: list of four qubits\n    \"\"\"\n", "input": "", "output": "\n    yield fswap(qubits[1], qubits[2]),\n    yield fermi_fourier_trans_2(qubits[0], qubits[1])\n    yield fermi_fourier_trans_2(qubits[2], qubits[3])\n    yield fswap(qubits[1], qubits[2])\n    yield fermi_fourier_trans_2(qubits[0], qubits[1])\n    yield cirq.S(qubits[2]) ** 3\n    yield fermi_fourier_trans_2(qubits[2], qubits[3])\n    yield fswap(qubits[1], qubits[2])", "category": "Python"}, {"instruction": "def _add_tag(self, tag):\n        # type: (str) -> bool\n        \"\"\"Add a tag\n\n        Args:\n            tag (str): Tag to add\n\n        Returns:\n            bool: True if tag added or False if tag already present\n        \"\"\"\n", "input": "", "output": "        tags = self.data.get('tags', None)\n        if tags:\n            if tag in [x['name'] for x in tags]:\n                return False\n        else:\n            tags = list()\n        tags.append({'name': tag})\n        self.data['tags'] = tags\n        return True", "category": "Python"}, {"instruction": "def _subclass_must_implement(self, fn):\n        \"\"\"\n        Returns a NotImplementedError for a function that should be implemented.\n        :param fn: name of the function\n        \"\"\"\n", "input": "", "output": "        m = \"Missing function implementation in {}: {}\".format(type(self), fn)\n        return NotImplementedError(m)", "category": "Python"}, {"instruction": "def from_json_dict(cls,\n                       json_dict  # type: Dict[str, Any]\n                       ):\n        # type: (...) -> EnumSpec\n        \"\"\" Make a EnumSpec object from a dictionary containing its\n            properties.\n\n            :param dict json_dict: This dictionary must contain an\n                `'enum'` key specifying the permitted values. In\n                addition, it must contain a `'hashing'` key, whose\n                contents are passed to :class:`FieldHashingProperties`.\n        \"\"\"\n", "input": "", "output": "        # noinspection PyCompatibility\n        result = cast(EnumSpec,  # Appease the gods of Mypy.\n                      super().from_json_dict(json_dict))\n\n        format_ = json_dict['format']\n        result.values = set(format_['values'])\n\n        return result", "category": "Python"}, {"instruction": "def amplitude_by_fft(self, data_frame):\n        \"\"\"\n            This methods extract the fft components and sum the ones from lower to upper freq as per \\\n            :cite:`Kassavetis2015`\n\n            :param data_frame: the data frame\n            :type data_frame: pandas.DataFrame\n            :return ampl: the ampl\n            :rtype ampl: float\n            :return freq: the freq\n            :rtype freq: float\n        \"\"\"\n", "input": "", "output": "        signal_length = len(data_frame.filtered_signal)\n        normalised_transformed_signal = data_frame.transformed_signal.values / signal_length\n\n        k = np.arange(signal_length)\n        T = signal_length / self.sampling_frequency\n        f = k / T  # two sides frequency range\n\n        f = f[range(int(signal_length / 2))]  # one side frequency range\n        ts = normalised_transformed_signal[range(int(signal_length / 2))]\n        ampl = sum(abs(ts[(f > self.lower_frequency) & (f < self.upper_frequency)]))\n        freq = f[abs(ts).argmax(axis=0)]\n\n        logging.debug(\"tremor ampl calculated\")\n\n        return ampl, freq", "category": "Python"}, {"instruction": "def _bits_ports_and_isrom_from_memory(mem):\n    \"\"\" Helper to extract mem bits and ports for estimation. \"\"\"\n", "input": "", "output": "    is_rom = False\n    bits = 2**mem.addrwidth * mem.bitwidth\n    read_ports = len(mem.readport_nets)\n    try:\n        write_ports = len(mem.writeport_nets)\n    except AttributeError:  # dealing with ROMs\n        if not isinstance(mem, RomBlock):\n            raise PyrtlInternalError('Mem with no writeport_nets attribute'\n                                     ' but not a ROM? Thats an error')\n        write_ports = 0\n        is_rom = True\n    ports = max(read_ports, write_ports)\n    return bits, ports, is_rom", "category": "Python"}, {"instruction": "def configure_logging(level=logging.DEBUG):\n    '''Configures the root logger for command line applications.\n\n    A stream handler will be added to the logger that directs\n    messages to the standard error stream.\n\n    By default, *no* messages will be filtered out: set a higher\n    level on derived/child loggers to achieve filtering.\n\n    Warning\n    -------\n    Logging should only be configured once at the main entry point of the\n    application!\n    '''\n", "input": "", "output": "    fmt = '%(asctime)s | %(levelname)-8s | %(name)-40s | %(message)s'\n    datefmt = '%Y-%m-%d %H:%M:%S'\n    formatter = logging.Formatter(fmt=fmt, datefmt=datefmt)\n\n    logger = logging.getLogger()  # returns the root logger\n\n    stderr_handler = logging.StreamHandler(stream=sys.stderr)\n    stderr_handler.name = 'err'\n    stderr_handler.setLevel(level)\n    stderr_handler.setFormatter(formatter)\n    logger.addHandler(stderr_handler)", "category": "Python"}, {"instruction": "def is_docstring(self, node):\n        \"\"\"Determine if the given node is a docstring, as long as it is at the\n        correct place in the node tree.\"\"\"\n", "input": "", "output": "        return isinstance(node, ast.Str) or (isinstance(node, ast.Expr) and\n                                             isinstance(node.value, ast.Str))", "category": "Python"}, {"instruction": "def read_padding(fp, size, divisor=2):\n    \"\"\"\n    Read padding bytes for the given byte size.\n\n    :param fp: file-like object\n    :param divisor: divisor of the byte alignment\n    :return: read byte size\n    \"\"\"\n", "input": "", "output": "    remainder = size % divisor\n    if remainder:\n        return fp.read(divisor - remainder)\n    return b''", "category": "Python"}, {"instruction": "def get_idd_code(self, ip):\n        ''' Get idd_code '''\n", "input": "", "output": "        rec = self.get_all(ip)\n        return rec and rec.idd_code", "category": "Python"}, {"instruction": "def strip_head(sequence, values):\n    \"\"\"Strips elements of `values` from the beginning of `sequence`.\"\"\"\n", "input": "", "output": "    values = set(values)\n    return list(itertools.dropwhile(lambda x: x in values, sequence))", "category": "Python"}, {"instruction": "def no_selenium_errors(func):\n    \"\"\"\n    Decorator to create an `EmptyPromise` check function that is satisfied\n    only when `func` executes without a Selenium error.\n\n    This protects against many common test failures due to timing issues.\n    For example, accessing an element after it has been modified by JavaScript\n    ordinarily results in a `StaleElementException`.  Methods decorated\n    with `no_selenium_errors` will simply retry if that happens, which makes tests\n    more robust.\n\n    Args:\n        func (callable): The function to execute, with retries if an error occurs.\n\n    Returns:\n        Decorated function\n    \"\"\"\n", "input": "", "output": "    def _inner(*args, **kwargs):  # pylint: disable=missing-docstring\n        try:\n            return_val = func(*args, **kwargs)\n        except WebDriverException:\n            LOGGER.warning(u'Exception ignored during retry loop:', exc_info=True)\n            return False\n        else:\n            return return_val\n\n    return _inner", "category": "Python"}, {"instruction": "def compute_control_digit(clabe: str) -> str:\n    \"\"\"\n    Compute CLABE control digit according to\n    https://es.wikipedia.org/wiki/CLABE#D.C3.ADgito_control\n    \"\"\"\n", "input": "", "output": "    clabe = [int(i) for i in clabe]\n    weighted = [c * w % 10 for c, w in\n                zip(clabe[:CLABE_LENGTH - 1], CLABE_WEIGHTS)]\n    summed = sum(weighted) % 10\n    control_digit = (10 - summed) % 10\n    return str(control_digit)", "category": "Python"}, {"instruction": "def amax(data, axis=None, mapper=None, blen=None, storage=None,\n         create='array', **kwargs):\n    \"\"\"Compute the maximum value.\"\"\"\n", "input": "", "output": "    return reduce_axis(data, axis=axis, reducer=np.amax,\n                       block_reducer=np.maximum, mapper=mapper,\n                       blen=blen, storage=storage, create=create, **kwargs)", "category": "Python"}, {"instruction": "def delete_stripe_gateway_by_id(cls, stripe_gateway_id, **kwargs):\n        \"\"\"Delete StripeGateway\n\n        Delete an instance of StripeGateway by its ID.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.delete_stripe_gateway_by_id(stripe_gateway_id, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str stripe_gateway_id: ID of stripeGateway to delete. (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._delete_stripe_gateway_by_id_with_http_info(stripe_gateway_id, **kwargs)\n        else:\n            (data) = cls._delete_stripe_gateway_by_id_with_http_info(stripe_gateway_id, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def cleanup(pin=None, assert_exists=False):\r\n    \"\"\"Cleanup the pin by closing and unexporting it.\r\n\r\n    Args:\r\n        pin (int, optional): either the pin to clean up or None (default).\r\n            If None, clean up all pins.\r\n        assert_exists: if True, raise a ValueError if the pin was not\r\n            setup. Otherwise, this function is a NOOP.\r\n    \"\"\"\n", "input": "", "output": "    if pin is None:\r\n        # Take a list of keys because we will be deleting from _open\r\n        for pin in list(_open):\r\n            cleanup(pin)\r\n        return\r\n    if not isinstance(pin, int):\r\n        raise TypeError(\"pin must be an int, got: {}\".format(pin))\r\n\r\n    state = _open.get(pin)\r\n    if state is None:\r\n        if assert_exists:\r\n            raise ValueError(\"pin {} was not setup\".format(pin))\r\n        return\r\n    state.value.close()\r\n    state.direction.close()\r\n    if os.path.exists(gpiopath(pin)):\r\n        log.debug(\"Unexporting pin {0}\".format(pin))\r\n        with _export_lock:\r\n            with open(pjoin(gpio_root, 'unexport'), 'w') as f:\r\n                _write(f, pin)\r\n\r\n    del _open[pin]", "category": "Python"}, {"instruction": "def verticalfreq(Pot,R):\n    \"\"\"\n    \n    NAME:\n    \n       verticalfreq\n        \n    PURPOSE:\n    \n        calculate the vertical frequency at R in the potential Pot\n    \n    INPUT:\n\n       Pot - Potential instance or list thereof\n    \n       R - Galactocentric radius (can be Quantity)\n    \n    OUTPUT:\n    \n        vertical frequency\n    \n    HISTORY:\n    \n        2012-07-25 - Written - Bovy (IAS@MPIA)\n    \n    \"\"\"\n", "input": "", "output": "    from .planarPotential import planarPotential\n    if isinstance(Pot,(Potential,planarPotential)):\n        return Pot.verticalfreq(R,use_physical=False)\n    return nu.sqrt(evaluatez2derivs(Pot,R,0.,use_physical=False))", "category": "Python"}, {"instruction": "def _translate(self, options):\n        \"\"\"\n        Perform translation of feed options passed in as keyword\n        arguments to CouchDB/Cloudant equivalent.\n        \"\"\"\n", "input": "", "output": "        translation = dict()\n        for key, val in iteritems_(options):\n            self._validate(key, val, feed_arg_types(self._source))\n            try:\n                if isinstance(val, STRTYPE):\n                    translation[key] = val\n                elif not isinstance(val, NONETYPE):\n                    arg_converter = TYPE_CONVERTERS.get(type(val), json.dumps)\n                    translation[key] = arg_converter(val)\n            except Exception as ex:\n                raise CloudantArgumentError(115, key, ex)\n        return translation", "category": "Python"}, {"instruction": "def _rgetattr(obj, key):\n    \"\"\"Recursive getattr for handling dots in keys.\"\"\"\n", "input": "", "output": "    for k in key.split(\".\"):\n        obj = getattr(obj, k)\n    return obj", "category": "Python"}, {"instruction": "def key_dict( from_dict ):\n\t\"\"\"Returns dict from_dict['unicode_save_field'] = 'original key with unicode' \"\"\"\n", "input": "", "output": "\tnew_dict = {}\n\told2new = {}\n\tnew2old = {}\n\tfor key in from_dict:\n\t\tk = normalizeUnicode(key,'identifier')\n\t\tif k != key:\n\t\t\ti = ''\n\t\t\twhile new_dict.has_key(\"%s%s\" % (k,i) ):\n\t\t\t\tif not i:\n\t\t\t\t\ti = 1\n\t\t\t\telse:\n\t\t\t\t\ti += 1\n\t\t\tk = \"%s%s\" % (k,i)\n\t\t\told2new[key] = k\n\t\t\tnew2old[k] = key\n\t\tnew_dict[k] = from_dict[key]\n\treturn (new_dict.keys(), new_dict, old2new, new2old)", "category": "Python"}, {"instruction": "def start(label, at=None):\n    \"\"\"Begins the countdown\"\"\"\n", "input": "", "output": "    t = at if at is not None else time.time()\n    marker = Marker().start(t)\n    labels[label] = marker.dumps()", "category": "Python"}, {"instruction": "def set_auto_commit(self, auto_commit):\n        \"\"\"Sets auto-commit mode for this connection.\n\n        If a connection is in auto-commit mode, then all its SQL statements will be executed and committed as individual\n        transactions. Otherwise, its SQL statements are grouped into transactions that are terminated by a call to\n        either the method commit or the method rollback. By default, new connections are in auto-commit mode.\n\n        NOTE: If this method is called during a transaction and the auto-commit mode is changed, the transaction is\n        committed. If this method is called and the auto-commit mode is not changed, the call is a no-op.\n\n        :param auto_commit: `True` to enable auto-commit mode; `False` to disable it\n        \"\"\"\n", "input": "", "output": "        auto_commit = bool(auto_commit)\n        if auto_commit == self._auto_commit:\n            return\n\n        self._auto_commit = auto_commit\n\n        if self.is_connected() and self._transaction_nesting_level != 0:\n            self.commit_all()", "category": "Python"}, {"instruction": "def render_xml(result, cfg, **kwargs):\n    \"\"\"\n    Render to output a result in XML format\n    \"\"\"\n", "input": "", "output": "    # Raw mode\n    if cfg.dis == 'raw':\n        return {'data': {'text/plain': result.decode('utf-8')},\n                'metadata': {}}\n    # Table\n    try:\n        import xml.etree.cElementTree as ET\n    except ImportError:\n        import xml.etree.ElementTree as ET\n    root = ET.fromstring(result)\n    try:\n        ns = {'ns': re.match(r'\\{([^}]+)\\}', root.tag).group(1)}\n    except Exception:\n        raise KrnlException('Invalid XML data: cannot get namespace')\n    columns = [c.attrib['name'] for c in root.find('ns:head', ns)]\n    results = root.find('ns:results', ns)\n    nrow = len(results)\n    j = xml_iterator(columns, results, set(cfg.lan), add_vtype=cfg.typ)\n    n, data = html_table(j, limit=cfg.lmt, withtype=cfg.typ)\n    data += div('Total: {}, Shown: {}', nrow, n, css=\"tinfo\")\n    return {'data': {'text/html': div(data)},\n            'metadata': {}}", "category": "Python"}, {"instruction": "def data(self, *args):\n        '''Add or retrieve data values for this :class:`Html`.'''\n", "input": "", "output": "        data = self._data\n        if not args:\n            return data or {}\n        result, adding = self._attrdata('data', *args)\n        if adding:\n            if data is None:\n                self._extra['data'] = {}\n            add = self._visitor.add_data\n            for key, value in result.items():\n                add(self, key, value)\n            return self\n        else:\n            return result", "category": "Python"}, {"instruction": "def get_stop_words(self, language, fail_safe=False):\n        \"\"\"\n        Returns a StopWord object initialized with the stop words collection\n        requested by ``language``.\n        If the requested language is not available a StopWordError is raised.\n        If ``fail_safe`` is set to True, an empty StopWord object is returned.\n        \"\"\"\n", "input": "", "output": "        try:\n            language = self.language_codes[language]\n        except KeyError:\n            pass\n\n        collection = self.LOADED_LANGUAGES_CACHE.get(language)\n\n        if collection is None:\n            try:\n                collection = self._get_stop_words(language)\n                self.LOADED_LANGUAGES_CACHE[language] = collection\n            except StopWordError as error:\n                if not fail_safe:\n                    raise error\n                collection = []\n\n        stop_words = StopWord(language, collection)\n        return stop_words", "category": "Python"}, {"instruction": "def _one_iteration(self, F, Ybus, V, Vm, Va, pv, pq, pvpq):\n        \"\"\" Performs one Newton iteration.\n        \"\"\"\n", "input": "", "output": "        J = self._build_jacobian(Ybus, V, pv, pq, pvpq)\n\n        # Update step.\n        dx = -1 * spsolve(J, F)\n#        dx = -1 * linalg.lstsq(J.todense(), F)[0]\n\n        # Update voltage vector.\n        npv = len(pv)\n        npq = len(pq)\n        if npv > 0:\n            Va[pv] = Va[pv] + dx[range(npv)]\n        if npq > 0:\n            Va[pq] = Va[pq] + dx[range(npv, npv + npq)]\n            Vm[pq] = Vm[pq] + dx[range(npv + npq, npv + npq + npq)]\n\n        V = Vm * exp(1j * Va)\n        Vm = abs(V) # Avoid wrapped round negative Vm.\n        Va = angle(V)\n\n        return V, Vm, Va", "category": "Python"}, {"instruction": "def resolve(obj, attr, fallback=None):\n    \"\"\"\n    Resolves obj.attr to a value, calling it as a function if necessary.\n    :param obj:\n    :param attr: a string name of a property or function\n    :param fallback: the value to return if none can be resolved\n    :return: the result of the attribute, or fallback if object/attr not found\n    \"\"\"\n", "input": "", "output": "    if obj is None:\n        return fallback\n    value = getattr(obj, attr, fallback)\n    if callable(value):\n        return value()\n    return value", "category": "Python"}, {"instruction": "def hash_file(file_obj,\n              hash_function=hashlib.md5):\n    \"\"\"\n    Get the hash of an open file- like object.\n\n    Parameters\n    ---------\n    file_obj: file like object\n    hash_function: function to use to hash data\n\n    Returns\n    ---------\n    hashed: str, hex version of result\n    \"\"\"\n", "input": "", "output": "    # before we read the file data save the current position\n    # in the file (which is probably 0)\n    file_position = file_obj.tell()\n    # create an instance of the hash object\n    hasher = hash_function()\n    # read all data from the file into the hasher\n    hasher.update(file_obj.read())\n    # get a hex version of the result\n    hashed = hasher.hexdigest()\n    # return the file object to its original position\n    file_obj.seek(file_position)\n\n    return hashed", "category": "Python"}, {"instruction": "def skip_internal(app, what, name, obj, skip, options):\n    \"\"\"Skip rendering autodoc when the docstring contains a line with\n    only the string `:internal:`.\n    \"\"\"\n", "input": "", "output": "    docstring = inspect.getdoc(obj) or \"\"\n\n    if skip or re.search(r\"^\\s*:internal:\\s*$\", docstring, re.M) is not None:\n        return True", "category": "Python"}, {"instruction": "def send(self, data):\n        \"\"\"Send data through websocket\"\"\"\n", "input": "", "output": "        log.debug('Sending %s' % data)\n        if not self._socket:\n            log.warn('No connection')\n            return\n        self._socket.send_bytes(data.encode('utf-8'))", "category": "Python"}, {"instruction": "def set_file_position(body, pos):\n    \"\"\"\n    If a position is provided, move file to that point.\n    Otherwise, we'll attempt to record a position for future use.\n    \"\"\"\n", "input": "", "output": "    if pos is not None:\n        rewind_body(body, pos)\n    elif getattr(body, 'tell', None) is not None:\n        try:\n            pos = body.tell()\n        except (IOError, OSError):\n            # This differentiates from None, allowing us to catch\n            # a failed `tell()` later when trying to rewind the body.\n            pos = _FAILEDTELL\n\n    return pos", "category": "Python"}, {"instruction": "def process_from_web():\n    \"\"\"Return a TrrustProcessor based on the online interaction table.\n\n    Returns\n    -------\n    TrrustProcessor\n        A TrrustProcessor object that has a list of INDRA Statements in its\n        statements attribute.\n    \"\"\"\n", "input": "", "output": "    logger.info('Downloading table from %s' % trrust_human_url)\n    res = requests.get(trrust_human_url)\n    res.raise_for_status()\n    df = pandas.read_table(io.StringIO(res.text))\n    tp = TrrustProcessor(df)\n    tp.extract_statements()\n    return tp", "category": "Python"}, {"instruction": "def is_packet_type(cls):\n    \"\"\"Check if class is one the packet types.\"\"\"\n", "input": "", "output": "    from .packet_types import EddystoneUIDFrame, EddystoneURLFrame, \\\n                              EddystoneEncryptedTLMFrame, EddystoneTLMFrame, \\\n                              EddystoneEIDFrame, IBeaconAdvertisement, \\\n                              EstimoteTelemetryFrameA, EstimoteTelemetryFrameB\n    return (cls in [EddystoneURLFrame, EddystoneUIDFrame, EddystoneEncryptedTLMFrame, \\\n                    EddystoneTLMFrame, EddystoneEIDFrame, IBeaconAdvertisement, \\\n                    EstimoteTelemetryFrameA, EstimoteTelemetryFrameB])", "category": "Python"}, {"instruction": "def register_as_type(self, locator, object_factory):\n        \"\"\"\n        Registers a component using its type (a constructor function).\n\n        :param locator: a locator to identify component to be created.\n\n        :param object_factory: a component type.\n        \"\"\"\n", "input": "", "output": "        if locator == None:\n            raise Exception(\"Locator cannot be null\")\n        if object_factory == None:\n            raise Exception(\"Factory cannot be null\")\n\n        def factory(locator):\n            return object_factory()\n\n        self._registrations.append(Registration(locator, factory))", "category": "Python"}, {"instruction": "def get_single_review_comments(self, id):\n        \"\"\"\n        :calls: `GET /repos/:owner/:repo/pulls/:number/review/:id/comments <https://developer.github.com/v3/pulls/reviews/>`_\n        :param id: integer\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.PullRequestComment.PullRequestComment`\n        \"\"\"\n", "input": "", "output": "        assert isinstance(id, (int, long)), id\n        return github.PaginatedList.PaginatedList(\n            github.PullRequestComment.PullRequestComment,\n            self._requester,\n            self.url + \"/reviews/\" + str(id) + \"/comments\",\n            None\n        )", "category": "Python"}, {"instruction": "def apply_config_to_machine_group(self, project_name, config_name, group_name):\r\n        \"\"\" apply a logtail config to a machine group\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :type config_name: string\r\n        :param config_name: the logtail config name to apply\r\n        \r\n        :type group_name: string\r\n        :param group_name: the machine group name \r\n\r\n        :return: ApplyConfigToMachineGroupResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\n", "input": "", "output": "        headers = {}\r\n        params = {}\r\n        resource = \"/machinegroups/\" + group_name + \"/configs/\" + config_name\r\n        (resp, header) = self._send(\"PUT\", project_name, None, resource, params, headers)\r\n        return ApplyConfigToMachineGroupResponse(header, resp)", "category": "Python"}, {"instruction": "def from_json(cls, data):\n        \"\"\"Create a Data Collection from a dictionary.\n\n        Args:\n            {\n                \"header\": A Ladybug Header,\n                \"values\": An array of values,\n                \"datetimes\": An array of datetimes,\n                \"validated_a_period\": Boolean for whether header analysis_period is valid\n            }\n        \"\"\"\n", "input": "", "output": "        assert 'header' in data, 'Required keyword \"header\" is missing!'\n        assert 'values' in data, 'Required keyword \"values\" is missing!'\n        assert 'datetimes' in data, 'Required keyword \"datetimes\" is missing!'\n        collection = cls(Header.from_json(data['header']), data['values'],\n                         [DateTime.from_json(dat) for dat in data['datetimes']])\n        if 'validated_a_period' in data:\n            collection._validated_a_period = data['validated_a_period']\n        return collection", "category": "Python"}, {"instruction": "def discrete(self, vertices, scale=1.0):\n        \"\"\"\n        Discretize the arc entity into line sections.\n\n        Parameters\n        ------------\n        vertices : (n, dimension) float\n            Points in space\n        scale : float\n            Size of overall scene for numerical comparisons\n\n        Returns\n        -------------\n        discrete: (m, dimension) float, linear path in space\n        \"\"\"\n", "input": "", "output": "        discrete = discretize_arc(vertices[self.points],\n                                  close=self.closed,\n                                  scale=scale)\n        return self._orient(discrete)", "category": "Python"}, {"instruction": "def add_password_arg(cmd, psw, ___required=False):\n    \"\"\"Append password switch to commandline.\n    \"\"\"\n", "input": "", "output": "    if UNRAR_TOOL == ALT_TOOL:\n        return\n    if psw is not None:\n        cmd.append('-p' + psw)\n    else:\n        cmd.append('-p-')", "category": "Python"}, {"instruction": "def get_category_metrics(self, category):\n        \"\"\"Get metrics belonging to the given category\"\"\"\n", "input": "", "output": "        slug_list = self._category_slugs(category)\n        return self.get_metrics(slug_list)", "category": "Python"}, {"instruction": "def process_gps_position(self, helper, sess):\n        \"\"\"\n        just print the current GPS position\n        \"\"\"\n", "input": "", "output": "\n        gps_position = helper.get_snmp_value(sess, helper, self.oids['oid_gps_position'])\n\n        if gps_position:\n            helper.add_summary(gps_position)\n        else:\n            helper.add_summary(\"Could not retrieve GPS position\")\n            helper.status(unknown)", "category": "Python"}, {"instruction": "def load_commands(self, obj):\n        \"\"\"\n        Load commands defined on an arbitrary object.\n\n        All functions decorated with the :func:`subparse.command` decorator\n        attached the specified object will be loaded. The object may\n        be a dictionary, an arbitrary python object, or a dotted path.\n\n        The dotted path may be absolute, or relative to the current package\n        by specifying a leading '.' (e.g. ``'.commands'``).\n\n        \"\"\"\n", "input": "", "output": "        if isinstance(obj, str):\n            if obj.startswith('.') or obj.startswith(':'):\n                package = caller_package()\n                if obj in ['.', ':']:\n                    obj = package.__name__\n                else:\n                    obj = package.__name__ + obj\n            obj = pkg_resources.EntryPoint.parse('x=%s' % obj).resolve()\n        command.discover_and_call(obj, self.command)", "category": "Python"}, {"instruction": "def parse(cls, data):\n        \"\"\"\n        Parse a Config structure out of a Python dict (that's likely deserialized from YAML).\n\n        :param data: Config-y dict\n        :type data: dict\n        :return: Config object\n        :rtype: valohai_yaml.objs.Config\n        \"\"\"\n", "input": "", "output": "        parsers = {\n            'step': ([], Step.parse),\n            'endpoint': ([], Endpoint.parse),\n        }\n        for datum in data:\n            assert isinstance(datum, dict)\n            for type, (items, parse) in parsers.items():\n                if type in datum:\n                    items.append(parse(datum[type]))\n                    break\n            else:\n                raise ValueError('No parser for {0}'.format(datum))\n        inst = cls(\n            steps=parsers['step'][0],\n            endpoints=parsers['endpoint'][0],\n        )\n        inst._original_data = data\n        return inst", "category": "Python"}, {"instruction": "def addSwitch(self, name=None):\n        '''\n        Add a new switch to the topology.\n        '''\n", "input": "", "output": "        if name is None:\n            while True:\n                name = 's' + str(self.__snum)\n                self.__snum += 1\n                if name not in self.__nxgraph:\n                    break\n        self.__addNode(name, Switch)\n        return name", "category": "Python"}, {"instruction": "def delete_meta_features(self, path):\n        \"\"\"Deletes meta-features of base learner if it exists\n\n        Args:\n            path (str): Absolute/local path of xcessiv folder\n        \"\"\"\n", "input": "", "output": "        if os.path.exists(self.meta_features_path(path)):\n            os.remove(self.meta_features_path(path))", "category": "Python"}, {"instruction": "def set_or_clear_breakpoint(self):\r\n        \"\"\"Set/Clear breakpoint\"\"\"\n", "input": "", "output": "        editorstack = self.get_current_editorstack()\r\n        if editorstack is not None:\r\n            self.switch_to_plugin()\r\n            editorstack.set_or_clear_breakpoint()", "category": "Python"}, {"instruction": "def md5_8_name(self, url):\n        \"\"\" \u628a\u4e0b\u8f7d\u7684\u6587\u4ef6\u91cd\u547d\u540d\u4e3a\u5730\u5740\u7684md5\u524d8\u4f4d \"\"\"\n", "input": "", "output": "        m = hashlib.md5()\n        m.update(url.encode('utf-8'))\n        return m.hexdigest()[:8] + os.path.splitext(url)[1]", "category": "Python"}, {"instruction": "def set_flow_node_ref_list(self, value):\n        \"\"\"\n        Setter for 'flow_node_ref' field.\n        :param value - a new value of 'flow_node_ref' field. Must be a list of String objects (ID of referenced nodes).\n        \"\"\"\n", "input": "", "output": "        if value is None or not isinstance(value, list):\n            raise TypeError(\"FlowNodeRefList new value must be a list\")\n        else:\n            for element in value:\n                if not isinstance(element, str):\n                    raise TypeError(\"FlowNodeRefList elements in variable must be of String class\")\n            self.__flow_node_ref_list = value", "category": "Python"}, {"instruction": "def _init_cycle_dict(self):\n        \"\"\" Populate a cycle dict \"\"\"\n", "input": "", "output": "        dict_arr = np.zeros(self.epochs, dtype=int)\n        length_arr = np.zeros(self.epochs, dtype=int)\n        start_arr = np.zeros(self.epochs, dtype=int)\n\n        c_len = self.cycle_len\n        idx = 0\n\n        for i in range(self.cycles):\n            current_start = idx\n            for j in range(c_len):\n                dict_arr[idx] = i\n                length_arr[idx] = c_len\n                start_arr[idx] = current_start\n                idx += 1\n\n            c_len *= self.cycle_mult\n\n        return dict_arr, length_arr, start_arr", "category": "Python"}, {"instruction": "def get(self, user_id, client_id, type, fields=None, include_fields=True):\n        \"\"\"List device credentials.\n\n        Args:\n            user_id (str): The user_id of the devices to retrieve.\n\n            client_id (str): The client_id of the devices to retrieve.\n\n            type (str): The type of credentials (public_key, refresh_token).\n\n            fields (list, optional): A list of fields to include or exclude\n                (depending on include_fields) from the result, empty to\n                retrieve all fields\n\n            include_fields (bool, optional): True if the fields specified are\n                to be included in the result, False otherwise\n                (defaults to true)\n\n\n        See: https://auth0.com/docs/api/management/v2#!/Device_Credentials/get_device_credentials\n        \"\"\"\n", "input": "", "output": "\n        params = {\n            'fields': fields and ','.join(fields) or None,\n            'include_fields': str(include_fields).lower(),\n            'user_id': user_id,\n            'client_id': client_id,\n            'type': type,\n        }\n        return self.client.get(self._url(), params=params)", "category": "Python"}, {"instruction": "def within_range(df, items=None):\n    \"\"\"\n    Assert that a DataFrame is within a range.\n\n    Parameters\n    ==========\n    df : DataFame\n    items : dict\n      mapping of columns (k) to a (low, high) tuple (v)\n      that ``df[k]`` is expected to be between.\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n", "input": "", "output": "    for k, (lower, upper) in items.items():\n        if (lower > df[k]).any() or (upper < df[k]).any():\n            bad = (lower > df[k]) | (upper < df[k])\n            raise AssertionError(\"Outside range\", bad)\n    return df", "category": "Python"}, {"instruction": "def update_translations(self, project_id, file_path=None,\n                            language_code=None, overwrite=False, fuzzy_trigger=None):\n        \"\"\"\n        Updates translations\n\n        overwrite: set it to True if you want to overwrite definitions\n        fuzzy_trigger: set it to True to mark corresponding translations from the\n            other languages as fuzzy for the updated values\n        \"\"\"\n", "input": "", "output": "        return self._upload(\n            project_id=project_id,\n            updating=self.UPDATING_TRANSLATIONS,\n            file_path=file_path,\n            language_code=language_code,\n            overwrite=overwrite,\n            fuzzy_trigger=fuzzy_trigger\n        )", "category": "Python"}, {"instruction": "def render_impl(template_file, ctx=None, paths=None, filters=None):\n    \"\"\"\n    :param template_file: Absolute or relative path to the template file\n    :param ctx: Context dict needed to instantiate templates\n    :param filters: Custom filters to add into template engine\n    :return: Compiled result (str)\n    \"\"\"\n", "input": "", "output": "    env = tmpl_env(make_template_paths(template_file, paths))\n\n    if env is None:\n        return copen(template_file).read()\n\n    if filters is not None:\n        env.filters.update(filters)\n\n    if ctx is None:\n        ctx = {}\n\n    return env.get_template(os.path.basename(template_file)).render(**ctx)", "category": "Python"}, {"instruction": "def remove(self, song):\n        \"\"\"Remove song from playlist. O(n)\n\n        If song is current song, remove the song and play next. Otherwise,\n        just remove it.\n        \"\"\"\n", "input": "", "output": "        if song in self._songs:\n            if self._current_song is None:\n                self._songs.remove(song)\n            elif song == self._current_song:\n                next_song = self.next_song\n                # \u968f\u673a\u6a21\u5f0f\u4e0b\u6216\u8005\u6b4c\u5355\u53ea\u5269\u4e00\u9996\u6b4c\u66f2\uff0c\u4e0b\u4e00\u9996\u53ef\u80fd\u548c\u5f53\u524d\u6b4c\u66f2\u76f8\u540c\n                if next_song == self.current_song:\n                    self.current_song = None\n                    self._songs.remove(song)\n                    self.current_song = self.next_song\n                else:\n                    self.current_song = self.next_song\n                    self._songs.remove(song)\n            else:\n                self._songs.remove(song)\n            logger.debug('Remove {} from player playlist'.format(song))\n        else:\n            logger.debug('Remove failed: {} not in playlist'.format(song))\n\n        if song in self._bad_songs:\n            self._bad_songs.remove(song)", "category": "Python"}, {"instruction": "def set_path(self, file_path):\n        \"\"\"\n        Set the path of the database.\n        Create the file if it does not exist.\n        \"\"\"\n", "input": "", "output": "        if not file_path:\n            self.read_data = self.memory_read\n            self.write_data = self.memory_write\n        elif not is_valid(file_path):\n            self.write_data(file_path, {})\n\n        self.path = file_path", "category": "Python"}, {"instruction": "def pvwatts_dc(self, g_poa_effective, temp_cell):\n        \"\"\"\n        Calcuates DC power according to the PVWatts model using\n        :py:func:`pvwatts_dc`, `self.module_parameters['pdc0']`, and\n        `self.module_parameters['gamma_pdc']`.\n\n        See :py:func:`pvwatts_dc` for details.\n        \"\"\"\n", "input": "", "output": "        kwargs = _build_kwargs(['temp_ref'], self.module_parameters)\n\n        return pvwatts_dc(g_poa_effective, temp_cell,\n                          self.module_parameters['pdc0'],\n                          self.module_parameters['gamma_pdc'],\n                          **kwargs)", "category": "Python"}, {"instruction": "async def async_enqueue_download(self, resource):\n        '''\n        Enqueue the download of the given foreign resource.\n        '''\n", "input": "", "output": "        worker = self.pick_sticky(resource.url_string)\n        await worker.enqueue(enums.Task.DOWNLOAD, (resource,))", "category": "Python"}, {"instruction": "def get_default_classes(self):\n        \"\"\"Returns a flattened string of the cell's CSS classes.\"\"\"\n", "input": "", "output": "        if not self.url:\n            self.column.classes = [cls for cls in self.column.classes\n                                   if cls != \"anchor\"]\n        column_class_string = self.column.get_final_attrs().get('class', \"\")\n        classes = set(column_class_string.split(\" \"))\n        if self.column.status:\n            classes.add(self.get_status_class(self.status))\n\n        if self.inline_edit_available:\n            classes.add(\"inline_edit_available\")\n\n        return list(classes)", "category": "Python"}, {"instruction": "def getline(self, lnum=None):\n        \"\"\"Get a line from the current buffer.\n\n        Args:\n            lnum (Optional[str]): Number of the line to get, current if ``None``.\n\n        Todo:\n            - Give this more behavior of Vim ``getline()``?\n            - ``buffer[index]`` is zero-based, this is probably too confusing\n        \"\"\"\n", "input": "", "output": "        return self._vim.current.buffer[lnum] if lnum else self._vim.current.line", "category": "Python"}, {"instruction": "def get_ops(self, ops):\n        '''Return timings for dictionary ops holding the operation names as\n        keys and the number of applications as values.'''\n", "input": "", "output": "        time = 0.\n        for op, count in ops.items():\n            time += self.get(op) * count\n        return time", "category": "Python"}, {"instruction": "def chisquare(n_ij, weighted):\n    \"\"\"\n    Calculates the chisquare for a matrix of ind_v x dep_v\n    for the unweighted and SPSS weighted case\n    \"\"\"\n", "input": "", "output": "    if weighted:\n        m_ij = n_ij / n_ij\n\n        nan_mask = np.isnan(m_ij)\n        m_ij[nan_mask] = 0.000001  # otherwise it breaks the chi-squared test\n\n        w_ij = m_ij\n        n_ij_col_sum = n_ij.sum(axis=1)\n        n_ij_row_sum = n_ij.sum(axis=0)\n        alpha, beta, eps = (1, 1, 1)\n        while eps > 10e-6:\n            alpha = alpha * np.vstack(n_ij_col_sum / m_ij.sum(axis=1))\n            beta = n_ij_row_sum / (alpha * w_ij).sum(axis=0)\n            eps = np.max(np.absolute(w_ij * alpha * beta - m_ij))\n            m_ij = w_ij * alpha * beta\n\n    else:\n        m_ij = (np.vstack(n_ij.sum(axis=1)) * n_ij.sum(axis=0)) / n_ij.sum().astype(float)\n\n    dof = (n_ij.shape[0] - 1) * (n_ij.shape[1] - 1)\n    chi, p_val = stats.chisquare(n_ij, f_exp=m_ij, ddof=n_ij.size - 1 - dof, axis=None)\n\n    return (chi, p_val, dof)", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"Sends data to the callback functions.\"\"\"\n", "input": "", "output": "        while True:\n            result = json.loads(self.ws.recv())\n            if result[\"cmd\"] == \"chat\" and not result[\"nick\"] == self.nick:\n                for handler in list(self.on_message):\n                    handler(self, result[\"text\"], result[\"nick\"])\n            elif result[\"cmd\"] == \"onlineAdd\":\n                self.online_users.append(result[\"nick\"])\n                for handler in list(self.on_join):\n                    handler(self, result[\"nick\"])\n            elif result[\"cmd\"] == \"onlineRemove\":\n                self.online_users.remove(result[\"nick\"])\n                for handler in list(self.on_leave):\n                    handler(self, result[\"nick\"])\n            elif result[\"cmd\"] == \"onlineSet\":\n                for nick in result[\"nicks\"]:\n                    self.online_users.append(nick)", "category": "Python"}, {"instruction": "def _set_cycle_time(self):\n        \"\"\"\n        Set next cycle update time synced to nearest second or 0.1 of second.\n        \"\"\"\n", "input": "", "output": "        now = time()\n        try:\n            cycle_time = now - self._cycle_time\n            if cycle_time < 0:\n                cycle_time = 0\n        except AttributeError:\n            cycle_time = 0\n        cycle_time += self.cycle_time\n        if cycle_time == int(cycle_time):\n            self._cycle_time = math.ceil(now + cycle_time)\n        else:\n            self._cycle_time = math.ceil((now + cycle_time) * 10) / 10\n        self._cycle_time = now + self.cycle_time", "category": "Python"}, {"instruction": "def add_transcripts(self,txs):\n      \"\"\"We traverse through the other transcripts and try to add to these groups\n      \"\"\"\n", "input": "", "output": "      passed = []\n      for tx2 in txs:\n         for tx1 in self._initial:\n            jov = tx1.junction_overlap(tx2,self._tolerance)\n            sub = jov.is_subset()\n            if sub == 1 or sub == 2:\n               passed.append(tx2)\n               break\n      if len(passed) == 0:\n         sys.stderr.write(\"Warning unable to add\\n\")\n         return\n      for tx in txs:\n         self.add_transcript(tx)\n      return", "category": "Python"}, {"instruction": "def _constructUrl(self, route):\n        \"\"\"Construct a URL for the given route on this service, based on the\n        rootUrl\"\"\"\n", "input": "", "output": "        return liburls.api(\n            self.options['rootUrl'],\n            self.serviceName,\n            self.apiVersion,\n            route.rstrip('/'))", "category": "Python"}, {"instruction": "def get_json_or_yaml(file_path, content):\n    \"\"\" Generate JSON or YAML depending on the file extension. \"\"\"\n", "input": "", "output": "    if os.path.splitext(file_path)[1] == \".json\":\n        return json.dumps(content, sort_keys=False, indent=4, separators=(',', ': '))\n    else:\n        return inginious.common.custom_yaml.dump(content)", "category": "Python"}, {"instruction": "def split(self, decl_string):\n        \"\"\"implementation details\"\"\"\n", "input": "", "output": "        assert self.has_pattern(decl_string)\n        return self.name(decl_string), self.args(decl_string)", "category": "Python"}, {"instruction": "def _get_all_dependencies_of(name, deps=set(), force=False):\n    '''Returns list of dependencies of the given dap from Dapi recursively'''\n", "input": "", "output": "    first_deps = _get_api_dependencies_of(name, force=force)\n    for dep in first_deps:\n        dep = _strip_version_from_dependency(dep)\n        if dep in deps:\n            continue\n        # we do the following not to resolve the dependencies of already installed daps\n        if dap in get_installed_daps():\n            continue\n        deps |= _get_all_dependencies_of(dep, deps)\n    return deps | set([name])", "category": "Python"}, {"instruction": "def ocsp_urls(self):\n        \"\"\"\n        :return:\n            A list of zero or more unicode strings of the OCSP URLs for this\n            cert\n        \"\"\"\n", "input": "", "output": "\n        if not self.authority_information_access_value:\n            return []\n\n        output = []\n        for entry in self.authority_information_access_value:\n            if entry['access_method'].native == 'ocsp':\n                location = entry['access_location']\n                if location.name != 'uniform_resource_identifier':\n                    continue\n                url = location.native\n                if url.lower().startswith(('http://', 'https://', 'ldap://', 'ldaps://')):\n                    output.append(url)\n        return output", "category": "Python"}, {"instruction": "def _traverse_repos(self, callback, repo_name=None):\n        '''\n        Traverse through all repo files and apply the functionality provided in\n        the callback to them\n        '''\n", "input": "", "output": "        repo_files = []\n        if os.path.exists(self.opts['spm_repos_config']):\n            repo_files.append(self.opts['spm_repos_config'])\n\n        for (dirpath, dirnames, filenames) in salt.utils.path.os_walk('{0}.d'.format(self.opts['spm_repos_config'])):\n            for repo_file in filenames:\n                if not repo_file.endswith('.repo'):\n                    continue\n                repo_files.append(repo_file)\n\n        for repo_file in repo_files:\n            repo_path = '{0}.d/{1}'.format(self.opts['spm_repos_config'], repo_file)\n            with salt.utils.files.fopen(repo_path) as rph:\n                repo_data = salt.utils.yaml.safe_load(rph)\n                for repo in repo_data:\n                    if repo_data[repo].get('enabled', True) is False:\n                        continue\n                    if repo_name is not None and repo != repo_name:\n                        continue\n                    callback(repo, repo_data[repo])", "category": "Python"}, {"instruction": "def get_string(self, stringid):\n        '''Returns the localized string from strings.xml for the given\n        stringid.\n        '''\n", "input": "", "output": "        stringid = int(stringid)\n        if not hasattr(self, '_strings'):\n            self._strings = {}\n        if not stringid in self._strings:\n            self._strings[stringid] = self.addon.getLocalizedString(stringid)\n        return self._strings[stringid]", "category": "Python"}, {"instruction": "def quantile(x, q, weights=None):\n    \"\"\"\n    Like numpy.percentile, but:\n\n    * Values of q are quantiles [0., 1.] rather than percentiles [0., 100.]\n    * scalar q not supported (q must be iterable)\n    * optional weights on x\n\n    \"\"\"\n", "input": "", "output": "    if weights is None:\n        return np.percentile(x, [100. * qi for qi in q])\n    else:\n        idx = np.argsort(x)\n        xsorted = x[idx]\n        cdf = np.add.accumulate(weights[idx])\n        cdf /= cdf[-1]\n        return np.interp(q, cdf, xsorted).tolist()", "category": "Python"}, {"instruction": "def tohdf5(input_files, output_file, n_events, conv_times_to_jte, **kwargs):\n    \"\"\"Convert Any file to HDF5 file\"\"\"\n", "input": "", "output": "    if len(input_files) > 1:\n        cprint(\n            \"Preparing to convert {} files to HDF5.\".format(len(input_files))\n        )\n\n    from km3pipe import Pipeline    # noqa\n    from km3pipe.io import GenericPump, HDF5Sink, HDF5MetaData    # noqa\n\n    for input_file in input_files:\n        cprint(\"Converting '{}'...\".format(input_file))\n        if len(input_files) > 1:\n            output_file = input_file + '.h5'\n\n        meta_data = kwargs.copy()\n        meta_data['origin'] = input_file\n\n        pipe = Pipeline()\n        pipe.attach(HDF5MetaData, data=meta_data)\n        pipe.attach(GenericPump, filenames=input_file, **kwargs)\n        pipe.attach(StatusBar, every=250)\n        if conv_times_to_jte:\n            from km3modules.mc import MCTimeCorrector\n            pipe.attach(MCTimeCorrector)\n        pipe.attach(HDF5Sink, filename=output_file, **kwargs)\n        pipe.drain(n_events)\n        cprint(\"File '{}' was converted.\".format(input_file))", "category": "Python"}, {"instruction": "def captureException(self, exc_info=None, **kwargs):\n        \"\"\"\n        Creates an event from an exception.\n\n        >>> try:\n        >>>     exc_info = sys.exc_info()\n        >>>     client.captureException(exc_info)\n        >>> finally:\n        >>>     del exc_info\n\n        If exc_info is not provided, or is set to True, then this method will\n        perform the ``exc_info = sys.exc_info()`` and the requisite clean-up\n        for you.\n\n        ``kwargs`` are passed through to ``.capture``.\n        \"\"\"\n", "input": "", "output": "        if exc_info is None or exc_info is True:\n            exc_info = sys.exc_info()\n\n        return self.capture(\n            'raven.events.Exception', exc_info=exc_info, **kwargs)", "category": "Python"}, {"instruction": "def cleanup():\n    \"\"\"P4P sequenced shutdown.  Intended to be atexit.  Idenpotent.\n    \"\"\"\n", "input": "", "output": "    _log.debug(\"P4P atexit begins\")\n    # clean provider registry\n    from .server import clearProviders, _cleanup_servers\n    clearProviders()\n\n    # close client contexts\n    from .client.raw import _cleanup_contexts\n    _cleanup_contexts()\n\n    # stop servers\n    _cleanup_servers()\n\n    # shutdown default work queue\n    from .util import _defaultWorkQueue\n    _defaultWorkQueue.stop()\n    _log.debug(\"P4P atexit completes\")", "category": "Python"}, {"instruction": "def obfn_g1(self, Y1):\n        r\"\"\"Compute :math:`g_1(\\mathbf{y_1})` component of ADMM objective\n        function.\n        \"\"\"\n", "input": "", "output": "\n        return np.linalg.norm((self.wl1 * Y1).ravel(), 1)", "category": "Python"}, {"instruction": "def maxsize(self, size):\n        \"\"\"Resize the cache, evicting the oldest items if necessary.\"\"\"\n", "input": "", "output": "        if size < 0:\n            raise ValueError('maxsize must be non-negative')\n        with self._lock:\n            self._enforce_size_limit(size)\n            self._maxsize = size", "category": "Python"}, {"instruction": "def is_all_field_none(self):\n        \"\"\"\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        if self._view_balance is not None:\n            return False\n\n        if self._view_old_events is not None:\n            return False\n\n        if self._view_new_events is not None:\n            return False\n\n        return True", "category": "Python"}, {"instruction": "def get_dependent_data_items(self, data_item: DataItem.DataItem) -> typing.List[DataItem.DataItem]:\n        \"\"\"Return the list of data items containing data that directly depends on data in this item.\"\"\"\n", "input": "", "output": "        with self.__dependency_tree_lock:\n            return [data_item for data_item in self.__dependency_tree_source_to_target_map.get(weakref.ref(data_item), list()) if isinstance(data_item, DataItem.DataItem)]", "category": "Python"}, {"instruction": "def ajax_count_plus(self, slug):\n        '''\n        post count plus one via ajax.\n        '''\n", "input": "", "output": "        output = {\n            'status': 1 if MWiki.view_count_plus(slug) else 0,\n        }\n\n        return json.dump(output, self)", "category": "Python"}, {"instruction": "def has_documented_type_or_fields(self, include_inherited_fields=False):\n        \"\"\"Returns whether this type, or any of its fields, are documented.\n\n        Use this when deciding whether to create a block of documentation for\n        this type.\n        \"\"\"\n", "input": "", "output": "        if self.doc:\n            return True\n        else:\n            return self.has_documented_fields(include_inherited_fields)", "category": "Python"}, {"instruction": "def parse_400_row(row: list) -> tuple:\n    \"\"\" Interval event record (400) \"\"\"\n", "input": "", "output": "\n    return EventRecord(int(row[1]), int(row[2]), row[3], row[4], row[5])", "category": "Python"}, {"instruction": "def pandoc(args, filein=None, fileout=None):\n    \"\"\"Execute pandoc with the given arguments\"\"\"\n", "input": "", "output": "    cmd = [u'pandoc']\n\n    if filein:\n        cmd.append(filein)\n\n    if fileout:\n        cmd.append('-o')\n        cmd.append(fileout)\n\n    cmd.extend(args.split())\n\n    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n    out, err = proc.communicate()\n    if proc.returncode:\n        raise PandocError('pandoc exited with return code {}\\n{}'.format(proc.returncode, str(err)))\n    return out.decode('utf-8')", "category": "Python"}, {"instruction": "def flags(self, index):\r\n        \"\"\"Set flags\"\"\"\n", "input": "", "output": "        return Qt.ItemFlags(QAbstractTableModel.flags(self, index) |\r\n                            Qt.ItemIsEditable)", "category": "Python"}, {"instruction": "def filter(self, iterable):  # noqa A001\n        \"\"\"\n        Filter.\n\n        `CSSMatch` can cache certain searches for tags of the same document,\n        so if we are given a tag, all tags are from the same document,\n        and we can take advantage of the optimization.\n\n        Any other kind of iterable could have tags from different documents or detached tags,\n        so for those, we use a new `CSSMatch` for each item in the iterable.\n        \"\"\"\n", "input": "", "output": "\n        if CSSMatch.is_tag(iterable):\n            return CSSMatch(self.selectors, iterable, self.namespaces, self.flags).filter()\n        else:\n            return [node for node in iterable if not CSSMatch.is_navigable_string(node) and self.match(node)]", "category": "Python"}, {"instruction": "def verify_appium_is_running(self):\n        \"\"\"Verify that Appium is running so it can be used for local iOS tests.\"\"\"\n", "input": "", "output": "        process = Popen(['ps -e | grep \"Appium\"'], shell=True, stdout=PIPE)\n        (grep_output, _grep_error) = process.communicate()\n        lines = grep_output.split('\\n')\n        for line in lines:\n            if 'Appium.app' in line:\n                self.stdout.write('Appium is already running')\n                return True\n        self.stdout.write('Please launch and configure Appium first')\n        return False", "category": "Python"}, {"instruction": "def list_plugins():\n    \"\"\"Return list of all plugin classes registered as a list of tuples:\n\n        [(name, plugin_class)]\n    \"\"\"\n", "input": "", "output": "    plugin_eps = pkg_resources.iter_entry_points('ofxstatement')\n    return sorted((ep.name, ep.load()) for ep in plugin_eps)", "category": "Python"}, {"instruction": "def copy(self):\n        \"\"\"Adds menus to itself, required by ViewBox\"\"\"\n", "input": "", "output": "        # copied from pyqtgraph ViewBoxMenu\n        m = QtGui.QMenu()\n        for sm in self.subMenus():\n            if isinstance(sm, QtGui.QMenu):\n                m.addMenu(sm)\n            else:\n                m.addAction(sm)\n        m.setTitle(self.title())\n        return m", "category": "Python"}, {"instruction": "def storage(self, sf=None, args=None):\n        \"\"\"Common storage interface with :class:`~flask_pluginkit.LocalStorage` or :class:`~flask_pluginkit.RedisStorage`,\n        sf is a custom storage interface classes, args is its parameters, highest priority.\n\n        :param sf: class based :class:`~flask_pluginkit.BaseStorage`\n\n        :param args: class init args\n\n        :returns: class instance\n        \"\"\"\n", "input": "", "output": "        from .utils import BaseStorage, LocalStorage, RedisStorage\n        if sf and isinstance(sf, BaseStorage):\n            return sf(args) if args else sf()\n        if self.s3 == \"local\":\n            return LocalStorage()\n        elif self.s3 == \"redis\":\n            return RedisStorage(self.s3_redis)", "category": "Python"}, {"instruction": "def update_ontology(ont_url, rdf_path):\n    \"\"\"Load an ontology formatted like Eidos' from github.\"\"\"\n", "input": "", "output": "    yaml_root = load_yaml_from_url(ont_url)\n    G = rdf_graph_from_yaml(yaml_root)\n    save_hierarchy(G, rdf_path)", "category": "Python"}, {"instruction": "def check_balances(self, account=None):\n        ''' Fetches an account balance and makes\n        necessary conversions\n        '''\n", "input": "", "output": "        a = self.account(account)\n        if a is not False and a is not None:\n            self.sbdbal = Amount(a['sbd_balance']).amount\n            self.steembal = Amount(a['balance']).amount\n            self.votepower = a['voting_power']\n            self.lastvotetime = a['last_vote_time']\n            vs = Amount(a['vesting_shares']).amount\n            dvests = Amount(a['delegated_vesting_shares']).amount\n            rvests = Amount(a['received_vesting_shares']).amount\n            vests = (float(vs) - float(dvests)) + float(rvests)\n            try:\n                self.global_props()\n                self.steempower_delegated = self.util.vests_to_sp(dvests)\n                self.steempower_raw = self.util.vests_to_sp(vs)\n                self.steempower = self.util.vests_to_sp(vests)\n            except Exception as e:\n                self.msg.error_message(e)\n            else:\n                return [self.sbdbal, self.steembal, self.steempower, \n                        self.votepower, self.lastvotetime]\n        return False", "category": "Python"}, {"instruction": "def generate_admin_resource():\n    \"\"\"Create the Blockade admin resource for the REST services.\"\"\"\n", "input": "", "output": "    logger.debug(\"[#] Setting up the admin resource\")\n    client = boto3.client('apigateway', region_name=PRIMARY_REGION)\n    existing = get_api_gateway_resource(\"admin\")\n    if existing:\n        logger.debug(\"[#] API admin resource already created\")\n        return True\n    matches = [x for x in client.get_rest_apis().get('items', list())\n               if x['name'] == API_GATEWAY]\n    match = matches.pop()\n    resource_id = get_api_gateway_resource('/')\n    response = client.create_resource(\n        restApiId=match.get('id'),\n        parentId=resource_id,\n        pathPart='admin'\n    )\n    logger.info(\"[#] Successfully setup the admin resource\")\n\n    return response", "category": "Python"}, {"instruction": "def show_gateway_device(self, gateway_device_id, **_params):\n        \"\"\"Fetch a gateway device.\"\"\"\n", "input": "", "output": "        return self.get(self.gateway_device_path % gateway_device_id,\n                        params=_params)", "category": "Python"}, {"instruction": "def grouped_mappings(self,id):\n        \"\"\"\n        return all mappings for a node, grouped by ID prefix\n        \"\"\"\n", "input": "", "output": "        g = self.get_xref_graph()\n        m = {}\n        for n in g.neighbors(id):\n            [prefix, local] = n.split(':')\n            if prefix not in m:\n                m[prefix] = []\n            m[prefix].append(n)\n        return m", "category": "Python"}, {"instruction": "def doc_children(self, doctype, limiters=[]):\n        \"\"\"Finds all grand-children of this element's docstrings that match\n        the specified doctype. If 'limiters' is specified, only docstrings\n        with those doctypes are searched.\n        \"\"\"\n", "input": "", "output": "        result = []\n        for doc in self.docstring:\n            if len(limiters) == 0 or doc.doctype in limiters:\n                result.extend(doc.children(doctype))\n\n        return result", "category": "Python"}, {"instruction": "def compare_profiles(profile1, profile2):\n\n    \"\"\"\n        Given two profiles, determine the ratio of similarity, i.e.\n        the hamming distance between the strings.\n\n        Args:\n            profile1/2 (str): profile string\n        Returns:\n            similarity_ratio (float): the ratio of similiarity (0-1)\n    \"\"\"\n", "input": "", "output": "\n    length = len(profile1)\n\n    profile1 = np.array(list(profile1))\n    profile2 = np.array(list(profile2))\n\n    similarity_array = profile1 == profile2\n\n    matches = np.sum(similarity_array)\n\n    similarity_ratio = matches/length\n\n    return similarity_ratio", "category": "Python"}, {"instruction": "def retrieve(self, session, lookup_keys, *args, **kwargs):\n        \"\"\"\n        Retrieves a model using the lookup keys provided.\n        Only one model should be returned by the lookup_keys\n        or else the manager will fail.\n\n        :param Session session: The SQLAlchemy session to use\n        :param dict lookup_keys: A dictionary mapping the fields\n            and their expected values\n        :return: The dictionary of keys and values for the retrieved\n            model.  The only values returned will be those specified by\n            fields attrbute on the class\n        :rtype: dict\n        :raises: NotFoundException\n        \"\"\"\n", "input": "", "output": "        model = self._get_model(lookup_keys, session)\n        return self.serialize_model(model)", "category": "Python"}, {"instruction": "def is_duplicated(self, item):\n        \"\"\"Check whether the item has been in the cache\n\n        If the item has not been seen before, then hash it and put it into\n        the cache, otherwise indicates the item is duplicated. When the cache\n        size exceeds capacity, discard the earliest items in the cache.\n\n        Args:\n            item (object): The item to be checked and stored in cache. It must\n                be immutable or a list/dict.\n        Returns:\n            bool: Whether the item has been in cache.\n        \"\"\"\n", "input": "", "output": "        if isinstance(item, dict):\n            hashable_item = json.dumps(item, sort_keys=True)\n        elif isinstance(item, list):\n            hashable_item = frozenset(item)\n        else:\n            hashable_item = item\n        if hashable_item in self._cache:\n            return True\n        else:\n            if self.cache_capacity > 0 and len(\n                    self._cache) >= self.cache_capacity:\n                self._cache.popitem(False)\n            self._cache[hashable_item] = 1\n            return False", "category": "Python"}, {"instruction": "def disableHook(self, msgObj):\n        \"\"\"\n        Disable yank-pop.\n\n        The ``enableHook`` method (see below) connects this method\n        to the ``qtesigKeyseqComplete`` signal to catch\n        consecutive calls to this ``yank-pop`` macro. Once the user\n        issues a key sequence for any other macro but this one, the\n        kill-list index will be set to a negative index, effectively\n        disabling the macro.\n        \"\"\"\n", "input": "", "output": "        # Unpack the data structure.\n        macroName, keysequence = msgObj.data\n        if macroName != self.qteMacroName():\n            self.qteMain.qtesigKeyseqComplete.disconnect(\n                self.disableHook)\n            self.killListIdx = -1", "category": "Python"}, {"instruction": "def translations(self):\n        \"\"\"\n        Yield all six translations of a nucleotide sequence.\n\n        @return: A generator that produces six L{TranslatedRead} instances.\n        \"\"\"\n", "input": "", "output": "        rc = self.reverseComplement().sequence\n        for reverseComplemented in False, True:\n            for frame in 0, 1, 2:\n                seq = rc if reverseComplemented else self.sequence\n                # Get the suffix of the sequence for translation. I.e.,\n                # skip 0, 1, or 2 initial bases, depending on the frame.\n                # Note that this makes a copy of the sequence, which we can\n                # then safely append 'N' bases to to adjust its length to\n                # be zero mod 3.\n                suffix = seq[frame:]\n                lengthMod3 = len(suffix) % 3\n                if lengthMod3:\n                    suffix += ('NN' if lengthMod3 == 1 else 'N')\n                yield TranslatedRead(self, translate(suffix), frame,\n                                     reverseComplemented)", "category": "Python"}, {"instruction": "def cleanup(self):\n    \"\"\"Basic cleanup after modules.\n\n    The state's output becomes the input for the next stage. Any errors are\n    moved to the global_errors attribute so that they can be reported at a\n    later stage.\n    \"\"\"\n", "input": "", "output": "    # Move any existing errors to global errors\n    self.global_errors.extend(self.errors)\n    self.errors = []\n\n    # Make the previous module's output available to the next module\n    self.input = self.output\n    self.output = []", "category": "Python"}, {"instruction": "def ichunks_list(list_, chunksize):\n    \"\"\"\n    input must be a list.\n\n    SeeAlso:\n        ichunks\n\n    References:\n        http://stackoverflow.com/questions/434287/iterate-over-a-list-in-chunks\n    \"\"\"\n", "input": "", "output": "    return (list_[ix:ix + chunksize] for ix in range(0, len(list_), chunksize))", "category": "Python"}, {"instruction": "def print_capabilities_by_ext(self, strict_type_matching: bool = False):\n        \"\"\"\n        Used to print the list of all file extensions that can be parsed by this parser registry.\n        :return:\n        \"\"\"\n", "input": "", "output": "        print('\\nCapabilities by file extension: ')\n        l = self.get_capabilities_by_ext(strict_type_matching=strict_type_matching)\n        pprint({ext: get_pretty_type_keys_dict(parsers) for ext, parsers in l.items()})\n        print('\\n')", "category": "Python"}, {"instruction": "def calculate_size(name, index):\n    \"\"\" Calculates the request payload size\"\"\"\n", "input": "", "output": "    data_size = 0\n    data_size += calculate_size_str(name)\n    data_size += INT_SIZE_IN_BYTES\n    return data_size", "category": "Python"}, {"instruction": "def whoarewe(self,\n\t             shutit_pexpect_child=None,\n\t             note=None,\n\t             loglevel=logging.DEBUG):\n\t\t\"\"\"Returns the current group.\n\n\t\t@param shutit_pexpect_child:    See send()\n\t\t@param note:     See send()\n\n\t\t@return: the first group found\n\t\t@rtype: string\n\t\t\"\"\"\n", "input": "", "output": "\t\tshutit_global.shutit_global_object.yield_to_draw()\n\t\tshutit_pexpect_child = shutit_pexpect_child or self.get_current_shutit_pexpect_session().pexpect_child\n\t\tshutit_pexpect_session = self.get_shutit_pexpect_session_from_child(shutit_pexpect_child)\n\t\treturn shutit_pexpect_session.whoarewe(note=note,\n\t\t                                       loglevel=loglevel)", "category": "Python"}, {"instruction": "def watch_statuses(self, observer, batch_ids):\n        \"\"\"Allows a component to register to be notified when a set of\n        batches is no longer PENDING. Expects to be able to call the\n        \"notify_batches_finished\" method on the registered component, sending\n        the statuses of the batches.\n\n        Args:\n            observer (object): Must implement \"notify_batches_finished\" method\n            batch_ids (list of str): The ids of the batches to watch\n        \"\"\"\n", "input": "", "output": "        with self._lock:\n            statuses = self.get_statuses(batch_ids)\n            if self._has_no_pendings(statuses):\n                observer.notify_batches_finished(statuses)\n            else:\n                self._observers[observer] = statuses", "category": "Python"}, {"instruction": "def _filter_file(src, dest, subst):\n    \"\"\"Copy src to dest doing substitutions on the fly.\n    \"\"\"\n", "input": "", "output": "    substre = re.compile(r'\\$(%s)' % '|'.join(subst.keys()))\n    def repl(m):\n        return subst[m.group(1)]\n    with open(src, \"rt\") as sf, open(dest, \"wt\") as df:\n        while True:\n            l = sf.readline()\n            if not l:\n                break\n            df.write(re.sub(substre, repl, l))", "category": "Python"}, {"instruction": "def _parse_result(self, ok, match, fh=None):\n        \"\"\"Parse a matching result line into a result instance.\"\"\"\n", "input": "", "output": "        peek_match = None\n        try:\n            if fh is not None and self._try_peeking:\n                peek_match = self.yaml_block_start.match(fh.peek())\n        except StopIteration:\n            pass\n        if peek_match is None:\n            return Result(\n                ok,\n                number=match.group(\"number\"),\n                description=match.group(\"description\").strip(),\n                directive=Directive(match.group(\"directive\")),\n            )\n        indent = peek_match.group(\"indent\")\n        concat_yaml = self._extract_yaml_block(indent, fh)\n        return Result(\n            ok,\n            number=match.group(\"number\"),\n            description=match.group(\"description\").strip(),\n            directive=Directive(match.group(\"directive\")),\n            raw_yaml_block=concat_yaml,\n        )", "category": "Python"}, {"instruction": "def has_permission(self, request, *args, **kwargs):\n        \"\"\"\n        Figures out if the current user has permissions for this view.\n        \"\"\"\n", "input": "", "output": "        self.kwargs = kwargs\n        self.args = args\n        self.request = request\n\n        if not getattr(self, 'permission', None):\n            return True\n        else:\n            return request.user.has_perm(self.permission)", "category": "Python"}, {"instruction": "def insert_system(cur, system_name, encoded_data=None):\n    \"\"\"Insert a system name into the cache.\n\n    Args:\n        cur (:class:`sqlite3.Cursor`):\n            An sqlite3 cursor. This function is meant to be run within a :obj:`with` statement.\n\n        system_name (str):\n            The unique name of a system\n\n        encoded_data (dict, optional):\n            If a dictionary is provided, it will be populated with the serialized data. This is\n            useful for preventing encoding the same information many times.\n\n    \"\"\"\n", "input": "", "output": "    if encoded_data is None:\n        encoded_data = {}\n\n    if 'system_name' not in encoded_data:\n        encoded_data['system_name'] = system_name\n\n    insert = \"INSERT OR IGNORE INTO system(system_name) VALUES (:system_name);\"\n    cur.execute(insert, encoded_data)", "category": "Python"}, {"instruction": "def get_subgraph_by_annotation_value(graph, annotation, values):\n    \"\"\"Induce a sub-graph over all edges whose annotations match the given key and value.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param str annotation: The annotation to group by\n    :param values: The value(s) for the annotation\n    :type values: str or iter[str]\n    :return: A subgraph of the original BEL graph\n    :rtype: pybel.BELGraph\n    \"\"\"\n", "input": "", "output": "    if isinstance(values, str):\n        values = {values}\n\n    return get_subgraph_by_annotations(graph, {annotation: values})", "category": "Python"}, {"instruction": "def QueryDatabases(self, query, options=None):\n        \"\"\"Queries databases.\n\n        :param (str or dict) query:\n        :param dict options:\n            The request options for the request.\n\n        :return: Query Iterable of Databases.\n        :rtype:\n            query_iterable.QueryIterable\n\n        \"\"\"\n", "input": "", "output": "        if options is None:\n            options = {}\n\n        def fetch_fn(options):\n            return self.__QueryFeed('/dbs',\n                                    'dbs',\n                                    '',\n                                    lambda r: r['Databases'],\n                                    lambda _, b: b,\n                                    query,\n                                    options), self.last_response_headers\n        return query_iterable.QueryIterable(self, query, options, fetch_fn)", "category": "Python"}, {"instruction": "def get(self, **kwargs):\n        \"\"\"Get the details for a specific notification.\"\"\"\n", "input": "", "output": "\n        # NOTE(trebskit) should actually be find_one, but\n        # monasca does not support expected response format\n\n        url = '%s/%s' % (self.base_url, kwargs['notification_id'])\n        resp = self.client.list(path=url)\n        return resp", "category": "Python"}, {"instruction": "def on_step_end(self, step, logs):\n        \"\"\" Update statistics of episode after each step \"\"\"\n", "input": "", "output": "        episode = logs['episode']\n        self.observations[episode].append(logs['observation'])\n        self.rewards[episode].append(logs['reward'])\n        self.actions[episode].append(logs['action'])\n        self.metrics[episode].append(logs['metrics'])\n        self.step += 1", "category": "Python"}, {"instruction": "def snmp_server_context_vrf_name(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        snmp_server = ET.SubElement(config, \"snmp-server\", xmlns=\"urn:brocade.com:mgmt:brocade-snmp\")\n        context = ET.SubElement(snmp_server, \"context\")\n        context_name_key = ET.SubElement(context, \"context-name\")\n        context_name_key.text = kwargs.pop('context_name')\n        vrf_name = ET.SubElement(context, \"vrf-name\")\n        vrf_name.text = kwargs.pop('vrf_name')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def safejoin(base, *elements):\n    \"\"\"Safely joins paths together.\n\n    The result will always be a subdirectory under `base`, otherwise ValueError\n    is raised.\n\n    Args:\n        base (str): base path\n        elements (list of strings): path elements to join to base\n\n    Returns:\n        elements joined to base\n\n    \"\"\"\n", "input": "", "output": "    # TODO: do we really want to be absolute here?\n    base = os.path.abspath(base)\n    path = os.path.join(base, *elements)\n    path = os.path.normpath(path)\n    if not path_is_inside(path, base):\n        raise ValueError('target path is outside of the base path')\n    return path", "category": "Python"}, {"instruction": "def use_plenary_assessment_taken_view(self):\n        \"\"\"Pass through to provider AssessmentTakenLookupSession.use_plenary_assessment_taken_view\"\"\"\n", "input": "", "output": "        self._object_views['assessment_taken'] = PLENARY\n        # self._get_provider_session('assessment_taken_lookup_session') # To make sure the session is tracked\n        for session in self._get_provider_sessions():\n            try:\n                session.use_plenary_assessment_taken_view()\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def body_echo(cls, request,\n                  foo: (Ptypes.body, String('A body parameter'))) -> [\n            (200, 'Ok', String)]:\n        '''Echo the body parameter.'''\n", "input": "", "output": "        log.info('Echoing body param, value is: {}'.format(foo))\n        for i in range(randint(0, MAX_LOOP_DURATION)):\n            yield\n        msg = 'The value sent was: {}'.format(foo)\n        Respond(200, msg)", "category": "Python"}, {"instruction": "def getInfo(self, CorpNum, MgtKeyType, MgtKey):\n        \"\"\" \uc0c1\ud0dc\uc815\ubcf4 \ud655\uc778\n            args\n                CorpNum : \ud68c\uc6d0 \uc0ac\uc5c5\uc790 \ubc88\ud638\n                MgtKeyType : \uad00\ub9ac\ubc88\ud638 \uc720\ud615 one of ['SELL','BUY','TRUSTEE']\n                MgtKey : \ud30c\ud2b8\ub108 \uad00\ub9ac\ubc88\ud638\n            return\n                \ucc98\ub9ac\uacb0\uacfc. consist of code and message\n            raise\n                PopbillException\n        \"\"\"\n", "input": "", "output": "        if MgtKeyType not in self.__MgtKeyTypes:\n            raise PopbillException(-99999999, \"\uad00\ub9ac\ubc88\ud638 \ud615\ud0dc\uac00 \uc62c\ubc14\ub974\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.\")\n        if MgtKey == None or MgtKey == \"\":\n            raise PopbillException(-99999999, \"\uad00\ub9ac\ubc88\ud638\uac00 \uc785\ub825\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\")\n\n        return self._httpget('/Taxinvoice/' + MgtKeyType + '/' + MgtKey, CorpNum)", "category": "Python"}, {"instruction": "def dev_tools_installer(cv, ctx, site):\n    \"\"\"\n    Installer factory\n    @param  cv: Current version (The version of Developer Tools we are installing)\n    @type   cv: ips_vagrant.common.version.Version\n    @return:    Installer instance\n    @rtype:     ips_vagrant.installer.dev_tools.latest.DevToolsInstaller\n    \"\"\"\n", "input": "", "output": "    log = logging.getLogger('ipsv.installer.dev_tools')\n    log.info('Loading installer for Dev Tools %s', cv)\n    iv = None\n    for v in versions:\n        vstring = '.'.join(map(str, v)) if v else 'latest'\n        # cvstring = '.'.join(map(str, cv)) if cv else 'latest'\n        log.debug('Checking if version %s >= %s', vstring, cv.vstring)\n        if (v is None) or (v >= cv):\n            log.debug('Changing installer version to %s', vstring)\n            iv = v\n\n    log.info('Returning installer version %s', '.'.join(map(str, iv)) if iv else 'latest')\n    return versions[iv](ctx, site)", "category": "Python"}, {"instruction": "def write(s = ''):\n    \"\"\"\n    Automates the process of typing by converting a string into a set of press() and hold() calls\n\n    :param s: string to be written\n    :return: None\n    \"\"\"\n", "input": "", "output": "\n    for char in s:\n        if char.isupper():  # Handles uppercase\n            hold('shift', char.lower())\n        elif char == \" \":  # Handles spaces\n            press('spacebar')\n        elif char == \"\\n\":  # Handles newline\n            press('enter')\n        elif char in (')', '!', '@', '#', '$', '%', '^', '&', '*', '('):  # Handles shift & number\n            hold('shift', str((')', '!', '@', '#', '$', '%', '^', '&', '*', '(').index(char)))\n        elif char in ('{', '}', '<', '>', '?', ':', '\"', '_', '+', '~'):\n            hold('shift', ('[', ']', ',', '.', '/', ';', \"'\", '-', '=', '`')[('{', '}', '<', '>', '?', ':', '\"', '_',\n                                                                              '+', '~').index(char)])\n        else:\n            press(char)", "category": "Python"}, {"instruction": "def handle_connection_exec(client):\n    \"\"\"\n    Alternate connection handler. No output redirection.\n    \"\"\"\n", "input": "", "output": "    class ExitExecLoop(Exception):\n        pass\n\n    def exit():\n        raise ExitExecLoop()\n\n    client.settimeout(None)\n    fh = os.fdopen(client.detach() if hasattr(client, 'detach') else client.fileno())\n\n    with closing(client):\n        with closing(fh):\n            try:\n                payload = fh.readline()\n                while payload:\n                    _LOG(\"Running: %r.\" % payload)\n                    eval(compile(payload, '<manhole>', 'exec'), {'exit': exit}, _MANHOLE.locals)\n                    payload = fh.readline()\n            except ExitExecLoop:\n                _LOG(\"Exiting exec loop.\")", "category": "Python"}, {"instruction": "def zcount(self, name, min, max):\n        \"\"\"\n        Returns the number of elements in the sorted set at key ``name`` with\n        a score between ``min`` and ``max``.\n\n        :param name: str\n        :param min: float\n        :param max: float\n        :return: Future()\n        \"\"\"\n", "input": "", "output": "        with self.pipe as pipe:\n            return pipe.zcount(self.redis_key(name), min, max)", "category": "Python"}, {"instruction": "def safeEncodeAttribute(self, encValue):\n\t\t\"\"\"docstring for safeEncodeAttribute\"\"\"\n", "input": "", "output": "\t\tencValue = encValue.replace(u'&', u'&amp;')\n\t\tencValue = encValue.replace(u'<', u'&lt;')\n\t\tencValue = encValue.replace(u'>', u'&gt;')\n\t\tencValue = encValue.replace(u'\"', u'&quot;')\n\t\tencValue = encValue.replace(u'{', u'&#123;')\n\t\tencValue = encValue.replace(u'[', u'&#91;')\n\t\tencValue = encValue.replace(u\"''\", u'&#39;&#39;')\n\t\tencValue = encValue.replace(u'ISBN', u'&#73;SBN')\n\t\tencValue = encValue.replace(u'RFC', u'&#82;FC')\n\t\tencValue = encValue.replace(u'PMID', u'&#80;MID')\n\t\tencValue = encValue.replace(u'|', u'&#124;')\n\t\tencValue = encValue.replace(u'__', u'&#95;_')\n\t\tencValue = encValue.replace(u'\\n', u'&#10;')\n\t\tencValue = encValue.replace(u'\\r', u'&#13;')\n\t\tencValue = encValue.replace(u'\\t', u'&#9;')\n\t\treturn encValue", "category": "Python"}, {"instruction": "def _database_string(self):\n        \"\"\"The database string corresponding to this client's project.\n\n        This value is lazy-loaded and cached.\n\n        Will be of the form\n\n            ``projects/{project_id}/databases/{database_id}``\n\n        but ``database_id == '(default)'`` for the time being.\n\n        Returns:\n            str: The fully-qualified database string for the current\n            project. (The default database is also in this string.)\n        \"\"\"\n", "input": "", "output": "        if self._database_string_internal is None:\n            # NOTE: database_root_path() is a classmethod, so we don't use\n            #       self._firestore_api (it isn't necessary).\n            db_str = firestore_client.FirestoreClient.database_root_path(\n                self.project, self._database\n            )\n            self._database_string_internal = db_str\n\n        return self._database_string_internal", "category": "Python"}, {"instruction": "def handle_repl(locals):\n    \"\"\"\n    Dumps stacktraces and runs an interactive prompt (REPL).\n    \"\"\"\n", "input": "", "output": "    dump_stacktraces()\n    namespace = {\n        'dump_stacktraces': dump_stacktraces,\n        'sys': sys,\n        'os': os,\n        'socket': socket,\n        'traceback': traceback,\n    }\n    if locals:\n        namespace.update(locals)\n    ManholeConsole(namespace).interact()", "category": "Python"}, {"instruction": "def detach_user_policy(policy_name, user_name,\n                  region=None, key=None, keyid=None, profile=None):\n    '''\n    Detach a managed policy to a user.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.detach_user_policy mypolicy myuser\n    '''\n", "input": "", "output": "    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    policy_arn = _get_policy_arn(policy_name, region, key, keyid, profile)\n    try:\n        conn.detach_user_policy(policy_arn, user_name)\n        log.info('Detached %s policy from IAM user %s.', policy_name, user_name)\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        log.error('Failed to detach %s policy from IAM user %s.', policy_name, user_name)\n        return False\n    return True", "category": "Python"}, {"instruction": "def get_request_authorization(method, resource, key, params, headers):\r\n        \"\"\" :return bytes (PY2) or string (PY2) \"\"\"\n", "input": "", "output": "        if not key:\r\n            return six.b('')\r\n        content = method + \"\\n\"\r\n        if 'Content-MD5' in headers:\r\n            content += headers['Content-MD5']\r\n        content += '\\n'\r\n        if 'Content-Type' in headers:\r\n            content += headers['Content-Type']\r\n        content += \"\\n\"\r\n        content += headers['Date'] + \"\\n\"\r\n        content += Util.canonicalized_log_headers(headers)\r\n        content += Util.canonicalized_resource(resource, params)\r\n        return Util.hmac_sha1(content, key)", "category": "Python"}, {"instruction": "def read_grid_from_file(filename):\n    \"\"\"\n    Read the results of a full set of calculations from file\n    \"\"\"\n", "input": "", "output": "    try:\n        f = open(filename, mode='r')\n        full_res = ast.literal_eval(f.read())\n        f.close()\n    except SyntaxError:\n        print('Problems reading ', filename)\n        full_res = {'grid': 0, 'all_done': False}\n    except (OSError, IOError):\n        full_res = {'grid': 0, 'all_done': False}\n    return full_res", "category": "Python"}, {"instruction": "def database_remove_tags(object_id, input_params={}, always_retry=True, **kwargs):\n    \"\"\"\n    Invokes the /database-xxxx/removeTags API method.\n\n    For more info, see: https://wiki.dnanexus.com/API-Specification-v1.0.0/Tags#API-method%3A-%2Fclass-xxxx%2FremoveTags\n    \"\"\"\n", "input": "", "output": "    return DXHTTPRequest('/%s/removeTags' % object_id, input_params, always_retry=always_retry, **kwargs)", "category": "Python"}, {"instruction": "def set_hook_touch(self, fpath, action):\n        \"\"\"Allows running certain action when the specified file is touched.\n\n        :param str|unicode fpath: File path.\n\n        :param str|unicode|list|HookAction|list[HookAction] action:\n\n        \"\"\"\n", "input": "", "output": "        self._set('hook-touch', '%s %s' % (fpath, action), multi=True)\n\n        return self._section", "category": "Python"}, {"instruction": "def generate_challenge(self):\n        # local import to avoid circular import\n        from two_factor.utils import totp_digits\n\n        \"\"\"\n        Sends the current TOTP token to `self.number` using `self.method`.\n        \"\"\"\n", "input": "", "output": "        no_digits = totp_digits()\n        token = str(totp(self.bin_key, digits=no_digits)).zfill(no_digits)\n        if self.method == 'call':\n            make_call(device=self, token=token)\n        else:\n            send_sms(device=self, token=token)", "category": "Python"}, {"instruction": "def Size(self):\n        \"\"\"\n        Get the total size in bytes of the object.\n\n        Returns:\n            int: size.\n        \"\"\"\n", "input": "", "output": "        script_size = GetVarSize(self.Code.Script)\n        parameterlist_size = GetVarSize(self.Code.ParameterList)\n        parameterreturntype_size = s.uint8\n\n        return super(ContractState, self).Size() + script_size + parameterlist_size + parameterreturntype_size + s.uint8 + GetVarSize(self.Name) + GetVarSize(self.CodeVersion) + GetVarSize(self.Author) + GetVarSize(self.Email) + GetVarSize(self.Description)", "category": "Python"}, {"instruction": "def strip_pkcs7_padding(m, block_size):\n    \"\"\"\n    Same as PKCS#5 padding, except generalized to block sizes other than 8.\n    \"\"\"\n", "input": "", "output": "    if len(m) < block_size or len(m) % block_size != 0:\n        raise BadPaddingException(\"Unable to strip padding: invalid message length\")\n\n    m = bytearray(m) # py2/3 compatibility: always returns individual indexed elements as ints\n    last_byte = m[-1]\n    # the <last_byte> bytes of m must all have value <last_byte>, otherwise something's wrong\n    if (last_byte <= 0 or last_byte > block_size) or (m[-last_byte:] != bytearray([last_byte])*last_byte):\n        raise BadPaddingException(\"Unable to strip padding: invalid padding found\")\n\n    return bytes(m[:-last_byte])", "category": "Python"}, {"instruction": "def populate_pages(self, parent=None, child=5, depth=5):\n        \"\"\"Create a population of :class:`Page <pages.models.Page>`\n        for testing purpose.\"\"\"\n", "input": "", "output": "        User = get_user_model()\n        from basic_cms.models import Content\n        author = User.objects.all()[0]\n        if depth == 0:\n            return\n        p = self.model(parent=parent, author=author, status=self.model.PUBLISHED)\n        p.save()\n        p = self.get(id=p.id)\n        Content(body='page-' + str(p.id), type='title', language=settings.PAGE_DEFAULT_LANGUAGE, page=p).save()\n        Content(body='page-' + str(p.id), type='slug', language=settings.PAGE_DEFAULT_LANGUAGE, page=p).save()\n        for child in range(1, child + 1):\n            self.populate_pages(parent=p, child=child, depth=(depth - 1))", "category": "Python"}, {"instruction": "def translate_path(self, path):\n        \"\"\"Translate a /-separated PATH to the local filename syntax.\n\n        Components that mean special things to the local file system\n        (e.g. drive or directory names) are ignored.  (XXX They should\n        probably be diagnosed.)\n\n        \"\"\"\n", "input": "", "output": "        # abandon query parameters\n        path = path.split('?',1)[0]\n        path = path.split('#',1)[0]\n        path = posixpath.normpath(urllib_parse.unquote(path))\n        words = path.split('/')\n        words = filter(None, words)\n        path = os.getcwd()\n        for word in words:\n            drive, word = os.path.splitdrive(word)\n            head, word = os.path.split(word)\n            if word in (os.curdir, os.pardir): continue\n            path = os.path.join(path, word)\n        return path", "category": "Python"}, {"instruction": "def _apply_section_hosts(self, section, hosts):\n        \"\"\"\n        Add the variables for each entry in a 'hosts' section to the hosts\n        belonging to that entry.\n        \"\"\"\n", "input": "", "output": "        for entry in section['entries']:\n            for hostname in self.expand_hostdef(entry['name']):\n                if hostname not in hosts:\n                    # Expanded host or child host or something else refers to a\n                    # host that isn't actually defined. Ansible skips this, so\n                    # we will too.\n                    continue\n                host = hosts[hostname]\n                for var_key, var_val in entry['hostvars'].items():\n                    host['hostvars'][var_key] = var_val", "category": "Python"}, {"instruction": "def _closeResources(self):\n        \"\"\" Disconnects signals.\n            Is called by self.finalize when the cti is deleted.\n        \"\"\"\n", "input": "", "output": "        self.viewBox.sigRangeChangedManually.disconnect(self.setAutoRangeOff)\n        self.viewBox.sigRangeChanged.disconnect(self.refreshMinMax)", "category": "Python"}, {"instruction": "def handle_scd(self, conn, args):\n        \"\"\"No support for smart-card device protocol.\"\"\"\n", "input": "", "output": "        reply = {\n            (b'GETINFO', b'version'): self.version,\n        }.get(args)\n        if reply is None:\n            raise AgentError(b'ERR 100696144 No such device <SCD>')\n        keyring.sendline(conn, b'D ' + reply)", "category": "Python"}, {"instruction": "def to_base_10_int(n, input_base):\r\n    \"\"\"\r\n    Converts an integer in any base into it's decimal representation.\r\n\r\n    Args:\r\n        n - An integer represented as a tuple of digits in the specified base.\r\n        input_base - the base of the input number.\r\n\r\n    Returns:\r\n        integer converted into base 10.\r\n\r\n    Example:\r\n        >>> to_base_10_int((8,1), 16)\r\n        129\r\n    \"\"\"\n", "input": "", "output": "    return sum(c * input_base ** i for i, c in enumerate(n[::-1]))", "category": "Python"}, {"instruction": "def import_issue(self, subject, priority, status,\n                     issue_type, severity, **attrs):\n        \"\"\"\n        Import and issue and returns a :class:`Issue` resource.\n\n        :param subject: subject of :class:`Issue`\n        :param priority: priority of :class:`Issue`\n        :param status: status of :class:`Issue`\n        :param issue_type: issue type of :class:`Issue`\n        :param severity: severity of :class:`Issue`\n        :param attrs: optional :class:`Issue` attributes\n        \"\"\"\n", "input": "", "output": "        return Issues(self.requester).import_(\n            self.id, subject, priority, status,\n            issue_type, severity, **attrs\n        )", "category": "Python"}, {"instruction": "def _setup(module, extras):\n    \"\"\"Install common submodules\"\"\"\n", "input": "", "output": "\n    Qt.__binding__ = module.__name__\n\n    for name in list(_common_members) + extras:\n        try:\n            submodule = _import_sub_module(\n                module, name)\n        except ImportError:\n            try:\n                # For extra modules like sip and shiboken that may not be\n                # children of the binding.\n                submodule = __import__(name)\n            except ImportError:\n                continue\n\n        setattr(Qt, \"_\" + name, submodule)\n\n        if name not in extras:\n            # Store reference to original binding,\n            # but don't store speciality modules\n            # such as uic or QtUiTools\n            setattr(Qt, name, _new_module(name))", "category": "Python"}, {"instruction": "def _collect_uncolored_outputs(unspent_outputs, amount):\n        \"\"\"\n        Returns a list of uncolored outputs for the specified amount.\n\n        :param list[SpendableOutput] unspent_outputs: The list of available outputs.\n        :param int amount: The amount to collect.\n        :return: A list of outputs, and the total amount collected.\n        :rtype: (list[SpendableOutput], int)\n        \"\"\"\n", "input": "", "output": "        total_amount = 0\n        result = []\n        for output in unspent_outputs:\n            if output.output.asset_id is None:\n                result.append(output)\n                total_amount += output.output.value\n\n            if total_amount >= amount:\n                return result, total_amount\n\n        raise InsufficientFundsError", "category": "Python"}, {"instruction": "def learn_response(self, statement, previous_statement=None):\n        \"\"\"\n        Learn that the statement provided is a valid response.\n        \"\"\"\n", "input": "", "output": "        if not previous_statement:\n            previous_statement = statement.in_response_to\n\n        if not previous_statement:\n            previous_statement = self.get_latest_response(statement.conversation)\n            if previous_statement:\n                previous_statement = previous_statement.text\n\n        previous_statement_text = previous_statement\n\n        if not isinstance(previous_statement, (str, type(None), )):\n            statement.in_response_to = previous_statement.text\n        elif isinstance(previous_statement, str):\n            statement.in_response_to = previous_statement\n\n        self.logger.info('Adding \"{}\" as a response to \"{}\"'.format(\n            statement.text,\n            previous_statement_text\n        ))\n\n        # Save the input statement\n        return self.storage.create(**statement.serialize())", "category": "Python"}, {"instruction": "def hasLogger(self, logger):\r\n        \"\"\"\r\n        Returns whether or not the inputed logger is tracked by this widget.\r\n        \r\n        :param      logger | <str> || <logging.Logger>\r\n        \"\"\"\n", "input": "", "output": "        if isinstance(logger, logging.Logger):\r\n            logger = logging.name\r\n        \r\n        return logger in self._loggers", "category": "Python"}, {"instruction": "def ddspmt(t, peak_delay=6, under_delay=16, peak_disp=1, under_disp=1,\n           p_u_ratio=6):\n    \"\"\" SPM canonical HRF dispersion derivative, values for time values `t`\n\n    Parameters\n    ----------\n    t : array-like\n        vector of times at which to sample HRF\n\n    Returns\n    -------\n    hrf : array\n        vector length ``len(t)`` of samples from HRF at times `t`\n\n    Notes\n    -----\n    [1] This is the canonical HRF dispersion derivative function as used in SPM\n    [2] It is the numerical difference between the HRF sampled at time `t`, and\n    values at `t` for another HRF shape with a small change in the peak\n    dispersion parameter (``peak_disp`` in func:`spm_hrf_compat`).\n\n    References:\n    -----\n    [1] http://nipy.org/\n    [2] https://github.com/fabianp/hrf_estimation\n    \"\"\"\n", "input": "", "output": "\n    _spm_dd_func = partial(spmt, peak_delay=peak_delay,\n                           under_delay=under_delay,\n                           under_disp=under_disp, p_u_ratio=p_u_ratio,\n                           peak_disp=1.01)\n\n    return (spmt(t) - _spm_dd_func(t)) / 0.01", "category": "Python"}, {"instruction": "def set_victims_to(self, victims_to):\n        \"\"\"Update victims_to in Cache and backend.\"\"\"\n", "input": "", "output": "        assert victims_to is None or isinstance(victims_to, Cache), \\\n            \"store_to needs to be None or a Cache object.\"\n        assert victims_to is None or victims_to.cl_size == self.cl_size, \\\n            \"cl_size may only increase towards main memory.\"\n        self.victims_to = victims_to\n        self.backend.victims_to = victims_to.backend", "category": "Python"}, {"instruction": "def _getOrCreate(cls):\n        \"\"\"\n        Internal function to get or create global BarrierTaskContext. We need to make sure\n        BarrierTaskContext is returned from here because it is needed in python worker reuse\n        scenario, see SPARK-25921 for more details.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(cls._taskContext, BarrierTaskContext):\n            cls._taskContext = object.__new__(cls)\n        return cls._taskContext", "category": "Python"}, {"instruction": "def value(self, key, default=None):\r\n        \"\"\"\r\n        Returns the value for the given key for this settings instance.\r\n        \r\n        :return     <variant>\r\n        \"\"\"\n", "input": "", "output": "        if self._customFormat:\r\n            return self._customFormat.value(key, default)\r\n        else:\r\n            return unwrapVariant(super(XSettings, self).value(key))", "category": "Python"}, {"instruction": "def validate_url(value):\n    \"\"\" Validate url. \"\"\"\n", "input": "", "output": "    if not re.match(VIMEO_URL_RE, value) and not re.match(YOUTUBE_URL_RE, value):\n        raise ValidationError('Invalid URL - only Youtube, Vimeo can be used.')", "category": "Python"}, {"instruction": "def next_frame_base_range(rhp):\n  \"\"\"Basic tuning grid.\"\"\"\n", "input": "", "output": "  rhp.set_float(\"dropout\", 0.2, 0.6)\n  rhp.set_discrete(\"hidden_size\", [64, 128, 256])\n  rhp.set_int(\"num_compress_steps\", 5, 8)\n  rhp.set_discrete(\"batch_size\", [4, 8, 16, 32])\n  rhp.set_int(\"num_hidden_layers\", 1, 3)\n  rhp.set_int(\"filter_double_steps\", 1, 6)\n  rhp.set_float(\"learning_rate_constant\", 1., 4.)\n  rhp.set_int(\"learning_rate_warmup_steps\", 500, 3000)\n  rhp.set_float(\"initializer_gain\", 0.8, 1.8)", "category": "Python"}, {"instruction": "def get_interface_detail_output_interface_ifHCOutErrors(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_interface_detail = ET.Element(\"get_interface_detail\")\n        config = get_interface_detail\n        output = ET.SubElement(get_interface_detail, \"output\")\n        interface = ET.SubElement(output, \"interface\")\n        interface_type_key = ET.SubElement(interface, \"interface-type\")\n        interface_type_key.text = kwargs.pop('interface_type')\n        interface_name_key = ET.SubElement(interface, \"interface-name\")\n        interface_name_key.text = kwargs.pop('interface_name')\n        ifHCOutErrors = ET.SubElement(interface, \"ifHCOutErrors\")\n        ifHCOutErrors.text = kwargs.pop('ifHCOutErrors')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def dpop(self, **kwds):\n        \"\"\"Pops and returns the first element that matches the\n        given specification. If no elements are found\n        raises IndexError.\n        \"\"\"\n", "input": "", "output": "        item = self.dget(**kwds)\n        self.remove(item)\n        return item", "category": "Python"}, {"instruction": "def get_version(release_level=True):\n    \"\"\"\n    Return the formatted version information\n    \"\"\"\n", "input": "", "output": "    vers = [\"%(major)i.%(minor)i.%(micro)i\" % __version_info__]\n    if release_level and __version_info__['releaselevel'] != 'final':\n        vers.append('%(releaselevel)s%(serial)i' % __version_info__)\n    return ''.join(vers)", "category": "Python"}, {"instruction": "def fit(self, validation_data=None, **kwargs):\n        \"\"\"\n        Args:\n            validation_data (DataFlow or InputSource): to be used for inference.\n                The inference callback is added as the first in the callback list.\n                If you need to use it in a different order, please write it in the callback list manually.\n            kwargs: same arguments as :meth:`Trainer.train_with_defaults`.\n        \"\"\"\n", "input": "", "output": "        callbacks = kwargs.pop('callbacks', [])\n        if validation_data is not None:\n            # There is no way to guess where users want this callback. So we have to choose one.\n            # MinSaver may need results from this callback,\n            # so we put this callback at first.\n            callbacks.insert(0, InferenceRunner(\n                validation_data, ScalarStats(self._stats_to_inference)))\n        self.trainer.train_with_defaults(callbacks=callbacks, **kwargs)", "category": "Python"}, {"instruction": "def copy(self):\n        \"\"\"Correctly copies the `Record`\n\n        # Returns\n\n        `Record`\n\n        > A completely decoupled copy of the original\n        \"\"\"\n", "input": "", "output": "        c = copy.copy(self)\n        c._fieldDict = c._fieldDict.copy()\n        return c", "category": "Python"}, {"instruction": "def indexXY(self, index):\n        \"\"\"Coordinates for the test row at *index*\n\n        Re-implemented from :meth:`AbstractDragView<sparkle.gui.abstract_drag_view.AbstractDragView.indexXY>`\n        \"\"\"\n", "input": "", "output": "        # just want the top left of row selected\n        row = index.row()\n        if row == -1:\n            row = self.model().rowCount()\n        y = self.rowHeight(0)*row\n        return 0, y", "category": "Python"}, {"instruction": "def _forward(self, x_dot_parameters):\n        \"\"\" Helper to calculate the forward weights.  \"\"\"\n", "input": "", "output": "        return forward(self._lattice, x_dot_parameters, \n                       self.state_machine.n_states)", "category": "Python"}, {"instruction": "def integer(self, x):\n        \"\"\"\n        returns a plain integer\n        \"\"\"\n", "input": "", "output": "        if type(x) is str:\n            hex = binascii.unhexlify(x)\n            return int.from_bytes(hex, 'big')\n\n        return x.value if isinstance(x, FiniteField.Value) else x", "category": "Python"}, {"instruction": "def error(message, *args, **kwargs):\n    \"\"\" print an error message \"\"\"\n", "input": "", "output": "\n    print('[!] ' + message.format(*args, **kwargs))\n    sys.exit(1)", "category": "Python"}, {"instruction": "def convert_case(self, value, case):\n        \"\"\"Convert case.\"\"\"\n", "input": "", "output": "\n        if self.is_bytes:\n            cased = []\n            for c in value:\n                if c in _ASCII_LETTERS:\n                    cased.append(c.lower() if case == _LOWER else c.upper())\n                else:\n                    cased.append(c)\n            return \"\".join(cased)\n        else:\n            return value.lower() if case == _LOWER else value.upper()", "category": "Python"}, {"instruction": "def create_subprocess(self, command):\n        \"\"\"\n        http://www.chiark.greenend.org.uk/ucgi/~cjwatson/blosxom/2009-07-02-python-sigpipe.html\n        \"\"\"\n", "input": "", "output": "\n        def subprocess_setup():\n            # Python installs a SIGPIPE handler by default. This is usually not what\n            # non-Python subprocesses expect.\n            signal.signal(signal.SIGPIPE, signal.SIG_DFL)\n\n        return subprocess.Popen(command,\n                                stdin=self._input_pipe,\n                                stdout=subprocess.PIPE,\n                                preexec_fn=subprocess_setup,\n                                close_fds=True)", "category": "Python"}, {"instruction": "def get_vnetwork_hosts_input_datacenter(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_vnetwork_hosts = ET.Element(\"get_vnetwork_hosts\")\n        config = get_vnetwork_hosts\n        input = ET.SubElement(get_vnetwork_hosts, \"input\")\n        datacenter = ET.SubElement(input, \"datacenter\")\n        datacenter.text = kwargs.pop('datacenter')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def get_opts(opts):\n    \"\"\"\n    Validate options and apply defaults for options not supplied.\n\n    :param opts: dictionary mapping str->str.\n    :return: dictionary mapping str->Opt. All possible keys are present.\n    \"\"\"\n", "input": "", "output": "    defaults = {\n        'board': None,\n        'terrain': Opt.random,\n        'numbers': Opt.preset,\n        'ports': Opt.preset,\n        'pieces': Opt.preset,\n        'players': Opt.preset,\n    }\n    _opts = defaults.copy()\n    if opts is None:\n        opts = dict()\n    try:\n        for key, val in opts.copy().items():\n            if key == 'board':\n                # board is a string, not a regular opt, and gets special handling\n                # in _read_tiles_from_string\n                continue\n            opts[key] = Opt(val)\n        _opts.update(opts)\n    except Exception:\n        raise ValueError('Invalid options={}'.format(opts))\n    logging.debug('used defaults=\\n{}\\n on opts=\\n{}\\nreturned total opts=\\n{}'.format(\n        pprint.pformat(defaults),\n        pprint.pformat(opts),\n        pprint.pformat(_opts)))\n    return _opts", "category": "Python"}, {"instruction": "def subs2seqs(self) -> Dict[str, List[str]]:\n        \"\"\"A |collections.defaultdict| containing the node-specific\n        information provided by XML `sequences` element.\n\n        >>> from hydpy.auxs.xmltools import XMLInterface\n        >>> from hydpy import data\n        >>> interface = XMLInterface('single_run.xml', data.get_path('LahnH'))\n        >>> series_io = interface.series_io\n        >>> subs2seqs = series_io.writers[2].subs2seqs\n        >>> for subs, seq in sorted(subs2seqs.items()):\n        ...     print(subs, seq)\n        node ['sim', 'obs']\n        \"\"\"\n", "input": "", "output": "        subs2seqs = collections.defaultdict(list)\n        nodes = find(self.find('sequences'), 'node')\n        if nodes is not None:\n            for seq in nodes:\n                subs2seqs['node'].append(strip(seq.tag))\n        return subs2seqs", "category": "Python"}, {"instruction": "def main():\n    \"\"\"\n    Entry point for the Windows loopback tool.\n    \"\"\"\n", "input": "", "output": "\n    parser = argparse.ArgumentParser(description='%(prog)s add/remove Windows loopback adapters')\n    parser.add_argument('-a', \"--add\", nargs=3, action=parse_add_loopback(), help=\"add a Windows loopback adapter\")\n    parser.add_argument(\"-r\", \"--remove\", action=\"store\", help=\"remove a Windows loopback adapter\")\n    try:\n        args = parser.parse_args()\n    except argparse.ArgumentTypeError as e:\n        raise SystemExit(e)\n\n    # devcon is required to install/remove Windows loopback adapters\n    devcon_path = shutil.which(\"devcon\")\n    if not devcon_path:\n        raise SystemExit(\"Could not find devcon.exe\")\n\n    from win32com.shell import shell\n    if not shell.IsUserAnAdmin():\n        raise SystemExit(\"You must run this script as an administrator\")\n\n    try:\n        if args.add:\n            add_loopback(devcon_path, args.add[0], args.add[1], args.add[2])\n        if args.remove:\n            remove_loopback(devcon_path, args.remove)\n    except SystemExit as e:\n        print(e)\n        os.system(\"pause\")", "category": "Python"}, {"instruction": "def disable_inheritance(path, objectType, copy=True):\n    '''\n    Disable inheritance on an object\n\n    Args:\n        path: The path to the object\n        objectType: The type of object (FILE, DIRECTORY, REGISTRY)\n        copy: True will copy the Inherited ACEs to the DACL before disabling inheritance\n\n    Returns (dict): A dictionary containing the results\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt 'minion-id' win_dacl.disable_inheritance c:\\temp directory\n    '''\n", "input": "", "output": "    dc = daclConstants()\n    objectType = dc.getObjectTypeBit(objectType)\n    path = dc.processPath(path, objectType)\n\n    return _set_dacl_inheritance(path, objectType, False, copy, None)", "category": "Python"}, {"instruction": "def execute(self, query):\n        \"\"\"\n        Execute arbitrary queries on the db.\n\n        .. seealso::\n\n                :class:`FeatureDB.schema` may be helpful when writing your own\n                queries.\n\n        Parameters\n        ----------\n\n        query : str\n\n            Query to execute -- trailing \";\" optional.\n\n        Returns\n        -------\n        A sqlite3.Cursor object that can be iterated over.\n        \"\"\"\n", "input": "", "output": "        c = self.conn.cursor()\n        return c.execute(query)", "category": "Python"}, {"instruction": "def setup_layout(self, orientation=None):\n        \"\"\"Setup the layout for the tooltip in the given orientation\n\n        :param layout: the orentation of the layout\n        :type layout: QtCore.Qt.Orientation | None\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        if orientation == QtCore.Qt.Horizontal or orientation is None:\n            layout = QtGui.QHBoxLayout()\n        elif orientation == QtCore.Qt.Vertical:\n            layout = QtGui.QVBoxLayout()\n        else:\n            raise TypeError('Orientation is of wrong type! Allowed is QtCore.Qt.Horizontal and QtCore.Qt.Vertical. Given: %s' % orientation)\n        layout.setContentsMargins(0, 0, 0, 0)\n        layout.setSpacing(0)\n        self.setLayout(layout)", "category": "Python"}, {"instruction": "def gauge(self, slug, current_value):\n        \"\"\"Set the value for a Gauge.\n\n        * ``slug`` -- the unique identifier (or key) for the Gauge\n        * ``current_value`` -- the value that the gauge should display\n\n        \"\"\"\n", "input": "", "output": "        k = self._gauge_key(slug)\n        self.r.sadd(self._gauge_slugs_key, slug)  # keep track of all Gauges\n        self.r.set(k, current_value)", "category": "Python"}, {"instruction": "def main():\n    \"\"\"Entry point when running as script from commandline.\"\"\"\n", "input": "", "output": "    from docopt import docopt\n    args = docopt(__doc__)\n    infile = args['INFILE']\n    outfile = args['OUTFILE']\n    i3extract(infile, outfile)", "category": "Python"}, {"instruction": "def preview(file):\n    \"\"\"Return appropriate template and pass the file and an embed flag.\"\"\"\n", "input": "", "output": "    tree, limit_reached, error = make_tree(file)\n    list = children_to_list(tree)['children']\n    return render_template(\n        \"invenio_previewer/zip.html\",\n        file=file,\n        tree=list,\n        limit_reached=limit_reached,\n        error=error,\n        js_bundles=current_previewer.js_bundles + ['previewer_fullscreen_js'],\n        css_bundles=current_previewer.css_bundles,\n    )", "category": "Python"}, {"instruction": "def _get_title_or_id_from_uid(uid):\n    \"\"\"Returns the title or ID from the given UID\n    \"\"\"\n", "input": "", "output": "    try:\n        obj = api.get_object_by_uid(uid)\n    except api.APIError:\n        return \"<Deleted {}>\".format(uid)\n    title_or_id = api.get_title(obj) or api.get_id(obj)\n    return title_or_id", "category": "Python"}, {"instruction": "def acquire_writer(self):\n        \"\"\"\n        Acquire a write lock, only one thread can hold this lock\n        and only when no read locks are also held.\n        \"\"\"\n", "input": "", "output": "        with self.mutex:\n            while self.rwlock != 0:\n                self._writer_wait()\n            self.rwlock = -1", "category": "Python"}, {"instruction": "def get_edge_by_hash(self, edge_hash: str) -> Optional[Edge]:\n        \"\"\"Look up an edge by the hash of a PyBEL edge data dictionary.\"\"\"\n", "input": "", "output": "        return self.session.query(Edge).filter(Edge.sha512 == edge_hash).one_or_none()", "category": "Python"}, {"instruction": "def lksprob(alam):\n    \"\"\"\nComputes a Kolmolgorov-Smirnov t-test significance level.  Adapted from\nNumerical Recipies.\n\nUsage:   lksprob(alam)\n\"\"\"\n", "input": "", "output": "    fac = 2.0\n    sum = 0.0\n    termbf = 0.0\n    a2 = -2.0*alam*alam\n    for j in range(1,201):\n        term = fac*math.exp(a2*j*j)\n        sum = sum + term\n        if math.fabs(term) <= (0.001*termbf) or math.fabs(term) < (1.0e-8*sum):\n            return sum\n        fac = -fac\n        termbf = math.fabs(term)\n    return 1.0", "category": "Python"}, {"instruction": "def _fill(self, values):\n        \"\"\"Add extra values to fill the line\"\"\"\n", "input": "", "output": "        if not self._previous_line:\n            self._previous_line = values\n            return super(StackedLine, self)._fill(values)\n        new_values = values + list(reversed(self._previous_line))\n        self._previous_line = values\n        return new_values", "category": "Python"}, {"instruction": "def lookup(self, query=''):\n        \"\"\"looks up all contacts where name or address match query\"\"\"\n", "input": "", "output": "        res = []\n        query = re.compile('.*%s.*' % re.escape(query), self.reflags)\n        for name, email in self.get_contacts():\n            if query.match(name) or query.match(email):\n                res.append((name, email))\n        return res", "category": "Python"}, {"instruction": "def publish_state(self, state):\n        \"\"\"Publish thing state to AWS IoT.\n\n        Args:\n            state (dict): object state. Must be JSON serializable (i.e., not\n                have circular references).\n        \"\"\"\n", "input": "", "output": "        message = json.dumps({'state': {'reported': state}})\n        self.client.publish(self.topic, message)\n        self._state = state", "category": "Python"}, {"instruction": "def _listen_commands(self):\n        \"\"\"Monitor new updates and send them further to\n        self._respond_commands, where bot actions\n        are decided.\n        \"\"\"\n", "input": "", "output": "\n        self._last_update = None\n        update_body = {'timeout': 2}\n\n        while True:\n            latest = self._last_update\n            # increase offset to filter out older updates\n            update_body.update({'offset': latest + 1} if latest else {})\n            update_resp = self.client.get_updates(update_body)\n            update_resp.add_done_callback(self._respond_commands)\n            yield gen.sleep(5)", "category": "Python"}, {"instruction": "async def stop(self):\n        \"\"\"\n        Permanently stops the :class:`RTCRtpTransceiver`.\n        \"\"\"\n", "input": "", "output": "        await self.__receiver.stop()\n        await self.__sender.stop()\n        self.__stopped = True", "category": "Python"}, {"instruction": "def build(self, builder):\n        \"\"\"\n        Build this item\n        :param builder: \n        :return: \n        \"\"\"\n", "input": "", "output": "        params = dict(Context=self.context, Name=self.name)\n        builder.start(\"Alias\", params)\n        builder.end(\"Alias\")", "category": "Python"}, {"instruction": "def cosi_pdf(z,k=1):\n    \"\"\"Equation (11) of Morton & Winn (2014)\n    \"\"\"\n", "input": "", "output": "    return 2*k/(np.pi*np.sinh(k)) * quad(cosi_integrand,z,1,args=(k,z))[0]", "category": "Python"}, {"instruction": "def receive_device_value(self, raw_value: int):\n        \"\"\"\n        Set a new value, called from within the joystick implementation class when parsing the event queue.\n\n        :param raw_value: the raw value from the joystick hardware\n\n        :internal:\n        \"\"\"\n", "input": "", "output": "\n        new_value = self._input_to_raw_value(raw_value)\n        self.__value = new_value\n        if new_value > self.max:\n            self.max = new_value\n        elif new_value < self.min:\n            self.min = new_value", "category": "Python"}, {"instruction": "def _pywrap_tensorflow():\n  \"\"\"Provide pywrap_tensorflow access in TensorBoard.\n\n  pywrap_tensorflow cannot be accessed from tf.python.pywrap_tensorflow\n  and needs to be imported using\n  `from tensorflow.python import pywrap_tensorflow`. Therefore, we provide\n  a separate accessor function for it here.\n\n  NOTE: pywrap_tensorflow is not part of TensorFlow API and this\n  dependency will go away soon.\n\n  Returns:\n    pywrap_tensorflow import, if available.\n\n  Raises:\n    ImportError: if we couldn't import pywrap_tensorflow.\n  \"\"\"\n", "input": "", "output": "  try:\n    from tensorboard.compat import notf  # pylint: disable=g-import-not-at-top\n  except ImportError:\n    try:\n      from tensorflow.python import pywrap_tensorflow  # pylint: disable=g-import-not-at-top\n      return pywrap_tensorflow\n    except ImportError:\n      pass\n  from tensorboard.compat.tensorflow_stub import pywrap_tensorflow  # pylint: disable=g-import-not-at-top\n  return pywrap_tensorflow", "category": "Python"}, {"instruction": "def roundness(self, value):\n        \"\"\"\n        Set the roundness of the vowel.\n\n        :param str value: the value to be set\n        \"\"\"\n", "input": "", "output": "        if (value is not None) and (not value in DG_V_ROUNDNESS):\n            raise ValueError(\"Unrecognized value for roundness: '%s'\" % value)\n        self.__roundness = value", "category": "Python"}, {"instruction": "def get_interfaces(device_name=None, **kwargs):\n    '''\n    .. versionadded:: 2019.2.0\n\n    Returns interfaces for a specific device using arbitrary netbox filters\n\n    device_name\n        The name of the device, e.g., ``edge_router``\n    kwargs\n        Optional arguments to be used for filtering\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion netbox.get_interfaces edge_router name=\"et-0/0/5\"\n\n    '''\n", "input": "", "output": "    if not device_name:\n        device_name = __opts__['id']\n    netbox_device = get_('dcim', 'devices', name=device_name)\n    return filter_('dcim',\n                   'interfaces',\n                   device_id=netbox_device['id'],\n                   **kwargs)", "category": "Python"}, {"instruction": "def hdrval(cls):\n        \"\"\"Construct dictionary mapping display column title to\n        IterationStats entries.\n        \"\"\"\n", "input": "", "output": "\n        hdrmap = {'Itn': 'Iter'}\n        hdrmap.update(cls.hdrval_objfun)\n        hdrmap.update({'r': 'PrimalRsdl', 's': 'DualRsdl', u('\u03c1'): 'Rho'})\n        return hdrmap", "category": "Python"}, {"instruction": "def complete(self):\n        \"\"\"https://developers.coinbase.com/api/v2#complete-request-money\"\"\"\n", "input": "", "output": "        response = self.api_client._post(self.resource_path, 'complete')\n        return self.api_client._make_api_object(response, APIObject)", "category": "Python"}, {"instruction": "def get_molecule(self, index=0):\n        \"\"\"Get a molecule from the trajectory\n\n           Optional argument:\n            | ``index``  --  The frame index [default=0]\n        \"\"\"\n", "input": "", "output": "        return Molecule(self.numbers, self.geometries[index], self.titles[index], symbols=self.symbols)", "category": "Python"}, {"instruction": "def df_outliers(df,sensitivity = 1.5):\n    \"\"\" Finds outliers in the dataframe.\n    Parameters:\n    df - DataFrame\n        The DataFrame to analyze.\n    sensitivity - number, default 1.5\n        The value to multipy by the iter-quartile range when determining outliers. This number is used\n        for categorical data as well.\n    \"\"\"\n", "input": "", "output": "    outlier_df = df.copy()\n    dtypes = _basics.col_dtypes(df)\n    for col_name in df.columns:\n        outlier_df.loc[~outliers(df[col_name],'bool',dtypes[col_name],sensitivity),col_name] = np.nan\n    outlier_df = outlier_df.dropna(how = 'all')\n    return outlier_df", "category": "Python"}, {"instruction": "def nsx_controller_connection_addr_method(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        nsx_controller = ET.SubElement(config, \"nsx-controller\", xmlns=\"urn:brocade.com:mgmt:brocade-tunnels\")\n        name_key = ET.SubElement(nsx_controller, \"name\")\n        name_key.text = kwargs.pop('name')\n        connection_addr = ET.SubElement(nsx_controller, \"connection-addr\")\n        method = ET.SubElement(connection_addr, \"method\")\n        method.text = kwargs.pop('method')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def get_editor_style_by_name(name):\n    \"\"\"\n    Get Style class.\n    This raises `pygments.util.ClassNotFound` when there is no style with this\n    name.\n    \"\"\"\n", "input": "", "output": "    if name == 'vim':\n        vim_style = Style.from_dict(default_vim_style)\n    else:\n        vim_style = style_from_pygments_cls(get_style_by_name(name))\n\n    return merge_styles([\n        vim_style,\n        Style.from_dict(style_extensions),\n    ])", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"\n        Stop the interface\n\n        :rtype: None\n        \"\"\"\n", "input": "", "output": "        self.debug(\"()\")\n        try:\n            self.unjoin()\n            time.sleep(2)\n        except:\n            self.exception(\"Failed to leave audience\")\n        super(SensorClient, self).stop()", "category": "Python"}, {"instruction": "def parse_altitude(cls, distance, unit):\n        \"\"\"\n        Parse altitude managing units conversion\n        \"\"\"\n", "input": "", "output": "        if distance is not None:\n            distance = float(distance)\n            CONVERTERS = {\n                'km': lambda d: d,\n                'm': lambda d: units.kilometers(meters=d),\n                'mi': lambda d: units.kilometers(miles=d),\n                'ft': lambda d: units.kilometers(feet=d),\n                'nm': lambda d: units.kilometers(nautical=d),\n                'nmi': lambda d: units.kilometers(nautical=d)\n            }\n            try:\n                return CONVERTERS[unit](distance)\n            except KeyError:\n                raise NotImplementedError(\n                    'Bad distance unit specified, valid are: %r' %\n                    CONVERTERS.keys()\n                )\n        else:\n            return distance", "category": "Python"}, {"instruction": "def check_array_struct(array):\n    \"\"\"\n        Check to ensure arrays are symmetrical, for example:\n        [[1, 2, 3], [1, 2]] is invalid\n    \"\"\"\n", "input": "", "output": "\n    #If a list is transformed into a numpy array and the sub elements\n    #of this array are still lists, then numpy failed to fully convert\n    #the list, meaning it is not symmetrical.\n    try:\n        arr = np.array(array)\n    except:\n        raise HydraError(\"Array %s is not valid.\"%(array,))\n    if type(arr[0]) is list:\n        raise HydraError(\"Array %s is not valid.\"%(array,))", "category": "Python"}, {"instruction": "def create(self, name, suffix, description, default_value, display=None):\n        \"\"\"Create a new Metric\n\n        :param str name: Name of metric\n        :param str suffix: Metric unit\n        :param str description: Description of what the metric is measuring\n        :param int default_value: Default value to use when a point is added\n        :param int display: Display the chart on the status page\n        :return: Created metric data (:class:`dict`)\n\n        .. seealso:: https://docs.cachethq.io/reference#metrics\n        \"\"\"\n", "input": "", "output": "        data = ApiParams()\n        data['name'] = name\n        data['suffix'] = suffix\n        data['description'] = description\n        data['default_value'] = default_value\n        data['display'] = display\n        return self._post('metrics', data=data)['data']", "category": "Python"}, {"instruction": "def statistics(self):\n        \"\"\"\n        Access the statistics\n\n        :returns: twilio.rest.taskrouter.v1.workspace.workflow.workflow_statistics.WorkflowStatisticsList\n        :rtype: twilio.rest.taskrouter.v1.workspace.workflow.workflow_statistics.WorkflowStatisticsList\n        \"\"\"\n", "input": "", "output": "        if self._statistics is None:\n            self._statistics = WorkflowStatisticsList(\n                self._version,\n                workspace_sid=self._solution['workspace_sid'],\n                workflow_sid=self._solution['sid'],\n            )\n        return self._statistics", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"Run yala.\"\"\"\n", "input": "", "output": "        print('Yala is running. It may take several seconds...')\n        try:\n            check_call('yala setup.py tests kytos', shell=True)\n            print('No linter error found.')\n        except CalledProcessError:\n            print('Linter check failed. Fix the error(s) above and try again.')\n            sys.exit(-1)", "category": "Python"}, {"instruction": "def which(cls, cmd):\n        \"\"\"Return an absolute path of the command.\n\n        It behaves like \"which command\".\n        \"\"\"\n", "input": "", "output": "        abs_path_cmd = None\n        if sys.version_info >= (3, 3):\n            abs_path_cmd = shutil.which(cmd)\n        else:\n            abs_path_cmd = find_executable(cmd)\n        return abs_path_cmd", "category": "Python"}, {"instruction": "def run_validators(self, value):\n        \"\"\"Validate Empty values, otherwise defer to the parent\"\"\"\n", "input": "", "output": "        if value is self.Empty:\n            return\n\n        return super(OpaqueKeyField, self).run_validators(value)", "category": "Python"}, {"instruction": "def setup_update_timer(self, reset=False):\n        \"\"\"Schedule a Timer Thread.\"\"\"\n", "input": "", "output": "        _LOGGER.debug(\"Timer: firing again in %d seconds\", self._interval)\n        self.update_status_timer = threading.Timer(\n            self._interval, self.update_status, [True])\n        self.update_status_timer.setDaemon(True)\n        self.update_status_timer.start()", "category": "Python"}, {"instruction": "def from_record(cls, record, crs):\n        \"\"\"Load vector from record.\"\"\"\n", "input": "", "output": "        if 'type' not in record:\n            raise TypeError(\"The data isn't a valid record.\")\n\n        return cls(to_shape(record), crs)", "category": "Python"}, {"instruction": "def user_info(self):\n        \"\"\"\n        General user info information as returned in /account in our API\n        \"\"\"\n", "input": "", "output": "        account_uri = self.uri.split('/api/v1')[0] + '/account'\n        req = self.request(account_uri)\n        account_info = req.get()\n        user_data = HTMLParser(account_info.content)\n        return user_data", "category": "Python"}, {"instruction": "def eval_permission(self, token, resource, scope, submit_request=False):\n        \"\"\"\n        Evalutes if user has permission for scope on resource.\n\n        :param str token: client access token\n        :param str resource: resource to access\n        :param str scope: scope on resource\n        :param boolean submit_request: submit request if not allowed to access?\n        rtype: boolean\n        \"\"\"\n", "input": "", "output": "        return self.eval_permissions(\n            token=token,\n            resource_scopes_tuples=[(resource, scope)],\n            submit_request=submit_request\n        )", "category": "Python"}, {"instruction": "def assert_inbounds(num, low, high, msg='', eq=False, verbose=not util_arg.QUIET):\n    r\"\"\"\n    Args:\n        num (scalar):\n        low (scalar):\n        high (scalar):\n        msg (str):\n    \"\"\"\n", "input": "", "output": "    from utool import util_str\n    if util_arg.NO_ASSERTS:\n        return\n    passed = util_alg.inbounds(num, low, high, eq=eq)\n    if isinstance(passed, np.ndarray):\n        passflag = np.all(passed)\n    else:\n        passflag = passed\n    if not passflag:\n        failednum = num.compress(~passed) if isinstance(num, np.ndarray) else num\n        failedlow = low.compress(~passed) if isinstance(low, np.ndarray) else low\n        failedhigh = high.compress(~passed) if isinstance(high, np.ndarray) else high\n        msg_ = 'num=%r is out of bounds=(%r, %r)' % (failednum, failedlow, failedhigh)\n        raise AssertionError(msg_ + '\\n' + msg)\n    else:\n        if verbose:\n            op = '<=' if eq else '<'\n            fmtstr = 'Passed assert_inbounds: {low} {op} {num} {op} {high}'\n            print(fmtstr.format(low=low, op=op, num=util_str.truncate_str(str(num)), high=high))", "category": "Python"}, {"instruction": "def generate_host_meta(template=None, *args, **kwargs):\n    \"\"\"Generate a host-meta XRD document.\n\n    Template specific key-value pairs need to be passed as ``kwargs``, see classes.\n\n    :arg template: Ready template to fill with args, for example \"diaspora\" (optional)\n    :returns: Rendered XRD document (str)\n    \"\"\"\n", "input": "", "output": "    if template == \"diaspora\":\n        hostmeta = DiasporaHostMeta(*args, **kwargs)\n    else:\n        hostmeta = BaseHostMeta(*args, **kwargs)\n    return hostmeta.render()", "category": "Python"}, {"instruction": "def validate_value(self, value):\n        \"\"\"\n        We call validation from the underlying form field\n        \"\"\"\n", "input": "", "output": "        field = self.instance.preference.setup_field()\n        value = field.to_python(value)\n        field.validate(value)\n        field.run_validators(value)\n        return value", "category": "Python"}, {"instruction": "def get_relationship_admin_session(self):\n        \"\"\"Gets the ``OsidSession`` associated with the relationship administration service.\n\n        return: (osid.relationship.RelationshipAdminSession) - a\n                ``RelationshipAdminSession``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented - ``supports_relationship_admin()`` is\n                ``false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_relationship_admin()`` is ``true``.*\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_relationship_admin():\n            raise errors.Unimplemented()\n        # pylint: disable=no-member\n        return sessions.RelationshipAdminSession(runtime=self._runtime)", "category": "Python"}, {"instruction": "def update(self, value):\n    \"\"\"Update the mean and variance estimates.\n\n    Args:\n      value: Batch or single value tensor.\n\n    Returns:\n      Summary tensor.\n    \"\"\"\n", "input": "", "output": "    with tf.name_scope(self._name + '/update'):\n      if value.shape.ndims == self._mean.shape.ndims:\n        # Add a batch dimension if necessary.\n        value = value[None, ...]\n      count = tf.shape(value)[0]\n      with tf.control_dependencies([self._count.assign_add(count)]):\n        step = tf.cast(self._count, tf.float32)\n        mean_delta = tf.reduce_sum(value - self._mean[None, ...], 0)\n        new_mean = self._mean + mean_delta / step\n        new_mean = tf.cond(self._count > 1, lambda: new_mean, lambda: value[0])\n        var_delta = (\n            value - self._mean[None, ...]) * (value - new_mean[None, ...])\n        new_var_sum = self._var_sum + tf.reduce_sum(var_delta, 0)\n      with tf.control_dependencies([new_mean, new_var_sum]):\n        update = self._mean.assign(new_mean), self._var_sum.assign(new_var_sum)\n      with tf.control_dependencies(update):\n        if value.shape.ndims == 1:\n          value = tf.reduce_mean(value)\n        return self._summary('value', tf.reduce_mean(value))", "category": "Python"}, {"instruction": "def writefile(openedfile, newcontents):\n    \"\"\"Set the contents of a file.\"\"\"\n", "input": "", "output": "    openedfile.seek(0)\n    openedfile.truncate()\n    openedfile.write(newcontents)", "category": "Python"}, {"instruction": "async def close_async(self):\n        \"\"\"Close async connection.\n        \"\"\"\n", "input": "", "output": "        if self._async_wait:\n            await self._async_wait\n        if self._async_conn:\n            conn = self._async_conn\n            self._async_conn = None\n            self._async_wait = None\n            self._task_data = None\n            await conn.close()", "category": "Python"}, {"instruction": "def bind_env(self, action, env):\n        \"\"\" Bind an environment variable to an argument action.  The env\n        value will traditionally be something uppercase like `MYAPP_FOO_ARG`.\n\n        Note that the ENV value is assigned using `set_defaults()` and as such\n        it will be overridden if the argument is set via `parse_args()` \"\"\"\n", "input": "", "output": "        if env in self._env_actions:\n            raise ValueError('Duplicate ENV variable: %s' % env)\n        self._env_actions[env] = action\n        action.env = env", "category": "Python"}, {"instruction": "def get_cache_time(\n        self, path: str, modified: Optional[datetime.datetime], mime_type: str\n    ) -> int:\n        \"\"\"Override to customize cache control behavior.\n\n        Return a positive number of seconds to make the result\n        cacheable for that amount of time or 0 to mark resource as\n        cacheable for an unspecified amount of time (subject to\n        browser heuristics).\n\n        By default returns cache expiry of 10 years for resources requested\n        with ``v`` argument.\n        \"\"\"\n", "input": "", "output": "        return self.CACHE_MAX_AGE if \"v\" in self.request.arguments else 0", "category": "Python"}, {"instruction": "def get_all_accounts(bucket, region='us-west-2', json_path='accounts.json', **filters):\n    \"\"\"Fetches all the accounts from SWAG.\"\"\"\n", "input": "", "output": "    swag_opts = {\n        'swag.type': 's3',\n        'swag.bucket_name': bucket,\n        'swag.bucket_region': region,\n        'swag.data_file': json_path,\n        'swag.schema_version': 1\n    }\n\n    swag = SWAGManager(**parse_swag_config_options(swag_opts))\n    accounts = swag.get_all()\n    accounts = [account for account in accounts['accounts'] if is_sub_dict(filters, account)]\n    return {'accounts': accounts}", "category": "Python"}, {"instruction": "def _norm_default(x):\n    \"\"\"Default Euclidean norm implementation.\"\"\"\n", "input": "", "output": "    # Lazy import to improve `import odl` time\n    import scipy.linalg\n\n    if _blas_is_applicable(x.data):\n        nrm2 = scipy.linalg.blas.get_blas_funcs('nrm2', dtype=x.dtype)\n        norm = partial(nrm2, n=native(x.size))\n    else:\n        norm = np.linalg.norm\n    return norm(x.data.ravel())", "category": "Python"}, {"instruction": "async def _create_connection(self, req: 'ClientRequest',\n                                 traces: List['Trace'],\n                                 timeout: 'ClientTimeout') -> ResponseHandler:\n        \"\"\"Create connection.\n\n        Has same keyword arguments as BaseEventLoop.create_connection.\n        \"\"\"\n", "input": "", "output": "        if req.proxy:\n            _, proto = await self._create_proxy_connection(\n                req, traces, timeout)\n        else:\n            _, proto = await self._create_direct_connection(\n                req, traces, timeout)\n\n        return proto", "category": "Python"}, {"instruction": "def get_prefixed_config(self, section, option, **kwargs):\n        \"\"\"\n        TODO.\n        \"\"\"\n", "input": "", "output": "        cfg = Config.instance()\n        default = cfg.get_expanded(section, option, **kwargs)\n        return cfg.get_expanded(section, \"{}_{}\".format(self.workflow_type, option),\n            default=default, **kwargs)", "category": "Python"}, {"instruction": "def get_csv_from_metadata(dsn, d):\n    \"\"\"\n    Two goals. Get all csv from metadata, and return new metadata with generated filenames to match files.\n\n    :param str dsn: Dataset name\n    :param dict d: Metadata\n    :return dict _csvs: Csv\n    \"\"\"\n", "input": "", "output": "    logger_csvs.info(\"enter get_csv_from_metadata\")\n    _csvs = OrderedDict()\n    _d = copy.deepcopy(d)\n\n    try:\n        if \"paleoData\" in _d:\n            # Process paleoData section\n            _d[\"paleoData\"], _csvs = _get_csv_from_section(_d[\"paleoData\"], \"{}.paleo\".format(dsn), _csvs)\n\n        if \"chronData\" in _d:\n            _d[\"chronData\"], _csvs = _get_csv_from_section(_d[\"chronData\"], \"{}.chron\".format(dsn), _csvs)\n\n    except Exception as e:\n        print(\"Error: get_csv_from_metadata: {}, {}\".format(dsn, e))\n        logger_csvs.error(\"get_csv_from_metadata: {}, {}\".format(dsn, e))\n\n    logger_csvs.info(\"exit get_csv_from_metadata\")\n    return _d, _csvs", "category": "Python"}, {"instruction": "def get_values(js_dict, value='value'):\n    \"\"\"Get values from input data.\n\n    Args:\n      js_dict (dict): dictionary containing dataset data and metadata.\n      value (string, optional): name of the value column. Defaults to 'value'.\n\n    Returns:\n      values (list): list of dataset values.\n\n    \"\"\"\n", "input": "", "output": "\n    values = js_dict[value]\n    if type(values) is list:\n        if type(values[0]) is not dict or tuple:\n            return values\n    # being not a list of dicts or tuples leaves us with a dict...\n    values = {int(key): value for (key, value) in values.items()}\n\n    if js_dict.get('size'):\n        max_val = np.prod(np.array((js_dict['size'])))\n    else:\n        max_val = np.prod(np.array((js_dict['dimension']['size'])))\n    vals = max_val * [None]\n    for (key, value) in values.items():\n        vals[key] = value\n\n    values = vals\n    return values", "category": "Python"}, {"instruction": "def _derX(self,x,y):\n        '''\n        Returns the derivative with respect to x of the interpolated function\n        at each value in x,y. Only called internally by HARKinterpolator2D.derivativeX.\n        '''\n", "input": "", "output": "        if _isscalar(x):\n            x_pos = max(min(self.xSearchFunc(self.x_list,x),self.x_n-1),1)\n            y_pos = max(min(self.ySearchFunc(self.y_list,y),self.y_n-1),1)\n        else:\n            x_pos = self.xSearchFunc(self.x_list,x)\n            x_pos[x_pos < 1] = 1\n            x_pos[x_pos > self.x_n-1] = self.x_n-1\n            y_pos = self.ySearchFunc(self.y_list,y)\n            y_pos[y_pos < 1] = 1\n            y_pos[y_pos > self.y_n-1] = self.y_n-1\n        beta = (y - self.y_list[y_pos-1])/(self.y_list[y_pos] - self.y_list[y_pos-1])\n        dfdx = (\n              ((1-beta)*self.f_values[x_pos,y_pos-1]\n            +  beta*self.f_values[x_pos,y_pos]) -\n              ((1-beta)*self.f_values[x_pos-1,y_pos-1]\n            +  beta*self.f_values[x_pos-1,y_pos]))/(self.x_list[x_pos] - self.x_list[x_pos-1])\n        return dfdx", "category": "Python"}, {"instruction": "def push(self, value: Union[int, bytes]) -> None:\n        \"\"\"\n        Push an item onto the stack.\n        \"\"\"\n", "input": "", "output": "        if len(self.values) > 1023:\n            raise FullStack('Stack limit reached')\n\n        validate_stack_item(value)\n\n        self.values.append(value)", "category": "Python"}, {"instruction": "def _handle_id(self, node, scope, ctxt, stream):\n        \"\"\"Handle an ID node (return a field object for the ID)\n\n        :node: TODO\n        :scope: TODO\n        :ctxt: TODO\n        :stream: TODO\n        :returns: TODO\n\n        \"\"\"\n", "input": "", "output": "        if node.name == \"__root\":\n            return self._root\n        if node.name == \"__this\" or node.name == \"this\":\n            return ctxt\n\n        self._dlog(\"handling id {}\".format(node.name))\n        field = scope.get_id(node.name)\n\n        is_lazy = getattr(node, \"is_lazy\", False)\n\n        if field is None and not is_lazy:\n            raise errors.UnresolvedID(node.coord, node.name)\n        elif is_lazy:\n            return LazyField(node.name, scope)\n\n        return field", "category": "Python"}, {"instruction": "def get_match(self, match_id):\n        \"\"\"Get a multiplayer match.\n\n        Parameters\n        ----------\n        match_id\n            The ID of the match to retrieve. This is the ID that you see in a online multiplayer match summary.\n            This does not correspond the in-game game ID.\"\"\"\n", "input": "", "output": "        return self._make_req(endpoints.MATCH, dict(\n            k=self.key,\n            mp=match_id), Match)", "category": "Python"}, {"instruction": "def create_from_axis_angle(cls, angle, ax, ay, az, degrees=False):\n        \"\"\" Classmethod to create a quaternion from an axis-angle representation. \n        (angle should be in radians).\n        \"\"\"\n", "input": "", "output": "        if degrees:\n            angle = np.radians(angle)\n        while angle < 0:\n            angle += np.pi*2\n        angle2 = angle/2.0\n        sinang2 = np.sin(angle2)\n        return Quaternion(np.cos(angle2), ax*sinang2, ay*sinang2, az*sinang2)", "category": "Python"}, {"instruction": "def add_tracked_motors(self, tracked_motors):\n        \"\"\"Add new motors to the recording\"\"\"\n", "input": "", "output": "        new_mockup_motors = map(self.get_mockup_motor, tracked_motors)\n        self.tracked_motors = list(set(self.tracked_motors + new_mockup_motors))", "category": "Python"}, {"instruction": "def _GetDebuggee(self):\n    \"\"\"Builds the debuggee structure.\"\"\"\n", "input": "", "output": "    major_version = 'v' + version.__version__.split('.')[0]\n    python_version = ''.join(platform.python_version().split('.')[:2])\n    agent_version = ('google.com/python%s-gcp/%s' % (python_version,\n                                                     major_version))\n\n    debuggee = {\n        'project': self._project_number,\n        'description': self._GetDebuggeeDescription(),\n        'labels': self._debuggee_labels,\n        'agentVersion': agent_version,\n    }\n\n    source_context = self._ReadAppJsonFile('source-context.json')\n    if source_context:\n      debuggee['sourceContexts'] = [source_context]\n\n    debuggee['uniquifier'] = self._ComputeUniquifier(debuggee)\n\n    return debuggee", "category": "Python"}, {"instruction": "def update_repository_configuration(id, external_repository=None, prebuild_sync=None):\n    \"\"\"\n    Update an existing RepositoryConfiguration with new information\n    \"\"\"\n", "input": "", "output": "    to_update_id = id\n\n    rc_to_update = pnc_api.repositories.get_specific(id=to_update_id).content\n\n    if external_repository is None:\n        external_repository = rc_to_update.external_url\n    else:\n        rc_to_update.external_url = external_repository\n\n    if prebuild_sync is not None:\n        rc_to_update.pre_build_sync_enabled = prebuild_sync\n\n    if not external_repository and prebuild_sync:\n        logging.error(\"You cannot enable prebuild sync without external repository\")\n        return\n\n    response = utils.checked_api_call(pnc_api.repositories, 'update', id=to_update_id, body=rc_to_update)\n    if response:\n        return response.content", "category": "Python"}, {"instruction": "def from_dict(config):\n        '''\n        Instantiate a new ProxyConfig from a dictionary that represents a\n        client configuration, as described in `the documentation`_.\n\n        .. _the documentation:\n            https://docs.docker.com/network/proxy/#configure-the-docker-client\n        '''\n", "input": "", "output": "        return ProxyConfig(\n            http=config.get('httpProxy'),\n            https=config.get('httpsProxy'),\n            ftp=config.get('ftpProxy'),\n            no_proxy=config.get('noProxy'),\n        )", "category": "Python"}, {"instruction": "def detect_functions_called(contract):\n        \"\"\" Returns a list of InternallCall, SolidityCall\n            calls made in a function\n\n        Returns:\n            (list): List of all InternallCall, SolidityCall\n        \"\"\"\n", "input": "", "output": "        result = []\n\n        # Obtain all functions reachable by this contract.\n        for func in contract.all_functions_called:\n            # Loop through all nodes in the function, add all calls to a list.\n            for node in func.nodes:\n                for ir in node.irs:\n                    if isinstance(ir, (InternalCall, SolidityCall)):\n                        result.append(ir.function)\n        return result", "category": "Python"}, {"instruction": "def to_int(value: Any) -> int:\n        \"\"\"\n        Cast a value as its equivalent int for indy predicate argument. Raise ValueError for any input but\n        int, stringified int, or boolean.\n\n        :param value: value to coerce.\n        \"\"\"\n", "input": "", "output": "\n        if isinstance(value, (bool, int)):\n            return int(value)\n        return int(str(value))", "category": "Python"}, {"instruction": "def _pad(self, b):\n        \"\"\"\n        Will padd the param to be of the correct length for the encryption alg.\n\n        :type b: bytes\n        :rtype: bytes\n        \"\"\"\n", "input": "", "output": "        return b + (self.bs - len(b) % self.bs) * chr(self.bs - len(b) % self.bs).encode(\"UTF-8\")", "category": "Python"}, {"instruction": "def _check_source(cls, source_file_hash, source):\n        \"\"\"Checks if a pre-trained token embedding source name is valid.\n\n\n        Parameters\n        ----------\n        source : str\n            The pre-trained token embedding source.\n        \"\"\"\n", "input": "", "output": "        embedding_name = cls.__name__.lower()\n        if source not in source_file_hash:\n            raise KeyError('Cannot find pre-trained source {} for token embedding {}. '\n                           'Valid pre-trained file names for embedding {}: {}'.format(\n                               source, embedding_name, embedding_name,\n                               ', '.join(source_file_hash.keys())))", "category": "Python"}, {"instruction": "async def get_scene(self, scene_id, from_cache=True) -> Scene:\n        \"\"\"Get a scene resource instance.\n\n        :raises a ResourceNotFoundException when no scene found.\n        :raises a PvApiError when something is wrong with the hub.\n        \"\"\"\n", "input": "", "output": "        if not from_cache:\n            await self.get_scenes()\n        for _scene in self.scenes:\n            if _scene.id == scene_id:\n                return _scene\n        raise ResourceNotFoundException(\"Scene not found scene_id: {}\".format(scene_id))", "category": "Python"}, {"instruction": "def extract_lrzip (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Extract a LRZIP archive.\"\"\"\n", "input": "", "output": "    cmdlist = [cmd, '-d']\n    if verbosity > 1:\n        cmdlist.append('-v')\n    outfile = util.get_single_outfile(outdir, archive)\n    cmdlist.extend([\"-o\", outfile, os.path.abspath(archive)])\n    return cmdlist", "category": "Python"}, {"instruction": "def guess_path_encoding(file_path, default=DEFAULT_ENCODING):\n    \"\"\"Wrapper to open that damn file for you, lazy bastard.\"\"\"\n", "input": "", "output": "    with io.open(file_path, 'rb') as fh:\n        return guess_file_encoding(fh, default=default)", "category": "Python"}, {"instruction": "def get_start_time(self):\n        \"\"\"Gets the start time for this assessment.\n\n        return: (osid.calendaring.DateTime) - the designated start time\n        raise:  IllegalState - ``has_start_time()`` is ``false``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for osid.assessment.AssessmentOffered.get_start_time_template\n        if not bool(self._my_map['startTime']):\n            raise errors.IllegalState()\n        dt = self._my_map['startTime']\n        return DateTime(dt.year, dt.month, dt.day, dt.hour, dt.minute, dt.second, dt.microsecond)", "category": "Python"}, {"instruction": "def expand_mapping_target(namespaces, val):\n    \"\"\"Expand a mapping target, expressed as a comma-separated list of\n    CURIE-like strings potentially prefixed with ^ to express inverse\n    properties, into a list of (uri, inverse) tuples, where uri is a URIRef\n    and inverse is a boolean.\"\"\"\n", "input": "", "output": "\n    vals = [v.strip() for v in val.split(',')]\n    ret = []\n    for v in vals:\n        inverse = False\n        if v.startswith('^'):\n            inverse = True\n            v = v[1:]\n        ret.append((expand_curielike(namespaces, v), inverse))\n    return ret", "category": "Python"}, {"instruction": "def clean_expired_user_attempts(attempt_time: datetime = None) -> int:\n    \"\"\"\n    Clean expired user attempts from the database.\n    \"\"\"\n", "input": "", "output": "\n    if settings.AXES_COOLOFF_TIME is None:\n        log.debug('AXES: Skipping clean for expired access attempts because no AXES_COOLOFF_TIME is configured')\n        return 0\n\n    threshold = get_cool_off_threshold(attempt_time)\n    count, _ = AccessAttempt.objects.filter(attempt_time__lt=threshold).delete()\n    log.info('AXES: Cleaned up %s expired access attempts from database that were older than %s', count, threshold)\n    return count", "category": "Python"}, {"instruction": "def immediateAssignmentReject():\n    \"\"\"IMMEDIATE ASSIGNMENT REJECT Section 9.1.20\"\"\"\n", "input": "", "output": "    a = L2PseudoLength(l2pLength=0x13)\n    b = TpPd(pd=0x6)\n    c = MessageType(mesType=0x3a)  # 00111010\n    d = PageModeAndSpareHalfOctets()\n    f = RequestReference()\n    g = WaitIndication()\n    h = RequestReference()\n    i = WaitIndication()\n    j = RequestReference()\n    k = WaitIndication()\n    l = RequestReference()\n    m = WaitIndication()\n    n = IraRestOctets()\n    packet = a / b / c / d / f / g / h / i / j / k / l / m / n\n    return packet", "category": "Python"}, {"instruction": "def discrete_index(self, indices):\n        \"\"\"get elements by discrete indices\n\n        :param indices: list\n            discrete indices\n        :return: elements\n        \"\"\"\n", "input": "", "output": "        elements = []\n        for i in indices:\n            elements.append(self[i])\n        return elements", "category": "Python"}, {"instruction": "def allreduce_grads(self):\n        \"\"\"For each parameter, reduce the gradients from different contexts.\n\n        Should be called after `autograd.backward()`, outside of `record()` scope,\n        and before `trainer.update()`.\n\n        For normal parameter updates, `step()` should be used, which internally calls\n        `allreduce_grads()` and then `update()`. However, if you need to get the reduced\n        gradients to perform certain transformation, such as in gradient clipping, then\n        you may want to manually call `allreduce_grads()` and `update()` separately.\n        \"\"\"\n", "input": "", "output": "        if not self._kv_initialized:\n            self._init_kvstore()\n        if self._params_to_init:\n            self._init_params()\n        assert not (self._kvstore and self._update_on_kvstore), \\\n                'allreduce_grads() when parameters are updated on kvstore ' \\\n                'is not supported. Try setting `update_on_kvstore` ' \\\n                'to False when creating trainer.'\n\n        self._allreduce_grads()", "category": "Python"}, {"instruction": "def pixel_data(self):\n        \"\"\"\n        Returns the pixel data stored in the Image object.\n\n        Returns\n        -------\n        out : numpy.array\n            The pixel data of the Image object. It returns a multi-dimensional\n            numpy array, where the shape of the array represents the shape of\n            the image (height, weight, channels).\n\n        See Also\n        --------\n        width, channels, height\n\n        Examples\n        --------\n        >>> img = turicreate.Image('https://static.turi.com/datasets/images/sample.jpg')\n        >>> image_array = img.pixel_data\n        \"\"\"\n", "input": "", "output": "\n        from .. import extensions as _extensions\n        data = _np.zeros((self.height, self.width, self.channels), dtype=_np.uint8)\n        _extensions.image_load_to_numpy(self, data.ctypes.data, data.strides)\n        if self.channels == 1:\n            data = data.squeeze(2)\n        return data", "category": "Python"}, {"instruction": "def get_user_token():\n    \"\"\"Return the authenticated user's auth token\"\"\"\n", "input": "", "output": "    if not hasattr(stack.top, 'current_user'):\n        return ''\n    current_user = stack.top.current_user\n    return current_user.get('token', '')", "category": "Python"}, {"instruction": "def _convert_credentials(token_url, username=None, password=None, refresh_token=None):\n    \"\"\"\n    Converts username/password credentials to token credentials by\n    using Yamcs as the authenticaton server.\n    \"\"\"\n", "input": "", "output": "    if username and password:\n        data = {'grant_type': 'password', 'username': username, 'password': password}\n    elif refresh_token:\n        data = {'grant_type': 'refresh_token', 'refresh_token': refresh_token}\n    else:\n        raise NotImplementedError()\n\n    response = requests.post(token_url, data=data)\n    if response.status_code == 401:\n        raise Unauthorized('401 Client Error: Unauthorized')\n    elif response.status_code == 200:\n        d = response.json()\n        expiry = datetime.utcnow() + timedelta(seconds=d['expires_in'])\n        return Credentials(access_token=d['access_token'],\n                           refresh_token=d['refresh_token'],\n                           expiry=expiry)\n    else:\n        raise YamcsError('{} Server Error'.format(response.status_code))", "category": "Python"}, {"instruction": "def agent_updated(self, context, admin_state_up, host):\n        \"\"\"Updates cfg agent on <host> to enable or disable it.\"\"\"\n", "input": "", "output": "        self._host_notification(context, 'agent_updated',\n                                {'admin_state_up': admin_state_up}, host)", "category": "Python"}, {"instruction": "def hist_overflow(val, val_max, **kwds):\n    \"\"\" Make a histogram with an overflow bar above val_max \"\"\"\n", "input": "", "output": "    import pylab, numpy\n\n    overflow = len(val[val>=val_max])\n    pylab.hist(val[val<val_max], **kwds)\n\n    if 'color' in kwds:\n        color = kwds['color']\n    else:\n        color = None\n\n    if overflow > 0:\n        rect = pylab.bar(val_max+0.05, overflow, .5, color=color)[0]\n        pylab.text(rect.get_x(),\n                   1.10*rect.get_height(), '%s+' % val_max)", "category": "Python"}, {"instruction": "def errback(self, result):\n        \"\"\"Begin the callback chain with the first errback.\n\n        result -- A BaseException derivative.\n\n        \"\"\"\n", "input": "", "output": "\n        assert(isinstance(result, BaseException))\n        self._start_callbacks(result, True)", "category": "Python"}, {"instruction": "def if_then_else(cls,\n            condition: 'TensorFluent',\n            true_case: 'TensorFluent',\n            false_case: 'TensorFluent') -> 'TensorFluent':\n        '''Returns a TensorFluent for the control op if-then-else.\n\n        Args:\n            condition: Boolean fluent for the if condition.\n            true_case: Fluent returned in the true clause.\n            false_case: Fluent returned in the false clause.\n\n        Returns:\n            A TensorFluent wrapping the if-then-else control statement.\n\n        Raises:\n            ValueError: If cases don't have same shape.\n        '''\n", "input": "", "output": "        true = TensorFluent.constant(True, tf.bool)\n        false = TensorFluent.constant(False, tf.bool)\n        ite = (condition == true) * true_case + (condition == false) * false_case\n        if true_case.dtype == tf.bool and false_case.dtype == tf.bool:\n            ite = ite.cast(tf.bool)\n        return ite", "category": "Python"}, {"instruction": "def upgrade_thirdparty_tools(args, remotes):\n    \"\"\"Install and update third party tools used in the pipeline.\n\n    Creates a manifest directory with installed programs on the system.\n    \"\"\"\n", "input": "", "output": "    cbl = get_cloudbiolinux(remotes)\n    if args.toolconf and os.path.exists(args.toolconf):\n        package_yaml = args.toolconf\n    else:\n        package_yaml = os.path.join(cbl[\"dir\"], \"contrib\", \"flavor\",\n                                    \"ngs_pipeline_minimal\", \"packages-conda.yaml\")\n    sys.path.insert(0, cbl[\"dir\"])\n    cbl_conda = __import__(\"cloudbio.package.conda\", fromlist=[\"conda\"])\n    cbl_conda.install_in(_get_conda_bin(), args.tooldir, package_yaml)\n    manifest_dir = os.path.join(_get_data_dir(), \"manifest\")\n    print(\"Creating manifest of installed packages in %s\" % manifest_dir)\n    cbl_manifest = __import__(\"cloudbio.manifest\", fromlist=[\"manifest\"])\n    if os.path.exists(manifest_dir):\n        for fname in os.listdir(manifest_dir):\n            if not fname.startswith(\"toolplus\"):\n                os.remove(os.path.join(manifest_dir, fname))\n    cbl_manifest.create(manifest_dir, args.tooldir)", "category": "Python"}, {"instruction": "def Decrypt(self, encrypted_data):\n    \"\"\"Decrypts the encrypted data.\n\n    Args:\n      encrypted_data (bytes): encrypted data.\n\n    Returns:\n      tuple[bytes, bytes]: decrypted data and remaining encrypted data.\n    \"\"\"\n", "input": "", "output": "    index_split = -(len(encrypted_data) % DES3.block_size)\n    if index_split:\n      remaining_encrypted_data = encrypted_data[index_split:]\n      encrypted_data = encrypted_data[:index_split]\n    else:\n      remaining_encrypted_data = b''\n\n    decrypted_data = self._des3_cipher.decrypt(encrypted_data)\n\n    return decrypted_data, remaining_encrypted_data", "category": "Python"}, {"instruction": "def config(ctx):\n    \"\"\"Show access token and other configuration settings.\n\n    The access token and command verbosity level can be set on the\n    command line, as environment variables, and in mapbox.ini config\n    files.\n    \"\"\"\n", "input": "", "output": "    ctx.default_map = ctx.obj['cfg']\n    click.echo(\"CLI:\")\n    click.echo(\"access-token = {0}\".format(ctx.obj['access_token']))\n    click.echo(\"verbosity = {0}\".format(ctx.obj['verbosity']))\n    click.echo(\"\")\n\n    click.echo(\"Environment:\")\n    if 'MAPBOX_ACCESS_TOKEN' in os.environ:\n        click.echo(\"MAPBOX_ACCESS_TOKEN = {0}\".format(\n            os.environ['MAPBOX_ACCESS_TOKEN']))\n    if 'MapboxAccessToken' in os.environ:\n        click.echo(\"MapboxAccessToken = {0}\".format(\n            os.environ['MapboxAccessToken']))\n    if 'MAPBOX_VERBOSE' in os.environ:\n        click.echo(\"MAPBOX_VERBOSE = {0}\".format(\n            os.environ['MAPBOX_VERBOSE']))\n    click.echo(\"\")\n\n    if 'config_file' in ctx.obj:\n        click.echo(\"Config file {0}:\".format(ctx.obj['config_file']))\n        for key, value in ctx.default_map.items():\n            click.echo(\"{0} = {1}\".format(key, value))\n        click.echo(\"\")", "category": "Python"}, {"instruction": "def pick_kmersize(fq):\n    \"\"\"\n    pick an appropriate kmer size based off of https://www.biostars.org/p/201474/\n    tl;dr version: pick 31 unless the reads are very small, if not then guess\n    that readlength / 2 is about right.\n    \"\"\"\n", "input": "", "output": "    if bam.is_bam(fq):\n        readlength = bam.estimate_read_length(fq)\n    else:\n        readlength = fastq.estimate_read_length(fq)\n    halfread = int(round(readlength / 2))\n    if halfread >= 31:\n        kmersize = 31\n    else:\n        kmersize = halfread\n    if kmersize % 2 == 0:\n        kmersize += 1\n    return kmersize", "category": "Python"}, {"instruction": "def _call_config(self, method, *args, **kwargs):\n        \"\"\"Call the configuration at the given method which must take a section name\n        as first argument\"\"\"\n", "input": "", "output": "        return getattr(self._config, method)(self._section_name, *args, **kwargs)", "category": "Python"}, {"instruction": "def channels_add_owner(self, room_id, user_id=None, username=None, **kwargs):\n        \"\"\"Gives the role of owner for a user in the current channel.\"\"\"\n", "input": "", "output": "        if user_id:\n            return self.__call_api_post('channels.addOwner', roomId=room_id, userId=user_id, kwargs=kwargs)\n        elif username:\n            return self.__call_api_post('channels.addOwner', roomId=room_id, username=username, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('userID or username required')", "category": "Python"}, {"instruction": "def infer_location(\n            self,\n            location_query,\n            max_distance,\n            google_key,\n            foursquare_client_id,\n            foursquare_client_secret,\n            limit\n        ):\n        \"\"\"In-place location inferring\n\n        See infer_location function\n\n        Args:\n        Returns:\n            :obj:`Segment`: self\n        \"\"\"\n", "input": "", "output": "\n        self.location_from = infer_location(\n            self.points[0],\n            location_query,\n            max_distance,\n            google_key,\n            foursquare_client_id,\n            foursquare_client_secret,\n            limit\n        )\n        self.location_to = infer_location(\n            self.points[-1],\n            location_query,\n            max_distance,\n            google_key,\n            foursquare_client_id,\n            foursquare_client_secret,\n            limit\n        )\n\n        return self", "category": "Python"}, {"instruction": "def get_setup_version(location, reponame, pkgname=None, archive_commit=None):\n    \"\"\"Helper for use in setup.py to get the current version from either\n    git describe or the .version file (if available).\n\n    Set pkgname to the package name if it is different from the\n    repository name.\n\n    To ensure git information is included in a git archive, add\n    setup.py to .gitattributes (in addition to __init__):\n    ```\n    __init__.py export-subst\n    setup.py export-subst\n    ```\n    Then supply \"$Format:%h$\" for archive_commit.\n\n    \"\"\"\n", "input": "", "output": "    import warnings\n    pkgname = reponame if pkgname is None else pkgname\n    if archive_commit is None:\n        warnings.warn(\"No archive commit available; git archives will not contain version information\")\n    return Version.setup_version(os.path.dirname(os.path.abspath(location)),reponame,pkgname=pkgname,archive_commit=archive_commit)", "category": "Python"}, {"instruction": "def get_authentic_node_name(self, node_name: str) -> Optional[str]:\r\n        \"\"\"\r\n        Returns the exact, authentic node name for the given node name if a node corresponding to\r\n        the given name exists in the graph (maybe not locally yet) or `None` otherwise.\r\n\r\n        By default, this method checks whether a node with the given name exists locally in the\r\n        graph and return `node_name` if it does or `None` otherwise.\r\n\r\n        In `Graph` extensions that are used by applications where the user can enter potentially\r\n        incorrect node names, this method should be overridden to improve usability.\r\n\r\n        Arguments:\r\n            node_name (str): The node name to return the authentic node name for.\r\n\r\n        Returns:\r\n            The authentic name of the node corresponding to the given node name or\r\n            `None` if no such node exists.\r\n        \"\"\"\n", "input": "", "output": "        node: Node = self._nodes.get_node_by_name(node_name)\r\n        return node.name if node is not None else None", "category": "Python"}, {"instruction": "def to_frame(self):\n        \"\"\"\n        Return transactions as a pandas DataFrame.\n\n        \"\"\"\n", "input": "", "output": "        col_names = _column_names_from_metadata(\n            t.metadata for t in self.transactions)\n\n        def trow(t):\n            return tz.concatv(\n                (t.amount, t.subaccount),\n                (t.metadata.get(c) for c in col_names))\n        rows = [trow(t) for t in self.transactions]\n\n        if len(rows) == 0:\n            return pd.DataFrame(columns=COLS + col_names)\n\n        return pd.DataFrame(rows, columns=COLS + col_names)", "category": "Python"}, {"instruction": "def to_unicode(data, encoding=Constants.default_codec, errors=Constants.codec_error):\n    \"\"\"\n    Converts given data to unicode string using package default settings, fighting **The Hell**!\n\n    Usage::\n\n        >>> to_unicode(\"myData\")\n        u'myData'\n        >>> to_unicode(\"\u6c49\u5b57/\u6f22\u5b57\")\n        u'\\u6c49\\u5b57/\\u6f22\\u5b57'\n\n    :param data: Data to convert.\n    :type data: object\n    :param encoding: File encoding codec.\n    :type encoding: unicode\n    :param errors: File encoding errors handling.\n    :type errors: unicode\n    :return: Unicode data.\n    :rtype: unicode\n    \"\"\"\n", "input": "", "output": "\n    if isinstance(data, type(\"\")):\n        return data\n    else:\n        try:\n            return unicode(data, encoding, errors)\n        except TypeError:\n            return unicode(str(data), encoding, errors)", "category": "Python"}, {"instruction": "def _check_version():\n    \"\"\"Check renku version.\"\"\"\n", "input": "", "output": "    from ._config import APP_NAME\n\n    if VersionCache.load(APP_NAME).is_fresh:\n        return\n\n    from pkg_resources import parse_version\n    from renku.version import __version__\n\n    version = parse_version(__version__)\n    allow_prereleases = version.is_prerelease\n\n    latest_version = find_latest_version(\n        'renku', allow_prereleases=allow_prereleases\n    )\n\n    if version < latest_version:\n        click.secho(\n            'You are using renku version {version}, however version '\n            '{latest_version} is available.\\n'\n            'You should consider upgrading ...'.format(\n                version=__version__,\n                latest_version=latest_version,\n            ),\n            fg='yellow',\n            bold=True,\n        )\n\n    VersionCache(pypi_version=str(latest_version)).dump(APP_NAME)", "category": "Python"}, {"instruction": "def redshift(self, z):\n        \"\"\"Apply :ref:`redshift <pysynphot-redshift>` to the spectrum.\n\n        Redshifted spectrum is never analytic even if the input\n        spectrum is. Output units are always Angstrom and PHOTLAM\n        regardless of user units.\n\n        Parameters\n        ----------\n        z : number\n            Redshift value.\n\n        Returns\n        -------\n        copy : `ArraySourceSpectrum`\n            Redshifted spectrum.\n\n        \"\"\"\n", "input": "", "output": "        # By default, apply only the doppler shift.\n\n        waveunits = self.waveunits\n        fluxunits = self.fluxunits\n        self.convert('angstrom')\n        self.convert('photlam')\n        newwave = self.wave.astype(N.float64) * (1.0 + z)\n        copy = ArraySourceSpectrum(wave=newwave,\n                                   flux=self.flux,\n                                   waveunits=self.waveunits,\n                                   fluxunits=self.fluxunits,\n                                   name=\"%s at z=%g\" % (self.name, z))\n\n        self.convert(waveunits)\n        self.convert(fluxunits)\n        return copy", "category": "Python"}, {"instruction": "def result(self, *args, **kwargs):\n        \"\"\"\n        Construye la consulta SQL\n        \"\"\"\n", "input": "", "output": "        prettify = kwargs.get('pretty', False)\n\n        sql = 'CREATE %s %s' % (self._type, self._class)\n        \n        if prettify:\n            sql += '\\n'\n        else:\n            sql += ' '\n\n        if self._type.lower() == 'edge':\n            sql += \" FROM %s TO %s \" % (self._from, self._to)\n        \n        if self._cluster:\n            sql += 'CLUSTER %s' % self._cluster\n            if prettify:\n                sql += '\\n'\n            else:\n                sql += ' '\n        \n        if self.data:\n            sql += 'CONTENT ' + json.dumps(self.data)\n        return sql", "category": "Python"}, {"instruction": "def authenticate(self, provider):\n        \"\"\"\n        Starts OAuth authorization flow, will redirect to 3rd party site.\n        \"\"\"\n", "input": "", "output": "        callback_url = url_for(\".callback\", provider=provider, _external=True)\n        provider = self.get_provider(provider)\n        session['next'] = request.args.get('next') or ''\n        return provider.authorize(callback_url)", "category": "Python"}, {"instruction": "def concatenate(self, other):\n        \"\"\"Return a new name which is the concatenation of self and other.\n        @rtype: dns.name.Name object\n        @raises AbsoluteConcatenation: self is absolute and other is\n        not the empty name\n        \"\"\"\n", "input": "", "output": "\n        if self.is_absolute() and len(other) > 0:\n            raise AbsoluteConcatenation\n        labels = list(self.labels)\n        labels.extend(list(other.labels))\n        return Name(labels)", "category": "Python"}, {"instruction": "def _ReadSemanticDataTypeDefinition(\n      self, definitions_registry, definition_values, data_type_definition_class,\n      definition_name, supported_definition_values):\n    \"\"\"Reads a semantic data type definition.\n\n    Args:\n      definitions_registry (DataTypeDefinitionsRegistry): data type definitions\n          registry.\n      definition_values (dict[str, object]): definition values.\n      data_type_definition_class (str): data type definition class.\n      definition_name (str): name of the definition.\n      supported_definition_values (set[str]): names of the supported definition\n          values.\n\n    Returns:\n      SemanticDataTypeDefinition: semantic data type definition.\n\n    Raises:\n      DefinitionReaderError: if the definitions values are missing or if\n          the format is incorrect.\n    \"\"\"\n", "input": "", "output": "    return self._ReadDataTypeDefinition(\n        definitions_registry, definition_values, data_type_definition_class,\n        definition_name, supported_definition_values)", "category": "Python"}, {"instruction": "def clearScreen(cls):\n        \"\"\"Clear the screen\"\"\"\n", "input": "", "output": "        if \"win32\" in sys.platform:\n            os.system('cls')\n        elif \"linux\" in sys.platform:\n            os.system('clear')\n        elif 'darwin' in sys.platform:\n            os.system('clear')\n        else:\n            cit.err(\"No clearScreen for \" + sys.platform)", "category": "Python"}, {"instruction": "def point_from_cols_vals(cols, vals):\n        \"\"\"Create a dict from columns and values lists.\n\n        :param cols: List of columns\n        :param vals: List of values\n        :return: Dict where keys are columns.\n        \"\"\"\n", "input": "", "output": "        point = {}\n        for col_index, col_name in enumerate(cols):\n            point[col_name] = vals[col_index]\n\n        return point", "category": "Python"}, {"instruction": "def resolve_polytomy(\n        self,\n        dist=1.0,\n        support=100,\n        recursive=True):\n        \"\"\"\n        Returns a copy of the tree with all polytomies randomly resolved.\n        Does not transform tree in-place.\n        \"\"\"\n", "input": "", "output": "        nself = self.copy()\n        nself.treenode.resolve_polytomy(\n            default_dist=dist,\n            default_support=support,\n            recursive=recursive)\n        nself._coords.update()\n        return nself", "category": "Python"}, {"instruction": "def lineAndColumnAt(s, pos):\n    r\"\"\"Return line and column of `pos` (0-based!) in `s`. Lines start with\n    1, columns with 0.\n\n    Examples:\n\n    >>> lineAndColumnAt(\"0123\\n56\", 5)\n    (2, 0)\n    >>> lineAndColumnAt(\"0123\\n56\", 6)\n    (2, 1)\n    >>> lineAndColumnAt(\"0123\\n56\", 0)\n    (1, 0)\n    \"\"\"\n", "input": "", "output": "    if pos >= len(s):\n        raise IndexError(\"`pos` %d not in string\" % pos)\n    # *don't* count last '\\n', if it is at pos!\n    line = s.count('\\n',0,pos)\n    if line:\n        return line + 1, pos - s.rfind('\\n',0,pos) - 1\n    else:\n        return 1, pos", "category": "Python"}, {"instruction": "def reply_to(self, value):\n        \"\"\"The reply to email address\n\n        :param value: The reply to email address\n        :type value: ReplyTo, str, tuple\n        \"\"\"\n", "input": "", "output": "        if isinstance(value, str):\n            value = ReplyTo(value, None)\n        if isinstance(value, tuple):\n            value = ReplyTo(value[0], value[1])\n        self._reply_to = value", "category": "Python"}, {"instruction": "def siblings(self):\n        \"\"\"\n        Tuple of nodes with the same parent.\n\n        >>> from anytree import Node\n        >>> udo = Node(\"Udo\")\n        >>> marc = Node(\"Marc\", parent=udo)\n        >>> lian = Node(\"Lian\", parent=marc)\n        >>> loui = Node(\"Loui\", parent=marc)\n        >>> lazy = Node(\"Lazy\", parent=marc)\n        >>> udo.siblings\n        ()\n        >>> marc.siblings\n        ()\n        >>> lian.siblings\n        (Node('/Udo/Marc/Loui'), Node('/Udo/Marc/Lazy'))\n        >>> loui.siblings\n        (Node('/Udo/Marc/Lian'), Node('/Udo/Marc/Lazy'))\n        \"\"\"\n", "input": "", "output": "        parent = self.parent\n        if parent is None:\n            return tuple()\n        else:\n            return tuple([node for node in parent.children if node != self])", "category": "Python"}, {"instruction": "def pinyin_syllable_to_zhuyin(s):\n    \"\"\"Convert Pinyin syllable *s* to a Zhuyin syllable.\"\"\"\n", "input": "", "output": "    pinyin_syllable, tone = _parse_pinyin_syllable(s)\n    try:\n        zhuyin_syllable = _PINYIN_MAP[pinyin_syllable.lower()]['Zhuyin']\n    except KeyError:\n        raise ValueError('Not a valid syllable: %s' % s)\n    return zhuyin_syllable + _ZHUYIN_TONES[tone]", "category": "Python"}, {"instruction": "def cluster_application_attempt_info(self, application_id, attempt_id):\n        \"\"\"\n        With the application attempts API, you can obtain an extended info about\n        an application attempt.\n\n        :param str application_id: The application id\n        :param str attempt_id: The attempt id\n        :returns: API response object with JSON data\n        :rtype: :py:class:`yarn_api_client.base.Response`\n        \"\"\"\n", "input": "", "output": "        path = '/ws/v1/cluster/apps/{appid}/appattempts/{attemptid}'.format(\n            appid=application_id, attemptid=attempt_id)\n\n        return self.request(path)", "category": "Python"}, {"instruction": "def Emulation_setCPUThrottlingRate(self, rate):\n\t\t\"\"\"\n\t\tFunction path: Emulation.setCPUThrottlingRate\n\t\t\tDomain: Emulation\n\t\t\tMethod name: setCPUThrottlingRate\n\t\t\n\t\t\tWARNING: This function is marked 'Experimental'!\n\t\t\n\t\t\tParameters:\n\t\t\t\tRequired arguments:\n\t\t\t\t\t'rate' (type: number) -> Throttling rate as a slowdown factor (1 is no throttle, 2 is 2x slowdown, etc).\n\t\t\tNo return value.\n\t\t\n\t\t\tDescription: Enables CPU throttling to emulate slow CPUs.\n\t\t\"\"\"\n", "input": "", "output": "\t\tassert isinstance(rate, (float, int)\n\t\t    ), \"Argument 'rate' must be of type '['float', 'int']'. Received type: '%s'\" % type(\n\t\t    rate)\n\t\tsubdom_funcs = self.synchronous_command('Emulation.setCPUThrottlingRate',\n\t\t    rate=rate)\n\t\treturn subdom_funcs", "category": "Python"}, {"instruction": "def get_metadata(dist):\n    \"\"\"\n    Return dictionary of metadata for given dist\n\n    @param dist: distribution\n    @type dist: pkg_resources Distribution object\n\n    @returns: dict of metadata or None\n\n    \"\"\"\n", "input": "", "output": "    if not dist.has_metadata('PKG-INFO'):\n        return\n\n    msg = email.message_from_string(dist.get_metadata('PKG-INFO'))\n    metadata = {}\n    for header in [l for l in msg._headers]:\n        metadata[header[0]] = header[1]\n\n    return metadata", "category": "Python"}, {"instruction": "def filter_using_summary(fq, args):\n    \"\"\"Use quality scores from albacore summary file for filtering\n\n    Use the summary file from albacore for more accurate quality estimate\n    Get the dataframe from nanoget, convert to dictionary\n    \"\"\"\n", "input": "", "output": "    data = {entry[0]: entry[1] for entry in process_summary(\n        summaryfile=args.summary,\n        threads=\"NA\",\n        readtype=args.readtype,\n        barcoded=False)[\n        [\"readIDs\", \"quals\"]].itertuples(index=False)}\n    try:\n        for record in SeqIO.parse(fq, \"fastq\"):\n            if data[record.id] > args.quality \\\n                    and args.length <= len(record) <= args.maxlength:\n                print(record[args.headcrop:args.tailcrop].format(\"fastq\"), end=\"\")\n    except KeyError:\n        logging.error(\"mismatch between summary and fastq: \\\n                       {} was not found in the summary file.\".format(record.id))\n        sys.exit('\\nERROR: mismatch between sequencing_summary and fastq file: \\\n                 {} was not found in the summary file.\\nQuitting.'.format(record.id))", "category": "Python"}, {"instruction": "def _initialize_cfg(self):\n        \"\"\"\n        Re-create the DiGraph\n        \"\"\"\n", "input": "", "output": "\n        self.kb.functions = FunctionManager(self.kb)\n\n        self._jobs_to_analyze_per_function = defaultdict(set)\n        self._completed_functions = set()", "category": "Python"}, {"instruction": "def use_sequestered_assessment_part_view(self):\n        \"\"\"Pass through to provider AssessmentPartLookupSession.use_sequestered_assessment_part_view\"\"\"\n", "input": "", "output": "        # Does this need to be re-implemented to match the other non-sub-package view setters?\n        self._containable_views['assessment_part'] = SEQUESTERED\n        self._get_sub_package_provider_session('assessment_authoring',\n                                               'assessment_part_lookup_session')\n        for session in self._provider_sessions:\n            for provider_session_name, provider_session in self._provider_sessions[session].items():\n                try:\n                    provider_session.use_sequestered_assessment_part_view()\n                except AttributeError:\n                    pass", "category": "Python"}, {"instruction": "def check_auth(email, password):\n    \"\"\"Check if a username/password combination is valid.\n    \"\"\"\n", "input": "", "output": "    try:\n        user = User.get(User.email == email)\n    except User.DoesNotExist:\n        return False\n    return password == user.password", "category": "Python"}, {"instruction": "def make_env(env_type, real_env, sim_env_kwargs):\n  \"\"\"Factory function for envs.\"\"\"\n", "input": "", "output": "  return {\n      \"real\": lambda: real_env.new_like(  # pylint: disable=g-long-lambda\n          batch_size=sim_env_kwargs[\"batch_size\"],\n          store_rollouts=False,\n      ),\n      \"simulated\": lambda: rl_utils.SimulatedBatchGymEnvWithFixedInitialFrames(  # pylint: disable=g-long-lambda\n          **sim_env_kwargs\n      ),\n  }[env_type]()", "category": "Python"}, {"instruction": "def aes_obj(self, sec_key, encrypt_sec_key=True):\n        \"\"\"\n            \u751f\u6210aes\u52a0\u5bc6\u6240\u9700\u8981\u7684 ``key, iv``\n\n        .. warning:: \u6bcf\u4e2aAES.new\u5bf9\u8c61, \u53ea\u80fd\u4f7f\u7528\u4e00\u6b21\n\n        :param sec_key:\n        :type sec_key: str\n        :param encrypt_sec_key: ``\u5982\u679c\u4e3atrue, \u5219md5\u52a0\u5bc6 sec_key, \u751f\u6210 key,iv, \u5426\u5219\u76f4\u63a5\u4f7f\u7528 sec_key, \u6765\u4f5c\u4e3akey,iv``\n        :type encrypt_sec_key:  bool\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        # \u4f7f\u7528 md5\u52a0\u5bc6 key, \u83b7\u53d632\u4e2a\u957f\u5ea6, \u62c6\u5206\u4e3a\u4e24\u4e2a16\u957f\u5ea6\u4f5c\u4e3a AES\u7684 key, iv\n        p = self.encrypt(sec_key) if encrypt_sec_key else sec_key\n        key, iv = p[:self.bs], p[16:]\n\n        return AES.new(helper.to_bytes(key), self.aes_mode, helper.to_bytes(iv))", "category": "Python"}, {"instruction": "def remove(self, obj_or_string, commit=True):\n        \"\"\"\n        Removes an object from the index.\n        :param obj_or_string:\n        :param commit:\n        \"\"\"\n", "input": "", "output": "        if not self.setup_complete:\n            try:\n                self.setup()\n            except elasticsearch.TransportError as e:\n                if not self.silently_fail:\n                    raise\n                doc_id = get_identifier(obj_or_string)\n                self.log.error(\"Failed to remove document '%s' from Elasticsearch: %s\", doc_id, e)\n                return\n\n        for language in self.languages:\n            # self.log.debug('removing {0} from index {1}'.format(obj_or_string, language))\n            self.index_name = self._index_name_for_language(language)\n            with translation.override(language):\n                super(ElasticsearchMultilingualSearchBackend, self).remove(obj_or_string,\n                                                                           commit=commit)", "category": "Python"}, {"instruction": "def _forward_outbound(self, channel):\n        \"\"\" Forward outbound traffic (ssh -> websockets) \"\"\"\n", "input": "", "output": "        try:\n            while True:\n                wait_read(channel.fileno())\n                data = channel.recv(1024)\n                if not len(data):\n                    return\n                self._websocket.send(json.dumps({'data': data}))\n        finally:\n            self.close()", "category": "Python"}, {"instruction": "def bulk_change_and_save(iterable, update_only_changed_fields=False, save_kwargs=None, **changed_fields):\n    \"\"\"\n    Changes a given `changed_fields` on each object in a given `iterable`, saves objects\n    and returns the changed objects.\n    \"\"\"\n", "input": "", "output": "    return [\n        change_and_save(obj, update_only_changed_fields=update_only_changed_fields, save_kwargs=save_kwargs,\n                        **changed_fields)\n        for obj in iterable\n    ]", "category": "Python"}, {"instruction": "def GetAttributes(self, urns, age=NEWEST_TIME):\n    \"\"\"Retrieves all the attributes for all the urns.\"\"\"\n", "input": "", "output": "    urns = set([utils.SmartUnicode(u) for u in urns])\n    to_read = {urn: self._MakeCacheInvariant(urn, age) for urn in urns}\n\n    # Urns not present in the cache we need to get from the database.\n    if to_read:\n      for subject, values in data_store.DB.MultiResolvePrefix(\n          to_read,\n          AFF4_PREFIXES,\n          timestamp=self.ParseAgeSpecification(age),\n          limit=None):\n\n        # Ensure the values are sorted.\n        values.sort(key=lambda x: x[-1], reverse=True)\n\n        yield utils.SmartUnicode(subject), values", "category": "Python"}, {"instruction": "def _get_password(params):\n        \"\"\"Get the password for a database connection from :mod:`keyring`\n\n        Args:\n            params (dict): database configuration, as defined in :mod:`ozelot.config`\n\n        Returns:\n            str: password\n        \"\"\"\n", "input": "", "output": "        user_name = params['user']\n        service_name = params['host'] + ':' + params['driver']\n        return keyring.get_password(service_name=service_name,\n                                    username=user_name)", "category": "Python"}, {"instruction": "def focus_first_reply(self):\n        \"\"\"move focus to first reply to currently focussed message\"\"\"\n", "input": "", "output": "        mid = self.get_selected_mid()\n        newpos = self._tree.first_child_position(mid)\n        if newpos is not None:\n            newpos = self._sanitize_position((newpos,))\n            self.body.set_focus(newpos)", "category": "Python"}, {"instruction": "def exclude_from(l, containing = [], equal_to = []):\n    \"\"\"Exclude elements in list l containing any elements from list ex.\n    Example:\n        >>> l = ['bob', 'r', 'rob\\r', '\\r\\nrobert']\n        >>> containing = ['\\n', '\\r']\n        >>> equal_to = ['r']\n        >>> exclude_from(l, containing, equal_to)\n        ['bob']\n    \"\"\"\n", "input": "", "output": "      \n    cont = lambda li: any(c in li for c in containing)\n    eq = lambda li: any(e == li for e in equal_to)\n    return [li for li in l if not (cont(li) or eq(li))]", "category": "Python"}, {"instruction": "def p_primary_expr_no_brace_4(self, p):\n        \"\"\"primary_expr_no_brace : LPAREN expr RPAREN\"\"\"\n", "input": "", "output": "        if isinstance(p[2], self.asttypes.GroupingOp):\n            # this reduces the grouping operator to one.\n            p[0] = p[2]\n        else:\n            p[0] = self.asttypes.GroupingOp(expr=p[2])\n            p[0].setpos(p)", "category": "Python"}, {"instruction": "def _parseFreePBXconf(self):\n        \"\"\"Parses FreePBX configuration file /etc/amportal for user and password\n        for Asterisk Manager Interface.\n        \n        @return: True if configuration file is found and parsed successfully.\n        \n        \"\"\"\n", "input": "", "output": "        amiuser = None\n        amipass = None\n        if os.path.isfile(confFileFreePBX):\n            try:\n                fp = open(confFileFreePBX, 'r')\n                data = fp.read()\n                fp.close()\n            except:\n                raise IOError('Failed reading FreePBX configuration file: %s'\n                    % confFileFreePBX)\n            for (key, val) in re.findall('^(AMPMGR\\w+)\\s*=\\s*(\\S+)\\s*$',\n                data, re.MULTILINE):\n                if key == 'AMPMGRUSER':\n                    amiuser = val\n                elif key == 'AMPMGRPASS':\n                    amipass = val\n            if amiuser and amipass:\n                self._amiuser = amiuser\n                self._amipass = amipass\n                return True\n        return False", "category": "Python"}, {"instruction": "def main(args=None):\n    \"\"\"Main function for usage of psyplot from the command line\n\n    This function creates a parser that parses command lines to the\n    :func:`make_plot` functions or (if the ``psyplot_gui`` module is\n    present, to the :func:`psyplot_gui.start_app` function)\n\n    Returns\n    -------\n    psyplot.parser.FuncArgParser\n        The parser that has been used from the command line\"\"\"\n", "input": "", "output": "    try:\n        from psyplot_gui import get_parser as _get_parser\n    except ImportError:\n        logger.debug('Failed to import gui', exc_info=True)\n        parser = get_parser(create=False)\n        parser.update_arg('output', required=True)\n        parser.create_arguments()\n        parser.parse2func(args)\n    else:\n        parser = _get_parser(create=False)\n        parser.create_arguments()\n        parser.parse_known2func(args)", "category": "Python"}, {"instruction": "def d8hdisttostrm(np, p, src, dist, thresh, workingdir=None,\n                      mpiexedir=None, exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run D8 horizontal distance down to stream.\n        \"\"\"\n", "input": "", "output": "        fname = TauDEM.func_name('d8hdisttostrm')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-p': p, '-src': src},\n                          workingdir,\n                          {'-thresh': thresh},\n                          {'-dist': dist},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "category": "Python"}, {"instruction": "def flatten_params(self, data, base_key=None):\n        \"\"\" Flatten out nested arrays and dicts in query params into correct format \"\"\"\n", "input": "", "output": "        result = {}\n\n        if data is None:\n            return result\n\n        map_data = None\n        if not isinstance(data, collections.Mapping):\n            map_data = []\n            for idx, val in enumerate(data):\n                map_data.append([str(idx), val])\n        else:\n            map_data = list(data.items())\n\n        for key, value in map_data:\n            if not base_key is None:\n                key = base_key + \"[\" + key + \"]\"\n\n            if isinstance(value, basestring) or not hasattr(value, \"__iter__\"):\n                result[key] = value\n            else:\n                result.update(self.flatten_params(value, key))\n\n        return result", "category": "Python"}, {"instruction": "def paths_to_polygons(paths, scale=None):\n    \"\"\"\n    Given a sequence of connected points turn them into\n    valid shapely Polygon objects.\n\n    Parameters\n    -----------\n    paths : (n,) sequence\n        Of (m,2) float, closed paths\n    scale: float\n        Approximate scale of drawing for precision\n\n    Returns\n    -----------\n    polys: (p,) list\n        shapely.geometry.Polygon\n        None\n    \"\"\"\n", "input": "", "output": "    polygons = [None] * len(paths)\n    for i, path in enumerate(paths):\n        if len(path) < 4:\n            # since the first and last vertices are identical in\n            # a closed loop a 4 vertex path is the minimum for\n            # non-zero area\n            continue\n        try:\n            polygons[i] = repair_invalid(Polygon(path), scale)\n        except ValueError:\n            # raised if a polygon is unrecoverable\n            continue\n        except BaseException:\n            log.error('unrecoverable polygon', exc_info=True)\n    polygons = np.array(polygons)\n    return polygons", "category": "Python"}, {"instruction": "def has_index(self, columns):\n        \"\"\"Check if an index exists to cover the given ``columns``.\"\"\"\n", "input": "", "output": "        if not self.exists:\n            return False\n        columns = set([normalize_column_name(c) for c in columns])\n        if columns in self._indexes:\n            return True\n        for column in columns:\n            if not self.has_column(column):\n                return False\n        indexes = self.db.inspect.get_indexes(self.name, schema=self.db.schema)\n        for index in indexes:\n            if columns == set(index.get('column_names', [])):\n                self._indexes.append(columns)\n                return True\n        return False", "category": "Python"}, {"instruction": "def last_name(anon, obj, field, val):\n    \"\"\"\n    Returns a random second name\n    \"\"\"\n", "input": "", "output": "    return anon.faker.last_name(field=field)", "category": "Python"}, {"instruction": "def optimize(order):\n    \"\"\" optimize(impl)\n    Tries to set the implementation type of all registered :py:class:`Operation` classes to *impl*.\n    This has no effect when an op does not implement that type.\n\n    The behavior is equivalent to:\n\n    .. code-block:: python\n\n       for op in Operation.__subclasses__():\n           if impl in op.impls:\n               op.use_impl(impl)\n\n    *impl* can also be a list or tuple of valid implementation types representing a preferred order.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(order, (list, tuple)):\n        order = [order]\n\n    for op in Operation.__subclasses__():\n        for impl in order:\n            if impl in op.impls:\n                op.use_impl(impl)\n                break", "category": "Python"}, {"instruction": "def get_eod_frame(self):\n        \"\"\"Return the eod market data frame for pricing\"\"\"\n", "input": "", "output": "        close = self.pxs.close\n        mktval = self.get_mkt_val(close)\n        dvds = self.pxs.dvds\n        df = pd.DataFrame({'close': close, 'mkt_val': mktval, 'dvds': dvds})\n        df.index.name = 'date'\n        return df", "category": "Python"}, {"instruction": "def extractInputForTP(self, tm):\n    \"\"\"\n    Extract inputs for TP from the state of temporal memory\n    three information are extracted\n    1. correctly predicted cells\n    2. all active cells\n    3. bursting cells (unpredicted input)\n    \"\"\"\n", "input": "", "output": "\n    # bursting cells in layer 4\n    burstingColumns = tm.activeState[\"t\"].sum(axis=1)\n    burstingColumns[ burstingColumns < tm.cellsPerColumn ] = 0\n    burstingColumns[ burstingColumns == tm.cellsPerColumn ] = 1\n    # print \"Bursting column indices=\",burstingColumns.nonzero()[0]\n\n    # correctly predicted cells in layer 4\n    correctlyPredictedCells = numpy.zeros(self._inputDimensions).astype(realDType)\n    idx = (tm.predictedState[\"t-1\"] + tm.activeState[\"t\"]) == 2\n    idx = idx.reshape(self._inputDimensions)\n    correctlyPredictedCells[idx] = 1.0\n    # print \"Predicted->active cell indices=\",correctlyPredictedCells.nonzero()[0]\n\n    # all currently active cells in layer 4\n    spInputVector = tm.learnState[\"t\"].reshape(self._inputDimensions)\n    # spInputVector = tm.activeState[\"t\"].reshape(self._inputDimensions)\n\n    return (correctlyPredictedCells, spInputVector, burstingColumns)", "category": "Python"}, {"instruction": "def get_units(self,*args,**kwargs):\n        \"\"\"\n            Returns the units of a Dimension\n        \"\"\"\n", "input": "", "output": "        if len(args) == 1:\n            return self.spike_times.get_label(args[0]).units\n        return [self.spike_times.get_label(a).units for a in args]", "category": "Python"}, {"instruction": "def canparse(argparser, args):\n    \"\"\"Determines if argparser can parse args.\"\"\"\n", "input": "", "output": "    old_error_method = argparser.error\n    argparser.error = _raise_ValueError\n    try:\n        argparser.parse_args(args)\n    except ValueError:\n        return False\n    else:\n        return True\n    finally:\n        argparser.error = old_error_method", "category": "Python"}, {"instruction": "def compute_v(self, memory_antecedent):\n    \"\"\"Compute value Tensor v.\n\n    Args:\n      memory_antecedent: a Tensor with dimensions\n        {memory_input_dim} + other_dims\n    Returns:\n      a Tensor with dimensions\n        memory_heads_dims + {value_dim} + other_dims\n    \"\"\"\n", "input": "", "output": "    if self.shared_kv:\n      raise ValueError(\"compute_v cannot be called with shared_kv\")\n    ret = mtf.einsum(\n        [memory_antecedent, self.wv], reduced_dims=[self.memory_input_dim])\n    if self.combine_dims:\n      ret = mtf.replace_dimensions(ret, ret.shape.dims[-1], self.v_dims)\n    return ret", "category": "Python"}, {"instruction": "def extractall(self, path=None, members=None, pwd=None):\n        \"\"\"Extract all files into current directory.\n\n        Parameters:\n\n            path\n                optional destination path\n            members\n                optional filename or :class:`RarInfo` instance list to extract\n            pwd\n                optional password to use\n        \"\"\"\n", "input": "", "output": "        fnlist = []\n        if members is not None:\n            for m in members:\n                if isinstance(m, RarInfo):\n                    fnlist.append(m.filename)\n                else:\n                    fnlist.append(m)\n        self._extract(fnlist, path, pwd)", "category": "Python"}, {"instruction": "def get_median_mag(self, area, rake):\n        \"\"\"\n        Return magnitude (Mw) given the area and rake.\n\n        Setting the rake to ``None`` causes their \"All\" rupture-types\n        to be applied.\n\n        :param area:\n            Area in square km.\n        :param rake:\n            Rake angle (the rupture propagation direction) in degrees,\n            from -180 to 180.\n        \"\"\"\n", "input": "", "output": "        assert rake is None or -180 <= rake <= 180\n        if rake is None:\n            # their \"All\" case\n            return 4.07 + 0.98 * log10(area)\n        elif (-45 <= rake <= 45) or (rake > 135) or (rake < -135):\n            # strike slip\n            return 3.98 + 1.02 * log10(area)\n        elif rake > 0:\n            # thrust/reverse\n            return 4.33 + 0.90 * log10(area)\n        else:\n            # normal\n            return 3.93 + 1.02 * log10(area)", "category": "Python"}, {"instruction": "def base64_encodestring(instr):\n    '''\n    Encode a string as base64 using the \"legacy\" Python interface.\n\n    Among other possible differences, the \"legacy\" encoder includes\n    a newline ('\\\\n') character after every 76 characters and always\n    at the end of the encoded string.\n    '''\n", "input": "", "output": "    return salt.utils.stringutils.to_unicode(\n        base64.encodestring(salt.utils.stringutils.to_bytes(instr)),\n        encoding='utf8' if salt.utils.platform.is_windows() else None\n    )", "category": "Python"}, {"instruction": "def command(self, request_type, uri, payload):\n        \"\"\"Build and send a command.\"\"\"\n", "input": "", "output": "        self.command_count += 1\n\n        if payload is None:\n            payload = {}\n\n        message = {\n            'id': \"{}_{}\".format(type, self.command_count),\n            'type': request_type,\n            'uri': \"ssap://{}\".format(uri),\n            'payload': payload,\n        }\n\n        self.last_response = None\n\n        try:\n            loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(loop)\n            loop.run_until_complete(asyncio.wait_for(self._command(message), self.timeout_connect, loop=loop))\n        finally:\n            loop.close()", "category": "Python"}, {"instruction": "def update_project_template(push):\n    \"\"\"Update an existing ballet project from the upstream template\"\"\"\n", "input": "", "output": "    import ballet.update\n    import ballet.util.log\n    ballet.util.log.enable(level='INFO',\n                           format=ballet.util.log.SIMPLE_LOG_FORMAT,\n                           echo=False)\n    ballet.update.update_project_template(push=push)", "category": "Python"}, {"instruction": "def with_siblings(graph, outputs):\n    \"\"\"Include all missing siblings.\"\"\"\n", "input": "", "output": "    siblings = set()\n    for node in outputs:\n        siblings |= graph.siblings(node)\n    return siblings", "category": "Python"}, {"instruction": "def load_lists(keys=[], values=[], name='NT'):\n    \"\"\" Map namedtuples given a pair of key, value lists. \"\"\"\n", "input": "", "output": "    mapping = dict(zip(keys, values))\n    return mapper(mapping, _nt_name=name)", "category": "Python"}, {"instruction": "def slackbuild(self, name, sbo_file):\n        \"\"\"Read SlackBuild file\n        \"\"\"\n", "input": "", "output": "        return URL(self.sbo_url + name + sbo_file).reading()", "category": "Python"}, {"instruction": "def fileinfo(fileobj, filename=None, content_type=None, existing=None):\n        \"\"\"Tries to extract from the given input the actual file object, filename and content_type\n\n        This is used by the create and replace methods to correctly deduce their parameters\n        from the available information when possible.\n        \"\"\"\n", "input": "", "output": "        return _FileInfo(fileobj, filename, content_type).get_info(existing)", "category": "Python"}, {"instruction": "def pop(self):\n        \"\"\"Retrieve the next element in line, this will remove it from the queue\"\"\"\n", "input": "", "output": "        if self.minimize:\n            return self.data.pop(0)[1]\n        else:\n            return self.data.pop()[1]", "category": "Python"}, {"instruction": "def authority(self, column=None, value=None, **kwargs):\n        \"\"\"Provides codes and associated authorizing statutes.\"\"\"\n", "input": "", "output": "        return self._resolve_call('GIC_AUTHORITY', column, value, **kwargs)", "category": "Python"}, {"instruction": "def methods(self, methods):\n        \"\"\"Setter method; for a description see the getter method.\"\"\"\n", "input": "", "output": "        # We make sure that the dictionary is a NocaseDict object, and that the\n        # property values are CIMMethod objects:\n        # pylint: disable=attribute-defined-outside-init\n        self._methods = NocaseDict()\n        if methods:\n            try:\n                # This is used for iterables:\n                iterator = methods.items()\n            except AttributeError:\n                # This is used for dictionaries:\n                iterator = methods\n            for item in iterator:\n                if isinstance(item, CIMMethod):\n                    key = item.name\n                    value = item\n                elif isinstance(item, tuple):\n                    key, value = item\n                else:\n                    raise TypeError(\n                        _format(\"Input object for methods has invalid item in \"\n                                \"iterable: {0!A}\", item))\n                self.methods[key] = _cim_method(key, value)", "category": "Python"}, {"instruction": "def create(input_width, input_height, input_channels=1, output_dim=512):\n    \"\"\" Vel factory function \"\"\"\n", "input": "", "output": "    def instantiate(**_):\n        return NatureCnn(\n            input_width=input_width, input_height=input_height, input_channels=input_channels,\n            output_dim=output_dim\n        )\n\n    return ModelFactory.generic(instantiate)", "category": "Python"}, {"instruction": "def find_usage(self):\n        \"\"\"\n        Determine the current usage for each limit of this service,\n        and update corresponding Limit via\n        :py:meth:`~.AwsLimit._add_current_usage`.\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"Checking usage for service %s\", self.service_name)\n        for lim in self.limits.values():\n            lim._reset_usage()\n        try:\n            self.connect()\n            resp = self.conn.get_send_quota()\n        except EndpointConnectionError as ex:\n            logger.warning('Skipping SES: %s', str(ex))\n            return\n        except ClientError as ex:\n            if ex.response['Error']['Code'] in ['AccessDenied', '503']:\n                logger.warning('Skipping SES: %s', ex)\n                return\n            raise\n        self.limits['Daily sending quota']._add_current_usage(\n            resp['SentLast24Hours']\n        )\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "category": "Python"}, {"instruction": "def dependency_tree(installed_keys, root_key):\n    \"\"\"\n    Calculate the dependency tree for the package `root_key` and return\n    a collection of all its dependencies.  Uses a DFS traversal algorithm.\n\n    `installed_keys` should be a {key: requirement} mapping, e.g.\n        {'django': from_line('django==1.8')}\n    `root_key` should be the key to return the dependency tree for.\n    \"\"\"\n", "input": "", "output": "    dependencies = set()\n    queue = collections.deque()\n\n    if root_key in installed_keys:\n        dep = installed_keys[root_key]\n        queue.append(dep)\n\n    while queue:\n        v = queue.popleft()\n        key = key_from_req(v)\n        if key in dependencies:\n            continue\n\n        dependencies.add(key)\n\n        for dep_specifier in v.requires():\n            dep_name = key_from_req(dep_specifier)\n            if dep_name in installed_keys:\n                dep = installed_keys[dep_name]\n\n                if dep_specifier.specifier.contains(dep.version):\n                    queue.append(dep)\n\n    return dependencies", "category": "Python"}, {"instruction": "def _generate_symbol(path, width, height, command='C'):\n    \"\"\"Sequence generator for SVG path.\"\"\"\n", "input": "", "output": "    if len(path) == 0:\n        return\n\n    # Initial point.\n    yield 'M'\n    yield path[0].anchor[1] * width\n    yield path[0].anchor[0] * height\n    yield command\n\n    # Closed path or open path\n    points = (zip(path, path[1:] + path[0:1]) if path.is_closed()\n              else zip(path, path[1:]))\n\n    # Rest of the points.\n    for p1, p2 in points:\n        yield p1.leaving[1] * width\n        yield p1.leaving[0] * height\n        yield p2.preceding[1] * width\n        yield p2.preceding[0] * height\n        yield p2.anchor[1] * width\n        yield p2.anchor[0] * height\n\n    if path.is_closed():\n        yield 'Z'", "category": "Python"}, {"instruction": "def complete_run(self, text, line, b, e):\n        ''' Autocomplete file names with .forth ending. '''\n", "input": "", "output": "        # Don't break on path separators.\n        text = line.split()[-1]\n\n        # Try to find files with a forth file ending, .fs.\n        forth_files = glob.glob(text + '*.fs')\n\n        # Failing that, just try and complete something.\n        if len(forth_files) == 0:\n            return [f.split(os.path.sep)[-1] for f in glob.glob(text + '*')]\n\n        forth_files = [f.split(os.path.sep)[-1] for f in forth_files]\n        return forth_files", "category": "Python"}, {"instruction": "def create(cls, request_bytes, custom_headers=None):\n        \"\"\"\n        Create a new public attachment. Create a POST request with a payload\n        that contains a binary representation of the file, without any JSON\n        wrapping. Make sure you define the MIME type (i.e. image/jpeg, or\n        image/png) in the Content-Type header. You are required to provide a\n        description of the attachment using the X-Bunq-Attachment-Description\n        header.\n\n        :type custom_headers: dict[str, str]|None\n\n        :rtype: BunqResponseStr\n        \"\"\"\n", "input": "", "output": "\n        if custom_headers is None:\n            custom_headers = {}\n\n        api_client = client.ApiClient(cls._get_api_context())\n        endpoint_url = cls._ENDPOINT_URL_CREATE\n        response_raw = api_client.post(endpoint_url, request_bytes,\n                                       custom_headers)\n\n        return BunqResponseStr.cast_from_bunq_response(\n            cls._process_for_uuid(response_raw)\n        )", "category": "Python"}, {"instruction": "def master_call(self, **kwargs):\n        '''\n        Execute a wheel function through the master network interface (eauth).\n        '''\n", "input": "", "output": "        load = kwargs\n        load['cmd'] = 'wheel'\n        interface = self.opts['interface']\n        if interface == '0.0.0.0':\n            interface = '127.0.0.1'\n        master_uri = 'tcp://{}:{}'.format(\n            salt.utils.zeromq.ip_bracket(interface),\n            six.text_type(self.opts['ret_port'])\n        )\n        channel = salt.transport.client.ReqChannel.factory(self.opts,\n                                                           crypt='clear',\n                                                           master_uri=master_uri,\n                                                           usage='master_call')\n        try:\n            ret = channel.send(load)\n        finally:\n            channel.close()\n        if isinstance(ret, collections.Mapping):\n            if 'error' in ret:\n                salt.utils.error.raise_error(**ret['error'])\n        return ret", "category": "Python"}, {"instruction": "def _(f, x):\n    \"\"\"\n    fmap for dict like, not `f` should have signature `f::key->value->(key, value)`\n    \"\"\"\n", "input": "", "output": "    result = {}\n    for k, v in x.items():\n        k_, v_ = f(k, v)\n        result[k_] = v_\n    return result", "category": "Python"}, {"instruction": "def __map_entity(self, entity: dal.AssetClass) -> AssetClass:\n        \"\"\" maps the entity onto the model object \"\"\"\n", "input": "", "output": "        mapper = self.__get_mapper()\n        ac = mapper.map_entity(entity)\n        return ac", "category": "Python"}, {"instruction": "def DateField(formatter=types.DEFAULT_DATE_FORMAT, default=NOTHING,\n              required=True, repr=True, cmp=True, key=None):\n    \"\"\"\n    Create new date field on a model.\n\n    :param formatter: date formatter string (default: \"%Y-%m-%d\")\n    :param default: any date or string that can be converted to a date value\n    :param bool required: whether or not the object is invalid if not provided.\n    :param bool repr: include this field should appear in object's repr.\n    :param bool cmp: include this field in generated comparison.\n    :param string key: override name of the value when converted to dict.\n    \"\"\"\n", "input": "", "output": "    default = _init_fields.init_default(required, default, None)\n    validator = _init_fields.init_validator(required, date)\n    converter = converters.to_date_field(formatter)\n    return attrib(default=default, converter=converter, validator=validator,\n                  repr=repr, cmp=cmp,\n                  metadata=dict(formatter=formatter, key=key))", "category": "Python"}, {"instruction": "def check_input_data_type(self):\n        \"\"\"Check the input data types of the state\n\n        Checks all input data ports if the handed data is not of the specified type and generate an error logger message\n        with details of the found type conflict.\n        \"\"\"\n", "input": "", "output": "        for data_port in self.input_data_ports.values():\n            if data_port.name in self.input_data and self.input_data[data_port.name] is not None:\n                #check for class\n                if not isinstance(self.input_data[data_port.name], data_port.data_type):\n                    logger.error(\"{0} had an data port error: Input of execute function must be of type '{1}' not '{2}'\"\n                                 \" as current value '{3}'\".format(self, data_port.data_type.__name__,\n                                                               type(self.input_data[data_port.name]).__name__,\n                                                               self.input_data[data_port.name]))", "category": "Python"}, {"instruction": "def createBamHeader(self, baseHeader):\n        \"\"\"\n        Creates a new bam header based on the specified header from the\n        parent BAM file.\n        \"\"\"\n", "input": "", "output": "        header = dict(baseHeader)\n        newSequences = []\n        for index, referenceInfo in enumerate(header['SQ']):\n            if index < self.numChromosomes:\n                referenceName = referenceInfo['SN']\n                # The sequence dictionary in the BAM file has to match up\n                # with the sequence ids in the data, so we must be sure\n                # that these still match up.\n                assert referenceName == self.chromosomes[index]\n                newReferenceInfo = {\n                    'AS': self.referenceSetName,\n                    'SN': referenceName,\n                    'LN': 0,  # FIXME\n                    'UR': 'http://example.com',\n                    'M5': 'dbb6e8ece0b5de29da56601613007c2a',  # FIXME\n                    'SP': 'Human'\n                }\n                newSequences.append(newReferenceInfo)\n        header['SQ'] = newSequences\n        return header", "category": "Python"}, {"instruction": "def _comment_system_for_file(contents):\n    \"\"\"For file contents, return the comment system.\"\"\"\n", "input": "", "output": "    if contents[0] == \"#\":\n        return FileCommentSystem(begin=\"#\", middle=\"\", end=\"\", single=\"#\")\n    elif contents[:2] == \"/*\":\n        return FileCommentSystem(begin=\"/*\", middle=\"*\", end=\"*/\", single=\"//\")\n    elif contents[:2] == \"//\":\n        return FileCommentSystem(begin=\"//\", middle=\"//\", end=\"\", single=\"//\")\n    elif contents[:3] == \"rem\":\n        return FileCommentSystem(begin=\"rem\",\n                                 middle=\"rem\",\n                                 end=\"\",\n                                 single=\"rem\")\n    else:\n        raise RuntimeError(\"Couldn't detect comment \"\n                           \"system from {0}\".format(contents[:3]))", "category": "Python"}, {"instruction": "def _make_tuple(x):\n  \"\"\"TF has an obnoxious habit of being lenient with single vs tuple.\"\"\"\n", "input": "", "output": "  if isinstance(x, prettytensor.PrettyTensor):\n    if x.is_sequence():\n      return tuple(x.sequence)\n    else:\n      return (x.tensor,)\n  elif isinstance(x, tuple):\n    return x\n  elif (isinstance(x, collections.Sequence) and\n        not isinstance(x, six.string_types)):\n    return tuple(x)\n  else:\n    return (x,)", "category": "Python"}, {"instruction": "def get_key_filename(vm_):\n    '''\n    Check SSH private key file and return absolute path if exists.\n    '''\n", "input": "", "output": "    key_filename = config.get_cloud_config_value(\n        'ssh_private_key', vm_, __opts__, search_global=False, default=None\n    )\n    if key_filename is not None:\n        key_filename = os.path.expanduser(key_filename)\n        if not os.path.isfile(key_filename):\n            raise SaltCloudConfigError(\n                'The defined ssh_private_key \\'{0}\\' does not exist'.format(\n                    key_filename\n                )\n            )\n\n        return key_filename", "category": "Python"}, {"instruction": "async def apply_command(self, cmd):\n        \"\"\"\n        applies a command\n\n        This calls the pre and post hooks attached to the command,\n        as well as :meth:`cmd.apply`.\n\n        :param cmd: an applicable command\n        :type cmd: :class:`~alot.commands.Command`\n        \"\"\"\n", "input": "", "output": "        # FIXME: What are we guarding for here? We don't mention that None is\n        # allowed as a value fo cmd.\n        if cmd:\n            if cmd.prehook:\n                await cmd.prehook(ui=self, dbm=self.dbman, cmd=cmd)\n            try:\n                if asyncio.iscoroutinefunction(cmd.apply):\n                    await cmd.apply(self)\n                else:\n                    cmd.apply(self)\n            except Exception as e:\n                self._error_handler(e)\n            else:\n                if cmd.posthook:\n                    logging.info('calling post-hook')\n                    await cmd.posthook(ui=self, dbm=self.dbman, cmd=cmd)", "category": "Python"}, {"instruction": "def available_parameters_subset(self, mx_params):\n        \"\"\"\n        Takes an mxnet parameter collect (from Block.collect_params()) and\n        subsets it with the parameters available in this base network.\n        \"\"\"\n", "input": "", "output": "        from copy import copy\n        from collections import OrderedDict\n        subset_params = copy(mx_params)\n        subset_params._params = OrderedDict([\n            (k, v) for k, v in mx_params.items() if k in self.weight_names\n        ])\n        return subset_params", "category": "Python"}, {"instruction": "def get_png_img_html(blob: Union[bytes, memoryview],\n                     extra_html_class: str = None) -> str:\n    \"\"\"\n    Converts a PNG blob to an HTML IMG tag with embedded data.\n    \"\"\"\n", "input": "", "output": "    return ", "category": "Python"}, {"instruction": "def sort_cms_models(cms_models):\n    \"\"\"\n    Sort a set of CMS-related models in a custom (predefined) order.\n    \"\"\"\n", "input": "", "output": "    cms_models.sort(key=lambda model: (\n        get_cms_model_order(model['name']) if is_cms_app(model['app_name']) else 999,\n        model['app_name'],\n        model['title']\n    ))", "category": "Python"}, {"instruction": "def invenio_query_factory(parser=None, walkers=None):\n    \"\"\"Create a parser returning Elastic Search DSL query instance.\"\"\"\n", "input": "", "output": "    parser = parser or Main\n    walkers = walkers or [PypegConverter()]\n    walkers.append(ElasticSearchDSL())\n\n    def invenio_query(pattern):\n        query = pypeg2.parse(pattern, parser, whitespace=\"\")\n        for walker in walkers:\n            query = query.accept(walker)\n        return query\n    return invenio_query", "category": "Python"}, {"instruction": "def scroll_x(self, pixels, relative=True):\n        '''\n        Negative values for @pixels scroll left, positive values scroll right;\n        @relative determines whether to set the scroll-x value or change as \n        above.\n        '''\n", "input": "", "output": "        if relative:\n            self.offset.x += int(pixels)\n        else:\n            self.offset.x = int(pixels)\n        self._update()", "category": "Python"}, {"instruction": "def update(self, friendly_name=None, description=None):\n    \"\"\" Selectively updates Dataset information.\n\n    Args:\n      friendly_name: if not None, the new friendly name.\n      description: if not None, the new description.\n\n    Returns:\n    \"\"\"\n", "input": "", "output": "    self._get_info()\n\n    if self._info:\n      if friendly_name:\n        self._info['friendlyName'] = friendly_name\n      if description:\n        self._info['description'] = description\n      try:\n        self._api.datasets_update(self._name_parts, self._info)\n      except Exception as e:\n        raise e\n      finally:\n        self._info = None", "category": "Python"}, {"instruction": "def SourceShowFile(self, node):\n        \"\"\"Show the given file in the source-code view (attempt it anyway)\"\"\"\n", "input": "", "output": "        filename = self.adapter.filename( node )\n        if filename and self.sourceFileShown != filename:\n            try:\n                data = open(filename).read()\n            except Exception, err:\n                # TODO: load from zips/eggs? What about .pyc issues?\n                return None\n            else:\n                #self.sourceCodeControl.setText(data)\n                self.sourceCodeControl.ClearAll()\n                self.sourceCodeControl.AppendText( data )\n        return filename", "category": "Python"}, {"instruction": "def upload(path, token, folder):\n    \"\"\"\n    The egg that the provided path points to will be uploaded to Databricks.\n    \"\"\"\n", "input": "", "output": "    config = _load_config(CFG_FILE)\n    token = _resolve_input(token, 'token', 'token', config)\n    folder = _resolve_input(folder, 'folder', 'prod_folder', config)\n\n    update_databricks(\n        logger,\n        path,\n        token,\n        folder,\n        update_jobs=False,\n        cleanup=False\n    )", "category": "Python"}, {"instruction": "def X2_chain_H0_omgega_m(self, args):\n        \"\"\"\n        routine to compute X^2\n        :param args:\n        :return:\n        \"\"\"\n", "input": "", "output": "        #extract parameters\n        [H0, omega_m] = args\n        Ode0 = self._omega_lambda_fixed\n        logL_H0, bool_H0 = self.prior_H0(H0)\n        logL_omega_m, bool_omega_m = self.prior_omega_m(omega_m)\n        logL = logL_H0 + logL_omega_m\n        if bool_H0 is True and bool_omega_m is True:\n            logL += self.LCDM_lensLikelihood(H0, omega_m, Ode0)\n        return logL + logL_H0 + logL_omega_m, None", "category": "Python"}, {"instruction": "def get_public_agents():\n    \"\"\"Provides a list of hostnames / private IPs that are public agents in the cluster\"\"\"\n", "input": "", "output": "    agent_list = []\n    agents = __get_all_agents()\n    for agent in agents:\n        for reservation in agent[\"reserved_resources\"]:\n            if \"slave_public\" in reservation:\n                agent_list.append(agent[\"hostname\"])\n\n    return agent_list", "category": "Python"}, {"instruction": "def insert_into_shaders(self, vertex, fragment):\n        \"\"\"Apply the insertions to shader code.\"\"\"\n", "input": "", "output": "        to_insert = defaultdict(str)\n        to_insert.update({key: '\\n'.join(self._to_insert[key]) + '\\n'\n                          for key in self._to_insert})\n        return _insert_glsl(vertex, fragment, to_insert)", "category": "Python"}, {"instruction": "def __similarity(s1, s2, ngrams_fn, n=3):\n    \"\"\"\n        The fraction of n-grams matching between two sequences\n\n        Args:\n            s1: a string\n            s2: another string\n            n: an int for the n in n-gram\n\n        Returns:\n            float: the fraction of n-grams matching\n    \"\"\"\n", "input": "", "output": "    ngrams1, ngrams2 = set(ngrams_fn(s1, n=n)), set(ngrams_fn(s2, n=n))\n    matches = ngrams1.intersection(ngrams2)\n    return 2 * len(matches) / (len(ngrams1) + len(ngrams2))", "category": "Python"}, {"instruction": "def frac(x, context=None):\n    \"\"\"\n    Return the fractional part of ``x``.\n\n    The result has the same sign as ``x``.\n\n    \"\"\"\n", "input": "", "output": "    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_frac,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "category": "Python"}, {"instruction": "def check_image_is_4d(img, min_num_volumes=2):\n    \"\"\"Ensures the image loaded is 3d and nothing else.\"\"\"\n", "input": "", "output": "\n    if len(img.shape) < 4:\n        raise ValueError('Input volume must be 4D!')\n    elif len(img.shape) == 4:\n        for dim_size in img.shape[:3]:\n            if dim_size < 1:\n                raise ValueError('Atleast one slice must exist in each dimension')\n        if img.shape[3] < min_num_volumes:\n            raise ValueError('Input volume is 4D '\n                             'with less than {} volumes!'.format(min_num_volumes))\n    elif len(img.shape) > 4:\n        raise ValueError('Too many dimensions : more than 4.\\n'\n                         'Invalid shape of image : {}'.format(img.shape))\n\n    return img", "category": "Python"}, {"instruction": "def cast_to_swimlane(self, value):\n        \"\"\"Restore swimlane format, attempting to keep initial IDs for any previously existing values\"\"\"\n", "input": "", "output": "        value = super(ListField, self).cast_to_swimlane(value)\n\n        if not value:\n            return None\n\n        # Copy initial values to pop IDs out as each value is hydrated back to server format, without modifying initial\n        # cache of value -> list(ids) map\n        value_ids = deepcopy(self._initial_value_to_ids_map)\n\n        return [self._build_list_item(item, value_ids[item].pop(0) if value_ids[item] else None) for item in value]", "category": "Python"}, {"instruction": "def form_out(self, _form=None):\n        \"\"\"\n        Renders form. Applies form modifiers, then writes\n        result to response payload. If supplied, given form\n        object instance will be used instead of view's\n        default ObjectForm.\n\n        Args:\n             _form (:py:attr:`~zengine.forms.json_form.JsonForm`):\n              Form object to override `self.object_form`\n        \"\"\"\n", "input": "", "output": "        _form = _form or self.object_form\n        self.output['forms'] = _form.serialize()\n        self._add_meta_props(_form)\n        self.output['forms']['grouping'] = _form.Meta.grouping\n        self.output['forms']['constraints'] = _form.Meta.constraints\n        self._patch_form(self.output['forms'])\n        self.set_client_cmd('form')", "category": "Python"}, {"instruction": "def get_instance(self, payload):\n        \"\"\"\n        Build an instance of AuthCallsCredentialListMappingInstance\n\n        :param dict payload: Payload response from the API\n\n        :returns: twilio.rest.api.v2010.account.sip.domain.auth_types.auth_calls_mapping.auth_calls_credential_list_mapping.AuthCallsCredentialListMappingInstance\n        :rtype: twilio.rest.api.v2010.account.sip.domain.auth_types.auth_calls_mapping.auth_calls_credential_list_mapping.AuthCallsCredentialListMappingInstance\n        \"\"\"\n", "input": "", "output": "        return AuthCallsCredentialListMappingInstance(\n            self._version,\n            payload,\n            account_sid=self._solution['account_sid'],\n            domain_sid=self._solution['domain_sid'],\n        )", "category": "Python"}, {"instruction": "def pause(msg=\"Press Enter to Continue...\"):\n    \"\"\"press to continue\"\"\"\n", "input": "", "output": "    print('\\n' + Fore.YELLOW + msg + Fore.RESET, end='')\n    input()", "category": "Python"}, {"instruction": "def complete_config(config):\n    \"\"\"Complete config with default values\"\"\"\n", "input": "", "output": "\n    if not config.has_section('run'):\n        config.add_section('run')\n\n    values = {\n        'basedir': os.getcwd(),\n        'task_control': 'control.yaml',\n    }\n\n    for k, v in values.items():\n        if not config.has_option('run', k):\n            config.set('run', k, v)\n\n    return config", "category": "Python"}, {"instruction": "def get_enumeration(rq, v, endpoint, metadata={}, auth=None):\n    \"\"\"\n    Returns a list of enumerated values for variable 'v' in query 'rq'\n    \"\"\"\n", "input": "", "output": "    # glogger.debug(\"Metadata before processing enums: {}\".format(metadata))\n    # We only fire the enum filling queries if indicated by the query metadata\n    if 'enumerate' not in metadata:\n        return None\n    enumDict = _getDictWithKey(v, metadata['enumerate'])\n    if enumDict:\n        return enumDict[v]\n    if v in metadata['enumerate']:\n        return get_enumeration_sparql(rq, v, endpoint, auth)\n    return None", "category": "Python"}, {"instruction": "def from_def(cls, obj):\n        \"\"\" Builds a profile object from a raw player summary object \"\"\"\n", "input": "", "output": "        prof = cls(obj[\"steamid\"])\n        prof._cache = obj\n\n        return prof", "category": "Python"}, {"instruction": "def get_point_from_bins_and_idx(self, chi1_bin, chi2_bin, idx):\n        \"\"\"Find masses and spins given bin numbers and index.\n\n        Given the chi1 bin, chi2 bin and an index, return the masses and spins\n        of the point at that index. Will fail if no point exists there.\n\n        Parameters\n        -----------\n        chi1_bin : int\n            The bin number for chi1.\n        chi2_bin : int\n            The bin number for chi2.\n        idx : int\n            The index within the chi1, chi2 bin.\n\n        Returns\n        --------\n        mass1 : float\n            Mass of heavier body.\n        mass2 : float\n            Mass of lighter body.\n        spin1z : float\n            Spin of heavier body.\n        spin2z : float\n            Spin of lighter body.\n        \"\"\"\n", "input": "", "output": "        mass1 = self.massbank[chi1_bin][chi2_bin]['mass1s'][idx]\n        mass2 = self.massbank[chi1_bin][chi2_bin]['mass2s'][idx]\n        spin1z = self.massbank[chi1_bin][chi2_bin]['spin1s'][idx]\n        spin2z = self.massbank[chi1_bin][chi2_bin]['spin2s'][idx]\n        return mass1, mass2, spin1z, spin2z", "category": "Python"}, {"instruction": "def next_line(self):\n        \"\"\"Read the next line from the line generator and split it\"\"\"\n", "input": "", "output": "        self.line = next(self.lines)  # Will raise StopIteration when there are no more lines\n        self.values = self.line.split()", "category": "Python"}, {"instruction": "def load_images(input_dir, batch_shape):\n  \"\"\"Read png images from input directory in batches.\n\n  Args:\n    input_dir: input directory\n    batch_shape: shape of minibatch array, i.e. [batch_size, height, width, 3]\n\n  Yields:\n    filenames: list file names without path of each image\n      Length of this list could be less than batch_size, in this case only\n      first few images of the result are elements of the minibatch.\n    images: array with all images from this batch\n  \"\"\"\n", "input": "", "output": "  images = np.zeros(batch_shape)\n  filenames = []\n  idx = 0\n  batch_size = batch_shape[0]\n  for filepath in tf.gfile.Glob(os.path.join(input_dir, '*.png')):\n    with tf.gfile.Open(filepath) as f:\n      images[idx, :, :, :] = imread(f, mode='RGB').astype(np.float) / 255.0\n    filenames.append(os.path.basename(filepath))\n    idx += 1\n    if idx == batch_size:\n      yield filenames, images\n      filenames = []\n      images = np.zeros(batch_shape)\n      idx = 0\n  if idx > 0:\n    yield filenames, images", "category": "Python"}, {"instruction": "def conditions(self, sp_entity_id):\n        \"\"\" Return a saml.Condition instance\n\n        :param sp_entity_id: The SP entity ID\n        :return: A saml.Condition instance\n        \"\"\"\n", "input": "", "output": "        return factory(saml.Conditions,\n                       not_before=instant(),\n                       # How long might depend on who's getting it\n                       not_on_or_after=self.not_on_or_after(sp_entity_id),\n                       audience_restriction=[factory(\n                           saml.AudienceRestriction,\n                           audience=[factory(saml.Audience,\n                                             text=sp_entity_id)])])", "category": "Python"}, {"instruction": "def get(self, palette):\n        \"\"\"\n        Colors API using schema\n        This example is using marshmallow schemas\n        \"\"\"\n", "input": "", "output": "        all_colors = {\n            'cmyk': ['cian', 'magenta', 'yellow', 'black'],\n            'rgb': ['red', 'green', 'blue']\n        }\n        if palette == 'all':\n            result = all_colors\n        else:\n            result = {palette: all_colors.get(palette)}\n        return jsonify(result)", "category": "Python"}, {"instruction": "def _generate_replacement(interface_number, segment_number):\n        \"\"\"\n        This will generate replacement string for\n        {port0} => {port9}\n        {segment0} => {segment9}\n        \"\"\"\n", "input": "", "output": "        replacements = {}\n        for i in range(0, 9):\n            replacements[\"port\" + str(i)] = interface_number + i\n            replacements[\"segment\" + str(i)] = segment_number + i\n        return replacements", "category": "Python"}, {"instruction": "def replay_sgf(sgf_contents):\n    \"\"\"Wrapper for sgf files, returning go.PositionWithContext instances.\n\n    It does NOT return the very final position, as there is no follow up.\n    To get the final position, call pwc.position.play_move(pwc.next_move)\n    on the last PositionWithContext returned.\n\n    Example usage:\n    with open(filename) as f:\n        for position_w_context in replay_sgf(f.read()):\n            print(position_w_context.position)\n    \"\"\"\n", "input": "", "output": "    root_node = get_sgf_root_node(sgf_contents)\n    props = root_node.properties\n    assert int(sgf_prop(props.get('GM', ['1']))) == 1, \"Not a Go SGF!\"\n\n    komi = 0\n    if props.get('KM') is not None:\n        komi = float(sgf_prop(props.get('KM')))\n    result = utils.parse_game_result(sgf_prop(props.get('RE', '')))\n\n    pos = Position(komi=komi)\n    current_node = root_node\n    while pos is not None and current_node.next is not None:\n        pos = handle_node(pos, current_node)\n        maybe_correct_next(pos, current_node.next)\n        next_move = get_next_move(current_node)\n        yield PositionWithContext(pos, next_move, result)\n        current_node = current_node.next", "category": "Python"}, {"instruction": "def prepare_gold(ctx, annotations, gout):\n    \"\"\"Prepare bc-evaluate gold file from annotations supplied by CHEMDNER.\"\"\"\n", "input": "", "output": "    click.echo('chemdataextractor.chemdner.prepare_gold')\n    for line in annotations:\n        pmid, ta, start, end, text, category = line.strip().split('\\t')\n        gout.write('%s\\t%s:%s:%s\\n' % (pmid, ta, start, end))", "category": "Python"}, {"instruction": "def track_list(self,*args):\r\n        '''\r\n        return the list of tracks contained if the dataset\r\n        '''\n", "input": "", "output": "        noargs = len(args) == 0\r\n        return np.unique(self.track) if noargs else np.unique(self.track.compress(args[0]))", "category": "Python"}, {"instruction": "def is_binary(self):\n        \"\"\"Return true if this is a binary file.\"\"\"\n", "input": "", "output": "        with open(self.path, 'rb') as fin:\n            CHUNKSIZE = 1024\n            while 1:\n                chunk = fin.read(CHUNKSIZE)\n                if b'\\0' in chunk:\n                    return True\n                if len(chunk) < CHUNKSIZE:\n                    break\n        return False", "category": "Python"}, {"instruction": "def get_available_palettes(chosen_palette):\n    ''' Given a chosen palette, returns tuple of those available,\n        or None when not found.\n\n        Because palette support of a particular level is almost always a\n        superset of lower levels, this should return all available palettes.\n\n        Returns:\n            Boolean, None: is tty or None if not found.\n    '''\n", "input": "", "output": "    result = None\n    try:\n        result = ALL_PALETTES[:ALL_PALETTES.index(chosen_palette)+1]\n    except ValueError:\n        pass\n    return result", "category": "Python"}, {"instruction": "def render_to_response(self, context, **response_kwargs):\n        \"\"\"\n        Returns a response with a template rendered with the given context.\n        \"\"\"\n", "input": "", "output": "        context[\"ajax_form_id\"] = self.ajax_form_id\n        # context[\"base_template\"] = \"towel_bootstrap/modal.html\"\n        return self.response_class(\n            request=self.request,\n            template=self.get_template_names(),\n            context=context,\n            **response_kwargs\n        )", "category": "Python"}, {"instruction": "def cli(env, identifier):\n    \"\"\"Cancel a dedicated host server immediately\"\"\"\n", "input": "", "output": "\n    mgr = SoftLayer.DedicatedHostManager(env.client)\n\n    host_id = helpers.resolve_id(mgr.resolve_ids, identifier, 'dedicated host')\n\n    if not (env.skip_confirmations or formatting.no_going_back(host_id)):\n        raise exceptions.CLIAbort('Aborted')\n\n    mgr.cancel_host(host_id)\n\n    click.secho('Dedicated Host %s was cancelled' % host_id, fg='green')", "category": "Python"}, {"instruction": "def _create_entry(self, url, title, tags):\n        \"\"\"\n            Create an entry\n            :param url:  url to save\n            :param title: title to set\n            :param tags: tags to set\n            :return: status\n        \"\"\"\n", "input": "", "output": "        try:\n            self.pocket.add(url=url, title=title, tags=tags)\n            sentence = str('pocket {} created').format(url)\n            logger.debug(sentence)\n            status = True\n        except Exception as e:\n            logger.critical(e)\n            update_result(self.trigger_id, msg=e, status=False)\n            status = False\n        return status", "category": "Python"}, {"instruction": "def compareImage(self, body, plotShape=\"circle\", imageScalar=2, imageEncoding=\"base64/png\"):\n        \"\"\"Get an overlay image for two expressions\n        Args:\n            body, ExpressionOperation: The JSON encoded comparison array to be evaluated (required)\n            plotShape, str: The image shape (optional)\n            imageScalar, int: The scale of the image (optional)\n            imageEncoding, str: The encoding of the returned image (optional)\n        Returns:\n            str with the raw byte data of the image\n        Raises:\n            CorticalioException: if the request was not successful\n        \"\"\"\n", "input": "", "output": "        return self._image.getOverlayImage(self._retina, body, plotShape, imageScalar, imageEncoding)", "category": "Python"}, {"instruction": "def split_metadata_params(headers):\n    \"\"\"\n    Given a dict of headers for s3, seperates those that are boto3\n    parameters and those that must be metadata\n    \"\"\"\n", "input": "", "output": "\n    params = {}\n    metadata = {}\n    for header_name in headers:\n        if header_name.lower() in header_mapping:\n            params[header_mapping[header_name.lower()]] = headers[header_name]\n        else:\n            metadata[header_name] = headers[header_name]\n    return metadata, params", "category": "Python"}, {"instruction": "def cleanup():\n    \"\"\"Saves all the open databases to JSON so that the kernel can be shut down\n    without losing in-memory collections.\n    \"\"\"\n", "input": "", "output": "    failed = {}\n    success = []\n    for dbname, db in dbs.items():\n        try:\n            #Force the database save, even if the time hasn't elapsed yet.\n            db.save(True)\n            success.append(dbname)\n        except: # pragma: no cover\n            import sys, traceback\n            xcls, xerr = sys.exc_info()[0:2]\n            failed[dbname] = traceback.format_tb(sys.exc_info()[2])\n\n    for sdb in success:\n        if writeable:\n            msg.okay(\"Project {0}.{1} saved successfully.\".format(*sdb), 0)\n    for fdb, tb in failed.items(): # pragma: no cover\n        msg.err(\"Project {1}.{2} save failed:\\n{0}\".format(tb, *fdb),\n                prefix=False)", "category": "Python"}, {"instruction": "def respond_client(self, answer, socket):\n        \"\"\"Send an answer to the client.\"\"\"\n", "input": "", "output": "        response = pickle.dumps(answer, -1)\n        socket.sendall(response)\n        self.read_list.remove(socket)\n        socket.close()", "category": "Python"}, {"instruction": "def engagement_context(self):\n        \"\"\"\n        Access the engagement_context\n\n        :returns: twilio.rest.studio.v1.flow.engagement.engagement_context.EngagementContextList\n        :rtype: twilio.rest.studio.v1.flow.engagement.engagement_context.EngagementContextList\n        \"\"\"\n", "input": "", "output": "        if self._engagement_context is None:\n            self._engagement_context = EngagementContextList(\n                self._version,\n                flow_sid=self._solution['flow_sid'],\n                engagement_sid=self._solution['sid'],\n            )\n        return self._engagement_context", "category": "Python"}, {"instruction": "def temporary_object_path(self, name):\n        \"\"\"\n        Returns the path to a temporary object, before we know its hash.\n        \"\"\"\n", "input": "", "output": "        return os.path.join(self._path, self.TMP_OBJ_DIR, name)", "category": "Python"}, {"instruction": "def do_show(self, args):\n        \"\"\"Show the current structure of __root (no args),\n        or show the result of the expression (something that can be eval'd).\n        \"\"\"\n", "input": "", "output": "        args = args.strip()\n\n        to_show = self._interp._root\n        if args != \"\":\n            try:\n                to_show = self._interp.eval(args)\n            except Exception as e:\n                print(\"ERROR: \" + e.message)\n                return False\n\n        if hasattr(to_show, \"_pfp__show\"):\n            print(to_show._pfp__show())\n        else:\n            print(repr(to_show))", "category": "Python"}, {"instruction": "def read_entity_alias(self, alias_id, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Query the entity alias by its identifier.\n\n        Supported methods:\n            GET: /{mount_point}/entity-alias/id/{id}. Produces: 200 application/json\n\n        :param alias_id: Identifier of entity alias.\n        :type alias_id: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        api_path = '/v1/{mount_point}/entity-alias/id/{id}'.format(\n            mount_point=mount_point,\n            id=alias_id,\n        )\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "category": "Python"}, {"instruction": "def _styleof(expr, styles):\n    \"\"\"Merge style dictionaries in order\"\"\"\n", "input": "", "output": "    style = dict()\n    for expr_filter, sty in styles:\n        if expr_filter(expr):\n            style.update(sty)\n    return style", "category": "Python"}, {"instruction": "def _map_arguments(self, args):\n        \"\"\"Map from the top-level arguments to the arguments provided to\n        the indiviudal links \"\"\"\n", "input": "", "output": "        data = args.get('data')\n        comp = args.get('comp')\n        ft1file = args.get('ft1file')\n        scratch = args.get('scratch', None)\n        dry_run = args.get('dry_run', None)\n\n        self._set_link('split-and-bin', SplitAndBin_SG,\n                       comp=comp, data=data,\n                       hpx_order_max=args.get('hpx_order_ccube', 9),\n                       ft1file=ft1file,\n                       scratch=scratch,\n                       dry_run=dry_run)\n\n        self._set_link('coadd-split', CoaddSplit_SG,\n                       comp=comp, data=data,\n                       ft1file=ft1file)\n\n        self._set_link('expcube2', Gtexpcube2_SG,\n                       comp=comp, data=data,\n                       hpx_order_max=args.get('hpx_order_expcube', 5),\n                       dry_run=dry_run)", "category": "Python"}, {"instruction": "def aggregate_periods(self, periods):\n        \"\"\"Returns list of ndarrays averaged to a given number of periods.\n\n        Arguments:\n        periods -- desired number of periods as int\n        \"\"\"\n", "input": "", "output": "        try:\n            fieldname = self.raster_field.name\n        except TypeError:\n            raise exceptions.FieldDoesNotExist('Raster field not found')\n        arrays = self.arrays(fieldname)\n        arr = arrays[0]\n        if len(arrays) > 1:\n            if getattr(arr, 'ndim', 0) > 2:\n                arrays = np.vstack(arrays)\n            fill = getattr(arr, 'fill_value', None)\n            arr = np.ma.masked_values(arrays, fill, copy=False)\n        # Try to reshape using equal sizes first and fall back to unequal\n        # splits.\n        try:\n            means = arr.reshape((periods, -1)).mean(axis=1)\n        except ValueError:\n            means = np.array([a.mean() for a in np.array_split(arr, periods)])\n        obj = self[0]\n        setattr(obj, fieldname, means)\n        return [obj]", "category": "Python"}, {"instruction": "def set_state(self, state):\n        \"\"\"\n        Set the state of this device to on or off.\n        \"\"\"\n", "input": "", "output": "        self.basicevent.SetBinaryState(BinaryState=int(state))\n        self._state = int(state)", "category": "Python"}, {"instruction": "def save_batch(server_context, assay_id, batch):\n    # type: (ServerContext, int, Batch) -> Union[Batch, None]\n    \"\"\"\n    Saves a modified batch.\n    :param server_context: A LabKey server context. See utils.create_server_context.\n    :param assay_id: The assay protocol id.\n    :param batch: The Batch to save.\n    :return:\n    \"\"\"\n", "input": "", "output": "    result = save_batches(server_context, assay_id, [batch])\n\n    if result is not None:\n        return result[0]\n    return None", "category": "Python"}, {"instruction": "def shutdown(name, **kwargs):\n    '''\n    Shuts down the device.\n\n    .. code-block:: yaml\n\n            shut the device:\n              junos:\n                - shutdown\n                - in_min: 10\n\n    Parameters:\n      Optional\n        * kwargs:\n            * reboot:\n              Whether to reboot instead of shutdown. (default=False)\n            * at:\n              Specify time for reboot. (To be used only if reboot=yes)\n            * in_min:\n              Specify delay in minutes for shutdown\n    '''\n", "input": "", "output": "    ret = {'name': name, 'changes': {}, 'result': True, 'comment': ''}\n    ret['changes'] = __salt__['junos.shutdown'](**kwargs)\n    return ret", "category": "Python"}, {"instruction": "def make_code_obsolete(code):\n    \"\"\" Moves folders associated with PDB code to obsolete folder in global_settings[\"database_dir\"]\n\n    Parameters\n    ----------\n    code : str\n        PDB accession code\n\n    Returns\n    -------\n    None\n    \"\"\"\n", "input": "", "output": "    fs = FileSystem(code=code)\n    if os.path.exists(fs.parent_dir):\n        # Move to obsolete folder\n        destination_dir = os.path.join(fs._data_dir, 'obsolete', code[1:3], code)\n        if os.path.exists(destination_dir):\n            shutil.rmtree(destination_dir)\n        shutil.move(fs.parent_dir, destination_dir)\n\n        # Remove containing (two-letter) folder if empty, else pass.\n        two_letter_dir = os.path.dirname(fs.parent_dir)\n        try:\n            os.rmdir(two_letter_dir)\n        except OSError:\n            pass\n\n    return", "category": "Python"}, {"instruction": "def get_conn(conn_type):\n    '''\n    Return a conn object for the passed VM data\n    '''\n", "input": "", "output": "    vm_ = get_configured_provider()\n\n    kwargs = vm_.copy()  # pylint: disable=E1103\n\n    kwargs['username'] = vm_['username']\n    kwargs['auth_endpoint'] = vm_.get('identity_url', None)\n    kwargs['region'] = vm_['compute_region']\n\n    conn = getattr(suop, conn_type)\n\n    return conn(**kwargs)", "category": "Python"}, {"instruction": "def _ensure_unicode_string(string):\n        \"\"\"Returns a unicode string for string.\n\n        :param string:\n            The input string.\n        :type string:\n            `basestring`\n\n        :returns:\n            A unicode string.\n        :rtype:\n            `unicode`\n        \"\"\"\n", "input": "", "output": "        if not isinstance(string, six.text_type):\n            string = string.decode('utf-8')\n        return string", "category": "Python"}, {"instruction": "def get_object(self, bucket, object_name):\n        \"\"\"\n        Get an object from a bucket.\n        \"\"\"\n", "input": "", "output": "        details = self._details(\n            method=b\"GET\",\n            url_context=self._url_context(bucket=bucket, object_name=object_name),\n        )\n        d = self._submit(self._query_factory(details))\n        d.addCallback(itemgetter(1))\n        return d", "category": "Python"}, {"instruction": "def ReadBlobs(self, blob_ids, cursor=None):\n    \"\"\"Reads given blobs.\"\"\"\n", "input": "", "output": "    if not blob_ids:\n      return {}\n\n    query = (\"SELECT blob_id, blob_chunk \"\n             \"FROM blobs \"\n             \"FORCE INDEX (PRIMARY) \"\n             \"WHERE blob_id IN {} \"\n             \"ORDER BY blob_id, chunk_index ASC\").format(\n                 mysql_utils.Placeholders(len(blob_ids)))\n    cursor.execute(query, [blob_id.AsBytes() for blob_id in blob_ids])\n    results = {blob_id: None for blob_id in blob_ids}\n    for blob_id_bytes, blob in cursor.fetchall():\n      blob_id = rdf_objects.BlobID.FromBytes(blob_id_bytes)\n      if results[blob_id] is None:\n        results[blob_id] = blob\n      else:\n        results[blob_id] += blob\n    return results", "category": "Python"}, {"instruction": "def create_CTL(fname, tbl_name, col_list, TRUNC_OR_APPEND, delim=','): \n    \"\"\"\n    create_CTL(fname_control_file, tbl_name, src_file, cols, 'TRUNCATE')\n    \"\"\"\n", "input": "", "output": "    with open(fname, 'w') as ct:\n        ct.write('LOAD DATA\\n')\n        ct.write(TRUNC_OR_APPEND + '\\n')\n        ct.write('into table ' + tbl_name  + '\\n')\n        ct.write(\"fields terminated by '\" + delim + \"'\\n\")\n        ct.write('optionally Enclosed  by \\'\"\\'\\n')\n        ct.write('TRAILING NULLCOLS\\n')\n        ct.write('(\\n')\n        ct.write(',\\n'.join(c for c in col_list ))    \n        ct.write(')\\n')", "category": "Python"}, {"instruction": "def erase_display(self, method=EraseMethod.ALL_MOVE):\n        \"\"\" Clear the screen or part of the screen.\n            Arguments:\n                method: One of these possible values:\n                            EraseMethod.END or 0:\n                                Clear from cursor to the end of the screen.\n                            EraseMethod.START or 1:\n                                Clear from cursor to the start of the screen.\n                            EraseMethod.ALL_MOVE or 2:\n                                Clear all, and move home.\n                            EraseMethod.ALL_ERASE or 3:\n                                Clear all, and erase scrollback buffer.\n                            EraseMethod.ALL_MOVE_ERASE or 4:\n                                Like doing 2 and 3 in succession.\n                                This is a feature of Colr. It is not standard.\n                        Default: EraseMethod.ALL_MOVE (2)\n        \"\"\"\n", "input": "", "output": "        return self.chained(erase.display(method))", "category": "Python"}, {"instruction": "def _get_settings_file(self, imu_settings_file):\n        \"\"\"\n        Internal. Logic to check for a system wide RTIMU ini file. This is\n        copied to the home folder if one is not already found there.\n        \"\"\"\n", "input": "", "output": "\n        ini_file = '%s.ini' % imu_settings_file\n\n        home_dir = pwd.getpwuid(os.getuid())[5]\n        home_path = os.path.join(home_dir, self.SETTINGS_HOME_PATH)\n        if not os.path.exists(home_path):\n            os.makedirs(home_path)\n\n        home_file = os.path.join(home_path, ini_file)\n        home_exists = os.path.isfile(home_file)\n        system_file = os.path.join('/etc', ini_file)\n        system_exists = os.path.isfile(system_file)\n\n        if system_exists and not home_exists:\n            shutil.copyfile(system_file, home_file)\n\n        return RTIMU.Settings(os.path.join(home_path, imu_settings_file))", "category": "Python"}, {"instruction": "def extract_agg_damages(dstore, what):\n    \"\"\"\n    Aggregate damages of the given loss type and tags. Use it as\n    /extract/agg_damages/structural?taxonomy=RC&zipcode=20126\n\n    :returns:\n        array of shape (R, D), being R the number of realizations and D the\n        number of damage states, or an array of length 0 if there is no data\n        for the given tags\n    \"\"\"\n", "input": "", "output": "    loss_type, tags = get_loss_type_tags(what)\n    if 'dmg_by_asset' in dstore:  # scenario_damage\n        lti = dstore['oqparam'].lti[loss_type]\n        losses = dstore['dmg_by_asset'][:, :, lti, 0]\n    else:\n        raise KeyError('No damages found in %s' % dstore)\n    return _filter_agg(dstore['assetcol'], losses, tags)", "category": "Python"}, {"instruction": "def _authstr(self, auth):\n        \"\"\"Convert auth to str so that it can be hashed\"\"\"\n", "input": "", "output": "        if type(auth) is dict:\n            return '{' + ','.join([\"{0}:{1}\".format(k, auth[k]) for k in sorted(auth.keys())]) + '}'\n        return auth", "category": "Python"}, {"instruction": "def match(self, node, results=None):\n        \"\"\"\n        Does this pattern exactly match a node?\n\n        Returns True if it matches, False if not.\n\n        If results is not None, it must be a dict which will be\n        updated with the nodes matching named subpatterns.\n\n        Default implementation for non-wildcard patterns.\n        \"\"\"\n", "input": "", "output": "        if self.type is not None and node.type != self.type:\n            return False\n        if self.content is not None:\n            r = None\n            if results is not None:\n                r = {}\n            if not self._submatch(node, r):\n                return False\n            if r:\n                results.update(r)\n        if results is not None and self.name:\n            results[self.name] = node\n        return True", "category": "Python"}, {"instruction": "def update_coordinates(self, filename, update_port_locations=True):\n        \"\"\"Update the coordinates of this Compound from a file.\n\n        Parameters\n        ----------\n        filename : str\n            Name of file from which to load coordinates. Supported file types\n            are the same as those supported by load()\n        update_port_locations : bool, optional, default=True\n            Update the locations of Ports so that they are shifted along with\n            their anchor particles.  Note: This conserves the location of\n            Ports with respect to the anchor Particle, but does not conserve\n            the orientation of Ports with respect to the molecule as a whole.\n\n        See Also\n        --------\n        load : Load coordinates from a file\n\n        \"\"\"\n", "input": "", "output": "        if update_port_locations:\n            xyz_init = self.xyz\n            self = load(filename, compound=self, coords_only=True)\n            self._update_port_locations(xyz_init)\n        else:\n            self = load(filename, compound=self, coords_only=True)", "category": "Python"}, {"instruction": "def _find_module(mod_name):\n    \"\"\"\n    Iterate over each part instead of calling imp.find_module directly.\n    This function is able to find submodules (e.g. sickit.tree)\n    \"\"\"\n", "input": "", "output": "    path = None\n    for part in mod_name.split('.'):\n        if path is not None:\n            path = [path]\n        file, path, description = imp.find_module(part, path)\n        if file is not None:\n            file.close()\n    return path, description", "category": "Python"}, {"instruction": "def replace_tags(string, from_tag='i', to_tag='italic'):\n    \"\"\"\n    Replace tags such as <i> to <italic>\n    <sup> and <sub> are allowed and do not need to be replaced\n    This does not validate markup\n    \"\"\"\n", "input": "", "output": "    string = string.replace('<' + from_tag + '>', '<' + to_tag + '>')\n    string = string.replace('</' + from_tag + '>', '</' + to_tag + '>')\n    return string", "category": "Python"}, {"instruction": "def generate_section_signatures(self, pe, name, sig_length=512):\n        \"\"\"Generates signatures for all the sections in a PE file.\n\n        If the section contains any data a signature will be created\n        for it. The signature name will be a combination of the\n        parameter 'name' and the section number and its name.\n        \"\"\"\n", "input": "", "output": "\n        section_signatures = list()\n\n        for idx, section in enumerate(pe.sections):\n\n            if section.SizeOfRawData < sig_length:\n                continue\n\n            #offset = pe.get_offset_from_rva(section.VirtualAddress)\n            offset = section.PointerToRawData\n\n            sig_name = '%s Section(%d/%d,%s)' % (\n                name, idx + 1, len(pe.sections),\n                ''.join([c for c in section.Name if c in string.printable]))\n\n            section_signatures.append(\n                self.__generate_signature(\n                    pe, offset, sig_name, ep_only=False,\n                    section_start_only=True,\n                    sig_length=sig_length) )\n\n        return '\\n'.join(section_signatures)+'\\n'", "category": "Python"}, {"instruction": "def serialize_search(self, pid_fetcher, search_result, links=None,\n                         item_links_factory=None, **kwargs):\n        \"\"\"Serialize a search result.\n\n        :param pid_fetcher: Persistent identifier fetcher.\n        :param search_result: Elasticsearch search result.\n        :param links: Dictionary of links to add to response.\n        \"\"\"\n", "input": "", "output": "        return json.dumps(dict(\n            hits=dict(\n                hits=[self.transform_search_hit(\n                    pid_fetcher(hit['_id'], hit['_source']),\n                    hit,\n                    links_factory=item_links_factory,\n                    **kwargs\n                ) for hit in search_result['hits']['hits']],\n                total=search_result['hits']['total'],\n            ),\n            links=links or {},\n            aggregations=search_result.get('aggregations', dict()),\n        ), **self._format_args())", "category": "Python"}, {"instruction": "def set(self, name, value):\n        \"\"\"\n        Set the I{value} of a property by I{name}.\n        The value is validated against the definition and set\n        to the default when I{value} is None.\n        @param name: The property name.\n        @type name: str\n        @param value: The new property value.\n        @type value: any\n        @return: self\n        @rtype: L{Properties}\n        \"\"\"\n", "input": "", "output": "        self.provider(name).__set(name, value)\n        return self", "category": "Python"}, {"instruction": "def _bad_result(cls, error):\n        \"\"\"\n        :param str|unicode error: Syntax error from TryHaskell.\n        :rtype: TryHaskell.Result\n        \"\"\"\n", "input": "", "output": "        return cls.Result(ok=False, expr='', files={}, stdout='', type='', value=error)", "category": "Python"}, {"instruction": "def load_suffixes(self, filename):\n        \"\"\"\n        Build the suffix dictionary. The keys will be possible long versions, and the values will be the\n        accepted abbreviations. Everything should be stored using the value version, and you can search all\n        by using building a set of self.suffixes.keys() and self.suffixes.values().\n        \"\"\"\n", "input": "", "output": "        with open(filename, 'r') as f:\n            for line in f:\n                # Make sure we have key and value\n                if len(line.split(',')) != 2:\n                    continue\n                    # Strip off newlines.\n                self.suffixes[line.strip().split(',')[0]] = line.strip().split(',')[1]", "category": "Python"}, {"instruction": "def getBroadcast (self, ifname):\n        \"\"\"Get the broadcast addr for an interface.\n        @param ifname: interface name\n        @type ifname: string\n        \"\"\"\n", "input": "", "output": "        if sys.platform == 'darwin':\n            return ifconfig_inet(ifname).get('broadcast')\n        return self._getaddr(ifname, self.SIOCGIFBRDADDR)", "category": "Python"}, {"instruction": "def SCISetStyling(self, line: int, col: int,\n                      numChar: int, style: bytearray):\n        \"\"\"\n        Pythonic wrapper for the SCI_SETSTYLING command.\n\n        For example, the following code applies style #3\n        to the first five characters in the second line\n        of the widget:\n\n            SCISetStyling((0, 1), 5, 3)\n\n        |Args|\n\n        * ``line`` (**int**): line number where to start styling.\n        * ``col`` (**int**): column number where to start styling.\n        * ``numChar`` (**int**): number of characters to style.\n        * ``style`` (**int**): Scintilla style number.\n\n        |Returns|\n\n        **None**\n\n        |Raises|\n\n        * **QtmacsArgumentError** if at least one argument has an invalid type.\n        \"\"\"\n", "input": "", "output": "        if not self.isPositionValid(line, col):\n            return\n\n        pos = self.positionFromLineIndex(line, col)\n        self.SendScintilla(self.SCI_STARTSTYLING, pos, 0xFF)\n        self.SendScintilla(self.SCI_SETSTYLING, numChar, style)", "category": "Python"}, {"instruction": "def dehydrate_point(value):\n    \"\"\" Dehydrator for Point data.\n\n    :param value:\n    :type value: Point\n    :return:\n    \"\"\"\n", "input": "", "output": "    dim = len(value)\n    if dim == 2:\n        return Structure(b\"X\", value.srid, *value)\n    elif dim == 3:\n        return Structure(b\"Y\", value.srid, *value)\n    else:\n        raise ValueError(\"Cannot dehydrate Point with %d dimensions\" % dim)", "category": "Python"}]