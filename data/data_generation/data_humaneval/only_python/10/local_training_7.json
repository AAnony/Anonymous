[{"instruction": "def node_to_mfd(node, taglist):\n    \"\"\"\n    Reads the node to return a magnitude frequency distribution\n    \"\"\"\n", "input": "", "output": "    if \"incrementalMFD\" in taglist:\n        mfd = node_to_evenly_discretized(\n            node.nodes[taglist.index(\"incrementalMFD\")])\n    elif \"truncGutenbergRichterMFD\" in taglist:\n        mfd = node_to_truncated_gr(\n            node.nodes[taglist.index(\"truncGutenbergRichterMFD\")])\n    else:\n        mfd = None\n    return mfd", "category": "Python"}, {"instruction": "def str_matcher(mode):\r\n    \"\"\"\r\n    generate token strings' cache\r\n    \"\"\"\n", "input": "", "output": "\r\n    def f_raw(inp_str, pos):\r\n        return unique_literal_cache_pool[mode] if inp_str.startswith(mode, pos) else None\r\n\r\n    def f_collection(inp_str, pos):\r\n        for each in mode:\r\n            if inp_str.startswith(each, pos):\r\n                return unique_literal_cache_pool[each]\r\n        return None\r\n\r\n    if isinstance(mode, str):\r\n        return f_raw\r\n\r\n    if len(mode) is 1:\r\n        mode = mode[0]\r\n        return f_raw\r\n\r\n    return f_collection", "category": "Python"}, {"instruction": "def print_header_content(nlh):\n    \"\"\"Return header content (doesn't actually print like the C library does).\n\n    https://github.com/thom311/libnl/blob/libnl3_2_25/lib/handlers.c#L34\n\n    Positional arguments:\n    nlh -- nlmsghdr class instance.\n    \"\"\"\n", "input": "", "output": "    answer = 'type={0} length={1} flags=<{2}> sequence-nr={3} pid={4}'.format(\n        nl_nlmsgtype2str(nlh.nlmsg_type, bytearray(), 32).decode('ascii'),\n        nlh.nlmsg_len,\n        nl_nlmsg_flags2str(nlh.nlmsg_flags, bytearray(), 128).decode('ascii'),\n        nlh.nlmsg_seq,\n        nlh.nlmsg_pid,\n    )\n    return answer", "category": "Python"}, {"instruction": "def _setup_logging(self):\n        \"\"\"\n        This is done in the :class:`Router` constructor for historical reasons.\n        It must be called before ExternalContext logs its first messages, but\n        after logging has been setup. It must also be called when any router is\n        constructed for a consumer app.\n        \"\"\"\n", "input": "", "output": "        # Here seems as good a place as any.\n        global _v, _vv\n        _v = logging.getLogger().level <= logging.DEBUG\n        _vv = IOLOG.level <= logging.DEBUG", "category": "Python"}, {"instruction": "def getElementsByClassName(self, className):\n        '''\n            getElementsByClassName - Get elements within this collection containing a specific class name\n\n            @param className - A single class name\n\n            @return - TagCollection of unique elements within this collection tagged with a specific class name\n        '''\n", "input": "", "output": "        ret = TagCollection()\n        if len(self) == 0:\n            return ret\n        _cmpFunc = lambda tag : tag.hasClass(className)\n        for tag in self:\n            TagCollection._subset(ret, _cmpFunc, tag)\n        \n        return ret", "category": "Python"}, {"instruction": "def show_item(name, id_):\n    '''\n    Show an item\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion rallydev.show_<item name> <item id>\n    '''\n", "input": "", "output": "    status, result = _query(action=name, command=id_)\n    return result", "category": "Python"}, {"instruction": "def _serialize_list(cls, list_):\n        \"\"\"\n        :type list_: list\n\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "\n        list_serialized = []\n\n        for item in list_:\n            item_serialized = cls.serialize(item)\n            list_serialized.append(item_serialized)\n\n        return list_serialized", "category": "Python"}, {"instruction": "def measurement_time_typical(self):\n        \"\"\"Typical time in milliseconds required to complete a measurement in normal mode\"\"\"\n", "input": "", "output": "        meas_time_ms = 1.0\n        if self.overscan_temperature != OVERSCAN_DISABLE:\n            meas_time_ms += (2 * _BME280_OVERSCANS.get(self.overscan_temperature))\n        if self.overscan_pressure != OVERSCAN_DISABLE:\n            meas_time_ms += (2 * _BME280_OVERSCANS.get(self.overscan_pressure) + 0.5)\n        if self.overscan_humidity != OVERSCAN_DISABLE:\n            meas_time_ms += (2 * _BME280_OVERSCANS.get(self.overscan_humidity) + 0.5)\n        return meas_time_ms", "category": "Python"}, {"instruction": "def init():\n    \"\"\"\n    Initializes the GLFW library.\n\n    Wrapper for:\n        int glfwInit(void);\n    \"\"\"\n", "input": "", "output": "    cwd = _getcwd()\n    res = _glfw.glfwInit()\n    os.chdir(cwd)\n    return res", "category": "Python"}, {"instruction": "def meta_features_path(self, path):\n        \"\"\"Returns path for meta-features\n\n        Args:\n            path (str): Absolute/local path of xcessiv folder\n        \"\"\"\n", "input": "", "output": "        return os.path.join(\n                path,\n                app.config['XCESSIV_META_FEATURES_FOLDER'],\n                str(self.id)\n            ) + '.npy'", "category": "Python"}, {"instruction": "def tabify_plugins(self, first, second):\r\n        \"\"\"Tabify plugin dockwigdets\"\"\"\n", "input": "", "output": "        self.tabifyDockWidget(first.dockwidget, second.dockwidget)", "category": "Python"}, {"instruction": "def get_ssl_termination(self, loadbalancer):\n        \"\"\"\n        Returns a dict representing the SSL termination configuration\n        for the load balancer. If SSL termination has not been configured,\n        returns an empty dict.\n        \"\"\"\n", "input": "", "output": "        uri = \"/loadbalancers/%s/ssltermination\" % utils.get_id(loadbalancer)\n        try:\n            resp, body = self.api.method_get(uri)\n        except exc.NotFound:\n            # For some reason, instead of returning an empty dict like the\n            # other API GET calls, this raises a 404.\n            return {}\n        return body.get(\"sslTermination\", {})", "category": "Python"}, {"instruction": "def expect_token(lexer: Lexer, kind: TokenKind) -> Token:\n    \"\"\"Expect the next token to be of the given kind.\n\n    If the next token is of the given kind, return that token after advancing the lexer.\n    Otherwise, do not change the parser state and throw an error.\n    \"\"\"\n", "input": "", "output": "    token = lexer.token\n    if token.kind == kind:\n        lexer.advance()\n        return token\n\n    raise GraphQLSyntaxError(\n        lexer.source, token.start, f\"Expected {kind.value}, found {token.kind.value}\"\n    )", "category": "Python"}, {"instruction": "def tag_add(package, tag, pkghash):\n    \"\"\"\n    Add a new tag for a given package hash.\n\n    Unlike versions, tags can have an arbitrary format, and can be modified\n    and deleted.\n\n    When a package is pushed, it gets the \"latest\" tag.\n    \"\"\"\n", "input": "", "output": "    team, owner, pkg = parse_package(package)\n    session = _get_session(team)\n\n    session.put(\n        \"{url}/api/tag/{owner}/{pkg}/{tag}\".format(\n            url=get_registry_url(team),\n            owner=owner,\n            pkg=pkg,\n            tag=tag\n        ),\n        data=json.dumps(dict(\n            hash=_match_hash(package, pkghash)\n        ))\n    )", "category": "Python"}, {"instruction": "def taxonomy(value):\n    \"\"\"\n    Any ASCII character goes into a taxonomy, except spaces.\n    \"\"\"\n", "input": "", "output": "    try:\n        value.encode('ascii')\n    except UnicodeEncodeError:\n        raise ValueError('tag %r is not ASCII' % value)\n    if re.search(r'\\s', value):\n        raise ValueError('The taxonomy %r contains whitespace chars' % value)\n    return value", "category": "Python"}, {"instruction": "def yieldroutes(func):\n    \"\"\" Return a generator for routes that match the signature (name, args)\n    of the func parameter. This may yield more than one route if the function\n    takes optional keyword arguments. The output is best described by example::\n\n        a()         -> '/a'\n        b(x, y)     -> '/b/:x/:y'\n        c(x, y=5)   -> '/c/:x' and '/c/:x/:y'\n        d(x=5, y=6) -> '/d' and '/d/:x' and '/d/:x/:y'\n    \"\"\"\n", "input": "", "output": "    import inspect # Expensive module. Only import if necessary.\n    path = '/' + func.__name__.replace('__','/').lstrip('/')\n    spec = inspect.getargspec(func)\n    argc = len(spec[0]) - len(spec[3] or [])\n    path += ('/:%s' * argc) % tuple(spec[0][:argc])\n    yield path\n    for arg in spec[0][argc:]:\n        path += '/:%s' % arg\n        yield path", "category": "Python"}, {"instruction": "def filter_device_by_class(vid, pid, device_class):\n    \"\"\"! @brief Test whether the device should be ignored by comparing bDeviceClass.\n    \n    This function checks the device's bDeviceClass to determine whether the it is likely to be\n    a CMSIS-DAP device. It uses the vid and pid for device-specific quirks.\n    \n    @retval True Skip the device.\n    @retval False The device is valid.\n    \"\"\"\n", "input": "", "output": "    # Check valid classes for CMSIS-DAP firmware.\n    if device_class in CMSIS_DAP_USB_CLASSES:\n        return False\n    # Old \"Mbed CMSIS-DAP\" firmware has an incorrect bDeviceClass.\n    if ((vid, pid) == ARM_DAPLINK_ID) and (device_class == USB_CLASS_COMMUNICATIONS):\n        return False\n    # Any other class indicates the device is not CMSIS-DAP.\n    return True", "category": "Python"}, {"instruction": "def find_existing_anchors(soup):\n  \"\"\"Return existing ids (and names) from a soup.\"\"\"\n", "input": "", "output": "  existing_anchors = set()\n  for tag in soup.find_all(True):\n    for attr in ['id', 'name']:\n      if tag.has_attr(attr):\n        existing_anchors.add(tag.get(attr))\n  return existing_anchors", "category": "Python"}, {"instruction": "def flush():\n    \"\"\"Try to flush all stdio buffers, both from python and from C.\"\"\"\n", "input": "", "output": "    try:\n        sys.stdout.flush()\n        sys.stderr.flush()\n    except (AttributeError, ValueError, IOError):\n        pass  # unsupported\n    try:\n        libc.fflush(None)\n    except (AttributeError, ValueError, IOError):\n        pass", "category": "Python"}, {"instruction": "def server_off(self):\n        ''' Stop experiment server '''\n", "input": "", "output": "        if (self.server.is_server_running() == 'yes' or\n                self.server.is_server_running() == 'maybe'):\n            self.server.shutdown()\n            print 'Please wait. This could take a few seconds.'\n            time.sleep(0.5)", "category": "Python"}, {"instruction": "def add_route(self, handler, uri, methods=frozenset({'GET'}), host=None,\n                  strict_slashes=False):\n        \"\"\"Create a blueprint route from a function.\n\n        :param handler: function for handling uri requests. Accepts function,\n                        or class instance with a view_class method.\n        :param uri: endpoint at which the route will be accessible.\n        :param methods: list of acceptable HTTP methods.\n        :return: function or class instance\n        \"\"\"\n", "input": "", "output": "        # Handle HTTPMethodView differently\n        if hasattr(handler, 'view_class'):\n            http_methods = (\n                'GET', 'POST', 'PUT', 'HEAD', 'OPTIONS', 'PATCH', 'DELETE')\n            methods = set()\n            for method in http_methods:\n                if getattr(handler.view_class, method.lower(), None):\n                    methods.add(method)\n\n        # handle composition view differently\n        if isinstance(handler, self._composition_view_class):\n            methods = handler.handlers.keys()\n\n        self.route(uri=uri, methods=methods, host=host,\n                   strict_slashes=strict_slashes)(handler)\n        return handler", "category": "Python"}, {"instruction": "def groups(self):\n        \"\"\"Return the names of groups in the file\n\n        Note that there is not necessarily a TDMS object associated with\n        each group name.\n\n        :rtype: List of strings.\n\n        \"\"\"\n", "input": "", "output": "\n        # Split paths into components and take the first (group) component.\n        object_paths = (\n            path_components(path)\n            for path in self.objects)\n        group_names = (path[0] for path in object_paths if len(path) > 0)\n\n        # Use an ordered dict as an ordered set to find unique\n        # groups in order.\n        groups_set = OrderedDict()\n        for group in group_names:\n            groups_set[group] = None\n        return list(groups_set)", "category": "Python"}, {"instruction": "def _validate(self, field, list_attribute):\n        \"\"\"\n        Validate the field when its value change.\n\n        :param field: The field.\n        :param list_attribute: The list attribute of field with validation.\n        \"\"\"\n", "input": "", "output": "\n        if not self.scripts_added:\n            self._generate_validation_scripts()\n        self.id_generator.generate_id(field)\n        self.script_list_fields_with_validation.append_text(\n            'hatemileValidationList.'\n            + list_attribute\n            + '.push(\"'\n            + field.get_attribute('id')\n            + '\");'\n        )", "category": "Python"}, {"instruction": "def createPushParser(SAX, chunk, size, URI):\n    \"\"\"Create a progressive XML parser context to build either an\n      event flow if the SAX object is not None, or a DOM tree\n       otherwise. \"\"\"\n", "input": "", "output": "    ret = libxml2mod.xmlCreatePushParser(SAX, chunk, size, URI)\n    if ret is None:raise parserError('xmlCreatePushParser() failed')\n    return parserCtxt(_obj=ret)", "category": "Python"}, {"instruction": "def _create_variable(orig_v, step, variables):\n    \"\"\"Create a new output variable, potentially over-writing existing or creating new.\n    \"\"\"\n", "input": "", "output": "    # get current variable, and convert to be the output of our process step\n    try:\n        v = _get_variable(orig_v[\"id\"], variables)\n    except ValueError:\n        v = copy.deepcopy(orig_v)\n        if not isinstance(v[\"id\"], six.string_types):\n            v[\"id\"] = _get_string_vid(v[\"id\"])\n    for key, val in orig_v.items():\n        if key not in [\"id\", \"type\"]:\n            v[key] = val\n    if orig_v.get(\"type\") != \"null\":\n        v[\"type\"] = orig_v[\"type\"]\n    v[\"id\"] = \"%s/%s\" % (step.name, get_base_id(v[\"id\"]))\n    return v", "category": "Python"}, {"instruction": "def terminal_size(kernel32=None):\n    \"\"\"Get the width and height of the terminal.\n\n    http://code.activestate.com/recipes/440694-determine-size-of-console-window-on-windows/\n    http://stackoverflow.com/questions/17993814/why-the-irrelevant-code-made-a-difference\n\n    :param kernel32: Optional mock kernel32 object. For testing.\n\n    :return: Width (number of characters) and height (number of lines) of the terminal.\n    :rtype: tuple\n    \"\"\"\n", "input": "", "output": "    if IS_WINDOWS:\n        kernel32 = kernel32 or ctypes.windll.kernel32\n        try:\n            return get_console_info(kernel32, kernel32.GetStdHandle(STD_ERROR_HANDLE))\n        except OSError:\n            try:\n                return get_console_info(kernel32, kernel32.GetStdHandle(STD_OUTPUT_HANDLE))\n            except OSError:\n                return DEFAULT_WIDTH, DEFAULT_HEIGHT\n\n    try:\n        device = __import__('fcntl').ioctl(0, __import__('termios').TIOCGWINSZ, '\\0\\0\\0\\0\\0\\0\\0\\0')\n    except IOError:\n        return DEFAULT_WIDTH, DEFAULT_HEIGHT\n    height, width = struct.unpack('hhhh', device)[:2]\n    return width, height", "category": "Python"}, {"instruction": "def find_outliers(group, delta):\n    \"\"\"\n    given a list of values, find those that are apart from the rest by\n    `delta`. the indexes for the outliers is returned, if any.\n\n    examples:\n\n    values = [100, 6, 7, 8, 9, 10, 150]\n    find_outliers(values, 5) -> [0, 6]\n\n    values = [5, 6, 5, 4, 5]\n    find_outliers(values, 3) -> []\n\n    \"\"\"\n", "input": "", "output": "    with_pos = sorted([pair for pair in enumerate(group)], key=lambda p: p[1])\n    outliers_start = outliers_end = -1\n\n    for i in range(0, len(with_pos) - 1):\n        cur = with_pos[i][1]\n        nex = with_pos[i + 1][1]\n\n        if nex - cur > delta:\n            # depending on where we are, outliers are the remaining\n            # items or the ones that we've already seen.\n            if i < (len(with_pos) - i):\n                # outliers are close to the start\n                outliers_start, outliers_end = 0, i + 1\n            else:\n                # outliers are close to the end\n                outliers_start, outliers_end = i + 1, len(with_pos)\n\n            break\n\n    if outliers_start != -1:\n        return [with_pos[i][0] for i in range(outliers_start, outliers_end)]\n    else:\n        return []", "category": "Python"}, {"instruction": "def cast_item(cls, key, value):\r\n        \"\"\"Cast schema item to the appropriate tag type.\"\"\"\n", "input": "", "output": "        schema_type = cls.schema.get(key)\r\n        if schema_type is None:\r\n            if cls.strict:\r\n                raise TypeError(f'Invalid key {key!r}')\r\n        elif not isinstance(value, schema_type):\r\n            try:\r\n                return schema_type(value)\r\n            except CastError:\r\n                raise\r\n            except Exception as exc:\r\n                raise CastError(value, schema_type) from exc\r\n        return value", "category": "Python"}, {"instruction": "async def fetchval(self, *args, column=0, timeout=None):\n        \"\"\"Execute the statement and return a value in the first row.\n\n        :param args: Query arguments.\n        :param int column: Numeric index within the record of the value to\n                           return (defaults to 0).\n        :param float timeout: Optional timeout value in seconds.\n                            If not specified, defaults to the value of\n                            ``command_timeout`` argument to the ``Connection``\n                            instance constructor.\n\n        :return: The value of the specified column of the first record.\n        \"\"\"\n", "input": "", "output": "        data = await self.__bind_execute(args, 1, timeout)\n        if not data:\n            return None\n        return data[0][column]", "category": "Python"}, {"instruction": "def assoc(self, index, value):\n        '''Return a new vector with value associated at index. The implicit\n        parameter is not modified.'''\n", "input": "", "output": "        newvec = ImmutableVector()\n        newvec.tree = self.tree.assoc(index, value)\n        if index >= self._length:\n            newvec._length = index+1\n        else:\n            newvec._length = self._length\n        return newvec", "category": "Python"}, {"instruction": "def decode(self, envelope, session, encoding=None, **kwargs):\n\t\t\"\"\" :meth:`.WMessengerOnionCoderLayerProto.decode` method implementation.\n\n\t\t:param envelope: original envelope\n\t\t:param session: original session\n\t\t:param encoding: encoding to use (default is 'utf-8')\n\t\t:param kwargs: additional arguments\n\n\t\t:return: WMessengerTextEnvelope\n\t\t\"\"\"\n", "input": "", "output": "\t\tmessage = envelope.message()\n\t\tmessage = message.decode() if encoding is None else message.decode(encoding)\n\t\treturn WMessengerTextEnvelope(message, meta=envelope)", "category": "Python"}, {"instruction": "def pformat(self, prefix=()):\r\n        '''\r\n        Makes a pretty ASCII format of the data, suitable for\r\n        displaying in a console or saving to a text file.\r\n        Returns a list of lines.\r\n        '''\n", "input": "", "output": "        nan = float(\"nan\")\r\n\r\n        def sformat(segment, stat):\r\n            FMT = \"n={0}, mean={1}, p50/95={2}/{3}, max={4}\"\r\n            line_segs = [segment]\r\n            for s in [stat]:\r\n                p = s.get_percentiles()\r\n                p50, p95 = p.get(0.50, nan), p.get(0.95, nan)\r\n                line_segs.append(FMT.format(s.n, s.mean, p50, p95, s.max))\r\n            return '{0}: {1}'.format(*line_segs)\r\n\r\n        lines = []\r\n        for path in sorted(self.path_stats.keys()):\r\n            lines.append('=====================')\r\n            for seg, stat in zip(path, self.path_stats[path]):\r\n                lines.append(sformat(seg, stat))\r\n        return lines", "category": "Python"}, {"instruction": "def from_dict(vpc_config, do_sanitize=False):\n    \"\"\"\n    Extracts subnets and security group ids as lists from a VpcConfig dict\n\n    Args:\n        vpc_config (dict): a VpcConfig dict containing 'Subnets' and 'SecurityGroupIds'\n        do_sanitize (bool): whether to sanitize the VpcConfig dict before extracting values\n\n    Returns:\n        Tuple of lists as (subnets, security_group_ids)\n        If vpc_config parameter is None, returns (None, None)\n\n    Raises:\n        ValueError if sanitize enabled and vpc_config is invalid\n        KeyError if sanitize disabled and vpc_config is missing key(s)\n    \"\"\"\n", "input": "", "output": "    if do_sanitize:\n        vpc_config = sanitize(vpc_config)\n    if vpc_config is None:\n        return None, None\n    return vpc_config[SUBNETS_KEY], vpc_config[SECURITY_GROUP_IDS_KEY]", "category": "Python"}, {"instruction": "def update(name=None,\n           pkgs=None,\n           refresh=True,\n           skip_verify=False,\n           normalize=True,\n           minimal=False,\n           obsoletes=False,\n           **kwargs):\n    '''\n    .. versionadded:: 2019.2.0\n\n    Calls :py:func:`pkg.upgrade <salt.modules.yumpkg.upgrade>` with\n    ``obsoletes=False``. Mirrors the CLI behavior of ``yum update``.\n    See :py:func:`pkg.upgrade <salt.modules.yumpkg.upgrade>` for\n    further documentation.\n\n    .. code-block:: bash\n\n        salt '*' pkg.update\n    '''\n", "input": "", "output": "    return upgrade(name, pkgs, refresh, skip_verify, normalize, minimal, obsoletes, **kwargs)", "category": "Python"}, {"instruction": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a Words object from a json dictionary.\"\"\"\n", "input": "", "output": "        args = {}\n        if 'words' in _dict:\n            args['words'] = [Word._from_dict(x) for x in (_dict.get('words'))]\n        else:\n            raise ValueError(\n                'Required property \\'words\\' not present in Words JSON')\n        return cls(**args)", "category": "Python"}, {"instruction": "def get_lower_supported(cls, previous_version):\n        \"\"\"\n        Return the lower supported protocol version. Beta versions are omitted.\n        \"\"\"\n", "input": "", "output": "        try:\n            version = next(v for v in sorted(ProtocolVersion.SUPPORTED_VERSIONS, reverse=True) if\n                           v not in ProtocolVersion.BETA_VERSIONS and v < previous_version)\n        except StopIteration:\n            version = 0\n\n        return version", "category": "Python"}, {"instruction": "def get_file(self,\n                 path,\n                 dest='',\n                 makedirs=False,\n                 saltenv='base',\n                 gzip=None,\n                 cachedir=None):\n        '''\n        Copies a file from the local files or master depending on\n        implementation\n        '''\n", "input": "", "output": "        raise NotImplementedError", "category": "Python"}, {"instruction": "def decode(input, fallback_encoding, errors='replace'):\n    \"\"\"\n    Decode a single string.\n\n    :param input: A byte string\n    :param fallback_encoding:\n        An :class:`Encoding` object or a label string.\n        The encoding to use if :obj:`input` does note have a BOM.\n    :param errors: Type of error handling. See :func:`codecs.register`.\n    :raises: :exc:`~exceptions.LookupError` for an unknown encoding label.\n    :return:\n        A ``(output, encoding)`` tuple of an Unicode string\n        and an :obj:`Encoding`.\n\n    \"\"\"\n", "input": "", "output": "    # Fail early if `encoding` is an invalid label.\n    fallback_encoding = _get_encoding(fallback_encoding)\n    bom_encoding, input = _detect_bom(input)\n    encoding = bom_encoding or fallback_encoding\n    return encoding.codec_info.decode(input, errors)[0], encoding", "category": "Python"}, {"instruction": "def initial_guesses(self):\n        \"\"\"\n        :return: Initial guesses for every parameter.\n        \"\"\"\n", "input": "", "output": "        return np.array([param.value for param in self.model.params])", "category": "Python"}, {"instruction": "def query_by_entity_uid(idd, kind=''):\n        '''\n        Query post2tag by certain post.\n        '''\n", "input": "", "output": "\n        if kind == '':\n            return TabPost2Tag.select(\n                TabPost2Tag,\n                TabTag.slug.alias('tag_slug'),\n                TabTag.name.alias('tag_name')\n            ).join(\n                TabTag, on=(TabPost2Tag.tag_id == TabTag.uid)\n            ).where(\n                (TabPost2Tag.post_id == idd) &\n                (TabTag.kind != 'z')\n            ).order_by(\n                TabPost2Tag.order\n            )\n        return TabPost2Tag.select(\n            TabPost2Tag,\n            TabTag.slug.alias('tag_slug'),\n            TabTag.name.alias('tag_name')\n        ).join(TabTag, on=(TabPost2Tag.tag_id == TabTag.uid)).where(\n            (TabTag.kind == kind) &\n            (TabPost2Tag.post_id == idd)\n        ).order_by(\n            TabPost2Tag.order\n        )", "category": "Python"}, {"instruction": "def rho2sigma(self, rho0, Ra, Rs):\n        \"\"\"\n        converts 3d density into 2d projected density parameter\n        :param rho0:\n        :param Ra:\n        :param Rs:\n        :return:\n        \"\"\"\n", "input": "", "output": "        return np.pi * rho0 * Ra * Rs / (Rs + Ra)", "category": "Python"}, {"instruction": "def delete_refund_transaction_by_id(cls, refund_transaction_id, **kwargs):\n        \"\"\"Delete RefundTransaction\n\n        Delete an instance of RefundTransaction by its ID.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.delete_refund_transaction_by_id(refund_transaction_id, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str refund_transaction_id: ID of refundTransaction to delete. (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._delete_refund_transaction_by_id_with_http_info(refund_transaction_id, **kwargs)\n        else:\n            (data) = cls._delete_refund_transaction_by_id_with_http_info(refund_transaction_id, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def build_directory():\n    '''\n    Build the directory for Whoosh database, and locale.\n    '''\n", "input": "", "output": "    if os.path.exists('locale'):\n        pass\n    else:\n        os.mkdir('locale')\n    if os.path.exists(WHOOSH_DB_DIR):\n        pass\n    else:\n        os.makedirs(WHOOSH_DB_DIR)", "category": "Python"}, {"instruction": "def Allowance(self, wallet, owner_addr, requestor_addr):\n        \"\"\"\n        Return the amount of tokens that the `requestor_addr` account can transfer from the `owner_addr` account.\n\n        Args:\n            wallet (neo.Wallets.Wallet): a wallet instance.\n            owner_addr (str): public address of the account to transfer the given amount from.\n            requestor_addr (str): public address of the account that requests the transfer.\n\n        Returns:\n            tuple:\n                InvocationTransaction: the transaction.\n                int: the transaction fee.\n                list: the neo VM evaluation stack results.\n        \"\"\"\n", "input": "", "output": "        invoke_args = [self.ScriptHash.ToString(), 'allowance',\n                       [PromptUtils.parse_param(owner_addr, wallet), PromptUtils.parse_param(requestor_addr, wallet)]]\n\n        tx, fee, results, num_ops, engine_success = TestInvokeContract(wallet, invoke_args, None, True)\n\n        return tx, fee, results", "category": "Python"}, {"instruction": "def decaying(start, stop, decay):\n    \"\"\"Yield an infinite series of linearly decaying values.\"\"\"\n", "input": "", "output": "\n    curr = float(start)\n    while True:\n        yield max(curr, stop)\n        curr -= (decay)", "category": "Python"}, {"instruction": "def linkify_s_by_sg(self, servicegroups):\n        \"\"\"Link services with servicegroups\n\n        :param servicegroups: Servicegroups\n        :type servicegroups: alignak.objects.servicegroup.Servicegroups\n        :return: None\n        \"\"\"\n", "input": "", "output": "        for serv in self:\n            new_servicegroups = []\n            if hasattr(serv, 'servicegroups') and serv.servicegroups != '':\n                for sg_name in serv.servicegroups:\n                    sg_name = sg_name.strip()\n                    servicegroup = servicegroups.find_by_name(sg_name)\n                    if servicegroup is not None:\n                        new_servicegroups.append(servicegroup.uuid)\n                    else:\n                        err = \"Error: the servicegroup '%s' of the service '%s' is unknown\" %\\\n                              (sg_name, serv.get_dbg_name())\n                        serv.add_error(err)\n            serv.servicegroups = new_servicegroups", "category": "Python"}, {"instruction": "def _dispatch_container(self, textgroup, directory):\n        \"\"\" Run the dispatcher over a textgroup within a try/except block\n\n        .. note:: This extraction allows to change the dispatch routine \\\n            without having to care for the error dispatching\n\n        :param textgroup: Textgroup object that needs to be dispatched\n        :param directory: Directory in which the textgroup was found\n        \"\"\"\n", "input": "", "output": "        try:\n            self._dispatch(textgroup, directory)\n        except UndispatchedTextError as E:\n            self.logger.error(\"Error dispatching %s \", directory)\n            if self.RAISE_ON_UNDISPATCHED is True:\n                raise E", "category": "Python"}, {"instruction": "def cli(ctx, lsftdi, lsusb, lsserial, info):\n    \"\"\"System tools.\\n\n       Install with `apio install system`\"\"\"\n", "input": "", "output": "\n    exit_code = 0\n\n    if lsftdi:\n        exit_code = System().lsftdi()\n    elif lsusb:\n        exit_code = System().lsusb()\n    elif lsserial:\n        exit_code = System().lsserial()\n    elif info:\n        click.secho('Platform: ', nl=False)\n        click.secho(get_systype(), fg='yellow')\n    else:\n        click.secho(ctx.get_help())\n\n    ctx.exit(exit_code)", "category": "Python"}, {"instruction": "def get_python_path() -> str:\r\n    \"\"\" Accurately get python executable \"\"\"\n", "input": "", "output": "    python_bin = None\r\n    if os.name == 'nt':\r\n        python_root = os.path.abspath(\r\n            os.path.join(os.__file__, os.pardir, os.pardir))\r\n        python_bin = os.path.join(python_root, 'python.exe')\r\n    else:\r\n        python_root = os.path.abspath(\r\n            os.path.join(os.__file__, os.pardir, os.pardir, os.pardir))\r\n        python = os.__file__.rsplit('/')[-2]\r\n        python_bin = os.path.join(python_root, 'bin', python)\r\n    return python_bin", "category": "Python"}, {"instruction": "def wrap_connection(func_, *args, **kwargs):\n    \"\"\"\n    conn (connection) must be the last positional argument\n    in all wrapped functions\n\n    :param func_: <function> to call\n    :param args:  <tuple> positional arguments\n    :param kwargs: <dict> keyword arguments\n    :return:\n    \"\"\"\n", "input": "", "output": "    if not args[-1]:\n        new_args = list(args)\n        new_args[-1] = connect()\n        args = tuple(new_args)\n    return func_(*args, **kwargs)", "category": "Python"}, {"instruction": "def cycle_data(self, repository, cycle_interval, chunk_size, sleep_time):\n        \"\"\"Delete data older than cycle_interval, splitting the target data\ninto chunks of chunk_size size.\"\"\"\n", "input": "", "output": "\n        max_timestamp = datetime.datetime.now() - cycle_interval\n\n        # seperate datums into chunks\n        while True:\n            perf_datums_to_cycle = list(self.filter(\n                repository=repository,\n                push_timestamp__lt=max_timestamp).values_list('id', flat=True)[:chunk_size])\n            if not perf_datums_to_cycle:\n                # we're done!\n                break\n            self.filter(id__in=perf_datums_to_cycle).delete()\n            if sleep_time:\n                # Allow some time for other queries to get through\n                time.sleep(sleep_time)\n\n        # also remove any signatures which are (no longer) associated with\n        # a job\n        for signature in PerformanceSignature.objects.filter(\n                repository=repository):\n            if not self.filter(signature=signature).exists():\n                signature.delete()", "category": "Python"}, {"instruction": "def listup_sentence(self, data, counter=0):\n        '''\n        Divide string into sentence list.\n\n        Args:\n            data:               string.\n            counter:            recursive counter.\n\n        Returns:\n            List of sentences.\n\n        '''\n", "input": "", "output": "        delimiter = self.delimiter_list[counter]\n        sentence_list = []\n        [sentence_list.append(sentence + delimiter) for sentence in data.split(delimiter) if sentence != \"\"]\n        if counter + 1 < len(self.delimiter_list):\n            sentence_list_r = []\n            [sentence_list_r.extend(self.listup_sentence(sentence, counter+1)) for sentence in sentence_list]\n            sentence_list = sentence_list_r\n\n        return sentence_list", "category": "Python"}, {"instruction": "def _config(self):\n        \"\"\"Value to be written to the device's config register \"\"\"\n", "input": "", "output": "        config = 0\n        if self.mode == MODE_NORMAL:\n            config += (self._t_standby << 5)\n        if self._iir_filter:\n            config += (self._iir_filter << 2)\n        return config", "category": "Python"}, {"instruction": "def parse_style_rules(styles: str) -> CSSRuleList:\n    \"\"\"Make CSSRuleList object from style string.\"\"\"\n", "input": "", "output": "    rules = CSSRuleList()\n    for m in _style_rule_re.finditer(styles):\n        rules.append(CSSStyleRule(m.group(1), parse_style_decl(m.group(2))))\n    return rules", "category": "Python"}, {"instruction": "def finalize(self):\n        \"\"\" Called at clean up, when the editor is closed. Can be used to disconnect signals.\n            This is often called after the client (e.g. the inspector) is updated. If you want to\n            take action before the update, override prepareCommit instead.\n            Be sure to call the finalize of the super class if you override this function.\n        \"\"\"\n", "input": "", "output": "        for subEditor in self._subEditors:\n            self.removeSubEditor(subEditor)\n\n        self.cti.model.sigItemChanged.disconnect(self.modelItemChanged)\n        self.resetButton.clicked.disconnect(self.resetEditorValue)\n        self.cti = None # just to make sure it's not used again.\n        self.delegate = None", "category": "Python"}, {"instruction": "def truncate(self):\n        \"\"\"\n        Run a truncate statement on the table\n\n        :rtype: None\n        \"\"\"\n", "input": "", "output": "        for sql, bindings in self._grammar.compile_truncate(self).items():\n            self._connection.statement(sql, bindings)", "category": "Python"}, {"instruction": "def listIDs(basedir):\n    \"\"\"Lists digital object identifiers of Pairtree directory structure.\n\n    Walks a Pairtree directory structure to get IDs. Prepends prefix\n    found in pairtree_prefix file. Outputs to standard output.\n    \"\"\"\n", "input": "", "output": "    prefix = ''\n    # check for pairtree_prefix file\n    prefixfile = os.path.join(basedir, 'pairtree_prefix')\n    if os.path.isfile(prefixfile):\n        rff = open(prefixfile, 'r')\n        prefix = rff.readline().strip()\n        rff.close()\n    # check for pairtree_root dir\n    root = os.path.join(basedir, 'pairtree_root')\n    if os.path.isdir(root):\n        objects = pairtree.findObjects(root)\n        for obj in objects:\n            doi = os.path.split(obj)[1]\n            # print with prefix and original chars in place\n            print(prefix + pairtree.deSanitizeString(doi))\n    else:\n        print('pairtree_root directory not found')", "category": "Python"}, {"instruction": "def settings_dir(self):\n        \"\"\"\n        Directory that contains the the settings for the project\n        \"\"\"\n", "input": "", "output": "        path = os.path.join(self.dir, '.dsb')\n        utils.create_dir(path)\n        return os.path.realpath(path)", "category": "Python"}, {"instruction": "def _map(value, istart, istop, ostart, ostop):\n    \"\"\"\n    Helper function that implements the Processing function map(). For more\n    details see https://processing.org/reference/map_.html\n    http://stackoverflow.com/questions/17134839/how-does-the-map-function-in-processing-work\n    \"\"\"\n", "input": "", "output": "    return ostart + (ostop - ostart) * ((value - istart) / (istop - istart))", "category": "Python"}, {"instruction": "def split_flanks(self, _, result):\n        \"\"\"Return `result` without flanking whitespace.\n        \"\"\"\n", "input": "", "output": "        if not result.strip():\n            self.left, self.right = \"\", \"\"\n            return result\n\n        match = self.flank_re.match(result)\n        assert match, \"This regexp should always match\"\n        self.left, self.right = match.group(1), match.group(3)\n        return match.group(2)", "category": "Python"}, {"instruction": "def _validate_mapping(index, strict=False):\n    \"\"\"Check that an index mapping JSON file exists.\"\"\"\n", "input": "", "output": "    try:\n        settings.get_index_mapping(index)\n    except IOError:\n        if strict:\n            raise ImproperlyConfigured(\"Index '%s' has no mapping file.\" % index)\n        else:\n            logger.warning(\"Index '%s' has no mapping, relying on ES instead.\", index)", "category": "Python"}, {"instruction": "def _is_bst(root, min_value=float('-inf'), max_value=float('inf')):\n    \"\"\"Check if the binary tree is a BST (binary search tree).\n\n    :param root: Root node of the binary tree.\n    :type root: binarytree.Node | None\n    :param min_value: Minimum node value seen.\n    :type min_value: int | float\n    :param max_value: Maximum node value seen.\n    :type max_value: int | float\n    :return: True if the binary tree is a BST, False otherwise.\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "    if root is None:\n        return True\n    return (\n        min_value < root.value < max_value and\n        _is_bst(root.left, min_value, root.value) and\n        _is_bst(root.right, root.value, max_value)\n    )", "category": "Python"}, {"instruction": "def compute_md5_for_data_asbase64(data):\n    # type: (obj) -> str\n    \"\"\"Compute MD5 hash for bits and encode as Base64\n    :param any data: data to compute MD5 for\n    :rtype: str\n    :return: MD5 for data\n    \"\"\"\n", "input": "", "output": "    hasher = blobxfer.util.new_md5_hasher()\n    hasher.update(data)\n    return blobxfer.util.base64_encode_as_string(hasher.digest())", "category": "Python"}, {"instruction": "def drop_dose(self):\n        \"\"\"\n        Drop the maximum dose and related response values.\n        \"\"\"\n", "input": "", "output": "        for fld in (\"doses\", \"ns\", \"means\", \"stdevs\"):\n            arr = getattr(self, fld)[:-1]\n            setattr(self, fld, arr)\n        self._validate()", "category": "Python"}, {"instruction": "def children_to_list(node):\n    \"\"\"Organize children structure.\"\"\"\n", "input": "", "output": "    if node['type'] == 'item' and len(node['children']) == 0:\n        del node['children']\n    else:\n        node['type'] = 'folder'\n        node['children'] = list(node['children'].values())\n        node['children'].sort(key=lambda x: x['name'])\n        node['children'] = map(children_to_list, node['children'])\n    return node", "category": "Python"}, {"instruction": "def getCertificateExpireDate(self, CorpNum):\n        \"\"\" \uacf5\uc778\uc778\uc99d\uc11c \ub9cc\ub8cc\uc77c \ud655\uc778, \ub4f1\ub85d\uc5ec\ubd80 \ud655\uc778\uc6a9\ub3c4\ub85c \ud65c\uc6a9\uac00\ub2a5\n            args\n                CorpNum : \ud655\uc778\ud560 \ud68c\uc6d0 \uc0ac\uc5c5\uc790\ubc88\ud638\n            return\n                \ub4f1\ub85d\uc2dc \ub9cc\ub8cc\uc77c\uc790, \ubbf8\ub4f1\ub85d\uc2dc \ud574\ub2f9 PopbillException raise.\n            raise\n                PopbillException\n        \"\"\"\n", "input": "", "output": "        result = self._httpget('/Taxinvoice?cfg=CERT', CorpNum)\n        return datetime.strptime(result.certificateExpiration, '%Y%m%d%H%M%S')", "category": "Python"}, {"instruction": "def store_dcnm_net_dict(self, net_dict, direc):\n        \"\"\"Storing the DCNM net dict. \"\"\"\n", "input": "", "output": "        if direc == 'in':\n            self.in_dcnm_net_dict = net_dict\n        else:\n            self.out_dcnm_net_dict = net_dict", "category": "Python"}, {"instruction": "def as_dict(self):\n        \"\"\" returns a dictionary view of the option\n        \n        :returns: the option converted in a dict\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        info = {}\n        info[\"type\"] = self.__class__.__name__\n        info[\"help\"] = self.help\n        info[\"default\"] = self.default\n        info[\"multi\"] = self.multi\n        info[\"uniq\"] = self.uniq\n        info[\"choices\"] = self.choices\n        # TODO appel rec sur les attrs\n        #info[\"attrs\"] = self.attrs\n        return info", "category": "Python"}, {"instruction": "def get_vms(self, vm_names=None):\n        \"\"\"\n        Returns the vm objects associated with vm_names\n        if vm_names is None, return all the vms in the prefix\n        Args:\n            vm_names (list of str): The names of the requested vms\n        Returns\n            dict: Which contains the requested vm objects indexed by name\n        Raises:\n            utils.LagoUserException: If a vm name doesn't exist\n        \"\"\"\n", "input": "", "output": "        if not vm_names:\n            return self._vms.copy()\n\n        missing_vms = []\n        vms = {}\n        for name in vm_names:\n            try:\n                vms[name] = self._vms[name]\n            except KeyError:\n                # TODO: add resolver by suffix\n                missing_vms.append(name)\n\n        if missing_vms:\n            raise utils.LagoUserException(\n                'The following vms do not exist: \\n{}'.format(\n                    '\\n'.join(missing_vms)\n                )\n            )\n\n        return vms", "category": "Python"}, {"instruction": "def add_current_vrf(self):\n        \"\"\" Add VRF to filter list session variable\n        \"\"\"\n", "input": "", "output": "\n        vrf_id = request.json['vrf_id']\n\n        if vrf_id is not None:\n\n            if vrf_id == 'null':\n                vrf = VRF()\n            else:\n                vrf_id = int(vrf_id)\n                vrf = VRF.get(vrf_id)\n\n            session['current_vrfs'][vrf_id] = { 'id': vrf.id, 'rt': vrf.rt,\n                    'name': vrf.name, 'description': vrf.description }\n            session.save()\n\n        return json.dumps(session.get('current_vrfs', {}))", "category": "Python"}, {"instruction": "def inputs_from_dataframe(df, delays=(1, 2, 3), inputs=(1, 2, -1), outputs=None, normalize=True, verbosity=1):\n    \"\"\" Build a sequence of vectors suitable for \"activation\" by a neural net\n\n    Identical to `dataset_from_dataframe`, except that only the input vectors are\n    returned (not a full DataSet instance) and default values for 2 arguments are changed:\n        outputs: None\n\n    And only the input vectors are return\n    \"\"\"\n", "input": "", "output": "    ds = input_dataset_from_dataframe(df=df, delays=delays, inputs=inputs, outputs=outputs,\n                                      normalize=normalize, verbosity=verbosity)\n    return ds['input']", "category": "Python"}, {"instruction": "def _dens(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _dens\n        PURPOSE:\n           evaluate the density for this potential\n        INPUT:\n           R - Galactocentric cylindrical radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           the density\n        HISTORY:\n           2015-06-15 - Written - Bovy (IAS)\n        \"\"\"\n", "input": "", "output": "        return 3./4./nu.pi*self._b2*(R**2.+z**2.+self._b2)**-2.5", "category": "Python"}, {"instruction": "def multithread_predict_dataflow(dataflows, model_funcs):\n    \"\"\"\n    Running multiple `predict_dataflow` in multiple threads, and aggregate the results.\n\n    Args:\n        dataflows: a list of DataFlow to be used in :func:`predict_dataflow`\n        model_funcs: a list of callable to be used in :func:`predict_dataflow`\n\n    Returns:\n        list of dict, in the format used by\n        `DetectionDataset.eval_or_save_inference_results`\n    \"\"\"\n", "input": "", "output": "    num_worker = len(model_funcs)\n    assert len(dataflows) == num_worker\n    if num_worker == 1:\n        return predict_dataflow(dataflows[0], model_funcs[0])\n    kwargs = {'thread_name_prefix': 'EvalWorker'} if sys.version_info.minor >= 6 else {}\n    with ThreadPoolExecutor(max_workers=num_worker, **kwargs) as executor, \\\n            tqdm.tqdm(total=sum([df.size() for df in dataflows])) as pbar:\n        futures = []\n        for dataflow, pred in zip(dataflows, model_funcs):\n            futures.append(executor.submit(predict_dataflow, dataflow, pred, pbar))\n        all_results = list(itertools.chain(*[fut.result() for fut in futures]))\n        return all_results", "category": "Python"}, {"instruction": "def d3logpdf_dlink3(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Third order derivative log-likelihood function at y given inverse link of f w.r.t inverse link of f\n\n        .. math::\n            \\\\frac{d^{3} \\\\ln p(y_{i}|\\\\lambda(f_{i}))}{d^{3}\\\\lambda(f)} = \\\\frac{2y_{i}}{\\\\lambda(f)^{3}} - \\\\frac{2(1-y_{i}}{(1-\\\\lambda(f))^{3}}\n\n        :param inv_link_f: latent variables passed through inverse link of f.\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: Y_metadata not used in bernoulli\n        :returns: third derivative of log likelihood evaluated at points inverse_link(f)\n        :rtype: Nx1 array\n        \"\"\"\n", "input": "", "output": "        assert np.atleast_1d(inv_link_f).shape == np.atleast_1d(y).shape\n        #d3logpdf_dlink3 = 2*(y/(inv_link_f**3) - (1-y)/((1-inv_link_f)**3))\n        state = np.seterr(divide='ignore')\n        # TODO check y \\in {0, 1} or {-1, 1}\n        d3logpdf_dlink3 = np.where(y==1, 2./(inv_link_f**3), -2./((1.-inv_link_f)**3))\n        np.seterr(**state)\n        return d3logpdf_dlink3", "category": "Python"}, {"instruction": "def _createShapelet(self, coeffs):\n        \"\"\"\n        returns a shapelet array out of the coefficients *a, up to order l\n\n        :param num_l: order of shapelets\n        :type num_l: int.\n        :param coeff: shapelet coefficients\n        :type coeff: floats\n        :returns:  complex array\n        :raises: AttributeError, KeyError\n        \"\"\"\n", "input": "", "output": "        n_coeffs = len(coeffs)\n        num_n = self._get_num_n(n_coeffs)\n        shapelets=np.zeros((num_n+1, num_n+1))\n        n = 0\n        k = 0\n        for coeff in coeffs:\n            shapelets[n-k][k] = coeff\n            k += 1\n            if k == n + 1:\n                n += 1\n                k = 0\n        return shapelets", "category": "Python"}, {"instruction": "def in_reply_to(self) -> Optional[UnstructuredHeader]:\n        \"\"\"The ``In-Reply-To`` header.\"\"\"\n", "input": "", "output": "        try:\n            return cast(UnstructuredHeader, self[b'in-reply-to'][0])\n        except (KeyError, IndexError):\n            return None", "category": "Python"}, {"instruction": "def save(self, must_create=False):\n        \"\"\"\n        Saves the current session data to the database. If 'must_create' is\n        True, a database error will be raised if the saving operation doesn't\n        create a *new* entry (as opposed to possibly updating an existing\n        entry).\n        \"\"\"\n", "input": "", "output": "        obj = Session(\n            session_key=self._get_or_create_session_key(),\n            session_data=self.encode(self._get_session(no_load=must_create)),\n            expire_date=self.get_expiry_date(),\n            user_agent=self.user_agent,\n            user_id=self.user_id,\n            ip=self.ip,\n        )\n        using = router.db_for_write(Session, instance=obj)\n        try:\n            with transaction.atomic(using):\n                obj.save(force_insert=must_create, using=using)\n        except IntegrityError as e:\n            if must_create and 'session_key' in str(e):\n                raise CreateError\n            raise", "category": "Python"}, {"instruction": "def add(self, dist):\n        \"\"\"\n        Add a distribution to the cache.\n        :param dist: The distribution to add.\n        \"\"\"\n", "input": "", "output": "        if dist.path not in self.path:\n            self.path[dist.path] = dist\n            self.name.setdefault(dist.key, []).append(dist)", "category": "Python"}, {"instruction": "def get_name(self, use_alias=True):\n        \"\"\"\n        Gets the name to reference the sorted field\n\n        :return: the name to reference the sorted field\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        if self.desc:\n            direction = 'DESC'\n        else:\n            direction = 'ASC'\n\n        if use_alias:\n            return '{0} {1}'.format(self.field.get_identifier(), direction)\n        return '{0} {1}'.format(self.field.get_select_sql(), direction)", "category": "Python"}, {"instruction": "def _setup_parser(self, filename=None):\n        \"\"\"\n        Configure the ConfigParser instance the way we want it.\n\n        Args:\n            filename (str) or None\n\n        Returns:\n            SafeConfigParser\n        \"\"\"\n", "input": "", "output": "        assert isinstance(filename, str) or filename is None\n\n        # If we are not overriding the config filename\n        if not filename:\n            filename = MACKUP_CONFIG_FILE\n\n        parser = configparser.SafeConfigParser(allow_no_value=True)\n        parser.read(os.path.join(os.path.join(os.environ['HOME'], filename)))\n\n        return parser", "category": "Python"}, {"instruction": "def _get_authorization_headers(self) -> dict:\n        \"\"\"Constructs and returns the Authorization header for the client app.\n\n        Args:\n            None\n\n        Returns:\n            header dict for communicating with the authorization endpoints\n        \"\"\"\n", "input": "", "output": "        auth = base64.encodestring((self.client_id + ':' + self.client_secret).encode('latin-1')).decode('latin-1')\n        auth = auth.replace('\\n', '').replace(' ', '')\n        auth = 'Basic {}'.format(auth)\n        headers = {'Authorization': auth}\n        return headers", "category": "Python"}, {"instruction": "def invert_map(map):\n\t\"\"\"\n\tGiven a dictionary, return another dictionary with keys and values\n\tswitched. If any of the values resolve to the same key, raises\n\ta ValueError.\n\n\t>>> numbers = dict(a=1, b=2, c=3)\n\t>>> letters = invert_map(numbers)\n\t>>> letters[1]\n\t'a'\n\t>>> numbers['d'] = 3\n\t>>> invert_map(numbers)\n\tTraceback (most recent call last):\n\t...\n\tValueError: Key conflict in inverted mapping\n\t\"\"\"\n", "input": "", "output": "\tres = dict((v, k) for k, v in map.items())\n\tif not len(res) == len(map):\n\t\traise ValueError('Key conflict in inverted mapping')\n\treturn res", "category": "Python"}, {"instruction": "def API_GET(self):  # pylint: disable=arguments-differ\n        \"\"\"\n            Returns {\"authenticated\": false} or {\"authenticated\": true, \"username\": \"your_username\"} (always 200 OK)\n        \"\"\"\n", "input": "", "output": "        if self.user_manager.session_logged_in():\n            return 200, {\"authenticated\": True, \"username\": self.user_manager.session_username()}\n        else:\n            return 200, {\"authenticated\": False}", "category": "Python"}, {"instruction": "def ordered_cols(self, columns, section):\n        \"\"\"Return ordered list of columns, from given columns and the name of the section\n        \"\"\"\n", "input": "", "output": "        columns = list(columns)  # might be a tuple\n        fixed_cols = [self.key]\n        if section.lower() == \"different\":\n            fixed_cols.extend([Differ.CHANGED_MATCH_KEY, Differ.CHANGED_OLD, Differ.CHANGED_NEW])\n        map(columns.remove, fixed_cols)\n        columns.sort()\n        return fixed_cols + columns", "category": "Python"}, {"instruction": "def monthly_clear_sky_conditions(self):\n        \"\"\"A list of 12 monthly clear sky conditions that are used on the design days.\"\"\"\n", "input": "", "output": "        if self._monthly_tau_diffuse is [] or self._monthly_tau_beam is []:\n            return [OriginalClearSkyCondition(i, 21) for i in xrange(1, 13)]\n        return [RevisedClearSkyCondition(i, 21, x, y) for i, x, y in zip(\n            list(xrange(1, 13)), self._monthly_tau_beam, self._monthly_tau_diffuse)]", "category": "Python"}, {"instruction": "def get_item(self, **kwargs):\n        \"\"\" Get collection item taking into account generated queryset\n        of parent view.\n\n        This method allows working with nested resources properly. Thus an item\n        returned by this method will belong to its parent view's queryset, thus\n        filtering out objects that don't belong to the parent object.\n\n        Returns an object from the applicable ACL. If ACL wasn't applied, it is\n        applied explicitly.\n        \"\"\"\n", "input": "", "output": "        if six.callable(self.context):\n            self.reload_context(es_based=False, **kwargs)\n\n        objects = self._parent_queryset()\n        if objects is not None and self.context not in objects:\n            raise JHTTPNotFound('{}({}) not found'.format(\n                self.Model.__name__,\n                self._get_context_key(**kwargs)))\n\n        return self.context", "category": "Python"}, {"instruction": "def as_completed(*async_result_wrappers):\n    \"\"\"\n    Yields results as they become available from asynchronous method calls.\n\n    Example usage\n\n    ::\n\n        async_calls = [service.call_method_async(\"do_stuff\", (x,)) for x in range(25)]\n\n        for async_call in gemstone.as_completed(*async_calls):\n            print(\"just finished with result \", async_call.result())\n\n    :param async_result_wrappers: :py:class:`gemstone.client.structs.AsyncMethodCall` instances.\n    :return: a generator that yields items as soon they results become available.\n\n    .. versionadded:: 0.5.0\n    \"\"\"\n", "input": "", "output": "    for item in async_result_wrappers:\n        if not isinstance(item, AsyncMethodCall):\n            raise TypeError(\"Got non-AsyncMethodCall object: {}\".format(item))\n\n    wrappers_copy = list(async_result_wrappers)\n\n    while len(wrappers_copy):\n        completed = list(filter(lambda x: x.finished(), wrappers_copy))\n        if not len(completed):\n            continue\n\n        for item in completed:\n            wrappers_copy.remove(item)\n            yield item", "category": "Python"}, {"instruction": "def _update_response_location_header(self, resource):\n        \"\"\"\n        Adds a new or replaces an existing Location header to the response\n        headers pointing to the URL of the given resource.\n        \"\"\"\n", "input": "", "output": "        location = resource_to_url(resource, request=self.request)\n        loc_hdr = ('Location', location)\n        hdr_names = [hdr[0].upper() for hdr in self.request.response.headerlist]\n        try:\n            idx = hdr_names.index('LOCATION')\n        except ValueError:\n            self.request.response.headerlist.append(loc_hdr)\n        else:\n            # Replace existing location header.\n            # FIXME: It is not clear under which conditions this happens, so\n            #        we do not have a test for it yet.\n            self.request.response.headerlist[idx] = loc_hdr", "category": "Python"}, {"instruction": "def display_string_dump(self, section_spec):\r\n        \"\"\" Display a strings dump of a section. section_spec is either a\r\n            section number or a name.\r\n        \"\"\"\n", "input": "", "output": "        section = _section_from_spec(self.elf_file, section_spec)\r\n        if section is None:\r\n            print(\"Section '%s' does not exist in the file!\" % section_spec)\r\n            return None\r\n\r\n        data = section.data()\r\n        dataptr = 0\r\n\r\n        strs = []\r\n        while dataptr < len(data):\r\n            while dataptr < len(data) and not 32 <= byte2int(data[dataptr]) <= 127:\r\n                dataptr += 1\r\n\r\n            if dataptr >= len(data):\r\n                break\r\n\r\n            endptr = dataptr\r\n            while endptr < len(data) and byte2int(data[endptr]) != 0:\r\n                endptr += 1\r\n\r\n            strs.append(binascii.b2a_hex(\r\n                data[dataptr:endptr]).decode().upper())\r\n            dataptr = endptr\r\n\r\n        return strs", "category": "Python"}, {"instruction": "def lately(self, count=15):\n        \"\"\"\n        Show ``count`` most-recently modified files by mtime\n\n        Yields:\n            tuple: (strftime-formatted mtime, self.fpath-relative file path)\n        \"\"\"\n", "input": "", "output": "        excludes = '|'.join(('*.pyc', '*.swp', '*.bak', '*~'))\n        cmd = ('''find . -printf \"%%T@ %%p\\\\n\" '''\n               '''| egrep -v '%s' '''\n               '''| sort -n '''\n               '''| tail -n %d''') % (excludes, count)\n        op = self.sh(cmd, shell=True)\n        for l in op.split('\\n'):\n            l = l.strip()\n            if not l:\n                continue\n            mtime, fname = l.split(' ', 1)\n            mtime = datetime.datetime.fromtimestamp(float(mtime))\n            mtimestr = dtformat(mtime)\n            yield mtimestr, fname", "category": "Python"}, {"instruction": "def ind2sub(ind, dimensions):\n    \"\"\"\n    Calculates subscripts for indices into regularly spaced matrixes.\n    \"\"\"\n", "input": "", "output": "    # check that the index is within range\n    if ind >= np.prod(dimensions):\n        raise RuntimeError(\"ind2sub: index exceeds array size\")\n    cum_dims = list(dimensions)\n    cum_dims.reverse()\n    m = 1\n    mult = []\n    for d in cum_dims:\n        m = m*d\n        mult.append(m)\n    mult.pop()\n    mult.reverse()\n    mult.append(1)\n    indices = []\n    for d in mult:\n        indices.append((ind/d)+1)\n        ind = ind - (ind/d)*d\n    return indices", "category": "Python"}, {"instruction": "def _Resample(self, stats, target_size):\n    \"\"\"Resamples the stats to have a specific number of data points.\"\"\"\n", "input": "", "output": "    t_first = stats[0][0]\n    t_last = stats[-1][0]\n    interval = (t_last - t_first) / target_size\n\n    result = []\n\n    current_t = t_first\n    current_v = 0\n    i = 0\n    while i < len(stats):\n      stat_t = stats[i][0]\n      stat_v = stats[i][1]\n      if stat_t <= (current_t + interval):\n        # Always add the last value in an interval to the result.\n        current_v = stat_v\n        i += 1\n      else:\n        result.append([current_t + interval, current_v])\n        current_t += interval\n\n    result.append([current_t + interval, current_v])\n    return result", "category": "Python"}, {"instruction": "def max_bipartite_matching2(bigraph):\n    \"\"\"Bipartie maximum matching\n\n    :param bigraph: adjacency list, index = vertex in U,\n                                    value = neighbor list in V\n    :comment: U and V can have different cardinalities\n    :returns: matching list, match[v] == u iff (u, v) in matching\n    :complexity: `O(|V|*|E|)`\n    \"\"\"\n", "input": "", "output": "    nU = len(bigraph)\n    # the following line works only in Python version \u2265 2.5\n    # nV = max(max(adjlist, default=-1) for adjlist in bigraph) + 1\n    nV = 0\n    for adjlist in bigraph:\n        for v in adjlist:\n            if v + 1 > nV:\n                nV = v + 1\n    match = [None] * nV\n    for u in range(nU):\n        augment(u, bigraph, [False] * nV, match)\n    return match", "category": "Python"}, {"instruction": "def render(self):\n        \"\"\"\n        Returns whole information on the progress for the current's bar state,\n        as a string.\n        \"\"\"\n", "input": "", "output": "        elements = [self.render_element(e) for e in self.widgets]\n        progressbar = self.separator.join(elements)\n        width = get_terminal_width()\n        if width:\n            # leave last column for cursor\n            progressbar = progressbar.ljust(width - 1)\n        return progressbar", "category": "Python"}, {"instruction": "def _startSchedulesNode(self, name, attrs):\n        \"\"\"Process the start of a node under xtvd/schedules\"\"\"\n", "input": "", "output": "\n        if name == 'schedule':\n            self._programId = attrs.get('program')\n            self._stationId = attrs.get('station')\n            self._time = self._parseDateTime(attrs.get('time'))\n            self._duration = self._parseDuration(attrs.get('duration'))\n            self._new = attrs.has_key('new')\n            self._stereo = attrs.has_key('stereo')\n            self._subtitled = attrs.has_key('subtitled')\n            self._hdtv = attrs.has_key('hdtv')\n            self._closeCaptioned = attrs.has_key('closeCaptioned')\n            self._ei = attrs.has_key('ei')\n            self._tvRating = attrs.get('tvRating')\n            self._dolby = attrs.get('dolby')\n            self._partNumber = None\n            self._partTotal = None\n        elif name == 'part':\n            self._partNumber = attrs.get('number')\n            self._partTotal = attrs.get('total')", "category": "Python"}, {"instruction": "def _check_disabled(self):\n        \"\"\"Check if health check is disabled.\n\n        It logs a message if health check is disabled and it also adds an item\n        to the action queue based on 'on_disabled' setting.\n\n        Returns:\n            True if check is disabled otherwise False.\n\n        \"\"\"\n", "input": "", "output": "        if self.config['check_disabled']:\n            if self.config['on_disabled'] == 'withdraw':\n                self.log.info(\"Check is disabled and ip_prefix will be \"\n                              \"withdrawn\")\n                self.log.info(\"adding %s in the queue\", self.ip_with_prefixlen)\n                self.action.put(self.del_operation)\n                self.log.info(\"Check is now permanently disabled\")\n            elif self.config['on_disabled'] == 'advertise':\n                self.log.info(\"check is disabled, ip_prefix wont be withdrawn\")\n                self.log.info(\"adding %s in the queue\", self.ip_with_prefixlen)\n                self.action.put(self.add_operation)\n                self.log.info('check is now permanently disabled')\n\n            return True\n\n        return False", "category": "Python"}, {"instruction": "def delete_from_gateway(\n        gateway, job, grouping_key=None, timeout=30, handler=default_handler):\n    \"\"\"Delete metrics from the given pushgateway.\n\n    `gateway` the url for your push gateway. Either of the form\n              'http://pushgateway.local', or 'pushgateway.local'.\n              Scheme defaults to 'http' if none is provided\n    `job` is the job label to be attached to all pushed metrics\n    `grouping_key` please see the pushgateway documentation for details.\n                   Defaults to None\n    `timeout` is how long delete will attempt to connect before giving up.\n              Defaults to 30s, can be set to None for no timeout.\n    `handler` is an optional function which can be provided to perform\n              requests to the 'gateway'.\n              Defaults to None, in which case an http or https request\n              will be carried out by a default handler.\n              See the 'prometheus_client.push_to_gateway' documentation\n              for implementation requirements.\n\n    This deletes metrics with the given job and grouping_key.\n    This uses the DELETE HTTP method.\"\"\"\n", "input": "", "output": "    _use_gateway('DELETE', gateway, job, None, grouping_key, timeout, handler)", "category": "Python"}, {"instruction": "def _get_fb_device(self):\n        \"\"\"\n        Internal. Finds the correct frame buffer device for the sense HAT\n        and returns its /dev name.\n        \"\"\"\n", "input": "", "output": "\n        device = None\n\n        for fb in glob.glob('/sys/class/graphics/fb*'):\n            name_file = os.path.join(fb, 'name')\n            if os.path.isfile(name_file):\n                with open(name_file, 'r') as f:\n                    name = f.read()\n                if name.strip() == self.SENSE_HAT_FB_NAME:\n                    fb_device = fb.replace(os.path.dirname(fb), '/dev')\n                    if os.path.exists(fb_device):\n                        device = fb_device\n                        break\n\n        return device", "category": "Python"}, {"instruction": "def smtp_msgdata(self):\n        \"\"\"This method handles not a real smtp command. It is called when the\n        whole message was received (multi-line DATA command is completed).\"\"\"\n", "input": "", "output": "        msg_data = self.arguments()\n        self._command_parser.switch_to_command_mode()\n        self._check_size_restrictions(msg_data)\n        decision, response_sent = self.is_allowed('accept_msgdata', msg_data, self._message)\n        if decision:\n            self._message.msg_data = msg_data\n            new_message = self._copy_basic_settings(self._message)\n            self._deliverer.new_message_accepted(self._message)\n            if not response_sent:\n                self.reply(250, 'OK')\n                # Now we must not loose the message anymore!\n            self._message = new_message\n        elif not decision:\n            raise PolicyDenial(response_sent, 550, 'Message content is not acceptable')", "category": "Python"}, {"instruction": "def RegisterRecordType(cls, record_class):\n        \"\"\"Register a known record type in KNOWN_CLASSES.\n\n        Args:\n            record_class (UpdateRecord): An update record subclass.\n        \"\"\"\n", "input": "", "output": "\n        record_type = record_class.MatchType()\n        if record_type not in UpdateRecord.KNOWN_CLASSES:\n            UpdateRecord.KNOWN_CLASSES[record_type] = []\n\n        UpdateRecord.KNOWN_CLASSES[record_type].append(record_class)", "category": "Python"}, {"instruction": "def assertNotEqual(first, second, message=None):\n    \"\"\"\n    Assert that first does not equal second.\n\n    :param first: First part to evaluate\n    :param second: Second part to evaluate\n    :param message: Failure message\n    :raises: TestStepFail if not first != second\n    \"\"\"\n", "input": "", "output": "    if not first != second:\n        raise TestStepFail(\n            format_message(message) if message is not None else \"Assert: %s == %s\" % (str(first),\n                                                                                      str(second)))", "category": "Python"}, {"instruction": "def getFlags (self, ifname):\n        \"\"\"Get the flags for an interface\"\"\"\n", "input": "", "output": "        try:\n            result = self._ioctl(self.SIOCGIFFLAGS, self._getifreq(ifname))\n        except IOError as msg:\n            log.warn(LOG_CHECK,\n                 \"error getting flags for interface %r: %s\", ifname, msg)\n            return 0\n        # extract the interface's flags from the return value\n        flags, = struct.unpack('H', result[16:18])\n        # return \"UP\" bit\n        return flags", "category": "Python"}, {"instruction": "def clear_matplotlib_ticks(ax=None, axis=\"both\"):\n    \"\"\"\n    Clears the default matplotlib axes, or the one specified by the axis\n    argument.\n\n    Parameters\n    ----------\n    ax: Matplotlib AxesSubplot, None\n        The subplot to draw on.\n    axis: string, \"both\"\n        The axis to clear: \"x\" or \"horizontal\", \"y\" or \"vertical\", or \"both\"\n    \"\"\"\n", "input": "", "output": "    if not ax:\n        return\n    if axis.lower() in [\"both\", \"x\", \"horizontal\"]:\n        ax.set_xticks([], [])\n    if axis.lower() in [\"both\", \"y\", \"vertical\"]:\n        ax.set_yticks([], [])", "category": "Python"}, {"instruction": "def _addrs2nodes(addrs, G):\n    \"\"\"Map agent addresses to nodes in the graph.\n    \"\"\"\n", "input": "", "output": "    for i, n in enumerate(G.nodes()):\n        G.node[n]['addr'] = addrs[i]", "category": "Python"}, {"instruction": "def load_if_not_loaded(widget, filenames, verbose=False, delay=0.1, force=False, local=True, evaluator=None):\n    \"\"\"\n    Load a javascript file to the Jupyter notebook context,\n    unless it was already loaded.\n    \"\"\"\n", "input": "", "output": "    if evaluator is None:\n        evaluator = EVALUATOR  # default if not specified.\n    for filename in filenames:\n        loaded = False\n        if force or not filename in LOADED_JAVASCRIPT:\n            js_text = get_text_from_file_name(filename, local)\n            if verbose:\n                print(\"loading javascript file\", filename, \"with\", evaluator)\n            evaluator(widget, js_text)\n            LOADED_JAVASCRIPT.add(filename)\n            loaded = True\n        else:\n            if verbose:\n                print (\"not reloading javascript file\", filename)\n        if loaded and delay > 0:\n            if verbose:\n                print (\"delaying to allow JS interpreter to sync.\")\n            time.sleep(delay)", "category": "Python"}, {"instruction": "def get_edges_with_citation(self, citation: Citation) -> List[Edge]:\n        \"\"\"Get the edges with the given citation.\"\"\"\n", "input": "", "output": "        return self.session.query(Edge).join(Evidence).filter(Evidence.citation == citation)", "category": "Python"}, {"instruction": "def plot(self, value=None, pixel=None):\n        \"\"\"\n        Plot the ROI\n        \"\"\"\n", "input": "", "output": "        # DEPRECATED\n        import ugali.utils.plotting\n\n        map_roi = np.array(hp.UNSEEN \\\n                              * np.ones(hp.nside2npix(self.config.params['coords']['nside_pixel'])))\n        \n        if value is None:\n            #map_roi[self.pixels] = ugali.utils.projector.angsep(self.lon, self.lat, self.centers_lon, self.centers_lat)\n            map_roi[self.pixels] = 1\n            map_roi[self.pixels_annulus] = 0\n            map_roi[self.pixels_target] = 2\n        elif value is not None and pixel is None:\n            map_roi[self.pixels] = value\n        elif value is not None and pixel is not None:\n            map_roi[pixel] = value\n        else:\n            logger.error(\"Can't parse input\")\n        \n        ugali.utils.plotting.zoomedHealpixMap('Region of Interest',\n                                              map_roi,\n                                              self.lon, self.lat,\n                                              self.config.params['coords']['roi_radius'])", "category": "Python"}, {"instruction": "def get_time(self, **params):\n        \"\"\"https://developers.coinbase.com/api/v2#time\"\"\"\n", "input": "", "output": "        response = self._get('v2', 'time', params=params)\n        return self._make_api_object(response, APIObject)", "category": "Python"}, {"instruction": "def python_value(self, value):\n        \"\"\"Return the value in the data base as an arrow object.\n\n        Returns:\n            arrow.Arrow: An instance of arrow with the field filled in.\n        \"\"\"\n", "input": "", "output": "        value = super(ArrowDateTimeField, self).python_value(value)\n\n        if (isinstance(value, (datetime.datetime, datetime.date,\n                               string_types))):\n            return arrow.get(value)\n\n        return value", "category": "Python"}, {"instruction": "def entity_data(self, entity_type, entity_id, history_index):\n        \"\"\"Return the data dict for an entity at a specific index of its\n        history.\n\n        \"\"\"\n", "input": "", "output": "        return self.entity_history(entity_type, entity_id)[history_index]", "category": "Python"}, {"instruction": "def build_subtree_strut(self, result, *args, **kwargs):\n        \"\"\"\n        Returns a dictionary in form of\n        {node:Resource, children:{node_id: Resource}}\n\n        :param result:\n        :return:\n        \"\"\"\n", "input": "", "output": "        return self.service.build_subtree_strut(result=result, *args, **kwargs)", "category": "Python"}, {"instruction": "def split_name(name):\n    \"\"\"Splits a (possibly versioned) name into unversioned name and version.\n\n       Returns a tuple ``(unversioned_name, version)``, where ``version`` may\n       be ``None``.\n    \"\"\"\n", "input": "", "output": "    s = name.rsplit('@', 1)\n    if len(s) == 1:\n        return s[0], None\n    else:\n        try:\n            return s[0], int(s[1])\n        except ValueError:\n            raise ValueError(\"Invalid Filetracker filename: version must \"\n                             \"be int, not %r\" % (s[1],))", "category": "Python"}, {"instruction": "def create_component(self, module_name):\n        '''Create a component out of a loaded module.\n\n        Turns a previously-loaded shared module into a component in the\n        manager. This will invalidate any objects that are children of this\n        node.\n\n        The @ref module_name argument can contain options that set various\n        properties of the new component. These must be appended to the module\n        name, prefixed by a question mark for each property, in key=value\n        format. For example, to change the instance name of the new component,\n        append '?instance_name=new_name' to the module name.\n\n        @param module_name Name of the module to turn into a component.\n        @raises FailedToCreateComponentError\n\n        '''\n", "input": "", "output": "        with self._mutex:\n            if not self._obj.create_component(module_name):\n                raise exceptions.FailedToCreateComponentError(module_name)\n            # The list of child components will have changed now, so it must be\n            # reparsed.\n            self._parse_component_children()", "category": "Python"}, {"instruction": "def parse_eggs_list(path):\n    \"\"\"Parse eggs list from the script at the given path\n    \"\"\"\n", "input": "", "output": "    with open(path, 'r') as script:\n        data = script.readlines()\n        start = 0\n        end = 0\n        for counter, line in enumerate(data):\n            if not start:\n                if 'sys.path[0:0]' in line:\n                    start = counter + 1\n            if counter >= start and not end:\n                if ']' in line:\n                    end = counter\n        script_eggs = tidy_eggs_list(data[start:end])\n    return script_eggs", "category": "Python"}, {"instruction": "def filename(out_dir, name, undefine=False):\n    \"\"\"Generate the filename\"\"\"\n", "input": "", "output": "    if undefine:\n        prefix = 'undef_'\n    else:\n        prefix = ''\n    return os.path.join(out_dir, '{0}{1}.hpp'.format(prefix, name.lower()))", "category": "Python"}, {"instruction": "def collect_tokens_until(self, token_type):\n        \"\"\"Yield the item tokens in a comma-separated tag collection.\"\"\"\n", "input": "", "output": "        self.next()\n        if self.current_token.type == token_type:\n            return\n\n        while True:\n            yield self.current_token\n\n            self.next()\n            if self.current_token.type == token_type:\n                return\n\n            if self.current_token.type != 'COMMA':\n                raise self.error(f'Expected comma but got '\n                                 f'{self.current_token.value!r}')\n            self.next()", "category": "Python"}, {"instruction": "def getSegmentOnCell(self, c, i, segIdx):\n    \"\"\"Return the segment on cell (c,i) with index sidx.\n    Returns the segment as following list:\n      [  [segmentID, sequenceSegmentFlag, frequency],\n         [col1, idx1, perm1],\n         [col2, idx2, perm2], ...\n      ]\n    \"\"\"\n", "input": "", "output": "    return self.cells[c][i][segIdx]", "category": "Python"}, {"instruction": "def get_keys(self, bucket, timeout=None):\n        \"\"\"\n        Fetch a list of keys for the bucket\n        \"\"\"\n", "input": "", "output": "        bucket_type = self._get_bucket_type(bucket.bucket_type)\n        url = self.key_list_path(bucket.name, bucket_type=bucket_type,\n                                 timeout=timeout)\n        status, _, body = self._request('GET', url)\n\n        if status == 200:\n            props = json.loads(bytes_to_str(body))\n            return props['keys']\n        else:\n            raise RiakError('Error listing keys.')", "category": "Python"}, {"instruction": "def __learn_oneself(self):\r\n        \"\"\"calculate cardinality, total and average string length\"\"\"\n", "input": "", "output": "        if not self.__parent_path or not self.__text_nodes:\r\n            raise Exception(\"This error occurred because the step constructor\\\r\n                            had insufficient textnodes or it had empty string\\\r\n                            for its parent xpath\")\r\n        # Iterate through text nodes and sum up text length\r\n        # TODO: consider naming this child_count or cardinality\r\n        # or branch_cnt\r\n        self.tnodes_cnt = len(self.__text_nodes)\r\n        # consider naming this total\r\n        self.ttl_strlen = sum([len(tnode) for tnode in self.__text_nodes])\r\n        # consider naming this average\r\n        self.avg_strlen = self.ttl_strlen/self.tnodes_cnt", "category": "Python"}, {"instruction": "def launch_browser(self, soup):\n        \"\"\"Launch a browser to display a page, for debugging purposes.\n\n        :param: soup: Page contents to display, supplied as a bs4 soup object.\n        \"\"\"\n", "input": "", "output": "        with tempfile.NamedTemporaryFile(delete=False, suffix='.html') as file:\n            file.write(soup.encode())\n        webbrowser.open('file://' + file.name)", "category": "Python"}, {"instruction": "def return_response(self, value=None):\n        # type: (Any) -> Tuple[Callback, Return]\n        \"\"\"Create a Return Response object to signal a return value\"\"\"\n", "input": "", "output": "        response = Return(id=self.id, value=value)\n        return self.callback, response", "category": "Python"}, {"instruction": "def searchExpressionLevelsInDb(\n            self, rnaQuantId, names=[], threshold=0.0, startIndex=0,\n            maxResults=0):\n        \"\"\"\n        :param rnaQuantId: string restrict search by quantification id\n        :param threshold: float minimum expression values to return\n        :return an array of dictionaries, representing the returned data.\n        \"\"\"\n", "input": "", "output": "        sql = (\"SELECT * FROM Expression WHERE \"\n               \"rna_quantification_id = ? \"\n               \"AND expression > ? \")\n        sql_args = (rnaQuantId, threshold)\n        if len(names) > 0:\n            sql += \"AND name in (\"\n            sql += \",\".join(['?' for name in names])\n            sql += \") \"\n            for name in names:\n                sql_args += (name,)\n        sql += sqlite_backend.limitsSql(\n            startIndex=startIndex, maxResults=maxResults)\n        query = self._dbconn.execute(sql, sql_args)\n        return sqlite_backend.iterativeFetch(query)", "category": "Python"}, {"instruction": "def _assemble_regulate_amount(self, stmt):\n        \"\"\"Example: p(HGNC:ELK1) => p(HGNC:FOS)\"\"\"\n", "input": "", "output": "        activates = isinstance(stmt, IncreaseAmount)\n        relation = get_causal_edge(stmt, activates)\n        self._add_nodes_edges(stmt.subj, stmt.obj, relation, stmt.evidence)", "category": "Python"}, {"instruction": "def _update_with_replace(self, other, key, val=None, **options):\n    \"\"\"\n    Replace value of a mapping object 'self' with 'other' has if both have same\n    keys on update. Otherwise, just keep the value of 'self'.\n\n    :param self: mapping object to update with 'other'\n    :param other: mapping object to update 'self'\n    :param key: key of mapping object to update\n    :param val: value to update self alternatively\n\n    :return: None but 'self' will be updated\n    \"\"\"\n", "input": "", "output": "    self[key] = other[key] if val is None else val", "category": "Python"}, {"instruction": "def formatTime (self, record, datefmt=None):\n        \"\"\"Returns the creation time of the given LogRecord as formatted text.\n\n        NOTE: The datefmt parameter and self.converter (the time\n        conversion method) are ignored.  BSD Syslog Protocol messages\n        always use local time, and by our convention, Syslog Protocol\n        messages use UTC.\n        \"\"\"\n", "input": "", "output": "        if self.bsd:\n            lt_ts = datetime.datetime.fromtimestamp(record.created)\n            ts = lt_ts.strftime(self.BSD_DATEFMT)\n            if ts[4] == '0':\n                ts = ts[0:4] + ' ' + ts[5:]\n        else:\n            utc_ts = datetime.datetime.utcfromtimestamp(record.created)\n            ts     = utc_ts.strftime(self.SYS_DATEFMT)\n        return ts", "category": "Python"}, {"instruction": "def tx_xml(self):\n        \"\"\"\n        Return the ``<c:tx>`` (tx is short for 'text') element for this\n        series as unicode text. This element contains the series name.\n        \"\"\"\n", "input": "", "output": "        return self._tx_tmpl.format(**{\n            'wksht_ref':   self._series.name_ref,\n            'series_name': self.name,\n            'nsdecls':     '',\n        })", "category": "Python"}, {"instruction": "def _locate_files_to_delete(algorithm, rotated_files, next_rotation_id):\n    \"\"\"Looks for hanoi_rotator generated files that occupy the same slot\n    that will be given to rotation_id.\n    \"\"\"\n", "input": "", "output": "    rotation_slot = algorithm.id_to_slot(next_rotation_id)\n    for a_path, a_rotation_id in rotated_files:\n        if rotation_slot == algorithm.id_to_slot(a_rotation_id):\n            yield a_path", "category": "Python"}, {"instruction": "def performXpath(parent, xpath):\n    \"\"\" Perform an XPath on an element and indicate if we need to loop over it to find something\n\n    :param parent: XML Node on which to perform XPath\n    :param xpath: XPath to run\n    :return: (Result, Need to loop Indicator)\n    \"\"\"\n", "input": "", "output": "    loop = False\n    if xpath.startswith(\".//\"):\n        result = parent.xpath(\n            xpath.replace(\".//\", \"./\", 1),\n            namespaces=XPATH_NAMESPACES\n        )\n        if len(result) == 0:\n            result = parent.xpath(\n                \"*[{}]\".format(xpath),\n                namespaces=XPATH_NAMESPACES\n            )\n            loop = True\n    else:\n        result = parent.xpath(\n            xpath,\n            namespaces=XPATH_NAMESPACES\n        )\n    return result[0], loop", "category": "Python"}, {"instruction": "def _field_controller_generator(self):\n        \"\"\"\n        Generates the methods called by the injected controller\n        \"\"\"\n", "input": "", "output": "        # Local variable, to avoid messing with \"self\"\n        stored_instance = self._ipopo_instance\n\n        def get_value(self, name):\n            # pylint: disable=W0613\n            ", "category": "Python"}, {"instruction": "def send_notification(self, method, params):\n        \"\"\"Send a notification\n        \"\"\"\n", "input": "", "output": "        msg = self._encoder.create_notification(method, params)\n        self._send_message(msg)", "category": "Python"}, {"instruction": "def custom_template_name(self):\n        \"\"\"\n        Returns the path for the custom special coverage template we want.\n        \"\"\"\n", "input": "", "output": "        base_path = getattr(settings, \"CUSTOM_SPECIAL_COVERAGE_PATH\", \"special_coverage/custom\")\n        if base_path is None:\n            base_path = \"\"\n        return \"{0}/{1}_custom.html\".format(\n            base_path, self.slug.replace(\"-\", \"_\")\n        ).lstrip(\"/\")", "category": "Python"}, {"instruction": "def elcm_session_list(irmc_info):\n    \"\"\"send an eLCM request to list all sessions\n\n    :param irmc_info: node info\n    :returns: dict object of sessions if succeed\n        {\n          'SessionList':\n          {\n            'Contains':\n            [\n              { 'Id': id1, 'Name': name1 },\n              { 'Id': id2, 'Name': name2 },\n              { 'Id': idN, 'Name': nameN },\n            ]\n          }\n        }\n    :raises: SCCIClientError if SCCI failed\n    \"\"\"\n", "input": "", "output": "    # Send GET request to the server\n    resp = elcm_request(irmc_info,\n                        method='GET',\n                        path='/sessionInformation/')\n\n    if resp.status_code == 200:\n        return _parse_elcm_response_body_as_json(resp)\n    else:\n        raise scci.SCCIClientError(('Failed to list sessions with '\n                                    'error code %s' % resp.status_code))", "category": "Python"}, {"instruction": "def set_section_order(self, section_name_list):\n        \"\"\"Set the order of the sections, which are by default unorderd.\n\n        Any unlisted sections that exist will be placed at the end of the\n        document in no particular order.\n        \"\"\"\n", "input": "", "output": "        self.section_headings = section_name_list[:]\n        for section_name in self.sections.keys():\n            if section_name not in section_name_list:\n                self.section_headings.append(section_name)\n        return", "category": "Python"}, {"instruction": "def really_unicode(in_string):\n    \"\"\"Make a string unicode. Really.\n\n    Ensure ``in_string`` is returned as unicode through a series of\n    progressively relaxed decodings.\n\n    Args:\n        in_string (str): The string to convert.\n\n    Returns:\n        str: Unicode.\n\n    Raises:\n        ValueError\n        \"\"\"\n", "input": "", "output": "    if isinstance(in_string, StringType):\n        for args in (('utf-8',), ('latin-1',), ('ascii', 'replace')):\n            try:\n                # pylint: disable=star-args\n                in_string = in_string.decode(*args)\n                break\n            except UnicodeDecodeError:\n                continue\n    if not isinstance(in_string, UnicodeType):\n        raise ValueError('%s is not a string at all.' % in_string)\n    return in_string", "category": "Python"}, {"instruction": "def append(x: T, xs: Iterable[T]) -> Iterator[T]:\n    \"\"\" Append a value to an iterable.\n\n    Parameters\n    ----------\n    x\n        An element of type T.\n    xs\n        An iterable of elements of type T.\n\n    Returns\n    -------\n    Iterator\n        An iterator that yields elements of *xs*, then yields *x*.\n\n\n    Examples\n    --------\n    >>> from delphi.utils.fp import append\n    >>> list(append(1, [2, 3]))\n    [2, 3, 1]\n\n    \"\"\"\n", "input": "", "output": "\n    return chain(xs, [x])", "category": "Python"}, {"instruction": "def values(self):\n    \"\"\"\n    Array values generator for powers from zero to upper power. Useful to cast\n    as list/tuple and for numpy/scipy integration (be careful: numpy use the\n    reversed from the output of this function used as input to a list or a\n    tuple constructor).\n    \"\"\"\n", "input": "", "output": "    if self._data:\n      for key in xrange(self.order + 1):\n        yield self[key]", "category": "Python"}, {"instruction": "def show(self):\n\t\t\"\"\"'Prints' (using self.om()) the basic stats about the loaded text. Eg:\n\t\t\t001776\tecstatic            \tP:'ecs.ta.tic                      \tS:PUU\tW:HLH\n\t\t\t001777\tbreath              \tP:'bre.ath                         \tS:PU\tW:LH\n\t\t\"\"\"\n", "input": "", "output": "\n\n\t\tif self.classname()==\"Corpus\":\n\t\t\tfor text in self.children:\n\t\t\t\ttext.om(\"## showing Text \" + text.name)\n\t\t\t\ttext.show()\n\t\telse:\n\t\t\twords=self.words()\n\t\t\tfor i in range(len(words)):\n\t\t\t\tword=words[i]\n\t\t\t\tword.om(str(i+1).zfill(6)+\"\\t\"+str(word.output_minform()),conscious=False)", "category": "Python"}, {"instruction": "def queue_put_stoppable(self, q, obj):\n        \"\"\" Put obj to queue, but will give up when the thread is stopped\"\"\"\n", "input": "", "output": "        while not self.stopped():\n            try:\n                q.put(obj, timeout=5)\n                break\n            except queue.Full:\n                pass", "category": "Python"}, {"instruction": "def num_mutations(self):\n        '''\n        :return: number of mutations in the container\n        '''\n", "input": "", "output": "        self._initialize()\n        res = super(Container, self).num_mutations()\n        return res", "category": "Python"}, {"instruction": "def get_pretty_version(self, diff_to_increase_ratio):\n        \"\"\"Pretty version\n\n        :param diff_to_increase_ratio: Ratio to convert number of changes into\n            version increases\n        :return: string: Pretty version of this repository\n        \"\"\"\n", "input": "", "output": "        version = self.get_version(diff_to_increase_ratio)\n        build = self.get_last_commit_hash()\n        return str(version) + \" (\" + build + \")\"", "category": "Python"}, {"instruction": "def ensure(assertion, message=None):\n    \"\"\"\n    Checks an assertion argument for truth-ness. Will return ``True`` or\n    explicitly raise ``AssertionError``. This is to deal with environments\n    using ``python -O` or ``PYTHONOPTIMIZE=``.\n\n    :param assertion: some value to evaluate for truth-ness\n    :param message: optional message used for raising AssertionError\n    \"\"\"\n", "input": "", "output": "    message = message or assertion\n\n    if not assertion:\n        raise AssertionError(message)\n\n    return True", "category": "Python"}, {"instruction": "def _poll_trigger(self):\n        \"\"\"\n        Trigger activity for the caller by writting a NUL to the self-pipe.\n    \"\"\"\n", "input": "", "output": "        try:\n            os.write(self._poll_send, '\\0'.encode('utf-8'))\n        except Exception as e:\n            log = self._getparam('log', self._discard)\n            log.debug(\"Ignoring self-pipe write error -- %s\", e)", "category": "Python"}, {"instruction": "def _iter_errors_custom(instance, checks, options):\n    \"\"\"Perform additional validation not possible merely with JSON schemas.\n\n    Args:\n        instance: The STIX object to be validated.\n        checks: A sequence of callables which do the checks.  Each callable\n            may be written to accept 1 arg, which is the object to check,\n            or 2 args, which are the object and a ValidationOptions instance.\n        options: ValidationOptions instance with settings affecting how\n            validation should be done.\n    \"\"\"\n", "input": "", "output": "    # Perform validation\n    for v_function in checks:\n        try:\n            result = v_function(instance)\n        except TypeError:\n            result = v_function(instance, options)\n        if isinstance(result, Iterable):\n            for x in result:\n                yield x\n        elif result is not None:\n            yield result\n\n    # Validate any child STIX objects\n    for field in instance:\n        if type(instance[field]) is list:\n            for obj in instance[field]:\n                if _is_stix_obj(obj):\n                    for err in _iter_errors_custom(obj, checks, options):\n                        yield err", "category": "Python"}, {"instruction": "def conf(self):\n        \"\"\"Runtime configuration interface for Spark.\n\n        This is the interface through which the user can get and set all Spark and Hadoop\n        configurations that are relevant to Spark SQL. When getting the value of a config,\n        this defaults to the value set in the underlying :class:`SparkContext`, if any.\n        \"\"\"\n", "input": "", "output": "        if not hasattr(self, \"_conf\"):\n            self._conf = RuntimeConfig(self._jsparkSession.conf())\n        return self._conf", "category": "Python"}, {"instruction": "def _FormatMessage(self, event):\n    \"\"\"Formats the message.\n\n    Args:\n      event (EventObject): event.\n\n    Returns:\n      str: message field.\n\n    Raises:\n      NoFormatterFound: if no event formatter can be found to match the data\n          type in the event.\n    \"\"\"\n", "input": "", "output": "    message, _ = self._output_mediator.GetFormattedMessages(event)\n    if message is None:\n      data_type = getattr(event, 'data_type', 'UNKNOWN')\n      raise errors.NoFormatterFound(\n          'Unable to find event formatter for: {0:s}.'.format(data_type))\n\n    return message", "category": "Python"}, {"instruction": "def register(self, signal, plugin, description=\"\"):\n        \"\"\"\n        Registers a new signal.\n\n        :param signal: Unique name of the signal\n        :param plugin: Plugin, which registers the new signal\n        :param description: Description of the reason or use case, why this signal is needed.\n                            Used for documentation.\n        \"\"\"\n", "input": "", "output": "        if signal in self.signals.keys():\n            raise Exception(\"Signal %s was already registered by %s\" % (signal, self.signals[signal].plugin.name))\n\n        self.signals[signal] = Signal(signal, plugin, self._namespace, description)\n        self.__log.debug(\"Signal %s registered by %s\" % (signal, plugin.name))\n        return self.signals[signal]", "category": "Python"}, {"instruction": "def halt(self):\n        \"\"\"Halts any/all running operations\"\"\"\n", "input": "", "output": "        self.explorer.halt()\n        self.protocoler.halt()\n        self.bs_calibrator.halt()\n        self.tone_calibrator.halt()\n        self.charter.halt()\n        self.mphone_calibrator.halt()", "category": "Python"}, {"instruction": "def encode(obj):\n    \"\"\" Encode an object for proper decoding by Java or ObjC\n    \"\"\"\n", "input": "", "output": "    if hasattr(obj, '__id__'):\n        return msgpack.ExtType(ExtType.REF, msgpack.packb(obj.__id__))\n    return obj", "category": "Python"}, {"instruction": "def rmdir(self, path):\n        \"\"\"\n        Remove the folder named C{path}.\n\n        @param path: name of the folder to remove\n        @type path: str\n        \"\"\"\n", "input": "", "output": "        path = self._adjust_cwd(path)\n        self._log(DEBUG, 'rmdir(%r)' % path)\n        self._request(CMD_RMDIR, path)", "category": "Python"}, {"instruction": "def check_in_out_dates(self):\n        \"\"\"\n        When date_order is less then check-in date or\n        Checkout date should be greater than the check-in date.\n        \"\"\"\n", "input": "", "output": "        if self.checkout and self.checkin:\n            if self.checkin < self.date_order:\n                raise ValidationError(_('Check-in date should be greater than \\\n                                         the current date.'))\n            if self.checkout < self.checkin:\n                raise ValidationError(_('Check-out date should be greater \\\n                                         than Check-in date.'))", "category": "Python"}, {"instruction": "def _make_name(current, new):\n    '''\n    Stops duplication between similarly named nested deploys, eg:\n\n    Turn:\n        Deploy Kubernetes master/Configure Kubernetes\n    Into:\n        Deploy Kubernetes master/Configure\n    '''\n", "input": "", "output": "\n    current_tokens = current.split()\n    new_tokens = new.split()\n\n    new = ' '.join(\n        new_token for new_token in new_tokens\n        if new_token not in current_tokens\n    )\n\n    return '/'.join((current, new))", "category": "Python"}, {"instruction": "def restoreSettings(self, settings, merge=False, loadProfile=True):\r\n        \"\"\"\r\n        Restores this profile from settings.\r\n        \r\n        :param      settings | <QSettings>\r\n        \"\"\"\n", "input": "", "output": "        value = unwrapVariant(settings.value('profile_toolbar'))\r\n        if not value:\r\n            return\r\n        \r\n        self.loadString(value, merge, loadProfile=loadProfile)", "category": "Python"}, {"instruction": "def replace(self, p_todos):\n        \"\"\" Replaces whole todolist with todo objects supplied as p_todos. \"\"\"\n", "input": "", "output": "        self.erase()\n        self.add_todos(p_todos)\n        self.dirty = True", "category": "Python"}, {"instruction": "def _evolve_reader(in_file):\n    \"\"\"Generate a list of region IDs and trees from a top_k_trees evolve.py file.\n    \"\"\"\n", "input": "", "output": "    cur_id_list = None\n    cur_tree = None\n    with open(in_file) as in_handle:\n        for line in in_handle:\n            if line.startswith(\"id,\"):\n                if cur_id_list:\n                    yield cur_id_list, cur_tree\n                cur_id_list = []\n                cur_tree = None\n            elif cur_tree is not None:\n                if line.strip() and not line.startswith(\"Number of non-empty\"):\n                    cur_tree.append(line.rstrip())\n            elif not line.strip() and cur_id_list and len(cur_id_list) > 0:\n                cur_tree = []\n            elif line.strip():\n                parts = []\n                for part in line.strip().split(\"\\t\"):\n                    if part.endswith(\",\"):\n                        part = part[:-1]\n                    parts.append(part)\n                if len(parts) > 4:\n                    nid, freq, _, _, support = parts\n                    cur_id_list.append((nid, freq, support.split(\"; \")))\n    if cur_id_list:\n        yield cur_id_list, cur_tree", "category": "Python"}, {"instruction": "def get_widths_mean_var(self, estimation):\n        \"\"\"Get estimation on the variance of widths' mean\n\n\n        Parameters\n        ----------\n        estimation : 1D arrary\n             Either prior of posterior estimation\n\n\n        Returns\n        -------\n\n        widths_mean_var : 2D array, in shape [K, 1]\n            Estimation on variance of widths' mean\n\n        \"\"\"\n", "input": "", "output": "        widths_mean_var = \\\n            estimation[self.map_offset[3]:].reshape(self.K, 1)\n        return widths_mean_var", "category": "Python"}, {"instruction": "def memory_map(self):\n        \"\"\" Returns a (very long) string containing a memory map\n            hex address: label\n        \"\"\"\n", "input": "", "output": "        return '\\n'.join(sorted(\"%04X: %s\" % (x.value, x.name) for x in self.global_labels.values() if x.is_address))", "category": "Python"}, {"instruction": "def _whitelist_blacklist(self, os_name):\n        '''\n        Determines if the OS should be ignored,\n        depending on the whitelist-blacklist logic\n        configured by the user.\n        '''\n", "input": "", "output": "        return napalm_logs.ext.check_whitelist_blacklist(os_name,\n                                                         whitelist=self.device_whitelist,\n                                                         blacklist=self.device_blacklist)", "category": "Python"}, {"instruction": "def MakePmfFromHist(hist, name=None):\n    \"\"\"Makes a normalized PMF from a Hist object.\n\n    Args:\n        hist: Hist object\n        name: string name\n\n    Returns:\n        Pmf object\n    \"\"\"\n", "input": "", "output": "    if name is None:\n        name = hist.name\n\n    # make a copy of the dictionary\n    d = dict(hist.GetDict())\n    pmf = Pmf(d, name)\n    pmf.Normalize()\n    return pmf", "category": "Python"}, {"instruction": "def convert_units(self, desired, guess=False):\n        \"\"\"\n        Convert the units of the mesh into a specified unit.\n\n        Parameters\n        ----------\n        desired : string\n          Units to convert to (eg 'inches')\n        guess : boolean\n          If self.units are not defined should we\n          guess the current units of the document and then convert?\n        \"\"\"\n", "input": "", "output": "        units._convert_units(self, desired, guess)\n        return self", "category": "Python"}, {"instruction": "def list_xz (archive, compression, cmd, verbosity, interactive):\n    \"\"\"List a XZ archive.\"\"\"\n", "input": "", "output": "    cmdlist = [cmd]\n    cmdlist.append('-l')\n    if verbosity > 1:\n        cmdlist.append('-v')\n    cmdlist.append(archive)\n    return cmdlist", "category": "Python"}, {"instruction": "def _determine_dimensions(num_of_pixels):\n    \"\"\"\n    Given a number of pixels, determines the largest width and height that define a\n      rectangle with such an area\n    \"\"\"\n", "input": "", "output": "    for x in xrange(int(math.sqrt(num_of_pixels)) + 1, 1, -1):\n        if num_of_pixels % x == 0:\n            return num_of_pixels // x, x\n    return 1, num_of_pixels", "category": "Python"}, {"instruction": "def _get_section_values(self, config, section):\n        \"\"\" extract src and dst values from a section\n        \"\"\"\n", "input": "", "output": "        src_host = self._get_hosts_from_names(config.get(section, 'src.host')) \\\n            if config.has_option(section, 'src.host') else None\n        src_file = [self._get_abs_filepath(config.get(section, 'src.file'))] \\\n            if config.has_option(section, 'src.file') else None\n        if src_host is None and src_file is None:\n            raise conferr('Section \"%s\" gets no sources' % section)\n\n        dst_host = self._get_hosts_from_names(config.get(section, 'dst.host')) \\\n            if config.has_option(section, 'dst.host') else None\n        dst_file = [self._get_abs_filepath(config.get(section, 'dst.file'))] \\\n            if config.has_option(section, 'dst.file') else None\n        if dst_host is None and dst_file is None:\n            raise conferr('Section \"%s\" gets no destinations' % section)\n\n        return (src_host, src_file, dst_host, dst_file)", "category": "Python"}, {"instruction": "def delete(self, **kwargs):\n        \"\"\"Delete all resources in this collection.\"\"\"\n", "input": "", "output": "        self.inflate()\n        for model in self._models:\n            model.delete(**kwargs)\n        return", "category": "Python"}, {"instruction": "def GetParentFileEntry(self):\n    \"\"\"Retrieves the parent file entry.\n\n    Returns:\n      TARFileEntry: parent file entry or None.\n    \"\"\"\n", "input": "", "output": "    location = getattr(self.path_spec, 'location', None)\n    if location is None:\n      return None\n\n    parent_location = self._file_system.DirnamePath(location)\n    if parent_location is None:\n      return None\n\n    if parent_location == '':\n      parent_location = self._file_system.PATH_SEPARATOR\n      is_root = True\n      is_virtual = True\n    else:\n      is_root = False\n      is_virtual = False\n\n    parent_path_spec = getattr(self.path_spec, 'parent', None)\n    path_spec = tar_path_spec.TARPathSpec(\n        location=parent_location, parent=parent_path_spec)\n    return TARFileEntry(\n        self._resolver_context, self._file_system, path_spec, is_root=is_root,\n        is_virtual=is_virtual)", "category": "Python"}, {"instruction": "def make_eventrule(date_rule, time_rule, cal, half_days=True):\n    \"\"\"\n    Constructs an event rule from the factory api.\n    \"\"\"\n", "input": "", "output": "\n    # Insert the calendar in to the individual rules\n    date_rule.cal = cal\n    time_rule.cal = cal\n\n    if half_days:\n        inner_rule = date_rule & time_rule\n    else:\n        nhd_rule = NotHalfDay()\n        nhd_rule.cal = cal\n        inner_rule = date_rule & time_rule & nhd_rule\n\n    return OncePerDay(rule=inner_rule)", "category": "Python"}, {"instruction": "def LegacyKextunload(self, cf_bundle_identifier):\n    \"\"\"Unload a kext by forking into kextunload.\"\"\"\n", "input": "", "output": "    error_code = OS_SUCCESS\n    bundle_identifier = self.CFStringToPystring(cf_bundle_identifier)\n    try:\n      subprocess.check_call(['/sbin/kextunload', '-b', bundle_identifier])\n    except subprocess.CalledProcessError as cpe:\n      logging.debug('failed to unload %s:%s', bundle_identifier, str(cpe))\n      error_code = -1\n    return error_code", "category": "Python"}, {"instruction": "def table_preview(table_name):\n    \"\"\"\n    Returns the first five rows of a table as JSON. Inlcudes all columns.\n    Uses Pandas' \"split\" JSON format.\n\n    \"\"\"\n", "input": "", "output": "    preview = orca.get_table(table_name).to_frame().head()\n    return (\n        preview.to_json(orient='split', date_format='iso'),\n        200,\n        {'Content-Type': 'application/json'})", "category": "Python"}, {"instruction": "def close_position(self, repay_only):\n        \"\"\" Close position.\n\n        Args:\n            repay_only (bool): Undocumented by cbpro.\n\n        Returns:\n            Undocumented\n\n        \"\"\"\n", "input": "", "output": "        params = {'repay_only': repay_only}\n        return self._send_message('post', '/position/close',\n                                  data=json.dumps(params))", "category": "Python"}, {"instruction": "def add_threading(self, flag):\n\t\t\"\"\"\n\t\tIndicates that this wrapper should use threading by appending an\n\t\targument with the specified `flag` followed by the number of threads\n\t\tspecified in the BioLite configuration file.\n\t\t\"\"\"\n", "input": "", "output": "\t\tthreads = min(int(config.get_resource('threads')), self.max_concurrency)\n\t\tif threads > 1:\n\t\t\tself.args.append(flag)\n\t\t\tself.args.append(threads)", "category": "Python"}, {"instruction": "def bake(self):\n        \"\"\"\n        Bake a `flake8` command so it's ready to execute and returns None.\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        self._flake8_command = sh.flake8.bake(\n            self.options,\n            self._tests,\n            _env=self.env,\n            _out=LOG.out,\n            _err=LOG.error)", "category": "Python"}, {"instruction": "def update(self, device_json=None, info_json=None, settings_json=None,\n               avatar_json=None):\n        \"\"\"Update the internal device json data.\"\"\"\n", "input": "", "output": "        if device_json:\n            UTILS.update(self._device_json, device_json)\n\n        if avatar_json:\n            UTILS.update(self._avatar_json, avatar_json)\n\n        if info_json:\n            UTILS.update(self._info_json, info_json)\n\n        if settings_json:\n            UTILS.update(self._settings_json, settings_json)", "category": "Python"}, {"instruction": "def run_psexec_command(cmd, args, host, username, password, port=445):\n    '''\n    Run a command remotly using the psexec protocol\n    '''\n", "input": "", "output": "    if has_winexe() and not HAS_PSEXEC:\n        ret_code = run_winexe_command(cmd, args, host, username, password, port)\n        return None, None, ret_code\n    service_name = 'PS-Exec-{0}'.format(uuid.uuid4())\n    stdout, stderr, ret_code = '', '', None\n    client = Client(host, username, password, port=port, encrypt=False, service_name=service_name)\n    client.connect()\n    try:\n        client.create_service()\n        stdout, stderr, ret_code = client.run_executable(cmd, args)\n    finally:\n        client.remove_service()\n        client.disconnect()\n    return stdout, stderr, ret_code", "category": "Python"}, {"instruction": "def image_to_base64(self, image):\n        \"\"\"\n        :param image: One of file object, path, or URL\n        :return: Base64 of image\n        \"\"\"\n", "input": "", "output": "        if isinstance(image, file_):\n            return base64.b64encode(image.read())\n        elif isinstance(image, basestring):\n            if image.startswith(\"http\"):\n                # it's URL\n                return base64.b64encode(requests.get(image).content)\n            # it's path\n            with open(image) as f:\n                return base64.b64encode(f.read())\n\n        raise ValueError(\"Unrecognizable image param: it must one of\"\n                         \" file object, path or URL, not %s\" % type(image))", "category": "Python"}, {"instruction": "def _distance_covariance_sqr_naive(x, y, exponent=1):\n    \"\"\"\n    Naive biased estimator for distance covariance.\n\n    Computes the unbiased estimator for distance covariance between two\n    matrices, using an :math:`O(N^2)` algorithm.\n    \"\"\"\n", "input": "", "output": "    a = _distance_matrix(x, exponent=exponent)\n    b = _distance_matrix(y, exponent=exponent)\n\n    return mean_product(a, b)", "category": "Python"}, {"instruction": "def zones(self):\n        \"\"\"\n        Return a new raw REST interface to zone resources\n\n        :rtype: :py:class:`ns1.rest.zones.Zones`\n        \"\"\"\n", "input": "", "output": "        import ns1.rest.zones\n        return ns1.rest.zones.Zones(self.config)", "category": "Python"}, {"instruction": "def he_uniform(name, shape, scale=1, dtype=tf.sg_floatx, summary=True, regularizer=None, trainable=True):\n    r\"\"\"See [He et al. 2015](http://arxiv.org/pdf/1502.01852v1.pdf)\n\n    Args:\n      name: The name of new variable\n      shape: A tuple/list of integers.\n      scale: A Python scalar. Scale to initialize. Default is 1.\n      dtype: The data type. Default is float32.\n      summary: If True, add this constant to tensor board summary.\n      regularizer:  A (Tensor -> Tensor or None) function; the result of applying it on a newly created variable\n        will be added to the collection tf.GraphKeys.REGULARIZATION_LOSSES and can be used for regularization\n      trainable: If True, add this constant to trainable collection. Default is True.\n\n    Returns:\n      A `Variable`.\n\n    \"\"\"\n", "input": "", "output": "    fin, _ = _get_fans(shape)\n    s = np.sqrt(1. * scale / fin)\n    return uniform(name, shape, s, dtype, summary, regularizer, trainable)", "category": "Python"}, {"instruction": "def get_angles(self) -> Tuple[List[float], List[float]]:\n        \"\"\"\n        Finds optimal angles with the quantum variational eigensolver method.\n\n        Stored VQE result\n\n        :returns: A tuple of the beta angles and the gamma angles for the optimal solution.\n        \"\"\"\n", "input": "", "output": "        stacked_params = np.hstack((self.betas, self.gammas))\n        vqe = VQE(self.minimizer, minimizer_args=self.minimizer_args,\n                  minimizer_kwargs=self.minimizer_kwargs)\n        cost_ham = reduce(lambda x, y: x + y, self.cost_ham)\n        # maximizing the cost function!\n        param_prog = self.get_parameterized_program()\n        result = vqe.vqe_run(param_prog, cost_ham, stacked_params, qc=self.qc,\n                             **self.vqe_options)\n        self.result = result\n        betas = result.x[:self.steps]\n        gammas = result.x[self.steps:]\n        return betas, gammas", "category": "Python"}, {"instruction": "def _compute_ticks(self, element, edges, widths, lims):\n        \"\"\"\n        Compute the ticks either as cyclic values in degrees or as roughly\n        evenly spaced bin centers.\n        \"\"\"\n", "input": "", "output": "        if self.xticks is None or not isinstance(self.xticks, int):\n            return None\n        if self.cyclic:\n            x0, x1, _, _ = lims\n            xvals = np.linspace(x0, x1, self.xticks)\n            labels = [\"%.0f\" % np.rad2deg(x) + '\\N{DEGREE SIGN}' for x in xvals]\n        elif self.xticks:\n            dim = element.get_dimension(0)\n            inds = np.linspace(0, len(edges), self.xticks, dtype=np.int)\n            edges = list(edges) + [edges[-1] + widths[-1]]\n            xvals = [edges[i] for i in inds]\n            labels = [dim.pprint_value(v) for v in xvals]\n        return [xvals, labels]", "category": "Python"}, {"instruction": "def sendcontrol(self, char):\n        '''Helper method that wraps send() with mnemonic access for sending control\n        character to the child (such as Ctrl-C or Ctrl-D).  For example, to send\n        Ctrl-G (ASCII 7, bell, '\\a')::\n            child.sendcontrol('g')\n        See also, sendintr() and sendeof().\n        '''\n", "input": "", "output": "        char = char.lower()\n        a = ord(char)\n        if 97 <= a <= 122:\n            a = a - ord('a') + 1\n            byte = bytes([a])\n            return self.pty.write(byte.decode('utf-8')), byte\n        d = {'@': 0, '`': 0,\n            '[': 27, '{': 27,\n            '\\\\': 28, '|': 28,\n            ']': 29, '}': 29,\n            '^': 30, '~': 30,\n            '_': 31,\n            '?': 127}\n        if char not in d:\n            return 0, b''\n\n        byte = bytes([d[char]])\n        return self.pty.write(byte.decode('utf-8')), byte", "category": "Python"}, {"instruction": "def reconciliateNs(self, tree):\n        \"\"\"This function checks that all the namespaces declared\n          within the given tree are properly declared. This is needed\n          for example after Copy or Cut and then paste operations.\n          The subtree may still hold pointers to namespace\n          declarations outside the subtree or invalid/masked. As much\n          as possible the function try to reuse the existing\n          namespaces found in the new environment. If not possible\n          the new namespaces are redeclared on @tree at the top of\n           the given subtree. \"\"\"\n", "input": "", "output": "        if tree is None: tree__o = None\n        else: tree__o = tree._o\n        ret = libxml2mod.xmlReconciliateNs(self._o, tree__o)\n        return ret", "category": "Python"}, {"instruction": "def list_all(zone=None, permanent=True):\n    '''\n    List everything added for or enabled in a zone\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' firewalld.list_all\n\n    List a specific zone\n\n    .. code-block:: bash\n\n        salt '*' firewalld.list_all my_zone\n    '''\n", "input": "", "output": "    _zone = {}\n    id_ = ''\n\n    if zone:\n        cmd = '--zone={0} --list-all'.format(zone)\n    else:\n        cmd = '--list-all'\n\n    if permanent:\n        cmd += ' --permanent'\n\n    for i in __firewall_cmd(cmd).splitlines():\n        if re.match('^[a-z0-9]', i, re.I):\n            zone_name = i.rstrip()\n        else:\n            if i.startswith('\\t'):\n                _zone[zone_name][id_].append(i.strip())\n                continue\n\n            (id_, val) = i.split(':', 1)\n\n            id_ = id_.strip()\n\n            if _zone.get(zone_name, None):\n                _zone[zone_name].update({id_: [val.strip()]})\n            else:\n                _zone[zone_name] = {id_: [val.strip()]}\n\n    return _zone", "category": "Python"}, {"instruction": "def get_queryset(self):\n        \"\"\"\n        Override :meth:``get_queryset``\n        \"\"\"\n", "input": "", "output": "        queryset = super(MultipleIDMixin, self).get_queryset()\n        if hasattr(self.request, 'query_params'):\n            ids = dict(self.request.query_params).get('ids[]')\n        else:\n            ids = dict(self.request.QUERY_PARAMS).get('ids[]')\n        if ids:\n            queryset = queryset.filter(id__in=ids)\n        return queryset", "category": "Python"}, {"instruction": "def verified(self):\n        \"\"\" True if bug was verified in given time frame \"\"\"\n", "input": "", "output": "        for who, record in self.logs:\n            if record[\"field_name\"] == \"status\" \\\n                    and record[\"added\"] == \"VERIFIED\":\n                return True\n        return False", "category": "Python"}, {"instruction": "def libvlc_media_get_stats(p_md, p_stats):\n    '''Get the current statistics about the media.\n    @param p_md:: media descriptor object.\n    @param p_stats:: structure that contain the statistics about the media (this structure must be allocated by the caller).\n    @return: true if the statistics are available, false otherwise \\libvlc_return_bool.\n    '''\n", "input": "", "output": "    f = _Cfunctions.get('libvlc_media_get_stats', None) or \\\n        _Cfunction('libvlc_media_get_stats', ((1,), (1,),), None,\n                    ctypes.c_int, Media, ctypes.POINTER(MediaStats))\n    return f(p_md, p_stats)", "category": "Python"}, {"instruction": "def _check_team_exists(team):\n    \"\"\"\n    Check that the team registry actually exists.\n    \"\"\"\n", "input": "", "output": "    if team is None:\n        return\n\n    hostname = urlparse(get_registry_url(team)).hostname\n    try:\n        socket.gethostbyname(hostname)\n    except IOError:\n        try:\n            # Do we have internet?\n            socket.gethostbyname('quiltdata.com')\n        except IOError:\n            message = \"Can't find quiltdata.com. Check your internet connection.\"\n        else:\n            message = \"Unable to connect to registry. Is the team name %r correct?\" % team\n        raise CommandException(message)", "category": "Python"}, {"instruction": "def set_poll_func(self, func, func_err_handler=None):\n\t\t'''Can be used to integrate pulse client into existing eventloop.\n\t\t\tFunction will be passed a list of pollfd structs and timeout value (seconds, float),\n\t\t\t\twhich it is responsible to use and modify (set poll flags) accordingly,\n\t\t\t\treturning int value >= 0 with number of fds that had any new events within timeout.\n\t\t\tfunc_err_handler defaults to traceback.print_exception(),\n\t\t\t\tand will be called on any exceptions from callback (to e.g. log these),\n\t\t\t\treturning poll error code (-1) to libpulse after that.'''\n", "input": "", "output": "\t\tif not func_err_handler: func_err_handler = traceback.print_exception\n\t\tself._pa_poll_cb = c.PA_POLL_FUNC_T(ft.partial(self._pulse_poll_cb, func, func_err_handler))\n\t\tc.pa.mainloop_set_poll_func(self._loop, self._pa_poll_cb, None)", "category": "Python"}, {"instruction": "def normalize_hex(hex_value):\n    \"\"\"\n    Normalize a hexadecimal color value to 6 digits, lowercase.\n\n    \"\"\"\n", "input": "", "output": "    match = HEX_COLOR_RE.match(hex_value)\n    if match is None:\n        raise ValueError(\n            u\"'{}' is not a valid hexadecimal color value.\".format(hex_value)\n        )\n    hex_digits = match.group(1)\n    if len(hex_digits) == 3:\n        hex_digits = u''.join(2 * s for s in hex_digits)\n    return u'#{}'.format(hex_digits.lower())", "category": "Python"}, {"instruction": "def db_credentials(self):\n        \"\"\"Return username and password for the KM3NeT WebDB.\"\"\"\n", "input": "", "output": "        try:\n            username = self.config.get('DB', 'username')\n            password = self.config.get('DB', 'password')\n        except Error:\n            username = input(\"Please enter your KM3NeT DB username: \")\n            password = getpass.getpass(\"Password: \")\n        return username, password", "category": "Python"}, {"instruction": "def delete(self, table, where=None, commit=True):\n        \"\"\"\n        :type table: string\n        :type where: dict\n        :type commit: bool\n        \"\"\"\n", "input": "", "output": "        where_q, _args = self._where_parser(where)\n\n        alias = self._tablename_parser(table)['alias']\n\n        _sql = ''.join(['DELETE ',\n                        alias + ' ' if alias else '',\n                        'FROM ', self._tablename_parser(table)['formatted_tablename'], where_q, ';'])\n\n        if self.debug:\n            return self.cur.mogrify(_sql, _args)\n\n        result = self.cur.execute(_sql, _args)\n        if commit:\n            self.commit()\n        return result", "category": "Python"}, {"instruction": "def url_unparse(components):\n    \"\"\"The reverse operation to :meth:`url_parse`.  This accepts arbitrary\n    as well as :class:`URL` tuples and returns a URL as a string.\n\n    :param components: the parsed URL as tuple which should be converted\n                       into a URL string.\n    \"\"\"\n", "input": "", "output": "    scheme, netloc, path, query, fragment = \\\n        normalize_string_tuple(components)\n    s = make_literal_wrapper(scheme)\n    url = s('')\n\n    # We generally treat file:///x and file:/x the same which is also\n    # what browsers seem to do.  This also allows us to ignore a schema\n    # register for netloc utilization or having to differenciate between\n    # empty and missing netloc.\n    if netloc or (scheme and path.startswith(s('/'))):\n        if path and path[:1] != s('/'):\n            path = s('/') + path\n        url = s('//') + (netloc or s('')) + path\n    elif path:\n        url += path\n    if scheme:\n        url = scheme + s(':') + url\n    if query:\n        url = url + s('?') + query\n    if fragment:\n        url = url + s('#') + fragment\n    return url", "category": "Python"}, {"instruction": "def get_KE_constraints(self):\n        \"\"\"Get linear constraints on KE matrix.\n        \"\"\"\n", "input": "", "output": "        C2 = np.eye(self.m)\n        C2 = C2[:self.m - 2, :]\n        to_be_deleted = []\n        for idx_vij_1 in range(self.m - 2):\n            idx_vij_2 = idx_vij_1 + 1\n            C2[idx_vij_1, idx_vij_2] = -1\n            i1 = np.where(self.C[idx_vij_1, :] == 1)[0][0]\n            i2 = np.where(self.C[idx_vij_2, :] == 1)[0][0]\n            j = np.where(self.C[idx_vij_1, :] == -1)[0][0]\n            if i1 == i2:\n                i = i1\n                k = np.where(self.C[idx_vij_2, :] == -1)[0][0]\n                i_indices = self.C[:, j] == 1\n                j_indices = self.C[:, k] == -1\n                idx_vij_3 = np.where(np.bitwise_and(\n                    i_indices, j_indices))[0][0]\n                #print('v{}{}, v{}{}, v{}{}\\n{}    {}    {}'.format(j,i,k,i,k,j,idx_vij_1,idx_vij_2,idx_vij_3))\n                C2[idx_vij_1, idx_vij_3] = 1\n            else:\n                #print('v{}{}, v{}{} not considered.'.format(j,i1,j,i2))\n                to_be_deleted.append(idx_vij_1)\n        C2 = np.delete(C2, to_be_deleted, axis=0)\n        b = np.zeros((C2.shape[0], 1))\n        return C2, b", "category": "Python"}, {"instruction": "def sync_account(self, broker_name, account_cookie):\n        \"\"\"\u540c\u6b65\u8d26\u6237\u4fe1\u606f\n\n        Arguments:\n            broker_id {[type]} -- [description]\n            account_cookie {[type]} -- [description]\n        \"\"\"\n", "input": "", "output": "        try:\n            if isinstance(self.broker[broker_name], QA_BacktestBroker):\n                pass\n            else:\n                self.session[account_cookie].sync_account(\n                    self.broker[broker_name].query_positions(account_cookie)\n                )\n            return True\n        except Exception as e:\n            print(e)\n            return False", "category": "Python"}, {"instruction": "def set_config(path):\n    \"\"\"\n    Set configuration for current session.\n    \"\"\"\n", "input": "", "output": "    logging.info(\"LOADING FROM: {}\".format(path))\n    session.config = load_config(path)\n    return session.config", "category": "Python"}, {"instruction": "def ellipsis_or_number(context, paginator, current_page):\n    \"\"\"\n    To avoid display a long pagination bar\n\n    :param context: template context\n    :param paginator: paginator_obj\n    :param current_page: int\n    :return: str or None\n    \"\"\"\n", "input": "", "output": "\n    # Checks is it first page\n    chosen_page = int(context['request'].GET['page']) if 'page' in context['request'].GET else 1\n\n    if current_page in (chosen_page + 1, chosen_page + 2, chosen_page - 1, chosen_page - 2,\n                        paginator.num_pages, paginator.num_pages - 1, 1, 2, chosen_page):\n        return current_page\n\n    if current_page in (chosen_page + 3, chosen_page - 3):\n        return '...'", "category": "Python"}, {"instruction": "def after_batch(self, stream_name: str, batch_data: Batch) -> None:\n        \"\"\"\n        If ``stream_name`` equals to :py:attr:`cxflow.constants.TRAIN_STREAM`,\n        increase the iterations counter and possibly stop the training; additionally, call :py:meth:`_check_train_time`.\n\n        :param stream_name: stream name\n        :param batch_data: ignored\n        :raise TrainingTerminated: if the number of iterations reaches ``self._iters``\n        \"\"\"\n", "input": "", "output": "        self._check_train_time()\n        if self._iters is not None and stream_name == self._train_stream_name:\n            self._iters_done += 1\n            if self._iters_done >= self._iters:\n                raise TrainingTerminated('Training terminated after iteration {}'.format(self._iters_done))", "category": "Python"}, {"instruction": "def copy_heroku_to_local(id):\n    \"\"\"Copy a Heroku database locally.\"\"\"\n", "input": "", "output": "    heroku_app = HerokuApp(dallinger_uid=id)\n    try:\n        subprocess.call([\"dropdb\", heroku_app.name])\n    except Exception:\n        pass\n\n    heroku_app.pg_pull()", "category": "Python"}, {"instruction": "def branch_order(h,section, path=[]):\n    \"\"\"\n    Returns the branch order of a section\n    \"\"\"\n", "input": "", "output": "    path.append(section)\n    sref = h.SectionRef(sec=section)\n    # has_parent returns a float... cast to bool\n    if sref.has_parent() < 0.9:\n        return 0 # section is a root\n    else:\n        nchild = len(list(h.SectionRef(sec=sref.parent).child))\n        if nchild <= 1.1:\n            return branch_order(h,sref.parent,path)\n        else:\n            return 1+branch_order(h,sref.parent,path)", "category": "Python"}, {"instruction": "def reboot(self, timeout=1):\n        \"\"\"Reboot the device\"\"\"\n", "input": "", "output": "        namespace = System.getServiceType(\"reboot\")\n        uri = self.getControlURL(namespace)\n\n        self.execute(uri, namespace, \"Reboot\", timeout=timeout)", "category": "Python"}, {"instruction": "def list_view_row_clicked(self, list_view, path, view_column):\n        \"\"\"\n        Function opens the firefox window with relevant link\n        \"\"\"\n", "input": "", "output": "        model = list_view.get_model()\n        text = model[path][0]\n        match = URL_FINDER.search(text)\n        if match is not None:\n            url = match.group(1)\n            import webbrowser\n\n            webbrowser.open(url)", "category": "Python"}, {"instruction": "def num_feats(self):\n        \"\"\" The number of features per time step in the corpus. \"\"\"\n", "input": "", "output": "        if not self._num_feats:\n            filename = self.get_train_fns()[0][0]\n            feats = np.load(filename)\n            # pylint: disable=maybe-no-member\n            if len(feats.shape) == 3:\n                # Then there are multiple channels of multiple feats\n                self._num_feats = feats.shape[1] * feats.shape[2]\n            elif len(feats.shape) == 2:\n                # Otherwise it is just of shape time x feats\n                self._num_feats = feats.shape[1]\n            else:\n                raise ValueError(\n                    \"Feature matrix of shape %s unexpected\" % str(feats.shape))\n        return self._num_feats", "category": "Python"}, {"instruction": "def intervalrecordlookup(table, start='start', stop='stop', include_stop=False):\n    \"\"\"\n    As :func:`petl.transform.intervals.intervallookup` but return records\n    instead of tuples.\n\n    \"\"\"\n", "input": "", "output": "\n    tree = recordtree(table, start=start, stop=stop)\n    return IntervalTreeLookup(tree, include_stop=include_stop)", "category": "Python"}, {"instruction": "def build_paragraph(content, hard_breaks=False):\n  \"\"\"\n  Returns *content* wrapped in `<p>` tags.\n\n  If *hard_breaks* is `True`, all line breaks are converted to `<br />` tags.\n\n  \"\"\"\n", "input": "", "output": "  lines = list(filter(None, [line.strip() for line in content.split('\\n')]))\n  if hard_breaks:\n    for line_number in range(len(lines) - 1):\n      lines[line_number] = \"{}<br />\".format(lines[line_number])\n  return \"<p>{}</p>\".format('\\n'.join(lines))", "category": "Python"}, {"instruction": "def get_group_policy(self, group_name, policy_name):\n        \"\"\"\n        Retrieves the specified policy document for the specified group.\n\n        :type group_name: string\n        :param group_name: The name of the group the policy is associated with.\n\n        :type policy_name: string\n        :param policy_name: The policy document to get.\n        \n        \"\"\"\n", "input": "", "output": "        params = {'GroupName' : group_name,\n                  'PolicyName' : policy_name}\n        return self.get_response('GetGroupPolicy', params, verb='POST')", "category": "Python"}, {"instruction": "def explode_rdn(rdn, notypes=0, flags=0):\n    \"\"\"\n    explode_rdn(rdn [, notypes=0]) -> list\n\n    This function takes a RDN and breaks it up into its component parts\n    if it is a multi-valued RDN.\n    The notypes parameter is used to specify that only the component's\n    attribute values be returned and not the attribute types.\n    \"\"\"\n", "input": "", "output": "    if not rdn:\n        return []\n    rdn_decomp = str2dn(rdn, flags)[0]\n    if notypes:\n        return [avalue or '' for atype, avalue, dummy in rdn_decomp]\n    else:\n        return ['='.join((atype, escape_dn_chars(avalue or '')))\n                for atype, avalue, dummy in rdn_decomp]", "category": "Python"}, {"instruction": "def assembly(self, value):\n        \"\"\"The assembly property.\n        \n        Args:\n            value (string). the property value.\n        \"\"\"\n", "input": "", "output": "        if value == self._defaults['assembly'] and 'assembly' in self._values:\n            del self._values['assembly']\n        else:\n            self._values['assembly'] = value", "category": "Python"}, {"instruction": "def join_options(options):\n    \"\"\"Given a list of option strings this joins them in the most appropriate\n    way and returns them in the form ``(formatted_string,\n    any_prefix_is_slash)`` where the second item in the tuple is a flag that\n    indicates if any of the option prefixes was a slash.\n    \"\"\"\n", "input": "", "output": "    rv = []\n    any_prefix_is_slash = False\n    for opt in options:\n        prefix = split_opt(opt)[0]\n        if prefix == '/':\n            any_prefix_is_slash = True\n        rv.append((len(prefix), opt))\n\n    rv.sort(key=lambda x: x[0])\n\n    rv = ', '.join(x[1] for x in rv)\n    return rv, any_prefix_is_slash", "category": "Python"}, {"instruction": "def submit(self, new_queue = None):\n    \"\"\"Sets the status of this job to 'submitted'.\"\"\"\n", "input": "", "output": "    self.status = 'submitted'\n    self.result = None\n    self.machine_name = None\n    if new_queue is not None:\n      self.queue_name = new_queue\n    for array_job in self.array:\n      array_job.status = 'submitted'\n      array_job.result = None\n      array_job.machine_name = None\n    self.submit_time = datetime.now()\n    self.start_time = None\n    self.finish_time = None", "category": "Python"}, {"instruction": "def prepare_modules(module_paths: list, available: dict) -> dict:\n    \"\"\"\n    Scan all paths for external modules and form key-value dict.\n    :param module_paths: list of external modules (either python packages or third-party scripts)\n    :param available: dict of all registered python modules (can contain python modules from module_paths)\n    :return: dict of external modules, where keys are filenames (same as stepnames) and values are the paths\n    \"\"\"\n", "input": "", "output": "    indexed = {}\n    for path in module_paths:\n        if not os.path.exists(path) and path not in available:\n            err = 'No such path: ' + path\n            error(err)\n        else:\n            for f in os.listdir(path):\n                mod_path = join(path, f)\n                if f in indexed:\n                    warning('Override ' + indexed[f] + ' with ' + mod_path)\n                indexed[f] = mod_path\n    return indexed", "category": "Python"}, {"instruction": "def where(self, field, value = None, operator = None):\n        \"\"\"\n        Establece condiciones para la consulta unidas por AND\n        \"\"\"\n", "input": "", "output": "        if field is None:\n            return self\n        conjunction = None\n        if value is None and isinstance(field, dict):\n            for f,v in field.items():\n                if self.where_criteria.size() > 0:\n                    conjunction = 'AND'\n                self.where_criteria.append(expressions.ConditionExpression(f, v, operator=operator, conjunction=conjunction))\n                \n        else:\n            if self.where_criteria.size() > 0:\n                    conjunction = 'AND'\n            self.where_criteria.append(expressions.ConditionExpression(field, value, operator=operator, conjunction=conjunction))\n        return self", "category": "Python"}, {"instruction": "def get_ap(self, apid):\n        \"\"\"\u67e5\u770b\u63a5\u5165\u70b9\n\n        \u7ed9\u51fa\u63a5\u5165\u70b9\u7684\u57df\u540d\u6216IP\uff0c\u67e5\u770b\u914d\u7f6e\u4fe1\u606f\uff0c\u5305\u62ec\u6240\u6709\u76d1\u542c\u7aef\u53e3\u7684\u914d\u7f6e\u3002\n\n        Args:\n            - apid:   \u63a5\u5165\u70b9ID\n\n        Returns:\n            \u8fd4\u56de\u4e00\u4e2atuple\u5bf9\u8c61\uff0c\u5176\u683c\u5f0f\u4e3a(<result>, <ResponseInfo>)\n            - result          \u6210\u529f\u8fd4\u56de\u63a5\u5165\u70b9\u4fe1\u606f\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            - ResponseInfo    \u8bf7\u6c42\u7684Response\u4fe1\u606f\n        \"\"\"\n", "input": "", "output": "        url = '{0}/v3/aps/{1}'.format(self.host, apid)\n        return self.__get(url)", "category": "Python"}, {"instruction": "def addChild(self, childJob):\n        \"\"\"\n        Adds childJob to be run as child of this job. Child jobs will be run \\\n        directly after this job's :func:`toil.job.Job.run` method has completed.\n\n        :param toil.job.Job childJob:\n        :return: childJob\n        :rtype: toil.job.Job\n        \"\"\"\n", "input": "", "output": "        self._children.append(childJob)\n        childJob._addPredecessor(self)\n        return childJob", "category": "Python"}, {"instruction": "def try_call(self, client_data=None, api_data=None, aux_data=None, *args, **kwargs):\n        \"\"\"\n        Calls the request catching all exceptions\n        :param client_data:\n        :param api_data:\n        :param aux_data:\n        :param args:\n        :param kwargs:\n        :return:\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.call(client_data, api_data, aux_data, *args, **kwargs)\n        except Exception as e:\n            self.last_exception = RequestFailed(cause=e)\n        return None", "category": "Python"}, {"instruction": "def filter_value(old: float, new: float, factor: float) -> float:\n    \"\"\" Linearly interpolate between two float values. \"\"\"\n", "input": "", "output": "    r_factor: float = 1 - factor\n    return old * r_factor + new * factor", "category": "Python"}, {"instruction": "def q2m(q):\n    \"\"\"\n    Find the rotation matrix corresponding to a specified unit quaternion.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/q2m_c.html\n\n    :param q: A unit quaternion.\n    :type q: 4-Element Array of floats\n    :return: A rotation matrix corresponding to q\n    :rtype: 3x3-Element Array of floats\n    \"\"\"\n", "input": "", "output": "    q = stypes.toDoubleVector(q)\n    mout = stypes.emptyDoubleMatrix()\n    libspice.q2m_c(q, mout)\n    return stypes.cMatrixToNumpy(mout)", "category": "Python"}, {"instruction": "def _get_head_block(self, request):\n        \"\"\"Fetches the request specified head block, or the chain head.\n\n        Note:\n            This method will fail if `_block_store` has not been set\n\n        Args:\n            request (object): The parsed protobuf request object\n\n        Returns:\n            Block: the block object at the head of the requested chain\n\n        Raises:\n            ResponseFailed: Failed to retrieve a head block\n        \"\"\"\n", "input": "", "output": "        if request.head_id:\n            if self._id_regex.fullmatch(request.head_id) is None:\n                LOGGER.debug('Invalid head id requested: %s', request.head_id)\n                raise _ResponseFailed(self._status.NO_ROOT)\n            try:\n                return self._block_store[request.head_id]\n            except KeyError as e:\n                LOGGER.debug('Unable to find block \"%s\" in store', e)\n                raise _ResponseFailed(self._status.NO_ROOT)\n\n        else:\n            return self._get_chain_head()", "category": "Python"}, {"instruction": "def left_axis_label(self, label, position=None,  rotation=60, offset=0.08,\n                        **kwargs):\n        \"\"\"\n        Sets the label on the left axis.\n\n        Parameters\n        ----------\n        label: String\n            The axis label\n        position: 3-Tuple of floats, None\n            The position of the text label\n        rotation: float, 60\n            The angle of rotation of the label\n        offset: float,\n            Used to compute the distance of the label from the axis\n        kwargs:\n            Any kwargs to pass through to matplotlib.\n        \"\"\"\n", "input": "", "output": "\n        if not position:\n            position = (-offset, 3./5, 2./5)\n        self._labels[\"left\"] = (label, position, rotation, kwargs)", "category": "Python"}, {"instruction": "def get_text(nodelist):\n    \"\"\"Return a concatenation of text fields from list of nodes\n    \"\"\"\n", "input": "", "output": "\n    s = ''\n    for node in nodelist:\n        if node.nodeType == Node.TEXT_NODE:\n            s += node.nodeValue + ', '\n\n    if len(s)>0: s = s[:-2]\n    return s", "category": "Python"}, {"instruction": "def PrettyPrinter(obj):\n  \"\"\"Pretty printers for AppEngine objects.\"\"\"\n", "input": "", "output": "\n  if ndb and isinstance(obj, ndb.Model):\n    return six.iteritems(obj.to_dict()), 'ndb.Model(%s)' % type(obj).__name__\n\n  if messages and isinstance(obj, messages.Enum):\n    return [('name', obj.name), ('number', obj.number)], type(obj).__name__\n\n  return None", "category": "Python"}, {"instruction": "def groupby(self, by=None, axis=0, level=None, as_index=True, sort=True,\n                group_keys=True, squeeze=False):\n        \"\"\"Returns a groupby on the schema rdd. This returns a GroupBy object.\n        Note that grouping by a column name will be faster than most other\n        options due to implementation.\"\"\"\n", "input": "", "output": "        from sparklingpandas.groupby import GroupBy\n        return GroupBy(self, by=by, axis=axis, level=level, as_index=as_index,\n                       sort=sort, group_keys=group_keys, squeeze=squeeze)", "category": "Python"}, {"instruction": "def copy(self):\n        \"\"\"Create a copy of the current one.\"\"\"\n", "input": "", "output": "        rv = object.__new__(self.__class__)\n        rv.__dict__.update(self.__dict__)\n        rv.symbols = self.symbols.copy()\n        return rv", "category": "Python"}, {"instruction": "def array_to_jsbuffer(array):\n  \"\"\"Serialize 1d NumPy array to JS TypedArray.\n\n  Data is serialized to base64-encoded string, which is much faster\n  and memory-efficient than json list serialization.\n\n  Args:\n    array: 1d NumPy array, dtype must be one of JS_ARRAY_TYPES.\n\n  Returns:\n    JS code that evaluates to a TypedArray as string.\n\n  Raises:\n    TypeError: if array dtype or shape not supported.\n  \"\"\"\n", "input": "", "output": "  if array.ndim != 1:\n    raise TypeError('Only 1d arrays can be converted JS TypedArray.')\n  if array.dtype.name not in JS_ARRAY_TYPES:\n    raise TypeError('Array dtype not supported by JS TypedArray.')\n  js_type_name = array.dtype.name.capitalize() + 'Array'\n  data_base64 = base64.b64encode(array.tobytes()).decode('ascii')\n  code = ", "category": "Python"}, {"instruction": "def ssh_version():\n    '''\n    Returns the version of the installed ssh command\n    '''\n", "input": "", "output": "    # This function needs more granular checks and to be validated against\n    # older versions of ssh\n    ret = subprocess.Popen(\n            ['ssh', '-V'],\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE).communicate()\n    try:\n        version_parts = ret[1].split(b',')[0].split(b'_')[1]\n        parts = []\n        for part in version_parts:\n            try:\n                parts.append(int(part))\n            except ValueError:\n                return tuple(parts)\n        return tuple(parts)\n    except IndexError:\n        return (2, 0)", "category": "Python"}, {"instruction": "def get_answers(self):\n        \"\"\" override this so only right answers are returned\n        :return:\n        \"\"\"\n", "input": "", "output": "        all_answers = self.my_osid_object._my_map['answers']\n        right_answers = [a for a in all_answers\n                         if a['genusTypeId'] != str(WRONG_ANSWER_GENUS_TYPE)]\n        return AnswerList(right_answers,\n                          runtime=self.my_osid_object._runtime,\n                          proxy=self.my_osid_object._proxy)", "category": "Python"}, {"instruction": "def create(self, **attributes):\n        \"\"\"\n        Create a collection of models and persist them to the database.\n\n        :param attributes: The models attributes\n        :type attributes: dict\n\n        :return: mixed\n        \"\"\"\n", "input": "", "output": "        results = self.make(**attributes)\n\n        if self._amount == 1:\n            if self._resolver:\n                results.set_connection_resolver(self._resolver)\n\n            results.save()\n        else:\n            if self._resolver:\n                results.each(lambda r: r.set_connection_resolver(self._resolver))\n\n            for result in results:\n                result.save()\n\n        return results", "category": "Python"}, {"instruction": "def check_module_name(self, name, offset=0):\n        \"\"\"\n        Checks a module name eg. some i3status modules cannot have an instance\n        name.\n        \"\"\"\n", "input": "", "output": "        if name in [\"general\"]:\n            return\n        split_name = name.split()\n        if len(split_name) > 1 and split_name[0] in I3S_SINGLE_NAMES:\n            self.current_token -= len(split_name) - 1 - offset\n            self.error(\"Invalid name cannot have 2 tokens\")\n        if len(split_name) > 2:\n            self.current_token -= len(split_name) - 2 - offset\n            self.error(\"Invalid name cannot have more than 2 tokens\")", "category": "Python"}, {"instruction": "def validate_enum_attribute(fully_qualified_name: str, spec: Dict[str, Any], attribute: str,\n                            candidates: Set[Union[str, int, float]]) -> Optional[InvalidValueError]:\n    \"\"\" Validates to ensure that the value of an attribute lies within an allowed set of candidates \"\"\"\n", "input": "", "output": "\n    if attribute not in spec:\n        return\n\n    if spec[attribute] not in candidates:\n        return InvalidValueError(fully_qualified_name, spec, attribute, candidates)", "category": "Python"}, {"instruction": "def create_report(self):\n        \"\"\"\n        creates a text report for the human user\n        :return: str\n        \"\"\"\n", "input": "", "output": "\n        if self.report:\n            self.report += \"\\n Operation took %s secs\" % round(\n                time.time() - self.t1)\n        else:\n            self.report = \"Operation failed: %s \\n\" % self.report\n        return self.report", "category": "Python"}, {"instruction": "def g_time_res(FlowPlant, IDTube, RadiusCoil, LengthTube, Temp):\n    \"\"\"G Residence Time calculated for a coiled tube flocculator.\"\"\"\n", "input": "", "output": "    return (g_coil(FlowPlant, IDTube, RadiusCoil, Temp).magnitude\n            * time_res_tube(IDTube, LengthTube, FlowPlant).magnitude\n            )", "category": "Python"}, {"instruction": "def post(self, request, format=None):\n        \"\"\" validate password change operation and return result \"\"\"\n", "input": "", "output": "        serializer_class = self.get_serializer_class()\n        serializer = serializer_class(data=request.data, instance=request.user)\n\n        if serializer.is_valid():\n            serializer.save()\n            return Response({'detail': _(u'Password successfully changed')})\n\n        return Response(serializer.errors, status=400)", "category": "Python"}, {"instruction": "def toggle_service_status(self, service_id):\n        \"\"\"Toggles the service status.\n\n        :param int service_id: The id of the service to delete\n        \"\"\"\n", "input": "", "output": "\n        svc = self.client['Network_Application_Delivery_Controller_'\n                          'LoadBalancer_Service']\n        return svc.toggleStatus(id=service_id)", "category": "Python"}, {"instruction": "def bestscan(self,seq):\n        \"\"\"\n        m.bestscan(seq) -- Return the score of the best match to the motif in the supplied sequence\n        \"\"\"\n", "input": "", "output": "        matches,endpoints,scores = self.scan(seq,-100)\n        if not scores: return -100\n        scores.sort()\n        best = scores[-1]\n        return best", "category": "Python"}, {"instruction": "def from_array(array):\n        \"\"\"\n        Deserialize a new Game from a given dictionary.\n\n        :return: new Game instance.\n        :rtype: Game\n        \"\"\"\n", "input": "", "output": "        if array is None or not array:\n            return None\n        # end if\n        assert_type_or_raise(array, dict, parameter_name=\"array\")\n\n        data = {}\n        data['title'] = u(array.get('title'))\n        data['description'] = u(array.get('description'))\n        data['photo'] = PhotoSize.from_array_list(array.get('photo'), list_level=1)\n        data['text'] = u(array.get('text')) if array.get('text') is not None else None\n        data['text_entities'] = MessageEntity.from_array_list(array.get('text_entities'), list_level=1) if array.get('text_entities') is not None else None\n        data['animation'] = Animation.from_array(array.get('animation')) if array.get('animation') is not None else None\n        data['_raw'] = array\n        return Game(**data)", "category": "Python"}, {"instruction": "def main():\n    \"\"\" Main program. \"\"\"\n", "input": "", "output": "    args = command.parse_args()\n\n    with btrfs.FileSystem(args.dir) as mount:\n        # mount.rescanSizes()\n\n        fInfo = mount.FS_INFO()\n        pprint.pprint(fInfo)\n\n        vols = mount.subvolumes\n\n        # for dev in mount.devices:\n        #     pprint.pprint(dev)\n\n        for vol in vols:\n            print(vol)\n\n    return 0", "category": "Python"}, {"instruction": "def delete_notificant(self, id, **kwargs):  # noqa: E501\n        \"\"\"Delete a specific notification target  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_notificant(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :return: ResponseContainerNotificant\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_notificant_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_notificant_with_http_info(id, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def _copy_attachment(self, name, data, mimetype, mfg_event):\n    \"\"\"Copies an attachment to mfg_event.\"\"\"\n", "input": "", "output": "    attachment = mfg_event.attachment.add()\n    attachment.name = name\n    if isinstance(data, unicode):\n      data = data.encode('utf8')\n    attachment.value_binary = data\n    if mimetype in test_runs_converter.MIMETYPE_MAP:\n      attachment.type = test_runs_converter.MIMETYPE_MAP[mimetype]\n    elif mimetype == test_runs_pb2.MULTIDIM_JSON:\n      attachment.type = mimetype\n    else:\n      attachment.type = test_runs_pb2.BINARY", "category": "Python"}, {"instruction": "def query(cls, index_name=None, filter_builder=None,\n              scan_index_forward=None, limit=None, **key_conditions):\n        \"\"\"High level query API.\n\n        :param key_filter: key conditions of the query.\n        :type key_filter: :class:`collections.Mapping`\n        :param filter_builder: filter expression builder.\n        :type filter_builder: :class:`~bynamodb.filterexps.Operator`\n        \"\"\"\n", "input": "", "output": "        query_kwargs = {\n            'key_conditions': build_condition(key_conditions, KEY_CONDITIONS),\n            'index_name': index_name,\n            'scan_index_forward': scan_index_forward,\n            'limit': limit\n        }\n        if filter_builder:\n            cls._build_filter_expression(filter_builder, query_kwargs)\n        return ResultSet(cls, 'query', query_kwargs)", "category": "Python"}, {"instruction": "def _get_next_occurrence(haystack, offset, needles):\n        \"\"\"\n        Find next occurence of one of the needles in the haystack\n\n        :return: tuple of (index, needle found)\n             or: None if no needle was found\"\"\"\n", "input": "", "output": "        # make map of first char to full needle (only works if all needles\n        # have different first characters)\n        firstcharmap = dict([(n[0], n) for n in needles])\n        firstchars = firstcharmap.keys()\n        while offset < len(haystack):\n            if haystack[offset] in firstchars:\n                possible_needle = firstcharmap[haystack[offset]]\n                if haystack[offset:offset + len(possible_needle)] == possible_needle:\n                    return offset, possible_needle\n            offset += 1\n        return None", "category": "Python"}, {"instruction": "def get_gallery_item_list(self, folder_name, skip=None, count=None, scope='content/read'):\n        \"\"\"\n        Get the item listing in a given folder in the Mxit user's gallery\n        User authentication required with the following scope: 'content/read'\n        \"\"\"\n", "input": "", "output": "\n        params = {}\n        if skip:\n            params['skip'] = skip\n        if count:\n            params['count'] = count\n\n        qs = '?' + urllib.urlencode(params) if params else ''\n\n        folder_item_list = _get(\n            token=self.oauth.get_user_token(scope),\n            uri='/user/media/list/' + urllib.quote(folder_name) + qs\n        )\n        try:\n            return json.loads(folder_item_list)\n        except:\n            raise MxitAPIException('Error parsing gallery folder list')", "category": "Python"}, {"instruction": "def send(self, name=None, value=None, **kwargs):\n        \"\"\"\n        Can accept a name/tag and value to be queued and then send anything in\n        the queue to the time series service.  Optional parameters include\n        setting quality, timestamp, or attributes.\n\n        See spec for queue() for complete list of options.\n\n        Example of sending a batch of values:\n\n            queue('temp', 70.1)\n            queue('humidity', 20.4)\n            send()\n\n        Example of sending one and flushing queue immediately\n\n            send('temp', 70.3)\n            send('temp', 70.4, quality=ts.GOOD, attributes={'unit': 'F'})\n\n\n        \"\"\"\n", "input": "", "output": "        if name and value:\n            self.queue(name, value, **kwargs)\n\n        timestamp = int(round(time.time() * 1000))\n\n        # The label \"name\" or \"tag\" is sometimes used ambiguously\n        msg = {\n            \"messageId\": timestamp,\n            \"body\": self._queue\n        }\n\n        self._queue = []\n\n        return self._send_to_timeseries(msg)", "category": "Python"}, {"instruction": "def requires_loaded(func):\n        \"\"\"\n        A decorator to ensure the resource data is loaded.\n        \"\"\"\n", "input": "", "output": "        def _wrapper(self, *args, **kwargs):\n            # If we don't have data, go load it.\n            if self._loaded_data is None:\n                self._loaded_data = self.loader.load(self.service_name)\n\n            return func(self, *args, **kwargs)\n\n        return _wrapper", "category": "Python"}, {"instruction": "def read_config(source, current_name):\n    \"\"\"Read the Sphinx config for one version.\n\n    :raise HandledError: If sphinx-build fails. Will be logged before raising.\n\n    :param str source: Source directory to pass to sphinx-build.\n    :param str current_name: The ref name of the current version being built.\n\n    :return: Specific Sphinx config values.\n    :rtype: dict\n    \"\"\"\n", "input": "", "output": "    log = logging.getLogger(__name__)\n    queue = multiprocessing.Queue()\n    config = Config.from_context()\n\n    with TempDir() as temp_dir:\n        argv = ('sphinx-build', source, temp_dir)\n        log.debug('Running sphinx-build for config values with args: %s', str(argv))\n        child = multiprocessing.Process(target=_read_config, args=(argv, config, current_name, queue))\n        child.start()\n        child.join()  # Block.\n        if child.exitcode != 0:\n            log.error('sphinx-build failed for branch/tag while reading config: %s', current_name)\n            raise HandledError\n\n    config = queue.get()\n    return config", "category": "Python"}, {"instruction": "def _draw_messages(self, painter):\n        \"\"\"\n        Draw messages from all subclass of CheckerMode currently\n        installed on the editor.\n\n        :type painter: QtGui.QPainter\n        \"\"\"\n", "input": "", "output": "        checker_modes = []\n        for m in self.editor.modes:\n            if isinstance(m, modes.CheckerMode):\n                checker_modes.append(m)\n        for checker_mode in checker_modes:\n            for msg in checker_mode.messages:\n                block = msg.block\n                color = QtGui.QColor(msg.color)\n                brush = QtGui.QBrush(color)\n                rect = QtCore.QRect()\n                rect.setX(self.sizeHint().width() / 4)\n                rect.setY(block.blockNumber() * self.get_marker_height())\n                rect.setSize(self.get_marker_size())\n                painter.fillRect(rect, brush)", "category": "Python"}, {"instruction": "def perimeter(self):\n        '''\n        Sum of the length of all sides, float.\n        '''\n", "input": "", "output": "        return sum([a.distance(b) for a, b in self.pairs()])", "category": "Python"}, {"instruction": "def config_read():\n    \"\"\"Read config info from config file.\"\"\"\n", "input": "", "output": "    config_file = (u\"{0}config.ini\".format(CONFIG_DIR))\n    if not os.path.isfile(config_file):\n        config_make(config_file)\n    config = configparser.ConfigParser(allow_no_value=True)\n    try:\n        config.read(config_file, encoding='utf-8')\n    except IOError:\n        print(\"Error reading config file: {}\".format(config_file))\n        sys.exit()\n    # De-duplicate provider-list\n    providers = config_prov(config)\n    # Read credentials for listed providers\n    (cred, to_remove) = config_cred(config, providers)\n    # remove unsupported and credential-less providers\n    for item in to_remove:\n        providers.remove(item)\n    return cred, providers", "category": "Python"}, {"instruction": "def _checkIndentationIssue(self, node, node_type, linenoDocstring):\n        \"\"\"\n        Check whether a docstring have consistent indentations.\n\n        @param node: the node currently checks by pylint\n        @param node_type: type of given node\n        @param linenoDocstring: line number the docstring begins\n        \"\"\"\n", "input": "", "output": "        indentDocstring = node.col_offset and node.col_offset or 0\n        indentDocstring += len(re.findall(r'\\n( *)", "category": "Python"}, {"instruction": "def _ipv4_text_to_int(self, ip_text):\n        \"\"\"convert ip v4 string to integer.\"\"\"\n", "input": "", "output": "        if ip_text is None:\n            return None\n        assert isinstance(ip_text, str)\n        return struct.unpack('!I', addrconv.ipv4.text_to_bin(ip_text))[0]", "category": "Python"}, {"instruction": "def line_type(self, line_type):\n        \"\"\"Sets the line_type of this ChartSettings.\n\n        Plot interpolation type.  linear is default  # noqa: E501\n\n        :param line_type: The line_type of this ChartSettings.  # noqa: E501\n        :type: str\n        \"\"\"\n", "input": "", "output": "        allowed_values = [\"linear\", \"step-before\", \"step-after\", \"basis\", \"cardinal\", \"monotone\"]  # noqa: E501\n        if line_type not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `line_type` ({0}), must be one of {1}\"  # noqa: E501\n                .format(line_type, allowed_values)\n            )\n\n        self._line_type = line_type", "category": "Python"}, {"instruction": "def data(self, value):\n        \"\"\"Setter for the _data attribute. Should be set from response.read()\n\n        :param value: The body of the response object for the LRSResponse\n        :type value: unicode\n        \"\"\"\n", "input": "", "output": "        if value is not None and not isinstance(value, unicode):\n            value = value.decode('utf-8')\n        self._data = value", "category": "Python"}, {"instruction": "def _update_method(self, oldmeth, newmeth):\n        \"\"\"Update a method object.\"\"\"\n", "input": "", "output": "        # XXX What if im_func is not a function?\n        if hasattr(oldmeth, 'im_func') and hasattr(newmeth, 'im_func'):\n            self._update(None, None, oldmeth.im_func, newmeth.im_func)\n        elif hasattr(oldmeth, '__func__') and hasattr(newmeth, '__func__'):\n            self._update(None, None, oldmeth.__func__, newmeth.__func__)\n        return oldmeth", "category": "Python"}, {"instruction": "def to_path_value(self, obj):\n        \"\"\"\n        Takes value and turn it into a string suitable for inclusion in\n        the path, by url-encoding.\n\n        :param obj: object or string value.\n\n        :return string: quoted value.\n        \"\"\"\n", "input": "", "output": "        if type(obj) == list:\n            return ','.join(obj)\n        else:\n            return str(obj)", "category": "Python"}, {"instruction": "def get_different_page(self, request, page):\n        \"\"\"\n        Returns a url that preserves the current querystring\n        while changing the page requested to `page`.\n        \"\"\"\n", "input": "", "output": "\n        if page:\n            qs = request.GET.copy()\n            qs['page'] = page\n            return \"%s?%s\" % (request.path_info, qs.urlencode())\n        return None", "category": "Python"}, {"instruction": "def p_example_multiline(self, p):\n        \"\"\"example_field : ID EQ NL INDENT ex_map NL DEDENT\"\"\"\n", "input": "", "output": "        p[0] = AstExampleField(\n            self.path, p.lineno(1), p.lexpos(1), p[1], p[5])", "category": "Python"}, {"instruction": "def docstring_section_lines(docstring, section_name):\n    \"\"\"\n    Return a section of a numpydoc string\n\n    Paramters\n    ---------\n    docstring : str\n        Docstring\n    section_name : str\n        Name of section to return\n\n    Returns\n    -------\n    section : str\n        Section minus the header\n    \"\"\"\n", "input": "", "output": "    lines = []\n    inside_section = False\n    underline = '-' * len(section_name)\n    expect_underline = False\n    for line in docstring.splitlines():\n        _line = line.strip().lower()\n\n        if expect_underline:\n            expect_underline = False\n            if _line == underline:\n                inside_section = True\n                continue\n\n        if _line == section_name:\n            expect_underline = True\n        elif _line in DOCSTRING_SECTIONS:\n            # next section\n            break\n        elif inside_section:\n            lines.append(line)\n    return '\\n'.join(lines)", "category": "Python"}, {"instruction": "def add(self, host, **kwargs):\n        \"\"\"\n        Add another host to the SSH configuration.\n\n        Parameters\n        ----------\n        host: The Host entry to add.\n        **kwargs: The parameters for the host (without \"Host\" parameter itself)\n        \"\"\"\n", "input": "", "output": "        if host in self.hosts_:\n            raise ValueError(\"Host %s: exists (use update).\" % host)\n        self.hosts_.add(host)\n        self.lines_.append(ConfigLine(line=\"\", host=None))\n        self.lines_.append(ConfigLine(line=\"Host %s\" % host, host=host, key=\"Host\", value=host))\n        for k, v in kwargs.items():\n            if type(v) not in [list, tuple]:\n                v = [v]\n            mapped_k = _remap_key(k)\n            for value in v:\n                self.lines_.append(ConfigLine(line=\"  %s %s\" % (mapped_k, str(value)), host=host, key=mapped_k, value=value))\n        self.lines_.append(ConfigLine(line=\"\", host=None))", "category": "Python"}, {"instruction": "def list_upgrades(jid,\n                  style='group',\n                  outputter='nested',\n                  ext_source=None):\n    '''\n    Show list of available pkg upgrades using a specified format style\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run pkg.list_upgrades jid=20141120114114417719 style=group\n    '''\n", "input": "", "output": "    mminion = salt.minion.MasterMinion(__opts__)\n    returner = _get_returner((\n        __opts__['ext_job_cache'],\n        ext_source,\n        __opts__['master_job_cache']\n    ))\n\n    data = mminion.returners['{0}.get_jid'.format(returner)](jid)\n    pkgs = {}\n\n    if style == 'group':\n        for minion in data:\n            results = data[minion]['return']\n            for pkg, pkgver in six.iteritems(results):\n                if pkg not in six.iterkeys(pkgs):\n                    pkgs[pkg] = {pkgver: {'hosts': []}}\n\n                if pkgver not in six.iterkeys(pkgs[pkg]):\n                    pkgs[pkg].update({pkgver: {'hosts': []}})\n\n                pkgs[pkg][pkgver]['hosts'].append(minion)\n\n    if outputter:\n        salt.output.display_output(pkgs, outputter, opts=__opts__)\n\n    return pkgs", "category": "Python"}, {"instruction": "def sunion(self, keys, *args):\n        \"\"\"Emulate sunion.\"\"\"\n", "input": "", "output": "        func = lambda left, right: left.union(right)\n        return self._apply_to_sets(func, \"SUNION\", keys, *args)", "category": "Python"}, {"instruction": "def expunge(self, instance):\r\n        '''Remove *instance* from the :class:`Session`. Instance could be a\r\n:class:`Model` or an id.\r\n\r\n:parameter instance: a :class:`Model` or an *id*\r\n:rtype: the :class:`Model` removed from session or ``None`` if\r\n    it was not in the session.\r\n'''\n", "input": "", "output": "        instance = self.pop(instance)\r\n        instance.session = None\r\n        return instance", "category": "Python"}, {"instruction": "def posterior_covariance_between_points(self, X1, X2):\n        \"\"\"\n        Computes the posterior covariance between points.\n\n        :param X1: some input observations\n        :param X2: other input observations\n        \"\"\"\n", "input": "", "output": "        return self.posterior.covariance_between_points(self.kern, self.X, X1, X2)", "category": "Python"}, {"instruction": "def separate_directions(di_block):\n    \"\"\"\n    Separates set of directions into two modes based on principal direction\n\n    Parameters\n    _______________\n    di_block : block of nested dec,inc pairs\n\n    Return\n    mode_1_block,mode_2_block :  two lists of nested dec,inc pairs\n    \"\"\"\n", "input": "", "output": "    ppars = doprinc(di_block)\n    di_df = pd.DataFrame(di_block)  # turn into a data frame for easy filtering\n    di_df.columns = ['dec', 'inc']\n    di_df['pdec'] = ppars['dec']\n    di_df['pinc'] = ppars['inc']\n    di_df['angle'] = angle(di_df[['dec', 'inc']].values,\n                           di_df[['pdec', 'pinc']].values)\n    mode1_df = di_df[di_df['angle'] <= 90]\n    mode2_df = di_df[di_df['angle'] > 90]\n    mode1 = mode1_df[['dec', 'inc']].values.tolist()\n    mode2 = mode2_df[['dec', 'inc']].values.tolist()\n    return mode1, mode2", "category": "Python"}, {"instruction": "def zip_html(self):\n        \"\"\"\n        Compress HTML documentation into a zip file.\n        \"\"\"\n", "input": "", "output": "        zip_fname = os.path.join(BUILD_PATH, 'html', 'pandas.zip')\n        if os.path.exists(zip_fname):\n            os.remove(zip_fname)\n        dirname = os.path.join(BUILD_PATH, 'html')\n        fnames = os.listdir(dirname)\n        os.chdir(dirname)\n        self._run_os('zip',\n                     zip_fname,\n                     '-r',\n                     '-q',\n                     *fnames)", "category": "Python"}, {"instruction": "def files(self, fs, path=\"/\"):\n        # type: (FS, Text) -> Iterator[Text]\n        \"\"\"Walk a filesystem, yielding absolute paths to files.\n\n        Arguments:\n            fs (FS): A filesystem instance.\n            path (str): A path to a directory on the filesystem.\n\n        Yields:\n            str: absolute path to files on the filesystem found\n            recursively within the given directory.\n\n        \"\"\"\n", "input": "", "output": "        _combine = combine\n        for _path, info in self._iter_walk(fs, path=path):\n            if info is not None and not info.is_dir:\n                yield _combine(_path, info.name)", "category": "Python"}, {"instruction": "def toYearFraction(date):\n    \"\"\"Converts :class:`datetime.date` or :class:`datetime.datetime` to decimal\n    year.\n\n    Parameters\n    ==========\n    date : :class:`datetime.date` or :class:`datetime.datetime`\n\n    Returns\n    =======\n    year : float\n        Decimal year\n\n    Notes\n    =====\n    The algorithm is taken from http://stackoverflow.com/a/6451892/2978652\n\n    \"\"\"\n", "input": "", "output": "\n    def sinceEpoch(date):\n        ", "category": "Python"}, {"instruction": "def ServicesSetUseDataTimestamp(self, sensor_id, service_id, parameters):\r\n        \"\"\"\r\n            Indicate whether a math service should use the original timestamps of the incoming data, or let CommonSense timestamp the aggregated data.\r\n            \r\n            @param sensors_id (int) - Sensor id of the sensor the service is connected to.\r\n            @param service_id (int) - Service id of the service for which to set the expression.\r\n            @param parameters (dictonary) - Parameters to set the expression of the math service.\r\n                    @note - http://www.sense-os.nl/85?nodeId=85&selectedId=11887\r\n                    \r\n            @return (bool) - Boolean indicating whether ServicesSetuseDataTimestamp was successful.\r\n        \"\"\"\n", "input": "", "output": "        if self.__SenseApiCall__('/sensors/{0}/services/{1}/SetUseDataTimestamp.json'.format(sensor_id, service_id), 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "category": "Python"}, {"instruction": "def _git_version():\n    \"\"\"If installed with 'pip installe -e .' from inside a git repo, the\n    current git revision as a string\"\"\"\n", "input": "", "output": "\n    import subprocess\n    import os\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in ['SYSTEMROOT', 'PATH']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env['LANGUAGE'] = 'C'\n        env['LANG'] = 'C'\n        env['LC_ALL'] = 'C'\n        FNULL = open(os.devnull, 'w')\n        cwd = os.path.dirname(os.path.realpath(__file__))\n        proc = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, stderr=FNULL, env=env, cwd=cwd)\n        out = proc.communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd(['git', 'rev-parse', 'HEAD'])\n        return out.strip().decode('ascii')\n    except OSError:\n        return \"unknown\"", "category": "Python"}, {"instruction": "def binary_float_to_decimal_float(number: Union[float, str]) -> float:\n    \"\"\"\n    Convert binary floating point to decimal floating point.\n\n    :param number: Binary floating point.\n    :return: Decimal floating point representation of binary floating point.\n    \"\"\"\n", "input": "", "output": "    if isinstance(number, str):\n        if number[0] == '-':\n            n_sign = -1\n        else:\n            n_sign = 1\n    elif isinstance(number, float):\n        n_sign = np.sign(number)\n        number = str(number)\n\n    deci = 0\n    for ndx, val in enumerate(number.split('.')[-1]):\n        deci += float(val) / 2**(ndx+1)\n    deci *= n_sign\n\n    return deci", "category": "Python"}, {"instruction": "def series_with_slh(self, other):\n        \"\"\"Series product with another :class:`SLH` object\n\n        Args:\n            other (SLH): An upstream SLH circuit.\n\n        Returns:\n            SLH: The combined system.\n        \"\"\"\n", "input": "", "output": "        new_S = self.S * other.S\n        new_L = self.S * other.L + self.L\n\n        def ImAdjoint(m):\n            return (m.H - m) * (I / 2)\n\n        delta = ImAdjoint(self.L.adjoint() * self.S * other.L)\n\n        if isinstance(delta, Matrix):\n            new_H = self.H + other.H + delta[0, 0]\n        else:\n            assert delta == 0\n            new_H = self.H + other.H\n\n        return SLH(new_S, new_L, new_H)", "category": "Python"}, {"instruction": "def find_Note(data, freq, bits):\n    \"\"\"Get the frequencies, feed them to find_notes and the return the Note\n    with the highest amplitude.\"\"\"\n", "input": "", "output": "    data = find_frequencies(data, freq, bits)\n    return sorted(find_notes(data), key=operator.itemgetter(1))[-1][0]", "category": "Python"}, {"instruction": "def broadcast(*sinks_):\n    \"\"\"The |broadcast| decorator creates a |push| object that receives a\n    message by ``yield`` and then sends this message on to all the given sinks.\n\n    .. |broadcast| replace:: :py:func:`broadcast`\n    \"\"\"\n", "input": "", "output": "    @push\n    def bc():\n        sinks = [s() for s in sinks_]\n        while True:\n            msg = yield\n            for s in sinks:\n                s.send(msg)\n\n    return bc", "category": "Python"}, {"instruction": "def addGenotype(\n            self, genotype_id, genotype_label,\n            genotype_type=None,\n            genotype_description=None\n    ):\n        \"\"\"\n        If a genotype_type is not supplied,\n        we will default to 'intrinsic_genotype'\n        :param genotype_id:\n        :param genotype_label:\n        :param genotype_type:\n        :param genotype_description:\n        :return:\n\n        \"\"\"\n", "input": "", "output": "        if genotype_type is None:\n            genotype_type = self.globaltt['intrinsic_genotype']\n\n        self.model.addIndividualToGraph(\n            genotype_id, genotype_label, genotype_type, genotype_description)\n        return", "category": "Python"}, {"instruction": "def glitter_startbody(context):\n    \"\"\"\n    Template tag which renders the glitter overlay and sidebar. This is only\n    shown to users with permission to edit the page.\n    \"\"\"\n", "input": "", "output": "    user = context.get('user')\n    path_body = 'glitter/include/startbody.html'\n    path_plus = 'glitter/include/startbody_%s_%s.html'\n    rendered = ''\n\n    if user is not None and user.is_staff:\n        templates = [path_body]\n        # We've got a page with a glitter object:\n        # - May need a different startbody template\n        # - Check if user has permission to add\n        glitter = context.get('glitter')\n        if glitter is not None:\n            opts = glitter.obj._meta.app_label, glitter.obj._meta.model_name\n            template_path = path_plus % opts\n            templates.insert(0, template_path)\n\n        template = context.template.engine.select_template(templates)\n        rendered = template.render(context)\n\n    return rendered", "category": "Python"}, {"instruction": "def set_XY(self, X, Y):\n        \"\"\"\n        Set the input / output data of the model\n        This is useful if we wish to change our existing data but maintain the same model\n\n        :param X: input observations\n        :type X: np.ndarray\n        :param Y: output observations\n        :type Y: np.ndarray or ObsAr\n        \"\"\"\n", "input": "", "output": "        self.update_model(False)\n        self.set_Y(Y)\n        self.set_X(X)\n        self.update_model(True)", "category": "Python"}, {"instruction": "def rmon_alarm_entry_alarm_rising_event_index(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        rmon = ET.SubElement(config, \"rmon\", xmlns=\"urn:brocade.com:mgmt:brocade-rmon\")\n        alarm_entry = ET.SubElement(rmon, \"alarm-entry\")\n        alarm_index_key = ET.SubElement(alarm_entry, \"alarm-index\")\n        alarm_index_key.text = kwargs.pop('alarm_index')\n        alarm_rising_event_index = ET.SubElement(alarm_entry, \"alarm-rising-event-index\")\n        alarm_rising_event_index.text = kwargs.pop('alarm_rising_event_index')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def _query(self):\n        '''\n        Query the broadcast for defined services.\n        :return:\n        '''\n", "input": "", "output": "        query = salt.utils.stringutils.to_bytes(\n            \"{}{}\".format(self.signature, time.time()))\n        self._socket.sendto(query, ('<broadcast>', self.port))\n\n        return query", "category": "Python"}, {"instruction": "def pause(env, identifier):\n    \"\"\"Pauses an active virtual server.\"\"\"\n", "input": "", "output": "\n    vsi = SoftLayer.VSManager(env.client)\n    vs_id = helpers.resolve_id(vsi.resolve_ids, identifier, 'VS')\n\n    if not (env.skip_confirmations or\n            formatting.confirm('This will pause the VS with id %s. Continue?'\n                               % vs_id)):\n        raise exceptions.CLIAbort('Aborted.')\n\n    env.client['Virtual_Guest'].pause(id=vs_id)", "category": "Python"}, {"instruction": "def lin_interp(x, rangeX, rangeY):\n    \"\"\"\n    Interpolate linearly variable x in rangeX onto rangeY.\n    \"\"\"\n", "input": "", "output": "    s = (x - rangeX[0]) / mag(rangeX[1] - rangeX[0])\n    y = rangeY[0] * (1 - s) + rangeY[1] * s\n    return y", "category": "Python"}, {"instruction": "def _ExtractCronJobIdFromPath(entry, event):\n  \"\"\"Extracts a CronJob ID from an APIAuditEntry's HTTP request path.\"\"\"\n", "input": "", "output": "  match = re.match(r\".*cron-job/([^/]+).*\", entry.http_request_path)\n  if match:\n    event.urn = \"aff4:/cron/{}\".format(match.group(1))", "category": "Python"}, {"instruction": "def to_inputs(self, indices=None):\n        \"\"\"Converts a Transaction's outputs to spendable inputs.\n\n            Note:\n                Takes the Transaction's outputs and derives inputs\n                from that can then be passed into `Transaction.transfer` as\n                `inputs`.\n                A list of integers can be passed to `indices` that\n                defines which outputs should be returned as inputs.\n                If no `indices` are passed (empty list or None) all\n                outputs of the Transaction are returned.\n\n            Args:\n                indices (:obj:`list` of int): Defines which\n                    outputs should be returned as inputs.\n\n            Returns:\n                :obj:`list` of :class:`~bigchaindb.common.transaction.\n                    Input`\n        \"\"\"\n", "input": "", "output": "        # NOTE: If no indices are passed, we just assume to take all outputs\n        #       as inputs.\n        indices = indices or range(len(self.outputs))\n        return [\n            Input(self.outputs[idx].fulfillment,\n                  self.outputs[idx].public_keys,\n                  TransactionLink(self.id, idx))\n            for idx in indices\n        ]", "category": "Python"}, {"instruction": "def asset_security_marks_path(cls, organization, asset):\n        \"\"\"Return a fully-qualified asset_security_marks string.\"\"\"\n", "input": "", "output": "        return google.api_core.path_template.expand(\n            \"organizations/{organization}/assets/{asset}/securityMarks\",\n            organization=organization,\n            asset=asset,\n        )", "category": "Python"}, {"instruction": "def templated(template=None, *templates):\n    \"\"\"\n    Decorate a view function with one or more default template name.\n    This will try templates in the custom folder first,\n    the theme's original ones second.\n\n    :param template: template name or template name list\n    \"\"\"\n", "input": "", "output": "\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            template_ = template\n            if template_ is None:\n                template_ = request.endpoint.split('.', 1)[1].replace(\n                    '.', '/') + '.html'\n            context = func(*args, **kwargs)\n            if context is None:\n                context = {}\n            elif not isinstance(context, dict):\n                return context\n            return custom_render_template(\n                list(chain(to_list(template_), templates)), **context)\n\n        return wrapper\n\n    return decorator", "category": "Python"}, {"instruction": "def pdmerge_respeta_tz(func_merge, tz_ant=None, *args, **kwargs):\n    \"\"\"\n    Programaci\u00f3n defensiva por issue: pandas BUG (a veces, el index pierde el tz):\n        - issue #7795: concat of objects with the same timezone get reset to UTC;\n        - issue #10567: DataFrame combine_first() loses timezone information for datetime columns\n            https://github.com/pydata/pandas/issues/10567\n    :param tz_ant: TZ de referencia para establecerlo al final si se ha perdido en la operaci\u00f3n\n    :param func_merge: puntero a funci\u00f3n que realiza la operaci\u00f3n de merge / join / combine / etc.\n    :param args: argumentos para func_merge\n    :param kwargs: argumentos para func_merge\n    :return: dataframe_merged con TZ anterior\n    \"\"\"\n", "input": "", "output": "    df_merged = func_merge(*args, **kwargs)\n    if tz_ant is not None and tz_ant != df_merged.index.tz:\n        # print_warn('Error pandas: join prevProg + demandaGeneracion pierde timezone (%s->%s)'\n        #            % (data_import[KEYS_DATA[0]].index.tz, tz_ant))\n        df_merged.index = df_merged.index.tz_convert(tz_ant)\n    return df_merged", "category": "Python"}, {"instruction": "def runSql(self, migrationName, version):\n        \"\"\"\n        Given a migration name and version lookup the sql file and run it.\n        \"\"\"\n", "input": "", "output": "        sys.stdout.write(\"Running migration %s to version %s: ...\"%(migrationName, version))\n        sqlPath = os.path.join(self.migrationDirectory, migrationName)\n        sql = open(sqlPath, \"r\").read()\n        try:\n            if self.session.is_active:\n                print \"session is active\"\n                self.session.commit()\n            self.session.begin()\n            executeBatch(self.session, sql)\n            self.session.add(models.Migration(version, migrationName))\n        except:\n            print \"\\n\"\n            self.session.rollback()\n            raise\n        else:\n            self.session.commit()\n        sys.stdout.write(\"\\r\")\n        sys.stdout.flush()\n        sys.stdout.write(\"Running migration %s to version %s: SUCCESS!\\n\"%(migrationName, version))", "category": "Python"}, {"instruction": "def subscribe(self, observer):\n        \"\"\"Subscribe an observer to this subject and return a subscription id\n\n        \"\"\"\n", "input": "", "output": "        sid = self._sn\n        self.observers[sid] = observer\n        self._sn += 1\n        return SubscribeID(self, sid)", "category": "Python"}, {"instruction": "def get_cmor_fp_meta(fp):\n    \"\"\"Processes a CMOR style file path.\n\n    Section 3.1 of the `Data Reference Syntax`_ details:\n\n        The standard CMIP5 output tool CMOR optionally writes output files\n        to a directory structure mapping DRS components to directory names as:\n\n            <activity>/<product>/<institute>/<model>/<experiment>/<frequency>/\n            <modeling_realm>/<variable_name>/<ensemble_member>/<CMOR filename>.nc\n\n    Arguments:\n        fp (str): A file path conforming to DRS spec.\n\n    Returns:\n        dict: Metadata as extracted from the file path.\n\n    .. _Data Reference Syntax:\n       http://cmip-pcmdi.llnl.gov/cmip5/docs/cmip5_data_reference_syntax.pdf\n    \"\"\"\n", "input": "", "output": "\n    # Copy metadata list then reverse to start at end of path\n    directory_meta = list(CMIP5_FP_ATTS)\n\n    # Prefer meta extracted from filename\n    meta = get_dir_meta(fp, directory_meta)\n    meta.update(get_cmor_fname_meta(fp))\n\n    return meta", "category": "Python"}, {"instruction": "def _group_by_batches(samples, check_fn):\n    \"\"\"Group data items into batches, providing details to retrieve results.\n    \"\"\"\n", "input": "", "output": "    batch_groups = collections.defaultdict(list)\n    singles = []\n    out_retrieve = []\n    extras = []\n    for data in [x[0] for x in samples]:\n        if check_fn(data):\n            batch = tz.get_in([\"metadata\", \"batch\"], data)\n            name = str(dd.get_sample_name(data))\n            if batch:\n                out_retrieve.append((str(batch), data))\n            else:\n                out_retrieve.append((name, data))\n            for vrn in data[\"variants\"]:\n                if vrn.get(\"population\", True):\n                    if batch:\n                        batch_groups[(str(batch), vrn[\"variantcaller\"])].append((vrn[\"vrn_file\"], data))\n                    else:\n                        singles.append((name, vrn[\"variantcaller\"], data, vrn[\"vrn_file\"]))\n        else:\n            extras.append(data)\n    return batch_groups, singles, out_retrieve, extras", "category": "Python"}, {"instruction": "def resource_for_link(link, includes, resources=None, locale=None):\n    \"\"\"Returns the resource that matches the link\"\"\"\n", "input": "", "output": "\n    if resources is not None:\n        cache_key = \"{0}:{1}:{2}\".format(\n            link['sys']['linkType'],\n            link['sys']['id'],\n            locale\n        )\n        if cache_key in resources:\n            return resources[cache_key]\n\n    for i in includes:\n        if (i['sys']['id'] == link['sys']['id'] and\n                i['sys']['type'] == link['sys']['linkType']):\n            return i\n    return None", "category": "Python"}, {"instruction": "def dump(obj=missing):\n    \"\"\"Print the object details to stdout._write (for the interactive\n    console of the web debugger.\n    \"\"\"\n", "input": "", "output": "    gen = DebugReprGenerator()\n    if obj is missing:\n        rv = gen.dump_locals(sys._getframe(1).f_locals)\n    else:\n        rv = gen.dump_object(obj)\n    sys.stdout._write(rv)", "category": "Python"}, {"instruction": "def get_frac_coords_from_lll(self, lll_frac_coords: Vector3Like) -> np.ndarray:\n        \"\"\"\n        Given fractional coordinates in the lll basis, returns corresponding\n        fractional coordinates in the lattice basis.\n        \"\"\"\n", "input": "", "output": "        return dot(lll_frac_coords, self.lll_mapping)", "category": "Python"}, {"instruction": "def enable_global_typechecked_decorator(flag = True, retrospective = True):\n    \"\"\"Enables or disables global typechecking mode via decorators.\n    See flag global_typechecked_decorator.\n    In contrast to setting the flag directly, this function provides\n    a retrospective option. If retrospective is true, this will also\n    affect already imported modules, not only future imports.\n    Does not work if checking_enabled is false.\n    Does not work reliably if checking_enabled has ever been set to\n    false during current run.\n    \"\"\"\n", "input": "", "output": "    global global_typechecked_decorator\n    global_typechecked_decorator = flag\n    if import_hook_enabled:\n        _install_import_hook()\n    if global_typechecked_decorator and retrospective:\n        _catch_up_global_typechecked_decorator()\n    return global_typechecked_decorator", "category": "Python"}, {"instruction": "def make_stacker_cmd_string(args, lib_path):\n    \"\"\"Generate stacker invocation script from command line arg list.\n\n    This is the standard stacker invocation script, with the following changes:\n    * Adding our explicit arguments to parse_args (instead of leaving it empty)\n    * Overriding sys.argv\n    * Adding embedded runway lib directory to sys.path\n    \"\"\"\n", "input": "", "output": "    if platform.system().lower() == 'windows':\n        # Because this will be run via subprocess, the backslashes on Windows\n        # will cause command errors\n        lib_path = lib_path.replace('\\\\', '/')\n    return (\"import sys;\"\n            \"sys.argv = ['stacker'] + {args};\"\n            \"sys.path.insert(1, '{lib_path}');\"\n            \"from stacker.logger import setup_logging;\"\n            \"from stacker.commands import Stacker;\"\n            \"stacker = Stacker(setup_logging=setup_logging);\"\n            \"args = stacker.parse_args({args});\"\n            \"stacker.configure(args);args.run(args)\".format(args=str(args),\n                                                            lib_path=lib_path))", "category": "Python"}, {"instruction": "def hasScoreBetterThan(self, score):\n        \"\"\"\n        Is there an HSP with a score better than a given value?\n\n        @return: A C{bool}, C{True} if there is at least one HSP in the\n        alignments for this title with a score better than C{score}.\n        \"\"\"\n", "input": "", "output": "        # Note: Do not assume that HSPs in an alignment are sorted in\n        # decreasing order (as they are in BLAST output). If we could\n        # assume that, we could just check the first HSP in each alignment.\n        for hsp in self.hsps():\n            if hsp.betterThan(score):\n                return True\n        return False", "category": "Python"}, {"instruction": "def get_config(self, key, default=MISSING):\n        \"\"\"Get the value of a persistent config key from the registry\n\n        If no default is specified and the key is not found ArgumentError is raised.\n\n        Args:\n            key (string): The key name to fetch\n            default (string): an optional value to be returned if key cannot be found\n\n        Returns:\n            string: the key's value\n        \"\"\"\n", "input": "", "output": "\n        keyname = \"config:\" + key\n\n        try:\n            return self.kvstore.get(keyname)\n        except KeyError:\n            if default is MISSING:\n                raise ArgumentError(\"No config value found for key\", key=key)\n\n            return default", "category": "Python"}, {"instruction": "def maybe_parse_user_type(t):\n    \"\"\"Try to coerce a user-supplied type directive into a list of types.\n\n    This function should be used in all places where a user specifies a type,\n    for consistency.\n\n    The policy for what defines valid user input should be clear from the implementation.\n    \"\"\"\n", "input": "", "output": "    is_type = isinstance(t, type)\n    is_preserved = isinstance(t, type) and issubclass(t, _preserved_iterable_types)\n    is_string = isinstance(t, string_types)\n    is_iterable = isinstance(t, Iterable)\n\n    if is_preserved:\n        return [t]\n    elif is_string:\n        return [t]\n    elif is_type and not is_iterable:\n        return [t]\n    elif is_iterable:\n        # Recur to validate contained types as well.\n        ts = t\n        return tuple(e for t in ts for e in maybe_parse_user_type(t))\n    else:\n        # If this raises because `t` cannot be formatted, so be it.\n        raise TypeError(\n            'Type specifications must be types or strings. Input: {}'.format(t)\n        )", "category": "Python"}, {"instruction": "def clone(cls, srcpath, destpath):\n        \"\"\"Copy a main repository to a new location.\"\"\"\n", "input": "", "output": "        try:\n            os.makedirs(destpath)\n        except OSError as e:\n            if not e.errno == errno.EEXIST:\n                raise\n        cmd = [SVNADMIN, 'dump', '--quiet', '.']\n        dump = subprocess.Popen(\n               cmd, cwd=srcpath, stdout=subprocess.PIPE,\n               stderr=subprocess.PIPE,\n        )\n        repo = cls.create(destpath)\n        repo.load(dump.stdout)\n        stderr = dump.stderr.read()\n        dump.stdout.close()\n        dump.stderr.close()\n        dump.wait()\n        if dump.returncode != 0:\n            raise subprocess.CalledProcessError(dump.returncode, cmd, stderr)\n        return repo", "category": "Python"}, {"instruction": "def delete_dimension(dimension_id,**kwargs):\n    \"\"\"\n        Delete a dimension from the DB. Raises and exception if the dimension does not exist\n    \"\"\"\n", "input": "", "output": "    try:\n        dimension = db.DBSession.query(Dimension).filter(Dimension.id==dimension_id).one()\n\n        db.DBSession.query(Unit).filter(Unit.dimension_id==dimension.id).delete()\n\n        db.DBSession.delete(dimension)\n        db.DBSession.flush()\n        return True\n    except NoResultFound:\n        raise ResourceNotFoundError(\"Dimension (dimension_id=%s) does not exist\"%(dimension_id))", "category": "Python"}, {"instruction": "def flatten(data):\n    \"\"\"Returns a flattened version of a list.\n\n    Courtesy of https://stackoverflow.com/a/12472564\n\n    Args:\n        data (`tuple` or `list`): Input data\n\n    Returns:\n        `list`\n    \"\"\"\n", "input": "", "output": "    if not data:\n        return data\n\n    if type(data[0]) in (list, tuple):\n        return list(flatten(data[0])) + list(flatten(data[1:]))\n\n    return list(data[:1]) + list(flatten(data[1:]))", "category": "Python"}, {"instruction": "def should_update(self):\n        \"\"\"Return True is the AMP should be updated:\n        - AMP is enable\n        - only update every 'refresh' seconds\n        \"\"\"\n", "input": "", "output": "        if self.timer.finished():\n            self.timer.set(self.refresh())\n            self.timer.reset()\n            return self.enable()\n        return False", "category": "Python"}, {"instruction": "def with_user_roles(roles):\n    \"\"\"\n    with_user_roles(roles)\n\n    It allows to check if a user has access to a view by adding the decorator\n    with_user_roles([])\n\n    Requires flask-login\n\n    In your model, you must have a property 'role', which will be invoked to\n    be compared to the roles provided.\n\n    If current_user doesn't have a role, it will throw a 403\n\n    If the current_user is not logged in will throw a 401\n\n    * Require Flask-Login\n    ---\n    Usage\n\n    @app.route('/user')\n    @login_require\n    @with_user_roles(['admin', 'user'])\n    def user_page(self):\n        return \"You've got permission to access this page.\"\n    \"\"\"\n", "input": "", "output": "    def wrapper(f):\n        @functools.wraps(f)\n        def wrapped(*args, **kwargs):\n            if current_user.is_authenticated():\n                if not hasattr(current_user, \"role\"):\n                    raise AttributeError(\"<'role'> doesn't exist in login 'current_user'\")\n                if current_user.role not in roles:\n                    return abort(403)\n            else:\n                return abort(401)\n            return f(*args, **kwargs)\n        return wrapped\n    return wrapper", "category": "Python"}, {"instruction": "def parse_reason(response, content):\n    \"\"\"\n    Google return a JSON document describing the error. We should parse this\n    and extract the error reason. See\n    https://developers.google.com/analytics/devguides/reporting/core/v3/coreErrors\n    \"\"\"\n", "input": "", "output": "    def first_error(data):\n        errors = data['error']['errors']\n        if len(errors) > 1:\n            # we have more than one error. We should capture that?\n            logging.info('Received {} errors'.format(len(errors)))\n        return errors[0]\n\n    try:\n        json_data = json.loads(content)\n        return first_error(json_data)['reason']\n    except:\n        return response.reason", "category": "Python"}, {"instruction": "def ddtodms(self, dd):\n        \"\"\"Take in dd string and convert to dms\"\"\"\n", "input": "", "output": "        negative = dd < 0\n        dd = abs(dd)\n        minutes,seconds = divmod(dd*3600,60)\n        degrees,minutes = divmod(minutes,60)\n        if negative:\n            if degrees > 0:\n                degrees = -degrees\n            elif minutes > 0:\n                minutes = -minutes\n            else:\n                seconds = -seconds\n        return (degrees,minutes,seconds)", "category": "Python"}, {"instruction": "def get_plugin(self, method):\n        \"\"\"\n        Return plugin object if CLI option is activated and method exists\n\n        @param method: name of plugin's method we're calling\n        @type method: string\n\n        @returns: list of plugins with `method`\n\n        \"\"\"\n", "input": "", "output": "        all_plugins = []\n        for entry_point in pkg_resources.iter_entry_points('yolk.plugins'):\n            plugin_obj = entry_point.load()\n            plugin = plugin_obj()\n            plugin.configure(self.options, None)\n            if plugin.enabled:\n                if not hasattr(plugin, method):\n                    self.logger.warn(\"Error: plugin has no method: %s\" % method)\n                    plugin = None\n                else:\n                    all_plugins.append(plugin)\n        return all_plugins", "category": "Python"}, {"instruction": "def sync_auth_groups():\n    \"\"\"\n    Syncs auth groups according to settings.AUTH_GROUPS\n    :return:\n    \"\"\"\n", "input": "", "output": "    from django.conf import settings\n    from django.contrib.auth.models import Group, Permission\n    from django.db.models import Q\n\n    for data in settings.AUTH_GROUPS:\n        g1 = Group.objects.get_or_create(name=data['name'])[0]\n        g1.permissions.clear()\n        g1.save()\n        args = []\n        print('*** Created group %s' % g1.name)\n        for perm in data['permissions']:\n            print(perm)\n            args.append(Q(codename=perm[0], content_type__app_label=perm[1]))\n            print('   *** Created permission %s' % perm[0])\n\n        g1.permissions.add(*list(Permission.objects.filter(\n            reduce(OR, args)\n        )))", "category": "Python"}, {"instruction": "def randomWalkFunction(requestContext, name, step=60):\n    \"\"\"\n    Short Alias: randomWalk()\n\n    Returns a random walk starting at 0. This is great for testing when there\n    is no real data in whisper.\n\n    Example::\n\n        &target=randomWalk(\"The.time.series\")\n\n    This would create a series named \"The.time.series\" that contains points\n    where x(t) == x(t-1)+random()-0.5, and x(0) == 0.\n\n    Accepts an optional second argument as step parameter (default step is\n    60 sec).\n    \"\"\"\n", "input": "", "output": "    delta = timedelta(seconds=step)\n    when = requestContext[\"startTime\"]\n    values = []\n    current = 0\n    while when < requestContext[\"endTime\"]:\n        values.append(current)\n        current += random.random() - 0.5\n        when += delta\n\n    return [TimeSeries(\n        name, int(epoch(requestContext[\"startTime\"])),\n        int(epoch(requestContext[\"endTime\"])),\n        step, values)]", "category": "Python"}, {"instruction": "def export(self, composite=False):\n        \"\"\"Export this name as a token.\n\n        This method exports the name into a byte string which can then be\n        imported by using the `token` argument of the constructor.\n\n        Args:\n            composite (bool): whether or not use to a composite token --\n                :requires-ext:`rfc6680`\n\n        Returns:\n            bytes: the exported name in token form\n\n        Raises:\n            MechanismNameRequiredError\n            BadNameTypeError\n            BadNameError\n        \"\"\"\n", "input": "", "output": "\n        if composite:\n            if rname_rfc6680 is None:\n                raise NotImplementedError(\"Your GSSAPI implementation does \"\n                                          \"not support RFC 6680 (the GSSAPI \"\n                                          \"naming extensions)\")\n\n            return rname_rfc6680.export_name_composite(self)\n        else:\n            return rname.export_name(self)", "category": "Python"}, {"instruction": "def enable_contactgroup_svc_notifications(self, contactgroup):\n        \"\"\"Enable service notifications for a contactgroup\n        Format of the line that triggers function call::\n\n        ENABLE_CONTACTGROUP_SVC_NOTIFICATIONS;<contactgroup_name>\n\n        :param contactgroup: contactgroup to enable\n        :type contactgroup: alignak.objects.contactgroup.Contactgroup\n        :return: None\n        \"\"\"\n", "input": "", "output": "        for contact_id in contactgroup.get_contacts():\n            self.enable_contact_svc_notifications(self.daemon.contacts[contact_id])", "category": "Python"}, {"instruction": "def _get_vrfs(self):\n        \"\"\"Get the current VRFs configured in the device.\n\n        :return: A list of vrf names as string\n        \"\"\"\n", "input": "", "output": "        vrfs = []\n        ios_cfg = self._get_running_config()\n        parse = HTParser(ios_cfg)\n        vrfs_raw = parse.find_lines(\"^vrf definition\")\n        for line in vrfs_raw:\n            #  raw format ['ip vrf <vrf-name>',....]\n            vrf_name = line.strip().split(' ')[2]\n            vrfs.append(vrf_name)\n        LOG.info(\"VRFs:%s\", vrfs)\n        return vrfs", "category": "Python"}, {"instruction": "def respond_unauthorized(self, request_authentication=False):\n\t\t\"\"\"\n\t\tRespond to the client that the request is unauthorized.\n\n\t\t:param bool request_authentication: Whether to request basic authentication information by sending a WWW-Authenticate header.\n\t\t\"\"\"\n", "input": "", "output": "\t\theaders = {}\n\t\tif request_authentication:\n\t\t\theaders['WWW-Authenticate'] = 'Basic realm=\"' + self.__config['server_version'] + '\"'\n\t\tself.send_response_full(b'Unauthorized', status=401, headers=headers)\n\t\treturn", "category": "Python"}, {"instruction": "async def _check_ip(self, addr):\n        \"\"\"\n        Async check ip with dnsbl providers.\n        Parameters:\n            * addr - ip address to check\n\n        Returns:\n            * DNSBLResult object\n        \"\"\"\n", "input": "", "output": "\n        tasks = []\n        for provider in self.providers:\n            tasks.append(self.dnsbl_request(addr, provider))\n        results = await asyncio.gather(*tasks)\n        return DNSBLResult(addr=addr, results=results)", "category": "Python"}, {"instruction": "def handle_error(self, error, req, schema):\n        \"\"\"Handles errors during parsing. Aborts the current HTTP request and\n        responds with a 422 error.\n        \"\"\"\n", "input": "", "output": "\n        status_code = getattr(error, \"status_code\", self.DEFAULT_VALIDATION_STATUS)\n        abort(status_code, exc=error, messages=error.messages, schema=schema)", "category": "Python"}, {"instruction": "def writearff(data, filename, relation_name=None, index=True):\n    \"\"\"Write ARFF file\n\n    Parameters\n    ----------\n    data : :class:`pandas.DataFrame`\n        DataFrame containing data\n\n    filename : string or file-like object\n        Path to ARFF file or file-like object. In the latter case,\n        the handle is closed by calling this function.\n\n    relation_name : string, optional, default: \"pandas\"\n        Name of relation in ARFF file.\n\n    index : boolean, optional, default: True\n        Write row names (index)\n    \"\"\"\n", "input": "", "output": "    if isinstance(filename, str):\n        fp = open(filename, 'w')\n\n        if relation_name is None:\n            relation_name = os.path.basename(filename)\n    else:\n        fp = filename\n\n        if relation_name is None:\n            relation_name = \"pandas\"\n\n    try:\n        data = _write_header(data, fp, relation_name, index)\n        fp.write(\"\\n\")\n        _write_data(data, fp)\n    finally:\n        fp.close()", "category": "Python"}, {"instruction": "def _setup_dir(self, base_dir):\n        \"\"\" Creates stats directory for storing stat files.\n\n            `base_dir`\n                Base directory.\n            \"\"\"\n", "input": "", "output": "        stats_dir = self._sdir(base_dir)\n\n        if not os.path.isdir(stats_dir):\n            try:\n                os.mkdir(stats_dir)\n            except OSError:\n                raise errors.DirectorySetupFail()", "category": "Python"}, {"instruction": "def handle_xmlrpc(self, request_text):\n        \"\"\"Handle a single XML-RPC request\"\"\"\n", "input": "", "output": "\n        response = self._marshaled_dispatch(request_text)\n\n        sys.stdout.write('Content-Type: text/xml\\n')\n        sys.stdout.write('Content-Length: %d\\n' % len(response))\n        sys.stdout.write('\\n')\n\n        sys.stdout.write(response)", "category": "Python"}, {"instruction": "def _check_required_fields(self, fields=None, either_fields=None):\n        ''' Check the values of the fields\n\n        If no value found in `fields`, an exception will be raised.\n        `either_fields` are the fields that one of them must have a value\n\n        Raises:\n            HSException: If no value found in at least one item of`fields`, or\n                no value found in one of the items of `either_fields`\n\n        Returns:\n            None\n\n        '''\n", "input": "", "output": "\n        for (key, value) in fields.items():\n            # If value is a dict, one of the fields in the dict is required ->\n            # exception if all are None\n            if not value:\n                raise HSException(\"Field '%s' is required.\" % key)\n        if either_fields is not None:\n            for field in either_fields:\n                if not any(field.values()):\n                    raise HSException(\"One of the following fields is required: %s\" % \", \".join(field.keys()))", "category": "Python"}, {"instruction": "def decrement_display_ref_count(self, amount: int=1):\n        \"\"\"Decrement display reference count to indicate this library item is no longer displayed.\"\"\"\n", "input": "", "output": "        assert not self._closed\n        self.__display_ref_count -= amount\n        if self.__display_ref_count == 0:\n            self.__is_master = False\n        if self.__data_item:\n            for _ in range(amount):\n                self.__data_item.decrement_data_ref_count()", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"Union[Datetime, None]: Datetime when the stage started.\"\"\"\n", "input": "", "output": "        if self._properties.get(\"startMs\") is None:\n            return None\n        return _helpers._datetime_from_microseconds(\n            int(self._properties.get(\"startMs\")) * 1000.0\n        )", "category": "Python"}, {"instruction": "def set_alias(self, alias, indices, **kwargs):\n        \"\"\"\n        Set an alias.\n        (See :ref:`es-guide-reference-api-admin-indices-aliases`)\n\n        This handles removing the old list of indices pointed to by the alias.\n\n        Warning: there is a race condition in the implementation of this\n        function - if another client modifies the indices which this alias\n        points to during this call, the old value of the alias may not be\n        correctly set.\n\n        :param alias: the name of an alias\n        :param indices: a list of indices\n        \"\"\"\n", "input": "", "output": "        indices = self.conn._validate_indices(indices)\n        try:\n            old_indices = self.get_alias(alias)\n        except IndexMissingException:\n            old_indices = []\n        commands = [['remove', index, alias, {}] for index in old_indices]\n        commands.extend([['add', index, alias,\n                          self._get_alias_params(**kwargs)] for index in indices])\n        if len(commands) > 0:\n            return self.change_aliases(commands)", "category": "Python"}, {"instruction": "def format_value(column_dict, value, key=None):\n    \"\"\"\n    Format a value coming from the database (for example converts datetimes to\n    strings)\n\n    :param column_dict: The column datas collected during inspection\n    :param value: A value coming from the database\n    :param key: The exportation key\n    \"\"\"\n", "input": "", "output": "    formatter = column_dict.get('formatter')\n    prop = column_dict['__col__']\n\n    res = value\n\n    if value in ('', None,):\n        res = ''\n\n    elif formatter is not None:\n        res = formatter(value)\n\n    else:\n        if hasattr(prop, \"columns\"):\n            sqla_column = prop.columns[0]\n            column_type = getattr(sqla_column.type, 'impl', sqla_column.type)\n\n            formatter = FORMATTERS_REGISTRY.get_formatter(column_type, key)\n            if formatter is not None:\n                res = formatter(value)\n    return res", "category": "Python"}, {"instruction": "def get_bytes_from_data(self, offset, data):\n        \"\"\".\"\"\"\n", "input": "", "output": "        if offset > len(data):\n            return b''\n        d = data[offset:]\n        if isinstance(d, bytearray):\n            return bytes(d)\n        return d", "category": "Python"}, {"instruction": "def attach_tasks(self, tasks: Tasks):\n        \"\"\"Attach a set of tasks.\n\n        A task cannot be scheduled or executed before it is attached to an\n        Engine.\n\n        >>> tasks = Tasks()\n        >>> spin.attach_tasks(tasks)\n        \"\"\"\n", "input": "", "output": "        if tasks._spin is not None and tasks._spin is not self:\n            logger.warning('Tasks already attached to a different Engine')\n        self._tasks.update(tasks)\n        tasks._spin = self", "category": "Python"}, {"instruction": "def delete(self):\n        \"\"\"\n        Delete the instance from redis storage.\n        \"\"\"\n", "input": "", "output": "        # Delete each field\n        for field_name in self._fields:\n            field = self.get_field(field_name)\n            if not isinstance(field, PKField):\n                # pk has no stored key\n                field.delete()\n        # Remove the pk from the model collection\n        self.connection.srem(self.get_field('pk').collection_key, self._pk)\n        # Deactivate the instance\n        delattr(self, \"_pk\")", "category": "Python"}, {"instruction": "def _superclasses_for_subject(self, graph, typeof):\n        \"\"\"helper, returns a list of all superclasses of a given class\"\"\"\n", "input": "", "output": "        # TODO - this might be replacing a fairly simple graph API query where\n        # it doesn't need to\n        classes = []\n        superclass = typeof\n        while True:\n            found = False\n            for p, o in self.schema_def.ontology[superclass]:\n                if self.schema_def.lexicon['subclass'] == str(p):\n                    found = True\n                    classes.append(o)\n                    superclass = o\n            if not found:\n                break\n        return classes", "category": "Python"}, {"instruction": "def compute_integrated_acquisition_withGradients(acquisition,x):\n    '''\n    Used to compute the acquisition function with gradients when samples of the hyper-parameters have been generated (used in GP_MCMC model).\n\n    :param acquisition: acquisition function with GpyOpt model type GP_MCMC.\n    :param x: location where the acquisition is evaluated.\n    '''\n", "input": "", "output": "\n    acqu_x = 0\n    d_acqu_x = 0\n\n    for i in range(acquisition.model.num_hmc_samples):\n        acquisition.model.model.kern[:] = acquisition.model.hmc_samples[i,:]\n        acqu_x_sample, d_acqu_x_sample = acquisition.acquisition_function_withGradients(x)\n        acqu_x += acqu_x_sample\n        d_acqu_x += d_acqu_x_sample\n\n    acqu_x = acqu_x/acquisition.model.num_hmc_samples\n    d_acqu_x = d_acqu_x/acquisition.model.num_hmc_samples\n\n    return acqu_x, d_acqu_x", "category": "Python"}, {"instruction": "def iter_content(self, chunk_size=10 * 1024, decode_unicode=None):\n        \"\"\"Iterates over the response data.  This avoids reading the content\n        at once into memory for large responses.  The chunk size is the number\n        of bytes it should read into memory.  This is not necessarily the\n        length of each item returned as decoding can take place.\n        \"\"\"\n", "input": "", "output": "        if self._content_consumed:\n            raise RuntimeError(\n                'The content for this response was already consumed'\n            )\n\n        def generate():\n            while 1:\n                chunk = self.raw.read(chunk_size)\n                if not chunk:\n                    break\n                yield chunk\n            self._content_consumed = True\n\n        gen = generate()\n\n        if 'gzip' in self.headers.get('content-encoding', ''):\n            gen = stream_decode_gzip(gen)\n\n        if decode_unicode is None:\n            decode_unicode = self.config.get('decode_unicode')\n\n        if decode_unicode:\n            gen = stream_decode_response_unicode(gen, self)\n\n        return gen", "category": "Python"}, {"instruction": "def get_colors(n, cmap='viridis', start=0., stop=1., alpha=1., return_hex=False):\n    \"\"\"\n    Return n-length list of RGBa colors from the passed colormap name and alpha.\n\n    Parameters\n    ----------\n    n : int\n        number of colors\n    cmap : string\n        name of a colormap\n    start : float\n        where to start in the colorspace\n    stop : float\n        where to end in the colorspace\n    alpha : float\n        opacity, the alpha channel for the RGBa colors\n    return_hex : bool\n        if True, convert RGBa colors to a hexadecimal string\n\n    Returns\n    -------\n    colors : list\n    \"\"\"\n", "input": "", "output": "    colors = [cm.get_cmap(cmap)(x) for x in np.linspace(start, stop, n)]\n    colors = [(r, g, b, alpha) for r, g, b, _ in colors]\n    if return_hex:\n        colors = rgb_color_list_to_hex(colors)\n    return colors", "category": "Python"}, {"instruction": "def oembed(url, params=\"\"):\n    \"\"\"\n    Render an OEmbed-compatible link as an embedded item.\n\n\n    :param url: A URL of an OEmbed provider.\n    :return: The OEMbed ``<embed>`` code.\n    \"\"\"\n", "input": "", "output": "    # Note: this method isn't currently very efficient - the data isn't\n    # cached or stored.\n    kwargs = dict(urlparse.parse_qsl(params))\n\n    try:\n        return mark_safe(get_oembed_data(\n            url,\n            **kwargs\n        )['html'])\n    except (KeyError, ProviderException):\n        if settings.DEBUG:\n            return \"No OEmbed data returned\"\n        return \"\"", "category": "Python"}, {"instruction": "def bounding_box(self):\n        \"\"\"\n        The minimal bounding box (`~regions.BoundingBox`) enclosing the\n        exact rectangular region.\n        \"\"\"\n", "input": "", "output": "\n        w2 = self.width / 2.\n        h2 = self.height / 2.\n        cos_angle = np.cos(self.angle)    # self.angle is a Quantity\n        sin_angle = np.sin(self.angle)    # self.angle is a Quantity\n        dx1 = abs(w2 * cos_angle - h2 * sin_angle)\n        dy1 = abs(w2 * sin_angle + h2 * cos_angle)\n        dx2 = abs(w2 * cos_angle + h2 * sin_angle)\n        dy2 = abs(w2 * sin_angle - h2 * cos_angle)\n        dx = max(dx1, dx2)\n        dy = max(dy1, dy2)\n\n        xmin = self.center.x - dx\n        xmax = self.center.x + dx\n        ymin = self.center.y - dy\n        ymax = self.center.y + dy\n\n        return BoundingBox.from_float(xmin, xmax, ymin, ymax)", "category": "Python"}, {"instruction": "def retrieve(self, request, *args, **kwargs):\n        \"\"\"\n        Optional `field` query parameter (can be list) allows to limit what fields are returned.\n        For example, given request /api/projects/<uuid>/?field=uuid&field=name you get response like this:\n\n        .. code-block:: javascript\n\n            {\n                \"uuid\": \"90bcfe38b0124c9bbdadd617b5d739f5\",\n                \"name\": \"Default\"\n            }\n        \"\"\"\n", "input": "", "output": "        return super(ProjectViewSet, self).retrieve(request, *args, **kwargs)", "category": "Python"}, {"instruction": "def attach(decorator, obj, recursive=True):\n    \"\"\"attach(decorator, class_or_module[, recursive = True])\n\n    Utility to attach a \\val{decorator} to the \\val{obj} instance.\n\n    If \\val{obj} is a module, the decorator will be attached to every\n    function and class in the module.\n\n    If \\val{obj} is a class, the decorator will be attached to every\n    method and subclass of the class.\n\n    if \\val{recursive} is \"True\" then subclasses will be decorated.\n    \"\"\"\n", "input": "", "output": "    if inspect.ismodule(obj):\n        for name, fn in inspect.getmembers(obj, inspect.isfunction):\n            setattr(obj, name, decorator(fn))\n        for name, klass in inspect.getmembers(obj, inspect.isclass):\n            attach_to_class(decorator, klass, recursive)\n    elif inspect.isclass(obj):\n        attach_to_class(decorator, obj, recursive)", "category": "Python"}, {"instruction": "def moments(self):\n        \"\"\"The first two time delay weighted statistical moments of the\n        MA coefficients.\"\"\"\n", "input": "", "output": "        moment1 = statstools.calc_mean_time(self.delays, self.coefs)\n        moment2 = statstools.calc_mean_time_deviation(\n            self.delays, self.coefs, moment1)\n        return numpy.array([moment1, moment2])", "category": "Python"}, {"instruction": "def init(opts):\n    '''\n    This function gets called when the proxy starts up.\n    We check opts to see if a fallback user and password are supplied.\n    If they are present, and the primary credentials don't work, then\n    we try the backup before failing.\n\n    Whichever set of credentials works is placed in the persistent\n    DETAILS dictionary and will be used for further communication with the\n    chassis.\n    '''\n", "input": "", "output": "    if 'host' not in opts['proxy']:\n        log.critical('No \"host\" key found in pillar for this proxy')\n        return False\n\n    DETAILS['host'] = opts['proxy']['host']\n\n    (username, password) = find_credentials()", "category": "Python"}, {"instruction": "def _construct_nested_stack(self):\n        \"\"\"Constructs a AWS::CloudFormation::Stack resource\n        \"\"\"\n", "input": "", "output": "        nested_stack = NestedStack(self.logical_id, depends_on=self.depends_on,\n                                   attributes=self.get_passthrough_resource_attributes())\n        nested_stack.Parameters = self.Parameters\n        nested_stack.NotificationArns = self.NotificationArns\n        application_tags = self._get_application_tags()\n        nested_stack.Tags = self._construct_tag_list(self.Tags, application_tags)\n        nested_stack.TimeoutInMinutes = self.TimeoutInMinutes\n        nested_stack.TemplateURL = self.TemplateUrl if self.TemplateUrl else \"\"\n\n        return nested_stack", "category": "Python"}, {"instruction": "def createLocationEncoder(t, w=15):\n  \"\"\"\n  A default coordinate encoder for encoding locations into sparse\n  distributed representations.\n  \"\"\"\n", "input": "", "output": "  encoder = CoordinateEncoder(name=\"positionEncoder\", n=t.l6CellCount, w=w)\n  return encoder", "category": "Python"}, {"instruction": "def dataframe(self):\n        \"\"\"\n        Returns a pandas DataFrame where each row is a representation of the\n        Game class. Rows are indexed by the boxscore string.\n        \"\"\"\n", "input": "", "output": "        frames = []\n        for game in self.__iter__():\n            df = game.dataframe\n            if df is not None:\n                frames.append(df)\n        if frames == []:\n            return None\n        return pd.concat(frames)", "category": "Python"}, {"instruction": "def store_field(self, state, field_name, field_type, value):\n        \"\"\"\n        Store a field of a given object, without resolving hierachy\n\n        :param state: angr state where we want to allocate the object attribute\n        :type SimState\n        :param field_name: name of the attribute\n        :type str\n        :param field_value: attibute's value\n        :type SimSootValue\n        \"\"\"\n", "input": "", "output": "        field_ref = SimSootValue_InstanceFieldRef(self.heap_alloc_id, self.type, field_name, field_type)\n        state.memory.store(field_ref, value)", "category": "Python"}, {"instruction": "def dragEnterEvent(self, event):\r\n        \"\"\"Override Qt method\"\"\"\n", "input": "", "output": "        mimeData = event.mimeData()\r\n        formats = list(mimeData.formats())\r\n\r\n        if \"parent-id\" in formats and \\\r\n          int(mimeData.data(\"parent-id\")) == id(self.ancestor):\r\n            event.acceptProposedAction()\r\n\r\n        QTabBar.dragEnterEvent(self, event)", "category": "Python"}, {"instruction": "def get_distance(a, b, xaxis=True):\n    \"\"\"\n    Returns the distance between two blast HSPs.\n    \"\"\"\n", "input": "", "output": "    if xaxis:\n        arange = (\"0\", a.qstart, a.qstop, a.orientation)  # 0 is the dummy chromosome\n        brange = (\"0\", b.qstart, b.qstop, b.orientation)\n    else:\n        arange = (\"0\", a.sstart, a.sstop, a.orientation)\n        brange = (\"0\", b.sstart, b.sstop, b.orientation)\n\n    dist, oo = range_distance(arange, brange, distmode=\"ee\")\n    dist = abs(dist)\n\n    return dist", "category": "Python"}, {"instruction": "def _fetch_data(self):\n        \"\"\"Converts inputspec to files\"\"\"\n", "input": "", "output": "        if (self.inputs.surface_target == \"fsnative\" or\n                self.inputs.volume_target != \"MNI152NLin2009cAsym\"):\n            # subject space is not support yet\n            raise NotImplementedError\n\n        annotation_files = sorted(glob(os.path.join(self.inputs.subjects_dir,\n                                                    self.inputs.surface_target,\n                                                    'label',\n                                                    '*h.aparc.annot')))\n        if not annotation_files:\n            raise IOError(\"Freesurfer annotations for %s not found in %s\" % (\n                          self.inputs.surface_target, self.inputs.subjects_dir))\n\n        label_file = str(get_template(\n            'MNI152NLin2009cAsym', resolution=2, desc='DKT31', suffix='dseg'))\n        return annotation_files, label_file", "category": "Python"}, {"instruction": "def predicate(self, name, func=None):\n        \"\"\"Define a new predicate (directly, or as a decorator).\n\n        E.g.::\n\n            @authz.predicate('ROOT')\n            def is_root(user, **ctx):\n                # return True of user is in group \"wheel\".\n\n        \"\"\"\n", "input": "", "output": "        if func is None:\n            return functools.partial(self.predicate, name)\n        self.predicates[name] = func\n        return func", "category": "Python"}, {"instruction": "def mapper_python(module, entry_point, globber='root', fext=JS_EXT):\n    \"\"\"\n    Default mapper using python style globber\n\n    Finds the latest path declared for the module at hand and extract\n    a list of importable JS modules using the es6 module import format.\n    \"\"\"\n", "input": "", "output": "\n    return mapper(\n        module, entry_point=entry_point, modpath='pkg_resources',\n        globber=globber, modname='python', fext=fext)", "category": "Python"}, {"instruction": "def get_serializer_class(configuration_model):\n    \"\"\" Returns a ConfigurationModel serializer class for the supplied configuration_model. \"\"\"\n", "input": "", "output": "    class AutoConfigModelSerializer(ModelSerializer):\n        ", "category": "Python"}, {"instruction": "def delete_customer(self, customer_id):\n        \"\"\"Deletes an existing customer.\"\"\"\n", "input": "", "output": "        request = self._delete(\"customers/\" + str(customer_id))\n        return self.responder(request)", "category": "Python"}, {"instruction": "def append_response_error_content(response, **kwargs):\n    \"\"\"\n    Provides a helper to act as callback function for the response event hook\n    and add a HTTP response error with reason message to ``response.reason``.\n    The ``response`` and ``**kwargs`` are necessary for this function to\n    properly operate as the callback.\n\n    :param response: HTTP response object\n    :param kwargs: HTTP request parameters\n    \"\"\"\n", "input": "", "output": "    if response.status_code >= 400:\n        try:\n            resp_dict = response_to_json_dict(response)\n            error = resp_dict.get('error', '')\n            reason = resp_dict.get('reason', '')\n            # Append to the existing response's reason\n            response.reason += ' {0} {1}'.format(error, reason)\n        except ValueError:\n            pass\n    return response", "category": "Python"}, {"instruction": "def fetch():\n    \"\"\"\n    Downloads the Peek & Graves (2010) dust map, placing it in\n    the data directory for :obj:`dustmap`.\n    \"\"\"\n", "input": "", "output": "    doi = '10.7910/DVN/VBSI4A'\n    \n    for component in ['dust', 'err']:\n        requirements = {'filename': 'PG_{}_4096_ngp.fits'.format(component)}\n        local_fname = os.path.join(\n            data_dir(),\n            'pg2010', 'PG_{}_4096_ngp.fits'.format(component))\n        print('Downloading P&G (2010) {} data file to {}'.format(\n            component, local_fname))\n        fetch_utils.dataverse_download_doi(\n            doi,\n            local_fname,\n            file_requirements=requirements)", "category": "Python"}, {"instruction": "def init_debug(self):\n        \"\"\"Initialize debugging features, such as a handler for USR2 to print a trace\"\"\"\n", "input": "", "output": "        import signal\n\n        def debug_trace(sig, frame):\n            ", "category": "Python"}, {"instruction": "def readf(prompt, default=None, minval=None, maxval=None,\n          allowed_single_chars=None, question_mark=True):\n    \"\"\"Return integer value read from keyboard\n\n    Parameters\n    ----------\n    prompt : str\n        Prompt string.\n    default : float or None\n        Default value.\n    minval :  float or None\n        Mininum allowed value.\n    maxval :  float or None\n        Maximum allowed value.\n    allowed_single_chars : str\n        String containing allowed valid characters.\n    question_mark : bool\n        If True, display question mark after prompt.\n\n    Returns\n    -------\n    result : float\n        Read value.\n\n    \"\"\"\n", "input": "", "output": "\n    return read_value(ftype=float,\n                      prompt=prompt,\n                      default=default,\n                      minval=minval,\n                      maxval=maxval,\n                      allowed_single_chars=allowed_single_chars,\n                      question_mark=question_mark)", "category": "Python"}, {"instruction": "def default_match_error_handler(exc):\n    \"\"\"\n    Default implementation for match error handling.\n    \"\"\"\n", "input": "", "output": "    if isinstance(exc, NotFound):\n        return http.NotFound()\n    elif isinstance(exc, MethodNotAllowed):\n        return http.MethodNotAllowed()\n    elif isinstance(exc, RequestRedirect):\n        return redirect(exc.new_url)\n    else:\n        raise exc", "category": "Python"}, {"instruction": "def addLimitedLogHandler(func):\n    \"\"\"\n    Add a custom log handler.\n\n    @param func: a function object with prototype (level, object, category,\n                 message) where level is either ERROR, WARN, INFO, DEBUG, or\n                 LOG, and the rest of the arguments are strings or None. Use\n                 getLevelName(level) to get a printable name for the log level.\n    @type func:  a callable function\n\n    @raises TypeError: TypeError if func is not a callable\n    \"\"\"\n", "input": "", "output": "    if not callable(func):\n        raise TypeError(\"func must be callable\")\n\n    if func not in _log_handlers_limited:\n        _log_handlers_limited.append(func)", "category": "Python"}, {"instruction": "def ui(root_url, path):\n    \"\"\"\n    Generate URL for a path in the Taskcluster ui.\n    The purpose of the function is to switch on rootUrl:\n    \"The driver for having a ui method is so we can just call ui with a path and any root url,\n    and the returned url should work for both our current deployment (with root URL = https://taskcluster.net)\n    and any future deployment. The returned value is essentially rootURL == 'https://taskcluster.net'\n    'https://tools.taskcluster.net/${path}'\n    '${rootURL}/${path}' \"\n    \"\"\"\n", "input": "", "output": "    root_url = root_url.rstrip('/')\n    path = path.lstrip('/')\n    if root_url == OLD_ROOT_URL:\n        return 'https://tools.taskcluster.net/{}'.format(path)\n    else:\n        return '{}/{}'.format(root_url, path)", "category": "Python"}, {"instruction": "def add_port_profile(self, profile_name, vlan_id, device_id):\n        \"\"\"Adds a port profile and its vlan_id to the table.\"\"\"\n", "input": "", "output": "        if not self.get_port_profile_for_vlan(vlan_id, device_id):\n            port_profile = ucsm_model.PortProfile(profile_id=profile_name,\n                                                  vlan_id=vlan_id,\n                                                  device_id=device_id,\n                                                  created_on_ucs=False)\n            with self.session.begin(subtransactions=True):\n                self.session.add(port_profile)\n            return port_profile", "category": "Python"}, {"instruction": "def make_dict_hash(o):\n    \"\"\"Make a hash from a dictionary, list, tuple or set to any level, containing\n    only other hashable types (including any lists, tuples, sets, and dictionaries).\n    \"\"\"\n", "input": "", "output": "    if isinstance(o, (set, tuple, list)):\n        return tuple([make_dict_hash(e) for e in o])\n    elif not isinstance(o, dict):\n        return hash(o)\n\n    new_o = copy.deepcopy(o)\n    for k, v in new_o.items():\n        new_o[k] = make_dict_hash(v)\n\n    return hash(tuple(frozenset(sorted(new_o.items()))))", "category": "Python"}, {"instruction": "def _build_tree(self, actor, content):\n        \"\"\"\n        Builds the tree for the given actor.\n\n        :param actor: the actor to process\n        :type actor: Actor\n        :param content: the rows of the tree collected so far\n        :type content: list\n        \"\"\"\n", "input": "", "output": "        depth = actor.depth\n        row = \"\"\n        for i in xrange(depth - 1):\n            row += \"| \"\n        if depth > 0:\n            row += \"|-\"\n        name = actor.name\n        if name != actor.__class__.__name__:\n            name = actor.__class__.__name__ + \" '\" + name + \"'\"\n        row += name\n        quickinfo = actor.quickinfo\n        if quickinfo is not None:\n            row += \" [\" + quickinfo + \"]\"\n        content.append(row)\n\n        if isinstance(actor, ActorHandler):\n            for sub in actor.actors:\n                self._build_tree(sub, content)", "category": "Python"}, {"instruction": "def _get_result(self) -> float:\n        \"\"\"Return current measurement result in lx.\"\"\"\n", "input": "", "output": "        try:\n            data = self._bus.read_word_data(self._i2c_add, self._mode)\n            self._ok = True\n        except OSError as exc:\n            self.log_error(\"Bad reading in bus: %s\", exc)\n            self._ok = False\n            return -1\n\n        count = data >> 8 | (data & 0xff) << 8\n        mode2coeff = 2 if self._high_res else 1\n        ratio = 1 / (1.2 * (self._mtreg / 69.0) * mode2coeff)\n        return ratio * count", "category": "Python"}, {"instruction": "def __execute_undef(self, instr):\n        \"\"\"Execute UNDEF instruction.\n        \"\"\"\n", "input": "", "output": "        op2_val = random.randint(0, instr.operands[2].size)\n\n        self.write_operand(instr.operands[2], op2_val)\n\n        return None", "category": "Python"}, {"instruction": "def DbGetClassAttributeProperty2(self, argin):\n        \"\"\" This command supports array property compared to the old command called\n        DbGetClassAttributeProperty. The old command has not been deleted from the\n        server for compatibility reasons.\n\n        :param argin: Str[0] = Tango class name\n        Str[1] = Attribute name\n        Str[n] = Attribute name\n        :type: tango.DevVarStringArray\n        :return: Str[0] = Tango class name\n        Str[1] = Attribute property  number\n        Str[2] = Attribute property 1 name\n        Str[3] = Attribute property 1 value number (array case)\n        Str[4] = Attribute property 1 value\n        Str[n] = Attribute property 1 value (array case)\n        Str[n + 1] = Attribute property 2 name\n        Str[n + 2] = Attribute property 2 value number (array case)\n        Str[n + 3] = Attribute property 2 value\n        Str[n + m] = Attribute property 2 value (array case)\n        :rtype: tango.DevVarStringArray \"\"\"\n", "input": "", "output": "        self._log.debug(\"In DbGetClassAttributeProperty2()\")\n        class_name = argin[0]\n        return self.db.get_class_attribute_property2(class_name, argin[1:])", "category": "Python"}, {"instruction": "def _setup_tf(self, stream=False):\n        \"\"\"\n        Setup terraform; either 'remote config' or 'init' depending on version.\n        \"\"\"\n", "input": "", "output": "        if self.tf_version < (0, 9, 0):\n            self._set_remote(stream=stream)\n            return\n        self._run_tf('init', stream=stream)\n        logger.info('Terraform initialized')", "category": "Python"}, {"instruction": "def get_token(self, user_id, password, redirect_uri,\n                  scope='/activities/update'):\n        \"\"\"Get the token.\n\n        Parameters\n        ----------\n        :param user_id: string\n\n            The id of the user used for authentication.\n        :param password: string\n            The user password.\n        :param redirect_uri: string\n            The redirect uri of the institution.\n        :param scope: string\n            The desired scope. For example '/activities/update',\n            '/read-limited', etc.\n\n        Returns\n        -------\n        :returns: string\n            The token.\n        \"\"\"\n", "input": "", "output": "        return super(MemberAPI, self).get_token(user_id, password,\n                                                redirect_uri, scope)", "category": "Python"}, {"instruction": "def serialize_number(x, fmt=SER_BINARY, outlen=None):\n    \"\"\" Serializes `x' to a string of length `outlen' in format `fmt' \"\"\"\n", "input": "", "output": "    ret = b''\n    if fmt == SER_BINARY:\n        while x:\n            x, r = divmod(x, 256)\n            ret = six.int2byte(int(r)) + ret\n        if outlen is not None:\n            assert len(ret) <= outlen\n            ret = ret.rjust(outlen, b'\\0')\n        return ret\n    assert fmt == SER_COMPACT\n    while x:\n        x, r = divmod(x, len(COMPACT_DIGITS))\n        ret = COMPACT_DIGITS[r:r + 1] + ret\n    if outlen is not None:\n        assert len(ret) <= outlen\n        ret = ret.rjust(outlen, COMPACT_DIGITS[0:1])\n    return ret", "category": "Python"}, {"instruction": "def vhost_remove(cls, name):\n        \"\"\" Delete a vhost in a webaccelerator \"\"\"\n", "input": "", "output": "        oper = cls.call('hosting.rproxy.vhost.delete', name)\n        cls.echo('Deleting your virtual host %s' % name)\n        cls.display_progress(oper)\n        cls.echo('Your virtual host have been removed')\n        return oper", "category": "Python"}, {"instruction": "def etm_register_write(self, register_index, value, delay=False):\n        \"\"\"Writes a value to an ETM register.\n\n        Args:\n          self (JLink): the ``JLink`` instance.\n          register_index (int): the register to write to.\n          value (int): the value to write to the register.\n          delay (bool): boolean specifying if the write should be buffered.\n\n        Returns:\n          ``None``\n        \"\"\"\n", "input": "", "output": "        self._dll.JLINKARM_ETM_WriteReg(int(register_index), int(value), int(delay))\n        return None", "category": "Python"}, {"instruction": "def _getgroup(string, depth):\n    \"\"\"\n    Get a group from the string, where group is a list of all the comma\n    separated substrings up to the next '}' char or the brace enclosed substring\n    if there is no comma\n    \"\"\"\n", "input": "", "output": "    out, comma = [], False\n    while string:\n        items, string = _getitem(string, depth)\n\n        if not string:\n            break\n        out += items\n\n        if string[0] == '}':\n            if comma:\n                return out, string[1:]\n            return ['{' + a + '}' for a in out], string[1:]\n\n        if string[0] == ',':\n            comma, string = True, string[1:]\n\n    return None", "category": "Python"}, {"instruction": "def build_profile_dict(basedir, profile_name):\n    \"\"\"Get the name and source dictionary for the test source.\n    \n    Parameters\n    ----------\n    \n    basedir : str\n        Path to the analysis directory\n        \n    profile_name : str\n        Key for the spatial from of the target\n\n    Returns\n    -------\n    \n    profile_name : str\n        Name of for this particular profile\n\n    src_name : str\n        Name of the source for this particular profile\n    \n    profile_dict : dict\n        Dictionary with the source parameters\n\n    \"\"\"\n", "input": "", "output": "    profile_path = os.path.join(basedir, \"profile_%s.yaml\" % profile_name)\n    profile_config = load_yaml(profile_path)\n    src_name = profile_config['name']\n    profile_dict = profile_config['source_model']\n    return profile_name, src_name, profile_dict", "category": "Python"}, {"instruction": "def set_remote(self, key=None, value=None, data=None, **kwdata):\n        \"\"\"\n        Set data for the remote end(s) of this conversation.\n\n        Data can be passed in either as a single dict, or as key-word args.\n\n        Note that, in Juju, setting relation data is inherently service scoped.\n        That is, if the conversation only includes a single unit, the data will\n        still be set for that unit's entire service.\n\n        However, if this conversation's scope encompasses multiple services,\n        the data will be set for all of those services.\n\n        :param str key: The name of a field to set.\n        :param value: A value to set. This value must be json serializable.\n        :param dict data: A mapping of keys to values.\n        :param **kwdata: A mapping of keys to values, as keyword arguments.\n        \"\"\"\n", "input": "", "output": "        if data is None:\n            data = {}\n        if key is not None:\n            data[key] = value\n        data.update(kwdata)\n        if not data:\n            return\n        for relation_id in self.relation_ids:\n            hookenv.relation_set(relation_id, data)", "category": "Python"}, {"instruction": "def contact_get_common_groups(self, contact_id):\n        \"\"\"\n        Returns groups common between a user and the contact with given id.\n\n        :return: Contact or Error\n        :rtype: Contact\n        \"\"\"\n", "input": "", "output": "        for group in self.wapi_functions.getCommonGroups(contact_id):\n            yield factory_chat(group, self)", "category": "Python"}, {"instruction": "def merge_x_y(collection_x, collection_y, filter_none=False):\n    \"\"\"\n    Merge two lists, creating a dictionary with key `label` and a set x and y\n    \"\"\"\n", "input": "", "output": "    data = {}\n    for item in collection_x:\n        #print item[0:-1]\n        #print item[-1]\n        label = datetimeutil.tuple_to_string(item[0:-1])\n        if filter_none and label == 'None-None':\n            continue\n        data[label] = {'label': label, 'x': item[-1], 'y': 0}\n    for item in collection_y:\n        #print item\n        label = datetimeutil.tuple_to_string(item[0:-1])\n        if filter_none and label == 'None-None':\n            continue\n        try:\n            data[label]['y'] = item[-1]\n        except KeyError:\n            data[label] = {'label': label, 'x': 0, 'y': item[-1]}\n\n    # Keys are not sorted\n    return data", "category": "Python"}, {"instruction": "def idgen(idbase, tint=None, bits=64):\n    '''\n    Generate an IRI as a hash of given information, or just make one up if None given\n    idbase -- Base URI for generating links\n    tint -- String that affects the sequence of IDs generated if sent None\n\n    >>> from bibframe.contrib.datachefids import idgen\n    >>> g = idgen(None)\n    >>> next(g) #Or g.send(None)\n    'gKNG1b7eySo'\n    >>> next(g)\n    'cXx7iv67-3E'\n    >>> g.send('spam')\n    'OZxOEos8e-k'\n    >>> next(g)\n    'mCFhsaWQ1_0'\n    >>> g.send('spam')\n    'OZxOEos8e-k'\n    >>> g.send('eggs')\n    'xQAd4Guk040'\n    >>> g.send('')\n    'AAAAAAAAAAA'\n    '''\n", "input": "", "output": "    counter = -1\n    to_hash = None\n    while True:\n        if to_hash is None:\n            to_hash = str(counter)\n            if tint: to_hash += tint\n        to_hash = simple_hashstring(to_hash, bits=bits)\n        to_hash = yield iri.absolutize(to_hash, idbase) if idbase else to_hash\n        counter += 1", "category": "Python"}, {"instruction": "def merge(self, frame):\n        \"\"\"\n        Add another DataFrame to the PStatCounter.\n        \"\"\"\n", "input": "", "output": "        for column, values in frame.iteritems():\n            # Temporary hack, fix later\n            counter = self._counters.get(column)\n            for value in values:\n                if counter is not None:\n                    counter.merge(value)", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"\n        start component\n        \"\"\"\n", "input": "", "output": "        loop = asyncio.get_event_loop()\n        if loop.is_closed():\n            asyncio.set_event_loop(asyncio.new_event_loop())\n            loop = asyncio.get_event_loop()\n\n        txaio.start_logging()\n        loop.run_until_complete(self.onConnect())", "category": "Python"}, {"instruction": "def bitrate(self):\n        \"\"\"Bitrate of the raw aac blocks, excluding framing/crc\"\"\"\n", "input": "", "output": "\n        assert self.parsed_frames, \"no frame parsed yet\"\n\n        if self._samples == 0:\n            return 0\n\n        return (8 * self._payload * self.frequency) // self._samples", "category": "Python"}, {"instruction": "def _get_foundation(self, i):\n        \"\"\"Return a :class:`Foundation` for some deck, creating it if\n        needed.\n\n        \"\"\"\n", "input": "", "output": "        if i >= len(self._foundations) or self._foundations[i] is None:\n            oldfound = list(self._foundations)\n            extend = i - len(oldfound) + 1\n            if extend > 0:\n                oldfound += [None] * extend\n            width = self.card_size_hint_x * self.width\n            height = self.card_size_hint_y * self.height\n            found = Foundation(\n                pos=self._get_foundation_pos(i), size=(width, height), deck=i\n            )\n            self.bind(\n                pos=found.upd_pos,\n                card_size_hint=found.upd_pos,\n                deck_hint_step=found.upd_pos,\n                size=found.upd_pos,\n                deck_x_hint_offsets=found.upd_pos,\n                deck_y_hint_offsets=found.upd_pos\n            )\n            self.bind(\n                size=found.upd_size,\n                card_size_hint=found.upd_size\n            )\n            oldfound[i] = found\n            self._foundations = oldfound\n        return self._foundations[i]", "category": "Python"}, {"instruction": "def _make_resource_result(f, futmap):\n        \"\"\"\n        Map per-resource results to per-resource futures in futmap.\n        The result value of each (successful) future is a ConfigResource.\n        \"\"\"\n", "input": "", "output": "        try:\n            result = f.result()\n            for resource, configs in result.items():\n                fut = futmap.get(resource, None)\n                if fut is None:\n                    raise RuntimeError(\"Resource {} not found in future-map: {}\".format(resource, futmap))\n                if resource.error is not None:\n                    # Resource-level exception\n                    fut.set_exception(KafkaException(resource.error))\n                else:\n                    # Resource-level success\n                    # configs will be a dict for describe_configs()\n                    # and None for alter_configs()\n                    fut.set_result(configs)\n        except Exception as e:\n            # Request-level exception, raise the same for all resources\n            for resource, fut in futmap.items():\n                fut.set_exception(e)", "category": "Python"}, {"instruction": "def draw_nodes(self):\n        \"\"\"\n        Draw nodes to screen.\n        \"\"\"\n", "input": "", "output": "        node_r = self.node_sizes\n        for i, node in enumerate(self.nodes):\n            x = self.node_coords[\"x\"][i]\n            y = self.node_coords[\"y\"][i]\n            color = self.node_colors[i]\n            node_patch = patches.Ellipse(\n                (x, y), node_r[i], node_r[i], lw=0, color=color, zorder=2\n            )\n            self.ax.add_patch(node_patch)", "category": "Python"}, {"instruction": "def find_in_sequence(cls, field, order, *args, **kw):\n\t\t\"\"\"Return a QuerySet iterating the results of a query in a defined order. Technically an aggregate.\n\t\t\n\t\tTo be successful one must be running MongoDB 3.4 or later. Document order will not be represented otherwise.\n\t\t\n\t\tBased on the technique described here: http://s.webcore.io/2O3i0N2E3h0r\n\t\tSee also: https://jira.mongodb.org/browse/SERVER-7528\n\t\t\"\"\"\n", "input": "", "output": "\t\t\n\t\tfield = traverse(cls, field)\n\t\torder = list(order)  # We need to coalesce the value to prepare for multiple uses.\n\t\tkw['sort'] = {'__order': 1}\n\t\tkw.setdefault('projection', {'__order': 0})\n\t\t\n\t\tcls, collection, stages, options = cls._prepare_aggregate(\n\t\t\t\tfield.any(order),\n\t\t\t\t{'$addFields': {'__order': {'$indexOfArray': [order, '$' + ~field]}}},\n\t\t\t\t*args,\n\t\t\t\t**kw\n\t\t\t)\n\t\t\n\t\tif __debug__:  # noqa\n\t\t\t# This \"foot shot avoidance\" check requires a server round-trip, potentially, so we only do this in dev.\n\t\t\tif tuple(collection.database.client.server_info()['versionArray'][:2]) < (3, 4):  # pragma: no cover\n\t\t\t\traise RuntimeError(\"Queryable.find_in_sequence only works against MongoDB server versions 3.4 or newer.\")\n\t\t\n\t\treturn collection.aggregate(stages, **options)", "category": "Python"}, {"instruction": "def qaoa_ansatz(gammas, betas):\n    \"\"\"\n    Function that returns a QAOA ansatz program for a list of angles betas and gammas. len(betas) ==\n    len(gammas) == P for a QAOA program of order P.\n\n    :param list(float) gammas: Angles over which to parameterize the cost Hamiltonian.\n    :param list(float) betas: Angles over which to parameterize the driver Hamiltonian.\n    :return: The QAOA ansatz program.\n    :rtype: Program.\n    \"\"\"\n", "input": "", "output": "    return Program([exponentiate_commuting_pauli_sum(h_cost)(g)\n                    + exponentiate_commuting_pauli_sum(h_driver)(b)\n                    for g, b in zip(gammas, betas)])", "category": "Python"}, {"instruction": "def fetch_limb(self, diam_sun, base_shape=(1280, 1280)):\n        \"\"\"\n        reads the solar limb template file and adjust to the requested solar diameter\n        :param diam_sun: diameter of sun in suvi pixels\n        :param base_shape: the shape of a suvi image\n        :return: product name, None, scaled image\n        \"\"\"\n", "input": "", "output": "        from scipy.ndimage.interpolation import zoom\n\n        fn = pkg_resources.resource_filename(\"suvitrainer\", \"path_length_280_noisy.fits\")\n        calculated_diam = int(os.path.basename(fn).split(\"_\")[2]) * 2\n        with fits.open(fn) as hdus:\n            limb_unscaled = hdus[0].data\n        scale_factor = diam_sun / calculated_diam\n        limb_scaled = zoom(limb_unscaled, scale_factor)\n        shape = limb_scaled.shape[0]\n        excess = int((shape - base_shape[0]) / 2.0)\n        limb_scaled = limb_scaled[excess:shape - excess, excess:shape - excess]\n        limb_scaled = limb_scaled[0:base_shape[0], 0:base_shape[1]]\n        return \"limb\", None, limb_scaled", "category": "Python"}, {"instruction": "def stop(self):\r\n        \"\"\"Stops a a playing animation.  A subsequent call to play will start from the beginning.\"\"\"\n", "input": "", "output": "        if self.state == PygAnimation.PLAYING:\r\n            self.index = 0  # set up for first image in list\r\n            self.elapsed = 0\r\n            self.nIterationsLeft = 0\r\n\r\n        elif self.state == PygAnimation.STOPPED:\r\n            pass  # nothing to do\r\n\r\n        elif self.state == PygAnimation.PAUSED:\r\n            self.index = 0  # set up for first image in list\r\n            self.elapsed = 0\r\n\r\n        self.state = PygAnimation.STOPPED", "category": "Python"}, {"instruction": "def bool(name, default=None, allow_none=False, fallback=None):\n    \"\"\"Get a boolean based environment value or the default.\n\n    Args:\n        name: The environment variable name\n        default: The default value to use if no environment variable is found\n        allow_none: If the return value can be `None` (i.e. optional)\n    \"\"\"\n", "input": "", "output": "    value = read(name, default, allow_none, fallback=fallback)\n    if isinstance(value, builtins.bool):\n        return value\n    elif isinstance(value, builtins.int):\n        return True if value > 0 else False\n    elif value is None and allow_none:\n        return None\n    else:\n        value_str = builtins.str(value).lower().strip()\n        return _strtobool(value_str)", "category": "Python"}, {"instruction": "def clamp(value, maximum=None):\n        ''' Clamp numeric values to be non-negative, an optionally, less than a\n        given maximum.\n\n        Args:\n            value (float) :\n                A number to clamp.\n\n            maximum (float, optional) :\n                A max bound to to clamp to. If None, there is no upper bound,\n                and values are only clamped to be non-negative. (default: None)\n\n        Returns:\n            float\n\n        '''\n", "input": "", "output": "        value = max(value, 0)\n\n        if maximum is not None:\n            return min(value, maximum)\n        else:\n            return value", "category": "Python"}, {"instruction": "def relation_ids(reltype=None):\n    \"\"\"A list of relation_ids\"\"\"\n", "input": "", "output": "    reltype = reltype or relation_type()\n    relid_cmd_line = ['relation-ids', '--format=json']\n    if reltype is not None:\n        relid_cmd_line.append(reltype)\n        return json.loads(\n            subprocess.check_output(relid_cmd_line).decode('UTF-8')) or []\n    return []", "category": "Python"}, {"instruction": "def unicodeInScripts(uv, scripts):\n    \"\"\" Check UnicodeData's ScriptExtension property for unicode codepoint\n    'uv' and return True if it intersects with the set of 'scripts' provided,\n    False if it does not intersect.\n    Return None for 'Common' script ('Zyyy').\n    \"\"\"\n", "input": "", "output": "    sx = unicodedata.script_extension(unichr(uv))\n    if \"Zyyy\" in sx:\n        return None\n    return not sx.isdisjoint(scripts)", "category": "Python"}, {"instruction": "def require(self, perm_name, **kwargs):\n        \"\"\"Use as a decorator on a view to require a permission.\n\n        Optional args:\n\n            - ``field`` The name of the model field to use for lookup\n              (this is only relevant when requiring a permission that\n              was registered with ``model=SomeModelClass``)\n\n        Examples::\n\n            @registry.require('can_do_stuff')\n            def view(request):\n                ...\n\n            @registry.require('can_do_stuff_with_model', field='alt_id')\n            def view_model(request, model_id):\n                ...\n\n        \"\"\"\n", "input": "", "output": "        view_decorator = self._get_entry(perm_name).view_decorator\n        return view_decorator(**kwargs) if kwargs else view_decorator", "category": "Python"}, {"instruction": "def get_sigmasqs(self, instruments=None):\n\t\t\"\"\"\n\t\tReturn dictionary of single-detector sigmas for each row in the\n\t\ttable.\n\t\t\"\"\"\n", "input": "", "output": "\t\tif len(self):\n\t\t\tif not instruments:\n\t\t\t\tinstruments = map(str, \\\n\t\t\t\t\tinstrument_set_from_ifos(self[0].ifos))\n\t\t\treturn dict((ifo, self.get_sigmasq(ifo))\\\n\t\t\t\t    for ifo in instruments)\n\t\telse:\n\t\t\treturn dict()", "category": "Python"}, {"instruction": "def migrate(module=None):\n    \"\"\"\n    Entrypoint to migrate the schema.\n    For the moment only create tables.\n    \"\"\"\n", "input": "", "output": "    if not module:\n        #Parsing\n        parser = argparse.ArgumentParser()\n        parser.add_argument('-s', '--service', type=str, help='Migrate the module', required=True,\n                            dest='module')\n        args = parser.parse_args()\n        module = args.module\n    #1. Load settings\n    farine.settings.load()\n    #2. Load the module\n    models = farine.discovery.import_models(module)\n    #3. Get the connection\n    db = farine.connectors.sql.setup(getattr(farine.settings, module))\n    #4. Create tables\n    for model in models:\n        model._meta.database = db\n        model.create_table(fail_silently=True)", "category": "Python"}, {"instruction": "def parse_headers_link(headers):\n    \"\"\"Returns the parsed header links of the response, if any.\"\"\"\n", "input": "", "output": "\n    header = CaseInsensitiveDict(headers).get('link')\n\n    l = {}\n\n    if header:\n        links = parse_link(header)\n\n        for link in links:\n            key = link.get('rel') or link.get('url')\n            l[key] = link\n\n    return l", "category": "Python"}, {"instruction": "def remove_old(self, max_log_time):\n        \"\"\"Remove all logs which are older than the specified time.\"\"\"\n", "input": "", "output": "        files = glob.glob('{}/queue-*'.format(self.log_dir))\n        files = list(map(lambda x: os.path.basename(x), files))\n\n        for log_file in files:\n            # Get time stamp from filename\n            name = os.path.splitext(log_file)[0]\n            timestamp = name.split('-', maxsplit=1)[1]\n\n            # Get datetime from time stamp\n            time = datetime.strptime(timestamp, '%Y%m%d-%H%M')\n            now = datetime.now()\n\n            # Get total delta in seconds\n            delta = now - time\n            seconds = delta.total_seconds()\n\n            # Delete log file, if the delta is bigger than the specified log time\n            if seconds > int(max_log_time):\n                log_filePath = os.path.join(self.log_dir, log_file)\n                os.remove(log_filePath)", "category": "Python"}, {"instruction": "def _prep_datum(self, datum, dialect, col, needs_conversion):\n        \"\"\"Puts a value in proper format for a SQL string\"\"\"\n", "input": "", "output": "        if datum is None or (needs_conversion and not str(datum).strip()):\n            return 'NULL'\n        pytype = self.columns[col]['pytype']\n\n        if needs_conversion:\n            if pytype == datetime.datetime:\n                datum = dateutil.parser.parse(datum)\n            elif pytype == bool:\n                datum = th.coerce_to_specific(datum)\n                if dialect.startswith('sqlite'):\n                    datum = 1 if datum else 0\n            else:\n                datum = pytype(str(datum))\n\n        if isinstance(datum, datetime.datetime) or isinstance(datum, datetime.date):\n            if dialect in self._datetime_format:\n                return datum.strftime(self._datetime_format[dialect])\n            else:\n                return \"'%s'\" % datum\n        elif hasattr(datum, 'lower'):\n            # simple SQL injection protection, sort of... ?\n            return \"'%s'\" % datum.replace(\"'\", \"''\")\n        else:\n            return datum", "category": "Python"}, {"instruction": "def getvec(gh, lat, lon):\n    \"\"\"\n    Evaluates the vector at a given latitude and longitude for a specified\n    set of coefficients\n\n    Parameters\n    ----------\n    gh : a list of gauss coefficients\n    lat : latitude of location\n    long : longitude of location\n\n    Returns\n    -------\n    vec : direction in [dec, inc, intensity]\n    \"\"\"\n", "input": "", "output": "    sv = []\n    pad = 120 - len(gh)\n    for x in range(pad):\n        gh.append(0.)\n    for x in range(len(gh)):\n        sv.append(0.)\n#! convert to colatitude for MB routine\n    itype = 1\n    colat = 90. - lat\n    date, alt = 2000., 0.  # use a dummy date and altitude\n    x, y, z, f = magsyn(gh, sv, date, date, itype, alt, colat, lon)\n    vec = cart2dir([x, y, z])\n    vec[2] = f\n    return vec", "category": "Python"}, {"instruction": "def _apply_secure(self, func, permission=None):\n        '''Enforce authentication on a given method/verb'''\n", "input": "", "output": "        self._handle_api_doc(func, {'security': 'apikey'})\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if not current_user.is_authenticated:\n                self.abort(401)\n\n            if permission is not None:\n                with permission.require():\n                    return func(*args, **kwargs)\n            else:\n                return func(*args, **kwargs)\n\n        return wrapper", "category": "Python"}, {"instruction": "def multi_agent_example():\n    \"\"\"A basic example of using multiple agents\"\"\"\n", "input": "", "output": "    env = holodeck.make(\"UrbanCity\")\n\n    cmd0 = np.array([0, 0, -2, 10])\n    cmd1 = np.array([0, 0, 5, 10])\n    for i in range(10):\n        env.reset()\n        # This will queue up a new agent to spawn into the environment, given that the coordinates are not blocked.\n        sensors = [Sensors.PIXEL_CAMERA, Sensors.LOCATION_SENSOR, Sensors.VELOCITY_SENSOR]\n        agent = AgentDefinition(\"uav1\", agents.UavAgent, sensors)\n        env.spawn_agent(agent, [1, 1, 5])\n\n        env.set_control_scheme(\"uav0\", ControlSchemes.UAV_ROLL_PITCH_YAW_RATE_ALT)\n        env.set_control_scheme(\"uav1\", ControlSchemes.UAV_ROLL_PITCH_YAW_RATE_ALT)\n\n        env.tick()  # Tick the environment once so the second agent spawns before we try to interact with it.\n\n        env.act(\"uav0\", cmd0)\n        env.act(\"uav1\", cmd1)\n        for _ in range(1000):\n            states = env.tick()\n            uav0_terminal = states[\"uav0\"][Sensors.TERMINAL]\n            uav1_reward = states[\"uav1\"][Sensors.REWARD]", "category": "Python"}, {"instruction": "def convertbase(number, base=10):\n    \"\"\"\n    Convert a number in base 10 to another base\n\n    :type number: number\n    :param number: The number to convert\n\n    :type base: integer\n    :param base: The base to convert to.\n    \"\"\"\n", "input": "", "output": "\n    integer = number\n    if not integer:\n        return '0'\n    sign = 1 if integer > 0 else -1\n    alphanum = string.digits + string.ascii_lowercase\n    nums = alphanum[:base]\n    res = ''\n    integer *= sign\n    while integer:\n        integer, mod = divmod(integer, base)\n        res += nums[mod]\n    return ('' if sign == 1 else '-') + res[::-1]", "category": "Python"}, {"instruction": "def _load_tasks(self, project):\n        '''load tasks from database'''\n", "input": "", "output": "        task_queue = project.task_queue\n\n        for task in self.taskdb.load_tasks(\n                self.taskdb.ACTIVE, project.name, self.scheduler_task_fields\n        ):\n            taskid = task['taskid']\n            _schedule = task.get('schedule', self.default_schedule)\n            priority = _schedule.get('priority', self.default_schedule['priority'])\n            exetime = _schedule.get('exetime', self.default_schedule['exetime'])\n            task_queue.put(taskid, priority, exetime)\n        project.task_loaded = True\n        logger.debug('project: %s loaded %d tasks.', project.name, len(task_queue))\n\n        if project not in self._cnt['all']:\n            self._update_project_cnt(project.name)\n        self._cnt['all'].value((project.name, 'pending'), len(project.task_queue))", "category": "Python"}, {"instruction": "def exceptions(self):\n        \"\"\"\n        Returns a list of ParamDoc objects (with empty names) of the\n        exception tags for the function.\n\n        >>> comments = parse_comments_for_file('examples/module_closure.js')\n        >>> fn1 = FunctionDoc(comments[1])\n        >>> fn1.exceptions[0].doc\n        'Another exception'\n        >>> fn1.exceptions[1].doc\n        'A fake exception'\n        >>> fn1.exceptions[1].type\n        'String'\n\n        \"\"\"\n", "input": "", "output": "        def make_param(text):\n            if '{' in text and '}' in text:\n                # Make sure param name is blank:\n                word_split = list(split_delimited('{}', ' ', text))\n                if word_split[1] != '':\n                    text = ' '.join([word_split[0], ''] + word_split[1:])\n            else:\n                # Handle old JSDoc format\n                word_split = text.split()\n                text = '{%s}  %s' % (word_split[0], ' '.join(word_split[1:]))\n            return ParamDoc(text)\n        return [make_param(text) for text in \n                self.get_as_list('throws') + self.get_as_list('exception')]", "category": "Python"}, {"instruction": "def _getcosmoheader(cosmo):\n    \"\"\" Output the cosmology to a string for writing to file \"\"\"\n", "input": "", "output": "\n    cosmoheader = (\"# Cosmology (flat) Om:{0:.3f}, Ol:{1:.3f}, h:{2:.2f}, \"\n                   \"sigma8:{3:.3f}, ns:{4:.2f}\".format(\n                       cosmo['omega_M_0'], cosmo['omega_lambda_0'], cosmo['h'],\n                       cosmo['sigma_8'], cosmo['n']))\n\n    return(cosmoheader)", "category": "Python"}, {"instruction": "def from_key(cls, *args):\n        \"\"\"\n        Return flyweight object with specified key, if it has already been created.\n\n        Returns:\n            cls or None: Previously constructed flyweight object with given\n            key or None if key not found\n        \"\"\"\n", "input": "", "output": "        key = args if len(args) > 1 else args[0]\n        return cls._instances.get(key, None)", "category": "Python"}, {"instruction": "def dump_to_stream(self, cnf, stream, **kwargs):\n        \"\"\"\n        Dump config 'cnf' to a file or file-like object 'stream'.\n\n        :param cnf: Shell variables data to dump\n        :param stream: Shell script file or file like object\n        :param kwargs: backend-specific optional keyword parameters :: dict\n        \"\"\"\n", "input": "", "output": "        for key, val in anyconfig.compat.iteritems(cnf):\n            stream.write(\"%s='%s'%s\" % (key, val, os.linesep))", "category": "Python"}, {"instruction": "def substitute(arg, value, replacement=None, else_=None):\n    \"\"\"\n    Substitute (replace) one or more values in a value expression\n\n    Parameters\n    ----------\n    value : expr-like or dict\n    replacement : expr-like, optional\n      If an expression is passed to value, this must be passed\n    else_ : expr, optional\n\n    Returns\n    -------\n    replaced : case statement (for now!)\n\n    \"\"\"\n", "input": "", "output": "    expr = arg.case()\n    if isinstance(value, dict):\n        for k, v in sorted(value.items()):\n            expr = expr.when(k, v)\n    else:\n        expr = expr.when(value, replacement)\n\n    if else_ is not None:\n        expr = expr.else_(else_)\n    else:\n        expr = expr.else_(arg)\n\n    return expr.end()", "category": "Python"}, {"instruction": "def load_trajectory(self,trajectory):\n        \"\"\"\n        Loads the trajectory files e.g. XTC, DCD, TRJ together with the topology\n        file as a MDAnalysis Universe. This will only be run if a trajectory has\n        been submitted for analysis.\n            Takes:\n                * topology * - a topology file e.g. GRO, PDB, INPCRD, CARD, DMS\n                * trajectory * - a trajectory file e.g. XTC, DCD, TRJ\n            Output:\n                * self.universe * - an MDAnalysis Universe consisting from the\n                topology and trajectory file.\n        \"\"\"\n", "input": "", "output": "\n        try:\n            self.universe.load_new(trajectory)\n        except IOError, ValueError:\n            print \"Check you trajectory file \" + trajectory +\"- it might be missing or misspelled.\"", "category": "Python"}, {"instruction": "def p_InUseTraitDefList(p):\n    '''\n    InUseTraitDefList : InUseTraitDef\n                      | InUseTraitDefList InUseTraitDef\n    '''\n", "input": "", "output": "    if len(p) <= 2:\n        p[0] = InUseTraitDefList(None, p[1])\n    else:\n        p[0] = InUseTraitDefList(p[1], p[2])", "category": "Python"}, {"instruction": "def register(self, prefix, viewset, base_name=None, router_class=None):\n        \"\"\"\n        Append the given viewset to the proper registry.\n        \"\"\"\n", "input": "", "output": "        if base_name is None:\n            base_name = self.get_default_base_name(viewset)\n\n        if router_class is not None:\n            kwargs = {'trailing_slash': bool(self.trailing_slash)}\n            single_object_router_classes = (\n                AuthenticationRouter, SingleObjectRouter)\n\n            if issubclass(router_class, single_object_router_classes):\n                router = router_class(**kwargs)\n                router.register(prefix, viewset, base_name=base_name)\n                self._single_object_registry.append(router)\n\n        else:\n            self.registry.append((prefix, viewset, base_name))", "category": "Python"}, {"instruction": "def _check(cls, name, val, can_be_zero=False, val_type=float):\n    \"\"\"Check init arguments.\n\n    Args:\n      name: name of the argument. For logging purpose.\n      val: value. Value has to be non negative number.\n      can_be_zero: whether value can be zero.\n      val_type: Python type of the value.\n\n    Returns:\n      The value.\n\n    Raises:\n      ValueError: when invalid value is passed in.\n      TypeError: when invalid value type is passed in.\n    \"\"\"\n", "input": "", "output": "    valid_types = [val_type]\n    if val_type is float:\n      valid_types.append(int)\n\n    if type(val) not in valid_types:\n      raise TypeError(\n          'Expect type %s for parameter %s' % (val_type.__name__, name))\n    if val < 0:\n      raise ValueError(\n          'Value for parameter %s has to be greater than 0' % name)\n    if not can_be_zero and val == 0:\n      raise ValueError(\n          'Value for parameter %s can not be 0' % name)\n    return val", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"main control loop for thread\"\"\"\n", "input": "", "output": "        while True:\n            try:\n                cursor = JSON_CLIENT.json_client['local']['oplog.rs'].find(\n                    {'ts': {'$gt': self.last_timestamp}})\n            except TypeError:\n                # filesystem, so .json_client is a bool and not iterable\n                pass\n            else:\n                # http://stackoverflow.com/questions/30401063/pymongo-tailing-oplog\n                cursor.add_option(2)  # tailable\n                cursor.add_option(8)  # oplog_replay\n                cursor.add_option(32)  # await data\n                self._retry()\n                for doc in cursor:\n                    self.last_timestamp = doc['ts']\n                    if doc['ns'] in self.receivers:\n                        self._run_namespace(doc)\n            time.sleep(1)", "category": "Python"}, {"instruction": "def apply_transform(self, matrix):\n        \"\"\"\n        Apply a transform to the sphere primitive\n\n        Parameters\n        ------------\n        matrix: (4,4) float, homogenous transformation\n        \"\"\"\n", "input": "", "output": "        matrix = np.asanyarray(matrix, dtype=np.float64)\n        if matrix.shape != (4, 4):\n            raise ValueError('shape must be 4,4')\n\n        center = np.dot(matrix,\n                        np.append(self.primitive.center, 1.0))[:3]\n        self.primitive.center = center", "category": "Python"}, {"instruction": "def add_tracks(self, track):\n        \"\"\"\n        Add a track or iterable of tracks.\n\n        Parameters\n        ----------\n\n        track : iterable or Track\n            Iterable of :class:`Track` objects, or a single :class:`Track`\n            object.\n        \"\"\"\n", "input": "", "output": "        from trackhub import BaseTrack\n        if isinstance(track, BaseTrack):\n            self.add_child(track)\n            self._tracks.append(track)\n        else:\n            for t in track:\n                self.add_child(t)\n                self._tracks.append(t)", "category": "Python"}, {"instruction": "def get_pid(self, uri):\n        \"\"\"\n        Get the pid for the property in this wikibase instance ( the one at `sparql_endpoint_url` ),\n         that corresponds to (i.e. has the equivalent property) `uri`\n        \"\"\"\n", "input": "", "output": "        # if the wikibase is wikidata, and we give a wikidata uri or a PID with no URI specified:\n        if (self.sparql_endpoint_url == 'https://query.wikidata.org/sparql' and\n                (uri.startswith(\"P\") or uri.startswith(\"http://www.wikidata.org/entity/\"))):\n            # don't look up anything, just return the same value\n            return uri\n        if uri.startswith(\"P\"):\n            uri = \"http://www.wikidata.org/entity/\" + uri\n        return self.URI_PID[uri]", "category": "Python"}, {"instruction": "def get_group_id(self, uuid=None):\n        \"\"\"Get group id based on uuid.\n\n        Args:\n            uuid (str): optional uuid. defaults to self.cuuid\n\n        Raises:\n            PyLmodUnexpectedData: No group data was returned.\n            requests.RequestException: Exception connection error\n\n        Returns:\n            int: numeric group id\n\n        \"\"\"\n", "input": "", "output": "        group_data = self.get_group(uuid)\n        try:\n            return group_data['response']['docs'][0]['id']\n        except (KeyError, IndexError):\n            failure_message = ('Error in get_group response data - '\n                               'got {0}'.format(group_data))\n            log.exception(failure_message)\n            raise PyLmodUnexpectedData(failure_message)", "category": "Python"}, {"instruction": "def json_response(self, data, code=200, indent=2, headers={}):\n        \"\"\"Useful for restful APIs\n        :param data: a native python object, must be json serialiable, otherwise will be handled by :py:meth:`Application.json_handle_weird`\n        :param code: the http status code\n        :param indent: how many characters to indent when generating the json\n        :param headers: a dict with optional headers\n        :returns: a :py:class:`flask.Response` with the option of prettifying\n        the json response\n        \"\"\"\n", "input": "", "output": "        headers = headers.copy()\n        headers['Content-Type'] = 'application/json'\n        payload = json.dumps(data, indent=indent, default=self.json_handle_weird)\n        return Response(payload, status=code, headers=headers)", "category": "Python"}, {"instruction": "def add_arc(self, src, dst, char):\n        \"\"\"\n        This function adds a new arc in a SFA state\n        Args:\n            src (int): The source state identifier\n            dst (int): The destination state identifier\n            char (str): The transition symbol\n        Returns:\n            None\n        \"\"\"\n", "input": "", "output": "        assert type(src) == type(int()) and type(dst) == type(int()), \\\n            \"State type should be integer.\"\n        while src >= len(self.states) or dst >= len(self.states):\n            self.add_state()\n        self.states[src].arcs.append(SFAArc(src, dst, char))", "category": "Python"}, {"instruction": "def is_valid_pep484_type_hint(typ_hint, allow_forward_refs: bool = False):\n    \"\"\"\n    Returns True if the provided type is a valid PEP484 type hint, False otherwise.\n\n    Note: string type hints (forward references) are not supported by default, since callers of this function in\n    parsyfiles lib actually require them to be resolved already.\n\n    :param typ_hint:\n    :param allow_forward_refs:\n    :return:\n    \"\"\"\n", "input": "", "output": "    # most common case first, to be faster\n    try:\n        if isinstance(typ_hint, type):\n            return True\n    except:\n        pass\n\n    # optionally, check forward reference\n    try:\n        if allow_forward_refs and is_forward_ref(typ_hint):\n            return True\n    except:\n        pass\n\n    # finally check unions and typevars\n    try:\n        return is_union_type(typ_hint) or is_typevar(typ_hint)\n    except:\n        return False", "category": "Python"}, {"instruction": "def make_directory(self, directory_name, *args, **kwargs):\n\t\t\"\"\" :meth:`.WNetworkClientProto.make_directory` method implementation\n\t\t\"\"\"\n", "input": "", "output": "\t\tself.dav_client().mkdir(self.join_path(self.session_path(), directory_name))", "category": "Python"}, {"instruction": "def server_hardware(self):\n        \"\"\"\n        Gets the ServerHardware API client.\n\n        Returns:\n            ServerHardware:\n        \"\"\"\n", "input": "", "output": "        if not self.__server_hardware:\n            self.__server_hardware = ServerHardware(self.__connection)\n        return self.__server_hardware", "category": "Python"}, {"instruction": "def raise_exception(self, exception):\n        \"\"\"\n        Raises an exception in wait()\n\n        :param exception: An Exception object\n        \"\"\"\n", "input": "", "output": "        self.__data = None\n        self.__exception = exception\n        self.__event.set()", "category": "Python"}, {"instruction": "def any(self, array, role = None):\n        \"\"\"\n            Return ``True`` if ``array`` is ``True`` for any members of the entity.\n\n            ``array`` must have the dimension of the number of persons in the simulation\n\n            If ``role`` is provided, only the entity member with the given role are taken into account.\n\n            Example:\n\n            >>> salaries = household.members('salary', '2018-01')  # e.g. [2000, 1500, 0, 0, 0]\n            >>> household.any(salaries >= 1800)\n            >>> array([True])\n        \"\"\"\n", "input": "", "output": "        sum_in_entity = self.sum(array, role = role)\n        return (sum_in_entity > 0)", "category": "Python"}, {"instruction": "def __set_date(self, value):\n        '''\n        Sets the invoice date.\n        @param value:datetime\n        '''\n", "input": "", "output": "        value = date_to_datetime(value)\n        if value > datetime.now() + timedelta(hours=14, minutes=1): #More or less 14 hours from now in case the submitted date was local\n            raise ValueError(\"Date cannot be in the future.\")\n\n        if self.__due_date and value.date() > self.__due_date:\n            raise ValueError(\"Date cannot be posterior to the due date.\")\n\n        self.__date = value", "category": "Python"}, {"instruction": "def _costfcn(self, x):\n        \"\"\" Evaluates the objective function, gradient and Hessian for OPF.\n        \"\"\"\n", "input": "", "output": "        f = self._f(x)\n        df = self._df(x)\n        d2f = self._d2f(x)\n\n        return f, df, d2f", "category": "Python"}, {"instruction": "def set_fingerprint(fullpath, fingerprint=None):\n    \"\"\" Set the last known modification time for a file \"\"\"\n", "input": "", "output": "    try:\n        fingerprint = fingerprint or utils.file_fingerprint(fullpath)\n\n        record = model.FileFingerprint.get(file_path=fullpath)\n        if record:\n            record.set(fingerprint=fingerprint,\n                       file_mtime=os.stat(fullpath).st_mtime)\n        else:\n            record = model.FileFingerprint(\n                file_path=fullpath,\n                fingerprint=fingerprint,\n                file_mtime=os.stat(fullpath).st_mtime)\n        orm.commit()\n    except FileNotFoundError:\n        orm.delete(fp for fp in model.FileFingerprint if fp.file_path == fullpath)", "category": "Python"}, {"instruction": "def send_audio(self,\n                   left_channel,\n                   right_channel,\n                   frame_counter=None):\n        \"\"\"Add the audio samples to the stream. The left and the right\n        channel should have the same shape.\n\n        :param left_channel: array containing the audio signal.\n        :type left_channel: numpy array with shape (k, )\n            containing values between -1.0 and 1.0. l can be any integer\n        :param right_channel: array containing the audio signal.\n        :type right_channel: numpy array with shape (k, )\n            containing values between -1.0 and 1.0. l can be any integer\n        :param frame_counter: frame position number within stream.\n            Provide this when multi-threading to make sure frames don't\n            switch position\n        :type frame_counter: int\n        \"\"\"\n", "input": "", "output": "        if frame_counter is None:\n            frame_counter = self.audio_frame_counter\n            self.audio_frame_counter += 1\n\n        self.q_audio.put((frame_counter, left_channel, right_channel))", "category": "Python"}, {"instruction": "def main():\n    \"\"\"Main entry.\"\"\"\n", "input": "", "output": "    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--host\", type=str, required=True)\n    parser.add_argument(\"--user\", type=str, required=True)\n    parser.add_argument(\"--password\", type=str)\n    parser.add_argument(\"--token\", type=str)\n    args = parser.parse_args()\n    if not args.password and not args.token:\n        print('password or token is required')\n        exit(1)\n    example(args.host, args.user, args.password, args.token)", "category": "Python"}, {"instruction": "def show_vcs_output_vcs_nodes_vcs_node_info_node_switch_wwn(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        show_vcs = ET.Element(\"show_vcs\")\n        config = show_vcs\n        output = ET.SubElement(show_vcs, \"output\")\n        vcs_nodes = ET.SubElement(output, \"vcs-nodes\")\n        vcs_node_info = ET.SubElement(vcs_nodes, \"vcs-node-info\")\n        node_switch_wwn = ET.SubElement(vcs_node_info, \"node-switch-wwn\")\n        node_switch_wwn.text = kwargs.pop('node_switch_wwn')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def get_portchannel_info_by_intf_output_lacp_receive_machine_state(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_portchannel_info_by_intf = ET.Element(\"get_portchannel_info_by_intf\")\n        config = get_portchannel_info_by_intf\n        output = ET.SubElement(get_portchannel_info_by_intf, \"output\")\n        lacp = ET.SubElement(output, \"lacp\")\n        receive_machine_state = ET.SubElement(lacp, \"receive-machine-state\")\n        receive_machine_state.text = kwargs.pop('receive_machine_state')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def usufyToGmlExport(d, fPath):\n    \"\"\"\n    Workaround to export data to a .gml file.\n\n    Args:\n    -----\n        d: Data to export.\n        fPath: File path for the output file.\n    \"\"\"\n", "input": "", "output": "    # Reading the previous gml file\n    try:\n        oldData=nx.read_gml(fPath)\n    except UnicodeDecodeError as e:\n        print(\"UnicodeDecodeError:\\t\" + str(e))\n        print(\"Something went wrong when reading the .gml file relating to the decoding of UNICODE.\")\n        import time as time\n        fPath+=\"_\" +str(time.time())\n        print(\"To avoid losing data, the output file will be renamed to use the timestamp as:\\n\" + fPath + \"_\" + str(time.time()))\n        print()\n        # No information has been recovered\n        oldData = nx.Graph()\n    except Exception as e:\n        # No information has been recovered\n        oldData = nx.Graph()\n\n    newGraph = _generateGraphData(d, oldData)\n\n    # Writing the gml file\n    nx.write_gml(newGraph,fPath)", "category": "Python"}, {"instruction": "def save(self, content):\n        \"\"\"\n        Save any given content to the instance file.\n        :param content: (str or bytes)\n        :return: (None)\n        \"\"\"\n", "input": "", "output": "        # backup existing file if needed\n        if os.path.exists(self.file_path) and not self.assume_yes:\n            message = \"Overwrite existing {}? (y/n) \"\n            if not confirm(message.format(self.filename)):\n                self.backup()\n\n        # write file\n        self.output(\"Saving \" + self.filename)\n        with open(self.file_path, \"wb\") as handler:\n            if not isinstance(content, bytes):\n                content = bytes(content, \"utf-8\")\n            handler.write(content)\n        self.yeah(\"Done!\")", "category": "Python"}, {"instruction": "def get_visible_child(self, parent, locator, params=None, timeout=None):\n        \"\"\"\n        Get child-element both present AND visible in the DOM.\n\n        If timeout is 0 (zero) return WebElement instance or None, else we wait and retry for timeout and raise\n        TimeoutException should the element not be found.\n\n        :param parent: parent-element\n        :param locator: locator tuple\n        :param params: (optional) locator params\n        :param timeout: (optional) time to wait for element (default: self._explicit_wait)\n        :return: WebElement instance\n        \"\"\"\n", "input": "", "output": "        return self.get_present_child(parent, locator, params, timeout, True)", "category": "Python"}, {"instruction": "def render_table_ordering(context, index, title):\n    \"\"\"\n    Renders a TABLE LAYOUT ordering using given index and a human-readable title for it.\n    Note that a new context is created for the template (containing only very few elements needed\n    for displaying).\n\n    Usage in template: {% render_table_ordering \"model_field__submodel_field\" \"Human-readable Title\" %}\n    \"\"\"\n", "input": "", "output": "\n    # title + plain url (=without o paramater)\n    new_context = { 'title' : title, 'plain_url' : context['view'].get_query_string(remove=['o']) }\n\n    # ordered url\n    new_context['ordered_url'] = new_context['plain_url'] + '&o=' + index\n\n    # toggled ordering url (only if needed)\n    if 'o' in context['request'].GET and ( context['request'].GET['o'] == index or context['request'].GET['o'] == '-%s' % index):\n        new_context['toggled_url'] = new_context['plain_url'] + '&o=' + (index if context['request'].GET['o'].startswith('-') else '-' + index) \n        new_context['order_direction'] = 'ascending' if context['request'].GET['o'].startswith('-') else 'descending'\n\n    return new_context", "category": "Python"}, {"instruction": "def add_nexusport_binding(port_id, vlan_id, vni, switch_ip, instance_id,\n                          is_native=False, ch_grp=0):\n    \"\"\"Adds a nexusport binding.\"\"\"\n", "input": "", "output": "    LOG.debug(\"add_nexusport_binding() called\")\n    session = bc.get_writer_session()\n    binding = nexus_models_v2.NexusPortBinding(port_id=port_id,\n                  vlan_id=vlan_id,\n                  vni=vni,\n                  switch_ip=switch_ip,\n                  instance_id=instance_id,\n                  is_native=is_native,\n                  channel_group=ch_grp)\n    session.add(binding)\n    session.flush()\n    return binding", "category": "Python"}, {"instruction": "def libvlc_media_list_player_set_playback_mode(p_mlp, e_mode):\n    '''Sets the playback mode for the playlist.\n    @param p_mlp: media list player instance.\n    @param e_mode: playback mode specification.\n    '''\n", "input": "", "output": "    f = _Cfunctions.get('libvlc_media_list_player_set_playback_mode', None) or \\\n        _Cfunction('libvlc_media_list_player_set_playback_mode', ((1,), (1,),), None,\n                    None, MediaListPlayer, PlaybackMode)\n    return f(p_mlp, e_mode)", "category": "Python"}, {"instruction": "def get_effective_domain_id(request):\n    \"\"\"Gets the id of the default domain.\n\n    If the requests default domain is the same as DEFAULT_DOMAIN,\n    return None.\n    \"\"\"\n", "input": "", "output": "    default_domain = get_default_domain(request)\n    domain_id = default_domain.get('id')\n    domain_name = default_domain.get('name')\n    return None if domain_name == DEFAULT_DOMAIN else domain_id", "category": "Python"}, {"instruction": "def use_comparative_vault_view(self):\n        \"\"\"The returns from the lookup methods may omit or translate elements based on this session, such as authorization, and not result in an error.\n\n        This view is used when greater interoperability is desired at\n        the expense of precision.\n\n        *compliance: mandatory -- This method is must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.BinLookupSession.use_comparative_bin_view\n        self._catalog_view = COMPARATIVE\n        if self._catalog_session is not None:\n            self._catalog_session.use_comparative_catalog_view()", "category": "Python"}, {"instruction": "def from_array(array):\n        \"\"\"\n        Deserialize a new StickerSet from a given dictionary.\n\n        :return: new StickerSet instance.\n        :rtype: StickerSet\n        \"\"\"\n", "input": "", "output": "        if array is None or not array:\n            return None\n        # end if\n        assert_type_or_raise(array, dict, parameter_name=\"array\")\n        from pytgbot.api_types.receivable.media import Sticker\n        \n\n        data = {}\n        data['name'] = u(array.get('name'))\n        data['title'] = u(array.get('title'))\n        data['contains_masks'] = bool(array.get('contains_masks'))\n        data['stickers'] = Sticker.from_array_list(array.get('stickers'), list_level=1)\n        data['_raw'] = array\n        return StickerSet(**data)", "category": "Python"}, {"instruction": "def launch_next(self, task=None, result=None):\n        \"\"\" Launch next task or finish operation\n\n        :param kser.sequencing.task.Task task: previous task\n        :param cdumay_result.Result result: previous task result\n\n        :return: Execution result\n        :rtype: cdumay_result.Result\n        \"\"\"\n", "input": "", "output": "        if task:\n            next_task = self.next(task)\n            if next_task:\n                return next_task.send(result=result)\n            else:\n                return self.set_status(task.status, result)\n        elif len(self.tasks) > 0:\n            return self.tasks[0].send(result=result)\n        else:\n            return Result(retcode=1, stderr=\"Nothing to do, empty operation !\")", "category": "Python"}, {"instruction": "def parse_config(filename, header):\n    \"\"\" Parses the provided filename and returns ``SettingParser`` if the\n        parsing was successful and header matches the header defined in the\n        file.\n\n        Returns ``SettingParser`` instance.\n\n        * Raises a ``ParseError`` exception if header doesn't match or parsing\n          fails.\n        \"\"\"\n", "input": "", "output": "\n    parser = SettingParser(filename)\n    if parser.header != header:\n        header_value = parser.header or ''\n        raise ParseError(u\"Unexpected header '{0}', expecting '{1}'\"\n                         .format(common.from_utf8(header_value), header))\n\n    return parser", "category": "Python"}, {"instruction": "def getenv(key, value=None):\n    \"\"\"Like `os.getenv` but returns unicode under Windows + Python 2\n\n    Args:\n        key (pathlike): The env var to get\n        value (object): The value to return if the env var does not exist\n    Returns:\n        `fsnative` or `object`:\n            The env var or the passed value if it doesn't exist\n    \"\"\"\n", "input": "", "output": "\n    key = path2fsn(key)\n    if is_win and PY2:\n        return environ.get(key, value)\n    return os.getenv(key, value)", "category": "Python"}, {"instruction": "def get(self, path):\n        \"\"\"\n        Get the content of a file, indentified by its path relative to the folder configured\n        in PyGreen. If the file extension is one of the extensions that should be processed\n        through Mako, it will be processed.\n        \"\"\"\n", "input": "", "output": "        data = self.app.test_client().get(\"/%s\" % path).data\n        return data", "category": "Python"}, {"instruction": "def push_ctx(app=None):\n    \"\"\"Creates new test context(s) for the given app\n\n    If the app is not None, it overrides any existing app and/or request\n    context. In other words, we will use the app that was passed in to create\n    a new test request context on the top of the stack. If, however, nothing\n    was passed in, we will assume that another app and/or  request context is\n    already in place and use that to run the test suite. If no app or request\n    context can be found, an AssertionError is emitted to let the user know\n    that they must somehow specify an application for testing.\n\n    \"\"\"\n", "input": "", "output": "    if app is not None:\n        ctx = app.test_request_context()\n        ctx.fixtures_request_context = True\n        ctx.push()\n        if _app_ctx_stack is not None:\n            _app_ctx_stack.top.fixtures_app_context = True\n\n    # Make sure that we have an application in the current context\n    if (_app_ctx_stack is None or _app_ctx_stack.top is None) and _request_ctx_stack.top is None:\n        raise AssertionError('A Flask application must be specified for Fixtures to work.')", "category": "Python"}, {"instruction": "def ensure_dir_exists(cls, filepath):\n        \"\"\"Ensure that a directory exists\n\n        If it doesn't exist, try to create it and protect against a race condition\n        if another process is doing the same.\n        \"\"\"\n", "input": "", "output": "        directory = os.path.dirname(filepath)\n        if directory != '' and not os.path.exists(directory):\n            try:\n                os.makedirs(directory)\n            except OSError as e:\n                if e.errno != errno.EEXIST:\n                    raise", "category": "Python"}, {"instruction": "def build_header_parsers():\n    \"\"\"Return mapping for parsers to use for each VCF header type\n\n    Inject the WarningHelper into the parsers.\n    \"\"\"\n", "input": "", "output": "    result = {\n        \"ALT\": MappingHeaderLineParser(header.AltAlleleHeaderLine),\n        \"contig\": MappingHeaderLineParser(header.ContigHeaderLine),\n        \"FILTER\": MappingHeaderLineParser(header.FilterHeaderLine),\n        \"FORMAT\": MappingHeaderLineParser(header.FormatHeaderLine),\n        \"INFO\": MappingHeaderLineParser(header.InfoHeaderLine),\n        \"META\": MappingHeaderLineParser(header.MetaHeaderLine),\n        \"PEDIGREE\": MappingHeaderLineParser(header.PedigreeHeaderLine),\n        \"SAMPLE\": MappingHeaderLineParser(header.SampleHeaderLine),\n        \"__default__\": StupidHeaderLineParser(),  # fallback\n    }\n    return result", "category": "Python"}, {"instruction": "def Query(self, queue, limit=1):\n    \"\"\"Retrieves tasks from a queue without leasing them.\n\n    This is good for a read only snapshot of the tasks.\n\n    Args:\n       queue: The task queue that this task belongs to, usually client.Queue()\n         where client is the ClientURN object you want to schedule msgs on.\n       limit: Number of values to fetch.\n\n    Returns:\n        A list of Task() objects.\n    \"\"\"\n", "input": "", "output": "    # This function is usually used for manual testing so we also accept client\n    # ids and get the queue from it.\n    if isinstance(queue, rdf_client.ClientURN):\n      queue = queue.Queue()\n\n    return self.data_store.QueueQueryTasks(queue, limit=limit)", "category": "Python"}, {"instruction": "def full2ph(trans, n_pstates):\n    \"\"\"\n    Convert a full transmat to the respective p-state and h-state transmats\n    \"\"\"\n", "input": "", "output": "    n_hstates = len(trans) / n_pstates\n\n    htrans = np.zeros((n_pstates, n_pstates, n_hstates, n_hstates))\n    for pidx1, pidx2 in product(range(n_pstates), range(n_pstates)):\n        idx1 = pidx1 * n_hstates\n        idx2 = pidx2 * n_hstates\n        htrans[pidx1, pidx2] = trans[idx1:idx1 + n_hstates, idx2:idx2 + n_hstates]\n\n    ptrans = normalize(htrans.sum(axis=-1).sum(axis=-1), axis=1)\n    htrans = normalize(htrans, axis=3)\n\n    return ptrans, htrans", "category": "Python"}, {"instruction": "def lstm_init_states(batch_size):\n    \"\"\" Returns a tuple of names and zero arrays for LSTM init states\"\"\"\n", "input": "", "output": "    hp = Hyperparams()\n    init_shapes = lstm.init_states(batch_size=batch_size, num_lstm_layer=hp.num_lstm_layer, num_hidden=hp.num_hidden)\n    init_names = [s[0] for s in init_shapes]\n    init_arrays = [mx.nd.zeros(x[1]) for x in init_shapes]\n    return init_names, init_arrays", "category": "Python"}, {"instruction": "def virtual_temperature(temperature, mixing, molecular_weight_ratio=mpconsts.epsilon):\n    r\"\"\"Calculate virtual temperature.\n\n    This calculation must be given an air parcel's temperature and mixing ratio.\n    The implementation uses the formula outlined in [Hobbs2006]_ pg.80.\n\n    Parameters\n    ----------\n    temperature: `pint.Quantity`\n        The temperature\n    mixing : `pint.Quantity`\n        dimensionless mass mixing ratio\n    molecular_weight_ratio : `pint.Quantity` or float, optional\n        The ratio of the molecular weight of the constituent gas to that assumed\n        for air. Defaults to the ratio for water vapor to dry air.\n        (:math:`\\epsilon\\approx0.622`).\n\n    Returns\n    -------\n    `pint.Quantity`\n        The corresponding virtual temperature of the parcel\n\n    Notes\n    -----\n    .. math:: T_v = T \\frac{\\text{w} + \\epsilon}{\\epsilon\\,(1 + \\text{w})}\n\n    \"\"\"\n", "input": "", "output": "    return temperature * ((mixing + molecular_weight_ratio)\n                          / (molecular_weight_ratio * (1 + mixing)))", "category": "Python"}, {"instruction": "def adapt(self, d, x):\n        \"\"\"\n        Adapt weights according one desired value and its input.\n\n        Args:\n\n        * `d` : desired value (float)\n\n        * `x` : input array (1-dimensional array)\n        \"\"\"\n", "input": "", "output": "        self.update_memory_x(x)\n        m_d, m_x = self.read_memory()\n        # estimate\n        y = np.dot(self.w, x-m_x) + m_d\n        e = d - y\n        nu = self.mu / (self.eps + np.dot(x-m_x, x-m_x))\n        dw = nu * e * (x-m_x)\n        self.w += dw\n        self.update_memory_d(d)", "category": "Python"}, {"instruction": "async def _connect_sentinel(self, address, timeout, pools):\n        \"\"\"Try to connect to specified Sentinel returning either\n        connections pool or exception.\n        \"\"\"\n", "input": "", "output": "        try:\n            with async_timeout(timeout, loop=self._loop):\n                pool = await create_pool(\n                    address, minsize=1, maxsize=2,\n                    parser=self._parser_class,\n                    loop=self._loop)\n            pools.append(pool)\n            return pool\n        except asyncio.TimeoutError as err:\n            sentinel_logger.debug(\n                \"Failed to connect to Sentinel(%r) within %ss timeout\",\n                address, timeout)\n            return err\n        except Exception as err:\n            sentinel_logger.debug(\n                \"Error connecting to Sentinel(%r): %r\", address, err)\n            return err", "category": "Python"}, {"instruction": "def video_augmentation(features, hue=False, saturate=False, contrast=False):\n  \"\"\"Augments video with optional hue, saturation and constrast.\n\n  Args:\n    features: dict, with keys \"inputs\", \"targets\".\n              features[\"inputs\"], 4-D Tensor, shape=(THWC)\n              features[\"targets\"], 4-D Tensor, shape=(THWC)\n    hue: bool, apply hue_transform.\n    saturate: bool, apply saturation transform.\n    contrast: bool, apply constrast transform.\n  Returns:\n    augment_features: dict with transformed \"inputs\" and \"targets\".\n  \"\"\"\n", "input": "", "output": "  inputs, targets = features[\"inputs\"], features[\"targets\"]\n  in_steps = common_layers.shape_list(inputs)[0]\n\n  # makes sure that the same augmentation is applied to both input and targets.\n  # if input is 4-D, then tf.image applies the same transform across the batch.\n  video = tf.concat((inputs, targets), axis=0)\n  if hue:\n    video = tf.image.random_hue(video, max_delta=0.2)\n  if saturate:\n    video = tf.image.random_saturation(video, lower=0.5, upper=1.5)\n  if contrast:\n    video = tf.image.random_contrast(video, lower=0.5, upper=1.5)\n  features[\"inputs\"], features[\"targets\"] = video[:in_steps], video[in_steps:]\n  return features", "category": "Python"}, {"instruction": "def show(cls, report_name, data):\n        \"\"\"\n        Shows a report by issuing a GET request to the /reports/report_name\n        endpoint.\n\n        Args:\n            `report_name`: the name of the report to show\n\n            `data`: the parameters for the report\n        \"\"\"\n", "input": "", "output": "        conn = Qubole.agent()\n        return conn.get(cls.element_path(report_name), data)", "category": "Python"}, {"instruction": "def singularize(plural):\n    \"\"\"Convert plural word to its singular form.\n\n    Args:\n        plural: A word in its plural form.\n    Returns:\n        The word in its singular form.\n    \"\"\"\n", "input": "", "output": "    if plural in UNCOUNTABLES:\n        return plural\n    for i in IRREGULAR:\n        if i[1] == plural:\n            return i[0]\n    for i in SINGULARIZE_PATTERNS:\n        if re.search(i[0], plural):\n            return re.sub(i[0], i[1], plural)\n    return plural", "category": "Python"}, {"instruction": "def xyz2lonlat(x,y,z):\n    \"\"\"\n    Convert x,y,z representation of points *on the unit sphere* of the\n    spherical triangulation to lon / lat (radians).\n\n    Note - no check is made here that (x,y,z) are unit vectors\n    \"\"\"\n", "input": "", "output": "\n    xs = np.array(x)\n    ys = np.array(y)\n    zs = np.array(z)\n\n    lons = np.arctan2(ys, xs)\n    lats = np.arcsin(zs)\n\n    return lons, lats", "category": "Python"}, {"instruction": "def fix_imports(script):\n    \"\"\"\n    Replace \"from PyQt5 import\" by \"from pyqode.qt import\".\n\n    :param script: script path\n    \"\"\"\n", "input": "", "output": "    with open(script, 'r') as f_script:\n        lines = f_script.read().splitlines()\n    new_lines = []\n    for l in lines:\n        if l.startswith(\"import \"):\n            l = \"from . \" + l\n        if \"from PyQt5 import\" in l:\n            l = l.replace(\"from PyQt5 import\", \"from pyqode.qt import\")\n        new_lines.append(l)\n    with open(script, 'w') as f_script:\n        f_script.write(\"\\n\".join(new_lines))", "category": "Python"}, {"instruction": "def _unmarshal_parts(pkg_reader, package, part_factory):\n        \"\"\"\n        Return a dictionary of |Part| instances unmarshalled from\n        *pkg_reader*, keyed by partname. Side-effect is that each part in\n        *pkg_reader* is constructed using *part_factory*.\n        \"\"\"\n", "input": "", "output": "        parts = {}\n        for partname, content_type, reltype, blob in pkg_reader.iter_sparts():\n            parts[partname] = part_factory(\n                partname, content_type, reltype, blob, package\n            )\n        return parts", "category": "Python"}, {"instruction": "def getdirs(self, section, option, raw=False, vars=None, fallback=[]):\n        \"\"\"\n        A convenience method which coerces the option in the specified section to a list of directories.\n        \"\"\"\n", "input": "", "output": "        globs = self.getlist(section, option, fallback=[])\n        return [f for g in globs for f in glob.glob(g) if os.path.isdir(f)]", "category": "Python"}, {"instruction": "def browserify_file(entry_point, output_file, babelify=False, export_as=None):\n    \"\"\"\n    Browserify a single javascript entry point plus non-external\n    dependencies into a single javascript file. Generates source maps\n    in debug mode. Minifies the output in release mode.\n\n    By default, it is not possible to ``require()`` any exports from the entry\n    point or included files. If ``export_as`` is specified, any module exports\n    in the specified entry point are exposed for ``require()`` with the\n    name specified by ``export_as``.\n    \"\"\"\n", "input": "", "output": "    from .modules import browserify\n\n    if not isinstance(entry_point, str):\n        raise RuntimeError('Browserify File compiler takes a single entry point as input.')\n\n    return {\n        'dependencies_fn': browserify.browserify_deps_file,\n        'compiler_fn': browserify.browserify_compile_file,\n        'input': entry_point,\n        'output': output_file,\n        'kwargs': {\n            'babelify': babelify,\n            'export_as': export_as,\n        },\n    }", "category": "Python"}, {"instruction": "def get_stp_mst_detail_output_msti_port_if_state(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_stp_mst_detail = ET.Element(\"get_stp_mst_detail\")\n        config = get_stp_mst_detail\n        output = ET.SubElement(get_stp_mst_detail, \"output\")\n        msti = ET.SubElement(output, \"msti\")\n        instance_id_key = ET.SubElement(msti, \"instance-id\")\n        instance_id_key.text = kwargs.pop('instance_id')\n        port = ET.SubElement(msti, \"port\")\n        if_state = ET.SubElement(port, \"if-state\")\n        if_state.text = kwargs.pop('if_state')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def read_hdf5_dict(h5f, names=None, path=None, on_missing='error', **kwargs):\n    \"\"\"Read a `DataQualityDict` from an HDF5 file\n    \"\"\"\n", "input": "", "output": "    if path:\n        h5f = h5f[path]\n\n    # allow alternative keyword argument name (FIXME)\n    if names is None:\n        names = kwargs.pop('flags', None)\n\n    # try and get list of names automatically\n    if names is None:\n        try:\n            names = find_flag_groups(h5f, strict=True)\n        except KeyError:\n            names = None\n        if not names:\n            raise ValueError(\"Failed to automatically parse available flag \"\n                             \"names from HDF5, please give a list of names \"\n                             \"to read via the ``names=`` keyword\")\n\n    # read data\n    out = DataQualityDict()\n    for name in names:\n        try:\n            out[name] = read_hdf5_flag(h5f, name, **kwargs)\n        except KeyError as exc:\n            if on_missing == 'ignore':\n                pass\n            elif on_missing == 'warn':\n                warnings.warn(str(exc))\n            else:\n                raise ValueError('no H5Group found for flag '\n                                 '{0!r}'.format(name))\n\n    return out", "category": "Python"}, {"instruction": "def create_marker_table(self):\n        \"\"\"\n        Create marker table if it doesn't exist.\n\n        Using a separate connection since the transaction might have to be reset.\n        \"\"\"\n", "input": "", "output": "        connection = self.connect()\n        connection.autocommit = True\n        cursor = connection.cursor()\n        if self.use_db_timestamps:\n            sql = ", "category": "Python"}, {"instruction": "def update_many(self, **kwargs):\n        \"\"\" Update multiple objects from collection.\n\n        First ES is queried, then the results are used to query DB.\n        This is done to make sure updated objects are those filtered\n        by ES in the 'index' method (so user updates what he saw).\n        \"\"\"\n", "input": "", "output": "        db_objects = self.get_dbcollection_with_es(**kwargs)\n        return self.Model._update_many(\n            db_objects, self._json_params, self.request)", "category": "Python"}, {"instruction": "def s2time(secs, show_secs=True, show_fracs=True):\n    \"\"\"Converts seconds to time\"\"\"\n", "input": "", "output": "    try:\n        secs = float(secs)\n    except:\n        return \"--:--:--.--\"\n    wholesecs = int(secs)\n    centisecs = int((secs - wholesecs) * 100)\n    hh = int(wholesecs / 3600)\n    hd = int(hh % 24)\n    mm = int((wholesecs / 60) - (hh*60))\n    ss = int(wholesecs - (hh*3600) - (mm*60))\n    r = \"{:02d}:{:02d}\".format(hd, mm)\n    if show_secs:\n        r += \":{:02d}\".format(ss)\n    if show_fracs:\n        r += \".{:02d}\".format(centisecs)\n    return r", "category": "Python"}, {"instruction": "def get_arglist(self, objtxt):\r\n        \"\"\"Get func/method argument list\"\"\"\n", "input": "", "output": "        obj, valid = self._eval(objtxt)\r\n        if valid:\r\n            return getargtxt(obj)", "category": "Python"}, {"instruction": "def run_pandoc(text='', args=None):\n    \"\"\"\n    Low level function that calls Pandoc with (optionally)\n    some input text and/or arguments\n    \"\"\"\n", "input": "", "output": "\n    if args is None:\n        args = []\n\n    pandoc_path = which('pandoc')\n    if pandoc_path is None or not os.path.exists(pandoc_path):\n        raise OSError(\"Path to pandoc executable does not exists\")\n\n    proc = Popen([pandoc_path] + args, stdin=PIPE, stdout=PIPE, stderr=PIPE)\n    out, err = proc.communicate(input=text.encode('utf-8'))\n    exitcode = proc.returncode\n    if exitcode != 0:\n        raise IOError(err)\n    return out.decode('utf-8')", "category": "Python"}, {"instruction": "def powerlaw(f, log10_A=-16, gamma=5):\n    \"\"\"Power-law PSD.\n\n    :param f: Sampling frequencies\n    :param log10_A: log10 of red noise Amplitude [GW units]\n    :param gamma: Spectral index of red noise process\n    \"\"\"\n", "input": "", "output": "\n    fyr = 1 / 3.16e7\n    return (10**log10_A)**2 / 12.0 / np.pi**2 * fyr**(gamma-3) * f**(-gamma)", "category": "Python"}, {"instruction": "def spkgeo(targ, et, ref, obs):\n    \"\"\"\n    Compute the geometric state (position and velocity) of a target\n    body relative to an observing body.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/spkgeo_c.html\n\n    :param targ: Target body.\n    :type targ: int\n    :param et: Target epoch.\n    :type et: float\n    :param ref: Target reference frame.\n    :type ref: str\n    :param obs: Observing body.\n    :type obs: int\n    :return: State of target, Light time.\n    :rtype: tuple\n    \"\"\"\n", "input": "", "output": "    targ = ctypes.c_int(targ)\n    et = ctypes.c_double(et)\n    ref = stypes.stringToCharP(ref)\n    obs = ctypes.c_int(obs)\n    state = stypes.emptyDoubleVector(6)\n    lt = ctypes.c_double()\n    libspice.spkgeo_c(targ, et, ref, obs, state, ctypes.byref(lt))\n    return stypes.cVectorToPython(state), lt.value", "category": "Python"}, {"instruction": "def get(self, sid):\n        \"\"\"\n        Constructs a WorkerChannelContext\n\n        :param sid: The sid\n\n        :returns: twilio.rest.taskrouter.v1.workspace.worker.worker_channel.WorkerChannelContext\n        :rtype: twilio.rest.taskrouter.v1.workspace.worker.worker_channel.WorkerChannelContext\n        \"\"\"\n", "input": "", "output": "        return WorkerChannelContext(\n            self._version,\n            workspace_sid=self._solution['workspace_sid'],\n            worker_sid=self._solution['worker_sid'],\n            sid=sid,\n        )", "category": "Python"}, {"instruction": "def flatten( iterables ):\n    \"\"\" Flatten an iterable, except for string elements. \"\"\"\n", "input": "", "output": "    for it in iterables:\n        if isinstance(it, str):\n            yield it\n        else:\n            for element in it:\n                yield element", "category": "Python"}, {"instruction": "def is_scn(self):\n        \"\"\"Page contains Leica SCN XML in ImageDescription tag.\"\"\"\n", "input": "", "output": "        if self.index > 1 or not self.description:\n            return False\n        d = self.description\n        return d[:14] == '<?xml version=' and d[-6:] == '</scn>'", "category": "Python"}, {"instruction": "def plan(self):\n        \"\"\"Create website CFN module and run stacker diff.\"\"\"\n", "input": "", "output": "        if self.options.get('environments', {}).get(self.context.env_name):\n            self.setup_website_module(command='plan')\n        else:\n            LOGGER.info(\"Skipping staticsite plan of %s; no environment \"\n                        \"config found for this environment/region\",\n                        self.options['path'])", "category": "Python"}, {"instruction": "def barrier(self):\n        \"\"\"\n        .. note:: Experimental\n\n        Sets a global barrier and waits until all tasks in this stage hit this barrier.\n        Similar to `MPI_Barrier` function in MPI, this function blocks until all tasks\n        in the same stage have reached this routine.\n\n        .. warning:: In a barrier stage, each task much have the same number of `barrier()`\n            calls, in all possible code branches.\n            Otherwise, you may get the job hanging or a SparkException after timeout.\n\n        .. versionadded:: 2.4.0\n        \"\"\"\n", "input": "", "output": "        if self._port is None or self._secret is None:\n            raise Exception(\"Not supported to call barrier() before initialize \" +\n                            \"BarrierTaskContext.\")\n        else:\n            _load_from_socket(self._port, self._secret)", "category": "Python"}, {"instruction": "def all_regions(self):\n        \"\"\"Get a tuple of all chromosome, start and end regions.\n        \"\"\"\n", "input": "", "output": "        regions = []\n        for sq in self._bam.header[\"SQ\"]:\n            regions.append((sq[\"SN\"], 1, int(sq[\"LN\"])))\n        return regions", "category": "Python"}, {"instruction": "def _get_mr_params(self):\n    \"\"\"Converts self to model.MapreduceSpec.params.\"\"\"\n", "input": "", "output": "    return {\"force_writes\": self._force_writes,\n            \"done_callback\": self.done_callback_url,\n            \"user_params\": self.user_params,\n            \"shard_max_attempts\": self.shard_max_attempts,\n            \"task_max_attempts\": self._task_max_attempts,\n            \"task_max_data_processing_attempts\":\n                self._task_max_data_processing_attempts,\n            \"queue_name\": self.queue_name,\n            \"base_path\": self._base_path,\n            \"app_id\": self._app,\n            \"api_version\": self._api_version}", "category": "Python"}, {"instruction": "def tree(self, path, max_depth, full_path=False, include_stat=False):\n        \"\"\"DFS generator which starts from a given path and goes up to a max depth.\n\n        :param path: path from which the DFS will start\n        :param max_depth: max depth of DFS (0 means no limit)\n        :param full_path: should the full path of the child node be returned\n        :param include_stat: return the child Znode's stat along with the name & level\n        \"\"\"\n", "input": "", "output": "        for child_level_stat in self.do_tree(path, max_depth, 0, full_path, include_stat):\n            yield child_level_stat", "category": "Python"}, {"instruction": "def list_tags(self, image_name):\n        # type: (str) -> Iterator[str]\n        \"\"\" List all tags for the given image stored in the registry.\n\n        Args:\n            image_name (str):\n                The name of the image to query. The image must be present on the\n                registry for this call to return any values.\n        Returns:\n            list[str]: List of tags for that image.\n        \"\"\"\n", "input": "", "output": "        tags_url = self.registry_url + '/v2/{}/tags/list'\n\n        r = self.get(tags_url.format(image_name), auth=self.auth)\n        data = r.json()\n\n        if 'tags' in data:\n            return reversed(sorted(data['tags']))\n\n        return []", "category": "Python"}, {"instruction": "def receive_notification(self):\n        \"\"\"wait for the next incoming message.\n        intended to be used when we have nothing to send but want to receive\n        notifications.\n        \"\"\"\n", "input": "", "output": "        if not self._endpoint.receive_messages():\n            raise EOFError(\"EOF\")\n        self._process_input_notification()\n        self._process_input_request()", "category": "Python"}, {"instruction": "def quaternion_imag(quaternion):\n    \"\"\"Return imaginary part of quaternion.\n\n    >>> quaternion_imag([3, 0, 1, 2])\n    array([0., 1., 2.])\n\n    \"\"\"\n", "input": "", "output": "    return np.array(quaternion[1:4], dtype=np.float64, copy=True)", "category": "Python"}, {"instruction": "def run_migration_list(self, path, migrations, pretend=False):\n        \"\"\"\n        Run a list of migrations.\n\n        :type migrations: list\n\n        :type pretend: bool\n        \"\"\"\n", "input": "", "output": "        if not migrations:\n            self._note(\"<info>Nothing to migrate</info>\")\n\n            return\n\n        batch = self._repository.get_next_batch_number()\n\n        for f in migrations:\n            self._run_up(path, f, batch, pretend)", "category": "Python"}, {"instruction": "def _from_dict(cls, _dict):\n        \"\"\"Initialize a QueryRelationsArgument object from a json dictionary.\"\"\"\n", "input": "", "output": "        args = {}\n        if 'entities' in _dict:\n            args['entities'] = [\n                QueryEntitiesEntity._from_dict(x)\n                for x in (_dict.get('entities'))\n            ]\n        return cls(**args)", "category": "Python"}, {"instruction": "def versions(ctx, archive_name):\n    '''\n    Get an archive's versions\n    '''\n", "input": "", "output": "\n    _generate_api(ctx)\n\n    var = ctx.obj.api.get_archive(archive_name)\n    click.echo(pprint.pformat(map(str, var.get_versions())))", "category": "Python"}, {"instruction": "def to_array(self):\n        \"\"\"\n        Serializes this ReplyKeyboardRemove to a dictionary.\n\n        :return: dictionary representation of this object.\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        array = super(ReplyKeyboardRemove, self).to_array()\n        array['remove_keyboard'] = bool(self.remove_keyboard)  # type bool\n        if self.selective is not None:\n            array['selective'] = bool(self.selective)  # type bool\n        return array", "category": "Python"}, {"instruction": "def enable_logging(log_level):\n    # type: (typing.Union[None, int]) -> None\n    \"\"\"Configure the root logger and a logfile handler.\n\n    Args:\n        log_level: The logging level to set the logger handler.\n    \"\"\"\n", "input": "", "output": "    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n    logfile_handler = logging.StreamHandler(_LOGFILE_STREAM)\n    logfile_handler.setLevel(logging.DEBUG)\n    logfile_handler.setFormatter(logging.Formatter(\n        '%(levelname)s [%(asctime)s][%(name)s] %(message)s'))\n    root_logger.addHandler(logfile_handler)\n    if signal.getsignal(signal.SIGTERM) == signal.SIG_DFL:\n        signal.signal(signal.SIGTERM, _logfile_sigterm_handler)\n    if log_level:\n        handler = logging.StreamHandler()\n        handler.setFormatter(_LogColorFormatter())\n        root_logger.setLevel(log_level)\n        root_logger.addHandler(handler)", "category": "Python"}, {"instruction": "def set(self, param, value):\n        \"\"\"\n        Sets a parameter in the embedded param map.\n        \"\"\"\n", "input": "", "output": "        self._shouldOwn(param)\n        try:\n            value = param.typeConverter(value)\n        except ValueError as e:\n            raise ValueError('Invalid param value given for param \"%s\". %s' % (param.name, e))\n        self._paramMap[param] = value", "category": "Python"}, {"instruction": "def _in_conf_dir(filename):\n    \"\"\"\n    Put the `filename` to the configuration directory context / path.\n    \"\"\"\n", "input": "", "output": "    return os.path.join(\n        os.path.dirname(__file__),\n        \"templates/conf\",\n        filename\n    )", "category": "Python"}, {"instruction": "def portal(self, portalID=None):\n        \"\"\"returns a specific reference to a portal\"\"\"\n", "input": "", "output": "        if portalID is None:\n            portalID = self.portalSelf.id\n        url = \"%s/%s\" % (self.root, portalID)\n        return Portal(url=url,\n                  securityHandler=self._securityHandler,\n                  proxy_url=self._proxy_url,\n                  proxy_port=self._proxy_port,\n                  initalize=True)", "category": "Python"}, {"instruction": "def _get_storage_vol(conn, pool, vol):\n    '''\n    Helper function getting a storage volume. Will throw a libvirtError\n    if the pool or the volume couldn't be found.\n    '''\n", "input": "", "output": "    pool_obj = conn.storagePoolLookupByName(pool)\n    return pool_obj.storageVolLookupByName(vol)", "category": "Python"}, {"instruction": "def get_config_parameter_boolean(config: ConfigParser,\n                                 section: str,\n                                 param: str,\n                                 default: bool) -> bool:\n    \"\"\"\n    Get Boolean parameter from ``configparser`` ``.INI`` file.\n\n    Args:\n        config: :class:`ConfigParser` object\n        section: section name within config file\n        param: name of parameter within section\n        default: default value\n    Returns:\n        parameter value, or default\n    \"\"\"\n", "input": "", "output": "    try:\n        value = config.getboolean(section, param)\n    except (TypeError, ValueError, NoOptionError):\n        log.warning(\n            \"Configuration variable {} not found or improper in section [{}]; \"\n            \"using default of {!r}\", param, section, default)\n        value = default\n    return value", "category": "Python"}, {"instruction": "def validate_authentication(self, username, password, handler):\n        \"\"\"authenticate user with password\n        \"\"\"\n", "input": "", "output": "        user = authenticate(\n            **{self.username_field: username, 'password': password}\n        )\n        account = self.get_account(username)\n        if not (user and account):\n            raise AuthenticationFailed(\"Authentication failed.\")", "category": "Python"}, {"instruction": "def logging_syslog_server_port(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        logging = ET.SubElement(config, \"logging\", xmlns=\"urn:brocade.com:mgmt:brocade-ras\")\n        syslog_server = ET.SubElement(logging, \"syslog-server\")\n        syslogip_key = ET.SubElement(syslog_server, \"syslogip\")\n        syslogip_key.text = kwargs.pop('syslogip')\n        use_vrf_key = ET.SubElement(syslog_server, \"use-vrf\")\n        use_vrf_key.text = kwargs.pop('use_vrf')\n        port = ET.SubElement(syslog_server, \"port\")\n        port.text = kwargs.pop('port')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def join_all(joinables, timeout):\n    \"\"\"Wait on a list of objects that can be joined with a total\n    timeout represented by ``timeout``.\n\n    Parameters:\n      joinables(object): Objects with a join method.\n      timeout(int): The total timeout in milliseconds.\n    \"\"\"\n", "input": "", "output": "    started, elapsed = current_millis(), 0\n    for ob in joinables:\n        ob.join(timeout=timeout / 1000)\n        elapsed = current_millis() - started\n        timeout = max(0, timeout - elapsed)", "category": "Python"}, {"instruction": "def _get_values(self, lst, list_columns):\n        \"\"\"\n            Get Values: formats values for list template.\n            returns [{'col_name':'col_value',....},{'col_name':'col_value',....}]\n\n            :param lst:\n                The list of item objects from query\n            :param list_columns:\n                The list of columns to include\n        \"\"\"\n", "input": "", "output": "        retlst = []\n        for item in lst:\n            retdict = {}\n            for col in list_columns:\n                retdict[col] = self._get_attr_value(item, col)\n            retlst.append(retdict)\n        return retlst", "category": "Python"}, {"instruction": "def get_public_inline_preview_url(self, id, submission_id=None):\r\n        \"\"\"\r\n        Get public inline preview url.\r\n\r\n        Determine the URL that should be used for inline preview of the file.\r\n        \"\"\"\n", "input": "", "output": "        path = {}\r\n        data = {}\r\n        params = {}\r\n\r\n        # REQUIRED - PATH - id\r\n        ", "category": "Python"}, {"instruction": "def find_path(self, start, end, grid):\n        \"\"\"\n        find a path from start to end node on grid using the A* algorithm\n        :param start: start node\n        :param end: end node\n        :param grid: grid that stores all possible steps/tiles as 2D-list\n        :return:\n        \"\"\"\n", "input": "", "output": "        start.g = 0\n        start.f = 0\n        return super(AStarFinder, self).find_path(start, end, grid)", "category": "Python"}, {"instruction": "def add_rows(self, rows, header=True):\n        \"\"\"Add several rows in the rows stack\n\n        - The 'rows' argument can be either an iterator returning arrays,\n          or a by-dimensional array\n        - 'header' specifies if the first row should be used as the header\n          of the table\n        \"\"\"\n", "input": "", "output": "\n        # nb: don't use 'iter' on by-dimensional arrays, to get a\n        #     usable code for python 2.1\n        if header:\n            if hasattr(rows, '__iter__') and hasattr(rows, 'next'):\n                self.header(rows.next())\n            else:\n                self.header(rows[0])\n                rows = rows[1:]\n        for row in rows:\n            self.add_row(row)\n        return self", "category": "Python"}, {"instruction": "def finish(self):\n\t\t\"\"\"record the current stack process as finished\"\"\"\n", "input": "", "output": "\t\tself.report(fraction=1.0)\n\t\tkey = self.stack_key\n\t\tif key is not None:\n\t\t\tif self.data.get(key) is None:\n\t\t\t\tself.data[key] = []\n\t\t\tstart_time = self.current_times.get(key) or time()\n\t\t\tself.data[key].append(Dict(runtime=time()-start_time, **self.params))", "category": "Python"}, {"instruction": "def susvd(x, x_obs, rho, penalties):\n    \"\"\"\n    Sequential unfolding SVD\n\n    Parameters\n    ----------\n    x : Tensor\n\n    x_obs : array_like\n\n    rho : float\n\n    penalties : array_like\n        penalty for each unfolding of the input tensor\n    \"\"\"\n", "input": "", "output": "\n    assert type(x) == Tensor, \"Input array must be a Tensor\"\n\n    while True:\n\n        # proximal operator for the Fro. norm\n        x = squared_error(x, rho, x_obs)\n\n        # sequential singular value thresholding\n        for ix, penalty in enumerate(penalties):\n            x = x.unfold(ix).svt(penalty / rho).fold()\n\n        yield x", "category": "Python"}, {"instruction": "def as_html(self):\n        \"\"\"Generate HTML to display map.\"\"\"\n", "input": "", "output": "        if not self._folium_map:\n            self.draw()\n        return self._inline_map(self._folium_map, self._width, self._height)", "category": "Python"}, {"instruction": "def compute_from_ll(self,ll):\n        \"\"\"\n        m.compute_from_ll(ll) -- Build motif from an inputed log-likelihood matrix\n\n        (This function reverse-calculates the probability matrix and background frequencies\n        that were used to construct the log-likelihood matrix)\n        \"\"\"\n", "input": "", "output": "        self.ll    = ll\n        self.width = len(ll)\n        self._compute_bg_from_ll()\n        self._compute_logP_from_ll()\n        self._compute_ambig_ll()\n        self._compute_bits()\n        self._compute_oneletter()\n        self._maxscore()", "category": "Python"}, {"instruction": "def get_outcome_for_state_id(self, state_id):\n        \"\"\" Returns the final outcome of the child state specified by the state_id.\n\n        :param state_id: The id of the state to get the final outcome for.\n        :return:\n        \"\"\"\n", "input": "", "output": "        return_value = None\n        for s_id, name_outcome_tuple in self.final_outcomes_dict.items():\n            if s_id == state_id:\n                return_value = name_outcome_tuple[1]\n                break\n        return return_value", "category": "Python"}, {"instruction": "def list_tags(tags):\n    \"\"\"Print tags in dict so they allign with listing above.\"\"\"\n", "input": "", "output": "    tags_sorted = sorted(list(tags.items()), key=operator.itemgetter(0))\n    tag_sec_spacer = \"\"\n    c = 1\n    ignored_keys = [\"Name\", \"aws:ec2spot:fleet-request-id\"]\n    pad_col = {1: 38, 2: 49}\n    for k, v in tags_sorted:\n        # if k != \"Name\":\n        if k not in ignored_keys:\n            if c < 3:\n                padamt = pad_col[c]\n                sys.stdout.write(\"  {2}{0}:{3} {1}\".\n                                 format(k, v, C_HEAD2, C_NORM).ljust(padamt))\n                c += 1\n                tag_sec_spacer = \"\\n\"\n            else:\n                sys.stdout.write(\"{2}{0}:{3} {1}\\n\".format(k, v, C_HEAD2,\n                                                           C_NORM))\n                c = 1\n                tag_sec_spacer = \"\"\n    print(tag_sec_spacer)", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"\n        Closes the socket.\n        \"\"\"\n", "input": "", "output": "        self.acquire()\n        try:\n            if self.transport is not None:\n                self.transport.close()\n            super(Rfc5424SysLogHandler, self).close()\n        finally:\n            self.release()", "category": "Python"}, {"instruction": "def delete_route(self, route_table_id, destination_cidr_block):\n        \"\"\"\n        Deletes a route from a route table within a VPC.\n\n        :type route_table_id: str\n        :param route_table_id: The ID of the route table with the route.\n\n        :type destination_cidr_block: str\n        :param destination_cidr_block: The CIDR address block used for\n                                       destination match.\n\n        :rtype: bool\n        :return: True if successful\n        \"\"\"\n", "input": "", "output": "        params = {\n            'RouteTableId': route_table_id,\n            'DestinationCidrBlock': destination_cidr_block\n        }\n\n        return self.get_status('DeleteRoute', params)", "category": "Python"}, {"instruction": "def to_dict(self):\n        \"\"\"\n        Returns a dictionary with all of the information\n        about the entity.\n        \"\"\"\n", "input": "", "output": "        return {'type': self.__class__.__name__,\n                'points': self.points.tolist(),\n                'knots': self.knots.tolist(),\n                'closed': self.closed}", "category": "Python"}, {"instruction": "def on_configuration_changed(self, config):\n        \"\"\" Handles a screen configuration change.\n\n        \"\"\"\n", "input": "", "output": "        self.width = config['width']\n        self.height = config['height']\n        self.orientation = ('square', 'portrait', 'landscape')[\n                            config['orientation']]", "category": "Python"}, {"instruction": "def notebook_system_output():\n    \"\"\"Get a context manager that attempts to use `wurlitzer\n    <https://github.com/minrk/wurlitzer>`__ to capture system-level\n    stdout/stderr within a Jupyter Notebook shell, without affecting normal\n    operation when run as a Python script. For example:\n\n    >>> sys_pipes = sporco.util.notebook_system_output()\n    >>> with sys_pipes():\n    >>>    command_producing_system_level_output()\n\n\n    Returns\n    -------\n    sys_pipes : context manager\n      Context manager that handles output redirection when run within a\n      Jupyter Notebook shell\n    \"\"\"\n", "input": "", "output": "\n    from contextlib import contextmanager\n    @contextmanager\n    def null_context_manager():\n        yield\n\n    if in_notebook():\n        try:\n            from wurlitzer import sys_pipes\n        except ImportError:\n            sys_pipes = null_context_manager\n    else:\n        sys_pipes = null_context_manager\n\n    return sys_pipes", "category": "Python"}, {"instruction": "def initial_export(agent_id, force):\n    \"\"\"Performs the initial data export for an agent\"\"\"\n", "input": "", "output": "    agent = LiveSyncAgent.find_first(id=agent_id)\n    if agent is None:\n        print 'No such agent'\n        return\n    if agent.backend is None:\n        print cformat('Cannot run agent %{red!}{}%{reset} (backend not found)').format(agent.name)\n        return\n    print cformat('Selected agent: %{white!}{}%{reset} ({})').format(agent.name, agent.backend.title)\n    if agent.initial_data_exported and not force:\n        print 'The initial export has already been performed for this agent.'\n        print cformat('To re-run it, use %{yellow!}--force%{reset}')\n        return\n\n    agent.create_backend().run_initial_export(Event.find(is_deleted=False))\n    agent.initial_data_exported = True\n    db.session.commit()", "category": "Python"}, {"instruction": "def identifier_list_cmp(a, b):\n    \"\"\"Compare two identifier list (pre-release/build components).\n\n    The rule is:\n        - Identifiers are paired between lists\n        - They are compared from left to right\n        - If all first identifiers match, the longest list is greater.\n\n    >>> identifier_list_cmp(['1', '2'], ['1', '2'])\n    0\n    >>> identifier_list_cmp(['1', '2a'], ['1', '2b'])\n    -1\n    >>> identifier_list_cmp(['1'], ['1', '2'])\n    -1\n    \"\"\"\n", "input": "", "output": "    identifier_pairs = zip(a, b)\n    for id_a, id_b in identifier_pairs:\n        cmp_res = identifier_cmp(id_a, id_b)\n        if cmp_res != 0:\n            return cmp_res\n    # alpha1.3 < alpha1.3.1\n    return base_cmp(len(a), len(b))", "category": "Python"}, {"instruction": "def cli(yamlfile, format, output):\n    \"\"\" Generate an OWL representation of a biolink model \"\"\"\n", "input": "", "output": "    print(OwlSchemaGenerator(yamlfile, format).serialize(output=output))", "category": "Python"}, {"instruction": "def rotate_dom_by_yaw(self, dom_id, heading, centre_point=None):\n        \"\"\"Rotate a DOM by a given (yaw) heading.\"\"\"\n", "input": "", "output": "        pmts = self.pmts[self.pmts.dom_id == dom_id]\n        if centre_point is None:\n            centre_point = self.dom_positions[dom_id]\n\n        for pmt in pmts:\n            pmt_pos = np.array([pmt.pos_x, pmt.pos_y, pmt.pos_z])\n            pmt_dir = np.array([pmt.dir_x, pmt.dir_y, pmt.dir_z])\n            pmt_radius = np.linalg.norm(centre_point - pmt_pos)\n            index = self._pmt_index_by_pmt_id[pmt.pmt_id]\n            pmt_ref = self.pmts[index]\n\n            dir_rot = qrot_yaw([pmt.dir_x, pmt.dir_y, pmt.dir_z], heading)\n            pos_rot = pmt_pos - pmt_dir * pmt_radius + dir_rot * pmt_radius\n\n            pmt_ref.dir_x = dir_rot[0]\n            pmt_ref.dir_y = dir_rot[1]\n            pmt_ref.dir_z = dir_rot[2]\n            pmt_ref.pos_x = pos_rot[0]\n            pmt_ref.pos_y = pos_rot[1]\n            pmt_ref.pos_z = pos_rot[2]\n        self.reset_caches()", "category": "Python"}, {"instruction": "def has_image(self, digest, mime_type, index, size=500):\n        \"\"\"Tell if there is a preview image.\"\"\"\n", "input": "", "output": "        cache_key = f\"img:{index}:{size}:{digest}\"\n        return mime_type.startswith(\"image/\") or cache_key in self.cache", "category": "Python"}, {"instruction": "def add(self, obj):\n        \"\"\" Adds a member to the collection.\n\n        :param obj: Object to add.\n\n        Example:\n            >>> mycollection.add(pump.Person('bob@example.org'))\n        \"\"\"\n", "input": "", "output": "        activity = {\n            \"verb\": \"add\",\n            \"object\": {\n                \"objectType\": obj.object_type,\n                \"id\": obj.id\n            },\n            \"target\": {\n                \"objectType\": self.object_type,\n                \"id\": self.id\n            }\n        }\n\n        self._post_activity(activity)\n\n        # Remove the cash so it's re-generated next time it's needed\n        self._members = None", "category": "Python"}, {"instruction": "def add_table(self, rows, cols, left, top, width, height):\n        \"\"\"\n        Add a |GraphicFrame| object containing a table with the specified\n        number of *rows* and *cols* and the specified position and size.\n        *width* is evenly distributed between the columns of the new table.\n        Likewise, *height* is evenly distributed between the rows. Note that\n        the ``.table`` property on the returned |GraphicFrame| shape must be\n        used to access the enclosed |Table| object.\n        \"\"\"\n", "input": "", "output": "        graphicFrame = self._add_graphicFrame_containing_table(\n            rows, cols, left, top, width, height\n        )\n        graphic_frame = self._shape_factory(graphicFrame)\n        return graphic_frame", "category": "Python"}, {"instruction": "def face_angles_sparse(mesh):\n    \"\"\"\n    A sparse matrix representation of the face angles.\n\n    Returns\n    ----------\n    sparse: scipy.sparse.coo_matrix with:\n            dtype: float\n            shape: (len(mesh.vertices), len(mesh.faces))\n    \"\"\"\n", "input": "", "output": "    matrix = coo_matrix((mesh.face_angles.flatten(),\n                         (mesh.faces_sparse.row, mesh.faces_sparse.col)),\n                        mesh.faces_sparse.shape)\n    return matrix", "category": "Python"}, {"instruction": "def cp(hdfs_src, hdfs_dst):\n    \"\"\"Copy a file\n\n    :param hdfs_src: Source (str)\n    :param hdfs_dst: Destination (str)\n    :raises: IOError: If unsuccessful\n    \"\"\"\n", "input": "", "output": "    cmd = \"hadoop fs -cp %s %s\" % (hdfs_src, hdfs_dst)\n    rcode, stdout, stderr = _checked_hadoop_fs_command(cmd)", "category": "Python"}, {"instruction": "def import_stringified_func(funcstring):\n    \"\"\"\n    Import a string that represents a module and function, e.g. {module}.{funcname}.\n\n    Given a function f, import_stringified_func(stringify_func(f)) will return the same function.\n    :param funcstring: String to try to import\n    :return: callable\n    \"\"\"\n", "input": "", "output": "    assert isinstance(funcstring, str)\n\n    modulestring, funcname = funcstring.rsplit('.', 1)\n\n    mod = importlib.import_module(modulestring)\n\n    func = getattr(mod, funcname)\n    return func", "category": "Python"}, {"instruction": "def add_predicate(self, key, value, predicate_type='equals'):\n        \"\"\"\n        add key, value, type combination of a predicate\n\n        :param key: query KEY parameter\n        :param value: the value used in the predicate\n        :param predicate_type: the type of predicate (e.g. ``equals``)\n        \"\"\"\n", "input": "", "output": "        if predicate_type not in operators:\n            predicate_type = operator_lkup.get(predicate_type)\n        if predicate_type:\n            self.predicates.append({'type': predicate_type,\n                                    'key': key,\n                                    'value': value\n                                    })\n        else:\n            raise Exception(\"predicate type not a valid operator\")", "category": "Python"}, {"instruction": "def merge(self, config):\n\t\t\"\"\" Load configuration from given configuration.\n\n\t\t:param config: config to load. If config is a string type, then it's treated as .ini filename\n\t\t:return: None\n\t\t\"\"\"\n", "input": "", "output": "\t\tif isinstance(config, ConfigParser) is True:\n\t\t\tself.update(config)\n\t\telif isinstance(config, str):\n\t\t\tself.read(config)", "category": "Python"}, {"instruction": "def unit_response(self):\n        \"\"\"Calculate :ref:`pysynphot-formula-uresp`.\n\n        .. warning::\n\n            Result is correct only if ``self.waveunits`` is in Angstrom.\n\n        Returns\n        -------\n        ans : float\n            Bandpass unit response.\n\n        \"\"\"\n", "input": "", "output": "        hc = units.HC\n\n        if hasattr(self, 'primary_area'):\n            area = self.primary_area\n        else:\n            area = refs.PRIMARY_AREA\n\n        wave = self.GetWaveSet()\n        thru = self(wave)\n\n        return hc / (area * self.trapezoidIntegration(wave, thru*wave))", "category": "Python"}, {"instruction": "def check_python_version():\n    \"\"\"Checks the python version, exists if != 2.7.\"\"\"\n", "input": "", "output": "    python_major, python_minor = sys.version_info[:2]\n\n    if python_major != 2 or python_minor != 7:\n        sys.stderr.write(\"pyGenClean requires python 2.7\")\n        sys.exit(1)", "category": "Python"}, {"instruction": "def models(self):\n        '''\n        generator to return the tuple of model and its schema to create on aws.\n        '''\n", "input": "", "output": "        model_dict = self._build_all_dependencies()\n        while True:\n            model = self._get_model_without_dependencies(model_dict)\n            if not model:\n                break\n            yield (model, self._models().get(model))", "category": "Python"}, {"instruction": "def get_policy_configuration_revision(self, project, configuration_id, revision_id):\n        \"\"\"GetPolicyConfigurationRevision.\n        Retrieve a specific revision of a given policy by ID.\n        :param str project: Project ID or project name\n        :param int configuration_id: The policy configuration ID.\n        :param int revision_id: The revision ID.\n        :rtype: :class:`<PolicyConfiguration> <azure.devops.v5_0.policy.models.PolicyConfiguration>`\n        \"\"\"\n", "input": "", "output": "        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if configuration_id is not None:\n            route_values['configurationId'] = self._serialize.url('configuration_id', configuration_id, 'int')\n        if revision_id is not None:\n            route_values['revisionId'] = self._serialize.url('revision_id', revision_id, 'int')\n        response = self._send(http_method='GET',\n                              location_id='fe1e68a2-60d3-43cb-855b-85e41ae97c95',\n                              version='5.0',\n                              route_values=route_values)\n        return self._deserialize('PolicyConfiguration', response)", "category": "Python"}, {"instruction": "def pack_into(self, buf, offset, write_payload):\n    \"\"\"Pack to framed binary message.\n\n    \"\"\"\n", "input": "", "output": "    return self._get_framed(buf, offset, write_payload)", "category": "Python"}, {"instruction": "def ip_to_host(ip):\n    '''\n    Returns the hostname of a given IP\n    '''\n", "input": "", "output": "    try:\n        hostname, aliaslist, ipaddrlist = socket.gethostbyaddr(ip)\n    except Exception as exc:\n        log.debug('salt.utils.network.ip_to_host(%r) failed: %s', ip, exc)\n        hostname = None\n    return hostname", "category": "Python"}, {"instruction": "def refitMappings(self):\n        \"\"\"\n        Refit (normalize) all of the nsprefix mappings.\n        \"\"\"\n", "input": "", "output": "        for n in self.branch:\n            n.nsprefixes = {}\n        n = self.node\n        for u, p in self.prefixes.items():\n            n.addPrefix(p, u)", "category": "Python"}, {"instruction": "def compile(script, vars={}, library_paths=[]):\n    \"\"\"\n    Compile a jq script, retuning a script object.\n\n    library_paths is a list of strings that defines the module search path.\n    \"\"\"\n", "input": "", "output": "\n    return _pyjq.Script(script.encode('utf-8'), vars=vars,\n                        library_paths=library_paths)", "category": "Python"}, {"instruction": "def precompute_optimzation_Y(laplacian_matrix, n_samples, relaxation_kwds):\n    \"\"\"compute Lk, neighbors and subset to index map for projected == False\"\"\"\n", "input": "", "output": "    relaxation_kwds.setdefault('presave',False)\n    relaxation_kwds.setdefault('presave_name','pre_comp_current.npy')\n    relaxation_kwds.setdefault('verbose',False)\n    if relaxation_kwds['verbose']:\n        print ('Making Lk and nbhds')\n    Lk_tensor, nbk, si_map = \\\n        compute_Lk(laplacian_matrix, n_samples, relaxation_kwds['subset'])\n    if relaxation_kwds['presave']:\n        raise NotImplementedError('Not yet implemented presave')\n    return { 'Lk': Lk_tensor, 'nbk': nbk, 'si_map': si_map }", "category": "Python"}, {"instruction": "def get_apis(self):\n        \"\"\"Returns set of api names referenced in this Registry\n\n        :return: set of api name strings\n        \"\"\"\n", "input": "", "output": "        out = set(x.api for x in self.types.values() if x.api)\n        for ft in self.features.values():\n            out.update(ft.get_apis())\n        for ext in self.extensions.values():\n            out.update(ext.get_apis())\n        return out", "category": "Python"}, {"instruction": "def split_by_count(items, count, filler=None):\n    \"\"\"Split the items into tuples of count items each\n\n    >>> split_by_count([0,1,2,3], 2)\n    [(0, 1), (2, 3)]\n\n    If there are a mutiple of count items then filler makes no difference\n    >>> split_by_count([0,1,2,7,8,9], 3, 0) == split_by_count([0,1,2,7,8,9], 3)\n    True\n\n    If there are not a multiple of count items, then any extras are discarded\n    >>> split_by_count([0,1,2,7,8,9,6], 3)\n    [(0, 1, 2), (7, 8, 9)]\n\n    Specifying a filler expands the \"lost\" group\n    >>> split_by_count([0,1,2,7,8,9,6], 3, 0)\n    [(0, 1, 2), (7, 8, 9), (6, 0, 0)]\n    \"\"\"\n", "input": "", "output": "    if filler is not None:\n        items = items[:]\n        while len(items) % count:\n            items.append(filler)\n    iterator = iter(items)\n    iterators = [iterator] * count\n    return list(zip(*iterators))", "category": "Python"}, {"instruction": "def page(self, method, uri, params=None, data=None, headers=None, auth=None, timeout=None,\n             allow_redirects=False):\n        \"\"\"\n        Makes an HTTP request.\n        \"\"\"\n", "input": "", "output": "        return self.request(\n            method,\n            uri,\n            params=params,\n            data=data,\n            headers=headers,\n            auth=auth,\n            timeout=timeout,\n            allow_redirects=allow_redirects,\n        )", "category": "Python"}, {"instruction": "def dok15_s(k15):\n    \"\"\"\n    calculates least-squares matrix for 15 measurements from Jelinek [1976]\n    \"\"\"\n", "input": "", "output": "#\n    A, B = design(15)  # get design matrix for 15 measurements\n    sbar = np.dot(B, k15)  # get mean s\n    t = (sbar[0] + sbar[1] + sbar[2])  # trace\n    bulk = old_div(t, 3.)  # bulk susceptibility\n    Kbar = np.dot(A, sbar)  # get best fit values for K\n    dels = k15 - Kbar  # get deltas\n    dels, sbar = old_div(dels, t), old_div(sbar, t)  # normalize by trace\n    So = sum(dels**2)\n    sigma = np.sqrt(old_div(So, 9.))  # standard deviation\n    return sbar, sigma, bulk", "category": "Python"}, {"instruction": "def get_output(self, transaction_hash, output_index):\n        \"\"\"\n        Gets an output and information about its asset ID and asset quantity.\n\n        :param bytes transaction_hash: The hash of the transaction containing the output.\n        :param int output_index: The index of the output.\n        :return: An object containing the output as well as its asset ID and asset quantity.\n        :rtype: Future[TransactionOutput]\n        \"\"\"\n", "input": "", "output": "        cached_output = yield from self._cache.get(transaction_hash, output_index)\n\n        if cached_output is not None:\n            return cached_output\n\n        transaction = yield from self._transaction_provider(transaction_hash)\n\n        if transaction is None:\n            raise ValueError('Transaction {0} could not be retrieved'.format(bitcoin.core.b2lx(transaction_hash)))\n\n        colored_outputs = yield from self.color_transaction(transaction)\n\n        for index, output in enumerate(colored_outputs):\n            yield from self._cache.put(transaction_hash, index, output)\n\n        return colored_outputs[output_index]", "category": "Python"}, {"instruction": "def idict(self, in_dict={}):\n        \"\"\"Return a dict that uses this server's IRC casemapping.\n\n        All keys in this dictionary are stored and compared using this server's casemapping.\n        \"\"\"\n", "input": "", "output": "        new_dict = IDict(in_dict)\n        new_dict.set_std(self.features.get('casemapping'))\n        if not self._casemap_set:\n            self._imaps.append(new_dict)\n        return new_dict", "category": "Python"}, {"instruction": "def _construct_filename(self, batchno):\n        \"\"\"Construct a filename for a database.\n\n        Parameters:\n        batchno -- batch number for the rotated database.\n\n        Returns the constructed path as a string.\n\n        \"\"\"\n", "input": "", "output": "        return os.path.join(self.dirpath,\n                            \"{0}.{1}\".format(self.prefix, batchno))", "category": "Python"}, {"instruction": "def worker_recover(worker, lbn, profile='default'):\n    '''\n    Set the worker to recover\n    this module will fail if it is in OK state\n\n    CLI Examples:\n\n    .. code-block:: bash\n\n        salt '*' modjk.worker_recover node1 loadbalancer1\n        salt '*' modjk.worker_recover node1 loadbalancer1 other-profile\n    '''\n", "input": "", "output": "\n    cmd = {\n        'cmd': 'recover',\n        'mime': 'prop',\n        'w': lbn,\n        'sw': worker,\n    }\n    return _do_http(cmd, profile)", "category": "Python"}, {"instruction": "def _extract_tag_from_data(self, data, tag_name=b'packet'):\n        \"\"\"Gets data containing a (part of) tshark xml.\n\n        If the given tag is found in it, returns the tag data and the remaining data.\n        Otherwise returns None and the same data.\n\n        :param data: string of a partial tshark xml.\n        :return: a tuple of (tag, data). tag will be None if none is found.\n        \"\"\"\n", "input": "", "output": "        opening_tag = b'<' + tag_name + b'>'\n        closing_tag = opening_tag.replace(b'<', b'</')\n        tag_end = data.find(closing_tag)\n        if tag_end != -1:\n            tag_end += len(closing_tag)\n            tag_start = data.find(opening_tag)\n            return data[tag_start:tag_end], data[tag_end:]\n        return None, data", "category": "Python"}, {"instruction": "def perform_index_validation(self, data):\n        \"\"\"\n        Validate any unique indexes specified on the model.\n        This should happen after all the normal fields have been validated.\n        This can add error messages to multiple fields.\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        # Build a list of dict containing query values for each unique index.\n        index_data = []\n        for columns, unique in self.instance._meta.indexes:\n            if not unique:\n                continue\n            index_data.append({col: data.get(col, None) for col in columns})\n\n        # Then query for each unique index to see if the value is unique.\n        for index in index_data:\n            query = self.instance.filter(**index)\n            # If we have a primary key, need to exclude the current record from the check.\n            if self.pk_field and self.pk_value:\n                query = query.where(~(self.pk_field == self.pk_value))\n            if query.count():\n                err = ValidationError('index', fields=str.join(', ', index.keys()))\n                for col in index.keys():\n                    self.add_error(col, err)", "category": "Python"}, {"instruction": "def _fw_rule_delete(self, drvr_name, data):\n        \"\"\"Function that updates its local cache after a rule is deleted. \"\"\"\n", "input": "", "output": "        rule_id = data.get('firewall_rule_id')\n        tenant_id = self.tenant_db.get_rule_tenant(rule_id)\n\n        if tenant_id not in self.fwid_attr:\n            LOG.error(\"Invalid tenant id for FW delete %s\", tenant_id)\n            return\n        tenant_obj = self.fwid_attr[tenant_id]\n        # Guess actual FW/policy need not be deleted if this is the active\n        # rule, Openstack does not allow it to be deleted\n        tenant_obj.delete_rule(rule_id)\n        self.tenant_db.del_rule_tenant(rule_id)", "category": "Python"}, {"instruction": "def _render_content(self, content, **settings):\n        \"\"\"\n        Perform widget rendering, but do not print anything.\n        \"\"\"\n", "input": "", "output": "        bar_len = int(settings[self.SETTING_BAR_WIDTH])\n        if not bar_len:\n            bar_len = TERMINAL_WIDTH - 10\n        percent = content\n        progress = \"\"\n        progress += str(settings[self.SETTING_BAR_CHAR]) * int(bar_len * percent)\n        s = {k: settings[k] for k in (self.SETTING_FLAG_PLAIN,)}\n        s.update(settings[self.SETTING_BAR_FORMATING])\n        progress = self.fmt_text(progress, **s)\n        progress += ' ' * int(bar_len - int(bar_len * percent))\n        return \"{:6.2f}% [{:s}]\".format(percent * 100, progress)", "category": "Python"}, {"instruction": "def match_all(d_SMEFT, parameters=None):\n    \"\"\"Match the SMEFT Warsaw basis onto the WET JMS basis.\"\"\"\n", "input": "", "output": "    p = default_parameters.copy()\n    if parameters is not None:\n        # if parameters are passed in, overwrite the default values\n        p.update(parameters)\n    C = wilson.util.smeftutil.wcxf2arrays_symmetrized(d_SMEFT)\n    C['vT'] = 246.22\n    C_WET = match_all_array(C, p)\n    C_WET = wilson.translate.wet.rotate_down(C_WET, p)\n    C_WET = wetutil.unscale_dict_wet(C_WET)\n    d_WET = wilson.util.smeftutil.arrays2wcxf(C_WET)\n    basis = wcxf.Basis['WET', 'JMS']\n    keys = set(d_WET.keys()) & set(basis.all_wcs)\n    d_WET = {k: d_WET[k] for k in keys}\n    return d_WET", "category": "Python"}, {"instruction": "def human_sum_11(X, y, model_generator, method_name):\n    \"\"\" SUM (true/true)\n\n    This tests how well a feature attribution method agrees with human intuition\n    for a SUM operation. This metric deals\n    specifically with the question of credit allocation for the following function\n    when all three inputs are true:\n    if fever: +2 points\n    if cough: +2 points\n\n    transform = \"identity\"\n    sort_order = 2\n    \"\"\"\n", "input": "", "output": "    return _human_sum(X, model_generator, method_name, True, True)", "category": "Python"}, {"instruction": "def to_dict(self):\n        \"\"\" extract the data of the content and return it as a dictionary\n        \"\"\"\n", "input": "", "output": "\n        # 1. extract the schema fields\n        data = self.extract_fields()\n\n        # 2. include custom key-value pairs listed in the mapping dictionary\n        for key, attr in self.attributes.iteritems():\n            if key in self.ignore:\n                continue  # skip ignores\n            # fetch the mapped attribute\n            value = getattr(self.context, attr, None)\n            if value is None:\n                value = getattr(self, attr, None)\n            # handle function calls\n            if callable(value):\n                value = value()\n            # map the value to the given key from the mapping\n            data[key] = api.to_json_value(self.context, key, value)\n        return data", "category": "Python"}, {"instruction": "def isRunning(self, waitTime=0):\n        \"\"\" If PID isn't set yet, checks if there is a window with the specified title. \"\"\"\n", "input": "", "output": "        waitUntil = time.time() + waitTime\n        while True:\n            if self.getPID() > 0:\n                return True\n            else:\n                self._pid = PlatformManager.getWindowPID(PlatformManager.getWindowByTitle(re.escape(self._title)))\n\n            # Check if we've waited long enough\n            if time.time() > waitUntil:\n                break\n            else:\n                time.sleep(self._defaultScanRate)\n        return self.getPID() > 0", "category": "Python"}, {"instruction": "def userinfo(self, access_token):\n\n        \"\"\"Returns the user information based on the Auth0 access token.\n        This endpoint will work only if openid was granted as a scope for the access_token.\n\n        Args:\n            access_token (str): Auth0 access token (obtained during login).\n\n        Returns:\n            The user profile.\n        \"\"\"\n", "input": "", "output": "\n        return self.get(\n            url='https://{}/userinfo'.format(self.domain),\n            headers={'Authorization': 'Bearer {}'.format(access_token)}\n        )", "category": "Python"}, {"instruction": "def lowercase_to_camelcase(python_input, camelcase_input=None):\n    \n    '''\n        a function to recursively convert data with lowercase key names into camelcase keys \n        \n    :param camelcase_input: list or dictionary with lowercase keys \n    :param python_input: [optional] list or dictionary with default camelcase keys in output\n    :return: dictionary with camelcase key names\n    '''\n", "input": "", "output": "    \n    if camelcase_input:\n        if python_input.__class__ != camelcase_input.__class__:\n            raise ValueError('camelcase_input type %s does not match python_input type %s' % (camelcase_input.__class__, python_input.__class__))\n    if isinstance(python_input, dict):\n        return _to_camelcase_dict(python_input, camelcase_input)\n    elif isinstance(python_input, list):\n        return _ingest_list(python_input, _to_camelcase_dict, camelcase_input)\n    else:\n        return python_input", "category": "Python"}, {"instruction": "def open(self, path, binary=False):\n        \"\"\"Open file and return a stream.\"\"\"\n", "input": "", "output": "        if binary:\n            return open(path, \"rb\")\n        return open(path, encoding=\"utf-8\")", "category": "Python"}, {"instruction": "def chat_react(self, msg_id, emoji='smile', **kwargs):\n        \"\"\"Updates the text of the chat message.\"\"\"\n", "input": "", "output": "        return self.__call_api_post('chat.react', messageId=msg_id, emoji=emoji, kwargs=kwargs)", "category": "Python"}, {"instruction": "def subscribe_registration_ids_to_topic(self, registration_ids, topic_name):\n        \"\"\"\n        Subscribes a list of registration ids to a topic\n\n        Args:\n            registration_ids (list): ids to be subscribed\n            topic_name (str): name of topic\n\n        Returns:\n            True: if operation succeeded\n\n        Raises:\n            InvalidDataError: data sent to server was incorrectly formatted\n            FCMError: an error occured on the server\n        \"\"\"\n", "input": "", "output": "        url = 'https://iid.googleapis.com/iid/v1:batchAdd'\n        payload = {\n            'to': '/topics/' + topic_name,\n            'registration_tokens': registration_ids,\n        }\n        response = self.requests_session.post(url, json=payload)\n        if response.status_code == 200:\n            return True\n        elif response.status_code == 400:\n            error = response.json()\n            raise InvalidDataError(error['error'])\n        else:\n            raise FCMError()", "category": "Python"}, {"instruction": "def post(self, url, post_params=None):\n        \"\"\"\n        Make a HTTP POST request to the Reader API.\n\n        :param url: url to which to make a POST request.\n        :param post_params: parameters to be sent in the request's body.\n        \"\"\"\n", "input": "", "output": "        params = urlencode(post_params)\n        logger.debug('Making POST request to %s with body %s', url, params)\n        return self.oauth_session.post(url, data=params)", "category": "Python"}, {"instruction": "def add_or_update(self, app_id, value):\n        '''\n        Adding or updating the evalution.\n        :param app_id:  the ID of the post.\n        :param value: the evaluation\n        :return:  in JSON format.\n        '''\n", "input": "", "output": "        MEvaluation.add_or_update(self.userinfo.uid, app_id, value)\n\n        out_dic = {\n            'eval0': MEvaluation.app_evaluation_count(app_id, 0),\n            'eval1': MEvaluation.app_evaluation_count(app_id, 1)\n        }\n\n        return json.dump(out_dic, self)", "category": "Python"}, {"instruction": "def new(self, bootstrap_with=None, use_timer=False, incr=False,\n            with_proof=False):\n        \"\"\"\n            Actual constructor of the solver.\n        \"\"\"\n", "input": "", "output": "\n        assert not incr or not with_proof, 'Incremental mode and proof tracing cannot be set together.'\n\n        if not self.glucose:\n            self.glucose = pysolvers.glucose3_new()\n\n            if bootstrap_with:\n                for clause in bootstrap_with:\n                    self.add_clause(clause)\n\n            self.use_timer = use_timer\n            self.call_time = 0.0  # time spent for the last call to oracle\n            self.accu_time = 0.0  # time accumulated for all calls to oracle\n\n            if incr:\n                pysolvers.glucose3_setincr(self.glucose)\n\n            if with_proof:\n                self.prfile = tempfile.TemporaryFile()\n                pysolvers.glucose3_tracepr(self.glucose, self.prfile)", "category": "Python"}, {"instruction": "def process_shells_ordered(self, shells):\n        \"\"\"Processing a list of shells one after the other.\"\"\"\n", "input": "", "output": "        output = []\n        for shell in shells:\n            entry = shell['entry']\n            config = ShellConfig(script=entry['script'], title=entry['title'] if 'title' in entry else '',\n                                 model=shell['model'], env=shell['env'], item=shell['item'],\n                                 dry_run=shell['dry_run'], debug=shell['debug'], strict=shell['strict'],\n                                 variables=shell['variables'],\n                                 temporary_scripts_path=shell['temporary_scripts_path'])\n            result = Adapter(self.process_shell(get_creator_by_name(shell['creator']), entry, config))\n            output += result.output\n            self.__handle_variable(entry, result.output)\n            if not result.success:\n                return {'success': False, 'output': output}\n        return {'success': True, 'output': output}", "category": "Python"}, {"instruction": "def chunkreverse(integers, dtype='L'):\n    \"\"\"Yield integers of dtype bit-length reverting their bit-order.\n\n    >>> list(chunkreverse([0b10000000, 0b11000000, 0b00000001], 'B'))\n    [1, 3, 128]\n\n    >>> list(chunkreverse([0x8000, 0xC000, 0x0001], 'H'))\n    [1, 3, 32768]\n    \"\"\"\n", "input": "", "output": "    if dtype in ('B', 8):\n        return map(RBYTES.__getitem__, integers)\n\n    fmt = '{0:0%db}' % NBITS[dtype]\n\n    return (int(fmt.format(chunk)[::-1], 2) for chunk in integers)", "category": "Python"}, {"instruction": "def call_env_before_read_docs(cls, kb_app, sphinx_app: Sphinx,\n                                  sphinx_env: BuildEnvironment,\n                                  docnames: List[str]):\n        \"\"\" On env-read-docs, do callbacks\"\"\"\n", "input": "", "output": "\n        for callback in EventAction.get_callbacks(kb_app,\n                                                  SphinxEvent.EBRD):\n            callback(kb_app, sphinx_app, sphinx_env, docnames)", "category": "Python"}, {"instruction": "def gen_ncx(self, book_idx):\n        \"\"\"\n        \u751f\u6210NCX\u6587\u4ef6\u5185\u5bb9\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        start, end = self._start_end_of_index(book_idx)\n        ncx_base = ", "category": "Python"}, {"instruction": "def application_adapter(obj, request):\n    \"\"\"\n    Adapter for rendering a :class:`pyramid_urireferencer.models.ApplicationResponse` to json.\n\n    :param pyramid_urireferencer.models.ApplicationResponse obj: The response to be rendered.\n    :rtype: :class:`dict`\n    \"\"\"\n", "input": "", "output": "    return {\n        'title': obj.title,\n        'uri': obj.uri,\n        'service_url': obj.service_url,\n        'success': obj.success,\n        'has_references': obj.has_references,\n        'count': obj.count,\n        'items': [{\n                      'uri': i.uri,\n                      'title': i.title\n                  } for i in obj.items] if obj.items is not None else None\n    }", "category": "Python"}, {"instruction": "def create_design_matrix_2(Z, data, Y_len, lag_no):\n    \"\"\"\n    For Python 2.7 - cythonized version only works for 3.5\n    \"\"\"\n", "input": "", "output": "    row_count = 1\n\n    for lag in range(1, lag_no+1):\n        for reg in range(Y_len):\n            Z[row_count, :] = data[reg][(lag_no-lag):-lag]\n            row_count += 1\n\n    return Z", "category": "Python"}, {"instruction": "def _get_shell_pid():\n    \"\"\"Returns parent process pid.\"\"\"\n", "input": "", "output": "    proc = Process(os.getpid())\n\n    try:\n        return proc.parent().pid\n    except TypeError:\n        return proc.parent.pid", "category": "Python"}, {"instruction": "def activate():\n    \"\"\"Install the path-based import components.\"\"\"\n", "input": "", "output": "\n    global PathFinder, FileFinder, ff_path_hook\n\n    path_hook_index = len(sys.path_hooks)\n    sys.path_hooks.append(ff_path_hook)\n    # Resetting sys.path_importer_cache values,\n    # to support the case where we have an implicit package inside an already loaded package,\n    # since we need to replace the default importer.\n    sys.path_importer_cache.clear()\n\n    # Setting up the meta_path to change package finding logic\n    pathfinder_index = len(sys.meta_path)\n    sys.meta_path.append(PathFinder)\n\n    return path_hook_index, pathfinder_index", "category": "Python"}, {"instruction": "def read_header(self):\n        \"\"\"Reads header portion of packet\"\"\"\n", "input": "", "output": "        format = '!HHHHHH'\n        length = struct.calcsize(format)\n        info = struct.unpack(format,\n                self.data[self.offset:self.offset + length])\n        self.offset += length\n\n        self.id = info[0]\n        self.flags = info[1]\n        self.num_questions = info[2]\n        self.num_answers = info[3]\n        self.num_authorities = info[4]\n        self.num_additionals = info[5]", "category": "Python"}, {"instruction": "def get_previous_identifiers(self):\n        \"\"\"\n        :returns: SimpleIdentifierCollection\n        \"\"\"\n", "input": "", "output": "        previous_identifiers = None\n        stack = self.get_run_as_identifiers_stack()  # TBD:  must confirm logic\n\n        if stack:\n            if (len(stack) == 1):\n                previous_identifiers = self.identifiers\n            else:\n                # always get the one behind the current\n                previous_identifiers = stack[1]\n        return previous_identifiers", "category": "Python"}, {"instruction": "def update_meta_info(self):\n        \"\"\"Extract metadata from myself\"\"\"\n", "input": "", "output": "        result = super(BaseStructuredCalibration, self).update_meta_info()\n\n        result['instrument'] = self.instrument\n        result['uuid'] = self.uuid\n        result['tags'] = self.tags\n        result['type'] = self.name()\n\n        minfo = self.meta_info\n        try:\n            result['mode'] = minfo['mode_name']\n            origin = minfo['origin']\n            date_obs = origin['date_obs']\n        except KeyError:\n            origin = {}\n            date_obs = \"1970-01-01T00:00:00.00\"\n\n        result['observation_date'] = conv.convert_date(date_obs)\n        result['origin'] = origin\n\n        return result", "category": "Python"}, {"instruction": "def _post_keywords(self, **kwargs):\n        \"\"\"Configure keyword arguments for Open311 POST requests.\"\"\"\n", "input": "", "output": "        if self.jurisdiction and 'jurisdiction_id' not in kwargs:\n            kwargs['jurisdiction_id'] = self.jurisdiction\n        if 'address' in kwargs:\n            address = kwargs.pop('address')\n            kwargs['address_string'] = address\n        if 'name' in kwargs:\n            first, last = kwargs.pop('name').split(' ')\n            kwargs['first_name'] = first\n            kwargs['last_name'] = last\n        if 'api_key' not in kwargs:\n            kwargs['api_key'] = self.api_key\n        return kwargs", "category": "Python"}, {"instruction": "def tags(self, *names):\n        \"\"\"\n        Begin executing a new tags operation.\n\n        :param names: The tags\n        :type names: tuple\n\n        :rtype: cachy.tagged_cache.TaggedCache\n        \"\"\"\n", "input": "", "output": "        if len(names) == 1 and isinstance(names[0], list):\n            names = names[0]\n\n        return TaggedCache(self, TagSet(self, names))", "category": "Python"}, {"instruction": "def fin(self):\n        '''Indicate that this message is finished processing'''\n", "input": "", "output": "        self.connection.fin(self.id)\n        self.processed = True", "category": "Python"}, {"instruction": "def _get_max_sigma(self, R):\n        \"\"\"Calculate maximum sigma of scanner RAS coordinates\n\n        Parameters\n        ----------\n\n        R : 2D array, with shape [n_voxel, n_dim]\n            The coordinate matrix of fMRI data from one subject\n\n        Returns\n        -------\n\n        max_sigma : float\n            The maximum sigma of scanner coordinates.\n\n        \"\"\"\n", "input": "", "output": "\n        max_sigma = 2.0 * math.pow(np.nanmax(np.std(R, axis=0)), 2)\n        return max_sigma", "category": "Python"}, {"instruction": "def is_header(line):\n    \"\"\"\n    If a line has only one column, then it is a Section or Subsection\n    header.\n    :param line: Line to check\n    :return: boolean -If line is header\n    \"\"\"\n", "input": "", "output": "    if len(line) == 1:\n        return True\n    for idx, val in enumerate(line):\n        if idx > 0 and val:\n            return False\n    return True", "category": "Python"}, {"instruction": "def create_idea(self, *args, **kwargs):\n        \"\"\" :allowed_param: 'title', 'text', 'campaignId', 'tags' (optional), 'customFields' (optional)\n        \"\"\"\n", "input": "", "output": "        kwargs.update({'headers': {'content-type':'application/json'}})\n        return bind_api(\n            api=self,\n            path='/idea',\n            method='POST',\n            payload_type='idea',\n            post_param=['title', 'text', 'campaignId', 'tags', 'customFields']\n        )(*args, **kwargs)", "category": "Python"}, {"instruction": "def user_avatar_update(self, userid, payload):\n        ''' updated avatar by userid '''\n", "input": "", "output": "        response, status_code = self.__pod__.User.post_v1_admin_user_uid_avatar_update(\n            sessionToken=self.__session,\n            uid=userid,\n            payload=payload\n        ).result()\n        self.logger.debug('%s: %s' % (status_code, response))\n        return status_code, response", "category": "Python"}, {"instruction": "def MRS(self, params):\n        \"\"\"\n        MRS Rj, Rspecial\n\n        Copy the value of Rspecial to Rj\n        Rspecial can be APSR, IPSR, or EPSR\n        \"\"\"\n", "input": "", "output": "        Rj, Rspecial = self.get_two_parameters(self.TWO_PARAMETER_COMMA_SEPARATED, params)\n\n        self.check_arguments(LR_or_general_purpose_registers=(Rj,), special_registers=(Rspecial,))\n\n        def MRS_func():\n            # TODO add combination registers IEPSR, IAPSR, and EAPSR\n            # TODO needs to use APSR, IPSR, EPSR, IEPSR, IAPSR, EAPSR, PSR, MSP, PSP, PRIMASK, or CONTROL.\n            # http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.dui0553a/CHDBIBGJ.html\n            if Rspecial == 'PSR':\n                self.register[Rj] = self.register['APSR'] | self.register['IPSR'] | self.register['EPSR']\n            else:\n                self.register[Rj] = self.register[Rspecial]\n\n        return MRS_func", "category": "Python"}, {"instruction": "def get_project(self, project_name):\n        \"\"\"Return the project with a given name.\n\n        :param project_name: The name to search for.\n        :type project_name: str\n        :return: The project that has the name ``project_name`` or ``None``\n            if no project is found.\n        :rtype: :class:`pytodoist.todoist.Project`\n\n        >>> from pytodoist import todoist\n        >>> user = todoist.login('john.doe@gmail.com', 'password')\n        >>> project = user.get_project('Inbox')\n        >>> print(project.name)\n        Inbox\n        \"\"\"\n", "input": "", "output": "        for project in self.get_projects():\n            if project.name == project_name:\n                return project", "category": "Python"}, {"instruction": "def show_abierrors(self, nids=None, stream=sys.stdout):\n        \"\"\"\n        Write to the given stream the list of ABINIT errors for all tasks whose status is S_ABICRITICAL.\n\n        Args:\n            nids: optional list of node identifiers used to filter the tasks.\n            stream: File-like object. Default: sys.stdout\n        \"\"\"\n", "input": "", "output": "        lines = []\n        app = lines.append\n\n        for task in self.iflat_tasks(status=self.S_ABICRITICAL, nids=nids):\n            header = \"=== \" + task.qout_file.path + \"===\"\n            app(header)\n            report = task.get_event_report()\n\n            if report is not None:\n                app(\"num_errors: %s, num_warnings: %s, num_comments: %s\" % (\n                    report.num_errors, report.num_warnings, report.num_comments))\n                app(\"*** ERRORS ***\")\n                app(\"\\n\".join(str(e) for e in report.errors))\n                app(\"*** BUGS ***\")\n                app(\"\\n\".join(str(b) for b in report.bugs))\n\n            else:\n                app(\"get_envent_report returned None!\")\n\n            app(\"=\" * len(header) + 2*\"\\n\")\n\n        return stream.writelines(lines)", "category": "Python"}, {"instruction": "def from_stream(cls, stream, offset):\n        \"\"\"\n        Return a new |_IfdEntries| instance parsed from *stream* starting at\n        *offset*.\n        \"\"\"\n", "input": "", "output": "        ifd_parser = _IfdParser(stream, offset)\n        entries = dict((e.tag, e.value) for e in ifd_parser.iter_entries())\n        return cls(entries)", "category": "Python"}, {"instruction": "def scan(subtitles):\n    \"\"\"Remove advertising from subtitles.\"\"\"\n", "input": "", "output": "\n    from importlib.util import find_spec\n\n    try:\n        import subnuker\n    except ImportError:\n        fatal('Unable to scan subtitles. Please install subnuker.')\n\n    # check whether aeidon is available\n    aeidon = find_spec('aeidon') is not None\n\n    if sys.stdin.isatty():\n        # launch subnuker from the existing terminal\n        args = (['--aeidon'] if aeidon else []) + \\\n            ['--gui', '--regex'] + subtitles\n        subnuker.main(args)\n    else:\n        # launch subnuker from a new terminal\n        args = (['--aeidon'] if aeidon else []) + \\\n            ['--gui', '--regex']\n        execute(Config.TERMINAL,\n                '--execute',\n                'subnuker',\n                *args + subtitles)", "category": "Python"}, {"instruction": "def _xml_to_dict(xmltree):\n    '''\n    Convert an XML tree into a dict\n    '''\n", "input": "", "output": "    if sys.version_info < (2, 7):\n        children_len = len(xmltree.getchildren())\n    else:\n        children_len = len(xmltree)\n\n    if children_len < 1:\n        name = xmltree.tag\n        if '}' in name:\n            comps = name.split('}')\n            name = comps[1]\n        return {name: xmltree.text}\n\n    xmldict = {}\n    for item in xmltree:\n        name = item.tag\n        if '}' in name:\n            comps = name.split('}')\n            name = comps[1]\n        if name not in xmldict:\n            if sys.version_info < (2, 7):\n                children_len = len(item.getchildren())\n            else:\n                children_len = len(item)\n\n            if children_len > 0:\n                xmldict[name] = _xml_to_dict(item)\n            else:\n                xmldict[name] = item.text\n        else:\n            if not isinstance(xmldict[name], list):\n                tempvar = xmldict[name]\n                xmldict[name] = []\n                xmldict[name].append(tempvar)\n            xmldict[name].append(_xml_to_dict(item))\n    return xmldict", "category": "Python"}, {"instruction": "def calculate_concat_output_shapes(operator):\n    '''\n    Allowed input/output patterns are\n        1. [N_1, C, H, W], ..., [N_n, C, H, W] ---> [N_1 + ... + N_n, C, H, W]\n        2. [N, C_1, H, W], ..., [N, C_n, H, W] ---> [N, C_1 + ... + C_n, H, W]\n    '''\n", "input": "", "output": "    check_input_and_output_numbers(operator, input_count_range=[1, None], output_count_range=[1, 1])\n\n    output_shape = copy.deepcopy(operator.inputs[0].type.shape)\n    dims = []\n    for variable in operator.inputs:\n        if variable.type.shape[0] != 'None' and variable.type.shape[0] != output_shape[0]:\n            raise RuntimeError('Only dimensions along C-axis can be different')\n        if variable.type.shape[2] != 'None' and variable.type.shape[2] != output_shape[2]:\n            raise RuntimeError('Only dimensions along C-axis can be different')\n        if variable.type.shape[3] != 'None' and variable.type.shape[3] != output_shape[3]:\n            raise RuntimeError('Only dimensions along C-axis can be different')\n        dims.append(variable.type.shape[1])\n\n    output_shape[1] = 'None' if 'None' in dims else sum(dims)\n    operator.outputs[0].type.shape = output_shape", "category": "Python"}, {"instruction": "def target_str_to_list(target_str):\n    \"\"\" Parses a targets string into a list of individual targets. \"\"\"\n", "input": "", "output": "    new_list = list()\n    for target in target_str.split(','):\n        target = target.strip()\n        target_list = target_to_list(target)\n        if target_list:\n            new_list.extend(target_list)\n        else:\n            LOGGER.info(\"{0}: Invalid target value\".format(target))\n            return None\n    return list(collections.OrderedDict.fromkeys(new_list))", "category": "Python"}, {"instruction": "def _main():\n    \"\"\" Command line interface for testing.\n    \"\"\"\n", "input": "", "output": "    import pprint\n    import tempfile\n\n    try:\n        image = sys.argv[1]\n    except IndexError:\n        print(\"Usage: python -m pyrobase.webservice.imgur <url>\")\n    else:\n        try:\n            pprint.pprint(copy_image_from_url(image, cache_dir=tempfile.gettempdir()))\n        except UploadError as exc:\n            print(\"Upload error. %s\" % exc)", "category": "Python"}, {"instruction": "def _new_session(retry_timeout_config):\n        \"\"\"\n        Return a new `requests.Session` object.\n        \"\"\"\n", "input": "", "output": "        retry = requests.packages.urllib3.Retry(\n            total=None,\n            connect=retry_timeout_config.connect_retries,\n            read=retry_timeout_config.read_retries,\n            method_whitelist=retry_timeout_config.method_whitelist,\n            redirect=retry_timeout_config.max_redirects)\n        session = requests.Session()\n        session.mount('https://',\n                      requests.adapters.HTTPAdapter(max_retries=retry))\n        session.mount('http://',\n                      requests.adapters.HTTPAdapter(max_retries=retry))\n        return session", "category": "Python"}, {"instruction": "async def on_raw(self, message):\n        \"\"\" Handle a single message. \"\"\"\n", "input": "", "output": "        self.logger.debug('<< %s', message._raw)\n        if not message._valid:\n            self.logger.warning('Encountered strictly invalid IRC message from server: %s',\n                                message._raw)\n\n        if isinstance(message.command, int):\n            cmd = str(message.command).zfill(3)\n        else:\n            cmd = message.command\n\n        # Invoke dispatcher, if we have one.\n        method = 'on_raw_' + cmd.lower()\n        try:\n            # Set _top_level so __getattr__() can decide whether to return on_unknown or _ignored for unknown handlers.\n            # The reason for this is that features can always call super().on_raw_* safely and thus don't need to care for other features,\n            # while unknown messages for which no handlers exist at all are still logged.\n            self._handler_top_level = True\n            handler = getattr(self, method)\n            self._handler_top_level = False\n\n            await handler(message)\n        except:\n            self.logger.exception('Failed to execute %s handler.', method)", "category": "Python"}, {"instruction": "def shutdown(self, channel=Channel.CHANNEL_ALL, shutdown_hardware=True):\n        \"\"\"\n        Shuts down all CAN interfaces and/or the hardware interface.\n\n        :param int channel:\n            CAN channel, to be used (:data:`Channel.CHANNEL_CH0`, :data:`Channel.CHANNEL_CH1` or\n            :data:`Channel.CHANNEL_ALL`)\n        :param bool shutdown_hardware: If true then the hardware interface will be closed too.\n        \"\"\"\n", "input": "", "output": "        # shutdown each channel if it's initialized\n        for _channel, is_initialized in self._ch_is_initialized.items():\n            if is_initialized and (_channel == channel or channel == Channel.CHANNEL_ALL or shutdown_hardware):\n                UcanDeinitCanEx(self._handle, _channel)\n                self._ch_is_initialized[_channel] = False\n\n        # shutdown hardware\n        if self._hw_is_initialized and shutdown_hardware:\n            UcanDeinitHardware(self._handle)\n            self._hw_is_initialized = False\n            self._handle = Handle(INVALID_HANDLE)", "category": "Python"}, {"instruction": "def get_netG():\n    \"\"\"Get net G\"\"\"\n", "input": "", "output": "    # build the generator\n    netG = nn.Sequential()\n    with netG.name_scope():\n        # input is Z, going into a convolution\n        netG.add(nn.Conv2DTranspose(ngf * 8, 4, 1, 0, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf*8) x 4 x 4\n        netG.add(nn.Conv2DTranspose(ngf * 4, 4, 2, 1, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf*4) x 8 x 8\n        netG.add(nn.Conv2DTranspose(ngf * 2, 4, 2, 1, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf*2) x 16 x 16\n        netG.add(nn.Conv2DTranspose(ngf, 4, 2, 1, use_bias=False))\n        netG.add(nn.BatchNorm())\n        netG.add(nn.Activation('relu'))\n        # state size. (ngf) x 32 x 32\n        netG.add(nn.Conv2DTranspose(nc, 4, 2, 1, use_bias=False))\n        netG.add(nn.Activation('tanh'))\n        # state size. (nc) x 64 x 64\n\n    return netG", "category": "Python"}, {"instruction": "def stats(self):\n        \"\"\"\n        Database usage statistics. Used by quota.\n        \"\"\"\n", "input": "", "output": "        res = {}\n        db = self._collection.database\n        res['dbstats'] = db.command('dbstats')\n        res['data'] = db.command('collstats', self._collection.name)\n        res['totals'] = {'count': res['data']['count'],\n                         'size': res['data']['size']\n                         }\n        return res", "category": "Python"}, {"instruction": "def full(self, external=False):\n        '''Get the full image URL in respect with ``max_size``'''\n", "input": "", "output": "        return self.fs.url(self.filename, external=external) if self.filename else None", "category": "Python"}, {"instruction": "def _temporary_config():\n    '''Temporarily replace the global configuration.\n\n    Use this in a 'with' statement.  The inner block may freely manipulate\n    the global configuration; the original global configuration is restored\n    at exit.\n\n    >>> with yakonfig.yakonfig._temporary_config():\n    ...   yakonfig.yakonfig.set_global_config({'a': 'b'})\n    ...   print yakonfig.yakonfig.get_global_config('a')\n    b\n\n    '''\n", "input": "", "output": "    global _config_cache, _config_file_path\n    old_cc = _config_cache\n    old_cfp = _config_file_path\n    clear_global_config()\n    yield\n    _config_cache = old_cc\n    _config_file_path = old_cfp", "category": "Python"}, {"instruction": "def update_record(self, name, address, ttl=60):\n        \"\"\"Updates a record, creating it if not exists.\"\"\"\n", "input": "", "output": "        record_id = self._get_record(name)\n        if record_id is None:\n            return self._create_record(name, address, ttl)\n        return self._update_record(record_id, name, address, ttl)", "category": "Python"}, {"instruction": "def save_config(self, cmd=\"write mem\", confirm=False, confirm_response=\"\"):\n        \"\"\"Save config: write mem\"\"\"\n", "input": "", "output": "        return super(OneaccessOneOSBase, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )", "category": "Python"}, {"instruction": "def _get_normal_name(orig_enc):\n    \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n", "input": "", "output": "    # Only care about the first 12 characters.\n    enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n    if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n        return \"utf-8\"\n    if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or \\\n       enc.startswith((\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")):\n        return \"iso-8859-1\"\n    return orig_enc", "category": "Python"}, {"instruction": "def validate_and_convert_nexson(nexson, output_version, allow_invalid, **kwargs):\n    \"\"\"Runs the nexson validator and returns a converted 4 object:\n        nexson, annotation, validation_log, nexson_adaptor\n\n    `nexson` is the nexson dict.\n    `output_version` is the version of nexson syntax to be used after validation.\n    if `allow_invalid` is False, and the nexson validation has errors, then\n        a GitWorkflowError will be generated before conversion.\n    \"\"\"\n", "input": "", "output": "    try:\n        if TRACE_FILES:\n            _write_to_next_free('input', nexson)\n        annotation, validation_log, nexson_adaptor = ot_validate(nexson, **kwargs)\n        if TRACE_FILES:\n            _write_to_next_free('annotation', annotation)\n    except:\n        msg = 'exception in ot_validate: ' + traceback.format_exc()\n        raise GitWorkflowError(msg)\n    if (not allow_invalid) and validation_log.has_error():\n        raise GitWorkflowError('ot_validation failed: ' + json.dumps(annotation))\n    nexson = convert_nexson_format(nexson, output_version)\n    if TRACE_FILES:\n        _write_to_next_free('converted', nexson)\n    return nexson, annotation, validation_log, nexson_adaptor", "category": "Python"}, {"instruction": "def shards(self, index=None, params=None):\n        \"\"\"\n        The shards command is the detailed view of what nodes contain which shards.\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/cat-shards.html>`_\n\n        :arg index: A comma-separated list of index names to limit the returned\n            information\n        :arg bytes: The unit in which to display byte values, valid choices are:\n            'b', 'k', 'kb', 'm', 'mb', 'g', 'gb', 't', 'tb', 'p', 'pb'\n        :arg format: a short version of the Accept header, e.g. json, yaml\n        :arg h: Comma-separated list of column names to display\n        :arg help: Return help information, default False\n        :arg local: Return local information, do not retrieve the state from\n            master node (default: false)\n        :arg master_timeout: Explicit operation timeout for connection to master\n            node\n        :arg s: Comma-separated list of column names or column aliases to sort\n            by\n        :arg v: Verbose mode. Display column headers, default False\n        \"\"\"\n", "input": "", "output": "        return self.transport.perform_request('GET', _make_path('_cat',\n            'shards', index), params=params)", "category": "Python"}, {"instruction": "def task_add(self, description, tags=None, **kw):\n        \"\"\" Add a new task.\n\n        Takes any of the keywords allowed by taskwarrior like proj or prior.\n        \"\"\"\n", "input": "", "output": "\n        task = self._stub_task(description, tags, **kw)\n\n        task['status'] = Status.PENDING\n\n        # TODO -- check only valid keywords\n\n        if not 'entry' in task:\n            task['entry'] = str(int(time.time()))\n\n        if not 'uuid' in task:\n            task['uuid'] = str(uuid.uuid4())\n\n        id = self._task_add(task, Status.PENDING)\n        task['id'] = id\n        return task", "category": "Python"}, {"instruction": "def getTotal(self):\n        \"\"\" Compute TotalPrice \"\"\"\n", "input": "", "output": "        total = 0\n        for lineitem in self.supplyorder_lineitems:\n            total += Decimal(lineitem['Quantity']) * \\\n                     Decimal(lineitem['Price']) *  \\\n                     ((Decimal(lineitem['VAT']) /100) + 1)\n        return total", "category": "Python"}, {"instruction": "def calc_tr(calc_fn, *args, **kwargs):\n    '''\n    calc_tr(calc_fn, ...) yields a copy of calc_fn in which the afferent and efferent values of the\n      function have been translated. The translation is found from merging the list of 0 or more\n      dictionary arguments given left-to-right followed by the keyword arguments. If the calc_fn\n      that is given is not a @calc function explicitly, calc_tr will attempt to coerce it to one.\n    '''\n", "input": "", "output": "    if not is_calc(calc_fn):\n        calc_fn = calc(calc_fn)\n    return calc_fn.tr(*args, **kwargs)", "category": "Python"}, {"instruction": "def translate_word(word, dictionary=['simplified']):\n    '''\n    Return the set of translations for a single character or word, if\n    available.\n    '''\n", "input": "", "output": "    if not dictionaries:\n        init()\n    for d in dictionary:\n        if word in dictionaries[d]:\n            return dictionaries[d][word]\n    return None", "category": "Python"}, {"instruction": "def price_dataframe(symbols=('sne',),\n    start=datetime.datetime(2008, 1, 1),\n    end=datetime.datetime(2009, 12, 31),\n    price_type='actual_close',\n    cleaner=util.clean_dataframe,\n    ):\n    \"\"\"Retrieve the prices of a list of equities as a DataFrame (columns = symbols)\n\n    Arguments:\n      symbols (list of str): Ticker symbols like \"GOOG\", \"AAPL\", etc\n        e.g. [\"AAPL\", \" slv \", GLD\", \"GOOG\", \"$SPX\", \"XOM\", \"msft\"]\n      start (datetime): The date at the start of the period being analyzed.\n      end (datetime): The date at the end of the period being analyzed.\n        Yahoo data stops at 2013/1/1\n    \"\"\"\n", "input": "", "output": "    if isinstance(price_type, basestring):\n        price_type = [price_type]\n    start = nlp.util.normalize_date(start or datetime.date(2008, 1, 1))\n    end = nlp.util.normalize_date(end or datetime.date(2009, 12, 31))\n    symbols = util.make_symbols(symbols)\n    df = get_dataframes(symbols)\n    # t = du.getNYSEdays(start, end, datetime.timedelta(hours=16))\n    # df = clean_dataframes(dataobj.get_data(t, symbols, price_type))\n    if not df or len(df) > 1:\n        return cleaner(df)\n    else:\n        return cleaner(df[0])", "category": "Python"}, {"instruction": "def pubsub_pop_message(self, deadline=None):\n        \"\"\"Pops a message for a subscribed client.\n\n        Args:\n            deadline (int): max number of seconds to wait (None => no timeout)\n\n        Returns:\n            Future with the popped message as result (or None if timeout\n                or ConnectionError object in case of connection errors\n                or ClientError object if you are not subscribed)\n        \"\"\"\n", "input": "", "output": "        if not self.subscribed:\n            excep = ClientError(\"you must subscribe before using \"\n                                \"pubsub_pop_message\")\n            raise tornado.gen.Return(excep)\n        reply = None\n        try:\n            reply = self._reply_list.pop(0)\n            raise tornado.gen.Return(reply)\n        except IndexError:\n            pass\n        if deadline is not None:\n            td = timedelta(seconds=deadline)\n            yield self._condition.wait(timeout=td)\n        else:\n            yield self._condition.wait()\n        try:\n            reply = self._reply_list.pop(0)\n        except IndexError:\n            pass\n        raise tornado.gen.Return(reply)", "category": "Python"}, {"instruction": "def gets(self):\n        \"\"\"\n        Read line from stdin.\n\n        The trailing newline will be omitted.\n        :return: string:\n        \"\"\"\n", "input": "", "output": "        ret = self.stdin.readline()\n        if ret == '':\n            raise EOFError  # To break out of EOF loop\n        return ret.rstrip('\\n')", "category": "Python"}, {"instruction": "def cli(env, sortby, columns, datacenter, username, storage_type):\n    \"\"\"List file storage.\"\"\"\n", "input": "", "output": "    file_manager = SoftLayer.FileStorageManager(env.client)\n    file_volumes = file_manager.list_file_volumes(datacenter=datacenter,\n                                                  username=username,\n                                                  storage_type=storage_type,\n                                                  mask=columns.mask())\n\n    table = formatting.Table(columns.columns)\n    table.sortby = sortby\n\n    for file_volume in file_volumes:\n        table.add_row([value or formatting.blank()\n                       for value in columns.row(file_volume)])\n\n    env.fout(table)", "category": "Python"}, {"instruction": "def _dataset_merge_filestore_resource(self, resource, updated_resource, filestore_resources, ignore_fields):\n        # type: (hdx.data.Resource, hdx.data.Resource, List[hdx.data.Resource], List[str]) -> None\n        \"\"\"Helper method to merge updated resource from dataset into HDX resource read from HDX including filestore.\n\n        Args:\n            resource (hdx.data.Resource): Resource read from HDX\n            updated_resource (hdx.data.Resource): Updated resource from dataset\n            filestore_resources (List[hdx.data.Resource]): List of resources that use filestore (to be appended to)\n            ignore_fields (List[str]): List of fields to ignore when checking resource\n\n        Returns:\n            None\n        \"\"\"\n", "input": "", "output": "        if updated_resource.get_file_to_upload():\n            resource.set_file_to_upload(updated_resource.get_file_to_upload())\n            filestore_resources.append(resource)\n        merge_two_dictionaries(resource, updated_resource)\n        resource.check_required_fields(ignore_fields=ignore_fields)\n        if resource.get_file_to_upload():\n            resource['url'] = Dataset.temporary_url", "category": "Python"}, {"instruction": "def unlock(self):\r\n        \"\"\"\r\n        Release this lock.\r\n\r\n        This deletes the directory with the given name.\r\n\r\n        @raise: Any exception os.readlink() may raise, or\r\n        ValueError if the lock is not owned by this process.\r\n        \"\"\"\n", "input": "", "output": "        pid = readlink(self.name)\r\n        if int(pid) != os.getpid():\r\n            raise ValueError(\"Lock %r not owned by this process\" % (self.name,))\r\n        rmlink(self.name)\r\n        self.locked = False", "category": "Python"}, {"instruction": "def check_internal_ip(request):\n    \"\"\" request is an AsgiRequest \"\"\"\n", "input": "", "output": "    remote_addr = (request.META[\"HTTP_X_FORWARDED_FOR\"] if \"HTTP_X_FORWARDED_FOR\" in request.META else request.META.get(\"REMOTE_ADDR\", \"\"))\n    return remote_addr in settings.INTERNAL_IPS", "category": "Python"}, {"instruction": "def create_app_id(self, app_id, policies, display_name=None, mount_point='app-id', **kwargs):\n        \"\"\"POST /auth/<mount point>/map/app-id/<app_id>\n\n        :param app_id:\n        :type app_id:\n        :param policies:\n        :type policies:\n        :param display_name:\n        :type display_name:\n        :param mount_point:\n        :type mount_point:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "\n        # app-id can have more than 1 policy. It is easier for the user to pass in the\n        # policies as a list so if they do, we need to convert to a , delimited string.\n        if isinstance(policies, (list, set, tuple)):\n            policies = ','.join(policies)\n\n        params = {\n            'value': policies\n        }\n\n        # Only use the display_name if it has a value. Made it a named param for user\n        # convienence instead of leaving it as part of the kwargs\n        if display_name:\n            params['display_name'] = display_name\n\n        params.update(kwargs)\n\n        return self._adapter.post('/v1/auth/{}/map/app-id/{}'.format(mount_point, app_id), json=params)", "category": "Python"}, {"instruction": "def run(self, command, **kwargs):\n        \"\"\"Run a command on the remote host.\n\n        This is just a wrapper around ``RemoteTask(self.hostname, ...)``\n        \"\"\"\n", "input": "", "output": "        return RemoteTask(self.hostname, command,\n                          identity_file=self._identity_file, **kwargs)", "category": "Python"}, {"instruction": "def delete_secret(self, path, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Delete the secret at the specified location.\n\n        Supported methods:\n            DELETE: /{mount_point}/{path}. Produces: 204 (empty body)\n\n\n        :param path: Specifies the path of the secret to delete.\n            This is specified as part of the URL.\n        :type path: str | unicode\n        :param mount_point: The \"path\" the secret engine was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the delete_secret request.\n        :rtype: requests.Response\n        \"\"\"\n", "input": "", "output": "        api_path = '/v1/{mount_point}/{path}'.format(mount_point=mount_point, path=path)\n        return self._adapter.delete(\n            url=api_path,\n        )", "category": "Python"}, {"instruction": "def replace_uuid_w_names(self, resp):\n        \"\"\" Replace the uuid's with names.\n\n        Parameters\n        ----------\n        resp     : ???\n            ???\n\n        Returns\n        -------\n        ???\n            ???\n\n        \"\"\"\n", "input": "", "output": "        \n        col_mapper = self.get_point_name(resp.context)[\"?point\"].to_dict()\n        resp.df.rename(columns=col_mapper, inplace=True)\n        return resp", "category": "Python"}, {"instruction": "def webhook_handler(request):\n    \"\"\"Receives the webhook from mbed cloud services\n\n    Passes the raw http body directly to mbed sdk, to notify that a webhook was received\n    \"\"\"\n", "input": "", "output": "    body = request.stream.read().decode('utf8')\n    print('webhook handler saw:', body)\n    api.notify_webhook_received(payload=body)\n\n    # nb. protected references are not part of the API.\n    # this is just to demonstrate that the asyncid is stored\n    print('key store contains:', api._db.keys())", "category": "Python"}, {"instruction": "def insertOrderedList(self):\r\n        \"\"\"\r\n        Inserts an ordered list into the editor.\r\n        \"\"\"\n", "input": "", "output": "        cursor = self.editor().textCursor()\r\n        currlist = cursor.currentList()\r\n        new_style = QTextListFormat.ListDecimal\r\n        indent = 1\r\n        \r\n        if currlist:\r\n            format = currlist.format()\r\n            indent = format.indent() + 1\r\n            style  = format.style()\r\n            \r\n            if style == QTextListFormat.ListDecimal:\r\n                new_style = QTextListFormat.ListLowerRoman\r\n            elif style == QTextListFormat.ListLowerRoman:\r\n                new_style = QTextListFormat.ListUpperAlpha\r\n            elif style == QTextListFormat.ListUpperAlpha:\r\n                new_style = QTextListFormat.ListLowerAlpha\r\n        \r\n        new_format = QTextListFormat()\r\n        new_format.setStyle(new_style)\r\n        new_format.setIndent(indent)\r\n        new_list = cursor.createList(new_format)\r\n        \r\n        self.editor().setFocus()\r\n        \r\n        return new_list", "category": "Python"}, {"instruction": "def client_factory(self):\n        \"\"\"\n        Custom client factory to set proxy options.\n        \"\"\"\n", "input": "", "output": "        \n        if self._service.production:\n            url = self.production_url\n        else:\n            url = self.testing_url\n        \n        proxy_options = dict()\n        https_proxy_setting = os.environ.get('PAYEX_HTTPS_PROXY') or os.environ.get('https_proxy')\n        http_proxy_setting = os.environ.get('PAYEX_HTTP_PROXY') or os.environ.get('http_proxy')\n        \n        if https_proxy_setting:\n            proxy_options['https'] = https_proxy_setting\n        if http_proxy_setting:\n            proxy_options['http'] = http_proxy_setting\n        \n        return client.Client(url, proxy=proxy_options)", "category": "Python"}, {"instruction": "def toggle_white_spaces(self):\n        \"\"\"\n        Toggles document white spaces display.\n\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        text_option = self.get_default_text_option()\n        if text_option.flags().__int__():\n            text_option = QTextOption()\n            text_option.setTabStop(self.tabStopWidth())\n        else:\n            text_option.setFlags(\n                text_option.flags() | QTextOption.ShowTabsAndSpaces | QTextOption.ShowLineAndParagraphSeparators)\n        self.set_default_text_option(text_option)\n        return True", "category": "Python"}, {"instruction": "def get_urlpatterns(self):\n        \"\"\" Returns the URL patterns managed by the considered factory / application. \"\"\"\n", "input": "", "output": "        return [\n            path(_('topics/'), self.latest_topics_feed(), name='latest_topics'),\n            path(\n                _('forum/<str:forum_slug>-<int:forum_pk>/topics/'),\n                self.latest_topics_feed(),\n                name='forum_latest_topics',\n            ),\n            path(\n                _('forum/<str:forum_slug>-<int:forum_pk>/topics/all/'),\n                self.latest_topics_feed(),\n                {'descendants': True},\n                name='forum_latest_topics_with_descendants',\n            ),\n        ]", "category": "Python"}, {"instruction": "def _on_disconnected(self, device):\n        \"\"\"Callback function called when a disconnected event has been received.\n        This resets any open interfaces on the virtual device and clears any\n        in progress traces and streams.\n        It is executed in the baBLE working thread: should not be blocking.\n\n        Args:\n            device (dict): Information about the newly connected device\n        \"\"\"\n", "input": "", "output": "\n        self._logger.debug(\"Device disconnected event: {}\".format(device))\n\n        if self.streaming:\n            self.device.close_streaming_interface()\n            self.streaming = False\n\n        if self.tracing:\n            self.device.close_tracing_interface()\n            self.tracing = False\n\n        self.device.connected = False\n        self.connected = False\n        self._connection_handle = 0\n        self.header_notif = False\n        self.payload = False\n\n        self._clear_reports()\n        self._clear_traces()\n\n        self._defer(self.set_advertising, [True])\n        self._audit('ClientDisconnected')", "category": "Python"}, {"instruction": "def find_model(config, obj, mods):\n    \"\"\"Given a list of mods (as returned by py_resources) attempts to\n    determine if a given Python obj fits one of the models\"\"\"\n", "input": "", "output": "    for mod in mods:\n        if mod[0] != config:\n            continue\n\n        if len(mod) == 2:\n            return mod[1]\n\n        if len(mod) == 3 and mod[1] in obj:\n            return mod[2]\n\n    return None", "category": "Python"}, {"instruction": "def instruction_NEG_memory(self, opcode, ea, m):\n        \"\"\" Negate memory \"\"\"\n", "input": "", "output": "        if opcode == 0x0 and ea == 0x0 and m == 0x0:\n            self._wrong_NEG += 1\n            if self._wrong_NEG > 10:\n                raise RuntimeError(\"Wrong PC ???\")\n        else:\n            self._wrong_NEG = 0\n\n        r = m * -1 # same as: r = ~m + 1\n\n#        log.debug(\"$%04x NEG $%02x from %04x to $%02x\" % (\n#             self.program_counter, m, ea, r,\n#         ))\n        self.clear_NZVC()\n        self.update_NZVC_8(0, m, r)\n        return ea, r & 0xff", "category": "Python"}, {"instruction": "def _bind_length_scalar_handlers(tids, scalar_factory, lns=_NON_ZERO_LENGTH_LNS):\n    \"\"\"Binds a set of scalar handlers for an inclusive range of low-nibble values.\n\n    Args:\n        tids (Sequence[int]): The Type IDs to bind to.\n        scalar_factory (Callable): The factory for the scalar parsing function.\n            This function can itself return a function representing a thunk to defer the\n            scalar parsing or a direct value.\n        lns (Sequence[int]): The low-nibble lengths to bind to.\n    \"\"\"\n", "input": "", "output": "    handler = partial(_length_scalar_handler, scalar_factory)\n    return _bind_length_handlers(tids, handler, lns)", "category": "Python"}, {"instruction": "def _from_directory(self, dirname, prefix=\"\"):\n        \"\"\"Add dataset from files in a directory.\n\n        Parameters\n        ----------\n        dirname : string\n            Directory name.\n        prefix : string\n            Prefix.\n        \"\"\"\n", "input": "", "output": "        ps = [os.path.join(here, dirname, p) for p in os.listdir(os.path.join(here, dirname))]\n        n = prefix + wt_kit.string2identifier(os.path.basename(dirname))\n        setattr(self, n, ps)", "category": "Python"}, {"instruction": "def _parse_raw_bytes(raw_bytes):\n    \"\"\"Convert a string of hexadecimal values to decimal values parameters\n\n    Example: '0x2E 0xF1 0x80 0x28 0x00 0x1A 0x01 0x00' is converted to:\n              46, 241, [128, 40, 0, 26, 1, 0]\n\n    :param raw_bytes: string of hexadecimal values\n    :returns: 3 decimal values\n    \"\"\"\n", "input": "", "output": "    bytes_list = [int(x, base=16) for x in raw_bytes.split()]\n    return bytes_list[0], bytes_list[1], bytes_list[2:]", "category": "Python"}, {"instruction": "def sync_tools(\n            self, all_=False, destination=None, dry_run=False, public=False,\n            source=None, stream=None, version=None):\n        \"\"\"Copy Juju tools into this model.\n\n        :param bool all_: Copy all versions, not just the latest\n        :param str destination: Path to local destination directory\n        :param bool dry_run: Don't do the actual copy\n        :param bool public: Tools are for a public cloud, so generate mirrors\n            information\n        :param str source: Path to local source directory\n        :param str stream: Simplestreams stream for which to sync metadata\n        :param str version: Copy a specific major.minor version\n\n        \"\"\"\n", "input": "", "output": "        raise NotImplementedError()", "category": "Python"}, {"instruction": "def make_interval(long_name, short_name):\n    \"\"\" Create an interval segment \"\"\"\n", "input": "", "output": "    return Group(\n        Regex(\"(-+)?[0-9]+\")\n        + (\n            upkey(long_name + \"s\")\n            | Regex(long_name + \"s\").setParseAction(upcaseTokens)\n            | upkey(long_name)\n            | Regex(long_name).setParseAction(upcaseTokens)\n            | upkey(short_name)\n            | Regex(short_name).setParseAction(upcaseTokens)\n        )\n    ).setResultsName(long_name)", "category": "Python"}, {"instruction": "def compile(self, prog, features=Features.ALL):\n        \"\"\"Currently this compiler simply returns an interpreter instead of compiling\n        TODO: Write this compiler to increase LPProg run speed and to prevent exceeding maximum recursion depth\n\n        Args:\n            prog (str): A string containing the program.\n            features (FeatureSet): The set of features to enable during compilation.\n\n        Returns:\n            LPProg\n        \"\"\"\n", "input": "", "output": "        return LPProg(Parser(Tokenizer(prog, features), features).program(), features)", "category": "Python"}, {"instruction": "def categories(self):\n        \"\"\"List[:class:`CategoryChannel`]: A list of categories that belongs to this guild.\n\n        This is sorted by the position and are in UI order from top to bottom.\n        \"\"\"\n", "input": "", "output": "        r = [ch for ch in self._channels.values() if isinstance(ch, CategoryChannel)]\n        r.sort(key=lambda c: (c.position, c.id))\n        return r", "category": "Python"}, {"instruction": "def switch_delete_record_for_userid(self, userid):\n        \"\"\"Remove userid switch record from switch table.\"\"\"\n", "input": "", "output": "        with get_network_conn() as conn:\n            conn.execute(\"DELETE FROM switch WHERE userid=?\",\n                         (userid,))\n            LOG.debug(\"Switch record for user %s is removed from \"\n                      \"switch table\" % userid)", "category": "Python"}, {"instruction": "def _edits1(word: str) -> Set[str]:\n    \"\"\"\n    Return a set of words with edit distance of 1 from the input word\n    \"\"\"\n", "input": "", "output": "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n    deletes = [L + R[1:] for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n    replaces = [L + c + R[1:] for L, R in splits if R for c in thai_letters]\n    inserts = [L + c + R for L, R in splits for c in thai_letters]\n\n    return set(deletes + transposes + replaces + inserts)", "category": "Python"}, {"instruction": "def add(self, key, val, minutes):\n        \"\"\"\n        Store an item in the cache if it does not exist.\n\n        :param key: The cache key\n        :type key: str\n\n        :param val: The cache value\n        :type val: mixed\n\n        :param minutes: The lifetime in minutes of the cached value\n        :type minutes: int\n\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        return self._memcache.add(self._prefix + key, val, minutes * 60)", "category": "Python"}, {"instruction": "def _Open(self, path_spec, mode='rb'):\n    \"\"\"Opens the file system object defined by path specification.\n\n    Args:\n      path_spec (PathSpec): a path specification.\n      mode (Optional[str]): file access mode. The default is 'rb' which\n          represents read-only binary.\n\n    Raises:\n      AccessError: if the access to open the file was denied.\n      IOError: if the file system object could not be opened.\n      PathSpecError: if the path specification is incorrect.\n      ValueError: if the path specification is invalid.\n    \"\"\"\n", "input": "", "output": "    if not path_spec.HasParent():\n      raise errors.PathSpecError(\n          'Unsupported path specification without parent.')\n\n    file_object = resolver.Resolver.OpenFileObject(\n        path_spec.parent, resolver_context=self._resolver_context)\n\n    try:\n      tsk_image_object = tsk_image.TSKFileSystemImage(file_object)\n      tsk_volume = pytsk3.Volume_Info(tsk_image_object)\n    except:\n      file_object.close()\n      raise\n\n    self._file_object = file_object\n    self._tsk_volume = tsk_volume", "category": "Python"}, {"instruction": "def setup_icons(self, ):\n        \"\"\"Set all icons on buttons\n\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        plus_icon = get_icon('glyphicons_433_plus_bright.png', asicon=True)\n        self.addnew_tb.setIcon(plus_icon)", "category": "Python"}, {"instruction": "def set_avatar(self, avatar, subject_descriptor):\n        \"\"\"SetAvatar.\n        [Preview API]\n        :param :class:`<Avatar> <azure.devops.v5_1.graph.models.Avatar>` avatar:\n        :param str subject_descriptor:\n        \"\"\"\n", "input": "", "output": "        route_values = {}\n        if subject_descriptor is not None:\n            route_values['subjectDescriptor'] = self._serialize.url('subject_descriptor', subject_descriptor, 'str')\n        content = self._serialize.body(avatar, 'Avatar')\n        self._send(http_method='PUT',\n                   location_id='801eaf9c-0585-4be8-9cdb-b0efa074de91',\n                   version='5.1-preview.1',\n                   route_values=route_values,\n                   content=content)", "category": "Python"}, {"instruction": "def element_statistics(tree, element_type):\n    \"\"\"\n    Prints the names and counts of all elements present in an\n    `etree._ElementTree`, e.g. a SaltDocument::\n\n        SStructure: 65\n        SSpan: 32\n        SToken: 154\n        STextualDS: 1\n\n    Parameters\n    ----------\n    tree : lxml.etree._ElementTree\n        an ElementTree that represents a complete SaltXML document\n    element_type : str\n        an XML tag, e.g. 'nodes', 'edges', 'labels'\n    \"\"\"\n", "input": "", "output": "    elements = get_elements(tree, element_type)\n    stats = defaultdict(int)\n    for i, element in enumerate(elements):\n        stats[get_xsi_type(element)] += 1\n    for (etype, count) in stats.items():\n        print \"{0}: {1}\".format(etype, count)", "category": "Python"}, {"instruction": "def profile_option(f):\n    \"\"\"\n    Configures --profile option for CLI\n\n    :param f: Callback Function to be passed to Click\n    \"\"\"\n", "input": "", "output": "    def callback(ctx, param, value):\n        state = ctx.ensure_object(Context)\n        state.profile = value\n        return value\n\n    return click.option('--profile',\n                        expose_value=False,\n                        help='Select a specific profile from your credential file to get AWS credentials.',\n                        callback=callback)(f)", "category": "Python"}, {"instruction": "def getVideoStreamTextureGL(self, hTrackedCamera, eFrameType, nFrameHeaderSize):\n        \"\"\"Access a shared GL texture for the specified tracked camera stream\"\"\"\n", "input": "", "output": "\n        fn = self.function_table.getVideoStreamTextureGL\n        pglTextureId = glUInt_t()\n        pFrameHeader = CameraVideoStreamFrameHeader_t()\n        result = fn(hTrackedCamera, eFrameType, byref(pglTextureId), byref(pFrameHeader), nFrameHeaderSize)\n        return result, pglTextureId, pFrameHeader", "category": "Python"}, {"instruction": "def setLevel(self, level):\n        r\"\"\"Overrides the parent method to adapt the formatting string to the level.\n        \n        Parameters\n        ----------\n        level : int\n            The new log level to set. See the logging levels in the logging module for details.\n            \n        Examples\n        --------\n        >>> import logging\n        >>> Logger.setLevel(logging.DEBUG)\n        \"\"\"\n", "input": "", "output": "        if logging.DEBUG >= level:\n            formatter = logging.Formatter(\"%(asctime)s [%(levelname)-8s] %(message)s (in %(module)s.%(funcName)s:%(lineno)s)\", \n                                          \"%d.%m.%Y %H:%M:%S\") \n            self._handler.setFormatter(formatter)\n        else:\n            formatter = logging.Formatter(\"%(asctime)s [%(levelname)-8s] %(message)s\", \n                                          \"%d.%m.%Y %H:%M:%S\") \n            self._handler.setFormatter(formatter)\n            \n        NativeLogger.setLevel(self, level)", "category": "Python"}, {"instruction": "def do_check(vext_files):\n    \"\"\"\n    Attempt to import everything in the 'test-imports' section of specified\n    vext_files\n\n    :param: list of vext filenames (without paths), '*' matches all.\n    :return: True if test_imports was successful from all files\n    \"\"\"\n", "input": "", "output": "    import vext\n    # not efficient ... but then there shouldn't be many of these\n\n    all_specs = set(vext.gatekeeper.spec_files_flat())\n    if vext_files == ['*']:\n        vext_files = all_specs\n    unknown_specs = set(vext_files) - all_specs\n    for fn in unknown_specs:\n        print(\"%s is not an installed vext file.\" % fn, file=sys.stderr)\n\n    if unknown_specs:\n        return False\n\n    check_passed = True\n    for fn in [join(vext.gatekeeper.spec_dir(), fn) for fn in vext_files]:\n        f = open_spec(open(fn))\n        modules = f.get('test_import', [])\n        for success, module in vext.gatekeeper.test_imports(modules):\n            if not success:\n                check_passed = False\n            line = \"import %s: %s\" % (module, '[success]' if success else '[failed]')\n            print(line)\n        print('')\n\n    return check_passed", "category": "Python"}, {"instruction": "def df_to_html(df, percentage_columns=None):  # pragma: no cover\n    \"\"\"Return a nicely formatted HTML code string for the given dataframe.\n\n    Arguments\n    ---------\n    df : pandas.DataFrame\n        A dataframe object.\n    percentage_columns : iterable\n        A list of cloumn names to be displayed with a percentage sign.\n\n    Returns\n    -------\n    str\n        A nicely formatted string for the given dataframe.\n    \"\"\"\n", "input": "", "output": "    big_dataframe_setup()\n    try:\n        res = '<br><h2> {} </h2>'.format(df.name)\n    except AttributeError:\n        res = ''\n    df.style.set_properties(**{'text-align': 'center'})\n    res += df.to_html(formatters=_formatters_dict(\n        input_df=df,\n        percentage_columns=percentage_columns\n    ))\n    res += '<br>'\n    return res", "category": "Python"}, {"instruction": "def Title(self):\n        \"\"\"Return the name of the Organisation\n        \"\"\"\n", "input": "", "output": "        field = self.getField(\"Name\")\n        field = field and field.get(self) or \"\"\n        return safe_unicode(field).encode(\"utf-8\")", "category": "Python"}, {"instruction": "def parse_cfg(self):\n        \"\"\" parses the given config file for experiments. \"\"\"\n", "input": "", "output": "        self.cfgparser = ConfigParser()\n        if not self.cfgparser.read(self.options.config):\n            raise SystemExit('config file %s not found.'%self.options.config) \n\n        # Change the current working directory to be relative to 'experiments.cfg'\n        projectDir = os.path.dirname(self.options.config)\n        projectDir = os.path.abspath(projectDir)\n        os.chdir(projectDir)", "category": "Python"}, {"instruction": "def __onMouseButtonEvent(self, event=None):\n        \"\"\" general mouse press/release events. Here, event is\n        a MplEvent from matplotlib.  This routine just dispatches\n        to the appropriate onLeftDown, onLeftUp, onRightDown, onRightUp....\n        methods.\n        \"\"\"\n", "input": "", "output": "        if event is None:\n            return\n        button = event.button or 1\n\n        handlers = {(1, 'button_press_event'):   self.onLeftDown,\n                    (1, 'button_release_event'): self.onLeftUp,\n                    (3, 'button_press_event'):   self.onRightDown,\n                    }\n        # (3,'button_release_event'): self.onRightUp}\n\n        handle_event = handlers.get((button, event.name), None)\n        if hasattr(handle_event, '__call__'):\n            handle_event(event)\n        event.guiEvent.Skip()", "category": "Python"}, {"instruction": "def suffix(args):\n    \"\"\"\n    %prog suffix fastqfile CAG\n\n    Filter reads based on suffix.\n    \"\"\"\n", "input": "", "output": "    p = OptionParser(suffix.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 2:\n        sys.exit(not p.print_help())\n\n    fastqfile, sf = args\n    fw = must_open(opts.outfile, \"w\")\n    nreads = nselected = 0\n    for rec in iter_fastq(fastqfile):\n        nreads += 1\n        if rec is None:\n            break\n        if rec.seq.endswith(sf):\n            print(rec, file=fw)\n            nselected += 1\n    logging.debug(\"Selected reads with suffix {0}: {1}\".\\\n                  format(sf, percentage(nselected, nreads)))", "category": "Python"}, {"instruction": "def allowed_actions(self):\n        \"\"\"\n        Shorthand to access :attr:`~.xso.Actions.allowed_actions` of the\n        :attr:`response`.\n\n        If no response has been received yet or if the response specifies no\n        set of valid actions, this is the minimal set of allowed actions (\n        :attr:`~.ActionType.EXECUTE` and :attr:`~.ActionType.CANCEL`).\n        \"\"\"\n", "input": "", "output": "\n        if self._response is not None and self._response.actions is not None:\n            return self._response.actions.allowed_actions\n        return {adhoc_xso.ActionType.EXECUTE,\n                adhoc_xso.ActionType.CANCEL}", "category": "Python"}, {"instruction": "def rapid_to_gssha(self):\n        \"\"\"\n        Prepare RAPID data for simulation\n        \"\"\"\n", "input": "", "output": "        # if no streamflow given, download forecast\n        if self.path_to_rapid_qout is None and self.connection_list_file:\n            rapid_qout_directory = os.path.join(self.gssha_directory, 'rapid_streamflow')\n            try:\n                os.mkdir(rapid_qout_directory)\n            except OSError:\n                pass\n            self.path_to_rapid_qout = self.download_spt_forecast(rapid_qout_directory)\n\n        # prepare input for GSSHA if user wants\n        if self.path_to_rapid_qout is not None and self.connection_list_file:\n            self.event_manager.prepare_rapid_streamflow(self.path_to_rapid_qout,\n                                                        self.connection_list_file)\n            self.simulation_modified_input_cards.append('CHAN_POINT_INPUT')", "category": "Python"}, {"instruction": "def fade_speed(self, value):\n        \"\"\"\n        Setter for **self.__fade_speed** attribute.\n\n        :param value: Attribute value.\n        :type value: float\n        \"\"\"\n", "input": "", "output": "\n        if value is not None:\n            assert type(value) is float, \"'{0}' attribute: '{1}' type is not 'float'!\".format(\"fade_speed\", value)\n            assert value >= 0, \"'{0}' attribute: '{1}' need to be exactly positive!\".format(\"fade_speed\", value)\n        self.__fade_speed = value", "category": "Python"}, {"instruction": "def delete_candidate(self, candidate):\n        \"\"\"Delete a CandidateElection.\"\"\"\n", "input": "", "output": "        CandidateElection.objects.filter(\n            candidate=candidate, election=self\n        ).delete()", "category": "Python"}, {"instruction": "def add_comment(repo: GithubRepository, pull_id: int, text: str) -> None:\n    \"\"\"\n    References:\n        https://developer.github.com/v3/issues/comments/#create-a-comment\n    \"\"\"\n", "input": "", "output": "    url = (\"https://api.github.com/repos/{}/{}/issues/{}/comments\"\n           \"?access_token={}\".format(repo.organization,\n                                     repo.name,\n                                     pull_id,\n                                     repo.access_token))\n    data = {\n        'body': text\n    }\n    response = requests.post(url, json=data)\n\n    if response.status_code != 201:\n        raise RuntimeError('Add comment failed. Code: {}. Content: {}.'.format(\n            response.status_code, response.content))", "category": "Python"}, {"instruction": "def update(self, campaign_id, search_channels, nonsearch_channels, outside_discount, nick=None):\n        '''xxxxx.xxxxx.campaign.platform.update\n        ===================================\n        \u53d6\u5f97\u4e00\u4e2a\u63a8\u5e7f\u8ba1\u5212\u7684\u6295\u653e\u5e73\u53f0\u8bbe\u7f6e'''\n", "input": "", "output": "        request = TOPRequest('xxxxx.xxxxx.campaign.platform.update')\n        request['campaign_id'] = campaign_id\n        request['search_channels'] = search_channels\n        request['nonsearch_channels'] = nonsearch_channels\n        request['outside_discount'] = outside_discount\n        if nick!=None: request['nick'] = nick\n        self.create(self.execute(request), fields=['success','result','success','result_code','result_message'], models={'result':CampaignPlatform})\n        return self.result", "category": "Python"}, {"instruction": "def should_fork(self):\n        \"\"\"\n        In addition to asynchronous tasks, new-style modules should be forked\n        if:\n\n        * the user specifies mitogen_task_isolation=fork, or\n        * the new-style module has a custom module search path, or\n        * the module is known to leak like a sieve.\n        \"\"\"\n", "input": "", "output": "        return (\n            super(NewStylePlanner, self).should_fork() or\n            (self._inv.task_vars.get('mitogen_task_isolation') == 'fork') or\n            (self._inv.module_name in self.ALWAYS_FORK_MODULES) or\n            (len(self.get_module_map()['custom']) > 0)\n        )", "category": "Python"}, {"instruction": "def reply(self):\n        \"\"\"\n        Returns a callable and an iterable respectively. Those can be used to\n        both transmit a message and/or iterate over incoming messages,\n        that were requested by a request socket. Note that the iterable returns\n        as many parts as sent by requesters. Also, the sender function has a\n        ``print`` like signature, with an infinite number of arguments. Each one\n        being a part of the complete message.\n\n        :rtype: (function, generator)\n        \"\"\"\n", "input": "", "output": "        sock = self.__sock(zmq.REP)\n        return self.__send_function(sock), self.__recv_generator(sock)", "category": "Python"}, {"instruction": "def prepare(self, p_args):\n        \"\"\"\n        Prepares list of operations to execute based on p_args, list of\n        todo items contained in _todo_ids attribute and _subcommand\n        attribute.\n        \"\"\"\n", "input": "", "output": "        if self._todo_ids:\n            id_position = p_args.index('{}')\n\n            # Not using MultiCommand abilities would make EditCommand awkward\n            if self._multi:\n                p_args[id_position:id_position + 1] = self._todo_ids\n                self._operations.append(p_args)\n            else:\n                for todo_id in self._todo_ids:\n                    operation_args = p_args[:]\n                    operation_args[id_position] = todo_id\n                    self._operations.append(operation_args)\n        else:\n            self._operations.append(p_args)\n\n        self._create_label()", "category": "Python"}, {"instruction": "def remove_vlan(self, vlan_resource_id):\n        \"\"\"\n        Remove a VLAN\n        :param vlan_resource_id:\n        :return:\n        \"\"\"\n", "input": "", "output": "        vlan_id = {'VLanResourceId': vlan_resource_id}\n        json_scheme = self.gen_def_json_scheme('SetRemoveVLan', vlan_id)\n        json_obj = self.call_method_post(method='SetRemoveVLan', json_scheme=json_scheme)\n        return True if json_obj['Success'] is True else False", "category": "Python"}, {"instruction": "def objectprep(self):\n        \"\"\"\n        If the script is being run as part of a pipeline, create and populate the objects for the current analysis\n        \"\"\"\n", "input": "", "output": "        for sample in self.metadata:\n            setattr(sample, self.analysistype, GenObject())\n            # Set the destination folder\n            sample[self.analysistype].outputdir = os.path.join(self.path, self.analysistype)\n            # Make the destination folder\n            make_path(sample[self.analysistype].outputdir)\n            sample[self.analysistype].baitedfastq = os.path.join(\n                sample[self.analysistype].outputdir,\n                '{at}_targetMatches.fastq.gz'.format(at=self.analysistype))\n            # Set the file type for the downstream analysis\n            sample[self.analysistype].filetype = self.filetype\n            if self.filetype == 'fasta':\n                sample[self.analysistype].assemblyfile = sample.general.bestassemblyfile", "category": "Python"}, {"instruction": "def get_conversion_factor(self, in_nc, num_nc_files):\r\n        \"\"\"get conversion_factor\"\"\"\n", "input": "", "output": "        data_in_nc = Dataset(in_nc)\r\n\r\n        # convert from kg/m^2 (i.e. mm) to m\r\n        conversion_factor = 0.001\r\n\r\n        # ECMWF units are in m\r\n        if data_in_nc.variables[self.runoff_vars[0]] \\\r\n                .getncattr(\"units\") == \"m\":\r\n            conversion_factor = 1\r\n\r\n        # ftp://hydro1.sci.gsfc.nasa.gov/data/s4pa/GLDAS_V1/README.GLDAS.pdf\r\n        if \"s\" in data_in_nc.variables[self.runoff_vars[0]] \\\r\n                .getncattr(\"units\"):\r\n            # that means kg/m^2/s in GLDAS v1 that is 3-hr avg,\r\n            # so multiply by 3 hr (ex. 3*3600). Assumed same\r\n            # for others (ex. 1*3600).\r\n            # If combining files, need to take average of these,\r\n            # so divide by number of files\r\n            conversion_factor *= \\\r\n                self.simulation_time_step_seconds / \\\r\n                num_nc_files\r\n        data_in_nc.close()\r\n\r\n        return conversion_factor", "category": "Python"}, {"instruction": "def delete(self):\n        \"\"\"Returns a response after attempting to delete the list.\n        \"\"\"\n", "input": "", "output": "        if not self.email_enabled:\n            raise EmailNotEnabledError(\"See settings.EMAIL_ENABLED\")\n        return requests.delete(\n            f\"{self.api_url}/{self.address}\", auth=(\"api\", self.api_key)\n        )", "category": "Python"}, {"instruction": "def get_dashboard_panels_visibility_by_section(section_name):\n    \"\"\"\n    Return a list of pairs as values that represents the role-permission\n    view relation for the panel section passed in.\n    :param section_name: the panels section id.\n    :return: a list of tuples.\n    \"\"\"\n", "input": "", "output": "    registry_info = get_dashboard_registry_record()\n    if section_name not in registry_info:\n        # Registry hasn't been set, do it at least for this section\n        registry_info = \\\n            setup_dashboard_panels_visibility_registry(section_name)\n\n    pairs = registry_info.get(section_name)\n    pairs = get_strings(pairs)\n    if pairs is None:\n        # In the registry, but with None value?\n        setup_dashboard_panels_visibility_registry(section_name)\n        return get_dashboard_panels_visibility_by_section(section_name)\n\n    pairs = pairs.split(',')\n    if len(pairs) == 0 or len(pairs) % 2 != 0:\n        # Non-valid or malformed value\n        setup_dashboard_panels_visibility_registry(section_name)\n        return get_dashboard_panels_visibility_by_section(section_name)\n\n    result = [\n        (pairs[i], pairs[i + 1]) for i in range(len(pairs)) if i % 2 == 0]\n    return result", "category": "Python"}, {"instruction": "def write(self, basename=\"/tmp/resynclist.xml\"):\n        \"\"\"Write a single sitemap or sitemapindex XML document.\n\n        Must be overridden to support multi-file lists.\n        \"\"\"\n", "input": "", "output": "        self.default_capability()\n        fh = open(basename, 'w')\n        s = self.new_sitemap()\n        s.resources_as_xml(self, fh=fh, sitemapindex=self.sitemapindex)\n        fh.close()", "category": "Python"}, {"instruction": "def ServicesPost (self, sensor_id, parameters):\r\n        \"\"\"\r\n            Create a new service in CommonSense, attached to a specific sensor. \r\n            If ServicesPost was successful, the service details, including its service_id, can be obtained from getResponse(), and should be a json string.\r\n            \r\n            @param sensor_id (int) - The sensor id of the sensor to connect the service to.\r\n            @param parameters (dictionary) - The specifics of the service to create.\r\n                    @note: http://www.sense-os.nl/81?nodeId=81&selectedId=11887\r\n            \r\n            @return (bool) - Boolean indicating whether ServicesPost was successful.\r\n        \"\"\"\n", "input": "", "output": "        if self.__SenseApiCall__('/sensors/{0}/services.json'.format(sensor_id), 'POST', parameters = parameters):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "category": "Python"}, {"instruction": "def dquoteEscape(param):\n    \"\"\"\n    Return param, or \"param\" if ',' or ';' or ':' is in param.\n    \"\"\"\n", "input": "", "output": "    if param.find('\"') >= 0:\n        raise VObjectError(\"Double quotes aren't allowed in parameter values.\")\n    for char in ',;:':\n        if param.find(char) >= 0:\n            return '\"' + param + '\"'\n    return param", "category": "Python"}, {"instruction": "def canonicalize(self, include_nodes=True, sorted=False):\n        \"\"\"Generates a canonical :class:`etc.Node` object from this mock node.\n        \"\"\"\n", "input": "", "output": "        node_class = Directory if self.dir else Value\n        kwargs = {attr: getattr(self, attr) for attr in node_class.__slots__}\n        if self.dir:\n            if include_nodes:\n                nodes = [node.canonicalize() for node in\n                         six.viewvalues(kwargs['nodes'])]\n                if sorted:\n                    nodes.sort(key=lambda n: n.key)\n                kwargs['nodes'] = nodes\n            else:\n                kwargs['nodes'] = []\n        return node_class(**kwargs)", "category": "Python"}, {"instruction": "def _bisect(seq):\n        \"\"\"\n        Return a (medial_value, greater_values, lesser_values) 3-tuple\n        obtained by bisecting sequence *seq*.\n        \"\"\"\n", "input": "", "output": "        if len(seq) == 0:\n            return [], None, []\n        mid_idx = int(len(seq)/2)\n        mid = seq[mid_idx]\n        greater = seq[mid_idx+1:]\n        lesser = seq[:mid_idx]\n        return mid, greater, lesser", "category": "Python"}, {"instruction": "def get_rulesets(ruledir, recurse):\n    \"\"\"\n    List of ruleset objects extracted from the yaml directory\n    \"\"\"\n", "input": "", "output": "    if os.path.isdir(ruledir) and recurse:\n        yaml_files = [y for x in os.walk(ruledir) for y in glob(os.path.join(x[0], '*.yaml'))]\n    elif os.path.isdir(ruledir) and not recurse:\n        yaml_files = get_files(ruledir, 'yaml')\n    elif os.path.isfile(ruledir):\n        yaml_files = [ruledir]\n    extracted_files = extract_yaml(yaml_files)\n    rulesets = []\n    for extracted_yaml in extracted_files:\n        rulesets.append(ruleset.Ruleset(extracted_yaml))\n    return rulesets", "category": "Python"}, {"instruction": "def barlam(self, wavelengths=None):\n        \"\"\"Calculate :ref:`mean log wavelength <synphot-formula-barlam>`.\n\n        Parameters\n        ----------\n        wavelengths : array-like, `~astropy.units.quantity.Quantity`, or `None`\n            Wavelength values for sampling.\n            If not a Quantity, assumed to be in Angstrom.\n            If `None`, `waveset` is used.\n\n        Returns\n        -------\n        bar_lam : `~astropy.units.quantity.Quantity`\n            Mean log wavelength.\n\n        \"\"\"\n", "input": "", "output": "        x = self._validate_wavelengths(wavelengths).value\n        y = self(x).value\n        num = np.trapz(y * np.log(x) / x, x=x)\n        den = np.trapz(y / x, x=x)\n\n        if num == 0 or den == 0:  # pragma: no cover\n            bar_lam = 0.0\n        else:\n            bar_lam = np.exp(abs(num / den))\n\n        return bar_lam * self._internal_wave_unit", "category": "Python"}, {"instruction": "def convert(f, output=sys.stdout):\n    \"\"\"Convert Plan 9 file to PNG format.  Works with either uncompressed\n    or compressed files.\n    \"\"\"\n", "input": "", "output": "\n    r = f.read(11)\n    if r == 'compressed\\n':\n        png(output, *decompress(f))\n    else:\n        png(output, *glue(f, r))", "category": "Python"}, {"instruction": "def recv(self, filename, dest_file, timeout=None):\n    \"\"\"Retrieve a file from the device into the file-like dest_file.\"\"\"\n", "input": "", "output": "    transport = DataFilesyncTransport(self.stream)\n    transport.write_data('RECV', filename, timeout)\n    for data_msg in transport.read_until_done('DATA', timeout):\n      dest_file.write(data_msg.data)", "category": "Python"}, {"instruction": "def _get_sorted_inputs(filename, delimiter=\"\\n\"):\n  \"\"\"Returning inputs sorted according to decreasing length.\n\n  This causes inputs of similar lengths to be processed in the same batch,\n  facilitating early stopping for short sequences.\n\n  Longer sequences are sorted first so that if you're going to get OOMs,\n  you'll see it in the first batch.\n\n  Args:\n    filename: path to file with inputs, 1 per line.\n    delimiter: str, delimits records in the file.\n\n  Returns:\n    a sorted list of inputs\n\n  \"\"\"\n", "input": "", "output": "  tf.logging.info(\"Getting sorted inputs\")\n  with tf.gfile.Open(filename) as f:\n    text = f.read()\n    records = text.split(delimiter)\n    inputs = [record.strip() for record in records]\n    # Strip the last empty line.\n    if not inputs[-1]:\n      inputs.pop()\n  input_lens = [(i, -len(line.split())) for i, line in enumerate(inputs)]\n  sorted_input_lens = sorted(input_lens, key=operator.itemgetter(1))\n  # We'll need the keys to rearrange the inputs back into their original order\n  sorted_keys = {}\n  sorted_inputs = []\n  for i, (index, _) in enumerate(sorted_input_lens):\n    sorted_inputs.append(inputs[index])\n    sorted_keys[index] = i\n  return sorted_inputs, sorted_keys", "category": "Python"}, {"instruction": "def set_left_table(self, left_table=None):\n        \"\"\"\n        Sets the left table for this join clause. If no table is specified, the first table\n        in the query will be used\n\n        :type left_table: str or dict or :class:`Table <querybuilder.tables.Table>` or None\n        :param left_table: The left table being joined with. This can be a string of the table\n            name, a dict of {'alias': table}, or a ``Table`` instance. Defaults to the first table\n            in the query.\n        \"\"\"\n", "input": "", "output": "        if left_table:\n            self.left_table = TableFactory(\n                table=left_table,\n                owner=self.owner,\n            )\n        else:\n            self.left_table = self.get_left_table()", "category": "Python"}, {"instruction": "def serialize_image(ctx, document, elem, root):\n    \"\"\"Serialize image element.\n\n    This is not abstract enough.\n    \"\"\"\n", "input": "", "output": "\n    _img = etree.SubElement(root, 'img')\n    # make path configurable\n\n    if elem.rid in document.relationships[ctx.options['relationship']]:\n        img_src = document.relationships[ctx.options['relationship']][elem.rid].get('target', '')\n        img_name, img_extension = os.path.splitext(img_src)\n\n        _img.set('src', 'static/{}{}'.format(elem.rid, img_extension))\n\n    fire_hooks(ctx, document, elem, _img, ctx.get_hook('img'))\n\n    return root", "category": "Python"}, {"instruction": "def wait_socket(host, port, timeout=120):\n    '''\n    Wait for socket opened on remote side. Return False after timeout\n    '''\n", "input": "", "output": "    return wait_result(lambda: check_socket(host, port), True, timeout)", "category": "Python"}, {"instruction": "def from_python_src(\n        cls,\n        pySrc,\n        lambdas_path,\n        json_filename: str,\n        stem: str,\n        save_file: bool = False,\n    ):\n        \"\"\"Builds GrFN object from Python source code.\"\"\"\n", "input": "", "output": "        asts = [ast.parse(pySrc)]\n        pgm_dict = genPGM.create_pgm_dict(\n            lambdas_path,\n            asts,\n            json_filename,\n            {\"FileName\": f\"{stem}.py\"},  # HACK\n        )\n        lambdas = importlib.__import__(stem + \"_lambdas\")\n        return cls.from_dict(pgm_dict, lambdas)", "category": "Python"}, {"instruction": "def detect_console_encoding():\n    \"\"\"\n    Try to find the most capable encoding supported by the console.\n    slightly modified from the way IPython handles the same issue.\n    \"\"\"\n", "input": "", "output": "    global _initial_defencoding\n\n    encoding = None\n    try:\n        encoding = sys.stdout.encoding or sys.stdin.encoding\n    except (AttributeError, IOError):\n        pass\n\n    # try again for something better\n    if not encoding or 'ascii' in encoding.lower():\n        try:\n            encoding = locale.getpreferredencoding()\n        except Exception:\n            pass\n\n    # when all else fails. this will usually be \"ascii\"\n    if not encoding or 'ascii' in encoding.lower():\n        encoding = sys.getdefaultencoding()\n\n    # GH#3360, save the reported defencoding at import time\n    # MPL backends may change it. Make available for debugging.\n    if not _initial_defencoding:\n        _initial_defencoding = sys.getdefaultencoding()\n\n    return encoding", "category": "Python"}, {"instruction": "def clear_vdp_vsi(self, port_uuid):\n        \"\"\"Stores the vNIC specific info for VDP Refresh.\n\n        :param uuid: vNIC UUID\n        \"\"\"\n", "input": "", "output": "        try:\n            LOG.debug(\"Clearing VDP VSI MAC %(mac)s UUID %(uuid)s\",\n                      {'mac': self.vdp_vif_map[port_uuid].get('mac'),\n                       'uuid': self.vdp_vif_map[port_uuid].get('vsiid')})\n            del self.vdp_vif_map[port_uuid]\n        except Exception:\n            LOG.error(\"VSI does not exist\")\n        self.clear_oui(port_uuid)", "category": "Python"}, {"instruction": "def ro(self):\n\t\t\"\"\" Return read-only copy\n\n\t\t:return: WHTTPCookieJar\n\t\t\"\"\"\n", "input": "", "output": "\t\tro_jar = WHTTPCookieJar()\n\t\tfor cookie in self.__cookies.values():\n\t\t\tro_jar.add_cookie(cookie.ro())\n\t\tro_jar.__ro_flag = True\n\t\treturn ro_jar", "category": "Python"}, {"instruction": "def info(self, msg, *args, **kwargs) -> Task:  # type: ignore\n        \"\"\"\n        Log msg with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        await logger.info(\"Houston, we have an interesting problem\", exc_info=1)\n        \"\"\"\n", "input": "", "output": "        return self._make_log_task(logging.INFO, msg, args, **kwargs)", "category": "Python"}, {"instruction": "def valid_hostnames(hostname_list):\n    \"\"\"\n    Check if the supplied list of strings are valid hostnames\n    :param hostname_list: A list of strings\n    :return: True if the strings are valid hostnames, False if not\n    \"\"\"\n", "input": "", "output": "    for entry in hostname_list:\n        if len(entry) > 255:\n            return False\n        allowed = re.compile('(?!-)[A-Z\\d-]{1,63}(?<!-)$', re.IGNORECASE)\n        if not all(allowed.match(x) for x in entry.split(\".\")):\n            return False\n    return True", "category": "Python"}, {"instruction": "def get_unique_sentence_indices( text ):\n    ''' Returns a list of sentence indices for the whole text. For each token in text, \n        the list contains index of the sentence the word belongs to, and the indices\n        are unique over the whole text. '''\n", "input": "", "output": "    # Add sentence annotation (if missing)\n    if not text.is_tagged( SENTENCES ):\n        text.tokenize_sentences()\n    # Collect (unique) sent indices over the whole text\n    sent_indices = []\n    sent_id = 0\n    for sub_text in text.split_by( SENTENCES ):\n        for word in sub_text.words:\n            sent_indices.append( sent_id )\n        sent_id += 1\n    assert len(sent_indices) == len(text.words), '(!) Number of sent indices should match nr of words!'\n    return sent_indices", "category": "Python"}, {"instruction": "def delete_website_configuration(self, headers=None):\n        \"\"\"\n        Removes all website configuration from the bucket.\n        \"\"\"\n", "input": "", "output": "        response = self.connection.make_request('DELETE', self.name,\n                query_args='website', headers=headers)\n        body = response.read()\n        boto.log.debug(body)\n        if response.status == 204:\n            return True\n        else:\n            raise self.connection.provider.storage_response_error(\n                response.status, response.reason, body)", "category": "Python"}, {"instruction": "def setSample(self, sample):\n        \"\"\"Set sample points from the population. Should be a RDD\"\"\"\n", "input": "", "output": "        if not isinstance(sample, RDD):\n            raise TypeError(\"samples should be a RDD, received %s\" % type(sample))\n        self._sample = sample", "category": "Python"}, {"instruction": "def addParts(parentPart, childPath, count, index):\n    \"\"\"\n    BUILD A hierarchy BY REPEATEDLY CALLING self METHOD WITH VARIOUS childPaths\n    count IS THE NUMBER FOUND FOR self PATH\n    \"\"\"\n", "input": "", "output": "    if index == None:\n        index = 0\n    if index == len(childPath):\n        return\n    c = childPath[index]\n    parentPart.count = coalesce(parentPart.count, 0) + count\n\n    if parentPart.partitions == None:\n        parentPart.partitions = FlatList()\n    for i, part in enumerate(parentPart.partitions):\n        if part.name == c.name:\n            addParts(part, childPath, count, index + 1)\n            return\n\n    parentPart.partitions.append(c)\n    addParts(c, childPath, count, index + 1)", "category": "Python"}, {"instruction": "def terminating_sip_domains(self):\n        \"\"\"\n        Access the terminating_sip_domains\n\n        :returns: twilio.rest.trunking.v1.trunk.terminating_sip_domain.TerminatingSipDomainList\n        :rtype: twilio.rest.trunking.v1.trunk.terminating_sip_domain.TerminatingSipDomainList\n        \"\"\"\n", "input": "", "output": "        if self._terminating_sip_domains is None:\n            self._terminating_sip_domains = TerminatingSipDomainList(\n                self._version,\n                trunk_sid=self._solution['sid'],\n            )\n        return self._terminating_sip_domains", "category": "Python"}, {"instruction": "def get_messages(module):\n    \"\"\"Discovers all protobuf Message classes in a given import module.\n\n    Args:\n        module (module): A Python module; :func:`dir` will be run against this\n            module to find Message subclasses.\n\n    Returns:\n        dict[str, google.protobuf.message.Message]: A dictionary with the\n            Message class names as keys, and the Message subclasses themselves\n            as values.\n    \"\"\"\n", "input": "", "output": "    answer = collections.OrderedDict()\n    for name in dir(module):\n        candidate = getattr(module, name)\n        if inspect.isclass(candidate) and issubclass(candidate, message.Message):\n            answer[name] = candidate\n    return answer", "category": "Python"}, {"instruction": "def set_extra_selections(self, key, extra_selections):\r\n        \"\"\"Set extra selections for a key.\r\n\r\n        Also assign draw orders to leave current_cell and current_line\r\n        in the backgrund (and avoid them to cover other decorations)\r\n\r\n        NOTE: This will remove previous decorations added to  the same key.\r\n\r\n        Args:\r\n            key (str) name of the extra selections group.\r\n            extra_selections (list of sourcecode.api.TextDecoration).\r\n        \"\"\"\n", "input": "", "output": "        # use draw orders to highlight current_cell and current_line first\r\n        draw_order = DRAW_ORDERS.get(key)\r\n        if draw_order is None:\r\n            draw_order = DRAW_ORDERS.get('on_top')\r\n\r\n        for selection in extra_selections:\r\n            selection.draw_order = draw_order\r\n\r\n        self.clear_extra_selections(key)\r\n        self.extra_selections_dict[key] = extra_selections", "category": "Python"}, {"instruction": "def toFormMarkup(self, form_tag_attrs=None):\n        \"\"\"Returns the form markup for this response.\n\n        @param form_tag_attrs: Dictionary of attributes to be added to\n            the form tag. 'accept-charset' and 'enctype' have defaults\n            that can be overridden. If a value is supplied for\n            'action' or 'method', it will be replaced.\n\n        @returntype: str\n\n        @since: 2.1.0\n        \"\"\"\n", "input": "", "output": "        return self.fields.toFormMarkup(self.request.return_to,\n                                        form_tag_attrs=form_tag_attrs)", "category": "Python"}, {"instruction": "def get_name_at( self, name, block_number, include_expired=False ):\n        \"\"\"\n        Generate and return the sequence of of states a name record was in\n        at a particular block number.\n        \"\"\"\n", "input": "", "output": "        cur = self.db.cursor()\n        return namedb_get_name_at(cur, name, block_number, include_expired=include_expired)", "category": "Python"}, {"instruction": "def _get_image_data(image, colors):\n    \"\"\"Performs the LZW compression as described by Matthew Flickinger.\n\n    This isn't fast, but it works.\n    http://www.matthewflickinger.com/lab/whatsinagif/lzw_image_data.asp\n    \"\"\"\n", "input": "", "output": "    lzw_code_size, coded_bits = _lzw_encode(image, colors)\n    coded_bytes = ''.join(\n        '{{:0{}b}}'.format(nbits).format(val) for val, nbits in coded_bits)\n    coded_bytes = '0' * ((8 - len(coded_bytes)) % 8) + coded_bytes\n    coded_data = list(\n        reversed([\n            int(coded_bytes[8*i:8*(i+1)], 2)\n            for i in range(len(coded_bytes) // 8)\n        ])\n    )\n    output = [struct.pack('<B', lzw_code_size)]\n    # Must output the data in blocks of length 255\n    block_length = min(255, len(coded_data))\n    while block_length > 0:\n        block = struct.pack(\n            '<{}B'.format(block_length + 1),\n            block_length,\n            *coded_data[:block_length]\n        )\n        output.append(block)\n        coded_data = coded_data[block_length:]\n        block_length = min(255, len(coded_data))\n    return b''.join(output)", "category": "Python"}, {"instruction": "def compress(x, y):\n    \"\"\"\n    Given a x,y coordinate, encode in \"compressed format\"\n    Returned is always 33 bytes.\n    \"\"\"\n", "input": "", "output": "    polarity = \"02\" if y % 2 == 0 else \"03\"\n\n    wrap = lambda x: x\n    if not is_py2:\n        wrap = lambda x: bytes(x, 'ascii')\n\n    return unhexlify(wrap(\"%s%0.64x\" % (polarity, x)))", "category": "Python"}, {"instruction": "def build_model(self, n_features, n_classes):\n        \"\"\"Create the computational graph of the model.\n\n        :param n_features: Number of features.\n        :param n_classes: number of classes.\n        :return: self\n        \"\"\"\n", "input": "", "output": "        self._create_placeholders(n_features, n_classes)\n        self._create_layers(n_classes)\n\n        self.cost = self.loss.compile(self.mod_y, self.input_labels)\n        self.train_step = self.trainer.compile(self.cost)\n        self.accuracy = Evaluation.accuracy(self.mod_y, self.input_labels)", "category": "Python"}, {"instruction": "def load_tasks_from_dir(self, dir_path, propagate_exceptions=False):\n        \"\"\" Imports all python modules in specified directories and returns subclasses of BaseTask from them\n\n        :param propagate_exceptions: a flag that indicates if exceptions from single file import shall be raised during the\n            whole directory lookup\n        :param dir_path: fully qualified directory path, where all python modules will be search for subclasses of BaseTask\n        :type dir_path: `str`\n        :return: a dict of CustomTasks, where key is CustomTask.name, and value is a CustomClass task itself\n        :rtype: `dict`\n        \"\"\"\n", "input": "", "output": "        if not os.path.exists(dir_path):\n            raise GOSTaskException()\n        if os.path.isfile(dir_path):\n            raise GOSTaskException()\n        result = {}\n        for file_basename in os.listdir(dir_path):\n            full_file_path = os.path.join(dir_path, file_basename)\n            try:\n                result.update(self.load_tasks_from_file(full_file_path))\n            except (GOSTaskException, GOSIOException):\n                if propagate_exceptions:\n                    raise\n        return result", "category": "Python"}, {"instruction": "def isChar(ev):\n    \"\"\" Check if an event may be a typed character\n    \"\"\"\n", "input": "", "output": "    text = ev.text()\n    if len(text) != 1:\n        return False\n\n    if ev.modifiers() not in (Qt.ShiftModifier, Qt.KeypadModifier, Qt.NoModifier):\n        return False\n\n    asciiCode = ord(text)\n    if asciiCode <= 31 or asciiCode == 0x7f:  # control characters\n        return False\n\n    if text == ' ' and ev.modifiers() == Qt.ShiftModifier:\n        return False  # Shift+Space is a shortcut, not a text\n\n    return True", "category": "Python"}, {"instruction": "def pickAttachment(self):\r\n        \"\"\"\r\n        Prompts the user to select an attachment to add to this edit.\r\n        \"\"\"\n", "input": "", "output": "        filename = QFileDialog.getOpenFileName(self.window(),\r\n                                               'Select Attachment',\r\n                                               '',\r\n                                               'All Files (*.*)')\r\n        \r\n        if type(filename) == tuple:\r\n            filename = nativestring(filename[0])\r\n        \r\n        filename = nativestring(filename)\r\n        if filename:\r\n            self.addAttachment(os.path.basename(filename), filename)", "category": "Python"}, {"instruction": "def predicatesIn(G: Graph, n: Node) -> Set[TriplePredicate]:\n    \"\"\" predicatesIn(G, n) is the set of predicates in arcsIn(G, n). \"\"\"\n", "input": "", "output": "    return {p for _, p in G.subject_predicates(n)}", "category": "Python"}, {"instruction": "def delete_device(self, device_id):\n        \"\"\"Delete device from catalog.\n\n        :param str device_id: ID of device in catalog to delete (Required)\n        :return: void\n        \"\"\"\n", "input": "", "output": "        api = self._get_api(device_directory.DefaultApi)\n        return api.device_destroy(id=device_id)", "category": "Python"}, {"instruction": "def align_nab(tar, ref):\n    \"\"\"Aligns the N-CA and CA-CB vector of the target monomer.\n\n    Parameters\n    ----------\n    tar: ampal.Residue\n        The residue that will be aligned to the reference.\n    ref: ampal.Residue\n        The reference residue for the alignment.\n    \"\"\"\n", "input": "", "output": "    rot_trans_1 = find_transformations(\n        tar['N'].array, tar['CA'].array, ref['N'].array, ref['CA'].array)\n    apply_trans_rot(tar, *rot_trans_1)\n    rot_ang_ca_cb = dihedral(tar['CB'], ref['CA'], ref['N'], ref['CB'])\n    tar.rotate(rot_ang_ca_cb, ref['N'].array - ref['CA'].array, ref['N'].array)\n    return", "category": "Python"}, {"instruction": "def create(cls, name, members=None, comment=None):\n        \"\"\"\n        Create the TCP Service group\n\n        :param str name: name of tcp service group\n        :param list element: tcp services by element or href\n        :type element: list(str,Element)\n        :raises CreateElementFailed: element creation failed with reason\n        :return: instance with meta\n        :rtype: TCPServiceGroup\n        \"\"\"\n", "input": "", "output": "        element = [] if members is None else element_resolver(members)\n        json = {'name': name,\n                'element': element,\n                'comment': comment}\n\n        return ElementCreator(cls, json)", "category": "Python"}, {"instruction": "def CreateReply(self, **attributes):\n        \"\"\"Create a new packet as a reply to this one. This method\n        makes sure the authenticator and secret are copied over\n        to the new instance.\n        \"\"\"\n", "input": "", "output": "        return CoAPacket(CoAACK, self.id,\n            self.secret, self.authenticator, dict=self.dict,\n            **attributes)", "category": "Python"}, {"instruction": "def temporal_snr(signal_dset,noise_dset,mask=None,prefix='temporal_snr.nii.gz'):\n    '''Calculates temporal SNR by dividing average signal of ``signal_dset`` by SD of ``noise_dset``.\n    ``signal_dset`` should be a dataset that contains the average signal value (i.e., nothing that has\n    been detrended by removing the mean), and ``noise_dset`` should be a dataset that has all possible\n    known signal fluctuations (e.g., task-related effects) removed from it (the residual dataset from a \n    deconvolve works well)'''\n", "input": "", "output": "    for d in [('mean',signal_dset), ('stdev',noise_dset)]:\n        new_d = nl.suffix(d[1],'_%s' % d[0])\n        cmd = ['3dTstat','-%s' % d[0],'-prefix',new_d]\n        if mask:\n            cmd += ['-mask',mask]\n        cmd += [d[1]]\n        nl.run(cmd,products=new_d)\n    nl.calc([nl.suffix(signal_dset,'_mean'),nl.suffix(noise_dset,'_stdev')],'a/b',prefix=prefix)", "category": "Python"}, {"instruction": "def iter(self, start=0, stop=-1, withscores=False, reverse=None):\n        \"\"\" Return a range of values from sorted set name between\n            @start and @end sorted in ascending order unless @reverse or\n            :prop:reversed.\n\n            @start and @end: #int, can be negative, indicating the end of\n                the range.\n            @withscores: #bool indicates to return the scores along with the\n                members, as a list of |(member, score)| pairs\n            @reverse: #bool indicating whether to sort the results descendingly\n\n            -> yields members or |(member, score)| #tuple pairs\n        \"\"\"\n", "input": "", "output": "        reverse = reverse if reverse is not None else self.reversed\n        _loads = self._loads\n        for member in self._client.zrange(\n           self.key_prefix, start=start, end=stop, withscores=withscores,\n           desc=reverse, score_cast_func=self.cast):\n            if withscores:\n                yield (_loads(member[0]), self.cast(member[1]))\n            else:\n                yield _loads(member)", "category": "Python"}, {"instruction": "def filter_out_mutations_in_normal(tumordf, normaldf, most_common_maf_min=0.2,\n                                   most_common_count_maf_threshold=20,\n                                   most_common_count_min=1):\n    \"\"\"Remove mutations that are in normal\"\"\"\n", "input": "", "output": "    df = tumordf.merge(normaldf, on=[\"chrom\", \"pos\"], suffixes=(\"_T\", \"_N\"))\n\n    # filters\n    common_al = (df.most_common_al_count_T == df.most_common_count_T) & (df.most_common_al_T == df.most_common_al_N)\n    common_indel = (df.most_common_indel_count_T == df.most_common_count_T) & \\\n        (df.most_common_indel_T == df.imost_common_indel_N)\n    normal_criteria = ((df.most_common_count_N >= most_common_count_maf_threshold) &\n                       (df.most_common_maf_N > most_common_maf_min)) | \\\n        ((df.most_common_count_N < most_common_count_maf_threshold) &\n         (df.most_common_count_N > most_common_count_min))\n    df = df[~(common_al | common_indel) & normal_criteria]\n\n    # restore column names of tumor\n    for c in df.columns:\n        if c.endswith(\"_N\"):\n            del df[c]\n            df.columns = [c[:-2] if c.endswith(\"_T\") else c for c in df.columns]\n\n    return df", "category": "Python"}, {"instruction": "def doc_id(self):\n        \"\"\"Returns the couchbase document's id, object property.\n\n        :returns: The document id (that is created from :attr:'doc_type' and\n            :attr:'__key_field__' value, or auto-hashed document id at first\n            saving).\n        :rtype: unicode\n        \"\"\"\n", "input": "", "output": "        if self.id:\n            return '%s_%s' % (self.doc_type, self.id.lower())\n        return self._hashed_key", "category": "Python"}, {"instruction": "def expose_request(func):\n    \"\"\"\n    A decorator that adds an expose_request flag to the underlying callable.\n\n    @raise TypeError: C{func} must be callable.\n    \"\"\"\n", "input": "", "output": "    if not python.callable(func):\n        raise TypeError(\"func must be callable\")\n\n    if isinstance(func, types.UnboundMethodType):\n        setattr(func.im_func, '_pyamf_expose_request', True)\n    else:\n        setattr(func, '_pyamf_expose_request', True)\n\n    return func", "category": "Python"}, {"instruction": "def _normalize_roots(file_roots):\n    '''\n    Normalize file or pillar roots.\n    '''\n", "input": "", "output": "    for saltenv, dirs in six.iteritems(file_roots):\n        normalized_saltenv = six.text_type(saltenv)\n        if normalized_saltenv != saltenv:\n            file_roots[normalized_saltenv] = file_roots.pop(saltenv)\n        if not isinstance(dirs, (list, tuple)):\n            file_roots[normalized_saltenv] = []\n        file_roots[normalized_saltenv] = \\\n                _expand_glob_path(file_roots[normalized_saltenv])\n    return file_roots", "category": "Python"}, {"instruction": "def _report_problem(self, problem, level=logging.ERROR):\n        '''Report a given problem'''\n", "input": "", "output": "        problem = self.basename + ': ' + problem\n        if self._logger.isEnabledFor(level):\n            self._problematic = True\n        if self._check_raises:\n            raise DapInvalid(problem)\n        self._logger.log(level, problem)", "category": "Python"}, {"instruction": "def current_state_str(self):\n        \"\"\"Return string representation of the current state of the sensor.\"\"\"\n", "input": "", "output": "        if self.sample_ok:\n            msg = ''\n            temperature = self._get_value_opc_attr('temperature')\n            if temperature is not None:\n                msg += 'Temp: %s \u00baC, ' % temperature\n            humidity = self._get_value_opc_attr('humidity')\n            if humidity is not None:\n                msg += 'Humid: %s %%, ' % humidity\n            pressure = self._get_value_opc_attr('pressure')\n            if pressure is not None:\n                msg += 'Press: %s mb, ' % pressure\n            light_level = self._get_value_opc_attr('light_level')\n            if light_level is not None:\n                msg += 'Light: %s lux, ' % light_level\n            return msg[:-2]\n        else:\n            return \"Bad sample\"", "category": "Python"}, {"instruction": "def get_current_state_m(self):\n        \"\"\"Returns the state model of the currently open tab\"\"\"\n", "input": "", "output": "        page_id = self.view.notebook.get_current_page()\n        if page_id == -1:\n            return None\n        page = self.view.notebook.get_nth_page(page_id)\n        state_identifier = self.get_state_identifier_for_page(page)\n        return self.tabs[state_identifier]['state_m']", "category": "Python"}, {"instruction": "def _Execute(cmd, args, time_limit=-1, use_client_context=False, cwd=None):\n  \"\"\"Executes cmd.\"\"\"\n", "input": "", "output": "  run = [cmd]\n  run.extend(args)\n  env = os.environ.copy()\n  if use_client_context:\n    env.pop(\"LD_LIBRARY_PATH\", None)\n    env.pop(\"PYTHON_PATH\", None)\n    context = \"client\"\n  else:\n    context = \"system\"\n  logging.info(\"Executing %s in %s context.\", \" \".join(run), context)\n  p = subprocess.Popen(\n      run,\n      stdin=subprocess.PIPE,\n      stdout=subprocess.PIPE,\n      stderr=subprocess.PIPE,\n      env=env,\n      cwd=cwd)\n\n  alarm = None\n  if time_limit > 0:\n    alarm = threading.Timer(time_limit, HandleAlarm, (p,))\n    alarm.setDaemon(True)\n    alarm.start()\n\n  stdout, stderr, exit_status = b\"\", b\"\", -1\n  start_time = time.time()\n  try:\n    stdout, stderr = p.communicate()\n    exit_status = p.returncode\n  except IOError:\n    # If we end up here, the time limit was exceeded\n    pass\n  finally:\n    if alarm:\n      alarm.cancel()\n      alarm.join()\n\n  return (stdout, stderr, exit_status, time.time() - start_time)", "category": "Python"}, {"instruction": "def remove_filtered_edges(graph, edge_predicates=None):\n    \"\"\"Remove edges passing the given edge predicates.\n\n    :param pybel.BELGraph graph: A BEL graph\n    :param edge_predicates: A predicate or list of predicates\n    :type edge_predicates: None or ((pybel.BELGraph, tuple, tuple, int) -> bool) or iter[(pybel.BELGraph, tuple, tuple, int) -> bool]]\n    :return:\n    \"\"\"\n", "input": "", "output": "    edges = list(filter_edges(graph, edge_predicates=edge_predicates))\n    graph.remove_edges_from(edges)", "category": "Python"}, {"instruction": "def unwrap(self, token=None):\n        \"\"\"Return the original response inside the given wrapping token.\n\n        Unlike simply reading cubbyhole/response (which is deprecated), this endpoint provides additional validation\n        checks on the token, returns the original value on the wire rather than a JSON string representation of it, and\n        ensures that the response is properly audit-logged.\n\n        Supported methods:\n            POST: /sys/wrapping/unwrap. Produces: 200 application/json\n\n        :param token: Specifies the wrapping token ID. This is required if the client token is not the wrapping token.\n            Do not use the wrapping token in both locations.\n        :type token: str | unicode\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        params = {}\n        if token is not None:\n            params['token'] = token\n\n        api_path = '/v1/sys/wrapping/unwrap'\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n\n        return response.json()", "category": "Python"}, {"instruction": "def merge_config(self, user_config):\n        '''\n        Take a dictionary of user preferences and use them to update the default\n        data, model, and conversation configurations.\n        '''\n", "input": "", "output": "\n        # provisioanlly update the default configurations with the user preferences\n        temp_data_config = copy.deepcopy(self.data_config).update(user_config)\n        temp_model_config = copy.deepcopy(self.model_config).update(user_config)\n        temp_conversation_config = copy.deepcopy(self.conversation_config).update(user_config)\n\n        # if the new configurations validate, apply them\n        if validate_data_config(temp_data_config):\n            self.data_config = temp_data_config\n        if validate_model_config(temp_model_config):\n            self.model_config = temp_model_config\n        if validate_conversation_config(temp_conversation_config):\n            self.conversation_config = temp_conversation_config", "category": "Python"}, {"instruction": "def _write_exports(exports, edict):\n    '''\n    Write an exports file to disk\n\n    If multiple shares were initially configured per line, like:\n\n        /media/storage /media/data *(ro,sync,no_subtree_check)\n\n    ...then they will be saved to disk with only one share per line:\n\n        /media/storage *(ro,sync,no_subtree_check)\n        /media/data *(ro,sync,no_subtree_check)\n    '''\n", "input": "", "output": "    with salt.utils.files.fopen(exports, 'w') as efh:\n        for export in edict:\n            line = salt.utils.stringutils.to_str(export)\n            for perms in edict[export]:\n                hosts = perms['hosts']\n                options = ','.join(perms['options'])\n                line += ' {0}({1})'.format(hosts, options)\n            efh.write('{0}\\n'.format(line))", "category": "Python"}, {"instruction": "def exception(self, event=None, *args, **kw):\n        \"\"\"\n        Process event and call :meth:`logging.Logger.error` with the result,\n        after setting ``exc_info`` to `True`.\n        \"\"\"\n", "input": "", "output": "        if not self._logger.isEnabledFor(logging.ERROR):\n            return\n\n        kw = self._add_base_info(kw)\n        kw['level'] = \"exception\"\n        kw.setdefault('exc_info', True)\n        return self.error(event, *args, **kw)", "category": "Python"}, {"instruction": "def _ensure_min_resources(progs, cores, memory, min_memory):\n    \"\"\"Ensure setting match minimum resources required for used programs.\n    \"\"\"\n", "input": "", "output": "    for p in progs:\n        if p in min_memory:\n            if not memory or cores * memory < min_memory[p]:\n                memory = float(min_memory[p]) / cores\n    return cores, memory", "category": "Python"}, {"instruction": "def grid_visual(data):\n  \"\"\"\n  This function displays a grid of images to show full misclassification\n  :param data: grid data of the form;\n      [nb_classes : nb_classes : img_rows : img_cols : nb_channels]\n  :return: if necessary, the matplot figure to reuse\n  \"\"\"\n", "input": "", "output": "  import matplotlib.pyplot as plt\n\n  # Ensure interactive mode is disabled and initialize our graph\n  plt.ioff()\n  figure = plt.figure()\n  figure.canvas.set_window_title('Cleverhans: Grid Visualization')\n\n  # Add the images to the plot\n  num_cols = data.shape[0]\n  num_rows = data.shape[1]\n  num_channels = data.shape[4]\n  for y in range(num_rows):\n    for x in range(num_cols):\n      figure.add_subplot(num_rows, num_cols, (x + 1) + (y * num_cols))\n      plt.axis('off')\n\n      if num_channels == 1:\n        plt.imshow(data[x, y, :, :, 0], cmap='gray')\n      else:\n        plt.imshow(data[x, y, :, :, :])\n\n  # Draw the plot and return\n  plt.show()\n  return figure", "category": "Python"}, {"instruction": "def prepare(self):\n        \"\"\"\n        does some basic validation\n        \"\"\"\n", "input": "", "output": "        try:\n            assert(type(self.sender) is Channel)\n            assert(type(self.receiver) is Channel)\n            return True\n        except:\n            return False", "category": "Python"}, {"instruction": "def unblockall(self):\n        '''\n        Remove all blocks from the queue and all sub-queues\n        '''\n", "input": "", "output": "        for q in self.queues.values():\n            q.unblockall()\n        self.blockEvents.clear()", "category": "Python"}, {"instruction": "def optional(name, default) -> 'Wildcard':\n        \"\"\"Create a `Wildcard` that matches a single argument with a default value.\n\n        If the wildcard does not match, the substitution will contain the\n        default value instead.\n\n        Args:\n            name:\n                The name for the wildcard.\n            default:\n                The default value of the wildcard.\n\n        Returns:\n            A n optional wildcard.\n        \"\"\"\n", "input": "", "output": "        return Wildcard(min_count=1, fixed_size=True, variable_name=name, optional=default)", "category": "Python"}, {"instruction": "def _mark_dirty(self, xblock, value):\n        \"\"\"Set this field to dirty on the xblock.\"\"\"\n", "input": "", "output": "        # pylint: disable=protected-access\n\n        # Deep copy the value being marked as dirty, so that there\n        # is a baseline to check against when saving later\n        if self not in xblock._dirty_fields:\n            xblock._dirty_fields[self] = copy.deepcopy(value)", "category": "Python"}, {"instruction": "def get_residue_id_to_type_map(self):\n        '''Returns a dictionary mapping 6-character residue IDs (Chain, residue number, insertion code e.g. \"A 123B\") to the\n           corresponding one-letter amino acid.\n\n           Caveat: This function ignores occupancy - this function should be called once occupancy has been dealt with appropriately.'''\n", "input": "", "output": "\n        resid2type = {}\n        atomlines = self.parsed_lines['ATOM  ']\n        for line in atomlines:\n            resname = line[17:20]\n            if resname in allowed_PDB_residues_types and line[13:16] == 'CA ':\n                resid2type[line[21:27]] = residue_type_3to1_map.get(resname) or protonated_residue_type_3to1_map.get(resname)\n        return resid2type", "category": "Python"}, {"instruction": "def _update_pdf(population, fitnesses, pdfs, quantile):\n    \"\"\"Find a better pdf, based on fitnesses.\"\"\"\n", "input": "", "output": "    # First we determine a fitness threshold based on a quantile of fitnesses\n    fitness_threshold = _get_quantile_cutoff(fitnesses, quantile)\n\n    # Then check all of our possible pdfs with a stochastic program\n    return _best_pdf(pdfs, population, fitnesses, fitness_threshold)", "category": "Python"}, {"instruction": "def _all_correct_list(array):\n    \"\"\"\n    Make sure, that all items in `array` has good type and size.\n\n    Args:\n        array (list): Array of python types.\n\n    Returns:\n        True/False\n    \"\"\"\n", "input": "", "output": "    if type(array) not in _ITERABLE_TYPES:\n        return False\n\n    for item in array:\n        if not type(item) in _ITERABLE_TYPES:\n            return False\n\n        if len(item) != 2:\n            return False\n\n    return True", "category": "Python"}, {"instruction": "def _call(self, x, out=None):\n        \"\"\"Return ``self(x[, out])``.\"\"\"\n", "input": "", "output": "        # TODO: pass reasonable options on to the interpolator\n        def linear(arg, out=None):\n            ", "category": "Python"}, {"instruction": "def get_field_list(fields, schema):\n  \"\"\" Convert a field list spec into a real list of field names.\n\n      For tables, we return only the top-level non-RECORD fields as Google charts\n      can't handle nested data.\n  \"\"\"\n", "input": "", "output": "  # If the fields weren't supplied get them from the schema.\n  if schema:\n    all_fields = [f['name'] for f in schema._bq_schema if f['type'] != 'RECORD']\n\n  if isinstance(fields, list):\n    if schema:\n      # validate fields exist\n      for f in fields:\n        if f not in all_fields:\n          raise Exception('Cannot find field %s in given schema' % f)\n    return fields\n  if isinstance(fields, basestring) and fields != '*':\n    if schema:\n      # validate fields exist\n      for f in fields.split(','):\n        if f not in all_fields:\n          raise Exception('Cannot find field %s in given schema' % f)\n      return fields.split(',')\n  if not schema:\n    return []\n  return all_fields", "category": "Python"}, {"instruction": "def _encode_ndef_uri_type(self, data):\n        \"\"\"\n        Implement NDEF URI Identifier Code.\n\n        This is a small hack to replace some well known prefixes (such as http://)\n        with a one byte code. If the prefix is not known, 0x00 is used.\n        \"\"\"\n", "input": "", "output": "        t = 0x0\n        for (code, prefix) in uri_identifiers:\n            if data[:len(prefix)].decode('latin-1').lower() == prefix:\n                t = code\n                data = data[len(prefix):]\n                break\n        data = yubico_util.chr_byte(t) + data\n        return data", "category": "Python"}, {"instruction": "def authorization_header(self):\n        \"\"\"\n        Returns a string containing the authorization header used to authenticate\n        with GenePattern. This string is included in the header of subsequent\n        requests sent to GenePattern.\n        \"\"\"\n", "input": "", "output": "        return 'Basic %s' % base64.b64encode(bytes(self.username + ':' + self.password, 'ascii')).decode('ascii')", "category": "Python"}, {"instruction": "def validate_ipv6(self, id_vlan):\n        \"\"\"Validates ACL - IPv6 of VLAN from its identifier.\n\n        Assigns 1 to 'acl_valida_v6'.\n\n        :param id_vlan: Identifier of the Vlan. Integer value and greater than zero.\n\n        :return: None\n\n        :raise InvalidParameterError: Vlan identifier is null and invalid.\n        :raise VlanNaoExisteError: Vlan not registered.\n        :raise DataBaseError: Networkapi failed to access the database.\n        :raise XMLError: Networkapi failed to generate the XML response.\n        \"\"\"\n", "input": "", "output": "\n        if not is_valid_int_param(id_vlan):\n            raise InvalidParameterError(\n                u'The identifier of Vlan is invalid or was not informed.')\n\n        url = 'vlan/' + str(id_vlan) + '/validate/' + IP_VERSION.IPv6[0] + '/'\n\n        code, xml = self.submit(None, 'PUT', url)\n\n        return self.response(code, xml)", "category": "Python"}, {"instruction": "def get_output(script, expanded):\n    \"\"\"Runs the script and obtains stdin/stderr.\n\n    :type script: str\n    :type expanded: str\n    :rtype: str | None\n\n    \"\"\"\n", "input": "", "output": "    env = dict(os.environ)\n    env.update(settings.env)\n\n    is_slow = shlex.split(expanded) in settings.slow_commands\n    with logs.debug_time(u'Call: {}; with env: {}; is slow: '.format(\n            script, env, is_slow)):\n        result = Popen(expanded, shell=True, stdin=PIPE,\n                       stdout=PIPE, stderr=STDOUT, env=env)\n        if _wait_output(result, is_slow):\n            output = result.stdout.read().decode('utf-8')\n            logs.debug(u'Received output: {}'.format(output))\n            return output\n        else:\n            logs.debug(u'Execution timed out!')\n            return None", "category": "Python"}, {"instruction": "def camel_to_snake_case(name):\n    \"\"\"Takes a camelCased string and converts to snake_case.\"\"\"\n", "input": "", "output": "    pattern = r'[A-Z][a-z]+|[A-Z]+(?![a-z])'\n    return '_'.join(map(str.lower, re.findall(pattern, name)))", "category": "Python"}, {"instruction": "def ping(self, timestamp=None):\n        \"\"\"\n        ping [timestamp]\n            Ping the Varnish cache process, keeping the connection alive.\n        \"\"\"\n", "input": "", "output": "        cmd = 'ping'\n        if timestamp: cmd += ' %s' % timestamp\n        return tuple(map(float, self.fetch(cmd)[1].split()[1:]))", "category": "Python"}, {"instruction": "def load_object(self, obj):\n        \"\"\"\n        Find and return the template associated to the given object.\n\n        Arguments:\n\n          obj: an instance of a user-defined class.\n\n          search_dirs: the list of directories in which to search.\n\n        \"\"\"\n", "input": "", "output": "        locator = self._make_locator()\n\n        path = locator.find_object(obj, self.search_dirs)\n\n        return self.read(path)", "category": "Python"}, {"instruction": "def body(self):\n        \"\"\"Response body.\n\n        :raises: :class:`ContentLimitExceeded`, :class:`ContentDecodingError`\n        \"\"\"\n", "input": "", "output": "        content = []\n        length = 0\n        for chunk in self:\n            content.append(chunk)\n            length += len(chunk)\n            if self.length_limit and length > self.length_limit:\n                self.close()\n                raise ContentLimitExceeded(\"Content length is more than %d \"\n                                           \"bytes\" % self.length_limit)\n\n        return b(\"\").join(content)", "category": "Python"}, {"instruction": "def to_dataframe(self, dtypes=None):\n        \"\"\"Create a :class:`pandas.DataFrame` of all rows in the stream.\n\n        This method requires the pandas libary to create a data frame and the\n        fastavro library to parse row blocks.\n\n        .. warning::\n            DATETIME columns are not supported. They are currently parsed as\n            strings in the fastavro library.\n\n        Args:\n            dtypes ( \\\n                Map[str, Union[str, pandas.Series.dtype]] \\\n            ):\n                Optional. A dictionary of column names pandas ``dtype``s. The\n                provided ``dtype`` is used when constructing the series for\n                the column specified. Otherwise, the default pandas behavior\n                is used.\n\n        Returns:\n            pandas.DataFrame:\n                A data frame of all rows in the stream.\n        \"\"\"\n", "input": "", "output": "        if pandas is None:\n            raise ImportError(_PANDAS_REQUIRED)\n\n        frames = []\n        for page in self.pages:\n            frames.append(page.to_dataframe(dtypes=dtypes))\n        return pandas.concat(frames)", "category": "Python"}, {"instruction": "def _get_distance_segment_coefficients(self, rval):\n        \"\"\"\n        Returns the coefficients describing the distance attenuation shape\n        for three different distance bins, equations 12a - 12c\n        \"\"\"\n", "input": "", "output": "        # Get distance segment ends\n        nsites = len(rval)\n        # Equation 12a\n        f_0 = np.log10(self.CONSTS[\"r0\"] / rval)\n        f_0[rval > self.CONSTS[\"r0\"]] = 0.0\n\n        # Equation 12b\n        f_1 = np.log10(rval)\n        f_1[rval > self.CONSTS[\"r1\"]] = np.log10(self.CONSTS[\"r1\"])\n        # Equation 12c\n        f_2 = np.log10(rval / self.CONSTS[\"r2\"])\n        f_2[rval <= self.CONSTS[\"r2\"]] = 0.0\n        return f_0, f_1, f_2", "category": "Python"}, {"instruction": "def mavlink_packet(self, msg):\n        '''handle an incoming mavlink packet'''\n", "input": "", "output": "\n        # check for any closed graphs\n        for i in range(len(self.graphs) - 1, -1, -1):\n            if not self.graphs[i].is_alive():\n                self.graphs[i].close()\n                self.graphs.pop(i)\n\n        # add data to the rest\n        for g in self.graphs:\n            g.add_mavlink_packet(msg)", "category": "Python"}, {"instruction": "def trim(self):\n        \"\"\"Clear not used counters\"\"\"\n", "input": "", "output": "        for key, value in list(iteritems(self.counters)):\n            if value.empty():\n                del self.counters[key]", "category": "Python"}, {"instruction": "def _write_private_key_file(self, tag, filename, data, password=None):\n        \"\"\"\n        Write an SSH2-format private key file in a form that can be read by\n        ssh or openssh.  If no password is given, the key is written in\n        a trivially-encoded format (base64) which is completely insecure.  If\n        a password is given, DES-EDE3-CBC is used.\n\n        @param tag: C{\"RSA\"} or C{\"DSA\"}, the tag used to mark the data block.\n        @type tag: str\n        @param filename: name of the file to write.\n        @type filename: str\n        @param data: data blob that makes up the private key.\n        @type data: str\n        @param password: an optional password to use to encrypt the file.\n        @type password: str\n\n        @raise IOError: if there was an error writing the file.\n        \"\"\"\n", "input": "", "output": "        f = open(filename, 'w', 0600)\n        # grrr... the mode doesn't always take hold\n        os.chmod(filename, 0600)\n        self._write_private_key(tag, f, data, password)\n        f.close()", "category": "Python"}, {"instruction": "def unwrap(self) -> T:\n        \"\"\"\n        Returns the success value in the :class:`Result`.\n\n        Returns:\n            The success value in the :class:`Result`.\n\n        Raises:\n            ``ValueError`` with the message provided by the error value\n             if the :class:`Result` is a :meth:`Result.Err` value.\n\n        Examples:\n            >>> Ok(1).unwrap()\n            1\n            >>> try:\n            ...     Err(1).unwrap()\n            ... except ValueError as e:\n            ...     print(e)\n            1\n        \"\"\"\n", "input": "", "output": "        if self._is_ok:\n            return cast(T, self._val)\n        raise ValueError(self._val)", "category": "Python"}, {"instruction": "def get_consumer_groups(self, consumer_group_id=None, names_only=False):\n        \"\"\"Get information on all the available consumer-groups.\n\n        If names_only is False, only list of consumer-group ids are sent.\n        If names_only is True, Consumer group offset details are returned\n        for all consumer-groups or given consumer-group if given in dict\n        format as:-\n\n        {\n            'group-id':\n            {\n                'topic':\n                {\n                    'partition': offset-value,\n                    ...\n                    ...\n                }\n            }\n        }\n\n        :rtype: dict of consumer-group offset details\n        \"\"\"\n", "input": "", "output": "        if consumer_group_id is None:\n            group_ids = self.get_children(\"/consumers\")\n        else:\n            group_ids = [consumer_group_id]\n\n        # Return consumer-group-ids only\n        if names_only:\n            return {g_id: None for g_id in group_ids}\n\n        consumer_offsets = {}\n        for g_id in group_ids:\n            consumer_offsets[g_id] = self.get_group_offsets(g_id)\n        return consumer_offsets", "category": "Python"}, {"instruction": "def main(input_filename, format):\n    \"\"\"\n    Calculate the fingerprint hashses of the referenced audio file and save\n    to disk as a pickle file\n    \"\"\"\n", "input": "", "output": "\n    # open the file & convert to wav\n    song_data = AudioSegment.from_file(input_filename, format=format)\n    song_data = song_data.set_channels(1)  # convert to mono\n    wav_tmp = song_data.export(format=\"wav\")  # write to a tmp file buffer\n    wav_tmp.seek(0)\n    rate, wav_data = wavfile.read(wav_tmp)\n\n    rows_per_second = (1 + (rate - WIDTH)) // FRAME_STRIDE\n\n    # Calculate a coarser window for matching\n    window_size = (rows_per_second // TIME_STRIDE, (WIDTH // 2) // FREQ_STRIDE)\n    peaks = resound.get_peaks(np.array(wav_data), window_size=window_size)\n\n    # half width (nyquist freq) & half size (window is +/- around the middle)\n    f_width = WIDTH // (2 * FREQ_STRIDE) * 2\n    t_gap = 1 * rows_per_second\n    t_width = 2 * rows_per_second\n    fingerprints = resound.hashes(peaks, f_width=f_width, t_gap=t_gap, t_width=t_width)  # hash, offset pairs\n\n    return fingerprints", "category": "Python"}, {"instruction": "def _check_query(self, query, style_cols=None):\n        \"\"\"Checks if query from Layer or QueryLayer is valid\"\"\"\n", "input": "", "output": "        try:\n            self.sql_client.send(\n                utils.minify_sql((\n                    'EXPLAIN',\n                    'SELECT',\n                    '  {style_cols}{comma}',\n                    '  the_geom, the_geom_webmercator',\n                    'FROM ({query}) _wrap;',\n                )).format(query=query,\n                          comma=',' if style_cols else '',\n                          style_cols=(','.join(style_cols)\n                                      if style_cols else '')),\n                do_post=False)\n        except Exception as err:\n            raise ValueError(('Layer query `{query}` and/or style column(s) '\n                              '{cols} are not valid: {err}.'\n                              '').format(query=query,\n                                         cols=', '.join(['`{}`'.format(c)\n                                                         for c in style_cols]),\n                                         err=err))", "category": "Python"}, {"instruction": "def _sync_outlineexplorer_file_order(self):\r\n        \"\"\"\r\n        Order the root file items of the outline explorer as in the tabbar\r\n        of the current EditorStack.\r\n        \"\"\"\n", "input": "", "output": "        if self.outlineexplorer is not None:\r\n            self.outlineexplorer.treewidget.set_editor_ids_order(\r\n                [finfo.editor.get_document_id() for finfo in self.data])", "category": "Python"}, {"instruction": "def create_director(self, service_id, version_number, \n\t\tname, \n\t\tquorum=75,\n\t\t_type=FastlyDirectorType.RANDOM,\n\t\tretries=5,\n\t\tshield=None):\n\t\t\"\"\"Create a director for a particular service and version.\"\"\"\n", "input": "", "output": "\t\tbody = self._formdata({\n\t\t\t\"name\": name,\n\t\t\t\"quorum\": quorum,\n\t\t\t\"type\": _type,\n\t\t\t\"retries\": retries,\n\t\t\t\"shield\": shield,\n\n\t\t}, FastlyDirector.FIELDS)\n\t\tcontent = self._fetch(\"/service/%s/version/%d/director\" % (service_id, version_number), method=\"POST\", body=body)\n\t\treturn FastlyDirector(self, content)", "category": "Python"}, {"instruction": "def find_rule(self, rule):\n        \"\"\"Find a Rule by name.\"\"\"\n", "input": "", "output": "        defrule = lib.EnvFindDefrule(self._env, rule.encode())\n        if defrule == ffi.NULL:\n            raise LookupError(\"Rule '%s' not found\" % defrule)\n\n        return Rule(self._env, defrule)", "category": "Python"}, {"instruction": "def _crc16_checksum(bytes):\n    \"\"\"Returns the CRC-16 checksum of bytearray bytes\n\n    Ported from Java implementation at: http://introcs.cs.princeton.edu/java/61data/CRC16CCITT.java.html\n\n    Initial value changed to 0x0000 to match Stellar configuration.\n    \"\"\"\n", "input": "", "output": "    crc = 0x0000\n    polynomial = 0x1021\n\n    for byte in bytes:\n        for i in range(8):\n            bit = (byte >> (7 - i) & 1) == 1\n            c15 = (crc >> 15 & 1) == 1\n            crc <<= 1\n            if c15 ^ bit:\n                crc ^= polynomial\n\n    return crc & 0xFFFF", "category": "Python"}, {"instruction": "def is_uncertainty_edition_allowed(self, analysis_brain):\n        \"\"\"Checks if the edition of the uncertainty field is allowed\n\n        :param analysis_brain: Brain that represents an analysis\n        :return: True if the user can edit the result field, otherwise False\n        \"\"\"\n", "input": "", "output": "\n        # Only allow to edit the uncertainty if result edition is allowed\n        if not self.is_result_edition_allowed(analysis_brain):\n            return False\n\n        # Get the ananylsis object\n        obj = api.get_object(analysis_brain)\n\n        # Manual setting of uncertainty is not allowed\n        if not obj.getAllowManualUncertainty():\n            return False\n\n        # Result is a detection limit -> uncertainty setting makes no sense!\n        if obj.getDetectionLimitOperand() in [LDL, UDL]:\n            return False\n\n        return True", "category": "Python"}, {"instruction": "def quad_genz_keister_24 ( order ):\n    \"\"\"\n    Hermite Genz-Keister 24 rule.\n\n    Args:\n        order (int):\n            The quadrature order. Must be in the interval (0, 8).\n\n    Returns:\n        (:py:data:typing.Tuple[numpy.ndarray, numpy.ndarray]):\n            Abscissas and weights\n\n    Examples:\n        >>> abscissas, weights = quad_genz_keister_24(1)\n        >>> print(numpy.around(abscissas, 4))\n        [-1.7321  0.      1.7321]\n        >>> print(numpy.around(weights, 4))\n        [0.1667 0.6667 0.1667]\n    \"\"\"\n", "input": "", "output": "    order = sorted(GENZ_KEISTER_24.keys())[order]\n\n    abscissas, weights = GENZ_KEISTER_24[order]\n    abscissas = numpy.array(abscissas)\n    weights = numpy.array(weights)\n\n    weights /= numpy.sum(weights)\n    abscissas *= numpy.sqrt(2)\n\n    return abscissas, weights", "category": "Python"}, {"instruction": "def build_k5_graph():\n    \"\"\"Makes a new K5 graph.\n       Ref: http://mathworld.wolfram.com/Pentatope.html\"\"\"\n", "input": "", "output": "    graph = UndirectedGraph()\n\n    # K5 has 5 nodes\n    for _ in range(5):\n        graph.new_node()\n\n    # K5 has 10 edges\n    # --Edge: a\n    graph.new_edge(1, 2)\n    # --Edge: b\n    graph.new_edge(2, 3)\n    # --Edge: c\n    graph.new_edge(3, 4)\n    # --Edge: d\n    graph.new_edge(4, 5)\n    # --Edge: e\n    graph.new_edge(5, 1)\n    # --Edge: f\n    graph.new_edge(1, 3)\n    # --Edge: g\n    graph.new_edge(1, 4)\n    # --Edge: h\n    graph.new_edge(2, 4)\n    # --Edge: i\n    graph.new_edge(2, 5)\n    # --Edge: j\n    graph.new_edge(3, 5)\n\n    return graph", "category": "Python"}, {"instruction": "def save_cookie(self, response, key='session', expires=None,\n                    session_expires=None, max_age=None, path='/', domain=None,\n                    secure=None, httponly=False, force=False):\n        \"\"\"Saves the SecureCookie in a cookie on response object.  All\n        parameters that are not described here are forwarded directly\n        to :meth:`~BaseResponse.set_cookie`.\n\n        :param response: a response object that has a\n                         :meth:`~BaseResponse.set_cookie` method.\n        :param key: the name of the cookie.\n        :param session_expires: the expiration date of the secure cookie\n                                stored information.  If this is not provided\n                                the cookie `expires` date is used instead.\n        \"\"\"\n", "input": "", "output": "        if force or self.should_save:\n            data = self.serialize(session_expires or expires)\n            response.set_cookie(key, data, expires=expires, max_age=max_age,\n                                path=path, domain=domain, secure=secure,\n                                httponly=httponly)", "category": "Python"}, {"instruction": "def read(self):\n        \"\"\"Read a wire format DNS message and build a dns.message.Message\n        object.\"\"\"\n", "input": "", "output": "\n        l = len(self.wire)\n        if l < 12:\n            raise ShortHeader\n        (self.message.id, self.message.flags, qcount, ancount,\n         aucount, adcount) = struct.unpack('!HHHHHH', self.wire[:12])\n        self.current = 12\n        if dns.opcode.is_update(self.message.flags):\n            self.updating = True\n        self._get_question(qcount)\n        if self.question_only:\n            return\n        self._get_section(self.message.answer, ancount)\n        self._get_section(self.message.authority, aucount)\n        self._get_section(self.message.additional, adcount)\n        if self.current != l:\n            raise TrailingJunk\n        if self.message.multi and self.message.tsig_ctx and \\\n               not self.message.had_tsig:\n            self.message.tsig_ctx.update(self.wire)", "category": "Python"}, {"instruction": "def to_markdown_(self) -> str:\n        \"\"\"Convert the main dataframe to markdown\n\n        :return: markdown data\n        :rtype: str\n\n        :example: ``ds.to_markdown_()``\n        \"\"\"\n", "input": "", "output": "        try:\n            renderer = pytablewriter.MarkdownTableWriter\n            data = self._build_export(renderer)\n            return data\n        except Exception as e:\n            self.err(e, \"Can not convert data to markdown\")", "category": "Python"}, {"instruction": "def read_cz_lsm_time_stamps(fd, byte_order):\n    \"\"\"Read LSM time stamps from file and return as list.\"\"\"\n", "input": "", "output": "    size, count = struct.unpack(byte_order+'II', fd.read(8))\n    if size != (8 + 8 * count):\n        raise ValueError(\"lsm_time_stamps block is too short\")\n    return struct.unpack(('%s%dd' % (byte_order, count)),\n                         fd.read(8*count))", "category": "Python"}, {"instruction": "def add_field(self, field_name: str, field: Field, vocab: Vocabulary = None) -> None:\n        \"\"\"\n        Add the field to the existing fields mapping.\n        If we have already indexed the Instance, then we also index `field`, so\n        it is necessary to supply the vocab.\n        \"\"\"\n", "input": "", "output": "        self.fields[field_name] = field\n        if self.indexed:\n            field.index(vocab)", "category": "Python"}, {"instruction": "def get_ipv4_table(self):\n        \"\"\"Returns global IPv4 table.\n\n        Creates the table if it does not exist.\n        \"\"\"\n", "input": "", "output": "\n        vpn_table = self._global_tables.get(RF_IPv4_UC)\n        # Lazy initialize the table.\n        if not vpn_table:\n            vpn_table = Ipv4Table(self._core_service, self._signal_bus)\n            self._global_tables[RF_IPv4_UC] = vpn_table\n            self._tables[(None, RF_IPv4_UC)] = vpn_table\n\n        return vpn_table", "category": "Python"}, {"instruction": "def rename_files(files, name=None):\n    \"\"\"\n    Given a list of file paths for elevation files, this function will rename\n    those files to the format required by the pyDEM package.\n\n    This assumes a .tif extension.\n\n    Parameters\n    -----------\n    files : list\n        A list of strings of the paths to the elevation files that will be\n        renamed\n    name : str (optional)\n        Default = None. A suffix to the filename. For example\n        <filename>_suffix.tif\n\n    Notes\n    ------\n    The files are renamed in the same directory as the original file locations\n    \"\"\"\n", "input": "", "output": "    for fil in files:\n        elev_file = GdalReader(file_name=fil)\n        elev, = elev_file.raster_layers\n        fn = get_fn(elev, name)\n        del elev_file\n        del elev\n        fn = os.path.join(os.path.split(fil)[0], fn)\n        os.rename(fil, fn)\n        print \"Renamed\", fil, \"to\", fn", "category": "Python"}, {"instruction": "def entropy_variance(data, class_attr=None,\n    method=DEFAULT_CONTINUOUS_METRIC):\n    \"\"\"\n    Calculates the variance fo a continuous class attribute, to be used as an\n    entropy metric.\n    \"\"\"\n", "input": "", "output": "    assert method in CONTINUOUS_METRICS, \"Unknown entropy variance metric: %s\" % (method,)\n    assert (class_attr is None and isinstance(data, dict)) \\\n        or (class_attr is not None and isinstance(data, list))\n    if isinstance(data, dict):\n        lst = data\n    else:\n        lst = [record.get(class_attr) for record in data]\n    return get_variance(lst)", "category": "Python"}, {"instruction": "def clone(self):\n        ''' Returns a shallow copy of the current instance, except that all\n        variables are deep-cloned.\n        '''\n", "input": "", "output": "        clone = copy(self)\n        clone.variables = {k: v.clone() for (k, v) in self.variables.items()}\n        return clone", "category": "Python"}, {"instruction": "def rollback(self, date):\n        \"\"\"Roll date backward to nearest start of year\"\"\"\n", "input": "", "output": "        if self.onOffset(date):\n            return date\n        else:\n            return date - YearBegin(month=self.month)", "category": "Python"}, {"instruction": "def _delete(self, url, **kwargs):\n        \"\"\"\n        Wrapper for the HTTP DELETE request.\n        \"\"\"\n", "input": "", "output": "\n        response = retry_request(self)(self._http_delete)(url, **kwargs)\n\n        if self.raw_mode:\n            return response\n\n        if response.status_code >= 300:\n            error = get_error(response)\n            if self.raise_errors:\n                raise error\n            return error\n\n        return response", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"Start operating the strategy.\n\n        Subclasses that override start() should call the super method before\n        it does anything that uses the ioloop. This will attach to the sensor\n        as an observer if :attr:`OBSERVE_UPDATES` is True, and sets\n        :attr:`_ioloop_thread_id` using `thread.get_ident()`.\n\n        \"\"\"\n", "input": "", "output": "        def first_run():\n            self._ioloop_thread_id = get_thread_ident()\n            if self.OBSERVE_UPDATES:\n                self.attach()\n        self.ioloop.add_callback(first_run)", "category": "Python"}, {"instruction": "def pix2vec(nside, ipix, nest=False):\n    \"\"\"Drop-in replacement for healpy `~healpy.pixelfunc.pix2vec`.\"\"\"\n", "input": "", "output": "    lon, lat = healpix_to_lonlat(ipix, nside, order='nested' if nest else 'ring')\n    return ang2vec(*_lonlat_to_healpy(lon, lat))", "category": "Python"}, {"instruction": "def next_trigger_frequency(self, utc_now=None):\n        \"\"\" :param utc_now: optional parameter to be used by Unit Tests as a definition of \"now\"\n            :return: datetime instance presenting next trigger time of the event \"\"\"\n", "input": "", "output": "        if utc_now is None:\n            utc_now = datetime.utcnow()\n\n        def wind_days(start_date):\n            while True:\n                if self.day_of_week == EVERY_DAY or start_date.weekday() == int(self.day_of_week):\n                    return start_date.replace(hour=self.time_of_day.hour, minute=self.time_of_day.minute)\n                else:\n                    start_date += timedelta(days=1)\n\n        if utc_now.time() > self.time_of_day.time():\n            return wind_days(utc_now + timedelta(days=1))\n        else:\n            return wind_days(utc_now)", "category": "Python"}, {"instruction": "def spelling(self):\n        \"\"\"Return the spelling of the entity pointed at by the cursor.\"\"\"\n", "input": "", "output": "        if not hasattr(self, '_spelling'):\n            self._spelling = conf.lib.clang_getCursorSpelling(self)\n\n        return self._spelling", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Cleanup client resources and disconnect from MongoDB.\n\n        On MongoDB >= 3.6, end all server sessions created by this client by\n        sending one or more endSessions commands.\n\n        Close all sockets in the connection pools and stop the monitor threads.\n        If this instance is used again it will be automatically re-opened and\n        the threads restarted.\n\n        .. versionchanged:: 3.6\n           End all server sessions created by this client.\n        \"\"\"\n", "input": "", "output": "        session_ids = self._topology.pop_all_sessions()\n        if session_ids:\n            self._end_sessions(session_ids)\n        # Stop the periodic task thread and then run _process_periodic_tasks\n        # to send pending killCursor requests before closing the topology.\n        self._kill_cursors_executor.close()\n        self._process_periodic_tasks()\n        self._topology.close()", "category": "Python"}, {"instruction": "def register(self, hook):\n        \"\"\"\n        Register a hook.\n\n        @hook: a HookBase subclass reference.\n        \"\"\"\n", "input": "", "output": "        assert callable(hook), \\\n            \"Hook must be a callable\"\n        assert issubclass(hook, HookBase), \\\n            \"The hook does not inherit from HookBase\"\n\n        self._registry.append(hook)", "category": "Python"}, {"instruction": "def create(self, ogpgs):\n        \"\"\"\n        Method to create object group permissions general\n\n        :param ogpgs: List containing vrf desired to be created on database\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        data = {'ogpgs': ogpgs}\n        return super(ApiObjectGroupPermissionGeneral, self).post('api/v3/object-group-perm-general/', data)", "category": "Python"}, {"instruction": "def _render_module_flags(self, module, flags, output_lines, prefix=''):\n    \"\"\"Returns a help string for a given module.\"\"\"\n", "input": "", "output": "    if not isinstance(module, str):\n      module = module.__name__\n    output_lines.append('\\n%s%s:' % (prefix, module))\n    self._render_flag_list(flags, output_lines, prefix + '  ')", "category": "Python"}, {"instruction": "def nchunks_initialized(self):\n        \"\"\"The number of chunks that have been initialized with some data.\"\"\"\n", "input": "", "output": "\n        # key pattern for chunk keys\n        prog = re.compile(r'\\.'.join([r'\\d+'] * min(1, self.ndim)))\n\n        # count chunk keys\n        return sum(1 for k in listdir(self.chunk_store, self._path) if prog.match(k))", "category": "Python"}, {"instruction": "def msg_intro(self):\n        \"\"\"Introductory message disabled above heading.\"\"\"\n", "input": "", "output": "        delim = self.style.attr_minor(self.style.delimiter)\n        txt = self.intro_msg_fmt.format(delim=delim).rstrip()\n        return self.term.center(txt)", "category": "Python"}, {"instruction": "def _parse_add_url(url):\n    \"\"\" return a tuple (host, job_name) from a url \"\"\"\n", "input": "", "output": "    parsed_url = urlparse(url)\n    job_name = None\n    paths = parsed_url.path.strip(\"/\").split(\"/\")\n    for i, path in enumerate(paths):\n        if path == \"job\" and len(paths) > i:\n            job_name = paths[i + 1]\n    if job_name is None:\n        raise ConfigException(\"Unable to parse valid job from {0}\".format(url))\n    return (\n        \"{0}://{1}\".format(parsed_url.scheme, parsed_url.netloc),\n        job_name\n    )", "category": "Python"}, {"instruction": "def contract(s):\n    \"\"\"\n    >>> assert contract(\"1 1 1 2 2 3\") == \"3*1 2*2 1*3\"\n    >>> assert contract(\"1 1 3 2 3\") == \"2*1 1*3 1*2 1*3\"\n    \"\"\"\n", "input": "", "output": "    if not s: return s\n\n    tokens = s.split()\n    old = tokens[0]\n    count = [[1, old]]\n\n    for t in tokens[1:]:\n        if t == old:\n            count[-1][0] += 1\n        else:\n            old = t\n            count.append([1, t])\n\n    return \" \".join(\"%d*%s\" % (c, t) for c, t in count)", "category": "Python"}, {"instruction": "def direct_bind(self, username, password):\n        \"\"\"\n        Bind to username/password directly\n        \"\"\"\n", "input": "", "output": "        log.debug(\"Performing direct bind\")\n\n        ctx = {'username':username, 'password':password}\n        scope = self.config.get('SCOPE', ldap.SCOPE_SUBTREE)\n        user = self.config['BIND_DN'] % ctx\n\n        try:\n            log.debug(\"Binding with the BIND_DN %s\" % user)\n            self.conn.simple_bind_s(user, password)\n        except ldap.INVALID_CREDENTIALS:\n            if self._raise_errors:\n                raise ldap.INVALID_CREDENTIALS(\"Unable to do a direct bind with BIND_DN %s\" % user)\n            return None\n        results = self.conn.search_s(user, scope, attrlist=self.attrlist)\n        self.conn.unbind_s()\n        return self.format_results(results)", "category": "Python"}, {"instruction": "def available(name):\n    '''\n    Returns ``True`` if the specified service is available, otherwise returns\n    ``False``.\n\n    We look up the name with the svcs command to get back the FMRI\n    This allows users to use simpler service names\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.available net-snmp\n    '''\n", "input": "", "output": "    cmd = '/usr/bin/svcs -H -o FMRI {0}'.format(name)\n    name = __salt__['cmd.run'](cmd, python_shell=False)\n    return name in get_all()", "category": "Python"}, {"instruction": "def _apiv1_run(self, action, headers=None, **kwargs):\n        \"\"\"\n        Kept for backwards compatibility of this client\n        See \"self.clear_reference_language\"\n        \"\"\"\n", "input": "", "output": "        warnings.warn(\n            \"POEditor API v1 is deprecated. Use POEditorAPI._run method to call API v2\",\n            DeprecationWarning, stacklevel=2\n        )\n\n        url = \"https://poeditor.com/api/\"\n        payload = kwargs\n        payload.update({'action': action, 'api_token': self.api_token})\n\n        return self._make_request(url, payload, headers)", "category": "Python"}, {"instruction": "def value(self, kind, device):\n        \"\"\"\n        Get the value for the device object associated with this filter.\n\n        If :meth:`match` is False for the device, the return value of this\n        method is undefined.\n        \"\"\"\n", "input": "", "output": "        self._log.debug(_('{0}(match={1!r}, {2}={3!r}) used for {4}',\n                          self.__class__.__name__,\n                          self._match,\n                          kind, self._values[kind],\n                          device.object_path))\n        return self._values[kind]", "category": "Python"}, {"instruction": "def refresh(self):\n        \"\"\"\n        explicitely refresh user interface; useful when changing widgets dynamically\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"refresh user interface\")\n        try:\n            with self.refresh_lock:\n                self.draw_screen()\n        except AssertionError:\n            logger.warning(\"application is not running\")\n            pass", "category": "Python"}, {"instruction": "def write(*args):\n    \"\"\"Like print(), but recognizes tensors and arrays and show\n    more details about them.\n\n    Example:\n        hl.write(\"My Tensor\", my_tensor)\n    \n        Prints:\n            My Tensor  float32 (10, 3, 224, 224)  min: 0.0  max: 1.0\n    \"\"\"\n", "input": "", "output": "    s = \"\"\n    for a in args:\n        # Convert tensors to Numpy arrays\n        a = to_data(a)\n\n        if isinstance(a, np.ndarray):\n            # Numpy Array\n            s += (\"\\t\" if s else \"\") + \"Tensor  {} {}  min: {:.3f}  max: {:.3f}\".format(\n                a.dtype, a.shape, a.min(), a.max())\n            print(s)\n            s = \"\"\n        elif isinstance(a, list):\n            s += (\"\\t\" if s else \"\") + \"list    len: {}  {}\".format(len(a), a[:10])\n        else:\n            s += (\" \" if s else \"\") + str(a)\n    if s:\n        print(s)", "category": "Python"}, {"instruction": "def removeComments( self, comment = None ):\n        \"\"\"\n        Inserts comments into the editor based on the current selection.\\\n        If no comment string is supplied, then the comment from the language \\\n        will be used.\n        \n        :param      comment | <str> || None\n        \n        :return     <bool> | success\n        \"\"\"\n", "input": "", "output": "        if ( not comment ):\n            lang = self.language()\n            if ( lang ):\n                comment = lang.lineComment()\n        \n        if ( not comment ):\n            return False\n        \n        startline, startcol, endline, endcol = self.getSelection()\n        len_comment = len(comment)\n        line, col = self.getCursorPosition()\n        \n        for lineno in range(startline, endline+1 ):\n            self.setSelection(lineno, 0, lineno, len_comment)\n            if ( self.selectedText() == comment ):\n                self.removeSelectedText()\n        \n        self.setSelection(startline, startcol, endline, endcol)    \n        self.setCursorPosition(line, col)\n        \n        return True", "category": "Python"}, {"instruction": "def is_updated(self):\n        \"\"\"\n        Checks if a resource has been updated since last publish.\n        Returns False if resource has not been published before.\n        \"\"\"\n", "input": "", "output": "\n        if not self.is_published:\n            return False\n\n        return sanitize_date(self.sys['published_at']) < sanitize_date(self.sys['updated_at'])", "category": "Python"}, {"instruction": "def get_params(self, *args, **kwargs):\n        '''\n        Create an array or positional or named parameters\n        Mixing positional and named parameters in one\n        call is not possible.\n        '''\n", "input": "", "output": "        kwargs.update(self._data)\n        if args and kwargs:\n            raise ValueError('Cannot mix positional and named parameters')\n        if args:\n            return list(args)\n        else:\n            return kwargs", "category": "Python"}, {"instruction": "def shift(x, offset, dim, wrap, name=None):\n  \"\"\"Shift operation.\n\n  Shift x right by +offset in dimension dim.\n\n  Args:\n    x: a Tensor\n    offset: an integer. If negative, shift left instead of right.\n    dim: a Dimension of x\n    wrap: a boolean - whether to wrap (True) or pad with zeros (False).\n    name: an optional string\n\n  Returns:\n    a Tensor with the same shape and dtype as x\n  \"\"\"\n", "input": "", "output": "  return ShiftOperation(x, offset, dim, wrap, name=name).outputs[0]", "category": "Python"}, {"instruction": "def _flow_for_request(self):\n        \"\"\"\n        Build a flow with the correct absolute callback URL for this request.\n        :return:\n        \"\"\"\n", "input": "", "output": "        flow = copy(self.flow)\n        redirect_uri = current_app.config['OVERWRITE_REDIRECT_URI']\n        if not redirect_uri:\n            flow.redirect_uri = url_for('_oidc_callback', _external=True)\n        else:\n            flow.redirect_uri = redirect_uri\n        return flow", "category": "Python"}, {"instruction": "def transaction(self, collections, action, waitForSync = False, lockTimeout = None, params = None) :\n        \"\"\"Execute a server-side transaction\"\"\"\n", "input": "", "output": "        payload = {\n                \"collections\": collections,\n                \"action\": action,\n                \"waitForSync\": waitForSync}\n        if lockTimeout is not None:\n                payload[\"lockTimeout\"] = lockTimeout\n        if params is not None:\n            payload[\"params\"] = params\n\n        self.connection.reportStart(action)\n\n        r = self.connection.session.post(self.transactionURL, data = json.dumps(payload, default=str))\n\n        self.connection.reportItem()\n\n        data = r.json()\n\n        if (r.status_code == 200 or r.status_code == 201 or r.status_code == 202) and not data.get(\"error\") :\n            return data\n        else :\n            raise TransactionError(data[\"errorMessage\"], action, data)", "category": "Python"}, {"instruction": "def geo_max_distance(left, right):\n    \"\"\"Returns the 2-dimensional maximum distance between two geometries in\n    projected units. If g1 and g2 is the same geometry the function will\n    return the distance between the two vertices most far from each other\n    in that geometry\n\n    Parameters\n    ----------\n    left : geometry\n    right : geometry\n\n    Returns\n    -------\n    MaxDistance : double scalar\n    \"\"\"\n", "input": "", "output": "    op = ops.GeoMaxDistance(left, right)\n    return op.to_expr()", "category": "Python"}, {"instruction": "async def parse_form(self, req: Request, name: str, field: Field) -> typing.Any:\n        \"\"\"Pull a form value from the request.\"\"\"\n", "input": "", "output": "        post_data = self._cache.get(\"post\")\n        if post_data is None:\n            self._cache[\"post\"] = await req.post()\n        return core.get_value(self._cache[\"post\"], name, field)", "category": "Python"}, {"instruction": "def send(self, message, body_params=None):\n        \"\"\".. versionchanged:: 0.8.4\n        Send a message to CM\n\n        :param message: a message instance\n        :type message: :class:`.Msg`, :class:`.MsgProto`\n        :param body_params: a dict with params to the body (only :class:`.MsgProto`)\n        :type body_params: dict\n        \"\"\"\n", "input": "", "output": "        if not self.connected:\n            self._LOG.debug(\"Trying to send message when not connected. (discarded)\")\n        else:\n            if body_params and isinstance(message, MsgProto):\n                proto_fill_from_dict(message.body, body_params)\n\n            CMClient.send(self, message)", "category": "Python"}, {"instruction": "def times(self, query):\n        \"\"\" Retorna o resultado da busca ao Cartola por um determinado termo de pesquisa.\n\n        Args:\n            query (str): Termo para utilizar na busca.\n\n        Returns:\n            Uma lista de inst\u00e2ncias de cartolafc.TimeInfo, uma para cada time contento o termo utilizado na busca.\n        \"\"\"\n", "input": "", "output": "        url = '{api_url}/times'.format(api_url=self._api_url)\n        data = self._request(url, params=dict(q=query))\n        return [TimeInfo.from_dict(time_info) for time_info in data]", "category": "Python"}, {"instruction": "def pendulum_time_to_datetime_time(x: Time) -> datetime.time:\n    \"\"\"\n    Takes a :class:`pendulum.Time` and returns a :class:`datetime.time`.\n    Used, for example, where a database backend insists on\n    :class:`datetime.time`.\n    \"\"\"\n", "input": "", "output": "    return datetime.time(\n        hour=x.hour, minute=x.minute, second=x.second,\n        microsecond=x.microsecond,\n        tzinfo=x.tzinfo\n    )", "category": "Python"}, {"instruction": "def stops(self, rt, direction):\n        \"\"\"\n        Return a list of stops for a particular route.\n        \n        Arguments:\n            `rt`: route designator\n            `dir`: route direction (INBOUND, OUTBOUND)\n        \n        Response:\n            `stop`: (stop container) contains list of \n                `stpid`: unique ID number for bus stop\n                `stpnm`: stop name (what shows up on the display in the bus,\n                         e.g., \"Forbes and Murray\")\n                `lat`, `lng`: location of stop\n                \n        http://realtime.portauthority.org/bustime/apidoc/v1/main.jsp?section=stops.jsp        \n        \"\"\"\n", "input": "", "output": "        url = self.endpoint('STOPS', dict(rt=rt, dir=direction))\n        return self.response(url)", "category": "Python"}, {"instruction": "def append(self, x):\n        \"\"\"\n        Append the given element to the list and synchronize the textual\n        representation.\n        \"\"\"\n", "input": "", "output": "        super(EntriesList, self).append(x)\n\n        if self.entries_collection is not None:\n            self.entries_collection.add_entry(self.date, x)", "category": "Python"}, {"instruction": "def add_documents(self, docs):\n        \"\"\"Update dictionary from a collection of documents. Each document is a list\n        of tokens.\n\n        Args:\n            docs (list): documents to add.\n        \"\"\"\n", "input": "", "output": "        for sent in docs:\n            sent = map(self.process_token, sent)\n            self._token_count.update(sent)", "category": "Python"}, {"instruction": "def spherical_to_cartesian(rho, phi, theta):\n    \"\"\" Maps spherical coordinates (rho,phi,theta) to cartesian coordinates\n    (x,y,z) where phi is in [0,2*pi] and theta is in [0,pi].\n\n    Parameters\n    ----------\n    rho : {numpy.array, float}\n        The radial amplitude.\n    phi : {numpy.array, float}\n        The azimuthal angle.\n    theta : {numpy.array, float}\n        The polar angle.\n\n    Returns\n    -------\n    x : {numpy.array, float}\n        X-coordinate.\n    y : {numpy.array, float}\n        Y-coordinate.\n    z : {numpy.array, float}\n        Z-coordinate.\n    \"\"\"\n", "input": "", "output": "    x = rho * numpy.cos(phi) * numpy.sin(theta)\n    y = rho * numpy.sin(phi) * numpy.sin(theta)\n    z = rho * numpy.cos(theta)\n    return x, y, z", "category": "Python"}, {"instruction": "def _get_description(self):\n        \"\"\"Return human readable description error description.\n\n        This description should explain everything that went wrong during\n        deserialization.\n\n        \"\"\"\n", "input": "", "output": "        return \", \".join([\n            part for part in [\n                \"missing: {}\".format(self.missing) if self.missing else \"\",\n                (\n                    \"forbidden: {}\".format(self.forbidden)\n                    if self.forbidden else \"\"\n                ),\n                \"invalid: {}:\".format(self.invalid) if self.invalid else \"\",\n                (\n                    \"failed to parse: {}\".format(self.failed)\n                    if self.failed else \"\"\n                )\n            ] if part\n        ])", "category": "Python"}, {"instruction": "def squid_to_guid(squid):\n    '''\n    Converts a compressed GUID (SQUID) back into a GUID\n\n    Args:\n\n        squid (str): A valid compressed GUID\n\n    Returns:\n        str: A valid GUID\n    '''\n", "input": "", "output": "    squid_pattern = re.compile(r'^(\\w{8})(\\w{4})(\\w{4})(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)(\\w\\w)$')\n    squid_match = squid_pattern.match(squid)\n    guid = ''\n    if squid_match is not None:\n        guid = '{' + \\\n               squid_match.group(1)[::-1]+'-' + \\\n               squid_match.group(2)[::-1]+'-' + \\\n               squid_match.group(3)[::-1]+'-' + \\\n               squid_match.group(4)[::-1]+squid_match.group(5)[::-1] + '-'\n        for index in range(6, 12):\n            guid += squid_match.group(index)[::-1]\n        guid += '}'\n    return guid", "category": "Python"}, {"instruction": "def header_remove(self, value):\n        \"\"\"Automatically remove specified HTTP header from the response.\n\n        :param str|unicode value:\n\n        \"\"\"\n", "input": "", "output": "        self._set('del-header', value, multi=True)\n\n        return self._section", "category": "Python"}, {"instruction": "def load(self, cellpy_file, parent_level=\"CellpyData\"):\n        \"\"\"Loads a cellpy file.\n\n        Args:\n            cellpy_file (path, str): Full path to the cellpy file.\n            parent_level (str, optional): Parent level\n\n        \"\"\"\n", "input": "", "output": "\n        try:\n            self.logger.debug(\"loading cellpy-file (hdf5):\")\n            self.logger.debug(cellpy_file)\n            new_datasets = self._load_hdf5(cellpy_file, parent_level)\n            self.logger.debug(\"cellpy-file loaded\")\n        except AttributeError:\n            new_datasets = []\n            self.logger.warning(\"This cellpy-file version is not supported by\"\n                                \"current reader (try to update cellpy).\")\n\n        if new_datasets:\n            for dataset in new_datasets:\n                self.datasets.append(dataset)\n        else:\n            # raise LoadError\n            self.logger.warning(\"Could not load\")\n            self.logger.warning(str(cellpy_file))\n\n        self.number_of_datasets = len(self.datasets)\n        self.status_datasets = self._validate_datasets()\n        self._invent_a_name(cellpy_file)\n        return self", "category": "Python"}, {"instruction": "def _sumterm_prime(lexer):\n    \"\"\"Return a sum term' expression, eliminates left recursion.\"\"\"\n", "input": "", "output": "    tok = next(lexer)\n    # '|' XORTERM SUMTERM'\n    if isinstance(tok, OP_or):\n        xorterm = _xorterm(lexer)\n        sumterm_prime = _sumterm_prime(lexer)\n        if sumterm_prime is None:\n            return xorterm\n        else:\n            return ('or', xorterm, sumterm_prime)\n    # null\n    else:\n        lexer.unpop_token(tok)\n        return None", "category": "Python"}, {"instruction": "def set_duration(self, dur):\n        \"\"\"See :meth:`AbstractCalibrationRunner<sparkle.run.calibration_runner.AbstractCalibrationRunner.set_duration>`\"\"\"\n", "input": "", "output": "        # this may be set at any time, and is not checked before run, so set\n        # all stim components\n        for comp in self.stim_components:\n            comp.setDuration(dur)\n        self.reftone.setDuration(dur)", "category": "Python"}, {"instruction": "def hexdumpLine (bytes, length=None):\n  \"\"\"hexdumpLine(bytes[, length])\n\n  Returns a single hexdump formatted line for bytes.  If length is\n  greater than len(bytes), the line will be padded with ASCII space\n  characters to indicate no byte data is present.\n\n  Used by hexdump().\n  \"\"\"\n", "input": "", "output": "  line = \"\"\n\n  if length is None:\n    length = len(bytes)\n\n  for n in xrange(0, length, 2):\n    if n < len(bytes) - 1:\n      line += \"%02x%02x  \" % (bytes[n], bytes[n + 1])\n    elif n < len(bytes):\n      line += \"%02x    \"   % bytes[n]\n    else:\n      line += \"      \"\n\n  line += \"*\"\n\n  for n in xrange(length):\n    if n < len(bytes):\n      if bytes[n] in xrange(32, 127):\n        line += \"%c\" % bytes[n]\n      else:\n        line += \".\"\n    else:\n      line += \" \"\n\n  line += \"*\"\n  return line", "category": "Python"}, {"instruction": "def same_kind(self, other):\n        \"\"\"\n        Return True if \"other\" is an object of the same type and it was\n        instantiated with the same parameters\n        \"\"\"\n", "input": "", "output": "\n        return type(self) is type(other) and self._same_parameters(other)", "category": "Python"}, {"instruction": "def helper(*commands):\n    \"\"\"Decorate a function to be the helper function of commands.\n\n    Arguments:\n        commands: Names of command that should trigger this function object.\n\n    ---------------------------\n    Interface of helper methods:\n\n        @helper('some-command')\n        def help_foo(self, args):\n            '''\n            Arguments:\n                args: A list of arguments.\n\n            Returns:\n                A string that is the help message.\n            '''\n            pass\n    \"\"\"\n", "input": "", "output": "    def decorated_func(f):\n        f.__help_targets__ = list(commands)\n        return f\n    return decorated_func", "category": "Python"}, {"instruction": "def _check_import_source():\n        \"\"\"Check if tlgu imported, if not import it.\"\"\"\n", "input": "", "output": "        path_rel = '~/cltk_data/greek/software/greek_software_tlgu/tlgu.h'\n        path = os.path.expanduser(path_rel)\n        if not os.path.isfile(path):\n            try:\n                corpus_importer = CorpusImporter('greek')\n                corpus_importer.import_corpus('greek_software_tlgu')\n            except Exception as exc:\n                logger.error('Failed to import TLGU: %s', exc)\n                raise", "category": "Python"}, {"instruction": "def set_ylim_cb(self, redraw=True):\n        \"\"\"Set plot limit based on user values.\"\"\"\n", "input": "", "output": "        try:\n            ymin = float(self.w.y_lo.get_text())\n        except Exception:\n            set_min = True\n        else:\n            set_min = False\n\n        try:\n            ymax = float(self.w.y_hi.get_text())\n        except Exception:\n            set_max = True\n        else:\n            set_max = False\n\n        if set_min or set_max:\n            self.tab_plot.draw()\n            self.set_ylimits_widgets(set_min=set_min, set_max=set_max)\n\n        if not (set_min and set_max):\n            self.tab_plot.ax.set_ylim(ymin, ymax)\n            if redraw:\n                self.tab_plot.draw()", "category": "Python"}, {"instruction": "def import_class(classpath):\n    \"\"\"Import the class referred to by the fully qualified class path.\n\n    Args:\n        classpath: A full \"foo.bar.MyClass\" path to a class definition.\n\n    Returns:\n        The class referred to by the classpath.\n\n    Raises:\n        ImportError: If an error occurs while importing the module.\n        AttributeError: IF the class does not exist in the imported module.\n    \"\"\"\n", "input": "", "output": "    modname, classname = classpath.rsplit(\".\", 1)\n    module = importlib.import_module(modname)\n    klass  = getattr(module, classname)\n    return klass", "category": "Python"}, {"instruction": "def set_hook(self, phase, action):\n        \"\"\"Allows setting hooks (attaching actions) for various uWSGI phases.\n\n        :param str|unicode phase: See constants in ``.phases``.\n\n        :param str|unicode|list|HookAction|list[HookAction] action:\n\n        \"\"\"\n", "input": "", "output": "        self._set('hook-%s' % phase, action, multi=True)\n\n        return self._section", "category": "Python"}, {"instruction": "def get_stp_mst_detail_output_cist_port_edge_port(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_stp_mst_detail = ET.Element(\"get_stp_mst_detail\")\n        config = get_stp_mst_detail\n        output = ET.SubElement(get_stp_mst_detail, \"output\")\n        cist = ET.SubElement(output, \"cist\")\n        port = ET.SubElement(cist, \"port\")\n        edge_port = ET.SubElement(port, \"edge-port\")\n        edge_port.text = kwargs.pop('edge_port')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def clean_old_jobs():\n    '''\n    Clean out minions's return data for old jobs.\n\n    Normally, hset 'ret:<jid>' are saved with a TTL, and will eventually\n    get cleaned by redis.But for jobs with some very late minion return, the\n    corresponding hset's TTL will be refreshed to a too late timestamp, we'll\n    do manually cleaning here.\n    '''\n", "input": "", "output": "    serv = _get_serv(ret=None)\n    ret_jids = serv.keys('ret:*')\n    living_jids = set(serv.keys('load:*'))\n    to_remove = []\n    for ret_key in ret_jids:\n        load_key = ret_key.replace('ret:', 'load:', 1)\n        if load_key not in living_jids:\n            to_remove.append(ret_key)\n    if to_remove:\n        serv.delete(*to_remove)\n        log.debug('clean old jobs: %s', to_remove)", "category": "Python"}, {"instruction": "def truncate_to_field_length(self, field, value):\n        \"\"\"Truncate the value of a string field to the field's max length.\n\n        Use this in a validator to check/truncate values before inserting them into the database.\n        Copy the below example code after ``@validates`` to your model class and replace ``field1`` and ``field2`` with\n        your field name(s).\n\n        :Example:\n\n            from sqlalchemy.orm import validates\n            # ... omitting other imports ...\n\n            class MyModel(base.Base):\n\n                field1 = Column(String(128))\n                field2 = Column(String(64))\n\n                @validates('field1', 'field2')\n                def truncate(self, field, value):\n                    return self.truncate_to_field_length(field, value)\n\n        Args:\n            field (str): field name to validate\n            value (str/unicode): value to validate\n\n        Returns:\n            str/unicode: value truncated to field max length\n\n        \"\"\"\n", "input": "", "output": "        max_len = getattr(self.__class__, field).prop.columns[0].type.length\n        if value and len(value) > max_len:\n            return value[:max_len]\n        else:\n            return value", "category": "Python"}, {"instruction": "def headloss_gen(Area, Vel, PerimWetted, Length, KMinor, Nu, PipeRough):\n    \"\"\"Return the total head lossin the general case.\n\n    Total head loss is a combination of major and minor losses.\n    This equation applies to both laminar and turbulent flows.\n    \"\"\"\n", "input": "", "output": "    #Inputs do not need to be checked here because they are checked by\n    #functions this function calls.\n    return (headloss_exp_general(Vel, KMinor).magnitude\n            + headloss_fric_general(Area, PerimWetted, Vel,\n                                     Length, Nu, PipeRough).magnitude)", "category": "Python"}, {"instruction": "def delete_from_all_link_group(self, group):\n        \"\"\"Delete a device to an All-Link Group.\"\"\"\n", "input": "", "output": "        msg = StandardSend(self._address,\n                           COMMAND_DELETE_FROM_ALL_LINK_GROUP_0X02_NONE,\n                           cmd2=group)\n        self._send_msg(msg)", "category": "Python"}, {"instruction": "def enhance(self, inverse=False, gamma=1.0, stretch=\"no\",\n                stretch_parameters=None, **kwargs):\n        \"\"\"Image enhancement function. It applies **in this order** inversion,\n        gamma correction, and stretching to the current image, with parameters\n        *inverse* (see :meth:`Image.invert`), *gamma* (see\n        :meth:`Image.gamma`), and *stretch* (see :meth:`Image.stretch`).\n        \"\"\"\n", "input": "", "output": "        self.invert(inverse)\n        if stretch_parameters is None:\n            stretch_parameters = {}\n\n        stretch_parameters.update(kwargs)\n        self.stretch(stretch, **stretch_parameters)\n        self.gamma(gamma)", "category": "Python"}, {"instruction": "def search(self, **kwargs):\n        \"\"\"\n        Method to search ipv6's based on extends search.\n\n        :param search: Dict containing QuerySets to find ipv6's.\n        :param include: Array containing fields to include on response.\n        :param exclude: Array containing fields to exclude on response.\n        :param fields:  Array containing fields to override default fields.\n        :param kind: Determine if result will be detailed ('detail') or basic ('basic').\n        :return: Dict containing ipv6's\n        \"\"\"\n", "input": "", "output": "\n        return super(ApiNetworkIPv6, self).get(self.prepare_url('api/v3/networkv6/',\n                                                                kwargs))", "category": "Python"}, {"instruction": "def socket(self, socket_type, identity=None, mechanism=None):\n        \"\"\"\n        Create and register a new socket.\n\n        :param socket_type: The type of the socket.\n        :param loop: An optional event loop to associate the socket with.\n\n        This is the preferred method to create new sockets.\n        \"\"\"\n", "input": "", "output": "        socket = Socket(\n            context=self,\n            socket_type=socket_type,\n            identity=identity,\n            mechanism=mechanism,\n            loop=self.loop,\n        )\n        self.register_child(socket)\n        return socket", "category": "Python"}, {"instruction": "def ReadApprovalRequest(self, requestor_username, approval_id):\n    \"\"\"Reads an approval request object with a given id.\"\"\"\n", "input": "", "output": "    try:\n      return self.approvals_by_username[requestor_username][approval_id]\n    except KeyError:\n      raise db.UnknownApprovalRequestError(\"Can't find approval with id: %s\" %\n                                           approval_id)", "category": "Python"}, {"instruction": "def n_day(date_string):\n        \"\"\"\n        date_string string in format \"(number|a) day(s) ago\"\n        \"\"\"\n", "input": "", "output": "        today = datetime.date.today()\n        match = re.match(r'(\\d{1,3}|a) days? ago', date_string)\n        groups = match.groups()\n        if groups:\n            decrement = groups[0]\n            if decrement == 'a':\n                decrement = 1\n            return today - datetime.timedelta(days=int(decrement))\n        return None", "category": "Python"}, {"instruction": "def get_envs_in_group(group_name, nova_creds):\n    \"\"\"\n    Takes a group_name and finds any environments that have a SUPERNOVA_GROUP\n    configuration line that matches the group_name.\n    \"\"\"\n", "input": "", "output": "    envs = []\n    for key, value in nova_creds.items():\n        supernova_groups = value.get('SUPERNOVA_GROUP', [])\n        if hasattr(supernova_groups, 'startswith'):\n            supernova_groups = [supernova_groups]\n        if group_name in supernova_groups:\n            envs.append(key)\n        elif group_name == 'all':\n            envs.append(key)\n    return envs", "category": "Python"}, {"instruction": "def cursor(self):\n        \"\"\"\n        Get a cursor for the current connection. For internal use only.\n        \"\"\"\n", "input": "", "output": "        cursor = self.mdr.cursor()\n        with self.transaction():\n            try:\n                yield cursor\n                if cursor.rowcount != -1:\n                    self.last_row_count = cursor.rowcount\n                self.last_row_id = getattr(cursor, 'lastrowid', None)\n            except:\n                self.last_row_count = None\n                self.last_row_id = None\n                _safe_close(cursor)\n                raise", "category": "Python"}, {"instruction": "def set(self, subscribed, ignored):\n        \"\"\"Set the user's subscription for this subscription\n\n        :param bool subscribed: (required), determines if notifications should\n            be received from this thread.\n        :param bool ignored: (required), determines if notifications should be\n            ignored from this thread.\n        \"\"\"\n", "input": "", "output": "        sub = {'subscribed': subscribed, 'ignored': ignored}\n        json = self._json(self._put(self._api, data=dumps(sub)), 200)\n        self.__init__(json, self._session)", "category": "Python"}, {"instruction": "def break_down_cookie(cookie):\n    \"\"\" Breaks down vSphere SOAP cookie\n    :param cookie: vSphere SOAP cookie\n    :type cookie: str\n    :return: Dictionary with cookie_name: cookie_value\n    \"\"\"\n", "input": "", "output": "    cookie_a = cookie.split(';')\n    cookie_name = cookie_a[0].split('=')[0]\n    cookie_text = ' {0}; ${1}'.format(cookie_a[0].split('=')[1],\n                                      cookie_a[1].lstrip())\n    return {cookie_name: cookie_text}", "category": "Python"}, {"instruction": "def writexml(self, writer, indent=\"\", addindent=\"\", newl=\"\"):\n        \"\"\"\n        Write the continuous  dataset using sonic visualiser xml conventions\n        \"\"\"\n", "input": "", "output": "        # dataset = self.data.appendChild(self.doc.createElement('dataset'))\n        # dataset.setAttribute('id', str(imodel))\n        # dataset.setAttribute('dimensions', '2')\n        writer.write('%s<dataset id=\"%s\" dimensions=\"%s\">%s' % (indent, self.datasetid, self.dimensions, newl))\n        indent2 = indent + addindent\n        for l, x, y in zip(self.labels, self.frames, self.values):\n            writer.write('%s<point label=\"%s\" frame=\"%d\" value=\"%f\"/>%s' % (indent2, self.int2label[l], x, y, newl))\n        writer.write('%s</dataset>%s' % (indent, newl))", "category": "Python"}, {"instruction": "def update_source_model(sm_node, fname):\n    \"\"\"\n    :param sm_node: a sourceModel Node object containing sourceGroups\n    \"\"\"\n", "input": "", "output": "    i = 0\n    for group in sm_node:\n        if 'srcs_weights' in group.attrib:\n            raise InvalidFile('srcs_weights must be removed in %s' % fname)\n        if not group.tag.endswith('sourceGroup'):\n            raise InvalidFile('wrong NRML, got %s instead of '\n                              'sourceGroup in %s' % (group.tag, fname))\n        psrcs = []\n        others = []\n        for src in group:\n            try:\n                del src.attrib['tectonicRegion']  # make the trt implicit\n            except KeyError:\n                pass  # already missing\n            if src.tag.endswith('pointSource'):\n                psrcs.append(src)\n            else:\n                others.append(src)\n        others.sort(key=lambda src: (src.tag, src['id']))\n        i, sources = _pointsources2multipoints(psrcs, i)\n        group.nodes = sources + others", "category": "Python"}, {"instruction": "def set_indent(indent=2, logger=\"TaskLogger\"):\n    \"\"\"Set the indent function\n\n    Convenience function to set the indent size\n\n    Parameters\n    ----------\n    indent : int, optional (default: 2)\n        number of spaces by which to indent based on the\n        number of tasks currently running`\n\n    Returns\n    -------\n    logger : TaskLogger\n    \"\"\"\n", "input": "", "output": "    tasklogger = get_tasklogger(logger)\n    tasklogger.set_indent(indent)\n    return tasklogger", "category": "Python"}, {"instruction": "def check(self, url_data):\n        \"\"\"Check content.\"\"\"\n", "input": "", "output": "        log.debug(LOG_PLUGIN, \"checking content for warning regex\")\n        content = url_data.get_content()\n        # add warnings for found matches, up to the maximum allowed number\n        match = self.warningregex.search(content)\n        if match:\n            # calculate line number for match\n            line = content.count('\\n', 0, match.start())\n            # add a warning message\n            msg = _(\"Found %(match)r at line %(line)d in link contents.\")\n            url_data.add_warning(msg % {\"match\": match.group(), \"line\": line})", "category": "Python"}, {"instruction": "def updateD_G(self, x):\n        \"\"\"\n        Compute Gradient for update of D\n\n        See [2] for derivation of Gradient\n        \"\"\"\n", "input": "", "output": "        self.precompute(x)\n        g = zeros(len(x))\n        Ai = zeros(self.A.shape[0])\n        for i in range(len(g)):\n            Ai = self.A[:, i]\n            g[i] = (self.E * (dot(self.AD, outer(self.R[:, i], Ai)) +\n                    dot(outer(Ai, self.R[i, :]), self.ADt))).sum()\n        return -2 * g", "category": "Python"}, {"instruction": "async def _client_run_async(self):\n        \"\"\"MessageSender Link is now open - perform message send\n        on all pending messages.\n        Will return True if operation successful and client can remain open for\n        further work.\n\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        # pylint: disable=protected-access\n        self._waiting_messages = 0\n        async with self._pending_messages_lock:\n            self._pending_messages = await self._filter_pending_async()\n        if self._backoff and not self._waiting_messages:\n            _logger.info(\"Client told to backoff - sleeping for %r seconds\", self._backoff)\n            await self._connection.sleep_async(self._backoff)\n            self._backoff = 0\n        await asyncio.shield(self._connection.work_async())\n        return True", "category": "Python"}, {"instruction": "async def sign_in(self, event=None):\n        \"\"\"\n        Note the `event` argument. This is required since this callback\n        may be called from a ``widget.bind`` (such as ``'<Return>'``),\n        which sends information about the event we don't care about.\n\n        This callback logs out if authorized, signs in if a code was\n        sent or a bot token is input, or sends the code otherwise.\n        \"\"\"\n", "input": "", "output": "        self.sign_in_label.configure(text='Working...')\n        self.sign_in_entry.configure(state=tkinter.DISABLED)\n        if await self.cl.is_user_authorized():\n            await self.cl.log_out()\n            self.destroy()\n            return\n\n        value = self.sign_in_entry.get().strip()\n        if self.code:\n            self.set_signed_in(await self.cl.sign_in(code=value))\n        elif ':' in value:\n            self.set_signed_in(await self.cl.sign_in(bot_token=value))\n        else:\n            self.code = await self.cl.send_code_request(value)\n            self.sign_in_label.configure(text='Code:')\n            self.sign_in_entry.configure(state=tkinter.NORMAL)\n            self.sign_in_entry.delete(0, tkinter.END)\n            self.sign_in_entry.focus()\n            return", "category": "Python"}, {"instruction": "def api_key_post(params, request_path, _async=False):\n    \"\"\"\n    from \u706b\u5e01demo, \u6784\u9020post\u8bf7\u6c42\u5e76\u8c03\u7528post\u65b9\u6cd5\n    :param params:\n    :param request_path:\n    :return:\n    \"\"\"\n", "input": "", "output": "    method = 'POST'\n    timestamp = datetime.datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%S')\n    params_to_sign = {\n        'AccessKeyId': ACCESS_KEY,\n        'SignatureMethod': 'HmacSHA256',\n        'SignatureVersion': '2',\n        'Timestamp': timestamp\n    }\n\n    host_url = TRADE_URL\n    host_name = urllib.parse.urlparse(host_url).hostname\n    host_name = host_name.lower()\n    secret_sign = createSign(params_to_sign, method, host_name,\n                                             request_path, SECRET_KEY)\n    params_to_sign['Signature'] = secret_sign\n    if PRIVATE_KEY:\n        params_to_sign['PrivateSignature'] = createPrivateSign(secret_sign, PRIVATE_KEY)\n    url = host_url + request_path + '?' + urllib.parse.urlencode(params_to_sign)\n    return http_post_request(url, params, _async=_async)", "category": "Python"}, {"instruction": "def set_param(self, name, value):\n        \"\"\"Set a GO-PCA Server parameter.\n\n        Parameters\n        ----------\n        name: str\n            The parameter name.\n        value: ?\n            The parameter value.\n        \"\"\"\n", "input": "", "output": "        if name not in self.param_names:\n            raise ValueError('No GO-PCA Server parameter named \"%s\"!' %(param))\n        self.__params[name] = value", "category": "Python"}, {"instruction": "def _exclusively_used(self, context, hosting_device, tenant_id):\n        \"\"\"Checks if only <tenant_id>'s resources use <hosting_device>.\"\"\"\n", "input": "", "output": "        return (context.session.query(hd_models.SlotAllocation).filter(\n            hd_models.SlotAllocation.hosting_device_id == hosting_device['id'],\n            hd_models.SlotAllocation.logical_resource_owner != tenant_id).\n            first() is None)", "category": "Python"}, {"instruction": "def launch_browser(self, soup=None):\n        \"\"\"Launch a browser to display a page, for debugging purposes.\n\n        :param: soup: Page contents to display, supplied as a bs4 soup object.\n            Defaults to the current page of the ``StatefulBrowser`` instance.\n        \"\"\"\n", "input": "", "output": "        if soup is None:\n            soup = self.get_current_page()\n        super(StatefulBrowser, self).launch_browser(soup)", "category": "Python"}, {"instruction": "def get_library_version(module_name):\n    \"\"\"\n    Get version number from ``module_name``'s ``__version__`` attribute.\n\n    .. versionadded:: 1.2.0\n\n    :param module_name: The module name, e.g. ``luma.oled``.\n    :type module_name: str\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    try:\n        module = importlib.import_module('luma.' + module_name)\n        if hasattr(module, '__version__'):\n            return module.__version__\n        else:\n            return None\n    except ImportError:\n        return None", "category": "Python"}, {"instruction": "def load(fin, dtype=np.float32, max_vocab=None):\n    \"\"\"\n    Refer to :func:`word_embedding_loader.loader.glove.load` for the API.\n    \"\"\"\n", "input": "", "output": "    vocab = {}\n    line = next(fin)\n    data = line.strip().split(b' ')\n    assert len(data) == 2\n    words = int(data[0])\n    if max_vocab is not None:\n        words = min(max_vocab, words)\n    size = int(data[1])\n    arr = np.empty((words, size), dtype=dtype)\n    i = 0\n    for n_line, line in enumerate(fin):\n        if i >= words:\n            break\n        token, v = _load_line(line, dtype, size)\n        if token in vocab:\n            parse_warn(b'Duplicated vocabulary ' + token)\n            continue\n        arr[i, :] = v\n        vocab[token] = i\n        i += 1\n    if i != words:\n        # Use + instead of formatting because python 3.4.* does not allow\n        # format with bytes\n        parse_warn(\n            b'EOF before the defined size (read ' + bytes(i) + b', expected '\n            + bytes(words) + b')'\n        )\n        arr = arr[:i, :]\n    return arr, vocab", "category": "Python"}, {"instruction": "def _simple_blockify(tuples, dtype):\n    \"\"\" return a single array of a block that has a single dtype; if dtype is\n    not None, coerce to this dtype\n    \"\"\"\n", "input": "", "output": "    values, placement = _stack_arrays(tuples, dtype)\n\n    # CHECK DTYPE?\n    if dtype is not None and values.dtype != dtype:  # pragma: no cover\n        values = values.astype(dtype)\n\n    block = make_block(values, placement=placement)\n    return [block]", "category": "Python"}, {"instruction": "def uninitialize_ui(self):\n        \"\"\"\n        Uninitializes the Component ui.\n\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        LOGGER.debug(\"> Uninitializing '{0}' Component ui.\".format(self.__class__.__name__))\n\n        self.__remove_actions()\n\n        # Signals / Slots.\n        self.__view.expanded.disconnect(self.__view__expanded)\n        self.__view.doubleClicked.disconnect(self.__view__doubleClicked)\n        self.__view.selectionModel().selectionChanged.disconnect(self.__view_selectionModel__selectionChanged)\n        self.__script_editor.Script_Editor_tabWidget.currentChanged.disconnect(\n            self.__script_editor_Script_Editor_tabWidget__currentChanged)\n        self.__script_editor.model.project_registered.disconnect(self.__script_editor_model__project_registered)\n\n        self.__view_remove_actions()\n\n        self.__model = None\n        self.__delegate = None\n        self.__view = None\n\n        self.initialized_ui = False\n        return True", "category": "Python"}, {"instruction": "def render_block_to_string(template_name, block_name, context=None):\n    \"\"\"\n    Loads the given template_name and renders the given block with the given\n    dictionary as context. Returns a string.\n\n        template_name\n            The name of the template to load and render. If it's a list of\n            template names, Django uses select_template() instead of\n            get_template() to find the template.\n    \"\"\"\n", "input": "", "output": "\n    # Like render_to_string, template_name can be a string or a list/tuple.\n    if isinstance(template_name, (tuple, list)):\n        t = loader.select_template(template_name)\n    else:\n        t = loader.get_template(template_name)\n\n    # Create the context instance.\n    context = context or {}\n\n    # The Django backend.\n    if isinstance(t, DjangoTemplate):\n        return django_render_block(t, block_name, context)\n\n    elif isinstance(t, Jinja2Template):\n        from render_block.jinja2 import jinja2_render_block\n        return jinja2_render_block(t, block_name, context)\n\n    else:\n        raise UnsupportedEngine(\n            'Can only render blocks from the Django template backend.')", "category": "Python"}, {"instruction": "def make_list(item_or_items):\n    \"\"\"\n    Makes a list out of the given items.\n    Examples:\n        >>> make_list(1)\n        [1]\n        >>> make_list('str')\n        ['str']\n        >>> make_list(('i', 'am', 'a', 'tuple'))\n        ['i', 'am', 'a', 'tuple']\n        >>> print(make_list(None))\n        None\n        >>> # An instance of lists is unchanged\n        >>> l = ['i', 'am', 'a', 'list']\n        >>> l_res = make_list(l)\n        >>> l_res\n        ['i', 'am', 'a', 'list']\n        >>> l_res is l\n        True\n\n    Args:\n        item_or_items: A single value or an iterable.\n    Returns:\n        Returns the given argument as an list.\n    \"\"\"\n", "input": "", "output": "    if item_or_items is None:\n        return None\n    if isinstance(item_or_items, list):\n        return item_or_items\n    if hasattr(item_or_items, '__iter__') and not isinstance(item_or_items, str):\n        return list(item_or_items)\n    return [item_or_items]", "category": "Python"}, {"instruction": "def login(self, url, use_token=True, **kwargs):\n        \"\"\"Perform a login request.\n\n        Associated request is typically to a path prefixed with \"/v1/auth\") and optionally stores the client token sent\n            in the resulting Vault response for use by the :py:meth:`hvac.adapters.Adapter` instance under the _adapater\n            Client attribute.\n\n        :param url: Path to send the authentication request to.\n        :type url: str | unicode\n        :param use_token: if True, uses the token in the response received from the auth request to set the \"token\"\n            attribute on the the :py:meth:`hvac.adapters.Adapter` instance under the _adapater Client attribute.\n        :type use_token: bool\n        :param kwargs: Additional keyword arguments to include in the params sent with the request.\n        :type kwargs: dict\n        :return: The response of the auth request.\n        :rtype: requests.Response\n        \"\"\"\n", "input": "", "output": "        response = self.post(url, **kwargs).json()\n\n        if use_token:\n            self.token = response['auth']['client_token']\n\n        return response", "category": "Python"}, {"instruction": "def instruction_MUL(self, opcode):\n        \"\"\"\n        Multiply the unsigned binary numbers in the accumulators and place the\n        result in both accumulators (ACCA contains the most-significant byte of\n        the result). Unsigned multiply allows multiple-precision operations.\n\n        The C (carry) bit allows rounding the most-significant byte through the\n        sequence: MUL, ADCA #0.\n\n        source code forms: MUL\n\n        CC bits \"HNZVC\": --a-a\n        \"\"\"\n", "input": "", "output": "        r = self.accu_a.value * self.accu_b.value\n        self.accu_d.set(r)\n        self.Z = 1 if r == 0 else 0\n        self.C = 1 if r & 0x80 else 0", "category": "Python"}, {"instruction": "def p_namespace_name(p):\n    '''namespace_name : namespace_name NS_SEPARATOR STRING\n                      | STRING'''\n", "input": "", "output": "    if len(p) == 4:\n        p[0] = p[1] + p[2] + p[3]\n    else:\n        p[0] = p[1]", "category": "Python"}, {"instruction": "def getcomponentsdetails(self, product, force_refresh=False):\n        \"\"\"\n        Wrapper around Product.get(include_fields=[\"components\"]),\n        returning only the \"components\" data for the requested product,\n        slightly reworked to a dict mapping of components.name: components,\n        for historical reasons.\n\n        This uses the product cache, but will update it if the product\n        isn't found or \"components\" isn't cached for the product.\n\n        In cases like bugzilla.redhat.com where there are tons of\n        components for some products, this API will time out. You\n        should use product_get instead.\n        \"\"\"\n", "input": "", "output": "        proddict = self._lookup_product_in_cache(product)\n\n        if (force_refresh or not proddict or \"components\" not in proddict):\n            self.refresh_products(names=[product],\n                                  include_fields=[\"name\", \"id\", \"components\"])\n            proddict = self._lookup_product_in_cache(product)\n\n        ret = {}\n        for compdict in proddict[\"components\"]:\n            ret[compdict[\"name\"]] = compdict\n        return ret", "category": "Python"}, {"instruction": "def quoted_or_list(items: List[str]) -> Optional[str]:\n    \"\"\"Given [A, B, C] return \"'A', 'B', or 'C'\".\n\n    Note: We use single quotes here, since these are also used by repr().\n    \"\"\"\n", "input": "", "output": "    return or_list([f\"'{item}'\" for item in items])", "category": "Python"}, {"instruction": "def convert_to_int(value, from_base):\n        \"\"\"\n        Convert value to an int.\n\n        :param value: the value to convert\n        :type value: sequence of int\n        :param int from_base: base of value\n        :returns: the conversion result\n        :rtype: int\n        :raises ConvertError: if from_base is less than 2\n        :raises ConvertError: if elements in value outside bounds\n\n        Preconditions:\n          * all integers in value must be at least 0\n          * all integers in value must be less than from_base\n          * from_base must be at least 2\n\n        Complexity: O(len(value))\n        \"\"\"\n", "input": "", "output": "        if from_base < 2:\n            raise BasesValueError(\n               from_base,\n               \"from_base\",\n               \"must be greater than 2\"\n            )\n\n        if any(x < 0 or x >= from_base for x in value):\n            raise BasesValueError(\n               value,\n               \"value\",\n               \"elements must be at least 0 and less than %s\" % from_base\n            )\n        return reduce(lambda x, y: x * from_base + y, value, 0)", "category": "Python"}, {"instruction": "def add_play(self, choice, count=1):\n        \"\"\"Increments the play count for a given experiment choice\"\"\"\n", "input": "", "output": "        self.redis.hincrby(EXPERIMENT_REDIS_KEY_TEMPLATE % self.name, \"%s:plays\" % choice, count)\n        self._choices = None", "category": "Python"}, {"instruction": "def _add_epsilon_states(self, stateset, gathered_epsilons):\n    '''\n    stateset is the list of initial states\n    gathered_epsilons is a dictionary of (dst: src) epsilon dictionaries\n    '''\n", "input": "", "output": "    for i in list(stateset):\n      if i not in gathered_epsilons:\n        gathered_epsilons[i] = {}\n        q = _otq()\n        q.append(i)\n        while q:\n          s = q.popleft()\n          for j in self._transitions.setdefault(s, {}).setdefault(NFA.EPSILON, set()):\n            gathered_epsilons[i][j] = s if j not in gathered_epsilons[i] else self.choose(s, j)\n            q.append(j)\n      stateset.update(gathered_epsilons[i].keys())", "category": "Python"}, {"instruction": "def create_package(package_format, owner, repo, **kwargs):\n    \"\"\"Create a new package in a repository.\"\"\"\n", "input": "", "output": "    client = get_packages_api()\n\n    with catch_raise_api_exception():\n        upload = getattr(client, \"packages_upload_%s_with_http_info\" % package_format)\n\n        data, _, headers = upload(\n            owner=owner, repo=repo, data=make_create_payload(**kwargs)\n        )\n\n    ratelimits.maybe_rate_limit(client, headers)\n    return data.slug_perm, data.slug", "category": "Python"}, {"instruction": "def compute_u_val_for_sky_loc_stat_no_phase(hplus, hcross, hphccorr,\n                                 hpnorm=None , hcnorm=None, indices=None):\n    \"\"\"The max-over-sky location (no phase) detection statistic maximizes over\n    an amplitude and the ratio of F+ and Fx, encoded in a variable called u.\n    Here we return the value of u for the given indices.\n\n\n    \"\"\"\n", "input": "", "output": "    if indices is not None:\n        hplus = hplus[indices]\n        hcross = hcross[indices]\n\n    if hpnorm is not None:\n        hplus = hplus * hpnorm\n    if hcnorm is not None:\n        hcross = hcross * hcnorm\n\n    rhoplusre=numpy.real(hplus)\n    rhocrossre=numpy.real(hcross)\n    overlap=numpy.real(hphccorr)\n\n    denom = (-rhocrossre+overlap*rhoplusre)\n    # Initialize tan_kappa array\n    u_val = denom * 0.\n    # Catch the denominator -> 0 case\n    bad_lgc = (denom == 0)\n    u_val[bad_lgc] = 1E17\n    # Otherwise do normal statistic\n    u_val[~bad_lgc] = (-rhoplusre+overlap*rhocrossre) / \\\n        (-rhocrossre+overlap*rhoplusre)\n    coa_phase = numpy.zeros(len(indices), dtype=numpy.float32)\n\n    return u_val, coa_phase", "category": "Python"}, {"instruction": "def print_smart_tasks():\n    \"\"\"Print smart tasks as JSON\"\"\"\n", "input": "", "output": "    print(\"Printing information about smart tasks\")\n    tasks = api(gateway.get_smart_tasks())\n    if len(tasks) == 0:\n        exit(bold(\"No smart tasks defined\"))\n\n    container = []\n    for task in tasks:\n        container.append(api(task).task_control.raw)\n    print(jsonify(container))", "category": "Python"}, {"instruction": "def validate_username(username):\n    \"\"\" Validate the new username. If the username is invalid, raises\n    :py:exc:`UsernameInvalid`.\n\n    :param username: Username to validate.\n    \"\"\"\n", "input": "", "output": "\n    # Check username looks ok\n\n    if not username.islower():\n        raise UsernameInvalid(six.u('Username must be all lowercase'))\n    if len(username) < 2:\n        raise UsernameInvalid(six.u('Username must be at least 2 characters'))\n    if not username_re.search(username):\n        raise UsernameInvalid(settings.USERNAME_VALIDATION_ERROR_MSG)\n\n    return username", "category": "Python"}, {"instruction": "def _platform(self) -> Optional[str]:\n        \"\"\"Extract platform.\"\"\"\n", "input": "", "output": "        try:\n            return str(self.journey.MainStop.BasicStop.Dep.Platform.text)\n        except AttributeError:\n            return None", "category": "Python"}, {"instruction": "def set_primary_contact(self, email):\n        \"\"\"assigns the primary contact for this client\"\"\"\n", "input": "", "output": "        params = {\"email\": email}\n        response = self._put(self.uri_for('primarycontact'), params=params)\n        return json_to_py(response)", "category": "Python"}, {"instruction": "def port_number_range(prange):\n    \"\"\" Port number range validation and expansion. \"\"\"\n", "input": "", "output": "    # first, try it as a normal port number\n    try:\n        return port_number(prange)\n    except ValueError:\n        pass\n    # then, consider it as a range with the format \"x-y\" and expand it\n    try:\n        bounds = list(map(int, re.match(r'^(\\d+)\\-(\\d+)$', prange).groups()))\n        if bounds[0] > bounds[1]:\n            raise AttributeError()\n    except (AttributeError, TypeError):\n        raise ValueError(\"Bad port number range\")\n    return list(range(bounds[0], bounds[1] + 1))", "category": "Python"}, {"instruction": "def get_settings(self):\n        \"\"\"GetSettings.\n        [Preview API]\n        :rtype: :class:`<NotificationAdminSettings> <azure.devops.v5_0.notification.models.NotificationAdminSettings>`\n        \"\"\"\n", "input": "", "output": "        response = self._send(http_method='GET',\n                              location_id='cbe076d8-2803-45ff-8d8d-44653686ea2a',\n                              version='5.0-preview.1')\n        return self._deserialize('NotificationAdminSettings', response)", "category": "Python"}, {"instruction": "def create_mssql_pymssql(username, password, host, port, database, **kwargs):  # pragma: no cover\n    \"\"\"\n    create an engine connected to a mssql database using pymssql.\n    \"\"\"\n", "input": "", "output": "    return create_engine(\n        _create_mssql_pymssql(username, password, host, port, database),\n        **kwargs\n    )", "category": "Python"}, {"instruction": "def compute_dominance_frontier(graph, domtree):\n    \"\"\"\n    Compute a dominance frontier based on the given post-dominator tree.\n\n    This implementation is based on figure 2 of paper An Efficient Method of Computing Static Single Assignment\n    Form by Ron Cytron, etc.\n\n    :param graph:   The graph where we want to compute the dominance frontier.\n    :param domtree: The dominator tree\n    :returns:       A dict of dominance frontier\n    \"\"\"\n", "input": "", "output": "\n    df = {}\n\n    # Perform a post-order search on the dominator tree\n    for x in networkx.dfs_postorder_nodes(domtree):\n\n        if x not in graph:\n            # Skip nodes that are not in the graph\n            continue\n\n        df[x] = set()\n\n        # local set\n        for y in graph.successors(x):\n            if x not in domtree.predecessors(y):\n                df[x].add(y)\n\n        # up set\n        if x is None:\n            continue\n\n        for z in domtree.successors(x):\n            if z is x:\n                continue\n            if z not in df:\n                continue\n            for y in df[z]:\n                if x not in list(domtree.predecessors(y)):\n                    df[x].add(y)\n\n    return df", "category": "Python"}, {"instruction": "def commodity_channel_index(close_data, high_data, low_data, period):\n    \"\"\"\n    Commodity Channel Index.\n\n    Formula:\n    CCI = (TP - SMA(TP)) / (0.015 * Mean Deviation)\n    \"\"\"\n", "input": "", "output": "    catch_errors.check_for_input_len_diff(close_data, high_data, low_data)\n    catch_errors.check_for_period_error(close_data, period)\n    tp = typical_price(close_data, high_data, low_data)\n    cci = ((tp - sma(tp, period)) /\n           (0.015 * np.mean(np.absolute(tp - np.mean(tp)))))\n    return cci", "category": "Python"}, {"instruction": "def try_pop(d, key, default):\n    \"\"\"\n    >>> d = {\"a\": \"b\", \"c\": \"d\", \"e\": \"f\"}\n    >>> try_pop(d, \"g\", \"default\")\n    'default'\n    >>> d\n    {'a': 'b', 'c': 'd', 'e': 'f'}\n    >>> try_pop(d, \"c\", \"default\")\n    'd'\n    >>> d\n    {'a': 'b', 'e': 'f'}\n    \"\"\"\n", "input": "", "output": "    value = d.get(key, default)\n    if key in d:\n        d.pop(key)\n    return value", "category": "Python"}, {"instruction": "def detect(self):\n        \"\"\"\n        Resolve the hostname to an IP address through the operating system.\n\n        Depending on the 'family' option, either ipv4 or ipv6 resolution is\n        carried out.\n\n        If multiple IP addresses are found, the first one is returned.\n\n        :return: ip address\n        \"\"\"\n", "input": "", "output": "        theip = next(iter(resolve(self.opts_hostname, self.opts_family)), None)\n        self.set_current_value(theip)\n        return theip", "category": "Python"}, {"instruction": "def getFormattedHTML(self, indent='  '):\n        '''\n            getFormattedHTML - Get formatted and xhtml of this document, replacing the original whitespace\n                with a pretty-printed version\n\n            @param indent - space/tab/newline of each level of indent, or integer for how many spaces per level\n        \n            @return - <str> Formatted html\n\n            @see getHTML - Get HTML with original whitespace\n\n            @see getMiniHTML - Get HTML with only functional whitespace remaining\n        '''\n", "input": "", "output": "        from .Formatter import AdvancedHTMLFormatter\n        html = self.getHTML()\n        formatter = AdvancedHTMLFormatter(indent, None) # Do not double-encode\n        formatter.feed(html)\n        return formatter.getHTML()", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"Run the hight quarter, lunch the turrets and wait for results\n        \"\"\"\n", "input": "", "output": "        elapsed = 0\n        run_time = self.config['run_time']\n        start_time = time.time()\n        t = time.time\n        self.turrets_manager.start(self.transaction_context)\n        self.started = True\n\n        while elapsed <= run_time:\n            try:\n                self._run_loop_action()\n                self._print_status(elapsed)\n                elapsed = t() - start_time\n            except (Exception, KeyboardInterrupt):\n                print(\"\\nStopping test, sending stop command to turrets\")\n                self.turrets_manager.stop()\n                self.stats_handler.write_remaining()\n                traceback.print_exc()\n                break\n\n        self.turrets_manager.stop()\n        print(\"\\n\\nProcessing all remaining messages... This could take time depending on message volume\")\n        t = time.time()\n        self.result_collector.unbind(self.result_collector.LAST_ENDPOINT)\n        self._clean_queue()\n        print(\"took %s\" % (time.time() - t))", "category": "Python"}, {"instruction": "def _create_field(self, field_id):\n        \"\"\"\n        Creates the field with the specified parameters.\n\n        :param field_id: identifier for the field\n        :return: the basic rule for the field\n        \"\"\"\n", "input": "", "output": "        # Field configuration info\n        config = self._field_configs[field_id]\n\n        adapter = self._adapters[config['type']]\n\n        if 'name' in config:\n            name = config['name']\n        else:\n            name = None\n\n        if 'size' in config:\n            columns = config['size']\n        else:\n            columns = None\n\n        if 'values' in config:\n            values = config['values']\n        else:\n            values = None\n\n        field = adapter.get_field(name, columns, values)\n\n        if 'results_name' in config:\n            field = field.setResultsName(config['results_name'])\n        else:\n            field = field.setResultsName(field_id)\n\n        return field", "category": "Python"}, {"instruction": "def is_modified(self):\n        \"\"\"\n        Returns whether model is modified or not\n        \"\"\"\n", "input": "", "output": "        if len(self.__modified_data__) or len(self.__deleted_fields__):\n            return True\n\n        for value in self.__original_data__.values():\n            try:\n                if value.is_modified():\n                    return True\n            except AttributeError:\n                pass\n\n        return False", "category": "Python"}, {"instruction": "def decode_iter(data, codec_options=DEFAULT_CODEC_OPTIONS):\n    \"\"\"Decode BSON data to multiple documents as a generator.\n\n    Works similarly to the decode_all function, but yields one document at a\n    time.\n\n    `data` must be a string of concatenated, valid, BSON-encoded\n    documents.\n\n    :Parameters:\n      - `data`: BSON data\n      - `codec_options` (optional): An instance of\n        :class:`~bson.codec_options.CodecOptions`.\n\n    .. versionchanged:: 3.0\n       Replaced `as_class`, `tz_aware`, and `uuid_subtype` options with\n       `codec_options`.\n\n    .. versionadded:: 2.8\n    \"\"\"\n", "input": "", "output": "    if not isinstance(codec_options, CodecOptions):\n        raise _CODEC_OPTIONS_TYPE_ERROR\n\n    position = 0\n    end = len(data) - 1\n    while position < end:\n        obj_size = _UNPACK_INT(data[position:position + 4])[0]\n        elements = data[position:position + obj_size]\n        position += obj_size\n\n        yield _bson_to_dict(elements, codec_options)", "category": "Python"}, {"instruction": "def grains_dict(self):\n        \"\"\"Allowing to lookup grain by either label or duration\n\n        For backward compatibility\"\"\"\n", "input": "", "output": "        d = {grain.duration: grain for grain in self.grains()}\n        d.update({grain.label: grain for grain in self.grains()})\n        return d", "category": "Python"}, {"instruction": "def WriteXml(self, w, option, elementName=None):\n\t\t\"\"\"\tMethod writes the xml representation of the generic managed object.\t\"\"\"\n", "input": "", "output": "\t\tif elementName == None:\n\t\t\tx = w.createElement(self.classId)\n\t\telse:\n\t\t\tx = w.createElement(elementName)\n\n\t\tfor prop in self.__dict__['properties']:\n\t\t\tx.setAttribute(UcsUtils.WordL(prop), self.__dict__['properties'][prop])\n\t\tx_child = self.childWriteXml(w, option)\n\t\tfor xc in x_child:\n\t\t\tif (xc != None):\n\t\t\t\tx.appendChild(xc)\n\t\treturn x", "category": "Python"}, {"instruction": "def this_week():\n        \"\"\" Return start and end date of the current week. \"\"\"\n", "input": "", "output": "        since = TODAY + delta(weekday=MONDAY(-1))\n        until = since + delta(weeks=1)\n        return Date(since), Date(until)", "category": "Python"}, {"instruction": "def calc_device_links(self):\n        \"\"\"\n        Calculate a router or VirtualBox link\n        \"\"\"\n", "input": "", "output": "        for connection in self.interfaces:\n            int_type = connection['from'][0]\n            int_name = connection['from'].replace(int_type,\n                                                  PORT_TYPES[int_type.upper()])\n            # Get the source port id\n            src_port = None\n            for port in self.node['ports']:\n                if int_name == port['name']:\n                    src_port = port['id']\n                    break\n            dest_temp = connection['to'].split(' ')\n\n            if len(dest_temp) == 2:\n                conn_to = {'device': dest_temp[0],\n                           'port': dest_temp[1]}\n            else:\n                conn_to = {'device': 'NIO',\n                           'port': dest_temp[0]}\n\n            self.calc_link(self.node['id'], src_port, int_name, conn_to)", "category": "Python"}, {"instruction": "def do_handshake(self):\n        \"\"\"Start the SSL handshake.\n\n        This method only needs to be called if this transport was created with\n        *do_handshake_on_connect* set to False (the default is True).\n\n        The handshake needs to be synchronized between the both endpoints, so\n        that SSL record level data is not incidentially interpreted as\n        plaintext. Usually this is done by starting the handshake directly\n        after a connection is established, but you can also use an application\n        level protocol.\n        \"\"\"\n", "input": "", "output": "        if self._error:\n            raise compat.saved_exc(self._error)\n        elif self._closing or self._handle.closed:\n            raise TransportError('SSL transport is closing/closed')\n        self._write_backlog.append([b'', True])\n        self._process_write_backlog()", "category": "Python"}, {"instruction": "def create_order(dk_api, kitchen, recipe_name, variation_name, node_name=None):\n        \"\"\"\n        returns a string.\n        :param dk_api: -- api object\n        :param kitchen: string\n        :param recipe_name: string  -- kitchen name, string\n        :param variation_name: string -- name of the recipe variation_name to be run\n        :param node_name: string -- name of the single node to run\n        :rtype: DKReturnCode\n        \"\"\"\n", "input": "", "output": "        rc = dk_api.create_order(kitchen, recipe_name, variation_name, node_name)\n        if rc.ok():\n            s = 'Order ID is: %s' % rc.get_payload()\n        else:\n            m = rc.get_message().replace('\\\\n','\\n')\n            e = m.split('the logfile errors are:')\n            if len(e) > 1:\n                e2 = DKCloudCommandRunner._decompress(e[-1])\n                errors = e2.split('|')\n                re = e[0] + \" \" + 'the logfile errors are: '\n                for e in errors:\n                    re += '\\n%s' % e\n            else:\n                re = m\n            s = 'DKCloudCommand.create_order failed\\nmessage: %s\\n' % re\n        rc.set_message(s)\n        return rc", "category": "Python"}, {"instruction": "def _RemoveAuthorizedKeys(self, user):\n    \"\"\"Remove a Linux user account's authorized keys file to prevent login.\n\n    Args:\n      user: string, the Linux user account to remove access.\n    \"\"\"\n", "input": "", "output": "    pw_entry = self._GetUser(user)\n    if not pw_entry:\n      return\n\n    home_dir = pw_entry.pw_dir\n    authorized_keys_file = os.path.join(home_dir, '.ssh', 'authorized_keys')\n    if os.path.exists(authorized_keys_file):\n      try:\n        os.remove(authorized_keys_file)\n      except OSError as e:\n        message = 'Could not remove authorized keys for user %s. %s.'\n        self.logger.warning(message, user, str(e))", "category": "Python"}, {"instruction": "def editissue(self, project_id, issue_id, **kwargs):\n        \"\"\"\n        Edit an existing issue data\n\n        :param project_id: project id\n        :param issue_id: issue id\n        :return: true if success\n        \"\"\"\n", "input": "", "output": "        data = {'id': project_id, 'issue_id': issue_id}\n        if kwargs:\n            data.update(kwargs)\n        request = requests.put(\n            '{0}/{1}/issues/{2}'.format(self.projects_url, project_id, issue_id),\n            headers=self.headers, data=data, verify=self.verify_ssl, auth=self.auth, timeout=self.timeout)\n\n        if request.status_code == 200:\n            return request.json()\n        else:\n            return False", "category": "Python"}, {"instruction": "def get_stats_value(self, item, value):\n        \"\"\"Return the stats object for a specific item=value in JSON format.\n\n        Stats should be a list of dict (processlist, network...)\n        \"\"\"\n", "input": "", "output": "        if not isinstance(self.stats, list):\n            return None\n        else:\n            if value.isdigit():\n                value = int(value)\n            try:\n                return self._json_dumps({value: [i for i in self.stats if i[item] == value]})\n            except (KeyError, ValueError) as e:\n                logger.error(\n                    \"Cannot get item({})=value({}) ({})\".format(item, value, e))\n                return None", "category": "Python"}, {"instruction": "def make_structure_from_geos(geos):\n    '''Creates a structure out of a list of geometry objects.'''\n", "input": "", "output": "    model_structure=initialize_res(geos[0])\n    for i in range(1,len(geos)):\n        model_structure=add_residue(model_structure, geos[i])\n\n    return model_structure", "category": "Python"}, {"instruction": "def subscribe(self, subscription_channel, **observer_params):\n        \"\"\"Subscribe to a channel\n\n        This adds a channel to the router, configures it, starts it, and returns its observer\n        \"\"\"\n", "input": "", "output": "        return self.get_channel(subscription_channel, **observer_params).ensure_started().observer", "category": "Python"}, {"instruction": "def load_extensions_from_config(**config):\n    \"\"\"\n    Loads extensions\n    \"\"\"\n", "input": "", "output": "    extensions = []\n    if 'EXTENSIONS' in config:\n        for ext in config['EXTENSIONS']:\n            try:\n                extensions.append(locate(ext))\n            except Exception as e:\n                print(e)\n    return extensions", "category": "Python"}, {"instruction": "def get_date(self):\n        \"\"\" Collects sensing date of the product.\n\n        :return: Sensing date\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        if self.safe_type == EsaSafeType.OLD_TYPE:\n            name = self.product_id.split('_')[-2]\n            date = [name[1:5], name[5:7], name[7:9]]\n        else:\n            name = self.product_id.split('_')[2]\n            date = [name[:4], name[4:6], name[6:8]]\n        return '-'.join(date_part.lstrip('0') for date_part in date)", "category": "Python"}, {"instruction": "def kernel(self, kernel):\n        \"\"\"\n        Sets the kernel.\n\n        :param kernel: the kernel to set\n        :type kernel: Kernel\n        \"\"\"\n", "input": "", "output": "        result = javabridge.static_call(\n            \"weka/classifiers/KernelHelper\", \"setKernel\",\n            \"(Ljava/lang/Object;Lweka/classifiers/functions/supportVector/Kernel;)Z\",\n            self.jobject, kernel.jobject)\n        if not result:\n            raise Exception(\"Failed to set kernel!\")", "category": "Python"}, {"instruction": "def is_private(self, key, sources):\n        \"\"\"Check if attribute is private.\"\"\"\n", "input": "", "output": "        # aliases are always public.\n        if key == ENTRY.ALIAS:\n            return False\n        return all([\n            SOURCE.PRIVATE in self.get_source_by_alias(x)\n            for x in sources.split(',')\n        ])", "category": "Python"}, {"instruction": "def read_parameters(self):\n        \"\"\"\n        Read a parameters.out file\n\n        Returns\n        -------\n        dict\n            A dictionary containing all the configuration used by MAGICC\n        \"\"\"\n", "input": "", "output": "        param_fname = join(self.out_dir, \"PARAMETERS.OUT\")\n\n        if not exists(param_fname):\n            raise FileNotFoundError(\"No PARAMETERS.OUT found\")\n\n        with open(param_fname) as nml_file:\n            parameters = dict(f90nml.read(nml_file))\n            for group in [\"nml_years\", \"nml_allcfgs\", \"nml_outputcfgs\"]:\n                parameters[group] = dict(parameters[group])\n                for k, v in parameters[group].items():\n                    parameters[group][k] = _clean_value(v)\n                parameters[group.replace(\"nml_\", \"\")] = parameters.pop(group)\n            self.config = parameters\n        return parameters", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self, 'word') and self.word is not None:\n            _dict['word'] = self.word\n        if hasattr(self, 'sounds_like') and self.sounds_like is not None:\n            _dict['sounds_like'] = self.sounds_like\n        if hasattr(self, 'display_as') and self.display_as is not None:\n            _dict['display_as'] = self.display_as\n        return _dict", "category": "Python"}, {"instruction": "def set_mode_px4(self, mode, custom_mode, custom_sub_mode):\n        '''enter arbitrary mode'''\n", "input": "", "output": "        if isinstance(mode, str):\n            mode_map = self.mode_mapping()\n            if mode_map is None or mode not in mode_map:\n                print(\"Unknown mode '%s'\" % mode)\n                return\n            # PX4 uses two fields to define modes\n            mode, custom_mode, custom_sub_mode = px4_map[mode]\n        self.mav.command_long_send(self.target_system, self.target_component,\n                                   mavlink.MAV_CMD_DO_SET_MODE, 0, mode, custom_mode, custom_sub_mode, 0, 0, 0, 0)", "category": "Python"}, {"instruction": "def _first_not_none(seq, supplier_func):\n        \"\"\"Applies supplier_func to each element in seq, returns 1st not None\n\n        :param seq: Sequence of object\n        :type seq: iterable\n        :param supplier_func: Function that extracts the desired value from\n            elements in seq\n        :type supplier_func: function\n        \"\"\"\n", "input": "", "output": "        for i in seq:\n            obj = supplier_func(i)\n            if obj is not None:\n                return obj\n\n        return None", "category": "Python"}, {"instruction": "def invalid_code(self, code, card_id=None):\n        \"\"\"\n        \u8bbe\u7f6e\u5361\u5238\u5931\u6548\n        \"\"\"\n", "input": "", "output": "        card_data = {\n            'code': code\n        }\n        if card_id:\n            card_data['card_id'] = card_id\n        return self._post(\n            'card/code/unavailable',\n            data=card_data\n        )", "category": "Python"}, {"instruction": "def remove_checksum(path):\n    \"\"\"\n    Remove the checksum of an image from cache if exists\n    \"\"\"\n", "input": "", "output": "\n    path = '{}.md5sum'.format(path)\n    if os.path.exists(path):\n        os.remove(path)", "category": "Python"}, {"instruction": "def _get_scalexy(self, ims_width, ims_height):\n        \"\"\"Returns scale_x, scale_y for bitmap display\"\"\"\n", "input": "", "output": "\n        # Get cell attributes\n        cell_attributes = self.code_array.cell_attributes[self.key]\n        angle = cell_attributes[\"angle\"]\n\n        if abs(angle) == 90:\n            scale_x = self.rect[3] / float(ims_width)\n            scale_y = self.rect[2] / float(ims_height)\n\n        else:\n            # Normal case\n            scale_x = self.rect[2] / float(ims_width)\n            scale_y = self.rect[3] / float(ims_height)\n\n        return scale_x, scale_y", "category": "Python"}, {"instruction": "def _check_toplevel_misplaced(item):\n    \"\"\"Check for algorithm keys accidentally placed at the top level.\n    \"\"\"\n", "input": "", "output": "    problem_keys = [k for k in item.keys() if k in ALGORITHM_KEYS]\n    if len(problem_keys) > 0:\n        raise ValueError(\"Unexpected configuration keywords found in top level of %s: %s\\n\"\n                         \"This should be placed in the 'algorithm' section.\"\n                         % (item[\"description\"], problem_keys))\n    problem_keys = [k for k in item.keys() if k not in TOPLEVEL_KEYS]\n    if len(problem_keys) > 0:\n        raise ValueError(\"Unexpected configuration keywords found in top level of %s: %s\\n\"\n                         % (item[\"description\"], problem_keys))", "category": "Python"}, {"instruction": "def recursively_update(d, d2):\n  \"\"\"dict.update but which merges child dicts (dict2 takes precedence where there's conflict).\"\"\"\n", "input": "", "output": "  for k, v in d2.items():\n    if k in d:\n      if isinstance(v, dict):\n        recursively_update(d[k], v)\n        continue\n    d[k] = v", "category": "Python"}, {"instruction": "async def insert_offer(self, **params):\n\t\t\"\"\"Inserts new offer to database (related to buyers account)\n\t\tAccepts:\n\t\t\t- cid\n\t\t\t- buyer address\n\t\t\t- price\n\t\t\t- access type\n\t\t\t- transaction id\n\t\t\t- owner public key\n\t\t\t- owner address\n\t\t\t- coin ID\n\t\t\"\"\"\n", "input": "", "output": "\t\tif params.get(\"message\"):\n\t\t\tparams = json.loads(params.get(\"message\", \"{}\"))\n\t\t\n\t\tif not params:\n\t\t\treturn {\"error\":400, \"reason\":\"Missed required fields\"}\n\n\t\t# Check if required fields exists\n\t\tcid = int(params.get(\"cid\", 0))\n\t\ttxid = params.get(\"txid\")\n\t\tcoinid = params.get(\"coinid\")\n\t\tpublic_key = params.get(\"public_key\")\n\n\t\tdatabase = client[coinid]\n\t\toffer_collection = database[settings.OFFER]\n\t\tawait offer_collection.insert_one({\"cid\":cid, \"txid\":txid, \n\t\t\t\t\t\t\t\t\t\t\t\"confirmed\":None, \"coinid\":coinid, \n\t\t\t\t\t\t\t\t\t\t\t\"public_key\":public_key})\n\n\t\treturn {\"result\":\"ok\"}", "category": "Python"}, {"instruction": "def is_username_valid(username):\n    \"\"\"\n    Check if a valid username.\n    valid:\n        oracle\n        bill-gates\n        steve.jobs\n        micro_soft\n    not valid\n        Bill Gates - no space allowed\n        me@yo.com - @ is not a valid character\n    :param username: string\n    :return:\n    \"\"\"\n", "input": "", "output": "    pattern = re.compile(r\"^[a-zA-Z0-9_.-]+$\")\n    return bool(pattern.match(username))", "category": "Python"}, {"instruction": "def initWithEventUriList(uriList):\n        \"\"\"\n        Set a custom list of event uris. The results will be then computed on this list - no query will be done (all conditions will be ignored).\n        \"\"\"\n", "input": "", "output": "        q = QueryEvents()\n        assert isinstance(uriList, list), \"uriList has to be a list of strings that represent event uris\"\n        q.queryParams = { \"action\": \"getEvents\", \"eventUriList\": \",\".join(uriList) }\n        return q", "category": "Python"}, {"instruction": "def _det_tc(detector_name, ra, dec, tc, ref_frame='geocentric'):\n    \"\"\"Returns the coalescence time of a signal in the given detector.\n\n    Parameters\n    ----------\n    detector_name : string\n        The name of the detector, e.g., 'H1'.\n    ra : float\n        The right ascension of the signal, in radians.\n    dec : float\n        The declination of the signal, in radians.\n    tc : float\n        The GPS time of the coalescence of the signal in the `ref_frame`.\n    ref_frame : {'geocentric', string}\n        The reference frame that the given coalescence time is defined in.\n        May specify 'geocentric', or a detector name; default is 'geocentric'.\n\n    Returns\n    -------\n    float :\n        The GPS time of the coalescence in detector `detector_name`.\n    \"\"\"\n", "input": "", "output": "    if ref_frame == detector_name:\n        return tc\n    detector = Detector(detector_name)\n    if ref_frame == 'geocentric':\n        return tc + detector.time_delay_from_earth_center(ra, dec, tc)\n    else:\n        other = Detector(ref_frame)\n        return tc + detector.time_delay_from_detector(other, ra, dec, tc)", "category": "Python"}, {"instruction": "def show_system_info_input_rbridge_id(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        show_system_info = ET.Element(\"show_system_info\")\n        config = show_system_info\n        input = ET.SubElement(show_system_info, \"input\")\n        rbridge_id = ET.SubElement(input, \"rbridge-id\")\n        rbridge_id.text = kwargs.pop('rbridge_id')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def validate_subfolders(filedir, metadata):\n    \"\"\"\n    Check that all folders in the given directory have a corresponding\n    entry in the metadata file, and vice versa.\n\n    :param filedir: This field is the target directory from which to\n        match metadata\n    :param metadata: This field contains the metadata to be matched.\n    \"\"\"\n", "input": "", "output": "    if not os.path.isdir(filedir):\n        print(\"Error: \" + filedir + \" is not a directory\")\n        return False\n    subfolders = os.listdir(filedir)\n    for subfolder in subfolders:\n        if subfolder not in metadata:\n            print(\"Error: folder \" + subfolder +\n                  \" present on disk but not in metadata\")\n            return False\n    for subfolder in metadata:\n        if subfolder not in subfolders:\n            print(\"Error: folder \" + subfolder +\n                  \" present in metadata but not on disk\")\n            return False\n    return True", "category": "Python"}, {"instruction": "def honeypot_exempt(view_func):\n    \"\"\"\n        Mark view as exempt from honeypot validation\n    \"\"\"\n", "input": "", "output": "    # borrowing liberally from django's csrf_exempt\n    def wrapped(*args, **kwargs):\n        return view_func(*args, **kwargs)\n    wrapped.honeypot_exempt = True\n    return wraps(view_func, assigned=available_attrs(view_func))(wrapped)", "category": "Python"}, {"instruction": "def _worker_handler(future, worker, pipe, timeout):\n    \"\"\"Worker lifecycle manager.\n\n    Waits for the worker to be perform its task,\n    collects result, runs the callback and cleans up the process.\n\n    \"\"\"\n", "input": "", "output": "    result = _get_result(future, pipe, timeout)\n\n    if isinstance(result, BaseException):\n        if isinstance(result, ProcessExpired):\n            result.exitcode = worker.exitcode\n\n        future.set_exception(result)\n    else:\n        future.set_result(result)\n\n    if worker.is_alive():\n        stop_process(worker)", "category": "Python"}, {"instruction": "def addButton(\n        self,\n        fnc,\n        states=(\"On\", \"Off\"),\n        c=(\"w\", \"w\"),\n        bc=(\"dg\", \"dr\"),\n        pos=(20, 40),\n        size=24,\n        font=\"arial\",\n        bold=False,\n        italic=False,\n        alpha=1,\n        angle=0,\n    ):\n        \"\"\"Add a button to the renderer window.\n        \n        :param list states: a list of possible states ['On', 'Off']\n        :param c:      a list of colors for each state\n        :param bc:     a list of background colors for each state\n        :param pos:    2D position in pixels from left-bottom corner\n        :param size:   size of button font\n        :param str font:   font type (arial, courier, times)\n        :param bool bold:   bold face (False)\n        :param bool italic: italic face (False)\n        :param float alpha:  opacity level\n        :param float angle:  anticlockwise rotation in degrees\n\n        .. hint:: |buttons| |buttons.py|_\n        \"\"\"\n", "input": "", "output": "        return addons.addButton(fnc, states, c, bc, pos, size, font, bold, italic, alpha, angle)", "category": "Python"}, {"instruction": "def get_query_parameters(config_parameters, date_time=datetime.datetime.now()):\n    \"\"\" Merge the given parameters with the airflow macros. Enables macros (like '@_ds') in sql.\n\n    Args:\n      config_parameters: The user-specified list of parameters in the cell-body.\n      date_time: The timestamp at which the parameters need to be evaluated. E.g. when the table\n          is <project-id>.<dataset-id>.logs_%(_ds)s, the '_ds' evaluates to the current date-time.\n\n    Returns:\n      A list of query parameters that are in the format for the BQ service\n    \"\"\"\n", "input": "", "output": "    merged_parameters = Query.merge_parameters(config_parameters, date_time=date_time,\n                                               macros=False, types_and_values=True)\n    # We're exposing a simpler schema format than the one actually required by BigQuery to make\n    # magics easier. We need to convert between the two formats\n    parsed_params = []\n    for key, value in merged_parameters.items():\n      parsed_params.append({\n        'name': key,\n        'parameterType': {\n          'type': value['type']\n        },\n        'parameterValue': {\n          'value': value['value']\n        }\n      })\n    return parsed_params", "category": "Python"}, {"instruction": "def load_config_file(config_file: str) -> HmipConfig:\n    \"\"\"Loads the config ini file.\n    :raises a FileNotFoundError when the config file does not exist.\"\"\"\n", "input": "", "output": "    _config = configparser.ConfigParser()\n    with open(config_file, \"r\") as fl:\n        _config.read_file(fl)\n        logging_filename = _config.get(\"LOGGING\", \"FileName\", fallback=\"hmip.log\")\n        if logging_filename == \"None\":\n            logging_filename = None\n\n        _hmip_config = HmipConfig(\n            _config[\"AUTH\"][\"AuthToken\"],\n            _config[\"AUTH\"][\"AccessPoint\"],\n            int(_config.get(\"LOGGING\", \"Level\", fallback=30)),\n            logging_filename,\n            _config._sections,\n        )\n        return _hmip_config", "category": "Python"}, {"instruction": "def apply_filter(self, criteria):\n        \"\"\"\n        Apply the given filter criteria on the given column.\n        :param str criteria: the criteria to apply\n        criteria example:\n        {\n          \"color\": \"string\",\n          \"criterion1\": \"string\",\n          \"criterion2\": \"string\",\n          \"dynamicCriteria\": \"string\",\n          \"filterOn\": \"string\",\n          \"icon\": {\"@odata.type\": \"microsoft.graph.workbookIcon\"},\n          \"values\": {\"@odata.type\": \"microsoft.graph.Json\"}\n        }\n        \"\"\"\n", "input": "", "output": "        url = self.build_url(self._endpoints.get('apply_filter'))\n        return bool(self.session.post(url, data={'criteria': criteria}))", "category": "Python"}, {"instruction": "def to_css(self):\n        \"\"\"Return a stylesheet that will show the wrapped error at the top of\n        the browser window.\n        \"\"\"\n", "input": "", "output": "        # TODO should this include the traceback?  any security concerns?\n        prefix = self.format_prefix()\n        original_error = self.format_original_error()\n        sass_stack = self.format_sass_stack()\n\n        message = prefix + \"\\n\" + sass_stack + original_error\n\n        # Super simple escaping: only quotes and newlines are illegal in css\n        # strings\n        message = message.replace('\\\\', '\\\\\\\\')\n        message = message.replace('\"', '\\\\\"')\n        # use the maximum six digits here so it doesn't eat any following\n        # characters that happen to look like hex\n        message = message.replace('\\n', '\\\\00000A')\n\n        return BROWSER_ERROR_TEMPLATE.format('\"' + message + '\"')", "category": "Python"}, {"instruction": "def _round(self, multiple=1):\n        \"\"\"\n        This is the environment implementation of\n        :meth:`BaseKerning.round`. **multiple** will be an ``int``.\n\n        Subclasses may override this method.\n        \"\"\"\n", "input": "", "output": "        for pair, value in self.items():\n            value = int(normalizers.normalizeRounding(\n                value / float(multiple))) * multiple\n            self[pair] = value", "category": "Python"}, {"instruction": "def encrypt_password(raw_password, algorithm='sha1', salt=None):\n    \"\"\"\n    Returns a string of the hexdigest of the given plaintext password and salt\n    using the given algorithm ('md5', 'sha1' or other supported by hashlib).\n    \"\"\"\n", "input": "", "output": "    if salt is None:\n        salt = binascii.hexlify(os.urandom(3))[:5]\n    else:\n        salt = salt.encode('utf-8')\n\n    raw_password = raw_password.encode('utf-8')\n    hash = hashlib.new(algorithm, salt+raw_password).hexdigest()\n    return '{}${}${}'.format(algorithm, salt.decode('utf-8'), hash)", "category": "Python"}, {"instruction": "def is_valid_hostname(hostname):\n    '''Return True if hostname is valid, otherwise False.'''\n", "input": "", "output": "    if not isinstance(hostname, str):\n        raise TypeError('hostname must be a string')\n    # strip exactly one dot from the right, if present\n    if hostname and hostname[-1] == \".\":\n        hostname = hostname[:-1]\n    if not hostname or len(hostname) > 253:\n        return False\n    labels = hostname.split('.')\n    # the TLD must be not all-numeric\n    if re.match(NUMERIC_REGEX, labels[-1]):\n        return False\n    return all(LABEL_REGEX.match(label) for label in labels)", "category": "Python"}, {"instruction": "def get_user_info(self, user_id, **kwargs):\n        \"\"\"\n        Retrieves information about a user,\n        the result is only limited to what the callee has access to view.\n        :param user_id:\n        :param kwargs:\n        :return:\n        \"\"\"\n", "input": "", "output": "        return GetUserInfo(settings=self.settings, **kwargs).call(\n            user_id=user_id,\n            **kwargs\n        )", "category": "Python"}, {"instruction": "def add_special(self, name):\n        \"\"\"Register a special name like `loop`.\"\"\"\n", "input": "", "output": "        self.undeclared.discard(name)\n        self.declared.add(name)", "category": "Python"}, {"instruction": "def check_dataframe_for_duplicate_records(obs_id_col, alt_id_col, df):\n    \"\"\"\n    Checks a cross-sectional dataframe of long-format data for duplicate\n    observations. Duplicate observations are defined as rows with the same\n    observation id value and the same alternative id value.\n\n    Parameters\n    ----------\n    obs_id_col : str.\n        Denotes the column in `df` that contains the observation ID\n        values for each row.\n    alt_id_col : str.\n        Denotes the column in `df` that contains the alternative ID\n        values for each row.\n    df : pandas dataframe.\n        The dataframe of long format data that is to be checked for duplicates.\n\n    Returns\n    -------\n    None.\n    \"\"\"\n", "input": "", "output": "    if df.duplicated(subset=[obs_id_col, alt_id_col]).any():\n        msg = \"One or more observation-alternative_id pairs is not unique.\"\n        raise ValueError(msg)\n\n    return None", "category": "Python"}, {"instruction": "def db(self, request):\n        '''Single Database Query'''\n", "input": "", "output": "        with self.mapper.begin() as session:\n            world = session.query(World).get(randint(1, 10000))\n        return Json(self.get_json(world)).http_response(request)", "category": "Python"}, {"instruction": "def flatten_all_but_last(a):\n  \"\"\"Flatten all dimensions of a except the last.\"\"\"\n", "input": "", "output": "  ret = tf.reshape(a, [-1, tf.shape(a)[-1]])\n  if not tf.executing_eagerly():\n    ret.set_shape([None] + a.get_shape().as_list()[-1:])\n  return ret", "category": "Python"}, {"instruction": "def get_elapsed_time(self, issue):\n        \"\"\"\n        Gets the elapsed time since the last mark (either the updated time of the last log or the time that the issue was\n        marked in progress)\n        \"\"\"\n", "input": "", "output": "        last_mark = None\n\n        # Get the last mark from the work logs\n        worklogs = self.get_worklog(issue)\n        if worklogs:\n            last_worklog = worklogs[-1]\n            last_mark = dateutil.parser.parse(last_worklog.raw['updated'])\n\n        # If no worklogs, get the time since the issue was marked In Progress\n        if not last_mark:\n            last_mark = self.get_datetime_issue_in_progress(issue)\n\n        if last_mark:\n            now = datetime.datetime.now(dateutil.tz.tzlocal())\n            delta = now - last_mark\n            minutes = int(utils.timedelta_total_seconds(delta) / 60)\n            if minutes > 0:\n                return str(minutes) + 'm'\n            else:\n                return None", "category": "Python"}, {"instruction": "def _dump(self):\n        \"\"\"For debugging, dump the entire data structure.\"\"\"\n", "input": "", "output": "        pp = pprint.PrettyPrinter(indent=4)\n\n        print(\"=== Variables ===\")\n        print(\"-- Globals --\")\n        pp.pprint(self._global)\n        print(\"-- Bot vars --\")\n        pp.pprint(self._var)\n        print(\"-- Substitutions --\")\n        pp.pprint(self._sub)\n        print(\"-- Person Substitutions --\")\n        pp.pprint(self._person)\n        print(\"-- Arrays --\")\n        pp.pprint(self._array)\n\n        print(\"=== Topic Structure ===\")\n        pp.pprint(self._topics)\n        print(\"=== %Previous Structure ===\")\n        pp.pprint(self._thats)\n\n        print(\"=== Includes ===\")\n        pp.pprint(self._includes)\n\n        print(\"=== Inherits ===\")\n        pp.pprint(self._lineage)\n\n        print(\"=== Sort Buffer ===\")\n        pp.pprint(self._sorted)\n\n        print(\"=== Syntax Tree ===\")\n        pp.pprint(self._syntax)", "category": "Python"}, {"instruction": "def eval_expression(value):\n    \"\"\" Evaluate a full time expression \"\"\"\n", "input": "", "output": "    start = eval_function(value.ts_expression[0])\n    interval = eval_interval(value.ts_expression[2])\n    op = value.ts_expression[1]\n    if op == \"+\":\n        return start + interval\n    elif op == \"-\":\n        return start - interval\n    else:\n        raise SyntaxError(\"Unrecognized operator %r\" % op)", "category": "Python"}, {"instruction": "def paintEvent(self, event):\r\n        \"\"\"\r\n        Overloads the paint even to render this button.\r\n        \"\"\"\n", "input": "", "output": "        if self.isHoverable() and self.icon().isNull():\r\n            return\r\n\r\n        # initialize the painter\r\n        painter = QtGui.QStylePainter()\r\n        painter.begin(self)\r\n        try:\r\n            option = QtGui.QStyleOptionToolButton()\r\n            self.initStyleOption(option)\r\n\r\n            # generate the scaling and rotating factors\r\n            x_scale = 1\r\n            y_scale = 1\r\n\r\n            if self.flipHorizontal():\r\n                x_scale = -1\r\n            if self.flipVertical():\r\n                y_scale = -1\r\n\r\n            center = self.rect().center()\r\n            painter.translate(center.x(), center.y())\r\n            painter.rotate(self.angle())\r\n            painter.scale(x_scale, y_scale)\r\n            painter.translate(-center.x(), -center.y())\r\n\r\n            painter.drawComplexControl(QtGui.QStyle.CC_ToolButton, option)\r\n        finally:\r\n            painter.end()", "category": "Python"}, {"instruction": "def clean_highlight(self):\n        \"\"\"\n        Remove the empty highlight\n        \"\"\"\n", "input": "", "output": "        if not self.valid:\n            return\n\n        for hit in self._results['hits']['hits']:\n            if 'highlight' in hit:\n                hl = hit['highlight']\n                for key, item in list(hl.items()):\n                    if not item:\n                        del hl[key]", "category": "Python"}, {"instruction": "def search_word(confluence, word):\r\n    \"\"\"\r\n    Get all found pages with order by created date\r\n    :param confluence:\r\n    :param word:\r\n    :return: json answer\r\n    \"\"\"\n", "input": "", "output": "    cql = \"siteSearch ~ {} order by created\".format(word)\r\n    answers = confluence.cql(cql)\r\n    for answer in answers.get('results'):\r\n        print(answer)", "category": "Python"}, {"instruction": "def bdd_common_after_all(context_or_world):\n    \"\"\"Common after all method in behave or lettuce\n\n    :param context_or_world: behave context or lettuce world\n    \"\"\"\n", "input": "", "output": "    # Close drivers\n    DriverWrappersPool.close_drivers(scope='session', test_name='multiple_tests',\n                                     test_passed=context_or_world.global_status['test_passed'])\n\n    # Update tests status in Jira\n    change_all_jira_status()", "category": "Python"}, {"instruction": "def previous(self, rows: List[Row]) -> List[Row]:\n        \"\"\"\n        Takes an expression that evaluates to a single row, and returns the row that occurs before\n        the input row in the original set of rows. If the input row happens to be the top row, we\n        will return an empty list.\n        \"\"\"\n", "input": "", "output": "        if not rows:\n            return []\n        input_row_index = self._get_row_index(rows[0])\n        if input_row_index > 0:\n            return [self.table_data[input_row_index - 1]]\n        return []", "category": "Python"}, {"instruction": "def unsubscribe(self, peer_jid):\n        \"\"\"\n        Unsubscribe from the presence of the given `peer_jid`.\n        \"\"\"\n", "input": "", "output": "        self.client.enqueue(\n            stanza.Presence(type_=structs.PresenceType.UNSUBSCRIBE,\n                            to=peer_jid)\n        )", "category": "Python"}, {"instruction": "def publish(self, topic=\"/controller\", qos=0, payload=None):\n        \"\"\"\n        publish(self, topic, payload=None, qos=0, retain=False)\n        Returns a tuple (result, mid), where result is MQTT_ERR_SUCCESS to\n        indicate success or MQTT_ERR_NO_CONN if the client is not currently\n        connected.  mid is the message ID for the publish request. The mid\n        value can be used to track the publish request by checking against the\n        mid argument in the on_publish() callback if it is defined.\n        \"\"\"\n", "input": "", "output": "        result = self.client.publish(topic,\n                                     payload=json.dumps(payload),\n                                     qos=qos)\n        if result[0] == mqtt.MQTT_ERR_NO_CONN:\n            raise RuntimeError(\"No connection\")\n        return result[1]", "category": "Python"}, {"instruction": "def latexsnippet(code, kvs, staffsize=17, initiallines=1):\n    \"\"\"Take in account key/values\"\"\"\n", "input": "", "output": "    snippet = ''\n    staffsize = int(kvs['staffsize']) if 'staffsize' in kvs \\\n        else staffsize\n    initiallines = int(kvs['initiallines']) if 'initiallines' in kvs \\\n        else initiallines\n    annotationsize = .5 * staffsize\n    if 'mode' in kvs:\n        snippet = (\n            \"\\\\greannotation{{\\\\fontsize{%s}{%s}\\\\selectfont{}%s}}\\n\" %\n            (annotationsize, annotationsize, kvs['mode'])\n        ) + snippet\n    if 'annotation' in kvs:\n        snippet = (\n            \"\\\\grechangedim{annotationseparation}{%s mm}{fixed}\\n\"\n            \"\\\\greannotation{{\\\\fontsize{%s}{%s}\\\\selectfont{}%s}}\\n\" %\n            (staffsize / 60, annotationsize, annotationsize, kvs['annotation'])\n        ) + snippet\n    snippet = (\n        \"\\\\gresetinitiallines{%s}\\n\" % initiallines +\n        \"\\\\grechangestaffsize{%s}\\n\" % staffsize +\n        \"\\\\grechangestyle{initial}{\\\\fontsize{%s}{%s}\\\\selectfont{}}\" %\n        (2.5 * staffsize, 2.5 * staffsize)\n    ) + snippet\n    snippet = \"\\\\setlength{\\\\parskip}{0pt}\\n\" + snippet + code\n    return snippet", "category": "Python"}, {"instruction": "def confidence_interval(self, alpha=0.9):\n        \"\"\"Compute confidence interval on accumulated metric values\n\n        Parameters\n        ----------\n        alpha : float, optional\n            Probability that the returned confidence interval contains\n            the true metric value.\n\n        Returns\n        -------\n        (center, (lower, upper))\n            with center the mean of the conditional pdf of the metric value\n            and (lower, upper) is a confidence interval centered on the median,\n            containing the estimate to a probability alpha.\n\n        See Also:\n        ---------\n        scipy.stats.bayes_mvs\n\n        \"\"\"\n", "input": "", "output": "        m, _, _ = scipy.stats.bayes_mvs(\n            [r[self.metric_name_] for _, r in self.results_], alpha=alpha)\n        return m", "category": "Python"}, {"instruction": "def tai_timestamp():\n    \"\"\"Return current TAI timestamp.\"\"\"\n", "input": "", "output": "    timestamp = time.time()\n    date = datetime.utcfromtimestamp(timestamp)\n    if date.year < 1972:\n        return timestamp\n    offset = 10 + timestamp\n    leap_seconds = [\n        (1972, 1, 1),\n        (1972, 7, 1),\n        (1973, 1, 1),\n        (1974, 1, 1),\n        (1975, 1, 1),\n        (1976, 1, 1),\n        (1977, 1, 1),\n        (1978, 1, 1),\n        (1979, 1, 1),\n        (1980, 1, 1),\n        (1981, 7, 1),\n        (1982, 7, 1),\n        (1983, 7, 1),\n        (1985, 7, 1),\n        (1988, 1, 1),\n        (1990, 1, 1),\n        (1991, 1, 1),\n        (1992, 7, 1),\n        (1993, 7, 1),\n        (1994, 7, 1),\n        (1996, 1, 1),\n        (1997, 7, 1),\n        (1999, 1, 1),\n        (2006, 1, 1),\n        (2009, 1, 1),\n        (2012, 7, 1),\n        (2015, 7, 1),\n        (2017, 1, 1),\n    ]\n    for idx, leap_date in enumerate(leap_seconds):\n        if leap_date >= (date.year, date.month, date.day):\n            return idx - 1 + offset\n    return len(leap_seconds) - 1 + offset", "category": "Python"}, {"instruction": "def kappa(self, x, y, kwargs, diff=diff):\n        \"\"\"\n        computes the convergence\n        :return: kappa\n        \"\"\"\n", "input": "", "output": "        f_xx, f_xy, f_yx, f_yy = self.hessian(x, y, kwargs, diff=diff)\n        kappa = 1./2 * (f_xx + f_yy)\n        return kappa", "category": "Python"}, {"instruction": "def transaction(self, transaction_id, expand_merchant=False):\n        \"\"\"\n        Returns an individual transaction, fetched by its id.\n\n        Official docs:\n            https://monzo.com/docs/#retrieve-transaction\n\n        :param transaction_id: Monzo transaction ID\n        :type transaction_id: str\n        :param expand_merchant: whether merchant data should be included\n        :type expand_merchant: bool\n        :returns: Monzo transaction details\n        :rtype: MonzoTransaction\n        \"\"\"\n", "input": "", "output": "        endpoint = '/transactions/{}'.format(transaction_id)\n\n        data = dict()\n        if expand_merchant:\n            data['expand[]'] = 'merchant'\n\n        response = self._get_response(\n            method='get', endpoint=endpoint, params=data,\n        )\n\n        return MonzoTransaction(data=response.json()['transaction'])", "category": "Python"}, {"instruction": "def execute(self, input_data):\n        ''' Execute the PEIndicators worker '''\n", "input": "", "output": "        raw_bytes = input_data['sample']['raw_bytes']\n\n        # Analyze the output of pefile for any anomalous conditions.\n        # Have the PE File module process the file\n        try:\n            self.pefile_handle = pefile.PE(data=raw_bytes, fast_load=False)\n        except (AttributeError, pefile.PEFormatError), error:\n            return {'error': str(error), 'indicator_list': [{'Error': 'PE module failed!'}]}\n\n        indicators = []\n        indicators += [{'description': warn, 'severity': 2, 'category': 'PE_WARN'} \n                       for warn in self.pefile_handle.get_warnings()]\n\n        # Automatically invoke any method of this class that starts with 'check'\n        check_methods = self._get_check_methods()\n        for check_method in check_methods:\n            hit_data = check_method()\n            if hit_data:\n                indicators.append(hit_data)\n\n        return {'indicator_list': indicators}", "category": "Python"}, {"instruction": "def get_context_first_matching_object(context, context_lookups):\n    \"\"\"\n    Return the first object found in the context,\n    from a list of keys, with the matching key.\n    \"\"\"\n", "input": "", "output": "    for key in context_lookups:\n        context_object = context.get(key)\n        if context_object:\n            return key, context_object\n    return None, None", "category": "Python"}, {"instruction": "def get_timeout(self, callback=None):\n        \"\"\"\n        Get the visibility timeout for the queue.\n        \n        :rtype: int\n        :return: The number of seconds as an integer.\n        \"\"\"\n", "input": "", "output": "        def got_timeout(a):\n            if callable(callback):\n                callback(int(a['VisibilityTimeout']))\n        self.get_attributes('VisibilityTimeout', callback=got_timeout)", "category": "Python"}, {"instruction": "def get_build_path(self):\n        \"\"\"\n        Used to determine where to build the page. Override this if you\n        would like your page at a different location. By default it\n        will be built at self.get_url() + \"/index.html\"\n        \"\"\"\n", "input": "", "output": "        target_path = path.join(settings.BUILD_DIR, self.get_url().lstrip('/'))\n        if not self.fs.exists(target_path):\n            logger.debug(\"Creating {}\".format(target_path))\n            self.fs.makedirs(target_path)\n        return os.path.join(target_path, 'index.html')", "category": "Python"}, {"instruction": "def get_from_postcode(self, postcode, distance, skip_cache=False):\n        \"\"\"\n        Calls `postcodes.get_from_postcode` but checks correctness of \n        `distance`, and by default utilises a local cache.\n\n        :param skip_cache: optional argument specifying whether to skip \n                           the cache and make an explicit request.\n\n        :raises IllegalPointException: if the latitude or longitude \n                                       are out of bounds.\n\n        :returns: a list of dicts containing postcode data within the \n                  specified distance.\n        \"\"\"\n", "input": "", "output": "        distance = float(distance)\n        if distance < 0:\n            raise IllegalDistanceException(\"Distance must not be negative\")\n        # remove spaces and change case here due to caching\n        postcode = postcode.lower().replace(' ', '')\n        return self._lookup(skip_cache, get_from_postcode, postcode, \n                            float(distance))", "category": "Python"}, {"instruction": "def _secret_yaml(loader, node):\n    \"\"\"Load secrets and embed it into the configuration YAML.\"\"\"\n", "input": "", "output": "    fname = os.path.join(os.path.dirname(loader.name), \"secrets.yaml\")\n\n    try:\n        with open(fname, encoding=\"utf-8\") as secret_file:\n            secrets = YAML(typ=\"safe\").load(secret_file)\n    except FileNotFoundError:\n        raise ValueError(\"Secrets file {} not found\".format(fname)) from None\n\n    try:\n        return secrets[node.value]\n    except KeyError:\n        raise ValueError(\"Secret {} not found\".format(node.value)) from None", "category": "Python"}, {"instruction": "def cast2theano_var(self, array_like, name=None):\n        '''Cast `numpy.ndarray` into `theano.tensor` keeping `dtype` and `ndim`\n        compatible\n        '''\n", "input": "", "output": "        # extract the information of the input value\n        array = np.asarray(array_like)\n        args = (name, array.dtype)\n        ndim = array.ndim\n\n        # cast with the information above\n        if ndim == 0:\n            return T.scalar(*args)\n        elif ndim == 1:\n            return T.vector(*args)\n        elif ndim == 2:\n            return T.matrix(*args)\n        elif ndim == 3:\n            return T.tensor3(*args)\n        elif ndim == 4:\n            return T.tensor4(*args)\n        else:\n            raise ValueError('extheano.jit.Compiler: Unsupported type or shape')", "category": "Python"}, {"instruction": "def decrypt(*args, **kwargs):\n    \"\"\" Decrypts legacy or spec-compliant JOSE token.\n    First attempts to decrypt the token in a legacy mode\n    (https://tools.ietf.org/html/draft-ietf-oauth-json-web-token-19).\n    If it is not a valid legacy token then attempts to decrypt it in a\n    spec-compliant way (http://tools.ietf.org/html/rfc7519)\n    \"\"\"\n", "input": "", "output": "    try:\n        return legacy_decrypt(*args, **kwargs)\n    except (NotYetValid, Expired) as e:\n        # these should be raised immediately.\n        # The token has been decrypted successfully to get to here.\n        # decrypting using `legacy_decrypt` will not help things.\n        raise e\n    except (Error, ValueError) as e:\n        return spec_compliant_decrypt(*args, **kwargs)", "category": "Python"}, {"instruction": "def get_bundle(self, bundle_name, extensions=None):\n        \"\"\" Get all the chunks contained in a bundle \"\"\"\n", "input": "", "output": "        if self.stats.get('status') == 'done':\n            bundle = self.stats.get('chunks', {}).get(bundle_name, None)\n            if bundle is None:\n                raise KeyError('No such bundle {0!r}.'.format(bundle_name))\n            test = self._chunk_filter(extensions)\n            return [self._add_url(c) for c in bundle if test(c)]\n        elif self.stats.get('status') == 'error':\n            raise RuntimeError(\"{error}: {message}\".format(**self.stats))\n        else:\n            raise RuntimeError(\n                \"Bad webpack stats file {0} status: {1!r}\"\n                .format(self.state.stats_file, self.stats.get('status')))", "category": "Python"}, {"instruction": "def get_next_sort_string(self, field):\n        \"\"\"\n        If we're already sorted by the field then the sort query\n        returned reverses the sort order.\n        \"\"\"\n", "input": "", "output": "        # self.sort_field is the currect sort field\n        if field == self.sort_field:\n            next_sort = self.toggle_sort_order() + field\n        else:\n            default_order_for_field = \\\n                self._allowed_sort_fields[field]['default_direction']\n            next_sort = default_order_for_field + field\n        return self.get_sort_string(next_sort)", "category": "Python"}, {"instruction": "def getMessagesRN(self, CorpNum, RequestNum, UserID=None):\r\n        \"\"\" \ubb38\uc790 \uc804\uc1a1\uacb0\uacfc \uc870\ud68c\r\n            args\r\n                CorpNum : \ud31d\ube4c\ud68c\uc6d0 \uc0ac\uc5c5\uc790\ubc88\ud638\r\n                RequestNum : \uc804\uc1a1\uc694\uccad\uc2dc \ud560\ub2f9\ud55c \uc804\uc1a1\uc694\uccad\ubc88\ud638\r\n                UserID : \ud31d\ube4c\ud68c\uc6d0 \uc544\uc774\ub514\r\n            return\r\n                \uc804\uc1a1\uc815\ubcf4 as list\r\n            raise\r\n                PopbillException\r\n        \"\"\"\n", "input": "", "output": "        if RequestNum == None or RequestNum == '':\r\n            raise PopbillException(-99999999, \"\uc694\uccad\ubc88\ud638\uac00 \uc785\ub825\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\")\r\n\r\n        return self._httpget('/Message/Get/' + RequestNum, CorpNum, UserID)", "category": "Python"}, {"instruction": "def _register_lltd_specific_class(*attr_types):\n    \"\"\"This can be used as a class decorator; if we want to support Python\n    2.5, we have to replace\n\n@_register_lltd_specific_class(x[, y[, ...]])\nclass LLTDAttributeSpecific(LLTDAttribute):\n[...]\n\nby\n\nclass LLTDAttributeSpecific(LLTDAttribute):\n[...]\nLLTDAttributeSpecific = _register_lltd_specific_class(x[, y[, ...]])(\n    LLTDAttributeSpecific\n)\n\n    \"\"\"\n", "input": "", "output": "    def _register(cls):\n        for attr_type in attr_types:\n            SPECIFIC_CLASSES[attr_type] = cls\n        type_fld = LLTDAttribute.fields_desc[0].copy()\n        type_fld.default = attr_types[0]\n        cls.fields_desc = [type_fld] + cls.fields_desc\n        return cls\n    return _register", "category": "Python"}, {"instruction": "async def crawl_raw(self, res):\n        \"\"\" crawl the raw doc, and save it asynchronous.\n\n        :param res: {'url','', 'name': ''}\n        :type res: ``dict``\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        cnt = await self.async_get(res)\n        if cnt:\n            loop_ = asyncio.get_event_loop()\n            await loop_.run_in_executor(None, self.write_hd, res.get('name'), cnt)\n            return True\n        else:\n            return False", "category": "Python"}, {"instruction": "def is_admin():\n    \"\"\"\n    https://stackoverflow.com/a/19719292\n    @return: True if the current user is an 'Admin' whatever that\n    means (root on Unix), otherwise False.\n    Warning: The inner function fails unless you have Windows XP SP2 or\n    higher. The failure causes a traceback to be printed and this\n    function to return False.\n    \"\"\"\n", "input": "", "output": "\n    if os.name == 'nt':\n        import ctypes\n        import traceback\n        # WARNING: requires Windows XP SP2 or higher!\n        try:\n            return ctypes.windll.shell32.IsUserAnAdmin()\n        except:\n            traceback.print_exc()\n            return False\n    else:\n        # Check for root on Posix\n        return os.getuid() == 0", "category": "Python"}, {"instruction": "def compute_key(cli, familly, discriminant=None):\n    \"\"\"This function is used to compute a unique key from all connection parametters.\"\"\"\n", "input": "", "output": "    hash_key = hashlib.sha256()\n    hash_key.update(familly)\n    hash_key.update(cli.host)\n    hash_key.update(cli.user)\n    hash_key.update(cli.password)\n    if discriminant:\n        if isinstance(discriminant, list):\n            for i in discriminant:\n                if i is not None and i is not False:\n                    hash_key.update(str(i))\n        elif isinstance(discriminant, tuple):\n            for i in discriminant:\n                if i is not None and i is not False:\n                    hash_key.update(str(i))\n        else:\n            hash_key.update(discriminant)\n    hash_key = hash_key.hexdigest()\n    cli.log.debug(\"hash_key: \" + hash_key)\n    return hash_key", "category": "Python"}, {"instruction": "def usergroups_users_update(\n        self, *, usergroup: str, users: List[str], **kwargs\n    ) -> SlackResponse:\n        \"\"\"Update the list of users for a User Group\n\n        Args:\n            usergroup (str): The encoded ID of the User Group to update.\n                e.g. 'S0604QSJC'\n            users (list): A list user IDs that represent the entire list of\n                users for the User Group. e.g. ['U060R4BJ4', 'U060RNRCZ']\n        \"\"\"\n", "input": "", "output": "        self._validate_xoxp_token()\n        kwargs.update({\"usergroup\": usergroup, \"users\": users})\n        return self.api_call(\"usergroups.users.update\", json=kwargs)", "category": "Python"}, {"instruction": "def pop_back(self):\r\n        '''Remove the last element from the :class:`Sequence`.'''\n", "input": "", "output": "        backend = self.backend\r\n        return backend.execute(backend.structure(self).pop_back(),\r\n                               self.value_pickler.loads)", "category": "Python"}, {"instruction": "def setStopAction(self, action, *args, **kwargs):\n\t\t\"\"\"\n\t\tSet a function to call when run() is stopping, after the main action is called.\n\n\t\tParameters\n\t\t----------\n\t\taction: function pointer\n\t\t\tThe function to call.\n\t\t*args\n\t\t\tPositional arguments to pass to action.\n\t\t**kwargs:\n\t\t\tKeyword arguments to pass to action.\n\t\t\"\"\"\n", "input": "", "output": "\t\tself.stop_action = action\n\t\tself.stop_args = args\n\t\tself.stop_kwargs = kwargs", "category": "Python"}, {"instruction": "def load(self, read_tuple_name):\n        \"\"\"Load RNF values from a read tuple name.\n\n\t\tArgs:\n\t\t\tread_tuple_name (str): Read tuple name which the values are taken from.\n\t\t\"\"\"\n", "input": "", "output": "        self.prefix_width = 0\n        self.read_tuple_id_width = 0\n        self.genome_id_width = 0\n        self.chr_id_width = 0\n        self.coor_width = 0\n\n        parts = read_tuple_name.split(\"__\")\n        self.prefix_width = len(parts[0])\n        self.read_tuple_id_width = len(parts[1])\n\n        segments = parts[2][1:-1].split(\"),(\")\n        for segment in segments:\n            int_widths = list(map(len, segment.split(\",\")))\n            self.genome_id_width = max(self.genome_id_width, int_widths[0])\n            self.chr_id_width = max(self.chr_id_width, int_widths[1])\n            self.coor_width = max(self.coor_width, int_widths[2], int_widths[3])", "category": "Python"}, {"instruction": "def expand(self,x,y,z):\n        \"\"\"\n        Expands the bounding\n        \"\"\"\n", "input": "", "output": "        if x != None:\n            if self.minx is None or x < self.minx:\n                self.minx = x\n            if self.maxx is None or x > self.maxx:\n                self.maxx = x\n        if y != None:\n            if self.miny is None or y < self.miny:\n                self.miny = y\n            if self.maxy is None or y > self.maxy:\n                self.maxy = y\n        if z != None:\n            if self.minz is None or z < self.minz:\n                self.minz = z\n            if self.maxz is None or z > self.maxz:\n                self.maxz = z", "category": "Python"}, {"instruction": "def get_instance(self, payload):\n        \"\"\"\n        Build an instance of ExecutionInstance\n\n        :param dict payload: Payload response from the API\n\n        :returns: twilio.rest.studio.v1.flow.execution.ExecutionInstance\n        :rtype: twilio.rest.studio.v1.flow.execution.ExecutionInstance\n        \"\"\"\n", "input": "", "output": "        return ExecutionInstance(self._version, payload, flow_sid=self._solution['flow_sid'], )", "category": "Python"}, {"instruction": "def _reset_annot_refs(self):\n        \"\"\"Invalidate / delete all annots of this page.\"\"\"\n", "input": "", "output": "        for annot in self._annot_refs.values():\n            if annot:\n                annot._erase()\n        self._annot_refs.clear()", "category": "Python"}, {"instruction": "def disconnect(self, close=True):\n        \"\"\"\n        Closes the connection as well as logs off any of the\n        Disconnects the TCP connection and shuts down the socket listener\n        running in a thread.\n\n        :param close: Will close all sessions in the connection as well as the\n            tree connections of each session.\n        \"\"\"\n", "input": "", "output": "        if close:\n            for session in list(self.session_table.values()):\n                session.disconnect(True)\n\n        log.info(\"Disconnecting transport connection\")\n        self.transport.disconnect()", "category": "Python"}, {"instruction": "def create_manager(arguments):\n  \"\"\"A simple wrapper to JobManager() that places the statefile on the correct path by default\"\"\"\n", "input": "", "output": "\n  if arguments.statefile is None:\n    arguments.statefile = os.path.join(os.path.dirname(arguments.logdir), 'submitted.db')\n\n  arguments.statefile = os.path.realpath(arguments.statefile)\n\n  return manager.JobManager(statefile=arguments.statefile)", "category": "Python"}, {"instruction": "def read_data(self):\n        \"\"\"\n            Read data from the ndata file reference\n\n            :param reference: the reference from which to read\n            :return: a numpy array of the data; maybe None\n        \"\"\"\n", "input": "", "output": "        with self.__lock:\n            absolute_file_path = self.__file_path\n            #logging.debug(\"READ data file %s\", absolute_file_path)\n            with open(absolute_file_path, \"rb\") as fp:\n                local_files, dir_files, eocd = parse_zip(fp)\n                return read_data(fp, local_files, dir_files, b\"data.npy\")\n            return None", "category": "Python"}, {"instruction": "def add_process(self, name, cmd, quiet=False, env=None, cwd=None):\n        \"\"\"\n        Add a process to this manager instance. The process will not be started\n        until :func:`~honcho.manager.Manager.loop` is called.\n        \"\"\"\n", "input": "", "output": "        assert name not in self._processes, \"process names must be unique\"\n        proc = self._process_ctor(cmd,\n                                  name=name,\n                                  quiet=quiet,\n                                  colour=next(self._colours),\n                                  env=env,\n                                  cwd=cwd)\n        self._processes[name] = {}\n        self._processes[name]['obj'] = proc\n\n        # Update printer width to accommodate this process name\n        self._printer.width = max(self._printer.width, len(name))\n\n        return proc", "category": "Python"}, {"instruction": "def mktime_tz(data):\n    \"\"\"Turn a 10-tuple as returned by parsedate_tz() into a POSIX timestamp.\"\"\"\n", "input": "", "output": "    if data[9] is None:\n        # No zone info, so localtime is better assumption than GMT\n        return time.mktime(data[:8] + (-1,))\n    else:\n        t = calendar.timegm(data)\n        return t - data[9]", "category": "Python"}, {"instruction": "def cumsum(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Return SparseDataFrame of cumulative sums over requested axis.\n\n        Parameters\n        ----------\n        axis : {0, 1}\n            0 for row-wise, 1 for column-wise\n\n        Returns\n        -------\n        y : SparseDataFrame\n        \"\"\"\n", "input": "", "output": "        nv.validate_cumsum(args, kwargs)\n\n        if axis is None:\n            axis = self._stat_axis_number\n\n        return self.apply(lambda x: x.cumsum(), axis=axis)", "category": "Python"}, {"instruction": "def _get_course_descriptor_path(self, courseid):\n        \"\"\"\n        :param courseid: the course id of the course\n        :raise InvalidNameException, CourseNotFoundException\n        :return: the path to the descriptor of the course\n        \"\"\"\n", "input": "", "output": "        if not id_checker(courseid):\n            raise InvalidNameException(\"Course with invalid name: \" + courseid)\n        course_fs = self.get_course_fs(courseid)\n        if course_fs.exists(\"course.yaml\"):\n            return courseid+\"/course.yaml\"\n        if course_fs.exists(\"course.json\"):\n            return courseid+\"/course.json\"\n        raise CourseNotFoundException()", "category": "Python"}, {"instruction": "def draw(self):\r\n        '''\r\n        Draws samples from the `fake` distribution.\r\n\r\n        Returns:\r\n            `np.ndarray` of samples.\r\n        '''\n", "input": "", "output": "        observed_arr = self.noise_sampler.generate()\r\n        _ = self.__encoder_decoder_controller.encoder.inference(observed_arr)\r\n        arr = self.__encoder_decoder_controller.encoder.get_feature_points()\r\n        return arr", "category": "Python"}, {"instruction": "def get_video_transcript(video_id, language_code):\n    \"\"\"\n    Get video transcript info\n\n    Arguments:\n        video_id(unicode): A video id, it can be an edx_video_id or an external video id extracted from\n        external sources of a video component.\n        language_code(unicode): it will be the language code of the requested transcript.\n    \"\"\"\n", "input": "", "output": "    transcript = VideoTranscript.get_or_none(video_id=video_id, language_code=language_code)\n    return TranscriptSerializer(transcript).data if transcript else None", "category": "Python"}, {"instruction": "def log_interp(x, xp, *args, **kwargs):\n    \"\"\"Wrap log_interpolate_1d for deprecated log_interp.\"\"\"\n", "input": "", "output": "    return log_interpolate_1d(x, xp, *args, **kwargs)", "category": "Python"}, {"instruction": "def visible_to_user(self, user):\n        \"\"\"Get a list of visible events for a given user (usually request.user).\n\n        These visible events will be those that either have no groups\n        assigned to them (and are therefore public) or those in which\n        the user is a member.\n\n        \"\"\"\n", "input": "", "output": "\n        return (Event.objects.filter(approved=True).filter(Q(groups__in=user.groups.all()) | Q(groups__isnull=True) | Q(user=user)))", "category": "Python"}, {"instruction": "def threshold(np, acc, stream_raster, threshold=100., workingdir=None,\n                  mpiexedir=None, exedir=None, log_file=None, runtime_file=None, hostfile=None):\n        \"\"\"Run threshold for stream raster\"\"\"\n", "input": "", "output": "        fname = TauDEM.func_name('threshold')\n        return TauDEM.run(FileClass.get_executable_fullpath(fname, exedir),\n                          {'-ssa': acc}, workingdir,\n                          {'-thresh': threshold},\n                          {'-src': stream_raster},\n                          {'mpipath': mpiexedir, 'hostfile': hostfile, 'n': np},\n                          {'logfile': log_file, 'runtimefile': runtime_file})", "category": "Python"}, {"instruction": "def standardize_input_data(data):\n    \"\"\"\n    Ensure utf-8 encoded strings are passed to the indico API\n    \"\"\"\n", "input": "", "output": "    if type(data) == bytes:\n        data = data.decode('utf-8')\n    if type(data) == list:\n        data = [\n            el.decode('utf-8') if type(data) == bytes else el\n            for el in data\n        ]\n    return data", "category": "Python"}, {"instruction": "def ntp_authentication_key_encryption_type_sha1_type_sha1(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        ntp = ET.SubElement(config, \"ntp\", xmlns=\"urn:brocade.com:mgmt:brocade-ntp\")\n        authentication_key = ET.SubElement(ntp, \"authentication-key\")\n        keyid_key = ET.SubElement(authentication_key, \"keyid\")\n        keyid_key.text = kwargs.pop('keyid')\n        encryption_type = ET.SubElement(authentication_key, \"encryption-type\")\n        sha1_type = ET.SubElement(encryption_type, \"sha1-type\")\n        sha1 = ET.SubElement(sha1_type, \"sha1\")\n        sha1.text = kwargs.pop('sha1')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def breakpoint(self, name):\n        \"\"\"\n        Register the breakpoint of the name. Usage:\n        > breakpoint('setup-done')\n\n        The name should be used in a call of Driver.register_breakpoint(name)\n        method, which returns the Deferred, which will be fired by this\n        command.\n        \"\"\"\n", "input": "", "output": "        if name not in self._breakpoints:\n            self.warning(\"Reached breakpoint %s but found no \"\n                         \"callback registered\")\n            return\n        cb = self._breakpoints[name]\n        cb.callback(None)\n        return cb", "category": "Python"}, {"instruction": "def parse(self, file=None, string=None):\n        \"\"\"\n        SAX parse XML text.\n        @param file: Parse a python I{file-like} object.\n        @type file: I{file-like} object.\n        @param string: Parse string XML.\n        @type string: str\n        \"\"\"\n", "input": "", "output": "        timer = metrics.Timer()\n        timer.start()\n        sax, handler = self.saxparser()\n        if file is not None:\n            sax.parse(file)\n            timer.stop()\n            metrics.log.debug('sax (%s) duration: %s', file, timer)\n            return handler.nodes[0]\n        if string is not None:\n            source = InputSource(None)\n            source.setByteStream(StringIO(string))\n            sax.parse(source)\n            timer.stop()\n            metrics.log.debug('%s\\nsax duration: %s', string, timer)\n            return handler.nodes[0]", "category": "Python"}, {"instruction": "def getReturnPage(siteHistory,prior=False):\n    '''\n    This helper function is called in various places to get the return page from current\n    session data.  The session data (in the 'SITE_HISTORY' key) is a required argument.\n    '''\n", "input": "", "output": "\n    expiry = parse_datetime(\n        siteHistory.get('expiry',''),\n    )\n    if prior:\n        returnPage = siteHistory.get('priorPage',None)\n        returnPageName = siteHistory.get('priorPageName',None)\n    else:\n        returnPage = siteHistory.get('returnPage',None)\n        returnPageName = siteHistory.get('returnPageName',None)\n\n    if expiry and expiry >= timezone.now() and returnPage[0]:\n        return {\n            'url': reverse(returnPage[0],kwargs=returnPage[1]),\n            'title': returnPageName,\n        }\n    else:\n        return {'url': None, 'title': None}", "category": "Python"}, {"instruction": "def convert_to(obj, ac_ordered=False, ac_dict=None, **options):\n    \"\"\"\n    Convert a mapping objects to a dict or object of 'to_type' recursively.\n    Borrowed basic idea and implementation from bunch.unbunchify. (bunch is\n    distributed under MIT license same as this.)\n\n    :param obj: A mapping objects or other primitive object\n    :param ac_ordered: Use OrderedDict instead of dict to keep order of items\n    :param ac_dict: Callable to convert 'obj' to mapping object\n    :param options: Optional keyword arguments.\n\n    :return: A dict or OrderedDict or object of 'cls'\n\n    >>> OD = anyconfig.compat.OrderedDict\n    >>> convert_to(OD((('a', 1) ,)), cls=dict)\n    {'a': 1}\n    >>> convert_to(OD((('a', OD((('b', OD((('c', 1), ))), ))), )), cls=dict)\n    {'a': {'b': {'c': 1}}}\n    \"\"\"\n", "input": "", "output": "    options.update(ac_ordered=ac_ordered, ac_dict=ac_dict)\n    if anyconfig.utils.is_dict_like(obj):\n        return _make_recur(obj, convert_to, **options)\n    if anyconfig.utils.is_list_like(obj):\n        return _make_iter(obj, convert_to, **options)\n\n    return obj", "category": "Python"}, {"instruction": "def integer_fractional_parts(number):\r\n    \"\"\"\r\n    Returns a tuple of the integer and fractional parts of a number.\r\n\r\n    Args:\r\n        number(iterable container): A number in the following form:\r\n            (..., \".\", int, int, int, ...)\r\n\r\n    Returns:\r\n        (integer_part, fractional_part): tuple.\r\n\r\n    Example:\r\n        >>> integer_fractional_parts((1,2,3,\".\",4,5,6))\r\n        ((1, 2, 3), ('.', 4, 5, 6))\r\n    \"\"\"\n", "input": "", "output": "    radix_point = number.index(\".\")\r\n    integer_part = number[:radix_point]\r\n    fractional_part = number[radix_point:]\r\n    return(integer_part, fractional_part)", "category": "Python"}, {"instruction": "def update_views(self):\n        \"\"\"Update stats views.\"\"\"\n", "input": "", "output": "        # Call the father's method\n        super(Plugin, self).update_views()\n\n        # Add specifics informations\n        # Alert\n        for i in self.stats:\n            self.views[i[self.get_key()]]['used']['decoration'] = self.get_alert(\n                i['used'], maximum=i['size'], header=i['mnt_point'])", "category": "Python"}, {"instruction": "def create_verifier(self, request, credentials):\n        \"\"\"Create and save a new request token.\n\n        :param request: OAuthlib request.\n        :type request: oauthlib.common.Request\n        :param credentials: A dict of extra token credentials.\n        :returns: The verifier as a dict.\n        \"\"\"\n", "input": "", "output": "        verifier = {\n            'oauth_token': request.resource_owner_key,\n            'oauth_verifier': self.token_generator(),\n        }\n        verifier.update(credentials)\n        self.request_validator.save_verifier(\n            request.resource_owner_key, verifier, request)\n        return verifier", "category": "Python"}, {"instruction": "def _get_all_attributes(network):\n    \"\"\"\n        Get all the complex mode attributes in the network so that they\n        can be used for mapping to resource scenarios later.\n    \"\"\"\n", "input": "", "output": "    attrs = network.attributes\n    for n in network.nodes:\n        attrs.extend(n.attributes)\n    for l in network.links:\n        attrs.extend(l.attributes)\n    for g in network.resourcegroups:\n        attrs.extend(g.attributes)\n\n    return attrs", "category": "Python"}, {"instruction": "def _main_ctxmgr(func):\n    '''\n    A decorator wrapper for :class:`ServerMainContextManager`\n\n    Usage example:\n\n    .. code:: python\n\n       @aiotools.main\n       def mymain():\n           server_args = do_init()\n           stop_sig = yield server_args\n           if stop_sig == signal.SIGINT:\n               do_graceful_shutdown()\n           else:\n               do_forced_shutdown()\n\n       aiotools.start_server(..., main_ctxmgr=mymain, ...)\n    '''\n", "input": "", "output": "    @functools.wraps(func)\n    def helper(*args, **kwargs):\n        return ServerMainContextManager(func, args, kwargs)\n    return helper", "category": "Python"}, {"instruction": "def __meta_metadata(self, field, key):\n        \"\"\"Return the value for key for the field in the metadata\"\"\"\n", "input": "", "output": "        mf = ''\n        try:\n            mf = str([f[key] for f in self.metadata\n                     if f['field_name'] == field][0])\n        except IndexError:\n            print(\"%s not in metadata field:%s\" % (key, field))\n            return mf\n        else:\n            return mf", "category": "Python"}, {"instruction": "def del_token(token):\n    '''\n    Delete an eauth token by name\n\n    CLI Example:\n\n    .. code-block:: shell\n\n        salt-run auth.del_token 6556760736e4077daa601baec2b67c24\n    '''\n", "input": "", "output": "    token_path = os.path.join(__opts__['token_dir'], token)\n    if os.path.exists(token_path):\n        return os.remove(token_path) is None\n    return False", "category": "Python"}, {"instruction": "def _user_perm_cache(self):\n        \"\"\"\n        cached_permissions will generate the cache in a lazy fashion.\n        \"\"\"\n", "input": "", "output": "        # Check to see if the cache has been primed.\n        if not self.user:\n            return {}\n        cache_filled = getattr(\n            self.user,\n            '_authority_perm_cache_filled',\n            False,\n        )\n        if cache_filled:\n            # Don't really like the name for this, but this matches how Django\n            # does it.\n            return self.user._authority_perm_cache\n\n        # Prime the cache.\n        self._prime_user_perm_caches()\n        return self.user._authority_perm_cache", "category": "Python"}, {"instruction": "def get_configuration(head, update, head_source=None):\n    \"\"\"\n    This function return the right configuration for the inspire_merge\n    function in according to the given sources. Both parameters can not be None.\n\n    Params:\n        head(dict): the HEAD record\n        update(dict): the UPDATE record\n        head_source(string): the source of the HEAD record\n\n    Returns:\n        MergerConfigurationOperations: an object containing\n        the rules needed to merge HEAD and UPDATE\n    \"\"\"\n", "input": "", "output": "    head_source = (head_source or get_head_source(head))\n    update_source = get_acquisition_source(update)\n\n    if not is_arxiv_and_publisher(head_source, update_source) and is_manual_merge(head, update):\n        return ManualMergeOperations\n\n    if head_source == 'arxiv':\n        if update_source == 'arxiv':\n            return ArxivOnArxivOperations\n        else:\n            return PublisherOnArxivOperations\n    else:\n        if update_source == 'arxiv':\n            return ArxivOnPublisherOperations\n        else:\n            return PublisherOnPublisherOperations", "category": "Python"}, {"instruction": "def HasDataStream(self, name, case_sensitive=True):\n    \"\"\"Determines if the file entry has specific data stream.\n\n    Args:\n      name (str): name of the data stream.\n      case_sensitive (Optional[bool]): True if the name is case sensitive.\n\n    Returns:\n      bool: True if the file entry has the data stream.\n\n    Raises:\n      ValueError: if the name is not string.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(name, py2to3.STRING_TYPES):\n      raise ValueError('Name is not a string.')\n\n    name_lower = name.lower()\n\n    for data_stream in self._GetDataStreams():\n      if data_stream.name == name:\n        return True\n\n      if not case_sensitive and data_stream.name.lower() == name_lower:\n        return True\n\n    return False", "category": "Python"}, {"instruction": "def flatten(items):\n    \"\"\"\n    Yield items from any nested iterable.\n\n    >>> list(flatten([[1, 2, 3], [[4, 5], 6, 7]]))\n    [1, 2, 3, 4, 5, 6, 7]\n    \"\"\"\n", "input": "", "output": "    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n            for sub_x in flatten(x):\n                yield sub_x\n        else:\n            yield x", "category": "Python"}, {"instruction": "def get_or_create(cls, dname):\n        \"\"\"\n        Get the requested domain, or create it if it doesn't exist already\n        @param  dname:  Domain name\n        @type   dname:  str\n        @rtype: Domain\n        \"\"\"\n", "input": "", "output": "        Domain = cls\n        dname = dname.hostname if hasattr(dname, 'hostname') else dname\n        extras = 'www.{dn}'.format(dn=dname) if dname not in ('localhost', ) and not \\\n            re.match('^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$', dname) else None\n        # Fetch the domain entry if it already exists\n        logging.getLogger('ipsv.sites.domain').debug('Checking if the domain %s has already been registered', dname)\n        domain = Session.query(Domain).filter(Domain.name == dname).first()\n\n        # Otherwise create it now\n        if not domain:\n            logging.getLogger('ipsv.sites.domain')\\\n                .debug('Domain name does not yet exist, creating a new database entry')\n            domain = Domain(name=dname, extras=extras)\n            Session.add(domain)\n            Session.commit()\n\n        return domain", "category": "Python"}, {"instruction": "def from_edges(edges):\r\n        \"\"\" Return DirectedGraph created from edges\r\n        :param edges:\r\n        :return: DirectedGraph\r\n        \"\"\"\n", "input": "", "output": "        dag = DirectedGraph()\r\n        for _u, _v in edges:\r\n            dag.add_edge(_u, _v)\r\n        return dag", "category": "Python"}, {"instruction": "def commit(self):\n\t\t\"\"\"\n\t\tCommit change made by the add_* methods.\n\t\tAll previous values with no update will be lost.\n\t\tThis method is automatically called by the updater thread.\n\t\t\"\"\"\n", "input": "", "output": "\n\t\t# Generate index before acquiring lock to keep locked section fast\n\t\t# Works because this thread is the only writer of self.pending\n\t\tpending_idx = sorted(list(self.pending.keys()), key=lambda k: tuple(int(part) for part in k.split('.')))\n\n\t\t# Commit new data\n\t\ttry:\n\t\t\tself.lock.acquire()\n\t\t\tself.data=self.pending\n\t\t\tself.pending=dict()\n\t\t\tself.data_idx = pending_idx\n\t\tfinally:\n\t\t\tself.lock.release()", "category": "Python"}, {"instruction": "def keep_(self, *cols) -> \"Ds\":\n        \"\"\"\n        Returns a dataswim instance with a dataframe limited\n        to some columns\n\n        :param cols: names of the columns\n        :type cols: str\n        :return: a dataswim instance\n        :rtype: Ds\n\n        :example: ``ds2 = ds.keep_(\"Col 1\", \"Col 2\")``\n        \"\"\"\n", "input": "", "output": "        try:\n            ds2 = self._duplicate_(self.df[list(cols)])\n        except Exception as e:\n            self.err(e, \"Can not remove colums\")\n            return\n        self.ok(\"Columns\", \" ,\".join(cols), \"kept\")\n        return ds2", "category": "Python"}, {"instruction": "def _unicode_sub_super(string, mapping, max_len=None):\n    \"\"\"Try to render a subscript or superscript string in unicode, fall back on\n    ascii if this is not possible\"\"\"\n", "input": "", "output": "    string = str(string)\n    if string.startswith('(') and string.endswith(')'):\n        len_string = len(string) - 2\n    else:\n        len_string = len(string)\n    if max_len is not None:\n        if len_string > max_len:\n            raise KeyError(\"max_len exceeded\")\n    unicode_letters = []\n    for letter in string:\n        unicode_letters.append(mapping[letter])\n    return ''.join(unicode_letters)", "category": "Python"}, {"instruction": "def write_configuration(self, out, secret_attrs=False):\n        \"\"\"Type-specific configuration for backward compatibility\"\"\"\n", "input": "", "output": "        key_order = ['repo_nexml2json',\n                     'number_of_shards',\n                     'initialization', ]\n        cd = self.get_configuration_dict(secret_attrs=secret_attrs)\n        for k in key_order:\n            if k in cd:\n                out.write('  {} = {}'.format(k, cd[k]))\n        for n, shard in enumerate(self._shards):\n            out.write('Shard {}:\\n'.format(n))\n            shard.write_configuration(out)", "category": "Python"}, {"instruction": "def setup_parser():\n    '''Set up the command-line options.'''\n", "input": "", "output": "    parser = argparse.ArgumentParser(description='Add random pages to existing site.')\n    parser.add_argument('-d','--directory', action='store', default= os.getcwd(),\n                                 help='Site directory (must be a valid s2 structure).')\n    parser.add_argument('-n','--number', action='store', type=int, default = 20,\n                                 help='Number of pages to generate.')\n\n    return parser", "category": "Python"}, {"instruction": "def insert_inexistence(self, table, kwargs, condition):\n        \"\"\".. :py:method::\n\n        Usage::\n\n            >>> insert('hospital', {'id': '12de3wrv', 'province': 'shanghai'}, {'id': '12de3wrv'})\n            insert into hospital (id, province) select '12de3wrv', 'shanghai' where not exists (select 1 from hospital where id='12de3wrv' limit 1);\n\n        \"\"\"\n", "input": "", "output": "        sql = \"insert into \" + table + \" ({}) \"\n        select = \"select {} \"\n        condition = \"where not exists (select 1 from \" + table + \"{} limit 1);\".format( self.parse_condition(condition) )\n        keys, values = [], []\n        [ (keys.append(k), values.append(v)) for k, v in kwargs.iteritems() ]\n        sql = sql.format(', '.join(keys)) + select.format( ', '.join(['%s']*len(values)) ) + condition\n        super(PGWrapper, self).execute(sql, values, result=False)", "category": "Python"}, {"instruction": "def join(self, room_str):\n        \"\"\"Joins room id or alias even if it must first be created.\"\"\"\n", "input": "", "output": "        response = self.user_api.join_room(room_str)\n        return self._mkroom(response[\"room_id\"])", "category": "Python"}, {"instruction": "def function(self, x, y, sigma0, Rs, e1, e2, center_x=0, center_y=0):\n        \"\"\"\n        returns double integral of NFW profile\n        \"\"\"\n", "input": "", "output": "        phi_G, q = param_util.ellipticity2phi_q(e1, e2)\n        x_shift = x - center_x\n        y_shift = y - center_y\n        cos_phi = np.cos(phi_G)\n        sin_phi = np.sin(phi_G)\n        e = abs(1 - q)\n        x_ = (cos_phi*x_shift+sin_phi*y_shift)*np.sqrt(1 - e)\n        y_ = (-sin_phi*x_shift+cos_phi*y_shift)*np.sqrt(1 + e)\n        f_ = self.spherical.function(x_, y_, sigma0, Rs)\n        return f_", "category": "Python"}, {"instruction": "def get_arg_type_descriptors(self):\n        \"\"\"\n        The parameter type descriptor list for a method, or None for a\n        field.  Type descriptors are shorthand identifiers for the\n        builtin java types.\n        \"\"\"\n", "input": "", "output": "\n        if not self.is_method:\n            return tuple()\n\n        tp = _typeseq(self.get_descriptor())\n        tp = _typeseq(tp[0][1:-1])\n\n        return tp", "category": "Python"}, {"instruction": "def persist(self, path_to_file):\n        \"\"\"\n        Saves the image to disk on a file\n\n        :param path_to_file: path to the target file\n        :type path_to_file: str\n        :return: `None`\n        \"\"\"\n", "input": "", "output": "        with open(path_to_file, 'wb') as f:\n            f.write(self.data)", "category": "Python"}, {"instruction": "def Size(self):\n        \"\"\"\n        Get the total size in bytes of the object.\n\n        Returns:\n            int: size.\n        \"\"\"\n", "input": "", "output": "        corrected_hashes = list(map(lambda i: UInt256(data=binascii.unhexlify(i)), self.HashStart))\n        return GetVarSize(corrected_hashes) + self.hash_stop.Size", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self,\n                   'normalized_text') and self.normalized_text is not None:\n            _dict['normalized_text'] = self.normalized_text\n        if hasattr(self, 'start_time') and self.start_time is not None:\n            _dict['start_time'] = self.start_time\n        if hasattr(self, 'end_time') and self.end_time is not None:\n            _dict['end_time'] = self.end_time\n        if hasattr(self, 'confidence') and self.confidence is not None:\n            _dict['confidence'] = self.confidence\n        return _dict", "category": "Python"}, {"instruction": "def dict_partial_cmp(target_dict, dict_list, ducktype):\n    \"\"\"\n    Whether partial dict are in dict_list or not\n    \"\"\"\n", "input": "", "output": "    for called_dict in dict_list:\n        # ignore invalid test case\n        if len(target_dict) > len(called_dict):\n            continue\n        # get the intersection of two dicts\n        intersection = {}\n        for item in target_dict:\n            dtype = ducktype(target_dict[item])\n            if hasattr(dtype, \"mtest\"):\n                if item in called_dict and dtype.mtest(called_dict[item]):\n                    intersection[item] = target_dict[item]\n            else:\n                if item in called_dict and dtype == called_dict[item]:\n                    intersection[item] = target_dict[item]\n        if intersection == target_dict:\n            return True\n    # if no any arguments matched to called_args, return False\n    return False", "category": "Python"}, {"instruction": "def p_cmp_expression(tok):\n        \"\"\"cmp_expression : term OP_LIKE cmp_expression\n                          | term OP_IN cmp_expression\n                          | term OP_IS cmp_expression\n                          | term OP_EQ cmp_expression\n                          | term OP_NE cmp_expression\n                          | term OP_GT cmp_expression\n                          | term OP_GE cmp_expression\n                          | term OP_LT cmp_expression\n                          | term OP_LE cmp_expression\n                          | term\"\"\"\n", "input": "", "output": "        if len(tok) == 4:\n            tok[0] = ComparisonBinOpRule(tok[2], tok[1], tok[3])\n        else:\n            tok[0] = tok[1]", "category": "Python"}, {"instruction": "def get_groupnames(self, hostgroups):\n        \"\"\"Get names of the host's hostgroups\n\n        :return: comma separated names of hostgroups alphabetically sorted\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        group_names = []\n        for hostgroup_id in self.hostgroups:\n            hostgroup = hostgroups[hostgroup_id]\n            group_names.append(hostgroup.get_name())\n        return ','.join(sorted(group_names))", "category": "Python"}, {"instruction": "def hybrid_forward(self, F, x):\n        \"\"\"Perform pixel-shuffling on the input.\"\"\"\n", "input": "", "output": "        f1, f2 = self._factors\n                                                      # (N, f1*f2*C, H, W)\n        x = F.reshape(x, (0, -4, -1, f1 * f2, 0, 0))  # (N, C, f1*f2, H, W)\n        x = F.reshape(x, (0, 0, -4, f1, f2, 0, 0))    # (N, C, f1, f2, H, W)\n        x = F.transpose(x, (0, 1, 4, 2, 5, 3))        # (N, C, H, f1, W, f2)\n        x = F.reshape(x, (0, 0, -3, -3))              # (N, C, H*f1, W*f2)\n        return x", "category": "Python"}, {"instruction": "def _delete_guest(self, guest_id):\n        \"\"\"Deletes a guest and returns 'Cancelled' or and Exception message\"\"\"\n", "input": "", "output": "        msg = 'Cancelled'\n        try:\n            self.guest.deleteObject(id=guest_id)\n        except SoftLayer.SoftLayerAPIError as e:\n            msg = 'Exception: ' + e.faultString\n\n        return msg", "category": "Python"}, {"instruction": "def _get_calibration_for_mchits(hits, lookup):\n    \"\"\"Append the position, direction and t0 columns and add t0 to time\"\"\"\n", "input": "", "output": "    n_hits = len(hits)\n    cal = np.empty((n_hits, 9))\n    for i in range(n_hits):\n        cal[i] = lookup[hits['pmt_id'][i]]\n    dir_x = cal[:, 3]\n    dir_y = cal[:, 4]\n    dir_z = cal[:, 5]\n    du = cal[:, 7]\n    floor = cal[:, 8]\n    pos_x = cal[:, 0]\n    pos_y = cal[:, 1]\n    pos_z = cal[:, 2]\n    t0 = cal[:, 6]\n\n    return [dir_x, dir_y, dir_z, du, floor, pos_x, pos_y, pos_z, t0]", "category": "Python"}, {"instruction": "def update_refresh_state(self, id_or_uri, refresh_state_data):\n        \"\"\"\n        Refreshes a given intelligent power delivery device.\n\n        Args:\n            id_or_uri:\n                Can be either the power device id or the uri\n            refresh_state_data:\n                Power device refresh request\n\n        Returns:\n            str: The power state\n        \"\"\"\n", "input": "", "output": "        uri = self._client.build_uri(id_or_uri) + \"/refreshState\"\n        return self._client.update(refresh_state_data, uri=uri)", "category": "Python"}, {"instruction": "def get_video_transcript_data(video_id, language_code):\n    \"\"\"\n    Get video transcript data\n\n    Arguments:\n        video_id(unicode): An id identifying the Video.\n        language_code(unicode): it will be the language code of the requested transcript.\n\n    Returns:\n        A dict containing transcript file name and its content.\n    \"\"\"\n", "input": "", "output": "    video_transcript = VideoTranscript.get_or_none(video_id, language_code)\n    if video_transcript:\n        try:\n            return dict(file_name=video_transcript.filename, content=video_transcript.transcript.file.read())\n        except Exception:\n            logger.exception(\n                '[edx-val] Error while retrieving transcript for video=%s -- language_code=%s',\n                video_id,\n                language_code\n            )\n            raise", "category": "Python"}, {"instruction": "def update_user_auth_stat(self, user, success=True):\n        \"\"\"\n            Update authentication successful to user.\n\n            :param user:\n                The authenticated user model\n            :param success:\n                Default to true, if false increments fail_login_count on user model\n        \"\"\"\n", "input": "", "output": "        if not user.login_count:\n            user.login_count = 0\n        if not user.fail_login_count:\n            user.fail_login_count = 0\n        if success:\n            user.login_count += 1\n            user.fail_login_count = 0\n        else:\n            user.fail_login_count += 1\n        user.last_login = datetime.datetime.now()\n        self.update_user(user)", "category": "Python"}, {"instruction": "def set_function_name(f, name, cls):\n    \"\"\"\n    Bind the name/qualname attributes of the function\n    \"\"\"\n", "input": "", "output": "    f.__name__ = name\n    f.__qualname__ = '{klass}.{name}'.format(\n        klass=cls.__name__,\n        name=name)\n    f.__module__ = cls.__module__\n    return f", "category": "Python"}, {"instruction": "def set_json(self, obj, status=HttpStatusCodes.HTTP_200):\n        \"\"\"Helper method to set a JSON response.\n\n        Args:\n            obj (:obj:`object`): JSON serializable object\n            status (:obj:`str`, optional): Status code of the response\n        \"\"\"\n", "input": "", "output": "        obj = json.dumps(obj, sort_keys=True, default=lambda x: str(x))\n        self.set_status(status)\n        self.set_header(HttpResponseHeaders.CONTENT_TYPE, 'application/json')\n        self.set_content(obj)", "category": "Python"}, {"instruction": "def add_rotating_file_handler(logger=None, file_path=\"out.log\",\n                              level=logging.INFO,\n                              log_format=log_formats.easy_read,\n                              max_bytes=10*sizes.mb, backup_count=5,\n                              **handler_kwargs):\n    \"\"\" Adds a rotating file handler to the specified logger.\n\n    :param logger: logging name or object to modify, defaults to root logger\n    :param file_path: path to file to log to\n    :param level: logging level to set handler at\n    :param log_format: log formatter\n    :param max_bytes: Max file size in bytes before rotating\n    :param backup_count: Number of backup files\n    :param handler_kwargs: options to pass to the handler\n    \"\"\"\n", "input": "", "output": "    if not isinstance(logger, logging.Logger):\n        logger = logging.getLogger(logger)\n\n    logger.addHandler(get_file_handler(file_path, level, log_format,\n                                       handler=RotatingFileHandler,\n                                       maxBytes=max_bytes,\n                                       backupCount=backup_count,\n                                       **handler_kwargs))", "category": "Python"}, {"instruction": "def get_unf(N=100):\n    \"\"\"\n    Generates N uniformly distributed directions\n    using the way described in Fisher et al. (1987).\n    Parameters\n    __________\n    N : number of directions, default is 100\n\n    Returns\n    ______\n    array of nested dec,inc pairs\n    \"\"\"\n", "input": "", "output": "#\n# get uniform directions  [dec,inc]\n    z = random.uniform(-1., 1., size=N)\n    t = random.uniform(0., 360., size=N)  # decs\n    i = np.arcsin(z) * 180. / np.pi  # incs\n    return np.array([t, i]).transpose()\n\n# def get_unf(N): #Jeff's way\n    ", "category": "Python"}, {"instruction": "def num_columns(self):\n        \"\"\"Number of columns displayed.\"\"\"\n", "input": "", "output": "        if self.term.is_a_tty:\n            return self.term.width // self.hint_width\n        return 1", "category": "Python"}, {"instruction": "def get_service(self, bundle, reference):\n        # type: (Any, ServiceReference) -> Any\n        \"\"\"\n        Retrieves the service corresponding to the given reference\n\n        :param bundle: The bundle requiring the service\n        :param reference: A service reference\n        :return: The requested service\n        :raise BundleException: The service could not be found\n        \"\"\"\n", "input": "", "output": "        with self.__svc_lock:\n            if reference.is_factory():\n                return self.__get_service_from_factory(bundle, reference)\n\n            # Be sure to have the instance\n            try:\n                service = self.__svc_registry[reference]\n\n                # Indicate the dependency\n                imports = self.__bundle_imports.setdefault(bundle, {})\n                imports.setdefault(reference, _UsageCounter()).inc()\n                reference.used_by(bundle)\n                return service\n            except KeyError:\n                # Not found\n                raise BundleException(\n                    \"Service not found (reference: {0})\".format(reference)\n                )", "category": "Python"}, {"instruction": "def main():\n    \"\"\"Invoked by the script installed by setuptools.\"\"\"\n", "input": "", "output": "    parser.name('tinman')\n    parser.description(__desc__)\n\n    p = parser.get()\n    p.add_argument('-p', '--path',\n                   action='store',\n                   dest='path',\n                   help='Path to prepend to the Python system path')\n\n    helper.start(Controller)", "category": "Python"}, {"instruction": "def permission_delete(self, token, id):\n        \"\"\"\n        Removing a Permission.\n\n        https://www.keycloak.org/docs/latest/authorization_services/index.html#removing-a-permission\n\n        :param str token: client access token\n        :param str id: permission id\n\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        return self._realm.client.delete(\n            '{}/{}'.format(self.well_known['policy_endpoint'], id),\n            headers=self.get_headers(token)\n        )", "category": "Python"}, {"instruction": "def _recv_flow_ok(self, method_frame):\n        '''\n        Receive a flow control ack from the broker.\n        '''\n", "input": "", "output": "        self.channel._active = method_frame.args.read_bit()\n        if self._flow_control_cb is not None:\n            self._flow_control_cb()", "category": "Python"}, {"instruction": "def get_python_shell():\n    \"\"\"Determine python shell\n\n    get_python_shell() returns\n\n    'shell' (started python on command line using \"python\")\n    'ipython' (started ipython on command line using \"ipython\")\n    'ipython-notebook' (e.g., running in Spyder or started with \"ipython qtconsole\")\n    'jupyter-notebook' (running in a Jupyter notebook)\n\n    See also https://stackoverflow.com/a/37661854\n    \"\"\"\n", "input": "", "output": "\n    env = os.environ\n    shell = \"shell\"\n    program = os.path.basename(env[\"_\"])\n\n    if \"jupyter-notebook\" in program:\n        shell = \"jupyter-notebook\"\n    elif \"JPY_PARENT_PID\" in env or \"ipython\" in program:\n        shell = \"ipython\"\n        if \"JPY_PARENT_PID\" in env:\n            shell = \"ipython-notebook\"\n\n    return shell", "category": "Python"}, {"instruction": "def mid_lvl_cmds_send(self, target, hCommand, uCommand, rCommand, force_mavlink1=False):\n                '''\n                Mid Level commands sent from the GS to the autopilot. These are only\n                sent when being operated in mid-level commands mode\n                from the ground.\n\n                target                    : The system setting the commands (uint8_t)\n                hCommand                  : Commanded Altitude in meters (float)\n                uCommand                  : Commanded Airspeed in m/s (float)\n                rCommand                  : Commanded Turnrate in rad/s (float)\n\n                '''\n", "input": "", "output": "                return self.send(self.mid_lvl_cmds_encode(target, hCommand, uCommand, rCommand), force_mavlink1=force_mavlink1)", "category": "Python"}, {"instruction": "def analyze(self, output_file=None):\n        \"\"\"\n        Analyzes the parsed results from Flake8 or PEP 8 output and creates FileResult instances\n        :param output_file: If specified, output will be written to this file instead of stdout.\n        \"\"\"\n", "input": "", "output": "\n        fr = None\n        for path, code, line, char, desc in self.parser.parse():\n\n            # Create a new FileResult and register it if we have changed to a new file\n            if path not in self.files:\n                # Update statistics\n                if fr:\n                    self.update_stats(fr)\n                fr = FileResult(path)\n                self.files[path] = fr\n\n            # Add line to the FileResult\n            fr.add_error(code, line, char, desc)\n\n        # Add final FileResult to statistics, if any were parsed\n        if fr:\n            self.update_stats(fr)\n\n        # Generate HTML file\n        self.generate(output_file=output_file)", "category": "Python"}, {"instruction": "def splitdrive(self):\n        \"\"\" p.splitdrive() -> Return ``(p.drive, <the rest of p>)``.\n\n        Split the drive specifier from this path.  If there is\n        no drive specifier, :samp:`{p.drive}` is empty, so the return value\n        is simply ``(Path(''), p)``.  This is always the case on Unix.\n\n        .. seealso:: :func:`os.path.splitdrive`\n        \"\"\"\n", "input": "", "output": "        drive, rel = self.module.splitdrive(self)\n        return self._next_class(drive), rel", "category": "Python"}, {"instruction": "def _select1(data, field, depth, output):\n    \"\"\"\n    SELECT A SINGLE FIELD\n    \"\"\"\n", "input": "", "output": "    for d in data:\n        for i, f in enumerate(field[depth:]):\n            d = d[f]\n            if d == None:\n                output.append(None)\n                break\n            elif is_list(d):\n                _select1(d, field, i + 1, output)\n                break\n        else:\n            output.append(d)", "category": "Python"}, {"instruction": "def hash(ctx, message_id, message):\n    \"\"\" Take a single mail message and show its canonicalised form and hash.\n\n    Mainly used to debug message hashing.\n    \"\"\"\n", "input": "", "output": "    conf = Config(message_id=message_id)\n\n    mail = Mail(message, conf)\n\n    logger.info(mail.header_text)\n    logger.info('-' * 70)\n    logger.info('Hash: {}'.format(mail.hash_key))", "category": "Python"}, {"instruction": "def flatsize(obj, align=0, **opts):\n    '''Return the flat size of an object (in bytes),\n       optionally aligned to a given power of 2.\n\n       See function **basicsize** for a description of\n       the other options.  See the documentation of\n       this module for the definition of flat size.\n    '''\n", "input": "", "output": "    v = _typedefof(obj, **opts)\n    if v:\n        if align > 1:\n            m = align - 1\n            if (align & m) != 0:\n                raise ValueError('invalid option: %s=%r' % ('align', align))\n        else:\n            m = 0\n        v = v.flat(obj, m)\n    return v", "category": "Python"}, {"instruction": "def check(self, line):\n        \"\"\"\n        Find first occurrence of 'line' in file.\n\n        This searches each line as a whole, if you want to see if a substring is in a line, use .grep() or .egrep()\n\n        If found, return the line; this makes it easier to chain methods.\n\n        :param line: String; whole line to find.\n        :return: String or False.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(line, str):\n            raise TypeError(\"Parameter 'line' not a 'string', is {0}\".format(type(line)))\n        if line in self.contents:\n            return line\n        return False", "category": "Python"}]