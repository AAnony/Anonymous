[{"instruction": "def dump_credibilities(self, output):\n        \"\"\"Dump credibilities of all products.\n\n        Args:\n          output: a writable object.\n        \"\"\"\n", "input": "", "output": "        for p in self.products:\n            json.dump({\n                \"product_id\": p.name,\n                \"credibility\": self.credibility(p)\n            }, output)\n            output.write(\"\\n\")", "category": "Python"}, {"instruction": "def description(self):\n        \"\"\"\n        This read-only attribute is a sequence of 7-item sequences.\n        \"\"\"\n", "input": "", "output": "        if self._closed:\n            return\n\n        description = []\n        for col in self._result[\"cols\"]:\n            description.append((col,\n                                None,\n                                None,\n                                None,\n                                None,\n                                None,\n                                None))\n        return tuple(description)", "category": "Python"}, {"instruction": "def job_set_properties(object_id, input_params={}, always_retry=True, **kwargs):\n    \"\"\"\n    Invokes the /job-xxxx/setProperties API method.\n\n    For more info, see: https://wiki.dnanexus.com/API-Specification-v1.0.0/Applets-and-Entry-Points#API-method%3A-%2Fjob-xxxx%2FsetProperties\n    \"\"\"\n", "input": "", "output": "    return DXHTTPRequest('/%s/setProperties' % object_id, input_params, always_retry=always_retry, **kwargs)", "category": "Python"}, {"instruction": "def add(self, name: str, sig: Tuple, obj: object) -> None:\n        \"\"\"\n        Add a file to the cache\n        :param name: name of the object to be pickled\n        :param sig: signature for object\n        :param obj: object to pickle\n        \"\"\"\n", "input": "", "output": "        if self._cache_directory is not None:\n            if name in self._cache:\n                os.remove(os.path.join(self._cache_directory, self._cache[name].loc))\n            fname = os.path.join(self._cache_directory, str(uuid.uuid4()))\n            with open(fname, 'wb') as f:\n                pickle.dump(obj, f)\n            self._cache[name] = _PickleJar.CacheEntry(sig, fname)\n            self._update()", "category": "Python"}, {"instruction": "def _populate_inception_bottlenecks(scope):\n  \"\"\"Add Inception bottlenecks and their pre-Relu versions to the graph.\"\"\"\n", "input": "", "output": "  graph = tf.get_default_graph()\n  for op in graph.get_operations():\n    if op.name.startswith(scope+'/') and 'Concat' in op.type:\n      name = op.name.split('/')[1]\n      pre_relus = []\n      for tower in op.inputs[1:]:\n        if tower.op.type == 'Relu':\n          tower = tower.op.inputs[0]\n        pre_relus.append(tower)\n      concat_name = scope + '/' + name + '_pre_relu'\n      _ = tf.concat(pre_relus, -1, name=concat_name)", "category": "Python"}, {"instruction": "def mount(location, access='rw', root=None):\n    '''\n    Mount an image\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' guest.mount /srv/images/fedora.qcow\n    '''\n", "input": "", "output": "    if root is None:\n        root = os.path.join(\n            tempfile.gettempdir(),\n            'guest',\n            location.lstrip(os.sep).replace('/', '.')\n        )\n        log.debug('Using root %s', root)\n    if not os.path.isdir(root):\n        try:\n            os.makedirs(root)\n        except OSError:\n            # Somehow the path already exists\n            pass\n    while True:\n        if os.listdir(root):\n            # Stuff is in there, don't use it\n            hash_type = getattr(hashlib, __opts__.get('hash_type', 'md5'))\n            rand = hash_type(os.urandom(32)).hexdigest()\n            root = os.path.join(\n                tempfile.gettempdir(),\n                'guest',\n                location.lstrip(os.sep).replace('/', '.') + rand\n            )\n            log.debug('Establishing new root as %s', root)\n        else:\n            break\n    cmd = 'guestmount -i -a {0} --{1} {2}'.format(location, access, root)\n    __salt__['cmd.run'](cmd, python_shell=False)\n    return root", "category": "Python"}, {"instruction": "def get_points(self):\n        \"\"\"Get the set of points that is used to draw the object.\n\n        Points are returned in *data* coordinates.\n        \"\"\"\n", "input": "", "output": "        if hasattr(self, 'points'):\n            points = self.crdmap.to_data(self.points)\n        else:\n            points = []\n        return points", "category": "Python"}, {"instruction": "def probe(self, params):\n        \"\"\"\n        Evaulates a single point x, to obtain the value y and then records them\n        as observations.\n\n        Notes\n        -----\n        If x has been previously seen returns a cached value of y.\n\n        Parameters\n        ----------\n        x : ndarray\n            a single point, with len(x) == self.dim\n\n        Returns\n        -------\n        y : float\n            target function value.\n        \"\"\"\n", "input": "", "output": "        x = self._as_array(params)\n\n        try:\n            target = self._cache[_hashable(x)]\n        except KeyError:\n            params = dict(zip(self._keys, x))\n            target = self.target_func(**params)\n            self.register(x, target)\n        return target", "category": "Python"}, {"instruction": "def info(self, request):\n        \"\"\"Return height of the latest committed block.\"\"\"\n", "input": "", "output": "\n        self.abort_if_abci_chain_is_not_synced()\n\n        # Check if BigchainDB supports the Tendermint version\n        if not (hasattr(request, 'version') and tendermint_version_is_compatible(request.version)):\n            logger.error(f'Unsupported Tendermint version: {getattr(request, \"version\", \"no version\")}.'\n                         f' Currently, BigchainDB only supports {__tm_supported_versions__}. Exiting!')\n            sys.exit(1)\n\n        logger.info(f\"Tendermint version: {request.version}\")\n\n        r = ResponseInfo()\n        block = self.bigchaindb.get_latest_block()\n        if block:\n            chain_shift = 0 if self.chain is None else self.chain['height']\n            r.last_block_height = block['height'] - chain_shift\n            r.last_block_app_hash = block['app_hash'].encode('utf-8')\n        else:\n            r.last_block_height = 0\n            r.last_block_app_hash = b''\n        return r", "category": "Python"}, {"instruction": "def statuscategories(self):\n        \"\"\"Get a list of status category Resources from the server.\n\n        :rtype: List[StatusCategory]\n        \"\"\"\n", "input": "", "output": "        r_json = self._get_json('statuscategory')\n        statuscategories = [StatusCategory(self._options, self._session, raw_stat_json)\n                            for raw_stat_json in r_json]\n        return statuscategories", "category": "Python"}, {"instruction": "def route(app_or_blueprint, rule, **options):\n    \"\"\"An alternative to :meth:`flask.Flask.route` or :meth:`flask.Blueprint.route` that\n    always adds the ``POST`` method to the allowed endpoint request methods.\n\n    You should use this for all your view functions that would need to use Sijax.\n\n    We're doing this because Sijax uses ``POST`` for data passing,\n    which means that every endpoint that wants Sijax support\n    would have to accept ``POST`` requests.\n\n    Registering functions that would use Sijax should happen like this::\n\n        @flask_sijax.route(app, '/')\n        def index():\n            pass\n\n    If you remember to make your view functions accessible via POST\n    like this, you can avoid using this decorator::\n\n        @app.route('/', methods=['GET', 'POST'])\n        def index():\n            pass\n    \"\"\"\n", "input": "", "output": "    def decorator(f):\n        methods = options.pop('methods', ('GET', 'POST'))\n        if 'POST' not in methods:\n            methods = tuple(methods) + ('POST',)\n        options['methods'] = methods\n        app_or_blueprint.add_url_rule(rule, None, f, **options)\n        return f\n    return decorator", "category": "Python"}, {"instruction": "def write(self, data):\n        \"\"\"\n        Write ``data`` to the file.\n\n        :type data: bytes\n        :param data: the data to be written to the file\n        :rtype: int\n        :return: the number of bytes written\n        \"\"\"\n", "input": "", "output": "        _complain_ifclosed(self.closed)\n        if self.__encoding:\n            self.f.write(data.encode(self.__encoding, self.__errors))\n            return len(data)\n        else:\n            return self.f.write(data)", "category": "Python"}, {"instruction": "def standard_settings(self):\n        \"\"\"Sets up standard settings for a nice visualization.\"\"\"\n", "input": "", "output": "        cmd.set('bg_rgb', [1.0, 1.0, 1.0])  # White background\n        cmd.set('depth_cue', 0)  # Turn off depth cueing (no fog)\n        cmd.set('cartoon_side_chain_helper', 1)  # Improve combined visualization of sticks and cartoon\n        cmd.set('cartoon_fancy_helices', 1)  # Nicer visualization of helices (using tapered ends)\n        cmd.set('transparency_mode', 1)  # Turn on multilayer transparency\n        cmd.set('dash_radius', 0.05)\n        self.set_custom_colorset()", "category": "Python"}, {"instruction": "def init_config(self, app):\n        \"\"\"Initialize configuration.\n\n        .. note:: Change Flask-CORS and Flask-Limiter defaults.\n\n        :param app: An instance of :class:`flask.Flask`.\n        \"\"\"\n", "input": "", "output": "        config_apps = ['REST_', 'CORS_', ]\n        for k in dir(config):\n            if any([k.startswith(prefix) for prefix in config_apps]):\n                app.config.setdefault(k, getattr(config, k))", "category": "Python"}, {"instruction": "def _reduce(self, op, name, axis=0, skipna=True, numeric_only=None,\n                filter_type=None, **kwds):\n        \"\"\" perform the reduction type operation if we can \"\"\"\n", "input": "", "output": "        func = getattr(self, name, None)\n        if func is None:\n            raise TypeError(\"{klass} cannot perform the operation {op}\".format(\n                            klass=self.__class__.__name__, op=name))\n        return func(skipna=skipna, **kwds)", "category": "Python"}, {"instruction": "def get_scores(self, beatmap_id, *, username=None, mode=OsuMode.osu, mods=None, limit=50):\n        \"\"\"Get the top scores for a given beatmap.\n\n        Parameters\n        ----------\n        beatmap_id\n            Individual Beatmap ID to lookup.\n        username : str or int\n            A `str` representing the user's username, or an `int` representing the user's id. If specified, restricts returned scores to the specified user.\n        mode : :class:`osuapi.enums.OsuMode`\n            The osu! game mode for which to look up. Defaults to osu!standard.\n        mods : :class:`osuap:class:`osuapi.enums.OsuMod`\n            If specified, restricts returned scores to the specified mods.\n        limit\n            Number of results to return. Defaults to 50, maximum 100.\n        \"\"\"\n", "input": "", "output": "        return self._make_req(endpoints.SCORES, dict(\n            k=self.key,\n            b=beatmap_id,\n            u=username,\n            type=_username_type(username),\n            m=mode.value,\n            mods=mods.value if mods else None,\n            limit=limit), JsonList(BeatmapScore))", "category": "Python"}, {"instruction": "def get_queryset(self):\n        \"\"\"\n        Return the published Tags with option counts.\n        \"\"\"\n", "input": "", "output": "        self.entries_qs = Entry.published.all()\n        return Tag.objects.usage_for_queryset(\n            self.entries_qs, counts=True)", "category": "Python"}, {"instruction": "def count_overlap(self, time, other_object, other_time):\n        \"\"\"\n        Counts the number of points that overlap between this STObject and another STObject. Used for tracking.\n        \"\"\"\n", "input": "", "output": "        ti = np.where(time == self.times)[0][0]\n        ma = np.where(self.masks[ti].ravel() == 1)\n        oti = np.where(other_time == other_object.times)[0]\n        obj_coords = np.zeros(self.masks[ti].sum(), dtype=[('x', int), ('y', int)])\n        other_obj_coords = np.zeros(other_object.masks[oti].sum(), dtype=[('x', int), ('y', int)])\n        obj_coords['x'] = self.i[ti].ravel()[ma]\n        obj_coords['y'] = self.j[ti].ravel()[ma]\n        other_obj_coords['x'] = other_object.i[oti][other_object.masks[oti] == 1]\n        other_obj_coords['y'] = other_object.j[oti][other_object.masks[oti] == 1]\n        return float(np.intersect1d(obj_coords,\n                                    other_obj_coords).size) / np.maximum(self.masks[ti].sum(),\n                                                                         other_object.masks[oti].sum())", "category": "Python"}, {"instruction": "def subjects(self):\n        \"\"\" Return identifiers for all the subjects that are in the cache.\n\n        :return: list of subject identifiers\n        \"\"\"\n", "input": "", "output": "\n        subj = [i[\"subject_id\"] for i in self._cache.find()]\n\n        return list(set(subj))", "category": "Python"}, {"instruction": "def interfaces(self):\n        '''The list of interfaces this port provides or uses.\n\n        This list will be created at the first reference to this property.\n        This means that the first reference may be delayed by CORBA calls,\n        but others will return quickly (unless a delayed reparse has been\n        triggered).\n\n        '''\n", "input": "", "output": "        with self._mutex:\n            if not self._interfaces:\n                profile = self._obj.get_port_profile()\n                self._interfaces = [SvcInterface(intf) \\\n                                    for intf in profile.interfaces]\n        return self._interfaces", "category": "Python"}, {"instruction": "def fill(duration, point):\n    \"\"\"\n    fills the subsequence of the point with repetitions of its subsequence and\n    sets the ``duration`` of each point.\n    \"\"\"\n", "input": "", "output": "    point['sequence'] = point['sequence'] * (point[DURATION_64] / (8 * duration)) | add({DURATION_64: duration})\n    return point", "category": "Python"}, {"instruction": "def listFieldsFromWorkitem(self, copied_from, keep=False):\n        \"\"\"List all the attributes to be rendered directly from some\n        to-be-copied workitems\n\n        More details, please refer to\n        :class:`rtcclient.template.Templater.listFieldsFromWorkitem`\n        \"\"\"\n", "input": "", "output": "\n        return self.templater.listFieldsFromWorkitem(copied_from,\n                                                     keep=keep)", "category": "Python"}, {"instruction": "def condor_submit(submit_file):\n    \"\"\"\n    Submit a condor job described by the given file. Parse an external id for\n    the submission or return None and a reason for the failure.\n    \"\"\"\n", "input": "", "output": "    external_id = None\n    try:\n        submit = Popen(('condor_submit', submit_file), stdout=PIPE, stderr=STDOUT)\n        message, _ = submit.communicate()\n        if submit.returncode == 0:\n            external_id = parse_external_id(message, type='condor')\n        else:\n            message = PROBLEM_PARSING_EXTERNAL_ID\n    except Exception as e:\n        message = str(e)\n    return external_id, message", "category": "Python"}, {"instruction": "def set_network_connection(self, network):\n        \"\"\"\n        Set the network connection for the remote device.\n\n        Example of setting airplane mode::\n\n            driver.mobile.set_network_connection(driver.mobile.AIRPLANE_MODE)\n        \"\"\"\n", "input": "", "output": "        mode = network.mask if isinstance(network, self.ConnectionType) else network\n        return self.ConnectionType(self._driver.execute(\n            Command.SET_NETWORK_CONNECTION, {\n                'name': 'network_connection',\n                'parameters': {'type': mode}})['value'])", "category": "Python"}, {"instruction": "def enqueue(self, future):\n    \"\"\"\n    Enqueue a future to be processed by one of the threads in the pool.\n    The future must be bound to a worker and not have been started yet.\n    \"\"\"\n", "input": "", "output": "\n    future.enqueue()\n    with self._lock:\n      if self._shutdown:\n        raise RuntimeError('ThreadPool has been shut down and can no '\n          'longer accept futures.')\n      self._queue.append(future)\n      if len(self._running) == len(self._workers):\n        self._new_worker()\n      self._lock.notify_all()", "category": "Python"}, {"instruction": "def sort(self):\n        \"\"\"\n        Sort the families by template name.\n\n\n        .. rubric:: Example\n\n        >>> party = Party(families=[Family(template=Template(name='b')),\n        ...                         Family(template=Template(name='a'))])\n        >>> party[0]\n        Family of 0 detections from template b\n        >>> party.sort()[0]\n        Family of 0 detections from template a\n        \"\"\"\n", "input": "", "output": "        self.families.sort(key=lambda x: x.template.name)\n        return self", "category": "Python"}, {"instruction": "def report_changes(self, content):\n        \"\"\"\n        1) Write changes in file,\n        2) Commit changes in git\n        3.1) If something changed, return tuple(True, changes)\n        3.2) If nothing changed, return tuple(False, None)\n\n        If style is \"verbose\", return changes in human-friendly format,\n        else use unified diff\n        \"\"\"\n", "input": "", "output": "        self.write(content)\n        if self.commit():\n            return True, self.reporter.report()\n        else:\n            return False, None", "category": "Python"}, {"instruction": "def check_process_by_name(self, process):\n        \"\"\"Checks if a named process is found in /proc/[0-9]*/cmdline.\n        Returns either True or False.\"\"\"\n", "input": "", "output": "        status = False\n        cmd_line_glob = \"/proc/[0-9]*/cmdline\"\n        try:\n            cmd_line_paths = glob.glob(cmd_line_glob)\n            for path in cmd_line_paths:\n                f = open(path, 'r')\n                cmd_line = f.read().strip()\n                if process in cmd_line:\n                    status = True\n        except IOError as e:\n            return False\n        return status", "category": "Python"}, {"instruction": "def user_exists_p(login, connector):\n    \"\"\"\n    Determine if user exists in specified environment.\n    \"\"\"\n", "input": "", "output": "    url = '/users/' + login + '/'\n    _r = connector.get(url)\n    return (_r.status_code == Constants.PULP_GET_OK)", "category": "Python"}, {"instruction": "def validate_units(self, value):\n        \"\"\"Validate units, assuming that it was called by _validate_type_*.\"\"\"\n", "input": "", "output": "        self.validate_quantity(value)\n        self.units_type = inspect.stack()[1][3].split('_')[-1]\n        assert self.units_type, (\"`validate_units` should not be called \"\n                                 \"directly. It should be called by a \"\n                                 \"_validate_type_* methods that sets \"\n                                 \"`units_type`\")\n        units = getattr(pq, self.units_map[self.units_type])\n        if not value.simplified.units == units:\n            self._error('%s' % value,\n                        \"Must have dimensions of %s.\" % self.units_type)\n        return True", "category": "Python"}, {"instruction": "def write_fastq(filename):\n    \"\"\"\n    return a handle for FASTQ writing, handling gzipped files\n    \"\"\"\n", "input": "", "output": "    if filename:\n        if filename.endswith('gz'):\n            filename_fh = gzip.open(filename, mode='wb')\n        else:\n            filename_fh = open(filename, mode='w')\n    else:\n        filename_fh = None\n    return filename_fh", "category": "Python"}, {"instruction": "def DeleteArtifact(self, name, cursor=None):\n    \"\"\"Deletes an artifact with given name from the database.\"\"\"\n", "input": "", "output": "\n    cursor.execute(\"DELETE FROM artifacts WHERE name = %s\", [name])\n\n    if cursor.rowcount == 0:\n      raise db.UnknownArtifactError(name)", "category": "Python"}, {"instruction": "def load_clients(self, path=None, apis=[]):\n        \"\"\"Generate client libraries for the given apis, without starting an\n        api server\"\"\"\n", "input": "", "output": "\n        if not path:\n            raise Exception(\"Missing path to api swagger files\")\n\n        if type(apis) is not list:\n            raise Exception(\"'apis' should be a list of api names\")\n\n        if len(apis) == 0:\n            raise Exception(\"'apis' is an empty list - Expected at least one api name\")\n\n        for api_name in apis:\n            api_path = os.path.join(path, '%s.yaml' % api_name)\n            if not os.path.isfile(api_path):\n                raise Exception(\"Cannot find swagger specification at %s\" % api_path)\n            log.info(\"Loading api %s from %s\" % (api_name, api_path))\n            ApiPool.add(\n                api_name,\n                yaml_path=api_path,\n                timeout=self.timeout,\n                error_callback=self.error_callback,\n                formats=self.formats,\n                do_persist=False,\n                local=False,\n            )\n\n        return self", "category": "Python"}, {"instruction": "def _pdf(self, **kwargs):\n        \"\"\"Returns the pdf at the given values. The keyword arguments must\n        contain all of parameters in self's params. Unrecognized arguments are\n        ignored.\n        \"\"\"\n", "input": "", "output": "        if kwargs in self:\n            vals = numpy.array([numpy.log(10) * self._norm * kwargs[param]\n                                for param in kwargs.keys()])\n            return 1.0 / numpy.prod(vals)\n        else:\n            return 0.", "category": "Python"}, {"instruction": "def scale(self, scalar, ignored_terms=None):\n        \"\"\"Multiply the polynomial by the given scalar.\n\n        Args:\n            scalar (number):\n                Value to multiply the polynomial by.\n\n            ignored_terms (iterable, optional):\n                Biases associated with these terms are not scaled.\n\n        \"\"\"\n", "input": "", "output": "\n        if ignored_terms is None:\n            ignored_terms = set()\n        else:\n            ignored_terms = {asfrozenset(term) for term in ignored_terms}\n\n        for term in self:\n            if term not in ignored_terms:\n                self[term] *= scalar", "category": "Python"}, {"instruction": "def isnumber(*args):\n    \"\"\"Checks if value is an integer, long integer or float.\n\n    NOTE: Treats booleans as numbers, where True=1 and False=0.\n    \"\"\"\n", "input": "", "output": "    return all(map(lambda c: isinstance(c, int) or isinstance(c, float), args))", "category": "Python"}, {"instruction": "def _add(self, obj, data, **kwargs):\n        \"\"\" Update the object directly.\n\n        .. code-block:: python\n\n            DBSession.sacrud(Users)._add(UserObj, {'name': 'Gennady'})\n        \"\"\"\n", "input": "", "output": "        if isinstance(obj, sqlalchemy.orm.query.Query):\n            obj = obj.one()\n        obj = self.preprocessing(obj=obj or self.table)\\\n            .add(self.session, data, self.table)\n        self.session.add(obj)\n        if kwargs.get('commit', self.commit) is True:\n            try:\n                self.session.commit()\n            except AssertionError:\n                transaction.commit()\n        return obj", "category": "Python"}, {"instruction": "def writln(line, unit):\n    \"\"\"\n    Internal undocumented command for writing a text line to a logical unit\n\n    No URL available; relevant lines from SPICE source:\n\n    FORTRAN SPICE, writln.f::\n\n        C$Procedure      WRITLN ( Write a text line to a logical unit )\n              SUBROUTINE WRITLN ( LINE, UNIT )\n              CHARACTER*(*)      LINE\n              INTEGER            UNIT\n\n        C     Variable  I/O  Description\n        C     --------  ---  --------------------------------------------------\n        C     LINE       I   The line which is to be written to UNIT.\n        C     UNIT       I   The Fortran unit number to use for output.\n\n    CSPICE, writln.c::\n\n        /* $Procedure      WRITLN ( Write a text line to a logical unit ) */\n        /* Subroutine */ int writln_(char *line, integer *unit, ftnlen line_len)\n\n    :param line: The line which is to be written to UNIT.\n    :type line: str\n    :param unit: The Fortran unit number to use for output.\n    :type unit: int\n    \"\"\"\n", "input": "", "output": "    lineP    = stypes.stringToCharP(line)\n    unit     = ctypes.c_int(unit)\n    line_len = ctypes.c_int(len(line))\n    libspice.writln_(lineP, ctypes.byref(unit), line_len)", "category": "Python"}, {"instruction": "def make_config_data(*, guided):\n    \"\"\"\n    Makes the data necessary to construct a functional config file\n    \"\"\"\n", "input": "", "output": "    config_data = {}\n    config_data[INCLUDE_DIRS_KEY] = _make_include_dirs(guided=guided)\n    config_data[RUNTIME_DIRS_KEY] = _make_runtime_dirs(guided=guided)\n    config_data[RUNTIME_KEY] = _make_runtime()\n\n    return config_data", "category": "Python"}, {"instruction": "def main():\n    \"\"\"Run the LockingProtocol.\"\"\"\n", "input": "", "output": "    args = parse_input()\n    args.lock = True\n    args.question = []\n    args.all = False\n    args.timeout = 0\n    args.verbose = False\n    args.interactive = False\n\n    try:\n        assign = assignment.load_assignment(args.config, args)\n\n        msgs = messages.Messages()\n\n        lock.protocol(args, assign).run(msgs)\n    except (ex.LoadingException, ex.SerializeException) as e:\n        log.warning('Assignment could not instantiate', exc_info=True)\n        print('Error: ' + str(e).strip())\n        exit(1)\n    except (KeyboardInterrupt, EOFError):\n        log.info('Quitting...')\n    else:\n        assign.dump_tests()", "category": "Python"}, {"instruction": "def url_should_not_contain(self, url):\n    \"\"\"Assert the absolute URL of the browser does not contain the provided.\"\"\"\n", "input": "", "output": "\n    if url in world.browser.current_url:\n        raise AssertionError(\n            \"Browser URL expected not to contain {!r}, got {!r}.\".format(\n                url, world.browser.current_url))", "category": "Python"}, {"instruction": "def VerifyStructure(self, parser_mediator, line):\n    \"\"\"Verify that this file is a XChat scrollback log file.\n\n    Args:\n      parser_mediator (ParserMediator): mediates interactions between parsers\n          and other components, such as storage and dfvfs.\n      line (str): line from a text file.\n\n    Returns:\n      bool: True if the line was successfully parsed.\n    \"\"\"\n", "input": "", "output": "    structure = self.LOG_LINE\n\n    try:\n      parsed_structure = structure.parseString(line)\n    except pyparsing.ParseException:\n      logger.debug('Not a XChat scrollback log file')\n      return False\n\n    try:\n      int(parsed_structure.timestamp, 10)\n    except ValueError:\n      logger.debug('Not a XChat scrollback log file, invalid timestamp string')\n      return False\n\n    return True", "category": "Python"}, {"instruction": "def validate_bucket(self):\n        \"\"\"\n        Do a quick check to see if the s3 bucket is valid\n        :return:\n        \"\"\"\n", "input": "", "output": "        s3_check_cmd = \"aws s3 ls s3://{} --profile '{}' --region '{}'\".format(self.bucket_name, self.aws_project,\n                                                                               self.aws_regions[0])\n        print \"Checking for s3 bucket\"\n        try:\n            subprocess.check_output(shlex.split(s3_check_cmd))\n        except subprocess.CalledProcessError as e:\n            print \"Error: {}\".format(e)\n            print \"Unable to query s3 bucket: {}. Validate that it exists, and your user has sufficient permissions\"\\\n                .format(self.bucket_name)\n            sys.exit(5)", "category": "Python"}, {"instruction": "def check_threats(**args):\n\t\"\"\"\n\tfunction to check input filetype against threat extensions list \n\t\"\"\"\n", "input": "", "output": "\tis_high_threat = False\n\tfor val in THREAT_EXTENSIONS.values():\n\t\tif type(val) == list:\n\t\t\tfor el in val:\n\t\t\t\tif args['file_type'] == el:\n\t\t\t\t\tis_high_threat = True\n\t\t\t\t\tbreak\n\t\telse:\n\t\t\tif args['file_type'] == val:\n\t\t\t\tis_high_threat = True\n\t\t\t\tbreak\n\treturn is_high_threat", "category": "Python"}, {"instruction": "def move_forward(num_steps):\n    \"\"\"Moves the pen forward a few steps in the direction that its \"turtle\" is facing.\n\n    Arguments:\n        num_steps - a number like 20. A bigger number makes the pen move farther.\n    \"\"\"\n", "input": "", "output": "    assert int(num_steps) == num_steps, \"move_forward() only accepts integers, but you gave it \" + str(num_steps)\n\n    _make_cnc_request(\"move.forward./\" + str(num_steps))\n\n    state['turtle'].forward(num_steps)", "category": "Python"}, {"instruction": "def state(self, state: str) -> None:\n        \"\"\"Update state of event.\"\"\"\n", "input": "", "output": "        self._state = state\n        for callback in self._callbacks:\n            callback()", "category": "Python"}, {"instruction": "def get_mean(self, col, row):\n        \"\"\"\n        Returns the mean at this location (if valid location).\n\n        :param col: the 0-based column index\n        :type col: int\n        :param row: the 0-based row index\n        :type row: int\n        :return: the mean\n        :rtype: float\n        \"\"\"\n", "input": "", "output": "        return javabridge.call(self.jobject, \"getMean\", \"(II)D\", col, row)", "category": "Python"}, {"instruction": "def retrieve_xml(pdb_id, silent = True):\n    '''The RCSB website now compresses XML files.'''\n", "input": "", "output": "    xml_gz = retrieve_file_from_RCSB(get_rcsb_files_connection(), \"/download/%s.xml.gz\" % pdb_id, silent = silent)\n    cf = StringIO.StringIO()\n    cf.write(xml_gz)\n    cf.seek(0)\n    df = gzip.GzipFile(fileobj = cf, mode='rb')\n    contents = df.read()\n    df.close()\n    return contents", "category": "Python"}, {"instruction": "def data_play(Y, visualizer, frame_rate=30):\n    \"\"\"Play a data set using the data_show object given.\n\n    :Y: the data set to be visualized.\n    :param visualizer: the data show objectwhether to display during optimisation\n    :type visualizer: data_show\n\n    Example usage:\n\n    This example loads in the CMU mocap database (http://mocap.cs.cmu.edu) subject number 35 motion number 01. It then plays it using the mocap_show visualize object.\n\n    .. code-block:: python\n\n       data = GPy.util.datasets.cmu_mocap(subject='35', train_motions=['01'])\n       Y = data['Y']\n       Y[:, 0:3] = 0.   # Make figure walk in place\n       visualize = GPy.util.visualize.skeleton_show(Y[0, :], data['skel'])\n       GPy.util.visualize.data_play(Y, visualize)\n\n    \"\"\"\n", "input": "", "output": "\n\n    for y in Y:\n        visualizer.modify(y[None, :])\n        time.sleep(1./float(frame_rate))", "category": "Python"}, {"instruction": "def disable_host_freshness_check(self, host):\n        \"\"\"Disable freshness check for a host\n        Format of the line that triggers function call::\n\n        DISABLE_HOST_FRESHNESS_CHECK;<host_name>\n\n        :param host: host to edit\n        :type host: alignak.objects.host.Host\n        :return: None\n        \"\"\"\n", "input": "", "output": "        if host.check_freshness:\n            host.modified_attributes |= DICT_MODATTR[\"MODATTR_FRESHNESS_CHECKS_ENABLED\"].value\n            host.check_freshness = False\n            self.send_an_element(host.get_update_status_brok())", "category": "Python"}, {"instruction": "def register_backend(self, name, backend):\n        \"\"\"\n        Register a new backend that will be called for each processed event.\n\n        Note that backends are called in the order that they are registered.\n        \"\"\"\n", "input": "", "output": "        if not hasattr(backend, 'send') or not callable(backend.send):\n            raise ValueError('Backend %s does not have a callable \"send\" method.' % backend.__class__.__name__)\n        else:\n            self.backends[name] = backend", "category": "Python"}, {"instruction": "def write_burn_in(self, burn_in):\n        \"\"\"Write the given burn-in data to the given filename.\"\"\"\n", "input": "", "output": "        group = self[self.sampler_group]\n        group.attrs['burn_in_test'] = burn_in.burn_in_test\n        group.attrs['is_burned_in'] = burn_in.is_burned_in\n        group.attrs['burn_in_iteration'] = burn_in.burn_in_iteration\n        # set the defaut thin_start to be the burn_in_index\n        self.thin_start = burn_in.burn_in_index\n        # write individual test data\n        for tst in burn_in.burn_in_data:\n            key = 'burn_in_tests/{}'.format(tst)\n            try:\n                attrs = group[key].attrs\n            except KeyError:\n                group.create_group(key)\n                attrs = group[key].attrs\n            self.write_kwargs_to_attrs(attrs, **burn_in.burn_in_data[tst])", "category": "Python"}, {"instruction": "def getdata(self, tree, location, force_string=False):\n        \"\"\"Gets XML data from a specific element and handles types.\"\"\"\n", "input": "", "output": "        found = tree.xpath('%s/text()' % location)\n        if not found:\n            return None\n        else:\n            data = found[0]\n        if force_string:\n            return data\n        if data == 'True':\n            return True\n        elif data == 'False':\n            return False\n        else:\n            try:\n                return int(data)\n            except ValueError:\n                try:\n                    return float(data)\n                except ValueError:\n                    # It's a string\n                    return data", "category": "Python"}, {"instruction": "def find_base(path):\n\t\"\"\"Find the base of a glob.\"\"\"\n", "input": "", "output": "\tresult = _pattern.match(path)\n\n\tif result:\n\t\tbase = result.group(0)\n\telse:\n\t\tbase = \"./\"\n\n\tif base.endswith('/') or base.endswith('\\\\'):\n\t\treturn os.path.abspath(base)\n\telse:\n\t\treturn os.path.dirname(os.path.abspath(base))", "category": "Python"}, {"instruction": "def update_movie_ticket(self, code, ticket_class, show_time, duration,\n                            screening_room, seat_number, card_id=None):\n        \"\"\"\n        \u66f4\u65b0\u7535\u5f71\u7968\n        \"\"\"\n", "input": "", "output": "        ticket = {\n            'code': code,\n            'ticket_class': ticket_class,\n            'show_time': show_time,\n            'duration': duration,\n            'screening_room': screening_room,\n            'seat_number': seat_number\n        }\n        if card_id:\n            ticket['card_id'] = card_id\n        return self._post(\n            'card/movieticket/updateuser',\n            data=ticket\n        )", "category": "Python"}, {"instruction": "def get_progress(self):\r\n        \"\"\"Method to get the progress of work notice sending.\"\"\"\n", "input": "", "output": "        progress = self.json_response.get(\"progress\", None)\r\n        self.logger.info(\"%s\\t%s\" % (self.request_method, self.request_url))\r\n        return progress", "category": "Python"}, {"instruction": "def _change_mode(self, mode, major, minor):\n        \"\"\" Change mode of operation, with some sanity checks. \"\"\"\n", "input": "", "output": "        if self._mode:\n            if self._mode != mode:\n                raise RuntimeError('Can\\'t change mode (from %s to %s)' % (self._mode, mode))\n        self._require_version(major=major, minor=minor)\n        self._mode = mode\n        # when setting mode, we reset all flags\n        self.ticket_flags = YubiKeyConfigBits(0x0)\n        self.config_flags = YubiKeyConfigBits(0x0)\n        self.extended_flags = YubiKeyConfigBits(0x0)\n        if mode != 'YUBIKEY_OTP':\n            self.ticket_flag(mode, True)", "category": "Python"}, {"instruction": "def concat_sha256(secret, dk_len, other_info):\n    \"\"\"\n    The Concat KDF, using SHA256 as the hash function.\n\n    Note: Does not validate that otherInfo meets the requirements of\n    SP800-56A.\n\n    :param secret: The shared secret value\n    :param dk_len: Length of key to be derived, in bits\n    :param other_info: Other info to be incorporated (see SP800-56A)\n    :return: The derived key\n    \"\"\"\n", "input": "", "output": "    dkm = b''\n    dk_bytes = int(ceil(dk_len / 8.0))\n    counter = 0\n    while len(dkm) < dk_bytes:\n        counter += 1\n        counter_bytes = struct.pack(\"!I\", counter)\n        digest = hashes.Hash(hashes.SHA256(), backend=default_backend())\n        digest.update(counter_bytes)\n        digest.update(secret)\n        digest.update(other_info)\n        dkm += digest.finalize()\n    return dkm[:dk_bytes]", "category": "Python"}, {"instruction": "def is_type_factory(_type):\n    \"\"\"\n    Parameters\n    ----------\n    `_type` - a type to be compared against (e.g. type(x) == `_type`)\n    Returns\n    -------\n    validator - a function of a single argument x , which returns the\n                True if type(x) is equal to `_type`\n    \"\"\"\n", "input": "", "output": "\n    def inner(x):\n        if type(x) != _type:\n            raise ValueError(\"Value must have type '%s'\" % str(_type))\n\n    return inner", "category": "Python"}, {"instruction": "def exec_func_src2(func, globals_=None, locals_=None, sentinal=None,\n                   verbose=False, start=None, stop=None):\n    \"\"\"\n    execs a func and returns requested local vars.\n\n    Does not modify globals unless update=True (or in IPython)\n\n    SeeAlso:\n        ut.execstr_funckw\n    \"\"\"\n", "input": "", "output": "    import utool as ut\n    sourcecode = ut.get_func_sourcecode(func, stripdef=True, stripret=True)\n    if globals_ is None:\n        globals_ = ut.get_parent_frame().f_globals\n    if locals_ is None:\n        locals_ = ut.get_parent_frame().f_locals\n    if sentinal is not None:\n        sourcecode = ut.replace_between_tags(sourcecode, '', sentinal)\n    if start is not None or stop is not None:\n        sourcecode = '\\n'.join(sourcecode.splitlines()[slice(start, stop)])\n    if verbose:\n        print(ut.color_text(sourcecode, 'python'))\n    # TODO: find the name of every variable that was assigned in the function\n    # and get it from the context\n    locals2_ = locals_.copy()\n    globals2_ = globals_.copy()\n    six.exec_(sourcecode, globals2_, locals2_)\n    return locals2_", "category": "Python"}, {"instruction": "def register_error_handler(self, error: Union[Type[Exception], int], func: Callable) -> None:\n        \"\"\"Add an error handler function to the blueprint.\n\n        This is designed to be used on the blueprint directly, and\n        has the same arguments as\n        :meth:`~quart.Quart.register_error_handler`. An example usage,\n\n        .. code-block:: python\n\n            def not_found():\n                ...\n\n            blueprint = Blueprint(__name__)\n            blueprint.register_error_handler(404, not_found)\n        \"\"\"\n", "input": "", "output": "        self.record_once(lambda state: state.app.register_error_handler(error, func, self.name))", "category": "Python"}, {"instruction": "def dcm(self, dcm):\n        \"\"\"\n        Set the DCM\n        :param dcm: 3x3 array\n\n        \"\"\"\n", "input": "", "output": "        assert(len(dcm) == 3)\n        for sub in dcm:\n            assert(len(sub) == 3)\n\n        self._dcm = np.array(dcm)\n\n        # mark other representations as outdated, will get generated on next\n        # read\n        self._q = None\n        self._euler = None", "category": "Python"}, {"instruction": "def transform(self, X, y=None, **params):\n        \"\"\"\n        Transforms *X* from phase-space to Fourier-space, returning the design\n        matrix produced by :func:`Fourier.design_matrix` for input to a\n        regressor.\n\n        **Parameters**\n\n        X : array-like, shape = [n_samples, 1]\n            Column vector of phases.\n        y : None, optional\n            Unused argument for conformity (default None).\n\n        **Returns**\n\n        design_matrix : array-like, shape = [n_samples, 2*degree+1]\n            Fourier design matrix produced by :func:`Fourier.design_matrix`.\n        \"\"\"\n", "input": "", "output": "        data = numpy.dstack((numpy.array(X).T[0], range(len(X))))[0]\n        phase, order = data[data[:,0].argsort()].T\n        design_matrix = self.design_matrix(phase, self.degree)\n        return design_matrix[order.argsort()]", "category": "Python"}, {"instruction": "def remove_alert_tag(self, id, tag_value, **kwargs):  # noqa: E501\n        \"\"\"Remove a tag from a specific alert  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.remove_alert_tag(id, tag_value, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :param str tag_value: (required)\n        :return: ResponseContainer\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.remove_alert_tag_with_http_info(id, tag_value, **kwargs)  # noqa: E501\n        else:\n            (data) = self.remove_alert_tag_with_http_info(id, tag_value, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def connect(self, receiver, signal, function, description, sender=None):\n        \"\"\"\n        Connect a receiver to a signal\n\n        :param receiver: Name of the receiver\n        :type receiver: str\n        :param signal: Name of the signal. Must already be registered!\n        :type signal: str\n        :param function: Callable functions, which shall be executed, of signal is send.\n        :param description: Description of the reason or use case, why this connection is needed.\n                            Used for documentation.\n        \"\"\"\n", "input": "", "output": "        return self.__app.signals.connect(receiver, signal, function, self._plugin, description, sender)", "category": "Python"}, {"instruction": "def _pseudo_data(self, y, lp, mu):\n        \"\"\"\n        compute the pseudo data for a PIRLS iterations\n\n        Parameters\n        ---------\n        y : array-like of shape (n,)\n            containing target data\n        lp : array-like of shape (n,)\n            containing linear predictions by the model\n        mu : array-like of shape (n_samples,)\n            expected value of the targets given the model and inputs\n\n        Returns\n        -------\n        pseudo_data : np.array of shape (n,)\n        \"\"\"\n", "input": "", "output": "        return lp + (y - mu) * self.link.gradient(mu, self.distribution)", "category": "Python"}, {"instruction": "def positionedcrop(self,x,y,sheet_coord_system):\n        \"\"\"\n        Offset the bounds_template to this cf's location and store the\n        result in the 'bounds' attribute.\n\n        Also stores the input_sheet_slice for access by C.\n        \"\"\"\n", "input": "", "output": "        cf_row,cf_col = sheet_coord_system.sheet2matrixidx(x,y)\n        bounds_x,bounds_y=self.compute_bounds(sheet_coord_system).centroid()\n\n        b_row,b_col=sheet_coord_system.sheet2matrixidx(bounds_x,bounds_y)\n\n        row_offset = cf_row-b_row\n        col_offset = cf_col-b_col\n        self.translate(row_offset,col_offset)", "category": "Python"}, {"instruction": "def _frozensetload(l: Loader, value, type_) -> FrozenSet:\n    \"\"\"\n    This loads into something like FrozenSet[int]\n    \"\"\"\n", "input": "", "output": "    t = type_.__args__[0]\n    return frozenset(l.load(i, t) for i in value)", "category": "Python"}, {"instruction": "def organization_to_rdf(org, graph=None):\n    '''\n    Map a Resource domain model to a DCAT/RDF graph\n    '''\n", "input": "", "output": "    graph = graph or Graph(namespace_manager=namespace_manager)\n    if org.id:\n        org_url = url_for('organizations.show_redirect',\n                          org=org.id,\n                          _external=True)\n        id = URIRef(org_url)\n    else:\n        id = BNode()\n    o = graph.resource(id)\n    o.set(RDF.type, FOAF.Organization)\n    o.set(FOAF.name, Literal(org.name))\n    o.set(RDFS.label, Literal(org.name))\n    if org.url:\n        o.set(FOAF.homepage, URIRef(org.url))\n    return o", "category": "Python"}, {"instruction": "def run(self):\n    \"\"\"The main routine for a thread's work.\n\n    The thread pulls tasks from the manager's task queue and executes\n    them until it encounters a task with a function that is None.\n    \"\"\"\n", "input": "", "output": "    try:\n      while True:\n        aFunction, arguments = self.manager.taskQueue.get()\n        if aFunction is None:\n          break\n        aFunction(arguments)\n    except KeyboardInterrupt:\n      import thread\n      print >>sys.stderr, \"%s caught KeyboardInterrupt\" % threading.currentThread().getName()\n      thread.interrupt_main()\n    except Exception, x:\n      print >>sys.stderr, \"Something BAD happened in %s:\" % threading.currentThread().getName()\n      traceback.print_exc(file=sys.stderr)\n      print >>sys.stderr, x", "category": "Python"}, {"instruction": "def get_callback_pattern(expected_params, actual_params):\n        \"\"\"\n        Assembles a dictionary whith the parameters schema defined for this route\n        :param expected_params dict parameters schema defined for this route\n        :param actual_params dict actual url parameters\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        pattern = dict()\n        key = 0\n        for exp_param in expected_params:\n            if exp_param[0] == '{' and exp_param[-1:] == '}':\n                pattern[exp_param[1:-1]] = actual_params[key]\n            key = key + 1\n        return pattern", "category": "Python"}, {"instruction": "def invert(self):\n        \"\"\"Inverts the DFA final states\"\"\"\n", "input": "", "output": "        for state in self.states:\n            if state.final:\n                state.final = False\n            else:\n                state.final = True", "category": "Python"}, {"instruction": "def create_scratch_org(self, org_name, config_name, days=None, set_password=True):\n        \"\"\" Adds/Updates a scratch org config to the keychain from a named config \"\"\"\n", "input": "", "output": "        scratch_config = getattr(\n            self.project_config, \"orgs__scratch__{}\".format(config_name)\n        )\n        if days is not None:\n            # Allow override of scratch config's default days\n            scratch_config[\"days\"] = days\n        else:\n            # Use scratch config days or default of 1 day\n            scratch_config.setdefault(\"days\", 1)\n        scratch_config[\"set_password\"] = bool(set_password)\n        scratch_config[\"scratch\"] = True\n        scratch_config.setdefault(\"namespaced\", False)\n        scratch_config[\"config_name\"] = config_name\n        scratch_config[\"sfdx_alias\"] = \"{}__{}\".format(\n            self.project_config.project__name, org_name\n        )\n        org_config = ScratchOrgConfig(scratch_config, org_name)\n        self.set_org(org_config)", "category": "Python"}, {"instruction": "def _run_algorithm(self):\n        \"\"\" Runs nearest neighbor (NN) identification and feature scoring to yield MultiSURF scores. \"\"\"\n", "input": "", "output": "        nan_entries = np.isnan(self._X)\n\n        NNlist = [self._find_neighbors(datalen) for datalen in range(self._datalen)]\n\n        scores = np.sum(Parallel(n_jobs=self.n_jobs)(delayed(\n            MultiSURF_compute_scores)(instance_num, self.attr, nan_entries, self._num_attributes, self.mcmap,\n                                      NN_near, self._headers, self._class_type, self._X, self._y, self._labels_std, self.data_type)\n            for instance_num, NN_near in zip(range(self._datalen), NNlist)), axis=0)\n\n        return np.array(scores)", "category": "Python"}, {"instruction": "def set(path, value, version=-1, profile=None, hosts=None, scheme=None,\n        username=None, password=None, default_acl=None):\n    '''\n    Update znode with new value\n\n    path\n        znode to update\n\n    value\n        value to set in znode\n\n    version\n        only update znode if version matches (Default: -1 (always matches))\n\n    profile\n        Configured Zookeeper profile to authenticate with (Default: None)\n\n    hosts\n        Lists of Zookeeper Hosts (Default: '127.0.0.1:2181)\n\n    scheme\n        Scheme to authenticate with (Default: 'digest')\n\n    username\n        Username to authenticate (Default: None)\n\n    password\n        Password to authenticate (Default: None)\n\n    default_acl\n        Default acls to assign if a node is created in this connection (Default: None)\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minion1 zookeeper.set /test/name gtmanfred profile=prod\n\n    '''\n", "input": "", "output": "    conn = _get_zk_conn(profile=profile, hosts=hosts, scheme=scheme,\n                        username=username, password=password, default_acl=default_acl)\n    return conn.set(path, salt.utils.stringutils.to_bytes(value), version=version)", "category": "Python"}, {"instruction": "def keygrip_ed25519(vk):\n    \"\"\"Compute keygrip for Ed25519 public keys.\"\"\"\n", "input": "", "output": "    # pylint: disable=line-too-long\n    return _compute_keygrip([\n        ['p', util.num2bytes(0x7FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFED, size=32)],  # nopep8\n        ['a', b'\\x01'],\n        ['b', util.num2bytes(0x2DFC9311D490018C7338BF8688861767FF8FF5B2BEBE27548A14B235ECA6874A, size=32)],  # nopep8\n        ['g', util.num2bytes(0x04216936D3CD6E53FEC0A4E231FDD6DC5C692CC7609525A7B2C9562D608F25D51A6666666666666666666666666666666666666666666666666666666666666658, size=65)],  # nopep8\n        ['n', util.num2bytes(0x1000000000000000000000000000000014DEF9DEA2F79CD65812631A5CF5D3ED, size=32)],  # nopep8\n        ['q', vk.to_bytes()],\n    ])", "category": "Python"}, {"instruction": "def pulse(self, fade_in_time=1, fade_out_time=1, n=None, background=True):\n        \"\"\"\n        Make all LEDs fade in and out repeatedly. Note that this method will\n        only work if the *pwm* parameter was :data:`True` at construction time.\n\n        :param float fade_in_time:\n            Number of seconds to spend fading in. Defaults to 1.\n\n        :param float fade_out_time:\n            Number of seconds to spend fading out. Defaults to 1.\n\n        :type n: int or None\n        :param n:\n            Number of times to blink; :data:`None` (the default) means forever.\n\n        :param bool background:\n            If :data:`True` (the default), start a background thread to\n            continue blinking and return immediately. If :data:`False`, only\n            return when the blink is finished (warning: the default value of\n            *n* will result in this method never returning).\n        \"\"\"\n", "input": "", "output": "        on_time = off_time = 0\n        self.blink(\n            on_time, off_time, fade_in_time, fade_out_time, n, background)", "category": "Python"}, {"instruction": "def upload_chunk(self):\n        \"\"\"\n        Upload chunk of file.\n        \"\"\"\n", "input": "", "output": "        self._retried = 0\n        self._do_request()\n        self.offset = int(self.request.response_headers.get('upload-offset'))\n        if self.log_func:\n            msg = '{} bytes uploaded ...'.format(self.offset)\n            self.log_func(msg)", "category": "Python"}, {"instruction": "def _set_whitespaces_flags(self, show):\n        \"\"\" Sets show white spaces flag \"\"\"\n", "input": "", "output": "        doc = self.document()\n        options = doc.defaultTextOption()\n        if show:\n            options.setFlags(options.flags() |\n                             QtGui.QTextOption.ShowTabsAndSpaces)\n        else:\n            options.setFlags(\n                options.flags() & ~QtGui.QTextOption.ShowTabsAndSpaces)\n        doc.setDefaultTextOption(options)", "category": "Python"}, {"instruction": "def set_session(s):\n    \"\"\"\n    Configures the default connection with a preexisting :class:`cassandra.cluster.Session`\n\n    Note: the mapper presently requires a Session :attr:`~.row_factory` set to ``dict_factory``.\n    This may be relaxed in the future\n    \"\"\"\n", "input": "", "output": "\n    try:\n        conn = get_connection()\n    except CQLEngineException:\n        # no default connection set; initalize one\n        register_connection('default', session=s, default=True)\n        conn = get_connection()\n\n    if conn.session:\n        log.warning(\"configuring new default connection for cqlengine when one was already set\")\n\n    if s.row_factory is not dict_factory:\n        raise CQLEngineException(\"Failed to initialize: 'Session.row_factory' must be 'dict_factory'.\")\n    conn.session = s\n    conn.cluster = s.cluster\n\n    # Set default keyspace from given session's keyspace\n    if conn.session.keyspace:\n        from cassandra.cqlengine import models\n        models.DEFAULT_KEYSPACE = conn.session.keyspace\n\n    conn.setup_session()\n\n    log.debug(\"cqlengine default connection initialized with %s\", s)", "category": "Python"}, {"instruction": "def exact_or_minor_exe_version_match(executable_name,\n                                     exe_version_tuples,\n                                     version):\n    \"\"\"\n    IF there is an exact match then use it\n     OTHERWISE try to find a minor version match\n    \"\"\"\n", "input": "", "output": "    exe = exact_exe_version_match(executable_name,\n                                  exe_version_tuples,\n                                  version)\n\n    if not exe:\n        exe = minor_exe_version_match(executable_name,\n                                      exe_version_tuples,\n                                      version)\n    return exe", "category": "Python"}, {"instruction": "def predict(self, data, unkown=None):\n        \"\"\"\\\n        Classify data according to previous calibration.\n\n        :param data: sparse input matrix (ideal dtype is `numpy.float32`)\n        :type data: :class:`scipy.sparse.csr_matrix`\n        :param unkown: the label to attribute if no label is known\n        :returns: the labels guessed for data\n        :rtype: `numpy.array`\n        \"\"\"\n", "input": "", "output": "        assert self.classifier is not None, 'not calibrated'\n        bmus = self._som.bmus(data)\n        return self._predict_from_bmus(bmus, unkown)", "category": "Python"}, {"instruction": "def lift(self, func):\n        \"\"\"Map function over monadic value.\n\n        Takes a function and a monadic value and maps the function over the\n        monadic value\n\n        Haskell: liftM :: (Monad m) => (a -> b) -> m a -> m b\n\n        This is really the same function as Functor.fmap, but is instead\n        implemented using bind, and does not rely on us inheriting from\n        Functor.\n        \"\"\"\n", "input": "", "output": "\n        return self.bind(lambda x: self.unit(func(x)))", "category": "Python"}, {"instruction": "def requestFields(self, field_names, required=False, strict=False):\n        \"\"\"Add the given list of fields to the request\n\n        @param field_names: The simple registration data fields to request\n        @type field_names: [str]\n\n        @param required: Whether these values should be presented to\n            the user as required\n\n        @param strict: whether to raise an exception when a field is\n            added to a request more than once\n\n        @raise ValueError: when a field requested is not a simple\n            registration field or strict is set and a field was\n            requested more than once\n        \"\"\"\n", "input": "", "output": "        if isinstance(field_names, basestring):\n            raise TypeError('Fields should be passed as a list of '\n                            'strings (not %r)' % (type(field_names),))\n\n        for field_name in field_names:\n            self.requestField(field_name, required, strict=strict)", "category": "Python"}, {"instruction": "def is_nullable_list(val, vtype):\n    \"\"\"Return True if list contains either values of type `vtype` or None.\"\"\"\n", "input": "", "output": "    return (isinstance(val, list) and\n            any(isinstance(v, vtype) for v in val) and\n            all((isinstance(v, vtype) or v is None) for v in val))", "category": "Python"}, {"instruction": "def previous_calling_points(self):\n        \"\"\"\n        A list of CallingPoint objects.\n\n        This is the list of all previous calling points for the service,\n        including all associated services if multiple services join together\n        to form this service.\n        \"\"\"\n", "input": "", "output": "        calling_points = list()\n        for cpl in self._previous_calling_point_lists:\n            calling_points += cpl.calling_points\n        return calling_points", "category": "Python"}, {"instruction": "def mock_import(do_not_mock=None, **mock_kwargs):\n    \"\"\"\n    Mocks import statements by ignoring ImportErrors\n    and replacing the missing module with a Mock.\n\n    :param str|unicode|list[str|unicode] do_not_mock: names of modules\n        that should exists, and an ImportError could be raised for.\n    :param mock_kwargs: kwargs for MagicMock object.\n    :return: patch object\n    \"\"\"\n", "input": "", "output": "\n    do_not_mock = _to_list(do_not_mock)\n\n    def try_import(module_name, *args, **kwargs):\n        try:\n            return _builtins_import(module_name, *args, **kwargs)\n        except:    # intentionally catch all exceptions\n            if any((_match(module_name, prefix) for prefix in do_not_mock)):\n                # This is a module we need to import,\n                # so we raise the exception instead of mocking it\n                raise\n            # Mock external module so we can peacefully create our client\n            return mock.MagicMock(**mock_kwargs)\n\n    return mock.patch('six.moves.builtins.__import__', try_import)", "category": "Python"}, {"instruction": "def load_trajectory(name, skip=1, format=None):\n    '''Load a trajectory file into chemlab. You should call this\n    command after you load a `~chemlab.core.System` through\n    load_system or load_remote_system.\n\n    '''\n", "input": "", "output": "    df = datafile(name, format=format)\n    dt, coords = df.read('trajectory', skip=skip)\n    boxes = df.read('boxes')\n    viewer.current_traj = coords\n    viewer.frame_times = dt\n    \n    viewer.traj_controls.set_ticks(len(dt))\n    \n    def update(index):\n        \n        f = coords[index]        \n        \n        for fp in _frame_processors:\n            f = fp(coords, index)\n\n        # update the current representation\n        viewer.representation.update_positions(f)\n        viewer.representation.update_box(boxes[index])\n        current_system().r_array = f\n        current_system().box_vectors = boxes[index]\n        viewer.traj_controls.set_time(dt[index])\n        viewer.update()\n    \n    viewer.traj_controls.show()\n    viewer.traj_controls.frame_changed.connect(update)", "category": "Python"}, {"instruction": "def _find_video(self):\n        \"\"\"\n        Lookup and populate ``pybrightcove.video.Video`` object given a video\n        id or reference_id.\n        \"\"\"\n", "input": "", "output": "        data = None\n        if self.id:\n            data = self.connection.get_item(\n                'find_video_by_id', video_id=self.id)\n        elif self.reference_id:\n            data = self.connection.get_item(\n                'find_video_by_reference_id', reference_id=self.reference_id)\n\n        if data:\n            self._load(data)", "category": "Python"}, {"instruction": "def reduce_vocab_by_len(tokens, similarity=.87, limit=20, reverse=True):\n    \"\"\"Find spelling variations of similar words within a list of tokens to reduce token set size\n\n    Sorted by length (longest first unless reverse=False) before running through fuzzy-wuzzy\n    which results in longer key tokens.\n\n    Arguments:\n      tokens (list or set or tuple of str): token strings from which to eliminate similar spellings\n\n    Returns:\n      dict: { 'token': ('similar_token', 'similar_token2', ...), ...}\n\n    Examples:\n      FIXME: needs to return a dict of sets rather than dict of tuples so order doesn't matter\n      >> tokens = set(('on', 'hon', 'honey', 'ones', 'one', 'two', 'three'))\n      {'honey': ('on', 'hon', 'one'), 'ones': (), 'three': (), 'two': ()}\n    \"\"\"\n", "input": "", "output": "    tokens = set(tokens)\n    tokens_sorted = list(zip(*sorted([(len(tok), tok) for tok in tokens], reverse=reverse)))[1]\n    return reduce_vocab(tokens=tokens_sorted, similarity=similarity, limit=limit, sort_order=0)", "category": "Python"}, {"instruction": "def smart_scrub(df,col_name,error_rate = 0):\n    \"\"\" Scrubs from the front and back of an 'object' column in a DataFrame\n    until the scrub would semantically alter the contents of the column. If only a \n    subset of the elements in the column are scrubbed, then a boolean array indicating which\n    elements have been scrubbed is appended to the dataframe. Returns a tuple of the strings removed\n    from the front and back of the elements\n    df - DataFrame\n        DataFrame to scrub\n    col_name - string\n        Name of column to scrub\n    error_rate - number, default 0\n        The maximum amount of values this function can ignore while scrubbing, expressed as a\n        fraction of the total amount of rows in the dataframe. \n    \"\"\"\n", "input": "", "output": "    scrubf = smart_scrubf(df,col_name,error_rate)\n    scrubb = smart_scrubb(df,col_name,error_rate)\n    return (scrubf, scrubb)", "category": "Python"}, {"instruction": "def volumes_delete(storage_pool, logger):\n    \"\"\"Deletes all storage volume disks contained in the given storage pool.\"\"\"\n", "input": "", "output": "    try:\n        for vol_name in storage_pool.listVolumes():\n            try:\n                vol = storage_pool.storageVolLookupByName(vol_name)\n                vol.delete(0)\n            except libvirt.libvirtError:\n                logger.exception(\n                    \"Unable to delete storage volume %s.\", vol_name)\n    except libvirt.libvirtError:\n        logger.exception(\"Unable to delete storage volumes.\")", "category": "Python"}, {"instruction": "def serve_file(load, fnd):\n    '''\n    Return a chunk from a file based on the data received\n    '''\n", "input": "", "output": "    if 'env' in load:\n        # \"env\" is not supported; Use \"saltenv\".\n        load.pop('env')\n\n    ret = {'data': '',\n           'dest': ''}\n\n    if 'path' not in load or 'loc' not in load or 'saltenv' not in load:\n        return ret\n\n    if 'path' not in fnd or 'bucket' not in fnd:\n        return ret\n\n    gzip = load.get('gzip', None)\n\n    # get the saltenv/path file from the cache\n    cached_file_path = _get_cached_file_name(\n            fnd['bucket'],\n            load['saltenv'],\n            fnd['path'])\n\n    ret['dest'] = _trim_env_off_path([fnd['path']], load['saltenv'])[0]\n\n    with salt.utils.files.fopen(cached_file_path, 'rb') as fp_:\n        fp_.seek(load['loc'])\n        data = fp_.read(__opts__['file_buffer_size'])\n        if data and six.PY3 and not salt.utils.files.is_binary(cached_file_path):\n            data = data.decode(__salt_system_encoding__)\n        if gzip and data:\n            data = salt.utils.gzip_util.compress(data, gzip)\n            ret['gzip'] = gzip\n        ret['data'] = data\n    return ret", "category": "Python"}, {"instruction": "def comment_sync(self, comment):\n        \"\"\"Update comments to host and notify subscribers\"\"\"\n", "input": "", "output": "        self.host.update(key=\"comment\", value=comment)\n        self.host.emit(\"commented\", comment=comment)", "category": "Python"}, {"instruction": "def validate(self):\n\t\t\"\"\"Verify required headers\"\"\"\n", "input": "", "output": "\t\tfor header in self._requiredHeaders:\n\t\t\tif not self.headers.get(header, False):\n\t\t\t\traise errors.ParseError('Missing Notification Header: ' + header)", "category": "Python"}, {"instruction": "def _inception_table_links(self, href_list):\n        \"\"\"\n        Sometimes the EPA likes to nest their models and tables -- model within\n        a model within a model -- so this internal method tries to clear all\n        that up.\n        \"\"\"\n", "input": "", "output": "        tables = set()\n        for link in href_list:\n            if not link.startswith('http://'):\n                link = self.agency_url + link\n            html = urlopen(link).read()\n            doc = lh.fromstring(html)\n            area = doc.cssselect('map area')\n            if area:\n                # Then this is a model containing models.\n                tables.update((a.attrib['href'] for a in area))\n            else:\n                # The link is a table without additional models.\n                tables.update(link)\n        return tables", "category": "Python"}, {"instruction": "def on_network_adapter_change(self, network_adapter, change_adapter):\n        \"\"\"Triggered when settings of a network adapter of the\n        associated virtual machine have changed.\n\n        in network_adapter of type :class:`INetworkAdapter`\n\n        in change_adapter of type bool\n\n        raises :class:`VBoxErrorInvalidVmState`\n            Session state prevents operation.\n        \n        raises :class:`VBoxErrorInvalidObjectState`\n            Session type prevents operation.\n        \n        \"\"\"\n", "input": "", "output": "        if not isinstance(network_adapter, INetworkAdapter):\n            raise TypeError(\"network_adapter can only be an instance of type INetworkAdapter\")\n        if not isinstance(change_adapter, bool):\n            raise TypeError(\"change_adapter can only be an instance of type bool\")\n        self._call(\"onNetworkAdapterChange\",\n                     in_p=[network_adapter, change_adapter])", "category": "Python"}, {"instruction": "def _update_eof(self):\n        \"\"\"Update EOF flag.\"\"\"\n", "input": "", "output": "        self._aftermathmp()\n        self._ifile.close()\n        self._flag_e = True", "category": "Python"}, {"instruction": "def filterVariantAnnotation(self, vann):\n        \"\"\"\n        Returns true when an annotation should be included.\n        \"\"\"\n", "input": "", "output": "        # TODO reintroduce feature ID search\n        ret = False\n        if len(self._effects) != 0 and not vann.transcript_effects:\n            return False\n        elif len(self._effects) == 0:\n            return True\n        for teff in vann.transcript_effects:\n            if self.filterEffect(teff):\n                ret = True\n        return ret", "category": "Python"}, {"instruction": "def repeater(self, value):\n        \"\"\"\n        Define the repeater of the current chain part.\n\n        :param value:\n        :type value:\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        try:\n            value = int(value)\n            self.repeater_start = value\n            self.repeater_end = value\n            return self\n        except ValueError:\n            pass\n        if value == '+':\n            self.repeater_start = 1\n            self.repeater_end = None\n        if value == '*':\n            self.repeater_start = 0\n            self.repeater_end = None\n        elif value == '?':\n            self.repeater_start = 0\n            self.repeater_end = 1\n        else:\n            match = re.match(r'\\{\\s*(\\d*)\\s*,?\\s*(\\d*)\\s*\\}', value)\n            if match:\n                start = match.group(1)\n                end = match.group(2)\n                if start or end:\n                    self.repeater_start = int(start) if start else 0\n                    self.repeater_end = int(end) if end else None\n        return self", "category": "Python"}, {"instruction": "def delete_contribution(self, url):\n        \"\"\"Delete the contribution with this identifier\n\n        :rtype: bool\n        :returns: True if the contribution was deleted, False otherwise (eg. if it didn't exist)\n        \"\"\"\n", "input": "", "output": "\n        # first validate that this is a real contrib\n        try:\n            result = self.api_request(url)\n\n            if 'url' in result and 'documents' in result:\n                self.api_request(result['url'], method='DELETE')\n                return True\n        except:\n            pass\n\n        return False", "category": "Python"}, {"instruction": "def rename_service(self, serviceName, serviceType,\n                       serviceNewName, folder=None):\n        \"\"\"\n           Renames a published AGS Service\n           Inputs:\n              serviceName - old service name\n              serviceType - type of service\n              serviceNewName - new service name\n              folder - location of where the service lives, none means\n                       root folder.\n           Output:\n              JSON message as dictionary\n        \"\"\"\n", "input": "", "output": "        params = {\n            \"f\" : \"json\",\n            \"serviceName\" : serviceName,\n            \"serviceType\" : serviceType,\n            \"serviceNewName\" : serviceNewName\n        }\n        if folder is None:\n            uURL = self._url + \"/renameService\"\n        else:\n            uURL = self._url + \"/%s/renameService\" % folder\n        return self._post(url=uURL, param_dict=params,\n                             securityHandler=self._securityHandler,\n                             proxy_url=self._proxy_url,\n                             proxy_port=self._proxy_port)", "category": "Python"}, {"instruction": "def connection_made(self, transport):\n        \"\"\"Sets the :attr:`transport`, fire the ``connection_made`` event\n        and adds a :attr:`timeout` for idle connections.\n        \"\"\"\n", "input": "", "output": "        self.transport = transport\n        addr = self.transport.get_extra_info('peername')\n        if not addr:\n            addr = self.transport.get_extra_info('sockname')\n        self.address = addr\n        sock = transport.get_extra_info('socket')\n        if sock:\n            try:\n                sock.setsockopt(SOL_SOCKET, SO_KEEPALIVE, 1)\n            except (OSError, NameError):\n                pass\n        self.changed()\n        self.producer.logger.debug('new connection %s', self)\n        # let everyone know we have a connection with endpoint\n        self.event('connection_made').fire()", "category": "Python"}, {"instruction": "def return_max_phrase(run, idx, dictionary):\n        \"\"\"\n        Finds the maximal phrase in the run starting from the given index.  It uses the dictionary to find sequences of ids\n         that can be merged into a phrase.\n        :param run: a run of ids\n        :param idx: the position in the run to start looking for a merge sequence\n        :param dictionary: the dictionary to use to determine if a merge id is present.  This should be a dictionary of\n        dictionaries.  Each inner dictionary is a continuation of a mergable run.  The end of a run is donated by a None key\n        in a dictionary.  The value associated with the None key is the integer of the merge id.\n        :return: phrase_id or None, index after the phrase_id or the current index if no phrase was found.\n        \"\"\"\n", "input": "", "output": "        if idx < len(run) and run[idx] in dictionary:\n            id = run[idx]\n            rv, rv_idx = PhraseDictionary.return_max_phrase(run, idx + 1, dictionary[id])\n            if rv is not None:\n                return rv, rv_idx\n\n        if None in dictionary:\n            return dictionary[None], idx\n        else:\n            return None, None", "category": "Python"}, {"instruction": "def pull(configuration, *resources):\n    \"\"\"\n    Pull translations from all languages listed in conf/locale/config.yaml\n    where there is at least 10% reviewed translations.\n\n    If arguments are provided, they are specific resources to pull.  Otherwise,\n    all resources are pulled.\n\n    \"\"\"\n", "input": "", "output": "    print(\"Pulling conf/locale/config.yaml:locales from Transifex...\")\n\n    for lang in configuration.translated_locales:\n        cmd = 'tx pull -f --mode=reviewed --minimum-perc=3 -l {lang}'.format(lang=lang)\n        if resources:\n            for resource in resources:\n                execute(cmd + ' -r {resource}'.format(resource=resource))\n        else:\n            execute(cmd)\n    clean_translated_locales(configuration)", "category": "Python"}, {"instruction": "def set_type(self, sequence_type):\n        '''Set the type of a Sequence if it has not been set.'''\n", "input": "", "output": "        if not(self.sequence_type):\n            for id, r in self.sequence.iteritems():\n                assert(r.residue_type == None)\n                r.residue_type = sequence_type\n            self.sequence_type = sequence_type", "category": "Python"}, {"instruction": "def check_files_on_host(java_home, host, files, batch_size):\n    \"\"\"Check the files on the host. Files are grouped together in groups\n    of batch_size files. The dump class will be executed on each batch,\n    sequentially.\n\n    :param java_home: the JAVA_HOME of the broker\n    :type java_home: str\n    :param host: the host where the tool will be executed\n    :type host: str\n    :param files: the list of files to be analyzed\n    :type files: list of str\n    :param batch_size: the size of each batch\n    :type batch_size: int\n    \"\"\"\n", "input": "", "output": "    with closing(ssh_client(host)) as ssh:\n        for i, batch in enumerate(chunks(files, batch_size)):\n            command = check_corrupted_files_cmd(java_home, batch)\n            _, stdout, stderr = ssh.exec_command(command)\n            report_stderr(host, stderr)\n            print(\n                \"  {host}: file {n_file} of {total}\".format(\n                    host=host,\n                    n_file=(i * DEFAULT_BATCH_SIZE),\n                    total=len(files),\n                )\n            )\n            parse_output(host, stdout)", "category": "Python"}, {"instruction": "def createSummaryFile(results, maf, prefix):\n    \"\"\"Creat the final summary file containing plate bias results.\n\n    :param results: the list of all the significant results.\n    :param maf: the minor allele frequency of the significant results.\n    :param prefix: the prefix of all the files.\n\n    :type results: list\n    :type maf: dict\n    :type prefix: str\n\n    \"\"\"\n", "input": "", "output": "    o_filename = prefix + \".significant_SNPs.summary\"\n    try:\n        with open(o_filename, \"w\") as o_file:\n            print >>o_file, \"\\t\".join((\"chrom\", \"pos\", \"name\", \"maf\", \"p\",\n                                       \"odds\", \"plate\"))\n            for row in results:\n                print >>o_file, \"\\t\".join((\n                    row.chrom,\n                    row.pos,\n                    row.name,\n                    maf.get(row.name, \"N/A\"),\n                    row.p,\n                    row.odds,\n                    row.plate,\n                ))\n\n    except IOError:\n        msg = \"{}: cannot write file\".format(o_filename)\n        raise ProgramError(msg)", "category": "Python"}, {"instruction": "def extract_mail(issues):\n    \"\"\"Extract mails that sometimes leak from issue comments.\n    \"\"\"\n", "input": "", "output": "    contacts = set()\n    for idx, issue in enumerate(issues):\n        printmp('Fetching issue #%s' % idx)\n        for comment in issue.comments():\n            comm = comment.as_dict()\n            emails = list(email[0] for email in re.findall(MAIL_REGEX, comm['body'])\n                if not email[0].startswith('//') and not email[0].endswith('github.com') and\n                '@' in email[0])\n            contacts |= set(emails)\n    return contacts", "category": "Python"}, {"instruction": "def ip_to_array(ipaddress):\n    \"\"\"Convert a string representing an IPv4 address to 4 bytes.\"\"\"\n", "input": "", "output": "    res = []\n    for i in ipaddress.split(\".\"):\n        res.append(int(i))\n\n    assert len(res) == 4\n    return res", "category": "Python"}, {"instruction": "def muscle_inputorder(inputfastafile, alnfile, trunc_name=True):\n    \"\"\"\n    Fix for muscle -stable option according to here:\n    http://drive5.com/muscle/stable.html\n    \"\"\"\n", "input": "", "output": "    sh(\"cp {0} {0}.old\".format(alnfile), log=False)\n    maxi = 30 if trunc_name else 1000\n\n    aa = AlignIO.read(alnfile, \"clustal\")\n    alignment = dict((a.id[:maxi], a) for a in aa)\n    if trunc_name and len(alignment) < len(aa):\n        raise ValueError\\\n            (\"ERROR: The first 30 chars of your seq names are not unique\")\n\n    fw = must_open(alnfile, \"w\")\n    for rec in SeqIO.parse(inputfastafile, \"fasta\"):\n        a = alignment[rec.id[:maxi]]\n        fw.write(\">{0}\\n{1}\\n\".format(a.id[:maxi], a.seq))\n\n    fw.close()\n    sh(\"rm {0}.old\".format(alnfile), log=False)", "category": "Python"}, {"instruction": "def set_default_var(**vars):\n    \"\"\"Sets context variables using the key/value provided in the options\n    \"\"\"\n", "input": "", "output": "    for k, v in vars.iteritems():\n        current_context.vars.setdefault(k, v)", "category": "Python"}, {"instruction": "def open(self, mode=MODE_READ):\n        \"\"\"\n        Opens this repo in the specified mode.\n\n        TODO: figure out the correct semantics of this and document\n        the intended future behaviour as well as the current\n        transitional behaviour.\n        \"\"\"\n", "input": "", "output": "        if mode not in [MODE_READ, MODE_WRITE]:\n            error = \"Open mode must be '{}' or '{}'\".format(\n                MODE_READ, MODE_WRITE)\n            raise ValueError(error)\n        self._openMode = mode\n        if mode == MODE_READ:\n            self.assertExists()\n        if mode == MODE_READ:\n            # This is part of the transitional behaviour where\n            # we load the whole DB into memory to get access to\n            # the data model.\n            self.load()", "category": "Python"}, {"instruction": "def end_coordsys(self):\n        \"\"\"\n        Coordinate system at end of effect.\n\n        All axes are parallel to the original vector evaluation location, with\n        the origin moved to this effect's end point.\n\n        :return: coordinate system at end of effect\n        :rtype: :class:`CoordSys`\n        \"\"\"\n", "input": "", "output": "        coordsys = copy(self.location)\n        coordsys.origin = self.end_point\n        return coordsys", "category": "Python"}, {"instruction": "def set_extent_location(self, new_location, tag_location=None):\n        # type: (int, int) -> None\n        '''\n        A method to set the location of this UDF Terminating Descriptor.\n\n        Parameters:\n         new_location - The new extent this UDF Terminating Descriptor should be located at.\n         tag_location - The tag location to set for this UDF Terminator Descriptor.\n        Returns:\n         Nothing.\n        '''\n", "input": "", "output": "        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Terminating Descriptor not initialized')\n\n        self.new_extent_loc = new_location\n        if tag_location is None:\n            tag_location = new_location\n        self.desc_tag.tag_location = tag_location", "category": "Python"}, {"instruction": "def load_facts(self, facts):\n        \"\"\"Load a set of facts into the CLIPS data base.\n\n        The C equivalent of the CLIPS load-facts command.\n\n        Facts can be loaded from a string or from a text file.\n\n        \"\"\"\n", "input": "", "output": "        facts = facts.encode()\n\n        if os.path.exists(facts):\n            ret = lib.EnvLoadFacts(self._env, facts)\n            if ret == -1:\n                raise CLIPSError(self._env)\n        else:\n            ret = lib.EnvLoadFactsFromString(self._env, facts, -1)\n            if ret == -1:\n                raise CLIPSError(self._env)\n\n        return ret", "category": "Python"}, {"instruction": "def has_parent_books(self, book_id):\n        \"\"\"Tests if the ``Book`` has any parents.\n\n        arg:    book_id (osid.id.Id): a book ``Id``\n        return: (boolean) - ``true`` if the book has parents, f ``alse``\n                otherwise\n        raise:  NotFound - ``book_id`` is not found\n        raise:  NullArgument - ``book_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.BinHierarchySession.has_parent_bins\n        if self._catalog_session is not None:\n            return self._catalog_session.has_parent_catalogs(catalog_id=book_id)\n        return self._hierarchy_session.has_parents(id_=book_id)", "category": "Python"}, {"instruction": "def clear_masters(self):\n        \"\"\"Clear master packages if already exist in dependencies\n        or if added to install two or more times\n        \"\"\"\n", "input": "", "output": "        packages = []\n        for mas in Utils().remove_dbs(self.packages):\n            if mas not in self.dependencies:\n                packages.append(mas)\n        self.packages = packages", "category": "Python"}, {"instruction": "def updateEditorGeometry(self, editor, option, index):\n        \"\"\"Make sure the editor is the same size as the widget\n\n        By default it can get smaller because does not expand over viewport size.\n        This will make sure it will resize to the same size as the widget.\n\n        :param editor: the editor to update\n        :type editor: :class:`QtGui.QWidget`\n        :param option: the options for painting\n        :type option: QtGui.QStyleOptionViewItem\n        :param index: the index to paint\n        :type index: QtCore.QModelIndex\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        super(WidgetDelegate, self).updateEditorGeometry(editor, option, index)\n        editor.setGeometry(option.rect)\n        if self.keep_editor_size:\n            esh = editor.sizeHint()\n            osh = option.rect.size()\n            w = osh.width() if osh.width() > esh.width() else esh.width()\n            h = osh.height() if osh.height() > esh.height() else esh.height()\n            editor.resize(w, h)", "category": "Python"}, {"instruction": "def handler_context(self, type_, from_, cb, *, wildcard_resource=True):\n        \"\"\"\n        Context manager which temporarily registers a callback.\n\n        The arguments are the same as for :meth:`register_callback`.\n\n        When the context is entered, the callback `cb` is registered. When the\n        context is exited, no matter if an exception is raised or not, the\n        callback is unregistered.\n        \"\"\"\n", "input": "", "output": "        self.register_callback(\n            type_, from_, cb,\n            wildcard_resource=wildcard_resource\n        )\n        try:\n            yield\n        finally:\n            self.unregister_callback(\n                type_, from_,\n                wildcard_resource=wildcard_resource\n            )", "category": "Python"}, {"instruction": "def weld_str_get(array, i):\n    \"\"\"Retrieve character at index i.\n\n    Parameters\n    ----------\n    array : numpy.ndarray or WeldObject\n        Input data.\n    i : int\n        Index of character to retrieve. If greater than length of string, returns None.\n\n    Returns\n    -------\n    WeldObject\n        Representation of this computation.\n\n    \"\"\"\n", "input": "", "output": "    obj_id, weld_obj = create_weld_object(array)\n    index_literal = to_weld_literal(i, WeldLong())\n    missing_literal = default_missing_data_literal(WeldVec(WeldChar()))\n    missing_literal_id = get_weld_obj_id(weld_obj, missing_literal)\n\n    weld_template = ", "category": "Python"}, {"instruction": "def derivative(self, point=None):\n        \"\"\"Return the derivative operator.\n\n        The gradient is usually linear, but in case the 'constant'\n        ``pad_mode`` is used with nonzero ``pad_const``, the\n        derivative is given by the Gradient with ``pad_const=0``.\n\n        Parameters\n        ----------\n        point : `domain` element, optional\n            The point to take the derivative in. Does not change the result\n            since the operator is affine.\n        \"\"\"\n", "input": "", "output": "        if self.pad_mode == 'constant' and self.pad_const != 0:\n            return Gradient(self.domain, self.range, self.method,\n                            pad_mode=self.pad_mode,\n                            pad_const=0)\n        else:\n            return self", "category": "Python"}, {"instruction": "async def getUpdates(self,\n                         offset=None,\n                         limit=None,\n                         timeout=None,\n                         allowed_updates=None):\n        \"\"\" See: https://core.telegram.org/bots/api#getupdates \"\"\"\n", "input": "", "output": "        p = _strip(locals())\n        return await self._api_request('getUpdates', _rectify(p))", "category": "Python"}, {"instruction": "def set_record(self, name, record_id, record):\n        \"\"\"Save a record into the cache.\n\n        Args:\n            name (string): The name to save the model under.\n            record_id (int): The record id.\n            record (:class:`cinder_data.model.CinderModel`): The model\n        \"\"\"\n", "input": "", "output": "        if name not in self._cache:\n            self._cache[name] = {}\n        self._cache[name][record_id] = record", "category": "Python"}, {"instruction": "def scale(self, replicas):\n        \"\"\"\n        Scale service container.\n\n        Args:\n            replicas (int): The number of containers that should be running.\n\n        Returns:\n            bool: ``True`` if successful.\n        \"\"\"\n", "input": "", "output": "\n        if 'Global' in self.attrs['Spec']['Mode'].keys():\n            raise InvalidArgument('Cannot scale a global container')\n\n        service_mode = ServiceMode('replicated', replicas)\n        return self.client.api.update_service(self.id, self.version,\n                                              mode=service_mode,\n                                              fetch_current_spec=True)", "category": "Python"}, {"instruction": "def insert_many(objects, using=\"default\"):\n    \"\"\"Insert list of Django objects in one SQL query. Objects must be\n    of the same Django model. Note that save is not called and signals\n    on the model are not raised.\n\n    Mostly from: http://people.iola.dk/olau/python/bulkops.py\n    \"\"\"\n", "input": "", "output": "    if not objects:\n        return\n\n    import django.db.models\n    from django.db import connections\n    from django.db import transaction\n    con = connections[using]\n\n    model = objects[0].__class__\n    fields = [f for f in model._meta.fields\n              if not isinstance(f, django.db.models.AutoField)]\n    parameters = []\n    for o in objects:\n        params = tuple(f.get_db_prep_save(f.pre_save(o, True), connection=con)\n                       for f in fields)\n        parameters.append(params)\n\n    table = model._meta.db_table\n    column_names = \",\".join(con.ops.quote_name(f.column) for f in fields)\n    placeholders = \",\".join((\"%s\",) * len(fields))\n    con.cursor().executemany(\"insert into %s (%s) values (%s)\"\n                             % (table, column_names, placeholders), parameters)\n    transaction.commit_unless_managed(using=using)", "category": "Python"}, {"instruction": "def s2tc(s,base=25):\n    \"\"\"Converts seconds to timecode\"\"\"\n", "input": "", "output": "    try:\n        f = int(s*base)\n    except:\n        return \"--:--:--:--\"\n    hh  = int((f / base) / 3600)\n    hhd = int((hh % 24))\n    mm  = int(((f / base) / 60) - (hh*60))\n    ss  = int((f/base) - (hh*3600) - (mm*60))\n    ff  = int(f - (hh*3600*base) - (mm*60*base) - (ss*base))\n    return \"{:02d}:{:02d}:{:02d}:{:02d}\".format(hhd, mm, ss, ff)", "category": "Python"}, {"instruction": "def create_element(self, method, args=None):\n        \"\"\"\n        Evaluate a browser method and CSS selector against the document\n        (or an optional context DOMNode) and return a single\n        :class:`zombie.dom.DOMNode` object, e.g.,\n\n        browser._node('query', 'body > div')\n\n        ...roughly translates to the following Javascript...\n\n        browser.query('body > div')\n\n        :param method: the method (e.g., query) to call on the browser\n        :param selector: a string CSS selector\n                        (http://zombie.labnotes.org/selectors)\n        :param context: an (optional) instance of :class:`zombie.dom.DOMNode`\n        \"\"\"\n", "input": "", "output": "        if args is None:\n            arguments = ''\n        else:\n            arguments = \"(%s)\" % encode_args(args)\n        js = ", "category": "Python"}, {"instruction": "async def load_saved_device_info(self):\n        \"\"\"Load device information from the device info file.\"\"\"\n", "input": "", "output": "        _LOGGER.debug(\"Loading saved device info.\")\n        deviceinfo = []\n        if self._workdir:\n            _LOGGER.debug(\"Really Loading saved device info.\")\n            try:\n                device_file = '{}/{}'.format(self._workdir, DEVICE_INFO_FILE)\n                with open(device_file, 'r') as infile:\n                    try:\n                        deviceinfo = json.load(infile)\n                        _LOGGER.debug(\"Saved device file loaded\")\n                    except json.decoder.JSONDecodeError:\n                        _LOGGER.debug(\"Loading saved device file failed\")\n            except FileNotFoundError:\n                _LOGGER.debug(\"Saved device file not found\")\n        for device in deviceinfo:\n            self._add_saved_device_info(**device)", "category": "Python"}, {"instruction": "def get_new_version(current_version: str, level_bump: str) -> str:\n    \"\"\"\n    Calculates the next version based on the given bump level with semver.\n\n    :param current_version: The version the package has now.\n    :param level_bump: The level of the version number that should be bumped. Should be a `'major'`,\n                       `'minor'` or `'patch'`.\n    :return: A string with the next version number.\n    \"\"\"\n", "input": "", "output": "    debug('get_new_version(\"{}\", \"{}\")'.format(current_version, level_bump))\n    if not level_bump:\n        return current_version\n    return getattr(semver, 'bump_{0}'.format(level_bump))(current_version)", "category": "Python"}, {"instruction": "def notify(self, message):\n        \"\"\"Callback function which checks lines of output, tries to match\n        against regex defined in subclass's \"dispatch\" dict, and passes through\n        to a handler on match.\n        \"\"\"\n", "input": "", "output": "        for regex, handler in self.dispatch.items():\n            match = re.search(regex, message)\n            if match:\n                handler = getattr(self, handler)\n                return handler(match)", "category": "Python"}, {"instruction": "def insertTarget(self, name, path):\n        '''\n            Inserts a new target into the vault database\n            Returns the id of the created target\n        '''\n", "input": "", "output": "        sql = 'insert into {}(name, path) values (?,?);'.format(self.TABLE_ITEMS)\n\n        try:\n            _id = self.db.execute(sql, (name, path)).lastrowid\n            self.db.commit()\n\n            return _id\n        except sqlite3.IntegrityError:\n            return None", "category": "Python"}, {"instruction": "def _refresh_state(self):\n    \"\"\" Get the state of a job. If the job is complete this does nothing;\n        otherwise it gets a refreshed copy of the job resource.\n    \"\"\"\n", "input": "", "output": "    # TODO(gram): should we put a choke on refreshes? E.g. if the last call was less than\n    # a second ago should we return the cached value?\n    if self._is_complete:\n      return\n\n    try:\n      response = self._api.jobs_get(self._job_id)\n    except Exception as e:\n      raise e\n\n    if 'status' in response:\n      status = response['status']\n      if 'state' in status and status['state'] == 'DONE':\n        self._end_time = datetime.datetime.utcnow()\n        self._is_complete = True\n        self._process_job_status(status)\n\n    if 'statistics' in response:\n      statistics = response['statistics']\n      start_time = statistics.get('creationTime', None)\n      end_time = statistics.get('endTime', None)\n      if start_time and end_time and end_time >= start_time:\n        self._start_time = datetime.datetime.fromtimestamp(float(start_time) / 1000.0)\n        self._end_time = datetime.datetime.fromtimestamp(float(end_time) / 1000.0)", "category": "Python"}, {"instruction": "def save_hdf(self,filename,path=''):\n        \"\"\"Save to .h5 file.\n        \"\"\"\n", "input": "", "output": "        self.orbpop_long.save_hdf(filename,'{}/long'.format(path))\n        self.orbpop_short.save_hdf(filename,'{}/short'.format(path))", "category": "Python"}, {"instruction": "def hyphen(self):\n        '''\n        Returns ISBN number with segment hypenation\n        Data obtained from https://www.isbn-international.org/\n        https://www.isbn-international.org/export_rangemessage.xml\n        @return: ISBN formated as ISBN13 with hyphens\n        '''\n", "input": "", "output": "        if not ISBN.hyphenRange:\n            ISBN.hyphenRange = hyphen.ISBNRange()\n\n        return ISBN.hyphenRange.hyphenformat(self._id)", "category": "Python"}, {"instruction": "def handle_get(self):\n        \"\"\"Handles the HTTP GET request.\n\n        Interpret all HTTP GET requests as requests for server\n        documentation.\n        \"\"\"\n", "input": "", "output": "\n        response = self.generate_html_documentation().encode('utf-8')\n\n        print('Content-Type: text/html')\n        print('Content-Length: %d' % len(response))\n        print()\n        sys.stdout.flush()\n        sys.stdout.buffer.write(response)\n        sys.stdout.buffer.flush()", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"\n        Stop trapping WINCH signals and restore the previous WINCH handler.\n        \"\"\"\n", "input": "", "output": "\n        if self.original_handler is not None:\n            signal.signal(signal.SIGWINCH, self.original_handler)", "category": "Python"}, {"instruction": "def get_template(template):\n    \"\"\"Return a Jinja2 template by filename\n\n    Args:\n        template (str): Name of the template to return\n\n    Returns:\n        A Jinja2 Template object\n    \"\"\"\n", "input": "", "output": "    from cloud_inquisitor.database import db\n\n    tmpl = db.Template.find_one(template_name=template)\n    if not tmpl:\n        raise InquisitorError('No such template found: {}'.format(template))\n\n    tmplenv = Environment(loader=BaseLoader, autoescape=True)\n    tmplenv.filters['json_loads'] = json.loads\n    tmplenv.filters['slack_quote_join'] = lambda data: ', '.join('`{}`'.format(x) for x in data)\n\n    return tmplenv.from_string(tmpl.template)", "category": "Python"}, {"instruction": "def setItemStyle(self, itemStyle):\r\n        \"\"\"\r\n        Sets the item style that will be used for this widget.  If you are\r\n        trying to set a style on an item that has children, make sure to turn \r\n        off the useGroupStyleWithChildren option, or it will always display as\r\n        a group.\r\n        \r\n        :param      itemStyle | <XGanttWidgetItem.ItemStyle>\r\n        \"\"\"\n", "input": "", "output": "        self._itemStyle = itemStyle\r\n        \r\n        # initialize the group icon for group style\r\n        if itemStyle == XGanttWidgetItem.ItemStyle.Group and \\\r\n           self.icon(0).isNull():\r\n            ico = projexui.resources.find('img/folder_close.png')\r\n            expand_ico = projexui.resources.find('img/folder_open.png')\r\n            self.setIcon(0, QIcon(ico))\r\n            self.setExpandedIcon(0, QIcon(expand_ico))", "category": "Python"}, {"instruction": "def _searchable_form(self) -> 'Language':\n        \"\"\"\n        Convert a parsed language tag so that the information it contains is in\n        the best form for looking up information in the CLDR.\n        \"\"\"\n", "input": "", "output": "        if self._searchable is not None:\n            return self._searchable\n\n        self._searchable = self._filter_attributes(\n            {'language', 'script', 'region'}\n        ).simplify_script().prefer_macrolanguage()\n        return self._searchable", "category": "Python"}, {"instruction": "def main( gpu:Param(\"GPU to run on\", str)=None ):\n    \"\"\"Distrubuted training of CIFAR-10.\n    Fastest speed is if you run as follows:\n        python -m fastai.launch train_cifar.py\"\"\"\n", "input": "", "output": "    gpu = setup_distrib(gpu)\n    n_gpus = num_distrib()\n    path = url2path(URLs.CIFAR)\n    ds_tfms = ([*rand_pad(4, 32), flip_lr(p=0.5)], [])\n    workers = min(16, num_cpus()//n_gpus)\n    data = ImageDataBunch.from_folder(path, valid='test', ds_tfms=ds_tfms, bs=512//n_gpus,\n                                      num_workers=workers).normalize(cifar_stats)\n    learn = Learner(data, wrn_22(), metrics=accuracy)\n    if gpu is None: learn.model = nn.DataParallel(learn.model)\n    else: learn.to_distributed(gpu)\n    learn.to_fp16()\n    learn.fit_one_cycle(35, 3e-3, wd=0.4)", "category": "Python"}, {"instruction": "def read_label_file(path):\n    \"\"\"\n    Read the labels from an audacity label file.\n\n    Args:\n        path (str): Path to the label file.\n\n    Returns:\n        list: List of labels (start [sec], end [sec], label)\n\n    Example::\n\n        >>> read_label_file('/path/to/label/file.txt')\n        [\n            [0.0, 0.2, 'sie'],\n            [0.2, 2.2, 'hallo']\n        ]\n    \"\"\"\n", "input": "", "output": "    labels = []\n\n    for record in textfile.read_separated_lines_generator(path, separator='\\t', max_columns=3):\n        value = ''\n\n        if len(record) > 2:\n            value = str(record[2])\n\n        labels.append([float(_clean_time(record[0])), float(_clean_time(record[1])), value])\n\n    return labels", "category": "Python"}, {"instruction": "def write_default_config(self, filename):\n        \"\"\"Write the default config file.\n        \"\"\"\n", "input": "", "output": "        try:\n            with open(filename, 'wt') as file:\n                file.write(DEFAULT_CONFIG)\n            return True\n        except (IOError, OSError) as e:\n            print('Error writing %s: %s' % (filename, e.strerror or e), file=sys.stderr)\n            return False", "category": "Python"}, {"instruction": "def validate(self, document):\n        '''Check if the selected template exists.'''\n", "input": "", "output": "\n        template = document.text\n\n        if template not in self.builtin_templates:\n            raise ValidationError(\n                message=f'Template {template} not found. '\n                + f'Available templates are: {\", \".join(self.builtin_templates)}.',\n                cursor_position=0\n            )", "category": "Python"}, {"instruction": "def get_crypt_key(key_path):\n    \"\"\"\n    Get the user's PredixPy manifest key.  Generate and store one if not\n    yet generated.\n    \"\"\"\n", "input": "", "output": "    key_path = os.path.expanduser(key_path)\n    if os.path.exists(key_path):\n        with open(key_path, 'r') as data:\n            key = data.read()\n    else:\n        key = Fernet.generate_key()\n        with open(key_path, 'w') as output:\n            output.write(key)\n\n    return key", "category": "Python"}, {"instruction": "def check_bundler(self):\r\n        \"\"\"\r\n        Run the bundler check.\r\n        \"\"\"\n", "input": "", "output": "\r\n        def get_config(name):\r\n            return name if self.config('bundler.' + name) else ''\r\n\r\n        from pkg_resources import Requirement, resource_filename\r\n        relative_path = os.path.join('PyGitUp', 'check-bundler.rb')\r\n        bundler_script = resource_filename(Requirement.parse('git-up'),\r\n                                           relative_path)\r\n        assert os.path.exists(bundler_script), 'check-bundler.rb doesn\\'t ' \\\r\n                                               'exist!'\r\n\r\n        return_value = subprocess.call(\r\n            ['ruby', bundler_script, get_config('autoinstall'),\r\n             get_config('local'), get_config('rbenv')]\r\n        )\r\n\r\n        if self.testing:\r\n            assert return_value == 0, 'Errors while executing check-bundler.rb'", "category": "Python"}, {"instruction": "def _srels_for(phys_reader, source_uri):\n        \"\"\"\n        Return |_SerializedRelationships| instance populated with\n        relationships for source identified by *source_uri*.\n        \"\"\"\n", "input": "", "output": "        rels_xml = phys_reader.rels_xml_for(source_uri)\n        return _SerializedRelationships.load_from_xml(\n            source_uri.baseURI, rels_xml)", "category": "Python"}, {"instruction": "def list(path, filename=None, start=None, stop=None, recursive=False, directories=False):\n        \"\"\"\n        List files specified by dataPath.\n\n        Datapath may include a single wildcard ('*') in the filename specifier.\n\n        Returns sorted list of absolute path strings.\n        \"\"\"\n", "input": "", "output": "        path = uri_to_path(path)\n\n        if not filename and recursive:\n            return listrecursive(path)\n\n        if filename:\n            if os.path.isdir(path):\n                path = os.path.join(path, filename)\n            else:\n                path = os.path.join(os.path.dirname(path), filename)\n        else:\n            if os.path.isdir(path) and not directories:\n                path = os.path.join(path, \"*\")\n\n        files = glob.glob(path)\n\n        if not directories:\n            files = [fpath for fpath in files if not os.path.isdir(fpath)]\n\n        files.sort()\n        files = select(files, start, stop)\n        return files", "category": "Python"}, {"instruction": "def dif(a, b):\n    \"\"\" copy from http://stackoverflow.com/a/8545526 \"\"\"\n", "input": "", "output": "    return [i for i in range(len(a)) if a[i] != b[i]]", "category": "Python"}, {"instruction": "def percentile(p, arr=None):\n    \"\"\"Returns the pth percentile of the input array (the value that is at\n    least as great as p% of the values in the array).\n\n    If arr is not provided, percentile returns itself curried with p\n\n    >>> percentile(74.9, [1, 3, 5, 9])\n    5\n    >>> percentile(75, [1, 3, 5, 9])\n    5\n    >>> percentile(75.1, [1, 3, 5, 9])\n    9\n    >>> f = percentile(75)\n    >>> f([1, 3, 5, 9])\n    5\n    \"\"\"\n", "input": "", "output": "    if arr is None:\n        return lambda arr: percentile(p, arr)\n    if hasattr(p, '__iter__'):\n        return np.array([percentile(x, arr) for x in p])\n    if p == 0:\n        return min(arr)\n    assert 0 < p <= 100, 'Percentile requires a percent'\n    i = (p/100) * len(arr)\n    return sorted(arr)[math.ceil(i) - 1]", "category": "Python"}, {"instruction": "def from_taxdb(cls, con, root=None):\n        \"\"\"\n        Generate a TaxNode from a taxonomy database\n        \"\"\"\n", "input": "", "output": "        cursor = con.cursor()\n        if root is None:\n            cursor.execute(\n                \"SELECT tax_id, rank FROM nodes WHERE tax_id = parent_id\")\n        else:\n            cursor.execute(\n                \"SELECT tax_id, rank FROM nodes WHERE tax_id = ?\", [root])\n\n        tax_id, rank = cursor.fetchone()\n        root = cls(rank=rank, tax_id=tax_id)\n\n        def add_lineage(parent):\n            cursor.execute(", "category": "Python"}, {"instruction": "def connection_lost(self, exc):\n        \"\"\"Log when connection is closed, if needed call callback.\"\"\"\n", "input": "", "output": "        if exc:\n            log.exception('disconnected due to exception')\n        else:\n            log.info('disconnected because of close/abort.')\n        if self.disconnect_callback:\n            self.disconnect_callback(exc)", "category": "Python"}, {"instruction": "def _gen_get_more_command(cursor_id, coll, batch_size, max_await_time_ms):\n    \"\"\"Generate a getMore command document.\"\"\"\n", "input": "", "output": "    cmd = SON([('getMore', cursor_id),\n               ('collection', coll)])\n    if batch_size:\n        cmd['batchSize'] = batch_size\n    if max_await_time_ms is not None:\n        cmd['maxTimeMS'] = max_await_time_ms\n    return cmd", "category": "Python"}, {"instruction": "def _run_mpi_cmd(self, cmd):\n        \"\"\"\n        This runs the command you send in\n        \"\"\"\n", "input": "", "output": "        log(\"Number of Processes: {0}\".format(self.num_processors))\n        time_start = datetime.utcnow()\n\n        # Construct the taudem command line.\n        cmd = [self.mpiexec_path, '-n', str(self.num_processors)] + cmd\n        log(\"Command Line: {0}\".format(\" \".join(cmd)))\n        process = Popen(cmd, stdout=PIPE, stderr=PIPE, shell=False)\n        out, err = process.communicate()\n        if out:\n            log(\"OUTPUT:\")\n            for line in out.split(b'\\n'):\n                log(line)\n        if err:\n            log(err, severity=\"WARNING\")\n        log(\"Time to complete: {0}\".format(datetime.utcnow()-time_start))", "category": "Python"}, {"instruction": "def parse(cls, fptr, offset, length):\n        \"\"\"Parse JPX free box.\n\n        Parameters\n        ----------\n        f : file\n            Open file object.\n        offset : int\n            Start position of box in bytes.\n        length : int\n            Length of the box in bytes.\n\n        Returns\n        -------\n        FreeBox\n            Instance of the current free box.\n        \"\"\"\n", "input": "", "output": "        # Must seek to end of box.\n        nbytes = offset + length - fptr.tell()\n        fptr.read(nbytes)\n        return cls(length=length, offset=offset)", "category": "Python"}, {"instruction": "def delete(self):\n        \"\"\"\n            Destructor.\n        \"\"\"\n", "input": "", "output": "\n        if self.glucose:\n            pysolvers.glucose3_del(self.glucose)\n            self.glucose = None\n\n            if self.prfile:\n                self.prfile.close()", "category": "Python"}, {"instruction": "def _poster_frame_rId(self):\n        \"\"\"Return the rId of relationship to poster frame image.\n\n        The poster frame is the image used to represent the video before it's\n        played.\n        \"\"\"\n", "input": "", "output": "        _, poster_frame_rId = self._slide_part.get_or_add_image_part(\n            self._poster_frame_image_file\n        )\n        return poster_frame_rId", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"Terminate audio processing.\n\n        This waits until all pending audio buffers have been played\n        before it returns. If successful, the stream is considered\n        inactive.\n\n        \"\"\"\n", "input": "", "output": "        err = _pa.Pa_StopStream(self._stream)\n        if err == _pa.paStreamIsStopped:\n            return\n        self._handle_error(err)", "category": "Python"}, {"instruction": "def find(self, collection, query):\n        \"\"\"\n        Search a collection for the query provided. Just a raw interface to\n        mongo to do any query you want.\n\n        Args:\n            collection: The db collection. See main class documentation.\n            query: A mongo find query.\n        Returns:\n            pymongo Cursor object with the results.\n        \"\"\"\n", "input": "", "output": "        obj = getattr(self.db, collection)\n        result = obj.find(query)\n        return result", "category": "Python"}, {"instruction": "def set_trim_user(self, trim):\n        \"\"\" Sets 'trim_user' parameter. When set to True, \\\n        each tweet returned in a timeline will include a \\\n        user object including only the status authors numerical ID\n\n        :param trim: Boolean triggering the usage of the parameter\n        :raises: TwitterSearchException\n        \"\"\"\n", "input": "", "output": "\n        if not isinstance(trim, bool):\n            raise TwitterSearchException(1008)\n        self.arguments.update({'trim_user': 'true' if trim else 'false'})", "category": "Python"}, {"instruction": "def author(self):\n        \"\"\"\n        Returns the author to whom the work is attributed.\n\n        :return: an instance of `HucitWork` # TODO: check that's the case\n        \"\"\"\n", "input": "", "output": "        CreationEvent = self.session.get_class(surf.ns.EFRBROO['F27_Work_Conception'])\n        Person = self.session.get_class(surf.ns.EFRBROO['F10_Person'])\n        creation_event =  CreationEvent.get_by(efrbroo_R16_initiated=self).first()\n        return Person.get_by(efrbroo_P14i_performed = creation_event).first()", "category": "Python"}, {"instruction": "def __load_pst(self):\n        \"\"\"private method set the pst attribute\n        \"\"\"\n", "input": "", "output": "        if self.pst_arg is None:\n            return None\n        if isinstance(self.pst_arg, Pst):\n            self.__pst = self.pst_arg\n            return self.pst\n        else:\n            try:\n                self.log(\"loading pst: \" + str(self.pst_arg))\n                self.__pst = Pst(self.pst_arg)\n                self.log(\"loading pst: \" + str(self.pst_arg))\n                return self.pst\n            except Exception as e:\n                raise Exception(\"linear_analysis.__load_pst(): error loading\"+\\\n                                \" pest control from argument: \" +\n                                str(self.pst_arg) + '\\n->' + str(e))", "category": "Python"}, {"instruction": "def create(self, ipv6s):\n        \"\"\"\n        Method to create ipv6's\n\n        :param ipv6s: List containing vrf desired to be created on database\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        data = {'ips': ipv6s}\n        return super(ApiIPv6, self).post('api/v3/ipv6/', data)", "category": "Python"}, {"instruction": "def set(self, addr, values):\n        \"\"\"\n        Write list ``values`` to ``addr`` memory location in DataBank.\n\n        :param addr: Address to write to\n        :param values: list of values to write\n        :except IndexError: Raised if address range falls outside valid range\n        \"\"\"\n", "input": "", "output": "        addr -= self._start_addr\n        end = addr + len(values)\n        if not 0 <= addr <= end <= len(self._data):\n            addr += self._start_addr\n            raise IndexError(\"Invalid address range [{:#06x} - {:#06x}]\"\n                             .format(addr, addr + len(values)))\n        self._data[addr:end] = values", "category": "Python"}, {"instruction": "def _parse_acl_config(self, acl_config):\n        \"\"\"Parse configured ACLs and rules\n\n        ACLs are returned as a dict of rule sets:\n        {<eos_acl1_name>: set([<eos_acl1_rules>]),\n         <eos_acl2_name>: set([<eos_acl2_rules>]),\n         ...,\n        }\n        \"\"\"\n", "input": "", "output": "        parsed_acls = dict()\n        for acl in acl_config['aclList']:\n            parsed_acls[acl['name']] = set()\n            for rule in acl['sequence']:\n                parsed_acls[acl['name']].add(rule['text'])\n        return parsed_acls", "category": "Python"}, {"instruction": "def derive(self, modifier):\n        \"\"\"\n        Returns a new :class:`Event` instance that will fire\n        when this event fires. The value passed to the callbacks\n        to the new event is the return value of the given\n        `modifier` function which is passed the original value.\n        \"\"\"\n", "input": "", "output": "        def forward(value):\n            changed_value = modifier(value)\n            derived.fire(changed_value)\n        \n        derived = Event()\n        self.add_callback(forward)\n        return derived", "category": "Python"}, {"instruction": "def write_hier(self, GO_id, out=sys.stdout,\n                       len_dash=1, max_depth=None, num_child=None, short_prt=False,\n                       include_only=None, go_marks=None):\n        \"\"\"Write hierarchy for a GO Term.\"\"\"\n", "input": "", "output": "        gos_printed = set()\n        self[GO_id].write_hier_rec(gos_printed, out, len_dash, max_depth, num_child,\n            short_prt, include_only, go_marks)", "category": "Python"}, {"instruction": "def enable_autocuts(self, option):\n        \"\"\"Set ``autocuts`` behavior.\n\n        Parameters\n        ----------\n        option : {'on', 'override', 'once', 'off'}\n            Option for auto-cut behavior. A list of acceptable options can\n            also be obtained by :meth:`get_autocuts_options`.\n\n        Raises\n        ------\n        ginga.ImageView.ImageViewError\n            Invalid option.\n\n        \"\"\"\n", "input": "", "output": "        option = option.lower()\n        assert(option in self.autocuts_options), \\\n            ImageViewError(\"Bad autocuts option '%s': must be one of %s\" % (\n                str(self.autocuts_options)))\n        self.t_.set(autocuts=option)", "category": "Python"}, {"instruction": "def send_screen_to_connection(self, screen):\n        \"\"\"\n        Actually used for Curses\n        :param screen:\n        :return:void\n        \"\"\"\n", "input": "", "output": "        display_who_run_core = self.get_connection_who_have_to_run_core()\n        if not display_who_run_core:\n            raise Exception('Need Terminal object to do that')\n        display_who_run_core.initialize_screen(screen)", "category": "Python"}, {"instruction": "def assemble(cls, header_json, metadata_json, content_json):\n        ''' Creates a new message, assembled from JSON fragments.\n\n        Args:\n            header_json (``JSON``) :\n\n            metadata_json (``JSON``) :\n\n            content_json (``JSON``) :\n\n        Returns:\n            Message subclass\n\n        Raises:\n            MessageError\n\n        '''\n", "input": "", "output": "\n        try:\n            header = json_decode(header_json)\n        except ValueError:\n            raise MessageError(\"header could not be decoded\")\n\n        try:\n            metadata = json_decode(metadata_json)\n        except ValueError:\n            raise MessageError(\"metadata could not be decoded\")\n\n        try:\n            content = json_decode(content_json)\n        except ValueError:\n            raise MessageError(\"content could not be decoded\")\n\n        msg = cls(header, metadata, content)\n\n        msg._header_json = header_json\n        msg._metadata_json = metadata_json\n        msg._content_json = content_json\n\n        return msg", "category": "Python"}, {"instruction": "def _update(self):\n        \"\"\"Rebuilds the shaders, and repositions the objects\n           that are used internally by the ColorBarVisual\n        \"\"\"\n", "input": "", "output": "        self._colorbar.halfdim = self._halfdim\n        self._border.halfdim = self._halfdim\n\n        self._label.text = self._label_str\n        self._ticks[0].text = str(self._clim[0])\n        self._ticks[1].text = str(self._clim[1])\n\n        self._update_positions()\n\n        self._colorbar._update()\n        self._border._update()", "category": "Python"}, {"instruction": "def clean(self):\n        \"\"\" Cleans the data and throws ValidationError on failure \"\"\"\n", "input": "", "output": "        errors = {}\n        cleaned = {}\n\n        for name, validator in self.validate_schema.items():\n            val = getattr(self, name, None)\n            try:\n                cleaned[name] = validator.to_python(val)\n            except formencode.api.Invalid, err:\n                errors[name] = err\n\n        if errors:\n            raise ValidationError('Invalid data', errors)\n        return cleaned", "category": "Python"}, {"instruction": "def isConnected(self, fromName, toName):\n        \"\"\" Are these two layers connected this way? \"\"\"\n", "input": "", "output": "        for c in self.connections:\n            if (c.fromLayer.name == fromName and\n                c.toLayer.name == toName):\n                return 1\n        return 0", "category": "Python"}, {"instruction": "def register_all_in_module(module_instance, exclude_name_list=[], admin_class_list=None):\n    \"\"\"\n    :param module_instance: mostly the models module\n    :param exclude_name_list: class does not need to register or is already registered\n    :param admin_class_list:\n    :return:\n    \"\"\"\n", "input": "", "output": "    class_list = []\n    for name, obj in inspect.getmembers(module_instance):\n        if inspect.isclass(obj):\n            if obj.__name__ in exclude_name_list:\n                continue\n            class_list.append(obj)\n    #print class_list, admin_class_list\n    register_all(class_list, admin_class_list)", "category": "Python"}, {"instruction": "def construct_1d_object_array_from_listlike(values):\n    \"\"\"\n    Transform any list-like object in a 1-dimensional numpy array of object\n    dtype.\n\n    Parameters\n    ----------\n    values : any iterable which has a len()\n\n    Raises\n    ------\n    TypeError\n        * If `values` does not have a len()\n\n    Returns\n    -------\n    1-dimensional numpy array of dtype object\n    \"\"\"\n", "input": "", "output": "    # numpy will try to interpret nested lists as further dimensions, hence\n    # making a 1D array that contains list-likes is a bit tricky:\n    result = np.empty(len(values), dtype='object')\n    result[:] = values\n    return result", "category": "Python"}, {"instruction": "def url_read_text(url, verbose=True):\n    r\"\"\"\n    Directly reads text data from url\n    \"\"\"\n", "input": "", "output": "    data = url_read(url, verbose)\n    text = data.decode('utf8')\n    return text", "category": "Python"}, {"instruction": "def sgd(grad, x, callback=None, num_iters=200, step_size=0.1, mass=0.9):\n    \"\"\"Stochastic gradient descent with momentum.\n    grad() must have signature grad(x, i), where i is the iteration number.\"\"\"\n", "input": "", "output": "    velocity = np.zeros(len(x))\n    for i in range(num_iters):\n        g = grad(x, i)\n        if callback: callback(x, i, g)\n        velocity = mass * velocity - (1.0 - mass) * g\n        x = x + step_size * velocity\n    return x", "category": "Python"}, {"instruction": "def file_handler(self, handler_type, path, prefixed_path, source_storage):\n        \"\"\"\n        Create a dict with all kwargs of the `copy_file` or `link_file` method of the super class and add it to\n        the queue for later processing.\n        \"\"\"\n", "input": "", "output": "        if self.faster:\n            if prefixed_path not in self.found_files:\n                self.found_files[prefixed_path] = (source_storage, path)\n\n            self.task_queue.put({\n                'handler_type': handler_type,\n                'path': path,\n                'prefixed_path': prefixed_path,\n                'source_storage': source_storage\n            })\n            self.counter += 1\n        else:\n            if handler_type == 'link':\n                super(Command, self).link_file(path, prefixed_path, source_storage)\n            else:\n                super(Command, self).copy_file(path, prefixed_path, source_storage)", "category": "Python"}, {"instruction": "def default_52xhandler(response, resource, url, params):\n    \"\"\"\n    Default 52x handler that loops every second until a non 52x response is received.\n    :param response: The response of the last executed api request.\n    :param resource: The resource of the last executed api request.\n    :param url: The url of the last executed api request sans encoded query parameters.\n    :param params: The query params of the last executed api request in dictionary format.\n    \"\"\"\n", "input": "", "output": "    time.sleep(1)\n    return resource.execute(url, params)", "category": "Python"}, {"instruction": "def _patch_expand_path(self, settings, name, value):\n        \"\"\"\n        Patch a path to expand home directory and make absolute path.\n\n        Args:\n            settings (dict): Current settings.\n            name (str): Setting name.\n            value (str): Path to patch.\n\n        Returns:\n            str: Patched path to an absolute path.\n\n        \"\"\"\n", "input": "", "output": "        if os.path.isabs(value):\n            return os.path.normpath(value)\n\n        # Expand home directory if any\n        value = os.path.expanduser(value)\n\n        # If the path is not yet an absolute directory, make it so from base\n        # directory if not empty\n        if not os.path.isabs(value) and self.projectdir:\n            value = os.path.join(self.projectdir, value)\n\n        return os.path.normpath(value)", "category": "Python"}, {"instruction": "def process(self, input_data, topic=None, **kwargs):\n        \"\"\"\n        Splits tuple received from PacketHandler into packet UID and packet message.\n        Decodes packet and inserts into database backend.\n        Logs any exceptions raised.\n\n        Params:\n            input_data:  message received from inbound stream through PacketHandler\n            topic:       name of inbound stream message received from\n            **kwargs:    any args required for connected to the backend\n        \"\"\"\n", "input": "", "output": "        try:\n            split = input_data[1:-1].split(',', 1)\n            uid, pkt = int(split[0]), split[1]\n            defn = self.packet_dict[uid]\n            decoded = tlm.Packet(defn, data=bytearray(pkt))\n            self.dbconn.insert(decoded, **kwargs)\n        except Exception as e:\n            log.error('Data archival failed with error: {}.'.format(e))", "category": "Python"}, {"instruction": "def _stop(self):\n        \"\"\"\n        Stops the instantiation queue (called by its bundle activator)\n        \"\"\"\n", "input": "", "output": "        # Unregisters the iPOPO service listener\n        self.__context.remove_service_listener(self)\n\n        try:\n            # Try to register to factory events\n            with use_ipopo(self.__context) as ipopo:\n                ipopo.remove_listener(self)\n        except BundleException:\n            # Service not present anymore\n            pass", "category": "Python"}, {"instruction": "def parang (hourangle, declination, latitude):\n    \"\"\"Calculate the parallactic angle of a sky position.\n\n    This computes the parallactic angle of a sky position expressed in terms\n    of an hour angle and declination. Arguments:\n\n    hourangle\n      The hour angle of the location on the sky.\n    declination\n      The declination of the location on the sky.\n    latitude\n      The latitude of the observatory.\n\n    Inputs and outputs are all in radians. Implementation adapted from GBTIDL\n    ``parangle.pro``.\n\n    \"\"\"\n", "input": "", "output": "    return -np.arctan2 (-np.sin (hourangle),\n                        np.cos (declination) * np.tan (latitude)\n                        - np.sin (declination) * np.cos (hourangle))", "category": "Python"}, {"instruction": "def login(ctx):\n    \"\"\"Add an API key (saved in ~/.onecodex)\"\"\"\n", "input": "", "output": "    base_url = os.environ.get(\"ONE_CODEX_API_BASE\", \"https://app.onecodex.com\")\n    if not ctx.obj[\"API_KEY\"]:\n        _login(base_url)\n    else:\n        email = _login(base_url, api_key=ctx.obj[\"API_KEY\"])\n        ocx = Api(api_key=ctx.obj[\"API_KEY\"], telemetry=ctx.obj[\"TELEMETRY\"])\n\n        # TODO: This should be protected or built in as a first class resource\n        # with, e.g., connection error catching (it's not part of our formally documeted API at the moment)\n        if ocx._client.Account.instances()[\"email\"] != email:\n            click.echo(\"Your login credentials do not match the provided email!\", err=True)\n            _remove_creds()\n            ctx.exit(1)", "category": "Python"}, {"instruction": "def place_notes_at(self, notes, at):\n        \"\"\"Place notes at the given index.\"\"\"\n", "input": "", "output": "        for x in self.bar:\n            if x[0] == at:\n                x[0][2] += notes", "category": "Python"}, {"instruction": "def load_app(self, app):\n        \"\"\"\n        Tries to load an initial data class for a specified app. If the specified file does not exist,\n        an error will be raised. If the class does exist, but it isn't a subclass of `BaseInitialData`\n        then None will be returned.\n        :param app: The name of the app in which to load the initial data class. This should be the same\n            path as defined in settings.INSTALLED_APPS\n        :type app: str\n        :return: A subclass instance of BaseInitialData or None\n        :rtype: BaseInitialData or None\n        \"\"\"\n", "input": "", "output": "        if self.loaded_apps.get(app):\n            return self.loaded_apps.get(app)\n\n        self.loaded_apps[app] = None\n        initial_data_class = import_string(self.get_class_path(app))\n        if issubclass(initial_data_class, BaseInitialData):\n            self.log('Loaded app {0}'.format(app))\n            self.loaded_apps[app] = initial_data_class\n\n        return self.loaded_apps[app]", "category": "Python"}, {"instruction": "def pdb(self):\n        \"\"\"Start the python debugger\n\n        Calling pdb won't do anything in a multithread context\n        \"\"\"\n", "input": "", "output": "        if self.embed_disabled:\n            self.warning_log(\"Pdb is disabled when runned from the grid runner because of the multithreading\")  # noqa\n            return False\n\n        if BROME_CONFIG['runner']['play_sound_on_pdb']:\n            say(BROME_CONFIG['runner']['sound_on_pdb'])\n\n        set_trace()", "category": "Python"}, {"instruction": "def unreserve_resources(role):\n    \"\"\" Unreserves all the resources for all the slaves for the role.\n    \"\"\"\n", "input": "", "output": "    state = dcos_agents_state()\n    if not state or 'slaves' not in state.keys():\n        return False\n    all_success = True\n    for agent in state['slaves']:\n        if not unreserve_resource(agent, role):\n            all_success = False\n    return all_success", "category": "Python"}, {"instruction": "def print_plugin_args(plugin_path):\n    \"\"\"Print plugin parameters table.\"\"\"\n", "input": "", "output": "    args = config_utils.get_config_parameters(plugin_path)\n    args_format = \"{:20} {:10} {:^15} {:^10} {:25}\"\n    title = args_format.format(defs.NAME.upper(), defs.TYPE.upper(), defs.DEFAULT.upper(),\n                               defs.REQUIRED.upper(), defs.DESCRIPTION.upper())\n    click.secho(title)\n    click.secho(\"-\" * len(title))\n    for arg in args:\n        help_text = \" ({})\".format(arg[defs.HELP_TEXT]) if defs.HELP_TEXT in arg else \"\"\n        options = _parse_select_options(arg)\n        description = arg[defs.LABEL] + options + help_text\n        click.secho(args_format.format(arg[defs.VALUE], arg[defs.TYPE], str(arg.get(defs.DEFAULT, None)),\n                                       str(arg.get(defs.REQUIRED, False)), description))", "category": "Python"}, {"instruction": "def connect(self):\n        \"\"\"Connect to the Redis server if necessary.\n\n        :rtype: :class:`~tornado.concurrent.Future`\n        :raises: :class:`~tredis.exceptions.ConnectError`\n                 :class:`~tredis.exceptinos.RedisError`\n\n        \"\"\"\n", "input": "", "output": "        future = concurrent.Future()\n\n        if self.connected:\n            raise exceptions.ConnectError('already connected')\n\n        LOGGER.debug('%s connecting', self.name)\n        self.io_loop.add_future(\n            self._client.connect(self.host, self.port),\n            lambda f: self._on_connected(f, future))\n        return future", "category": "Python"}, {"instruction": "def _mount_resources(self):\n        '''Mount all registered resources onto the application.'''\n", "input": "", "output": "        rules = []\n        self.callback_map = {}\n        for ep in Resource:\n            for rule, callback in ep.get_routing_tuples():\n                log.debug('Path \"{}\" mapped to \"{}\"'.format(\n                    rule.rule, rule.endpoint))\n                rules.append(rule)\n                self.callback_map[rule.endpoint] = callback\n        self.url_map = Map(rules)", "category": "Python"}, {"instruction": "def get_bound(pts):\n    \"\"\"Compute a minimal rectangle that covers all the points.\"\"\"\n", "input": "", "output": "    (x0, y0, x1, y1) = (INF, INF, -INF, -INF)\n    for (x, y) in pts:\n        x0 = min(x0, x)\n        y0 = min(y0, y)\n        x1 = max(x1, x)\n        y1 = max(y1, y)\n    return (x0, y0, x1, y1)", "category": "Python"}, {"instruction": "def ListClientsForKeywords(self, keywords, start_time=None, cursor=None):\n    \"\"\"Lists the clients associated with keywords.\"\"\"\n", "input": "", "output": "    keywords = set(keywords)\n    hash_to_kw = {mysql_utils.Hash(kw): kw for kw in keywords}\n    result = {kw: [] for kw in keywords}\n\n    query = ", "category": "Python"}, {"instruction": "def get_dimension_index(self, name, value):\n        \"\"\"Converts a dimension ID string and a categody ID string into the \\\n           numeric index of that category in that dimension\n        Args:\n           name(string): ID string of the dimension.\n           value(string): ID string of the category.\n\n        Returns:\n           ndx[value](int): index of the category in the dimension.\n\n        \"\"\"\n", "input": "", "output": "\n        if 'index' not in self.get('dimension', {}). \\\n                get(name, {}).get('category', {}):\n            return 0\n        ndx = self['dimension'][name]['category']['index']\n\n        if isinstance(ndx, list):\n            return ndx.index(value)\n        else:\n            return ndx[value]", "category": "Python"}, {"instruction": "def swipe(self):\n        '''\n        Perform swipe action. if device platform greater than API 18, percent can be used and value between 0 and 1\n        Usages:\n        d().swipe.right()\n        d().swipe.left(steps=10)\n        d().swipe.up(steps=10)\n        d().swipe.down()\n        d().swipe(\"right\", steps=20)\n        d().swipe(\"right\", steps=20, percent=0.5)\n        '''\n", "input": "", "output": "        @param_to_property(direction=[\"up\", \"down\", \"right\", \"left\"])\n        def _swipe(direction=\"left\", steps=10, percent=1):\n            if percent == 1:\n                return self.jsonrpc.swipe(self.selector, direction, steps)\n            else:\n                return self.jsonrpc.swipe(self.selector, direction, percent, steps)\n        return _swipe", "category": "Python"}, {"instruction": "def _maybe_wait_for_initializing_instance(instance):\n  \"\"\"Starts instance if it's stopped, no-op otherwise.\"\"\"\n", "input": "", "output": "\n  if not instance:\n    return\n\n  if instance.state['Name'] == 'initializing':\n    while True:\n      print(f\"Waiting  for {instance} to leave state 'initializing'.\")\n      instance.reload()\n      if instance.state['Name'] == 'running':\n        break\n      time.sleep(10)", "category": "Python"}, {"instruction": "def cmyk(c, m, y, k):\n    \"\"\"\n    Create a spectra.Color object in the CMYK color space.\n\n    :param float c: c coordinate.\n    :param float m: m coordinate.\n    :param float y: y coordinate.\n    :param float k: k coordinate.\n\n    :rtype: Color\n    :returns: A spectra.Color object in the CMYK color space.\n    \"\"\"\n", "input": "", "output": "    return Color(\"cmyk\", c, m, y, k)", "category": "Python"}, {"instruction": "def by_type(self, type_name):\n        '''\n        Return an iterator of doc_ids of the documents of the\n        specified type.\n        '''\n", "input": "", "output": "        if IRestorator.providedBy(type_name):\n            type_name = type_name.type_name\n        return (x[1] for x in self._links if x[0] == type_name)", "category": "Python"}, {"instruction": "def workspaces(self):\n        \"\"\"\n        :rtype: twilio.rest.taskrouter.v1.workspace.WorkspaceList\n        \"\"\"\n", "input": "", "output": "        if self._workspaces is None:\n            self._workspaces = WorkspaceList(self)\n        return self._workspaces", "category": "Python"}, {"instruction": "def CB(self):\n        '''\n        Vertices C and B, list.\n\n        '''\n", "input": "", "output": "        try:\n            return self._CB\n        except AttributeError:\n            pass\n        self._CB = [self.C, self.B]\n        return self._CB", "category": "Python"}, {"instruction": "def child_regions(self):\n        \"\"\"This generator generates the list of direct child regions.\"\"\"\n", "input": "", "output": "        start, end = self.get_range()\n        block = self._trigger.next()\n        ref_lvl = self.scope_level\n        while block.blockNumber() <= end and block.isValid():\n            lvl = TextBlockHelper.get_fold_lvl(block)\n            trigger = TextBlockHelper.is_fold_trigger(block)\n            if lvl == ref_lvl and trigger:\n                yield FoldScope(block)\n            block = block.next()", "category": "Python"}, {"instruction": "def add(self, resource):\n        \"\"\"Add a resource change or an iterable collection of them.\n\n        Allows multiple resource_change objects for the same\n        resource (ie. URI) and preserves the order of addition.\n        \"\"\"\n", "input": "", "output": "        if isinstance(resource, collections.Iterable):\n            for r in resource:\n                self.add_if_changed(r)\n        else:\n            self.add_if_changed(resource)", "category": "Python"}, {"instruction": "def get_namespace_preorder_burn_info( outputs ):\n    \"\"\"\n    Given the set of outputs, find the fee sent \n    to our burn address.\n    \n    Return the fee and burn address on success as {'op_fee': ..., 'burn_address': ...}\n    Return None if not found\n    \"\"\"\n", "input": "", "output": "    if len(outputs) < 3:\n        # not a well-formed preorder \n        return None \n   \n    op_fee = outputs[2]['value']\n    burn_address = None\n\n    try:\n        burn_address = virtualchain.script_hex_to_address(outputs[2]['script'])\n        assert burn_address\n    except:\n        log.warning(\"Invalid burn script: {}\".format(outputs[2]['script']))\n        return None\n\n    return {'op_fee': op_fee, 'burn_address': burn_address}", "category": "Python"}, {"instruction": "def attachment_upload(instance, filename):\n    \"\"\"Stores the attachment in a \"per module/appname/primary key\" folder\"\"\"\n", "input": "", "output": "    return 'attachments/{app}_{model}/{pk}/{filename}'.format(\n        app=instance.content_object._meta.app_label,\n        model=instance.content_object._meta.object_name.lower(),\n        pk=instance.content_object.pk,\n        filename=filename,\n    )", "category": "Python"}, {"instruction": "def log(self, ctx='all'):\n    \"\"\"\n    Gets the build log output.\n\n    :param ctx: specifies which log message to show, it can be 'validate', 'build' or 'all'.\n    \"\"\"\n", "input": "", "output": "    path = '%s/%s.log' % (self.path, ctx)\n    if os.path.exists(path) is True:\n      with open(path, 'r') as f:\n        print(f.read())\n      return\n    validate_path = '%s/validate.log' % self.path\n    build_path = '%s/build.log' % self.path\n    out = []\n    with open(validate_path) as validate_log, open(build_path) as build_log:\n      for line in validate_log.readlines():\n        out.append(line)\n      for line in build_log.readlines():\n        out.append(line)\n    print(''.join(out))", "category": "Python"}, {"instruction": "def elasprep(self):\n    \"\"\"\n    dx4, dy4, dx2dy2, D = elasprep(dx,dy,Te,E=1E11,nu=0.25)\n    \n    Defines the variables that are required to create the 2D finite \n    difference solution coefficient matrix\n    \"\"\"\n", "input": "", "output": "    \n    if self.Method != 'SAS_NG':\n      self.dx4 = self.dx**4\n      self.dy4 = self.dy**4\n      self.dx2dy2 = self.dx**2 * self.dy**2\n    self.D = self.E*self.Te**3/(12*(1-self.nu**2))", "category": "Python"}, {"instruction": "def run(entry_point, drivers, loop = None):\n    ''' This is a runner wrapping the cyclotron \"run\" implementation. It takes\n    an additional parameter to provide a custom asyncio mainloop.\n    '''\n", "input": "", "output": "    program = setup(entry_point, drivers)\n    dispose = program.run()\n    if loop == None:\n        loop = asyncio.get_event_loop()\n\n    loop.run_forever()\n    dispose()", "category": "Python"}, {"instruction": "def axpy(x, y, a=1.0):\n    \"\"\"Quick level-1 call to BLAS y = a*x+y.\n\n    Parameters\n    ----------\n    x : array_like\n        nx1 real or complex vector\n    y : array_like\n        nx1 real or complex vector\n    a : float\n        real or complex scalar\n\n    Returns\n    -------\n    y : array_like\n        Input variable y is rewritten\n\n    Notes\n    -----\n    The call to get_blas_funcs automatically determines the prefix for the blas\n    call.\n\n    \"\"\"\n", "input": "", "output": "    from scipy.linalg import get_blas_funcs\n\n    fn = get_blas_funcs(['axpy'], [x, y])[0]\n    fn(x, y, a)", "category": "Python"}, {"instruction": "def levenshtein(left, right):\n    \"\"\"Computes the Levenshtein distance of the two given strings.\n\n    >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n    >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n    [Row(d=3)]\n    \"\"\"\n", "input": "", "output": "    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.levenshtein(_to_java_column(left), _to_java_column(right))\n    return Column(jc)", "category": "Python"}, {"instruction": "def by_youtube_id(cls, youtube_id):\n        \"\"\"\n        Look up video by youtube id\n        \"\"\"\n", "input": "", "output": "        qset = cls.objects.filter(\n            encoded_videos__profile__profile_name='youtube',\n            encoded_videos__url=youtube_id\n        ).prefetch_related('encoded_videos', 'courses')\n        return qset", "category": "Python"}, {"instruction": "def locate(self, pattern):\n        '''Find sequences matching a pattern.\n\n        :param pattern: Sequence for which to find matches.\n        :type pattern: str\n        :returns: Indices of pattern matches.\n        :rtype: list of ints\n\n        '''\n", "input": "", "output": "        if len(pattern) > len(self):\n            raise ValueError('Search pattern longer than searchable ' +\n                             'sequence.')\n        seq = self.seq\n\n        pattern = str(pattern).upper()\n        re_pattern = '(?=' + pattern + ')'\n        matches = [index.start() % len(self) for index in\n                   re.finditer(re_pattern, seq)]\n\n        return matches", "category": "Python"}, {"instruction": "def default_links_pagination_factory(page, urlkwargs):\n    \"\"\"Factory for record links generation.\"\"\"\n", "input": "", "output": "    endpoint = '.communities_list'\n\n    links = {\n        'self': url_for(endpoint, page=page.page, _external=True, **urlkwargs),\n    }\n\n    if page.has_prev:\n        links['prev'] = url_for(endpoint, page=page.prev_num, _external=True,\n                                **urlkwargs)\n    if page.has_next:\n        links['next'] = url_for(endpoint, page=page.next_num, _external=True,\n                                **urlkwargs)\n\n    return links", "category": "Python"}, {"instruction": "def load_pid(pidfile):\n    \"\"\"read pid from pidfile.\n    \"\"\"\n", "input": "", "output": "    if pidfile and os.path.isfile(pidfile):\n        with open(pidfile, \"r\", encoding=\"utf-8\") as fobj:\n            return int(fobj.readline().strip())\n    return 0", "category": "Python"}, {"instruction": "def get_file_contents(path, binary=False):\n    \"\"\"\n    Return the contents of the text file at path.\n    If it is a binary file,raise an IOError\n    \"\"\"\n", "input": "", "output": "    # if this isn't a text file, we should raise an IOError\n    f = open(path, 'r')\n    file_contents = f.read()\n    f.close()\n    if not binary and file_contents.find('\\000') >= 0:\n        raise IOError('Expected text file, got binary file')\n    return file_contents", "category": "Python"}, {"instruction": "def setup_rules_file():\n    \"\"\"\n    Copy the udev rules file for Opentrons Modules to opentrons_data directory\n    and trigger the new rules.\n    This rules file in opentrons_data is symlinked into udev rules directory\n\n    TODO: Move this file to resources and move the symlink to point to\n    /data/system/\n    \"\"\"\n", "input": "", "output": "    import shutil\n    import subprocess\n\n    rules_file = os.path.join(\n        os.path.abspath(os.path.dirname(__file__)), '..',\n        'config', 'modules', '95-opentrons-modules.rules')\n\n    shutil.copy2(\n        rules_file,\n        '/data/user_storage/opentrons_data/95-opentrons-modules.rules')\n\n    res0 = subprocess.run('udevadm control --reload-rules',\n                          shell=True, stdout=subprocess.PIPE).stdout.decode()\n    if res0:\n        log.warning(res0.strip())\n\n    res1 = subprocess.run('udevadm trigger',\n                          shell=True, stdout=subprocess.PIPE).stdout.decode()\n    if res1:\n        log.warning(res1.strip())", "category": "Python"}, {"instruction": "def F_t(X, Y, S, M_E, E, m0, rho):\r\n    ''' Compute the distortion '''\n", "input": "", "output": "    r = X.shape[1]\r\n    out1 = (((np.dot(np.dot(X, S), Y.T) - M_E) * E)**2).sum() / 2\r\n    out2 = rho * G(Y, m0, r)\r\n    out3 = rho * G(X, m0, r)\r\n\r\n    return out1 + out2 + out3", "category": "Python"}, {"instruction": "def _get_repo_url():\n    \"\"\"Return the base URL for Gluon dataset and model repository.\"\"\"\n", "input": "", "output": "    default_repo = 'https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/'\n    repo_url = os.environ.get('MXNET_GLUON_REPO', default_repo)\n    if repo_url[-1] != '/':\n        repo_url = repo_url+'/'\n    return repo_url", "category": "Python"}, {"instruction": "def get_location(self, obj):\n        \"\"\" return user's location \"\"\"\n", "input": "", "output": "        if not obj.city and not obj.country:\n            return None\n        elif obj.city and obj.country:\n            return '%s, %s' % (obj.city, obj.country)\n        elif obj.city or obj.country:\n            return obj.city or obj.country", "category": "Python"}, {"instruction": "def items(cls):\n        \"\"\"\n        :return: List of tuples consisting of every enum value in the form [('NAME', value), ...]\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        items = [(value.name, key) for key, value in cls.values.items()]\n        return sorted(items, key=lambda x: x[1])", "category": "Python"}, {"instruction": "def is_datetimelike(arr):\n    \"\"\"\n    Check whether an array-like is a datetime-like array-like.\n\n    Acceptable datetime-like objects are (but not limited to) datetime\n    indices, periodic indices, and timedelta indices.\n\n    Parameters\n    ----------\n    arr : array-like\n        The array-like to check.\n\n    Returns\n    -------\n    boolean\n        Whether or not the array-like is a datetime-like array-like.\n\n    Examples\n    --------\n    >>> is_datetimelike([1, 2, 3])\n    False\n    >>> is_datetimelike(pd.Index([1, 2, 3]))\n    False\n    >>> is_datetimelike(pd.DatetimeIndex([1, 2, 3]))\n    True\n    >>> is_datetimelike(pd.DatetimeIndex([1, 2, 3], tz=\"US/Eastern\"))\n    True\n    >>> is_datetimelike(pd.PeriodIndex([], freq=\"A\"))\n    True\n    >>> is_datetimelike(np.array([], dtype=np.datetime64))\n    True\n    >>> is_datetimelike(pd.Series([], dtype=\"timedelta64[ns]\"))\n    True\n    >>>\n    >>> dtype = DatetimeTZDtype(\"ns\", tz=\"US/Eastern\")\n    >>> s = pd.Series([], dtype=dtype)\n    >>> is_datetimelike(s)\n    True\n    \"\"\"\n", "input": "", "output": "\n    return (is_datetime64_dtype(arr) or is_datetime64tz_dtype(arr) or\n            is_timedelta64_dtype(arr) or\n            isinstance(arr, ABCPeriodIndex))", "category": "Python"}, {"instruction": "def add_multiifo_output_list_opt(self, opt, outputs):\n        \"\"\" Add an option that determines a list of outputs from multiple\n            detectors. Files will be supplied as --opt ifo1:input1 ifo2:input2\n            .....\n        \"\"\"\n", "input": "", "output": "        # NOTE: Here we have to use the raw arguments functionality as the\n        #       file and ifo are not space separated.\n        self.add_raw_arg(opt)\n        self.add_raw_arg(' ')\n        for outfile in outputs:\n            self.add_raw_arg(outfile.ifo)\n            self.add_raw_arg(':')\n            self.add_raw_arg(outfile.name)\n            self.add_raw_arg(' ')\n            self._add_output(outfile)", "category": "Python"}, {"instruction": "def copy(self, *args, **kwargs):\n        \"\"\"\n        Make a copy of this object.\n\n        See Also:\n            For arguments and description of behavior see `pandas docs`_.\n\n        .. _pandas docs: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.copy.html\n        \"\"\"\n", "input": "", "output": "        cls = self.__class__    # Note that type conversion does not perform copy\n        return cls(pd.Series(self).copy(*args, **kwargs))", "category": "Python"}, {"instruction": "def to_fs_from_unicode(unic):\r\n    \"\"\"\r\n    Return a byte string version of unic encoded using the file \r\n    system encoding.\r\n    \"\"\"\n", "input": "", "output": "    if is_unicode(unic):\r\n        try:\r\n            string = unic.encode(FS_ENCODING)\r\n        except (UnicodeError, TypeError):\r\n            pass\r\n        else:\r\n            return string\r\n    return unic", "category": "Python"}, {"instruction": "def _get_site_amplification_term(self, C, vs30):\n        \"\"\"\n        Returns the site amplification term for the case in which Vs30\n        is used directly\n        \"\"\"\n", "input": "", "output": "        return C[\"gamma\"] * np.log10(vs30 / self.CONSTS[\"Vref\"])", "category": "Python"}, {"instruction": "def upload(client, source_dir):\n    \"\"\"Upload listing files in source_dir. folder herachy.\"\"\"\n", "input": "", "output": "    print('')\n    print('upload store listings')\n    print('---------------------')\n    listings_folder = os.path.join(source_dir, 'listings')\n    langfolders = filter(os.path.isdir, list_dir_abspath(listings_folder))\n\n    for language_dir in langfolders:\n        language = os.path.basename(language_dir)\n        with open(os.path.join(language_dir, 'listing.json')) as listings_file:\n            listing = json.load(listings_file)\n        listing_response = client.update(\n            'listings', language=language, body=listing)\n\n        print('  Listing for language %s was updated.' %\n              listing_response['language'])", "category": "Python"}, {"instruction": "def get_library_progress(self):\n    \"\"\"Returns the reading progress for all books in the kindle library.\n\n    Returns:\n      A mapping of ASINs to `ReadingProgress` instances corresponding to the\n      books in the current user's library.\n    \"\"\"\n", "input": "", "output": "    kbp_dict = self._get_api_call('get_library_progress')\n    return {asin: KindleCloudReaderAPI._kbp_to_progress(kbp)\n            for asin, kbp in kbp_dict.iteritems()}", "category": "Python"}, {"instruction": "def props(obj, required, **kwargs):\n    \"\"\" Return a dictionary built from the combination of defaults, kwargs,\n    and the attributes of the given object.\n    \"\"\"\n", "input": "", "output": "    # Get the attributes of the template object\n    pr = get_obj_attrs(obj)\n\n    # Get the parameters to generate the waveform\n    # Note that keyword arguments override values in the template object\n    input_params = default_qnm_args.copy()\n    input_params.update(pr)\n    input_params.update(kwargs)\n\n    # Check if the required arguments are given\n    for arg in required:\n        if arg not in input_params:\n            raise ValueError('Please provide ' + str(arg))\n\n    return input_params", "category": "Python"}, {"instruction": "def _copy_finfo(finfo, storage_dir, pass_uptodate=False):\n    \"\"\"Copy a file into the output storage directory.\n    \"\"\"\n", "input": "", "output": "    out_file = _get_file_upload_path(finfo, storage_dir)\n    if not shared.up_to_date(out_file, finfo):\n        logger.info(\"Storing in local filesystem: %s\" % out_file)\n        shutil.copy(finfo[\"path\"], out_file)\n        return out_file\n    if pass_uptodate:\n        return out_file", "category": "Python"}, {"instruction": "def healthy(self):\n        \"\"\"Return 200 is healthy, else 500.\n\n        Override is_healthy() to change the health check.\n        \"\"\"\n", "input": "", "output": "        try:\n            if self.is_healthy():\n                return \"OK\", 200\n\n            else:\n                return \"FAIL\", 500\n\n        except Exception as e:\n            self.app.logger.exception(e)\n            return str(e), 500", "category": "Python"}, {"instruction": "def keygrip_nist256(vk):\n    \"\"\"Compute keygrip for NIST256 curve public keys.\"\"\"\n", "input": "", "output": "    curve = vk.curve.curve\n    gen = vk.curve.generator\n    g = (4 << 512) | (gen.x() << 256) | gen.y()\n    point = vk.pubkey.point\n    q = (4 << 512) | (point.x() << 256) | point.y()\n\n    return _compute_keygrip([\n        ['p', util.num2bytes(curve.p(), size=32)],\n        ['a', util.num2bytes(curve.a() % curve.p(), size=32)],\n        ['b', util.num2bytes(curve.b() % curve.p(), size=32)],\n        ['g', util.num2bytes(g, size=65)],\n        ['n', util.num2bytes(vk.curve.order, size=32)],\n        ['q', util.num2bytes(q, size=65)],\n    ])", "category": "Python"}, {"instruction": "def absent(name, auth=None, **kwargs):\n    '''\n    Ensure group does not exist\n\n    name\n        Name of the group\n\n    domain\n        The name or id of the domain\n    '''\n", "input": "", "output": "    ret = {'name': name,\n           'changes': {},\n           'result': True,\n           'comment': ''}\n\n    kwargs = __utils__['args.clean_kwargs'](**kwargs)\n\n    __salt__['keystoneng.setup_cloud'](auth)\n\n    kwargs['name'] = name\n    group = _common(kwargs)\n\n    if group:\n        if __opts__['test'] is True:\n            ret['result'] = None\n            ret['changes'] = {'id': group.id}\n            ret['comment'] = 'Group will be deleted.'\n            return ret\n\n        __salt__['keystoneng.group_delete'](name=group)\n        ret['changes']['id'] = group.id\n        ret['comment'] = 'Deleted group'\n\n    return ret", "category": "Python"}, {"instruction": "def delete_trigger_events(self, trigger_id, event_ids):\n        \"\"\"\n        Remove events from a point trigger\n\n        :param trigger_id: int\n        :param event_ids: list|tuple\n        :return: dict|str\n        \"\"\"\n", "input": "", "output": "\n        response = self._client.session.delete(\n            '{url}/{trigger_id}/events/delete'.format(\n                url=self.endpoint_url, trigger_id=trigger_id\n            ),\n            params={'events': event_ids}\n        )\n        return self.process_response(response)", "category": "Python"}, {"instruction": "def minimum(rule, artifact):\n    \"\"\"Evaluate artifact's value to be minimum of values returned by rule's\n    subrules.\n\n    This evaluation function ignores subrule weights.\n    \"\"\"\n", "input": "", "output": "    m = 1.0\n    for i in range(len(rule.R)):\n        e = rule.R[i](artifact)\n        if e is not None:\n            if e < m:\n                m = e\n    return m", "category": "Python"}, {"instruction": "def set_checksum(self):\n        \"\"\"Set byte 14 of the userdata to a checksum value.\"\"\"\n", "input": "", "output": "        data_sum = self.cmd1 + self.cmd2\n        for i in range(1, 14):\n            data_sum += self._userdata['d{:d}'.format(i)]\n        chksum = 0xff - (data_sum & 0xff) + 1\n        self._userdata['d14'] = chksum", "category": "Python"}, {"instruction": "def compare_modules(file_, imports):\n    \"\"\"Compare modules in a file to imported modules in a project.\n\n    Args:\n        file_ (str): File to parse for modules to be compared.\n        imports (tuple): Modules being imported in the project.\n\n    Returns:\n        tuple: The modules not imported in the project, but do exist in the\n               specified file.\n    \"\"\"\n", "input": "", "output": "    modules = parse_requirements(file_)\n\n    imports = [imports[i][\"name\"] for i in range(len(imports))]\n    modules = [modules[i][\"name\"] for i in range(len(modules))]\n    modules_not_imported = set(modules) - set(imports)\n\n    return modules_not_imported", "category": "Python"}, {"instruction": "def package_version(filename, varname):\n    \"\"\"Return package version string by reading `filename` and retrieving its\n       module-global variable `varnam`.\"\"\"\n", "input": "", "output": "    _locals = {}\n    with open(filename) as fp:\n        exec(fp.read(), None, _locals)\n    return _locals[varname]", "category": "Python"}, {"instruction": "def control_change(self, channel, control, value):\n        \"\"\"Send a control change message.\n\n        See the MIDI specification for more information.\n        \"\"\"\n", "input": "", "output": "        if control < 0 or control > 128:\n            return False\n        if value < 0 or value > 128:\n            return False\n        self.cc_event(channel, control, value)\n        self.notify_listeners(self.MSG_CC, {'channel': int(channel),\n            'control': int(control), 'value': int(value)})\n        return True", "category": "Python"}, {"instruction": "def renew_close_to_expiration(self, margin_in_seconds=A_DAY):\n        \"\"\"Automatically renew subscriptions that are close to expiring, or\n        have already expired. margin_in_seconds determines if a subscription is\n        in fact close to expiring. By default, said margin is set to be a\n        single day (24 hours).\n\n        This is a long-running method for any non-trivial usage of the\n        subscriber module, as renewal requires several http requests, and\n        subscriptions are processed serially. Because of that, it is\n        recommended to run this method in a celery task.\n\n        \"\"\"\n", "input": "", "output": "        subscriptions = self.storage.close_to_expiration(margin_in_seconds)\n        for subscription in subscriptions:\n            try:\n                self.subscribe_impl(**subscription)\n            except SubscriberError as e:\n                warn(RENEW_FAILURE % (subscription['topic_url'],\n                                      subscription['callback_id']), e)", "category": "Python"}, {"instruction": "def head(self, obs=5):\n        \"\"\"\n        display the first n rows of a table\n\n        :param obs: the number of rows of the table that you want to display. The default is 5\n        :return:\n        \"\"\"\n", "input": "", "output": "        topts = dict(self.dsopts)\n        topts['obs'] = obs\n        code = \"proc print data=\" + self.libref + '.' + self.table + self.sas._dsopts(topts) + \";run;\"\n\n        if self.sas.nosub:\n            print(code)\n            return\n\n        if self.results.upper() == 'PANDAS':\n            code = \"data _head ; set %s.%s %s; run;\" % (self.libref, self.table, self.sas._dsopts(topts))\n            return self._returnPD(code, '_head')\n        else:\n            ll = self._is_valid()\n            if self.HTML:\n                if not ll:\n                    ll = self.sas._io.submit(code)\n                if not self.sas.batch:\n                    self.sas.DISPLAY(self.sas.HTML(ll['LST']))\n                else:\n                    return ll\n            else:\n                if not ll:\n                    ll = self.sas._io.submit(code, \"text\")\n                if not self.sas.batch:\n                    print(ll['LST'])\n                else:\n                    return ll", "category": "Python"}, {"instruction": "def _handle_message_flow(self, app_message):\n        \"\"\"\n        Handle protocol flow for incoming and outgoing messages, depending on service level and according to MQTT\n        spec. paragraph 4.3-Quality of Service levels and protocol flows\n        :param app_message: PublishMessage to handle\n        :return: nothing.\n        \"\"\"\n", "input": "", "output": "        if app_message.qos == QOS_0:\n            yield from self._handle_qos0_message_flow(app_message)\n        elif app_message.qos == QOS_1:\n            yield from self._handle_qos1_message_flow(app_message)\n        elif app_message.qos == QOS_2:\n            yield from self._handle_qos2_message_flow(app_message)\n        else:\n            raise HBMQTTException(\"Unexcepted QOS value '%d\" % str(app_message.qos))", "category": "Python"}, {"instruction": "def encode_http_params(**kw):\n    '''\n    url paremeter encode\n    '''\n", "input": "", "output": "    try:\n        _fo = lambda k, v: '{name}={value}'.format(\n            name=k, value=to_basestring(quote(v)))\n    except:\n        _fo = lambda k, v: '%s=%s' % (k, to_basestring(quote(v)))\n\n    _en = utf8\n\n    return '&'.join([_fo(k, _en(v)) for k, v in kw.items() if not is_empty(v)])", "category": "Python"}, {"instruction": "def _language_to_voice_code(self, language):\n        \"\"\"\n        Translate a language value to a voice code.\n\n        If you want to mock support for a language\n        by using a voice for a similar language,\n        please add it to the ``LANGUAGE_TO_VOICE_CODE`` dictionary.\n\n        :param language: the requested language\n        :type  language: :class:`~aeneas.language.Language`\n        :rtype: string\n        \"\"\"\n", "input": "", "output": "        voice_code = self.rconf[RuntimeConfiguration.TTS_VOICE_CODE]\n        if voice_code is None:\n            try:\n                voice_code = self.LANGUAGE_TO_VOICE_CODE[language]\n            except KeyError as exc:\n                self.log_exc(u\"Language code '%s' not found in LANGUAGE_TO_VOICE_CODE\" % (language), exc, False, None)\n                self.log_warn(u\"Using the language code as the voice code\")\n                voice_code = language\n        else:\n            self.log(u\"TTS voice override in rconf\")\n        self.log([u\"Language to voice code: '%s' => '%s'\", language, voice_code])\n        return voice_code", "category": "Python"}, {"instruction": "def comparator(objective):\n    \"\"\"\n    Higher order function creating a compare function for objectives.\n\n    Args:\n        objective (cipy.algorithms.core.Objective): The objective to create a\n            compare for.\n\n    Returns:\n        callable: Function accepting two objectives to compare.\n\n    Examples:\n        >>> a = Minimum(0.1)\n        >>> b = Minimum(0.2)\n        >>> compare = comparator(a)\n        >>> comparison = compare(a, b) # False\n    \"\"\"\n", "input": "", "output": "\n    if isinstance(objective, Minimum):\n        return lambda l, r: l < r\n    else:\n        return lambda l, r: l > r", "category": "Python"}, {"instruction": "def _wait(self, timeout):\n        \"\"\"\n        Based upon an extract from threading.Condition().wait(). Immediately \n        tries to acquire the lock, and then sleeps for a period of time (going \n        1/2ms..1ms..2ms..4ms...50ms..50ms), repeating until the lock is \n        acquired or the timeout limit is reached.\n        \"\"\"\n", "input": "", "output": "        endtime = time.time() + timeout\n        # Initial delay of .5ms\n        delay = 0.0005\n        while 1:\n            if self._lock.acquire(0):\n                return\n            # Nope, let's see if we have some time left to sleep\n            remaining = endtime - time.time()\n            if remaining <= 0:\n                # Nope, let's break to the error\n                break\n            # Yes, let's increase the delay up to a maximum of 50ms, and \n            # limited to the remaining time\n            delay = min(delay * 2, remaining, .05)\n            time.sleep(delay)\n        raise ResyncWaitTimeout", "category": "Python"}, {"instruction": "def visit_Attribute(self, node: ast.Attribute) -> None:\n        \"\"\"Represent the attribute by dumping its source code.\"\"\"\n", "input": "", "output": "        if node in self._recomputed_values:\n            value = self._recomputed_values[node]\n\n            if _representable(value=value):\n                text = self._atok.get_text(node)\n                self.reprs[text] = value\n\n        self.generic_visit(node=node)", "category": "Python"}, {"instruction": "def applicable_file_flags(self):\n        \"\"\"\n        Return the applicable file flags attribute of the BFD file being\n        processed.\n\n        \"\"\"\n", "input": "", "output": "        if not self._ptr:\n            raise BfdException(\"BFD not initialized\")\n\n        return _bfd.get_bfd_attribute(\n            self._ptr, BfdAttributes.APPLICABLE_FILE_FLAGS)", "category": "Python"}, {"instruction": "def read_structure(self, cls=Structure):\n        \"\"\"Returns the crystalline structure.\"\"\"\n", "input": "", "output": "        if self.ngroups != 1:\n            raise NotImplementedError(\"In file %s: ngroups != 1\" % self.path)\n\n        return structure_from_ncdata(self, cls=cls)", "category": "Python"}, {"instruction": "def _from_rest_lower(model, props):\n    \"\"\" Lowercase fields requesting it during a REST deserialization \"\"\"\n", "input": "", "output": "\n    for field in model.to_lower:\n        try:\n            props[field] = props[field].lower()\n        except (AttributeError, KeyError):\n            continue", "category": "Python"}, {"instruction": "def ellipticity_lens_light(self, kwargs_lens_light, center_x=0, center_y=0, model_bool_list=None, deltaPix=None,\n                               numPix=None):\n        \"\"\"\n        make sure that the window covers all the light, otherwise the moments may give to low answers.\n\n        :param kwargs_lens_light:\n        :param center_x:\n        :param center_y:\n        :param model_bool_list:\n        :param deltaPix:\n        :param numPix:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if model_bool_list is None:\n            model_bool_list = [True] * len(kwargs_lens_light)\n        if numPix is None:\n            numPix = 100\n        if deltaPix is None:\n            deltaPix = 0.05\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        x_grid += center_x\n        y_grid += center_y\n        I_xy = self._lens_light_internal(x_grid, y_grid, kwargs_lens_light, model_bool_list=model_bool_list)\n        e1, e2 = analysis_util.ellipticities(I_xy, x_grid, y_grid)\n        return e1, e2", "category": "Python"}, {"instruction": "def deprecated(message):\n    \"\"\" Decorator for deprecating functions and methods.\n\n    ::\n\n        @deprecated(\"'foo' has been deprecated in favour of 'bar'\")\n        def foo(x):\n            pass\n\n    \"\"\"\n", "input": "", "output": "    def f__(f):\n        def f_(*args, **kwargs):\n            from warnings import warn\n            warn(message, category=DeprecationWarning, stacklevel=2)\n            return f(*args, **kwargs)\n        f_.__name__ = f.__name__\n        f_.__doc__ = f.__doc__\n        f_.__dict__.update(f.__dict__)\n        return f_\n    return f__", "category": "Python"}, {"instruction": "def _get_raw_xsrf_token(self) -> Tuple[Optional[int], bytes, float]:\n        \"\"\"Read or generate the xsrf token in its raw form.\n\n        The raw_xsrf_token is a tuple containing:\n\n        * version: the version of the cookie from which this token was read,\n          or None if we generated a new token in this request.\n        * token: the raw token data; random (non-ascii) bytes.\n        * timestamp: the time this token was generated (will not be accurate\n          for version 1 cookies)\n        \"\"\"\n", "input": "", "output": "        if not hasattr(self, \"_raw_xsrf_token\"):\n            cookie = self.get_cookie(\"_xsrf\")\n            if cookie:\n                version, token, timestamp = self._decode_xsrf_token(cookie)\n            else:\n                version, token, timestamp = None, None, None\n            if token is None:\n                version = None\n                token = os.urandom(16)\n                timestamp = time.time()\n            assert token is not None\n            assert timestamp is not None\n            self._raw_xsrf_token = (version, token, timestamp)\n        return self._raw_xsrf_token", "category": "Python"}, {"instruction": "def flairlist(self, r, limit=1000, after=None, before=None):\n        \"\"\"Login required.  Gets flairlist for subreddit `r`.  See https://github.com/reddit/reddit/wiki/API%3A-flairlist.\n        \n        However, the wiki docs are wrong (as of 2012/5/4).  Returns :class:`things.ListBlob` of :class:`things.Blob` objects, each object being a mapping with `user`, `flair\\_css\\_class`, and `flair\\_text` attributes.\n        \n        URL: ``http://www.reddit.com/r/<r>/api/flairlist``\n        \n        :param r: name of subreddit\n        :param limit: max number of items to return\n        :param after: full id of user to return entries after\n        :param before: full id of user to return entries *before* \n        \"\"\"\n", "input": "", "output": "        params = dict(limit=limit)\n        if after:\n            params['after'] = after\n        elif before:\n            params['before'] = before\n        b = self.get('r', r, 'api', 'flairlist', params=params)\n        return b.users", "category": "Python"}, {"instruction": "def xlim_change_check(self, idx):\n        '''handle xlim change requests from queue'''\n", "input": "", "output": "        if not self.xlim_pipe[1].poll():\n            return\n        xlim = self.xlim_pipe[1].recv()\n        if xlim is None:\n            return\n        #print(\"recv: \", self.graph_num, xlim)\n        if self.ax1 is not None and xlim != self.xlim:\n            self.xlim = xlim\n            self.fig.canvas.toolbar.push_current()\n            #print(\"setting: \", self.graph_num, xlim)\n            self.ax1.set_xlim(xlim)\n            # trigger the timer, this allows us to setup a v slow animation,\n            # which saves a lot of CPU\n            self.ani.event_source._on_timer()", "category": "Python"}, {"instruction": "def updatepLvlGrid(self):\n        '''\n        Update the grid of permanent income levels.  Currently only works for\n        infinite horizon models (cycles=0) and lifecycle models (cycles=1).  Not\n        clear what to do about cycles>1.  Identical to version in persistent\n        shocks model, but pLvl=0 is manually added to the grid (because there is\n        no closed form lower-bounding cFunc for pLvl=0).\n\n        Parameters\n        ----------\n        None\n\n        Returns\n        -------\n        None\n        '''\n", "input": "", "output": "        # Run basic version of this method\n        PersistentShockConsumerType.updatepLvlGrid(self)\n        for j in range(len(self.pLvlGrid)): # Then add 0 to the bottom of each pLvlGrid\n            this_grid = self.pLvlGrid[j]\n            self.pLvlGrid[j] = np.insert(this_grid,0,0.0001)", "category": "Python"}, {"instruction": "def re_match(self, pattern, haystack, **kwargs):\n        \"\"\"\n        re.match py3 compat\n        :param pattern:\n        :param haystack:\n        :return:\n        \"\"\"\n", "input": "", "output": "        try:\n            return re.match(pattern, haystack.decode('utf8'), **kwargs)\n\n        except Exception as e:\n            logger.debug('re.match exception: %s' % e)\n            self.trace_logger.log(e)", "category": "Python"}, {"instruction": "def showsyntaxerror(self, filename=None):\n        \"\"\"Display the syntax error that just occurred.\n\n        This doesn't display a stack trace because there isn't one.\n\n        If a filename is given, it is stuffed in the exception instead\n        of what was there before (because Python's parser always uses\n        \"<string>\" when reading from a string).\n\n        The output is written by self.write(), below.\n\n        \"\"\"\n", "input": "", "output": "        type, value, sys.last_traceback = sys.exc_info()\n        sys.last_type = type\n        sys.last_value = value\n        if filename and type is SyntaxError:\n            # Work hard to stuff the correct filename in the exception\n            try:\n                msg, (dummy_filename, lineno, offset, line) = value\n            except:\n                # Not the format we expect; leave it alone\n                pass\n            else:\n                # Stuff in the right filename\n                value = SyntaxError(msg, (filename, lineno, offset, line))\n                sys.last_value = value\n        list = traceback.format_exception_only(type, value)\n        map(self.write, list)", "category": "Python"}, {"instruction": "def init_process(self) -> None:\n        \"\"\"\n        GunicornWorker \u521d\u59cb\u5316\u56de\u8c03\n        \"\"\"\n", "input": "", "output": "        default_loop = asyncio.get_event_loop()\n        if default_loop.is_running():\n            default_loop.close()\n            self.loop = asyncio.new_event_loop()\n            asyncio.set_event_loop(self.loop)\n        else:\n            self.loop = default_loop\n        super().init_process()", "category": "Python"}, {"instruction": "def context_filter(self, context):\n        \"\"\"\n        Provide a context filter to search.\n        \n        :param str context: Context filter by name\n        \"\"\"\n", "input": "", "output": "        if context in CONTEXTS:\n            self._params.update(filter_context=context)\n            return self\n        raise InvalidSearchFilter(\n            'Context filter %r was invalid. Available filters: %s' %\n            (context, CONTEXTS))", "category": "Python"}, {"instruction": "def setGamma(self, x):\n        \"\"\" set electron energy, gamma value\n\n        :param x: new energy, gamma value\n        :return: None\n        \"\"\"\n", "input": "", "output": "        if x != self.gamma:\n            self.gamma = x\n            self.refresh = True", "category": "Python"}, {"instruction": "def log_transition(self, transition, from_state, instance, *args, **kwargs):\n        \"\"\"Generic transition logging.\"\"\"\n", "input": "", "output": "        save = kwargs.pop('save', True)\n        log = kwargs.pop('log', True)\n        super(Workflow, self).log_transition(\n            transition, from_state, instance, *args, **kwargs)\n        if save:\n            instance.save()\n        if log:\n            self.db_log(transition, from_state, instance, *args, **kwargs)", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Close a port on dummy_serial.\"\"\"\n", "input": "", "output": "        if VERBOSE:\n            _print_out('\\nDummy_serial: Closing port\\n')\n\n        if not self._isOpen:\n            raise IOError('Dummy_serial: The port is already closed')\n            \n        self._isOpen = False\n        self.port = None", "category": "Python"}, {"instruction": "def bytes_to_str(byteVal, decimals=1):\n    \"\"\"\n    Convert bytes to a human readable string.\n\n    :param byteVal: Value to convert in bytes\n    :type byteVal: int or float\n\n    :param decimal: Number of decimal to display\n    :type decimal: int\n\n    :returns: Number of byte with the best unit of measure\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    for unit, byte in BYTES:\n        if (byteVal >= byte):\n            if decimals == 0:\n                return '%s %s' % (int(round(byteVal / byte, 0)), unit)\n            return '%s %s' % (round(byteVal / byte, decimals), unit)\n    return '%s B' % byteVal", "category": "Python"}, {"instruction": "def embeddedFileCount(self):\n        \"\"\"Return number of embedded files.\"\"\"\n", "input": "", "output": "        if self.isClosed or self.isEncrypted:\n            raise ValueError(\"operation illegal for closed / encrypted doc\")\n\n        return _fitz.Document_embeddedFileCount(self)", "category": "Python"}, {"instruction": "def unary_operation(self, rule, right, **kwargs):\n        \"\"\"\n        Implementation of :py:func:`pynspect.traversers.RuleTreeTraverser.unary_operation` interface.\n        \"\"\"\n", "input": "", "output": "        return self.evaluate_unop(rule.operation, right, **kwargs)", "category": "Python"}, {"instruction": "def start(name, call=None):\n    '''\n    Start a node.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -a start mymachine\n    '''\n", "input": "", "output": "    if call != 'action':\n        raise SaltCloudSystemExit(\n            'The show_instance action must be called with -a or --action.'\n        )\n\n    data = query(action='ve', command='{0}/start'.format(name), method='PUT')\n\n    if 'error' in data:\n        return data['error']\n\n    return {'Started': '{0} was started.'.format(name)}", "category": "Python"}, {"instruction": "def get_user_roles(user):\n    \"\"\"Return a list of all of the user's roles.\"\"\"\n", "input": "", "output": "    if not hasattr(user, '_role_cache'):\n        user._role_cache = list(TenantRole.objects.filter(user=user).values_list(\n            'group', 'role', 'tenant'))\n    return user._role_cache", "category": "Python"}, {"instruction": "def import_attr(module_name, attr_name):\n    \"\"\"\n    Given a dotted Python path & an attribute name, imports the module &\n    returns the attribute.\n\n    If not found, raises ``UnknownCallableError``.\n\n    Ex::\n\n        choice = import_attr('random', 'choice')\n\n    :param module_name: The dotted Python path\n    :type module_name: string\n\n    :param attr_name: The attribute name\n    :type attr_name: string\n\n    :returns: attribute\n    \"\"\"\n", "input": "", "output": "    module = import_module(module_name)\n\n    try:\n        return getattr(module, attr_name)\n    except AttributeError as err:\n        raise UnknownCallableError(str(err))", "category": "Python"}, {"instruction": "def find_bad_footnote_urls(tagged_lines, include_tags=None):\n    \"\"\" Find lines in the list of 2-tuples of adoc-tagged lines that contain bad footnotes (only urls) \n\n    >>> sections = get_tagged_sections(BOOK_PATH)\n    >>> tagged_lines = list(sections[0][1])\n    >>> find_bad_footnote_urls(tagged_lines)\n    [[30, 'https://spacy.io/usage/linguistic-features#rule-based-morphology']]\n    \"\"\"\n", "input": "", "output": "    section_baddies = []\n    logger.debug(tagged_lines[:2])\n    for lineno, (tag, line) in enumerate(tagged_lines):\n        line_baddies = None\n        if tag is None or include_tags is None or tag in include_tags or any((tag.startswith(t) for t in include_tags)):\n            line_baddies = get_line_bad_footnotes(line=line, tag=tag)\n        if line_baddies and len(line_baddies) > 1:\n            section_baddies.append([lineno] + line_baddies[1:])\n        else:\n            pass\n            # section_baddies.append(line)\n    return section_baddies", "category": "Python"}, {"instruction": "def run_pre_save_hook(self, model, path, **kwargs):\n        \"\"\"Send request to store notebook to S3.\n\n        This hook offloads the storage request to the event loop.\n        When the event loop is available for execution of the request, the\n        storage of the notebook will be done and the write to storage occurs.\n\n        Parameters\n        ----------\n        model : str\n            The type of file\n        path : str\n            The storage location\n        \"\"\"\n", "input": "", "output": "        if model[\"type\"] != \"notebook\":\n            return\n\n        content = json.dumps(model[\"content\"])\n\n        loop = ioloop.IOLoop.current()\n\n        # Offload archival and schedule write to storage with the current event loop\n        loop.spawn_callback(\n            self.archive,\n            ArchiveRecord(\n                content=content, filepath=path, queued_time=ioloop.IOLoop.current().time()\n            ),\n        )", "category": "Python"}, {"instruction": "def ol(\n            self,\n            text):\n        \"\"\"*convert plain-text to MMD ordered list*\n\n        **Key Arguments:**\n            - ``text`` -- the text to convert to MMD ordered list\n\n        **Return:**\n            - ``ol`` -- the MMD ordered list\n\n        **Usage:**\n\n            To convert text to MMD ordered list:\n\n            .. code-block:: python\n\n                ol = md.ol(\" This is a list item   \")\n                print ol\n\n                # OUTPUT:\n                # 1.  This is a list item\n                #\n        \"\"\"\n", "input": "", "output": "        m = self.reWS.match(text)\n        ol = []\n        for thisIndex, l in enumerate(m.group(2).split(\"\\n\")):\n            thisIndex += 1\n            prefix, text, suffix = self._snip_whitespace(l)\n            ol.append(\"%(prefix)s%(thisIndex)s. %(text)s  \" % locals())\n\n        return (\"\\n\").join(ol) + \"\\n\\n\"", "category": "Python"}, {"instruction": "def forum_post_list(self, creator_id=None, creator_name=None,\n                        topic_id=None, topic_title_matches=None,\n                        topic_category_id=None, body_matches=None):\n        \"\"\"Return a list of forum posts.\n\n        Parameters:\n            creator_id (int):\n            creator_name (str):\n            topic_id (int):\n            topic_title_matches (str):\n            topic_category_id (int): Can be: 0, 1, 2 (General, Tags, Bugs &\n                                     Features respectively).\n            body_matches (str): Can be part of the post content.\n        \"\"\"\n", "input": "", "output": "        params = {\n            'search[creator_id]': creator_id,\n            'search[creator_name]': creator_name,\n            'search[topic_id]': topic_id,\n            'search[topic_title_matches]': topic_title_matches,\n            'search[topic_category_id]': topic_category_id,\n            'search[body_matches]': body_matches\n            }\n        return self._get('forum_posts.json', params)", "category": "Python"}, {"instruction": "def add_response_headers(self, headers, **overrides):\n        \"\"\"Adds the specified response headers while keeping existing ones in-tact\"\"\"\n", "input": "", "output": "        response_headers = self.route.get('response_headers', {}).copy()\n        response_headers.update(headers)\n        return self.where(response_headers=response_headers, **overrides)", "category": "Python"}, {"instruction": "def euclidean_dist(point1, point2):\n    \"\"\"Compute the Euclidean distance between two points.\n\n    Parameters\n    ----------\n    point1, point2 : 2-tuples of float\n        The input points.\n\n    Returns\n    -------\n    d : float\n        The distance between the input points.\n\n    Examples\n    --------\n    >>> point1 = (1.0, 2.0)\n    >>> point2 = (4.0, 6.0)  # (3., 4.) away, simplest Pythagorean triangle\n    >>> euclidean_dist(point1, point2)\n    5.0\n    \"\"\"\n", "input": "", "output": "    (x1, y1) = point1\n    (x2, y2) = point2\n    return math.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)", "category": "Python"}, {"instruction": "def get_box_comments(self, box_key):\n\t\t'''Gets comments in a box with the provided attributes.\n\t\tArgs:\n\t\t\tbox_key\t\t\tkey for box\n\t\t\treturn\t\t\t(status code, list of comment dicts)\n\t\t'''\n", "input": "", "output": "\t\turi = '/'.join([\n\t\t\t\t\t\tself.api_uri,\n\t\t\t\t\t\tself.boxes_suffix,\n\t\t\t\t\t\tbox_key,\n\t\t\t\t\t\tself.comments_suffix\n\t\t\t\t\t\t])\n\t\treturn self._req('get', uri)", "category": "Python"}, {"instruction": "def authenticate(self, request, username=None, password=None):\n        \"\"\"Authenticate a username-password pair.\n\n        Creates a new user if one is not already in the database.\n\n        Args:\n            username\n                The username of the `User` to authenticate.\n            password\n                The password of the `User` to authenticate.\n\n        Returns:\n            `User`\n\n        \"\"\"\n", "input": "", "output": "\n        if not isinstance(username, str):\n            return None\n\n        # remove all non-alphanumerics\n        username = re.sub(r'\\W', '', username)\n\n        krb_ticket = self.get_kerberos_ticket(username, password)\n\n        if krb_ticket == \"reset\":\n            user, status = User.objects.get_or_create(username=\"RESET_PASSWORD\", user_type=\"service\", id=999999)\n            return user\n\n        if not krb_ticket:\n            return None\n        else:\n            logger.debug(\"Authentication successful\")\n            try:\n                user = User.objects.get(username__iexact=username)\n            except User.DoesNotExist:\n                return None\n            return user", "category": "Python"}, {"instruction": "def msg2usernames(msg, **config):\n    ''' Return cached fedmsg.meta.msg2usernames(...) '''\n", "input": "", "output": "\n    if not _cache.is_configured:\n        _cache.configure(**config['fmn.rules.cache'])\n\n    key = \"|\".join(['usernames', msg['msg_id']]).encode('utf-8')\n    creator = lambda: fedmsg.meta.msg2usernames(msg, **config)\n    return _cache.get_or_create(key, creator)", "category": "Python"}, {"instruction": "def pack(self, fmt, data):\n        \"\"\"\n        Write bytes by packing them according to the provided format `fmt`.\n        For more information about the `fmt` format see: https://docs.python.org/3/library/struct.html\n\n        Args:\n            fmt (str): format string.\n            data (object): the data to write to the raw stream.\n\n        Returns:\n            int: the number of bytes written.\n        \"\"\"\n", "input": "", "output": "        return self.write_bytes(struct.pack(fmt, data))", "category": "Python"}, {"instruction": "def execute(self, conn, app, release_version, pset_hash, output_label, global_tag, transaction = False):\n        \"\"\"\n        returns id for a given application\n\n        This always requires all four variables to be set, because\n        you better have them in blockInsert\n        \"\"\"\n", "input": "", "output": "        binds = {}\n        binds[\"app_name\"]=app\n        binds[\"release_version\"]=release_version\n        binds[\"pset_hash\"]=pset_hash\n        binds[\"output_module_label\"]=output_label\n        binds[\"global_tag\"]=global_tag\n\n        result = self.dbi.processData(self.sql, binds, conn, transaction)\n\n        plist = self.formatDict(result)\n\n\tif len(plist) < 1: return -1\n        return plist[0][\"output_mod_config_id\"]", "category": "Python"}, {"instruction": "def deactivate(self):\n        \"\"\" deactivate the environment \"\"\"\n", "input": "", "output": "        try:\n            self.phase = PHASE.DEACTIVATE\n            self.logger.info(\"Deactivating environment %s...\" % self.namespace)\n            self.directory.rewrite_config = False\n            self.instantiate_features()\n            self._specialize()\n            for feature in self.features.run_order:\n                self.logger.info(\"Deactivating %s...\" % feature[0])\n                self.run_action(feature, 'deactivate')\n            self.clear_all()\n            self._finalize()\n        except Exception:\n            self.logger.debug(\"\", exc_info=sys.exc_info())\n            et, ei, tb = sys.exc_info()\n            reraise(et, ei, tb)", "category": "Python"}, {"instruction": "def from_address(text):\n    \"\"\"Convert an IPv4 or IPv6 address in textual form into a Name object whose\n    value is the reverse-map domain name of the address.\n    @param text: an IPv4 or IPv6 address in textual form (e.g. '127.0.0.1',\n    '::1')\n    @type text: str\n    @rtype: dns.name.Name object\n    \"\"\"\n", "input": "", "output": "    try:\n        parts = list(dns.ipv6.inet_aton(text).encode('hex_codec'))\n        origin = ipv6_reverse_domain\n    except Exception:\n        parts = ['%d' % ord(byte) for byte in dns.ipv4.inet_aton(text)]\n        origin = ipv4_reverse_domain\n    parts.reverse()\n    return dns.name.from_text('.'.join(parts), origin=origin)", "category": "Python"}, {"instruction": "def piece_at(self, square: Square) -> Optional[Piece]:\n        \"\"\"Gets the :class:`piece <chess.Piece>` at the given square.\"\"\"\n", "input": "", "output": "        piece_type = self.piece_type_at(square)\n        if piece_type:\n            mask = BB_SQUARES[square]\n            color = bool(self.occupied_co[WHITE] & mask)\n            return Piece(piece_type, color)\n        else:\n            return None", "category": "Python"}, {"instruction": "def bencode(obj):\n    \"\"\"Bencodes obj and returns it as a string\"\"\"\n", "input": "", "output": "    if isinstance(obj, int):\n        return \"i\" + str(obj) + \"e\"\n\n    if isinstance(obj, str):\n        if not obj:\n            return None\n        return str(len(obj)) + \":\" + obj\n\n    if isinstance(obj, list):\n        res = \"l\"\n        for elem in obj:\n            elem = bencode(elem)\n            if elem:\n                res += elem\n        return res + \"e\"\n\n    if isinstance(obj, dict):\n        res = \"d\"\n        for key in sorted(obj.keys()):\n            if key in obj:\n                value = bencode(obj[key])\n                key = bencode(key)\n                if key and value:\n                    res += key + value\n\n        return res + \"e\"\n\n    if isinstance(obj, unicode):\n        return bencode(obj.encode('utf-8'))\n\n    if isinstance(obj, collections.OrderedDict):\n        return bencode(dict(obj))\n    raise Exception(\"Unknown object: %s (%s)\" % (repr(obj), repr(type(obj))))", "category": "Python"}, {"instruction": "def _construct_key(previous_key, separator, new_key):\n    \"\"\"\n    Returns the new_key if no previous key exists, otherwise concatenates\n    previous key, separator, and new_key\n    :param previous_key:\n    :param separator:\n    :param new_key:\n    :return: a string if previous_key exists and simply passes through the\n    new_key otherwise\n    \"\"\"\n", "input": "", "output": "    if previous_key:\n        return u\"{}{}{}\".format(previous_key, separator, new_key)\n    else:\n        return new_key", "category": "Python"}, {"instruction": "def log_level_from_string(str_level):\n    \"\"\" Returns the proper log level core based on a given string\n\n    :param str_level: Log level string\n    :return: The log level code\n    \"\"\"\n", "input": "", "output": "    levels = {\n        'CRITICAL': logging.CRITICAL,\n        'ERROR': logging.ERROR,\n        'WARNING': logging.WARNING,\n        'INFO': logging.INFO,\n        'DEBUG': logging.DEBUG,\n    }\n    try:\n        return levels[str_level.upper()]\n    except KeyError:\n        pass\n    except AttributeError:\n        if str_level in [logging.DEBUG, logging.INFO, logging.WARNING,\n                         logging.ERROR, logging.CRITICAL]:\n            return str_level\n    return logging.NOTSET", "category": "Python"}, {"instruction": "def remove_from_category(self, category, name):\n        \"\"\"\n        Removes given action from given category.\n\n        :param category: Category to remove the action from.\n        :type category: unicode\n        :param name: Action name.\n        :type name: unicode\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        category = self.get_category(category)\n        if not isinstance(category, dict):\n            return False\n\n        del (category[name])\n        LOGGER.debug(\"> Removed '{0}' action from '{1}' category!\".format(category, name))\n        return True", "category": "Python"}, {"instruction": "def close_compute_projects(self, compute):\n        \"\"\"\n        Close projects running on a compute\n        \"\"\"\n", "input": "", "output": "        for project in self._projects.values():\n            if compute in project.computes:\n                yield from project.close()", "category": "Python"}, {"instruction": "def _get_doc():\n    '''Get a description of all the known available features'''\n", "input": "", "output": "    def get_docstring(func):\n        ", "category": "Python"}, {"instruction": "def phantom_decorate(f, get_or_add):\n    \"\"\"\n    Decorator for version-dependent fields.\n    If get_or_add is True (means get), we return s, self.phantom_value.\n    If it is False (means add), we return s.\n    \"\"\"\n", "input": "", "output": "    def wrapper(*args):\n        self, pkt, s = args[:3]\n        if phantom_mode(pkt):\n            if get_or_add:\n                return s, self.phantom_value\n            return s\n        return f(*args)\n    return wrapper", "category": "Python"}, {"instruction": "def get_index(cls, model, via_class=False):\n        '''\n        Returns the index name (as a string) for the given model as a class or a string.\n        :param model: model name or model class if via_class set to True.\n        :param via_class: set to True if parameter model is a class.\n        :raise KeyError: If the provided model does not have any index associated.\n        '''\n", "input": "", "output": "        try:\n            return cls._model_to_index[model] if via_class else cls._model_name_to_index[model]\n        except KeyError:\n            raise KeyError('Could not find any index defined for model {}. Is the model in one of the model index modules of BUNGIESEARCH[\"INDICES\"]?'.format(model))", "category": "Python"}, {"instruction": "def dInd_calc(TNR, TPR):\n    \"\"\"\n    Calculate dInd (Distance index).\n\n    :param TNR: specificity or true negative rate\n    :type TNR : float\n    :param TPR: sensitivity, recall, hit rate, or true positive rate\n    :type TPR : float\n    :return: dInd as float\n    \"\"\"\n", "input": "", "output": "    try:\n        result = math.sqrt(((1 - TNR)**2) + ((1 - TPR)**2))\n        return result\n    except Exception:\n        return \"None\"", "category": "Python"}, {"instruction": "def rename_column(self, column, new_name):\n        \"\"\"\n        This will rename the column.\n        The supplied column can be an integer or the old column name.\n        \"\"\"\n", "input": "", "output": "        if type(column) is not str: column = self.ckeys[column]\n        self.ckeys[self.ckeys.index(column)] = new_name\n        self.columns[new_name] = self.columns.pop(column)\n        return self", "category": "Python"}, {"instruction": "def bind_super(self, opr):\n        \"\"\" \u4e3a\u8d85\u7ea7\u7ba1\u7406\u5458\u6388\u6743\u6240\u6709\u6743\u9650\n        \"\"\"\n", "input": "", "output": "        for path in self.routes:\n            route = self.routes.get(path)\n            route['oprs'].append(opr)", "category": "Python"}, {"instruction": "def wcs_to_coords(w, shape):\n    \"\"\"Generate an N x D list of pixel center coordinates where N is\n    the number of pixels and D is the dimensionality of the map.\"\"\"\n", "input": "", "output": "    if w.naxis == 2:\n        y, x = wcs_to_axes(w, shape)\n    elif w.naxis == 3:\n        z, y, x = wcs_to_axes(w, shape)\n    else:\n        raise Exception(\"Wrong number of WCS axes %i\" % w.naxis)\n\n    x = 0.5 * (x[1:] + x[:-1])\n    y = 0.5 * (y[1:] + y[:-1])\n\n    if w.naxis == 2:\n        x = np.ravel(np.ones(shape) * x[:, np.newaxis])\n        y = np.ravel(np.ones(shape) * y[np.newaxis, :])\n        return np.vstack((x, y))\n\n    z = 0.5 * (z[1:] + z[:-1])\n    x = np.ravel(np.ones(shape) * x[:, np.newaxis, np.newaxis])\n    y = np.ravel(np.ones(shape) * y[np.newaxis, :, np.newaxis])\n    z = np.ravel(np.ones(shape) * z[np.newaxis, np.newaxis, :])\n\n    return np.vstack((x, y, z))", "category": "Python"}, {"instruction": "def insert(self, i, tag, affix, cmd=\"hassuf\", tagged=None):\n        \"\"\" Inserts a new rule that assigns the given tag to words with the given affix,\n            e.g., Morphology.append(\"RB\", \"-ly\").\n        \"\"\"\n", "input": "", "output": "        if affix.startswith(\"-\") and affix.endswith(\"-\"):\n            affix, cmd = affix[+1:-1], \"char\"\n        if affix.startswith(\"-\"):\n            affix, cmd = affix[+1:-0], \"hassuf\"\n        if affix.endswith(\"-\"):\n            affix, cmd = affix[+0:-1], \"haspref\"\n        if tagged:\n            r = [tagged, affix, \"f\"+cmd.lstrip(\"f\"), tag, \"x\"]\n        else:\n            r = [affix, cmd.lstrip(\"f\"), tag, \"x\"]\n        lazylist.insert(self, i, r)", "category": "Python"}, {"instruction": "def get_ie(instance, key, data):\n    \"\"\"http://git.kernel.org/cgit/linux/kernel/git/jberg/iw.git/tree/scan.c?id=v3.17#n981.\n\n    Positional arguments:\n    instance -- `ie_print` class instance.\n    key -- corresponding `ieprinters` dictionary key for the instance.\n    data -- bytearray data to read.\n\n    Returns:\n    Dictionary of parsed data with string keys.\n    \"\"\"\n", "input": "", "output": "    if not instance.print_:\n        return dict()\n    if len(data) < instance.minlen or len(data) > instance.maxlen:\n        if data:\n            return {'<invalid: {0} byte(s)>'.format(len(data)): ' '.join(format(x, '02x') for x in data)}\n        return {'<invalid: no data>': data}\n    return {instance.name: instance.print_(key, data)}", "category": "Python"}, {"instruction": "def load(cls,filename):\n        \"\"\"Load from stored files\"\"\"\n", "input": "", "output": "        filename = cls.correct_file_extension(filename)\n        with open(filename,'rb') as f:\n            return pickle.load(f)", "category": "Python"}, {"instruction": "def name_scope(name=None):\n    \"\"\"\n    This decorator wraps a function so that it runs inside a TensorFlow\n    name scope. The name is given by the `name` option; if this is None,\n    then the name of the function will be used.\n    ```\n    >>> @name_scope()\n    >>> def foo(...):\n    >>>     # now runs inside scope \"foo\"\n    >>> @name_scope('bar')\n    >>> def baz(...):\n    >>>     # now runs inside scope \"bar\", not \"baz\"\n    ```\n    \"\"\"\n", "input": "", "output": "    def name_scope_wrapper_decorator(method):\n        @functools.wraps(method)\n        def name_scope_wrapper(*args, **kwargs):\n            scope_name = name if name is not None else method.__name__\n            with tf.name_scope(scope_name):\n                return method(*args, **kwargs)\n        return name_scope_wrapper\n    return name_scope_wrapper_decorator", "category": "Python"}, {"instruction": "def handle_subscribed_event(self, event_obj, event_name):\n        \"\"\"Execute the registered handler of an event.\n\n        Retrieve the handler and its arguments, and execute the handler in a\n            new thread.\n\n        Args:\n            event_obj: Json object of the event.\n            event_name: Name of the event to call handler for.\n        \"\"\"\n", "input": "", "output": "        handler, args = self.handlers[event_name]\n        self.executor.submit(handler, event_obj, *args)", "category": "Python"}, {"instruction": "def ecs_tag_normalize(resources):\n    \"\"\"normalize tag format on ecs resources to match common aws format.\"\"\"\n", "input": "", "output": "    for r in resources:\n        if 'tags' in r:\n            r['Tags'] = [{'Key': t['key'], 'Value': t['value']} for t in r['tags']]\n            r.pop('tags')", "category": "Python"}, {"instruction": "def ASR(self, a):\n        \"\"\"\n        ASR (Arithmetic Shift Right) alias LSR (Logical Shift Right)\n\n        Shifts all bits of the register one place to the right. Bit seven is held\n        constant. Bit zero is shifted into the C (carry) bit.\n\n        source code forms: ASR Q; ASRA; ASRB\n\n        CC bits \"HNZVC\": uaa-s\n        \"\"\"\n", "input": "", "output": "        r = (a >> 1) | (a & 0x80)\n        self.clear_NZC()\n        self.C = get_bit(a, bit=0) # same as: self.C |= (a & 1)\n        self.update_NZ_8(r)\n        return r", "category": "Python"}, {"instruction": "def __is_surrogate_escaped(self, text):\n        \"\"\" Checks if surrogate is escaped\n        \"\"\"\n", "input": "", "output": "\n        try:\n            text.encode('utf-8')\n        except UnicodeEncodeError as e:\n            if e.reason == 'surrogates not allowed':\n                return True\n        return False", "category": "Python"}, {"instruction": "def attach_periodic(plot):\n    \"\"\"\n    Attaches plot refresh to all streams on the object.\n    \"\"\"\n", "input": "", "output": "    def append_refresh(dmap):\n        for dmap in get_nested_dmaps(dmap):\n            dmap.periodic._periodic_util = periodic(plot.document)\n    return plot.hmap.traverse(append_refresh, [DynamicMap])", "category": "Python"}, {"instruction": "def get(self, *args, **kwargs):\n        \"\"\"\n        Combination of :meth:`find` and :meth:`deserialize_item`;\n        the result from :meth:`find` is deserialized and returned.\n\n        Input is a :mod:`tinydb` query.\n\n        :return: deserialized object instance\n        :rtype: :class:`Component <cqparts.Component>`\n        \"\"\"\n", "input": "", "output": "        result = self.find(*args, **kwargs)\n        return self.deserialize_item(result)", "category": "Python"}, {"instruction": "def _create_segment(self, obj, segment, next_segment):\n        \"\"\"Create ``obj[segment]`` if missing.\n\n        The default value for a missing segment is based on the *next*\n        segment, unless a ``default`` is explicitly passed.\n\n        If the next segment is an int, the default will be a list with\n        the indicated number of items. Otherwise the default will be\n        a :class:`Settings` dict.\n\n        \"\"\"\n", "input": "", "output": "        if isinstance(next_segment, int):\n            value = [PLACEHOLDER] * (next_segment + 1)\n        else:\n            value = Settings()\n        if isinstance(obj, Mapping):\n            if segment not in obj:\n                obj[segment] = value\n        elif isinstance(obj, Sequence):\n            old_len = len(obj)\n            new_len = segment + 1\n            if new_len > old_len:\n                obj.extend([PLACEHOLDER] * (new_len - old_len))\n            if obj[segment] is PLACEHOLDER:\n                obj[segment] = value", "category": "Python"}, {"instruction": "def get_layers(self, class_: Type[L], became: bool=True) -> List[L]:\n        \"\"\"\n        Returns the list of layers of a given class. If no layers are present\n        then the list will be empty.\n\n        :param class_: class of the expected layers\n        :param became: Allow transformed layers in results\n        \"\"\"\n", "input": "", "output": "\n        out = self._index.get(class_, [])\n\n        if became:\n            out += self._transformed.get(class_, [])\n\n        return out", "category": "Python"}, {"instruction": "def pinyin_to_zhuyin(s):\n    \"\"\"Convert all Pinyin syllables in *s* to Zhuyin.\n\n    Spaces are added between connected syllables and syllable-separating\n    apostrophes are removed.\n\n    \"\"\"\n", "input": "", "output": "    return _convert(s, zhon.pinyin.syllable, pinyin_syllable_to_zhuyin,\n                    remove_apostrophes=True, separate_syllables=True)", "category": "Python"}, {"instruction": "def visit(self, node):\n        \"\"\" Replace the placeholder if it is one or continue. \"\"\"\n", "input": "", "output": "        if isinstance(node, Placeholder):\n            return self.placeholders[node.id]\n        else:\n            return super(PlaceholderReplace, self).visit(node)", "category": "Python"}, {"instruction": "def assign_to_portfolio(self, portfolio_name=None):\n        \"\"\"\n        Assign all the notes in this order to a portfolio\n\n        Parameters\n        ----------\n            portfolio_name -- The name of the portfolio to assign it to (new or existing)\n\n        Raises\n        ------\n        LendingClubError\n\n        Returns\n        -------\n        boolean\n            True on success\n        \"\"\"\n", "input": "", "output": "        assert self.order_id > 0, 'You need to execute this order before you can assign to a portfolio.'\n\n        # Get loan IDs as a list\n        loan_ids = self.loans.keys()\n\n        # Make a list of 1 order ID per loan\n        order_ids = [self.order_id]*len(loan_ids)\n\n        return self.lc.assign_to_portfolio(portfolio_name, loan_ids, order_ids)", "category": "Python"}, {"instruction": "def render_targets_weighted_spans(\n        targets,  # type: List[TargetExplanation]\n        preserve_density,  # type: Optional[bool]\n    ):\n    # type: (...) -> List[Optional[str]]\n    \"\"\" Return a list of rendered weighted spans for targets.\n    Function must accept a list in order to select consistent weight\n    ranges across all targets.\n    \"\"\"\n", "input": "", "output": "    prepared_weighted_spans = prepare_weighted_spans(\n        targets, preserve_density)\n\n    def _fmt_pws(pws):\n        # type: (PreparedWeightedSpans) -> str\n        name = ('<b>{}:</b> '.format(pws.doc_weighted_spans.vec_name)\n                if pws.doc_weighted_spans.vec_name else '')\n        return '{}{}'.format(name, render_weighted_spans(pws))\n\n    def _fmt_pws_list(pws_lst):\n        # type: (List[PreparedWeightedSpans]) -> str\n        return '<br/>'.join(_fmt_pws(pws) for pws in pws_lst)\n\n    return [_fmt_pws_list(pws_lst) if pws_lst else None\n            for pws_lst in prepared_weighted_spans]", "category": "Python"}, {"instruction": "def make_wheel_filename_generic(wheel):\n    \"\"\" Wheel filenames contain the python version and the python ABI version\n    for the wheel. https://www.python.org/dev/peps/pep-0427/#file-name-convention\n    Since we're distributing a rust binary this doesn't matter for us ... \"\"\"\n", "input": "", "output": "    name, version, python, abi, platform = wheel.split(\"-\")\n\n    # our binary handles multiple abi/versions of python\n    python, abi = \"py2.py3\", \"none\"\n\n    # hack, lets pretend to be manylinux1 so we can do a binary distribution\n    if platform == \"linux_x86_64.whl\":\n        platform = \"manylinux1_x86_64.whl\"\n    elif platform == \"linux_i686.whl\":\n        platform = \"manylinux1_i686.whl\"\n\n    return \"-\".join((name, version, python, abi, platform))", "category": "Python"}, {"instruction": "def _get_edge_sentences(\n    G: AnalysisGraph, source: str, target: str\n) -> List[str]:\n    \"\"\" Return the sentences that led to the construction of a specified edge.\n\n    Args:\n        G\n        source: The source of the edge.\n        target: The target of the edge.\n    \"\"\"\n", "input": "", "output": "\n    return chain.from_iterable(\n        [\n            [repr(e.text) for e in s.evidence]\n            for s in G.edges[source, target][\"InfluenceStatements\"]\n        ]\n    )", "category": "Python"}, {"instruction": "def close_client_socket(self, config, close_fd=True):\n\t\t\"\"\" :meth:`.WNetworkNativeTransportProto.close_client_socket` method implementation\n\t\t\"\"\"\n", "input": "", "output": "\t\tif close_fd is True:\n\t\t\tself.__client_socket.close()\n\t\tself.__client_socket = None", "category": "Python"}, {"instruction": "def delete(self, hard=False):\n\n        \"\"\"\n        Override the vanilla delete functionality to soft-delete\n        instead. Soft-delete is accomplished by setting the\n        status field to \"deleted\"\n\n        Arguments:\n\n        hard <bool=False> if true, do a hard delete instead, effectively\n        removing the object from the database\n        \"\"\"\n", "input": "", "output": "\n        if hard:\n            return models.Model.delete(self)\n        self.status = \"deleted\"\n        self.save()\n        for key in self._handleref.delete_cascade:\n            q = getattr(self, key).all()\n\n            if not hard:\n                # if we are soft deleting only trigger delete on\n                # objects that are not already deleted, as to avoid\n                # unnecessary re-saves and overriding of updated dates\n                q = q.exclude(status=\"deleted\")\n\n            for child in q:\n                child.delete(hard=hard)", "category": "Python"}, {"instruction": "def process_exception(self, request, exception):\n        \"\"\"\n        Disconnects the signal receiver to prevent it from staying active in case of an exception.\n        \"\"\"\n", "input": "", "output": "        if hasattr(threadlocal, 'actionslog'):\n            pre_save.disconnect(sender=LogAction, dispatch_uid=threadlocal.actionslog['signal_duid'])\n\n        return None", "category": "Python"}, {"instruction": "def _move_leadership(self, state):\n        \"\"\"Attempt to move a random partition to a random broker. If the\n        chosen movement is not possible, None is returned.\n\n        :param state: The starting state.\n\n        :return: The resulting State object if a leader change is found. None\n            if no change is found.\n        \"\"\"\n", "input": "", "output": "        partition = random.randint(0, len(self.cluster_topology.partitions) - 1)\n\n        # Moving zero weight partitions will not improve balance for any of the\n        # balance criteria. Disallow these movements here to avoid wasted\n        # effort.\n        if state.partition_weights[partition] == 0:\n            return None\n        if len(state.replicas[partition]) <= 1:\n            return None\n        dest_index = random.randint(1, len(state.replicas[partition]) - 1)\n        dest = state.replicas[partition][dest_index]\n        if (self.args.max_leader_changes is not None and\n                state.leader_movement_count >= self.args.max_leader_changes):\n            return None\n\n        return state.move_leadership(partition, dest)", "category": "Python"}, {"instruction": "def expose(dists):\n  \"\"\"Exposes vendored code in isolated chroots.\n\n  Any vendored distributions listed in ``dists`` will be unpacked to individual chroots for addition\n  to the ``sys.path``; ie: ``expose(['setuptools', 'wheel'])`` will unpack these vendored\n  distributions and yield the two chroot paths they were unpacked to.\n\n  :param dists: A list of vendored distribution names to expose.\n  :type dists: list of str\n  :raise: :class:`ValueError` if any distributions to expose cannot be found.\n  :returns: An iterator of exposed vendored distribution chroot paths.\n  \"\"\"\n", "input": "", "output": "  from pex.common import safe_delete\n\n  for path in VendorImporter.expose(dists, root=isolated()):\n    safe_delete(os.path.join(path, '__init__.py'))\n    yield path", "category": "Python"}, {"instruction": "def _combine_attribute(attr_1, attr_2, len_1, len_2):\n        \"\"\"\n        Helper function to combine trajectory properties such as site_properties or lattice\n        \"\"\"\n", "input": "", "output": "        if isinstance(attr_1, list) or isinstance(attr_2, list):\n            attribute = np.concatenate((attr_1, attr_2), axis=0)\n            attribute_changes = True\n        else:\n            if isinstance(attr_1, list) and isinstance(attr_2, list) and np.allclose(attr_1, attr_2):\n                attribute = attr_1\n                attribute_changes = False\n            else:\n                attribute = [attr_1.copy()] * len_1 if type(attr_1) != list else attr_1.copy()\n                attribute.extend([attr_2.copy()] * len_2 if type(attr_2 != list) else attr_2.copy())\n                attribute_changes = True\n        return attribute, attribute_changes", "category": "Python"}, {"instruction": "def aug_sysargv(cmdstr):\n    \"\"\" DEBUG FUNC modify argv to look like you ran a command \"\"\"\n", "input": "", "output": "    import shlex\n    argv = shlex.split(cmdstr)\n    sys.argv.extend(argv)", "category": "Python"}, {"instruction": "def build_single(mode):\n    \"\"\"Build, in the single-user mode.\"\"\"\n", "input": "", "output": "    if mode == 'force':\n        amode = ['-a']\n    else:\n        amode = []\n    if executable.endswith('uwsgi'):\n        # hack, might fail in some environments!\n        _executable = executable[:-5] + 'python'\n    else:\n        _executable = executable\n    p = subprocess.Popen([_executable, '-m', 'nikola', 'build'] + amode,\n                         stderr=subprocess.PIPE)\n    p.wait()\n    rl = p.stderr.readlines()\n    try:\n        out = ''.join(rl)\n    except TypeError:\n        out = ''.join(l.decode('utf-8') for l in rl)\n    return (p.returncode == 0), out", "category": "Python"}, {"instruction": "def find_config(test_file=None, defaults=None, root=os.curdir):\n    \"\"\"\n    Find the path to the default config file.\n\n    We look at :root: for the :default: config file. If we can't find it\n    there we start looking at the parent directory recursively until we\n    find a file named :default: and return the absolute path to it.\n    If we can't find anything, we return None.\n\n    Args:\n        default: The name of the config file we look for.\n        root: The directory to start looking for.\n\n    Returns:\n        Path to the default config file, None if we can't find anything.\n    \"\"\"\n", "input": "", "output": "    if defaults is None:\n        defaults = [\".benchbuild.yml\", \".benchbuild.yaml\"]\n\n    def walk_rec(cur_path, root):\n        cur_path = local.path(root) / test_file\n        if cur_path.exists():\n            return cur_path\n\n        new_root = local.path(root) / os.pardir\n        return walk_rec(cur_path, new_root) if new_root != root else None\n\n    if test_file is not None:\n        return walk_rec(test_file, root)\n\n    for test_file in defaults:\n        ret = walk_rec(test_file, root)\n        if ret is not None:\n            return ret", "category": "Python"}, {"instruction": "def update(self, deltat=1.0):\n        '''fly circles, then dive'''\n", "input": "", "output": "        DNFZ.update(self, deltat)\n        self.time_circling += deltat\n        self.setheading(self.heading + self.turn_rate * deltat)\n        self.move(self.drift_heading, self.drift_speed)\n        if self.getalt() > self.max_alt or self.getalt() < self.ground_height():\n            if self.getalt() > self.ground_height():\n                self.setclimbrate(self.dive_rate)\n            else:\n                self.setclimbrate(self.climb_rate)\n        if self.getalt() < self.ground_height():\n            self.setalt(self.ground_height())\n        if self.distance_from_home() > gen_settings.region_width:\n            self.randpos()\n            self.randalt()", "category": "Python"}, {"instruction": "def get(self, session, **kwargs):\n        '''taobao.fenxiao.products.get \u67e5\u8be2\u4ea7\u54c1\u5217\u8868\n        \n        \u67e5\u8be2\u4f9b\u5e94\u5546\u7684\u4ea7\u54c1\u6570\u636e\n        - \u5165\u53c2\u4f20\u5165pids\u5c06\u4f18\u5148\u67e5\u8be2\uff0c\u5373\u53ea\u6309\u8fd9\u4e2a\u6761\u4ef6\u67e5\u8be2\u3002 \n        - \u5165\u53c2\u4f20\u5165sku_number\u5c06\u4f18\u5148\u67e5\u8be2(\u6ca1\u6709\u4f20\u5165pids)\uff0c\u5373\u53ea\u6309\u8fd9\u4e2a\u6761\u4ef6\u67e5\u8be2(\u6700\u591a\u663e\u793a50\u6761) \n        - \u5165\u53c2fields\u4f20skus\u5c06\u67e5\u8be2sku\u7684\u6570\u636e\uff0c\u4e0d\u4f20\u8be5\u53c2\u6570\u9ed8\u8ba4\u4e0d\u67e5\u8be2\uff0c\u8fd4\u56de\u4ea7\u54c1\u7684\u5176\u5b83\u4fe1\u606f\u3002 \n        - \u5165\u53c2fields\u4f20\u5165images\u5c06\u67e5\u8be2\u591a\u56fe\u6570\u636e\uff0c\u4e0d\u4f20\u53ea\u8fd4\u56de\u4e3b\u56fe\u6570\u636e\u3002 \n        - \u5165\u53c2fields\u4ec5\u5bf9\u4f20\u5165pids\u751f\u6548\uff08\u53ea\u6709\u6309ID\u67e5\u8be2\u65f6\uff0c\u624d\u80fd\u67e5\u8be2\u989d\u5916\u7684\u6570\u636e\uff09 \n        - \u67e5\u8be2\u7ed3\u679c\u6309\u7167\u4ea7\u54c1\u53d1\u5e03\u65f6\u95f4\u5012\u5e8f\uff0c\u5373\u65f6\u95f4\u8fd1\u7684\u6570\u636e\u5728\u524d\u3002'''\n", "input": "", "output": "        request = TOPRequest('taobao.fenxiao.products.get')\n        for k, v in kwargs.iteritems():\n            if k not in ('outer_id', 'productcat_id', 'status', 'pids', 'fields', 'start_modified', 'end_modified', 'page_no','page_size','sku_number','is_authz') and v==None: continue\n            request[k] = v\n        self.create(self.execute(request, session))\n        return self.products", "category": "Python"}, {"instruction": "def at_rank(self, rank):\n        \"\"\"\n        Find the node above this node at rank ``rank``\n        \"\"\"\n", "input": "", "output": "        s = self\n        while s:\n            if s.rank == rank:\n                return s\n            s = s.parent\n        raise ValueError(\"No node at rank {0} for {1}\".format(\n            rank, self.tax_id))", "category": "Python"}, {"instruction": "def socket_reader(connection: socket, buffer_size: int = 1024):\n    \"\"\" read data from adb socket \"\"\"\n", "input": "", "output": "    while connection is not None:\n        try:\n            buffer = connection.recv(buffer_size)\n            # no output\n            if not len(buffer):\n                raise ConnectionAbortedError\n        except ConnectionAbortedError:\n            # socket closed\n            print('connection aborted')\n            connection.close()\n            yield None\n        except OSError:\n            # still operate connection after it was closed\n            print('socket closed')\n            connection.close()\n            yield None\n        else:\n            yield buffer", "category": "Python"}, {"instruction": "def OSPFNeighborState_originator_switch_info_switchVcsId(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        OSPFNeighborState = ET.SubElement(config, \"OSPFNeighborState\", xmlns=\"http://brocade.com/ns/brocade-notification-stream\")\n        originator_switch_info = ET.SubElement(OSPFNeighborState, \"originator-switch-info\")\n        switchVcsId = ET.SubElement(originator_switch_info, \"switchVcsId\")\n        switchVcsId.text = kwargs.pop('switchVcsId')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def run_parallel(workflow, n_threads):\n    \"\"\"Run a workflow in parallel threads.\n\n    :param workflow: Workflow or PromisedObject to evaluate.\n    :param n_threads: number of threads to use (in addition to the scheduler).\n    :returns: evaluated workflow.\n    \"\"\"\n", "input": "", "output": "    scheduler = Scheduler()\n    threaded_worker = Queue() >> thread_pool(\n        *repeat(worker, n_threads))\n\n    return scheduler.run(threaded_worker, get_workflow(workflow))", "category": "Python"}, {"instruction": "def get_package_versions(self, name):\n        \"\"\"\n        Gives all the compatible package canonical name\n\n        name : str\n            Name of the package\n        \"\"\"\n", "input": "", "output": "        package_data = self._packages.get(name)\n        versions = []\n\n        if package_data:\n            versions = sort_versions(list(package_data.get('versions', [])))\n\n        return versions", "category": "Python"}, {"instruction": "def _preprocess(df):\n    \"\"\"\n    given a DataFrame where records are stored row-wise, rearrange it\n    such that records are stored column-wise.\n    \"\"\"\n", "input": "", "output": "\n    df = df.stack()\n\n    df.index.rename([\"id\", \"time\"], inplace=True)  # .reset_index()\n    df.name = \"value\"\n    df = df.reset_index()\n\n    return df", "category": "Python"}, {"instruction": "def get_file_extension(self, filepath):\n        \"\"\"\n        This method check mimetype to define file extension.\n        If it can't, it use original-format metadata.\n        \"\"\"\n", "input": "", "output": "        mtype = magic.from_file(filepath, mime=True)\n        if type(mtype) == bytes:\n            mtype = mtype.decode(\"utf-8\")\n\n        if mtype == \"audio/mpeg\":\n            ext = \".mp3\"\n        elif mtype == \"audio/x-wav\":\n            ext = \".wav\"\n        else:\n            ext = \".\" + self.get(\"original-format\")\n        return ext", "category": "Python"}, {"instruction": "def number_of_modified_bytes(buf, fuzzed_buf):\n    \"\"\"Determine the number of differing bytes.\n\n    :param buf: original buffer.\n    :param fuzzed_buf: fuzzed buffer.\n    :return: number of different bytes.\n    :rtype: int\n    \"\"\"\n", "input": "", "output": "    count = 0\n    for idx, b in enumerate(buf):\n        if b != fuzzed_buf[idx]:\n            count += 1\n    return count", "category": "Python"}, {"instruction": "def fling_forward_vertically(self, *args, **selectors):\n        \"\"\"\n        Perform fling forward (vertically)action on the object which has *selectors* attributes.\n\n        Return whether the object can be fling or not.\n        \"\"\"\n", "input": "", "output": "        return self.device(**selectors).fling.vert.forward()", "category": "Python"}, {"instruction": "def handle_snapshot(config_spec, object_ref, reloc_spec, template, vm_):\n    '''\n    Returns a clone spec for cloning from shapshots\n    :rtype vim.vm.CloneSpec\n    '''\n", "input": "", "output": "    if 'snapshot' not in vm_:\n        return None\n\n    allowed_types = [\n        FLATTEN_DISK_FULL_CLONE,\n        COPY_ALL_DISKS_FULL_CLONE,\n        CURRENT_STATE_LINKED_CLONE,\n        QUICK_LINKED_CLONE,\n    ]\n\n    clone_spec = get_clonespec_for_valid_snapshot(\n        config_spec,\n        object_ref,\n        reloc_spec,\n        template,\n        vm_)\n    if not clone_spec:\n        raise SaltCloudSystemExit('Invalid disk move type specified'\n                                  ' supported types are'\n                                  ' {0}'.format(' '.join(allowed_types)))\n    return clone_spec", "category": "Python"}, {"instruction": "def _check_dedup(data):\n    \"\"\"Check configuration for de-duplication.\n\n    Defaults to no de-duplication for RNA-seq and small RNA, the\n    back compatible default. Allow overwriting with explicit\n    `mark_duplicates: true` setting.\n    Also defaults to false for no alignment inputs.\n    \"\"\"\n", "input": "", "output": "    if dd.get_analysis(data).lower() in [\"rna-seq\", \"smallrna-seq\"] or not dd.get_aligner(data):\n        dup_param = utils.get_in(data, (\"config\", \"algorithm\", \"mark_duplicates\"), False)\n    else:\n        dup_param = utils.get_in(data, (\"config\", \"algorithm\", \"mark_duplicates\"), True)\n    if dup_param and isinstance(dup_param, six.string_types):\n        logger.info(\"Warning: bcbio no longer support explicit setting of mark_duplicate algorithm. \"\n                    \"Using best-practice choice based on input data.\")\n        dup_param = True\n    return dup_param", "category": "Python"}, {"instruction": "def range(self, start, end=None, step=1, numSlices=None):\n        \"\"\"\n        Create a new RDD of int containing elements from `start` to `end`\n        (exclusive), increased by `step` every element. Can be called the same\n        way as python's built-in range() function. If called with a single argument,\n        the argument is interpreted as `end`, and `start` is set to 0.\n\n        :param start: the start value\n        :param end: the end value (exclusive)\n        :param step: the incremental step (default: 1)\n        :param numSlices: the number of partitions of the new RDD\n        :return: An RDD of int\n\n        >>> sc.range(5).collect()\n        [0, 1, 2, 3, 4]\n        >>> sc.range(2, 4).collect()\n        [2, 3]\n        >>> sc.range(1, 7, 2).collect()\n        [1, 3, 5]\n        \"\"\"\n", "input": "", "output": "        if end is None:\n            end = start\n            start = 0\n\n        return self.parallelize(xrange(start, end, step), numSlices)", "category": "Python"}, {"instruction": "def _release_waiter(self) -> None:\n        \"\"\"\n        Iterates over all waiters till found one that is not finsihed and\n        belongs to a host that has available connections.\n        \"\"\"\n", "input": "", "output": "        if not self._waiters:\n            return\n\n        # Having the dict keys ordered this avoids to iterate\n        # at the same order at each call.\n        queues = list(self._waiters.keys())\n        random.shuffle(queues)\n\n        for key in queues:\n            if self._available_connections(key) < 1:\n                continue\n\n            waiters = self._waiters[key]\n            while waiters:\n                waiter = waiters.popleft()\n                if not waiter.done():\n                    waiter.set_result(None)\n                    return", "category": "Python"}, {"instruction": "def tri_value(self):\n        \"\"\"\n        See the class documentation.\n        \"\"\"\n", "input": "", "output": "        # This emulates a reverse dependency of 'm && visibility' for\n        # non-optional choices, which is how the C implementation does it\n\n        val = 0 if self.is_optional else 1\n\n        if self.user_value is not None:\n            val = max(val, self.user_value)\n\n        # Warning: See Symbol._rec_invalidate(), and note that this is a hidden\n        # function call (property magic)\n        val = min(val, self.visibility)\n\n        # Promote m to y for boolean choices\n        return 2 if val == 1 and self.type is BOOL else val", "category": "Python"}, {"instruction": "def get_value_map(self):\n        \"\"\"Obtain the value-to-class mapping set by user.\n\n        :returns: The complete mapping as a dict of lists.\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        value_map = {}\n        tree_clone = self.tree_mapping_widget.invisibleRootItem().clone()\n        for tree_branch in tree_clone.takeChildren():\n            value_list = []\n            for tree_leaf in tree_branch.takeChildren():\n                value_list += [tree_leaf.data(0, Qt.UserRole)]\n            if value_list:\n                value_map[tree_branch.data(0, Qt.UserRole)] = value_list\n        return value_map", "category": "Python"}, {"instruction": "def parse_chunks(b):\n\t\"\"\"Parse PNG bytes into multiple chunks. \n\t\n\t:arg bytes b: The raw bytes of the PNG file.\n\t:return: A generator yielding :class:`Chunk`.\n\t:rtype: Iterator[Chunk]\n\t\"\"\"\n", "input": "", "output": "\t# skip signature\n\ti = 8\n\t# yield chunks\n\twhile i < len(b):\n\t\tdata_len, = struct.unpack(\"!I\", b[i:i+4])\n\t\ttype_ = b[i+4:i+8].decode(\"latin-1\")\n\t\tyield Chunk(type_, b[i:i+data_len+12])\n\t\ti += data_len + 12", "category": "Python"}, {"instruction": "def head(self, n=10):\n        \"\"\"\n        Display the top of the file.\n\n        Args:\n            n (int): Number of lines to display\n        \"\"\"\n", "input": "", "output": "        r = self.__repr__().split('\\n')\n        print('\\n'.join(r[:n]), end=' ')", "category": "Python"}, {"instruction": "def nucleotidesToStr(nucleotides, prefix=''):\n    \"\"\"\n    Convert offsets and base counts to a string.\n\n    @param nucleotides: A C{defaultdict(Counter)} instance, keyed\n        by C{int} offset, with nucleotides keying the Counters.\n    @param prefix: A C{str} to put at the start of each line.\n    @return: A C{str} representation of the offsets and nucleotide\n        counts for each.\n    \"\"\"\n", "input": "", "output": "    result = []\n    for offset in sorted(nucleotides):\n        result.append(\n            '%s%d: %s' % (prefix, offset,\n                          baseCountsToStr(nucleotides[offset])))\n    return '\\n'.join(result)", "category": "Python"}, {"instruction": "def __quick_validate(r, check=('ResultSet', 'Result', 'totalRecords')):\n    '''\n    Quick validation of JSON result set returned by XNAT.\n\n    :param r: Result set data in JSON format\n    :type r: dict\n    :param check: Fields to check\n    :type check: tuple\n    :returns: Result set is valid\n    :rtype: bool\n    '''\n", "input": "", "output": "    if 'ResultSet' in check and 'ResultSet' not in r:\n        raise ResultSetError('no ResultSet in server response')\n    if 'Result' in check and 'Result' not in r['ResultSet']:\n        raise ResultSetError('no Result in server response')\n    if 'totalRecords' in check and 'totalRecords' not in r['ResultSet']:\n        raise ResultSetError('no totalRecords in server response')\n    return True", "category": "Python"}, {"instruction": "def _retrieve_binary(self, file_name):\n        \"\"\"Retrieve a file in binary transfer mode.\"\"\"\n", "input": "", "output": "        with open(file_name, 'wb') as f:\n            return self.session.retrbinary('RETR ' + file_name, f.write)", "category": "Python"}, {"instruction": "def _createCheckAuthRequest(self, message):\n        \"\"\"Generate a check_authentication request message given an\n        id_res message.\n        \"\"\"\n", "input": "", "output": "        signed = message.getArg(OPENID_NS, 'signed')\n        if signed:\n            if isinstance(signed, bytes):\n                signed = str(signed, encoding=\"utf-8\")\n            for k in signed.split(','):\n                logging.info(k)\n                val = message.getAliasedArg(k)\n\n                # Signed value is missing\n                if val is None:\n                    logging.info('Missing signed field %r' % (k, ))\n                    return None\n\n        check_auth_message = message.copy()\n        check_auth_message.setArg(OPENID_NS, 'mode', 'check_authentication')\n        return check_auth_message", "category": "Python"}, {"instruction": "def ReadCronJobs(self, cronjob_ids=None):\n    \"\"\"Reads a cronjob from the database.\"\"\"\n", "input": "", "output": "    if cronjob_ids is None:\n      res = [job.Copy() for job in itervalues(self.cronjobs)]\n\n    else:\n      res = []\n      for job_id in cronjob_ids:\n        try:\n          res.append(self.cronjobs[job_id].Copy())\n        except KeyError:\n          raise db.UnknownCronJobError(\"Cron job with id %s not found.\" %\n                                       job_id)\n\n    for job in res:\n      lease = self.cronjob_leases.get(job.cron_job_id)\n      if lease:\n        job.leased_until, job.leased_by = lease\n    return res", "category": "Python"}, {"instruction": "def stats(self, ops=(min, max, np.median, sum)):\n        \"\"\"Compute statistics for each column and place them in a table.\"\"\"\n", "input": "", "output": "        names = [op.__name__ for op in ops]\n        ops = [_zero_on_type_error(op) for op in ops]\n        columns = [[op(column) for op in ops] for column in self.columns]\n        table = type(self)().with_columns(zip(self.labels, columns))\n        stats = table._unused_label('statistic')\n        table[stats] = names\n        table.move_to_start(stats)\n        return table", "category": "Python"}, {"instruction": "def _addToHosts(self, node, destinationIP=None):\n        \"\"\"\n        Add an \"privateIP hostname\" line to the /etc/hosts file. If destinationIP is given,\n        do this on the remote machine.\n\n        Azure VMs sometimes fail to initialize, causing the appliance to fail.\n        This error is given:\n           Failed to obtain the IP address for 'l7d41a19b-15a6-442c-8ba1-9678a951d824';\n           the DNS service may not be able to resolve it: Name or service not known.\n        This method is a fix.\n\n        :param node: Node to add to /etc/hosts.\n        :param destinationIP: A remote host's address\n        \"\"\"\n", "input": "", "output": "        cmd = \"echo %s %s | sudo tee --append /etc/hosts > /dev/null\" % (node.privateIP, node.name)\n        logger.debug(\"Running command %s on %s\" % (cmd, destinationIP))\n        if destinationIP:\n            subprocess.Popen([\"ssh\", \"-oStrictHostKeyChecking=no\", \"core@%s\" % destinationIP, cmd])\n        else:\n            subprocess.Popen(cmd, shell=True)", "category": "Python"}, {"instruction": "def close(self):\n    \"\"\"Closes the file-like object.\n\n    Raises:\n      IOError: if the file-like object was not opened or the close failed.\n      OSError: if the file-like object was not opened or the close failed.\n    \"\"\"\n", "input": "", "output": "    if not self._is_open:\n      raise IOError('Not opened.')\n\n    if not self._is_cached:\n      close_file_object = True\n    elif self._resolver_context.ReleaseFileObject(self):\n      self._is_cached = False\n      close_file_object = True\n    else:\n      close_file_object = False\n\n    if close_file_object:\n      self._Close()\n      self._is_open = False", "category": "Python"}, {"instruction": "def get_lldp_neighbors(self):\n        \"\"\"Return LLDP neighbors details.\"\"\"\n", "input": "", "output": "        lldp = junos_views.junos_lldp_table(self.device)\n        try:\n            lldp.get()\n        except RpcError as rpcerr:\n            # this assumes the library runs in an environment\n            # able to handle logs\n            # otherwise, the user just won't see this happening\n            log.error('Unable to retrieve the LLDP neighbors information:')\n            log.error(rpcerr.message)\n            return {}\n        result = lldp.items()\n\n        neighbors = {}\n        for neigh in result:\n            if neigh[0] not in neighbors.keys():\n                neighbors[neigh[0]] = []\n            neighbors[neigh[0]].append({x[0]: py23_compat.text_type(x[1]) for x in neigh[1]})\n\n        return neighbors", "category": "Python"}, {"instruction": "def create_h5py_with_large_cache(filename, cache_size_mb):\n    \"\"\"\nAllows to open the hdf5 file with specified cache size\n    \"\"\"\n", "input": "", "output": "    # h5py does not allow to control the cache size from the high level\n    # we employ the workaround\n    # sources:\n    #http://stackoverflow.com/questions/14653259/how-to-set-cache-settings-while-using-h5py-high-level-interface\n    #https://groups.google.com/forum/#!msg/h5py/RVx1ZB6LpE4/KH57vq5yw2AJ\n    propfaid = h5py.h5p.create(h5py.h5p.FILE_ACCESS)\n    settings = list(propfaid.get_cache())\n    settings[2] = 1024 * 1024 * cache_size_mb\n    propfaid.set_cache(*settings)\n    fid = h5py.h5f.create(filename, flags=h5py.h5f.ACC_EXCL, fapl=propfaid)\n    fin = h5py.File(fid)\n    return fin", "category": "Python"}, {"instruction": "def _repeat_word(word, N, word_space=\" \"):\n    \"\"\"\n    Return a repeated string\n\n    >>> word = \"PARIS\"\n\n    >>> _repeat_word(word, 5)\n    'PARIS PARIS PARIS PARIS PARIS'\n\n    >>> _repeat_word(word, 5, word_space=\"\")\n    'PARISPARISPARISPARISPARIS'\n    \"\"\"\n", "input": "", "output": "    message = (word_space + word) * N\n    message = message[len(word_space):]\n    return message", "category": "Python"}, {"instruction": "def _open(self):\n        \"\"\" Open connection to remote host. \"\"\"\n", "input": "", "output": "        if self._process is not None:\n            return\n\n        cmd = [\n            'ssh',\n            self._host,\n            'sudo',\n            'buttersink',\n            '--server',\n            '--mode',\n            self._mode,\n            self._directory\n        ]\n        logger.debug(\"Connecting with: %s\", cmd)\n        self._process = subprocess.Popen(\n            cmd,\n            stdin=subprocess.PIPE,\n            stderr=sys.stderr,\n            # stdout=sys.stdout,\n            stdout=subprocess.PIPE,\n        )\n\n        version = self.version()\n        logger.info(\"Remote version: %s\", version)", "category": "Python"}, {"instruction": "def quartic_paired_acquaintance_strategy(\n    qubit_pairs: Iterable[Tuple[ops.Qid, ops.Qid]]\n    ) -> Tuple[circuits.Circuit, Sequence[ops.Qid]]:\n    \"\"\"Acquaintance strategy for pairs of pairs.\n\n    Implements UpCCGSD ansatz from arXiv:1810.02327.\n    \"\"\"\n", "input": "", "output": "\n    qubit_pairs = tuple(\n            cast(Tuple[ops.Qid, ops.Qid], tuple(qubit_pair))\n            for qubit_pair in qubit_pairs)\n    qubits = qubit_pairs_to_qubit_order(qubit_pairs)\n    n_qubits = len(qubits)\n    swap_network = SwapNetworkGate((1,) * n_qubits, 2)(*qubits)\n    strategy = circuits.Circuit.from_ops(swap_network,\n            device=UnconstrainedAcquaintanceDevice)\n    expose_acquaintance_gates(strategy)\n    for i in reversed(range(0, n_qubits, 2)):\n        moment = ops.Moment([acquaint(*qubits[j: j + 4])\n                             for j in range(i % 4, n_qubits - 3, 4)])\n        strategy.insert(2 * i, moment)\n    return strategy, qubits", "category": "Python"}, {"instruction": "def update_cycles(self):\n        \"\"\"Enable cycles checkbox only if there are cycles marked, with no\n        errors.\"\"\"\n", "input": "", "output": "        self.idx_cycle.clear()\n\n        try:\n            self.cycles = self.parent.notes.annot.get_cycles()\n\n        except ValueError as err:\n            self.idx_cycle.setEnabled(False)\n            msg = 'There is a problem with the cycle markers: ' + str(err)\n            self.parent.statusBar().showMessage(msg)\n\n        else:\n            if self.cycles is None:\n                self.idx_cycle.setEnabled(False)\n            else:\n                self.idx_cycle.setEnabled(True)\n                for i in range(len(self.cycles)):\n                    self.idx_cycle.addItem(str(i+1))", "category": "Python"}, {"instruction": "def accumulate_items(items, reduce_each=False):\n    \"\"\" :return: item pairs as key: val, with vals under duplicate keys accumulated under each \"\"\"\n", "input": "", "output": "\n    if not items:\n        return {}\n\n    accumulated = defaultdict(list)\n    for key, val in items:\n        accumulated[key].append(val)\n\n    if not reduce_each:\n        return accumulated\n    else:\n        return {k: reduce_value(v, v) for k, v in iteritems(accumulated)}", "category": "Python"}, {"instruction": "def establish_scp_conn(self):\n        \"\"\"Establish the secure copy connection.\"\"\"\n", "input": "", "output": "        ssh_connect_params = self.ssh_ctl_chan._connect_params_dict()\n        self.scp_conn = self.ssh_ctl_chan._build_ssh_client()\n        self.scp_conn.connect(**ssh_connect_params)\n        self.scp_client = scp.SCPClient(self.scp_conn.get_transport())", "category": "Python"}, {"instruction": "def set_maxrad(self,maxrad, distribution_skip=True):\n        \"\"\"\n        Adds a constraint that rejects everything with Rsky > maxrad\n\n        Requires ``Rsky`` attribute, which should always have units.\n\n        :param maxrad:\n            The maximum angular value of Rsky.\n        :type maxrad:\n            :class:`astropy.units.Quantity`\n\n        :param distribution_skip:\n            This is by default ``True``.  *To be honest, I'm not\n            exactly sure why.  Might be important, might not\n            (don't remember).*\n\n        \"\"\"\n", "input": "", "output": "        self.maxrad = maxrad\n        self.apply_constraint(UpperLimit(self.Rsky,maxrad,\n                                         name='Max Rsky'),\n                              overwrite=True,\n                              distribution_skip=distribution_skip)", "category": "Python"}, {"instruction": "def get_wave_form(self):\r\n        ''' getter '''\n", "input": "", "output": "        if isinstance(self.__wave_form, WaveFormInterface) is False:\r\n            raise TypeError()\r\n        return self.__wave_form", "category": "Python"}, {"instruction": "def cmd_follow(self, args):\n        '''control following of vehicle'''\n", "input": "", "output": "        if len(args) < 2:\n            print(\"map follow 0|1\")\n            return\n        follow = int(args[1])\n        self.map.set_follow(follow)", "category": "Python"}, {"instruction": "def add_output_arg(self, out):\n        \"\"\" Add an output as an argument\n        \"\"\"\n", "input": "", "output": "        self.add_arg(out._dax_repr())\n        self._add_output(out)", "category": "Python"}, {"instruction": "def get_pythainlp_data_path() -> str:\n    \"\"\"\n    Return full path where PyThaiNLP keeps its (downloaded) data\n    \"\"\"\n", "input": "", "output": "    path = os.path.join(os.path.expanduser(\"~\"), PYTHAINLP_DATA_DIR)\n    if not os.path.exists(path):\n        os.makedirs(path)\n    return path", "category": "Python"}, {"instruction": "def set_message(self, title, msg, typ, url=None):\n        \"\"\"\n        Sets user notification message.\n\n        Args:\n            title: Msg. title\n            msg:  Msg. text\n            typ: Msg. type\n            url: Additional URL (if exists)\n\n        Returns:\n            Message ID.\n        \"\"\"\n", "input": "", "output": "        return self.user.send_notification(title=title,\n                                           message=msg,\n                                           typ=typ,\n                                           url=url)", "category": "Python"}, {"instruction": "def dijkstra_single_path_length(G, start, end):\r\n    \"\"\"\r\n    Compute shortest path length between satrt\r\n    and end for a weight graph. return -> (length, [path])\r\n    \"\"\"\n", "input": "", "output": "    if start not in G.vertices:\r\n        raise GraphInsertError(\"Vertex %s doesn't exist.\" % (start,))\r\n    if end not in G.vertices:\r\n        raise GraphInsertError(\"Vertex %s doesn't exist.\" % (end,))\r\n    dijkstra_data = dijkstra(G, start)\r\n    length = dijkstra_data[0][end]\r\n    path = [end]\r\n    current = end\r\n    while current is not start:\r\n        for vertex in dijkstra_data[1]:\r\n            if vertex is current:\r\n                path.append(dijkstra_data[1][vertex])\r\n                current = dijkstra_data[1][vertex]\r\n                break\r\n    return length, path", "category": "Python"}, {"instruction": "def _verifyDiscoveryResults(self, resp_msg, endpoint=None):\n        \"\"\"\n        Extract the information from an OpenID assertion message and\n        verify it against the original\n\n        @param endpoint: The endpoint that resulted from doing discovery\n        @param resp_msg: The id_res message object\n\n        @returns: the verified endpoint\n        \"\"\"\n", "input": "", "output": "        if resp_msg.getOpenIDNamespace() == OPENID2_NS:\n            return self._verifyDiscoveryResultsOpenID2(resp_msg, endpoint)\n        else:\n            return self._verifyDiscoveryResultsOpenID1(resp_msg, endpoint)", "category": "Python"}, {"instruction": "def send_zipfile(request, fileList):\n    \"\"\"                                                                         \n    Create a ZIP file on disk and transmit it in chunks of 8KB,                 \n    without loading the whole file into memory. A similar approach can          \n    be used for large dynamic PDF files.                                        \n    \"\"\"\n", "input": "", "output": "    temp = tempfile.TemporaryFile()\n    archive = zipfile.ZipFile(temp, 'w', zipfile.ZIP_DEFLATED)\n    for artist,files in fileList.iteritems():\n        for f in files:\n            archive.write(f[0], '%s/%s' % (artist, f[1]))\n    archive.close()\n    wrapper = FixedFileWrapper(temp)\n    response = HttpResponse(wrapper, content_type='application/zip')\n    response['Content-Disposition'] = 'attachment; filename=FrogSources.zip'\n    response['Content-Length'] = temp.tell()\n    temp.seek(0)\n    return response", "category": "Python"}, {"instruction": "def _ignore_sql(self, query):\n        \"\"\"Check to see if we should ignore the sql query.\"\"\"\n", "input": "", "output": "        return any([\n            re.search(pattern, query.get('sql')) for pattern in QC_SETTINGS['IGNORE_SQL_PATTERNS']\n        ])", "category": "Python"}, {"instruction": "def _initializer_wrapper(initializer, *args):\n    \"\"\"\n    Ignore SIGINT. During typical keyboard interrupts, the parent does the\n    killing.\n    \"\"\"\n", "input": "", "output": "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n    if initializer is not None:\n        initializer(*args)", "category": "Python"}, {"instruction": "def make_user_config_dir():\n    \"\"\"\n    Create the user s-tui config directory if it doesn't exist\n    \"\"\"\n", "input": "", "output": "    config_path = get_user_config_dir()\n\n    if not user_config_dir_exists():\n        try:\n            os.mkdir(config_path)\n            os.mkdir(os.path.join(config_path, 'hooks.d'))\n        except OSError:\n            return None\n\n    return config_path", "category": "Python"}, {"instruction": "def vizlib_binary_features(span1, span2):\n    \"\"\"\n    Visual-related features for a pair of spans\n    \"\"\"\n", "input": "", "output": "    if same_page((span1, span2)):\n        yield \"SAME_PAGE\", DEF_VALUE\n\n        if is_horz_aligned((span1, span2)):\n            yield \"HORZ_ALIGNED\", DEF_VALUE\n\n        if is_vert_aligned((span1, span2)):\n            yield \"VERT_ALIGNED\", DEF_VALUE\n\n        if is_vert_aligned_left((span1, span2)):\n            yield \"VERT_ALIGNED_LEFT\", DEF_VALUE\n\n        if is_vert_aligned_right((span1, span2)):\n            yield \"VERT_ALIGNED_RIGHT\", DEF_VALUE\n\n        if is_vert_aligned_center((span1, span2)):\n            yield \"VERT_ALIGNED_CENTER\", DEF_VALUE", "category": "Python"}, {"instruction": "def size(self):\n        \"\"\" Returns the total number of queued jobs on the queue \"\"\"\n", "input": "", "output": "\n        if self.id.endswith(\"/\"):\n            subqueues = self.get_known_subqueues()\n            if len(subqueues) == 0:\n                return 0\n            else:\n                with context.connections.redis.pipeline(transaction=False) as pipe:\n                    for subqueue in subqueues:\n                        pipe.get(\"queuesize:%s\" % subqueue)\n                    return [int(size or 0) for size in pipe.execute()]\n        else:\n            return int(context.connections.redis.get(\"queuesize:%s\" % self.id) or 0)", "category": "Python"}, {"instruction": "def convert(self, path, version, target = None):\n        \"\"\"Converts the specified file using the relevant template.\n\n        :arg path: the full path to the file to convert.\n        :arg version: the new version of the file.\n        :arg target: the optional path to save the file under. If not\n          specified, the file is saved based on the template file name.\n        \"\"\"\n", "input": "", "output": "        #Get the template and values out of the XML input file and\n        #write them in the format of the keywordless file.\n        values, template = self.parse(path)\n        lines = template.write(values, version)\n\n        #Finally, write the lines to the correct path.\n        if target is None:\n            target = os.path.join(os.path.dirname(path), template.name)\n\n        with open(os.path.expanduser(target), 'w') as f:\n            f.write(\"\\n\".join(lines))", "category": "Python"}, {"instruction": "def delete_model_translation(self, request, translation):\n        \"\"\"\n        Hook for deleting a translation.\n        This calls :func:`get_translation_objects` to collect all related objects for the translation.\n        By default, that includes the translations for inline objects.\n        \"\"\"\n", "input": "", "output": "        master = translation.master\n        for qs in self.get_translation_objects(request, translation.language_code, obj=master, inlines=self.delete_inline_translations):\n            if isinstance(qs, (tuple, list)):\n                # The objects are deleted one by one.\n                # This triggers the post_delete signals and such.\n                for obj in qs:\n                    obj.delete()\n            else:\n                # Also delete translations of inlines which the user has access to.\n                # This doesn't trigger signals, just like the regular\n                qs.delete()", "category": "Python"}, {"instruction": "def move_rule_after(self, other_rule):\n        \"\"\"\n        Add this rule after another. This process will make a copy of\n        the existing rule and add after the specified rule. If this\n        raises an exception, processing is stopped. Otherwise the original\n        rule is then deleted.\n        You must re-retrieve the new element after running this operation\n        as new references will be created.\n        \n        :param other_rule Rule: rule where this rule will be positioned after\n        :raises CreateRuleFailed: failed to duplicate this rule, no move\n            is made\n        \"\"\"\n", "input": "", "output": "        self.make_request(\n            CreateRuleFailed,\n            href=other_rule.get_relation('add_after'),\n            method='create',\n            json=self)\n        self.delete()", "category": "Python"}, {"instruction": "def init_app(self, app, env_file=None, verbose_mode=False):\n        \"\"\"Imports .env file.\"\"\"\n", "input": "", "output": "        if self.app is None:\n            self.app = app\n        self.verbose_mode = verbose_mode\n\n        if env_file is None:\n            env_file = os.path.join(os.getcwd(), \".env\")\n        if not os.path.exists(env_file):\n            warnings.warn(\"can't read {0} - it doesn't exist\".format(env_file))\n        else:\n            self.__import_vars(env_file)", "category": "Python"}, {"instruction": "def connection_class(self, adapter):\n        \"\"\"Get connection class by adapter\"\"\"\n", "input": "", "output": "        if self.adapters.get(adapter):\n            return self.adapters[adapter]\n        try:\n            class_prefix = getattr(\n                __import__('db.' + adapter, globals(), locals(),\n                           ['__class_prefix__']), '__class_prefix__')\n            driver = self._import_class('db.' + adapter + '.connection.' +\n                                        class_prefix + 'Connection')\n        except ImportError:\n            raise DBError(\"Must install adapter `%s` or doesn't support\" %\n                          (adapter))\n\n        self.adapters[adapter] = driver\n        return driver", "category": "Python"}, {"instruction": "def snapshot_get(repository, snapshot, ignore_unavailable=False, hosts=None, profile=None):\n    '''\n    .. versionadded:: 2017.7.0\n\n    Obtain snapshot residing in specified repository.\n\n    repository\n        Repository name\n    snapshot\n        Snapshot name, use _all to obtain all snapshots in specified repository\n    ignore_unavailable\n        Ignore unavailable snapshots\n\n    CLI example::\n\n        salt myminion elasticsearch.snapshot_get testrepo testsnapshot\n    '''\n", "input": "", "output": "    es = _get_instance(hosts, profile)\n\n    try:\n        return es.snapshot.get(repository=repository, snapshot=snapshot, ignore_unavailable=ignore_unavailable)\n    except elasticsearch.TransportError as e:\n        raise CommandExecutionError(\"Cannot obtain details of snapshot {0} in repository {1}, server returned code {2} with message {3}\".format(snapshot, repository, e.status_code, e.error))", "category": "Python"}, {"instruction": "def get_connection(self):\n        \"\"\"Creates a new connection\"\"\"\n", "input": "", "output": "        if self.host is None:\n            connector = SpamCUnixConnector\n            conn = connector(self.socket_file, self.backend_mod)\n        else:\n            connector = SpamCTcpConnector\n            conn = connector(\n                self.host,\n                self.port,\n                self.backend_mod,\n                is_ssl=self.is_ssl,\n                **self.ssl_args)\n        return conn", "category": "Python"}, {"instruction": "def realloc(self, ptr, size):\n        \"\"\"\n        A somewhat faithful implementation of libc `realloc`.\n\n        :param ptr:  the location in memory to be reallocated\n        :param size: the new size desired for the allocation\n        :returns:    the address of the allocation, or a NULL pointer if the allocation was freed or if no new allocation\n                     was made\n        \"\"\"\n", "input": "", "output": "        raise NotImplementedError(\"%s not implemented for %s\" % (self.realloc.__func__.__name__,\n                                                                 self.__class__.__name__))", "category": "Python"}, {"instruction": "def deposit(self, amount):\r\n        \"\"\"\r\n\r\n        :param amount: (int +/-) amount to be deposited or withdrawn in cents\r\n        \"\"\"\n", "input": "", "output": "        _json = {\"amount\": amount}\r\n        self.api_interface.piggy_bank_deposit(self, _json)", "category": "Python"}, {"instruction": "def _CreateDictReader(self, line_reader):\n    \"\"\"Returns a reader that processes each row and yields dictionaries.\n\n    csv.DictReader does this job well for single-character delimiters; parsers\n    that need multi-character delimiters need to override this method.\n\n    Args:\n      line_reader (iter): yields lines from a file-like object.\n\n    Returns:\n      iter: a reader of dictionaries, as returned by csv.DictReader().\n    \"\"\"\n", "input": "", "output": "    delimiter = self.DELIMITER\n    quotechar = self.QUOTE_CHAR\n    magic_test_string = self._MAGIC_TEST_STRING\n    # Python 3 csv module requires arguments to constructor to be of type str.\n    if py2to3.PY_3:\n      delimiter = delimiter.decode(self._encoding)\n      quotechar = quotechar.decode(self._encoding)\n      magic_test_string = magic_test_string.decode(self._encoding)\n\n    return csv.DictReader(\n        line_reader, delimiter=delimiter, fieldnames=self.COLUMNS,\n        quotechar=quotechar, restkey=magic_test_string,\n        restval=magic_test_string)", "category": "Python"}, {"instruction": "def banner_motd(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        banner = ET.SubElement(config, \"banner\", xmlns=\"urn:brocade.com:mgmt:brocade-aaa\")\n        motd = ET.SubElement(banner, \"motd\")\n        motd.text = kwargs.pop('motd')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def __parameter_enum(self, final_subfield):\n    \"\"\"Returns enum descriptor of final subfield if it is an enum.\n\n    An enum descriptor is a dictionary with keys as the names from the enum and\n    each value is a dictionary with a single key \"backendValue\" and value equal\n    to the same enum name used to stored it in the descriptor.\n\n    The key \"description\" can also be used next to \"backendValue\", but protorpc\n    Enum classes have no way of supporting a description for each value.\n\n    Args:\n      final_subfield: A simple field from the end of a subfield list.\n\n    Returns:\n      The enum descriptor for the field, if it's an enum descriptor, else\n          returns None.\n    \"\"\"\n", "input": "", "output": "    if isinstance(final_subfield, messages.EnumField):\n      enum_descriptor = {}\n      for enum_value in final_subfield.type.to_dict().keys():\n        enum_descriptor[enum_value] = {'backendValue': enum_value}\n      return enum_descriptor", "category": "Python"}, {"instruction": "def load(cls, filename, dir=None, main=False, **kwargs):\n        \"\"\"Import a notebook as a module from a filename.\n        \n        dir: The directory to load the file from.\n        main: Load the module in the __main__ context.\n        \n        > assert Notebook.load('loader.ipynb')\n        \"\"\"\n", "input": "", "output": "        name = main and \"__main__\" or Path(filename).stem\n        loader = cls(name, str(filename), **kwargs)\n        module = module_from_spec(FileModuleSpec(name, loader, origin=loader.path))\n        cwd = str(Path(loader.path).parent)\n        try:\n            with ExitStack() as stack:\n                sys.path.append(cwd)\n                loader.name != \"__main__\" and stack.enter_context(_installed_safely(module))\n                loader.exec_module(module)\n        finally:\n            sys.path.pop()\n\n        return module", "category": "Python"}, {"instruction": "def get_collections(self):\n        \"\"\"\n        Returns a flat list of the names of collections in the asset\n        service.\n\n        ..\n\n            ['wind-turbines', 'jet-engines']\n\n        \"\"\"\n", "input": "", "output": "        collections = []\n        for result in self._get_collections():\n            collections.append(result['collection'])\n\n        return collections", "category": "Python"}, {"instruction": "def mvhermgauss(H: int, D: int):\n    \"\"\"\n    Return the evaluation locations 'xn', and weights 'wn' for a multivariate\n    Gauss-Hermite quadrature.\n\n    The outputs can be used to approximate the following type of integral:\n    int exp(-x)*f(x) dx ~ sum_i w[i,:]*f(x[i,:])\n\n    :param H: Number of Gauss-Hermite evaluation points.\n    :param D: Number of input dimensions. Needs to be known at call-time.\n    :return: eval_locations 'x' (H**DxD), weights 'w' (H**D)\n    \"\"\"\n", "input": "", "output": "    gh_x, gh_w = hermgauss(H)\n    x = np.array(list(itertools.product(*(gh_x,) * D)))  # H**DxD\n    w = np.prod(np.array(list(itertools.product(*(gh_w,) * D))), 1)  # H**D\n    return x, w", "category": "Python"}, {"instruction": "def populate(self, priority, address, rtr, data):\n        \"\"\"\n        :return: None\n        \"\"\"\n", "input": "", "output": "        self.needs_low_priority(priority)\n        self.needs_rtr(rtr)\n        self.needs_no_data(data)\n        self.set_attributes(priority, address, rtr)", "category": "Python"}, {"instruction": "def map(self, callable):\n        \"\"\" Apply 'callable' function over all values. \"\"\"\n", "input": "", "output": "        for k,v in self.iteritems():\n            self[k] = callable(v)", "category": "Python"}, {"instruction": "def split_vcf(in_file, ref_file, config, out_dir=None):\n    \"\"\"Split a VCF file into separate files by chromosome.\n    \"\"\"\n", "input": "", "output": "    if out_dir is None:\n        out_dir = os.path.join(os.path.dirname(in_file), \"split\")\n    out_files = []\n    with open(ref.fasta_idx(ref_file, config)) as in_handle:\n        for line in in_handle:\n            chrom, size = line.split()[:2]\n            out_file = os.path.join(out_dir,\n                                    os.path.basename(replace_suffix(append_stem(in_file, \"-%s\" % chrom), \".vcf\")))\n            subset_vcf(in_file, (chrom, 0, size), out_file, config)\n            out_files.append(out_file)\n    return out_files", "category": "Python"}, {"instruction": "def proximal(self):\n        \"\"\"Return the `proximal factory` of the functional.\n\n        See Also\n        --------\n        odl.solvers.nonsmooth.proximal_operators.proximal_convex_conj_l1 :\n            `proximal factory` for convex conjuagte of L1-norm.\n        odl.solvers.nonsmooth.proximal_operators.proximal_convex_conj_l2 :\n            `proximal factory` for convex conjuagte of L2-norm.\n        odl.solvers.nonsmooth.proximal_operators.proximal_convex_conj_linfty :\n            `proximal factory` for convex conjuagte of Linfty-norm.\n        \"\"\"\n", "input": "", "output": "        if self.exponent == np.inf:\n            return proximal_convex_conj_l1(space=self.domain)\n        elif self.exponent == 2:\n            return proximal_convex_conj_l2(space=self.domain)\n        elif self.exponent == 1:\n            return proximal_convex_conj_linfty(space=self.domain)\n        else:\n            raise NotImplementedError('`proximal` only implemented for p=1, '\n                                      'p=2 or p=inf')", "category": "Python"}, {"instruction": "def dependencies(self):\n        \"\"\"\n        The number of extra rows needed for each of our inputs to compute this\n        term.\n        \"\"\"\n", "input": "", "output": "        extra_input_rows = max(0, self.window_length - 1)\n        out = {}\n        for term in self.inputs:\n            out[term] = extra_input_rows\n        out[self.mask] = 0\n        return out", "category": "Python"}, {"instruction": "def resolve_object_property(obj, path: str):\n    \"\"\"Resolves the value of a property on an object.\n\n    Is able to resolve nested properties. For example,\n    a path can be specified:\n\n        'other.beer.name'\n\n    Raises:\n        AttributeError:\n            In case the property could not be resolved.\n\n    Returns:\n        The value of the specified property.\n    \"\"\"\n", "input": "", "output": "\n    value = obj\n    for path_part in path.split('.'):\n        value = getattr(value, path_part)\n\n    return value", "category": "Python"}, {"instruction": "def main():\n    \"\"\"\n    This is the aeneas-cli \"hydra\" script,\n    to be compiled by pyinstaller.\n    \"\"\"\n", "input": "", "output": "    if FROZEN:\n        HydraCLI(invoke=\"aeneas-cli\").run(\n            arguments=sys.argv,\n            show_help=False\n        )\n    else:\n        HydraCLI(invoke=\"pyinstaller-aeneas-cli.py\").run(\n            arguments=sys.argv,\n            show_help=False\n        )", "category": "Python"}, {"instruction": "def infer_enum(node, context=None):\n    \"\"\" Specific inference function for enum Call node. \"\"\"\n", "input": "", "output": "    enum_meta = extract_node(\n        ", "category": "Python"}, {"instruction": "def get_user_permission_from_email(self, email):\n        \"\"\" Returns a user's permissions object when given the user email.\"\"\"\n", "input": "", "output": "        _id = self.get_user_id_from_email(email)\n        return self.get_user_permission(_id)", "category": "Python"}, {"instruction": "def pause(self):\n        \"\"\"Pause pulse capture\"\"\"\n", "input": "", "output": "        self._mq.send(\"p\", True, type=1)\n        self._paused = True", "category": "Python"}, {"instruction": "def shutdown(self):\n        \"\"\"Disconnect all connections and end the loop\n\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        log.debug('Shutting down %s' % self)\n        self.disconnect_all()\n        self._looping.clear()", "category": "Python"}, {"instruction": "def get_model(PARAMS):\n    '''Get model according to parameters'''\n", "input": "", "output": "    model = SVC()\n    model.C = PARAMS.get('C')\n    model.keral = PARAMS.get('keral')\n    model.degree = PARAMS.get('degree')\n    model.gamma = PARAMS.get('gamma')\n    model.coef0 = PARAMS.get('coef0')\n    \n    return model", "category": "Python"}, {"instruction": "def remove(self, uuid_or_id, identity=False):\n        \"\"\"Remove an identity from the registry.\n\n        This method removes the unique identity associated to the given\n        uuid and its related information, such as identities or enrollments.\n        When <identity> is set to True, it will remove the identity (not the\n        unique identity) which its id is <uuid_or_id>.\n\n        :param uuid_or_id: identifier of the unique identity or\n            identity to remove\n        :param identity: when it is set to True, it will remove an identity,\n            not a unique identity.  By default it is set to False.\n        \"\"\"\n", "input": "", "output": "        if not uuid_or_id:\n            return CMD_SUCCESS\n\n        try:\n            if not identity:\n                api.delete_unique_identity(self.db, uuid_or_id)\n            else:\n                api.delete_identity(self.db, uuid_or_id)\n\n            self.display('remove.tmpl',\n                         uuid_or_id=uuid_or_id, identity=identity)\n        except NotFoundError as e:\n            self.error(str(e))\n            return e.code\n\n        return CMD_SUCCESS", "category": "Python"}, {"instruction": "def loadbin(self, fobj, offset=0):\n        \"\"\"Load bin file into internal buffer. Not needed if source set in\n        constructor. This will overwrite addresses without warning\n        if object was already initialized.\n\n        @param  fobj        file name or file-like object\n        @param  offset      starting address offset\n        \"\"\"\n", "input": "", "output": "        fread = getattr(fobj, \"read\", None)\n        if fread is None:\n            f = open(fobj, \"rb\")\n            fread = f.read\n            fclose = f.close\n        else:\n            fclose = None\n\n        try:\n            self.frombytes(array('B', asbytes(fread())), offset=offset)\n        finally:\n            if fclose:\n                fclose()", "category": "Python"}, {"instruction": "def brute_force_most_distant(self, input_sample, num_samples,\n                                 num_params, k_choices,\n                                 num_groups=None):\n        \"\"\"Use brute force method to find most distant trajectories\n\n        Arguments\n        ---------\n        input_sample : numpy.ndarray\n        num_samples : int\n            The number of samples to generate\n        num_params : int\n            The number of parameters\n        k_choices : int\n            The number of optimal trajectories\n        num_groups : int, default=None\n            The number of groups\n\n        Returns\n        -------\n        list\n        \"\"\"\n", "input": "", "output": "        scores = self.find_most_distant(input_sample,\n                                        num_samples,\n                                        num_params,\n                                        k_choices,\n                                        num_groups)\n\n        maximum_combo = self.find_maximum(scores, num_samples, k_choices)\n\n        return maximum_combo", "category": "Python"}, {"instruction": "def f_comb(self, pos, sample):\n        \"\"\"return the value of the f_comb when epoch = pos\n\n        Parameters\n        ----------\n        pos: int\n            the epoch number of the position you want to predict\n        sample: list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        int\n            The expected matrix at pos with all the active function's prediction\n        \"\"\"\n", "input": "", "output": "        ret = 0\n        for i in range(self.effective_model_num):\n            model = self.effective_model[i]\n            y = self.predict_y(model, pos)\n            ret += sample[i] * y\n        return ret", "category": "Python"}, {"instruction": "def _wait_ready(self, timeout_sec=1):\n        \"\"\"Wait until the PN532 is ready to receive commands.  At most wait\n        timeout_sec seconds for the PN532 to be ready.  If the PN532 is ready\n        before the timeout is exceeded then True will be returned, otherwise\n        False is returned when the timeout is exceeded.\n        \"\"\"\n", "input": "", "output": "        start = time.time()\n        # Send a SPI status read command and read response.\n        self._gpio.set_low(self._cs)\n        self._busy_wait_ms(2)\n        response = self._spi.transfer([PN532_SPI_STATREAD, 0x00])\n        self._gpio.set_high(self._cs)\n        # Loop until a ready response is received.\n        while response[1] != PN532_SPI_READY:\n            # Check if the timeout has been exceeded.\n            if time.time() - start >= timeout_sec:\n                return False\n            # Wait a little while and try reading the status again.\n            time.sleep(0.01)\n            self._gpio.set_low(self._cs)\n            self._busy_wait_ms(2)\n            response = self._spi.transfer([PN532_SPI_STATREAD, 0x00])\n            self._gpio.set_high(self._cs)\n        return True", "category": "Python"}, {"instruction": "def get_user_details(self, response):\n        \"\"\"Return user details from GitHub account\"\"\"\n", "input": "", "output": "\n        account = response['account']\n        metadata = json.loads(account.get('json_metadata') or '{}')\n        account['json_metadata'] = metadata\n\n        return {\n            'id': account['id'],\n            'username': account['name'],\n            'name': metadata.get(\"profile\", {}).get('name', ''),\n            'account': account,\n        }", "category": "Python"}, {"instruction": "def removeDuplicates(inFileName, outFileName) :\n\t\"\"\"removes duplicated lines from a 'inFileName' CSV file, the results are witten in 'outFileName'\"\"\"\n", "input": "", "output": "\tf = open(inFileName)\n\tlegend = f.readline()\n\t\n\tdata = ''\n\th = {}\n\th[legend] = 0\n\t\n\tlines = f.readlines()\n\tfor l in lines :\n\t\tif not h.has_key(l) :\n\t\t\th[l] = 0\n\t\t\tdata += l\n\t\t\t\n\tf.flush()\n\tf.close()\n\tf = open(outFileName, 'w')\n\tf.write(legend+data)\n\tf.flush()\n\tf.close()", "category": "Python"}, {"instruction": "def csep_periodic_close(ra, rb, L):\n    \"\"\"Return the closest separation vector between each point in one set,\n    and every point in a second set, in periodic space.\n\n    Parameters\n    ----------\n    ra, rb: float array-like, shape (n, d) and (m, d) in d dimensions.\n        Two sets of points. `ra` is the set of points from which the closest\n        separation vectors to points `rb` are calculated.\n    L: float array, shape (d,)\n        System lengths.\n\n    Returns\n    -------\n    csep_close: float array-like, shape (n, m, d)\n        csep[i] is the closest separation vector from point ra[j]\n        to any point rb[i].\n        Note the un-intuitive vector direction.\n    \"\"\"\n", "input": "", "output": "    seps = csep_periodic(ra, rb, L)\n    seps_sq = np.sum(np.square(seps), axis=-1)\n\n    i_close = np.argmin(seps_sq, axis=-1)\n\n    i_all = list(range(len(seps)))\n    sep = seps[i_all, i_close]\n    sep_sq = seps_sq[i_all, i_close]\n    return sep, sep_sq", "category": "Python"}, {"instruction": "def filter_indices(self, options, verbosity, *args, **kwargs):\n        \"\"\"Filter indices and execute an action for each index.\"\"\"\n", "input": "", "output": "        index_name_map = {\n            index.__class__.__name__: index\n            for index in index_builder.indexes\n        }\n\n        # Process includes.\n        if options['index']:\n            indices = set(options['index'])\n        else:\n            indices = set(index_name_map.keys())\n\n        # Process excludes.\n        for index_name in options['exclude']:\n            if index_name not in index_name_map:\n                self.invalid_index(index_name)\n                return\n\n            indices.discard(index_name)\n\n        # Execute action for each remaining index.\n        for index_name in indices:\n            try:\n                index = index_name_map[index_name]\n            except KeyError:\n                self.invalid_index(index_name)\n                return\n\n            if verbosity > 0:\n                self.stdout.write(\"Processing index '{}'...\".format(index_name))\n            self.handle_index(index, *args, **kwargs)", "category": "Python"}, {"instruction": "def fill_with_defaults(process_input, input_schema):\n    \"\"\"Fill empty optional fields in input with default values.\"\"\"\n", "input": "", "output": "    for field_schema, fields, path in iterate_schema(process_input, input_schema):\n        if 'default' in field_schema and field_schema['name'] not in fields:\n            dict_dot(process_input, path, field_schema['default'])", "category": "Python"}, {"instruction": "def getCurrent(cls):\n        \"\"\"\n        :rtype: YowsupEnv\n        \"\"\"\n", "input": "", "output": "        if cls.__CURR is None:\n            env = DEFAULT\n            envs = cls.getRegisteredEnvs()\n            if env not in envs:\n                env = envs[0]\n            logger.debug(\"Env not set, setting it to %s\" % env)\n            cls.setEnv(env)\n        return cls.__CURR", "category": "Python"}, {"instruction": "def runStretchExperiment(numObjects=25):\n  \"\"\"\n  Generates a lot of random objects to profile the network.\n\n  Parameters:\n  ----------------------------\n  @param    numObjects (int)\n            Number of objects to create and learn.\n\n  \"\"\"\n", "input": "", "output": "  exp = L4L2Experiment(\n    \"profiling_experiment\",\n    enableLateralSP = True,\n    enableFeedForwardSP=True\n  )\n\n  objects = createObjectMachine(\n    machineType=\"simple\",\n    numInputBits=20,\n    sensorInputSize=1024,\n    externalInputSize=1024\n  )\n  objects.createRandomObjects(numObjects=numObjects, numPoints=10)\n  exp.learnObjects(objects.provideObjectsToLearn())\n  exp.printProfile()\n\n  inferConfig = {\n    \"numSteps\": len(objects[0]),\n    \"pairs\": {\n      0: objects[0]\n    }\n  }\n\n  exp.infer(objects.provideObjectToInfer(inferConfig), objectName=0)\n  exp.printProfile()\n\n  exp.plotInferenceStats(\n    fields=[\"L2 Representation\",\n            \"Overlap L2 with object\",\n            \"L4 Representation\"]\n  )", "category": "Python"}, {"instruction": "def can_vote(self, request):\n        \"\"\"\n        Determnines whether or not the current user can vote.\n        Returns a bool as well as a string indicating the current vote status,\n        with vote status being one of: 'closed', 'disabled', 'auth_required', 'can_vote', 'voted' \n        \"\"\"\n", "input": "", "output": "        modelbase_obj = self.modelbase_obj\n\n        # can't vote if liking is closed\n        if modelbase_obj.likes_closed:\n            return False, 'closed'\n        \n        # can't vote if liking is disabled\n        if not modelbase_obj.likes_enabled:\n            return False, 'disabled'\n\n        # anonymous users can't vote if anonymous likes are disabled\n        if not request.user.is_authenticated() and not modelbase_obj.anonymous_likes:\n            return False, 'auth_required'\n          \n        # return false if existing votes are found\n        if Vote.objects.filter(object_id=modelbase_obj.id, token=request.secretballot_token).count() == 0:\n            return True, 'can_vote'\n        else:\n            return False, 'voted'", "category": "Python"}, {"instruction": "def update_return_operation_by_id(cls, return_operation_id, return_operation, **kwargs):\n        \"\"\"Update ReturnOperation\n\n        Update attributes of ReturnOperation\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.update_return_operation_by_id(return_operation_id, return_operation, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str return_operation_id: ID of returnOperation to update. (required)\n        :param ReturnOperation return_operation: Attributes of returnOperation to update. (required)\n        :return: ReturnOperation\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._update_return_operation_by_id_with_http_info(return_operation_id, return_operation, **kwargs)\n        else:\n            (data) = cls._update_return_operation_by_id_with_http_info(return_operation_id, return_operation, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def run(args):\n    \"\"\"\n    Args:\n        args (argparse.Namespace)\n    \"\"\"\n", "input": "", "output": "    html_metadata_extractor = HTMLMetadataExtractor()\n\n    with warnings.catch_warnings():\n        warnings.simplefilter('ignore')\n\n        extractions = html_metadata_extractor.extract(html_text=args.input_file)\n        for e in extractions:\n            print(e.value)", "category": "Python"}, {"instruction": "def individual(self, ind_id):\n        \"\"\"Fetch a case from the database.\"\"\"\n", "input": "", "output": "        ind_obj = self.query(Individual).filter_by(ind_id=ind_id).first()\n        if ind_obj is None:\n            ind_obj = BaseIndividual(ind_id='unknown')\n        return ind_obj", "category": "Python"}, {"instruction": "def list_pools(**kwargs):\n    '''\n    List all storage pools.\n\n    :param connection: libvirt connection URI, overriding defaults\n    :param username: username to connect with, overriding defaults\n    :param password: password to connect with, overriding defaults\n\n    .. versionadded:: 2019.2.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.list_pools\n    '''\n", "input": "", "output": "    conn = __get_conn(**kwargs)\n    try:\n        return [pool.name() for pool in conn.listAllStoragePools()]\n    finally:\n        conn.close()", "category": "Python"}, {"instruction": "def set_parameters(self):\n        \"\"\"\n        Combines the parameters of the GMPE provided at the construction level\n        with the ones originally assigned to the backbone modified GMPE.\n        \"\"\"\n", "input": "", "output": "        for key in (ADMITTED_STR_PARAMETERS + ADMITTED_FLOAT_PARAMETERS +\n                    ADMITTED_SET_PARAMETERS):\n            try:\n                val = getattr(self.gmpe, key)\n            except AttributeError:\n                pass\n            else:\n                setattr(self, key, val)", "category": "Python"}, {"instruction": "def coerceType(self, ftype, value):\n        \"\"\"Returns unicode(value) after trying to coerce it into the SOLR field type.\n\n        @param ftype(string) The SOLR field type for the value\n        @param value(any) The value that is to be represented as Unicode text.\n\n        \"\"\"\n", "input": "", "output": "        if value is None:\n            return None\n        if ftype == 'string':\n            return str(value)\n        elif ftype == 'text':\n            return str(value)\n        elif ftype == 'int':\n            try:\n                v = int(value)\n                return str(v)\n            except Exception:\n                return None\n        elif ftype == 'float':\n            try:\n                v = float(value)\n                return str(v)\n            except Exception:\n                return None\n        elif ftype == 'date':\n            try:\n                v = datetime.datetime.strptime(value, '%b %d %Y %I:%M%p')\n                return v.isoformat()\n            except Exception:\n                return None\n        return str(value)", "category": "Python"}, {"instruction": "def add_pages(self, path='pages'):\n\t\t\"\"\"\n\t\tLook through a directory for markdown files and add them as pages.\n\t\t\"\"\"\n", "input": "", "output": "\t\tpages_path = os.path.join(self.root_path, path)\n\t\tpages = []\n\t\tfor file in _listfiles(pages_path):\n\t\t\tpage_dir = os.path.relpath(os.path.dirname(file), pages_path)\n\t\t\tif page_dir == '.':\n\t\t\t\tpage_dir = None\n\t\t\tpages.append(self.cm.Page.from_file(file, directory=page_dir))\n\t\tself.cm.add_pages(pages)", "category": "Python"}, {"instruction": "def certificate_issuer(self, value):\n        \"\"\"\n        An asn1crypto.x509.Certificate object of the issuer of the certificate.\n        This should only be set if the OCSP responder is not the issuer of\n        the certificate, but instead a special certificate only for OCSP\n        responses.\n        \"\"\"\n", "input": "", "output": "\n        if value is not None:\n            is_oscrypto = isinstance(value, asymmetric.Certificate)\n            if not is_oscrypto and not isinstance(value, x509.Certificate):\n                raise TypeError(_pretty_message(\n                    '''\n                    certificate_issuer must be an instance of\n                    asn1crypto.x509.Certificate or\n                    oscrypto.asymmetric.Certificate, not %s\n                    ''',\n                    _type_name(value)\n                ))\n\n            if is_oscrypto:\n                value = value.asn1\n\n        self._certificate_issuer = value", "category": "Python"}, {"instruction": "def create_authorization_response(self, uri, http_method='GET', body=None,\n                                      headers=None, scopes=None, credentials=None):\n        \"\"\"Extract response_type and route to the designated handler.\"\"\"\n", "input": "", "output": "        request = Request(\n            uri, http_method=http_method, body=body, headers=headers)\n        request.scopes = scopes\n        # TODO: decide whether this should be a required argument\n        request.user = None     # TODO: explain this in docs\n        for k, v in (credentials or {}).items():\n            setattr(request, k, v)\n        response_type_handler = self.response_types.get(\n            request.response_type, self.default_response_type_handler)\n        log.debug('Dispatching response_type %s request to %r.',\n                  request.response_type, response_type_handler)\n        return response_type_handler.create_authorization_response(\n            request, self.default_token_type)", "category": "Python"}, {"instruction": "def dfl_local_dir():\n    \"\"\"\n    Infers a default local directory, which is DFL_DIR_PARENT/<project name>,\n    where the project name is guessed according to the following rules.\n\n    If we detect we're in a repository, the project name is the repository name\n    (git only for now).\n\n    If we're not in a repository, and the script file sys.argv[0] is non-null,\n    then that is used.\n\n    Otherwise, we just say it's \"unknown\"\n    \"\"\"\n", "input": "", "output": "    project_name = git_repo()\n    if not project_name and sys.argv:\n        project_name = sys.argv[0]\n    if not project_name:\n        project_name = \"unknown\"\n    dirpath = os.path.join(DFL_DIR_PARENT, project_name)\n    return os.path.expanduser(dirpath)", "category": "Python"}, {"instruction": "def push_async_exit(self, exit):\n        \"\"\"Registers a coroutine function with the standard __aexit__ method\n        signature.\n        Can suppress exceptions the same way __aexit__ method can.\n        Also accepts any object with an __aexit__ method (registering a call\n        to the method instead of the object itself).\n        \"\"\"\n", "input": "", "output": "        _cb_type = type(exit)\n        try:\n            exit_method = _cb_type.__aexit__\n        except AttributeError:\n            # Not an async context manager, so assume it's a coroutine function\n            self._push_exit_callback(exit, False)\n        else:\n            self._push_async_cm_exit(exit, exit_method)\n        return exit", "category": "Python"}, {"instruction": "def run_queries(self, backfill_num_days=7):\n        \"\"\"\n        Run the data queries for the specified projects.\n\n        :param backfill_num_days: number of days of historical data to backfill,\n          if missing\n        :type backfill_num_days: int\n        \"\"\"\n", "input": "", "output": "        available_tables = self._get_download_table_ids()\n        logger.debug('Found %d available download tables: %s',\n                     len(available_tables), available_tables)\n        today_table = available_tables[-1]\n        yesterday_table = available_tables[-2]\n        self.query_one_table(today_table)\n        self.query_one_table(yesterday_table)\n        self.backfill_history(backfill_num_days, available_tables)", "category": "Python"}, {"instruction": "def set(state, host, ctid, save=True, **settings):\n    '''\n    Set OpenVZ container details.\n\n    + ctid: CTID of the container to set\n    + save: whether to save the changes\n    + settings: settings/arguments to apply to the container\n\n    Settings/arguments:\n        these are mapped directly to ``vztctl`` arguments, eg\n        ``hostname='my-host.net'`` becomes ``--hostname my-host.net``.\n    '''\n", "input": "", "output": "\n    args = ['{0}'.format(ctid)]\n\n    if save:\n        args.append('--save')\n\n    for key, value in six.iteritems(settings):\n        # Handle list values (eg --nameserver X --nameserver X)\n        if isinstance(value, list):\n            args.extend('--{0} {1}'.format(key, v) for v in value)\n        else:\n            args.append('--{0} {1}'.format(key, value))\n\n    yield 'vzctl set {0}'.format(' '.join(args))", "category": "Python"}, {"instruction": "def groups_invite(self, room_id, user_id, **kwargs):\n        \"\"\"Adds a user to the private group.\"\"\"\n", "input": "", "output": "        return self.__call_api_post('groups.invite', roomId=room_id, userId=user_id, kwargs=kwargs)", "category": "Python"}, {"instruction": "def set_errors(self):\r\n\t\t\"\"\"Set errors markup.\r\n\t\t\"\"\"\n", "input": "", "output": "\t\tif not self.field.errors or self.attrs.get(\"_no_errors\"):\r\n\t\t\treturn\r\n\t\t\r\n\t\tself.values[\"class\"].append(\"error\")\r\n\r\n\t\tfor error in self.field.errors:\r\n\t\t\tself.values[\"errors\"] += ERROR_WRAPPER % {\"message\": error}", "category": "Python"}, {"instruction": "def contains(self, data):\n        \"\"\"\n        Check if an item has been added to the bloomfilter.\n\n        :param bytes data: a bytestring representing the item to check.\n        :returns: a boolean indicating whether or not the item is present in\n            the bloomfilter. False-positives are possible, but a negative\n            return value is definitive.\n        \"\"\"\n", "input": "", "output": "        bfo = BitFieldOperation(self.database, self.key)\n        for bit_index in self._get_seeds(data):\n            bfo.get('u1', bit_index)\n        return all(bfo.execute())", "category": "Python"}, {"instruction": "def delete_property(self, key):\n        \"\"\"Remove a property from the document.\n\n        Calling code should use this method to remove properties on the\n        document instead of modifying ``properties`` directly.\n\n        If there is a property with the name in ``key``, it will be removed.\n        Otherwise, a ``KeyError`` will be thrown.\n\n        \"\"\"\n", "input": "", "output": "        if key in self.RESERVED_ATTRIBUTE_NAMES:\n            raise KeyError(key)\n        del self.o[key]", "category": "Python"}, {"instruction": "def _call_widget_constructed(widget):\n        \"\"\"Static method, called when a widget is constructed.\"\"\"\n", "input": "", "output": "        if Widget._widget_construction_callback is not None and callable(Widget._widget_construction_callback):\n            Widget._widget_construction_callback(widget)", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"The actual event loop.\n\n        Calls the ``owner``'s :py:meth:`~Component.start_event` method,\n        then calls its :py:meth:`~Component.new_frame_event` and\n        :py:meth:`~Component.new_config_event` methods as required until\n        :py:meth:`~Component.stop` is called. Finally the ``owner``'s\n        :py:meth:`~Component.stop_event` method is called before the\n        thread terminates.\n\n        \"\"\"\n", "input": "", "output": "        try:\n            self.owner.start_event()\n            while True:\n                while not self.incoming:\n                    time.sleep(0.01)\n                while self.incoming:\n                    command = self.incoming.popleft()\n                    if command is None:\n                        raise StopIteration()\n                    command()\n        except StopIteration:\n            pass\n        self.owner.stop_event()", "category": "Python"}, {"instruction": "def clean_session_table():\n    \"\"\"Automatically clean session table.\n\n    To enable a periodically clean of the session table, you should configure\n    the task as a celery periodic task.\n\n    .. code-block:: python\n\n        from datetime import timedelta\n        CELERYBEAT_SCHEDULE = {\n            'session_cleaner': {\n                'task': 'invenio_accounts.tasks.clean_session_table',\n                'schedule': timedelta(days=1),\n            },\n        }\n\n    See `Invenio-Celery <https://invenio-celery.readthedocs.io/>`_\n    documentation for further details.\n    \"\"\"\n", "input": "", "output": "    sessions = SessionActivity.query_by_expired().all()\n    for session in sessions:\n        delete_session(sid_s=session.sid_s)\n    db.session.commit()", "category": "Python"}, {"instruction": "def pipeline_status(url, pipeline_id, auth, verify_ssl):\n    \"\"\"Retrieve the current status for a pipeline.\n\n    Args:\n        url          (str): the host url in the form 'http://host:port/'.\n        pipeline_id  (str): the ID of of the exported pipeline.\n        auth         (tuple): a tuple of username, and password.\n        verify_ssl   (bool): whether to verify ssl certificates\n\n    Returns:\n        dict: the response json\n\n    \"\"\"\n", "input": "", "output": "    status_result = requests.get(url + '/' + pipeline_id + '/status', headers=X_REQ_BY, auth=auth, verify=verify_ssl)\n    status_result.raise_for_status()\n    logging.debug('Status request: ' + url + '/status')\n    logging.debug(status_result.json())\n    return status_result.json()", "category": "Python"}, {"instruction": "def list_submodules(module_name: str) -> List[str]:   # pylint: disable=invalid-sequence-index\n    \"\"\"\n    List full names of all the submodules in the given module.\n\n    :param module_name: name of the module of which the submodules will be listed\n    \"\"\"\n", "input": "", "output": "    _module = importlib.import_module(module_name)\n    return [module_name+'.'+submodule_name for _, submodule_name, _ in pkgutil.iter_modules(_module.__path__)]", "category": "Python"}, {"instruction": "def write(self, src, dest=None):\n    \"\"\"Schedules a write of the file at ``src`` to the ``dest`` path in this jar.\n\n    If the ``src`` is a file, then ``dest`` must be specified.\n\n    If the ``src`` is a directory then by default all descendant files will be added to the jar as\n    entries carrying their relative path.  If ``dest`` is specified it will be prefixed to each\n    descendant's relative path to form its jar entry path.\n\n    :param string src: the path to the pre-existing source file or directory\n    :param string dest: the path the source file or directory should have in this jar\n    \"\"\"\n", "input": "", "output": "    if not src or not isinstance(src, string_types):\n      raise ValueError('The src path must be a non-empty string, got {} of type {}.'.format(\n        src, type(src)))\n    if dest and not isinstance(dest, string_types):\n      raise ValueError('The dest entry path must be a non-empty string, got {} of type {}.'.format(\n        dest, type(dest)))\n    if not os.path.isdir(src) and not dest:\n      raise self.Error('Source file {} must have a jar destination specified'.format(src))\n\n    self._add_entry(self.FileSystemEntry(src, dest))", "category": "Python"}, {"instruction": "def set_text(self, text):\n        '''set the text field.'''\n", "input": "", "output": "        if text in [None, \"\"]:\n            return self.jsonrpc.clearTextField(self.selector)  # TODO no return\n        else:\n            return self.jsonrpc.setText(self.selector, text)", "category": "Python"}, {"instruction": "def set_subplot_xlabel(self, row, column, text):\n        \"\"\"Set a label for the x-axis of a subplot.\n\n        :param row,column: specify the subplot.\n        :param text: text of the label.\n\n        \"\"\"\n", "input": "", "output": "        subplot = self.get_subplot_at(row, column)\n        subplot.set_xlabel(text)", "category": "Python"}, {"instruction": "def lookup(self, auth, type, mapping, defer=False):\n        \"\"\" Look up a Resource ID by alias, owned Resource ID, or share activation code under the\n        client specified in <ClientID>.\n\n        Args:\n            auth: <cik>\n            type: Type of resource to lookup (alias | owner | shared)\n            mapping: Based on resource type defined above.\n        \"\"\"\n", "input": "", "output": "        return self._call('lookup', auth, [type, mapping], defer)", "category": "Python"}, {"instruction": "def print_summary(self):\n        \"\"\"\n        print out stats about loading operation\n        \"\"\"\n", "input": "", "output": "        if self.sources_valid:\n            printDebug(\n                \"----------\\nLoaded %d triples.\\n----------\" % len(\n                    self.rdflib_graph),\n                fg='white')\n            printDebug(\n                \"RDF sources loaded successfully: %d of %d.\" %\n                (len(self.sources_valid),\n                 len(self.sources_valid) + len(self.sources_invalid)),\n                fg='green')\n            for s in self.sources_valid:\n                printDebug(\"..... '\" + s + \"'\", fg='white')\n            printDebug(\"----------\", fg='white')\n        else:\n            printDebug(\"Sorry - no valid RDF was found\", fg='red')\n\n        if self.sources_invalid:\n            printDebug(\n                \"----------\\nRDF sources failed to load: %d.\\n----------\" %\n                (len(self.sources_invalid)),\n                fg='red')\n            for s in self.sources_invalid:\n                printDebug(\"-> \" + s, fg=\"red\")", "category": "Python"}, {"instruction": "def build_output_partitions(cls, name='inputTablePartitions', output_name='output'):\n        \"\"\"\n        Build an output table partition parameter\n\n        :param name: parameter name\n        :type name: str\n        :param output_name: bind input port name\n        :type output_name: str\n        :return: output description\n        :rtype: ParamDef\n        \"\"\"\n", "input": "", "output": "        obj = cls(name)\n        obj.exporter = 'get_output_table_partition'\n        obj.output_name = output_name\n        return obj", "category": "Python"}, {"instruction": "def localSslFixup(host, sslContext):\n    \"\"\"\n    Connections to 'localhost' do not need SSL verification as a certificate\n    will never match. The OS provides security by only allowing root to bind\n    to low-numbered ports.\n    \"\"\"\n", "input": "", "output": "    if not sslContext and host in ['localhost', '127.0.0.1', '::1']:\n        import ssl\n        if hasattr(ssl, '_create_unverified_context'):\n            sslContext = ssl._create_unverified_context()\n    return sslContext", "category": "Python"}, {"instruction": "def write_to_file(src, dst):\n    \"\"\"Write data from `src` into `dst`.\n\n    Args:\n        src (iterable): iterable that yields blocks of data to write\n        dst (file-like object): file-like object that must support\n            .write(block)\n\n    Returns:\n        number of bytes written to `dst`\n\n    \"\"\"\n", "input": "", "output": "    n = 0\n    for block in src:\n        dst.write(block)\n        n += len(block)\n    return n", "category": "Python"}, {"instruction": "def delete_user_avatar(self, username, avatar):\n        \"\"\"Delete a user's avatar.\n\n        :param username: the user to delete the avatar from\n        :param avatar: ID of the avatar to remove\n        \"\"\"\n", "input": "", "output": "        params = {'username': username}\n        url = self._get_url('user/avatar/' + avatar)\n        return self._session.delete(url, params=params)", "category": "Python"}, {"instruction": "def _get_next_time(lines: [dict], target: str) -> str:  # type: ignore\n    \"\"\"\n    Returns the next FROM target value or empty\n    \"\"\"\n", "input": "", "output": "    for line in lines:\n        if line[target] and not _is_tempo_or_prob(line['type']):\n            return line[target]\n    return ''", "category": "Python"}, {"instruction": "def htmlDocContentDumpFormatOutput(self, buf, encoding, format):\n        \"\"\"Dump an HTML document. \"\"\"\n", "input": "", "output": "        if buf is None: buf__o = None\n        else: buf__o = buf._o\n        libxml2mod.htmlDocContentDumpFormatOutput(buf__o, self._o, encoding, format)", "category": "Python"}, {"instruction": "def create_menu_box_with_icon_and_label(label_text):\n    \"\"\" Creates a MenuItem box, which is a replacement for the former ImageMenuItem. The box contains, a label\n        for the icon and one for the text.\n\n    :param label_text: The text, which is displayed for the text label\n    :return:\n    \"\"\"\n", "input": "", "output": "    box = Gtk.Box.new(Gtk.Orientation.HORIZONTAL, 10)\n    box.set_border_width(0)\n    icon_label = Gtk.Label()\n    text_label = Gtk.AccelLabel.new(label_text)\n    text_label.set_xalign(0)\n\n    box.pack_start(icon_label, False, False, 0)\n    box.pack_start(text_label, True, True, 0)\n    return box, icon_label, text_label", "category": "Python"}, {"instruction": "def write_from_file(library, session, filename, count):\n    \"\"\"Take data from a file and write it out synchronously.\n\n    Corresponds to viWriteFromFile function of the VISA library.\n\n    :param library: the visa library wrapped by ctypes.\n    :param session: Unique logical identifier to a session.\n    :param filename: Name of file from which data will be read.\n    :param count: Number of bytes to be written.\n    :return: Number of bytes actually transferred, return value of the library call.\n    :rtype: int, :class:`pyvisa.constants.StatusCode`\n    \"\"\"\n", "input": "", "output": "    return_count = ViUInt32()\n    ret = library.viWriteFromFile(session, filename, count, return_count)\n    return return_count, ret", "category": "Python"}, {"instruction": "def path_to_url(path):\n    # type: (Union[str, Text]) -> str\n    \"\"\"\n    Convert a path to a file: URL.  The path will be made absolute and have\n    quoted path parts.\n    \"\"\"\n", "input": "", "output": "    path = os.path.normpath(os.path.abspath(path))\n    url = urllib_parse.urljoin('file:', urllib_request.pathname2url(path))\n    return url", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"all links also need stop() to stop their runloops\"\"\"\n", "input": "", "output": "        self.keep_listening = False\n        # if threaded, kill threads before going down\n        if hasattr(self, 'join'):\n            self.join()\n        self.log(\"Went down.\")\n        return True", "category": "Python"}, {"instruction": "def download(self, path, file):\n        \"\"\"Download remote file to disk.\"\"\"\n", "input": "", "output": "\n        resp = self._sendRequest(\"GET\", path)\n        if resp.status_code == 200:\n            with open(file, \"wb\") as f:\n                f.write(resp.content)\n        else:\n            raise YaDiskException(resp.status_code, resp.content)", "category": "Python"}, {"instruction": "def rule_match(component, cmd):\n    '''see if one rule component matches'''\n", "input": "", "output": "    if component == cmd:\n        return True\n    expanded = rule_expand(component, cmd)\n    if cmd in expanded:\n        return True\n    return False", "category": "Python"}, {"instruction": "def extended(self, new_leaves: List[bytes]):\n        \"\"\"Returns a new tree equal to this tree extended with new_leaves.\"\"\"\n", "input": "", "output": "        new_tree = self.__copy__()\n        new_tree.extend(new_leaves)\n        return new_tree", "category": "Python"}, {"instruction": "def extract_js_links(bs4):\n    \"\"\"Extracting js links from BeautifulSoup object\n\n    :param bs4: `BeautifulSoup`\n    :return: `list` List of links\n    \"\"\"\n", "input": "", "output": "\n    links = extract_links(bs4)\n\n    real_js = [anchor for anchor in links if anchor.endswith(('.js', '.JS'))]\n\n    js_tags = [anchor['src'] for anchor in bs4.select('script[type=\"text/javascript\"]')\n               if anchor.has_attr('src')]\n\n    return list(set(real_js+js_tags))", "category": "Python"}, {"instruction": "def search(self, query):\n        \"\"\" Perform GitHub query \"\"\"\n", "input": "", "output": "        url = self.url + \"/\" + query\n        log.debug(\"GitHub query: {0}\".format(url))\n        try:\n            request = urllib2.Request(url, headers=self.headers)\n            response = urllib2.urlopen(request)\n            log.debug(\"Response headers:\\n{0}\".format(\n                unicode(response.info()).strip()))\n        except urllib2.URLError as error:\n            log.debug(error)\n            raise ReportError(\n                \"GitHub search on {0} failed.\".format(self.url))\n        result = json.loads(response.read())[\"items\"]\n        log.debug(\"Result: {0} fetched\".format(listed(len(result), \"item\")))\n        log.data(pretty(result))\n        return result", "category": "Python"}, {"instruction": "def _draw_text(self, coords, text, foreground, font):\n        \"\"\"Draw the text and shorten it if required\"\"\"\n", "input": "", "output": "        if text is None:\n            return None\n        x1_r, _, x2_r, _ = coords\n        while True:\n            text_id = self._timeline.create_text(\n                (0, 0), text=text,\n                fill=foreground if foreground != \"default\" else self._marker_foreground,\n                font=font if font != \"default\" else self._marker_font,\n                tags=(\"marker\",)\n            )\n            x1_t, _, x2_t, _ = self._timeline.bbox(text_id)\n            if (x2_t - x1_t) < (x2_r - x1_r):\n                break\n            self._timeline.delete(text_id)\n            text = text[:-4] + \"...\"\n        x, y = TimeLine.calculate_text_coords(coords)\n        self._timeline.coords(text_id, (x, y))\n        return text_id", "category": "Python"}, {"instruction": "def add_speaker(self, collection_name, metadata):\n        \"\"\"Add a new speaker to this collection.\n\n        :type collection_name: String\n        :param collection_name: the name of the collection to search\n        :type metadata: Dict\n        :param metadata: dictionary of metadata properties and values\n          for this speaker. Must include 'dcterms:identifier' a unique\n          identifier for the speaker.\n\n        :rtype: String\n        :returns: the URL of the newly created speaker, or None if there was an\n            error\n        \"\"\"\n", "input": "", "output": "\n        if 'dcterms:identifier' not in metadata:\n            raise APIError(msg=\"No identifier in speaker metadata\")\n\n        if '@context' not in metadata:\n            metadata['@context'] = CONTEXT\n\n        speakers_url = \"/speakers/\"+collection_name+\"/\"\n        resp = self.api_request(speakers_url, data=json.dumps(metadata), method=\"POST\")\n\n        if 'success' in resp:\n            return resp['success']['URI']\n        else:\n            return None", "category": "Python"}, {"instruction": "def get_pub_str(self, name='master'):\n        '''\n        Return the string representation of a public key\n        in the pki-directory\n        '''\n", "input": "", "output": "        path = os.path.join(self.opts['pki_dir'],\n                            name + '.pub')\n        if not os.path.isfile(path):\n            key = self.__get_keys()\n            if HAS_M2:\n                key.save_pub_key(path)\n            else:\n                with salt.utils.files.fopen(path, 'wb+') as wfh:\n                    wfh.write(key.publickey().exportKey('PEM'))\n        with salt.utils.files.fopen(path) as rfh:\n            return rfh.read()", "category": "Python"}, {"instruction": "def render_tag(tag, attrs=None, content=None, close=True):\n    \"\"\"\n    Render a HTML tag\n    \"\"\"\n", "input": "", "output": "    builder = \"<{tag}{attrs}>{content}\"\n    if content or close:\n        builder += \"</{tag}>\"\n    return format_html(\n        builder,\n        tag=tag,\n        attrs=mark_safe(flatatt(attrs)) if attrs else \"\",\n        content=text_value(content),\n    )", "category": "Python"}, {"instruction": "def validate_email(value: str) -> Tuple[str, str]:\n    \"\"\"\n    Brutally simple email address validation. Note unlike most email address validation\n    * raw ip address (literal) domain parts are not allowed.\n    * \"John Doe <local_part@domain.com>\" style \"pretty\" email addresses are processed\n    * the local part check is extremely basic. This raises the possibility of unicode spoofing, but no better\n        solution is really possible.\n    * spaces are striped from the beginning and end of addresses but no error is raised\n\n    See RFC 5322 but treat it with suspicion, there seems to exist no universally acknowledged test for a valid email!\n    \"\"\"\n", "input": "", "output": "    if email_validator is None:\n        raise ImportError('email-validator is not installed, run `pip install pydantic[email]`')\n\n    m = PRETTY_REGEX.fullmatch(value)\n    name: Optional[str] = None\n    if m:\n        name, value = m.groups()\n\n    email = value.strip()\n\n    try:\n        email_validator.validate_email(email, check_deliverability=False)\n    except email_validator.EmailNotValidError as e:\n        raise errors.EmailError() from e\n\n    return name or email[: email.index('@')], email.lower()", "category": "Python"}, {"instruction": "def _readuint(self, length, start):\n        \"\"\"Read bits and interpret as an unsigned int.\"\"\"\n", "input": "", "output": "        if not length:\n            raise InterpretError(\"Cannot interpret a zero length bitstring \"\n                                           \"as an integer.\")\n        offset = self._offset\n        startbyte = (start + offset) // 8\n        endbyte = (start + offset + length - 1) // 8\n\n        b = binascii.hexlify(bytes(self._datastore.getbyteslice(startbyte, endbyte + 1)))\n        assert b\n        i = int(b, 16)\n        final_bits = 8 - ((start + offset + length) % 8)\n        if final_bits != 8:\n            i >>= final_bits\n        i &= (1 << length) - 1\n        return i", "category": "Python"}, {"instruction": "def set_edge_label(self, edge, label):\n        \"\"\"\n        Set the label of an edge.\n\n        @type  edge: edge\n        @param edge: One edge.\n\n        @type  label: string\n        @param label: Edge label.\n        \"\"\"\n", "input": "", "output": "        self.set_edge_properties(edge, label=label )\n        if not self.DIRECTED:\n            self.set_edge_properties((edge[1], edge[0]) , label=label )", "category": "Python"}, {"instruction": "def generate_aead_simple(self, nonce, key_handle, data):\n        \"\"\"\n        Generate AEAD block from data for a specific key in a single step\n        (without using the YubiHSM internal buffer).\n\n        @param nonce: The nonce to use when creating the AEAD\n        @param key_handle: The key handle that can encrypt data into an AEAD\n        @param data: Data to put inside the AEAD\n        @type nonce: string\n        @type key_handle: integer or string\n        @type data: string\n\n        @returns: The generated AEAD on success.\n        @rtype: L{YHSM_GeneratedAEAD}\n\n        @see: L{pyhsm.aead_cmd.YHSM_Cmd_AEAD_Generate}\n        \"\"\"\n", "input": "", "output": "        return pyhsm.aead_cmd.YHSM_Cmd_AEAD_Generate(self.stick, nonce, key_handle, data).execute()", "category": "Python"}, {"instruction": "def is_deb_package_installed(pkg):\n    \"\"\" checks if a particular deb package is installed \"\"\"\n", "input": "", "output": "\n    with settings(hide('warnings', 'running', 'stdout', 'stderr'),\n                  warn_only=True, capture=True):\n\n        result = sudo('dpkg-query -l \"%s\" | grep -q ^.i' % pkg)\n        return not bool(result.return_code)", "category": "Python"}, {"instruction": "def pipe(self, cmd1, cmd2, quiet=False, estimated_size=None, **kwa):\n        \"\"\"Executes commands\"\"\"\n", "input": "", "output": "        if self.has_pv and not quiet:\n            pv = \"pv\" if estimated_size is None else \"pv --size {}\".format(estimated_size)\n            return self.shell(\"{} | {}| {}\".format(cmd1, pv, cmd2), **kwa)\n        else:\n            return self.shell(\"{} | {}\".format(cmd1, cmd2), **kwa)", "category": "Python"}, {"instruction": "def apod(date=None, concept_tags=None):\n    '''\n    HTTP REQUEST\n\n    GET https://api.nasa.gov/planetary/apod\n\n    QUERY PARAMETERS\n\n    Parameter\tType\tDefault\tDescription\n    date\tYYYY-MM-DD\ttoday\tThe date of the APOD image to retrieve\n    concept_tags\tbool\tFalse\tReturn an ordered dictionary of concepts from the APOD explanation\n    api_key\tstring\tDEMO_KEY\tapi.nasa.gov key for expanded usage\n    EXAMPLE QUERY\n\n    https://api.nasa.gov/planetary/apod?concept_tags=True&api_key=DEMO_KEY\n    '''\n", "input": "", "output": "    base_url = \"https://api.nasa.gov/planetary/apod?\"\n\n    if date:\n        try:\n            vali_date(date)\n            base_url += \"date=\" + date + \"&\"\n        except:\n            raise ValueError(\"Incorrect date format, should be YYYY-MM-DD\")\n    if concept_tags == True:\n        base_url += \"concept_tags=True\" + \"&\"\n\n    req_url = base_url + \"api_key=\" + nasa_api_key()\n\n    return dispatch_http_get(req_url)", "category": "Python"}, {"instruction": "def parse_command(self, string):\n        \"\"\"Parse out any possible valid command from an input string.\"\"\"\n", "input": "", "output": "        possible_command, _, rest = string.partition(\" \")\n        # Commands are case-insensitive, stored as lowercase\n        possible_command = possible_command.lower()\n        if possible_command not in self.commands:\n            return None, None\n\n        event = self.commands[possible_command][\"event\"]\n        args = shlex.split(rest.strip())\n        return event, args", "category": "Python"}, {"instruction": "def get_owner(self, default=True):\n        \"\"\"Return (User ID, Group ID) tuple\n\n        :param bool default: Whether to return default if not set.\n        :rtype: tuple[int, int]\n        \"\"\"\n", "input": "", "output": "        uid, gid = self.owner\n\n        if not uid and default:\n            uid = os.getuid()\n\n        if not gid and default:\n            gid = os.getgid()\n\n        return uid, gid", "category": "Python"}, {"instruction": "def set_neighbor_out_filter(neigh_ip_address, filters):\n    \"\"\"Sets the out_filter of a neighbor.\"\"\"\n", "input": "", "output": "    core = CORE_MANAGER.get_core_service()\n    peer = core.peer_manager.get_by_addr(neigh_ip_address)\n    peer.out_filters = filters\n    return True", "category": "Python"}, {"instruction": "def get_encoded_parameter(get_data, name, default=None, lowercase_urlencoding=False):\n        \"\"\"Return a URL encoded get parameter value\n        Prefer to extract the original encoded value directly from query_string since URL\n        encoding is not canonical. The encoding used by ADFS 3.0 is not compatible with\n        python's quote_plus (ADFS produces lower case hex numbers and quote_plus produces\n        upper case hex numbers)\n        \"\"\"\n", "input": "", "output": "\n        if name not in get_data:\n            return OneLogin_Saml2_Utils.case_sensitive_urlencode(default, lowercase_urlencoding)\n        if 'query_string' in get_data:\n            return OneLogin_Saml2_Utils.extract_raw_query_parameter(get_data['query_string'], name)\n        return OneLogin_Saml2_Utils.case_sensitive_urlencode(get_data[name], lowercase_urlencoding)", "category": "Python"}, {"instruction": "def create(self, name):\n        \"\"\" Creates a configuration file named *name*.\n\n        If there is already a configuration file with that name,\n        the existing file is returned.\n\n        :param name: The name of the configuration file.\n        :type name: ``string``\n\n        :return: The :class:`ConfigurationFile` object.\n        \"\"\"\n", "input": "", "output": "        # This has to be overridden to handle the plumbing of creating\n        # a ConfigurationFile (which is a Collection) instead of some\n        # Entity.\n        if not isinstance(name, basestring):\n            raise ValueError(\"Invalid name: %s\" % repr(name))\n        response = self.post(__conf=name)\n        if response.status == 303:\n            return self[name]\n        elif response.status == 201:\n            return ConfigurationFile(self.service, PATH_CONF % name, item=Stanza, state={'title': name})\n        else:\n            raise ValueError(\"Unexpected status code %s returned from creating a stanza\" % response.status)", "category": "Python"}, {"instruction": "def add_to_known_hosts(self, hosts, known_hosts=DEFAULT_KNOWN_HOSTS, dry=False):\n        \"\"\"\n        Add the remote host SSH public key to the `known_hosts` file.\n\n        :param hosts: the list of the remote `Host` objects.\n        :param known_hosts: the `known_hosts` file to store the SSH public keys.\n        :param dry: perform a dry run.\n        \"\"\"\n", "input": "", "output": "        to_add = []\n        with open(known_hosts) as fh:\n            known_hosts_set = set(line.strip() for line in fh.readlines())\n\n        cmd = ['ssh-keyscan'] + [host.hostname for host in hosts]\n        logger.debug('Call: %s',  ' '.join(cmd))\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n        for line in stdout.splitlines():\n            line = line.strip()\n            logger.info('[%s] Add the remote host SSH public key to [%s]...', line.split(' ', 1)[0], known_hosts)\n            if line not in known_hosts_set:\n                known_hosts_set.add(line)\n                to_add.append('{0}\\n'.format(line))\n\n        if not dry:\n            with open(known_hosts, 'a') as fh:\n                fh.writelines(to_add)", "category": "Python"}, {"instruction": "def market_if_touched(self, accountID, **kwargs):\n        \"\"\"\n        Shortcut to create a MarketIfTouched Order in an Account\n\n        Args:\n            accountID : The ID of the Account\n            kwargs : The arguments to create a MarketIfTouchedOrderRequest\n\n        Returns:\n            v20.response.Response containing the results from submitting\n            the request\n        \"\"\"\n", "input": "", "output": "        return self.create(\n            accountID,\n            order=MarketIfTouchedOrderRequest(**kwargs)\n        )", "category": "Python"}, {"instruction": "def __clear_inplace_widgets(self):\n        \"\"\"Remove all inplace edit widgets.\"\"\"\n", "input": "", "output": "        cols = self.__get_display_columns()\n        #print('Clear:', cols)\n        for c in cols:\n            if c in self._inplace_widgets:\n                widget = self._inplace_widgets[c]\n                widget.place_forget()\n                self._inplace_widgets_show.pop(c, None)", "category": "Python"}, {"instruction": "def apply_ants_transform(transform, data, data_type=\"point\", reference=None, **kwargs):\n    \"\"\"\n    Apply ANTsTransform to data\n\n    ANTsR function: `applyAntsrTransform`\n\n    Arguments\n    ---------\n    transform : ANTsTransform\n        transform to apply to image\n\n    data : ndarray/list/tuple\n        data to which transform will be applied\n\n    data_type : string\n        type of data\n        Options :\n            'point'\n            'vector'\n            'image'\n\n    reference : ANTsImage\n        target space for transforming image\n\n    kwargs : kwargs\n        additional options passed to `apply_ants_transform_to_image`\n\n    Returns\n    -------\n    ANTsImage if data_type == 'point'\n    OR\n    tuple if data_type == 'point' or data_type == 'vector'\n    \"\"\"\n", "input": "", "output": "    return transform.apply(data, data_type, reference, **kwargs)", "category": "Python"}, {"instruction": "def set_evernote_filter(self, date_triggered, trigger):\n        \"\"\"\n            build the filter that will be used by evernote\n            :param date_triggered:\n            :param trigger:\n            :return: filter\n        \"\"\"\n", "input": "", "output": "        new_date_triggered = arrow.get(str(date_triggered)[:-6],\n                                       'YYYY-MM-DD HH:mm:ss')\n\n        new_date_triggered = str(new_date_triggered).replace(\n            ':', '').replace('-', '').replace(' ', '')\n        date_filter = \"created:{} \".format(new_date_triggered[:-6])\n\n        notebook_filter = ''\n        if trigger.notebook:\n            notebook_filter = \"notebook:{} \".format(trigger.notebook)\n        tag_filter = \"tag:{} \".format(trigger.tag) if trigger.tag != '' else ''\n\n        complet_filter = ''.join((notebook_filter, tag_filter, date_filter))\n\n        return complet_filter", "category": "Python"}, {"instruction": "def rollforward(self, date):\n        \"\"\"Roll date forward to nearest start of year\"\"\"\n", "input": "", "output": "        if self.onOffset(date):\n            return date\n        else:\n            return date + YearBegin(month=self.month)", "category": "Python"}, {"instruction": "def add_to_gui(self, content):\n        \"\"\"\n        add content to the gui script.\n        \"\"\"\n", "input": "", "output": "        if not self.rewrite_config:\n            raise DirectoryException(\"Error! Directory was not intialized w/ rewrite_config.\")\n        if not self.gui_file:\n            self.gui_path, self.gui_file = self.__get_gui_handle(self.root_dir)\n        self.gui_file.write(content + '\\n')", "category": "Python"}, {"instruction": "def generate(self, inputs, context, beam_size):\n        \"\"\"\n        Autoregressive generator, works with SequenceGenerator class.\n        Executes decoder (in inference mode), applies log_softmax and topK for\n        inference with beam search decoding.\n\n        :param inputs: tensor with inputs to the decoder\n        :param context: context from the encoder\n        :param beam_size: beam size for the generator\n\n        returns: (words, logprobs, scores, new_context)\n            words: indices of topK tokens\n            logprobs: log probabilities of topK tokens\n            scores: scores from the attention module (for coverage penalty)\n            new_context: new decoder context, includes new hidden states for\n                decoder RNN cells\n        \"\"\"\n", "input": "", "output": "        logits, scores, new_context = self.decode(inputs, context, True)\n        logprobs = log_softmax(logits, dim=-1)\n        logprobs, words = logprobs.topk(beam_size, dim=-1)\n        return words, logprobs, scores, new_context", "category": "Python"}, {"instruction": "def namedObject(name):\n    \"\"\"Get a fully named module-global object.\n    \"\"\"\n", "input": "", "output": "    classSplit = name.split('.')\n    module = namedModule('.'.join(classSplit[:-1]))\n    return getattr(module, classSplit[-1])", "category": "Python"}, {"instruction": "def save_data(self, filename):\n        \"\"\"\n        Save the assimilated data to a file.\n\n        Args:\n            filename (str): filename to save the assimilated data to. Note\n                that if the filename ends with gz or bz2, the relevant gzip\n                or bz2 compression will be applied.\n        \"\"\"\n", "input": "", "output": "        with zopen(filename, \"wt\") as f:\n            json.dump(list(self._data), f, cls=MontyEncoder)", "category": "Python"}, {"instruction": "def processEnded(self, reason):\n        \"\"\"\n        Connected process shut down\n        \"\"\"\n", "input": "", "output": "        log_debug(\"{name} process exited\", name=self.name)\n        if self.deferred:\n            if reason.type == ProcessDone:\n                self.deferred.callback(reason.value.exitCode)\n            elif reason.type == ProcessTerminated:\n                self.deferred.errback(reason)\n        return self.deferred", "category": "Python"}, {"instruction": "def get_queryset(self, value, row, *args, **kwargs):\n        \"\"\"\n        Returns a queryset of all objects for this Model.\n\n        Overwrite this method if you want to limit the pool of objects from\n        which the related object is retrieved.\n\n        :param value: The field's value in the datasource.\n        :param row: The datasource's current row.\n\n        As an example; if you'd like to have ForeignKeyWidget look up a Person\n        by their pre- **and** lastname column, you could subclass the widget\n        like so::\n\n            class FullNameForeignKeyWidget(ForeignKeyWidget):\n                def get_queryset(self, value, row):\n                    return self.model.objects.filter(\n                        first_name__iexact=row[\"first_name\"],\n                        last_name__iexact=row[\"last_name\"]\n                    )\n        \"\"\"\n", "input": "", "output": "        return self.model.objects.all()", "category": "Python"}, {"instruction": "def autoencoder_residual_text():\n  \"\"\"Residual autoencoder model for text.\"\"\"\n", "input": "", "output": "  hparams = autoencoder_residual()\n  hparams.bottleneck_bits = 32\n  hparams.batch_size = 1024\n  hparams.hidden_size = 64\n  hparams.max_hidden_size = 512\n  hparams.bottleneck_noise = 0.0\n  hparams.bottom = {\n      \"inputs\": modalities.identity_bottom,\n      \"targets\": modalities.identity_bottom,\n  }\n  hparams.top = {\n      \"targets\": modalities.identity_top,\n  }\n  hparams.autoregressive_mode = \"none\"\n  hparams.sample_width = 1\n  return hparams", "category": "Python"}, {"instruction": "def get_xdg_env(env_name, fallback):\n    \"\"\" Used for XDG_* env variables to return fallback if unset *or* empty \"\"\"\n", "input": "", "output": "    env = os.environ.get(env_name)\n    return env if env else fallback", "category": "Python"}, {"instruction": "def file_format_name(self):\n        \"\"\"Return the current format name of the open bdf.\"\"\"\n", "input": "", "output": "        try:\n            return BfdFormatNamesLong[self.file_format]\n        except IndexError, err:\n            raise BfdException(\"Invalid format specified (%d)\" % self.file_format)", "category": "Python"}, {"instruction": "def get_topic_attributes(TopicArn, region=None, key=None, keyid=None, profile=None):\n    '''\n    Returns all of the properties of a topic.  Topic properties returned might differ based on the\n    authorization of the user.\n\n    CLI example::\n\n        salt myminion boto3_sns.get_topic_attributes someTopic region=us-west-1\n    '''\n", "input": "", "output": "    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        return conn.get_topic_attributes(TopicArn=TopicArn).get('Attributes')\n    except botocore.exceptions.ClientError as e:\n        log.error('Failed to garner attributes for SNS topic %s: %s', TopicArn, e)\n        return None", "category": "Python"}, {"instruction": "def fit_predict(self, features, target, sample_weight=None, groups=None):\n        \"\"\"Call fit and predict in sequence.\n\n        Parameters\n        ----------\n        features: array-like {n_samples, n_features}\n            Feature matrix\n        target: array-like {n_samples}\n            List of class labels for prediction\n        sample_weight: array-like {n_samples}, optional\n            Per-sample weights. Higher weights force TPOT to put more emphasis on those points\n        groups: array-like, with shape {n_samples, }, optional\n            Group labels for the samples used when performing cross-validation.\n            This parameter should only be used in conjunction with sklearn's Group cross-validation\n            functions, such as sklearn.model_selection.GroupKFold\n\n        Returns\n        ----------\n        array-like: {n_samples}\n            Predicted target for the provided features\n\n        \"\"\"\n", "input": "", "output": "        self.fit(features, target, sample_weight=sample_weight, groups=groups)\n\n        return self.predict(features)", "category": "Python"}, {"instruction": "def from_uuid(value: uuid.UUID) -> ulid.ULID:\n    \"\"\"\n    Create a new :class:`~ulid.ulid.ULID` instance from the given :class:`~uuid.UUID` value.\n\n    :param value: UUIDv4 value\n    :type value: :class:`~uuid.UUID`\n    :return: ULID from UUID value\n    :rtype: :class:`~ulid.ulid.ULID`\n    \"\"\"\n", "input": "", "output": "    return ulid.ULID(value.bytes)", "category": "Python"}, {"instruction": "def __reversed_filter(filterable, filter_, logic_operation='and'):\n        \"\"\" reverse filtering DataFrame using filter_ key-value conditions applying logic_operation\n        find rows where existing filterable columns (and its values) fitting the filter_ criterion\"\"\"\n", "input": "", "output": "        condition = []\n        try:\n            subscribers_for_any = filterable.query('type == \"__ANY__\"')\n        except pd.core.computation.ops.UndefinedVariableError:\n            subscribers_for_any = pd.DataFrame()\n        if not filter_:\n            return filterable\n        else:\n            for existing_col in filterable:\n                for meta_tag, meta_value in filter_.items():\n                    if meta_tag == existing_col:\n                        condition.append('{key} == \"{value}\"'.format(key=meta_tag, value=meta_value))\n            try:\n                res = filterable.query(\" {operation} \".format(operation=logic_operation).join(condition))\n            except pd.core.computation.ops.UndefinedVariableError:\n                return pd.DataFrame().append(subscribers_for_any)\n            else:\n                return res.append(subscribers_for_any)", "category": "Python"}, {"instruction": "def pressure_trend_text(trend):\n    \"\"\"Convert pressure trend to a string, as used by the UK met\n    office.\n\n    \"\"\"\n", "input": "", "output": "    _ = pywws.localisation.translation.ugettext\n    if trend > 6.0:\n        return _(u'rising very rapidly')\n    elif trend > 3.5:\n        return _(u'rising quickly')\n    elif trend > 1.5:\n        return _(u'rising')\n    elif trend >= 0.1:\n        return _(u'rising slowly')\n    elif trend < -6.0:\n        return _(u'falling very rapidly')\n    elif trend < -3.5:\n        return _(u'falling quickly')\n    elif trend < -1.5:\n        return _(u'falling')\n    elif trend <= -0.1:\n        return _(u'falling slowly')\n    return _(u'steady')", "category": "Python"}, {"instruction": "def default_namespace(self, value):\n        \"\"\"\n        Setter for **self.__default_namespace** attribute.\n\n        :param value: Attribute value.\n        :type value: unicode\n        \"\"\"\n", "input": "", "output": "\n        if value is not None:\n            assert type(value) is unicode, \"'{0}' attribute: '{1}' type is not 'unicode'!\".format(\n                \"default_namespace\", value)\n        self.__default_namespace = value", "category": "Python"}, {"instruction": "def _add_query_parameters(base_url, name_value_pairs):\n    \"\"\"Add one query parameter to a base URL.\n\n    :type base_url: string\n    :param base_url: Base URL (may already contain query parameters)\n\n    :type name_value_pairs: list of (string, string) tuples.\n    :param name_value_pairs: Names and values of the query parameters to add\n\n    :rtype: string\n    :returns: URL with additional query strings appended.\n    \"\"\"\n", "input": "", "output": "    if len(name_value_pairs) == 0:\n        return base_url\n\n    scheme, netloc, path, query, frag = urlsplit(base_url)\n    query = parse_qsl(query)\n    query.extend(name_value_pairs)\n    return urlunsplit((scheme, netloc, path, urlencode(query), frag))", "category": "Python"}, {"instruction": "def create_stack(self, name):\n        \"\"\"\n        Creates stack if necessary.\n        \"\"\"\n", "input": "", "output": "        deployment = find_exact(self.api.deployments, name=name)\n        if not deployment:\n            try:\n                # TODO: replace when python-rightscale handles non-json\n                self.api.client.post(\n                        '/api/deployments',\n                        data={'deployment[name]': name},\n                        )\n            except HTTPError as e:\n                log.error(\n                        'Failed to create stack %s. '\n                        'RightScale returned %d:\\n%s'\n                        % (name, e.response.status_code, e.response.content)\n                        )", "category": "Python"}, {"instruction": "def fill_tree(comments):\n    \"\"\"\n    Insert extra comments in the comments list, so that the root path of the first comment is always visible.\n    Use this in comments' pagination to fill in the tree information.\n\n    The inserted comments have an ``added_path`` attribute.\n    \"\"\"\n", "input": "", "output": "    if not comments:\n        return\n\n    it = iter(comments)\n    first = next(it)\n    extra_path_items = imap(_mark_as_root_path, first.root_path)\n    return chain(extra_path_items, [first], it)", "category": "Python"}, {"instruction": "def trimLeft(self, amount):\n    \"\"\"\n      Trim this fastqSequence in-place by removing <amount> nucleotides from\n      the 5' end (left end).\n\n      :param amount: the number of nucleotides to trim from the left-side of\n                     this sequence.\n    \"\"\"\n", "input": "", "output": "    if amount == 0:\n      return\n    self.sequenceData = self.sequenceData[amount:]\n    self.sequenceQual = self.sequenceQual[amount:]", "category": "Python"}, {"instruction": "def make_headers(headers):\n    \"\"\" Make the cache control headers based on a previous request's\n    response headers\n    \"\"\"\n", "input": "", "output": "    out = {}\n    if 'etag' in headers:\n        out['if-none-match'] = headers['etag']\n    if 'last-modified' in headers:\n        out['if-modified-since'] = headers['last-modified']\n    return out", "category": "Python"}, {"instruction": "def get_objective_banks(self):\n        \"\"\"Gets the objective bank list resulting from the search.\n\n        return: (osid.learning.ObjectiveBankList) - the objective bank\n                list\n        raise:  IllegalState - list already retrieved\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        if self.retrieved:\n            raise errors.IllegalState('List has already been retrieved.')\n        self.retrieved = True\n        return objects.ObjectiveBankList(self._results, runtime=self._runtime)", "category": "Python"}, {"instruction": "def _requirement_to_str_lowercase_name(requirement):\n    \"\"\"\n    Formats a packaging.requirements.Requirement with a lowercase name.\n\n    This is simply a copy of\n    https://github.com/pypa/packaging/blob/16.8/packaging/requirements.py#L109-L124\n    modified to lowercase the dependency name.\n\n    Previously, we were invoking the original Requirement.__str__ method and\n    lower-casing the entire result, which would lowercase the name, *and* other,\n    important stuff that should not be lower-cased (such as the marker). See\n    this issue for more information: https://github.com/pypa/pipenv/issues/2113.\n    \"\"\"\n", "input": "", "output": "\n    parts = [requirement.name.lower()]\n\n    if requirement.extras:\n        parts.append(\"[{0}]\".format(\",\".join(sorted(requirement.extras))))\n\n    if requirement.specifier:\n        parts.append(str(requirement.specifier))\n\n    if requirement.url:\n        parts.append(\"@ {0}\".format(requirement.url))\n\n    if requirement.marker:\n        parts.append(\"; {0}\".format(requirement.marker))\n\n    return \"\".join(parts)", "category": "Python"}, {"instruction": "def ucnstring_to_unicode(ucn_string):\n    \"\"\"Return ucnstring as Unicode.\"\"\"\n", "input": "", "output": "    ucn_string = ucnstring_to_python(ucn_string).decode('utf-8')\n\n    assert isinstance(ucn_string, text_type)\n    return ucn_string", "category": "Python"}, {"instruction": "def get_file_list(self, project, repository, query, limit=100000):\n        \"\"\"\n        Retrieve a page of files from particular directory of a repository.\n        The search is done recursively, so all files from any sub-directory of the specified directory will be returned.\n        The authenticated user must have REPO_READ permission for the specified repository to call this resource.\n        :param project:\n        :param repository:\n        :param query: the commit ID or ref (e.g. a branch or tag) to list the files at.\n                      If not specified the default branch will be used instead.\n        :param limit: OPTIONAL\n        :return:\n        \"\"\"\n", "input": "", "output": "        url = 'rest/api/1.0/projects/{project}/repos/{repository}/files'.format(project=project,\n                                                                                repository=repository)\n        params = {}\n        if query:\n            params['at'] = query\n        if limit:\n            params['limit'] = limit\n        return (self.get(url, params=params) or {}).get('values')", "category": "Python"}, {"instruction": "def create_gre_tunnel_no_encryption(cls, name, local_endpoint, remote_endpoint,\n                          mtu=0, pmtu_discovery=True, ttl=0,\n                          enabled=True, comment=None):\n        \"\"\"\n        Create a GRE Tunnel with no encryption. See `create_gre_tunnel_mode` for\n        constructor descriptions.\n        \"\"\"\n", "input": "", "output": "        return cls.create_gre_tunnel_mode(\n            name, local_endpoint, remote_endpoint, policy_vpn=None,\n            mtu=mtu, pmtu_discovery=pmtu_discovery, ttl=ttl,\n            enabled=enabled, comment=comment)", "category": "Python"}, {"instruction": "def isElement(self, node, name, nsuri=None):\n        \"\"\"Return true if the given node is an element with the given\n           name and optional namespace uri.\"\"\"\n", "input": "", "output": "        if node.nodeType != node.ELEMENT_NODE:\n            return 0\n        return node.localName == name and \\\n               (nsuri is None or self.nsUriMatch(node.namespaceURI, nsuri))", "category": "Python"}, {"instruction": "def register_members(self):\n        \"\"\"Collect the names of the class member and convert them to object\n        members.\n\n        Unlike Terms, the Group class members are converted into object\n        members, so the configuration data\n\n        \"\"\"\n", "input": "", "output": "\n        self._members = {\n            name: attr for name, attr in iteritems(type(self).__dict__) if isinstance(attr, Group)}\n\n        for name, m in iteritems(self._members):\n            m.init_descriptor(name, self)", "category": "Python"}, {"instruction": "def _load_model(self):\n        \"\"\"\n        Loads the peg and the hole models.\n        \"\"\"\n", "input": "", "output": "        super()._load_model()\n        self.mujoco_robot.set_base_xpos([0, 0, 0])\n\n        # Add arena and robot\n        self.model = MujocoWorldBase()\n        self.arena = EmptyArena()\n        if self.use_indicator_object:\n            self.arena.add_pos_indicator()\n        self.model.merge(self.arena)\n        self.model.merge(self.mujoco_robot)\n\n        # Load hole object\n        self.hole_obj = self.hole.get_collision(name=\"hole\", site=True)\n        self.hole_obj.set(\"quat\", \"0 0 0.707 0.707\")\n        self.hole_obj.set(\"pos\", \"0.11 0 0.18\")\n        self.model.merge_asset(self.hole)\n        self.model.worldbody.find(\".//body[@name='left_hand']\").append(self.hole_obj)\n\n        # Load cylinder object\n        self.cyl_obj = self.cylinder.get_collision(name=\"cylinder\", site=True)\n        self.cyl_obj.set(\"pos\", \"0 0 0.15\")\n        self.model.merge_asset(self.cylinder)\n        self.model.worldbody.find(\".//body[@name='right_hand']\").append(self.cyl_obj)\n        self.model.worldbody.find(\".//geom[@name='cylinder']\").set(\"rgba\", \"0 1 0 1\")", "category": "Python"}, {"instruction": "def __get_git_bin():\n    \"\"\"\n    Get git binary location.\n\n    :return: Check git location\n    \"\"\"\n", "input": "", "output": "    git = 'git'\n    alternatives = [\n        '/usr/bin/git'\n    ]\n    for alt in alternatives:\n        if os.path.exists(alt):\n            git = alt\n            break\n    return git", "category": "Python"}, {"instruction": "def intersects_any(self,\n                       ray_origins,\n                       ray_directions,\n                       **kwargs):\n        \"\"\"\n        Find out if each ray hit any triangle on the mesh.\n\n        Parameters\n        ------------\n        ray_origins:      (m,3) float, ray origin points\n        ray_directions:   (m,3) float, ray direction vectors\n\n        Returns\n        ---------\n        hit: boolean, whether any ray hit any triangle on the mesh\n        \"\"\"\n", "input": "", "output": "        index_tri, index_ray = self.intersects_id(ray_origins,\n                                                  ray_directions)\n        hit_any = np.zeros(len(ray_origins), dtype=np.bool)\n        hit_idx = np.unique(index_ray)\n        if len(hit_idx) > 0:\n            hit_any[hit_idx] = True\n        return hit_any", "category": "Python"}, {"instruction": "def build_accumulate(function: Callable[[Any, Any], Tuple[Any, Any]] = None, *,\n                     init: Any = NONE):\n    \"\"\" Decorator to wrap a function to return an Accumulate operator.\n\n    :param function: function to be wrapped\n    :param init: optional initialization for state\n    \"\"\"\n", "input": "", "output": "    _init = init\n\n    def _build_accumulate(function: Callable[[Any, Any], Tuple[Any, Any]]):\n        @wraps(function)\n        def _wrapper(init=NONE) -> Accumulate:\n            init = _init if init is NONE else init\n            if init is NONE:\n                raise TypeError('\"init\" argument has to be defined')\n            return Accumulate(function, init=init)\n        return _wrapper\n\n    if function:\n        return _build_accumulate(function)\n\n    return _build_accumulate", "category": "Python"}, {"instruction": "def new_histogram_with_implicit_reservoir(name, reservoir_type='uniform', *reservoir_args, **reservoir_kwargs):\n    \"\"\"\n    Build a new histogram metric and a reservoir from the given parameters\n    \"\"\"\n", "input": "", "output": "\n    reservoir = new_reservoir(reservoir_type, *reservoir_args, **reservoir_kwargs)\n    return new_histogram(name, reservoir)", "category": "Python"}, {"instruction": "def get_all(self, seq_set: SequenceSet) \\\n            -> Sequence[Tuple[int, CachedMessage]]:\n        \"\"\"Return the cached messages, and their sequence numbers, for the\n        given sequence set.\n\n        Args:\n            seq_set: The message sequence set.\n\n        \"\"\"\n", "input": "", "output": "        if seq_set.uid:\n            all_uids = seq_set.flatten(self.max_uid) & self._uids\n            return [(seq, self._cache[uid])\n                    for seq, uid in enumerate(self._sorted, 1)\n                    if uid in all_uids]\n        else:\n            all_seqs = seq_set.flatten(self.exists)\n            return [(seq, self._cache[uid])\n                    for seq, uid in enumerate(self._sorted, 1)\n                    if seq in all_seqs]", "category": "Python"}, {"instruction": "def convert_uniprot_to_entrez(self, uniprot):\n        \"\"\"Convert Uniprot Id to Entrez Id\"\"\"\n", "input": "", "output": "        # Submit request to NCBI eutils/Gene Database\n        server = \"http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?\" + self.options + \"&db=gene&term={0}\".format(\n            uniprot)\n        r = requests.get(server, headers={\"Content-Type\": \"text/xml\"})\n        if not r.ok:\n            r.raise_for_status()\n            sys.exit()\n        # Process Request\n        response = r.text\n        info = xmltodict.parse(response)\n        geneId = info['eSearchResult']['IdList']['Id']\n        # check to see if more than one result is returned\n        # if you have more than more result then check which Entrez Id returns the same uniprot Id entered.\n        if len(geneId) > 1:\n            for x in geneId:\n                c = self.convert_entrez_to_uniprot(x)\n                c = c.lower()\n                u = uniprot.lower()\n                if c == u:\n                    return x\n        else:\n            return geneId", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self, 'transcript') and self.transcript is not None:\n            _dict['transcript'] = self.transcript\n        if hasattr(self, 'confidence') and self.confidence is not None:\n            _dict['confidence'] = self.confidence\n        if hasattr(self, 'timestamps') and self.timestamps is not None:\n            _dict['timestamps'] = self.timestamps\n        if hasattr(self,\n                   'word_confidence') and self.word_confidence is not None:\n            _dict['word_confidence'] = self.word_confidence\n        return _dict", "category": "Python"}, {"instruction": "def set(self, element_to_set, value):\n        \"\"\"(Helper) Set thermostat\"\"\"\n", "input": "", "output": "        self._elk.send(ts_encode(self.index, value, element_to_set))", "category": "Python"}, {"instruction": "def emit(self, record):\n        \"\"\"Convert a :class:`logging.LogRecord` to GELF and emit it to Graylog\n        via an HTTP POST request\n\n        :param record: :class:`logging.LogRecord` to convert into a\n            Graylog GELF log and emit to Graylog via HTTP POST.\n        :type record: logging.LogRecord\n        \"\"\"\n", "input": "", "output": "        pickle = self.makePickle(record)\n        connection = httplib.HTTPConnection(\n            host=self.host,\n            port=self.port,\n            timeout=self.timeout\n        )\n        connection.request('POST', self.path, pickle, self.headers)", "category": "Python"}, {"instruction": "def listen(self, you):\n        \"\"\"\n        Request a callback for value modification.\n\n        Parameters\n        ----------\n        you : object\n            An instance having ``__call__`` attribute.\n        \"\"\"\n", "input": "", "output": "        self._listeners.append(you)\n        self.raw.talk_to(you)", "category": "Python"}, {"instruction": "def data_input_and_res_time_analysis(self):\n        \"\"\"\n        Loads the data into Data() - renumbers the residues, imports mol file in rdkit.\n        If there are trajectories to analyse, the residues that will be plotted are determined\n        from Residence_time() analysis.\n        \"\"\"\n", "input": "", "output": "        self.topol_data = Data()\n        self.topol_data.load_data(self.topology,self.mol_file,self.ligand,self.offset)\n        if len(self.trajectory) == 0:\n            self.topol_data.analyse_topology(self.topology,self.cutoff)\n        else:\n            self.res_time = Residence_time(self.topol_data,self.trajectory, self.start, self.end, self.skip,self.topology, self.ligand,self.offset)\n            self.res_time.measure_residence_time(self.cutoff)\n            self.res_time.define_residues_for_plotting_traj(self.analysis_cutoff)\n            self.topol_data.find_the_closest_atoms(self.topology)", "category": "Python"}, {"instruction": "def is_union(declaration):\n    \"\"\"\n    Returns True if declaration represents a C++ union\n\n    Args:\n        declaration (declaration_t): the declaration to be checked.\n\n    Returns:\n        bool: True if declaration represents a C++ union\n    \"\"\"\n", "input": "", "output": "    if not is_class(declaration):\n        return False\n    decl = class_traits.get_declaration(declaration)\n    return decl.class_type == class_declaration.CLASS_TYPES.UNION", "category": "Python"}, {"instruction": "def read2(self, how_much=128):  # FIXME: 128 might be too much ... what is largest?\n\t\t\"\"\"\n\t\tThis toggles the RTS pin and reads in data. It also converts the buffer\n\t\tback into a list of bytes and searches through the list to find valid\n\t\tpackets of info. If there is more than one packet, this returns an\n\t\tarray of valid packets.\n\t\t\"\"\"\n", "input": "", "output": "\t\tret = []\n\t\tself.setRTS(self.DD_READ)\n\n\t\theader = [0xFF, 0xFD, 0x00]\n\t\tptr = 0\n\t\twhile True:\n\t\t\tb = self.serial.read(1)\n\t\t\tif not b:\n\t\t\t\treturn None\n\t\t\tb = ord(b)\n\t\t\tprint('b', b)\n\t\t\tif b == header[ptr]:\n\t\t\t\tptr += 1\n\t\t\t\tret.append(b)\n\t\t\t\tif ptr == 3:  # found header\n\t\t\t\t\tprint('found header')\n\t\t\t\t\td = self.serial.read(1)  # ID\n\t\t\t\t\tret.append(ord(d))\n\t\t\t\t\tl, h = self.serial.read(2)  # length\n\t\t\t\t\tl = ord(l)\n\t\t\t\t\th = ord(h)\n\t\t\t\t\tret.append(l)\n\t\t\t\t\tret.append(h)\n\t\t\t\t\tlength = (h << 8) + l\n\t\t\t\t\tprint('length', length)\n\t\t\t\t\thow_many = length\n\t\t\t\t\twhile how_many > 0:\n\t\t\t\t\t\tprint('how_many', how_many)\n\t\t\t\t\t\td = self.serial.read(how_many)\n\t\t\t\t\t\td = self.decode(d)\n\t\t\t\t\t\tprint('read:', len(d))\n\t\t\t\t\t\thow_many -= len(d)\n\t\t\t\t\t\tfor i in d:\n\t\t\t\t\t\t\tret.append(i)\n\t\t\t\t\tprint('bye')\n\t\t\t\t\treturn [ret]\n\t\t\telse:\n\t\t\t\tret = []\n\t\t\t\tptr = 0\n\t\treturn ret", "category": "Python"}, {"instruction": "def set_computer_sleep(minutes):\n    '''\n    Set the amount of idle time until the computer sleeps. Pass \"Never\" of \"Off\"\n    to never sleep.\n\n    :param minutes: Can be an integer between 1 and 180 or \"Never\" or \"Off\"\n    :ptype: int, str\n\n    :return: True if successful, False if not\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' power.set_computer_sleep 120\n        salt '*' power.set_computer_sleep off\n    '''\n", "input": "", "output": "    value = _validate_sleep(minutes)\n    cmd = 'systemsetup -setcomputersleep {0}'.format(value)\n    salt.utils.mac_utils.execute_return_success(cmd)\n\n    return salt.utils.mac_utils.confirm_updated(\n        str(value),\n        get_computer_sleep,\n    )", "category": "Python"}, {"instruction": "def _enrich_link(self, glossary):\n        \"\"\"\n        Enrich the dict glossary['link'] with an identifier onto the model\n        \"\"\"\n", "input": "", "output": "        try:\n            Model = apps.get_model(*glossary['link']['model'].split('.'))\n            obj = Model.objects.get(pk=glossary['link']['pk'])\n            glossary['link'].update(identifier=str(obj))\n        except (KeyError, ObjectDoesNotExist):\n            pass", "category": "Python"}, {"instruction": "def ceph_is_installed(module):\n    \"\"\"\n    A helper callback to be executed after the connection is made to ensure\n    that Ceph is installed.\n    \"\"\"\n", "input": "", "output": "    ceph_package = Ceph(module.conn)\n    if not ceph_package.installed:\n        host = module.conn.hostname\n        raise RuntimeError(\n            'ceph needs to be installed in remote host: %s' % host\n        )", "category": "Python"}, {"instruction": "def construct_url(ip_address: str) -> str:\r\n    \"\"\"Construct the URL with a given IP address.\"\"\"\n", "input": "", "output": "    if 'http://' not in ip_address and 'https://' not in ip_address:\r\n        ip_address = '{}{}'.format('http://', ip_address)\r\n    if ip_address[-1] == '/':\r\n        ip_address = ip_address[:-1]\r\n    return ip_address", "category": "Python"}, {"instruction": "def getJobGraphWhoseServicesAreRunning(self, maxWait):\n        \"\"\"\n        :param float maxWait: Time in seconds to wait to get a jobGraph before returning\n        :return: a jobGraph added to scheduleServices whose services are running, or None if\n        no such job is available.\n        :rtype: JobGraph\n        \"\"\"\n", "input": "", "output": "        try:\n            jobGraph = self._jobGraphsWithServicesThatHaveStarted.get(timeout=maxWait)\n            self.jobGraphsWithServicesBeingStarted.remove(jobGraph)\n            assert self.jobsIssuedToServiceManager >= 0\n            self.jobsIssuedToServiceManager -= 1\n            return jobGraph\n        except Empty:\n            return None", "category": "Python"}, {"instruction": "def download_links(self, dir_path):\n    \"\"\"Download web pages or images from search result links.\n    \n    Args:\n      dir_path (str):\n        Path of directory to save downloads of :class:`api.results`.links\n    \"\"\"\n", "input": "", "output": "    links = self.links\n    if not path.exists(dir_path):\n      makedirs(dir_path)\n    for i, url in enumerate(links):\n      if 'start' in self.cseargs:\n        i += int(self.cseargs['start'])\n      ext = self.cseargs['fileType']\n      ext = '.html' if ext == '' else '.' + ext\n      file_name = self.cseargs['q'].replace(' ', '_') + '_' + str(i) + ext\n      file_path = path.join(dir_path, file_name)\n      r = requests.get(url, stream=True)\n      if r.status_code == 200:\n        with open(file_path, 'wb') as f:\n          r.raw.decode_content = True\n          shutil.copyfileobj(r.raw, f)", "category": "Python"}, {"instruction": "def dict_hash(dct):\n    \"\"\"Return a hash of the contents of a dictionary\"\"\"\n", "input": "", "output": "    dct_s = json.dumps(dct, sort_keys=True)\n\n    try:\n        m = md5(dct_s)\n    except TypeError:\n        m = md5(dct_s.encode())\n\n    return m.hexdigest()", "category": "Python"}, {"instruction": "def buckets_get(self, bucket, projection='noAcl'):\n    \"\"\"Issues a request to retrieve information about a bucket.\n\n    Args:\n      bucket: the name of the bucket.\n      projection: the projection of the bucket information to retrieve.\n    Returns:\n      A parsed bucket information dictionary.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n", "input": "", "output": "    args = {'projection': projection}\n    url = Api._ENDPOINT + (Api._BUCKET_PATH % bucket)\n    return google.datalab.utils.Http.request(url, credentials=self._credentials, args=args)", "category": "Python"}, {"instruction": "def format(self, model: AssetAllocationModel, full: bool = False):\n        \"\"\" Returns the view-friendly output of the aa model \"\"\"\n", "input": "", "output": "        self.full = full\n\n        # Header\n        output = f\"Asset Allocation model, total: {model.currency} {model.total_amount:,.2f}\\n\"\n\n        # Column Headers\n        for column in self.columns:\n            name = column['name']\n            if not self.full and name == \"loc.cur.\":\n                # Skip local currency if not displaying stocks.\n                continue\n            width = column[\"width\"]\n            output += f\"{name:^{width}}\"\n        output += \"\\n\"\n        output += f\"-------------------------------------------------------------------------------\\n\"\n\n        # Asset classes\n\n        view_model = ModelMapper(model).map_to_linear(self.full)\n        for row in view_model:\n            output += self.__format_row(row) + \"\\n\"\n\n        return output", "category": "Python"}, {"instruction": "def _GetNormalizedTimestamp(self):\n    \"\"\"Retrieves the normalized timestamp.\n\n    Returns:\n      decimal.Decimal: normalized timestamp, which contains the number of\n          seconds since January 1, 1970 00:00:00 and a fraction of second used\n          for increased precision, or None if the normalized timestamp cannot be\n          determined.\n    \"\"\"\n", "input": "", "output": "    if self._normalized_timestamp is None:\n      if (self._timestamp is not None and self._timestamp >= self._INT64_MIN and\n          self._timestamp <= self._INT64_MAX):\n        self._normalized_timestamp = (\n            decimal.Decimal(self._timestamp) /\n            definitions.MILLISECONDS_PER_SECOND)\n\n    return self._normalized_timestamp", "category": "Python"}, {"instruction": "def densities_close(rho0: Density, rho1: Density,\n                    tolerance: float = TOLERANCE) -> bool:\n    \"\"\"Returns True if densities are almost identical.\n\n    Closeness is measured with the metric Fubini-Study angle.\n    \"\"\"\n", "input": "", "output": "    return vectors_close(rho0.vec, rho1.vec, tolerance)", "category": "Python"}, {"instruction": "async def _reset_vector(self):\n        \"\"\"Background task to initialize this system in the event loop.\"\"\"\n", "input": "", "output": "\n        self._logger.debug(\"sensor_graph subsystem task starting\")\n\n        # If there is a persistent sgf loaded, send reset information.\n\n        self.initialized.set()\n\n        while True:\n            stream, reading = await self._inputs.get()\n\n            try:\n                await process_graph_input(self.graph, stream, reading, self._executor)\n                self.process_streamers()\n            except:  #pylint:disable=bare-except;This is a background task that should not die\n                self._logger.exception(\"Unhandled exception processing sensor_graph input (stream=%s), reading=%s\", stream, reading)\n            finally:\n                self._inputs.task_done()", "category": "Python"}, {"instruction": "def parse_dynamics(self, node):\n        \"\"\"\n        Parses <Dynamics>\n\n        @param node: Node containing the <Behaviour> element\n        @type node: xml.etree.Element\n        \"\"\"\n", "input": "", "output": "\n        self.current_dynamics = self.current_component_type.dynamics\n        self.current_regime = self.current_dynamics\n        self.process_nested_tags(node)\n        self.current_regime = None\n        self.current_dynamics = None", "category": "Python"}, {"instruction": "def download_options(dataset, node, entityids, api_key=None):\n    \"\"\"\n    The use of the download options request is to discover the different download\n    options for each scene. Some download options may exist but still be unavailable\n    due to disk usage and many other factors. If a download is unavailable\n    it may need to be ordered.\n    \n    :param dataset:\n    \n    :param node:\n    \n    :param entityIds:\n    \n    :param api_key:\n        API key is not required.\n    \"\"\"\n", "input": "", "output": "\n    payload = {\n        \"apiKey\": api_key,\n        \"datasetName\": dataset,\n        \"node\": node,\n        \"entityIds\": entityids\n    }\n\n    return json.dumps(payload)", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self, 'total') and self.total is not None:\n            _dict['total'] = self.total\n        if hasattr(self, 'pending') and self.pending is not None:\n            _dict['pending'] = self.pending\n        if hasattr(self, 'successful') and self.successful is not None:\n            _dict['successful'] = self.successful\n        if hasattr(self, 'failed') and self.failed is not None:\n            _dict['failed'] = self.failed\n        return _dict", "category": "Python"}, {"instruction": "def _xml_convert(self, element):\n        \"\"\"\n        convert the xml `element` into the corresponding python object\n        \"\"\"\n", "input": "", "output": "\n        children = list(element)\n\n        if len(children) == 0:\n            return self._type_convert(element.text)\n        else:\n            # if the fist child tag is list-item means all children are list-item\n            if children[0].tag == \"list-item\":\n                data = []\n                for child in children:\n                    data.append(self._xml_convert(child))\n            else:\n                data = {}\n                for child in children:\n                    data[child.tag] = self._xml_convert(child)\n\n            return data", "category": "Python"}, {"instruction": "def get_hostnames(self):\n        \"\"\"\n        Return the set of literal hostnames defined in the SSH config (both\n        explicit hostnames and wildcard entries).\n        \"\"\"\n", "input": "", "output": "        hosts = set()\n        for entry in self._config:\n            hosts.update(entry[\"host\"])\n        return hosts", "category": "Python"}, {"instruction": "def load(filepath=None, filecontent=None):\n    \"\"\" Read the json file located at `filepath`\n\n    If `filecontent` is specified, its content will be json decoded\n    and loaded instead.\n\n    Usage:\n        config.load(filepath=None, filecontent=None):\n        Provide either a filepath or a json string\n    \"\"\"\n", "input": "", "output": "    conf = DotDict()\n\n    assert filepath or filecontent\n    if not filecontent:\n        with io.FileIO(filepath) as handle:\n            filecontent = handle.read().decode('utf-8')\n    configs = json.loads(filecontent)\n    conf.update(configs.items())\n    return conf", "category": "Python"}, {"instruction": "def similar_datetime(anon, obj, field, val):\n    \"\"\"\n    Returns a datetime that is within plus/minus two years of the original datetime\n    \"\"\"\n", "input": "", "output": "    return anon.faker.datetime(field=field, val=val)", "category": "Python"}, {"instruction": "def registerPlexObject(cls):\n    \"\"\" Registry of library types we may come across when parsing XML. This allows us to\n        define a few helper functions to dynamically convery the XML into objects. See\n        buildItem() below for an example.\n    \"\"\"\n", "input": "", "output": "    etype = getattr(cls, 'STREAMTYPE', cls.TYPE)\n    ehash = '%s.%s' % (cls.TAG, etype) if etype else cls.TAG\n    if ehash in PLEXOBJECTS:\n        raise Exception('Ambiguous PlexObject definition %s(tag=%s, type=%s) with %s' %\n            (cls.__name__, cls.TAG, etype, PLEXOBJECTS[ehash].__name__))\n    PLEXOBJECTS[ehash] = cls\n    return cls", "category": "Python"}, {"instruction": "def write(self, data):\n        \"\"\"Writes data to device or interface synchronously.\n\n        Corresponds to viWrite function of the VISA library.\n\n        :param data: data to be written.\n        :type data: bytes\n        :return: Number of bytes actually transferred, return value of the library call.\n        :rtype: (int, VISAStatus)\n        \"\"\"\n", "input": "", "output": "\n        send_end, _ = self.get_attribute(constants.VI_ATTR_SEND_END_EN)\n\n        count = self.interface.write(data)\n\n        return count, StatusCode.success", "category": "Python"}, {"instruction": "def custom_update_user(sender, instance, attributes, user_modified, **kargs):\n    \"\"\" Default behaviour does not play nice with booleans encoded in SAML as u'true'/u'false'.\n        This will convert those attributes to real booleans when saving.\n    \"\"\"\n", "input": "", "output": "    for k, v in attributes.items():\n        u = set.intersection(set(v), set([u'true', u'false']))\n        if u:\n            setattr(instance, k, u.pop() == u'true')\n    return True", "category": "Python"}, {"instruction": "def get_namespace(taskfileinfo):\n    \"\"\"Return a suitable name for a namespace for the taskfileinfo\n\n    Returns the name of the shot/asset with a \"_1\" suffix.\n    When you create the namespace the number will automatically be incremented by Maya.\n\n    :param taskfileinfo: the taskfile info for the file that needs a namespace\n    :type taskfileinfo: :class:`jukeboxcore.filesys.TaskFileInfo`\n    :returns: a namespace suggestion\n    :rtype: str\n    :raises: None\n    \"\"\"\n", "input": "", "output": "    element = taskfileinfo.task.element\n    name = element.name\n    return name + \"_1\"", "category": "Python"}, {"instruction": "def download(url, file=None):\n    \"\"\"\n    Pass file as a filename, open file object, or None to return the request bytes\n\n    Args:\n        url (str): URL of file to download\n        file (Union[str, io, None]): One of the following:\n             - Filename of output file\n             - File opened in binary write mode\n             - None: Return raw bytes instead\n\n    Returns:\n        Union[bytes, None]: Bytes of file if file is None\n    \"\"\"\n", "input": "", "output": "    import urllib.request\n    import shutil\n    if isinstance(file, str):\n        file = open(file, 'wb')\n    try:\n        with urllib.request.urlopen(url) as response:\n            if file:\n                shutil.copyfileobj(response, file)\n            else:\n                return response.read()\n    finally:\n        if file:\n            file.close()", "category": "Python"}, {"instruction": "def _locate_bar_gen(icut, epos, transform1, transform2):\n    \"\"\"Generic function for the fine position of the CSU\"\"\"\n", "input": "", "output": "\n    epos_pix = coor_to_pix_1d(epos)\n\n    # transform ->\n    epos_pix_s = transform1(epos_pix)\n    icut2 = transform2(icut)\n    #\n\n    try:\n        res = position_half_h(icut2, epos_pix_s)\n\n        xint_s, next_peak_s, wpos1_s, wpos2_s, background_level, half_height = res\n        #\n\n        xint = transform1(xint_s)\n\n        #\n        epos_f = xint\n        error = 0\n    except ValueError:\n        error = 2\n        epos_f = epos\n\n    return epos_pix, epos_f, error", "category": "Python"}, {"instruction": "def copy(self):\n        \"\"\"Returns a copy of the object.\"\"\"\n", "input": "", "output": "        attrs = {k: self.__dict__[k].copy() for k in self.containers}\n        attrs.update({k: cp.deepcopy(self.__dict__[k]) for k in self.shared})\n        return self.__class__(**attrs)", "category": "Python"}, {"instruction": "def create_daily_trade_source(sids,\n                              sim_params,\n                              asset_finder,\n                              trading_calendar):\n    \"\"\"\n    creates trade_count trades for each sid in sids list.\n    first trade will be on sim_params.start_session, and daily\n    thereafter for each sid. Thus, two sids should result in two trades per\n    day.\n    \"\"\"\n", "input": "", "output": "    return create_trade_source(\n        sids,\n        timedelta(days=1),\n        sim_params,\n        asset_finder,\n        trading_calendar=trading_calendar,\n    )", "category": "Python"}, {"instruction": "def plot(self):\n        \"\"\"\n        Visualize the process.\n\n        :return: The generated figure.\n        :rtype: matplotlib.Figure\n        \"\"\"\n", "input": "", "output": "        fig, (ax1) = plt.subplots(1, 1, figsize=(10, 8))\n        self.plot_pauli_transfer_matrix(ax1)\n        return fig", "category": "Python"}, {"instruction": "def make_exception_message(exc):\n    \"\"\"\n    An exception is passed in and this function\n    returns the proper string depending on the result\n    so it is readable enough.\n    \"\"\"\n", "input": "", "output": "    if str(exc):\n        return '%s: %s\\n' % (exc.__class__.__name__, exc)\n    else:\n        return '%s\\n' % (exc.__class__.__name__)", "category": "Python"}, {"instruction": "def get_data(self, ftype='normal'):\n        \"\"\" Get file content as a bytestring.\n\n        :param ftype:       Select 'view' on file.\n        :type ftype:        str\n        :return:            File content\n        :rtype:             bytes\n        \"\"\"\n", "input": "", "output": "        camfile_p = ffi.new(\"CameraFile**\")\n        lib.gp_file_new(camfile_p)\n        lib.gp_camera_file_get(\n            self._cam._cam, self.directory.path.encode(), self.name.encode(),\n            backend.FILE_TYPES[ftype], camfile_p[0], self._cam._ctx)\n        data_p = ffi.new(\"char**\")\n        length_p = ffi.new(\"unsigned long*\")\n        lib.gp_file_get_data_and_size(camfile_p[0], data_p, length_p)\n        byt = bytes(ffi.buffer(data_p[0], length_p[0]))\n        # gphoto2 camera files MUST be freed.\n        lib.gp_file_free(camfile_p[0])\n        # just to be safe.\n        del data_p, length_p, camfile_p\n        return byt", "category": "Python"}, {"instruction": "def dens_alum_nanocluster(coag):\n    \"\"\"Return the density of the aluminum in the nanocluster.\n\n    This is useful for determining the volume of nanoclusters\n    given a concentration of aluminum.\n    \"\"\"\n", "input": "", "output": "    density = (coag.PrecipDensity * MOLEC_WEIGHT_ALUMINUM\n               * coag.PrecipAluminumMPM / coag.PrecipMolecWeight)\n    return density", "category": "Python"}, {"instruction": "def get_thumbsdir(self, path):\n        \"\"\"\n        path:\n            path of the source image\n        \"\"\"\n", "input": "", "output": "        # Thumbsdir could be a callable\n        # In that case, the path is built on the fly, based on the source path\n        thumbsdir = self.thumbsdir\n        if callable(self.thumbsdir):\n            thumbsdir = self.thumbsdir(path)\n        return thumbsdir", "category": "Python"}, {"instruction": "def constant_jump_targets_and_jumpkinds(self):\n        \"\"\"\n        A dict of the static jump targets of the basic block to their jumpkind.\n        \"\"\"\n", "input": "", "output": "        exits = dict()\n\n        if self.exit_statements:\n            for _, _, stmt_ in self.exit_statements:\n                exits[stmt_.dst.value] = stmt_.jumpkind\n\n        default_target = self.default_exit_target\n        if default_target is not None:\n            exits[default_target] = self.jumpkind\n\n        return exits", "category": "Python"}, {"instruction": "def cmd_line(line, ctrl):\n    clients = ctrl.modules\n    \"\"\" :type: list[WrapperClient] \"\"\"\n", "input": "", "output": "    if line == \"update start\":\n        for client in clients:\n            client.updating_start()\n    elif line == \"update stop\":\n        for client in clients:\n            client.updating_stop()\n    return line", "category": "Python"}, {"instruction": "def mapping_ref(self, es_mappings):\r\n        \"\"\"\r\n        Retruns a dictionary of mappings and the fiels names in dot notation\r\n\r\n        args:\r\n            mappings: es mapping defitions to parse\r\n        \"\"\"\n", "input": "", "output": "\r\n        new_map = {}\r\n        for key, value in es_mappings.items():\r\n            for sub_key, sub_value in value.items():\r\n                new_map[\"/\".join([key, sub_key])] = \\\r\n                        mapping_fields(sub_value['properties'])\r\n        return new_map", "category": "Python"}, {"instruction": "def weighted_median(y, w):\n    \"\"\"\n    Compute weighted median of `y` with weights `w`.\n    \"\"\"\n", "input": "", "output": "    items = sorted(zip(y, w))\n    midpoint = sum(w) / 2\n\n    yvals = []\n    wsum = 0\n\n    for yy, ww in items:\n        wsum += ww\n        if wsum > midpoint:\n            yvals.append(yy)\n            break\n        elif wsum == midpoint:\n            yvals.append(yy)\n    else:\n        yvals = y\n\n    return sum(yvals) / len(yvals)", "category": "Python"}, {"instruction": "def _get_value(self, exc_type, exc_value, exc_traceback):\n        \"\"\"\n        Convert exception info to a value for the values list.\n        \"\"\"\n", "input": "", "output": "        stack_info = get_stack_info(\n            iter_traceback_frames(exc_traceback),\n            transformer=self.transform,\n            capture_locals=self.client.capture_locals,\n        )\n\n        exc_module = getattr(exc_type, '__module__', None)\n        if exc_module:\n            exc_module = str(exc_module)\n        exc_type = getattr(exc_type, '__name__', '<unknown>')\n\n        return {\n            'value': to_unicode(exc_value),\n            'type': str(exc_type),\n            'module': to_unicode(exc_module),\n            'stacktrace': stack_info,\n        }", "category": "Python"}, {"instruction": "def oembed(url, class_=\"\"):\n    \"\"\"\n    Create OEmbed link\n\n    {{ url | oembed }}\n    :param url:\n    :param class_:\n    :return:\n    \"\"\"\n", "input": "", "output": "    o = \"<a href=\\\"{url}\\\" class=\\\"oembed {class_}\\\" ></a>\".format(url=url,\n                                                                   class_=class_)\n    return Markup(o)", "category": "Python"}, {"instruction": "def _proxy(self):\n        \"\"\"\n        Generate an instance context for the instance, the context is capable of\n        performing various actions.  All instance actions are proxied to the context\n\n        :returns: StepContextContext for this StepContextInstance\n        :rtype: twilio.rest.studio.v1.flow.engagement.step.step_context.StepContextContext\n        \"\"\"\n", "input": "", "output": "        if self._context is None:\n            self._context = StepContextContext(\n                self._version,\n                flow_sid=self._solution['flow_sid'],\n                engagement_sid=self._solution['engagement_sid'],\n                step_sid=self._solution['step_sid'],\n            )\n        return self._context", "category": "Python"}, {"instruction": "def bitset(bs, member_label=None, filename=None, directory=None, format=None,\n           render=False, view=False):\n    \"\"\"Graphviz source for the Hasse diagram of the domains' Boolean algebra.\"\"\"\n", "input": "", "output": "    if member_label is None:\n        member_label = MEMBER_LABEL\n\n    if filename is None:\n        kind = 'members' if member_label else 'bits'\n        filename = FILENAME % (bs.__name__, kind)\n\n    dot = graphviz.Digraph(\n        name=bs.__name__,\n        comment=repr(bs),\n        filename=filename,\n        directory=directory,\n        format=format,\n        edge_attr={'dir': 'none'}\n    )\n\n    node_name = NAME_GETTERS[0]\n\n    if callable(member_label):\n        node_label = member_label\n    else:\n        node_label = LABEL_GETTERS[member_label]\n\n    for i in range(bs.supremum + 1):\n        b = bs.fromint(i)\n        name = node_name(b)\n        dot.node(name, node_label(b))\n        dot.edges((name, node_name(b & ~a)) for a in b.atoms(reverse=True))\n\n    if render or view:\n        dot.render(view=view)  # pragma: no cover\n    return dot", "category": "Python"}, {"instruction": "def iter_genotypes(self):\n        \"\"\"Iterates on available markers.\n\n        Returns:\n            Genotypes instances.\n\n        \"\"\"\n", "input": "", "output": "        for info, dosage in self._bgen.iter_variants():\n            yield Genotypes(\n                Variant(\n                    info.name, CHROM_STR_ENCODE.get(info.chrom, info.chrom),\n                    info.pos, [info.a1, info.a2],\n                ),\n                dosage,\n                reference=info.a1,\n                coded=info.a2,\n                multiallelic=True,\n            )", "category": "Python"}, {"instruction": "def _guess_iface_name(netif):\n    \"\"\"\n    We attempt to guess the name of interfaces that are truncated from the\n    output of ifconfig -l.\n    If there is only one possible candidate matching the interface name then we\n    return it.\n    If there are none or more, then we return None.\n    \"\"\"\n", "input": "", "output": "    with os.popen('%s -l' % conf.prog.ifconfig) as fdesc:\n        ifaces = fdesc.readline().strip().split(' ')\n    matches = [iface for iface in ifaces if iface.startswith(netif)]\n    if len(matches) == 1:\n        return matches[0]\n    return None", "category": "Python"}, {"instruction": "def get_window(self):\n        \"\"\"Generate the bi-square window for this row\n\n        Returns\n        -------\n        window : `numpy.ndarray`\n        \"\"\"\n", "input": "", "output": "        # real frequencies\n        wfrequencies = self._get_indices() / self.duration\n        # dimensionless frequencies\n        xfrequencies = wfrequencies * self.qprime / self.frequency\n        # normalize and generate bi-square window\n        norm = self.ntiles / (self.duration * self.sampling) * (\n            315 * self.qprime / (128 * self.frequency)) ** (1/2.)\n        return (1 - xfrequencies ** 2) ** 2 * norm", "category": "Python"}, {"instruction": "def get_unread_messages():\n    \"\"\"Get all unread messages\"\"\"\n", "input": "", "output": "    mark_seen = request.args.get('mark_seen', True)\n    unread_msg = g.driver.get_unread()\n\n    if mark_seen:\n        for msg in unread_msg:\n            msg.chat.send_seen()\n\n    return jsonify(unread_msg)", "category": "Python"}, {"instruction": "def expand_single_values(var, scans):\n        \"\"\"Expand single valued variable to full scan lengths.\"\"\"\n", "input": "", "output": "        if scans.size == 1:\n            return var\n        else:\n            expanded = np.repeat(var, scans)\n            expanded.attrs = var.attrs\n            expanded.rename({expanded.dims[0]: 'y'})\n            return expanded", "category": "Python"}, {"instruction": "def bounds(self):\n        \"\"\"The bounds of the random variable.\n        \n        Set `self.i=0.95` to return the 95% interval if this is used for setting\n        bounds on optimizers/etc. where infinite bounds may not be useful.\n        \"\"\"\n", "input": "", "output": "        return [scipy.stats.lognorm.interval(self.i, s, loc=0, scale=em) for s, em in zip(self.sigma, self.emu)]", "category": "Python"}, {"instruction": "def _sync_io(self):\n        \"\"\"Update the stream with changes to the file object contents.\"\"\"\n", "input": "", "output": "        if self._file_epoch == self.file_object.epoch:\n            return\n\n        if self._io.binary:\n            contents = self.file_object.byte_contents\n        else:\n            contents = self.file_object.contents\n\n        self._set_stream_contents(contents)\n        self._file_epoch = self.file_object.epoch", "category": "Python"}, {"instruction": "def author_mail_from_git(self):\n        \"\"\" Get the author mail from git information. \"\"\"\n", "input": "", "output": "        try:\n            # launch git command and get answer\n            cmd = Popen([\"git\", \"config\", \"--get\", \"user.email\"], stdout=PIPE)\n            stdoutdata = cmd.communicate()\n            if (stdoutdata[0]):\n                self.author_mail = stdoutdata[0].rstrip(os.linesep)\n        except ImportError:\n            pass\n        except CalledProcessError:\n            pass\n        except OSError:\n            pass\n\n        return self.author_mail", "category": "Python"}, {"instruction": "def to_dict(self):\n        \"\"\" Return a dictionary of the job stats.\n\n        Returns:\n            dict: Dictionary of the stats.\n        \"\"\"\n", "input": "", "output": "        return {\n            'name': self.name,\n            'id': self.id,\n            'type': self.type,\n            'workflow_id': self.workflow_id,\n            'queue': self.queue,\n            'start_time': self.start_time,\n            'arguments': self.arguments,\n            'acknowledged': self.acknowledged,\n            'func_name': self.func_name,\n            'hostname': self.hostname,\n            'worker_name': self.worker_name,\n            'worker_pid': self.worker_pid,\n            'routing_key': self.routing_key\n        }", "category": "Python"}, {"instruction": "def get_logger(self, logfilename=None):\n        \"\"\"Setup logger.\n\n        Allow outputting to both a log file and console at the\n        same time.\n        \"\"\"\n", "input": "", "output": "        if self._logger is None:\n            self._logger = logging.getLogger('invenio_upgrader')\n            self._logger.setLevel(logging.INFO)\n\n            if logfilename:\n                fh = logging.FileHandler(logfilename)\n                fh.setLevel(logging.INFO)\n                fh.setFormatter(self._logger_file_fmtter)\n                self._logger.addHandler(fh)\n\n            ch = logging.StreamHandler(sys.stdout)\n            ch.setLevel(logging.INFO)\n            ch.setFormatter(self._logger_console_fmtter)\n\n            self._logger.addHandler(ch)\n\n            # Replace show warnings (documented in Python manual)\n            def showwarning(message, dummy_category, dummy_filename,\n                            dummy_lineno, *dummy_args):\n                self.warning_occured += 1\n                logger = self.get_logger()\n                logger.warning(message)\n            warnings.showwarning = showwarning\n\n            self._teardown_log_prefix()\n\n        return self._logger", "category": "Python"}, {"instruction": "def check_repository_existence(params):\n    \"\"\"Check repository existence.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n", "input": "", "output": "    repodir = os.path.join(params.outdir, params.name)\n    if os.path.isdir(repodir):\n        raise Conflict(\n            'Package repository \"{0}\" has already exists.'.format(repodir))", "category": "Python"}, {"instruction": "def levenshtein_distance(self, a, b):\n        '''This calculates the Levenshtein distance between a and b.\n        '''\n", "input": "", "output": "\n        n, m = len(a), len(b)\n        if n > m:\n            a,b = b,a\n            n,m = m,n\n        current = range(n+1)\n        for i in range(1,m+1):\n            previous, current = current, [i]+[0]*n\n            for j in range(1,n+1):\n                add, delete = previous[j]+1, current[j-1]+1\n                change = previous[j-1]\n                if a[j-1] != b[i-1]:\n                    change = change + 1\n                current[j] = min(add, delete, change)\n        return current[n]", "category": "Python"}, {"instruction": "def plot_temporal_firing_rate(self,time_dimension=0,resolution=1.0,units=None,min_t=None,max_t=None,weight_function=None,normalize_time=False,normalize_n=False,start_units_with_0=True,cell_dimension='N',**kwargs):\n        \"\"\"\n            Plots a firing rate plot.\n\n            Accepts the same keyword arguments as :func:`matplotlib.pylab.plot()` for lines (:class:`~matplotlib.lines.Line2D`), eg `color`, `linewidth` (or `lw`), `linestyle` (or `ls`).\n            See help for :func:`matplotlib.pylab.plot()`.\n        \"\"\"\n", "input": "", "output": "        if bool(self):\n            import matplotlib.pylab as plt\n            H,ed = self.temporal_firing_rate(time_dimension=time_dimension,resolution=resolution,units=units,min_t=min_t,max_t=max_t,weight_function=weight_function,normalize_time=normalize_time,normalize_n=normalize_n,start_units_with_0=start_units_with_0,cell_dimension=cell_dimension)\n            plt.plot(ed[1:],H,**kwargs)", "category": "Python"}, {"instruction": "def _get_wrapper(self):\n        \"\"\"\n        Return:\n            Wrapper object\n        Raise:\n            Exception if wrapper object cannot be found\n        \"\"\"\n", "input": "", "output": "        if self.args_type == \"MODULE_FUNCTION\":\n            return getattr(self.obj, self.prop)\n        elif self.args_type == \"FUNCTION\":\n            return getattr(self.g, self.obj.__name__)\n        elif self.args_type == \"PURE\":\n            return getattr(self.pure, \"func\")\n        else:\n            ErrorHandler.wrapper_object_not_found_error()", "category": "Python"}, {"instruction": "def getCell(self, row, column):\n        \"\"\" Returns the specified cell (if a raster is set for the region) \"\"\"\n", "input": "", "output": "        row = int(row)\n        column = int(column)\n        if self._raster[0] == 0 or self._raster[1] == 0:\n            return self\n        rowHeight = self.h / self._raster[0]\n        columnWidth = self.h / self._raster[1]\n        if column < 0:\n            # If column is negative, count backwards from the end\n            column = self._raster[1] - column\n            if column < 0:\n                # Bad column index, return last column\n                column = self._raster[1]\n        elif column > self._raster[1]:\n            # Bad column index, return first column\n            column = 0\n        if row < 0:\n            # If row is negative, count backwards from the end\n            row = self._raster[0] - row\n            if row < 0:\n                # Bad row index, return last row\n                row = self._raster[0]\n        elif row > self._raster[0]:\n            # Bad row index, return first row\n            row = 0\n        return Region(self.x+(column*columnWidth), self.y+(row*rowHeight), columnWidth, rowHeight)", "category": "Python"}, {"instruction": "def parse(cls, root):\n        \"\"\"\n        Create a new MDWrap by parsing root.\n\n        :param root: Element or ElementTree to be parsed into a MDWrap.\n        :raises exceptions.ParseError: If mdWrap does not contain MDTYPE\n        :raises exceptions.ParseError: If xmlData contains no children\n        \"\"\"\n", "input": "", "output": "        if root.tag != utils.lxmlns(\"mets\") + \"mdWrap\":\n            raise exceptions.ParseError(\n                \"MDWrap can only parse mdWrap elements with METS namespace.\"\n            )\n        mdtype = root.get(\"MDTYPE\")\n        if not mdtype:\n            raise exceptions.ParseError(\"mdWrap must have a MDTYPE\")\n        othermdtype = root.get(\"OTHERMDTYPE\")\n        document = root.xpath(\"mets:xmlData/*\", namespaces=utils.NAMESPACES)\n        if len(document) == 0:\n            raise exceptions.ParseError(\n                \"All mdWrap/xmlData elements must have at least one child; this\"\n                \" one has none\"\n            )\n        elif len(document) == 1:\n            document = document[0]\n\n        # Create a copy, so that the element is not moved by duplicate references.\n        document = copy.deepcopy(document)\n\n        return cls(document, mdtype, othermdtype)", "category": "Python"}, {"instruction": "def flatMapValues(self, f):\n        \"\"\"\n        Pass each value in the key-value pair RDD through a flatMap function\n        without changing the keys; this also retains the original RDD's\n        partitioning.\n\n        >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n        >>> def f(x): return x\n        >>> x.flatMapValues(f).collect()\n        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n        \"\"\"\n", "input": "", "output": "        flat_map_fn = lambda kv: ((kv[0], x) for x in f(kv[1]))\n        return self.flatMap(flat_map_fn, preservesPartitioning=True)", "category": "Python"}, {"instruction": "def execute(self, eopatch):\n        \"\"\" Mask values of `feature` according to the `mask_values` in `mask_feature`\n\n        :param eopatch: `eopatch` to be processed\n        :return: Same `eopatch` instance with masked `feature`\n        \"\"\"\n", "input": "", "output": "        feature_type, feature_name, new_feature_name = next(self.feature(eopatch))\n        mask_feature_type, mask_feature_name = next(self.mask_feature(eopatch))\n\n        data = np.copy(eopatch[feature_type][feature_name])\n        mask = eopatch[mask_feature_type][mask_feature_name]\n\n        if not isinstance(self.mask_values, list):\n            raise ValueError('Incorrect format or values of argument `mask_values`')\n\n        for value in self.mask_values:\n            data[mask.squeeze() == value] = self.no_data_value\n\n        eopatch.add_feature(feature_type, new_feature_name, data)\n\n        return eopatch", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self, 'values') and self.values is not None:\n            _dict['values'] = [x._to_dict() for x in self.values]\n        if hasattr(self, 'pagination') and self.pagination is not None:\n            _dict['pagination'] = self.pagination._to_dict()\n        return _dict", "category": "Python"}, {"instruction": "def _onClass(self, name, line, pos, absPosition,\n                 keywordLine, keywordPos,\n                 colonLine, colonPos, level):\n        \"\"\"Memorizes a class\"\"\"\n", "input": "", "output": "        self.__flushLevel(level)\n        c = Class(name, line, pos, absPosition, keywordLine, keywordPos,\n                  colonLine, colonPos)\n        if self.__lastDecorators is not None:\n            c.decorators = self.__lastDecorators\n            self.__lastDecorators = None\n        self.objectsStack.append(c)", "category": "Python"}, {"instruction": "def dataset_upload_file(self, path, quiet):\n        \"\"\" upload a dataset file\n\n            Parameters\n            ==========\n            path: the complete path to upload\n            quiet: suppress verbose output (default is False)\n        \"\"\"\n", "input": "", "output": "        file_name = os.path.basename(path)\n        content_length = os.path.getsize(path)\n        last_modified_date_utc = int(os.path.getmtime(path))\n        result = FileUploadInfo(\n            self.process_response(\n                self.datasets_upload_file_with_http_info(\n                    file_name, content_length, last_modified_date_utc)))\n\n        success = self.upload_complete(path, result.createUrl, quiet)\n\n        if success:\n            return result.token\n        return None", "category": "Python"}, {"instruction": "def send(self, data):\n        \"\"\"\n        Tries to send data to the client.\n\n        :param data: Data to be sent\n        :return: True if the data was sent, False on error\n        \"\"\"\n", "input": "", "output": "        if data is not None:\n            data = data.encode(\"UTF-8\")\n\n        try:\n            self.wfile.write(data)\n            self.wfile.flush()\n            return True\n\n        except IOError:\n            # An error occurred, mask it\n            # -> This allows to handle the command even if the client has been\n            # disconnect (i.e. \"echo stop 0 | nc localhost 9000\")\n            return False", "category": "Python"}, {"instruction": "def enforce_vertical_symmetry(pixmap):\n    '''Enforces vertical symmetry of the pixelmap.\n    Returns a pixelmap with all pixels mirrored in the middle.\n    The initial ones still remain.'''\n", "input": "", "output": "    mirror = []\n    for item in pixmap:\n        y = item[0]\n        x = item[1]\n\n        if x <= IMAGE_APEX:\n            diff_x = diff(x, IMAGE_APEX)\n            mirror.append((y, x + (2 * diff_x) - 1))\n\n        if x > IMAGE_APEX:\n            diff_x = diff(x, IMAGE_APEX)\n            mirror.append((y, x - (2 * diff_x) - 1))\n\n    return mirror + pixmap", "category": "Python"}, {"instruction": "def unpack(self, format_text):\n        \"\"\"Unpack bytes using struct modules format\n\n        :param format_text: struct's module format\n        :type  format_text: :class:`str`\n        :return data: result from :func:`struct.unpack_from`\n        :rtype: :class:`tuple`\n        \"\"\"\n", "input": "", "output": "        data = _unpack_from(format_text, self.data, self.offset)\n        self.offset += _calcsize(format_text)\n        return data", "category": "Python"}, {"instruction": "def u_probs(self):\n        \"\"\"Probability P(x_i==1|Non-match) as described in the FS framework.\"\"\"\n", "input": "", "output": "        log_u = self.kernel.feature_log_prob_[self._nonmatch_class_pos()]\n\n        return self._prob_inverse_transform(numpy.exp(log_u))", "category": "Python"}, {"instruction": "def _wrap_type_instantiation(self, type_cls):\n        \"\"\"Wrap the creation of the type so that we can provide\n        a null-stream to initialize it\"\"\"\n", "input": "", "output": "        def wrapper(*args, **kwargs):\n            # use args for struct arguments??\n            return type_cls(stream=self._null_stream)\n        return wrapper", "category": "Python"}, {"instruction": "def param_array(self):\n        \"\"\"\n        Array representing the parameters of this class.\n        There is only one copy of all parameters in memory, two during optimization.\n\n        !WARNING!: setting the parameter array MUST always be done in memory:\n        m.param_array[:] = m_copy.param_array\n        \"\"\"\n", "input": "", "output": "        if (self.__dict__.get('_param_array_', None) is None) or (self._param_array_.size != self.size):\n            self._param_array_ = np.empty(self.size, dtype=np.float64)\n        return self._param_array_", "category": "Python"}, {"instruction": "def write(self, bytestring):\n        '''Enqueue the given bytes to be written asychronously'''\n", "input": "", "output": "        with self._lock:\n            if self._closed:\n                raise IOError('Writer is closed')\n            self._byte_queue.put(bytestring)", "category": "Python"}, {"instruction": "def _dump_files_to_local_drive(bodies, theseUrls, log):\n    \"\"\"\n    *takes the files stored in memory and dumps them to the local drive*\n\n    ****Key Arguments:****\n      - ``bodies`` -- array of file data (currently stored in memory)\n      - ``theseUrls`` -- array of local files paths to dump the file data into\n      - ``log`` -- the logger\n\n    **Return:**\n      - ``None``\n    \"\"\"\n", "input": "", "output": "    j = 0\n    log.debug(\"attempting to write file data to local drive\")\n    log.debug('%s URLS = %s' % (len(theseUrls), str(theseUrls),))\n    for body in bodies:\n        try:\n            if theseUrls[j]:\n                with open(theseUrls[j], 'w') as f:\n                    f.write(body)\n                f.close()\n            j += 1\n        except Exception, e:\n            log.error(\n                \"could not write downloaded file to local drive - failed with this error %s: \" %\n                (str(e),))\n            return -1\n    return", "category": "Python"}, {"instruction": "def get_referenced_object(self):\n        \"\"\"\n        :rtype: core.BunqModel\n        :raise: BunqException\n        \"\"\"\n", "input": "", "output": "\n        if self._MonetaryAccountBank is not None:\n            return self._MonetaryAccountBank\n\n        if self._MonetaryAccountJoint is not None:\n            return self._MonetaryAccountJoint\n\n        if self._MonetaryAccountLight is not None:\n            return self._MonetaryAccountLight\n\n        if self._MonetaryAccountSavings is not None:\n            return self._MonetaryAccountSavings\n\n        raise exception.BunqException(self._ERROR_NULL_FIELDS)", "category": "Python"}, {"instruction": "def dhms(secs):\n    \"\"\"return days,hours,minutes and seconds\"\"\"\n", "input": "", "output": "    dhms = [0, 0, 0, 0]\n    dhms[0] = int(secs // 86400)\n    s = secs % 86400\n    dhms[1] = int(s // 3600)\n    s = secs % 3600\n    dhms[2] = int(s // 60)\n    s = secs % 60\n    dhms[3] = int(s+.5)\n    return dhms", "category": "Python"}, {"instruction": "def D(self, ID, asp):\n        \"\"\" Returns the dexter aspect of an object. \"\"\"\n", "input": "", "output": "        obj = self.chart.getObject(ID).copy()\n        obj.relocate(obj.lon - asp)\n        ID = 'D_%s_%s' % (ID, asp)\n        return self.G(ID, obj.lat, obj.lon)", "category": "Python"}, {"instruction": "def set_chat_title(self, *args, **kwargs):\n        \"\"\"See :func:`set_chat_title`\"\"\"\n", "input": "", "output": "        return set_chat_title(*args, **self._merge_overrides(**kwargs)).run()", "category": "Python"}, {"instruction": "def list_preferences_communication_channel_id(self, user_id, communication_channel_id):\r\n        \"\"\"\r\n        List preferences.\r\n\r\n        Fetch all preferences for the given communication channel\r\n        \"\"\"\n", "input": "", "output": "        path = {}\r\n        data = {}\r\n        params = {}\r\n\r\n        # REQUIRED - PATH - user_id\r\n        ", "category": "Python"}, {"instruction": "def get_assessments_offered(self):\n        \"\"\"Gets all ``AssessmentOffered`` elements.\n\n        In plenary mode, the returned list contains all known\n        assessments offered or an error results. Otherwise, the returned\n        list may contain only those assessments offered that are\n        accessible through this session.\n\n        return: (osid.assessment.AssessmentOfferedList) - a list of\n                ``AssessmentOffered`` elements\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure occurred\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.ResourceLookupSession.get_resources\n        # NOTE: This implementation currently ignores plenary view\n        collection = JSONClientValidated('assessment',\n                                         collection='AssessmentOffered',\n                                         runtime=self._runtime)\n        result = collection.find(self._view_filter()).sort('_id', DESCENDING)\n        return objects.AssessmentOfferedList(result, runtime=self._runtime, proxy=self._proxy)", "category": "Python"}, {"instruction": "def jsonify(self,\n                data: Any,\n                code: Union[int, Tuple[int, str, str]] = HTTPStatus.OK,\n                headers: Optional[Dict[str, str]] = None,\n                ):\n        \"\"\"\n        Convenience method to return json responses.\n\n        :param data: The python data to jsonify.\n        :param code: The HTTP status code to return.\n        :param headers: Any optional headers.\n        \"\"\"\n", "input": "", "output": "        return jsonify(data), code, headers or {}", "category": "Python"}, {"instruction": "def _create_user_profiles(self, profiles, file, valid_records,\n                              ip_user=False, year=None, week=None):\n        \"\"\"\n        Create user profiles with all the records visited or downloaded.\n\n        Returns: Dictionary with the user id and a record list.\n        {'2323': [1, 2, 4]}\n        \"\"\"\n", "input": "", "output": "        for record in file.get_records():\n            recid = record[2]\n            if not valid_records.get(recid, None):\n                # Record not valid\n                continue\n\n            if ip_user:\n                ip = record[4]\n                user_agent = record[5]\n                # Generate unique user id\n                user_id = \"{0}-{1}_{2}_{3}\".format(year, week, ip, user_agent)\n                try:\n                    uid = hashlib.md5(user_id.encode('utf-8')).hexdigest()\n                except UnicodeDecodeError:\n                    logger.info(\"UnicodeDecodeError {}\".format(user_id))\n            else:\n                uid = record[1]\n\n            profiles[uid].append(recid)\n\n        return profiles", "category": "Python"}, {"instruction": "def to_tokens(self, indices):\n        \"\"\"Converts token indices to tokens according to the vocabulary.\n\n\n        Parameters\n        ----------\n        indices : int or list of ints\n            A source token index or token indices to be converted.\n\n\n        Returns\n        -------\n        str or list of strs\n            A token or a list of tokens according to the vocabulary.\n        \"\"\"\n", "input": "", "output": "\n        to_reduce = False\n        if not isinstance(indices, (list, tuple)):\n            indices = [indices]\n            to_reduce = True\n\n        max_idx = len(self._idx_to_token) - 1\n\n        tokens = []\n        for idx in indices:\n            if not isinstance(idx, int) or idx > max_idx:\n                raise ValueError('Token index {} in the provided `indices` is invalid.'.format(idx))\n            else:\n                tokens.append(self._idx_to_token[idx])\n\n        return tokens[0] if to_reduce else tokens", "category": "Python"}, {"instruction": "def _Reg2Py(data, size, data_type):\n  \"\"\"Converts a Windows Registry value to the corresponding Python data type.\"\"\"\n", "input": "", "output": "  if data_type == winreg.REG_DWORD:\n    if size == 0:\n      return 0\n    # DWORD is an unsigned 32-bit integer, see:\n    # https://docs.microsoft.com/en-us/openspecs/windows_protocols/ms-dtyp/262627d8-3418-4627-9218-4ffe110850b2\n    return ctypes.cast(data, ctypes.POINTER(ctypes.c_uint32)).contents.value\n  elif data_type == winreg.REG_SZ or data_type == winreg.REG_EXPAND_SZ:\n    return ctypes.wstring_at(data, size // 2).rstrip(u\"\\x00\")\n  elif data_type == winreg.REG_MULTI_SZ:\n    return ctypes.wstring_at(data, size // 2).rstrip(u\"\\x00\").split(u\"\\x00\")\n  else:\n    if size == 0:\n      return None\n    return ctypes.string_at(data, size)", "category": "Python"}, {"instruction": "def add_supporting_publication(\n            self, evidence_line, publication, label=None, pub_type=None):\n        \"\"\"\n        <evidence> <evidence_has_supporting_reference> <source>\n        <source> <rdf:type> <type>\n        <source> <rdfs:label> \"label\"\n        :param evidence_line: str curie\n        :param publication: str curie\n        :param label: optional, str type as curie\n        :param type: optional, str type as curie\n        :return:\n        \"\"\"\n", "input": "", "output": "        self.graph.addTriple(\n            evidence_line, self.globaltt['evidence_has_supporting_reference'], publication)\n        self.model.addIndividualToGraph(publication, label, pub_type)\n        return", "category": "Python"}, {"instruction": "def generate(self, **kwargs):\n        \"\"\"\n        Generate some text from the database. By default only 70 words are\n        generated, but you can change this using keyword arguments.\n\n        Keyword arguments:\n\n            - ``wlen``: maximum length (words)\n            - ``words``: a list of words to use to begin the text with\n        \"\"\"\n", "input": "", "output": "        words = list(map(self._sanitize, kwargs.get('words', [])))\n        max_wlen = kwargs.get('wlen', 70)\n\n        wlen = len(words)\n\n        if wlen < 2:\n            if not self._db:\n                return ''\n\n            if wlen == 0:\n                words = sample(self._db.keys(), 1)[0].split(self._WSEP)\n            elif wlen == 1:\n                spl = [k for k in self._db.keys()\n                       if k.startswith(words[0]+self._WSEP)]\n                words.append(sample(spl, 1)[0].split(self._WSEP)[1])\n\n            wlen = 2\n\n        while wlen < max_wlen:\n            next_word = self._get(words[-2], words[-1])\n            if next_word is None:\n                break\n\n            words.append(next_word)\n            wlen += 1\n\n        return ' '.join(words)", "category": "Python"}, {"instruction": "def get_status_from_resource(self, response):\n        \"\"\"Process the latest status update retrieved from the same URL as\n        the previous request.\n\n        :param requests.Response response: latest REST call response.\n        :raises: BadResponse if status not 200 or 204.\n        \"\"\"\n", "input": "", "output": "        self._raise_if_bad_http_status_and_method(response)\n        if self._is_empty(response):\n            raise BadResponse('The response from long running operation '\n                              'does not contain a body.')\n\n        status = self._get_provisioning_state(response)\n        self.status = status or 'Succeeded'\n\n        self.parse_resource(response)", "category": "Python"}, {"instruction": "def getStat(cls, obj, name):\n    \"\"\"Gets the stat for the given object with the given name, or None if no such stat exists.\"\"\"\n", "input": "", "output": "    objClass = type(obj)\n    for theClass in objClass.__mro__:\n      if theClass == object:\n        break\n      for value in theClass.__dict__.values():\n        if isinstance(value, Stat) and value.getName() == name:\n          return value", "category": "Python"}, {"instruction": "def unset_key(dotenv_path, key_to_unset, quote_mode=\"always\"):\n    \"\"\"\n    Removes a given key from the given .env\n\n    If the .env path given doesn't exist, fails\n    If the given key doesn't exist in the .env, fails\n    \"\"\"\n", "input": "", "output": "    if not os.path.exists(dotenv_path):\n        warnings.warn(\"can't delete from %s - it doesn't exist.\" % dotenv_path)\n        return None, key_to_unset\n\n    removed = False\n    with rewrite(dotenv_path) as (source, dest):\n        for mapping in parse_stream(source):\n            if mapping.key == key_to_unset:\n                removed = True\n            else:\n                dest.write(mapping.original)\n\n    if not removed:\n        warnings.warn(\"key %s not removed from %s - key doesn't exist.\" % (key_to_unset, dotenv_path))\n        return None, key_to_unset\n\n    return removed, key_to_unset", "category": "Python"}, {"instruction": "def create_symlink_job(directory, checksums, filetype, symlink_path):\n    \"\"\"Create a symlink-creating DownloadJob for an already downloaded file.\"\"\"\n", "input": "", "output": "    pattern = NgdConfig.get_fileending(filetype)\n    filename, _ = get_name_and_checksum(checksums, pattern)\n    local_file = os.path.join(directory, filename)\n    full_symlink = os.path.join(symlink_path, filename)\n    return DownloadJob(None, local_file, None, full_symlink)", "category": "Python"}, {"instruction": "def friendly_name(self):\n        \"\"\"Get friendly name.\"\"\"\n", "input": "", "output": "        if len(self._client.get('config').get('name')):\n            return self._client.get('config').get('name')\n        return self._client.get('host').get('name')", "category": "Python"}, {"instruction": "def _get_all_ns_packages(self):\n        \"\"\"Return sorted list of all package namespaces\"\"\"\n", "input": "", "output": "        pkgs = self.distribution.namespace_packages or []\n        return sorted(flatten(map(self._pkg_names, pkgs)))", "category": "Python"}, {"instruction": "def get_last_live_chat(self):\n        \"\"\" Check if there is a live chat that ended in the last 3 days, and\n            return it. We will display a link to it on the articles page.\n        \"\"\"\n", "input": "", "output": "        now = datetime.now()\n\n        lcqs = self.get_query_set()\n        lcqs = lcqs.filter(\n            chat_ends_at__lte=now,\n            ).order_by('-chat_ends_at')\n        for itm in lcqs:\n            if itm.chat_ends_at + timedelta(days=3) > now:\n                return itm\n        return None", "category": "Python"}, {"instruction": "def add_common_files_to_file_list(self):\n        '''\n            The (several thousands) common-disease files from the repo tarball\n            are added to the files object.\n            try adding the 'common-disease-mondo' files as well?\n\n        '''\n", "input": "", "output": "        repo_dir = '/'.join((self.rawdir, 'git'))\n        common_disease_dir = '/'.join((\n            repo_dir,\n            'monarch-initiative-hpo-annotation-*', 'common-diseases-mondo/*.tab'))\n\n        # add the files to the self.files object\n        filelist = glob.glob(common_disease_dir)\n        fcount = 0\n        for small_file in filelist:\n            if small_file[-4:] == '.tab':\n                fcount += 1\n                self.files[\n                    'common' + str(fcount).zfill(7)] = {\n                        'file': '/'.join((common_disease_dir, small_file)),\n                    }\n        LOG.info(\"Found %d common disease files\", fcount)\n\n        return", "category": "Python"}, {"instruction": "def _from_string(cls, serialized):\n        \"\"\"\n        Return an instance of `cls` parsed from its `serialized` form.\n\n        Args:\n            cls: The :class:`OpaqueKey` subclass.\n            serialized (unicode): A serialized :class:`OpaqueKey`, with namespace already removed.\n\n        Raises:\n            InvalidKeyError: Should be raised if `serialized` is not a valid serialized key\n                understood by `cls`.\n        \"\"\"\n", "input": "", "output": "        if ':' not in serialized:\n            raise InvalidKeyError(\n                \"BlockTypeKeyV1 keys must contain ':' separating the block family from the block_type.\", serialized)\n        family, __, block_type = serialized.partition(':')\n        return cls(family, block_type)", "category": "Python"}, {"instruction": "def delete(self, to_delete):\n        \"\"\" Purge one or more items from the relevant caches \"\"\"\n", "input": "", "output": "        if not isinstance(to_delete, list):\n            to_delete = [to_delete]\n        for zenpy_object in to_delete:\n            object_type = get_object_type(zenpy_object)\n            object_cache = self.mapping.get(object_type, None)\n            if object_cache:\n                removed_object = object_cache.pop(zenpy_object.id, None)\n                if removed_object:\n                    log.debug(\"Cache RM: [%s %s]\" % (object_type.capitalize(), zenpy_object.id))", "category": "Python"}, {"instruction": "def create_ssh_config(remote_user='root', name='Auto Generated SSH Key',\n                      file_name='fabricbolt_private.key', email='deployments@fabricbolt.io', public_key_text=None,\n                      private_key_text=None):\n    \"\"\"Create SSH Key\"\"\"\n", "input": "", "output": "\n    if not private_key_text and not public_key_text:\n        key = RSA.generate(2048)\n        pubkey = key.publickey()\n\n        private_key_text = key.exportKey('PEM')\n        public_key_text = pubkey.exportKey('OpenSSH')\n\n    ssh_config = models.SSHConfig()\n    ssh_config.name = name\n    ssh_config.private_key_file.save(file_name, ContentFile(private_key_text))\n    ssh_config.public_key = '{} {}'.format(public_key_text, email)\n    ssh_config.remote_user = remote_user\n    ssh_config.save()\n\n    return ssh_config", "category": "Python"}, {"instruction": "def check_hook_mechanism_is_intact(module):\n    \"\"\"Check if the hook configuration is absent or has both register AND deregister.\n\n    :param module:\n    :return: True if valid plugin / module.\n    \"\"\"\n", "input": "", "output": "    result = True\n    if check_register_present(module):\n        result = not result\n    if check_deregister_present(module):\n        result = not result\n    return result", "category": "Python"}, {"instruction": "def create(backbone: ModelFactory, vmin: float, vmax: float, atoms: int,\n           input_block: typing.Optional[ModelFactory]=None):\n    \"\"\" Vel factory function \"\"\"\n", "input": "", "output": "    if input_block is None:\n        input_block = IdentityFactory()\n\n    return QDistributionalModelFactory(\n        input_block=input_block, backbone=backbone,\n        vmin=vmin,\n        vmax=vmax,\n        atoms=atoms\n    )", "category": "Python"}, {"instruction": "def authorize_url(self, client_id, redirect_uri, scope, state=None):\n        \"\"\"Get the authorization URL for your application, given the application's\n        client_id, redirect_uri, scope, and optional state data.\"\"\"\n", "input": "", "output": "        params = [\n            ('client_id', client_id),\n            ('redirect_uri', redirect_uri),\n            ('scope', scope)\n        ]\n        if state:\n            params.append(('state', state))\n        return \"%s?%s\" % (CreateSend.oauth_uri, urlencode(params))", "category": "Python"}, {"instruction": "def vector_str(p, decimal_places=2, print_zero=True):\n    '''Pretty-print the vector values.'''\n", "input": "", "output": "    style = '{0:.' + str(decimal_places) + 'f}'\n    return '[{0}]'.format(\", \".join([' ' if not print_zero and a == 0 else style.format(a) for a in p]))", "category": "Python"}, {"instruction": "def delist(values):\n    \"\"\"Reduce lists of zero or one elements to individual values.\"\"\"\n", "input": "", "output": "    assert isinstance(values, list)\n\n    if not values:\n        return None\n    elif len(values) == 1:\n        return values[0]\n\n    return values", "category": "Python"}, {"instruction": "def _env_is_exposed(env):\n    '''\n    Check if an environment is exposed by comparing it against a whitelist and\n    blacklist.\n    '''\n", "input": "", "output": "    if __opts__['svnfs_env_whitelist']:\n        salt.utils.versions.warn_until(\n            'Neon',\n            'The svnfs_env_whitelist config option has been renamed to '\n            'svnfs_saltenv_whitelist. Please update your configuration.'\n        )\n        whitelist = __opts__['svnfs_env_whitelist']\n    else:\n        whitelist = __opts__['svnfs_saltenv_whitelist']\n\n    if __opts__['svnfs_env_blacklist']:\n        salt.utils.versions.warn_until(\n            'Neon',\n            'The svnfs_env_blacklist config option has been renamed to '\n            'svnfs_saltenv_blacklist. Please update your configuration.'\n        )\n        blacklist = __opts__['svnfs_env_blacklist']\n    else:\n        blacklist = __opts__['svnfs_saltenv_blacklist']\n\n    return salt.utils.stringutils.check_whitelist_blacklist(\n        env,\n        whitelist=whitelist,\n        blacklist=blacklist,\n    )", "category": "Python"}, {"instruction": "def _transformBy(self, matrix, **kwargs):\n        \"\"\"\n        Subclasses may override this method.\n        \"\"\"\n", "input": "", "output": "        for point in self.points:\n            point.transformBy(matrix)", "category": "Python"}, {"instruction": "def _finalize(self):\n        \"\"\"Reset the status and tell the database to finalize the traces.\"\"\"\n", "input": "", "output": "        if self.status in ['running', 'halt']:\n            if self.verbose > 0:\n                print_('\\nSampling finished normally.')\n            self.status = 'ready'\n\n        self.save_state()\n        self.db._finalize()", "category": "Python"}, {"instruction": "def show_observee(self, user_id, observee_id):\r\n        \"\"\"\r\n        Show an observee.\r\n\r\n        Gets information about an observed user.\r\n        \r\n        *Note:* all users are allowed to view their own observees.\r\n        \"\"\"\n", "input": "", "output": "        path = {}\r\n        data = {}\r\n        params = {}\r\n\r\n        # REQUIRED - PATH - user_id\r\n        ", "category": "Python"}, {"instruction": "def MapFile(self, key_path_prefix, registry_file):\n    \"\"\"Maps the Windows Registry file to a specific key path prefix.\n\n    Args:\n      key_path_prefix (str): key path prefix.\n      registry_file (WinRegistryFile): Windows Registry file.\n    \"\"\"\n", "input": "", "output": "    self._registry_files[key_path_prefix.upper()] = registry_file\n    registry_file.SetKeyPathPrefix(key_path_prefix)", "category": "Python"}, {"instruction": "def parse(self):\n        \"\"\"\n        Reads all lines from the current data source and yields each FileResult objects\n        \"\"\"\n", "input": "", "output": "\n        if self.data is None:\n            raise ValueError('No input data provided, unable to parse')\n\n        for line in self.data:\n            parts = line.strip().split()\n            try:\n                path = parts[0]\n                code = parts[1]\n                path, line, char = path.split(':')[:3]\n\n                if not re.match(POSITION, line):\n                    continue\n                if not re.match(POSITION, char):\n                    continue\n                if not re.match(ERROR_CODE, code):\n                    continue\n                if not re.match(FILEPATH, path):\n                    continue\n\n            # For parts mismatch\n            except IndexError:\n                continue\n            # For unpack mismatch\n            except ValueError:\n                continue\n\n            yield path, code, line, char, ' '.join(parts[2:])", "category": "Python"}, {"instruction": "def timestamp_with_timezone(dt=None):\n    \"\"\"\n    Return a timestamp with a timezone for the configured locale.  If all else\n    fails, consider localtime to be UTC.\n    \"\"\"\n", "input": "", "output": "    dt = dt or datetime.now()\n    if timezone is None:\n        return dt.strftime('%Y-%m-%d %H:%M%z')\n    if not dt.tzinfo:\n        tz = timezone.get_current_timezone()\n        if not tz:\n            tz = timezone.utc\n        dt = dt.replace(tzinfo=timezone.get_current_timezone())\n    return dt.strftime(\"%Y-%m-%d %H:%M%z\")", "category": "Python"}, {"instruction": "def show_type(self, call_id, payload):\n        \"\"\"Show type of a variable or scala type.\"\"\"\n", "input": "", "output": "        if self.full_types_enabled:\n            tpe = payload['fullName']\n        else:\n            tpe = payload['name']\n\n        self.log.info('Displayed type %s', tpe)\n        self.editor.raw_message(tpe)", "category": "Python"}, {"instruction": "def round(value, decimal=None, digits=None, places=None):\n    \"\"\"\n    :param value:  THE VALUE TO ROUND\n    :param decimal: NUMBER OF DECIMAL PLACES TO ROUND (NEGATIVE IS LEFT-OF-DECIMAL)\n    :param digits: ROUND TO SIGNIFICANT NUMBER OF digits\n    :param places: SAME AS digits\n    :return:\n    \"\"\"\n", "input": "", "output": "    value = float(value)\n    if value == 0.0:\n        return \"0\"\n\n    digits = coalesce(digits, places)\n    if digits != None:\n        left_of_decimal = int(math.ceil(math.log10(abs(value))))\n        decimal = digits - left_of_decimal\n\n    right_of_decimal = max(decimal, 0)\n    format = \"{:.\" + text_type(right_of_decimal) + \"f}\"\n    return format.format(_round(value, decimal))", "category": "Python"}, {"instruction": "def register_game(game_name, game_mode=\"NoFrameskip-v4\"):\n  \"\"\"Create and register problems for the game.\n\n  Args:\n    game_name: str, one of the games in ATARI_GAMES, e.g. \"bank_heist\".\n    game_mode: the frame skip and sticky keys config.\n\n  Raises:\n    ValueError: if game_name or game_mode are wrong.\n  \"\"\"\n", "input": "", "output": "  if game_name not in ATARI_GAMES:\n    raise ValueError(\"Game %s not in ATARI_GAMES\" % game_name)\n  if game_mode not in ATARI_GAME_MODES:\n    raise ValueError(\"Unknown ATARI game mode: %s.\" % game_mode)\n  camel_game_name = misc_utils.snakecase_to_camelcase(game_name) + game_mode\n  # Create and register the Problem\n  cls = type(\"Gym%sRandom\" % camel_game_name,\n             (T2TGymEnv,), {\"base_env_name\": camel_game_name})\n  registry.register_problem(cls)", "category": "Python"}, {"instruction": "def multi_pop(d, *args):\n    \"\"\" pops multiple keys off a dict like object \"\"\"\n", "input": "", "output": "    retval = {}\n    for key in args:\n        if key in d:\n            retval[key] = d.pop(key)\n    return retval", "category": "Python"}, {"instruction": "def add_outputs(self, **kwargs):\n        \"\"\"Add workflow outputs.\n\n        The output type is added automatically, based on the steps in the steps\n        library.\n\n        Args:\n            kwargs (dict): A dict containing ``name=source name`` pairs.\n                ``name`` is the name of the workflow output (e.g.,\n                ``txt_files``) and source name is the name of the step that\n                produced this output plus the output name (e.g.,\n                ``saf-to-txt/out_files``).\n        \"\"\"\n", "input": "", "output": "        self._closed()\n\n        for name, source_name in kwargs.items():\n            obj = {}\n            obj['outputSource'] = source_name\n            obj['type'] = self.step_output_types[source_name]\n            self.wf_outputs[name] = obj", "category": "Python"}, {"instruction": "def load(language_dir, filename, encoding):\r\n  ''' Open and return the supplied json file '''\n", "input": "", "output": "  global _DICTIONARY\r\n  try:\r\n    json_file = filename + '.json'\r\n    with io.open(os.path.join(language_dir, json_file), 'r', encoding=encoding) as f:\r\n      _DICTIONARY = json.load(f)\r\n  except IOError:\r\n    raise IOError('{0} Language file not found at location {1}. '\r\n                  'Make sure that your translation file is in the '\r\n                  'listed language directory'.format(filename.title(), language_dir))", "category": "Python"}, {"instruction": "def __update_rating(uid, rating):\n        '''\n        Update the rating for post.\n        '''\n", "input": "", "output": "        entry = TabPost.update(\n            rating=rating\n        ).where(TabPost.uid == uid)\n        entry.execute()", "category": "Python"}, {"instruction": "def lock(self, id=str(uuid.uuid4()), ttl=DEFAULT_TIMEOUT):\n        \"\"\"Create a Lock object given an ID and timeout\n\n        :param id: ID for the lock, creates a new uuid if not provided\n        :param ttl: timeout\n        :return: Lock object\n        \"\"\"\n", "input": "", "output": "        return Lock(id, ttl=ttl, client=self)", "category": "Python"}, {"instruction": "def run_without_time_limit(self, cmd):\n    \"\"\"Runs docker command without time limit.\n\n    Args:\n      cmd: list with the command line arguments which are passed to docker\n        binary\n\n    Returns:\n      how long it took to run submission in seconds\n\n    Raises:\n      WorkerError: if error occurred during execution of the submission\n    \"\"\"\n", "input": "", "output": "    cmd = [DOCKER_BINARY, 'run', DOCKER_NVIDIA_RUNTIME] + cmd\n    logging.info('Docker command: %s', ' '.join(cmd))\n    start_time = time.time()\n    retval = subprocess.call(cmd)\n    elapsed_time_sec = int(time.time() - start_time)\n    logging.info('Elapsed time of attack: %d', elapsed_time_sec)\n    logging.info('Docker retval: %d', retval)\n    if retval != 0:\n      logging.warning('Docker returned non-zero retval: %d', retval)\n      raise WorkerError('Docker returned non-zero retval ' + str(retval))\n    return elapsed_time_sec", "category": "Python"}, {"instruction": "def checkASN(filename):\n    \"\"\"\n    Determine if the filename provided to the function belongs to\n    an association.\n\n    Parameters\n    ----------\n    filename: string\n\n    Returns\n    -------\n    validASN  : boolean value\n\n    \"\"\"\n", "input": "", "output": "    # Extract the file extn type:\n    extnType = filename[filename.rfind('_')+1:filename.rfind('.')]\n\n    # Determine if this extn name is valid for an assocation file\n    if isValidAssocExtn(extnType):\n        return True\n    else:\n        return False", "category": "Python"}, {"instruction": "def resize(self, dims):\n        \"\"\"Resize our drawing area to encompass a space defined by the\n        given dimensions.\n        \"\"\"\n", "input": "", "output": "        width, height = dims[:2]\n        self.dims = (width, height)\n        self.logger.debug(\"renderer reconfigured to %dx%d\" % (\n            width, height))\n        # create agg surface the size of the window\n        self.surface = agg.Draw(self.rgb_order, self.dims, 'black')", "category": "Python"}, {"instruction": "def add_label(self, name, addr):\n        \"\"\"\n        Add a new label to the symbol manager.\n\n        :param str name: Name of the label.\n        :param int addr: Address of the label.\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        # set the label\n        self._symbolization_needed = True\n\n        self.symbol_manager.new_label(addr, name=name, force=True)", "category": "Python"}, {"instruction": "def get_line(thing):\n    \"\"\"\n    Get the line number for something.\n    Parameters\n    ----------\n    thing : function, class, module\n\n    Returns\n    -------\n    int\n        Line number in the source file\n    \"\"\"\n", "input": "", "output": "    try:\n        return inspect.getsourcelines(thing)[1]\n    except TypeError:\n        # Might be a property\n        return inspect.getsourcelines(thing.fget)[1]\n    except Exception as e:\n        # print(thing)\n        raise e", "category": "Python"}, {"instruction": "def _parse_json(s):\n    '''\n    Parse json string into JsonDict.\n\n    >>> r = _parse_json(r'{\"name\":\"Michael\",\"score\":95}')\n    >>> r.name\n    u'Michael'\n    >>> r['score']\n    95\n    '''\n", "input": "", "output": "    return json.loads(s, object_hook=lambda pairs: JsonDict(pairs.iteritems()))", "category": "Python"}, {"instruction": "def update(self, data, decayFactor, timeUnit):\n        \"\"\"Update the centroids, according to data\n\n        :param data:\n          RDD with new data for the model update.\n        :param decayFactor:\n          Forgetfulness of the previous centroids.\n        :param timeUnit:\n          Can be \"batches\" or \"points\". If points, then the decay factor\n          is raised to the power of number of new points and if batches,\n          then decay factor will be used as is.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(data, RDD):\n            raise TypeError(\"Data should be of an RDD, got %s.\" % type(data))\n        data = data.map(_convert_to_vector)\n        decayFactor = float(decayFactor)\n        if timeUnit not in [\"batches\", \"points\"]:\n            raise ValueError(\n                \"timeUnit should be 'batches' or 'points', got %s.\" % timeUnit)\n        vectorCenters = [_convert_to_vector(center) for center in self.centers]\n        updatedModel = callMLlibFunc(\n            \"updateStreamingKMeansModel\", vectorCenters, self._clusterWeights,\n            data, decayFactor, timeUnit)\n        self.centers = array(updatedModel[0])\n        self._clusterWeights = list(updatedModel[1])\n        return self", "category": "Python"}, {"instruction": "def numpy_bins(self) -> List[np.ndarray]:\n        \"\"\"Numpy-like bins (if available).\"\"\"\n", "input": "", "output": "        return [binning.numpy_bins for binning in self._binnings]", "category": "Python"}, {"instruction": "def set_asset_dir(self):\n        \"\"\"Returns the ASSET_DIR environment variable\n\n        This method gets the ASSET_DIR environment variable for the\n        current asset install. It returns either the string value if\n        set or None if it is not set.\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        log = logging.getLogger(self.cls_logger + '.get_asset_dir')\n        try:\n            self.asset_dir = os.environ['ASSET_DIR']\n        except KeyError:\n            log.warn('Environment variable ASSET_DIR is not set!')\n        else:\n            log.info('Found environment variable ASSET_DIR: {a}'.format(a=self.asset_dir))", "category": "Python"}, {"instruction": "def set_l2cap_options (sock, options):\n    \"\"\"set_l2cap_options (sock, options)\n\n    Sets L2CAP options for the specified L2CAP socket.\n    The option list must be in the same format supplied by\n    get_l2cap_options().\n    \"\"\"\n", "input": "", "output": "    # TODO this should be in the C module, because it depends\n    # directly on struct l2cap_options layout.\n    s = struct.pack (\"HHHBBBH\", *options)\n    sock.setsockopt (SOL_L2CAP, L2CAP_OPTIONS, s)", "category": "Python"}, {"instruction": "def substitute_namespace_into_graph(self, graph):\n        \"\"\"\n        Creates a graph from the local namespace of the code (to be used after the execution of the code)\n\n        :param graph: The graph to use as a recipient of the namespace\n        :return: the updated graph\n        \"\"\"\n", "input": "", "output": "        for key, value in self.namespace.items():\n            try:\n                nodes = graph.vs.select(name=key)\n                for node in nodes:\n                    for k, v in value.items():\n                        node[k] = v\n            except:\n                pass\n            try:\n                nodes = graph.es.select(name=key)\n                for node in nodes:\n                    for k, v in value.items():\n                        node[k] = v\n            except:\n                pass\n        return graph", "category": "Python"}, {"instruction": "def get_filtered_keys(self, suffix, *args, **kwargs):\n        \"\"\"Returns the index keys to be used by the collection for the given args\n\n        For the parameters, see BaseIndex.get_filtered_keys\n\n        \"\"\"\n", "input": "", "output": "\n        args = self.prepare_args(args, transform=False)\n\n        for index in self._indexes:\n            if index.can_handle_suffix(suffix):\n                return index.get_filtered_keys(suffix, *args, **kwargs)", "category": "Python"}, {"instruction": "def set_custom_interpreters_list(self, value):\r\n        \"\"\"Update the list of interpreters used and the current one.\"\"\"\n", "input": "", "output": "        custom_list = self.get_option('custom_interpreters_list')\r\n        if value not in custom_list and value != get_python_executable():\r\n            custom_list.append(value)\r\n            self.set_option('custom_interpreters_list', custom_list)", "category": "Python"}, {"instruction": "async def download_file(self, Bucket, Key, Filename, ExtraArgs=None, Callback=None, Config=None):\n    \"\"\"Download an S3 object to a file.\n\n    Usage::\n\n        import boto3\n        s3 = boto3.resource('s3')\n        s3.meta.client.download_file('mybucket', 'hello.txt', '/tmp/hello.txt')\n\n    Similar behavior as S3Transfer's download_file() method,\n    except that parameters are capitalized.\n    \"\"\"\n", "input": "", "output": "    with open(Filename, 'wb') as open_file:\n        await download_fileobj(self, Bucket, Key, open_file, ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)", "category": "Python"}, {"instruction": "def SearchFetchable(session=None, **kwargs):\n    \"\"\"Search okcupid.com with the given parameters. Parameters are\n    registered to this function through\n    :meth:`~okcupyd.filter.Filters.register_filter_builder` of\n    :data:`~okcupyd.json_search.search_filters`.\n\n    :returns: A :class:`~okcupyd.util.fetchable.Fetchable` of\n              :class:`~okcupyd.profile.Profile` instances.\n\n    :param session: A logged in session.\n    :type session: :class:`~okcupyd.session.Session`\n    \"\"\"\n", "input": "", "output": "    session = session or Session.login()\n    return util.Fetchable(\n        SearchManager(\n            SearchJSONFetcher(session, **kwargs),\n            ProfileBuilder(session)\n        )\n    )", "category": "Python"}, {"instruction": "def clean_project(self, app_name=None, delete_all=False):\n        \"\"\"\n        Delete objects in current project in OpenShift cluster. If both parameters are passed,\n        delete all objects in project.\n        :param app_name: str, name of app\n        :param delete_all: bool, if true delete all objects in current project\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        if not app_name and not delete_all:\n            ConuException(\"You need to specify either app_name or set delete_all=True\")\n\n        if delete_all:\n            args = [\"--all\"]\n            logger.info('Deleting all objects in current project')\n        else:\n            args = \"-l app=%s\" % app_name\n            logger.info('Deleting all objects with label app=%s', app_name)\n\n        try:\n            o = run_cmd(self._oc_command([\"delete\", \"all\", args]),\n                        return_output=True)\n            o_lines = o.split('\\n')\n            for line in o_lines:\n                logger.info(line)\n        except subprocess.CalledProcessError as ex:\n            raise ConuException(\"Cleanup failed because of exception: %s\" % ex)", "category": "Python"}, {"instruction": "def writelines(self, lines, level=0):\n        \"\"\"Write multiple messages.\"\"\"\n", "input": "", "output": "        for line in lines:\n            for line in line.rstrip(u'\\n').split(u'\\n'):\n                self.write(line.rstrip(u'\\n'), level=level)", "category": "Python"}, {"instruction": "def merge_result(res):\n    \"\"\"\n    Merge all items in `res` into a list.\n\n    This command is used when sending a command to multiple nodes\n    and they result from each node should be merged into a single list.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(res, dict):\n        raise ValueError('Value should be of dict type')\n\n    result = set([])\n\n    for _, v in res.items():\n        for value in v:\n            result.add(value)\n\n    return list(result)", "category": "Python"}, {"instruction": "def return_values(self):\n        \"\"\" Guess what api we are using and return as public api does.\n        Private has {'id':'key', 'value':'keyvalue'} format, public has {'key':'keyvalue'}\n        \"\"\"\n", "input": "", "output": "\n        j = self.json()\n        #TODO: FIXME: get rid of old API when its support will be removed\n        public_api_value = j.get('returnValues')\n        old_private_value = j.get('endpoints')\n        new_private_value = self.__collect_interfaces_return(j.get('interfaces', {}))\n\n        retvals = new_private_value or old_private_value or public_api_value or []\n        # TODO: Public api hack.\n        if self._router.public_api_in_use:\n            return retvals\n        return self.__parse(retvals)", "category": "Python"}, {"instruction": "def delete(ctx, slot, force):\n    \"\"\"\n    Deletes the configuration of a slot.\n    \"\"\"\n", "input": "", "output": "    controller = ctx.obj['controller']\n    if not force and not controller.slot_status[slot - 1]:\n        ctx.fail('Not possible to delete an empty slot.')\n    force or click.confirm(\n        'Do you really want to delete'\n        ' the configuration of slot {}?'.format(slot), abort=True, err=True)\n    click.echo('Deleting the configuration of slot {}...'.format(slot))\n    try:\n        controller.zap_slot(slot)\n    except YkpersError as e:\n        _failed_to_write_msg(ctx, e)", "category": "Python"}, {"instruction": "def validate_number_attribute(\n        fully_qualified_name: str,\n        spec: Dict[str, Any],\n        attribute: str,\n        value_type: Union[Type[int], Type[float]] = int,\n        minimum: Optional[Union[int, float]] = None,\n        maximum: Optional[Union[int, float]] = None) -> Optional[InvalidNumberError]:\n    \"\"\" Validates to ensure that the value is a number of the specified type, and lies with the specified range \"\"\"\n", "input": "", "output": "\n    if attribute not in spec:\n        return\n\n    try:\n        value = value_type(spec[attribute])\n        if (minimum is not None and value < minimum) or (maximum is not None and value > maximum):\n            raise None\n    except:\n        return InvalidNumberError(fully_qualified_name, spec, attribute, value_type, minimum,\n                                  maximum)", "category": "Python"}, {"instruction": "def set_model_config(model_name, config, replace=False):\n    \"\"\"\n    This function should be only used in initialization phrase\n    :param model_name: model name it's should be string\n    :param config: config should be dict. e.g.\n        {'__mapping_only__', '__tablename__', '__ext_model__'}\n    :param replace: if True, then replace original config, False will update\n    \"\"\"\n", "input": "", "output": "    assert isinstance(model_name, str)\n    assert isinstance(config, dict)\n    \n    d = __models__.setdefault(model_name, {})\n    if replace:\n        d['config'] = config\n    else:\n        c = d.setdefault('config', {})\n        c.update(config)", "category": "Python"}, {"instruction": "def split_linear_constraints(A, l, u):\n    \"\"\" Returns the linear equality and inequality constraints.\n    \"\"\"\n", "input": "", "output": "    ieq = []\n    igt = []\n    ilt = []\n    ibx = []\n    for i in range(len(l)):\n        if abs(u[i] - l[i]) <= EPS:\n            ieq.append(i)\n        elif (u[i] > 1e10) and (l[i] > -1e10):\n            igt.append(i)\n        elif (l[i] <= -1e10) and (u[i] < 1e10):\n            ilt.append(i)\n        elif (abs(u[i] - l[i]) > EPS) and (u[i] < 1e10) and (l[i] > -1e10):\n            ibx.append(i)\n        else:\n            raise ValueError\n\n    Ae = A[ieq, :]\n    Ai = sparse([A[ilt, :], -A[igt, :], A[ibx, :], -A[ibx, :]])\n    be = u[ieq, :]\n    bi = matrix([u[ilt], -l[igt], u[ibx], -l[ibx]])\n\n    return Ae, be, Ai, bi", "category": "Python"}, {"instruction": "def cprint_map(text, cmap, **kwargs):\n    \"\"\"\n    Print colorize text.\n    cmap is a dict mapping keys to color options.\n    kwargs are passed to print function\n\n    Example:\n        cprint_map(\"Hello world\", {\"Hello\": \"red\"})\n    \"\"\"\n", "input": "", "output": "    try:\n        print(colored_map(text, cmap), **kwargs)\n    except TypeError:\n        # flush is not supported by py2.7\n        kwargs.pop(\"flush\", None)\n        print(colored_map(text, cmap), **kwargs)", "category": "Python"}, {"instruction": "def hvals(self, key, *, encoding=_NOTSET):\n        \"\"\"Get all the values in a hash.\"\"\"\n", "input": "", "output": "        return self.execute(b'HVALS', key, encoding=encoding)", "category": "Python"}, {"instruction": "def set_bind(self):\n        \"\"\"\n        Sets key bindings -- we need this more than once\n        \"\"\"\n", "input": "", "output": "        IntegerEntry.set_bind(self)\n        self.bind('<Next>', lambda e: self.set(0))", "category": "Python"}, {"instruction": "def setdict(self, D):\n        \"\"\"Set dictionary array.\"\"\"\n", "input": "", "output": "\n        self.D = np.asarray(D, dtype=self.dtype)\n        # Factorise dictionary for efficient solves\n        self.lu, self.piv = sl.cho_factor(self.D, 1.0)\n        self.lu = np.asarray(self.lu, dtype=self.dtype)", "category": "Python"}, {"instruction": "def _tokenize(cls, sentence):\n        \"\"\"\n        Split a sentence while preserving tags.\n        \"\"\"\n", "input": "", "output": "        while True:\n            match = cls._regex_tag.search(sentence)\n            if not match:\n                yield from cls._split(sentence)\n                return\n            chunk = sentence[:match.start()]\n            yield from cls._split(chunk)\n            tag = match.group(0)\n            yield tag\n            sentence = sentence[(len(chunk) + len(tag)):]", "category": "Python"}, {"instruction": "def get_metadata(realizations, kind):\n    \"\"\"\n    :param list realizations:\n        realization objects\n    :param str kind:\n        kind of data, i.e. a key in the datastore\n    :returns:\n        a dictionary with smlt_path, gsimlt_path, statistics, quantile_value\n    \"\"\"\n", "input": "", "output": "    metadata = {}\n    if kind.startswith('rlz-'):\n        rlz = realizations[int(kind[4:])]\n        metadata['smlt_path'] = '_'.join(rlz.sm_lt_path)\n        metadata['gsimlt_path'] = rlz.gsim_rlz.uid\n    elif kind.startswith('quantile-'):\n        metadata['statistics'] = 'quantile'\n        metadata['quantile_value'] = float(kind[9:])\n    elif kind == 'mean':\n        metadata['statistics'] = 'mean'\n    elif kind == 'max':\n        metadata['statistics'] = 'max'\n    elif kind == 'std':\n        metadata['statistics'] = 'std'\n    return metadata", "category": "Python"}, {"instruction": "def remove_entity_headers(headers, allowed=(\"expires\", \"content-location\")):\n    \"\"\"Remove all entity headers from a list or :class:`Headers` object.  This\n    operation works in-place.  `Expires` and `Content-Location` headers are\n    by default not removed.  The reason for this is :rfc:`2616` section\n    10.3.5 which specifies some entity headers that should be sent.\n\n    .. versionchanged:: 0.5\n       added `allowed` parameter.\n\n    :param headers: a list or :class:`Headers` object.\n    :param allowed: a list of headers that should still be allowed even though\n                    they are entity headers.\n    \"\"\"\n", "input": "", "output": "    allowed = set(x.lower() for x in allowed)\n    headers[:] = [\n        (key, value)\n        for key, value in headers\n        if not is_entity_header(key) or key.lower() in allowed\n    ]", "category": "Python"}, {"instruction": "def uninstall(self):\n        \"\"\"Uninstall the environment and links.\"\"\"\n", "input": "", "output": "        if path.isdir(self.env_bin):\n            self.remove_links()\n        if path.isdir(self.env):\n            print_pretty(\"<FG_BLUE>Removing env {}...<END>\".format(self.env))\n            shutil.rmtree(self.env)", "category": "Python"}, {"instruction": "def handle_database_error(cls, session, exception):\n        \"\"\"Rollback changes made and handle any type of error raised by the DBMS.\"\"\"\n", "input": "", "output": "\n        session.rollback()\n\n        if isinstance(exception, IntegrityError):\n            cls.handle_integrity_error(exception)\n        elif isinstance(exception, FlushError):\n            cls.handle_flush_error(exception)\n        else:\n            raise exception", "category": "Python"}, {"instruction": "def _step(self):\n        \"\"\"Private method do not call it directly or override it.\"\"\"\n", "input": "", "output": "        try:\n            new_value = self._device.get(self._channel)\n            if new_value != self._value:\n                self._callback(new_value)\n                self._value = new_value\n            time.sleep(self._polling_time)\n        except:\n            self.spine.log.exception(\"_PollingThread\")", "category": "Python"}, {"instruction": "def stop(self):\n        '''\n        stop the thread, return after thread stopped\n        '''\n", "input": "", "output": "        self._stop_event.set()\n        if self._func_stop_event is not None:\n            self._func_stop_event.set()\n        self.join(timeout=1)\n        if self.isAlive():\n            print('Failed to stop thread')", "category": "Python"}, {"instruction": "def findall(dir=os.curdir):\n    \"\"\"\n    Find all files under 'dir' and return the list of full filenames.\n    Unless dir is '.', return full filenames with dir prepended.\n    \"\"\"\n", "input": "", "output": "    files = _find_all_simple(dir)\n    if dir == os.curdir:\n        make_rel = functools.partial(os.path.relpath, start=dir)\n        files = map(make_rel, files)\n    return list(files)", "category": "Python"}, {"instruction": "def _call(self, x):\n        \"\"\"Return ``self(x)``.\"\"\"\n", "input": "", "output": "        result = 0\n        for i in range(0, self.domain.size - 1):\n            result += (self.scale * (x[i + 1] - x[i] ** 2) ** 2 +\n                       (x[i] - 1) ** 2)\n\n        return result", "category": "Python"}, {"instruction": "def result(self, *args, **kwargs):\n        \"\"\"\n        Construye la consulta SQL\n        \"\"\"\n", "input": "", "output": "        prettify = kwargs.get('pretty', False)\n\n        sql = 'UPDATE %s' % self._class\n        \n        if prettify:\n            sql += '\\n'\n        else:\n            sql += ' '\n        \n        if self.data:\n            sql += 'MERGE ' + json.dumps(self.data)\n            if prettify:\n                sql += '\\n'\n            else:\n                sql += ' '\n\n        if self.where_criteria.size() > 0:\n            sql +=  'WHERE '\n            sql +=  self.where_criteria.result()\n            if prettify:\n                sql += '\\n'\n            else:\n                sql += ' '\n        \n        return sql", "category": "Python"}, {"instruction": "def cee_map_precedence(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        cee_map = ET.SubElement(config, \"cee-map\", xmlns=\"urn:brocade.com:mgmt:brocade-cee-map\")\n        name_key = ET.SubElement(cee_map, \"name\")\n        name_key.text = kwargs.pop('name')\n        precedence = ET.SubElement(cee_map, \"precedence\")\n        precedence.text = kwargs.pop('precedence')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def load_scene(self, scene_path, start=False):\n        \"\"\" Loads a scene on the V-REP server.\n\n        :param str scene_path: path to a V-REP scene file\n        :param bool start: whether to directly start the simulation after loading the scene\n\n        .. note:: It is assumed that the scene file is always available on the server side.\n\n        \"\"\"\n", "input": "", "output": "        self.stop_simulation()\n\n        if not os.path.exists(scene_path):\n            raise IOError(\"No such file or directory: '{}'\".format(scene_path))\n\n        self.call_remote_api('simxLoadScene', scene_path, True)\n\n        if start:\n            self.start_simulation()", "category": "Python"}, {"instruction": "def _property_names(self):\n        \"\"\"\n            Gather up properties and cached_properties which may be methods\n            that were decorated. Need to inspect class versions b/c doing\n            getattr on them could cause unwanted side effects.\n        \"\"\"\n", "input": "", "output": "        property_names = []\n\n        for name in dir(self):\n            try:\n                attr = getattr(type(self), name)\n\n                if isinstance(attr, property) or isinstance(attr, cached_property):\n                    property_names.append(name)\n\n            except AttributeError:\n                pass\n\n        return property_names", "category": "Python"}, {"instruction": "def setattr(self, key, value):\n\t\t\"\"\" This method sets attribute of a Managed Object. \"\"\"\n", "input": "", "output": "\t\tif (UcsUtils.FindClassIdInMoMetaIgnoreCase(self.classId) != None):\n\t\t\tif (key in _ManagedObjectMeta[self.classId]):\n\t\t\t\tpropMeta = UcsUtils.GetUcsPropertyMeta(self.classId, key)\n\n\t\t\t\tif (propMeta.ValidatePropertyValue(value) == False):\n\t\t\t\t\t# print \"Validation Failure\"\n\t\t\t\t\treturn False\n\n\t\t\t\tif (propMeta.mask != None):\n\t\t\t\t\tself.dirtyMask |= propMeta.mask\n\n\t\t\t\tself.__dict__[key] = value\n\t\t\telse:\n\t\t\t\tself.__dict__['XtraProperty'][key] = value\n\t\telse:\n\t\t\t", "category": "Python"}, {"instruction": "def _update_ssl_params(host):\n        \"\"\"\n        Update the host ssl params (port or scheme) if needed.\n        :param host:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if host[HostParsing.HOST] \\\n                and EsParser._is_secure_connection_type(host):\n            host[HostParsing.PORT] = EsParser.SSL_DEFAULT_PORT\n            host[HostParsing.USE_SSL] = True\n            parsed_url = urlparse(EsParser._fix_host_prefix(host[HostParsing.HOST]))\n            host[HostParsing.HOST] = parsed_url.hostname\n            host[HostParsing.SCHEME] = HostParsing.HTTPS\n        return host", "category": "Python"}, {"instruction": "def run(self, postfunc=lambda: None):\n        \"\"\"Run the jobs.\n\n        postfunc() will be invoked after the jobs has run. It will be\n        invoked even if the jobs are interrupted by a keyboard\n        interrupt (well, in fact by a signal such as either SIGINT,\n        SIGTERM or SIGHUP). The execution of postfunc() is protected\n        against keyboard interrupts and is guaranteed to run to\n        completion.\"\"\"\n", "input": "", "output": "        self._setup_sig_handler()\n        try:\n            self.job.start()\n        finally:\n            postfunc()\n            self._reset_sig_handler()", "category": "Python"}, {"instruction": "def hidePopup(self):\r\n        \"\"\"\r\n        Overloads the hide popup method to handle when the user hides\r\n        the popup widget.\r\n        \"\"\"\n", "input": "", "output": "        if self._treePopupWidget and self.showTreePopup():\r\n            self._treePopupWidget.close()\r\n        \r\n        super(XOrbRecordBox, self).hidePopup()", "category": "Python"}, {"instruction": "def SensorsTriggersNotificationsDelete(self, sensor_id, trigger_id, notification_id):\r\n        \"\"\"\r\n            Disconnect a notification from a sensor-trigger combination.\r\n            \r\n            @param sensor_id (int) - Sensor id if the sensor-trigger combination.\r\n            @param trigger_id (int) - Trigger id of the sensor-trigger combination.\r\n            @param notification_id (int) - Notification id of the notification to disconnect.\r\n            \r\n            @param (bool) - Boolean indicating whether SensorstriggersNotificationsDelete was successful.            \r\n        \"\"\"\n", "input": "", "output": "        if self.__SenseApiCall__('/sensors/{0}/triggers/{1}/notifications/{2}.json'.format(sensor_id, trigger_id, notification_id), 'DELETE'):\r\n            return True\r\n        else:\r\n            self.__error__ = \"api call unsuccessful\"\r\n            return False", "category": "Python"}, {"instruction": "def write_tile_if_changed(store, tile_data, coord, format):\n    \"\"\"\n    Only write tile data if different from existing.\n\n    Try to read the tile data from the store first. If the existing\n    data matches, don't write. Returns whether the tile was written.\n    \"\"\"\n", "input": "", "output": "\n    existing_data = store.read_tile(coord, format)\n    if not existing_data or \\\n       not tiles_are_equal(existing_data, tile_data, format):\n        store.write_tile(tile_data, coord, format)\n        return True\n    else:\n        return False", "category": "Python"}, {"instruction": "def write_source(self, source, comment=None, reject=False):\n        \"\"\"\n        Writes out data for a single source.\n        \"\"\"\n", "input": "", "output": "        if not self._header_written:\n            observations = [reading.get_observation() for reading in source.get_readings()]\n            self.write_headers(observations, self.sys_header)\n\n        self._write_source(source, comment=comment, reject=reject)", "category": "Python"}, {"instruction": "def unlock(self):\n        \"\"\"Unlock a previously locked server.\n        \"\"\"\n", "input": "", "output": "        cmd = {\"fsyncUnlock\": 1}\n        with self._socket_for_writes() as sock_info:\n            if sock_info.max_wire_version >= 4:\n                try:\n                    sock_info.command(\"admin\", cmd)\n                except OperationFailure as exc:\n                    # Ignore \"DB not locked\" to replicate old behavior\n                    if exc.code != 125:\n                        raise\n            else:\n                helpers._first_batch(sock_info, \"admin\", \"$cmd.sys.unlock\",\n                    {}, -1, True, self.codec_options,\n                    ReadPreference.PRIMARY, cmd, self._event_listeners)", "category": "Python"}, {"instruction": "def _fill_diagonals(self, m):\n        \"\"\"Fills diagonals of `nsites` matrices in `m` so rows sum to 0.\"\"\"\n", "input": "", "output": "        assert m.shape == (self.nsites, N_CODON, N_CODON)\n        for r in range(self.nsites):\n            scipy.fill_diagonal(m[r], 0)\n            m[r][self._diag_indices] -= scipy.sum(m[r], axis=1)", "category": "Python"}, {"instruction": "def return_logfile(filename, log_dir='/var/log'):\n        \"\"\"Return a path for logging file.\n\n        If ``log_dir`` exists and the userID is 0 the log file will be written\n        to the provided log directory. If the UserID is not 0 or log_dir does\n        not exist the log file will be written to the users home folder.\n\n        :param filename: ``str``\n        :param log_dir: ``str``\n        :return: ``str``\n        \"\"\"\n", "input": "", "output": "        if sys.platform == 'win32':\n            user = getpass.getuser()\n        else:\n            user = os.getuid()\n        home = os.path.expanduser('~')\n\n        if not os.path.isdir(log_dir):\n            return os.path.join(home, filename)\n\n        log_dir_stat = os.stat(log_dir)\n        if log_dir_stat.st_uid == user:\n            return os.path.join(log_dir, filename)\n        elif log_dir_stat.st_gid == user:\n            return os.path.join(log_dir, filename)\n        else:\n            return os.path.join(home, filename)", "category": "Python"}, {"instruction": "def apply_offset(self):\n    '''Naively apply query offset.'''\n", "input": "", "output": "    self._ensure_modification_is_safe()\n\n    if self.query.offset != 0:\n      self._iterable = \\\n        offset_gen(self.query.offset, self._iterable, self._skipped_inc)", "category": "Python"}, {"instruction": "def find_call(self, path, method):\n        \"\"\"Find callable for the specified URL path and HTTP method.\n\n        Args:\n            path (:obj:`str`): URL path to match\n            method (:obj:`str`): HTTP method\n\n        Note:\n            A trailing '/' is always assumed in the path.\n        \"\"\"\n", "input": "", "output": "        if not path.endswith('/'):\n            path += '/'\n        path = path.split('/')[1:]\n        return self._recursive_route_match(self._routes, path, method, [])", "category": "Python"}, {"instruction": "def collect(self):\n        \"\"\"\n        Collect and publish netfilter counters\n        \"\"\"\n", "input": "", "output": "        cmd = [self.config['bin'], \"list\"]\n\n        if str_to_bool(self.config['reset']):\n            cmd.append(\"reset\")\n\n        if str_to_bool(self.config['use_sudo']):\n            cmd.insert(0, self.config['sudo_cmd'])\n\n        # We avoid use of the XML format to mtaintain compatbility with older\n        # versions of nfacct and also to avoid the bug where pkts and bytes were\n        # flipped\n\n        # Each line is of the format:\n        # { pkts = 00000000000001121700, bytes = 00000000000587037355 } = ipv4;\n        matcher = re.compile(\"{ pkts = (.*), bytes = (.*) } = (.*);\")\n        lines = Popen(cmd, stdout=PIPE).communicate()[0].strip().splitlines()\n\n        for line in lines:\n            matches = re.match(matcher, line)\n            if matches:\n                num_packets = int(matches.group(1))\n                num_bytes = int(matches.group(2))\n                name = matches.group(3)\n                self.publish(name + \".pkts\", num_packets)\n                self.publish(name + \".bytes\", num_bytes)", "category": "Python"}, {"instruction": "def get(self, request, bot_id, format=None):\n        \"\"\"\n        Get list of environment variables\n        ---\n        serializer: EnvironmentVarSerializer\n        responseMessages:\n            - code: 401\n              message: Not authenticated\n        \"\"\"\n", "input": "", "output": "        return super(EnvironmentVarList, self).get(request, bot_id, format)", "category": "Python"}, {"instruction": "def get_visible_scopes(self):\n        \"\"\"Get list of non-internal scopes for token.\n\n        :returns: A list of scopes.\n        \"\"\"\n", "input": "", "output": "        return [k for k, s in current_oauth2server.scope_choices()\n                if k in self.scopes]", "category": "Python"}, {"instruction": "def get_state_change_with_balance_proof_by_locksroot(\n        storage: sqlite.SQLiteStorage,\n        canonical_identifier: CanonicalIdentifier,\n        locksroot: Locksroot,\n        sender: Address,\n) -> sqlite.StateChangeRecord:\n    \"\"\" Returns the state change which contains the corresponding balance\n    proof.\n\n    Use this function to find a balance proof for a call to unlock, which only\n    happens after settle, so the channel has the unblinded version of the\n    balance proof.\n    \"\"\"\n", "input": "", "output": "    return storage.get_latest_state_change_by_data_field({\n        'balance_proof.canonical_identifier.chain_identifier': str(\n            canonical_identifier.chain_identifier,\n        ),\n        'balance_proof.canonical_identifier.token_network_address': to_checksum_address(\n            canonical_identifier.token_network_address,\n        ),\n        'balance_proof.canonical_identifier.channel_identifier': str(\n            canonical_identifier.channel_identifier,\n        ),\n        'balance_proof.locksroot': serialize_bytes(locksroot),\n        'balance_proof.sender': to_checksum_address(sender),\n    })", "category": "Python"}, {"instruction": "def dedup_search_results(search_results):\n    \"\"\" dedup results\n    \"\"\"\n", "input": "", "output": "\n    known = set()\n    deduped_results = []\n\n    for i in search_results:\n\n        username = i['username']\n\n        if username in known:\n            continue\n\n        deduped_results.append(i)\n\n        known.add(username)\n\n    return deduped_results", "category": "Python"}, {"instruction": "def run(self, args=None):\n        \"\"\"Applicatin starting point.\n\n        This will run the associated method/function/module or print a help\n        list if it's an unknown keyword or the syntax is incorrect.\n\n        Keyword arguments:\n        args -- Custom application arguments (default sys.argv)\n\n        \"\"\"\n", "input": "", "output": "        # TODO: Add tests to how command line arguments are passed in\n        raw_args = self.__parser.parse_args(args=args)\n        args = vars(raw_args)\n        cmd = args.pop('cmd')\n        if hasattr(cmd, '__call__'):\n            cmd(**args)", "category": "Python"}, {"instruction": "def delete_user_login(self, id, user_id):\r\n        \"\"\"\r\n        Delete a user login.\r\n\r\n        Delete an existing login.\r\n        \"\"\"\n", "input": "", "output": "        path = {}\r\n        data = {}\r\n        params = {}\r\n\r\n        # REQUIRED - PATH - user_id\r\n        ", "category": "Python"}, {"instruction": "def parse_rdf(self):\n        \"\"\" Parses the relevant PG rdf file\n        \"\"\"\n", "input": "", "output": "        try:\n            self.metadata = pg_rdf_to_json(self.rdf_path)\n        except IOError as e:\n            raise NoRDFError(e)\n\n        if not self.authnames():\n            self.author = ''\n        elif len(self.authnames()) == 1:\n            self.author = self.authnames()[0]\n        else:\n            self.author = \"Various\"", "category": "Python"}, {"instruction": "def pid_exists(pid):\n    \"\"\" Determines if a system process identifer exists in process table.\n        \"\"\"\n", "input": "", "output": "    try:\n        os.kill(pid, 0)\n    except OSError as exc:\n        return exc.errno == errno.EPERM\n    else:\n        return True", "category": "Python"}, {"instruction": "def getStates(self, Corpnum, reciptNumList, UserID=None):\r\n        \"\"\" \uc804\uc1a1\ub0b4\uc5ed \uc694\uc57d\uc815\ubcf4 \ud655\uc778\r\n            args\r\n                CorpNum : \ud31d\ube4c\ud68c\uc6d0 \uc0ac\uc5c5\uc790\ubc88\ud638\r\n                reciptNumList : \ubb38\uc790\uc804\uc1a1 \uc811\uc218\ubc88\ud638 \ubc30\uc5f4\r\n                UserID : \ud31d\ube4c\ud68c\uc6d0 \uc544\uc774\ub514\r\n            return\r\n                \uc804\uc1a1\uc815\ubcf4 as list\r\n            raise\r\n                PopbillException\r\n        \"\"\"\n", "input": "", "output": "        if reciptNumList == None or len(reciptNumList) < 1:\r\n            raise PopbillException(-99999999, \"\uc811\uc218\ubc88\ud638\uac00 \uc785\ub825\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\")\r\n\r\n        postData = self._stringtify(reciptNumList)\r\n\r\n        return self._httppost('/Message/States', postData, Corpnum, UserID)", "category": "Python"}, {"instruction": "def _send_streamify(self, frame):\n        \"\"\"\n        Helper method to streamify a frame.\n        \"\"\"\n", "input": "", "output": "\n        # Get the state and framer\n        state = self._send_framer_state\n        framer = self._send_framer\n\n        # Reset the state as needed\n        state._reset(framer)\n\n        # Now pass the frame through streamify() and return the result\n        return framer.streamify(state, frame)", "category": "Python"}, {"instruction": "def show_yticklabels_for_all(self, row_column_list=None):\n        \"\"\"Show the y-axis tick labels for all specified subplots.\n\n        :param row_column_list: a list containing (row, column) tuples to\n            specify the subplots, or None to indicate *all* subplots.\n        :type row_column_list: list or None\n\n        \"\"\"\n", "input": "", "output": "        if row_column_list is None:\n            for subplot in self.subplots:\n                subplot.show_yticklabels()\n        else:\n            for row, column in row_column_list:\n                self.show_yticklabels(row, column)", "category": "Python"}, {"instruction": "def _compute_cell_extents_grid(bounding_rect=(0.03, 0.03, 0.97, 0.97),\n                                   num_rows=2, num_cols=6,\n                                   axis_pad=0.01):\n        \"\"\"\n        Produces array of num_rows*num_cols elements each containing the rectangular extents of\n        the corresponding cell the grid, whose position is within bounding_rect.\n        \"\"\"\n", "input": "", "output": "\n        left, bottom, width, height = bounding_rect\n        height_padding = axis_pad * (num_rows + 1)\n        width_padding = axis_pad * (num_cols + 1)\n        cell_height = float((height - height_padding) / num_rows)\n        cell_width = float((width - width_padding) / num_cols)\n\n        cell_height_padded = cell_height + axis_pad\n        cell_width_padded = cell_width + axis_pad\n\n        extents = list()\n        for row in range(num_rows - 1, -1, -1):\n            for col in range(num_cols):\n                extents.append((left + col * cell_width_padded,\n                                bottom + row * cell_height_padded,\n                                cell_width, cell_height))\n\n        return extents", "category": "Python"}, {"instruction": "def add_layout(self, layout):\n        \"\"\"\n        Add a Layout to the Frame.\n\n        :param layout: The Layout to be added.\n        \"\"\"\n", "input": "", "output": "        layout.register_frame(self)\n        self._layouts.append(layout)", "category": "Python"}, {"instruction": "def role_delete(role_id, endpoint_id):\n    \"\"\"\n    Executor for `globus endpoint role delete`\n    \"\"\"\n", "input": "", "output": "    client = get_client()\n    res = client.delete_endpoint_role(endpoint_id, role_id)\n    formatted_print(res, text_format=FORMAT_TEXT_RAW, response_key=\"message\")", "category": "Python"}, {"instruction": "def flags(self, index):\n        \"\"\"\"Determines interaction allowed with table cells.\n\n        See :qtdoc:`QAbstractItemModel<QAbstractItemModel.flags>`, \n        and :qtdoc:`subclassing<qabstractitemmodel.subclassing>`\n        \"\"\"\n", "input": "", "output": "        if index.isValid():\n            if self.model.editableRow(index.row()) and index.column() < 4:\n                return QtCore.Qt.ItemIsDragEnabled | \\\n                       QtCore.Qt.ItemIsEnabled | QtCore.Qt.ItemIsSelectable | \\\n                       QtCore.Qt.ItemIsEditable\n            else:\n                return QtCore.Qt.ItemIsSelectable | QtCore.Qt.ItemIsEnabled\n        else:\n            print 'flags: index invalid'", "category": "Python"}, {"instruction": "def geoid(self):\n        \"\"\"\"Return first child of the column, or self that is marked as a geographic identifier\"\"\"\n", "input": "", "output": "\n        if self.valuetype_class.is_geoid():\n            return self\n\n        for c in self.table.columns:\n            if c.parent == self.name and  c.valuetype_class.is_geoid():\n                return c", "category": "Python"}, {"instruction": "def cleanup_sweep_threads():\n    '''\n    Not used. Keeping this function in case we decide not to use\n    daemonized threads and it becomes necessary to clean up the\n    running threads upon exit.\n    '''\n", "input": "", "output": "\n    for dict_name, obj in globals().items():\n        if isinstance(obj, (TimedDict,)):\n            logging.info(\n                'Stopping thread for TimedDict {dict_name}'.format(\n                    dict_name=dict_name))\n            obj.stop_sweep()", "category": "Python"}, {"instruction": "def _find_elements(self, result, elements):\n    \"\"\"Find interesting elements from XML.\n\n    This function tries to only look for specified elements\n    without parsing the entire XML. The specified elements is better\n    located near the beginning.\n\n    Args:\n      result: response XML.\n      elements: a set of interesting element tags.\n\n    Returns:\n      A dict from element tag to element value.\n    \"\"\"\n", "input": "", "output": "    element_mapping = {}\n    result = StringIO.StringIO(result)\n    for _, e in ET.iterparse(result, events=('end',)):\n      if not elements:\n        break\n      if e.tag in elements:\n        element_mapping[e.tag] = e.text\n        elements.remove(e.tag)\n    return element_mapping", "category": "Python"}, {"instruction": "def add_options(self):\n        \"\"\" Add program options.\n        \"\"\"\n", "input": "", "output": "        super(ThemeSwitcher, self).add_options()\n\n        self.add_bool_option(\"-l\", \"--list\",\n            help=\"list available themes\")\n        self.add_bool_option(\"-c\", \"--current\",\n            help=\"print path to currently selected theme\")\n        self.add_bool_option(\"-n\", \"--next\",\n            help=\"rotate through selected themes, and print new path\")\n        self.add_bool_option(\"-a\", \"--all\",\n            help=\"remove any selections, and use all themes\")\n        self.add_value_option(\"-t\", \"--toggle\", \"NAME\",\n            help=\"toggle selection of a theme\")", "category": "Python"}, {"instruction": "def post(self, request, *args, **kwargs):\n        \"\"\"\n        Do the login and password protection.\n        \"\"\"\n", "input": "", "output": "        self.object = self.get_object()\n        self.login()\n        if self.object.password:\n            entry_password = self.request.POST.get('entry_password')\n            if entry_password:\n                if entry_password == self.object.password:\n                    self.request.session[self.session_key %\n                                         self.object.pk] = self.object.password\n                    return self.get(request, *args, **kwargs)\n                else:\n                    self.error = True\n            return self.password()\n        return self.get(request, *args, **kwargs)", "category": "Python"}, {"instruction": "def avl_rotate_single(root, direction):\n    \"\"\"\n    Single rotation, either 0 (left) or 1 (right).\n\n    Figure:\n                a,0 (left)\n                ---------->\n          a                   b\n           \\                /   \\\n            b             a       c\n             \\\n              c\n\n    a = root\n    save = root.right\n    \"\"\"\n", "input": "", "output": "    other_side = 1 - direction\n    save = root[other_side]\n    save.parent = root.parent\n    # root[other_side] = save[direction]\n    # save[direction] = root\n    root.set_child(other_side, save[direction])\n    save.set_child(direction, root)\n    rlh = height(root.left)\n    rrh = height(root.right)\n    slh = height(save[other_side])\n    root.balance = max(rlh, rrh) + 1\n    save.balance = max(slh, root.balance) + 1\n    return save", "category": "Python"}, {"instruction": "def mother(year=None):\n    \"\"\"\n    the 2nd Sunday in May\n    :param year: int\n    :return: Mother's day\n    \"\"\"\n", "input": "", "output": "    may_first = datetime.date(_year, 5, 1) if not year else datetime.date(int(year), 5, 1)\n    weekday_seq = may_first.weekday()\n    return datetime.date(may_first.year, 5, (14 - weekday_seq))", "category": "Python"}, {"instruction": "def current_revision(self):\n        \"\"\"\n        :return: The current :class:`revision.data.Revision`.\n        :rtype: :class:`revision.data.Revision`\n        \"\"\"\n", "input": "", "output": "        if self.current_index is None:\n            return None\n\n        if len(self.revisions) > self.current_index:\n            return self.revisions[self.current_index]\n\n        return None", "category": "Python"}, {"instruction": "def is_file(dirname):\n    '''Checks if a path is an actual file that exists'''\n", "input": "", "output": "    if not os.path.isfile(dirname):\n        msg = \"{0} is not an existing file\".format(dirname)\n        raise argparse.ArgumentTypeError(msg)\n    else:\n        return dirname", "category": "Python"}, {"instruction": "def run():\n    print(\"Environment\", os.environ)\n    try:\n        os.environ[\"SELENIUM\"]\n    except KeyError:\n        print(\"Please set the environment variable SELENIUM to Selenium URL\")\n        sys.exit(1)\n\n    driver = WhatsAPIDriver(client='remote', command_executor=os.environ[\"SELENIUM\"])\n    print(\"Waiting for QR\")\n    driver.wait_for_login()\n    print(\"Bot started\")\n\n    driver.subscribe_new_messages(NewMessageObserver())\n    print(\"Waiting for new messages...\")\n\n    \"\"\" Locks the main thread while the subscription in running \"\"\"\n", "input": "", "output": "    while True:\n        time.sleep(60)", "category": "Python"}, {"instruction": "def case_insensitive_file_search(directory, pattern):\n    \"\"\"\n    Looks for file with pattern with case insensitive search\n    \"\"\"\n", "input": "", "output": "    try:\n        return os.path.join(\n            directory,\n            [filename for filename in os.listdir(directory)\n             if re.search(pattern, filename, re.IGNORECASE)][0])\n    except IndexError:\n        print(\"{0} not found\".format(pattern))\n        raise", "category": "Python"}, {"instruction": "def make_abstract_dist(req):\n    # type: (InstallRequirement) -> DistAbstraction\n    \"\"\"Factory to make an abstract dist object.\n\n    Preconditions: Either an editable req with a source_dir, or satisfied_by or\n    a wheel link, or a non-editable req with a source_dir.\n\n    :return: A concrete DistAbstraction.\n    \"\"\"\n", "input": "", "output": "    if req.editable:\n        return IsSDist(req)\n    elif req.link and req.link.is_wheel:\n        return IsWheel(req)\n    else:\n        return IsSDist(req)", "category": "Python"}, {"instruction": "def get_relation_count_query(self, query, parent):\n        \"\"\"\n        Add the constraints for a relationship count query.\n\n        :type query: Builder\n        :type parent: Builder\n\n        :rtype: Builder\n        \"\"\"\n", "input": "", "output": "        parent_table = self._parent.get_table()\n\n        self._set_join(query)\n\n        query.select(QueryExpression('COUNT(*)'))\n\n        key = self.wrap('%s.%s' % (parent_table, self._first_key))\n\n        return query.where(self.get_has_compare_key(), '=', QueryExpression(key))", "category": "Python"}, {"instruction": "def bind_top_down(lower, upper, __fval=None, **fval):\n    \"\"\"Bind 2 layers for building.\n    When the upper layer is added as a payload of the lower layer, all the arguments  # noqa: E501\n    will be applied to them.\n\n    ex:\n        >>> bind_top_down(Ether, SNAP, type=0x1234)\n        >>> Ether()/SNAP()\n        <Ether  type=0x1234 |<SNAP  |>>\n    \"\"\"\n", "input": "", "output": "    if __fval is not None:\n        fval.update(__fval)\n    upper._overload_fields = upper._overload_fields.copy()\n    upper._overload_fields[lower] = fval", "category": "Python"}, {"instruction": "def get_page_info(self, addr):\n        \"\"\"!\n        @brief Get info about the page that contains this address.\n        \"\"\"\n", "input": "", "output": "        assert self.region is not None\n        if not self.region.contains_address(addr):\n            return None\n\n        info = PageInfo()\n        info.program_weight = self.region.program_page_weight\n        info.size = self.region.page_size\n        info.base_addr = addr - (addr % info.size)\n        return info", "category": "Python"}, {"instruction": "def fadeToColor(self, fSeconds, fRed, fGreen, fBlue, fAlpha, bBackground):\n        \"\"\"\n        Fades the view on the HMD to the specified color. The fade will take fSeconds, and the color values are between\n        0.0 and 1.0. This color is faded on top of the scene based on the alpha parameter. Removing the fade color instantly \n        would be FadeToColor( 0.0, 0.0, 0.0, 0.0, 0.0 ).  Values are in un-premultiplied alpha space.\n        \"\"\"\n", "input": "", "output": "\n        fn = self.function_table.fadeToColor\n        fn(fSeconds, fRed, fGreen, fBlue, fAlpha, bBackground)", "category": "Python"}, {"instruction": "def decorator(caller, func=None):\n    \"\"\"\n    decorator(caller) converts a caller function into a decorator;\n    decorator(caller, func) decorates a function using a caller.\n    \"\"\"\n", "input": "", "output": "    if func is None: # returns a decorator\n        fun = FunctionMaker(caller)\n        first_arg = inspect.getargspec(caller)[0][0]\n        src = 'def %s(%s): return _call_(caller, %s)' % (\n            caller.__name__, first_arg, first_arg)\n        return fun.make(src, dict(caller=caller, _call_=decorator),\n                        undecorated=caller)\n    else: # returns a decorated function\n        fun = FunctionMaker(func)\n        src = ", "category": "Python"}, {"instruction": "def interpolate_xarray_linear(xpoints, ypoints, values, shape, chunks=CHUNK_SIZE):\n    \"\"\"Interpolate linearly, generating a dask array.\"\"\"\n", "input": "", "output": "    from scipy.interpolate.interpnd import (LinearNDInterpolator,\n                                            _ndim_coords_from_arrays)\n\n    if isinstance(chunks, (list, tuple)):\n        vchunks, hchunks = chunks\n    else:\n        vchunks, hchunks = chunks, chunks\n\n    points = _ndim_coords_from_arrays(np.vstack((np.asarray(ypoints),\n                                                 np.asarray(xpoints))).T)\n\n    interpolator = LinearNDInterpolator(points, values)\n\n    grid_x, grid_y = da.meshgrid(da.arange(shape[1], chunks=hchunks),\n                                 da.arange(shape[0], chunks=vchunks))\n\n    # workaround for non-thread-safe first call of the interpolator:\n    interpolator((0, 0))\n    res = da.map_blocks(intp, grid_x, grid_y, interpolator=interpolator)\n\n    return DataArray(res, dims=('y', 'x'))", "category": "Python"}, {"instruction": "def run(self, scheduler_schedule_id, **kwargs):\n        \"\"\"\n        Deactivates the schedule specified by the ID `scheduler_schedule_id` in\n        the scheduler service.\n\n        Arguments:\n            scheduler_schedule_id {str} -- The ID of the schedule to deactivate\n        \"\"\"\n", "input": "", "output": "        log = self.get_logger(**kwargs)\n\n        self.scheduler.update_schedule(scheduler_schedule_id, {\"active\": False})\n        log.info(\n            \"Deactivated schedule %s in the scheduler service\", scheduler_schedule_id\n        )", "category": "Python"}, {"instruction": "def put_file_range(ase, offsets, data, timeout=None):\n    # type: (blobxfer.models.azure.StorageEntity,\n    #        blobxfer.models.upload.Offsets, bytes, int) -> None\n    \"\"\"Puts a range of bytes into the remote file\n    :param blobxfer.models.azure.StorageEntity ase: Azure StorageEntity\n    :param blobxfer.models.upload.Offsets offsets: upload offsets\n    :param bytes data: data\n    :param int timeout: timeout\n    \"\"\"\n", "input": "", "output": "    dir, fpath, _ = parse_file_path(ase.name)\n    ase.client.update_range(\n        share_name=ase.container,\n        directory_name=dir,\n        file_name=fpath,\n        data=data,\n        start_range=offsets.range_start,\n        end_range=offsets.range_end,\n        validate_content=False,  # integrity is enforced with HTTPS\n        timeout=timeout)", "category": "Python"}, {"instruction": "def _update_model(self, case, B, Bsrc, v_angle, p_srcinj, p_ref, ref_idx):\n        \"\"\" Updates the case with values computed from the voltage phase\n            angle solution.\n        \"\"\"\n", "input": "", "output": "        iref = ref_idx\n        base_mva = case.base_mva\n        buses = case.connected_buses\n        branches = case.online_branches\n\n        p_from = (Bsrc * v_angle + p_srcinj) * base_mva\n        p_to = -p_from\n\n        for i, branch in enumerate(branches):\n            branch.p_from = p_from[i]\n            branch.p_to = p_to[i]\n            branch.q_from = 0.0\n            branch.q_to = 0.0\n\n        for j, bus in enumerate(buses):\n            bus.v_angle = v_angle[j] * (180 / pi)\n            bus.v_magnitude = 1.0\n\n        # Update Pg for swing generator.\n        g_ref = [g for g in case.generators if g.bus == buses[iref]][0]\n        # Pg = Pinj + Pload + Gs\n        # newPg = oldPg + newPinj - oldPinj\n        p_inj = (B[iref, :] * v_angle - p_ref) * base_mva\n        g_ref.p += p_inj[0]", "category": "Python"}, {"instruction": "def _get_covars(self):\n        \"\"\"Covariance parameters for each mixture component.\n        The shape depends on `cvtype`::\n\n            (`n_states`, 'n_features')                if 'spherical',\n            (`n_features`, `n_features`)              if 'tied',\n            (`n_states`, `n_features`)                if 'diag',\n            (`n_states`, `n_features`, `n_features`)  if 'full'\n            \"\"\"\n", "input": "", "output": "        if self.covariance_type == 'full':\n            return self.covars_\n        elif self.covariance_type == 'diag':\n            return [np.diag(cov) for cov in self.covars_]\n        elif self.covariance_type == 'tied':\n            return [self.covars_] * self.n_components\n        elif self.covariance_type == 'spherical':\n            return [np.diag(cov) for cov in self.covars_]", "category": "Python"}, {"instruction": "def build(\n        documentPath,\n        outputUFOFormatVersion=3,\n        roundGeometry=True,\n        verbose=True,           # not supported\n        logPath=None,           # not supported\n        progressFunc=None,      # not supported\n        processRules=True,\n        logger=None,\n        useVarlib=False,\n        ):\n    \"\"\"\n        Simple builder for UFO designspaces.\n    \"\"\"\n", "input": "", "output": "    import os, glob\n    if os.path.isdir(documentPath):\n        # process all *.designspace documents in this folder\n        todo = glob.glob(os.path.join(documentPath, \"*.designspace\"))\n    else:\n        # process the\n        todo = [documentPath]\n    results = []\n    for path in todo:\n        document = DesignSpaceProcessor(ufoVersion=outputUFOFormatVersion)\n        document.useVarlib = useVarlib\n        document.roundGeometry = roundGeometry\n        document.read(path)\n        try:\n            r = document.generateUFO(processRules=processRules)\n            results.append(r)\n        except:\n            if logger:\n                logger.exception(\"ufoProcessor error\")\n        #results += document.generateUFO(processRules=processRules)\n        reader = None\n    return results", "category": "Python"}, {"instruction": "def zscore(self, axis=1):\n        \"\"\"\n        Subtract the mean and divide by standard deviation within or across records.\n\n        Parameters\n        ----------\n        axis : int, optional, default = 0\n            Which axis to zscore along, within (1) or across (0) records\n        \"\"\"\n", "input": "", "output": "        if axis == 1:\n            return self.map(lambda x: (x - mean(x)) / std(x))\n        elif axis == 0:\n            meanval = self.mean().toarray()\n            stdval = self.std().toarray()\n            return self.map(lambda x: (x - meanval) / stdval)\n        else:\n            raise Exception('Axis must be 0 or 1')", "category": "Python"}, {"instruction": "def _build_model(self):\n        \"\"\"\n        Build model.\n        \"\"\"\n", "input": "", "output": "\n        if \"input_dim\" not in self.settings:\n            raise ValueError(\"Model parameter input_dim cannot be None.\")\n\n        self.linear = nn.Linear(\n            self.settings[\"input_dim\"], self.cardinality, self.settings[\"bias\"]\n        )", "category": "Python"}, {"instruction": "def download(self):\n        \"\"\"\n        Request url and return his content\n        The Requested content will be cached into the default temp directory.\n        \"\"\"\n", "input": "", "output": "        if os.path.isfile(self.archive_path):\n            print(\"Use %r\" % self.archive_path)\n            with open(self.archive_path, \"rb\") as f:\n                content = f.read()\n        else:\n            print(\"Request: %r...\" % self.URL)\n            # Warning: HTTPS requests do not do any verification of the server's certificate.\n            f = urlopen(self.URL)\n            content = f.read()\n            with open(self.archive_path, \"wb\") as out_file:\n                out_file.write(content)\n\n        # Check SHA hash:\n        current_sha1 = hashlib.sha1(content).hexdigest()\n        assert current_sha1 == self.DOWNLOAD_SHA1, \"Download sha1 value is wrong! SHA1 is: %r\" % current_sha1\n        print(\"Download SHA1: %r, ok.\" % current_sha1)", "category": "Python"}, {"instruction": "def to_utf8(buf, errors='replace'):\n    \"\"\" Encodes a string into a UTF-8 compatible, ASCII string.\n\n        `buf`\n            string or unicode to convert.\n\n        Returns string.\n\n        * Raises a ``UnicodeEncodeError`` exception if encoding failed and\n          `errors` isn't set to 'replace'.\n        \"\"\"\n", "input": "", "output": "\n    if isinstance(buf, unicode):\n        return buf.encode('utf-8', errors)\n\n    else:\n        return buf", "category": "Python"}, {"instruction": "def delete(self):\n        \"\"\"Delete the file.\"\"\"\n", "input": "", "output": "        self.close()\n        if self.does_file_exist():\n            os.remove(self.path)", "category": "Python"}, {"instruction": "def create_description(self, complib=None, complevel=None,\n                           fletcher32=False, expectedrows=None):\n        \"\"\" create the description of the table from the axes & values \"\"\"\n", "input": "", "output": "\n        # provided expected rows if its passed\n        if expectedrows is None:\n            expectedrows = max(self.nrows_expected, 10000)\n\n        d = dict(name='table', expectedrows=expectedrows)\n\n        # description from the axes & values\n        d['description'] = {a.cname: a.typ for a in self.axes}\n\n        if complib:\n            if complevel is None:\n                complevel = self._complevel or 9\n            filters = _tables().Filters(\n                complevel=complevel, complib=complib,\n                fletcher32=fletcher32 or self._fletcher32)\n            d['filters'] = filters\n        elif self._filters is not None:\n            d['filters'] = self._filters\n\n        return d", "category": "Python"}, {"instruction": "def create_agent_signer(user_id):\n    \"\"\"Sign digest with existing GPG keys using gpg-agent tool.\"\"\"\n", "input": "", "output": "    sock = connect_to_agent(env=os.environ)\n    keygrip = get_keygrip(user_id)\n\n    def sign(digest):\n        ", "category": "Python"}, {"instruction": "def _restart_on_unavailable(restart):\n    \"\"\"Restart iteration after :exc:`.ServiceUnavailable`.\n\n    :type restart: callable\n    :param restart: curried function returning iterator\n    \"\"\"\n", "input": "", "output": "    resume_token = b\"\"\n    item_buffer = []\n    iterator = restart()\n    while True:\n        try:\n            for item in iterator:\n                item_buffer.append(item)\n                if item.resume_token:\n                    resume_token = item.resume_token\n                    break\n        except ServiceUnavailable:\n            del item_buffer[:]\n            iterator = restart(resume_token=resume_token)\n            continue\n\n        if len(item_buffer) == 0:\n            break\n\n        for item in item_buffer:\n            yield item\n\n        del item_buffer[:]", "category": "Python"}, {"instruction": "def agent():\n  \"\"\"Run the agent, connecting to a (remote) host started independently.\"\"\"\n", "input": "", "output": "  agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n  agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n\n  logging.info(\"Starting agent:\")\n  with remote_sc2_env.RemoteSC2Env(\n      map_name=FLAGS.map,\n      host=FLAGS.host,\n      host_port=FLAGS.host_port,\n      lan_port=FLAGS.lan_port,\n      name=FLAGS.agent_name or agent_name,\n      race=sc2_env.Race[FLAGS.agent_race],\n      step_mul=FLAGS.step_mul,\n      agent_interface_format=sc2_env.parse_agent_interface_format(\n          feature_screen=FLAGS.feature_screen_size,\n          feature_minimap=FLAGS.feature_minimap_size,\n          rgb_screen=FLAGS.rgb_screen_size,\n          rgb_minimap=FLAGS.rgb_minimap_size,\n          action_space=FLAGS.action_space,\n          use_feature_units=FLAGS.use_feature_units),\n      visualize=FLAGS.render) as env:\n    agents = [agent_cls()]\n    logging.info(\"Connected, starting run_loop.\")\n    try:\n      run_loop.run_loop(agents, env)\n    except remote_sc2_env.RestartException:\n      pass\n  logging.info(\"Done.\")", "category": "Python"}, {"instruction": "def _get_session_timeout_seconds(cls, session_server):\n        \"\"\"\n        :type session_server: core.SessionServer\n\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "\n        if session_server.user_company is not None:\n            return session_server.user_company.session_timeout\n        elif session_server.user_person is not None:\n            return session_server.user_person.session_timeout\n        elif session_server.user_api_key is not None:\n            return session_server \\\n                .user_api_key \\\n                .requested_by_user \\\n                .get_referenced_object() \\\n                .session_timeout\n        else:\n            raise BunqException()", "category": "Python"}, {"instruction": "def process_frame(self):\n        \"\"\"Get the input and output frame, then call\n        :py:meth:`transform`.\n\n        \"\"\"\n", "input": "", "output": "        input_name = self.inputs[0]\n        output_name = self.outputs[0]\n        in_frame = self.input_buffer[input_name].get()\n        out_frame = self.outframe_pool[output_name].get()\n        out_frame.initialise(in_frame)\n        if self.transform(in_frame, out_frame):\n            self.send(output_name, out_frame)\n        else:\n            raise StopIteration()", "category": "Python"}, {"instruction": "def get_channels(self):\n        \"\"\"Get the selected channel(s in order).\n\n        Returns\n        -------\n        list of str\n            name of each channel (without group), in original record order\n        \"\"\"\n", "input": "", "output": "        selectedItems = self.idx_chan.selectedItems()\n        selected_chan = [x.text().split('\u2014')[0] for x in selectedItems]\n        chan_in_order = []\n        for chan in self.one_grp['chan_to_plot']:\n            if chan in selected_chan:\n                chan_in_order.append(chan)\n\n        return chan_in_order", "category": "Python"}, {"instruction": "def close_all():\n    \"\"\"Close all open/active plotters\"\"\"\n", "input": "", "output": "    for key, p in _ALL_PLOTTERS.items():\n        p.close()\n    _ALL_PLOTTERS.clear()\n    return True", "category": "Python"}, {"instruction": "def character_from_structure(motivation):\n    \"\"\"Find a character for a given structure.\"\"\"\n", "input": "", "output": "    assert len(motivation) == 3\n\n    _c = {\n            \"+\": \"\u2ff0\",\n            \"-\": \"\u2ff1\",\n            '>': \"\u2ff1\",\n            \"\u624b\": \"\u624c\",\n            \"\u4eba\": \"\u4ebb\",\n            \"\u5200\": \"\u5202\",\n            \"\u4e1d\": \"\u7cf9\",\n            \"\u6c34\": \"\u6c35\",\n            \"0\": \"\u2ff4\",\n            }\n    structure = ''.join([_c.get(x, x) for x in motivation])\n    return _cd.IDS.get(structure, '?')", "category": "Python"}, {"instruction": "def get_fiat_prices(self, base=None, symbol=None):\n        \"\"\"Get fiat price for currency\n\n        https://docs.kucoin.com/#get-fiat-price\n\n        :param base: (optional) Fiat,eg.USD,EUR, default is USD.\n        :type base: string\n        :param symbol: (optional) Cryptocurrencies.For multiple cyrptocurrencies, please separate them with\n                       comma one by one. default is all\n        :type symbol: string\n\n        .. code:: python\n\n            prices = client.get_fiat_prices()\n\n        :returns: ApiResponse\n\n        .. code:: python\n\n            {\n                \"BTC\": \"3911.28000000\",\n                \"ETH\": \"144.55492453\",\n                \"LTC\": \"48.45888179\",\n                \"KCS\": \"0.45546856\"\n            }\n\n        :raises: KucoinResponseException, KucoinAPIException\n\n        \"\"\"\n", "input": "", "output": "\n        data = {}\n\n        if base is not None:\n            data['base'] = base\n        if symbol is not None:\n            data['currencies'] = symbol\n\n        return self._get('prices', False, data=data)", "category": "Python"}, {"instruction": "def _check_hint_bounds(self, ds):\n        '''\n        Checks for variables ending with _bounds, if they are not cell methods,\n        make the recommendation\n\n        :param netCDF4.Dataset ds: An open netCDF dataset\n        :rtype: list\n        :return: List of results\n        '''\n", "input": "", "output": "        ret_val = []\n        boundary_variables = cfutil.get_cell_boundary_variables(ds)\n        for name in ds.variables:\n            if name.endswith('_bounds') and name not in boundary_variables:\n                msg = ('{} might be a cell boundary variable but there are no variables that define it '\n                       'as a boundary using the `bounds` attribute.'.format(name))\n                result = Result(BaseCheck.LOW,\n                                True,\n                                self.section_titles['7.1'],\n                                [msg])\n                ret_val.append(result)\n\n        return ret_val", "category": "Python"}, {"instruction": "def __write_byte(self, address, value):\n        \"\"\"Write byte in memory.\n        \"\"\"\n", "input": "", "output": "        # Save previous address content.\n        if address in self._memory:\n            self.__memory_prev[address] = self._memory[address]\n\n        self._memory[address] = value & 0xff", "category": "Python"}, {"instruction": "def execute(self, query_string, params=None):\n    \"\"\"Executes a query. Returns the resulting cursor.\n\n    :query_string: the parameterized query string\n    :params: can be either a tuple or a dictionary, and must match the parameterization style of the\n             query\n    :return: a cursor object\n    \"\"\"\n", "input": "", "output": "    cr = self.connection.cursor()\n    logger.info(\"SQL: %s (%s)\", query_string, params)\n    self.last_query = (query_string, params)\n    t0 = time.time()\n    cr.execute(query_string, params or self.core.empty_params)\n    ms = (time.time() - t0) * 1000\n    logger.info(\"RUNTIME: %.2f ms\", ms)\n    self._update_cursor_stats(cr)\n    return cr", "category": "Python"}, {"instruction": "def to_file(self, filename):\n        \"\"\" Save the store to disk \"\"\"\n", "input": "", "output": "\n        # save compressed file\n        xml = self.to_xml()\n        f = gzip.open(filename, 'wb')\n        try:\n            f.write(xml.encode('utf-8'))\n        finally:\n            f.close()", "category": "Python"}, {"instruction": "def authenticate(self, api_key):\n        \"\"\"Logs user into Heroku with given api_key.\"\"\"\n", "input": "", "output": "        self._api_key = api_key\n\n        # Attach auth to session.\n        self._session.auth = ('', self._api_key)\n\n        return self._verify_api_key()", "category": "Python"}, {"instruction": "def _logfile_sigterm_handler(*_):\n    # type: (...) -> None\n    \"\"\"Handle exit signals and write out a log file.\n\n    Raises:\n        SystemExit: Contains the signal as the return code.\n    \"\"\"\n", "input": "", "output": "    logging.error('Received SIGTERM.')\n    write_logfile()\n    print('Received signal. Please see the log file for more information.',\n          file=sys.stderr)\n    sys.exit(signal)", "category": "Python"}, {"instruction": "def remove_properties_containing_None(properties_dict):\n    \"\"\"\n    removes keys from a dict those values == None\n    json schema validation might fail if they are set and\n    the type or format of the property does not match\n    \"\"\"\n", "input": "", "output": "    # remove empty properties - as validations may fail\n    new_dict  = dict()\n    for key in properties_dict.keys():\n        value = properties_dict[key]\n        if value is not None:\n            new_dict[key] = value\n    return new_dict", "category": "Python"}, {"instruction": "def ip_address(self):\n        \"\"\"\n        The IP address of the first interface listed in the droplet's\n        ``networks`` field (ordering IPv4 before IPv6), or `None` if there\n        are no interfaces\n        \"\"\"\n", "input": "", "output": "        networks = self.get(\"networks\", {})\n        v4nets = networks.get(\"v4\", [])\n        v6nets = networks.get(\"v6\", [])\n        try:\n            return (v4nets + v6nets)[0].ip_address\n        except IndexError:\n            return None", "category": "Python"}, {"instruction": "def set_http_application_url_input_config_http_app_url_url(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        set_http_application_url = ET.Element(\"set_http_application_url\")\n        config = set_http_application_url\n        input = ET.SubElement(set_http_application_url, \"input\")\n        config_http_app_url = ET.SubElement(input, \"config-http-app-url\")\n        url = ET.SubElement(config_http_app_url, \"url\")\n        url.text = kwargs.pop('url')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def tunnel_open(self):\n        ''' Open tunnel '''\n", "input": "", "output": "        if (self.server.is_server_running() == 'no' or\n                self.server.is_server_running() == 'maybe'):\n            print(\"Error: Sorry, you need to have the server running to open a \"\n                  \"tunnel.  Try 'server on' first.\")\n        else:\n            self.tunnel.open()", "category": "Python"}, {"instruction": "def make_constants(builtin_only=False, stoplist=None, verbose=None):\n    \"\"\"Return a decorator for optimizing global references.\n\n    Replaces global references with their currently defined values.\n    If not defined, the dynamic (runtime) global lookup is left undisturbed.\n    If builtin_only is True, then only builtins are optimized.\n    Variable names in the stoplist are also left undisturbed.\n    Also, folds constant attr lookups and tuples of constants.\n    If verbose is True, prints each substitution as is occurs.\n\n    :param bool builtin_only: only transform builtin objects.\n    :param list stoplist: attribute names to not transform.\n    :param function verbose: logger function which takes in parameter a message\n    \"\"\"\n", "input": "", "output": "\n    if stoplist is None:\n        stoplist = []\n\n    if isinstance(builtin_only, type(make_constants)):\n        raise ValueError(\"The bind_constants decorator must have arguments.\")\n\n    return lambda func: _make_constants(func, builtin_only, stoplist, verbose)", "category": "Python"}, {"instruction": "def run_local(client):\n  \"\"\"Starts a local web server and wait for a redirect.\"\"\"\n", "input": "", "output": "  webbrowser.open(client.get_auth_uri())\n  code = wait_for_redirect()\n  return client.get_token(code)", "category": "Python"}, {"instruction": "def bind_path(self, name, folder):\n        \"\"\"Adds a mask that maps to a given folder relative to `base_path`.\"\"\"\n", "input": "", "output": "        if not len(name) or name[0] != '/' or name[-1] != '/':\n            raise ValueError(\n                \"name must start and end with '/': {0}\".format(name))\n        self._folder_masks.insert(0, (name, folder))", "category": "Python"}, {"instruction": "def createCluster(self, hzVersion, xmlconfig):\n        \"\"\"\n        Parameters:\n         - hzVersion\n         - xmlconfig\n        \"\"\"\n", "input": "", "output": "        self.send_createCluster(hzVersion, xmlconfig)\n        return self.recv_createCluster()", "category": "Python"}, {"instruction": "def indent(text, n=2, ch=\" \"):\n    ''' Indent all the lines in a given block of text by a specified amount.\n\n    Args:\n        text (str) :\n            The text to indent\n\n        n (int, optional) :\n            The amount to indent each line by (default: 2)\n\n        ch (char, optional) :\n            What character to fill the indentation with (default: \" \")\n\n    '''\n", "input": "", "output": "    padding = ch * n\n    return \"\\n\".join(padding+line for line in text.split(\"\\n\"))", "category": "Python"}, {"instruction": "def fov(self):\n        \"\"\"\n        Get the field of view in degrees.\n\n        Returns\n        -------------\n        fov : (2,) float\n          XY field of view in degrees\n        \"\"\"\n", "input": "", "output": "        if self._fov is None:\n            fov = [2.0 * np.degrees(np.arctan((px / 2.0) / f))\n                   for px, f in zip(self._resolution, self._focal)]\n            fov = np.asanyarray(fov, dtype=np.float64)\n            self._fov = fov\n        return self._fov", "category": "Python"}, {"instruction": "def kindex(matrix, k):\n    \"\"\" Returns indices to select the kth nearest neighbour\"\"\"\n", "input": "", "output": "\n    ix = (np.arange(len(matrix)), matrix.argsort(axis=0)[k])\n    return ix", "category": "Python"}, {"instruction": "def _load_serialized_parts(phys_reader, pkg_srels, content_types):\n        \"\"\"\n        Return a list of |_SerializedPart| instances corresponding to the\n        parts in *phys_reader* accessible by walking the relationship graph\n        starting with *pkg_srels*.\n        \"\"\"\n", "input": "", "output": "        sparts = []\n        part_walker = PackageReader._walk_phys_parts(phys_reader, pkg_srels)\n        for partname, blob, srels in part_walker:\n            content_type = content_types[partname]\n            spart = _SerializedPart(partname, content_type, blob, srels)\n            sparts.append(spart)\n        return tuple(sparts)", "category": "Python"}, {"instruction": "def unselect(self):\n        \"\"\"\n        Select a Null bitmasp into this wxDC instance\n        \"\"\"\n", "input": "", "output": "        if sys.platform=='win32':\n            self.dc.SelectObject(wx.NullBitmap)\n            self.IsSelected = False", "category": "Python"}, {"instruction": "def from_json(cls, json, image_config=None):\n        \"\"\"Create a model instance\n\n        Arguments:\n          json (:py:class:`dict`): The parsed JSON data.\n          image_config (:py:class:`dict`): The API image configuration\n            data.\n\n        Returns:\n          :py:class:`BaseModel`: The model instance.\n\n        \"\"\"\n", "input": "", "output": "        cls.image_config = image_config\n        return cls(**{\n            attr: json.get(attr if key is None else key)\n            for attr, key in cls.JSON_MAPPING.items()\n        })", "category": "Python"}, {"instruction": "def geometry_range(crd_range, elev, crd_type):\n    \"\"\"\n    Range of coordinates. (e.g. 2 latitude coordinates, and 0 longitude coordinates)\n    :param crd_range: Latitude or Longitude values\n    :param elev: Elevation value\n    :param crd_type: Coordinate type, lat or lon\n    :return dict:\n    \"\"\"\n", "input": "", "output": "\n    d = OrderedDict()\n    coordinates = [[] for i in range(len(crd_range))]\n\n    # latitude\n    if crd_type == \"lat\":\n        for idx, i in enumerate(crd_range):\n            coordinates[idx] = [crd_range[idx], \"nan\"]\n            if elev:\n                coordinates[idx].append(elev)\n\n    # longitude\n    elif crd_type == \"lon\":\n        for idx, i in enumerate(crd_range):\n            coordinates[idx] = [\"nan\", crd_range[idx]]\n            if elev:\n                coordinates[idx].append(elev)\n\n    d[\"type\"] = \"Range\"\n    d[\"coordinates\"] = coordinates\n\n    return d", "category": "Python"}, {"instruction": "def filter(value):\n    \"\"\"Modifier decorator to force the inclusion or exclusion of an attribute.\n\n    This only modifies the behaviour of the :func:`create_patches` function\n    and the :func:`patches` decorator, given that their parameter\n    ``use_decorators`` is set to ``True``.\n\n    Parameters\n    ----------\n    value : bool\n        ``True`` to force inclusion, ``False`` to force exclusion, and ``None``\n        to inherit from the behaviour defined by :func:`create_patches` or\n        :func:`patches`.\n\n    Returns\n    -------\n    object\n        The decorated object.\n    \"\"\"\n", "input": "", "output": "    def decorator(wrapped):\n        data = get_decorator_data(_get_base(wrapped), set_default=True)\n        data.filter = value\n        return wrapped\n\n    return decorator", "category": "Python"}, {"instruction": "def cmd_kill():\n    \"\"\"Kills all active connections to the specified database(s).\"\"\"\n", "input": "", "output": "    db = connect()\n    count = terminate(db, args.databases)\n    if count == 0:\n        log.error(\"No connections could be killed\")\n        # Return status 1, like killall\n        sys.exit(1)", "category": "Python"}, {"instruction": "def add_hazard_curves(self, root, metadata, data):\n        \"\"\"\n        Add hazard curves stored into `data` as child of the `root`\n        element with `metadata`. See the documentation of the method\n        `serialize` and the constructor for a description of `data`\n        and `metadata`, respectively.\n        \"\"\"\n", "input": "", "output": "        hazard_curves = et.SubElement(root, 'hazardCurves')\n\n        _set_metadata(hazard_curves, metadata, _ATTR_MAP)\n\n        imls_elem = et.SubElement(hazard_curves, 'IMLs')\n        imls_elem.text = ' '.join(map(scientificformat, metadata['imls']))\n        gml_ns = nrml.SERIALIZE_NS_MAP['gml']\n\n        for hc in data:\n            hc_elem = et.SubElement(hazard_curves, 'hazardCurve')\n            gml_point = et.SubElement(hc_elem, '{%s}Point' % gml_ns)\n            gml_pos = et.SubElement(gml_point, '{%s}pos' % gml_ns)\n            gml_pos.text = '%s %s' % (hc.location.x, hc.location.y)\n            poes_elem = et.SubElement(hc_elem, 'poEs')\n            poes_elem.text = ' '.join(map(scientificformat, hc.poes))", "category": "Python"}, {"instruction": "def format_output(instances, flag):\n    \"\"\"return formatted string per instance\"\"\"\n", "input": "", "output": "    out = []\n    line_format = '{0}\\t{1}\\t{2}\\t{3}'\n    name_len = _get_max_name_len(instances) + 3\n    if flag:\n        line_format = '{0:<' + str(name_len+5) + '}{1:<16}{2:<65}{3:<16}'\n\n    for i in instances:\n        endpoint = \"{0}:{1}\".format(i['Endpoint']['Address'], i['Endpoint']['Port'])\n        out.append(\n            line_format.format(i['DBInstanceIdentifier'], i['DBInstanceStatus'], endpoint, i['Engine']))\n    return out", "category": "Python"}, {"instruction": "def ranges_intersect(rset):\n    \"\"\"\n    Recursively calls the range_intersect() - pairwise version.\n\n    >>> ranges_intersect([(48, 65), (45, 55), (50, 56)])\n    [50, 55]\n    \"\"\"\n", "input": "", "output": "    if not rset:\n        return None\n\n    a = rset[0]\n    for b in rset[1:]:\n        if not a:\n            return None\n        a = range_intersect(a, b)\n\n    return a", "category": "Python"}, {"instruction": "def _proxy(self):\n        \"\"\"\n        Generate an instance context for the instance, the context is capable of\n        performing various actions.  All instance actions are proxied to the context\n\n        :returns: ShortCodeContext for this ShortCodeInstance\n        :rtype: twilio.rest.api.v2010.account.short_code.ShortCodeContext\n        \"\"\"\n", "input": "", "output": "        if self._context is None:\n            self._context = ShortCodeContext(\n                self._version,\n                account_sid=self._solution['account_sid'],\n                sid=self._solution['sid'],\n            )\n        return self._context", "category": "Python"}, {"instruction": "def format_datetime(self, format='medium', locale='en_US'):\n        \"\"\"\n        Return a date string formatted to the given pattern.\n\n        .. testsetup::\n\n            from delorean import Delorean\n\n        .. doctest::\n\n            >>> d = Delorean(datetime(2015, 1, 1, 12, 30), timezone='US/Pacific')\n            >>> d.format_datetime(locale='en_US')\n            u'Jan 1, 2015, 12:30:00 PM'\n\n            >>> d.format_datetime(format='long', locale='de_DE')\n            u'1. Januar 2015 12:30:00 -0800'\n\n        :param format: one of \"full\", \"long\", \"medium\", \"short\", or a custom datetime pattern\n        :param locale: a locale identifier\n\n        \"\"\"\n", "input": "", "output": "        return format_datetime(self._dt, format=format, locale=locale)", "category": "Python"}, {"instruction": "def show_vpnservice(vpnservice, profile=None, **kwargs):\n    '''\n    Fetches information of a specific VPN service\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' neutron.show_vpnservice vpnservice-name\n\n    :param vpnservice: ID or name of vpn service to look up\n    :param profile: Profile to build on (Optional)\n    :return: VPN service information\n    '''\n", "input": "", "output": "    conn = _auth(profile)\n    return conn.show_vpnservice(vpnservice, **kwargs)", "category": "Python"}, {"instruction": "def avail_sizes(call=None):\n    '''\n    Return available Linode sizes.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud --list-sizes my-linode-config\n        salt-cloud -f avail_sizes my-linode-config\n    '''\n", "input": "", "output": "    if call == 'action':\n        raise SaltCloudException(\n            'The avail_locations function must be called with -f or --function.'\n        )\n\n    response = _query('avail', 'LinodePlans')\n\n    ret = {}\n    for item in response['DATA']:\n        name = item['LABEL']\n        ret[name] = item\n\n    return ret", "category": "Python"}, {"instruction": "def _enum_member_error(err, eid, name, value, bitmask):\n    \"\"\"Format enum member error.\"\"\"\n", "input": "", "output": "    exception, msg = ENUM_ERROR_MAP[err]\n    enum_name = idaapi.get_enum_name(eid)\n    return exception(('add_enum_member(enum=\"{}\", member=\"{}\", value={}, bitmask=0x{:08X}) '\n                      'failed: {}').format(\n        enum_name,\n        name,\n        value,\n        bitmask,\n        msg\n    ))", "category": "Python"}, {"instruction": "def refresh(self):\n      '''Refetch instance data from the API.\n      '''\n", "input": "", "output": "      # There's no GET endpoint for steps, so get the parent guide and loop\n      # through its steps until we find the right one.\n      response = requests.get('%s/guides/%s' % (API_BASE_URL, self.guideid))\n      attributes = response.json()\n      \n      for step in attributes['steps']:\n         if step['stepid'] == self.stepid:\n            self._update(step)\n            return\n      raise Exception('Step with id %s not found in guide %s.' \\\n                      % (self.stepid, self.guideid))", "category": "Python"}, {"instruction": "def _ProcessImportBySuffix(name, fromlist, globals):\n  \"\"\"Processes an import.\n\n  Calculates the possible names generated from an import and invokes\n  registered callbacks if needed.\n\n  Args:\n    name: Argument as passed to the importer.\n    fromlist: Argument as passed to the importer.\n    globals: Argument as passed to the importer.\n  \"\"\"\n", "input": "", "output": "  _import_local.nest_level -= 1\n\n  # To improve common code path performance, compute the loaded modules only\n  # if there are any import callbacks.\n  if _import_callbacks:\n    # Collect the names of all modules that might be newly loaded as a result\n    # of this import. Add them in a thread-local list.\n    _import_local.names |= _GenerateNames(name, fromlist, globals)\n\n    # Invoke the callbacks only on the top-level import call.\n    if _import_local.nest_level == 0:\n      _InvokeImportCallbackBySuffix(_import_local.names)\n\n  # To be safe, we clear the names set every time we exit a top level import.\n  if _import_local.nest_level == 0:\n    _import_local.names.clear()", "category": "Python"}, {"instruction": "def peek(quantity, min_type=EventType.firstevent, max_type=EventType.lastevent):\n    \"\"\"Return events at the front of the event queue, within the specified minimum and maximum type,\n    and do not remove them from the queue.\n\n    Args:\n        quantity (int): The maximum number of events to return.\n        min_type (int): The minimum value for the event type of the returned events.\n        max_type (int): The maximum value for the event type of the returned events.\n\n    Returns:\n        List[Event]: Events from the front of the event queue.\n\n    Raises:\n        SDLError: If there was an error retrieving the events.\n    \"\"\"\n", "input": "", "output": "\n    return _peep(quantity, lib.SDL_PEEKEVENT, min_type, max_type)", "category": "Python"}, {"instruction": "def htmlParseChunk(self, chunk, size, terminate):\n        \"\"\"Parse a Chunk of memory \"\"\"\n", "input": "", "output": "        ret = libxml2mod.htmlParseChunk(self._o, chunk, size, terminate)\n        return ret", "category": "Python"}, {"instruction": "def send_if_client(fctn):\n    \"\"\"Intercept and send to the server if bundle is in client mode.\"\"\"\n", "input": "", "output": "    @functools.wraps(fctn)\n    def _send_if_client(self, *args, **kwargs):\n        fctn_map = {'set_quantity': 'set_value'}\n        b = self._bundle\n        if b is not None and b.is_client:\n            # TODO: self._filter???\n            # TODO: args???\n            method = fctn_map.get(fctn.__name__, fctn.__name__)\n            d = self._filter if hasattr(self, '_filter') \\\n                else {'twig': self.twig}\n            d['bundleid'] = b._bundleid\n            for k, v in kwargs.items():\n                d[k] = v\n\n            logger.info('emitting to {}({}) to server'.format(method, d))\n            b._socketio.emit(method, d)\n\n            if fctn.__name__ in ['run_compute', 'run_fitting']:\n                # then we're expecting a quick response with an added jobparam\n                # let's add that now\n                self._bundle.client_update()\n        else:\n            return fctn(self, *args, **kwargs)\n    return _send_if_client", "category": "Python"}, {"instruction": "def _pack(self, seq='SEQUNSET'):\n        '''\n        Packs the command into *bytes*\n\n        :param seq: sequence number\n        :rtype: bytes\n        '''\n", "input": "", "output": "\n        return 'AT*{clsname}={seq}{argl_wc}\\r'.format(\n            clsname=type(self).__name__,\n            seq=seq,\n            argl_wc=b''.join(self._iter_packed_with_comma()).decode()\n        ).encode()", "category": "Python"}, {"instruction": "def predict(self, u):\n        '''Predicts the output value at u from the fitted polynomial expansion.\n        Therefore the method train() must be called first.\n\n        :param numpy.ndarray u: input value at which to predict the output.\n\n        :return: q_approx - the predicted value of the output at u\n\n        :rtype: float\n\n        *Sample Usage*::\n\n            >>> thePC = PolySurrogate(dimensions=2)\n            >>> U = thePC.getQuadraturePoints()\n            >>> Q = [myFunc(u) for u in U]\n            >>> thePC.train(U, Q)\n            >>> thePC.predict([0, 1])\n\n        '''\n", "input": "", "output": "        y, ysub = 0, np.zeros(self.N_poly)\n        for ip in range(self.N_poly):\n            inds = tuple(self.index_polys[ip])\n            ysub[ip] = self.coeffs[inds]*eval_poly(u, inds, self.J_list)\n            y += ysub[ip]\n\n        self.response_components = ysub\n        return y", "category": "Python"}, {"instruction": "def record(self):\n        # type: () -> bytes\n        '''\n        Record this Extended Attribute Record.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this Extended Attribute Record.\n        '''\n", "input": "", "output": "        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This XARecord is not yet initialized!')\n\n        return struct.pack(self.FMT, self._group_id, self._user_id,\n                           self._attributes, b'XA', self._filenum, b'\\x00' * 5)", "category": "Python"}, {"instruction": "def _monitor_task(self):\n        \"\"\"Wrapper that handles the actual asynchronous monitoring of the task\n        state.\n\n        \"\"\"\n", "input": "", "output": "        if self.task.state in states.UNREADY_STATES:\n            reactor.callLater(self.POLL_PERIOD, self._monitor_task)\n            return\n\n        if self.task.state == 'SUCCESS':\n            self.callback(self.task.result)\n        elif self.task.state == 'FAILURE':\n            self.errback(Failure(self.task.result))\n        elif self.task.state == 'REVOKED':\n            self.errback(\n                Failure(defer.CancelledError('Task {0}'.format(self.task.id))))\n        else:\n            self.errback(ValueError(\n                'Cannot respond to `{}` state'.format(self.task.state)\n            ))", "category": "Python"}, {"instruction": "def show(self, id):\n        \"\"\"GET /datastores/id: Show a specific item.\"\"\"\n", "input": "", "output": "        # url('DataStores', id=ID)\n        datastore = meta.Session.query(DataStore).get(id)\n\n        # do not raise RuntimeError from discover_datasources\n        # if in \"test\" mode\n        try:\n            datasources = discover_datasources(datastore.ogrstring)\n        except RuntimeError:\n            if \"test\" in request.params:\n                datasources = None\n            else:\n                raise\n\n        result = datastore.to_json()\n        result['datasources'] = datasources\n        return result", "category": "Python"}, {"instruction": "def list_accounts(self):\n        \"\"\"Lists CDN accounts for the active user.\"\"\"\n", "input": "", "output": "\n        account = self.client['Account']\n        mask = 'cdnAccounts[%s]' % ', '.join(['id',\n                                              'createDate',\n                                              'cdnAccountName',\n                                              'cdnSolutionName',\n                                              'cdnAccountNote',\n                                              'status'])\n        return account.getObject(mask=mask).get('cdnAccounts', [])", "category": "Python"}, {"instruction": "def chunkReceived(self, who, chunkNumber, chunkData):\n        \"\"\"\n        A chunk was received from the peer.\n        \"\"\"\n", "input": "", "output": "        def verifyError(error):\n            error.trap(VerifyError)\n            self.nexus.decreaseScore(who, self.authorities)\n        return self.nexus.verifyChunk(self.name,\n                                      who,\n                                      chunkNumber,\n                                      sha.new(chunkData).digest(),\n                                      self.authorities).addCallbacks(\n            lambda whatever: self.chunkVerified(who, chunkNumber, chunkData),\n            verifyError)", "category": "Python"}, {"instruction": "def _wait_for_state_change(self, target_states, update_interval=10):\n        \"\"\"\n        Blocking wait until target_state reached. update_interval is in seconds.\n\n        Warning: state change must begin before calling this method.\n        \"\"\"\n", "input": "", "output": "        while self.state not in target_states:\n            if self.state == 'error':\n                raise Exception('server is in error state')\n\n            # update server state every 10s\n            sleep(update_interval)\n            self.populate()", "category": "Python"}, {"instruction": "def read_long(self, base, offset=0):\n        \"\"\"\n        Return the int value of the four bytes at the file position defined by\n        self._base_offset + *base* + *offset*. If *base* is None, the long is\n        read from the current position in the stream. The endian setting of\n        this instance is used to interpret the byte layout of the long.\n        \"\"\"\n", "input": "", "output": "        fmt = '<L' if self._byte_order is LITTLE_ENDIAN else '>L'\n        return self._read_int(fmt, base, offset)", "category": "Python"}, {"instruction": "def sync_type(ks_name, type_model, connection=None):\n    \"\"\"\n    Inspects the type_model and creates / updates the corresponding type.\n\n    Note that the attributes removed from the type_model are not deleted on the database (this operation is not supported).\n    They become effectively ignored by (will not show up on) the type_model.\n\n    **This function should be used with caution, especially in production environments.\n    Take care to execute schema modifications in a single context (i.e. not concurrently with other clients).**\n\n    *There are plans to guard schema-modifying functions with an environment-driven conditional.*\n    \"\"\"\n", "input": "", "output": "    if not _allow_schema_modification():\n        return\n\n    if not issubclass(type_model, UserType):\n        raise CQLEngineException(\"Types must be derived from base UserType.\")\n\n    _sync_type(ks_name, type_model, connection=connection)", "category": "Python"}, {"instruction": "def closed(self, user):\n        \"\"\" Moved to CLOSED and not later moved to ASSIGNED \"\"\"\n", "input": "", "output": "        decision = False\n        for record in self.history:\n            # Completely ignore older changes\n            if record[\"when\"] < self.options.since.date:\n                continue\n            # Look for status change to CLOSED (unless already found)\n            if not decision and record[\"when\"] < self.options.until.date:\n                for change in record[\"changes\"]:\n                    if (change[\"field_name\"] == \"status\"\n                            and change[\"added\"] == \"CLOSED\"\n                            and record[\"who\"] in [user.email, user.name]):\n                        decision = True\n            # Make sure that the bug has not been later moved from CLOSED.\n            # (This would mean the bug was not closed for a proper reason.)\n            else:\n                for change in record[\"changes\"]:\n                    if (change[\"field_name\"] == \"status\"\n                            and change[\"removed\"] == \"CLOSED\"):\n                        decision = False\n        return decision", "category": "Python"}, {"instruction": "def get_data(self, entity):\n        \"\"\"Return serialized list of data objects on entity that user has `view` permission on.\"\"\"\n", "input": "", "output": "        data = self._filter_queryset('view_data', entity.data.all())\n\n        return self._serialize_data(data)", "category": "Python"}, {"instruction": "def save(self, unsave=False):\n        \"\"\"Save the object.\n\n        :returns: The json response from the server.\n\n        \"\"\"\n", "input": "", "output": "        url = self.reddit_session.config['unsave' if unsave else 'save']\n        data = {'id': self.fullname,\n                'executed': 'unsaved' if unsave else 'saved'}\n        response = self.reddit_session.request_json(url, data=data)\n        self.reddit_session.evict(self.reddit_session.config['saved'])\n        return response", "category": "Python"}, {"instruction": "def subtract(self, route):\n        \"\"\"\n            Remove the route entirely.\n        \"\"\"\n", "input": "", "output": "        for address in self.raw_maps.pop(route, NullHardwareMap()).iterkeys():\n            self.pop(address, NullHardwareNode())", "category": "Python"}, {"instruction": "def get_neighbors(self, site, r):\n        \"\"\"\n        Get all neighbors to a site within a sphere of radius r.  Excludes the\n        site itself.\n\n        Args:\n            site (Site): Site at the center of the sphere.\n            r (float): Radius of sphere.\n\n        Returns:\n            [(site, dist) ...] since most of the time, subsequent processing\n            requires the distance.\n        \"\"\"\n", "input": "", "output": "        nn = self.get_sites_in_sphere(site.coords, r)\n        return [(s, dist) for (s, dist) in nn if site != s]", "category": "Python"}, {"instruction": "def syllabify(word, compound=None):\n    '''Syllabify the given word, whether simplex or complex.'''\n", "input": "", "output": "    if compound is None:\n        compound = bool(re.search(r'(-| |=)', word))\n\n    syllabify = _syllabify_compound if compound else _syllabify\n    syll, rules = syllabify(word)\n\n    yield syll, rules\n\n    n = 7\n\n    if 'T4' in rules:\n        yield syllabify(word, T4=False)\n        n -= 1\n\n    if 'e' in rules:\n        yield syllabify(word, T1E=False)\n        n -= 1\n\n    if 'e' in rules and 'T4' in rules:\n        yield syllabify(word, T4=False, T1E=False)\n        n -= 1\n\n    # yield empty syllabifications and rules\n    for i in range(n):\n        yield '', ''", "category": "Python"}, {"instruction": "def external_commands(self):\n        \"\"\"Get the external commands from the daemon\n\n        Use a lock for this function to protect\n\n        :return: serialized external command list\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        res = []\n        with self.app.external_commands_lock:\n            for cmd in self.app.get_external_commands():\n                res.append(cmd.serialize())\n        return res", "category": "Python"}, {"instruction": "def append(self, *args):\n        \"\"\"\n        add arguments to the set\n        \"\"\"\n", "input": "", "output": "        self.args.append(args)\n        if self.started:\n            self.started = False\n        return self.length()", "category": "Python"}, {"instruction": "def step_note_that(context, remark):\n    \"\"\"\n    Used as generic step that provides an additional remark/hint\n    and enhance the readability/understanding without performing any check.\n\n    .. code-block:: gherkin\n\n        Given that today is \"April 1st\"\n          But note that \"April 1st is Fools day (and beware)\"\n    \"\"\"\n", "input": "", "output": "    log = getattr(context, \"log\", None)\n    if log:\n        log.info(u\"NOTE: %s;\" % remark)", "category": "Python"}, {"instruction": "def restore_from_disk(self, clean_old_snapshot=False):\n        \"\"\"Restore the state of the BF using previous snapshots.\n\n        :clean_old_snapshot: Delete the old snapshot on the disk (period < current - expiration)\n        \"\"\"\n", "input": "", "output": "        base_filename = \"%s/%s_%s_*.dat\" % (self.snapshot_path, self.name, self.expiration)\n        availables_snapshots = glob.glob(base_filename)\n        last_period = self.current_period - dt.timedelta(days=self.expiration-1)\n        for filename in availables_snapshots:\n            snapshot_period = dt.datetime.strptime(filename.split('_')[-1].strip('.dat'), \"%Y-%m-%d\")\n            if snapshot_period <  last_period and not clean_old_snapshot:\n                continue\n            else:\n                self._union_bf_from_file(filename)\n                if snapshot_period == self.current_period:\n                    self._union_bf_from_file(filename, current=True)\n\n            if snapshot_period < last_period and clean_old_snapshot:\n                os.remove(filename)\n        self.ready = True", "category": "Python"}, {"instruction": "def getAnnotations_via_id(self,\n                              annotation_ids,\n                              LIMIT=25,\n                              _print=True,\n                              crawl=False):\n        \"\"\"tids = list of strings or ints that are the ids of the annotations themselves\"\"\"\n", "input": "", "output": "        url_base = self.base_url + \\\n            '/api/1/term/get-annotation/{id}?key=' + self.api_key\n        urls = [\n            url_base.format(id=str(annotation_id))\n            for annotation_id in annotation_ids\n        ]\n        return self.get(urls, LIMIT=LIMIT, _print=_print, crawl=crawl)", "category": "Python"}, {"instruction": "def secure_required(view_func):\n    \"\"\"\n    Decorator to switch an url from http to https.\n\n    If a view is accessed through http and this decorator is applied to that\n    view, than it will return a permanent redirect to the secure (https)\n    version of the same view.\n\n    The decorator also must check that ``USERENA_USE_HTTPS`` is enabled. If\n    disabled, it should not redirect to https because the project doesn't\n    support it.\n\n    \"\"\"\n", "input": "", "output": "    def _wrapped_view(request, *args, **kwargs):\n        if not request.is_secure():\n            if getattr(settings, 'USERENA_USE_HTTPS', userena_settings.DEFAULT_USERENA_USE_HTTPS):\n                request_url = request.build_absolute_uri(request.get_full_path())\n                secure_url = request_url.replace('http://', 'https://')\n                return HttpResponsePermanentRedirect(secure_url)\n        return view_func(request, *args, **kwargs)\n    return wraps(view_func, assigned=available_attrs(view_func))(_wrapped_view)", "category": "Python"}, {"instruction": "def tabbedPane(self, req, tag):\n        \"\"\"\n        Render a tabbed pane tab for each top-level\n        L{xmantissa.ixmantissa.IPreferenceCollection} tab\n        \"\"\"\n", "input": "", "output": "        navigation = webnav.getTabs(self.aggregator.getPreferenceCollections())\n        pages = list()\n        for tab in navigation:\n            f = inevow.IRenderer(\n                    self.aggregator.store.getItemByID(tab.storeID))\n            f.tab = tab\n            if hasattr(f, 'setFragmentParent'):\n                f.setFragmentParent(self)\n            pages.append((tab.name, f))\n\n        f = tabbedPane.TabbedPaneFragment(pages, name='preference-editor')\n        f.setFragmentParent(self)\n        return f", "category": "Python"}, {"instruction": "def download_resource(self, download_url, target, guard):\n        \"\"\" Helper to download and install external resources.\n        \"\"\"\n", "input": "", "output": "        download_url = download_url.strip()\n        if not os.path.isabs(target):\n            target = os.path.join(config.config_dir, target)\n\n        if os.path.exists(os.path.join(target, guard)):\n            self.LOG.info(\"Already have '%s' in '%s'...\" % (download_url, target))\n            return\n\n        if not os.path.isdir(target):\n            os.makedirs(target)\n\n        self.LOG.info(\"Downloading '%s' to '%s'...\" % (download_url, target))\n        with closing(urllib2.urlopen(download_url)) as url_handle:\n            if download_url.endswith(\".zip\"):\n                with closing(ZipFile(StringIO(url_handle.read()))) as zip_handle:  # pylint: disable=no-member\n                    zip_handle.extractall(target)  # pylint: disable=no-member\n            else:\n                with open(os.path.join(target, guard), \"wb\") as file_handle:\n                    shutil.copyfileobj(url_handle, file_handle)", "category": "Python"}, {"instruction": "def incrementKeySequenceCounter(self, iIncrementValue=1):\n        \"\"\"increment the key sequence with a given value\n\n        Args:\n            iIncrementValue: specific increment value to be added\n\n        Returns:\n            True: successful to increment the key sequence with a given value\n            False: fail to increment the key sequence with a given value\n        \"\"\"\n", "input": "", "output": "        print '%s call incrementKeySequenceCounter' % self.port\n        print iIncrementValue\n        currentKeySeq = ''\n        try:\n            currentKeySeq = self.getKeySequenceCounter()\n            keySequence = int(currentKeySeq, 10) + iIncrementValue\n            print keySequence\n            return self.setKeySequenceCounter(keySequence)\n        except Exception, e:\n            ModuleHelper.WriteIntoDebugLogger('incrementKeySequenceCounter() Error: ' + str(e))", "category": "Python"}, {"instruction": "def version_parts(self, best=False):\n        \"\"\"\n        Return the version of the OS distribution, as a tuple of version\n        numbers.\n\n        For details, see :func:`distro.version_parts`.\n        \"\"\"\n", "input": "", "output": "        version_str = self.version(best=best)\n        if version_str:\n            version_regex = re.compile(r'(\\d+)\\.?(\\d+)?\\.?(\\d+)?')\n            matches = version_regex.match(version_str)\n            if matches:\n                major, minor, build_number = matches.groups()\n                return major, minor or '', build_number or ''\n        return '', '', ''", "category": "Python"}, {"instruction": "def _file_stat(self, mode, infostr, stat):\n        \"\"\" update stat regarding to file operation,\n            e.g. open, save, saveas, etc.\n        \"\"\"\n", "input": "", "output": "        action_str = mode.upper()\n        info_str = infostr\n\n        if stat == 'OK':\n            self.action_st_panel.SetBackgroundColour('#00FF00')\n        else:  # ERR\n            self.action_st_panel.SetBackgroundColour('#FF0000')\n        self.action_st.SetLabel(action_str)\n        if info_str is None:\n            info_str = 'Undefined File'\n        self.info_st.SetLabel(info_str)\n        if self.info_st.IsEllipsized():\n            self.info_st.SetToolTip(wx.ToolTip(info_str))\n\n        self.log.append({'stat': stat,\n                         'logstr':\n                             \"[{ts}] {acts:<10s} : {infs}\".format(ts=time.strftime(\n                                 \"%Y/%m/%d-%H:%M:%S\", time.localtime()),\n                                 acts=action_str,\n                                 infs=info_str)})", "category": "Python"}, {"instruction": "def file_name(self, file_type: Optional[FileType] = None) -> str:\n        \"\"\"Get a random file name with some extension.\n\n        :param file_type: Enum object FileType\n        :return: File name.\n\n        :Example:\n            legislative.txt\n        \"\"\"\n", "input": "", "output": "        name = self.__text.word()\n        ext = self.extension(file_type)\n\n        return '{name}{ext}'.format(\n            name=self.__sub(name),\n            ext=ext,\n        )", "category": "Python"}, {"instruction": "def get_text_or_url(args):\n    \"\"\"Determine if we need text or url output\"\"\"\n", "input": "", "output": "    redirect_mode = args.bang or args.search or args.lucky\n    if redirect_mode or args.url:\n        return 'url'\n    else:\n        return 'text'", "category": "Python"}, {"instruction": "def infos(cls, fqdn):\n        \"\"\" Display information about hosted certificates for a fqdn. \"\"\"\n", "input": "", "output": "        if isinstance(fqdn, (list, tuple)):\n            ids = []\n            for fqd_ in fqdn:\n                ids.extend(cls.infos(fqd_))\n            return ids\n\n        ids = cls.usable_id(fqdn)\n        if not ids:\n            return []\n\n        if not isinstance(ids, (list, tuple)):\n            ids = [ids]\n\n        return [cls.info(id_) for id_ in ids]", "category": "Python"}, {"instruction": "def sso_api_list():\n        \"\"\"\n        return sso related API\n        \"\"\"\n", "input": "", "output": "        ssourls = []\n\n        def collect(u, prefixre, prefixname):\n            _prefixname = prefixname + [u._regex, ]\n            urldisplayname = \" \".join(_prefixname)\n\n            if hasattr(u.urlconf_module, \"_MODULE_MAGIC_ID_\") \\\n                    and getattr(u.urlconf_module, \"_MODULE_MAGIC_ID_\") == MAGIC_ID:  # find aap name sso\n                ssourls.append(urldisplayname)\n\n        rooturl = import_by_path(settings.ROOT_URLCONF + \".urlpatterns\")\n        # traverse url matching tree to find the url statement including sso app,\n        # should be 1 element in the ssourls unless you assigned 1+ prefix url for sso app\n        traverse_urls(rooturl, resolverFunc=collect)\n\n        finalQ = Q()  # filter to get full url of all registered sso api\n        for prefix in ssourls:\n            finalQ |= Q(name__startswith=prefix)\n        return APIEntryPoint.objects.filter(finalQ)", "category": "Python"}, {"instruction": "def build_substr_idx(umis, umi_length, min_edit):\n    '''\n    Build a dictionary of nearest neighbours using substrings, can be used\n    to reduce the number of pairwise comparisons.\n    '''\n", "input": "", "output": "    substr_idx = collections.defaultdict(\n        lambda: collections.defaultdict(set))\n    slices = get_substr_slices(umi_length, min_edit + 1)\n    for idx in slices:\n        for u in umis:\n            u_sub = u[slice(*idx)]\n            substr_idx[idx][u_sub].add(u)\n    return substr_idx", "category": "Python"}, {"instruction": "def with_subprocess(cls):\n    \"\"\"a class decorator for Crontabber Apps.  This decorator gives the CronApp\n    a _run_proxy method that will execute the cron app as a single PG\n    transaction.  Commit and Rollback are automatic.  The cron app should do\n    no transaction management of its own.  The cron app should be short so that\n    the transaction is not held open too long.\n    \"\"\"\n", "input": "", "output": "\n    def run_process(self, command, input=None):\n        ", "category": "Python"}, {"instruction": "def people_in_space(self):\n        \"\"\"Number of people in space.\n        Be aware that does not mean these people are inside the ISS (it does\n        most of the time), They can be travelling to and from the station.\n\n        :return: Return a dict with number of people in space right now and\n        their name and their craft\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        data = requests.get('{}{}'.format(self.API_URL, self.API_PEOPLE),\n                            timeout=5)\n\n        if data.status_code is 200:\n            return data.json()\n        else:\n            raise Exception(\"Error server n {}\".format(data.status_code))", "category": "Python"}, {"instruction": "def fetch_num_records(self, table_name, where=None):\n        \"\"\"\n        Fetch the number of records in a table.\n\n        :param str table_name: Table name to get number of records.\n        :param where: |arg_select_where|\n        :type where: |arg_where_type|\n        :return:\n            Number of records in the table.\n            |None| if no value matches the conditions,\n            or the table not found in the database.\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "\n        return self.fetch_value(select=\"COUNT(*)\", table_name=table_name, where=where)", "category": "Python"}, {"instruction": "def getFilenames(self,pixels=None):\n        \"\"\"\n        Return the requested filenames.\n\n        Parameters:\n        -----------\n        pixels : requeseted pixels\n\n        Returns:\n        --------\n        filenames : recarray\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"Getting filenames...\")\n        if pixels is None:\n            return self.filenames\n        else:\n            return self.filenames[np.in1d(self.filenames['pix'],pixels)]", "category": "Python"}, {"instruction": "def get_data_item_by_uuid(self, data_item_uuid: uuid_module.UUID) -> DataItem:\n        \"\"\"Get the data item with the given UUID.\n\n        .. versionadded:: 1.0\n\n        Status: Provisional\n        Scriptable: Yes\n        \"\"\"\n", "input": "", "output": "        data_item = self._document_model.get_data_item_by_uuid(data_item_uuid)\n        return DataItem(data_item) if data_item else None", "category": "Python"}, {"instruction": "def filter_all(self, line):\n        \"\"\"Doing all filters for specified line\n\n        return filtered lines as list\n        For using with adaptive filters\n        \"\"\"\n", "input": "", "output": "        lines = [None] * 5\n        for filter_type in range(5):  # range save more than 'optimised' order\n            res = copyBarray(line)\n            self._filter_scanline(filter_type, line, res)\n            res.insert(0, filter_type)\n            lines[filter_type] = res\n        return lines", "category": "Python"}, {"instruction": "def ReadGRRUser(self, username, cursor=None):\n    \"\"\"Reads a user object corresponding to a given name.\"\"\"\n", "input": "", "output": "    cursor.execute(\n        \"SELECT username, password, ui_mode, canary_mode, user_type \"\n        \"FROM grr_users WHERE username_hash = %s\", [mysql_utils.Hash(username)])\n\n    row = cursor.fetchone()\n    if row is None:\n      raise db.UnknownGRRUserError(username)\n\n    return self._RowToGRRUser(row)", "category": "Python"}, {"instruction": "def uuid_object(title=\"Reference\", description=\"Select an object\", default=None, display=True):\n    \"\"\"Generates a regular expression controlled UUID field\"\"\"\n", "input": "", "output": "\n    uuid = {\n        'pattern': '^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{'\n                   '4}-['\n                   'a-fA-F0-9]{4}-[a-fA-F0-9]{12}$',\n        'type': 'string',\n        'title': title,\n        'description': description,\n    }\n\n    if not display:\n        uuid['x-schema-form'] = {\n            'condition': \"false\"\n        }\n\n    if default is not None:\n        uuid['default'] = default\n\n    return uuid", "category": "Python"}, {"instruction": "def get_apex(self, lat, height=None):\n        \"\"\" Calculate apex height\n\n        Parameters\n        -----------\n        lat : (float)\n            Latitude in degrees\n        height : (float or NoneType)\n            Height above the surface of the earth in km or NoneType to use\n            reference height (default=None)\n\n        Returns\n        ----------\n        apex_height : (float)\n            Height of the field line apex in km\n        \"\"\"\n", "input": "", "output": "        lat = helpers.checklat(lat, name='alat')\n        if height is None:\n            height = self.refh\n\n        cos_lat_squared = np.cos(np.radians(lat))**2\n        apex_height = (self.RE + height) / cos_lat_squared - self.RE\n\n        return apex_height", "category": "Python"}, {"instruction": "def fail(self, message, param=None, ctx=None):\n        \"\"\"Helper method to fail with an invalid value message.\"\"\"\n", "input": "", "output": "        raise BadParameter(message, ctx=ctx, param=param)", "category": "Python"}, {"instruction": "def inline(self) -> str:\n        \"\"\"\n        Return endpoint string\n\n        :return:\n        \"\"\"\n", "input": "", "output": "        inlined = [str(info) for info in (self.server, self.ipv4, self.ipv6, self.port, self.path) if info]\n        return SecuredBMAEndpoint.API + \" \" + \" \".join(inlined)", "category": "Python"}, {"instruction": "def _purge(self):\n        \"\"\"\n        Trim the cache down to max_size by evicting the\n        least-recently-used entries.\n        \"\"\"\n", "input": "", "output": "        if len(self.cache) <= self.max_size:\n            return\n\n        cache = self.cache\n        refcount = self.refcount\n        queue = self.queue\n        max_size = self.max_size\n\n        # purge least recently used entries, using refcount to count entries\n        # that appear multiple times in the queue\n        while len(cache) > max_size:\n            refc = 1\n            while refc:\n                k = queue.popleft()\n                refc = refcount[k] = refcount[k] - 1\n            del cache[k]\n            del refcount[k]", "category": "Python"}, {"instruction": "def _makeIndentAsColumn(self, block, column, offset=0):\n        \"\"\" Make indent equal to column indent.\n        Shiftted by offset\n        \"\"\"\n", "input": "", "output": "        blockText = block.text()\n        textBeforeColumn = blockText[:column]\n        tabCount = textBeforeColumn.count('\\t')\n\n        visibleColumn = column + (tabCount * (self._indenter.width - 1))\n        return self._makeIndentFromWidth(visibleColumn + offset)", "category": "Python"}, {"instruction": "def flushall(args):\n    \"\"\"Execute flushall in all cluster nodes.\n    \"\"\"\n", "input": "", "output": "    cluster = Cluster.from_node(ClusterNode.from_uri(args.cluster))\n    for node in cluster.masters:\n        node.flushall()", "category": "Python"}, {"instruction": "def losc_frame_urls(ifo, start_time, end_time):\n    \"\"\" Get a list of urls to losc frame files\n\n    Parameters\n    ----------\n    ifo: str\n        The name of the IFO to find the information about.\n    start_time: int\n        The gps time in GPS seconds\n    end_time: int\n        The end time in GPS seconds\n\n    Returns\n    -------\n    frame_files: list\n        A dictionary containing information about the files that span the\n        requested times.\n    \"\"\"\n", "input": "", "output": "    data = losc_frame_json(ifo, start_time, end_time)['strain']\n    return [d['url'] for d in data if d['format'] == 'gwf']", "category": "Python"}, {"instruction": "def remove_sites_from_neighbours( self, remove_labels ):\n        \"\"\"\n        Removes sites from the set of neighbouring sites if these have labels in remove_labels.\n\n        Args:\n            Remove_labels (List) or (Str): List of Site labels to be removed from the cluster neighbour set.\n\n        Returns:\n            None\n        \"\"\"\n", "input": "", "output": "        if type( remove_labels ) is str:\n            remove_labels = [ remove_labels ]\n        self.neighbours = set( n for n in self.neighbours if n.label not in remove_labels )", "category": "Python"}, {"instruction": "def get_point_name(self, context):\n        \"\"\" Get point name.\n\n        Parameters\n        ----------\n        context     : ???\n            ???\n\n        Returns\n        -------\n        ???\n            ???\n\n        \"\"\"\n", "input": "", "output": "        \n        metadata_table = self.parse_context(context)\n        return metadata_table.apply(self.strip_point_name, axis=1)", "category": "Python"}, {"instruction": "def create_and_register_access_db(filename: str,\n                                  dsn: str,\n                                  description: str) -> bool:\n    \"\"\"\n    (Windows only.)\n    Creates a Microsoft Access database and registers it with ODBC.\n\n    Args:\n        filename: filename of the database to create\n        dsn: ODBC data source name to create\n        description: description of the database\n\n    Returns:\n        bool: was the DSN created?\n    \"\"\"\n", "input": "", "output": "    fullfilename = os.path.abspath(filename)\n    create_string = fullfilename + \" General\"\n    # ... filename, space, sort order (\"General\" for English)\n    return (create_user_dsn(access_driver, CREATE_DB=create_string) and\n            register_access_db(filename, dsn, description))", "category": "Python"}, {"instruction": "def _get_png_size(version, scale, quiet_zone=4):\n    \"\"\"See: QRCode.get_png_size\n\n    This function was abstracted away from QRCode to allow for the output of\n    QR codes during the build process, i.e. for debugging. It works\n    just the same except you must specify the code's version. This is needed\n    to calculate the PNG's size.\n    \"\"\"\n", "input": "", "output": "    #Formula: scale times number of modules plus the border on each side\n    return (int(scale) * tables.version_size[version]) + (2 * quiet_zone * int(scale))", "category": "Python"}, {"instruction": "def restore_state(self):\n        \"\"\"Read last state of GUI from configuration file.\n\n        .. versionadded: 3.3\n        \"\"\"\n", "input": "", "output": "        settings = QSettings()\n        try:\n            last_path = settings.value('directory', type=str)\n        except TypeError:\n            last_path = ''\n        self.output_directory.setText(last_path)", "category": "Python"}, {"instruction": "def get_mem_total(self):\n        \"\"\"Calculate the total memory in the current service unit.\"\"\"\n", "input": "", "output": "        with open('/proc/meminfo') as meminfo_file:\n            for line in meminfo_file:\n                key, mem = line.split(':', 2)\n                if key == 'MemTotal':\n                    mtot, modifier = mem.strip().split(' ')\n                    return '%s%s' % (mtot, modifier[0].upper())", "category": "Python"}, {"instruction": "def make_madry_ngpu(nb_classes=10, input_shape=(None, 28, 28, 1), **kwargs):\n  \"\"\"\n  Create a multi-GPU model similar to Madry et al. (arXiv:1706.06083).\n  \"\"\"\n", "input": "", "output": "  layers = [Conv2DnGPU(32, (5, 5), (1, 1), \"SAME\"),\n            ReLU(),\n            MaxPool((2, 2), (2, 2), \"SAME\"),\n            Conv2DnGPU(64, (5, 5), (1, 1), \"SAME\"),\n            ReLU(),\n            MaxPool((2, 2), (2, 2), \"SAME\"),\n            Flatten(),\n            LinearnGPU(1024),\n            ReLU(),\n            LinearnGPU(nb_classes),\n            Softmax()]\n\n  model = MLPnGPU(nb_classes, layers, input_shape)\n  return model", "category": "Python"}, {"instruction": "def subtract_months(self, months: int) -> datetime:\n        \"\"\" Subtracts a number of months from the current value \"\"\"\n", "input": "", "output": "        self.value = self.value - relativedelta(months=months)\n        return self.value", "category": "Python"}, {"instruction": "def update_flagfile(flags_path, new_threshold):\n    \"\"\"Updates the flagfile at `flags_path`, changing the value for\n    `resign_threshold` to `new_threshold`\n    \"\"\"\n", "input": "", "output": "    if abs(new_threshold) > 1:\n        raise ValueError(\"Invalid new percentile for resign threshold\")\n    with tf.gfile.GFile(flags_path) as f:\n        lines = f.read()\n    if new_threshold > 0:\n        new_threshold *= -1\n    if not RESIGN_FLAG_REGEX.search(lines):\n        print(\"Resign threshold flag not found in flagfile {}!  Aborting.\".format(flags_path))\n        sys.exit(1)\n    old_threshold = RESIGN_FLAG_REGEX.search(lines).groups(1)\n    lines = re.sub(RESIGN_FLAG_REGEX, \"--resign_threshold={:.3f}\".format(new_threshold), lines)\n\n    if abs(float(old_threshold[0]) - new_threshold) < 0.001:\n        print(\"Not updating percentiles; {} ~= {:.3f}\".format(\n                old_threshold[0], new_threshold), flush=True)\n    else:\n        print(\"Updated percentile from {} to {:.3f}\".format(\n                old_threshold[0], new_threshold), flush=True)\n        with tf.gfile.GFile(flags_path, 'w') as f:\n            f.write(lines)", "category": "Python"}, {"instruction": "def extract_code_from_function(function):\n    \"\"\"Return code handled by function.\"\"\"\n", "input": "", "output": "    if not function.__name__.startswith('fix_'):\n        return None\n\n    code = re.sub('^fix_', '', function.__name__)\n    if not code:\n        return None\n\n    try:\n        int(code[1:])\n    except ValueError:\n        return None\n\n    return code", "category": "Python"}, {"instruction": "def summary(self):\n        \"\"\"The summary line of the docstring.\n\n        The summary line is the first non-empty line, as-per :pep:`257`.\n        This will be the empty string if the object does not have a docstring.\n\n        :type: str\n        \"\"\"\n", "input": "", "output": "        for line in self.docstring.splitlines():\n            line = line.strip()\n            if line:\n                return line\n\n        return \"\"", "category": "Python"}, {"instruction": "def generate_conf_file(argv: List[str]) -> bool:\n    \"\"\"\n    Convert a set of FHIR resources into their corresponding i2b2 counterparts.\n\n    :param argv: Command line arguments.  See: create_parser for details\n    :return:\n    \"\"\"\n", "input": "", "output": "    parser = ArgumentParser(description=\"Generate SQL db_conf file template\")\n    parser.add_argument(\"-f\", \"--configfile\", help=\"File name to generate (Default: db_conf)\", metavar=\"Config File\",\n                        default=\"db_conf\")\n    opts = parser.parse_args(argv)\n    if os.path.exists(opts.configfile):\n        print(f\"{opts.configfile} already exists!\")\n        return False\n    with open(opts.configfile, 'w') as f:\n        f.write(conf_template)\n    print(f\"{opts.configfile} generated\")\n    return True", "category": "Python"}, {"instruction": "def _bucket_exists(self):\n        \"\"\"Check if the bucket exists.\"\"\"\n", "input": "", "output": "        try:\n            self.s3client.get_bucket_location(Bucket=self.bucket)\n            return True\n        except ClientError as error:\n            LOG.error(error)\n            return False", "category": "Python"}, {"instruction": "def _flush_queue(self, q, ignore_priority=False):\n        \"\"\"\n        :param q: PriorityQueue instance holding GarbageCollector entries\n        :param ignore_priority: If True - all GarbageCollector entries should be resubmitted\n                If False - only those entries whose waiting time has expired will be resubmitted\n        \"\"\"\n", "input": "", "output": "        assert isinstance(q, PriorityQueue)\n\n        current_timestamp = compute_release_time(lag_in_minutes=0)\n        for _ in range(len(q)):\n            entry = q.pop()\n            assert isinstance(entry, PriorityEntry)\n\n            if ignore_priority or entry.release_time < current_timestamp:\n                self._resubmit_uow(entry.entry)\n            else:\n                q.put(entry)\n                break", "category": "Python"}, {"instruction": "def _is_logged_in_with_confirmed_email(user_manager):\n    \"\"\"| Returns True if user is logged in and has a confirmed email address.\n    | Returns False otherwise.\n    \"\"\"\n", "input": "", "output": "    # User must be logged in\n    if user_manager.call_or_get(current_user.is_authenticated):\n        # Is unconfirmed email allowed for this view by @allow_unconfirmed_email?\n        unconfirmed_email_allowed = \\\n            getattr(g, '_flask_user_allow_unconfirmed_email', False)\n        \n        # unconfirmed_email_allowed must be True or\n        # User must have at least one confirmed email address\n        if unconfirmed_email_allowed or user_manager.db_manager.user_has_confirmed_email(current_user):\n            return True\n\n    return False", "category": "Python"}, {"instruction": "def push_build_set(id, tag_prefix):\n    \"\"\"\n    Push build set to Brew\n    \"\"\"\n", "input": "", "output": "    req = swagger_client.BuildConfigSetRecordPushRequestRest()\n    req.tag_prefix = tag_prefix\n    req.build_config_set_record_id = id\n    response = utils.checked_api_call(pnc_api.build_push, 'push_record_set', body=req)\n    if response:\n        return utils.format_json_list(response)", "category": "Python"}, {"instruction": "def _read_hdf_columns(path_or_buf, columns, num_splits, kwargs):  # pragma: no cover\n    \"\"\"Use a Ray task to read columns from HDF5 into a Pandas DataFrame.\n\n    Note: Ray functions are not detected by codecov (thus pragma: no cover)\n\n    Args:\n        path_or_buf: The path of the HDF5 file.\n        columns: The list of column names to read.\n        num_splits: The number of partitions to split the column into.\n\n    Returns:\n         A list containing the split Pandas DataFrames and the Index as the last\n            element. If there is not `index_col` set, then we just return the length.\n            This is used to determine the total length of the DataFrame to build a\n            default Index.\n    \"\"\"\n", "input": "", "output": "\n    df = pandas.read_hdf(path_or_buf, columns=columns, **kwargs)\n    # Append the length of the index here to build it externally\n    return _split_result_for_readers(0, num_splits, df) + [len(df.index)]", "category": "Python"}, {"instruction": "def strategyKLogN(kls, n, k=4):\n\t\t\"\"\"Return the directory names to preserve under the KLogN purge strategy.\"\"\"\n", "input": "", "output": "\t\tassert(k>1)\n\t\ts = set([n])\n\t\ti = 0\n\t\t\n\t\twhile k**i <= n:\n\t\t\ts.update(range(n, n-k*k**i, -k**i))\n\t\t\ti += 1\n\t\t\tn -= n % k**i\n\t\t\n\t\treturn set(map(str, filter(lambda x:x>=0, s)))", "category": "Python"}, {"instruction": "def after_websocket(self, func: Callable, name: AppOrBlueprintKey=None) -> Callable:\n        \"\"\"Add an after websocket function.\n\n        This is designed to be used as a decorator. An example usage,\n\n        .. code-block:: python\n\n            @app.after_websocket\n            def func(response):\n                return response\n\n        Arguments:\n            func: The after websocket function itself.\n            name: Optional blueprint key name.\n        \"\"\"\n", "input": "", "output": "        handler = ensure_coroutine(func)\n        self.after_websocket_funcs[name].append(handler)\n        return func", "category": "Python"}, {"instruction": "def get_stp_brief_info_output_last_instance_instance_id(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_stp_brief_info = ET.Element(\"get_stp_brief_info\")\n        config = get_stp_brief_info\n        output = ET.SubElement(get_stp_brief_info, \"output\")\n        last_instance = ET.SubElement(output, \"last-instance\")\n        instance_id = ET.SubElement(last_instance, \"instance-id\")\n        instance_id.text = kwargs.pop('instance_id')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def reset(ctx):\n    \"\"\"\n    Reset OpenPGP application.\n\n    This action will wipe all OpenPGP data, and set all PINs to their default\n    values.\n    \"\"\"\n", "input": "", "output": "    click.echo(\"Resetting OpenPGP data, don't remove your YubiKey...\")\n    ctx.obj['controller'].reset()\n    click.echo('Success! All data has been cleared and default PINs are set.')\n    echo_default_pins()", "category": "Python"}, {"instruction": "def enabled(name, runas=None):\n    '''\n    Check if the specified service is enabled\n\n    :param str name: The name of the service to look up\n\n    :param str runas: User to run launchctl commands\n\n    :return: True if the specified service enabled, otherwise False\n    :rtype: bool\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.enabled org.cups.cupsd\n    '''\n", "input": "", "output": "    # Try to list the service.  If it can't be listed, it's not enabled\n    try:\n        list_(name=name, runas=runas)\n        return True\n    except CommandExecutionError:\n        return False", "category": "Python"}, {"instruction": "def _GetTimelineStatEntries(client_id, file_path, with_history=True):\n  \"\"\"Gets timeline entries from the appropriate data source (AFF4 or REL_DB).\"\"\"\n", "input": "", "output": "\n  if data_store.RelationalDBEnabled():\n    fn = _GetTimelineStatEntriesRelDB\n  else:\n    fn = _GetTimelineStatEntriesLegacy\n\n  for v in fn(client_id, file_path, with_history=with_history):\n    yield v", "category": "Python"}, {"instruction": "def NewDefaultServicePeriod(self):\n    \"\"\"Create a new ServicePeriod object, make it the default service period and\n    return it. The default service period is used when you create a trip without\n    providing an explict service period. \"\"\"\n", "input": "", "output": "    service_period = self._gtfs_factory.ServicePeriod()\n    service_period.service_id = util.FindUniqueId(self.service_periods)\n    # blank service won't validate in AddServicePeriodObject\n    self.SetDefaultServicePeriod(service_period, validate=False)\n    return service_period", "category": "Python"}, {"instruction": "def isel_points(self, dim='points', **indexers):\n        \"\"\"Return a new DataArray whose dataset is given by pointwise integer\n        indexing along the specified dimension(s).\n\n        See Also\n        --------\n        Dataset.isel_points\n        \"\"\"\n", "input": "", "output": "        ds = self._to_temp_dataset().isel_points(dim=dim, **indexers)\n        return self._from_temp_dataset(ds)", "category": "Python"}, {"instruction": "def loads(astring):\n        \"\"\"Decompress and deserialize string into Python object via marshal.\"\"\"\n", "input": "", "output": "        try:\n            return marshal.loads(zlib.decompress(astring))\n        except zlib.error as e:\n            raise SerializerError(\n                'Cannot decompress object (\"{}\")'.format(str(e))\n            )\n        except Exception as e:\n            # marshal module does not provide a proper Exception model\n            raise SerializerError(\n                'Cannot restore object (\"{}\")'.format(str(e))\n            )", "category": "Python"}, {"instruction": "def is_char_in_pairs(pos_char, pairs):\r\n        \"\"\"Return True if the charactor is in pairs of brackets or quotes.\"\"\"\n", "input": "", "output": "        for pos_left, pos_right in pairs.items():\r\n            if pos_left < pos_char < pos_right:\r\n                return True\r\n\r\n        return False", "category": "Python"}, {"instruction": "def validate_config(self):\n        '''validate_config\n\n        High-level api: Validate config against models. ConfigError is raised\n        if the config has issues.\n\n        Returns\n        -------\n\n        None\n            There is no return of this method.\n\n        Raises\n        ------\n\n        ConfigError\n            If config contains error.\n        '''\n", "input": "", "output": "\n        self.roots\n        for child in self.ele.getchildren():\n            self._validate_node(child)", "category": "Python"}, {"instruction": "def _IsMariaDB(cursor):\n  \"\"\"Checks if we are running against MariaDB.\"\"\"\n", "input": "", "output": "  for variable in [\"version\", \"version_comment\"]:\n    cursor.execute(\"SHOW VARIABLES LIKE %s;\", (variable,))\n    version = cursor.fetchone()\n    if version and \"MariaDB\" in version[1]:\n      return True\n  return False", "category": "Python"}, {"instruction": "def encode(self,\n               data: mx.sym.Symbol,\n               data_length: Optional[mx.sym.Symbol],\n               seq_len: int) -> Tuple[mx.sym.Symbol, mx.sym.Symbol, int]:\n        \"\"\"\n        Encodes data given sequence lengths of individual examples and maximum sequence length.\n        :param data: Input data.\n        :param data_length: Vector with sequence lengths.\n        :param seq_len: Maximum sequence length.\n        :return: Expected number of empty states (zero-filled).\n        \"\"\"\n", "input": "", "output": "        # outputs: (batch_size, seq_len, num_hidden)\n        outputs = mx.sym.dot(data, mx.sym.zeros((self.num_embed, self.num_hidden)))\n        return outputs, data_length, seq_len", "category": "Python"}, {"instruction": "def insert(self, idx, w, comment=''):\n        '''insert a waypoint'''\n", "input": "", "output": "        if idx >= self.count():\n            self.add(w, comment)\n            return\n        if idx < 0:\n            return\n        w = copy.copy(w)\n        if comment:\n            w.comment = comment\n        w.seq = idx\n        self.wpoints.insert(idx, w)\n        self.last_change = time.time()\n        self.reindex()", "category": "Python"}, {"instruction": "def default_arguments(cls):\n        \"\"\"Returns the available kwargs of the called class\"\"\"\n", "input": "", "output": "        func = cls.__init__\n        args = func.__code__.co_varnames\n        defaults = func.__defaults__\n        index = -len(defaults)\n        return {k: v for k, v in zip(args[index:], defaults)}", "category": "Python"}, {"instruction": "def attendance_locked(self):\n        \"\"\"Is it past 10PM on the day of the block?\"\"\"\n", "input": "", "output": "        now = datetime.datetime.now()\n        return now.date() > self.date or (now.date() == self.date and now.time() > datetime.time(settings.ATTENDANCE_LOCK_HOUR, 0))", "category": "Python"}, {"instruction": "def _GetSubFileEntries(self):\n    \"\"\"Retrieves sub file entries.\n\n    Yields:\n      ZipFileEntry: a sub file entry.\n    \"\"\"\n", "input": "", "output": "    if self._directory is None:\n      self._directory = self._GetDirectory()\n\n    zip_file = self._file_system.GetZipFile()\n    if self._directory and zip_file:\n      for path_spec in self._directory.entries:\n        location = getattr(path_spec, 'location', None)\n        if location is None:\n          continue\n\n        kwargs = {}\n        try:\n          kwargs['zip_info'] = zip_file.getinfo(location[1:])\n        except KeyError:\n          kwargs['is_virtual'] = True\n\n        yield ZipFileEntry(\n            self._resolver_context, self._file_system, path_spec, **kwargs)", "category": "Python"}, {"instruction": "def stage_tc_create_tag(self, tag, resource):\n        \"\"\"Add a tag to a resource.\n\n        Args:\n            tag (str): The tag to be added to the resource.\n            resource (obj): An instance of tcex resource class.\n        \"\"\"\n", "input": "", "output": "        tag_resource = resource.tags(self.tcex.safetag(tag))\n        tag_resource.http_method = 'POST'\n        t_response = tag_resource.request()\n        if t_response.get('status') != 'Success':\n            self.log.warning(\n                '[tcex] Failed adding tag \"{}\" ({}).'.format(tag, t_response.get('response').text)\n            )", "category": "Python"}, {"instruction": "def parse_address(netloc, default_port=8000):\n    '''Parse an internet address ``netloc`` and return a tuple with\n``host`` and ``port``.'''\n", "input": "", "output": "    if isinstance(netloc, tuple):\n        if len(netloc) != 2:\n            raise ValueError('Invalid address %s' % str(netloc))\n        return netloc\n    #\n    netloc = native_str(netloc)\n    auth = None\n    # Check if auth is available\n    if '@' in netloc:\n        auth, netloc = netloc.split('@')\n    if netloc.startswith(\"unix:\"):\n        host = netloc.split(\"unix:\")[1]\n        return '%s@%s' % (auth, host) if auth else host\n    # get host\n    if '[' in netloc and ']' in netloc:\n        host = netloc.split(']')[0][1:].lower()\n    elif ':' in netloc:\n        host = netloc.split(':')[0].lower()\n    elif netloc == \"\":\n        host = \"0.0.0.0\"\n    else:\n        host = netloc.lower()\n    # get port\n    netloc = netloc.split(']')[-1]\n    if \":\" in netloc:\n        port = netloc.split(':', 1)[1]\n        if not port.isdigit():\n            raise ValueError(\"%r is not a valid port number.\" % port)\n        port = int(port)\n    else:\n        port = default_port\n    return ('%s@%s' % (auth, host) if auth else host, port)", "category": "Python"}, {"instruction": "def add_filter(self, filter_or_string, *args, **kwargs):\n        \"\"\"\n        Appends a filter.\n        \"\"\"\n", "input": "", "output": "        self.filters.append(build_filter(filter_or_string, *args, **kwargs))\n\n        return self", "category": "Python"}, {"instruction": "def download_raw_report_object_to_file(self, key, file):\n        \"\"\"\n        Download a raw report object and store it in a file.\n\n        :param key: The key of the report object\n        :param file: A file-like object to store the report object.\n        \"\"\"\n", "input": "", "output": "        con = ConnectionManager().get_connection(self._connection_alias)\n        return con.download_to_file(self.raw_report_objects[key], file, append_base_url=False)", "category": "Python"}, {"instruction": "def get_point_cloud(self, pair):\n        \"\"\"Get 3D point cloud from image pair.\"\"\"\n", "input": "", "output": "        disparity = self.block_matcher.get_disparity(pair)\n        points = self.block_matcher.get_3d(disparity,\n                                           self.calibration.disp_to_depth_mat)\n        colors = cv2.cvtColor(pair[0], cv2.COLOR_BGR2RGB)\n        return PointCloud(points, colors)", "category": "Python"}, {"instruction": "def _publish_status(self, port):\n        '''\n        Publish status for specified port.\n\n        Parameters\n        ----------\n        port : str\n            Device name/port.\n        '''\n", "input": "", "output": "        if port not in self.open_devices:\n            status = {}\n        else:\n            device = self.open_devices[port].serial\n            properties = ('port', 'baudrate', 'bytesize', 'parity', 'stopbits',\n                          'timeout', 'xonxoff', 'rtscts', 'dsrdtr')\n            status = {k: getattr(device, k) for k in properties}\n        status_json = json.dumps(status)\n        self.mqtt_client.publish(topic='serial_device/%s/status' % port,\n                                 payload=status_json, retain=True)", "category": "Python"}, {"instruction": "def use_plenary_authorization_view(self):\n        \"\"\"Pass through to provider AuthorizationLookupSession.use_plenary_authorization_view\"\"\"\n", "input": "", "output": "        self._object_views['authorization'] = PLENARY\n        # self._get_provider_session('authorization_lookup_session') # To make sure the session is tracked\n        for session in self._get_provider_sessions():\n            try:\n                session.use_plenary_authorization_view()\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def _audience_condition_deserializer(obj_dict):\n  \"\"\" Deserializer defining how dict objects need to be decoded for audience conditions.\n\n  Args:\n    obj_dict: Dict representing one audience condition.\n\n  Returns:\n    List consisting of condition key with corresponding value, type and match.\n  \"\"\"\n", "input": "", "output": "  return [\n    obj_dict.get('name'),\n    obj_dict.get('value'),\n    obj_dict.get('type'),\n    obj_dict.get('match')\n  ]", "category": "Python"}, {"instruction": "def set_exception(self, exception):\n        \"\"\"Set the result of the future to the given exception.\n\n        Args:\n            exception (:exc:`Exception`): The exception raised.\n        \"\"\"\n", "input": "", "output": "        # Sanity check: A future can only complete once.\n        if self.done():\n            raise RuntimeError(\"set_exception can only be called once.\")\n\n        # Set the exception and trigger the future.\n        self._exception = exception\n        self._trigger()", "category": "Python"}, {"instruction": "def userToJson(user):\n    \"\"\"Returns a serializable User dict\n\n    :param user: User to get info for\n    :type user: User\n    :returns: dict\n    \"\"\"\n", "input": "", "output": "    obj = {\n        'id': user.id,\n        'username': user.username,\n        'name': user.get_full_name(),\n        'email': user.email,\n    }\n\n    return obj", "category": "Python"}, {"instruction": "def rept(ctx, text, number_times):\n    \"\"\"\n    Repeats text a given number of times\n    \"\"\"\n", "input": "", "output": "    if number_times < 0:\n        raise ValueError(\"Number of times can't be negative\")\n    return conversions.to_string(text, ctx) * conversions.to_integer(number_times, ctx)", "category": "Python"}, {"instruction": "async def add(self, setname, ip, timeout):\n        \"\"\"\n        Adds the given IP address to the specified set.\n\n        If timeout is specified, the IP will stay in the set for the given\n        duration. Else it will stay in the set during the set default timeout.\n\n        timeout must be given in seconds.\n\n        The resulting command looks like this:\n\n        ``nft add element inet firewall ellis_blacklist4 { 192.0.2.10 timeout 30s }``\n\n        \"\"\"\n", "input": "", "output": "        # We have to double-quote the '{' '}' at both ends for `format` to work.\n        if timeout > 0:\n            to_ban = \"{{ {0} timeout {1}s }}\".format(ip, timeout)\n        else:\n            to_ban = \"{{\u00a0{0} }}\".format(ip)\n\n        args = ['add', 'element', self.table_family, self.table_name, setname, to_ban]\n\n        return await self.start(__class__.CMD, *args)", "category": "Python"}, {"instruction": "def _print_header(data):\n    \"\"\"\n    Create vcf header to make\n    a valid vcf.\n    \"\"\"\n", "input": "", "output": "    print(\"##fileformat=VCFv4.2\", file=STDOUT, end=\"\")\n    print(\"##source=seqbuster2.3\", file=STDOUT, end=\"\")\n    print(\"##reference=mirbase\", file=STDOUT, end=\"\")\n    for pos in data:\n        print(\"##contig=<ID=%s>\" % pos[\"chrom\"], file=STDOUT, end=\"\")\n    print('##INFO=<ID=ID,Number=1,Type=String,Description=\"miRNA name\">', file=STDOUT, end=\"\")\n    print('##FORMAT=<ID=GT,Number=1,Type=Integer,Description=\"Genotype\">', file=STDOUT, end=\"\")\n    print('##FORMAT=<ID=NR,Number=A,Type=Integer,Description=\"Total reads supporting the variant\">', file=STDOUT, end=\"\")\n    print('##FORMAT=<ID=NS,Number=A,Type=Float,Description=\"Total number of different sequences supporting the variant\">', file=STDOUT, end=\"\")\n    print(\"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\tSAMP001\", file=STDOUT, end=\"\")", "category": "Python"}, {"instruction": "def update_record(self, identifier, rtype=None, name=None, content=None, **kwargs):\n        \"\"\"\n        Update a record. Identifier must be specified.\n        \"\"\"\n", "input": "", "output": "        if not rtype and kwargs.get('type'):\n            warnings.warn('Parameter \"type\" is deprecated, use \"rtype\" instead.',\n                          DeprecationWarning)\n            rtype = kwargs.get('type')\n\n        return self._update_record(identifier, rtype=rtype, name=name, content=content)", "category": "Python"}, {"instruction": "def macd(series, fast=3, slow=10, smooth=16):\n    \"\"\"\n    compute the MACD (Moving Average Convergence/Divergence)\n    using a fast and slow exponential moving avg'\n    return value is emaslow, emafast, macd which are len(x) arrays\n    \"\"\"\n", "input": "", "output": "    macd_line = rolling_weighted_mean(series, window=fast) - \\\n        rolling_weighted_mean(series, window=slow)\n    signal = rolling_weighted_mean(macd_line, window=smooth)\n    histogram = macd_line - signal\n    # return macd_line, signal, histogram\n    return pd.DataFrame(index=series.index, data={\n        'macd': macd_line.values,\n        'signal': signal.values,\n        'histogram': histogram.values\n    })", "category": "Python"}, {"instruction": "def main():\n    \"\"\"From: http://stackoverflow.com/questions/20801034/how-to-measure-download-speed-and-progress-using-requests\"\"\"\n", "input": "", "output": "    # Prepare.\n    if os.name == 'nt':\n        locale.setlocale(locale.LC_ALL, 'english-us')\n    else:\n        locale.resetlocale()\n    response = requests.get(OPTIONS['<url>'], stream=True)\n    content_length = None if OPTIONS['--ignore-length'] else int(response.headers.get('Content-Length'))\n    progress_bar = ProgressBarWget(content_length, eta_every=4)\n    thread = DownloadThread(response)\n    print_every_seconds = 0.25\n\n    # Download.\n    thread.start()\n    while True:\n        progress_bar.numerator = thread.bytes_downloaded\n        print(progress_bar, end='\\r')\n        sys.stdout.flush()\n\n        # For undefined downloads (no content-length), check if thread has stopped. Loop only checks defined downloads.\n        if not thread.isAlive():\n            progress_bar.force_done = True\n            break\n        if progress_bar.done:\n            break\n\n        time.sleep(print_every_seconds)\n    print(progress_bar)", "category": "Python"}, {"instruction": "def preview(num):\n    \"\"\"Prints the text of a problem.\"\"\"\n", "input": "", "output": "    # Define problem_text before echoing in case problem does not exist\n    problem_text = Problem(num).text\n    click.secho(\"Project Euler Problem %i\" % num, bold=True)\n    click.echo(problem_text)", "category": "Python"}, {"instruction": "def _get_pwm_values(self, brightness=None, color=None):\n        \"\"\"\n        Get the pwm values for a specific state of the led.\n\n        If a state argument is omitted, current value is used.\n\n        :param brightness: The brightness of the state.\n        :param color: The color of the state.\n        :return: The pwm values.\n        \"\"\"\n", "input": "", "output": "        if brightness is None:\n            brightness = self.brightness\n        if color is None:\n            color = self.color\n\n        return [(x / 255) * brightness for x in self._rgb_to_rgbw(color)]", "category": "Python"}, {"instruction": "def get_mapping_registry(content_type):\n    \"\"\"\n    Returns the data element registry for the given content type (a Singleton).\n\n    :Note: This only works after a representer for the given content type\n        has been created.\n    \"\"\"\n", "input": "", "output": "    reg = get_current_registry()\n    rpr_reg = reg.queryUtility(IRepresenterRegistry)\n    return rpr_reg.get_mapping_registry(content_type)", "category": "Python"}, {"instruction": "def _remove_duplicates(self):\n        \"\"\"\n        Remove every maximal rectangle contained by another one.\n        \"\"\"\n", "input": "", "output": "        contained = set()\n        for m1, m2 in itertools.combinations(self._max_rects, 2):\n            if m1.contains(m2):\n                contained.add(m2)\n            elif m2.contains(m1):\n                contained.add(m1)\n        \n        # Remove from max_rects\n        self._max_rects = [m for m in self._max_rects if m not in contained]", "category": "Python"}, {"instruction": "def get_type_data(name):\n    \"\"\"Return dictionary representation of type.\n\n    Can be used to initialize primordium.type.primitives.Type\n\n    \"\"\"\n", "input": "", "output": "    name = name.upper()\n    if name in ISO_LANGUAGE_CODES:\n        name = ISO_LANGUAGE_CODES[name]\n    if name in ISO_MAJOR_LANGUAGE_TYPES:\n        namespace = '639-2'\n        lang_name = ISO_MAJOR_LANGUAGE_TYPES[name]\n    elif name in ISO_OTHER_LANGUAGE_TYPES:\n        namespace = '639-3'\n        lang_name = ISO_OTHER_LANGUAGE_TYPES[name]\n    else:\n        raise NotFound('Language Type: ' + name)\n\n    return {\n        'authority': 'ISO',\n        'namespace': namespace,\n        'identifier': name,\n        'domain': 'DisplayText Languages',\n        'display_name': lang_name + ' Language Type',\n        'display_label': lang_name,\n        'description': ('The display text language type for the ' +\n                        lang_name + ' language.')\n    }", "category": "Python"}, {"instruction": "def container_exists(self, id=None, name=None):\n        \"\"\"\n        Checks if container exists already\n        \"\"\"\n", "input": "", "output": "        exists = False\n        if id and self.container_by_id(id):\n            exists = True\n        elif name and self.container_by_name(name):\n            exists = True\n\n        return exists", "category": "Python"}, {"instruction": "def get_json(self, nb=0):\n        \"\"\"Get the history as a dict of list (with list JSON compliant)\"\"\"\n", "input": "", "output": "        return {i: self.stats_history[i].history_json(nb=nb) for i in self.stats_history}", "category": "Python"}, {"instruction": "def copy_files(files_type, class_dir, output):\n    \"\"\"Copies the files from the input folder to the output folder\n    \"\"\"\n", "input": "", "output": "    # get the last part within the file\n    class_name = path.split(class_dir)[1]\n    for (files, folder_type) in files_type:\n        full_path = path.join(output, folder_type, class_name)\n\n        pathlib.Path(full_path).mkdir(\n            parents=True, exist_ok=True)\n        for f in files:\n            shutil.copy2(f, full_path)", "category": "Python"}, {"instruction": "def temp_files(self):\n        \"\"\"Return a list of the temporary files produced by this link.\n\n        This returns all files that were explicitly marked for removal.\n        \"\"\"\n", "input": "", "output": "        ret_list = []\n        for key, val in self.file_dict.items():\n            # For temp files we only want files that were marked for removal\n            if val & FileFlags.rm_mask:\n                ret_list.append(key)\n        return ret_list", "category": "Python"}, {"instruction": "def _get_default_bins(self):\n        \"\"\"Ported from the C++ function InitDefaultBucketsInner() in the following file.\n        https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/lib/histogram/histogram.cc\n        See the following tutorial for more details on how TensorFlow initialize bin distribution.\n        https://www.tensorflow.org/programmers_guide/tensorboard_histograms\"\"\"\n", "input": "", "output": "        if self._default_bins is None:\n            v = 1E-12\n            buckets = []\n            neg_buckets = []\n            while v < 1E20:\n                buckets.append(v)\n                neg_buckets.append(-v)\n                v *= 1.1\n            self._default_bins = neg_buckets[::-1] + [0] + buckets\n        return self._default_bins", "category": "Python"}, {"instruction": "def get_transform_version(transform):\n    \"\"\"\n    Internal API: Returns the version of the transform function based on the transform function's signature. Currently,\n    only two versions are supported (2 and 3). This is what version 2 transform functions look like:\n\n    def transform(request, response):\n        ...\n\n    Version 3 transforms have the additional config variable like so:\n\n    def transform(request, response, config):\n        ...\n\n    Or can have a varargs parameter as a third argument:\n\n    def transform(request, response, *args):\n        ...\n\n    In both cases, version 3 transforms will be passed a local copy of the canari configuration object as the third\n    argument. However, in the latter example, the configuration object will be stored in a tuple (i.e. (config,)).\n    \"\"\"\n", "input": "", "output": "\n    if sys.version_info[0] > 3:\n        spec = inspect.getfullargspec(transform)\n    else:\n        spec = inspect.getargspec(transform)\n\n    if spec.varargs:\n        return 3\n\n    n = len(spec.args)\n\n    if 2 <= n <= 3:\n        return n\n\n    raise Exception('Could not determine transform version.')", "category": "Python"}, {"instruction": "def rows(self):\n        '''Iterate over queryset objects'''\n", "input": "", "output": "        return (self.nested_row(o, n)\n                for o in self.queryset\n                for n in getattr(o, self.attribute, []))", "category": "Python"}, {"instruction": "def get_path(self, x: int, y: int) -> List[Tuple[int, int]]:\n        \"\"\"Return a list of (x, y) steps to reach the goal point, if possible.\n        \"\"\"\n", "input": "", "output": "        lib.TCOD_dijkstra_path_set(self._path_c, x, y)\n        path = []\n        pointer_x = ffi.new(\"int[2]\")\n        pointer_y = pointer_x + 1\n        while lib.TCOD_dijkstra_path_walk(self._path_c, pointer_x, pointer_y):\n            path.append((pointer_x[0], pointer_y[0]))\n        return path", "category": "Python"}, {"instruction": "def _parse_tensor(self, indices=False):\n        '''Parse a tensor.'''\n", "input": "", "output": "        if indices:\n            self.line = self._skip_lines(1)\n\n        tensor = np.zeros((3, 3))\n        for i in range(3):\n            tokens = self.line.split()\n            if indices:\n                tensor[i][0] = float(tokens[1])\n                tensor[i][1] = float(tokens[2])\n                tensor[i][2] = float(tokens[3])\n            else:\n                tensor[i][0] = float(tokens[0])\n                tensor[i][1] = float(tokens[1])\n                tensor[i][2] = float(tokens[2])\n            self.line = self._skip_lines(1)\n        return tensor", "category": "Python"}, {"instruction": "def populate_readme(\n    version, circleci_build, appveyor_build, coveralls_build, travis_build\n):\n    \"\"\"Populates ``README.rst`` with release-specific data.\n\n    This is because ``README.rst`` is used on PyPI.\n\n    Args:\n        version (str): The current version.\n        circleci_build (Union[str, int]): The CircleCI build ID corresponding\n            to the release.\n        appveyor_build (str): The AppVeyor build ID corresponding to the\n            release.\n        coveralls_build (Union[str, int]): The Coveralls.io build ID\n            corresponding to the release.\n        travis_build (int): The Travis CI build ID corresponding to\n            the release.\n    \"\"\"\n", "input": "", "output": "    with open(RELEASE_README_FILE, \"r\") as file_obj:\n        template = file_obj.read()\n    contents = template.format(\n        version=version,\n        circleci_build=circleci_build,\n        appveyor_build=appveyor_build,\n        coveralls_build=coveralls_build,\n        travis_build=travis_build,\n    )\n    with open(README_FILE, \"w\") as file_obj:\n        file_obj.write(contents)", "category": "Python"}, {"instruction": "def count_ruptures(self):\n        \"\"\"\n        See\n        :meth:`openquake.hazardlib.source.base.BaseSeismicSource.count_ruptures`\n        for description of parameters and return value.\n        \"\"\"\n", "input": "", "output": "        return (len(self.get_annual_occurrence_rates()) *\n                len(self.nodal_plane_distribution.data) *\n                len(self.hypocenter_distribution.data))", "category": "Python"}, {"instruction": "def next_page(self):\n        \"\"\"Move the screen page down through the history buffer.\"\"\"\n", "input": "", "output": "        if self.history.position < self.history.size and self.history.bottom:\n            mid = min(len(self.history.bottom),\n                      int(math.ceil(self.lines * self.history.ratio)))\n\n            self.history.top.extend(self.buffer[y] for y in range(mid))\n            self.history = self.history \\\n                ._replace(position=self.history.position + mid)\n\n            for y in range(self.lines - mid):\n                self.buffer[y] = self.buffer[y + mid]\n            for y in range(self.lines - mid, self.lines):\n                self.buffer[y] = self.history.bottom.popleft()\n\n            self.dirty = set(range(self.lines))", "category": "Python"}, {"instruction": "def get_query_param_from_url(url):\n    \"\"\"\n    \u4ece url \u4e2d\u83b7\u53d6 query \u53c2\u6570\u5b57\u5178\n\n    :param:\n        * url: (string) \u9700\u8981\u83b7\u53d6\u53c2\u6570\u5b57\u5178\u7684 url\n\n    :return:\n        * query_dict: (dict) query \u53c2\u6570\u7684\u6709\u5e8f\u5b57\u5178\uff0c\u5b57\u5178\u7684\u503c\u4e3a query \u503c\u7ec4\u6210\u7684\u5217\u8868\n\n    \u4e3e\u4f8b\u5982\u4e0b::\n\n        print('--- get_query_param_from_url demo---')\n        url = 'http://localhost:8811/mytest?page_number=1&page_size=10'\n        query_dict = get_query_param_from_url(url)\n        print(query_dict['page_size'])\n        print('---')\n\n    \u6267\u884c\u7ed3\u679c::\n\n        --- get_query_param_from_url demo---\n        ['10']\n        ---\n\n    \"\"\"\n", "input": "", "output": "    url_obj = urlsplit(url)\n    query_dict = parse_qs(url_obj.query)\n    \n    return OrderedDict(query_dict)", "category": "Python"}, {"instruction": "def _compile_type(self, schema):\n        \"\"\" Compile type schema: plain type matching \"\"\"\n", "input": "", "output": "        # Prepare self\n        self.compiled_type = const.COMPILED_TYPE.TYPE\n        self.name = get_type_name(schema)\n\n        # Error partials\n        err_type = self.Invalid(_(u'Wrong type'), self.name)\n\n        # Type check function\n        if six.PY2 and schema is basestring:\n            # Relaxed rule for Python2 basestring\n            typecheck = lambda v: isinstance(v, schema)\n        else:\n            # Strict type check for everything else\n            typecheck = lambda v: type(v) == schema\n\n        # Matcher\n        if self.matcher:\n            def match_type(v):\n                return typecheck(v), v\n            return match_type\n\n        # Validator\n        def validate_type(v):\n            # Type check\n            if not typecheck(v):\n                # expected=<type>, provided=<type>\n                raise err_type(get_type_name(type(v)))\n            # Fine\n            return v\n\n        return validate_type", "category": "Python"}, {"instruction": "def get_archive_type(path):\n    \"\"\"\n    Returns the contents type for the provided archive path.\n\n    Parameters\n    ----------\n    path : string\n        Directory to evaluate.\n\n    Returns\n    -------\n    Returns a string of: sframe, sgraph, raises TypeError for anything else\n    \"\"\"\n", "input": "", "output": "    if not is_directory_archive(path):\n        raise TypeError('Unable to determine the type of archive at path: %s' % path)\n\n    try:\n        ini_path = '/'.join([_convert_slashes(path), 'dir_archive.ini'])\n        parser = _ConfigParser.SafeConfigParser()\n        parser.read(ini_path)\n\n        contents = parser.get('metadata', 'contents')\n        return contents\n    except Exception as e:\n        raise TypeError('Unable to determine type of archive for path: %s' % path, e)", "category": "Python"}, {"instruction": "def search_next(self):\n        \"\"\"\n        Searchs the next search pattern in the document.\n\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        pattern = self.get_selected_text() or self.__search_pattern\n        if not pattern:\n            return False\n\n        return self.search(pattern, **{\"case_sensitive\": True,\n                                       \"whole_word\": False,\n                                       \"regular_expressions\": False,\n                                       \"backward_search\": False,\n                                       \"wrap_around\": True})", "category": "Python"}, {"instruction": "def _fill_text(self, text, width, indent):\n        \"\"\"Wraps text like HelpFormatter, but doesn't squash lines\n\n        This makes it easier to do lists and paragraphs.\n\n        \"\"\"\n", "input": "", "output": "        parts = text.split('\\n\\n')\n        for i, part in enumerate(parts):\n            # Check to see if it's a bulleted list--if so, then fill each line\n            if part.startswith('* '):\n                subparts = part.split('\\n')\n                for j, subpart in enumerate(subparts):\n                    subparts[j] = super(WrappedTextHelpFormatter, self)._fill_text(\n                        subpart, width, indent\n                    )\n                parts[i] = '\\n'.join(subparts)\n            else:\n                parts[i] = super(WrappedTextHelpFormatter, self)._fill_text(part, width, indent)\n\n        return '\\n\\n'.join(parts)", "category": "Python"}, {"instruction": "def readBimFile(basefilename):\n    \"\"\"\n    Helper fuinction that reads bim files\n    \"\"\"\n", "input": "", "output": "    # read bim file\n    bim_fn = basefilename+'.bim'\n    rv = SP.loadtxt(bim_fn,delimiter='\\t',usecols = (0,3),dtype=int)\n    return rv", "category": "Python"}, {"instruction": "def _update_params(self, constants):\n        \"\"\"Update the params.\"\"\"\n", "input": "", "output": "        constants = np.max(np.min(constants, 1))\n        self.params['r']['value'] = max([self.params['r']['value'],\n                                         constants])\n        epsilon = constants / self.params['r']['value']\n        influence = self._calculate_influence(epsilon)\n        # Account for learning rate\n        return influence * epsilon", "category": "Python"}, {"instruction": "def _on_connection(self, data, unique_id):\n        \"\"\"Called on collection operation\n\n        :param data: Received data\n        \"\"\"\n", "input": "", "output": "        if unique_id is None:\n            unique_id = self.stream_unique_id\n        self.connection_id = data.get('connectionId')\n        logger.info('[Connect: %s]: connection_id: %s' % (unique_id, self.connection_id))", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Closes the record and index files.\"\"\"\n", "input": "", "output": "        if not self.is_open:\n            return\n        super(IndexCreator, self).close()\n        self.fidx.close()", "category": "Python"}, {"instruction": "def write_pretty_dict_str(out, obj, indent=2):\n    \"\"\"writes JSON indented representation of `obj` to `out`\"\"\"\n", "input": "", "output": "    json.dump(obj,\n              out,\n              indent=indent,\n              sort_keys=True,\n              separators=(',', ': '),\n              ensure_ascii=False,\n              encoding=\"utf-8\")", "category": "Python"}, {"instruction": "def whowas(self, nick):\n        \"\"\"\n        Runs a WHOWAS on someone.\n        Required arguments:\n        * nick - Nick to run a WHOWAS on.\n        Returns a list:\n           [0] The user's nick.\n           [1] The user's ident.\n           [2] The user's host.\n           [3] The user's real name.\n        \"\"\"\n", "input": "", "output": "        with self.lock:\n            self.send('WHOWAS %s' % nick)\n\n            rwhowas = []\n            while self.readable():\n                msg = self._recv(expected_replies=('314', '312', '369'))\n                if msg[0] == '314':\n                    raw_whowas = msg[2].split()\n                    rwhowas = raw_whowas[0], raw_whowas[1], \\\n                        raw_whowas[2], raw_whowas[4][1:]\n                elif msg[0] == '312':\n                    pass\n                elif msg[0] == '369':\n                    return rwhowas", "category": "Python"}, {"instruction": "def parse_yaml(self, y):\n        '''Parse a YAML specification of a wait_time condition into this\n        object.\n\n        '''\n", "input": "", "output": "        super(WaitTime, self).parse_yaml(y)\n        self.wait_time = int(y['condition']['waitTime']['waitTime'])\n        return self", "category": "Python"}, {"instruction": "def clean(self, value):\n        \"\"\"clean a value, converting and performing bounds checking\"\"\"\n", "input": "", "output": "        if not isinstance(value, self.t):\n            value = self.t(value)\n\n        if not self.allow_negative and value < 0:\n            raise ValueError('value was negative')\n\n        if not self.allow_positive and value > 0:\n            raise ValueError('values was positive')\n\n        return value", "category": "Python"}, {"instruction": "def has_role(self, role_s):\n        \"\"\"\n        :param role_s: 1..N role identifiers (strings)\n        :type role_s:  Set of Strings\n\n        :returns: a set of tuple(s), containing the role and a Boolean\n                  indicating whether the user is a member of the Role\n        \"\"\"\n", "input": "", "output": "        if self.authorized:\n            return self.security_manager.has_role(self.identifiers, role_s)\n        msg = 'Cannot check permission when identifiers aren\\'t set!'\n        raise ValueError(msg)", "category": "Python"}, {"instruction": "def dump(obj, fp, **kw):\n    r\"\"\"Dump python object to file.\n\n    >>> import lazyxml\n    >>> data = {'demo': {'foo': 1, 'bar': 2}}\n    >>> lazyxml.dump(data, 'dump.xml')\n    >>> with open('dump-fp.xml', 'w') as fp:\n    >>>     lazyxml.dump(data, fp)\n\n    >>> from cStringIO import StringIO\n    >>> data = {'demo': {'foo': 1, 'bar': 2}}\n    >>> buffer = StringIO()\n    >>> lazyxml.dump(data, buffer)\n    >>> buffer.getvalue()\n    <?xml version=\"1.0\" encoding=\"utf-8\"?><demo><foo><![CDATA[1]]></foo><bar><![CDATA[2]]></bar></demo>\n    >>> buffer.close()\n\n    .. note::\n        ``kw`` argument have the same meaning as in :func:`dumps`\n\n    :param obj: data for dump to xml.\n    :param fp: a filename or a file or file-like object that support ``.write()`` to write the xml content\n\n    .. versionchanged:: 1.2\n        The `fp` is a filename of string before this. It can now be a file or file-like object that support ``.write()`` to write the xml content.\n    \"\"\"\n", "input": "", "output": "    xml = dumps(obj, **kw)\n    if isinstance(fp, basestring):\n        with open(fp, 'w') as fobj:\n            fobj.write(xml)\n    else:\n        fp.write(xml)", "category": "Python"}, {"instruction": "def doeigs_s(tau, Vdirs):\n    \"\"\"\n     get elements of s from eigenvaulues - note that this is very unstable\n     Input:\n         tau,V:\n           tau is an list of eigenvalues in decreasing order:\n              [t1,t2,t3]\n           V is an list of the eigenvector directions\n              [[V1_dec,V1_inc],[V2_dec,V2_inc],[V3_dec,V3_inc]]\n    Output:\n        The six tensor elements as a list:\n          s=[x11,x22,x33,x12,x23,x13]\n\n    \"\"\"\n", "input": "", "output": "    t = np.zeros((3, 3,), 'f')  # initialize the tau diagonal matrix\n    V = []\n    for j in range(3):\n        t[j][j] = tau[j]  # diagonalize tau\n    for k in range(3):\n        V.append(dir2cart([Vdirs[k][0], Vdirs[k][1], 1.0]))\n    V = np.transpose(V)\n    tmp = np.dot(V, t)\n    chi = np.dot(tmp, np.transpose(V))\n    return a2s(chi)", "category": "Python"}, {"instruction": "def get_map_data(self):\n        \"\"\"\n        Returns a serializable data set describing the map location\n        \"\"\"\n", "input": "", "output": "\n        return {\n            'containerSelector': '#' + self.get_map_element_id(),\n            'center': self.map_center_description,\n            'marker': self.map_marker_description or self.map_center_description,\n            'zoom': self.map_zoom,\n            'href': self.get_map_href(),\n            'key': getattr(settings, 'GOOGLE_MAPS_API_KEY', ''),\n            # Python's line-splitting is more cross-OS compatible, so we feed\n            # a pre-built array to the front-end\n            'description': [\n                line for line in self.map_description.splitlines() if line\n            ],\n        }", "category": "Python"}, {"instruction": "def exhaust_stream(f):\n    \"\"\"Helper decorator for methods that exhausts the stream on return.\"\"\"\n", "input": "", "output": "\n    def wrapper(self, stream, *args, **kwargs):\n        try:\n            return f(self, stream, *args, **kwargs)\n        finally:\n            exhaust = getattr(stream, \"exhaust\", None)\n            if exhaust is not None:\n                exhaust()\n            else:\n                while 1:\n                    chunk = stream.read(1024 * 64)\n                    if not chunk:\n                        break\n\n    return update_wrapper(wrapper, f)", "category": "Python"}, {"instruction": "def getInitialLiveForms(self):\n        \"\"\"\n        Make and return as many L{LiveForm} instances as are necessary to hold\n        our default values.\n\n        @return: some subforms.\n        @rtype: C{list} of L{LiveForm}\n        \"\"\"\n", "input": "", "output": "        liveForms = []\n        if self._defaultStuff:\n            for values in self._defaultStuff:\n                liveForms.append(self._makeDefaultLiveForm(values))\n        else:\n            # or only one, for the first new thing\n            liveForms.append(\n                self._makeALiveForm(\n                    self.parameters, self._newIdentifier(), False))\n        return liveForms", "category": "Python"}, {"instruction": "def to_choices_dict(choices):\n    \"\"\"\n    Convert choices into key/value dicts.\n\n    pairwise_choices([1]) -> {1: 1}\n    pairwise_choices([(1, '1st'), (2, '2nd')]) -> {1: '1st', 2: '2nd'}\n    pairwise_choices([('Group', ((1, '1st'), 2))]) -> {'Group': {1: '1st', 2: '2nd'}}\n    \"\"\"\n", "input": "", "output": "    # Allow single, paired or grouped choices style:\n    # choices = [1, 2, 3]\n    # choices = [(1, 'First'), (2, 'Second'), (3, 'Third')]\n    # choices = [('Category', ((1, 'First'), (2, 'Second'))), (3, 'Third')]\n    ret = OrderedDict()\n    for choice in choices:\n        if (not isinstance(choice, (list, tuple))):\n            # single choice\n            ret[choice] = choice\n        else:\n            key, value = choice\n            if isinstance(value, (list, tuple)):\n                # grouped choices (category, sub choices)\n                ret[key] = to_choices_dict(value)\n            else:\n                # paired choice (key, display value)\n                ret[key] = value\n    return ret", "category": "Python"}, {"instruction": "def process_set(line, annotations):\n    \"\"\"Convert annotations into nanopub_bel annotations format\"\"\"\n", "input": "", "output": "\n    matches = re.match('SET\\s+(\\w+)\\s*=\\s*\"?(.*?)\"?\\s*$', line)\n\n    key = None\n    if matches:\n        key = matches.group(1)\n        val = matches.group(2)\n\n    if key == \"STATEMENT_GROUP\":\n        annotations[\"statement_group\"] = val\n    elif key == \"Citation\":\n        annotations[\"citation\"] = process_citation(val)\n    elif key.lower() == \"support\" or key.lower() == \"evidence\":\n        annotations[\"evidence\"] = val\n    elif re.match(\"\\s*{.*?}\", val):\n        vals = convert_csv_str_to_list(val)\n        annotations[key] = vals\n    else:\n        annotations[key] = val\n\n    return annotations", "category": "Python"}, {"instruction": "def setnx(self, key, value):\n        \"\"\"Set the value of ``key`` to ``value`` if key doesn't exist\"\"\"\n", "input": "", "output": "        return self.set(key, value, nx=True)", "category": "Python"}, {"instruction": "def _loglikelihood(self, y, mu, weights=None):\n        \"\"\"\n        compute the log-likelihood of the dataset using the current model\n\n        Parameters\n        ---------\n        y : array-like of shape (n,)\n            containing target values\n        mu : array-like of shape (n_samples,)\n            expected value of the targets given the model and inputs\n        weights : array-like of shape (n,), optional\n            containing sample weights\n\n        Returns\n        -------\n        log-likelihood : np.array of shape (n,)\n            containing log-likelihood scores\n        \"\"\"\n", "input": "", "output": "        return self.distribution.log_pdf(y=y, mu=mu, weights=weights).sum()", "category": "Python"}, {"instruction": "def strip_rightmost(self):\n        \"\"\"\n        Strip the rightmost part of the language range. If the new rightmost\n        part is a singleton or ``x`` (i.e. starts an extension or private use\n        part), it is also stripped.\n\n        Return the newly created :class:`LanguageRange`.\n        \"\"\"\n", "input": "", "output": "\n        parts = self.print_str.split(\"-\")\n        parts.pop()\n        if parts and len(parts[-1]) == 1:\n            parts.pop()\n        return type(self).fromstr(\"-\".join(parts))", "category": "Python"}, {"instruction": "def slug(hans, style=Style.NORMAL, heteronym=False, separator='-',\n         errors='default', strict=True):\n    \"\"\"\u751f\u6210 slug \u5b57\u7b26\u4e32.\n\n    :param hans: \u6c49\u5b57\n    :type hans: unicode or list\n    :param style: \u6307\u5b9a\u62fc\u97f3\u98ce\u683c\uff0c\u9ed8\u8ba4\u662f :py:attr:`~pypinyin.Style.NORMAL` \u98ce\u683c\u3002\n                  \u66f4\u591a\u62fc\u97f3\u98ce\u683c\u8be6\u89c1 :class:`~pypinyin.Style`\n    :param heteronym: \u662f\u5426\u542f\u7528\u591a\u97f3\u5b57\n    :param separstor: \u4e24\u4e2a\u62fc\u97f3\u95f4\u7684\u5206\u9694\u7b26/\u8fde\u63a5\u7b26\n    :param errors: \u6307\u5b9a\u5982\u4f55\u5904\u7406\u6ca1\u6709\u62fc\u97f3\u7684\u5b57\u7b26\uff0c\u8be6\u60c5\u8bf7\u53c2\u8003\n                   :py:func:`~pypinyin.pinyin`\n    :param strict: \u662f\u5426\u4e25\u683c\u9075\u7167\u300a\u6c49\u8bed\u62fc\u97f3\u65b9\u6848\u300b\u6765\u5904\u7406\u58f0\u6bcd\u548c\u97f5\u6bcd\uff0c\u8be6\u89c1 :ref:`strict`\n    :return: slug \u5b57\u7b26\u4e32.\n\n    :raise AssertionError: \u5f53\u4f20\u5165\u7684\u5b57\u7b26\u4e32\u4e0d\u662f unicode \u5b57\u7b26\u65f6\u4f1a\u629b\u51fa\u8fd9\u4e2a\u5f02\u5e38\n\n    ::\n\n      >>> import pypinyin\n      >>> from pypinyin import Style\n      >>> pypinyin.slug('\u4e2d\u56fd\u4eba')\n      'zhong-guo-ren'\n      >>> pypinyin.slug('\u4e2d\u56fd\u4eba', separator=' ')\n      'zhong guo ren'\n      >>> pypinyin.slug('\u4e2d\u56fd\u4eba', style=Style.FIRST_LETTER)\n      'z-g-r'\n      >>> pypinyin.slug('\u4e2d\u56fd\u4eba', style=Style.CYRILLIC)\n      '\u0447\u0436\u0443\u043d1-\u0433\u043e2-\u0436\u044d\u043d\u044c2'\n    \"\"\"\n", "input": "", "output": "    return separator.join(chain(*pinyin(hans, style=style, heteronym=heteronym,\n                                        errors=errors, strict=strict)\n                                ))", "category": "Python"}, {"instruction": "def parent_dir(path):\n    '''Return the parent of a directory.'''\n", "input": "", "output": "    return os.path.abspath(os.path.join(path, os.pardir, os.pardir, '_build'))", "category": "Python"}, {"instruction": "def field2parameter(self, field, name=\"body\", default_in=\"body\"):\n        \"\"\"Return an OpenAPI parameter as a `dict`, given a marshmallow\n        :class:`Field <marshmallow.Field>`.\n\n        https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#parameterObject\n        \"\"\"\n", "input": "", "output": "        location = field.metadata.get(\"location\", None)\n        prop = self.field2property(field)\n        return self.property2parameter(\n            prop,\n            name=name,\n            required=field.required,\n            multiple=isinstance(field, marshmallow.fields.List),\n            location=location,\n            default_in=default_in,\n        )", "category": "Python"}, {"instruction": "def get_new_actions(self):\n        \"\"\" Wrapper function for do_get_new_actions\n        For stats purpose\n\n        :return: None\n        TODO: Use a decorator for timing this function\n        \"\"\"\n", "input": "", "output": "        try:\n            _t0 = time.time()\n            self.do_get_new_actions()\n            statsmgr.timer('actions.got.time', time.time() - _t0)\n        except RuntimeError:\n            logger.error(\"Exception like issue #1007\")", "category": "Python"}, {"instruction": "def get_structure(atoms, cls=None):\n        \"\"\"\n        Returns pymatgen structure from ASE Atoms.\n\n        Args:\n            atoms: ASE Atoms object\n            cls: The Structure class to instantiate (defaults to pymatgen structure)\n\n        Returns:\n            Equivalent pymatgen.core.structure.Structure\n        \"\"\"\n", "input": "", "output": "        symbols = atoms.get_chemical_symbols()\n        positions = atoms.get_positions()\n        lattice = atoms.get_cell()\n\n        cls = Structure if cls is None else cls\n        return cls(lattice, symbols, positions,\n                   coords_are_cartesian=True)", "category": "Python"}, {"instruction": "def Back(self, n = 1, dl = 0):\n        \"\"\"\u9000\u683c\u952en\u6b21\n        \"\"\"\n", "input": "", "output": "        self.Delay(dl)\n        self.keyboard.tap_key(self.keyboard.backspace_key, n)", "category": "Python"}, {"instruction": "def read_short(source, offset):\n    \"\"\"Reads a number from a byte array.\n\n    :param bytes source: Source byte string\n    :param int offset: Point in byte string to start reading\n    :returns: Read number and offset at point after read data\n    :rtype: tuple of ints\n    :raises: SerializationError if unable to unpack\n    \"\"\"\n", "input": "", "output": "    try:\n        (short,) = struct.unpack_from(\">H\", source, offset)\n        return short, offset + struct.calcsize(\">H\")\n    except struct.error:\n        raise SerializationError(\"Bad format of serialized context.\")", "category": "Python"}, {"instruction": "def _reduce_op(self, op, result=None):\n        '''\n        TODO: support for multidimensional arrays.\n        '''\n", "input": "", "output": "        template = 'result(for({arr},merger[{type}, {op}], |b, i, e| merge(b, e)))'\n        self.weldobj.weld_code = template.format(arr = self.weldobj.weld_code,\n                                                 type = self._weld_type.__str__(),\n                                                 op  = op)\n        return self._eval(restype = self._weld_type)", "category": "Python"}, {"instruction": "def create_index(self, index, index_type=GEO2D):\n        \"\"\"Create an index on a given attribute\n\n        :param str index: Attribute to set index on\n        :param str index_type: See PyMongo index types for further information, defaults to GEO2D index.\n        \"\"\"\n", "input": "", "output": "        self.logger.info(\"Adding %s index to stores on attribute: %s\" % (index_type, index))\n        yield self.collection.create_index([(index, index_type)])", "category": "Python"}, {"instruction": "def _proxy(self):\n        \"\"\"\n        Generate an instance context for the instance, the context is capable of\n        performing various actions.  All instance actions are proxied to the context\n\n        :returns: ActivityContext for this ActivityInstance\n        :rtype: twilio.rest.taskrouter.v1.workspace.activity.ActivityContext\n        \"\"\"\n", "input": "", "output": "        if self._context is None:\n            self._context = ActivityContext(\n                self._version,\n                workspace_sid=self._solution['workspace_sid'],\n                sid=self._solution['sid'],\n            )\n        return self._context", "category": "Python"}, {"instruction": "def _write_new_chunk(self):\n        \"\"\"\n        Called to request a new chunk of data to be read from the Crazyflie\n        \"\"\"\n", "input": "", "output": "        # Figure out the length of the next request\n        new_len = len(self._data)\n        if new_len > _WriteRequest.MAX_DATA_LENGTH:\n            new_len = _WriteRequest.MAX_DATA_LENGTH\n\n        logger.debug('Writing new chunk of {}bytes at 0x{:X}'.format(\n            new_len, self._current_addr))\n\n        data = self._data[:new_len]\n        self._data = self._data[new_len:]\n\n        pk = CRTPPacket()\n        pk.set_header(CRTPPort.MEM, CHAN_WRITE)\n        pk.data = struct.pack('<BI', self.mem.id, self._current_addr)\n        # Create a tuple used for matching the reply using id and address\n        reply = struct.unpack('<BBBBB', pk.data)\n        self._sent_reply = reply\n        # Add the data\n        pk.data += struct.pack('B' * len(data), *data)\n        self._sent_packet = pk\n        self.cf.send_packet(pk, expected_reply=reply, timeout=1)\n\n        self._addr_add = len(data)", "category": "Python"}, {"instruction": "def add_section(self, section):\n        \"\"\"\n        If section exists, returns log,\n        otherwise adds section and returns list of sections.\n        \"\"\"\n", "input": "", "output": "        # check if section already exists\n        if not self.config.has_section(section):\n            self.config.add_section(section)\n            # return updated sections\n            return (True, self.config.sections())\n        return (False, 'Section: ' + section + ' already exists')", "category": "Python"}, {"instruction": "def extend(self, new_leaves: List[bytes]):\n        \"\"\"Extend this tree with new_leaves on the end.\n\n        The algorithm works by using _push_subtree() as a primitive, calling\n        it with the maximum number of allowed leaves until we can add the\n        remaining leaves as a valid entire (non-full) subtree in one go.\n        \"\"\"\n", "input": "", "output": "        size = len(new_leaves)\n        final_size = self.tree_size + size\n        idx = 0\n        while True:\n            # keep pushing subtrees until mintree_size > remaining\n            max_h = self.__mintree_height\n            max_size = 1 << (max_h - 1) if max_h > 0 else 0\n            if max_h > 0 and size - idx >= max_size:\n                self._push_subtree(new_leaves[idx:idx + max_size])\n                idx += max_size\n            else:\n                break\n        # fill in rest of tree in one go, now that we can\n        if idx < size:\n            root_hash, hashes = self.__hasher._hash_full(new_leaves, idx, size)\n            self._update(final_size, self.hashes + hashes)\n        assert self.tree_size == final_size", "category": "Python"}, {"instruction": "def update_mapping(mapping: Dict[ops.Qid, LogicalIndex],\n                   operations: ops.OP_TREE\n                   ) -> None:\n    \"\"\"Updates a mapping (in place) from qubits to logical indices according to\n    a set of permutation gates. Any gates other than permutation gates are\n    ignored.\n\n    Args:\n        mapping: The mapping to update.\n        operations: The operations to update according to.\n    \"\"\"\n", "input": "", "output": "    for op in ops.flatten_op_tree(operations):\n        if (isinstance(op, ops.GateOperation) and\n            isinstance(op.gate, PermutationGate)):\n            op.gate.update_mapping(mapping, op.qubits)", "category": "Python"}, {"instruction": "def _tables_line(args):\n  \"\"\"Implements the BigQuery tables magic used to display tables in a dataset.\n\n   The supported syntax is:\n\n       %bigquery tables -p|--project <project_id>  -d|--dataset <dataset_id>\n\n  Args:\n    args: the arguments following '%bigquery tables'.\n  Returns:\n    The HTML rendering for the list of tables.\n  \"\"\"\n", "input": "", "output": "  filter_ = args['filter'] if args['filter'] else '*'\n  if args['dataset']:\n    if args['project'] is None:\n      datasets = [datalab.bigquery.Dataset(args['dataset'])]\n    else:\n      datasets = [datalab.bigquery.Dataset((args['project'], args['dataset']))]\n  else:\n    datasets = datalab.bigquery.Datasets(args['project'])\n\n  tables = []\n  for dataset in datasets:\n    tables.extend([str(table) for table in dataset if fnmatch.fnmatch(str(table), filter_)])\n\n  return _render_list(tables)", "category": "Python"}, {"instruction": "def _fix_syscall_ip(state):\n        \"\"\"\n        Resolve syscall information from the state, get the IP address of the syscall SimProcedure, and set the IP of\n        the state accordingly. Don't do anything if the resolution fails.\n\n        :param SimState state: the program state.\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        try:\n            bypass = o.BYPASS_UNSUPPORTED_SYSCALL in state.options\n            stub = state.project.simos.syscall(state, allow_unsupported=bypass)\n            if stub: # can be None if simos is not a subclass of SimUserspace\n                state.ip = stub.addr # fix the IP\n        except AngrUnsupportedSyscallError:\n            pass", "category": "Python"}, {"instruction": "def prune_chunks(self, tsn):\n        \"\"\"\n        Prune chunks up to the given TSN.\n        \"\"\"\n", "input": "", "output": "        pos = -1\n        size = 0\n        for i, chunk in enumerate(self.reassembly):\n            if uint32_gte(tsn, chunk.tsn):\n                pos = i\n                size += len(chunk.user_data)\n            else:\n                break\n\n        self.reassembly = self.reassembly[pos + 1:]\n        return size", "category": "Python"}, {"instruction": "def match_option_with_value(arguments, option, value):\n    \"\"\"\n    Check if a list of command line options contains an option with a value.\n\n    :param arguments: The command line arguments (a list of strings).\n    :param option: The long option (a string).\n    :param value: The expected value (a string).\n    :returns: :data:`True` if the command line contains the option/value pair,\n              :data:`False` otherwise.\n    \"\"\"\n", "input": "", "output": "    return ('%s=%s' % (option, value) in arguments or\n            contains_sublist(arguments, [option, value]))", "category": "Python"}, {"instruction": "def ReadAllArtifacts(self):\n    \"\"\"Lists all artifacts that are stored in the database.\"\"\"\n", "input": "", "output": "    artifacts = []\n\n    for artifact in itervalues(self.artifacts):\n      artifacts.append(artifact.Copy())\n\n    return artifacts", "category": "Python"}, {"instruction": "def _setLearningMode(self):\n    \"\"\"\n    Sets the learning mode.\n    \"\"\"\n", "input": "", "output": "    for column in self.L4Columns:\n      column.setParameter(\"learn\", 0, True)\n    for column in self.L6Columns:\n      column.setParameter(\"learn\", 0, True)\n\n    for column in self.L2Columns:\n      column.setParameter(\"learningMode\", 0, True)\n    for column in self.L5Columns:\n      column.setParameter(\"learningMode\", 0, True)", "category": "Python"}, {"instruction": "def update(self, data):\n        \"\"\" Update a list.\"\"\"\n", "input": "", "output": "        # if self.debug >= 2:\n        #     self.debug(data)\n        url = \"{base}/change_password\".format(\n            base=self.local_base_url\n        )\n        self._check(data)\n        res = self.core.create(url, data)\n        self.log.debug(\"result: %s\", res)\n        return res", "category": "Python"}, {"instruction": "def data_vectors(self):\n        \"\"\"The per-sample data in a vector.\n\n        Returns:\n            dict: A dict where the keys are the fields in the record and the\n            values are the corresponding arrays.\n\n        Examples:\n            >>> sampleset = dimod.SampleSet.from_samples([[-1, 1], [1, 1]], dimod.SPIN,\n                                                         energy=[-1, 1])\n            >>> sampleset.data_vectors['energy']\n            array([-1,  1])\n\n            Note that this is equivalent to, and less performant than:\n\n            >>> sampleset = dimod.SampleSet.from_samples([[-1, 1], [1, 1]], dimod.SPIN,\n                                                         energy=[-1, 1])\n            >>> sampleset.record['energy']\n            array([-1,  1])\n\n\n        \"\"\"\n", "input": "", "output": "        return {field: self.record[field] for field in self.record.dtype.names\n                if field != 'sample'}", "category": "Python"}, {"instruction": "def decode_int(self, str):\n        \"\"\" Decodes a short Base64 string into an integer.\n\n        Example:\n            ``decode_int('B7')`` returns ``123``.\n        \"\"\"\n", "input": "", "output": "        n = 0\n        for c in str:\n            n = n * self.BASE + self.ALPHABET_REVERSE[c]\n        return n", "category": "Python"}, {"instruction": "def consonants(self):\n        \"\"\"\n        Return a new IPAString, containing only the consonants in the current string.\n\n        :rtype: IPAString\n        \"\"\"\n", "input": "", "output": "        return IPAString(ipa_chars=[c for c in self.ipa_chars if c.is_consonant])", "category": "Python"}, {"instruction": "def _pidExists(pid):\n        \"\"\"\n        This will return True if the process associated with pid is still running on the machine.\n        This is based on stackoverflow question 568271.\n\n        :param int pid: ID of the process to check for\n        :return: True/False\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        assert pid > 0\n        try:\n            os.kill(pid, 0)\n        except OSError as err:\n            if err.errno == errno.ESRCH:\n                # ESRCH == No such process\n                return False\n            else:\n                raise\n        else:\n            return True", "category": "Python"}, {"instruction": "def ssl_proxy(self, value):\n        \"\"\"\n        Sets https proxy setting.\n\n        :Args:\n         - value: The https proxy value.\n        \"\"\"\n", "input": "", "output": "        self._verify_proxy_type_compatibility(ProxyType.MANUAL)\n        self.proxyType = ProxyType.MANUAL\n        self.sslProxy = value", "category": "Python"}, {"instruction": "def get_stats(self):\n        \"\"\"\n        Gets statistics for this NIO.\n\n        :returns: NIO statistics (string with packets in, packets out, bytes in, bytes out)\n        \"\"\"\n", "input": "", "output": "\n        stats = yield from self._hypervisor.send(\"nio get_stats {}\".format(self._name))\n        return stats[0]", "category": "Python"}, {"instruction": "def poke(url, accesskey=None, secretkey=None, __method__='GET', **req_args):\n    \"\"\"\n    Poke the Rancher API. Returns a Rod object instance. Central starting\n    point for the cattleprod package.\n    :param url: The full Rancher URL to the API endpoint.\n    :param accesskey: The rancher access key, optional.\n    :param secretkey: The rancher secret key, optional.\n    :param __method__: Internal method, don't use!\n    :param req_args: Arguments which are passed directly to the requests API.\n    The accesskey / secretkey values have precedence before simple auth\n    objects defined in here.\n    :return: A Rod instance, or anything that the URL returns on a GET request\n    \"\"\"\n", "input": "", "output": "    if accesskey and secretkey:\n        req_args['auth'] = (accesskey, secretkey)\n    tmp = requests.request(__method__.lower(), url, **req_args)\n    tmp.raise_for_status()\n    if tmp.headers.get('Content-Type').find(\"json\") != -1:\n        rv = _convert_to_rod(tmp.json(), **req_args)\n    else:\n        rv = tmp.content\n    return rv", "category": "Python"}, {"instruction": "def git_get_title_and_message(begin, end):\n    \"\"\"Get title and message summary for patches between 2 commits.\n\n    :param begin: first commit to look at\n    :param end: last commit to look at\n    :return: number of commits, title, message\n    \"\"\"\n", "input": "", "output": "    titles = git_get_log_titles(begin, end)\n    title = \"Pull request for \" + end\n    if len(titles) == 1:\n        title = titles[0]\n    pr_template = find_pull_request_template()\n    if pr_template:\n        message = get_pr_template_message(pr_template)\n    else:\n        if len(titles) == 1:\n            message = git_get_commit_body(end)\n        else:\n            message = \"\\n\".join(titles)\n\n    return (len(titles), title, message)", "category": "Python"}, {"instruction": "def alias(col, mapping):\n    \"\"\" Returns a collection of dictionaries with the keys renamed according to\n        the mapping\n\n        >>> libraries = [{\"isbn\": 1, \"ed\": 1}, {\"isbn\": 2, \"ed\": 2}]\n        >>> alias(libraries, {\"ed\": \"edition\"})\n        [{'edition': 1, 'isbn': 1}, {'edition': 2, 'isbn': 2}]\n\n        >>> alias({\"a\": 1}, {\"a\": \"b\"})\n        [{'b': 1}]\n    \"\"\"\n", "input": "", "output": "    if not is_list(col):\n        col = [col]\n\n    def _block(dct):\n        return rename(dct, mapping)\n\n    return map(_block, col)", "category": "Python"}, {"instruction": "def clean_conf_folder(self, locale):\n        \"\"\"Remove the configuration directory for `locale`\"\"\"\n", "input": "", "output": "        dirname = self.configuration.get_messages_dir(locale)\n        dirname.removedirs_p()", "category": "Python"}, {"instruction": "def deserializeGt(x, compressed=True):\n    \"\"\"\n    Deserializes an array of bytes, @x, into a Gt element.\n    \"\"\"\n", "input": "", "output": "    return _deserialize(x, GtElement, compressed, librelic.gt_read_bin_abi)", "category": "Python"}, {"instruction": "def cross_successors(state, last_action=None):\n        \"\"\"\n        Successors function for solving the cross.\n        \"\"\"\n", "input": "", "output": "        centres, edges = state\n        acts = sum([\n            [s, s.inverse(), s * 2] for s in\n            map(Step, \"RUFDRB\".replace(last_action.face if last_action else \"\", \"\", 1))\n            ], [])\n        for step in acts:\n            yield step, (centres, CrossSolver._rotate(edges, step))", "category": "Python"}, {"instruction": "def angular_distance(ra1, dec1, ra2, dec2):\n    \"\"\"\n    Returns the angular distance between two points, two sets of points, or a set of points and one point.\n\n    :param ra1: array or float, longitude of first point(s)\n    :param dec1: array or float, latitude of first point(s)\n    :param ra2: array or float, longitude of second point(s)\n    :param dec2: array or float, latitude of second point(s)\n    :return: angular distance(s) in degrees\n    \"\"\"\n", "input": "", "output": "\n    # Vincenty formula, slower than the Haversine formula in some cases, but stable also at antipodes\n\n    lon1 = np.deg2rad(ra1)\n    lat1 = np.deg2rad(dec1)\n    lon2 = np.deg2rad(ra2)\n    lat2 = np.deg2rad(dec2)\n\n    sdlon = np.sin(lon2 - lon1)\n    cdlon = np.cos(lon2 - lon1)\n    slat1 = np.sin(lat1)\n    slat2 = np.sin(lat2)\n    clat1 = np.cos(lat1)\n    clat2 = np.cos(lat2)\n\n    num1 = clat2 * sdlon\n    num2 = clat1 * slat2 - slat1 * clat2 * cdlon\n    denominator = slat1 * slat2 + clat1 * clat2 * cdlon\n\n    return np.rad2deg(np.arctan2(np.sqrt(num1 ** 2 + num2 ** 2), denominator))", "category": "Python"}, {"instruction": "def plat_specific_errors(*errnames):\n    \"\"\"Return error numbers for all errors in errnames on this platform.\n\n    The 'errno' module contains different global constants depending on\n    the specific platform (OS). This function will return the list of\n    numeric values for a given list of potential names.\n    \"\"\"\n", "input": "", "output": "    missing_attr = set([None, ])\n    unique_nums = set(getattr(errno, k, None) for k in errnames)\n    return list(unique_nums - missing_attr)", "category": "Python"}, {"instruction": "def iter_work_specs(self, limit=None, start=None):\n        '''\n        yield work spec dicts\n        '''\n", "input": "", "output": "        count = 0\n        ws_list, start = self.list_work_specs(limit, start)\n        while True:\n            for name_spec in ws_list:\n                yield name_spec[1]\n                count += 1\n                if (limit is not None) and (count >= limit):\n                    break\n            if not start:\n                break\n            if limit is not None:\n                limit -= count\n            ws_list, start = self.list_work_specs(limit, start)", "category": "Python"}, {"instruction": "def is_done(self, submissionid_or_submission, user_check=True):\n        \"\"\" Tells if a submission is done and its result is available \"\"\"\n", "input": "", "output": "        # TODO: not a very nice way to avoid too many database call. Should be refactored.\n        if isinstance(submissionid_or_submission, dict):\n            submission = submissionid_or_submission\n        else:\n            submission = self.get_submission(submissionid_or_submission, False)\n        if user_check and not self.user_is_submission_owner(submission):\n            return None\n        return submission[\"status\"] == \"done\" or submission[\"status\"] == \"error\"", "category": "Python"}, {"instruction": "def closeEvent( self, event ):\n        \"\"\"\n        Cleans up the scene before closing.\n        \n        :param      event | <QEvent>\n        \"\"\"\n", "input": "", "output": "        if ( self.cleanupOnClose() ):\n            scene = self.scene()\n            scene.cleanup()\n            self.setScene(None)\n        \n        super(XNodeWidget, self).closeEvent(event)", "category": "Python"}, {"instruction": "def nla_put_u16(msg, attrtype, value):\n    \"\"\"Add 16 bit integer attribute to Netlink message.\n\n    https://github.com/thom311/libnl/blob/libnl3_2_25/lib/attr.c#L588\n\n    Positional arguments:\n    msg -- Netlink message (nl_msg class instance).\n    attrtype -- attribute type (integer).\n    value -- numeric value to store as payload (int() or c_uint16()).\n\n    Returns:\n    0 on success or a negative error code.\n    \"\"\"\n", "input": "", "output": "    data = bytearray(value if isinstance(value, c_uint16) else c_uint16(value))\n    return nla_put(msg, attrtype, SIZEOF_U16, data)", "category": "Python"}, {"instruction": "async def handle(self):\n        \"\"\"\n        Listens on all the provided channels and handles the messages.\n        \"\"\"\n", "input": "", "output": "        # For each channel, launch its own listening coroutine\n        listeners = []\n        for key, value in self.beat_config.items():\n            listeners.append(asyncio.ensure_future(\n                self.listener(key)\n            ))\n\n\n        # For each beat configuration, launch it's own sending pattern\n        emitters = []\n        for key, value in self.beat_config.items():\n            emitters.append(asyncio.ensure_future(\n                self.emitters(key, value)\n            ))\n\n        # Wait for them all to exit\n        await asyncio.wait(emitters)\n        await asyncio.wait(listeners)", "category": "Python"}, {"instruction": "def set_tags(self, value):\n        \"\"\"Setter for tags attribute\"\"\"\n", "input": "", "output": "        log = (\n            'Sources fields \"tags\" should be a dict in the format '\n            '{\"include\": [ \"tag1\", \"tag2\", \"tagN\" ],'\n            '\"exclude\": [ \"tag1\", \"tag2\", \"tagN\" ] }'\n        )\n\n        if not isinstance(value, dict):\n            raise MalFormattedSource(log)\n\n        if not set(value.keys()).issubset(set([\"include\", \"exclude\"])):\n            raise MalFormattedSource(log)\n\n        for tag_list in value.values():\n            if not isinstance(tag_list, list):\n                raise MalFormattedSource(log)\n            if [tag for tag in tag_list if not isinstance(tag, str)]:\n                raise MalFormattedSource(log)\n\n        self._tags = value", "category": "Python"}, {"instruction": "def _new_mock_response(self, response, file_path):\n        '''Return a new mock Response with the content.'''\n", "input": "", "output": "        mock_response = copy.copy(response)\n\n        mock_response.body = Body(open(file_path, 'rb'))\n        mock_response.fields = NameValueRecord()\n\n        for name, value in response.fields.get_all():\n            mock_response.fields.add(name, value)\n\n        mock_response.fields['Content-Type'] = 'text/html; charset=\"utf-8\"'\n\n        return mock_response", "category": "Python"}, {"instruction": "def read_all(self):\n        \"\"\"Read all remaining data in the packet.\n\n        (Subsequent read() will return errors.)\n        \"\"\"\n", "input": "", "output": "        result = self._data[self._position:]\n        self._position = None  # ensure no subsequent read()\n        return result", "category": "Python"}, {"instruction": "def maybe_send_signal(self, signum, include_pgrp=True):\n    \"\"\"Send the signal `signum` send if the PID and/or PGRP chunks have been received.\n\n    No error is raised if the pid or pgrp are None or point to an already-dead process.\n    \"\"\"\n", "input": "", "output": "    remote_pid = self._maybe_last_pid()\n    if remote_pid is not None:\n      safe_kill(remote_pid, signum)\n    if include_pgrp:\n      remote_pgrp = self._maybe_last_pgrp()\n      if remote_pgrp:\n        safe_kill(remote_pgrp, signum)", "category": "Python"}, {"instruction": "def fmtval(value, colorstr=None, precision=None, spacing=True, trunc=True,\n           end=' '):\n    ''' Formats and returns a given number according to specifications. '''\n", "input": "", "output": "    colwidth = opts.colwidth\n    # get precision\n    if precision is None:\n        precision = opts.precision\n    fmt = '%%.%sf' % precision\n\n    # format with decimal mark, separators\n    result = locale.format(fmt, value, True)\n\n    if spacing:\n        result = '%%%ss' % colwidth % result\n\n    if trunc:\n        if len(result) > colwidth:   # truncate w/ellipsis\n            result = truncstr(result, colwidth)\n\n    # Add color if needed\n    if opts.incolor and colorstr:\n        return colorstr % result + end\n    else:\n        return result + end", "category": "Python"}, {"instruction": "def set_position(self, pos):\n        \"\"\"Seek in the current playing media.\"\"\"\n", "input": "", "output": "        time_in_ms = int(pos)*1000\n        return self.apple_tv.set_property('dacp.playingtime', time_in_ms)", "category": "Python"}, {"instruction": "def _get_start_end_nans(data):\n        \"\"\" Find NaN values in data that either start or end the time-series\n\n        Function to return a binary array of same size as data where `True` values correspond to NaN values present at\n        beginning or end of time-series. NaNs internal to the time-series are not included in the binary mask.\n\n        :param data: Array of observations of size TxNOBS\n        :type data: numpy.array\n        :return: Binary array of shape TxNOBS. `True` values indicate NaNs present at beginning or end of time-series\n        :rtype: numpy.array\n        \"\"\"\n", "input": "", "output": "        # find NaNs that start a time-series\n        start_nan = np.isnan(data)\n        for idx, row in enumerate(start_nan[:-1]):\n            start_nan[idx + 1] = np.logical_and(row, start_nan[idx + 1])\n        # find NaNs that end a time-series\n        end_nan = np.isnan(data)\n        for idx, row in enumerate(end_nan[-2::-1]):\n            end_nan[-idx-2] = np.logical_and(row, end_nan[-idx-1])\n\n        return np.logical_or(start_nan, end_nan)", "category": "Python"}, {"instruction": "def args_range(min_value, max_value, *args):\r\n    \"\"\"\r\n        \u68c0\u67e5\u53c2\u6570\u8303\u56f4\r\n    \"\"\"\n", "input": "", "output": "    not_null(*args)\r\n\r\n    if not all(map(lambda v: min_value <= v <= max_value, args)):\r\n        raise ValueError(\"Argument must be between {0} and {1}!\".format(min_value, max_value))", "category": "Python"}, {"instruction": "def total_time(self):\n        \"\"\"Total play time in seconds.\"\"\"\n", "input": "", "output": "        now_playing = self._setstate.nowPlayingInfo\n        if now_playing.HasField('duration'):\n            return int(now_playing.duration)\n\n        return None", "category": "Python"}, {"instruction": "def remove_nesting(dom, tag_name):\n    \"\"\"\n    Unwrap items in the node list that have ancestors with the same tag.\n    \"\"\"\n", "input": "", "output": "    for node in dom.getElementsByTagName(tag_name):\n        for ancestor in ancestors(node):\n            if ancestor is node:\n                continue\n            if ancestor is dom.documentElement:\n                break\n            if ancestor.tagName == tag_name:\n                unwrap(node)\n                break", "category": "Python"}, {"instruction": "def not_user_filter(config, message, fasnick=None, *args, **kw):\n    \"\"\" Everything except a particular user\n\n    Use this rule to exclude messages that are associated with one or more\n    users. Specify several users by separating them with a comma ','.\n    \"\"\"\n", "input": "", "output": "\n    fasnick = kw.get('fasnick', fasnick)\n    if not fasnick:\n        return False\n\n    fasnick = (fasnick or []) and fasnick.split(',')\n    valid = True\n    for nick in fasnick:\n        if nick.strip() in fmn.rules.utils.msg2usernames(message, **config):\n            valid = False\n            break\n\n    return valid", "category": "Python"}, {"instruction": "def parse_args():\n    \"\"\"Parse command line arguments.\"\"\"\n", "input": "", "output": "    parser = argparse.ArgumentParser(\n        description='Vocabulary extractor.',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    parser.add_argument('--max-size', type=int, default=None)\n    parser.add_argument('--min-freq', type=int, default=5)\n    parser.add_argument('--max-word-length', type=int, default=50)\n    parser.add_argument('files', type=str, nargs='+')\n    parser.add_argument('--vocab-output', type=str, default='vocab.json')\n    parser.add_argument('--counts-output', type=str, default='counts.json')\n    args = parser.parse_args()\n    return args", "category": "Python"}, {"instruction": "def decrease_weight(self, proxy):\n        \"\"\"Decreasing the weight of a proxy by multiplying dec_ratio\"\"\"\n", "input": "", "output": "        new_weight = proxy.weight * self.dec_ratio\n        if new_weight < self.weight_thr:\n            self.remove_proxy(proxy)\n        else:\n            proxy.weight = new_weight", "category": "Python"}, {"instruction": "def store_json(obj, destination):\n    \"\"\"store_json\n\nTakes in a json-portable object and a filesystem-based destination and stores\nthe json-portable object as JSON into the filesystem-based destination.\n\nThis is blind, dumb, and stupid; thus, it can fail if the object is more\ncomplex than simple dict, list, int, str, etc. type object structures.\n    \"\"\"\n", "input": "", "output": "    with open(destination, 'r+') as FH:\n        fcntl.lockf(FH, fcntl.LOCK_EX)\n        json_in = json.loads(FH.read())\n        json_in.update(obj)  # obj overwrites items in json_in...\n        FH.seek(0)\n        FH.write(json.dumps(json_in, sort_keys=True, indent=4,\n                            separators=(',', ': ')))", "category": "Python"}, {"instruction": "def quick_response(self, status_code):\r\n        \"\"\" Quickly construct response using a status code \"\"\"\n", "input": "", "output": "        translator = Translator(environ=self.environ)\r\n        if status_code == 404:\r\n            self.status(404)\r\n            self.message(translator.trans('http_messages.404'))\r\n        elif status_code == 401:\r\n            self.status(401)\r\n            self.message(translator.trans('http_messages.401'))\r\n        elif status_code == 400:\r\n            self.status(400)\r\n            self.message(translator.trans('http_messages.400'))\r\n        elif status_code == 200:\r\n            self.status(200)\r\n            self.message(translator.trans('http_messages.200'))", "category": "Python"}, {"instruction": "def get_vr(self, epoch=None):\n        \"\"\"get VR string from .spec Version, Release and Epoch\n\n        epoch is None: prefix epoch if present (default)\n        epoch is True: prefix epoch even if not present (0:)\n        epoch is False: omit epoch even if present\n        \"\"\"\n", "input": "", "output": "        version = self.get_tag('Version', expand_macros=True)\n        e = None\n        if epoch is None or epoch:\n            try:\n                e = self.get_tag('Epoch')\n            except exception.SpecFileParseError:\n                pass\n        if epoch is None and e:\n            epoch = True\n        if epoch:\n            if not e:\n                e = '0'\n            version = '%s:%s' % (e, version)\n        release = self.get_tag('Release')\n        release = re.sub(r'%\\{?\\??dist\\}?$', '', release)\n        release = self.expand_macro(release)\n        if release:\n            return '%s-%s' % (version, release)\n        return version", "category": "Python"}, {"instruction": "def get_eidos_scorer():\n    \"\"\"Return a SimpleScorer based on Eidos curated precision estimates.\"\"\"\n", "input": "", "output": "    table = load_eidos_curation_table()\n\n    # Get the overall precision\n    total_num = table['COUNT of RULE'].sum()\n    weighted_sum = table['COUNT of RULE'].dot(table['% correct'])\n    precision = weighted_sum / total_num\n    # We have to divide this into a random and systematic component, for now\n    # in an ad-hoc manner\n    syst_error = 0.05\n    rand_error = 1 - precision - syst_error\n    prior_probs = {'rand': {'eidos': rand_error}, 'syst': {'eidos': syst_error}}\n\n    # Get a dict of rule-specific errors.\n    subtype_probs = {'eidos':\n                     {k: 1.0-min(v, 0.95)-syst_error for k, v\n                      in zip(table['RULE'], table['% correct'])}}\n    scorer = SimpleScorer(prior_probs, subtype_probs)\n    return scorer", "category": "Python"}, {"instruction": "def dispatch(self, *args, **kwargs):\n        \"\"\"This decorator sets this view to have restricted permissions.\"\"\"\n", "input": "", "output": "        return super(AnimalMonthArchive, self).dispatch(*args, **kwargs)", "category": "Python"}, {"instruction": "def _has_file_rolled(self):\n        \"\"\"Check if the file has been rolled\"\"\"\n", "input": "", "output": "        # if the size is smaller then before, the file has\n        # probabilly been rolled\n        if self._fh:\n            size = self._getsize_of_current_file()\n            if size < self.oldsize:\n                return True\n\n            self.oldsize = size\n\n        return False", "category": "Python"}, {"instruction": "def adjust_doy_calendar(source, target):\n    \"\"\"Interpolate from one set of dayofyear range to another calendar.\n\n    Interpolate an array defined over a `dayofyear` range (say 1 to 360) to another `dayofyear` range (say 1\n    to 365).\n\n    Parameters\n    ----------\n    source : xarray.DataArray\n      Array with `dayofyear` coordinates.\n    target : xarray.DataArray\n      Array with `time` coordinate.\n\n    Returns\n    -------\n    xarray.DataArray\n      Interpolated source array over coordinates spanning the target `dayofyear` range.\n\n    \"\"\"\n", "input": "", "output": "    doy_max_source = source.dayofyear.max()\n\n    doy_max = infer_doy_max(target)\n    if doy_max_source == doy_max:\n        return source\n\n    return _interpolate_doy_calendar(source, doy_max)", "category": "Python"}, {"instruction": "def on_resize(self, width, height):\n        \"\"\"\n        Handle resized windows.\n        \"\"\"\n", "input": "", "output": "        width, height = self._update_perspective(width, height)\n        self.scene.camera.resolution = (width, height)\n        self.view['ball'].resize(self.scene.camera.resolution)\n        self.scene.camera.transform = self.view['ball'].pose", "category": "Python"}, {"instruction": "def python_to_couch(options):\n    \"\"\"\n    Translates query options from python style options into CouchDB/Cloudant\n    query options.  For example ``{'include_docs': True}`` will\n    translate to ``{'include_docs': 'true'}``.  Primarily meant for use by\n    code that formulates a query to retrieve results data from the\n    remote database, such as the database API convenience method\n    :func:`~cloudant.database.CouchDatabase.all_docs` or the View\n    :func:`~cloudant.view.View.__call__` callable, both used to retrieve data.\n\n    :param dict options: Python style parameters to be translated.\n\n    :returns: Dictionary of translated CouchDB/Cloudant query parameters\n    \"\"\"\n", "input": "", "output": "    translation = dict()\n    for key, val in iteritems_(options):\n        py_to_couch_validate(key, val)\n        translation.update(_py_to_couch_translate(key, val))\n    return translation", "category": "Python"}, {"instruction": "def authenticate_credentials(self, userid, password):\n        \"\"\"\n        Authenticate the userid and password against username and password.\n        \"\"\"\n", "input": "", "output": "        credentials = {\n            get_user_model().USERNAME_FIELD: userid,\n            'password': password\n        }\n        user = authenticate(**credentials)\n\n        if user is None:\n            raise exceptions.AuthenticationFailed(_('Invalid username/password.'))\n\n        if not user.is_active:\n            raise exceptions.AuthenticationFailed(_('User inactive or deleted.'))\n\n        return (user, None)", "category": "Python"}, {"instruction": "def positive_nonzero_int(string):\n    \"\"\"Convert string to positive integer greater than zero.\"\"\"\n", "input": "", "output": "    error_msg = 'Positive non-zero integer required, {string} given.'.format(string=string)\n    try:\n        value = int(string)\n    except ValueError:\n        raise ArgumentTypeError(error_msg)\n    if value <= 0:\n        raise ArgumentTypeError(error_msg)\n    return value", "category": "Python"}, {"instruction": "def icon_resource(name, package=None):\n    \"\"\"\n    Returns the absolute URI path to an image. If a package is not explicitly specified then the calling package name is\n    used.\n\n    :param name: path relative to package path of the image resource.\n    :param package: package name in dotted format.\n    :return: the file URI path to the image resource (i.e. file:///foo/bar/image.png).\n    \"\"\"\n", "input": "", "output": "    if not package:\n        package = '%s.resources.images' % calling_package()\n    name = resource_filename(package, name)\n    if not name.startswith('/'):\n        return 'file://%s' % abspath(name)\n    return 'file://%s' % name", "category": "Python"}, {"instruction": "def contents(self):\n        \"\"\"Return the names of the entries\"\"\"\n", "input": "", "output": "        rslt = []\n        for (dpos, dlen, ulen, flag, typcd, nm) in self.toc:\n            rslt.append(nm)\n        return rslt", "category": "Python"}, {"instruction": "def encode_request(name, value_list):\n    \"\"\" Encode request into client_message\"\"\"\n", "input": "", "output": "    client_message = ClientMessage(payload_size=calculate_size(name, value_list))\n    client_message.set_message_type(REQUEST_TYPE)\n    client_message.set_retryable(RETRYABLE)\n    client_message.append_str(name)\n    client_message.append_int(len(value_list))\n    for value_list_item in value_list:\n        client_message.append_data(value_list_item)\n    client_message.update_frame_length()\n    return client_message", "category": "Python"}, {"instruction": "def run(self, old_cmd):\n        \"\"\"Runs command from rule for passed command.\n\n        :type old_cmd: Command\n\n        \"\"\"\n", "input": "", "output": "        if self.side_effect:\n            self.side_effect(old_cmd, self.script)\n        if settings.alter_history:\n            shell.put_to_history(self.script)\n        # This depends on correct setting of PYTHONIOENCODING by the alias:\n        logs.debug(u'PYTHONIOENCODING: {}'.format(\n            os.environ.get('PYTHONIOENCODING', '!!not-set!!')))\n\n        print(self._get_script())", "category": "Python"}, {"instruction": "def get_extrapolated_diffusivity(temps, diffusivities, new_temp):\n    \"\"\"\n    Returns (Arrhenius) extrapolated diffusivity at new_temp\n\n    Args:\n        temps ([float]): A sequence of temperatures. units: K\n        diffusivities ([float]): A sequence of diffusivities (e.g.,\n            from DiffusionAnalyzer.diffusivity). units: cm^2/s\n        new_temp (float): desired temperature. units: K\n\n    Returns:\n        (float) Diffusivity at extrapolated temp in mS/cm.\n    \"\"\"\n", "input": "", "output": "    Ea, c, _ = fit_arrhenius(temps, diffusivities)\n    return c * np.exp(-Ea / (const.k / const.e * new_temp))", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"Start the sensor.\n        \"\"\"\n", "input": "", "output": "        self._cap = cv2.VideoCapture(self._device_id + cv2.CAP_V4L2)\n        if not self._cap.isOpened():\n            self._running = False\n            self._cap.release()\n            self._cap = None\n            return False\n\n        self._cap.set(cv2.CAP_PROP_FRAME_WIDTH, self._camera_intr.width)\n        self._cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self._camera_intr.height)\n        self._running = True\n\n        # Capture 5 frames to flush webcam sensor\n        for _ in range(5):\n            _ = self.frames()\n\n        return True", "category": "Python"}, {"instruction": "def get_extrapolated_conductivity(temps, diffusivities, new_temp, structure,\n                                  species):\n    \"\"\"\n    Returns extrapolated mS/cm conductivity.\n\n    Args:\n        temps ([float]): A sequence of temperatures. units: K\n        diffusivities ([float]): A sequence of diffusivities (e.g.,\n            from DiffusionAnalyzer.diffusivity). units: cm^2/s\n        new_temp (float): desired temperature. units: K\n        structure (structure): Structure used for the diffusivity calculation\n        species (string/Specie): conducting species\n\n    Returns:\n        (float) Conductivity at extrapolated temp in mS/cm.\n    \"\"\"\n", "input": "", "output": "    return get_extrapolated_diffusivity(temps, diffusivities, new_temp) \\\n        * get_conversion_factor(structure, species, new_temp)", "category": "Python"}, {"instruction": "def total_build_duration_for_chain(self, build_chain_id):\n        \"\"\"Returns the total duration for one specific build chain run\"\"\"\n", "input": "", "output": "        return sum([\n            int(self.__build_duration_for_id(id))\n            for id in self.__build_ids_of_chain(build_chain_id)\n        ])", "category": "Python"}, {"instruction": "def getWindowByPID(self, pid, order=0):\n        \"\"\" Returns a handle for the first window that matches the provided PID \"\"\"\n", "input": "", "output": "        if pid <= 0:\n            return None\n        EnumWindowsProc = ctypes.WINFUNCTYPE(\n            ctypes.c_bool,\n            ctypes.POINTER(ctypes.c_int),\n            ctypes.py_object)\n        def callback(hwnd, context):\n            if ctypes.windll.user32.IsWindowVisible(hwnd):\n                pid = ctypes.c_ulong()\n                ctypes.windll.user32.GetWindowThreadProcessId(hwnd, ctypes.byref(pid))\n                if context[\"pid\"] == int(pid.value) and not context[\"handle\"]:\n                    if context[\"order\"] > 0:\n                        context[\"order\"] -= 1\n                    else:\n                        context[\"handle\"] = hwnd\n            return True\n        data = {\"pid\": pid, \"handle\": None, \"order\": order}\n        ctypes.windll.user32.EnumWindows(EnumWindowsProc(callback), ctypes.py_object(data))\n        return data[\"handle\"]", "category": "Python"}, {"instruction": "def forwards_migration(apps, schema_editor):\n    \"\"\"\n    \"0002_recurrence_rules\" added malformed recurrence with trailing\n    semi-colons. While the JS parser on the front-end handles them,\n    the python parser will crash\n    \"\"\"\n", "input": "", "output": "    RecurrenceRule = apps.get_model('icekit_events', 'RecurrenceRule')\n    for rrule in RecurrenceRule.objects.all():\n        if ';\\n' in rrule.recurrence_rule:\n            parts = rrule.recurrence_rule.split(';\\n')\n            rrule.recurrence_rule = '\\n'.join(parts)\n            rrule.save()", "category": "Python"}, {"instruction": "def describe_jobflows(self, states=None, jobflow_ids=None,\n                           created_after=None, created_before=None):\n        \"\"\"\n        Retrieve all the Elastic MapReduce job flows on your account\n\n        :type states: list\n        :param states: A list of strings with job flow states wanted\n\n        :type jobflow_ids: list\n        :param jobflow_ids: A list of job flow IDs\n        :type created_after: datetime\n        :param created_after: Bound on job flow creation time\n\n        :type created_before: datetime\n        :param created_before: Bound on job flow creation time\n        \"\"\"\n", "input": "", "output": "        params = {}\n\n        if states:\n            self.build_list_params(params, states, 'JobFlowStates.member')\n        if jobflow_ids:\n            self.build_list_params(params, jobflow_ids, 'JobFlowIds.member')\n        if created_after:\n            params['CreatedAfter'] = created_after.strftime(\n                boto.utils.ISO8601)\n        if created_before:\n            params['CreatedBefore'] = created_before.strftime(\n                boto.utils.ISO8601)\n\n        return self.get_list('DescribeJobFlows', params, [('member', JobFlow)])", "category": "Python"}, {"instruction": "def _dispatcher(self, connection, event):\n        \"\"\"\n        Dispatch events to on_<event.type> method, if present.\n        \"\"\"\n", "input": "", "output": "        log.debug(\"_dispatcher: %s\", event.type)\n\n        def do_nothing(connection, event):\n            return None\n        method = getattr(self, \"on_\" + event.type, do_nothing)\n        method(connection, event)", "category": "Python"}, {"instruction": "def get_outcome_m(self, outcome_id):\n        \"\"\"Returns the outcome model for the given outcome id\n\n        :param outcome_id: The outcome id to search for\n        :return: The model of the outcome with the given id\n        \"\"\"\n", "input": "", "output": "        for outcome_m in self.outcomes:\n            if outcome_m.outcome.outcome_id == outcome_id:\n                return outcome_m\n        return False", "category": "Python"}, {"instruction": "def on_timer(self, _signum, _unused_frame):\n        \"\"\"Invoked by the Poll timer signal.\n\n        :param int _signum: The signal that was invoked\n        :param frame _unused_frame: The frame that was interrupted\n\n        \"\"\"\n", "input": "", "output": "        if self.is_shutting_down:\n            LOGGER.debug('Polling timer fired while shutting down')\n            return\n        if not self.polled:\n            self.poll()\n            self.polled = True\n            self.set_timer(5)  # Wait 5 seconds for results\n        else:\n            self.polled = False\n            self.poll_results_check()\n            self.set_timer(self.poll_interval)  # Wait poll interval duration\n\n            # If stats logging is enabled, log the stats\n            if self.log_stats_enabled:\n                self.log_stats()\n\n            # Increment the unresponsive children\n            for proc_name in self.poll_data['processes']:\n                self.unresponsive[proc_name] += 1\n\n            # Remove counters for processes that came back to life\n            for proc_name in list(self.unresponsive.keys()):\n                if proc_name not in self.poll_data['processes']:\n                    del self.unresponsive[proc_name]", "category": "Python"}, {"instruction": "def _send_unary_request(self, request):\n        \"\"\"Send a request using a separate unary request instead of over the\n        stream.\n\n        Args:\n            request (types.StreamingPullRequest): The stream request to be\n                mapped into unary requests.\n        \"\"\"\n", "input": "", "output": "        if request.ack_ids:\n            self._client.acknowledge(\n                subscription=self._subscription, ack_ids=list(request.ack_ids)\n            )\n\n        if request.modify_deadline_ack_ids:\n            # Send ack_ids with the same deadline seconds together.\n            deadline_to_ack_ids = collections.defaultdict(list)\n\n            for n, ack_id in enumerate(request.modify_deadline_ack_ids):\n                deadline = request.modify_deadline_seconds[n]\n                deadline_to_ack_ids[deadline].append(ack_id)\n\n            for deadline, ack_ids in six.iteritems(deadline_to_ack_ids):\n                self._client.modify_ack_deadline(\n                    subscription=self._subscription,\n                    ack_ids=ack_ids,\n                    ack_deadline_seconds=deadline,\n                )\n\n        _LOGGER.debug(\"Sent request(s) over unary RPC.\")", "category": "Python"}, {"instruction": "def write_implied_format(self, path, jpeg_quality=0, jpeg_progressive=0):\n        \"\"\"Write pix to the filename, with the extension indicating format.\n\n        jpeg_quality -- quality (iff JPEG; 1 - 100, 0 for default)\n        jpeg_progressive -- (iff JPEG; 0 for baseline seq., 1 for progressive)\n        \"\"\"\n", "input": "", "output": "        filename = fspath(path)\n        with _LeptonicaErrorTrap():\n            lept.pixWriteImpliedFormat(\n                os.fsencode(filename), self._cdata, jpeg_quality, jpeg_progressive\n            )", "category": "Python"}, {"instruction": "def reflect(vec1, vec2):\n    \"\"\"Take vec1 and reflect it about vec2.\"\"\"\n", "input": "", "output": "    if isinstance(vec1, Vector3) and isinstance(vec2, Vector3) \\\n            or isinstance(vec1, Vector4) and isinstance(vec2, Vector4):\n        return 2 * dot(vec1, vec2) * vec2 - vec2\n    else:\n        raise ValueError(\"vec1 and vec2 must both be a Vector type\")", "category": "Python"}, {"instruction": "def getVariantSet(self, id_):\n        \"\"\"\n        Returns the VariantSet with the specified name, or raises a\n        VariantSetNotFoundException otherwise.\n        \"\"\"\n", "input": "", "output": "        if id_ not in self._variantSetIdMap:\n            raise exceptions.VariantSetNotFoundException(id_)\n        return self._variantSetIdMap[id_]", "category": "Python"}, {"instruction": "def set_digit(self, pos, digit, decimal=False):\n        \"\"\"Set digit at position to provided value.  Position should be a value\n        of 0 to 3 with 0 being the left most digit on the display.  Digit should\n        be a number 0-9, character A-F, space (all LEDs off), or dash (-).\n        \"\"\"\n", "input": "", "output": "        if self.invert:\n            self.set_digit_raw(pos, IDIGIT_VALUES.get(str(digit).upper(), 0x00))\n        else:\n            self.set_digit_raw(pos, DIGIT_VALUES.get(str(digit).upper(), 0x00))\n\n        if decimal:\n            self.set_decimal(pos, True)", "category": "Python"}, {"instruction": "def get_privkey_address(privkey_info, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Get the address from a private key bundle\n    \"\"\"\n", "input": "", "output": "    if blockchain == 'bitcoin':\n        return btc_get_privkey_address(privkey_info, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))", "category": "Python"}, {"instruction": "def _type_check_pointers(utype):\n    \"\"\"Checks the user-derived type for non-nullified pointer array declarations\n    in its base definition.\n\n    Returns (list of offending members).\n    \"\"\"\n", "input": "", "output": "    result = []\n    for mname, member in utype.members.items():\n        if (\"pointer\" in member.modifiers and member.D > 0 and\n            (member.default is None or \"null\" not in member.default)):\n            result.append(member)\n\n    return result", "category": "Python"}, {"instruction": "def keys(self):\n        \"\"\"\n        :returns: a list of usable keys\n        :rtype: list\n\n        \"\"\"\n", "input": "", "output": "\n        keys = list()\n\n        for attribute_name, type_instance in inspect.getmembers(self):\n\n            # ignore parameters with __ and if they are methods\n            if attribute_name.startswith('__') or inspect.ismethod(type_instance):\n\n                continue\n\n            keys.append(attribute_name)\n\n        return keys", "category": "Python"}, {"instruction": "def isHiddenName(astr):\n    \"\"\" Return True if this string name denotes a hidden par or section \"\"\"\n", "input": "", "output": "    if astr is not None and len(astr) > 2 and astr.startswith('_') and \\\n       astr.endswith('_'):\n        return True\n    else:\n        return False", "category": "Python"}]