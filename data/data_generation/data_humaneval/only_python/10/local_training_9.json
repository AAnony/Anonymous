[{"instruction": "def __get_language_data(self, bundleId, languageId, fallback=False):\r\n        \"\"\"``GET /{serviceInstanceId}/v2/bundles/{bundleId}/{languageId}``\r\n\r\n        Gets the resource strings (key/value pairs) for the language. If\r\n        ``fallback`` is ``True``, source language value is used if translated\r\n        value is not available.\r\n        \"\"\"\n", "input": "", "output": "        url = self.__get_base_bundle_url() + '/' + bundleId + '/' + languageId\r\n        params = {'fallback': 'true'} if fallback else None\r\n        response = self.__perform_rest_call(requestURL=url, params=params)\r\n\r\n        if not response:\r\n            return None\r\n\r\n        languageData = response.get(self.__RESPONSE_RESOURCE_STRINGS_KEY)\r\n\r\n        return languageData", "category": "Python"}, {"instruction": "def setPalette(self, palette):\r\n        \"\"\"\r\n        Sets the palette for this button to the inputed palette.  This will\r\n        update the drop shadow to the palette's Shadow color property if\r\n        the shadowed mode is on.\r\n        \r\n        :param      palette | <QtGui.QPalette>\r\n        \"\"\"\n", "input": "", "output": "        super(XToolButton, self).setPalette(palette)\r\n        self.updateUi()", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Closes the record file.\"\"\"\n", "input": "", "output": "        if not self.is_open:\n            return\n        if self.writable:\n            check_call(_LIB.MXRecordIOWriterFree(self.handle))\n        else:\n            check_call(_LIB.MXRecordIOReaderFree(self.handle))\n        self.is_open = False\n        self.pid = None", "category": "Python"}, {"instruction": "def _applytfms(args):\n    \"\"\"\n    Applies ANTs' antsApplyTransforms to the input image.\n    All inputs are zipped in one tuple to make it digestible by\n    multiprocessing's map\n    \"\"\"\n", "input": "", "output": "    import nibabel as nb\n    from nipype.utils.filemanip import fname_presuffix\n    from niworkflows.interfaces.fixes import FixHeaderApplyTransforms as ApplyTransforms\n\n    in_file, in_xform, ifargs, index, newpath = args\n    out_file = fname_presuffix(in_file, suffix='_xform-%05d' % index,\n                               newpath=newpath, use_ext=True)\n\n    copy_dtype = ifargs.pop('copy_dtype', False)\n    xfm = ApplyTransforms(\n        input_image=in_file, transforms=in_xform, output_image=out_file, **ifargs)\n    xfm.terminal_output = 'allatonce'\n    xfm.resource_monitor = False\n    runtime = xfm.run().runtime\n\n    if copy_dtype:\n        nii = nb.load(out_file)\n        in_dtype = nb.load(in_file).get_data_dtype()\n\n        # Overwrite only iff dtypes don't match\n        if in_dtype != nii.get_data_dtype():\n            nii.set_data_dtype(in_dtype)\n            nii.to_filename(out_file)\n\n    return (out_file, runtime.cmdline)", "category": "Python"}, {"instruction": "def arg_name(self):\n        \"\"\"\n        Returns the name of the parameter as a command line flag\n        \"\"\"\n", "input": "", "output": "        if self.constraint is bool and self.value:\n            return '--no-%s' % self.name.replace('_', '-')\n        return '--%s' % self.name.replace('_', '-')", "category": "Python"}, {"instruction": "def to_unit_velocity_acceleration(self, va):\n        \"\"\" Convert velocities/accelerations to units of UnitConverter.\n\n        Converts velocity/ies and/or acceleration/s from units of motor\n        pitch per second (or second squared) to that of this\n        UnitConverter.\n\n        Parameters\n        ----------\n        va : int, float, or iterable of ints and floats\n            The velocities/accelerations to convert.\n\n        Returns\n        -------\n        converted_va : float or list of floats\n            The converted velocities/accelerations.\n\n        \"\"\"\n", "input": "", "output": "        if isinstance(va, collections.Iterable):\n            return [(x / self._va_to_motor) for x in va]\n        else:\n            return va / self._va_to_motor", "category": "Python"}, {"instruction": "def _get_xml(self, metric):\n        \"\"\"Returns the channel element of the RSS feed\"\"\"\n", "input": "", "output": "        self._opener = urllib2.build_opener()\n        self._opener.addheaders = [('User-agent', self.user_agent)]\n\n        if metric:\n            url = self.base_url + '?w={0}&u=c'.format(self.woeid)\n        else:\n            url = self.base_url + '?w={0}'.format(self.woeid)\n\n        return etree.parse(\n            self._opener.open(url)\n        ).getroot()[0]", "category": "Python"}, {"instruction": "def allProperties(self):\n        \"\"\"Helper that merges core, extended and custom properties\n\n        :return: mapping of all properties\n        \"\"\"\n", "input": "", "output": "        rval = {}\n        rval.update(self.coreProperties)\n        rval.update(self.extendedProperties)\n        rval.update(self.customProperties)\n        return rval", "category": "Python"}, {"instruction": "def available_readers(as_dict=False):\n    \"\"\"Available readers based on current configuration.\n\n    Args:\n        as_dict (bool): Optionally return reader information as a dictionary.\n                        Default: False\n\n    Returns: List of available reader names. If `as_dict` is `True` then\n             a list of dictionaries including additionally reader information\n             is returned.\n\n    \"\"\"\n", "input": "", "output": "    readers = []\n    for reader_configs in configs_for_reader():\n        try:\n            reader_info = read_reader_config(reader_configs)\n        except (KeyError, IOError, yaml.YAMLError):\n            LOG.warning(\"Could not import reader config from: %s\", reader_configs)\n            LOG.debug(\"Error loading YAML\", exc_info=True)\n            continue\n        readers.append(reader_info if as_dict else reader_info['name'])\n    return readers", "category": "Python"}, {"instruction": "def get_unspents(address, blockchain_client=ChainComClient()):\n    \"\"\" Get the spendable transaction outputs, also known as UTXOs or\n        unspent transaction outputs.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(blockchain_client, ChainComClient):\n        raise Exception('A ChainComClient object is required')\n\n    url = CHAIN_API_BASE_URL + '/bitcoin/addresses/' + address + '/unspents'\n\n    auth = blockchain_client.auth\n    if auth:\n        r = requests.get(url, auth=auth)\n    else:\n        r = requests.get(url + '?api-key-id=DEMO-4a5e1e4')\n\n    try:\n        unspents = r.json()\n    except ValueError, e:\n        raise Exception('Received non-JSON response from chain.com.')\n    \n    return format_unspents(unspents)", "category": "Python"}, {"instruction": "def select_labels(self, labels=None):\n        \"\"\" Prepare binar segmentation based on input segmentation and labels.\n\n        :param labels:\n        :return:\n        \"\"\"\n", "input": "", "output": "        self._resize_if_required()\n        segmentation = self._select_labels(self.resized_segmentation, labels)\n        # logger.debug(\"select labels in show_segmentation {} sum {}\".format(labels, np.sum(segmentation)))\n        self.resized_binar_segmentation = segmentation", "category": "Python"}, {"instruction": "def view_changed(self):\n        \"\"\" Called when this camera is changes its view. Also called\n        when its associated with a viewbox.\n        \"\"\"\n", "input": "", "output": "        if self._resetting:\n            return  # don't update anything while resetting (are in set_range)\n        if self._viewbox:\n            # Set range if necessary\n            if self._xlim is None:\n                args = self._set_range_args or ()\n                self.set_range(*args)\n            # Store default state if we have not set it yet\n            if self._default_state is None:\n                self.set_default_state()\n            # Do the actual update\n            self._update_transform()", "category": "Python"}, {"instruction": "def draw_marked_line(self, data, coordinates, linestyle, markerstyle,\n                         label, mplobj=None):\n        \"\"\"Draw a line that also has markers.\n\n        If this isn't reimplemented by a renderer object, by default, it will\n        make a call to BOTH draw_line and draw_markers when both markerstyle\n        and linestyle are not None in the same Line2D object.\n\n        \"\"\"\n", "input": "", "output": "        if linestyle is not None:\n            self.draw_line(data, coordinates, linestyle, label, mplobj)\n        if markerstyle is not None:\n            self.draw_markers(data, coordinates, markerstyle, label, mplobj)", "category": "Python"}, {"instruction": "def find_plugin(value,\n                key=DEFAULT_LOOKUP_KEY,\n                conn=None):\n    \"\"\"\n    get's the plugin matching the key and value\n\n    example: find_plugin(\"plugin1\", \"ServiceName\") => list of 0 or 1 item\n    example: find_plugin(\"plugin1\", \"Name\") => list of 0-to-many items\n\n    :param value:\n    :param key: <str> (default \"Name\")\n    :param conn:\n    :return:\n    \"\"\"\n", "input": "", "output": "    # cast to list to hide rethink internals from caller\n    result = list(RPC.filter({\n        key: value\n    }).run(conn))\n    return result", "category": "Python"}, {"instruction": "def cached(\n            cls,\n            release=MAX_ENSEMBL_RELEASE,\n            species=human,\n            server=ENSEMBL_FTP_SERVER):\n        \"\"\"\n        Construct EnsemblRelease if it's never been made before, otherwise\n        return an old instance.\n        \"\"\"\n", "input": "", "output": "        init_args_tuple = cls.normalize_init_values(release, species, server)\n        if init_args_tuple in cls._genome_cache:\n            genome = cls._genome_cache[init_args_tuple]\n        else:\n            genome = cls._genome_cache[init_args_tuple] = cls(*init_args_tuple)\n        return genome", "category": "Python"}, {"instruction": "def add_user(bridge_user):\n    \"\"\"\n    Add the bridge_user given\n    Return a list of BridgeUser objects with custom fields\n    \"\"\"\n", "input": "", "output": "    resp = post_resource(admin_uid_url(None) +\n                         (\"?%s\" % CUSTOM_FIELD),\n                         json.dumps(bridge_user.to_json_post(),\n                                    separators=(',', ':')))\n    return _process_json_resp_data(resp, no_custom_fields=True)", "category": "Python"}, {"instruction": "def AltTab(self, n=1, delay=0):\n        \"\"\"Press down Alt, then press n times Tab, then release Alt.\n        \"\"\"\n", "input": "", "output": "        self._delay(delay)\n        self.add(Command(\"KeyDown\", 'KeyDown \"%s\", %s' % (BoardKey.Alt, 1)))\n        for i in range(n):\n            self.add(Command(\"KeyPress\", 'KeyPress \"%s\", %s' % (BoardKey.Tab, 1)))\n        self.add(Command(\"KeyUp\", 'KeyUp \"%s\", %s' % (BoardKey.Alt, 1)))", "category": "Python"}, {"instruction": "def random_sample(col, n=5, filters=None):\n    \"\"\"Randomly select n document from query result set. If no query specified,\n    then from entire collection.\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u4ececollection\u4e2d\u968f\u673a\u9009\u62e9 ``n`` \u4e2a\u6837\u672c\u3002\n    \"\"\"\n", "input": "", "output": "    pipeline = list()\n    if filters is not None:\n        pipeline.append({\"$match\": filters})\n    pipeline.append({\"$sample\": {\"size\": n}})\n    return list(col.aggregate(pipeline))", "category": "Python"}, {"instruction": "def chunk(self, maxSize):\n        \"\"\"Splits the `Collection` into _maxSize_ size or smaller `Collections`\n\n        # Parameters\n\n        _maxSize_ : `int`\n\n        > The maximum number of elements in a retuned `Collection`\n\n\n        # Returns\n\n        `list [Collection]`\n\n        > A list of `Collections` that if all merged (`|` operator) would create the original\n        \"\"\"\n", "input": "", "output": "        chunks = []\n        currentSize = maxSize + 1\n        for i in self:\n            if currentSize >= maxSize:\n                currentSize = 0\n                chunks.append(type(self)({i}, name = 'Chunk-{}-of-{}'.format(len(chunks), self.name), quietStart = True))\n            else:\n                chunks[-1].add(i)\n            currentSize += 1\n        return chunks", "category": "Python"}, {"instruction": "def dlog(msg, log_path=DEFAULT_LOG_PATH):\n  \"\"\"A handy log utility for debugging multi-process, multi-threaded activities.\"\"\"\n", "input": "", "output": "  with open(log_path, 'a') as f:\n    f.write('\\n{}@{}: {}'.format(os.getpid(), threading.current_thread().name, msg))", "category": "Python"}, {"instruction": "def initialize_weights_nn(data, means, lognorm=True):\n    \"\"\"\n    Initializes the weights with a nearest-neighbor approach using the means.\n    \"\"\"\n", "input": "", "output": "    # TODO\n    genes, cells = data.shape\n    k = means.shape[1]\n    if lognorm:\n        data = log1p(cell_normalize(data))\n    for i in range(cells):\n        for j in range(k):\n            pass", "category": "Python"}, {"instruction": "def quote_identifier(identifier: str,\n                     mixed: Union[SQLCompiler, Engine, Dialect]) -> str:\n    \"\"\"\n    Converts an SQL identifier to a quoted version, via the SQL dialect in\n    use.\n\n    Args:\n        identifier: the identifier to be quoted\n        mixed: an SQLAlchemy :class:`SQLCompiler`, :class:`Engine`, or\n            :class:`Dialect` object\n\n    Returns:\n        the quoted identifier\n\n    \"\"\"\n", "input": "", "output": "    # See also http://sqlalchemy-utils.readthedocs.io/en/latest/_modules/sqlalchemy_utils/functions/orm.html  # noqa\n    return get_preparer(mixed).quote(identifier)", "category": "Python"}, {"instruction": "def create_gist(self, description, files, public=True):\n        \"\"\"Create a new gist.\n\n        If no login was provided, it will be anonymous.\n\n        :param str description: (required), description of gist\n        :param dict files: (required), file names with associated dictionaries\n            for content, e.g. ``{'spam.txt': {'content': 'File contents\n            ...'}}``\n        :param bool public: (optional), make the gist public if True\n        :returns: :class:`Gist <github3.gists.Gist>`\n        \"\"\"\n", "input": "", "output": "        new_gist = {'description': description, 'public': public,\n                    'files': files}\n        url = self._build_url('gists')\n        json = self._json(self._post(url, data=new_gist), 201)\n        return Gist(json, self) if json else None", "category": "Python"}, {"instruction": "def get_line(self, line=1):\n        \"\"\"Return a specific line.\"\"\"\n", "input": "", "output": "        verse_size = len(self.get_verse()) + 1\n        if line > 1:\n            verse = math.floor((line - 1) / verse_size)\n            line_in_verse = (line - 1) % verse_size\n            try:\n                return self.verses[verse][line_in_verse]\n            except IndexError:\n                return ''\n        else:\n            return self.verses[0][0]", "category": "Python"}, {"instruction": "async def start_transaction(connection_name: Optional[str] = None) -> BaseTransactionWrapper:\n    \"\"\"\n    Function to manually control your transaction.\n\n    Returns transaction object with ``.rollback()`` and ``.commit()`` methods.\n    All db calls in same coroutine context will run into transaction\n    before ending transaction with above methods.\n\n    :param connection_name: name of connection to run with, optional if you have only\n                            one db connection\n    \"\"\"\n", "input": "", "output": "    connection = _get_connection(connection_name)\n    transaction = connection._in_transaction()\n    await transaction.start()\n    return transaction", "category": "Python"}, {"instruction": "def shares(self):\n        \"\"\" A :class:`Feed <pypump.models.feed.Feed>`\n        of the people who've shared the object.\n\n        Example:\n            >>> for person in mynote.shares:\n            ...     print(person.webfinger)\n            ...\n            pypumptest1@pumpity.net\n            pypumptest2@pumpyourself.com\n        \"\"\"\n", "input": "", "output": "\n        endpoint = self.links[\"shares\"]\n        if self._shares is None:\n            self._shares = Feed(endpoint, pypump=self._pump)\n        return self._shares", "category": "Python"}, {"instruction": "def _get_hash_by_shell():\n    '''\n    Shell-out Python 3 for compute reliable hash\n    :return:\n    '''\n", "input": "", "output": "    id_ = __opts__.get('id', '')\n    id_hash = None\n    py_ver = sys.version_info[:2]\n    if py_ver >= (3, 3):\n        # Python 3.3 enabled hash randomization, so we need to shell out to get\n        # a reliable hash.\n        id_hash = __salt__['cmd.run']([sys.executable, '-c', 'print(hash(\"{0}\"))'.format(id_)],\n                                      env={'PYTHONHASHSEED': '0'})\n        try:\n            id_hash = int(id_hash)\n        except (TypeError, ValueError):\n            log.debug('Failed to hash the ID to get the server_id grain. Result of hash command: %s', id_hash)\n            id_hash = None\n    if id_hash is None:\n        # Python < 3.3 or error encountered above\n        id_hash = hash(id_)\n\n    return abs(id_hash % (2 ** 31))", "category": "Python"}, {"instruction": "def GetRootFileEntry(self):\n    \"\"\"Retrieves the root file entry.\n\n    Returns:\n      LVMFileEntry: root file entry or None if not available.\n    \"\"\"\n", "input": "", "output": "    path_spec = lvm_path_spec.LVMPathSpec(\n        location=self.LOCATION_ROOT, parent=self._path_spec.parent)\n    return self.GetFileEntryByPathSpec(path_spec)", "category": "Python"}, {"instruction": "def bg_process(func):\n    \"\"\"\n    A multiprocess decorator\n    :param func:\n    :return:\n    \"\"\"\n", "input": "", "output": "    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        p = multiprocessing.Process(target=func, args=args, kwargs=kwargs)\n        p.start()\n    return wrapper", "category": "Python"}, {"instruction": "def OECDas(self, to='name_short'):\n        \"\"\"\n        Return OECD member states in the specified classification\n\n        Parameters\n        ----------\n        to : str, optional\n            Output classification (valid str for an index of\n            country_data file), default: name_short\n\n        Returns\n        -------\n        Pandas DataFrame\n\n        \"\"\"\n", "input": "", "output": "        if isinstance(to, str):\n            to = [to]\n        return self.data[self.data.OECD > 0][to]", "category": "Python"}, {"instruction": "def _domain_event_watchdog_cb(conn, domain, action, opaque):\n    '''\n    Domain watchdog events handler\n    '''\n", "input": "", "output": "    _salt_send_domain_event(opaque, conn, domain, opaque['event'], {\n        'action': _get_libvirt_enum_string('VIR_DOMAIN_EVENT_WATCHDOG_', action)\n    })", "category": "Python"}, {"instruction": "def get(self, key, default=None):\n        \"\"\"Get an item - return default (None) if not present\"\"\"\n", "input": "", "output": "        if key not in self.table:\n            return default\n\n        return self[key]", "category": "Python"}, {"instruction": "def datetime(self):\n        \"\"\"A datetime representation of the location retrieval\"\"\"\n", "input": "", "output": "        return datetime.fromtimestamp(int(self.timestamp) / 1000, tz=pytz.utc)", "category": "Python"}, {"instruction": "def unpack_rpc_response(status, response=None, rpc_id=0, address=0):\n    \"\"\"Unpack an RPC status back in to payload or exception.\"\"\"\n", "input": "", "output": "\n    status_code = status & ((1 << 6) - 1)\n\n    if address == 8:\n        status_code &= ~(1 << 7)\n\n    if status == 0:\n        raise BusyRPCResponse()\n    elif status == 2:\n        raise RPCNotFoundError(\"rpc %d:%04X not found\" % (address, rpc_id))\n    elif status == 3:\n        raise RPCErrorCode(status_code)\n    elif status == 0xFF:\n        raise TileNotFoundError(\"tile %d not found\" % address)\n    elif status_code != 0:\n        raise RPCErrorCode(status_code)\n\n    if response is None:\n        response = b''\n\n    return response", "category": "Python"}, {"instruction": "def _get_desktop_size():\n  \"\"\"Get the desktop size.\"\"\"\n", "input": "", "output": "  if platform.system() == \"Linux\":\n    try:\n      xrandr_query = subprocess.check_output([\"xrandr\", \"--query\"])\n      sizes = re.findall(r\"\\bconnected primary (\\d+)x(\\d+)\", str(xrandr_query))\n      if sizes[0]:\n        return point.Point(int(sizes[0][0]), int(sizes[0][1]))\n    except:  # pylint: disable=bare-except\n      logging.error(\"Failed to get the resolution from xrandr.\")\n\n  # Most general, but doesn't understand multiple monitors.\n  display_info = pygame.display.Info()\n  return point.Point(display_info.current_w, display_info.current_h)", "category": "Python"}, {"instruction": "def trace(self, name, chain=-1):\n        \"\"\"Return the trace of a tallyable object stored in the database.\n\n        :Parameters:\n        name : string\n          The name of the tallyable object.\n        chain : int\n          The trace index. Setting `chain=i` will return the trace created by\n          the ith call to `sample`.\n        \"\"\"\n", "input": "", "output": "        if isinstance(name, str):\n            return self.db.trace(name, chain)\n        elif isinstance(name, Variable):\n            return self.db.trace(name.__name__, chain)\n        else:\n            raise ValueError(\n                'Name argument must be string or Variable, got %s.' %\n                name)", "category": "Python"}, {"instruction": "def _add_tag_manifest_file(zip_file, dir_name, tag_info_list):\n    \"\"\"Generate the tag manifest file and add it to the zip.\"\"\"\n", "input": "", "output": "    _add_tag_file(\n        zip_file, dir_name, tag_info_list, _gen_tag_manifest_file_tup(tag_info_list)\n    )", "category": "Python"}, {"instruction": "def _switch_tz_offset_sql(self, field_name, tzname):\n        \"\"\"\n        Returns the SQL that will convert field_name to UTC from tzname.\n        \"\"\"\n", "input": "", "output": "        field_name = self.quote_name(field_name)\n        if settings.USE_TZ:\n            if pytz is None:\n                from django.core.exceptions import ImproperlyConfigured\n                raise ImproperlyConfigured(\"This query requires pytz, \"\n                                           \"but it isn't installed.\")\n            tz = pytz.timezone(tzname)\n            td = tz.utcoffset(datetime.datetime(2000, 1, 1))\n\n            def total_seconds(td):\n                if hasattr(td, 'total_seconds'):\n                    return td.total_seconds()\n                else:\n                    return td.days * 24 * 60 * 60 + td.seconds\n\n            total_minutes = total_seconds(td) // 60\n            hours, minutes = divmod(total_minutes, 60)\n            tzoffset = \"%+03d:%02d\" % (hours, minutes)\n            field_name = \"CAST(SWITCHOFFSET(TODATETIMEOFFSET(%s, '+00:00'), '%s') AS DATETIME2)\" % (field_name, tzoffset)\n        return field_name", "category": "Python"}, {"instruction": "def update_one(self, update):\n        \"\"\"Update one document matching the selector.\n\n        :Parameters:\n          - `update` (dict): the update operations to apply\n        \"\"\"\n", "input": "", "output": "        self.__bulk.add_update(self.__selector,\n                               update, multi=False, upsert=True,\n                               collation=self.__collation)", "category": "Python"}, {"instruction": "def saveXml(self, xml):\r\n        \"\"\"\r\n        Saves the data for this tree to the inputed xml entry.\r\n        \r\n        :param      xml | <xml.etree.ElementTree.Element>\r\n        \r\n        :return     <bool> success\r\n        \"\"\"\n", "input": "", "output": "        if xml is None:\r\n            return False\r\n        \r\n        # save the grouping enabled information\r\n        xml.set('groupingActive', nativestring(self.isGroupingActive()))\r\n        \r\n        # save the grouping information\r\n        if self.groupBy():\r\n            xml.set('grouping', ','.join(self.groupBy()))\r\n        \r\n        # save standard tree options\r\n        return super(XOrbTreeWidget, self).saveXml(xml)", "category": "Python"}, {"instruction": "def _flush(self):\n        \"\"\"\n        Flush the write buffers of the stream if applicable.\n\n        In write mode, send the buffer content to the cloud object.\n        \"\"\"\n", "input": "", "output": "        # Flush buffer to specified range\n        buffer = self._get_buffer()\n        start = self._buffer_size * (self._seek - 1)\n        end = start + len(buffer)\n\n        future = self._workers.submit(\n            self._flush_range, buffer=buffer, start=start, end=end)\n        self._write_futures.append(future)\n        future.add_done_callback(partial(self._update_size, end))", "category": "Python"}, {"instruction": "def get_user_info(self, token):\n        \"\"\"\n        Retrieves the user info from the OAuth provider.\n\n        Arguments:\n            token (str): OAuth2 access token.\n\n        Returns:\n            dict\n\n        Raises:\n            UserInfoRetrievalFailed: Retrieval of user info from the remote server failed.\n        \"\"\"\n", "input": "", "output": "\n        url = self.get_user_info_url()\n\n        try:\n            headers = {'Authorization': 'Bearer {}'.format(token)}\n            response = requests.get(url, headers=headers)\n        except requests.RequestException:\n            logger.exception('Failed to retrieve user info due to a request exception.')\n            raise UserInfoRetrievalFailed\n\n        if response.status_code == 200:\n            return self.process_user_info_response(response.json())\n        else:\n            msg = 'Failed to retrieve user info. Server [{server}] responded with status [{status}].'.format(\n                server=url,\n                status=response.status_code\n            )\n            raise UserInfoRetrievalFailed(msg)", "category": "Python"}, {"instruction": "def classes_(self):\n        \"\"\"\n        Proxy property to smartly access the classes from the estimator or\n        stored locally on the score visualizer for visualization.\n        \"\"\"\n", "input": "", "output": "        if self.__classes is None:\n            try:\n                return self.estimator.classes_\n            except AttributeError:\n                return None\n        return self.__classes", "category": "Python"}, {"instruction": "def free_memory(self):\r\n        \"\"\"Free memory signal.\"\"\"\n", "input": "", "output": "        self.main.free_memory()\r\n        QTimer.singleShot(self.INITIAL_FREE_MEMORY_TIME_TRIGGER,\r\n                          lambda: self.main.free_memory())\r\n        QTimer.singleShot(self.SECONDARY_FREE_MEMORY_TIME_TRIGGER,\r\n                          lambda: self.main.free_memory())", "category": "Python"}, {"instruction": "def html(self, label, *msg):\n        \"\"\"\n        Prints html in notebook\n        \"\"\"\n", "input": "", "output": "        lbl = \"[\" + label + \"] \"\n        txt = lbl + \" \" + \" \".join(list(msg))\n        if self.notebook is True:\n            html = HTML(txt)\n            display(lbl + html)\n        else:\n            print(lbl + txt)", "category": "Python"}, {"instruction": "def StartService(service_name):\n  \"\"\"Start a Windows service with the given name.\n\n  Args:\n    service_name: string The name of the service to be started.\n  \"\"\"\n", "input": "", "output": "  try:\n    win32serviceutil.StartService(service_name)\n    logging.info(\"Service '%s' started.\", service_name)\n  except pywintypes.error as e:\n    if getattr(e, \"winerror\", None) == winerror.ERROR_SERVICE_DOES_NOT_EXIST:\n      logging.debug(\"Tried to start '%s', but the service is not installed.\",\n                    service_name)\n    else:\n      logging.exception(\"Encountered error trying to start '%s':\", service_name)", "category": "Python"}, {"instruction": "def new(self):\n        # type: () -> None\n        '''\n        A method to create a new UDF Partition Volume Descriptor.\n\n        Parameters:\n         None.\n        Returns:\n         Nothing.\n        '''\n", "input": "", "output": "        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF Partition Volume Descriptor already initialized')\n\n        self.desc_tag = UDFTag()\n        self.desc_tag.new(5)  # FIXME: we should let the user set serial_number\n\n        self.vol_desc_seqnum = 2\n        self.part_flags = 1  # FIXME: how should we set this?\n        self.part_num = 0  # FIXME: how should we set this?\n\n        self.part_contents = UDFEntityID()\n        self.part_contents.new(2, b'+NSR02')\n\n        self.part_contents_use = UDFPartitionHeaderDescriptor()\n        self.part_contents_use.new()\n\n        self.access_type = 1\n        self.part_start_location = 0  # This will get set later\n        self.part_length = 3  # This will get set later\n\n        self.impl_ident = UDFEntityID()\n        self.impl_ident.new(0, b'*pycdlib')\n\n        self.implementation_use = b'\\x00' * 128  # FIXME: we should let the user set this\n\n        self._initialized = True", "category": "Python"}, {"instruction": "def parse_profile_from_hcard(hcard: str, handle: str):\n    \"\"\"\n    Parse all the fields we can from a hCard document to get a Profile.\n\n    :arg hcard: HTML hcard document (str)\n    :arg handle: User handle in username@domain.tld format\n    :returns: ``federation.entities.diaspora.entities.DiasporaProfile`` instance\n    \"\"\"\n", "input": "", "output": "    from federation.entities.diaspora.entities import DiasporaProfile  # Circulars\n    doc = html.fromstring(hcard)\n    profile = DiasporaProfile(\n        name=_get_element_text_or_none(doc, \".fn\"),\n        image_urls={\n            \"small\": _get_element_attr_or_none(doc, \".entity_photo_small .photo\", \"src\"),\n            \"medium\": _get_element_attr_or_none(doc, \".entity_photo_medium .photo\", \"src\"),\n            \"large\": _get_element_attr_or_none(doc, \".entity_photo .photo\", \"src\"),\n        },\n        public=True if _get_element_text_or_none(doc, \".searchable\") == \"true\" else False,\n        id=handle,\n        handle=handle,\n        guid=_get_element_text_or_none(doc, \".uid\"),\n        public_key=_get_element_text_or_none(doc, \".key\"),\n    )\n    return profile", "category": "Python"}, {"instruction": "def get_comment(self):\n        \"\"\"ret: string()\"\"\"\n", "input": "", "output": "        self.skip()\n        offset = self.offset\n        m = syntax.re_comment.match(self.buf)\n        if m == None:\n            return None\n        else:\n            cmt = m.group(0)\n            self.set_buf(m.end())\n            # look for a multiline comment\n            if cmt[:2] == '/*' and cmt[-2:] != '*/':\n                i = self.buf.find('*/')\n                while i == -1:\n                    self.readline()\n                    # remove at most the same number of whitespace as\n                    # the comment start was indented\n                    j = 0\n                    while (j < offset and j < len(self.buf) and\n                           self.buf[j].isspace()):\n                        j = j + 1\n                    self.buf = self.buf[j:]\n                    cmt += '\\n'+self.buf.replace('\\n','')\n                    i = self.buf.find('*/')\n                self.set_buf(i+2)\n            self.skip()\n            return cmt", "category": "Python"}, {"instruction": "def hasannotation(self,Class,set=None):\n        \"\"\"Returns an integer indicating whether such as annotation exists, and if so, how many.\n\n        See :meth:`AllowTokenAnnotation.annotations`` for a description of the parameters.\"\"\"\n", "input": "", "output": "        return sum( 1 for _ in self.select(Class,set,True,default_ignore_annotations))", "category": "Python"}, {"instruction": "def get_nac_eigendisplacements_along_dir(self, direction):\n        \"\"\"\n        Returns the nac_eigendisplacements for the given direction (not necessarily a versor).\n        None if the direction is not present or nac_eigendisplacements has not been calculated.\n\n        Args:\n            direction: the direction as a list of 3 elements\n        Returns:\n            the eigendisplacements as a numpy array of complex numbers with shape\n            (3*len(structure), len(structure), 3). None if not found.\n        \"\"\"\n", "input": "", "output": "        versor = [i / np.linalg.norm(direction) for i in direction]\n        for d, e in self.nac_eigendisplacements:\n            if np.allclose(versor, d):\n                return e\n\n        return None", "category": "Python"}, {"instruction": "def get_identity(identity):\n    \"\"\"Returns some information about the currently authenticated identity\"\"\"\n", "input": "", "output": "    return flask.Response(\n        json.dumps(\n            {\n                'identity': {\n                    'id': identity.id,\n                    'etag': identity.etag,\n                    'name': identity.name,\n                    'fullname': identity.fullname,\n                    'email': identity.email,\n                    'timezone': identity.timezone,\n                    'teams': _encode_dict(identity.teams)\n                }\n            }\n        ), 200,\n        headers={'ETag': identity.etag},\n        content_type='application/json'\n    )", "category": "Python"}, {"instruction": "def is_connected(H, source_node, target_node):\n    \"\"\"Checks if a target node is connected to a source node. That is,\n    this method determines if a target node can be visited from the source\n    node in the sense of the 'Visit' algorithm.\n\n    Refer to 'visit's documentation for more details.\n\n    :param H: the hypergraph to check connectedness on.\n    :param source_node: the node to check connectedness to.\n    :param target_node: the node to check connectedness of.\n    :returns: bool -- whether target_node can be visited from source_node.\n\n    \"\"\"\n", "input": "", "output": "    visited_nodes, Pv, Pe = visit(H, source_node)\n    return target_node in visited_nodes", "category": "Python"}, {"instruction": "def remove_absolute_impute__roc_auc(X, y, model_generator, method_name, num_fcounts=11):\n    \"\"\" Remove Absolute (impute)\n    xlabel = \"Max fraction of features removed\"\n    ylabel = \"1 - ROC AUC\"\n    transform = \"one_minus\"\n    sort_order = 9\n    \"\"\"\n", "input": "", "output": "    return __run_measure(measures.remove_mask, X, y, model_generator, method_name, 0, num_fcounts, sklearn.metrics.roc_auc_score)", "category": "Python"}, {"instruction": "def amqp_publish(self, exchange, routing_key, body, properties=None):\n        \"\"\"Publish a message to RabbitMQ\n\n        :param str exchange: The exchange to publish the message to\n        :param str routing_key: The routing key to publish the message with\n        :param bytes body: The message body to send\n        :param dict properties: An optional dict of AMQP properties\n        :rtype: tornado.concurrent.Future\n\n        :raises: :exc:`sprockets.mixins.amqp.AMQPError`\n        :raises: :exc:`sprockets.mixins.amqp.NotReadyError`\n        :raises: :exc:`sprockets.mixins.amqp.PublishingError`\n\n        \"\"\"\n", "input": "", "output": "        properties = properties or {}\n        if hasattr(self, 'correlation_id') and getattr(self, 'correlation_id'):\n            properties.setdefault('correlation_id', self.correlation_id)\n        return self.application.amqp.publish(\n            exchange, routing_key, body, properties)", "category": "Python"}, {"instruction": "def get_clouds(wxdata: [str]) -> ([str], list):  # type: ignore\n    \"\"\"\n    Returns the report list and removed list of split cloud layers\n    \"\"\"\n", "input": "", "output": "    clouds = []\n    for i, item in reversed(list(enumerate(wxdata))):\n        if item[:3] in CLOUD_LIST or item[:2] == 'VV':\n            cloud = wxdata.pop(i)\n            clouds.append(make_cloud(cloud))\n    return wxdata, sorted(clouds, key=lambda cloud: (cloud.altitude, cloud.type))", "category": "Python"}, {"instruction": "def filter_alias_create_namespace(namespace):\n    \"\"\"\n    Filter alias name and alias command inside alias create namespace to appropriate strings.\n\n    Args\n        namespace: The alias create namespace.\n\n    Returns:\n        Filtered namespace where excessive whitespaces are removed in strings.\n    \"\"\"\n", "input": "", "output": "    def filter_string(s):\n        return ' '.join(s.strip().split())\n\n    namespace.alias_name = filter_string(namespace.alias_name)\n    namespace.alias_command = filter_string(namespace.alias_command)\n    return namespace", "category": "Python"}, {"instruction": "def _make_cmap(colors, position=None, bit=False):\n    '''\n    _make_cmap takes a list of tuples which contain RGB values. The RGB\n    values may either be in 8-bit [0 to 255] (in which bit must be set to\n    True when called) or arithmetic [0 to 1] (default). _make_cmap returns\n    a cmap with equally spaced colors.\n    Arrange your tuples so that the first color is the lowest value for the\n    colorbar and the last is the highest.\n    position contains values from 0 to 1 to dictate the location of each color.\n    '''\n", "input": "", "output": "    bit_rgb = np.linspace(0,1,256)\n    if position == None:\n        position = np.linspace(0,1,len(colors))\n    else:\n        if len(position) != len(colors):\n            sys.exit(\"position length must be the same as colors\")\n        elif position[0] != 0 or position[-1] != 1:\n            sys.exit(\"position must start with 0 and end with 1\")\n    palette = [(i, (float(r), float(g), float(b), float(a))) for\n    i, (r, g, b, a) in enumerate(colors)]\n    cmap = Colormap(*palette)\n    return cmap", "category": "Python"}, {"instruction": "def _set_session_cookie(self):\n        \"\"\"Set the session data cookie.\"\"\"\n", "input": "", "output": "        LOGGER.debug('Setting session cookie for %s', self.session.id)\n        self.set_secure_cookie(name=self._session_cookie_name,\n                               value=self.session.id,\n                               expires=self._cookie_expiration)", "category": "Python"}, {"instruction": "def delete_fabric_fw(self, tenant_id, fw_dict, is_fw_virt, result):\n        \"\"\"Top level routine to unconfigure the fabric. \"\"\"\n", "input": "", "output": "        try:\n            with self.mutex_lock:\n                ret = self.delete_fabric_fw_internal(tenant_id, fw_dict,\n                                                     is_fw_virt, result)\n        except Exception as exc:\n            LOG.error(\"Exception raised in delete fabric %s\", str(exc))\n            return False\n        return ret", "category": "Python"}, {"instruction": "def _overlapping(files):\n    \"\"\"Quick method to see if a file list contains overlapping files\n    \"\"\"\n", "input": "", "output": "    segments = set()\n    for path in files:\n        seg = file_segment(path)\n        for s in segments:\n            if seg.intersects(s):\n                return True\n        segments.add(seg)\n    return False", "category": "Python"}, {"instruction": "def _add_chrome_arguments(self, options):\n        \"\"\"Add Chrome arguments from properties file\n\n        :param options: chrome options object\n        \"\"\"\n", "input": "", "output": "        try:\n            for pref, pref_value in dict(self.config.items('ChromeArguments')).items():\n                pref_value = '={}'.format(pref_value) if pref_value else ''\n                self.logger.debug(\"Added chrome argument: %s%s\", pref, pref_value)\n                options.add_argument('{}{}'.format(pref, self._convert_property_type(pref_value)))\n        except NoSectionError:\n            pass", "category": "Python"}, {"instruction": "def clear(self, *args):\n        \"\"\"\n        Clears the LED matrix with a single colour, default is black / off\n\n        e.g. ap.clear()\n        or\n        ap.clear(r, g, b)\n        or\n        colour = (r, g, b)\n        ap.clear(colour)\n        \"\"\"\n", "input": "", "output": "\n        black = (0, 0, 0)  # default\n\n        if len(args) == 0:\n            colour = black\n        elif len(args) == 1:\n            colour = args[0]\n        elif len(args) == 3:\n            colour = args\n        else:\n            raise ValueError('Pixel arguments must be given as (r, g, b) or r, g, b')\n\n        self.set_pixels([colour] * 64)", "category": "Python"}, {"instruction": "def _read_time_from_string(str1):\n    \"\"\"\n    Reads the time from a string in the format HH:MM:SS.S and returns\n    :class: datetime.time\n    \"\"\"\n", "input": "", "output": "    full_time = [float(x) for x in str1.split(':')]\n    hour = int(full_time[0])\n    minute = int(full_time[1])\n    if full_time[2] > 59.99:\n        minute += 1\n        second = 0\n    else:\n        second = int(full_time[2])\n    microseconds = int((full_time[2] - floor(full_time[2])) * 1000000)\n    return datetime.time(hour, minute, second, microseconds)", "category": "Python"}, {"instruction": "def prior_draw(self, N=1):\n        \"\"\"\n        Draw ``N`` samples from the prior.\n        \"\"\"\n", "input": "", "output": "        p = np.random.ranf(size=(N, self.ndim))\n        p = (self._upper_right - self._lower_left) * p + self._lower_left\n        return p", "category": "Python"}, {"instruction": "def attrs(self) -> Mapping:\n        \"\"\"Dictionary of global attributes on this dataset\n        \"\"\"\n", "input": "", "output": "        if self._attrs is None:\n            self._attrs = OrderedDict()\n        return self._attrs", "category": "Python"}, {"instruction": "def create_cloud(self):\n        \"\"\"\n        Create instances for the cloud providers\n        \"\"\"\n", "input": "", "output": "        instances = []\n        for i in range(self.settings['NUMBER_NODES']):\n            new_instance = Instance.new(settings=self.settings, cluster=self)\n            instances.append(new_instance)\n\n        create_nodes = [instance.create(suffix=i) for i, instance in enumerate(instances)]\n        fetch_nodes = [instance.node for instance in instances]\n        self.driver.wait_until_running(fetch_nodes)\n\n        node_ids = [node.id for node in fetch_nodes]\n        all_nodes = self.driver.list_nodes()\n        new_nodes = [node for node in all_nodes if node.id in node_ids]\n        for instance, node in zip(instances, new_nodes):\n            instance.node = node\n\n        self.instances = instances", "category": "Python"}, {"instruction": "def structured_partlist(input, timeout=20, showgui=False):\n    '''export partlist by eagle, then parse it\n\n    :param input: .sch or .brd file name\n    :param timeout: int\n    :param showgui: Bool, True -> do not hide eagle GUI\n    :rtype: tuple of header list and dict list: (['part','value',..], [{'part':'C1', 'value':'1n'}, ..])\n    '''\n", "input": "", "output": "\n    s = raw_partlist(input=input, timeout=timeout, showgui=showgui)\n    return parse_partlist(s)", "category": "Python"}, {"instruction": "def force_text(s, encoding='utf-8',  errors='strict'):\n    \"\"\"A function turns \"s\" into text type, similar to django.utils.encoding.force_text\n    \"\"\"\n", "input": "", "output": "    if issubclass(type(s), str):\n        return s\n    try:\n        if isinstance(s, bytes):\n            s = str(s, encoding, errors)\n        else:\n            s = str(s)\n    except UnicodeDecodeError as e:\n        raise DjangoUnicodeDecodeError(s, *e.args)\n    return s", "category": "Python"}, {"instruction": "def _address_rxp(self, addr):\n        \"\"\" Create a regex string for addresses, that matches several representations:\n            - with(out) '0x' prefix\n            - `pex` version\n            This function takes care of maintaining additional lookup keys for substring matches.\n            In case the given string is no address, it returns the original string.\n        \"\"\"\n", "input": "", "output": "        try:\n            addr = to_checksum_address(addr)\n            rxp = '(?:0x)?' + pex(address_checksum_and_decode(addr)) + f'(?:{addr.lower()[10:]})?'\n            self._extra_keys[pex(address_checksum_and_decode(addr))] = addr.lower()\n            self._extra_keys[addr[2:].lower()] = addr.lower()\n        except ValueError:\n            rxp = addr\n        return rxp", "category": "Python"}, {"instruction": "def get_assessments(self):\n        \"\"\"Gets any assessments associated with this activity.\n\n        return: (osid.assessment.AssessmentList) - list of assessments\n        raise:  IllegalState - ``is_assessment_based_activity()`` is\n                ``false``\n        raise:  OperationFailed - unable to complete request\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for osid.learning.Activity.get_assets_template\n        if not bool(self._my_map['assessmentIds']):\n            raise errors.IllegalState('no assessmentIds')\n        mgr = self._get_provider_manager('ASSESSMENT')\n        if not mgr.supports_assessment_lookup():\n            raise errors.OperationFailed('Assessment does not support Assessment lookup')\n\n        # What about the Proxy?\n        lookup_session = mgr.get_assessment_lookup_session(proxy=getattr(self, \"_proxy\", None))\n        lookup_session.use_federated_bank_view()\n        return lookup_session.get_assessments_by_ids(self.get_assessment_ids())", "category": "Python"}, {"instruction": "def get_http_info(self, request):\n        \"\"\"\n        Determine how to retrieve actual data by using request.mimetype.\n        \"\"\"\n", "input": "", "output": "        if self.is_json_type(request.mimetype):\n            retriever = self.get_json_data\n        else:\n            retriever = self.get_form_data\n        return self.get_http_info_with_retriever(request, retriever)", "category": "Python"}, {"instruction": "def on_ctcp(self, connection, event):\n        \"\"\"Default handler for ctcp events.\n\n        Replies to VERSION and PING requests and relays DCC requests\n        to the on_dccchat method.\n        \"\"\"\n", "input": "", "output": "        nick = event.source.nick\n        if event.arguments[0] == \"VERSION\":\n            connection.ctcp_reply(nick, \"VERSION \" + self.get_version())\n        elif event.arguments[0] == \"PING\":\n            if len(event.arguments) > 1:\n                connection.ctcp_reply(nick, \"PING \" + event.arguments[1])\n        elif (\n            event.arguments[0] == \"DCC\"\n                and event.arguments[1].split(\" \", 1)[0] == \"CHAT\"):\n            self.on_dccchat(connection, event)", "category": "Python"}, {"instruction": "def google_app_engine_ndb_delete_expired_sessions(dormant_for=86400, limit=500):\n    \"\"\"\n    Deletes expired sessions\n    A session is expired if it expires date is set and has passed or\n    if it has not been accessed for a given period of time.\n\n    :param dormant_for: seconds since last access to delete sessions, defaults to 24 hours.\n    :type dormant_for: int\n    :param limit: amount to delete in one call of the method, the maximum and default for this is the NDB fetch limit of 500\n    :type limit: int\n    \"\"\"\n", "input": "", "output": "    from vishnu.backend.client.google_app_engine_ndb import VishnuSession\n    from google.appengine.ext import ndb\n    from datetime import datetime\n    from datetime import timedelta\n\n    now = datetime.utcnow()\n    last_accessed = now - timedelta(seconds=dormant_for)\n\n    query = VishnuSession.query(ndb.OR(\n        ndb.AND(VishnuSession.expires <= now, VishnuSession.expires != None),\n        VishnuSession.last_accessed <= last_accessed\n    ))\n    results = query.fetch(keys_only=True, limit=limit)\n\n    ndb.delete_multi(results)\n\n    return len(results) < limit", "category": "Python"}, {"instruction": "def _VarintBytes(value):\n  \"\"\"Encode the given integer as a varint and return the bytes.  This is only\n  called at startup time so it doesn't need to be fast.\"\"\"\n", "input": "", "output": "\n  pieces = []\n  _EncodeVarint(pieces.append, value)\n  return b\"\".join(pieces)", "category": "Python"}, {"instruction": "def bulk_get(cls, exports, api=None):\n        \"\"\"\n        Retrieve exports in bulk.\n        :param exports: Exports to be retrieved.\n        :param api: Api instance.\n        :return: list of ExportBulkRecord objects.\n        \"\"\"\n", "input": "", "output": "        api = api or cls._API\n        export_ids = [Transform.to_export(export) for export in exports]\n        data = {'export_ids': export_ids}\n\n        response = api.post(url=cls._URL['bulk_get'], data=data)\n        return ExportBulkRecord.parse_records(response=response, api=api)", "category": "Python"}, {"instruction": "def add_license(key, description, safety_checks=True,\n                service_instance=None):\n    '''\n    Adds a license to the vCenter or ESXi host\n\n    key\n        License key.\n\n    description\n        License description added in as a label.\n\n    safety_checks\n        Specify whether to perform safety check or to skip the checks and try\n        performing the required task\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter/ESXi host.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.add_license key=<license_key> desc='License desc'\n    '''\n", "input": "", "output": "    log.trace('Adding license \\'%s\\'', key)\n    salt.utils.vmware.add_license(service_instance, key, description)\n    return True", "category": "Python"}, {"instruction": "def get_handler(progname, fmt=None, datefmt=None, project_id=None,\n                credentials=None, debug_thread_worker=False, **_):\n    \"\"\"Helper function to create a Stackdriver handler.\n\n    See `ulogger.stackdriver.CloudLoggingHandlerBuilder` for arguments\n    and supported keyword arguments.\n\n    Returns:\n        (obj): Instance of `google.cloud.logging.handlers.\n                            CloudLoggingHandler`\n    \"\"\"\n", "input": "", "output": "    builder = CloudLoggingHandlerBuilder(\n        progname, fmt=fmt, datefmt=datefmt, project_id=project_id,\n        credentials=credentials, debug_thread_worker=debug_thread_worker)\n    return builder.get_handler()", "category": "Python"}, {"instruction": "def reset_server_and_request_check(self, address):\n        \"\"\"Clear our pool for a server, mark it Unknown, and check it soon.\"\"\"\n", "input": "", "output": "        with self._lock:\n            self._reset_server(address)\n            self._request_check(address)", "category": "Python"}, {"instruction": "def _build_tpm(tpm):\n        \"\"\"Validate the TPM passed by the user and convert to multidimensional\n        form.\n        \"\"\"\n", "input": "", "output": "        tpm = np.array(tpm)\n\n        validate.tpm(tpm)\n\n        # Convert to multidimensional state-by-node form\n        if is_state_by_state(tpm):\n            tpm = convert.state_by_state2state_by_node(tpm)\n        else:\n            tpm = convert.to_multidimensional(tpm)\n\n        utils.np_immutable(tpm)\n\n        return (tpm, utils.np_hash(tpm))", "category": "Python"}, {"instruction": "def zoom_in_frag(self, curr_frag):\n        \"\"\"\n        :param curr_frag:\n        \"\"\"\n", "input": "", "output": "        level = curr_frag[1]\n        frag = curr_frag[0]\n        output = []\n        if level > 0:\n            str_level = str(level)\n            sub_low = self.spec_level[str_level][\"fragments_dict\"][frag][\n                \"sub_low_index\"\n            ]\n            sub_high = self.spec_level[str_level][\"fragments_dict\"][frag][\n                \"sub_high_index\"\n            ]\n            new_level = level - 1\n            for i in range(sub_low, sub_high + 1):\n                output.append((i, new_level))\n        else:\n            output.append(curr_frag)\n        return output", "category": "Python"}, {"instruction": "def cancel_order(self, order_id):\n        \"\"\"\n        Send a request to cancel an order, return the response.\n\n        Arguments:\n        order_id - the order id to cancel\n        \"\"\"\n", "input": "", "output": "        request = '/v1/order/cancel'\n        url = self.base_url + request\n        params = {\n            'request': request,\n            'nonce': self.get_nonce(),\n            'order_id': order_id\n        }\n\n        return requests.post(url, headers=self.prepare(params))", "category": "Python"}, {"instruction": "def dbg_print(self):\n        \"\"\"\n        Print out debugging information\n        \"\"\"\n", "input": "", "output": "        for region_id, region in self.regions.items():\n            print(\"Region [%s]:\" % region_id)\n            region.dbg_print(indent=2)", "category": "Python"}, {"instruction": "def _raise_for_status(response):\n    \"\"\"Raises stored :class:`HTTPError`, if one occurred.\n\n    This is the :meth:`requests.models.Response.raise_for_status` method,\n    modified to add the response from Space-Track, if given.\n    \"\"\"\n", "input": "", "output": "\n    http_error_msg = ''\n\n    if 400 <= response.status_code < 500:\n        http_error_msg = '%s Client Error: %s for url: %s' % (\n            response.status_code, response.reason, response.url)\n\n    elif 500 <= response.status_code < 600:\n        http_error_msg = '%s Server Error: %s for url: %s' % (\n            response.status_code, response.reason, response.url)\n\n    if http_error_msg:\n        spacetrack_error_msg = None\n\n        try:\n            json = response.json()\n            if isinstance(json, Mapping):\n                spacetrack_error_msg = json['error']\n        except (ValueError, KeyError):\n            pass\n\n        if not spacetrack_error_msg:\n            spacetrack_error_msg = response.text\n\n        if spacetrack_error_msg:\n            http_error_msg += '\\nSpace-Track response:\\n' + spacetrack_error_msg\n\n        raise requests.HTTPError(http_error_msg, response=response)", "category": "Python"}, {"instruction": "def __get_datasetname(d, filename):\n    \"\"\"\n    Get the filename based on the dataset name in the metadata\n    :param str filename: Filename.lpd\n    :return str: Filename\n    \"\"\"\n", "input": "", "output": "    try:\n        filename = d[\"dataSetName\"]\n    except KeyError:\n        logger_excel.info(\"get_datasetname: KeyError: No dataSetName found. Reverting to: {}\".format(filename))\n    return filename", "category": "Python"}, {"instruction": "def _run__group(self, action, replace):\n        \"\"\"\n        Run a group of actions in sequence.\n\n        >>> Action().run(\"several\", actions={\n        ...     \"several\": {\n        ...         \"type\": \"group\",\n        ...         \"actions\": [\"hello\",\"call\",\"then\"]\n        ...     }, \"hello\": {\n        ...         \"type\": \"exec\",\n        ...         \"cmd\": \"echo version=%{version}\"\n        ...     }, \"call\": {\n        ...         \"type\": \"hook\",\n        ...         \"url\": \"http://reflex.cold.org\"\n        ...     }, \"then\": {\n        ...         \"type\": \"exec\",\n        ...         \"cmd\": \"echo finished\"\n        ... }}, replace={\n        ...     \"version\": \"1712.10\"\n        ... })\n        version=1712.10\n        \"\"\"\n", "input": "", "output": "\n        for target in action.get('actions', []):\n            Action().run(target, actions=self.actions, replace=replace)", "category": "Python"}, {"instruction": "async def refresh_token(self, refresh_token):\n        \"\"\"\n        :param refresh_token: an openid refresh-token from a previous token request\n        \"\"\"\n", "input": "", "output": "        async with self._client_session() as client:\n            well_known = await self._get_well_known(client)\n\n            try:\n                return await self._post(\n                    client,\n                    well_known['token_endpoint'],\n                    data={\n                        'grant_type': GRANT_TYPE_REFRESH_TOKEN,\n                        'refresh_token': refresh_token,\n                    }\n                )\n            except aiohttp.ClientResponseError as e:\n                raise ConfigException('oidc: failed to refresh access token')", "category": "Python"}, {"instruction": "def _compute_childtab_next_l_index(self, lcptab):\n        \"\"\"Computes the child 'next l index' array in O(n) based on the LCP table.\n\n        Abouelhoda et al. (2004).\n        \"\"\"\n", "input": "", "output": "        stack = [0]\n        n = len(lcptab)\n        childtab_next_l_index = np.zeros(n, dtype=np.int)  # Zeros / -1 ?\n        for i in xrange(n):\n            while lcptab[i] < lcptab[stack[-1]]:\n                stack.pop()\n            if lcptab[i] == lcptab[stack[-1]]:\n                last_index = stack.pop()\n                childtab_next_l_index[last_index] = i\n            stack.append(i)\n        return childtab_next_l_index", "category": "Python"}, {"instruction": "async def AddPortMapping(NewRemoteHost: str, NewExternalPort: int, NewProtocol: str, NewInternalPort: int,\n                       NewInternalClient: str, NewEnabled: int, NewPortMappingDescription: str,\n                       NewLeaseDuration: str) -> None:\n        \"\"\"Returns None\"\"\"\n", "input": "", "output": "        raise NotImplementedError()", "category": "Python"}, {"instruction": "def _find_input_dependencies(self, inputs):\n        \"\"\"Use the predecessor tree to find dependencies based on inputs.\n\n        Returns: A list of transaction ids.\n        \"\"\"\n", "input": "", "output": "        dependencies = []\n        for address in inputs:\n            dependencies.extend(\n                self._predecessor_tree.find_read_predecessors(address))\n        return dependencies", "category": "Python"}, {"instruction": "def visit_If(self, node):\n        \"\"\"Eliminate dead code.\"\"\"\n", "input": "", "output": "        # do not optimize ifs that have a block inside so that it doesn't\n        # break super().\n        if node.find(nodes.Block) is not None:\n            return self.generic_visit(node)\n        try:\n            val = self.visit(node.test).as_const()\n        except nodes.Impossible:\n            return self.generic_visit(node)\n        if val:\n            body = node.body\n        else:\n            body = node.else_\n        result = []\n        for node in body:\n            result.extend(self.visit_list(node))\n        return result", "category": "Python"}, {"instruction": "def lookup_consumer(self, key):\n        \"\"\"\n        Search through keys\n        \"\"\"\n", "input": "", "output": "        if not self.consumers:\n            log.critical((\"No consumers defined in settings.\"\n                          \"Have you created a configuration file?\"))\n            return None\n\n        consumer = self.consumers.get(key)\n        if not consumer:\n            log.info(\"Did not find consumer, using key: %s \", key)\n            return None\n\n        secret = consumer.get('secret', None)\n        if not secret:\n            log.critical(('Consumer %s, is missing secret'\n                          'in settings file, and needs correction.'), key)\n            return None\n        return oauth2.Consumer(key, secret)", "category": "Python"}, {"instruction": "def bold_if_not_blank(x: Optional[str]) -> str:\n    \"\"\"\n    HTML-emboldens content, unless blank.\n    \"\"\"\n", "input": "", "output": "    if x is None:\n        return u\"{}\".format(x)\n    return u\"<b>{}</b>\".format(x)", "category": "Python"}, {"instruction": "def to_yaml(cls, dumper, vividict):\n        \"\"\"Implementation for the abstract method of the base class YAMLObject\n        \"\"\"\n", "input": "", "output": "        dictionary = cls.vividict_to_dict(vividict)\n        node = dumper.represent_mapping(cls.yaml_tag, dictionary)\n        return node", "category": "Python"}, {"instruction": "def bfill(self, dim, limit=None):\n        '''Fill NaN values by propogating values backward\n\n        *Requires bottleneck.*\n\n        Parameters\n        ----------\n        dim : str\n            Specifies the dimension along which to propagate values when\n            filling.\n        limit : int, default None\n            The maximum number of consecutive NaN values to backward fill. In\n            other words, if there is a gap with more than this number of\n            consecutive NaNs, it will only be partially filled. Must be greater\n            than 0 or None for no limit.\n\n        Returns\n        -------\n        Dataset\n        '''\n", "input": "", "output": "        from .missing import bfill, _apply_over_vars_with_dim\n\n        new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)\n        return new", "category": "Python"}, {"instruction": "def nameop_set_collided( cls, nameop, history_id_key, history_id ):\n        \"\"\"\n        Mark a nameop as collided\n        \"\"\"\n", "input": "", "output": "        nameop['__collided__'] = True\n        nameop['__collided_history_id_key__'] = history_id_key \n        nameop['__collided_history_id__'] = history_id", "category": "Python"}, {"instruction": "def _Q_to_filter(self, q):\n        \"\"\"\n        Convert a Q object to filter\n        :param q: a Q Object\n        :return: a filter object\n        \"\"\"\n", "input": "", "output": "        default_filter = ANDFilter\n        if q.connector == \"OR\":\n            default_filter = ORFilter\n        filters = []\n        for child in q.children:\n            if isinstance(child, Q):\n                if child.children:\n                    filters.append(self._Q_to_filter(child))\n            elif isinstance(child, tuple):\n                field, value = child\n                filters.append(self._build_inner_filter(field, value))\n        if len(filters) == 1:\n            filter = filters[0]\n            if q.negated:\n                return NotFilter(filter)\n            return filter\n        if q.negated:\n            return NotFilter(default_filter(filters))\n        return default_filter(filters)", "category": "Python"}, {"instruction": "def create_data_figs(self):\n        \"\"\"\n        Generate the data and figs files for the report\n\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        logger.info(\"Generating the report data and figs from %s to %s\",\n                    self.start_date, self.end_date)\n\n        self.get_sec_overview()\n        self.get_sec_project_activity()\n        self.get_sec_project_community()\n        self.get_sec_project_process()\n\n        logger.info(\"Data and figs done\")", "category": "Python"}, {"instruction": "def setProperty(self, name, value):\n        '''\n        Called by the engine to set a driver property value.\n\n        @param name: Name of the property\n        @type name: str\n        @param value: Property value\n        @type value: object\n        '''\n", "input": "", "output": "        self._push(self._driver.setProperty, (name, value))", "category": "Python"}, {"instruction": "def as_items(self, decode=False):\n        \"\"\"\n        Return a list of 2-tuples consisting of key/score.\n        \"\"\"\n", "input": "", "output": "        items = self.database.zrange(self.key, 0, -1, withscores=True)\n        if decode:\n            items = [(_decode(k), score) for k, score in items]\n        return items", "category": "Python"}, {"instruction": "def nextLunarEclipse(date):\n    \"\"\" Returns the Datetime of the maximum phase of the\n    next global lunar eclipse.\n\n    \"\"\"\n", "input": "", "output": "\n    eclipse = swe.lunarEclipseGlobal(date.jd, backward=False)\n    return Datetime.fromJD(eclipse['maximum'], date.utcoffset)", "category": "Python"}, {"instruction": "def woe(df, feature_name, target_name):\n    \"\"\"Calculate weight of evidence.\n\n    Parameters\n    ----------\n    df: Dataframe\n    feature_name: str\n        Column name to encode.\n    target_name: str\n        Target column name.\n\n    Returns\n    -------\n    Series\n\n    \"\"\"\n", "input": "", "output": "\n    def group_woe(group):\n        event = float(group.sum())\n        non_event = group.shape[0] - event\n\n        rel_event = event / event_total\n        rel_non_event = non_event / non_event_total\n\n        return np.log(rel_non_event / rel_event) * 100\n\n    if df[target_name].nunique() > 2:\n        raise ValueError('Target column should be binary (1/0).')\n\n    event_total = float(df[df[target_name] == 1.0].shape[0])\n    non_event_total = float(df.shape[0] - event_total)\n\n    woe_vals = df.groupby(feature_name)[target_name].transform(group_woe)\n    return woe_vals", "category": "Python"}, {"instruction": "def get_exit_time(self):\n        \"\"\"\n        Determines when has this process finished running.\n        If the process is still alive, the current time is returned instead.\n\n        @rtype:  win32.SYSTEMTIME\n        @return: Process exit time.\n        \"\"\"\n", "input": "", "output": "        if self.is_alive():\n            ExitTime = win32.GetSystemTimeAsFileTime()\n        else:\n            if win32.PROCESS_ALL_ACCESS == win32.PROCESS_ALL_ACCESS_VISTA:\n                dwAccess = win32.PROCESS_QUERY_LIMITED_INFORMATION\n            else:\n                dwAccess = win32.PROCESS_QUERY_INFORMATION\n            hProcess = self.get_handle(dwAccess)\n            ExitTime = win32.GetProcessTimes(hProcess)[1]\n        return win32.FileTimeToSystemTime(ExitTime)", "category": "Python"}, {"instruction": "def ip_address_list(ips):\n    \"\"\" IP address range validation and expansion. \"\"\"\n", "input": "", "output": "    # first, try it as a single IP address\n    try:\n        return ip_address(ips)\n    except ValueError:\n        pass\n    # then, consider it as an ipaddress.IPv[4|6]Network instance and expand it\n    return list(ipaddress.ip_network(u(ips)).hosts())", "category": "Python"}, {"instruction": "def _new_placeholder_pic(self, image_file):\n        \"\"\"\n        Return a new `p:pic` element depicting the image in *image_file*,\n        suitable for use as a placeholder. In particular this means not\n        having an `a:xfrm` element, allowing its extents to be inherited from\n        its layout placeholder.\n        \"\"\"\n", "input": "", "output": "        rId, desc, image_size = self._get_or_add_image(image_file)\n        shape_id, name = self.shape_id, self.name\n        pic = CT_Picture.new_ph_pic(shape_id, name, desc, rId)\n        pic.crop_to_fit(image_size, (self.width, self.height))\n        return pic", "category": "Python"}, {"instruction": "def wrap_notification(self, data):\n        \"\"\"Convert notification JSON to a notification class.\"\"\"\n", "input": "", "output": "        if \"method\" in data:\n            method = data[\"method\"]\n            params = data[\"params\"]\n            change = params[0]\n            if method == \"notifyPowerStatus\":\n                return PowerChange.make(**change)\n            elif method == \"notifyVolumeInformation\":\n                return VolumeChange.make(**change)\n            elif method == \"notifyPlayingContentInfo\":\n                return ContentChange.make(**change)\n            elif method == \"notifySettingsUpdate\":\n                return SettingChange.make(**change)\n            elif method == \"notifySWUpdateInfo\":\n                return SoftwareUpdateChange.make(**change)\n            else:\n                _LOGGER.warning(\"Got unknown notification type: %s\", method)\n        elif \"result\" in data:\n            result = data[\"result\"][0]\n            if \"enabled\" in result and \"enabled\" in result:\n                return NotificationChange(**result)\n        else:\n            _LOGGER.warning(\"Unknown notification, returning raw: %s\", data)\n            return data", "category": "Python"}, {"instruction": "def connect(self):\n        \"\"\"Create the low-level AMQP connection to RabbitMQ.\n\n        :rtype: pika.adapters.tornado_connection.TornadoConnection\n\n        \"\"\"\n", "input": "", "output": "        self.set_state(self.STATE_CONNECTING)\n        self.handle = tornado_connection.TornadoConnection(\n            self._connection_parameters,\n            on_open_callback=self.on_open,\n            on_open_error_callback=self.on_open_error,\n            stop_ioloop_on_close=False,\n            custom_ioloop=self.io_loop)", "category": "Python"}, {"instruction": "def get_context(args):\n  \"\"\"\n  Returns a context from the namespace *args* (command line arguments).\n\n  \"\"\"\n", "input": "", "output": "  context = {}\n  if args.revision:\n    context['version'] = args.revision\n  if args.datestamp:\n    context['timestamp'] = \"{:%Y-%m-%d}\".format(datetime.utcnow())\n  if args.timestamp:\n    context['timestamp'] = \"{:%Y-%m-%d %H:%M}\".format(datetime.utcnow())\n  if args.template:\n    context['template'] = args.template.read()\n  if args.css:\n    context['css'] = args.css.read()\n  if args.js:\n    context['js'] = args.js.read()\n  return context", "category": "Python"}, {"instruction": "def append(self, observation, action, reward, terminal, training=True):\n        \"\"\"Append a reward to the memory\n\n        # Argument\n            observation (dict): Observation returned by environment\n            action (int): Action taken to obtain this observation\n            reward (float): Reward obtained by taking this action\n            terminal (boolean): Is the state terminal\n        \"\"\"\n", "input": "", "output": "        super(EpisodeParameterMemory, self).append(observation, action, reward, terminal, training=training)\n        if training:\n            self.intermediate_rewards.append(reward)", "category": "Python"}, {"instruction": "def make_reply(self):\n        \"\"\"\n        Creates a copy of the message, exchanging sender and receiver\n\n        Returns:\n          spade.message.Message: a new message with exchanged sender and receiver\n\n        \"\"\"\n", "input": "", "output": "        return Message(\n            to=str(self.sender),\n            sender=str(self.to),\n            body=self.body,\n            thread=self.thread,\n            metadata=self.metadata\n        )", "category": "Python"}, {"instruction": "def connect_rds(aws_access_key_id=None, aws_secret_access_key=None, **kwargs):\n    \"\"\"\n    :type aws_access_key_id: string\n    :param aws_access_key_id: Your AWS Access Key ID\n\n    :type aws_secret_access_key: string\n    :param aws_secret_access_key: Your AWS Secret Access Key\n\n    :rtype: :class:`boto.rds.RDSConnection`\n    :return: A connection to RDS\n    \"\"\"\n", "input": "", "output": "    from boto.rds import RDSConnection\n    return RDSConnection(aws_access_key_id, aws_secret_access_key, **kwargs)", "category": "Python"}, {"instruction": "def calc_z0_and_conv_factor_from_ratio_of_harmonics(z, z2, NA=0.999):\n    \"\"\"\n    Calculates the Conversion Factor and physical amplitude of motion in nms \n    by comparison of the ratio of the heights of the z signal and \n    second harmonic of z.\n\n    Parameters\n    ----------\n    z : ndarray\n        array containing z signal in volts\n    z2 : ndarray\n        array containing second harmonic of z signal in volts\n    NA : float\n        NA of mirror used in experiment\n\n    Returns\n    -------\n    z0 : float\n        Physical average amplitude of motion in nms\n    ConvFactor : float\n        Conversion Factor between volts and nms\n    \"\"\"\n", "input": "", "output": "    V1 = calc_mean_amp(z)\n    V2 = calc_mean_amp(z2)\n    ratio = V2/V1\n    beta = 4*ratio\n    laserWavelength = 1550e-9 # in m\n    k0 = (2*pi)/(laserWavelength)\n    WaistSize = laserWavelength/(pi*NA)\n    Zr = pi*WaistSize**2/laserWavelength\n    z0 = beta/(k0 - 1/Zr)\n    ConvFactor = V1/z0\n    T0 = 300\n    return z0, ConvFactor", "category": "Python"}, {"instruction": "def parse_value(self, value):\n        \"\"\"Parse string into instance of `time`.\"\"\"\n", "input": "", "output": "        if value is None:\n            return value\n        if isinstance(value, datetime.time):\n            return value\n        return parse(value).timetz()", "category": "Python"}, {"instruction": "def fprint(expr, print_ascii=False):\n    r\"\"\"This function chooses whether to use ascii characters to represent\n    a symbolic expression in the notebook or to use sympy's pprint.\n\n    >>> from sympy import cos\n    >>> omega=Symbol(\"omega\")\n    >>> fprint(cos(omega),print_ascii=True)\n    cos(omega)\n\n\n    \"\"\"\n", "input": "", "output": "    if print_ascii:\n        pprint(expr, use_unicode=False, num_columns=120)\n    else:\n        return expr", "category": "Python"}, {"instruction": "def _describe_atom(topology, index):\n    \"\"\"\n    Returns a string describing the given atom\n\n    :param topology:\n    :param index:\n    :return:\n    \"\"\"\n", "input": "", "output": "    at = topology.atom(index)\n    if topology.n_chains > 1:\n        return \"%s %i %s %i %i\" % (at.residue.name, at.residue.resSeq, at.name, at.index, at.residue.chain.index )\n    else:\n        return \"%s %i %s %i\"    % (at.residue.name, at.residue.resSeq, at.name, at.index)", "category": "Python"}, {"instruction": "def _basis_notes_path(name, data_dir):\n    '''Form a path to the notes for a basis set'''\n", "input": "", "output": "\n    data_dir = fix_data_dir(data_dir)\n    bs_data = _get_basis_metadata(name, data_dir)\n\n    # the notes file is the same as the base file name, with a .notes extension\n    filebase = bs_data['basename']\n    file_path = os.path.join(data_dir, filebase + '.notes')\n    return file_path", "category": "Python"}, {"instruction": "def _default(self):\n        \"\"\" Get the default function return \"\"\"\n", "input": "", "output": "\n        if self._default_args:\n            return self._func(\n                *self._default_args,\n                **self._default_kwargs)\n\n        return self._func(**self._default_kwargs)", "category": "Python"}, {"instruction": "def to_checksum_address(value: AnyStr) -> ChecksumAddress:\n    \"\"\"\n    Makes a checksum address given a supported format.\n    \"\"\"\n", "input": "", "output": "    norm_address = to_normalized_address(value)\n    address_hash = encode_hex(keccak(text=remove_0x_prefix(norm_address)))\n\n    checksum_address = add_0x_prefix(\n        \"\".join(\n            (\n                norm_address[i].upper()\n                if int(address_hash[i], 16) > 7\n                else norm_address[i]\n            )\n            for i in range(2, 42)\n        )\n    )\n    return ChecksumAddress(HexAddress(checksum_address))", "category": "Python"}, {"instruction": "def parse(key_string: str) -> 'Key':\n        \"\"\" Parses a flat key string and returns a key \"\"\"\n", "input": "", "output": "        parts = key_string.split(Key.PARTITION)\n        key_type = KeyType.DIMENSION\n        if parts[3]:\n            key_type = KeyType.TIMESTAMP\n        return Key(key_type, parts[0], parts[1], parts[2].split(Key.DIMENSION_PARTITION)\n                   if parts[2] else [],\n                   parser.parse(parts[3]) if parts[3] else None)", "category": "Python"}, {"instruction": "def widen(self):\n        \"\"\"Increase the interval size.\"\"\"\n", "input": "", "output": "        t, h = self.time, self.half_duration\n        h *= self.scaling_coeff_x\n        self.set_interval((t - h, t + h))", "category": "Python"}, {"instruction": "def _get_ancestors_of(self, obs_nodes_list):\n        \"\"\"\n        Returns a list of all ancestors of all the observed nodes.\n\n        Parameters\n        ----------\n        obs_nodes_list: string, list-type\n            name of all the observed nodes\n        \"\"\"\n", "input": "", "output": "        if not obs_nodes_list:\n            return set()\n        return set(obs_nodes_list) | set(self.parent_node)", "category": "Python"}, {"instruction": "def unfix_parameters(self):\n        \"\"\"Helper function that unfixes all parameters\"\"\"\n", "input": "", "output": "        for W, b in zip(self.W_list, self.b_list):\n            W.unfix()\n            b.unfix()", "category": "Python"}, {"instruction": "def fifo_for_evict(cache_dict, evict_number=1):\n    \"\"\"\n    Use FIFO(First In First Out) strategy for evicting, it will find an item by earliest birthday date then remove it.\n\n    Test:\n    >>> from common_cache import CacheItem\n    >>> dict = {}\n    >>> dict['a'] = CacheItem(key='a', value=0, expire=5)\n    >>> dict['b'] = CacheItem(key='b', value=1, expire=5)\n    >>> dict['c'] = CacheItem(key='c', value=2, expire=5)\n    >>> len(dict)\n    3\n    >>> evicted_keys = fifo_for_evict(dict, evict_number=2)\n    >>> len(dict)\n    1\n    >>> len(evicted_keys)\n    2\n    >>> evicted_keys\n    ['a', 'b']\n    >>> evicted_keys = fifo_for_evict(dict, evict_number=10)\n    >>> len(dict)\n    0\n    >>> len(evicted_keys)\n    1\n    \"\"\"\n", "input": "", "output": "    ordered_dict = sorted(cache_dict.items(), key=lambda t: t[1]['birthday'])\n    evicted_keys = []\n    if len(cache_dict) < evict_number:\n        evict_number = len(cache_dict)\n    for i in range(evict_number):\n        item = ordered_dict[i]\n        key = item[0]\n        cache_dict.pop(key)\n        evicted_keys.append(key)\n    return evicted_keys", "category": "Python"}, {"instruction": "def get_consumption(self):\n        \"\"\"Get current power consumption in mWh.\"\"\"\n", "input": "", "output": "        self.get_status()\n        try:\n            self.consumption = self.data['power']\n        except TypeError:\n            self.consumption = 0\n\n        return self.consumption", "category": "Python"}, {"instruction": "def jacobian(f, x, param):\n    \"\"\"\n    Calculates the jacobian matrix for d/dx_i (f(x))\n    \"\"\"\n", "input": "", "output": "\n    N = len(x)\n    M = len(param)\n    J = np.zeros((N, M))\n    for i, x_i in enumerate(x):\n        for j in range(M):\n            parameters=[]\n            for k, p_k in enumerate(param):\n                    parameters.append(AdFloat(p_k, k == j))\n            # parameters = [AdFloat(p_k, k == j) for k, p_k in enumerate(param)]\n            val = f(AdFloat(x_i,0), parameters)\n            J[i, j] = val.dx\n\n    return J", "category": "Python"}, {"instruction": "def sumMerge(dict1, dict2):\n    \"\"\"\n    Adds two dictionaries together, and merges into the first, dict1.\n    Returns first dict.\n    \"\"\"\n", "input": "", "output": "    for key in dict2:\n        dict1[key] = list(map(lambda a,b: a + b, dict1.get(key, [0,0,0,0]), dict2[key]))\n    return dict1", "category": "Python"}, {"instruction": "def type(self):\n        \"\"\"Returns 'number', 'string', 'date' or 'unknown' based on the type of the value\"\"\"\n", "input": "", "output": "        if isinstance(self.value, numbers.Number):\n            return \"number\"\n        if isinstance(self.value, basestring):\n            return \"string\"\n        return \"unknown\"", "category": "Python"}, {"instruction": "def get_count_sql(self):\n        \"\"\"\n        Build a SELECT query which returns the count of items for an unlimited SELECT\n\n        :return:\n            A SQL SELECT query which returns the count of items for an unlimited query based on this SQLBuilder\n        \"\"\"\n", "input": "", "output": "        sql = 'SELECT COUNT(*) FROM ' + self.tables\n        if len(self.where_clauses) > 0:\n            sql += ' WHERE '\n            sql += ' AND '.join(self.where_clauses)\n        return sql", "category": "Python"}, {"instruction": "def load_adjusted_array(self, domain, columns, dates, sids, mask):\n        \"\"\"\n        Load by delegating to sub-loaders.\n        \"\"\"\n", "input": "", "output": "        out = {}\n        for col in columns:\n            try:\n                loader = self._loaders.get(col)\n                if loader is None:\n                    loader = self._loaders[col.unspecialize()]\n            except KeyError:\n                raise ValueError(\"Couldn't find loader for %s\" % col)\n            out.update(\n                loader.load_adjusted_array(domain, [col], dates, sids, mask)\n            )\n        return out", "category": "Python"}, {"instruction": "def _parse_line_section(self, line):\n        \"\"\"\n        Parse a line containing a group definition. Returns a tuple:\n        (group_type, group_name), where group_type is in the set ('hosts',\n        'children', 'vars').\n\n        For example:\n            [prod]\n        Returns:\n            ('hosts', 'prod')\n\n        For example:\n            [prod:children]\n        Returns:\n            ('children', 'prod')\n        \"\"\"\n", "input": "", "output": "        m = re.match(\"\\[(.*)\\]\", line)\n        group_def = m.groups()[0]\n        if ':' in group_def:\n            group_name, group_type = group_def.split(':')\n        else:\n            group_name = group_def\n            group_type = 'hosts'\n\n        return (group_type, group_name)", "category": "Python"}, {"instruction": "def header(self, headers):\n        \"\"\"Add headers to the email\n\n        :param value: A list of Header objects or a dict of header key/values\n        :type value: Header, list(Header), dict\n        \"\"\"\n", "input": "", "output": "        if isinstance(headers, list):\n            for h in headers:\n                self.add_header(h)\n        else:\n            self.add_header(headers)", "category": "Python"}, {"instruction": "def get_variance(seq):\n    \"\"\"\n    Batch variance calculation.\n    \"\"\"\n", "input": "", "output": "    m = get_mean(seq)\n    return sum((v-m)**2 for v in seq)/float(len(seq))", "category": "Python"}, {"instruction": "def _normalize_array_idx(self, idx):\n        \"\"\"\n        In Java, all array indices are represented by a 32 bit integer and\n        consequently we are using in the Soot engine a 32bit bitvector for this.\n        This function normalize the given index to follow this \"convention\".\n\n        :return: Index as a 32bit bitvector.\n        \"\"\"\n", "input": "", "output": "        if isinstance(idx, SimActionObject):\n            idx = idx.to_claripy()\n        if self.arch.memory_endness == \"Iend_LE\":\n            return idx.reversed.get_bytes(index=0, size=4).reversed\n        else:\n            return idx.get_bytes(index=0, size=4)", "category": "Python"}, {"instruction": "def _extend_blocks(result, blocks=None):\n    \"\"\" return a new extended blocks, givin the result \"\"\"\n", "input": "", "output": "    from pandas.core.internals import BlockManager\n    if blocks is None:\n        blocks = []\n    if isinstance(result, list):\n        for r in result:\n            if isinstance(r, list):\n                blocks.extend(r)\n            else:\n                blocks.append(r)\n    elif isinstance(result, BlockManager):\n        blocks.extend(result.blocks)\n    else:\n        blocks.append(result)\n    return blocks", "category": "Python"}, {"instruction": "def _has_perm(self, user, permission):\n        \"\"\" Check whether the user has the given permission\n\n        @return True if user is granted with access, False if not.\n        \"\"\"\n", "input": "", "output": "\n        if user.is_superuser:\n            return True\n        if user.is_active:\n            perms = [perm.split('.')[1] for perm in user.get_all_permissions()]\n            return permission in perms\n        return False", "category": "Python"}, {"instruction": "def to_ulcer_index(prices):\n    \"\"\"\n    Converts from prices -> `Ulcer index <https://www.investopedia.com/terms/u/ulcerindex.asp>`_\n\n    See https://en.wikipedia.org/wiki/Ulcer_index\n\n    Args:\n        * prices (Series, DataFrame): Prices\n\n    \"\"\"\n", "input": "", "output": "    dd = prices.to_drawdown_series()\n    return np.divide(np.sqrt(np.sum(np.power(dd, 2))), dd.count())", "category": "Python"}, {"instruction": "def get_device_info(self) -> dict:\n        '''\n        Queries Temp-Deck for it's build version, model, and serial number\n\n        returns: dict\n            Where keys are the strings 'version', 'model', and 'serial',\n            and each value is a string identifier\n\n            {\n                'serial': '1aa11bb22',\n                'model': '1aa11bb22',\n                'version': '1aa11bb22'\n            }\n\n        Example input from Temp-Deck's serial response:\n            \"serial:aa11bb22 model:aa11bb22 version:aa11bb22\"\n        '''\n", "input": "", "output": "        try:\n            return self._recursive_get_info(DEFAULT_COMMAND_RETRIES)\n        except (MagDeckError, SerialException, SerialNoResponse) as e:\n            return {'error': str(e)}", "category": "Python"}, {"instruction": "def cross_product(self, p1, p2):\n        \"\"\"Returns the cross product of two XYPoints.\"\"\"\n", "input": "", "output": "        return (p1.x * p2.y - p1.y * p2.x)", "category": "Python"}, {"instruction": "def post_mortem_excepthook(type, value, tb):\n    \"\"\"\n    For post mortem exception handling, print a banner and enable post\n    mortem debugging.\n    \"\"\"\n", "input": "", "output": "    clear_post_mortem()\n    ipython_shell = get_ipython()\n    ipython_shell.showtraceback((type, value, tb))\n    p = pdb.Pdb(ipython_shell.colors)\n\n    if not type == SyntaxError:\n        # wait for stderr to print (stderr.flush does not work in this case)\n        time.sleep(0.1)\n        _print('*' * 40)\n        _print('Entering post mortem debugging...')\n        _print('*' * 40)\n        #  add ability to move between frames\n        p.send_initial_notification = False\n        p.reset()\n        frame = tb.tb_frame\n        prev = frame\n        while frame.f_back:\n            prev = frame\n            frame = frame.f_back\n        frame = prev\n        # wait for stdout to print\n        time.sleep(0.1)\n        p.interaction(frame, tb)", "category": "Python"}, {"instruction": "def is_editable(obj, request):\n    \"\"\"\n    Returns ``True`` if the object is editable for the request. First\n    check for a custom ``editable`` handler on the object, otherwise\n    use the logged in user and check change permissions for the\n    object's model.\n    \"\"\"\n", "input": "", "output": "    if hasattr(obj, \"is_editable\"):\n        return obj.is_editable(request)\n    else:\n        codename = get_permission_codename(\"change\", obj._meta)\n        perm = \"%s.%s\" % (obj._meta.app_label, codename)\n        return (request.user.is_authenticated() and\n                has_site_permission(request.user) and\n                request.user.has_perm(perm))", "category": "Python"}, {"instruction": "def _save_to_hdx(self, action, id_field_name, file_to_upload=None):\n        # type: (str, str, Optional[str]) -> None\n        \"\"\"Creates or updates an HDX object in HDX, saving current data and replacing with returned HDX object data\n        from HDX\n\n        Args:\n            action (str): Action to perform: 'create' or 'update'\n            id_field_name (str): Name of field containing HDX object identifier\n            file_to_upload (Optional[str]): File to upload to HDX\n\n        Returns:\n            None\n        \"\"\"\n", "input": "", "output": "        result = self._write_to_hdx(action, self.data, id_field_name, file_to_upload)\n        self.old_data = self.data\n        self.data = result", "category": "Python"}, {"instruction": "def _time_to_minutes(self, time_str):\n\t\t\"\"\"\n\t\tConvert a time string to the equivalent number in minutes as an int.\n\t\tReturn 0 if the time_str is not a valid amount of time.\n\n\t\t>>> jump = JumprunProApi('skydive-warren-county')\n\t\t>>> jump._time_to_minutes('34 minutes')\n\t\t34\n\t\t>>> jump._time_to_minutes('1 hour, 30 minutes')\n\t\t90\n\t\t>>> jump._time_to_minutes('jfksadjfkas')\n\t\t0\n\t\t\"\"\"\n", "input": "", "output": "\t\tminutes = 0\n\t\ttry:\n\t\t\tcall_time_obj = parser.parse(time_str)\n\n\t\t\tminutes = call_time_obj.minute\n\t\t\tminutes += call_time_obj.hour * 60\n\t\texcept ValueError:\n\t\t\tminutes = 0\n\n\t\treturn minutes", "category": "Python"}, {"instruction": "def add_error(self, property_name, message):\n    \"\"\"Add an error for the given property.\"\"\"\n", "input": "", "output": "    if property_name not in self.errors:\n      self.errors[property_name] = []\n    self.errors[property_name].append(message)", "category": "Python"}, {"instruction": "def add_user(self, **kwargs):\n        \"\"\"Add a User object, with properties specified in ``**kwargs``.\"\"\"\n", "input": "", "output": "        user = self.UserClass(**kwargs)\n        if hasattr(user, 'active'):\n            user.active = True\n        self.db_adapter.add_object(user)\n        return user", "category": "Python"}, {"instruction": "def get_boundingbox(self):\n        \"\"\"\n        Return minimum and maximum x and z coordinates of the chunks that\n        make up this world save\n        \"\"\"\n", "input": "", "output": "        b = BoundingBox()\n        for rx,rz in self.regionfiles.keys():\n            region = self.get_region(rx,rz)\n            rx,rz = 32*rx,32*rz\n            for cc in region.get_chunk_coords():\n                x,z = (rx+cc['x'],rz+cc['z'])\n                b.expand(x,None,z)\n        return b", "category": "Python"}, {"instruction": "def save(self, other: merkle_tree.MerkleTree):\n        \"\"\"Save this tree into a dumb data object for serialisation.\n\n        The object must have attributes tree_size:int and hashes:list.\n        \"\"\"\n", "input": "", "output": "        other.__tree_size = self.__tree_size\n        other.__hashes = self.__hashes", "category": "Python"}, {"instruction": "def remove(self, source, delfiles=True):\n        \"\"\"Remove a dataset from the persist store\n\n        source : str or DataSource or Lo\n            If a str, this is the unique ID of the original source, which is\n            the key of the persisted dataset within the store. If a source,\n            can be either the original or the persisted source.\n        delfiles : bool\n            Whether to remove the on-disc artifact\n        \"\"\"\n", "input": "", "output": "        source = self.get_tok(source)\n        with self.fs.open(self.path, 'rb') as f:\n            data = yaml.safe_load(f.read().decode())\n        data['sources'].pop(source, None)\n        with self.fs.open(self.path, 'wb') as fo:\n            fo.write(yaml.dump(data, default_flow_style=False).encode())\n        if delfiles:\n            path = posixpath.join(self.pdir, source)\n            try:\n                self.fs.rm(path, True)\n            except Exception as e:\n                logger.debug(\"Failed to delete persisted data dir %s\" % path)\n        self._entries.pop(source, None)", "category": "Python"}, {"instruction": "def get_balancer_by_name(name, profile, **libcloud_kwargs):\n    '''\n    Get the details for a load balancer by name\n\n    :param name: Name of a load balancer you want to fetch\n    :type  name: ``str``\n\n    :param profile: The profile key\n    :type  profile: ``str``\n\n    :param libcloud_kwargs: Extra arguments for the driver's list_balancers method\n    :type  libcloud_kwargs: ``dict``\n\n    :return: the load balancer details\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion libcloud_storage.get_balancer_by_name my_balancer profile1\n    '''\n", "input": "", "output": "    conn = _get_driver(profile=profile)\n    libcloud_kwargs = salt.utils.args.clean_kwargs(**libcloud_kwargs)\n    balancers = conn.list_balancers(**libcloud_kwargs)\n    match = [b for b in balancers if b.name == name]\n    if len(match) == 1:\n        return _simple_balancer(match[0])\n    elif len(match) > 1:\n        raise ValueError(\"Ambiguous argument, found mulitple records\")\n    else:\n        raise ValueError(\"Bad argument, found no records\")", "category": "Python"}, {"instruction": "def db_ws010c(self, value=None):\n        \"\"\"  Corresponds to IDD Field `db_ws010c`\n        Mean coincident dry-bulb temperature to wind speed corresponding to 1.0% cumulative frequency for coldest month\n\n        Args:\n            value (float): value for IDD Field `db_ws010c`\n                Unit: C\n                if `value` is None it will not be checked against the\n                specification and is assumed to be a missing value\n\n        Raises:\n            ValueError: if `value` is not a valid value\n        \"\"\"\n", "input": "", "output": "        if value is not None:\n            try:\n                value = float(value)\n            except ValueError:\n                raise ValueError('value {} need to be of type float '\n                                 'for field `db_ws010c`'.format(value))\n\n        self._db_ws010c = value", "category": "Python"}, {"instruction": "def get_current_orga(request, hproject, availableOrga):\n    \"\"\"Return the current orga to use\"\"\"\n", "input": "", "output": "\n    # If nothing available return 404\n    if len(availableOrga) == 0:\n        raise Http404\n\n    # Find the current orga\n    currentOrgaId = request.session.get('plugit-orgapk-' + str(hproject.pk), None)\n\n    # If we don't have a current one select the first available\n    if currentOrgaId is None:\n        (tmpOrga, _) = availableOrga[0]\n        currentOrgaId = tmpOrga.pk\n    else:\n        # If the current Orga is not among the available ones reset to the first one\n        availableOrgaIds = [o.pk for (o, r) in availableOrga]\n        if currentOrgaId not in availableOrgaIds:\n            (tmpOrga, _) = availableOrga[0]\n            currentOrgaId = tmpOrga.pk\n\n    from organizations.models import Organization\n\n    realCurrentOrga = get_object_or_404(Organization, pk=currentOrgaId)\n\n    return realCurrentOrga", "category": "Python"}, {"instruction": "def prune(self, filter_func=None, from_stash='active', to_stash='pruned'):\n        \"\"\"\n        Prune unsatisfiable states from a stash.\n\n        This function will move all unsatisfiable states in the given stash into a different stash.\n\n        :param filter_func: Only prune states that match this filter.\n        :param from_stash:  Prune states from this stash. (default: 'active')\n        :param to_stash:    Put pruned states in this stash. (default: 'pruned')\n\n        :returns:           The simulation manager, for chaining.\n        :rtype:             SimulationManager\n        \"\"\"\n", "input": "", "output": "        def _prune_filter(state):\n            to_prune = not filter_func or filter_func(state)\n            if to_prune and not state.satisfiable():\n                if self._hierarchy:\n                    self._hierarchy.unreachable_state(state)\n                    self._hierarchy.simplify()\n                return True\n            return False\n\n        self.move(from_stash, to_stash, _prune_filter)\n        return self", "category": "Python"}, {"instruction": "def create_git_tree(self, tree, base_tree=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `POST /repos/:owner/:repo/git/trees <http://developer.github.com/v3/git/trees>`_\n        :param tree: list of :class:`github.InputGitTreeElement.InputGitTreeElement`\n        :param base_tree: :class:`github.GitTree.GitTree`\n        :rtype: :class:`github.GitTree.GitTree`\n        \"\"\"\n", "input": "", "output": "        assert all(isinstance(element, github.InputGitTreeElement) for element in tree), tree\n        assert base_tree is github.GithubObject.NotSet or isinstance(base_tree, github.GitTree.GitTree), base_tree\n        post_parameters = {\n            \"tree\": [element._identity for element in tree],\n        }\n        if base_tree is not github.GithubObject.NotSet:\n            post_parameters[\"base_tree\"] = base_tree._identity\n        headers, data = self._requester.requestJsonAndCheck(\n            \"POST\",\n            self.url + \"/git/trees\",\n            input=post_parameters\n        )\n        return github.GitTree.GitTree(self._requester, headers, data, completed=True)", "category": "Python"}, {"instruction": "def get_par_css_dataframe(self):\n        \"\"\" get a dataframe of composite scaled sensitivities.  Includes both\n        PEST-style and Hill-style.\n\n        Returns\n        -------\n        css : pandas.DataFrame\n\n        \"\"\"\n", "input": "", "output": "\n        assert self.jco is not None\n        assert self.pst is not None\n        jco = self.jco.to_dataframe()\n        weights = self.pst.observation_data.loc[jco.index,\"weight\"].copy().values\n        jco = (jco.T * weights).T\n\n        dss_sum = jco.apply(np.linalg.norm)\n        css = (dss_sum / float(self.pst.nnz_obs)).to_frame()\n        css.columns = [\"pest_css\"]\n        # log transform stuff\n        self.pst.add_transform_columns()\n        parval1 = self.pst.parameter_data.loc[dss_sum.index,\"parval1_trans\"].values\n        css.loc[:,\"hill_css\"] = (dss_sum * parval1) / (float(self.pst.nnz_obs)**2)\n        return css", "category": "Python"}, {"instruction": "def requestSubsystem(self, subsystem):\n        \"\"\"Request a subsystem and return a deferred reply.\n        \"\"\"\n", "input": "", "output": "        data = common.NS(subsystem)\n        return self.sendRequest('subsystem', data, wantReply=True)", "category": "Python"}, {"instruction": "def cut_psf(psf_data, psf_size):\n    \"\"\"\n    cut the psf properly\n    :param psf_data: image of PSF\n    :param psf_size: size of psf\n    :return: re-sized and re-normalized PSF\n    \"\"\"\n", "input": "", "output": "    kernel = image_util.cut_edges(psf_data, psf_size)\n    kernel = kernel_norm(kernel)\n    return kernel", "category": "Python"}, {"instruction": "def get_edges(self, indexed=None):\n        \"\"\"Edges of the mesh\n        \n        Parameters\n        ----------\n        indexed : str | None\n           If indexed is None, return (Nf, 3) array of vertex indices,\n           two per edge in the mesh.\n           If indexed is 'faces', then return (Nf, 3, 2) array of vertex\n           indices with 3 edges per face, and two vertices per edge.\n\n        Returns\n        -------\n        edges : ndarray\n            The edges.\n        \"\"\"\n", "input": "", "output": "        \n        if indexed is None:\n            if self._edges is None:\n                self._compute_edges(indexed=None)\n            return self._edges\n        elif indexed == 'faces':\n            if self._edges_indexed_by_faces is None:\n                self._compute_edges(indexed='faces')\n            return self._edges_indexed_by_faces\n        else:\n            raise Exception(\"Invalid indexing mode. Accepts: None, 'faces'\")", "category": "Python"}, {"instruction": "def compute(self, inputs, outputs):\n    \"\"\"\n    Get the next record from the queue and encode it.\n    @param inputs This parameter is ignored. The data comes from the queue\n    @param outputs See definition in the spec above.\n    \"\"\"\n", "input": "", "output": "    if len(self.queue) > 0:\n      data = self.queue.pop()\n\n    else:\n      raise Exception(\"CoordinateSensor: No data to encode: queue is empty\")\n\n    outputs[\"resetOut\"][0] = data[\"reset\"]\n    outputs[\"sequenceIdOut\"][0] = data[\"sequenceId\"]\n    sdr = self.encoder.encode((numpy.array(data[\"coordinate\"]), self.radius))\n    outputs[\"dataOut\"][:] = sdr\n\n    if self.verbosity > 1:\n      print \"CoordinateSensor outputs:\"\n      print \"Coordinate = \", data[\"coordinate\"]\n      print \"sequenceIdOut: \", outputs[\"sequenceIdOut\"]\n      print \"resetOut: \", outputs[\"resetOut\"]\n      print \"dataOut: \", outputs[\"dataOut\"].nonzero()[0]", "category": "Python"}, {"instruction": "def filter_304_headers(headers):\n    \"\"\"Filter a list of headers to include in a \"304 Not Modified\" response.\"\"\"\n", "input": "", "output": "    return [(k, v) for k, v in headers if k.lower() not in _filter_from_304]", "category": "Python"}, {"instruction": "def ext_xsect(scatterer, h_pol=True):\n    \"\"\"Extinction cross section for the current setup, with polarization.    \n\n    Args:\n        scatterer: a Scatterer instance.\n        h_pol: If True (default), use horizontal polarization.\n        If False, use vertical polarization.\n\n    Returns:\n        The extinction cross section.\n    \"\"\"\n", "input": "", "output": "\n    if scatterer.psd_integrator is not None:\n        try:\n            return scatterer.psd_integrator.get_angular_integrated(\n                scatterer.psd, scatterer.get_geometry(), \"ext_xsect\")\n        except AttributeError:\n            # Fall back to the usual method of computing this from S\n            pass\n\n    old_geom = scatterer.get_geometry()\n    (thet0, thet, phi0, phi, alpha, beta) = old_geom\n    try:\n        scatterer.set_geometry((thet0, thet0, phi0, phi0, alpha, beta))\n        S = scatterer.get_S()        \n    finally:\n        scatterer.set_geometry(old_geom)\n\n\n\n    if h_pol:\n        return 2 * scatterer.wavelength * S[1,1].imag\n    else:\n        return 2 * scatterer.wavelength * S[0,0].imag", "category": "Python"}, {"instruction": "def random_rollout_subsequences(rollouts, num_subsequences, subsequence_length):\n  \"\"\"Chooses a random frame sequence of given length from a set of rollouts.\"\"\"\n", "input": "", "output": "  def choose_subsequence():\n    # TODO(koz4k): Weigh rollouts by their lengths so sampling is uniform over\n    # frames and not rollouts.\n    rollout = random.choice(rollouts)\n    try:\n      from_index = random.randrange(len(rollout) - subsequence_length + 1)\n    except ValueError:\n      # Rollout too short; repeat.\n      return choose_subsequence()\n    return rollout[from_index:(from_index + subsequence_length)]\n\n  return [choose_subsequence() for _ in range(num_subsequences)]", "category": "Python"}, {"instruction": "async def get_source_list(self, scheme: str = \"\") -> List[Source]:\n        \"\"\"Return available sources for playback.\"\"\"\n", "input": "", "output": "        res = await self.services[\"avContent\"][\"getSourceList\"](scheme=scheme)\n        return [Source.make(**x) for x in res]", "category": "Python"}, {"instruction": "def resize(widthWindow, heightWindow):\n\t\"\"\"Setup 3D projection for window\"\"\"\n", "input": "", "output": "\tglViewport(0, 0, widthWindow, heightWindow)\n\tglMatrixMode(GL_PROJECTION)\n\tglLoadIdentity()\n\tgluPerspective(70, 1.0*widthWindow/heightWindow, 0.001, 10000.0)\n\tglMatrixMode(GL_MODELVIEW)\n\tglLoadIdentity()", "category": "Python"}, {"instruction": "def children_sum( self, children,node ):\n        \"\"\"Calculate children's total sum\"\"\"\n", "input": "", "output": "        return sum( [self.value(value,node) for value in children] )", "category": "Python"}, {"instruction": "def user(self, value):\n        \"\"\"\n        Sets the user on the current request. This is necessary to maintain\n        compatibility with django.contrib.auth where the user property is\n        set in the login and logout functions.\n\n        Note that we also set the user on Django's underlying `HttpRequest`\n        instance, ensuring that it is available to any middleware in the stack.\n        \"\"\"\n", "input": "", "output": "        self._user = value\n        self._request.user = value", "category": "Python"}, {"instruction": "def authenticated(function):\n    \"\"\"Re-authenticate if session expired.\"\"\"\n", "input": "", "output": "    def wrapped(session, *args, **kwargs):\n        ", "category": "Python"}, {"instruction": "def vd(inc, sd):\n    \"\"\"\n    Calculate vertical distance.\n\n    :param inc: (float) inclination angle in degrees\n    :param sd:  (float) slope distance in any units\n    \"\"\"\n", "input": "", "output": "    return abs(sd * math.sin(math.radians(inc)))", "category": "Python"}, {"instruction": "def randurl(self):\n        \"\"\" -> a random url-like #str via :prop:randdomain, :prop:randtld,\n                and :prop:randpath\n        \"\"\"\n", "input": "", "output": "        return \"{}://{}.{}/{}\".format(\n            self.random.choice((\"http\", \"https\")),\n            self.randdomain, self.randtld, self.randpath)", "category": "Python"}, {"instruction": "def read_wav(self, filename):\n        \"\"\"Read sample data for this sample from a WAV file.\n\n        :param filename: the file from which to read\n        \"\"\"\n", "input": "", "output": "        wave_input = None\n\n        try:\n            wave_input = wave.open(filename, 'r')\n            wave_frames = bytearray(\n                wave_input.readframes(wave_input.getnframes()))\n\n            self.sample_data = [x >> 4 for x in wave_frames]\n\n        finally:\n            if wave_input is not None:\n                wave_input.close()", "category": "Python"}, {"instruction": "def set_itunes_author_name(self):\n        \"\"\"Parses author name from itunes tags and sets value\"\"\"\n", "input": "", "output": "        try:\n            self.itunes_author_name = self.soup.find('itunes:author').string\n        except AttributeError:\n            self.itunes_author_name = None", "category": "Python"}, {"instruction": "def delete_resource(self, uri, purge=False):\n        \"\"\"Delete file or folder\n\n        uri -- mediafire URI\n\n        Keyword arguments:\n        purge -- delete the resource without sending it to Trash.\n        \"\"\"\n", "input": "", "output": "        try:\n            resource = self.get_resource_by_uri(uri)\n        except ResourceNotFoundError:\n            # Nothing to remove\n            return None\n\n        if isinstance(resource, File):\n            result = self.delete_file(uri, purge)\n        elif isinstance(resource, Folder):\n            result = self.delete_folder(uri, purge)\n        else:\n            raise ValueError('Unsupported resource: {}'.format(type(resource)))\n\n        return result", "category": "Python"}, {"instruction": "def batch_geoparse(self, text_list):\n        \"\"\"\n        Batch geoparsing function. Take in a list of text documents and return a list of lists\n        of the geoparsed documents. The speed improvements come exclusively from using spaCy's `nlp.pipe`.\n\n        Parameters\n        ----------\n        text_list : list of strs\n            List of documents. The documents should not have been pre-processed by spaCy.\n\n        Returns\n        -------\n        processed : list of list of dictionaries.\n            The list is the same length as the input list of documents. Each element is a list of dicts, one for\n            each geolocated entity.\n        \"\"\"\n", "input": "", "output": "        if not self.threads:\n            print(\"batch_geoparsed should be used with threaded searches. Please set `threads=True` when initializing the geoparser.\")\n        nlped_docs = list(nlp.pipe(text_list, as_tuples=False, n_threads=multiprocessing.cpu_count()))\n        processed = []\n        for i in tqdm(nlped_docs, disable=not self.progress):\n            p = self.geoparse(i)\n            processed.append(p)\n        return processed", "category": "Python"}, {"instruction": "def seconds_to_hms(seconds):\n    \"\"\"\n    Converts seconds float to 'hh:mm:ss.ssssss' format.\n    \"\"\"\n", "input": "", "output": "    hours = int(seconds / 3600.0)\n    minutes = int((seconds / 60.0) % 60.0)\n    secs = float(seconds % 60.0)\n    return \"{0:02d}:{1:02d}:{2:02.6f}\".format(hours, minutes, secs)", "category": "Python"}, {"instruction": "def send_image(self, url, name, **imageinfo):\n        \"\"\"Send a pre-uploaded image to the room.\n\n        See http://matrix.org/docs/spec/r0.0.1/client_server.html#m-image\n        for imageinfo\n\n        Args:\n            url (str): The mxc url of the image.\n            name (str): The filename of the image.\n            imageinfo (): Extra information about the image.\n        \"\"\"\n", "input": "", "output": "        return self.client.api.send_content(\n            self.room_id, url, name, \"m.image\",\n            extra_information=imageinfo\n        )", "category": "Python"}, {"instruction": "def easeOutElastic(n, amplitude=1, period=0.3):\n    \"\"\"An elastic tween function that overshoots the destination and then \"rubber bands\" into the destination.\n\n    Args:\n      n (float): The time progress, starting at 0.0 and ending at 1.0.\n\n    Returns:\n      (float) The line progress, starting at 0.0 and ending at 1.0. Suitable for passing to getPointOnLine().\n    \"\"\"\n", "input": "", "output": "    _checkRange(n)\n\n    if amplitude < 1:\n        amplitude = 1\n        s = period / 4\n    else:\n        s = period / (2 * math.pi) * math.asin(1 / amplitude)\n\n    return amplitude * 2**(-10*n) * math.sin((n-s)*(2*math.pi / period)) + 1", "category": "Python"}, {"instruction": "def _set_powercfg_value(scheme, sub_group, setting_guid, power, value):\n    '''\n    Sets the AC/DC values of a setting with the given power for the given scheme\n    '''\n", "input": "", "output": "    if scheme is None:\n        scheme = _get_current_scheme()\n\n    cmd = 'powercfg /set{0}valueindex {1} {2} {3} {4}' \\\n          ''.format(power, scheme, sub_group, setting_guid, value * 60)\n    return __salt__['cmd.retcode'](cmd, python_shell=False) == 0", "category": "Python"}, {"instruction": "def grad_hook(module, name, writer, bins):\n    \"\"\" Factory for grad_hook closures \"\"\"\n", "input": "", "output": "    def hook(grad):\n        writer.add_histogram('{}/grad'.format(name.replace('.','/')),\n                             grad.detach().cpu().numpy(),\n                             module.global_step-1,\n                             bins=bins)\n    return hook", "category": "Python"}, {"instruction": "def load_ratings(data_home, size):\n    \"\"\"Load all samples in the dataset.\n    \"\"\"\n", "input": "", "output": "\n    if size == '100k':\n        with open(os.path.join(data_home, 'u.data'), encoding='ISO-8859-1') as f:\n            lines = list(map(lambda l: list(map(int, l.rstrip().split('\\t'))), f.readlines()))\n    elif size == '1m':\n        with open(os.path.join(data_home, 'ratings.dat'), encoding='ISO-8859-1') as f:\n            lines = list(map(lambda l: list(map(int, l.rstrip().split('::'))), f.readlines()))\n\n    ratings = []\n\n    for l in lines:\n        # Since we consider positive-only feedback setting, ratings < 5 will be excluded.\n        if l[2] == 5:\n            ratings.append(l)\n\n    ratings = np.asarray(ratings)\n\n    # sorted by timestamp\n    return ratings[np.argsort(ratings[:, 3])]", "category": "Python"}, {"instruction": "def rms(x, name=None):\n    \"\"\"\n    Returns:\n        root mean square of tensor x.\n    \"\"\"\n", "input": "", "output": "    if name is None:\n        name = x.op.name + '/rms'\n        with tfv1.name_scope(None):   # name already contains the scope\n            return tf.sqrt(tf.reduce_mean(tf.square(x)), name=name)\n    return tf.sqrt(tf.reduce_mean(tf.square(x)), name=name)", "category": "Python"}, {"instruction": "def from_file(filename, section='matrix'):\n    \"\"\"\n    Generate a matrix from a .ini file. Configuration is expected to be in a ``[matrix]`` section.\n    \"\"\"\n", "input": "", "output": "    config = parse_config(open(filename), section=section)\n    return from_config(config)", "category": "Python"}, {"instruction": "def get_socket(host, port, timeout=None):\n    \"\"\"\n    Return a socket.\n\n    :param str host: the hostname to connect to\n    :param int port: the port number to connect to\n    :param timeout: if specified, set the socket timeout\n    \"\"\"\n", "input": "", "output": "    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket(af, socktype, proto)\n            if timeout is not None:\n                sock.settimeout(timeout)\n            sock.connect(sa)\n            return sock\n\n        except error:\n            if sock is not None:\n                sock.close()\n\n    raise error", "category": "Python"}, {"instruction": "def default_java_namespace(self, target):\n    \"\"\"Returns the default_java_namespace used for Thrift generation.\n\n    :param target: The target to extract the default_java_namespace from.\n    :type target: :class:`pants.backend.codegen.targets.java_thrift_library.JavaThriftLibrary`\n    :returns: The default Java namespace used when not specified in the IDL.\n    :rtype: string\n    \"\"\"\n", "input": "", "output": "    self._check_target(target)\n    return target.default_java_namespace or self._default_default_java_namespace", "category": "Python"}, {"instruction": "def timerEvent(self, event):\n        \"\"\" Reimplemented to hide the widget when the hide timer fires.\n        \"\"\"\n", "input": "", "output": "        if event.timerId() == self._hide_timer.timerId():\n            self._hide_timer.stop()\n            self.hide()", "category": "Python"}, {"instruction": "def get_mark(self, mark_id):\n        \"\"\"\n        Returns the markable object for the supplied identifier\n        @type mark_id: string\n        @param mark_id: term identifier\n        \"\"\"\n", "input": "", "output": "        if mark_id in self.idx:\n            return Cmarkable(self.idx[mark_id], self.type)\n        else:\n            return None", "category": "Python"}, {"instruction": "def _parse_hextet(cls, hextet_str):\n        \"\"\"Convert an IPv6 hextet string into an integer.\n\n        Args:\n            hextet_str: A string, the number to parse.\n\n        Returns:\n            The hextet as an integer.\n\n        Raises:\n            ValueError: if the input isn't strictly a hex number from\n              [0..FFFF].\n\n        \"\"\"\n", "input": "", "output": "        # Whitelist the characters, since int() allows a lot of bizarre stuff.\n        if not cls._HEX_DIGITS.issuperset(hextet_str):\n            raise ValueError(\"Only hex digits permitted in %r\" % hextet_str)\n        # We do the length check second, since the invalid character error\n        # is likely to be more informative for the user\n        if len(hextet_str) > 4:\n            msg = \"At most 4 characters permitted in %r\"\n            raise ValueError(msg % hextet_str)\n        # Length check means we can skip checking the integer value\n        return int(hextet_str, 16)", "category": "Python"}, {"instruction": "def retrieve_and_parse_profile(handle):\n    \"\"\"\n    Retrieve the remote user and return a Profile object.\n\n    :arg handle: User handle in username@domain.tld format\n    :returns: ``federation.entities.Profile`` instance or None\n    \"\"\"\n", "input": "", "output": "    hcard = retrieve_diaspora_hcard(handle)\n    if not hcard:\n        return None\n    profile = parse_profile_from_hcard(hcard, handle)\n    try:\n        profile.validate()\n    except ValueError as ex:\n        logger.warning(\"retrieve_and_parse_profile - found profile %s but it didn't validate: %s\",\n                       profile, ex)\n        return None\n    return profile", "category": "Python"}, {"instruction": "def list_all_countries_geo_zones(cls, **kwargs):\n        \"\"\"List CountriesGeoZones\n\n        Return a list of CountriesGeoZones\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.list_all_countries_geo_zones(async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param int page: page number\n        :param int size: page size\n        :param str sort: page order\n        :return: page[CountriesGeoZone]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._list_all_countries_geo_zones_with_http_info(**kwargs)\n        else:\n            (data) = cls._list_all_countries_geo_zones_with_http_info(**kwargs)\n            return data", "category": "Python"}, {"instruction": "def fint(value):\n    \"\"\"integer\"\"\"\n", "input": "", "output": "    if isinstance(value, str_types):\n        # strings might have been saved wrongly as booleans\n        value = value.lower()\n        if value == \"false\":\n            value = 0\n        elif value == \"true\":\n            value = 1\n        elif value:\n            value = int(float(value))\n        else:\n            raise ValueError(\"empty string\")\n    else:\n        value = int(float(value))\n    return value", "category": "Python"}, {"instruction": "def glyph_path(self, glyphs):\n        \"\"\"Adds closed paths for the glyphs to the current path.\n        The generated path if filled,\n        achieves an effect similar to that of :meth:`show_glyphs`.\n\n        :param glyphs:\n            The glyphs to show.\n            See :meth:`show_text_glyphs` for the data structure.\n\n        \"\"\"\n", "input": "", "output": "        glyphs = ffi.new('cairo_glyph_t[]', glyphs)\n        cairo.cairo_glyph_path(self._pointer, glyphs, len(glyphs))\n        self._check_status()", "category": "Python"}, {"instruction": "def pre_facet_sqs(self):\n        \"\"\"\n        Return the queryset used for generating facets, before any facets\n        are applied\n        \"\"\"\n", "input": "", "output": "        sqs = SearchQuerySet()\n\n        if self.query:\n            sqs = sqs.filter(\n                SQ(content=AutoQuery(self.query)) |  # Search `text` document\n                SQ(get_title=AutoQuery(self.query)) | # boosted field\n                SQ(boosted_search_terms=AutoQuery(self.query)) # boosted field\n            )\n\n        return sqs", "category": "Python"}, {"instruction": "def dictionary(values):\n    \"\"\"\n    This function generates dictionary\n    from values parameter.\n    For example this:\n    example = {\n        'streetway': ('street', {'language': 'en'}),\n        'first_name': ('first', {'language': 'en'})\n    }\n    chance.dictionary(example)\n    will output something like this:\n    {'streetway': 'Jabhuru Point', 'first_name': 'Eunice'}\n\n    :param values: dict\n    :return: dict\n    \"\"\"\n", "input": "", "output": "    result = dict()\n    for key in values:\n        fname = values[key][0]\n        if fname not in functions_map:\n            result[key] = values[key]\n        else:\n            params = values[key][1] if len(values[key]) == 2 else {}\n            result[key] = functions_map[fname](**params)\n    return result", "category": "Python"}, {"instruction": "def map(self, lat, long, zoom=13, tiles=\"map\"):\n        \"\"\"\n        Sets a map\n        \"\"\"\n", "input": "", "output": "        try:\n            self.dsmap = self._map(lat, long, zoom, tiles)\n        except Exception as e:\n            self.err(e, self.map, \"Can not get map\")", "category": "Python"}, {"instruction": "def open(self):\n        \"\"\"Opens an existing cache.\n\n        \"\"\"\n", "input": "", "output": "        try:\n            self.graph.open(self.cache_uri, create=False)\n            self._add_namespaces(self.graph)\n            self.is_open = True\n        except Exception:\n            raise InvalidCacheException('The cache is invalid or not created')", "category": "Python"}, {"instruction": "def make_serializable(json):\n    \"\"\"This function ensures that the dictionary is JSON serializable. If not,\n    keys with non-serializable values are removed from the return value.\n\n    Args:\n        json (dict): Dictionary to convert to serializable\n\n    Returns:\n        new_dict (dict): New dictionary with non JSON serializable values removed\n    \"\"\"\n", "input": "", "output": "    new_dict = dict()\n    for key, value in iteritems(json):\n        if is_valid_json(value):\n            new_dict[key] = value\n\n    return new_dict", "category": "Python"}, {"instruction": "def com_google_fonts_check_unique_glyphnames(ttFont):\n  \"\"\"Font contains unique glyph names?\"\"\"\n", "input": "", "output": "  if ttFont.sfntVersion == b'\\x00\\x01\\x00\\x00' and ttFont.get(\n      \"post\") and ttFont[\"post\"].formatType == 3.0:\n    yield SKIP, (\"TrueType fonts with a format 3.0 post table contain no\"\n                 \" glyph names.\")\n  else:\n    import re\n    glyphs = []\n    duplicated_glyphIDs = []\n    for _, g in enumerate(ttFont.getGlyphOrder()):\n      glyphID = re.sub(r'#\\w+', '', g)\n      if glyphID in glyphs:\n        duplicated_glyphIDs.append(glyphID)\n      else:\n        glyphs.append(glyphID)\n\n    if len(duplicated_glyphIDs) == 0:\n      yield PASS, \"Font contains unique glyph names.\"\n    else:\n      yield FAIL, (\"The following glyph names\"\n                   \" occur twice: {}\").format(duplicated_glyphIDs)", "category": "Python"}, {"instruction": "def _parse_mode(mode):\n    '''\n    Converts ls mode output (rwxrwxrwx) -> integer (755).\n    '''\n", "input": "", "output": "\n    result = ''\n    # owner, group, world\n    for group in [mode[0:3], mode[3:6], mode[6:9]]:\n        if group in SYMBOL_TO_OCTAL_PERMISSIONS:\n            result = '{0}{1}'.format(result, SYMBOL_TO_OCTAL_PERMISSIONS[group])\n        else:\n            result = '{0}0'.format(result)\n\n    # Return as an integer\n    return int(result)", "category": "Python"}, {"instruction": "def read_namespaced_stateful_set_status(self, name, namespace, **kwargs):\n        \"\"\"\n        read status of the specified StatefulSet\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.read_namespaced_stateful_set_status(name, namespace, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str name: name of the StatefulSet (required)\n        :param str namespace: object name and auth scope, such as for teams and projects (required)\n        :param str pretty: If 'true', then the output is pretty printed.\n        :return: V1StatefulSet\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.read_namespaced_stateful_set_status_with_http_info(name, namespace, **kwargs)\n        else:\n            (data) = self.read_namespaced_stateful_set_status_with_http_info(name, namespace, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def create_connection(self, alias='default', **kwargs):\n        \"\"\"\n        Construct an instance of ``elasticsearch.Elasticsearch`` and register\n        it under given alias.\n        \"\"\"\n", "input": "", "output": "        kwargs.setdefault('serializer', serializer)\n        conn = self._conns[alias] = Elasticsearch(**kwargs)\n        return conn", "category": "Python"}, {"instruction": "def set_items(self, items):\n        \"\"\"expects a list of key, value to work with\"\"\"\n", "input": "", "output": "        res = []\n        max_value = max(sum((rec[1] for rec in items)), 1)\n        for key, val in items:\n            res.append((key, val, val * 1.0 / max_value))\n        self._items = res", "category": "Python"}, {"instruction": "def set_default_mode(self, default_mode):\n        \"\"\"Set the default mode when alarms are turned 'on'.\"\"\"\n", "input": "", "output": "        if default_mode.lower() not in (CONST.MODE_AWAY, CONST.MODE_HOME):\n            raise AbodeException(ERROR.INVALID_DEFAULT_ALARM_MODE)\n\n        self._default_alarm_mode = default_mode.lower()", "category": "Python"}, {"instruction": "def do_execute(self):\n        \"\"\"\n        The actual execution of the actor.\n\n        :return: None if successful, otherwise error message\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        evl = self.input.payload\n        if isinstance(evl, Evaluation):\n            summary = evl.summary(title=self.resolve_option(\"title\"), complexity=bool(self.resolve_option(\"complexity\")))\n            if bool(self.resolve_option(\"matrix\")):\n                summary += \"\\n\" + evl.matrix(title=self.resolve_option(\"title\"))\n        else:\n            summary = evl.cluster_results\n        self._output.append(Token(summary))\n        return None", "category": "Python"}, {"instruction": "def _execute_insert(self, conn, keys, data_iter):\n        \"\"\"Execute SQL statement inserting data\n\n        Parameters\n        ----------\n        conn : sqlalchemy.engine.Engine or sqlalchemy.engine.Connection\n        keys : list of str\n           Column names\n        data_iter : generator of list\n           Each item contains a list of values to be inserted\n        \"\"\"\n", "input": "", "output": "        data = [dict(zip(keys, row)) for row in data_iter]\n        conn.execute(self.table.insert(), data)", "category": "Python"}, {"instruction": "def create_user(username, password, **kwargs):\r\n    \"\"\"\r\n    return flag, result(result can be an User object or just True, {} for errors)\r\n    \"\"\"\n", "input": "", "output": "    try:\r\n        User = get_model('user')\r\n        user = User.get(User.c.username==username)\r\n        if user:\r\n            return False, {'username':\"Username is already existed!\"}\r\n        user = User(username=username, password=password, **kwargs)\r\n        user.set_password(password)\r\n        user.save()\r\n        return True, user\r\n    except Exception as e:\r\n        log.exception(e)\r\n        return False, {'_': \"Creating user failed!\"}", "category": "Python"}, {"instruction": "def count_objects_by_tags(self, metric, scraper_config):\n        \"\"\" Count objects by whitelisted tags and submit counts as gauges. \"\"\"\n", "input": "", "output": "        config = self.object_count_params[metric.name]\n        metric_name = \"{}.{}\".format(scraper_config['namespace'], config['metric_name'])\n        object_counter = Counter()\n\n        for sample in metric.samples:\n            tags = [\n                self._label_to_tag(l, sample[self.SAMPLE_LABELS], scraper_config) for l in config['allowed_labels']\n            ] + scraper_config['custom_tags']\n            object_counter[tuple(sorted(tags))] += sample[self.SAMPLE_VALUE]\n\n        for tags, count in iteritems(object_counter):\n            self.gauge(metric_name, count, tags=list(tags))", "category": "Python"}, {"instruction": "def save_xml(self, doc, element):\n        '''Save this preceding condition into an xml.dom.Element object.'''\n", "input": "", "output": "        super(Preceding, self).save_xml(doc, element)\n        pre_element = doc.createElementNS(RTS_NS, RTS_NS_S + 'Preceding')\n        if self.timeout:\n            pre_element.setAttributeNS(RTS_NS, RTS_NS_S + 'timeout',\n                    str(self.timeout))\n        if self.sending_timing:\n            pre_element.setAttributeNS(RTS_NS, RTS_NS_S + 'sendingTiming',\n                                   self.sending_timing)\n        for pc in self._preceding_components:\n            new_element = doc.createElementNS(RTS_NS,\n                                              RTS_NS_S + 'PrecedingComponents')\n            pc.save_xml(doc, new_element)\n            pre_element.appendChild(new_element)\n        element.appendChild(pre_element)", "category": "Python"}, {"instruction": "def iterate_symbols():\n    \"\"\"\n    Return an iterator yielding registered netcodes.\n    \"\"\"\n", "input": "", "output": "    for prefix in search_prefixes():\n        package = importlib.import_module(prefix)\n        for importer, modname, ispkg in pkgutil.walk_packages(path=package.__path__, onerror=lambda x: None):\n            network = network_for_netcode(modname)\n            if network:\n                yield network.symbol.upper()", "category": "Python"}, {"instruction": "def rest_put(url, data, timeout, show_error=False):\n    '''Call rest put method'''\n", "input": "", "output": "    try:\n        response = requests.put(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                data=data, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None", "category": "Python"}, {"instruction": "def setUp(self):\n        \"\"\"Prepare to run test case.\n\n        Start harness service, init golden devices, reset DUT and open browser.\n        \"\"\"\n", "input": "", "output": "        if self.__class__ is HarnessCase:\n            return\n\n        logger.info('Setting up')\n        # clear files\n        logger.info('Deleting all .pdf')\n        os.system('del /q \"%HOMEDRIVE%%HOMEPATH%\\\\Downloads\\\\NewPdf_*.pdf\"')\n        logger.info('Deleting all .xlsx')\n        os.system('del /q \"%HOMEDRIVE%%HOMEPATH%\\\\Downloads\\\\ExcelReport*.xlsx\"')\n        logger.info('Deleting all .pcapng')\n        os.system('del /q \"%s\\\\Captures\\\\*.pcapng\"' % settings.HARNESS_HOME)\n\n        # using temp files to fix excel downloading fail\n        logger.info('Empty files in temps')\n        os.system('del /q \"%s\\\\Thread_Harness\\\\temp\\\\*.*\"' % settings.HARNESS_HOME)\n\n        # create directory\n        os.system('mkdir %s' % self.result_dir)\n        self._init_harness()\n        self._init_devices()\n        self._init_dut()\n        self._init_rf_shield()", "category": "Python"}, {"instruction": "def vibrational_free_energy(self, temperature, volume):\n        \"\"\"\n        Vibrational Helmholtz free energy, A_vib(V, T).\n        Eq(4) in doi.org/10.1016/j.comphy.2003.12.001\n\n        Args:\n            temperature (float): temperature in K\n            volume (float)\n\n        Returns:\n            float: vibrational free energy in eV\n        \"\"\"\n", "input": "", "output": "        y = self.debye_temperature(volume) / temperature\n        return self.kb * self.natoms * temperature * (\n            9./8. * y + 3 * np.log(1 - np.exp(-y)) - self.debye_integral(y))", "category": "Python"}, {"instruction": "def secure(self, targets, recursive):\n        '''\n            Saves information about each target file and/or folder and\n            creates a hard link from the file(s) to the vault directory\n        '''\n", "input": "", "output": "        for target in targets:\n            if os.path.isfile(target):\n                path, name = os.path.split(os.path.realpath(target))\n\n                target = Target(name, path)\n                target.secure()\n            else:\n                targets += self._fetchFilesFromFolder(target, recursive)", "category": "Python"}, {"instruction": "def population_chart_to_png_extractor(impact_report, component_metadata):\n    \"\"\"Creating population donut chart.\n\n    :param impact_report: the impact report that acts as a proxy to fetch\n        all the data that extractor needed\n    :type impact_report: safe.report.impact_report.ImpactReport\n\n    :param component_metadata: the component metadata. Used to obtain\n        information about the component we want to render\n    :type component_metadata: safe.report.report_metadata.\n        ReportComponentsMetadata\n\n    :return: context for rendering phase\n    :rtype: dict\n\n    .. versionadded:: 4.0\n    \"\"\"\n", "input": "", "output": "    context = {}\n\n    population_donut_path = impact_report.component_absolute_output_path(\n        'population-chart')\n\n    if not population_donut_path:\n        return context\n\n    context['filepath'] = population_donut_path\n\n    return context", "category": "Python"}, {"instruction": "def find_recursive_dependency(self):\n    \"\"\"Return a list of nodes that have a recursive dependency.\"\"\"\n", "input": "", "output": "    nodes_on_path = []\n\n    def helper(nodes):\n      for node in nodes:\n        cycle = node in nodes_on_path\n        nodes_on_path.append(node)\n        if cycle or helper(self.deps.get(node, [])):\n          return True\n        nodes_on_path.pop()\n      return False\n\n    helper(self.unordered)\n    return nodes_on_path", "category": "Python"}, {"instruction": "def walkRelocatables(self, shouldRelocateCommand=_shouldRelocateCommand):\n        \"\"\"\n        for all relocatable commands\n        yield (command_index, command_name, filename)\n        \"\"\"\n", "input": "", "output": "        for (idx, (lc, cmd, data)) in enumerate(self.commands):\n            if shouldRelocateCommand(lc.cmd):\n                name = _RELOCATABLE_NAMES[lc.cmd]\n                ofs = cmd.name - sizeof(lc.__class__) - sizeof(cmd.__class__)\n                yield idx, name, data[ofs:data.find(B('\\x00'), ofs)].decode(\n                        sys.getfilesystemencoding())", "category": "Python"}, {"instruction": "def get_forms(self, *args, **kwargs):\n        \"\"\"Find forms by standard BeautifulSoup arguments.\n        :args: Positional arguments to `BeautifulSoup::find_all`\n        :args: Keyword arguments to `BeautifulSoup::find_all`\n\n        :return: List of BeautifulSoup tags\n\n        \"\"\"\n", "input": "", "output": "        forms = self.find_all(_form_ptn, *args, **kwargs)\n        return [\n            Form(form)\n            for form in forms\n        ]", "category": "Python"}, {"instruction": "def _getitem(string, depth=0):\n    \"\"\"\n    Get an item from the string (where item is up to the next ',' or '}' or the\n    end of the string)\n    \"\"\"\n", "input": "", "output": "    out = [\"\"]\n    while string:\n        char = string[0]\n        if depth and (char == ',' or char == '}'):\n            return out, string\n        if char == '{':\n            groups_string = _getgroup(string[1:], depth+1)\n            if groups_string is not None:\n                groups, string = groups_string\n                out = [a + g for a in out for g in groups]\n                continue\n        if char == '\\\\' and len(string) > 1:\n            string, char = string[1:], char + string[1]\n\n        out, string = [a + char for a in out], string[1:]\n\n    return out, string", "category": "Python"}, {"instruction": "def mesh(**kwargs):\n    \"\"\"\n    Create parameters for a new mesh dataset.\n\n    Generally, this will be used as an input to the kind argument in\n    :meth:`phoebe.frontend.bundle.Bundle.add_dataset`\n\n    :parameter **kwargs: defaults for the values of any of the parameters\n    :return: a :class:`phoebe.parameters.parameters.ParameterSet` of all newly\n        created :class:`phoebe.parameters.parameters.Parameter`s\n    \"\"\"\n", "input": "", "output": "\n    obs_params = []\n\n    syn_params, constraints = mesh_syn(syn=False, **kwargs)\n    obs_params += syn_params.to_list()\n\n    obs_params += [SelectParameter(qualifier='include_times', value=kwargs.get('include_times', []), description='append to times from the following datasets/time standards', choices=['t0@system'])]\n\n    obs_params += [SelectParameter(qualifier='columns', value=kwargs.get('columns', []), description='columns to expose within the mesh', choices=_mesh_columns)]\n    #obs_params += mesh_dep(**kwargs).to_list()\n\n    return ParameterSet(obs_params), constraints", "category": "Python"}, {"instruction": "def patch_class(input_class):\n    \"\"\"Create a new class based on the input_class.\n\n    :param class input_class:  The class to patch.\n    :rtype class:\n    \"\"\"\n", "input": "", "output": "    class Instantiator(object):\n        @classmethod\n        def _doubles__new__(self, *args, **kwargs):\n            pass\n\n    new_class = type(input_class.__name__, (input_class, Instantiator), {})\n\n    return new_class", "category": "Python"}, {"instruction": "def fromdeltas(cls, deltas):\n\t\t\"\"\"\n\t\tConstruct an offsetvector from a dictionary of offset\n\t\tdeltas as returned by the .deltas attribute.\n\n\t\tExample:\n\n\t\t>>> x = offsetvector({\"H1\": 0, \"L1\": 10, \"V1\": 20})\n\t\t>>> y = offsetvector.fromdeltas(x.deltas)\n\t\t>>> y\n\t\toffsetvector({'V1': 20, 'H1': 0, 'L1': 10})\n\t\t>>> y == x\n\t\tTrue\n\n\t\tSee also .deltas, .fromkeys()\n\t\t\"\"\"\n", "input": "", "output": "\t\treturn cls((key, value) for (refkey, key), value in deltas.items())", "category": "Python"}, {"instruction": "def addrownumbers(table, start=1, step=1, field='row'):\n    \"\"\"\n    Add a field of row numbers. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['A', 9],\n        ...           ['C', 2],\n        ...           ['F', 1]]\n        >>> table2 = etl.addrownumbers(table1)\n        >>> table2\n        +-----+-----+-----+\n        | row | foo | bar |\n        +=====+=====+=====+\n        |   1 | 'A' |   9 |\n        +-----+-----+-----+\n        |   2 | 'C' |   2 |\n        +-----+-----+-----+\n        |   3 | 'F' |   1 |\n        +-----+-----+-----+\n\n    Parameters `start` and `step` control the numbering.\n\n    \"\"\"\n", "input": "", "output": "\n    return AddRowNumbersView(table, start, step, field)", "category": "Python"}, {"instruction": "def as_dict(self, use_preliminary=False):\n        \"\"\"Create a copy of the config in form of a dict\n\n        :param bool use_preliminary: Whether to include the preliminary config\n        :return: A dict with the copy of the config\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        config = dict()\n        for key in self.config.keys:\n            if use_preliminary and key in self.preliminary_config:\n                value = self.preliminary_config[key]\n            else:\n                value = self.config.get_config_value(key)\n            config[key] = value\n        return config", "category": "Python"}, {"instruction": "def unique_slug_required(form, slug):\n    \"\"\"Enforce a unique slug accross all pages and websistes.\"\"\"\n", "input": "", "output": "\n    if hasattr(form, 'instance') and form.instance.id:\n        if Content.objects.exclude(page=form.instance).filter(\n            body=slug, type=\"slug\").count():\n            raise forms.ValidationError(error_dict['another_page_error'])\n    elif Content.objects.filter(body=slug, type=\"slug\").count():\n        raise forms.ValidationError(error_dict['another_page_error'])\n    return slug", "category": "Python"}, {"instruction": "def SymmetricDifference(self, scriptnames):\n\t\t'''Takes in a set, list, or tuple scriptnames and returns the symmetric difference (as a list)\n\t\tof scriptnames and the stored names.'''\n", "input": "", "output": "\t\tscriptnames = set(scriptnames)\n\t\tmyscripts = set(self.scripts.keys())\n\t\treturn list(scriptnames.difference(myscripts).union(myscripts.difference(scriptnames)))", "category": "Python"}, {"instruction": "def read_egginfo_json(pkg_name, filename=DEFAULT_JSON, working_set=None):\n    \"\"\"\n    Read json from egginfo of a package identified by `pkg_name` that's\n    already installed within the current Python environment.\n    \"\"\"\n", "input": "", "output": "\n    working_set = working_set or default_working_set\n    dist = find_pkg_dist(pkg_name, working_set=working_set)\n    return read_dist_egginfo_json(dist, filename)", "category": "Python"}, {"instruction": "def get_events(self, *args, **kwargs):\n        \"\"\"Fetches lists of events.\n\n        get /v1/public/events\n\n        :returns:  EventDataWrapper\n\n        >>> #Find all the events that involved both Hulk and Wolverine\n        >>> #hulk's id: 1009351\n        >>> #wolverine's id: 1009718\n        >>> m = Marvel(public_key, private_key)\n        >>> response = m.get_events(characters=\"1009351,1009718\")\n        >>> print response.data.total\n        38\n        >>> events = response.data.results\n        >>> print events[1].title\n        Age of Apocalypse\n        \"\"\"\n", "input": "", "output": "\n        response = json.loads(self._call(Event.resource_url(), self._params(kwargs)).text)\n        return EventDataWrapper(self, response)", "category": "Python"}, {"instruction": "def print_long(filename, stat, print_func):\n    \"\"\"Prints detailed information about the file passed in.\"\"\"\n", "input": "", "output": "    size = stat_size(stat)\n    mtime = stat_mtime(stat)\n    file_mtime = time.localtime(mtime)\n    curr_time = time.time()\n    if mtime > (curr_time + SIX_MONTHS) or mtime < (curr_time - SIX_MONTHS):\n        print_func('%6d %s %2d %04d  %s' % (size, MONTH[file_mtime[1]],\n                                            file_mtime[2], file_mtime[0],\n                                            decorated_filename(filename, stat)))\n    else:\n        print_func('%6d %s %2d %02d:%02d %s' % (size, MONTH[file_mtime[1]],\n                                                file_mtime[2], file_mtime[3], file_mtime[4],\n                                                decorated_filename(filename, stat)))", "category": "Python"}, {"instruction": "def steal_docstring_from(obj):\n    \"\"\"Decorator that lets you steal a docstring from another object\n\n    Example\n    -------\n\n    ::\n\n    @steal_docstring_from(superclass.meth)\n    def meth(self, arg):\n        \"Extra subclass documentation\"\n        pass\n\n    In this case the docstring of the new 'meth' will be copied from superclass.meth, and\n    if an additional dosctring was defined for meth it will be appended to the superclass\n    docstring with a two newlines inbetween.\n    \"\"\"\n", "input": "", "output": "    def deco(fn):\n        docs = [obj.__doc__]\n        if fn.__doc__:\n            docs.append(fn.__doc__)\n        fn.__doc__ = '\\n\\n'.join(docs)\n        return fn\n\n    return deco", "category": "Python"}, {"instruction": "def genes_with_homology_models(self):\n        \"\"\"DictList: All genes that have at least one homology model.\"\"\"\n", "input": "", "output": "        return DictList(x for x in self.genes_with_structures if x.protein.num_structures_homology > 0)", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self, 'status') and self.status is not None:\n            _dict['status'] = self.status\n        if hasattr(self, 'url') and self.url is not None:\n            _dict['url'] = self.url\n        return _dict", "category": "Python"}, {"instruction": "def _kill_all_kids(self, sig):\n        \"\"\"\n        Kill all subprocesses (and its subprocesses) that executor started.\n\n        This function tries to kill all leftovers in process tree that current\n        executor may have left. It uses environment variable to recognise if\n        process have origin in this Executor so it does not give 100 % and\n        some daemons fired by subprocess may still be running.\n\n        :param int sig: signal used to stop process run by executor.\n        :return: process ids (pids) of killed processes\n        :rtype list\n        \"\"\"\n", "input": "", "output": "        pids = processes_with_env(ENV_UUID, self._uuid)\n        for pid in pids:\n            log.debug(\"Killing process %d ...\", pid)\n            try:\n                os.kill(pid, sig)\n            except OSError as err:\n                if err.errno in IGNORED_ERROR_CODES:\n                    # the process has died before we tried to kill it.\n                    pass\n                else:\n                    raise\n            log.debug(\"Killed process %d.\", pid)\n        return pids", "category": "Python"}, {"instruction": "def _compress_json(self, j):\n        \"\"\"Compress the BLOB data portion of the usernotes.\n\n        Arguments:\n            j: the JSON in Schema v5 format (dict)\n\n        Returns a dict with the 'users' key removed and 'blob' key added\n        \"\"\"\n", "input": "", "output": "        compressed_json = copy.copy(j)\n        compressed_json.pop('users', None)\n\n        compressed_data = zlib.compress(\n            json.dumps(j['users']).encode('utf-8'),\n            self.zlib_compression_strength\n        )\n        b64_data = base64.b64encode(compressed_data).decode('utf-8')\n\n        compressed_json['blob'] = b64_data\n\n        return compressed_json", "category": "Python"}, {"instruction": "def run_code_in_container(cli, image, code, mount, entrypoint):\n    \"\"\" Run `code` in a container, returning its ID\n    \"\"\"\n", "input": "", "output": "    kwargs = {\n        'image': image,\n    }\n\n    if entrypoint:\n        kwargs['entrypoint'] = '/bin/bash'\n        kwargs['command'] = '-c {}'.format(quote(code))\n    else:\n        kwargs['command'] = '/bin/bash -c {}'.format(quote(code))\n\n    if mount:\n        binds = []\n        volumes = []\n\n        for m in mount:\n            part = m.split(':')\n\n            if len(part) == 3:\n                pass\n            elif len(part) == 2:\n                part.append('rw')\n            else:\n                raise\n\n            src, target, mode = part\n            src = os.path.abspath(os.path.expanduser(src))\n\n            binds.append('{}:{}:{}'.format(src, target, mode))\n            volumes.append(target)\n\n        kwargs['host_config'] = cli.create_host_config(binds=binds)\n        kwargs['volumes'] = volumes\n\n    container = cli.create_container(**kwargs)\n    container_id = container['Id']\n\n    cli.start(container=container_id)\n\n    return container_id", "category": "Python"}, {"instruction": "def initrd(self, initrd):\n        \"\"\"\n        Sets the initrd path for this QEMU VM.\n\n        :param initrd: QEMU initrd path\n        \"\"\"\n", "input": "", "output": "\n        initrd = self.manager.get_abs_image_path(initrd)\n\n        log.info('QEMU VM \"{name}\" [{id}] has set the QEMU initrd path to {initrd}'.format(name=self._name,\n                                                                                           id=self._id,\n                                                                                           initrd=initrd))\n        if \"asa\" in initrd:\n            self.project.emit(\"log.warning\", {\"message\": \"Warning ASA 8 is not supported by GNS3 and Cisco, you need to use ASAv. Depending of your hardware and OS this could not work or you could be limited to one instance. If ASA 8 is not booting their is no GNS3 solution, you need to upgrade to ASAv.\"})\n        self._initrd = initrd", "category": "Python"}, {"instruction": "def _build_circle(self):\n        \"\"\"\n            Creates hash ring.\n        \"\"\"\n", "input": "", "output": "        total_weight = 0\n        for node in self._nodes:\n            total_weight += self._weights.get(node, 1)\n\n        for node in self._nodes:\n            weight = self._weights.get(node, 1)\n\n            ks = math.floor((40 * len(self._nodes) * weight) / total_weight)\n\n            for i in xrange(0, int(ks)):\n                b_key = self._md5_digest('%s-%s-salt' % (node, i))\n\n                for l in xrange(0, 4):\n                    key = ((b_key[3 + l * 4] << 24)\n                           | (b_key[2 + l * 4] << 16)\n                           | (b_key[1 + l * 4] << 8)\n                           | b_key[l * 4])\n\n                    self._hashring[key] = node\n                    self._sorted_keys.append(key)\n\n        self._sorted_keys.sort()", "category": "Python"}, {"instruction": "def _parse_raw(self, raw):\n        \"\"\"Parse a raw dictionary to create a resource.\n\n        :type raw: Dict[str, Any]\n        \"\"\"\n", "input": "", "output": "        self.raw = raw\n        if not raw:\n            raise NotImplementedError(\"We cannot instantiate empty resources: %s\" % raw)\n        dict2resource(raw, self, self._options, self._session)", "category": "Python"}, {"instruction": "def _open(self, skip=0):\n        \"\"\" Perform HID initialization \"\"\"\n", "input": "", "output": "        usb_device = self._get_usb_device(skip)\n\n        if usb_device:\n            usb_conf = usb_device.configurations[0]\n            self._usb_int = usb_conf.interfaces[0][0]\n        else:\n            raise YubiKeyUSBHIDError('No USB YubiKey found')\n\n        try:\n            self._usb_handle = usb_device.open()\n            self._usb_handle.detachKernelDriver(0)\n        except Exception as error:\n            if 'could not detach kernel driver from interface' in str(error):\n                self._debug('The in-kernel-HID driver has already been detached\\n')\n            else:\n                self._debug(\"detachKernelDriver not supported!\\n\")\n\n        try:\n            self._usb_handle.setConfiguration(1)\n        except usb.USBError:\n            self._debug(\"Unable to set configuration, ignoring...\\n\")\n        self._usb_handle.claimInterface(self._usb_int)\n        return True", "category": "Python"}, {"instruction": "def from_scanner(self, x, y, z):\n        \"\"\"\n        Converts a 3d position in the scanner reference frame to the MRSData space\n\n        :param x:\n        :param y:\n        :param z:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if self.transform is None:\n            raise ValueError(\"No transform set for MRSData object {}\".format(self))\n\n        transformed_point = numpy.linalg.inv(self.transform) * numpy.matrix([x, y, z, 1]).T\n\n        return numpy.squeeze(numpy.asarray(transformed_point))[0:3]", "category": "Python"}, {"instruction": "def add_feature(self, feature={}, organism=None, sequence=None):\n        \"\"\"\n        Add a feature\n\n        :type feature: dict\n        :param feature: Feature information\n\n        :type organism: str\n        :param organism: Organism Common Name\n\n        :type sequence: str\n        :param sequence: Sequence Name\n\n        :rtype: dict\n        :return: A standard apollo feature dictionary ({\"features\": [{...}]})\n        \"\"\"\n", "input": "", "output": "\n        data = {\n            'features': feature,\n        }\n        data = self._update_data(data, organism, sequence)\n        return self.post('addFeature', data)", "category": "Python"}, {"instruction": "def make_metatiles(size, tiles, date_time=None):\n    \"\"\"\n    Group by layers, and make metatiles out of all the tiles which share those\n    properties relative to the \"top level\" tile which is parent of them all.\n    Provide a 6-tuple date_time to set the timestamp on each tile within the\n    metatile, or leave it as None to use the current time.\n    \"\"\"\n", "input": "", "output": "\n    groups = defaultdict(list)\n    for tile in tiles:\n        key = tile['layer']\n        groups[key].append(tile)\n\n    metatiles = []\n    for group in groups.itervalues():\n        parent = _parent_tile(t['coord'] for t in group)\n        metatiles.extend(make_multi_metatile(parent, group, date_time))\n\n    return metatiles", "category": "Python"}, {"instruction": "def correct_invalid_start(self, scansion: str) -> str:\n        \"\"\"\n        The third syllable of a hendecasyllabic line is long, so we will convert it.\n\n        :param scansion: scansion string\n        :return: scansion string with corrected start\n\n        >>> print(HendecasyllableScanner().correct_invalid_start(\n        ... \"- U U  U U  - U   -  U - U\").strip())\n        - U -  U U  - U   -  U - U\n        \"\"\"\n", "input": "", "output": "        mark_list = string_utils.mark_list(scansion)\n        vals = list(scansion.replace(\" \", \"\"))\n        corrected = vals[:2] + [self.constants.STRESSED] + vals[3:]\n        new_line = list(\" \" * len(scansion))\n        for idx, car in enumerate(corrected):\n            new_line[mark_list[idx]] = car\n        return \"\".join(new_line)", "category": "Python"}, {"instruction": "def cancel_offer(self, offer_id):\n        \"\"\"\n        Cancelles an offer\n\n        :param offer_id: the offer id\n        :return Response\n        \"\"\"\n", "input": "", "output": "        return self._create_put_request(\n            resource=OFFERS,\n            billomat_id=offer_id,\n            command=CANCEL,\n        )", "category": "Python"}, {"instruction": "def find_protein_complexes(model):\n    \"\"\"\n    Find reactions that are catalyzed by at least a heterodimer.\n\n    Parameters\n    ----------\n    model : cobra.Model\n        The metabolic model under investigation.\n\n    Returns\n    -------\n    list\n        Reactions whose gene-protein-reaction association contains at least one\n        logical AND combining different gene products (heterodimer).\n\n    \"\"\"\n", "input": "", "output": "    complexes = []\n    for rxn in model.reactions:\n        if not rxn.gene_reaction_rule:\n            continue\n        size = find_top_level_complex(rxn.gene_reaction_rule)\n        if size >= 2:\n            complexes.append(rxn)\n    return complexes", "category": "Python"}, {"instruction": "def bind(cls, ns, location=None):\n        \"\"\"\n        Bind a namespace to a schema location (URI).\n        This is used for imports that don't specify a schemaLocation.\n        @param ns: A namespace-uri.\n        @type ns: str\n        @param location: The (optional) schema location for the\n            namespace.  (default=ns).\n        @type location: str\n        \"\"\"\n", "input": "", "output": "        if location is None:\n            location = ns\n        cls.locations[ns] = location", "category": "Python"}, {"instruction": "def loss_ratio_exceedance_matrix(self, loss_ratios):\n        \"\"\"\n        Compute the LREM (Loss Ratio Exceedance Matrix).\n        \"\"\"\n", "input": "", "output": "        # LREM has number of rows equal to the number of loss ratios\n        # and number of columns equal to the number of imls\n        lrem = numpy.empty((len(loss_ratios), len(self.imls)))\n        for row, loss_ratio in enumerate(loss_ratios):\n            for col, (mean_loss_ratio, stddev) in enumerate(\n                    zip(self.mean_loss_ratios, self.stddevs)):\n                lrem[row, col] = self.distribution.survival(\n                    loss_ratio, mean_loss_ratio, stddev)\n        return lrem", "category": "Python"}, {"instruction": "def add_administrator(self, project_id, name, email):\n        \"\"\"\n        Adds a contributor to a project language\n        \"\"\"\n", "input": "", "output": "        self._run(\n            url_path=\"contributors/add\",\n            id=project_id,\n            name=name,\n            email=email,\n            admin=True\n        )\n        return True", "category": "Python"}, {"instruction": "def _rm_is_header_line(parts, n):\n  \"\"\"\n  determine whether a pre-split string is a repeat-masker alignment header.\n\n  headers have no special structure or symbol to mark them, so this is based\n  only on the number of elements, and what data type they are.\n  \"\"\"\n", "input": "", "output": "  if (n == 15 and parts[8] == \"C\"):\n    return True\n  if (n == 14 and parts[0].isdigit()):\n    return True", "category": "Python"}, {"instruction": "def get_index(self, name):\n        \"\"\"get an index by name\n\n        TODO: Combine indexes of relevant catalogs depending on the portal_type\n        which is searched for.\n        \"\"\"\n", "input": "", "output": "        catalog = self.get_catalog()\n        index = catalog._catalog.getIndex(name)\n        logger.debug(\"get_index={} of catalog '{}' --> {}\".format(\n            name, catalog.__name__, index))\n        return index", "category": "Python"}, {"instruction": "def __addTab(self, filePath):\n        \"\"\"Returns existing tab index. Creates a new one if it isn't opened and returns its index\n        otherwise.\"\"\"\n", "input": "", "output": "        for i in range(self.tabBar.count()):\n            widget = self.pages.widget(i)\n            if not widget.isStatic and filePath == widget.filePath:\n                return i\n        tab = SubtitleEditor(filePath, self._subtitleData, self)\n        newIndex = self.tabBar.addTab(self._createTabName(tab.name, tab.history.isClean()))\n        tab.history.cleanChanged.connect(\n            lambda clean: self._cleanStateForFileChanged(filePath, clean))\n        self.pages.addWidget(tab)\n        return newIndex", "category": "Python"}, {"instruction": "def flush(self):\n        \"\"\" Emits the current queue and clears the queue \"\"\"\n", "input": "", "output": "        self.notify(tuple(self._queue))\n        self._queue.clear()", "category": "Python"}, {"instruction": "def rotation_matrix(self):\n        \"\"\"\n        Compute the rotation matrix of the (normalized) quaternion.\n\n        Returns\n        -------\n        R : :class:`~numpy.ndarray`\n            A 3 by 3 rotation matrix (has shape ``(3,3)``).\n\n        \"\"\"\n", "input": "", "output": "        v, theta = self.v_theta\n        c = np.cos(theta)\n        s = np.sin(theta)\n\n        return np.array([[v[0] * v[0] * (1. - c) + c,\n                          v[0] * v[1] * (1. - c) - v[2] * s,\n                          v[0] * v[2] * (1. - c) + v[1] * s],\n                         [v[1] * v[0] * (1. - c) + v[2] * s,\n                          v[1] * v[1] * (1. - c) + c,\n                          v[1] * v[2] * (1. - c) - v[0] * s],\n                         [v[2] * v[0] * (1. - c) - v[1] * s,\n                          v[2] * v[1] * (1. - c) + v[0] * s,\n                          v[2] * v[2] * (1. - c) + c]])", "category": "Python"}, {"instruction": "def create(self, type_, *args, **kwargs) -> Any:\n        \"\"\"\n        Creates a new attribute of `type_`, appending it to the attribute\n        table and returning it.\n        \"\"\"\n", "input": "", "output": "        attribute = type_(self, *args, **kwargs)\n        self._table.append(attribute)\n        return attribute", "category": "Python"}, {"instruction": "def validate_password2(self, value):\n        \"\"\" password_confirmation check \"\"\"\n", "input": "", "output": "        if value != self.initial_data['password1']:\n            raise serializers.ValidationError(_('Password confirmation mismatch'))\n        return value", "category": "Python"}, {"instruction": "def set(self):\n        \"\"\"set the event to triggered\n\n        after calling this method, all greenlets waiting on the event will be\n        rescheduled, and calling :meth:`wait` will not block until\n        :meth:`clear` has been called\n        \"\"\"\n", "input": "", "output": "        self._is_set = True\n        scheduler.state.awoken_from_events.update(self._waiters)\n        del self._waiters[:]", "category": "Python"}, {"instruction": "def on_size(self, event):\n        '''handle window size changes'''\n", "input": "", "output": "        state = self.state\n        self.need_redraw = True\n        if state.report_size_changes:\n            # tell owner the new size\n            size = self.frame.GetSize()\n            if size != self.last_size:\n                self.last_size = size\n                state.out_queue.put(MPImageNewSize(size))", "category": "Python"}, {"instruction": "def calculate_shannon_entropy(self, data):\n        \"\"\"Returns the entropy of a given string.\n\n        Borrowed from: http://blog.dkbza.org/2007/05/scanning-data-for-entropy-anomalies.html.\n\n        :param data:  string. The word to analyze.\n        :returns:       float, between 0.0 and 8.0\n        \"\"\"\n", "input": "", "output": "        if not data:  # pragma: no cover\n            return 0\n\n        entropy = 0\n        for x in self.charset:\n            p_x = float(data.count(x)) / len(data)\n            if p_x > 0:\n                entropy += - p_x * math.log(p_x, 2)\n\n        return entropy", "category": "Python"}, {"instruction": "def DbGetClassForDevice(self, argin):\n        \"\"\" Get Tango class for the specified device.\n\n        :param argin: Device name\n        :type: tango.DevString\n        :return: Device Tango class\n        :rtype: tango.DevString \"\"\"\n", "input": "", "output": "        self._log.debug(\"In DbGetClassForDevice()\")\n        return self.db.get_class_for_device(argin)", "category": "Python"}, {"instruction": "def mail_sent_contains_html(self):\n    \"\"\"\n    Test that an email contains the HTML (assert HTML in) in the multiline as\n    one of its MIME alternatives.\n\n    The HTML is normalised by passing through Django's\n    :func:`django.test.html.parse_html`.\n\n    Example:\n\n    .. code-block:: gherkin\n\n        And I have sent an email with the following HTML alternative:\n        \\\"\\\"\\\"\n        <p><strong>Name:</strong> Sir Panda</p>\n        <p><strong>Phone:</strong> 0400000000</p>\n        <p><strong>Email:</strong> sir.panda@pand.as</p>\n        \\\"\\\"\\\"\n    \"\"\"\n", "input": "", "output": "\n    for email in mail.outbox:\n        try:\n            html = next(content for content, mime in email.alternatives\n                        if mime == 'text/html')\n            dom1 = parse_html(html)\n            dom2 = parse_html(self.multiline)\n\n            assert_in(dom1, dom2)\n\n        except AssertionError as exc:\n            print(\"Email did not match\", exc)\n            # we intentionally eat the exception\n            continue\n\n        return True\n\n    raise AssertionError(\"No email contained the HTML\")", "category": "Python"}, {"instruction": "def encode(self, V, P, X, CC, seqNum, M, PT, SSRC, payload):\n        \"\"\"Encode the RTP packet with header fields and payload.\"\"\"\n", "input": "", "output": "        timestamp = int(time())\n        header = bytearray(HEADER_SIZE)\n        # Fill the header bytearray with RTP header fields\n        # ...\n        header[0] = header[0] | V << 6; \n        header[0] = header[0] | P << 5; \n        header[0] = header[0] | X << 4; \n        header[0] = header[0] | CC; \n        header[1] = header[1] | M << 7; \n        header[1] = header[1] | PT; \n        header[2] = (seqNum >> 8) & 0xFF; \n        header[3] = seqNum & 0xFF; \n        header[4] = (timestamp >> 24) & 0xFF; \n        header[5] = (timestamp >> 16) & 0xFF;\n        header[6] = (timestamp >> 8) & 0xFF;\n        header[7] = timestamp & 0xFF;\n        header[8] = (SSRC >> 24) & 0xFF; \n        header[9] = (SSRC >> 16) & 0xFF;\n        header[10] = (SSRC >> 8) & 0xFF;\n        header[11] = SSRC & 0xFF\n\n        self.header = header\n        \n        # Get the payload\n        # ...\n        self.payload = payload", "category": "Python"}, {"instruction": "def get_binstar(args=None, cls=None):\n    \"\"\"\n    DEPRECATED METHOD,\n\n    use `get_server_api`\n    \"\"\"\n", "input": "", "output": "\n    warnings.warn(\n        'method get_binstar is deprecated, please use `get_server_api`',\n        DeprecationWarning\n    )\n\n    token = getattr(args, 'token', None)\n    log_level = getattr(args, 'log_level', logging.INFO)\n    site = getattr(args, 'site', None)\n\n    aserver_api = get_server_api(token=token, site=site, log_level=log_level, cls=cls)\n    return aserver_api", "category": "Python"}, {"instruction": "def fit(self, sequences, y=None):\n        \"\"\"Fit the kcenters clustering on the data\n\n        Parameters\n        ----------\n        sequences : list of array-like, each of shape [sequence_length, n_features]\n            A list of multivariate timeseries, or ``md.Trajectory``. Each\n            sequence may have a different length, but they all must have the\n            same number of features, or the same number of atoms if they are\n            ``md.Trajectory``s.\n\n        Returns\n        -------\n        self\n        \"\"\"\n", "input": "", "output": "        MultiSequenceClusterMixin.fit(self, sequences)\n        self.cluster_center_indices_ = self._split_indices(self.cluster_center_indices_)\n        return self", "category": "Python"}, {"instruction": "def load_tbl_from_csv(fname):\r\n\t\"\"\"\r\n\tread a CSV file to list without worrying about odd characters\r\n\t\"\"\"\n", "input": "", "output": "\timport csv\r\n\r\n\trows_to_load = []\r\n\r\n\twith open(fname, 'r', encoding='cp1252', errors='ignore') as csvfile:\r\n\t\tcsvreader = csv.reader(csvfile, delimiter = ',' )\r\n\r\n\t\treader = csv.reader(csvfile)\r\n\r\n\t\trows_to_load = list(reader)\r\n\r\n\treturn rows_to_load", "category": "Python"}, {"instruction": "def save_form_data(self, instance, data):\n        \"\"\"\n        The ``KeywordsWidget`` field will return data as a string of\n        comma separated IDs for the ``Keyword`` model - convert these\n        into actual ``AssignedKeyword`` instances. Also delete\n        ``Keyword`` instances if their last related ``AssignedKeyword``\n        instance is being removed.\n        \"\"\"\n", "input": "", "output": "        from yacms.generic.models import Keyword\n        related_manager = getattr(instance, self.name)\n        # Get a list of Keyword IDs being removed.\n        old_ids = [str(a.keyword_id) for a in related_manager.all()]\n        new_ids = data.split(\",\")\n        removed_ids = set(old_ids) - set(new_ids)\n        # Remove current AssignedKeyword instances.\n        related_manager.all().delete()\n        # Convert the data into AssignedKeyword instances.\n        if data:\n            data = [related_manager.create(keyword_id=i) for i in new_ids]\n        # Remove keywords that are no longer assigned to anything.\n        Keyword.objects.delete_unused(removed_ids)\n        super(KeywordsField, self).save_form_data(instance, data)", "category": "Python"}, {"instruction": "def my_solid_angle(center, coords):\n    \"\"\"\n    Helper method to calculate the solid angle of a set of coords from the\n    center.\n\n    Args:\n        center:\n            Center to measure solid angle from.\n        coords:\n            List of coords to determine solid angle.\n\n    Returns:\n        The solid angle.\n    \"\"\"\n", "input": "", "output": "    o = np.array(center)\n    r = [np.array(c) - o for c in coords]\n    r.append(r[0])\n    n = [np.cross(r[i + 1], r[i]) for i in range(len(r) - 1)]\n    n.append(np.cross(r[1], r[0]))\n    phi = 0.0\n    for i in range(len(n) - 1):\n        try:\n            value = math.acos(-np.dot(n[i], n[i + 1]) / (np.linalg.norm(n[i]) * np.linalg.norm(n[i + 1])))\n        except ValueError:\n            mycos = -np.dot(n[i], n[i + 1]) / (np.linalg.norm(n[i]) * np.linalg.norm(n[i + 1]))\n            if 0.999999999999 < mycos < 1.000000000001:\n                value = math.acos(1.0)\n            elif -0.999999999999 > mycos > -1.000000000001:\n                value = math.acos(-1.0)\n            else:\n                raise SolidAngleError(mycos)\n        phi += value\n    return phi + (3 - len(r)) * math.pi", "category": "Python"}, {"instruction": "def get(self, name, default=None):\n        \"\"\"\n        Returns an extension instance with a given name.\n\n        In case there are few extensions with a given name, the first one\n        will be returned. If no extensions with a given name are exist,\n        the `default` value will be returned.\n\n        :param name: (str) an extension name\n        :param default: (object) a fallback value\n        :returns: (object) an extension instance\n        \"\"\"\n", "input": "", "output": "        try:\n            value = self[name]\n        except KeyError:\n            value = default\n        return value", "category": "Python"}, {"instruction": "def AddFileWithUnknownHash(client_path, blob_refs, use_external_stores=True):\n  \"\"\"Add a new file consisting of given blob IDs.\"\"\"\n", "input": "", "output": "  precondition.AssertType(client_path, db.ClientPath)\n  precondition.AssertIterableType(blob_refs, rdf_objects.BlobReference)\n  return AddFilesWithUnknownHashes(\n      {client_path: blob_refs},\n      use_external_stores=use_external_stores)[client_path]", "category": "Python"}, {"instruction": "def which(software, strip_newline=True):\n    '''get_install will return the path to where an executable is installed.\n    '''\n", "input": "", "output": "    if software is None:\n        software = \"singularity\"\n    cmd = ['which', software ]\n    try:\n        result = run_command(cmd)\n        if strip_newline is True:\n            result['message'] = result['message'].strip('\\n')\n        return result\n\n    except: # FileNotFoundError\n        return None", "category": "Python"}, {"instruction": "def refreshAlignmentUi(self):\r\n        \"\"\"\r\n        Refreshes the alignment UI information.\r\n        \"\"\"\n", "input": "", "output": "        align = self.alignment()\r\n        for name, value in (('align_left', Qt.AlignLeft),\r\n                            ('align_right', Qt.AlignRight),\r\n                            ('align_center', Qt.AlignHCenter),\r\n                            ('align_justify', Qt.AlignJustify)):\r\n            \r\n            act = self._actions[name]\r\n            act.blockSignals(True)\r\n            act.setChecked(value == align)\r\n            act.blockSignals(False)", "category": "Python"}, {"instruction": "def get_domain_reports(self, domains):\n        \"\"\"Retrieves the most recent VT info for a set of domains.\n\n        Args:\n            domains: list of string domains.\n        Returns:\n            A dict with the domain as key and the VT report as value.\n        \"\"\"\n", "input": "", "output": "        api_name = 'virustotal-domain-reports'\n\n        (all_responses, domains) = self._bulk_cache_lookup(api_name, domains)\n        responses = self._request_reports(\"domain\", domains, 'domain/report')\n\n        for domain, response in zip(domains, responses):\n            if self._cache:\n                self._cache.cache_value(api_name, domain, response)\n            all_responses[domain] = response\n\n        return all_responses", "category": "Python"}, {"instruction": "def __dump(df,relation='data',description=''):\n    \"\"\"\n    dump DataFrame to liac-arff\n    :param DataFrame df: \n    :param str relation: \n    :param str description: \n    :rtype: dict\n    :return: liac-arff dict \n    \"\"\"\n", "input": "", "output": "    attrs = []\n    for col in df.columns:\n        attr = col.split('@')\n        if attr[1].count('{')>0 and attr[1].count('}')>0:\n            vals = attr[1].replace('{','').replace('}','').split(',')\n            attrs.append((attr[0],vals))\n        else:\n            attrs.append((attr[0],attr[1]))\n\n    data = list(df.values)\n    result = {\n        'attributes':attrs,\n        'data':data,\n        'description':description,\n        'relation':relation\n    }\n    return result", "category": "Python"}, {"instruction": "def rename_feature(self, mapobject_type_name, name, new_name):\n        '''Renames a feature.\n\n        Parameters\n        ----------\n        mapobject_type_name: str\n            name of the segmented objects type\n        name: str\n            name of the feature that should be renamed\n        new_name: str\n            name that should be given to the feature\n\n        See also\n        --------\n        :func:`tmserver.api.feature.update_feature`\n        :class:`tmlib.models.feature.Feature`\n        '''\n", "input": "", "output": "        logger.info(\n            'rename feature \"%s\" of experiment \"%s\", mapobject type \"%s\"',\n            name, self.experiment_name, mapobject_type_name\n        )\n        content = {\n            'name': new_name,\n        }\n        feature_id = self._get_feature_id(mapobject_type_name, name)\n        url = self._build_api_url(\n            '/experiments/{experiment_id}/features/{feature_id}'.format(\n                experiment_id=self._experiment_id, feature_id=feature_id\n            )\n        )\n        res = self._session.put(url, json=content)\n        res.raise_for_status()", "category": "Python"}, {"instruction": "def get_bareground_fn():\n    \"\"\"Calls external shell script `get_bareground.sh` to fetch:\n\n    ~2010 global bare ground, 30 m\n\n    Note: unzipped file size is 64 GB! Original products are uncompressed, and tiles are available globally (including empty data over ocean)\n\n    The shell script will compress all downloaded tiles using lossless LZW compression.\n\n    http://landcover.usgs.gov/glc/BareGroundDescriptionAndDownloads.php\n    \"\"\"\n", "input": "", "output": "    bg_fn = os.path.join(datadir, 'bare2010/bare2010.vrt')\n    if not os.path.exists(bg_fn):\n        cmd = ['get_bareground.sh',]\n        sys.exit(\"Missing bareground data source. If already downloaded, specify correct datadir. If not, run `%s` to download\" % cmd[0])\n        #subprocess.call(cmd)\n    return bg_fn", "category": "Python"}, {"instruction": "def countByValue(self):\n        \"\"\"\n        Return a new DStream in which each RDD contains the counts of each\n        distinct value in each RDD of this DStream.\n        \"\"\"\n", "input": "", "output": "        return self.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x+y)", "category": "Python"}, {"instruction": "def qos_rcv_queue_multicast_threshold_traffic_class5(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        qos = ET.SubElement(config, \"qos\", xmlns=\"urn:brocade.com:mgmt:brocade-qos\")\n        rcv_queue = ET.SubElement(qos, \"rcv-queue\")\n        multicast = ET.SubElement(rcv_queue, \"multicast\")\n        threshold = ET.SubElement(multicast, \"threshold\")\n        traffic_class5 = ET.SubElement(threshold, \"traffic-class5\")\n        traffic_class5.text = kwargs.pop('traffic_class5')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def denorm(self,arr):\n        \"\"\"Reverse the normalization done to a batch of images.\n\n        Arguments:\n            arr: of shape/size (N,3,sz,sz)\n        \"\"\"\n", "input": "", "output": "        if type(arr) is not np.ndarray: arr = to_np(arr)\n        if len(arr.shape)==3: arr = arr[None]\n        return self.transform.denorm(np.rollaxis(arr,1,4))", "category": "Python"}, {"instruction": "def projection(self, axis):\n        \"\"\"Sums all data along all other axes, then return Hist1D\"\"\"\n", "input": "", "output": "        axis = self.get_axis_number(axis)\n        projected_hist = np.sum(self.histogram, axis=self.other_axes(axis))\n        return Hist1d.from_histogram(projected_hist, bin_edges=self.bin_edges[axis])", "category": "Python"}, {"instruction": "def create_from_tuples(self, tuples, **args):\n        \"\"\"\n        Creates from a list of (subj,subj_name,obj) tuples\n        \"\"\"\n", "input": "", "output": "        amap = {}\n        subject_label_map = {}\n        for a in tuples:\n            subj = a[0]\n            subject_label_map[subj] = a[1]\n            if subj not in amap:\n                amap[subj] = []\n            amap[subj].append(a[2])\n\n        aset = AssociationSet(subject_label_map=subject_label_map, association_map=amap, **args)\n        return aset", "category": "Python"}, {"instruction": "def strftime(date_time=None, time_format=None):\n        \"\"\"\n        \u5c06 datetime \u5bf9\u8c61\u8f6c\u6362\u4e3a str\n\n        :param:\n            * date_time: (obj) datetime \u5bf9\u8c61\n            * time_format: (sting) \u65e5\u671f\u683c\u5f0f\u5b57\u7b26\u4e32\n        :return:\n            * date_time_str: (string) \u65e5\u671f\u5b57\u7b26\u4e32\n        \"\"\"\n", "input": "", "output": "        if not date_time:\n            datetime_now = datetime.now()\n        else:\n            datetime_now = date_time\n        if not time_format:\n            time_format = '%Y/%m/%d %H:%M:%S'\n        return datetime.strftime(datetime_now, time_format)", "category": "Python"}, {"instruction": "def set_value(self, complete_name, value):\n        \"\"\"\n        Set the value for the supplied parameter.\n        \"\"\"\n", "input": "", "output": "        element = self.toc.get_element_by_complete_name(complete_name)\n\n        if not element:\n            logger.warning(\"Cannot set value for [%s], it's not in the TOC!\",\n                           complete_name)\n            raise KeyError('{} not in param TOC'.format(complete_name))\n        elif element.access == ParamTocElement.RO_ACCESS:\n            logger.debug('[%s] is read only, no trying to set value',\n                         complete_name)\n            raise AttributeError('{} is read-only!'.format(complete_name))\n        else:\n            varid = element.ident\n            pk = CRTPPacket()\n            pk.set_header(CRTPPort.PARAM, WRITE_CHANNEL)\n            if self._useV2:\n                pk.data = struct.pack('<H', varid)\n            else:\n                pk.data = struct.pack('<B', varid)\n\n            try:\n                value_nr = eval(value)\n            except TypeError:\n                value_nr = value\n\n            pk.data += struct.pack(element.pytype, value_nr)\n            self.param_updater.request_param_setvalue(pk)", "category": "Python"}, {"instruction": "def array_to_schedule(array, events, slots):\n    \"\"\"Convert a schedule from array to schedule form\n\n    Parameters\n    ----------\n    array : np.array\n        An E by S array (X) where E is the number of events and S the\n        number of slots. Xij is 1 if event i is scheduled in slot j and\n        zero otherwise\n    events : list or tuple\n        of :py:class:`resources.Event` instances\n    slots : list or tuple\n        of :py:class:`resources.Slot` instances\n\n    Returns\n    -------\n    list\n        A list of instances of :py:class:`resources.ScheduledItem`\n    \"\"\"\n", "input": "", "output": "    scheduled = np.transpose(np.nonzero(array))\n    return [\n        ScheduledItem(event=events[item[0]], slot=slots[item[1]])\n        for item in scheduled\n    ]", "category": "Python"}, {"instruction": "def get(self, sid):\n        \"\"\"\n        Constructs a AvailableAddOnExtensionContext\n\n        :param sid: The unique Extension Sid\n\n        :returns: twilio.rest.preview.marketplace.available_add_on.available_add_on_extension.AvailableAddOnExtensionContext\n        :rtype: twilio.rest.preview.marketplace.available_add_on.available_add_on_extension.AvailableAddOnExtensionContext\n        \"\"\"\n", "input": "", "output": "        return AvailableAddOnExtensionContext(\n            self._version,\n            available_add_on_sid=self._solution['available_add_on_sid'],\n            sid=sid,\n        )", "category": "Python"}, {"instruction": "def serial_lock(self, lock):\n        '''lock or unlock the port'''\n", "input": "", "output": "        mav = self.master.mav\n        if lock:\n            flags = mavutil.mavlink.SERIAL_CONTROL_FLAG_EXCLUSIVE\n            self.locked = True\n        else:\n            flags = 0\n            self.locked = False\n        mav.serial_control_send(self.serial_settings.port,\n                                flags,\n                                0, 0, 0, [0]*70)", "category": "Python"}, {"instruction": "def disable_contactgroup_host_notifications(self, contactgroup):\n        \"\"\"Disable host notifications for a contactgroup\n        Format of the line that triggers function call::\n\n        DISABLE_CONTACTGROUP_HOST_NOTIFICATIONS;<contactgroup_name>\n\n        :param contactgroup: contactgroup to disable\n        :type contactgroup: alignak.objects.contactgroup.Contactgroup\n        :return: None\n        \"\"\"\n", "input": "", "output": "        for contact_id in contactgroup.get_contacts():\n            self.disable_contact_host_notifications(self.daemon.contacts[contact_id])", "category": "Python"}, {"instruction": "def get_numpy_status():\n    \"\"\"\n    Returns a dictionary containing a boolean specifying whether NumPy\n    is up-to-date, along with the version string (empty string if\n    not installed).\n    \"\"\"\n", "input": "", "output": "    numpy_status = {}\n    try:\n        import numpy\n        numpy_version = numpy.__version__\n        numpy_status['up_to_date'] = parse_version(\n            numpy_version) >= parse_version(NUMPY_MIN_VERSION)\n        numpy_status['version'] = numpy_version\n    except ImportError:\n        traceback.print_exc()\n        numpy_status['up_to_date'] = False\n        numpy_status['version'] = \"\"\n    return numpy_status", "category": "Python"}, {"instruction": "def R_package_path(package):\n    \"\"\"\n    return the path to an installed R package\n    \"\"\"\n", "input": "", "output": "    local_sitelib = R_sitelib()\n    rscript = Rscript_cmd()\n    cmd = ", "category": "Python"}, {"instruction": "def delete(method, hmc, uri, uri_parms, logon_required):\n        \"\"\"Operation: Delete User.\"\"\"\n", "input": "", "output": "        try:\n            user = hmc.lookup_by_uri(uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        # Check user type\n        type_ = user.properties['type']\n        if type_ == 'pattern-based':\n            raise BadRequestError(\n                method, uri, reason=312,\n                message=\"Cannot delete pattern-based user {!r}\".\n                format(user.name))\n        # Delete the mocked resource\n        user.manager.remove(user.oid)", "category": "Python"}, {"instruction": "def get_render_configurations(self, request, **kwargs):\n        \"\"\"Render image interface\"\"\"\n", "input": "", "output": "\n        data = self.process_form_data(self._get_form_defaults(), kwargs)\n        variable_set = self.get_variable_set(self.service.variable_set.order_by('index'), data)\n\n        base_config = ImageConfiguration(\n            extent=data['bbox'],\n            size=data['size'],\n            image_format=data['image_format'],\n            background_color=TRANSPARENT_BACKGROUND_COLOR if data.get('transparent') else DEFAULT_BACKGROUND_COLOR\n        )\n\n        return base_config, self.apply_time_to_configurations([RenderConfiguration(v) for v in variable_set], data)", "category": "Python"}, {"instruction": "def save(self, *args, **kwargs):\n        \"\"\"Saves an animation\n\n        A wrapper around :meth:`matplotlib.animation.Animation.save`\n        \"\"\"\n", "input": "", "output": "        self.timeline.index -= 1  # required for proper starting point for save\n        self.animation.save(*args, **kwargs)", "category": "Python"}, {"instruction": "def dad_status_output_dad_status_entries_message(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        dad_status = ET.Element(\"dad_status\")\n        config = dad_status\n        output = ET.SubElement(dad_status, \"output\")\n        dad_status_entries = ET.SubElement(output, \"dad-status-entries\")\n        message = ET.SubElement(dad_status_entries, \"message\")\n        message.text = kwargs.pop('message')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def _looks_like_typing_subscript(node):\n    \"\"\"Try to figure out if a Subscript node *might* be a typing-related subscript\"\"\"\n", "input": "", "output": "    if isinstance(node, nodes.Name):\n        return node.name in TYPING_MEMBERS\n    elif isinstance(node, nodes.Attribute):\n        return node.attrname in TYPING_MEMBERS\n    elif isinstance(node, nodes.Subscript):\n        return _looks_like_typing_subscript(node.value)\n    return False", "category": "Python"}, {"instruction": "def save_as(self, fname, obj=None):\n        \"\"\" Save DICOM file given a GDCM DICOM object.\n        Examples of a GDCM DICOM object:\n        * gdcm.Writer()\n        * gdcm.Reader()\n        * gdcm.Anonymizer()\n\n        :param fname: DICOM file name to be saved\n        :param obj: DICOM object to be saved, if None, Anonymizer() is used\n        \"\"\"\n", "input": "", "output": "        writer = gdcm.Writer()\n        writer.SetFileName(fname)\n        if obj is None and self._anon_obj:\n            obj = self._anon_obj\n        else:\n            raise ValueError(\"Need DICOM object, e.g. obj=gdcm.Anonymizer()\")\n        writer.SetFile(obj.GetFile())\n        if not writer.Write():\n            raise IOError(\"Could not save DICOM file\")\n        return True", "category": "Python"}, {"instruction": "def satisfiable(self, **kwargs):\n        \"\"\"\n        Whether the state's constraints are satisfiable\n        \"\"\"\n", "input": "", "output": "        if o.ABSTRACT_SOLVER in self.options or o.SYMBOLIC not in self.options:\n            extra_constraints = kwargs.pop('extra_constraints', ())\n            for e in extra_constraints:\n                if self.solver.is_false(e):\n                    return False\n\n            return self._satisfiable\n        else:\n            return self.solver.satisfiable(**kwargs)", "category": "Python"}, {"instruction": "def prt_gene_aart(self, geneids, prt=sys.stdout):\n        \"\"\"For each gene, print ASCII art which represents its associated GO IDs.\"\"\"\n", "input": "", "output": "        patgene = self.datobj.kws[\"fmtgene\"]\n        itemid2name = self.datobj.kws.get(\"itemid2name\")\n        prt.write(\"\\n{HDR}\\n\".format(HDR=self.str_hdr()))\n        for geneid in geneids:\n            symbol = \"\" if itemid2name is None else itemid2name.get(geneid, \"\")\n            prt.write(patgene.format(AART=self.gene2aart[geneid], ID=geneid, NAME=symbol))", "category": "Python"}, {"instruction": "def submit_combine(basename, readers, job_ids=None, project_name=None):\n    \"\"\"Submit a batch job to combine the outputs of a reading job.\n\n    This function is provided for backwards compatibility. You should use the\n    PmidSubmitter and submit_combine methods.\n    \"\"\"\n", "input": "", "output": "    sub = PmidSubmitter(basename, readers, project_name)\n    sub.job_list = job_ids\n    sub.submit_combine()\n    return sub", "category": "Python"}, {"instruction": "def setCustomCompletions(self, wordSet):\n        \"\"\"Add a set of custom completions to the editors completions.\n\n        This set is managed independently of the set of keywords and words from\n        the current document, and can thus be changed at any time.\n\n        \"\"\"\n", "input": "", "output": "        if not isinstance(wordSet, set):\n            raise TypeError('\"wordSet\" is not a set: %s' % type(wordSet))\n        self._completer.setCustomCompletions(wordSet)", "category": "Python"}, {"instruction": "def execute_process_async(func, *args, **kwargs):\n  \"\"\"\n  Executes `func` in a separate process. Memory and other resources are not\n  available. This gives true concurrency at the cost of losing access to\n  these resources. `args` and `kwargs` are\n  \"\"\"\n", "input": "", "output": "  global _GIPC_EXECUTOR\n  if _GIPC_EXECUTOR is None:\n    _GIPC_EXECUTOR = GIPCExecutor(\n      num_procs=settings.node.gipc_pool_size,\n      num_greenlets=settings.node.greenlet_pool_size)\n  return _GIPC_EXECUTOR.submit(func, *args, **kwargs)", "category": "Python"}, {"instruction": "def render_title(text, markup=True, no_smartquotes=False):\n    \"\"\" Convert a Markdown title to HTML \"\"\"\n", "input": "", "output": "\n    # HACK: If the title starts with something that looks like a list, save it\n    # for later\n    pfx, text = re.match(r'([0-9. ]*)(.*)', text).group(1, 2)\n    text = pfx + misaka.Markdown(TitleRenderer(),\n                                 extensions=TITLE_EXTENSIONS)(text)\n\n    if not markup:\n        strip = HTMLStripper()\n        strip.feed(text)\n        text = strip.get_data()\n\n    if not no_smartquotes:\n        text = misaka.smartypants(text)\n\n    return flask.Markup(text)", "category": "Python"}, {"instruction": "def unisim_csv_formatting(path, fname):\n    \"\"\"\n    Remove some useless stuff from the head of a csv file generated by unisim\n    and returns a pandas dataframe\n    \"\"\"\n", "input": "", "output": "    with open(path+fname, 'r') as fobj:\n        data = fobj.readlines()\n        header = data[9].split(\",\")[:-1]\n        unit_of_measures = data[10].split(\",\")[:-1]\n    data = pd.read_csv(path+fname,\n                       skiprows=10,\n                       index_col=0,\n                       usecols=(range(0, len(header))),\n                       na_values=('Shutdown', 'Bad',\n                                  'I/O Timeout', 'Scan Timeout'))\n    data.columns = header[1:]\n    data.unit = unit_of_measures[1:]\n    return data", "category": "Python"}, {"instruction": "def prune_database():\n    \"\"\"\n    Delete tokens that have expired from the database.\n\n    How (and if) you call this is entirely up you. You could expose it to an\n    endpoint that only administrators could call, you could run it as a cron,\n    set it up with flask cli, etc.\n    \"\"\"\n", "input": "", "output": "    now = datetime.now()\n    expired = TokenBlacklist.query.filter(TokenBlacklist.expires < now).all()\n    for token in expired:\n        db.session.delete(token)\n    db.session.commit()", "category": "Python"}, {"instruction": "def rotateAboutVectorMatrix(vec, theta_deg):\n    \"\"\"Construct the matrix that rotates vector a about\n    vector vec by an angle of theta_deg degrees\n\n    Taken from\n    http://en.wikipedia.org/wiki/Rotation_matrix#Rotation_matrix_from_axis_and_angle\n\n    Input:\n    theta_deg   (float) Angle through which vectors should be\n                rotated in degrees\n\n    Returns:\n    A matrix\n\n    To rotate a vector, premultiply by this matrix.\n    To rotate the coord sys underneath the vector, post multiply\n\n    \"\"\"\n", "input": "", "output": "    ct = np.cos(np.radians(theta_deg))\n    st = np.sin(np.radians(theta_deg))\n\n    # Ensure vector has normal length\n    vec /= np.linalg.norm(vec)\n    assert( np.all( np.isfinite(vec)))\n\n    # compute the three terms\n    term1 = ct * np.eye(3)\n\n    ucross = np.zeros( (3,3))\n    ucross[0] = [0, -vec[2], vec[1]]\n    ucross[1] = [vec[2], 0, -vec[0]]\n    ucross[2] = [-vec[1], vec[0], 0]\n\n    term2 = st*ucross\n\n    ufunny = np.zeros( (3,3))\n    for i in range(0,3):\n        for j in range(i,3):\n            ufunny[i,j] = vec[i]*vec[j]\n            ufunny[j,i] = ufunny[i,j]\n\n    term3 = (1-ct) * ufunny\n\n    return term1 + term2 + term3", "category": "Python"}, {"instruction": "def make_processor(self, name, mappings, processor_type, **kwargs):\n        \"\"\"\n        Instantiates a RmlProcessor and registers it in the manager\n\n        Args:\n        -----\n            name: the name to register the processor\n            mappings: the list RML mapping definitions to use\n            processor_type: the name of the RML processor to use\n        \"\"\"\n", "input": "", "output": "        from .processor import Processor\n        if self.processors.get(name):\n            raise LookupError(\"processor has already been created\")\n        if isinstance(mappings, list):\n            mappings = [self.get_rml(item) for item in mappings]\n        else:\n            mappings = [self.get_rml(mappings)]\n        self.processors[name] = Processor[processor_type](mappings, **kwargs)\n        self.processors[name].name = name\n        return self.processors[name]", "category": "Python"}, {"instruction": "def _handle_unknown_command(self):\n        \"\"\"Handle an unknown RES command.\n\n        The function makes sure to recv all message parts and respond with an\n        error.\n\n        \"\"\"\n", "input": "", "output": "        while self.query_socket.getsockopt(zmq.RCVMORE):\n            # Making sure we 'empty' enveloped message. Otherwise, we can't\n            # respond.\n            self.query_socket.recv()\n        self.query_socket.send(b\"ERROR Unknown request type\")", "category": "Python"}, {"instruction": "def get_grade_entries(self):\n        \"\"\"Gets the package list resulting from the search.\n\n        return: (osid.grading.GradeEntryList) - the grade entry list\n        raise:  IllegalState - list already retrieved\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        if self.retrieved:\n            raise errors.IllegalState('List has already been retrieved.')\n        self.retrieved = True\n        return objects.GradeEntryList(self._results, runtime=self._runtime)", "category": "Python"}, {"instruction": "def push(self, bot, channel_type, ar, user_id):\n        \"\"\"\n        Use this method to push message to user of bot.\n        The message should be packed into ActionResponse object.\n        This allows to push text messages, buttons, images.\n        This also allows to force current state of user.\n\n        :param bot: bot that will push user\n        :type bot: Bot\n        :param channel_type: one of [telegram, facebook, slack]\n        :type channel_type: str\n        :param ar: message packed in response object\n        :type ar: ActionResponse\n        :param user_id: user id in used channel\n        :type user_id: str\n        \"\"\"\n", "input": "", "output": "        self.client.push.__getattr__(bot.name).__call__(_method=\"POST\",\n                                                        _params=dict(id=user_id, channel=channel_type),\n                                                        _json=ar.to_json())", "category": "Python"}, {"instruction": "def queued(values, qsize):\n    \"\"\"\n    Queues up readings from *values* (the number of readings queued is\n    determined by *qsize*) and begins yielding values only when the queue is\n    full. For example, to \"cascade\" values along a sequence of LEDs::\n\n        from gpiozero import LEDBoard, Button\n        from gpiozero.tools import queued\n        from signal import pause\n\n        leds = LEDBoard(5, 6, 13, 19, 26)\n        btn = Button(17)\n\n        for i in range(4):\n            leds[i].source = queued(leds[i + 1], 5)\n            leds[i].source_delay = 0.01\n\n        leds[4].source = btn\n\n        pause()\n    \"\"\"\n", "input": "", "output": "    values = [_normalize(v) for v in values]\n    if qsize < 1:\n        raise ValueError(\"qsize must be 1 or larger\")\n    q = []\n    it = iter(values)\n    try:\n        for i in range(qsize):\n            q.append(next(it))\n        for i in cycle(range(qsize)):\n            yield q[i]\n            q[i] = next(it)\n    except StopIteration:\n        pass", "category": "Python"}, {"instruction": "def trigger_event(self, source, event, args):\n\t\t\"\"\"\n\t\tTrigger an event on the Entity\n\t\t* \\a source: The source of the event\n\t\t* \\a event: The event being triggered\n\t\t* \\a args: A list of arguments to pass to the callback\n\t\t\"\"\"\n", "input": "", "output": "\t\tactions = []\n\t\tfor action in event.actions:\n\t\t\tif callable(action):\n\t\t\t\tac = action(self, *args)\n\t\t\t\tif not ac:\n\t\t\t\t\t# Handle falsy returns\n\t\t\t\t\tcontinue\n\t\t\t\tif not hasattr(ac, \"__iter__\"):\n\t\t\t\t\tactions.append(ac)\n\t\t\t\telse:\n\t\t\t\t\tactions += action(self, *args)\n\t\t\telse:\n\t\t\t\tactions.append(action)\n\t\tret = source.game.trigger(self, actions, args)\n\t\tif event.once:\n\t\t\tself._events.remove(event)\n\n\t\treturn ret", "category": "Python"}, {"instruction": "def after_unassign(duplicate_analysis):\n    \"\"\"Removes the duplicate from the system\n    \"\"\"\n", "input": "", "output": "    analysis_events.after_unassign(duplicate_analysis)\n    parent = duplicate_analysis.aq_parent\n    logger.info(\"Removing duplicate '{}' from '{}'\"\n                .format(duplicate_analysis.getId(), parent.getId()))\n    parent.manage_delObjects([duplicate_analysis.getId()])", "category": "Python"}, {"instruction": "def get_doc_frequencies(dtm, min_val=1, proportions=False):\n    \"\"\"\n    For each word in the vocab of `dtm` (i.e. its columns), return how often it occurs at least `min_val` times.\n    If `proportions` is True, return proportions scaled to the number of documents instead of absolute numbers.\n    \"\"\"\n", "input": "", "output": "    if dtm.ndim != 2:\n        raise ValueError('`dtm` must be a 2D array/matrix')\n\n    doc_freq = np.sum(dtm >= min_val, axis=0)\n\n    if doc_freq.ndim != 1:\n        doc_freq = doc_freq.A.flatten()\n\n    if proportions:\n        return doc_freq / dtm.shape[0]\n    else:\n        return doc_freq", "category": "Python"}, {"instruction": "def digest(self, **args):\r\n        \"\"\"calculate a digest based on the hash of the XML content\"\"\"\n", "input": "", "output": "        return String(XML.canonicalized_string(self.root)).digest(**args)", "category": "Python"}, {"instruction": "def on_menu_clear_interpretation(self, event):\n        '''\n        clear all current interpretations.\n        '''\n", "input": "", "output": "\n        #  delete all previous interpretation\n        for sp in list(self.Data.keys()):\n            del self.Data[sp]['pars']\n            self.Data[sp]['pars'] = {}\n            self.Data[sp]['pars']['lab_dc_field'] = self.Data[sp]['lab_dc_field']\n            self.Data[sp]['pars']['er_specimen_name'] = self.Data[sp]['er_specimen_name']\n            self.Data[sp]['pars']['er_sample_name'] = self.Data[sp]['er_sample_name']\n        self.Data_samples = {}\n        self.Data_sites = {}\n        self.tmin_box.SetValue(\"\")\n        self.tmax_box.SetValue(\"\")\n        self.clear_boxes()\n        self.draw_figure(self.s)", "category": "Python"}, {"instruction": "def rewind(self):\n        '''rewind to start'''\n", "input": "", "output": "        self._index = 0\n        self.percent = 0\n        self.messages = {}\n        self._flightmode_index = 0\n        self._timestamp = None\n        self.flightmode = None\n        self.params = {}", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"Process from queue until it is empty.\"\"\"\n", "input": "", "output": "        try:\n            while not self.stopped:\n                scraperobj = jobs.get(False)\n                self.setName(scraperobj.getName())\n                try:\n                    self.getStrips(scraperobj)\n                finally:\n                    jobs.task_done()\n                    self.setName(self.origname)\n        except Empty:\n            pass\n        except KeyboardInterrupt:\n            thread.interrupt_main()", "category": "Python"}, {"instruction": "def load(cls, path):\n        \"\"\"\n        Load a SOM from a JSON file saved with this package..\n\n        Parameters\n        ----------\n        path : str\n            The path to the JSON file.\n\n        Returns\n        -------\n        s : cls\n            A som of the specified class.\n\n        \"\"\"\n", "input": "", "output": "        data = json.load(open(path))\n\n        weights = data['weights']\n        weights = np.asarray(weights, dtype=np.float64)\n\n        s = cls(data['map_dimensions'],\n                data['params']['lr']['orig'],\n                data['data_dimensionality'],\n                influence=data['params']['infl']['orig'],\n                lr_lambda=data['params']['lr']['factor'],\n                infl_lambda=data['params']['infl']['factor'])\n\n        s.weights = weights\n        s.trained = True\n\n        return s", "category": "Python"}, {"instruction": "def cmd(send, msg, args):\n    \"\"\"Translate something.\n\n    Syntax: {command} [--from <language code>] [--to <language code>] <text>\n    See https://cloud.google.com/translate/v2/translate-reference#supported_languages for a list of valid language codes\n\n    \"\"\"\n", "input": "", "output": "    parser = arguments.ArgParser(args['config'])\n    parser.add_argument('--lang', '--from', default=None)\n    parser.add_argument('--to', default='en')\n    parser.add_argument('msg', nargs='+')\n    try:\n        cmdargs = parser.parse_args(msg)\n    except arguments.ArgumentException as e:\n        send(str(e))\n        return\n    send(gen_translate(' '.join(cmdargs.msg), cmdargs.lang, cmdargs.to))", "category": "Python"}, {"instruction": "def save(self):\n        \"\"\"The save method for Animal class is over-ridden to set Alive=False when a Death date is entered.  This is not the case for a cause of death.\"\"\"\n", "input": "", "output": "        if self.Death:\n            self.Alive = False\n        super(Animal, self).save()", "category": "Python"}, {"instruction": "def get_projects():\n    \"\"\"\n    Returns a dict of projects present in the database.\n    :return: dict -> {<int_keys>: <project_name>}\n    \"\"\"\n", "input": "", "output": "    conn, c = open_data_base_connection()\n    try:\n        c.execute(", "category": "Python"}, {"instruction": "def iter_doc_objs(self, **kwargs):\n        \"\"\"Generator that iterates over all detected documents (eg, nexson studies)\n        and returns the doc object (deserialized from JSON) for each doc.\n        Order is by shard, but arbitrary within shards.\n        @TEMP not locked to prevent doc creation/deletion\n        \"\"\"\n", "input": "", "output": "        for shard in self._shards:\n            for doc_id, blob in shard.iter_doc_objs(**kwargs):\n                yield doc_id, blob", "category": "Python"}, {"instruction": "def mean_obliquity(jd_tdb):\n    \"\"\"Return the mean obliquity of the ecliptic in arcseconds.\n\n    `jd_tt` - TDB time as a Julian date float, or NumPy array of floats\n\n    \"\"\"\n", "input": "", "output": "    # Compute time in Julian centuries from epoch J2000.0.\n\n    t = (jd_tdb - T0) / 36525.0\n\n    # Compute the mean obliquity in arcseconds.  Use expression from the\n    # reference's eq. (39) with obliquity at J2000.0 taken from eq. (37)\n    # or Table 8.\n\n    epsilon = (((( -  0.0000000434   * t\n                   -  0.000000576  ) * t\n                   +  0.00200340   ) * t\n                   -  0.0001831    ) * t\n                   - 46.836769     ) * t + 84381.406\n\n    return epsilon", "category": "Python"}, {"instruction": "def intervals_containing(t, p):\n    \"\"\"Query the interval tree\n\n    :param t: root of the interval tree\n    :param p: value\n    :returns: a list of intervals containing p\n    :complexity: O(log n + m), where n is the number of intervals in t,\n                and m the length of the returned list\n    \"\"\"\n", "input": "", "output": "    INF = float('inf')\n    if t is None:\n        return []\n    if p < t.center:\n        retval = intervals_containing(t.left, p)\n        j = bisect_right(t.by_low, (p, (INF, INF)))\n        for i in range(j):\n            retval.append(t.by_low[i][1])\n    else:\n        retval = intervals_containing(t.right, p)\n        i = bisect_right(t.by_high, (p, (INF, INF)))\n        for j in range(i, len(t.by_high)):\n            retval.append(t.by_high[j][1])\n    return retval", "category": "Python"}, {"instruction": "def save_to_npy_file(self, parameter_space,\n                         result_parsing_function,\n                         filename, runs):\n        \"\"\"\n        Save results to a numpy array file format.\n        \"\"\"\n", "input": "", "output": "        np.save(filename, self.get_results_as_numpy_array(\n            parameter_space, result_parsing_function, runs=runs))", "category": "Python"}, {"instruction": "def find_replace(self, node):\n        \"\"\"Try to find replace node for current node.\n\n        Parameters\n        ----------\n        node : docutil node\n            Node to find replacement for.\n\n        Returns\n        -------\n        nodes : node or list of node\n            The replacement nodes of current node.\n            Returns None if no replacement can be found.\n        \"\"\"\n", "input": "", "output": "        newnode = None\n        if isinstance(node, nodes.Sequential):\n            newnode = self.auto_toc_tree(node)\n        elif isinstance(node, nodes.literal_block):\n            newnode = self.auto_code_block(node)\n        elif isinstance(node, nodes.literal):\n            newnode = self.auto_inline_code(node)\n        return newnode", "category": "Python"}, {"instruction": "def sigOmega(self,dangle):\n        \"\"\"\n        NAME:\n\n           sigmaOmega\n\n        PURPOSE:\n\n           calculate the 1D sigma in frequency as a function of angle, assuming a uniform time distribution up to a maximum time\n\n        INPUT:\n\n           dangle - angle offset\n\n        OUTPUT:\n\n           sigma Omega\n\n        HISTORY:\n\n           2013-12-05 - Written - Bovy (IAS)\n\n        \"\"\"\n", "input": "", "output": "        dOmin= dangle/self._tdisrupt\n        meandO= self._meandO\n        sO1D2= ((numpy.sqrt(2./numpy.pi)*numpy.sqrt(self._sortedSigOEig[2])\\\n                     *(meandO+dOmin)\\\n                     *numpy.exp(-0.5*(meandO-dOmin)**2.\\\n                                   /self._sortedSigOEig[2])/\n                (1.+special.erf((meandO-dOmin)\\\n                                    /numpy.sqrt(2.*self._sortedSigOEig[2]))))\\\n                   +meandO**2.+self._sortedSigOEig[2])\n        mO= self.meanOmega(dangle,oned=True,use_physical=False)\n        return numpy.sqrt(sO1D2-mO**2.)", "category": "Python"}, {"instruction": "def __loadindcomps(self):\n        ''' import industry comps '''\n", "input": "", "output": "        csv_path = os.path.join(os.path.dirname(__file__), self.stock_no_files)\n        with open(csv_path) as csv_file:\n            csv_data = csv.reader(csv_file)\n            result = {}\n            check_words = re.compile(r'^[\\d]{2,}[\\w]?')\n            for i in csv_data:\n                if check_words.match(i[2]):\n                    try:\n                        result[i[2]].append(i[0].decode('utf-8'))\n                    except (ValueError, KeyError):\n                        try:\n                            result[i[2]] = [i[0].decode('utf-8')]\n                        except KeyError:\n                            pass\n            return result", "category": "Python"}, {"instruction": "def _initialize_rest(self):\n        \"\"\"Used to initialize the View object on first use.\n        \"\"\"\n", "input": "", "output": "        if self._submit_context is None:\n            raise ValueError(\"View has not been created.\")\n        job = self._submit_context._job_access()\n        self._view_object = job.get_views(name=self.name)[0]", "category": "Python"}, {"instruction": "def filters(self):\n        \"\"\"List of filters available for the dataset.\"\"\"\n", "input": "", "output": "        if self._filters is None:\n            self._filters, self._attributes = self._fetch_configuration()\n        return self._filters", "category": "Python"}, {"instruction": "def float_constructor(loader, node):\n    \"\"\"Construct Decimal from YAML float encoding.\"\"\"\n", "input": "", "output": "    s = loader.construct_scalar(node)\n    if s == '.inf':\n        return Decimal('Infinity')\n    elif s == '-.inf':\n        return -Decimal('Infinity')\n    elif s == '.nan':\n        return Decimal('NaN')\n    return Decimal(s)", "category": "Python"}, {"instruction": "def inject(**params):\n\n    \"\"\"\n    A Logbook processor to inject arbitrary information into log records.\n\n    Simply pass in keyword arguments and use as a context manager:\n\n        >>> with inject(identifier=str(uuid.uuid4())).applicationbound():\n        ...     logger.debug('Something happened')\n    \"\"\"\n", "input": "", "output": "\n    def callback(log_record):\n        log_record.extra.update(params)\n    return logbook.Processor(callback)", "category": "Python"}, {"instruction": "def DP_calc(TPR, TNR):\n    \"\"\"\n    Calculate DP (Discriminant power).\n\n    :param TNR: specificity or true negative rate\n    :type TNR : float\n    :param TPR: sensitivity, recall, hit rate, or true positive rate\n    :type TPR : float\n    :return: DP as float\n    \"\"\"\n", "input": "", "output": "    try:\n        X = TPR / (1 - TPR)\n        Y = TNR / (1 - TNR)\n        return (math.sqrt(3) / math.pi) * (math.log(X, 10) + math.log(Y, 10))\n    except Exception:\n        return \"None\"", "category": "Python"}, {"instruction": "def create_token(self, request, **kwargs):\n        \"\"\"Create a new obfuscated url info to use for accessing unpublished content.\n\n        :param request: a WSGI request object\n        :param kwargs: keyword arguments (optional)\n        :return: `rest_framework.response.Response`\n        \"\"\"\n", "input": "", "output": "\n        data = {\n            \"content\": self.get_object().id,\n            \"create_date\": get_request_data(request)[\"create_date\"],\n            \"expire_date\": get_request_data(request)[\"expire_date\"]\n        }\n        serializer = ObfuscatedUrlInfoSerializer(data=data)\n        if not serializer.is_valid():\n            return Response(\n                serializer.errors,\n                status=status.HTTP_400_BAD_REQUEST,\n                content_type=\"application/json\",\n            )\n        serializer.save()\n\n        return Response(serializer.data, status=status.HTTP_200_OK, content_type=\"application/json\")", "category": "Python"}, {"instruction": "def step_command_output_should_contain_exactly_text(context, text):\n    \"\"\"\n    Verifies that the command output of the last command contains the\n    expected text.\n\n    .. code-block:: gherkin\n\n        When I run \"echo Hello\"\n        Then the command output should contain \"Hello\"\n    \"\"\"\n", "input": "", "output": "    expected_text = text\n    if \"{__WORKDIR__}\" in text or \"{__CWD__}\" in text:\n        expected_text = textutil.template_substitute(text,\n             __WORKDIR__ = posixpath_normpath(context.workdir),\n             __CWD__     = posixpath_normpath(os.getcwd())\n        )\n    actual_output  = context.command_result.output\n    textutil.assert_text_should_contain_exactly(actual_output, expected_text)", "category": "Python"}, {"instruction": "def mapPartitions(self, f, preservesPartitioning=False):\n        \"\"\"\n        Return a new RDD by applying a function to each partition of this RDD.\n\n        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n        >>> def f(iterator): yield sum(iterator)\n        >>> rdd.mapPartitions(f).collect()\n        [3, 7]\n        \"\"\"\n", "input": "", "output": "        def func(s, iterator):\n            return f(iterator)\n        return self.mapPartitionsWithIndex(func, preservesPartitioning)", "category": "Python"}, {"instruction": "def list_settings(self):\n        \"\"\"\n        Get list of all appropriate settings and their default values.\n        \"\"\"\n", "input": "", "output": "        result = super().list_settings()\n        result.append((self.SETTING_FLAG_HEADER, True))\n        result.append((self.SETTING_HEADER_CONTENT, 'Notice'))\n        result.append((self.SETTING_HEADER_FORMATING, {'attr': 'bold'}))\n        result.append((self.SETTING_FLAG_ENUMERATE, False))\n        result.append((self.SETTING_COLUMNS, None))\n        result.append((self.SETTING_ROW_HIGHLIGHT, None))\n        return result", "category": "Python"}, {"instruction": "def _read_header(filename):\n    \"\"\"Read the text header for each file\n\n    Parameters\n    ----------\n    channel_file : Path\n        path to single filename with the header\n\n    Returns\n    -------\n    dict\n        header\n    \"\"\"\n", "input": "", "output": "    with filename.open('rb') as f:\n        h = f.read(HDR_LENGTH).decode()\n\n        header = {}\n        for line in h.split('\\n'):\n            if '=' in line:\n                key, value = line.split(' = ')\n                key = key.strip()[7:]\n                value = value.strip()[:-1]\n                header[key] = value\n\n    return header", "category": "Python"}, {"instruction": "def set_widgets(self):\n        \"\"\"Set widgets on the Extent tab.\"\"\"\n", "input": "", "output": "        self.extent_dialog = ExtentSelectorDialog(\n            self.parent.iface,\n            self.parent.iface.mainWindow(),\n            extent=self.parent.dock.extent.user_extent,\n            crs=self.parent.dock.extent.crs)\n        self.extent_dialog.tool.rectangle_created.disconnect(\n            self.extent_dialog.stop_capture)\n        self.extent_dialog.clear_extent.connect(\n            self.parent.dock.extent.clear_user_analysis_extent)\n        self.extent_dialog.extent_defined.connect(\n            self.parent.dock.define_user_analysis_extent)\n        self.extent_dialog.capture_button.clicked.connect(\n            self.start_capture_coordinates)\n        self.extent_dialog.tool.rectangle_created.connect(\n            self.stop_capture_coordinates)\n\n        self.extent_dialog.label.setText(tr(\n            'Please specify extent of your analysis:'))\n\n        if self.swExtent:\n            self.swExtent.hide()\n\n        self.swExtent = self.extent_dialog.main_stacked_widget\n        self.layoutAnalysisExtent.addWidget(self.swExtent)", "category": "Python"}, {"instruction": "def get_plugins(namespace, interface=None, check_extras=True, load_now=False):\n    \"\"\"\n    helper to get a direct interface to _Plugins\n    \"\"\"\n", "input": "", "output": "    return _DB.add_namespace(namespace, interface, check_extras, load_now)", "category": "Python"}, {"instruction": "def read(self, symbol, as_of=None):\n        \"\"\"\n        Return current metadata saved for `symbol`\n\n        Parameters\n        ----------\n        symbol : `str`\n            symbol name for the item\n        as_of : `datetime.datetime`\n            return entry valid at given time\n\n        Returns\n        -------\n        metadata\n        \"\"\"\n", "input": "", "output": "        if as_of is not None:\n            res = self.find_one({'symbol': symbol, 'start_time': {'$lte': as_of}},\n                                sort=[('start_time', pymongo.DESCENDING)])\n        else:\n            res = self.find_one({'symbol': symbol}, sort=[('start_time', pymongo.DESCENDING)])\n        return res['metadata'] if res is not None else None", "category": "Python"}, {"instruction": "def _estimate_strains(self):\n        \"\"\"Estimate the strains by running an EQL site response.\n\n        This step was recommended in Section 8.3.1 of Zalachoris (2014).\n        \"\"\"\n", "input": "", "output": "        eql = EquivalentLinearCalculator()\n        eql(self._motion, self._profile, self._loc_input)", "category": "Python"}, {"instruction": "def update_resource_fields(self, data, data_to_add):\n        \"\"\"Update resource data with new fields.\n\n        Args:\n            data: resource data\n            data_to_update: dict of data to update resource data\n\n        Returnes:\n            Returnes dict\n        \"\"\"\n", "input": "", "output": "        for key, value in data_to_add.items():\n            if not data.get(key):\n                data[key] = value\n\n        return data", "category": "Python"}, {"instruction": "def parse_user_params(user_params):\n    \"\"\"\n    Parse the user params (-p/--params) and them as a dict.\n    \"\"\"\n", "input": "", "output": "    if user_params:\n        params = {}\n        try:\n            for param in options.params.split(','):\n                param_key, param_value = param.split('=', 1)\n                params[param_key] = param_value\n        except ValueError as e:\n            sys.stdout.write(\"Invalid params specified. Should be in format: <key=value>[,<key=value>..]\\n\")\n            sys.exit(1)\n        return params\n    else:\n        return {}", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"Start the sensor.\n        \"\"\"\n", "input": "", "output": "        running = self._webcam.start()\n        if not running:\n            return running\n\n        running &= self._phoxi.start()\n        if not running:\n            self._webcam.stop()\n        return running", "category": "Python"}, {"instruction": "def create(cls, d):\n        \"\"\" Create a :class:`~pypot.primitive.move.Move` from a dictionary. \"\"\"\n", "input": "", "output": "        move = cls(d['framerate'])\n        move._timed_positions.update(d['positions'])\n        return move", "category": "Python"}, {"instruction": "def get_last_response_xml(self, pretty_print_if_possible=False):\n        \"\"\"\n        Retrieves the raw XML (decrypted) of the last SAML response,\n        or the last Logout Response generated or processed\n        :returns: SAML response XML\n        :rtype: string|None\n        \"\"\"\n", "input": "", "output": "        response = None\n        if self.__last_response is not None:\n            if isinstance(self.__last_response, compat.str_type):\n                response = self.__last_response\n            else:\n                response = tostring(self.__last_response, encoding='unicode', pretty_print=pretty_print_if_possible)\n        return response", "category": "Python"}, {"instruction": "def morphChunkedDataLists(fromDataList, toDataList, stepList):\n    '''\n    Morph one set of data into another, in a stepwise fashion\n\n    A convenience function.  Given a set of paired data lists,\n    this will morph each one individually.\n\n    Returns a single list with all data combined together.\n    '''\n", "input": "", "output": "\n    assert(len(fromDataList) == len(toDataList))\n\n    # Morph the fromDataList into the toDataList\n    outputList = []\n    for x, y in zip(fromDataList, toDataList):\n\n        # We cannot morph a region if there is no data or only\n        # a single data point for either side\n        if (len(x) < 2) or (len(y) < 2):\n            continue\n\n        tmpList = [outputPitchList for _, outputPitchList\n                   in morphDataLists(x, y, stepList)]\n        outputList.append(tmpList)\n\n    # Transpose list\n    finalOutputList = outputList.pop(0)\n    for subList in outputList:\n        for i, subsubList in enumerate(subList):\n            finalOutputList[i].extend(subsubList)\n\n    return finalOutputList", "category": "Python"}, {"instruction": "def cipher_block (self, state):\n        \"\"\"Perform AES block cipher on input\"\"\"\n", "input": "", "output": "        # PKCS7 Padding\n        state=state+[16-len(state)]*(16-len(state))# Fails test if it changes the input with +=\n\n        self._add_round_key(state, 0)\n\n        for i in range(1, self._Nr):\n            self._sub_bytes(state)\n            self._shift_rows(state)\n            self._mix_columns(state, False)\n            self._add_round_key(state, i)\n\n        self._sub_bytes(state)\n        self._shift_rows(state)\n        self._add_round_key(state, self._Nr)\n        return state", "category": "Python"}, {"instruction": "def cogroup(self, other, numPartitions=None):\n        \"\"\"\n        Return a new DStream by applying 'cogroup' between RDDs of this\n        DStream and `other` DStream.\n\n        Hash partitioning is used to generate the RDDs with `numPartitions` partitions.\n        \"\"\"\n", "input": "", "output": "        if numPartitions is None:\n            numPartitions = self._sc.defaultParallelism\n        return self.transformWith(lambda a, b: a.cogroup(b, numPartitions), other)", "category": "Python"}, {"instruction": "def sanitize_gff_file(gff_fname,\n                      in_memory=True,\n                      in_place=False):\n    \"\"\"\n    Sanitize a GFF file.\n    \"\"\"\n", "input": "", "output": "    db = None\n    if is_gff_db(gff_fname):\n        # It's a database filename, so load it\n        db = gffutils.FeatureDB(gff_fname)\n    else:\n        # Need to create a database for file\n        if in_memory:\n            db = gffutils.create_db(gff_fname, \":memory:\",\n                                    verbose=False)\n        else:\n            db = get_gff_db(gff_fname)\n    if in_place:\n        gff_out = gffwriter.GFFWriter(gff_fname,\n                                      in_place=in_place)\n    else:\n        gff_out = gffwriter.GFFWriter(sys.stdout)\n    sanitized_db = sanitize_gff_db(db)\n    for gene_rec in sanitized_db.all_features(featuretype=\"gene\"):\n        gff_out.write_gene_recs(sanitized_db, gene_rec.id)\n    gff_out.close()", "category": "Python"}, {"instruction": "def getElementsByType(self, type):\n        \"\"\"\n        retrieves all Elements that are of type type\n        @type  type: class \n        @param type:  type of the element \n        \"\"\"\n", "input": "", "output": "        foundElements=[]\n        for element in self.getAllElementsOfHirarchy():\n            if isinstance(element, type):\n                foundElements.append(element)                \n                \n        return foundElements", "category": "Python"}, {"instruction": "def _format_line(self, side, flag, linenum, text):\n        \"\"\"Returns HTML markup of \"from\" / \"to\" text lines\n\n        side -- 0 or 1 indicating \"from\" or \"to\" text\n        flag -- indicates if difference on line\n        linenum -- line number (used for line number column)\n        text -- line text to be marked up\n        \"\"\"\n", "input": "", "output": "        try:\n            linenum = '%d' % linenum\n            id = ' id=\"%s%s\"' % (self._prefix[side], linenum)\n        except TypeError:\n            # handle blank lines where linenum is '>' or ''\n            id = ''\n        # replace those things that would get confused with HTML symbols\n        text = text.replace(\"&\", \"&amp;\"). \\\n            replace(\">\", \"&gt;\"). \\\n            replace(\"<\", \"&lt;\")\n\n        # make space non-breakable so they don't get compressed or line wrapped\n        text = text.replace(' ', '&nbsp;').rstrip()\n\n        color = ''\n        if '\\0^' in text or '\\0+' in text or '\\0-' in text:\n            color = ';background-color:{0}'\n            if side == 0:\n                color = color.format('#ffe6e6')\n            else:\n                color = color.format('#e3ffe3')\n        return self.TD_DIFF_HEADER.format(id, linenum, color, text)", "category": "Python"}, {"instruction": "def _send_tasks_and_stop_queuing(**kwargs):\n    \"\"\"Sends all delayed Celery tasks and stop queuing new ones for now.\"\"\"\n", "input": "", "output": "    log.info('Stopping queueing tasks and sending already queued ones.')\n    _stop_queuing_tasks()\n    task_queue = _get_task_queue()\n    while task_queue:\n        task, args, kwargs, extrakw = task_queue.pop(0)\n        task.original_apply_async(args=args, kwargs=kwargs, **extrakw)", "category": "Python"}, {"instruction": "def adjust_bounding_box(bbox):\n    \"\"\"Adjust the bounding box as specified by user.\n    Returns the adjusted bounding box.\n\n    - bbox: Bounding box computed from the canvas drawings.\n    It must be a four-tuple of numbers.\n    \"\"\"\n", "input": "", "output": "    for i in range(0, 4):\n        if i in bounding_box:\n            bbox[i] = bounding_box[i]\n        else:\n            bbox[i] += delta_bounding_box[i]\n    return bbox", "category": "Python"}, {"instruction": "def is_defined(self, objtxt, force_import=False):\r\n        \"\"\"Return True if object is defined\"\"\"\n", "input": "", "output": "        return isdefined(objtxt, force_import=force_import,\r\n                         namespace=self.locals)", "category": "Python"}, {"instruction": "def has_face_color(self):\n        \"\"\"Return True if this data set has face color information\"\"\"\n", "input": "", "output": "        for v in (self._face_colors, self._face_colors_indexed_by_faces,\n                  self._face_colors_indexed_by_edges):\n            if v is not None:\n                return True\n        return False", "category": "Python"}, {"instruction": "def _parse_join(client, command, actor, args):\n    \"\"\"Parse a JOIN and update channel states, then dispatch events.\n\n    Note that two events are dispatched here:\n        - JOIN, because a user joined the channel\n        - MEMBERS, because the channel's members changed\n    \"\"\"\n", "input": "", "output": "    actor = User(actor)\n    channel = args.lstrip(' :').lower()\n    if actor.nick == client.user.nick:\n        client.server.add_channel(channel)\n        client.user.host = actor.host # now we know our host per the server\n    channel = client.server.get_channel(channel)\n    channel.add_user(actor)\n    client.dispatch_event(\"JOIN\", actor, channel)\n    if actor.nick != client.user.nick:\n        # If this is us joining, the namreply will trigger this instead\n        client.dispatch_event(\"MEMBERS\", channel)", "category": "Python"}, {"instruction": "def gborders(img, alpha=1.0, sigma=1.0):\n    \"\"\"Stopping criterion for image borders.\"\"\"\n", "input": "", "output": "    # The norm of the gradient.\n    gradnorm = gaussian_gradient_magnitude(img, sigma, mode='constant')\n    return 1.0/np.sqrt(1.0 + alpha*gradnorm)", "category": "Python"}, {"instruction": "def _check_data(wavfile, station, channel, debug=0):\n    \"\"\"\n    Inner loop for parallel checks.\n\n    :type wavfile: str\n    :param wavfile: Wavefile path name to look in.\n    :type station: str\n    :param station: Channel name to check for\n    :type channel: str\n    :param channel: Channel name to check for\n    :type debug: int\n    :param debug: Debug level, if > 1, will output what it it working on.\n    \"\"\"\n", "input": "", "output": "    if debug > 1:\n        print('Checking ' + wavfile)\n    st = read(wavfile, headonly=True)\n    for tr in st:\n        if tr.stats.station == station and tr.stats.channel == channel:\n            return wavfile", "category": "Python"}, {"instruction": "def update(self, old_line, new_line, once=False):\n        \"\"\"Replace all lines matching `old_line` with `new_line`.\n\n        If ``once`` is set to True, remove only the first instance.\n        \"\"\"\n", "input": "", "output": "        nb = 0\n        for i, line in enumerate(self.lines):\n            if line.match(old_line):\n                self.lines[i] = new_line\n                nb += 1\n                if once:\n                    return nb\n        return nb", "category": "Python"}, {"instruction": "def get_traffic(self, subreddit):\n        \"\"\"Return the json dictionary containing traffic stats for a subreddit.\n\n        :param subreddit: The subreddit whose /about/traffic page we will\n            collect.\n\n        \"\"\"\n", "input": "", "output": "        url = self.config['subreddit_traffic'].format(\n            subreddit=six.text_type(subreddit))\n        return self.request_json(url)", "category": "Python"}, {"instruction": "def unix_time(self, dt):\n        \"\"\"Returns the number of seconds since the UNIX epoch for the given\n        datetime (dt).\n\n        PARAMETERS:\n        dt -- datetime\n        \"\"\"\n", "input": "", "output": "        epoch = datetime.utcfromtimestamp(0)\n        delta = dt - epoch\n        return int(delta.total_seconds())", "category": "Python"}, {"instruction": "def _interpret_ltude(value, name, psuffix, nsuffix):\n    \"\"\"Interpret a string, float, or tuple as a latitude or longitude angle.\n\n    `value` - The string to interpret.\n    `name` - 'latitude' or 'longitude', for use in exception messages.\n    `positive` - The string that indicates a positive angle ('N' or 'E').\n    `negative` - The string that indicates a negative angle ('S' or 'W').\n\n    \"\"\"\n", "input": "", "output": "    if not isinstance(value, str):\n        return Angle(degrees=_unsexagesimalize(value))\n\n    value = value.strip().upper()\n\n    if value.endswith(psuffix):\n        sign = +1.0\n    elif value.endswith(nsuffix):\n        sign = -1.0\n    else:\n        raise ValueError('your {0} string {1!r} does not end with either {2!r}'\n                         ' or {3!r}'.format(name, value, psuffix, nsuffix))\n\n    try:\n        value = float(value[:-1])\n    except ValueError:\n        raise ValueError('your {0} string {1!r} cannot be parsed as a floating'\n                         ' point number'.format(name, value))\n\n    return Angle(degrees=sign * value)", "category": "Python"}, {"instruction": "def make_str(value):\n    \"\"\"Converts a value into a valid string.\"\"\"\n", "input": "", "output": "    if isinstance(value, bytes):\n        try:\n            return value.decode(sys.getfilesystemencoding())\n        except UnicodeError:\n            return value.decode('utf-8', 'replace')\n    return text_type(value)", "category": "Python"}, {"instruction": "def get_relationship_family_session(self, proxy=None):\n        \"\"\"Gets the ``OsidSession`` to lookup relationship/family mappings.\n\n        arg:    proxy (osid.proxy.Proxy): a proxy\n        return: (osid.relationship.RelationshipFamilySession) - a\n                ``RelationshipFamilySession``\n        raise:  NullArgument - ``proxy`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented - ``supports_relationship_family()`` is\n                ``false``\n        *compliance: optional -- This method must be implemented if ``supports_relationship_family()``\n            is ``true``.*\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_relationship_family():\n            raise Unimplemented()\n        try:\n            from . import sessions\n        except ImportError:\n            raise OperationFailed()\n        proxy = self._convert_proxy(proxy)\n        try:\n            session = sessions.RelationshipFamilySession(proxy=proxy, runtime=self._runtime)\n        except AttributeError:\n            raise OperationFailed()\n        return session", "category": "Python"}, {"instruction": "def remove_exts(self, exts):\n        \"\"\"\n        Remove the files with the given extensions. Unlike rmtree, this function preserves the directory path.\n        Return list with the absolute paths of the files that have been removed.\n        \"\"\"\n", "input": "", "output": "        paths = []\n\n        for ext in list_strings(exts):\n            path = self.has_abiext(ext)\n            if not path: continue\n            try:\n                os.remove(path)\n                paths.append(path)\n            except IOError:\n                logger.warning(\"Exception while trying to remove file %s\" % path)\n\n        return paths", "category": "Python"}, {"instruction": "def onMessage(self, payload, is_binary):\n        \"\"\"\n        Called when a client sends a message\n        \"\"\"\n", "input": "", "output": "        if not is_binary:\n            payload = payload.decode('utf-8')\n\n            logger.debug(\"Incoming message ({peer}) : {message}\".format(\n                peer=self.peer, message=payload))\n\n            # Publish ON_RECEIVE message\n            self.factory.mease.publisher.publish(\n                message_type=ON_RECEIVE,\n                client_id=self._client_id,\n                client_storage=self.storage,\n                message=payload)", "category": "Python"}, {"instruction": "def invalidate_token(self, body, params=None):\n        \"\"\"\n        `<https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-invalidate-token.html>`_\n\n        :arg body: The token to invalidate\n        \"\"\"\n", "input": "", "output": "        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"DELETE\", \"/_security/oauth2/token\", params=params, body=body\n        )", "category": "Python"}, {"instruction": "def openSafeReplace(filepath, mode='w+b'):\n    \"\"\"Context manager to open a temporary file and replace the original file on\n    closing.\n    \"\"\"\n", "input": "", "output": "    tempfileName = None\n    #Check if the filepath can be accessed and is writable before creating the\n    #tempfile\n    if not _isFileAccessible(filepath):\n        raise IOError('File %s is not writtable' % (filepath, ))\n    with tempfile.NamedTemporaryFile(delete=False, mode=mode) as tmpf:\n        tempfileName = tmpf.name\n        yield tmpf\n    #Check if the filepath can be accessed and is writable before moving the\n    #tempfile\n    if not _isFileAccessible(filepath):\n        raise IOError('File %s is not writtable' % (filepath, ))\n    #Note: here unhandled exceptions may still occur because of race conditions,\n    #messing things up.\n    shutil.move(tempfileName, filepath)", "category": "Python"}, {"instruction": "def send(self, signum):\n        \"\"\"Send the given signal to the running process.\n\n        If the process is not running a RuntimeError with a message of \"No such\n        process\" should be emitted.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(signum, int):\n\n            raise TypeError(\n                \"Signals must be given as integers. Got {0}.\".format(\n                    type(signum),\n                ),\n            )\n\n        try:\n\n            os.kill(self.pid, signum)\n\n        except OSError as err:\n\n            if \"No such process\" in err.strerror:\n\n                raise RuntimeError(\"No such process {0}.\".format(self.pid))\n\n            raise err", "category": "Python"}, {"instruction": "def set_headline(self, level, message, timestamp=None, now_reference=None):\n        \"\"\"Set the persistent headline message for this service.\n\n        Args:\n            level (int): The level of the message (info, warning, error)\n            message (string): The message contents\n            timestamp (float): An optional monotonic value in seconds for when the message was created\n            now_reference (float): If timestamp is not relative to monotonic() as called from this\n                module then this should be now() as seen by whoever created the timestamp.\n        \"\"\"\n", "input": "", "output": "\n        if self.headline is not None and self.headline.message == message:\n            self.headline.created = monotonic()\n            self.headline.count += 1\n            return\n\n        msg_object = ServiceMessage(level, message, self._last_message_id, timestamp, now_reference)\n        self.headline = msg_object\n        self._last_message_id += 1", "category": "Python"}, {"instruction": "def _check_nan(self, epoch_data: EpochData) -> None:\n        \"\"\"\n        Raise an exception when some of the monitored data is NaN.\n\n        :param epoch_data: epoch data checked\n        :raise KeyError: if the specified variable is not found in the stream\n        :raise ValueError: if the variable value is of unsupported type and ``self._on_unknown_type`` is set to ``error``\n        \"\"\"\n", "input": "", "output": "        for stream_name in epoch_data.keys():\n            stream_data = epoch_data[stream_name]\n            variables = self._variables if self._variables is not None else stream_data.keys()\n            for variable in variables:\n                if variable not in stream_data:\n                    raise KeyError('Variable `{}` to be nan-checked was not found in the batch data for stream `{}`. '\n                                   'Available variables are `{}`.'.format(variable, stream_name, stream_data.keys()))\n\n                value = stream_data[variable]\n                if self._is_nan(variable, value):\n                    raise TrainingTerminated('Variable `{}` is NaN.'.format(variable))", "category": "Python"}, {"instruction": "def remove_var(var):\n    '''\n    Remove a variable from the make.conf\n\n    Return a dict containing the new value for the variable::\n\n        {'<variable>': {'old': '<old-value>',\n                        'new': '<new-value>'}}\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' makeconf.remove_var 'LINGUAS'\n    '''\n", "input": "", "output": "    makeconf = _get_makeconf()\n\n    old_value = get_var(var)\n\n    # If var is in file\n    if old_value is not None:\n        __salt__['file.sed'](makeconf, '^{0}=.*'.format(var), '')\n\n    new_value = get_var(var)\n    return {var: {'old': old_value, 'new': new_value}}", "category": "Python"}, {"instruction": "def getter(self):\n        \"\"\"Returns an instance of proper PackageGetter subclass. Always\n        returns the same instance.\n\n        Returns:\n            Instance of the proper PackageGetter subclass according to\n            provided argument.\n        Raises:\n            NoSuchSourceException if source to get the package from is unknown\n            NoSuchPackageException if the package is unknown on PyPI\n        \"\"\"\n", "input": "", "output": "        if not hasattr(self, '_getter'):\n            if not self.pypi:\n                self._getter = package_getters.LocalFileGetter(\n                    self.package,\n                    self.save_dir)\n            else:\n                logger.debug(\n                    '{0} does not exist as local file trying PyPI.'.format(\n                        self.package))\n                self._getter = package_getters.PypiDownloader(\n                    self.client,\n                    self.package,\n                    self.version,\n                    self.save_dir)\n\n        return self._getter", "category": "Python"}, {"instruction": "def top_k_logits(logits, k):\n    \"\"\"\n    Masks everything but the k top entries as -infinity (1e10).\n    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n    sum of the denominator.\n    \"\"\"\n", "input": "", "output": "    if k == 0:\n        return logits\n    else:\n        values = torch.topk(logits, k)[0]\n        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n        return torch.where(logits < batch_mins, torch.ones_like(logits) * -1e10, logits)", "category": "Python"}, {"instruction": "def is_valid(identifier):\n    \"\"\"\n    If the identifier is valid for Python, return True, otherwise False.\n    \"\"\"\n", "input": "", "output": "\n    return (\n        isinstance(identifier, six.string_types)\n        and bool(NAME_RE.search(identifier))\n        and not keyword.iskeyword(identifier)\n    )", "category": "Python"}, {"instruction": "def read_sha1(\n    file_path,\n    buf_size = None,\n    start_byte = 0,\n    read_size = None,\n    extra_hashers = [], # update(data) will be called on all of these\n):\n    '''\n    Determines the sha1 hash of a file in chunks, to prevent loading the entire file at once into memory\n    '''\n", "input": "", "output": "    read_size = read_size or os.stat(file_path).st_size\n    buf_size = buf_size or DEFAULT_BUFFER_SIZE\n\n    data_read = 0\n    total_sha1 = hashlib.sha1()\n    while data_read < read_size:\n        with open( file_path, 'rb', buffering = 0 ) as f:\n            f.seek( start_byte )\n            data = f.read( min(buf_size, read_size - data_read) )\n            assert( len(data) > 0 )\n            total_sha1.update( data )\n            for hasher in extra_hashers:\n                hasher.update( data )\n            data_read += len(data)\n            start_byte += len(data)\n    assert( data_read == read_size )\n\n    return total_sha1", "category": "Python"}, {"instruction": "def read_json(directory, data_files='data/js/tweets/*.js'):\n    '''\n    Scrape a twitter archive file.\n    Inspiration from https://github.com/mshea/Parse-Twitter-Archive\n    '''\n", "input": "", "output": "    files = path.join(directory, data_files)\n\n    for fname in iglob(files):\n        with open(fname, 'r') as f:\n            # Twitter's JSON first line is bogus\n            data = f.readlines()[1:]\n            data = \"\".join(data)\n            tweetlist = json.loads(data)\n\n        for tweet in tweetlist:\n            yield tweet", "category": "Python"}, {"instruction": "def printrdf(wflow, ctx, style):  # type: (Process, ContextType, Text) -> Text\n    \"\"\"Serialize the CWL document into a string, ready for printing.\"\"\"\n", "input": "", "output": "    rdf = gather(wflow, ctx).serialize(format=style, encoding='utf-8')\n    if not rdf:\n        return u\"\"\n    return rdf.decode('utf-8')", "category": "Python"}, {"instruction": "def _get_dstk_intersections(self, address, dstk_address):\n        \"\"\"\n        Find the unique tokens in the original address and the returned address.\n        \"\"\"\n", "input": "", "output": "        # Normalize both addresses\n        normalized_address = self._normalize(address)\n        normalized_dstk_address = self._normalize(dstk_address)\n        address_uniques = set(normalized_address) - set(normalized_dstk_address)\n        dstk_address_uniques = set(normalized_dstk_address) - set(normalized_address)\n        if self.logger: self.logger.debug(\"Address Uniques {0}\".format(address_uniques))\n        if self.logger: self.logger.debug(\"DSTK Address Uniques {0}\".format(dstk_address_uniques))\n        return (len(address_uniques), len(dstk_address_uniques))", "category": "Python"}, {"instruction": "def open_icmp_firewall(host):\n    \"\"\"Temporarily open the ICMP firewall. Tricks Windows into allowing\n    ICMP packets for a short period of time (~ 1 minute)\"\"\"\n", "input": "", "output": "    # We call ping with a timeout of 1ms: will return instantly\n    with open(os.devnull, 'wb') as DEVNULL:\n        return subprocess.Popen(\"ping -4 -w 1 -n 1 %s\" % host,\n                                shell=True,\n                                stdout=DEVNULL,\n                                stderr=DEVNULL).wait()", "category": "Python"}, {"instruction": "def fit(self, blocks, y=None):\n        \"\"\"\n        Fit a k-means clustering model using an ordered sequence of blocks.\n        \"\"\"\n", "input": "", "output": "        self.kmeans.fit(make_weninger_features(blocks))\n        # set the cluster center closest to the origin to exactly (0.0, 0.0)\n        self.kmeans.cluster_centers_.sort(axis=0)\n        self.kmeans.cluster_centers_[0, :] = np.zeros(2)\n        return self", "category": "Python"}, {"instruction": "def refine_pi_cation_laro(self, all_picat, stacks):\n        \"\"\"Just important for constellations with histidine involved. If the histidine ring is positioned in stacking\n        position to an aromatic ring in the ligand, there is in most cases stacking and pi-cation interaction reported\n        as histidine also carries a positive charge in the ring. For such cases, only report stacking.\n        \"\"\"\n", "input": "", "output": "        i_set = []\n        for picat in all_picat:\n            exclude = False\n            for stack in stacks:\n                if whichrestype(stack.proteinring.atoms[0]) == 'HIS' and picat.ring.obj == stack.ligandring.obj:\n                    exclude = True\n            if not exclude:\n                i_set.append(picat)\n        return i_set", "category": "Python"}, {"instruction": "def _update_belief(self, belief_prop, clique, clique_potential, message=None):\n        \"\"\"\n        Method for updating the belief.\n\n        Parameters:\n        ----------\n        belief_prop: Belief Propagation\n            Belief Propagation which needs to be updated.\n\n        in_clique: clique\n            The factor which needs to be updated corresponding to the input clique.\n\n        out_clique_potential: factor\n            Multiplying factor which will be multiplied to the factor corresponding to the clique.\n        \"\"\"\n", "input": "", "output": "        old_factor = belief_prop.junction_tree.get_factors(clique)\n        belief_prop.junction_tree.remove_factors(old_factor)\n        if message:\n            if message.scope() and clique_potential.scope():\n                new_factor = old_factor * message\n                new_factor = new_factor / clique_potential\n            else:\n                new_factor = old_factor\n        else:\n            new_factor = old_factor * clique_potential\n        belief_prop.junction_tree.add_factors(new_factor)\n        belief_prop.calibrate()", "category": "Python"}, {"instruction": "def remove(self, resource, lookup=None, parent=None, **kwargs):\n        \"\"\"Remove docs for resource.\n\n        :param resource: resource name\n        :param lookup: filter\n        :param parent: parent id\n        \"\"\"\n", "input": "", "output": "        kwargs.update(self._es_args(resource))\n        if parent:\n            kwargs['parent'] = parent\n\n        if lookup:\n            if lookup.get('_id'):\n                try:\n                    return self.elastic(resource).delete(id=lookup.get('_id'), refresh=True, **kwargs)\n                except elasticsearch.NotFoundError:\n                    return\n        return ValueError('there must be `lookup._id` specified')", "category": "Python"}, {"instruction": "def delete_source(self, id, **kwargs):  # noqa: E501\n        \"\"\"Delete metadata (description and tags) for a specific source  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_source(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :return: ResponseContainerSource\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_source_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_source_with_http_info(id, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def doane(data):\n        \"\"\"\n        Modified Doane modified\n        \"\"\"\n", "input": "", "output": "        from scipy.stats import skew\n        n = len(data)\n        sigma = np.sqrt(6. * (n - 2.) / (n + 1.) / (n + 3.))\n        return 1 + np.log2(n) + \\\n            np.log2(1 + np.abs(skew(data)) / sigma)", "category": "Python"}, {"instruction": "def vt_name_check(domain, vt_api):\n    \"\"\"Checks VirusTotal for occurrences of a domain name\"\"\"\n", "input": "", "output": "    if not is_fqdn(domain):\n        return None\n\n    url = 'https://www.virustotal.com/vtapi/v2/domain/report'\n    parameters = {'domain': domain, 'apikey': vt_api}\n    response = requests.get(url, params=parameters)\n    try:\n        return response.json()\n    except ValueError:\n        return None", "category": "Python"}, {"instruction": "def main(arg1=55, arg2='test', arg3=None):\n    \"\"\"\n    This is a sample program to show how a learning agent can\n    be logged using AIKIF. \n    The idea is that this main function is your algorithm, which\n    will run until it finds a successful result. The result is \n    returned and the time taken is logged.\n    \n    There can optionally be have additional functions \n    to call to allow for easy logging access\n    \"\"\"\n", "input": "", "output": "    print('Starting dummy AI algorithm with :', arg1, arg2, arg3)\n    \n    if arg3 is None:\n        arg3=[5,6,7,5,4,]\n    result = arg1 + arg3[0] * 7566.545  # dummy result\n    \n    print('Done - returning ', result)\n    return result", "category": "Python"}, {"instruction": "def _form_output(self):\n        \"\"\" Form the output \"\"\"\n", "input": "", "output": "        self.output = u''\n        if self.external_inner_xml:\n            self.output += u'<Dummy_tag_to_create_valid_xml_on_external_inner'\\\n                           '_xml>\\n'\n        self.output += u'<!-- BODY -->\\n{0}'.format(self.body_formatted)\n\n        if self.external_inner_xml:\n            for number, didl in enumerate(self.inner_xml):\n                self.output += u'\\n<!-- DIDL_{0} -->\\n{1}'.\\\n                    format(number, etree.tostring(didl, pretty_print=True))\n            self.output += u'</Dummy_tag_to_create_valid_xml_on_external_'\\\n                           'inner_xml>'", "category": "Python"}, {"instruction": "def _GenerateStatsTable(self, feed_merger):\n    \"\"\"Generate an HTML table of merge statistics.\n\n    Args:\n      feed_merger: The FeedMerger instance.\n\n    Returns:\n      The generated HTML as a string.\n    \"\"\"\n", "input": "", "output": "    rows = []\n    rows.append('<tr><th class=\"header\"/><th class=\"header\">Merged</th>'\n                '<th class=\"header\">Copied from old feed</th>'\n                '<th class=\"header\">Copied from new feed</th></tr>')\n    for merger in feed_merger.GetMergerList():\n      stats = merger.GetMergeStats()\n      if stats is None:\n        continue\n      merged, not_merged_a, not_merged_b = stats\n      rows.append('<tr><th class=\"header\">%s</th>'\n                  '<td class=\"header\">%d</td>'\n                  '<td class=\"header\">%d</td>'\n                  '<td class=\"header\">%d</td></tr>' %\n                  (merger.DATASET_NAME, merged, not_merged_a, not_merged_b))\n    return '<table>%s</table>' % '\\n'.join(rows)", "category": "Python"}, {"instruction": "def get_environment_template(self, name, network):\n        \"\"\"Get environments by template name\n\n        :param name: Template name.\n        :param network: IPv4 or IPv6.\n\n        :return: Following dictionary:\n\n        ::\n\n            {'ambiente': [divisao_dc - ambiente_logico - grupo_l3, other envs...] }\n\n        :raise InvalidParameterError: Invalid param.\n        :raise DataBaseError: Falha na networkapi ao acessar o banco de dados.\n        :raise XMLError: Falha na networkapi ao ler o XML de requisi\u00e7\u00e3o ou gerar o XML de resposta.\n        \"\"\"\n", "input": "", "output": "        url = 'environment/get_env_template/'\n\n        map_dict = dict()\n        map_dict['name'] = name\n        map_dict['network'] = network\n\n        code, xml = self.submit({'map': map_dict}, 'PUT', url)\n\n        return self.response(code, xml)", "category": "Python"}, {"instruction": "def change_column(self, name, options):\n        \"\"\"\n        Changes column details.\n\n        :param name: The column to change.\n        :type name: str\n\n        :param options: The new options.\n        :type options: str\n\n        :rtype: Table\n        \"\"\"\n", "input": "", "output": "        column = self.get_column(name)\n        column.set_options(options)\n\n        return self", "category": "Python"}, {"instruction": "def format_error_message(exception_message, task_exception=False):\n    \"\"\"Improve the formatting of an exception thrown by a remote function.\n\n    This method takes a traceback from an exception and makes it nicer by\n    removing a few uninformative lines and adding some space to indent the\n    remaining lines nicely.\n\n    Args:\n        exception_message (str): A message generated by traceback.format_exc().\n\n    Returns:\n        A string of the formatted exception message.\n    \"\"\"\n", "input": "", "output": "    lines = exception_message.split(\"\\n\")\n    if task_exception:\n        # For errors that occur inside of tasks, remove lines 1 and 2 which are\n        # always the same, they just contain information about the worker code.\n        lines = lines[0:1] + lines[3:]\n        pass\n    return \"\\n\".join(lines)", "category": "Python"}, {"instruction": "def maxNotchSize( self, orientation ):\r\n        \"\"\"\r\n        Returns the maximum size for this ruler based on its notches and the\r\n        given orientation.\r\n        \r\n        :param      orientation | <Qt.Orientation>\r\n        \r\n        :return     <int>\r\n        \"\"\"\n", "input": "", "output": "        metrics = QFontMetrics(QApplication.font())\r\n        \r\n        if orientation == Qt.Vertical:\r\n            notch = ''\r\n            for n in self.notches():\r\n                if len(nativestring(n)) > len(nativestring(notch)):\r\n                    notch = nativestring(n)\r\n            \r\n            return metrics.width(notch)\r\n        else:\r\n            return metrics.height()", "category": "Python"}, {"instruction": "def get_field_entry_key(node):\n    # type: (Field) -> str\n    \"\"\"Implements the logic to compute the key of a given field's entry\"\"\"\n", "input": "", "output": "    if node.alias:\n        return node.alias.value\n    return node.name.value", "category": "Python"}, {"instruction": "def cli(env, sortby, cpu, columns, datacenter, name, memory, disk, tag):\n    \"\"\"List dedicated host.\"\"\"\n", "input": "", "output": "    mgr = SoftLayer.DedicatedHostManager(env.client)\n    hosts = mgr.list_instances(cpus=cpu,\n                               datacenter=datacenter,\n                               hostname=name,\n                               memory=memory,\n                               disk=disk,\n                               tags=tag,\n                               mask=columns.mask())\n\n    table = formatting.Table(columns.columns)\n    table.sortby = sortby\n\n    for host in hosts:\n        table.add_row([value or formatting.blank()\n                       for value in columns.row(host)])\n\n    env.fout(table)", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self, 'dialog_node') and self.dialog_node is not None:\n            _dict['dialog_node'] = self.dialog_node\n        if hasattr(self, 'title') and self.title is not None:\n            _dict['title'] = self.title\n        if hasattr(self, 'conditions') and self.conditions is not None:\n            _dict['conditions'] = self.conditions\n        return _dict", "category": "Python"}, {"instruction": "def backends_to_mutate(self, namespace, stream):\n    \"\"\"\n    Return all the backends enabled for writing for `stream`.\n    \"\"\"\n", "input": "", "output": "    if namespace not in self.namespaces:\n      raise NamespaceMissing('`{}` namespace is not configured'\n                             .format(namespace))\n    return self.prefix_confs[namespace][self.get_matching_prefix(namespace,\n                                                                 stream)]", "category": "Python"}, {"instruction": "def get_file_format(self):\n        \"\"\"Get the file format description. This describes the type of\n        data stored on disk.\n        \"\"\"\n", "input": "", "output": "        # Have cached file format?\n        if self._file_fmt is not None:\n            return self._file_fmt\n\n        # Make the call to retrieve it.\n        desc = AudioStreamBasicDescription()\n        size = ctypes.c_int(ctypes.sizeof(desc))\n        check(_coreaudio.ExtAudioFileGetProperty(\n            self._obj, PROP_FILE_DATA_FORMAT, ctypes.byref(size),\n            ctypes.byref(desc)\n        ))\n\n        # Cache result.\n        self._file_fmt = desc\n        return desc", "category": "Python"}, {"instruction": "def addfield(self, pkt, s, val):\n        \"\"\"\n        Reconstruct the header because the TLS type may have been updated.\n        Then, append the content.\n        \"\"\"\n", "input": "", "output": "        res = b\"\"\n        for p in val:\n            res += self.i2m(pkt, p)\n        if (isinstance(pkt, _GenericTLSSessionInheritance) and\n            _tls_version_check(pkt.tls_session.tls_version, 0x0304) and\n                not isinstance(pkt, TLS13ServerHello)):\n            return s + res\n        if not pkt.type:\n            pkt.type = 0\n        hdr = struct.pack(\"!B\", pkt.type) + s[1:5]\n        return hdr + res", "category": "Python"}, {"instruction": "def is_file_opened(self, filename=None):\r\n        \"\"\"Return if filename is in the editor stack.\r\n\r\n        Args:\r\n            filename: Name of the file to search for.  If filename is None,\r\n                then checks if any file is open.\r\n\r\n        Returns:\r\n            True: If filename is None and a file is open.\r\n            False: If filename is None and no files are open.\r\n            None: If filename is not None and the file isn't found.\r\n            integer: Index of file name in editor stack.\r\n        \"\"\"\n", "input": "", "output": "        if filename is None:\r\n            # Is there any file opened?\r\n            return len(self.data) > 0\r\n        else:\r\n            return self.has_filename(filename)", "category": "Python"}, {"instruction": "def new_result(self, job, update_model=True):\n\t\t\"\"\"\n\t\tregisters finished runs\n\n\t\tEvery time a run has finished, this function should be called\n\t\tto register it with the result logger. If overwritten, make\n\t\tsure to call this method from the base class to ensure proper\n\t\tlogging.\n\n\n\t\tParameters\n\t\t----------\n\t\tjob: instance of hpbandster.distributed.dispatcher.Job\n\t\t\tcontains all necessary information about the job\n\t\tupdate_model: boolean\n\t\t\tdetermines whether a model inside the config_generator should be updated\n\t\t\"\"\"\n", "input": "", "output": "\t\tif not job.exception is None:\n\t\t\tself.logger.warning(\"job {} failed with exception\\n{}\".format(job.id, job.exception))", "category": "Python"}, {"instruction": "def label_durations(self, label_list_ids=None):\n        \"\"\"\n        Return a dictionary containing the total duration, every label-value in this corpus is occurring.\n\n        Args:\n            label_list_ids (list): If not None, only labels from label-lists with an id contained in this list\n                                   are considered.\n\n        Returns:\n            dict: A dictionary containing the total duration with the label-value as key.\n        \"\"\"\n", "input": "", "output": "        duration = collections.defaultdict(int)\n\n        for utterance in self.utterances.values():\n            for label_value, utt_count in utterance.label_total_duration(label_list_ids=label_list_ids).items():\n                duration[label_value] += utt_count\n\n        return duration", "category": "Python"}, {"instruction": "def lazy(func):\n    \"\"\" Decorator, which can be used for lazy imports\n\n            @lazy\n            def yaml():\n                import yaml\n                return yaml \"\"\"\n", "input": "", "output": "    try:\n        frame = sys._getframe(1)\n    except Exception:\n        _locals = None\n    else:\n        _locals = frame.f_locals\n    func_name = func.func_name if six.PY2 else func.__name__\n    return LazyStub(func_name, func, _locals)", "category": "Python"}, {"instruction": "def rotateCD(self,orient):\n        \"\"\" Rotates WCS CD matrix to new orientation given by 'orient'\n        \"\"\"\n", "input": "", "output": "        # Determine where member CRVAL position falls in ref frame\n        # Find out whether this needs to be rotated to align with\n        # reference frame.\n\n        _delta = self.get_orient() - orient\n        if _delta == 0.:\n            return\n\n        # Start by building the rotation matrix...\n        _rot = fileutil.buildRotMatrix(_delta)\n        # ...then, rotate the CD matrix and update the values...\n        _cd = N.array([[self.cd11,self.cd12],[self.cd21,self.cd22]],dtype=N.float64)\n        _cdrot = N.dot(_cd,_rot)\n        self.cd11 = _cdrot[0][0]\n        self.cd12 = _cdrot[0][1]\n        self.cd21 = _cdrot[1][0]\n        self.cd22 = _cdrot[1][1]\n        self.orient = orient", "category": "Python"}, {"instruction": "def display_system(sys, style='vdw'):\n    '''Display the system *sys* with the default viewer.\n\n    '''\n", "input": "", "output": "    \n    v = QtViewer()\n\n    #v.add_post_processing(FXAAEffect)\n    v.add_post_processing(SSAOEffect)    \n    \n    if style == 'vdw':\n        sr = v.add_renderer(AtomRenderer, sys.r_array, sys.type_array,\n                            backend='impostors')\n    if style == 'ball-and-stick':\n        sr = v.add_renderer(BallAndStickRenderer,\n                            sys.r_array,\n                            sys.type_array,\n                            sys.bonds)\n    \n    if sys.box_vectors is not None:\n        v.add_renderer(BoxRenderer, sys.box_vectors)\n        \n        # We autozoom on the box\n        a, b, c = sys.box_vectors\n        box_vertices = np.array([[0.0, 0.0, 0.0],\n                                 a, b, c,\n                                 a + b, a + c, b + c,\n                                 a + b + c])\n        v.widget.camera.autozoom(box_vertices)\n    else:\n        v.widget.camera.autozoom(sys.r_array)\n    \n    v.run()", "category": "Python"}, {"instruction": "def import_module(modulename):\n    \"\"\"\n    Static method for importing module modulename. Can handle relative imports as well.\n\n    :param modulename: Name of module to import. Can be relative\n    :return: imported module instance.\n    \"\"\"\n", "input": "", "output": "    module = None\n    try:\n        module = importlib.import_module(modulename)\n    except ImportError:\n        # If importing fails we see if the modulename has dots in it, split the name.\n        if \".\" in modulename:\n            modules = modulename.split(\".\")\n            package = \".\".join(modules[1:len(modules)])\n            # Might raise an ImportError again. If so, we really failed to import the module.\n            module = importlib.import_module(package)\n        else:\n            # No dots, really unable to import the module. Raise.\n            raise\n    return module", "category": "Python"}, {"instruction": "def install_hook(dialog=SimpleExceptionDialog, invoke_old_hook=False, **extra):\n    \"\"\"\n    install the configured exception hook wrapping the old exception hook\n\n    don't use it twice\n\n    :oparam dialog: a different exception dialog class\n    :oparam invoke_old_hook: should we invoke the old exception hook?\n    \"\"\"\n", "input": "", "output": "    global _old_hook\n    assert _old_hook is None\n\n    def new_hook(etype, eval, trace):\n        gobject.idle_add(dialog_handler, dialog, etype, eval, trace, extra)\n        if invoke_old_hook:\n            _old_hook(etype, eval, trace)\n\n    _old_hook = sys.excepthook\n    sys.excepthook = new_hook", "category": "Python"}, {"instruction": "def remove_member(self, login):\n        \"\"\"Remove ``login`` from this team.\n\n        :param str login: (required), login of the member to remove\n        :returns: bool\n        \"\"\"\n", "input": "", "output": "        warnings.warn(\n            'This is no longer supported by the GitHub API, see '\n            'https://developer.github.com/changes/2014-09-23-one-more-week'\n            '-before-the-add-team-member-api-breaking-change/',\n            DeprecationWarning)\n        url = self._build_url('members', login, base_url=self._api)\n        return self._boolean(self._delete(url), 204, 404)", "category": "Python"}, {"instruction": "def get_diffs(ptrms_vectors, ptrm_checks_vectors, ptrms_orig, checks_orig):\n    \"\"\"\n    input: ptrms_vectors, ptrm_checks_vectors, ptrms_orig, checks_orig\n    output: vector diffs between original and ptrm check, C\n    \"\"\"\n", "input": "", "output": "    ptrm_temps = numpy.array(ptrms_orig)[:,0]\n    check_temps = numpy.array(checks_orig)[:,0]\n    index = numpy.zeros(len(ptrm_temps))\n    for num, temp in enumerate(ptrm_temps):\n        if len(numpy.where(check_temps == temp)[0]):\n            index[num] = numpy.where(check_temps == temp)[0][0]\n        else:\n            index[num] = float('nan')\n    diffs = numpy.zeros((len(ptrms_vectors), 3))\n    for num, ptrm in enumerate(ptrms_vectors):\n        if numpy.isnan(index[num]):\n            diffs[num] = numpy.array([0,0,0])\n        else:\n            diffs[num] = ptrm_checks_vectors[int(index[num])] - ptrm\n    C = numpy.cumsum(diffs, 0)\n    #print \"diffs (should be same as to_sum\"\n    #print diffs\n    #print \"C (should be same as dpal_sum)\"\n    #print C\n    return diffs, C", "category": "Python"}, {"instruction": "def _generate_provisional_name(q, astrom_header, fits_header):\n    \"\"\"\n    Generates a name for an object given the information in its astrom\n    observation header and FITS header.\n    :param q: a queue of provisional names to return.\n    :type q: Queue\n    :param astrom_header:\n    :param fits_header:\n    \"\"\"\n", "input": "", "output": "    while True:\n        ef = get_epoch_field(astrom_header, fits_header)\n        epoch_field = ef[0] + ef[1]\n        count = storage.increment_object_counter(storage.MEASURE3, epoch_field)\n        try:\n            q.put(ef[1] + count)\n        except:\n            break", "category": "Python"}, {"instruction": "def parse_to_gvid(v):\n    \"\"\"Parse an ACS Geoid or a GVID to a GVID\"\"\"\n", "input": "", "output": "    from geoid.civick import GVid\n    from geoid.acs import AcsGeoid\n\n    m1 = ''\n\n    try:\n        return GVid.parse(v)\n    except ValueError as e:\n        m1 = str(e)\n\n    try:\n        return AcsGeoid.parse(v).convert(GVid)\n    except ValueError as e:\n        raise ValueError(\"Failed to parse to either ACS or GVid: {}; {}\".format(m1, str(e)))", "category": "Python"}, {"instruction": "def time(self,*args,**kwargs):\n        \"\"\"\n        NAME:\n           time\n        PURPOSE:\n           return the times at which the orbit is sampled\n        INPUT:\n           t - (default: integration times) time at which to get the time (for consistency reasons); default is to return the list of times at which the orbit is sampled\n           ro= (Object-wide default) physical scale for distances to use to convert\n           vo= (Object-wide default) physical scale for velocities to use to convert\n           use_physical= use to override Object-wide default for using a physical scale for output\n        OUTPUT:\n           t(t)\n        HISTORY:\n           2014-06-11 - Written - Bovy (IAS)\n        \"\"\"\n", "input": "", "output": "        if len(args) == 0:\n            try:\n                return self.t\n            except AttributeError:\n                return 0.\n        else: return args[0]", "category": "Python"}, {"instruction": "def valid_username(user):\n    '''\n    Validates a username based on the guidelines in `useradd(8)`\n    '''\n", "input": "", "output": "    if not isinstance(user, six.string_types):\n        return False\n\n    if len(user) > 32:\n        return False\n\n    return VALID_USERNAME.match(user) is not None", "category": "Python"}, {"instruction": "def display_notes(self):\n        \"\"\"Display information about scores and raters.\n        \"\"\"\n", "input": "", "output": "        if self.annot is not None:\n            short_xml_file = short_strings(basename(self.annot.xml_file))\n            self.idx_annotations.setText(short_xml_file)\n            # if annotations were loaded without dataset\n            if self.parent.overview.scene is None:\n                self.parent.overview.update()\n\n            if not self.annot.raters:\n                self.new_rater()\n\n            self.idx_rater.setText(self.annot.current_rater)\n            self.display_eventtype()\n            self.update_annotations()\n            self.display_stats()\n            self.epoch_length = self.annot.epoch_length", "category": "Python"}, {"instruction": "def do_move(self, dt, buttons):\n        \"\"\"\n        Updates velocity and returns Rects for start/finish positions\n        \"\"\"\n", "input": "", "output": "        assert isinstance(dt, int) or isinstance(dt, float)\n        assert isinstance(buttons, dict)\n\n        newVel = self.velocity\n\n        # Redirect existing vel to new direction.\n        nv = newVel.magnitude()\n        newVel = nv * self.impulse_dir\n\n        mv = buttons['up']\n        if mv != 0:\n            self.stats['battery'] -= self.battery_use['linear']\n            newVel += dt * mv * self.accel * self.impulse_dir\n        else:\n            brake = dt * self.deaccel\n            if nv < brake:\n                newVel *= 0\n            else:\n                newVel += brake * -self.impulse_dir\n\n        nv = newVel.magnitude()\n        if nv > self.top_speed:\n            newVel *= self.top_speed / nv\n\n        return newVel", "category": "Python"}, {"instruction": "def extensions(self):\n        \"\"\"\n        Access the extensions\n\n        :returns: twilio.rest.preview.marketplace.available_add_on.available_add_on_extension.AvailableAddOnExtensionList\n        :rtype: twilio.rest.preview.marketplace.available_add_on.available_add_on_extension.AvailableAddOnExtensionList\n        \"\"\"\n", "input": "", "output": "        if self._extensions is None:\n            self._extensions = AvailableAddOnExtensionList(\n                self._version,\n                available_add_on_sid=self._solution['sid'],\n            )\n        return self._extensions", "category": "Python"}, {"instruction": "def new(cls, password, rounds):\n        \"\"\"Creates a PasswordHash from the given password.\"\"\"\n", "input": "", "output": "        if isinstance(password, str):\n            password = password.encode('utf-8')\n        return cls(cls._new(password, rounds))", "category": "Python"}, {"instruction": "def is_tuple(type_: Type[Any]) -> bool:\n    '''\n    Tuple[int, str]\n    Tuple\n    '''\n", "input": "", "output": "    if HAS_TUPLEARGS:\n        # The tuple, Tuple thing is a difference between 3.6 and 3.7\n        # In 3.6 and before, Tuple had an __extra__ field, while Tuple[something]\n        # would have the normal __origin__ field.\n        #\n        # Those apply for Dict, List, Set, Tuple\n        return _generic_type_check(type_, tuple, Tuple)\n    else:\n        # Old python\n        return _issubclass(type_, Tuple) and _issubclass(type_, tuple) == False", "category": "Python"}, {"instruction": "def _get_client(self):\n        \"\"\"\n        S3 Boto3 client\n\n        Returns:\n            boto3.session.Session.client: client\n        \"\"\"\n", "input": "", "output": "        client_kwargs = self._storage_parameters.get('client', dict())\n\n        # Handles unsecure mode\n        if self._unsecure:\n            client_kwargs = client_kwargs.copy()\n            client_kwargs['use_ssl'] = False\n\n        return self._get_session().client(\"s3\", **client_kwargs)", "category": "Python"}, {"instruction": "def relative_time(d, other=None, ndigits=0):\n    \"\"\"Get a string representation of the difference between two\n    :class:`~datetime.datetime` objects or one\n    :class:`~datetime.datetime` and the current time. Handles past and\n    future times.\n\n    Args:\n        d (datetime): The first datetime object.\n        other (datetime): An optional second datetime object. If\n            unset, defaults to the current time as determined\n            :meth:`datetime.utcnow`.\n        ndigits (int): The number of decimal digits to round to,\n            defaults to ``0``.\n    Returns:\n        A short English-language string.\n\n    >>> now = datetime.utcnow()\n    >>> relative_time(now, ndigits=1)\n    '0 seconds ago'\n    >>> relative_time(now - timedelta(days=1, seconds=36000), ndigits=1)\n    '1.4 days ago'\n    >>> relative_time(now + timedelta(days=7), now, ndigits=1)\n    '1 week from now'\n\n    \"\"\"\n", "input": "", "output": "    drt, unit = decimal_relative_time(d, other, ndigits, cardinalize=True)\n    phrase = 'ago'\n    if drt < 0:\n        phrase = 'from now'\n    return '%g %s %s' % (abs(drt), unit, phrase)", "category": "Python"}, {"instruction": "def get_locations(self):\n        \"\"\"\n        Return the zipcodes mapping as a list of ``{zipcode: location}`` dicts.\n        The zipcodes file will be downloaded if necessary.\n        \"\"\"\n", "input": "", "output": "        if not self.zipcode_mapping:\n            self.download(overwrite=False)\n\n            zipcode_mapping = {}\n            with UnicodeReader(self.file_path, delimiter=';', encoding='latin1') as csv_reader:\n                # Skip header\n                next(csv_reader)\n                for line in csv_reader:\n                    zipcode_mapping[int(line[1])] = Location(\n                        official_name=line[0],\n                        canton=line[5],\n                        municipality=line[3]\n                    )\n            self.zipcode_mapping = zipcode_mapping\n\n        return self.zipcode_mapping", "category": "Python"}, {"instruction": "def string_result(result, func, arguments):\n    \"\"\"Errcheck function. Returns a string and frees the original pointer.\n\n    It assumes the result is a char *.\n    \"\"\"\n", "input": "", "output": "    if result:\n        # make a python string copy\n        s = bytes_to_str(ctypes.string_at(result))\n        # free original string ptr\n        libvlc_free(result)\n        return s\n    return None", "category": "Python"}, {"instruction": "def create_account(self, **kwargs):\n        \"\"\"\n        Create a new root account.\n\n        :calls: `POST /api/v1/accounts \\\n        <https://canvas.instructure.com/doc/api/accounts.html#method.accounts.create>`_\n\n        :rtype: :class:`canvasapi.account.Account`\n        \"\"\"\n", "input": "", "output": "        response = self.__requester.request(\n            'POST',\n            'accounts',\n            _kwargs=combine_kwargs(**kwargs)\n        )\n        return Account(self.__requester, response.json())", "category": "Python"}, {"instruction": "def boolValue(self):\n        \"\"\"\n        returns : (boolean) Value\n        \"\"\"\n", "input": "", "output": "        if self.lastValue == 1 or self.lastValue == \"active\":\n            self._key = 1\n            self._boolKey = True\n        else:\n            self._key = 0\n            self._boolKey = False\n        return self._boolKey", "category": "Python"}, {"instruction": "def connect(self):\n        \"\"\"\n        make amqp connection and create channels and queue binding\n        \"\"\"\n", "input": "", "output": "        self.connection = pika.BlockingConnection(BLOCKING_MQ_PARAMS)\n        self.client_queue = ClientQueue()\n        self.input_channel = self.connection.channel()\n\n        self.input_channel.exchange_declare(exchange=self.INPUT_EXCHANGE,\n                                            type='topic',\n                                            durable=True)\n        self.input_channel.queue_declare(queue=self.INPUT_QUEUE_NAME)\n        self.input_channel.queue_bind(exchange=self.INPUT_EXCHANGE, queue=self.INPUT_QUEUE_NAME)\n        log.info(\"Bind to queue named '%s' queue with exchange '%s'\" % (self.INPUT_QUEUE_NAME,\n                                                                        self.INPUT_EXCHANGE))", "category": "Python"}, {"instruction": "def load_yaml(file):\n    \"\"\"If pyyaml > 5.1 use full_load to avoid warning\"\"\"\n", "input": "", "output": "    if hasattr(yaml, \"full_load\"):\n        return yaml.full_load(file)\n    else:\n        return yaml.load(file)", "category": "Python"}, {"instruction": "def import_json(cls, filename):\n        \"\"\"\n        Import graph from the given file.  The file is expected\n        to contain UTF-8 encoded JSON data.\n\n        \"\"\"\n", "input": "", "output": "        with open(filename, 'rb') as f:\n            json_graph = f.read().decode('utf-8')\n        return cls.from_json(json_graph)", "category": "Python"}, {"instruction": "def recoverFile(filename):\n    \"\"\"parse an XML file and build a tree. Automatic support for\n      ZLIB/Compress compressed document is provided by default if\n      found at compile-time. In the case the document is not Well\n       Formed, it attempts to build a tree anyway \"\"\"\n", "input": "", "output": "    ret = libxml2mod.xmlRecoverFile(filename)\n    if ret is None:raise treeError('xmlRecoverFile() failed')\n    return xmlDoc(_obj=ret)", "category": "Python"}, {"instruction": "def _validate_params(self):\n        \"\"\"\n        method to sanitize model parameters\n\n        Parameters\n        ---------\n        None\n\n        Returns\n        -------\n        None\n        \"\"\"\n", "input": "", "output": "        self.distribution = NormalDist(scale=self.scale)\n        super(LinearGAM, self)._validate_params()", "category": "Python"}, {"instruction": "def send(self, sock, msg):\n    \"\"\"Send ``msg`` to destination ``sock``.\"\"\"\n", "input": "", "output": "    data = pickle.dumps(msg)\n    buf = struct.pack('>I', len(data)) + data\n    sock.sendall(buf)", "category": "Python"}, {"instruction": "def add_message(request, level, message, extra_tags='', fail_silently=False):\n    \"\"\"Attempts to add a message to the request using the 'messages' app.\"\"\"\n", "input": "", "output": "    if not horizon_message_already_queued(request, message):\n        if request.is_ajax():\n            tag = constants.DEFAULT_TAGS[level]\n            # if message is marked as safe, pass \"safe\" tag as extra_tags so\n            # that client can skip HTML escape for the message when rendering\n            if isinstance(message, SafeData):\n                extra_tags = extra_tags + ' safe'\n            request.horizon['async_messages'].append([tag,\n                                                      force_text(message),\n                                                      extra_tags])\n        else:\n            return _messages.add_message(request, level, message,\n                                         extra_tags, fail_silently)", "category": "Python"}, {"instruction": "def get_version(*file_paths):\n    \"\"\"Retrieves the version from path\"\"\"\n", "input": "", "output": "    filename = os.path.join(os.path.dirname(__file__), *file_paths)\n    print(\"Looking for version in: {}\".format(filename))\n    version_file = open(filename).read()\n    version_match = re.search(r\"^__version__ = ['\\\"]([^'\\\"]*)['\\\"]\", version_file, re.M)\n    if version_match:\n        return version_match.group(1)\n    raise RuntimeError('Unable to find version string.')", "category": "Python"}, {"instruction": "def task2ics():\n    \"\"\"Command line tool to convert from Taskwarrior to iCalendar\"\"\"\n", "input": "", "output": "    from argparse import ArgumentParser, FileType\n    from sys import stdout\n\n    parser = ArgumentParser(description='Converter from Taskwarrior to iCalendar syntax.')\n    parser.add_argument('indir', nargs='?', help='Input Taskwarrior directory (default to ~/.task)', default=expanduser('~/.task'))\n    parser.add_argument('outfile', nargs='?', type=FileType('w'), default=stdout,\n                        help='Output iCalendar file (default: stdout)')\n    args = parser.parse_args()\n\n    task = IcsTask(args.indir)\n    args.outfile.write(task.to_vobject().serialize())", "category": "Python"}, {"instruction": "def abstract(class_):\n    \"\"\"Mark the class as _abstract_ base class, forbidding its instantiation.\n\n    .. note::\n\n        Unlike other modifiers, ``@abstract`` can be applied\n        to all Python classes, not just subclasses of :class:`Object`.\n\n    .. versionadded:: 0.0.3\n    \"\"\"\n", "input": "", "output": "    if not inspect.isclass(class_):\n        raise TypeError(\"@abstract can only be applied to classes\")\n\n    abc_meta = None\n\n    # if the class is not already using a metaclass specific to ABC,\n    # we need to change that\n    class_meta = type(class_)\n    if class_meta not in (_ABCMetaclass, _ABCObjectMetaclass):\n        # decide what metaclass to use, depending on whether it's a subclass\n        # of our universal :class:`Object` or not\n        if class_meta is type:\n            abc_meta = _ABCMetaclass  # like ABCMeta, but can never instantiate\n        elif class_meta is ObjectMetaclass:\n            abc_meta = _ABCObjectMetaclass  # ABCMeta mixed with ObjectMetaclass\n        else:\n            raise ValueError(\n                \"@abstract cannot be applied to classes with custom metaclass\")\n\n    class_.__abstract__ = True\n    return metaclass(abc_meta)(class_) if abc_meta else class_", "category": "Python"}, {"instruction": "def get_gradebook_column_admin_session(self):\n        \"\"\"Gets the ``OsidSession`` associated with the gradebook column administration service.\n\n        return: (osid.grading.GradebookColumnAdminSession) - a\n                ``GradebookColumnAdminSession``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented - ``supports_gradebook_column_admin()`` is\n                ``false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_gradebook_column_admin()`` is ``true``.*\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_gradebook_column_admin():\n            raise errors.Unimplemented()\n        # pylint: disable=no-member\n        return sessions.GradebookColumnAdminSession(runtime=self._runtime)", "category": "Python"}, {"instruction": "def build_response(description, resource=None):\n    \"\"\"\n    Build a response definition.\n\n    \"\"\"\n", "input": "", "output": "    response = swagger.Response(\n        description=description,\n    )\n    if resource is not None:\n        response.schema = swagger.JsonReference({\n            \"$ref\": \"#/definitions/{}\".format(type_name(name_for(resource))),\n        })\n    return response", "category": "Python"}, {"instruction": "def lcsr(s1, s2):\n    '''longest common sequence ratio\n\n    >>> lcsr('ab', 'abcd')\n    0.5\n    '''\n", "input": "", "output": "    if s1 == s2:\n        return 1.0\n    return llcs(s1, s2) / max(1, len(s1), len(s2))", "category": "Python"}, {"instruction": "def list_ports(self, retrieve_all=True, **_params):\n        \"\"\"Fetches a list of all ports for a project.\"\"\"\n", "input": "", "output": "        # Pass filters in \"params\" argument to do_request\n        return self.list('ports', self.ports_path, retrieve_all,\n                         **_params)", "category": "Python"}, {"instruction": "def numericallySortFilenames(names):\n    \"\"\"\n    Sort (ascending) a list of file names by their numerical prefixes.\n    The number sorted on is the numeric prefix of the basename of\n    the given filename. E.g., '../output/1.json.bz2' will sort before\n    '../output/10.json.bz2'.\n\n    @param: A C{list} of file names, each of whose basename starts with a\n        string of digits.\n    @return: The sorted C{list} of full file names.\n    \"\"\"\n", "input": "", "output": "\n    def numericPrefix(name):\n        ", "category": "Python"}, {"instruction": "def shutdown_request(self, request):\n        \"\"\"\n        Called to shutdown and close an individual request.\n        \"\"\"\n", "input": "", "output": "\n        try:\n            request.shutdown(socket.SHUT_WR)\n        except socket.error:\n            pass\n\n        self.close_request(request)", "category": "Python"}, {"instruction": "def xml_marshal_delete_objects(object_names):\n    \"\"\"\n    Marshal Multi-Object Delete request body from object names.\n\n    :param object_names: List of object keys to be deleted.\n    :return: Serialized XML string for multi-object delete request body.\n    \"\"\"\n", "input": "", "output": "    root = s3_xml.Element('Delete')\n\n    # use quiet mode in the request - this causes the S3 Server to\n    # limit its response to only object keys that had errors during\n    # the delete operation.\n    quiet = s3_xml.SubElement(root, 'Quiet')\n    quiet.text = \"true\"\n\n    # add each object to the request.\n    for object_name in object_names:\n        object_elt = s3_xml.SubElement(root, 'Object')\n        key_elt = s3_xml.SubElement(object_elt, 'Key')\n        key_elt.text = object_name\n\n    # return the marshalled xml.\n    data = io.BytesIO()\n    s3_xml.ElementTree(root).write(data, encoding=None, xml_declaration=False)\n    return data.getvalue()", "category": "Python"}, {"instruction": "def random_crop(src, size):\n    \"\"\"Randomly crop src with size. Upsample result if src is smaller than size\"\"\"\n", "input": "", "output": "    h, w, _ = src.shape\n    new_w, new_h = scale_down((w, h), size)\n\n    x0 = random.randint(0, w - new_w)\n    y0 = random.randint(0, h - new_h)\n\n    out = fixed_crop(src, x0, y0, new_w, new_h, size)\n    return out, (x0, y0, new_w, new_h)", "category": "Python"}, {"instruction": "def dumps_tabledata(value, format_name=\"rst_grid_table\", **kwargs):\n    \"\"\"\n    :param tabledata.TableData value: Tabular data to dump.\n    :param str format_name:\n        Dumped format name of tabular data.\n        Available formats are described in\n        :py:meth:`~pytablewriter.TableWriterFactory.create_from_format_name`\n\n    :Example:\n        .. code:: python\n\n            >>> dumps_tabledata(value)\n            .. table:: sample_data\n\n                ======  ======  ======\n                attr_a  attr_b  attr_c\n                ======  ======  ======\n                     1     4.0  a\n                     2     2.1  bb\n                     3   120.9  ccc\n                ======  ======  ======\n    \"\"\"\n", "input": "", "output": "\n    from ._factory import TableWriterFactory\n\n    if not value:\n        raise TypeError(\"value must be a tabledata.TableData instance\")\n\n    writer = TableWriterFactory.create_from_format_name(format_name)\n\n    for attr_name, attr_value in kwargs.items():\n        setattr(writer, attr_name, attr_value)\n\n    writer.from_tabledata(value)\n\n    return writer.dumps()", "category": "Python"}, {"instruction": "def register_for_app(\n        self, app_label=None, exclude_models=None, exclude_model_classes=None\n    ):\n        \"\"\"Registers all models for this app_label.\n        \"\"\"\n", "input": "", "output": "        models = []\n        exclude_models = exclude_models or []\n        app_config = django_apps.get_app_config(app_label)\n        for model in app_config.get_models():\n            if model._meta.label_lower in exclude_models:\n                pass\n            elif exclude_model_classes and issubclass(model, exclude_model_classes):\n                pass\n            else:\n                models.append(model._meta.label_lower)\n        self.register(models)", "category": "Python"}, {"instruction": "def _GeneratorFromPath(path):\n  \"\"\"Create an event generator for file or directory at given path string.\"\"\"\n", "input": "", "output": "  if not path:\n    raise ValueError('path must be a valid string')\n  if io_wrapper.IsTensorFlowEventsFile(path):\n    return event_file_loader.EventFileLoader(path)\n  else:\n    return directory_watcher.DirectoryWatcher(\n        path,\n        event_file_loader.EventFileLoader,\n        io_wrapper.IsTensorFlowEventsFile)", "category": "Python"}, {"instruction": "def analysis_set_properties(object_id, input_params={}, always_retry=True, **kwargs):\n    \"\"\"\n    Invokes the /analysis-xxxx/setProperties API method.\n\n    For more info, see: https://wiki.dnanexus.com/API-Specification-v1.0.0/Workflows-and-Analyses#API-method%3A-%2Fanalysis-xxxx%2FsetProperties\n    \"\"\"\n", "input": "", "output": "    return DXHTTPRequest('/%s/setProperties' % object_id, input_params, always_retry=always_retry, **kwargs)", "category": "Python"}, {"instruction": "def simxGetVisionSensorDepthBuffer(clientID, sensorHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n", "input": "", "output": "    c_buffer  = ct.POINTER(ct.c_float)()\n    resolution = (ct.c_int*2)()\n    ret = c_GetVisionSensorDepthBuffer(clientID, sensorHandle, resolution, ct.byref(c_buffer), operationMode)\n    reso = []\n    buffer = []\n    if (ret == 0):\n        buffer = [None]*resolution[0]*resolution[1]\n        for i in range(resolution[0] * resolution[1]):\n            buffer[i] = c_buffer[i]\n        for i in range(2):\n            reso.append(resolution[i])\n    return ret, reso, buffer", "category": "Python"}, {"instruction": "def foreground(color):\n\t\"\"\"Set the foreground color.\"\"\"\n", "input": "", "output": "\tif color not in foreground_colors:\n\t\treturn\n\n\tif is_win32:\n\t\tlast_fg = foreground_colors[color][1]\n\t\tset_color_win32(last_fg | last_bg)\n\telse:\n\t\tset_color_ansi(foreground_colors[color][0])", "category": "Python"}, {"instruction": "def leaves(self, cls, level=0, intermediate=False):\n        \"\"\"\n        Returns an iterator of the HubComponent leaves that are of class `cls`.\n\n        If `intermediate` is True, then return any intermediate classes as\n        well.\n        \"\"\"\n", "input": "", "output": "        if intermediate:\n            if isinstance(self, cls):\n                yield self, level\n        elif len(self.children) == 0:\n            if isinstance(self, cls):\n                yield self, level\n            else:\n                raise StopIteration\n\n        for child in self.children:\n            for leaf, _level in child.leaves(cls, level + 1, intermediate=intermediate):\n                    yield leaf, _level", "category": "Python"}, {"instruction": "def writeFile(filename, content, encoding=None):\n    \"\"\"Write content to given filename. Checks for zero-sized files.\n    If encoding is given writes to a codec.open() file.\"\"\"\n", "input": "", "output": "    if not content:\n        raise OSError(\"empty content for file %s\" % filename)\n\n    def getfp(filename, encoding):\n        ", "category": "Python"}, {"instruction": "def do_transaction(self, function, *args, **kwargs):\n        \"\"\"execute a function within the context of a transaction\"\"\"\n", "input": "", "output": "        with self.config.db_transaction() as trans:\n            function(trans, *args, **kwargs)", "category": "Python"}, {"instruction": "def columns(self, model=None):\n        \"\"\"\n        Returns a generator that loops through the columns that are associated with this query.\n        \n        :return     <generator>(orb.Column)\n        \"\"\"\n", "input": "", "output": "        column = self.column(model=model)\n        if column:\n            yield column\n\n        check = self.__value\n        if not isinstance(check, (list, set, tuple)):\n            check = (check,)\n\n        for val in check:\n            if isinstance(val, (Query, QueryCompound)):\n                for col in val.columns(model):\n                    yield col", "category": "Python"}, {"instruction": "def GetCollectionNode(self, partition_key):\n        \"\"\"Gets the SelfLink/ID based link of the collection node that maps to the partition key\n        based on the hashing algorithm used for finding the node in the ring.\n\n        :param str partition_key:\n            The partition key to be used for finding the node in the ring.\n\n        :return:\n            The name of the collection mapped to that partition.\n        :rtype: str\n\n        \"\"\"\n", "input": "", "output": "        if partition_key is None:\n            raise ValueError(\"partition_key is None or empty.\")\n\n        partition_number = self._FindPartition(self._GetBytes(partition_key))\n        return self.partitions[partition_number].GetNode()", "category": "Python"}, {"instruction": "def WriteEventBody(self, event):\n    \"\"\"Writes the body of an event to the output.\n\n    Args:\n      event (EventObject): event.\n    \"\"\"\n", "input": "", "output": "    output_values = []\n    for field_name in self._fields:\n      output_value = self._dynamic_fields_helper.GetFormattedField(\n          event, field_name)\n\n      output_value = self._SanitizeField(output_value)\n      output_values.append(output_value)\n\n    output_line = '{0:s}\\n'.format(self._field_delimiter.join(output_values))\n    self._output_writer.Write(output_line)", "category": "Python"}, {"instruction": "def _phi2deriv(self,R,z,phi=0.,t=0.): #pragma: no cover\n        \"\"\"\n        NAME:\n           _phi2deriv\n        PURPOSE:\n           evaluate the second azimuthal derivative for this potential\n        INPUT:\n           R - Galactocentric cylindrical radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           the second azimuthal derivative\n        HISTORY:\n           2016-12-26 - Written - Bovy (UofT/CCA)\n        \"\"\"\n", "input": "", "output": "        raise AttributeError\n        # Implementation above does not work bc SCF.phi2deriv is not implemented\n        return self._scf.phi2deriv(R,z,phi=phi,use_physical=False)", "category": "Python"}, {"instruction": "def make_json_formatter(graph):\n    \"\"\"\n    Create the default json formatter.\n\n    \"\"\"\n", "input": "", "output": "\n    return {\n        \"()\": graph.config.logging.json_formatter.formatter,\n        \"fmt\": graph.config.logging.json_required_keys,\n    }", "category": "Python"}, {"instruction": "def _rank(self, ranking, n):\n    \"\"\" return the first n sentences with highest ranking \"\"\"\n", "input": "", "output": "    return nlargest(n, ranking, key=ranking.get)", "category": "Python"}, {"instruction": "def set_gl_state(self, preset=None, **kwargs):\n        \"\"\"Define the set of GL state parameters to use when drawing\n\n        Parameters\n        ----------\n        preset : str\n            Preset to use.\n        **kwargs : dict\n            Keyword arguments to `gloo.set_state`.\n        \"\"\"\n", "input": "", "output": "        for v in self._subvisuals:\n            v.set_gl_state(preset=preset, **kwargs)", "category": "Python"}, {"instruction": "def setup_ics(graph):\n    \"\"\"Make a list of internal coordinates based on the graph\n\n       Argument:\n        | ``graph`` -- A Graph instance.\n\n       The list of internal coordinates will include all bond lengths, all\n       bending angles, and all dihedral angles.\n    \"\"\"\n", "input": "", "output": "    ics = []\n    # A) Collect all bonds.\n    for i0, i1 in graph.edges:\n        ics.append(BondLength(i0, i1))\n    # B) Collect all bends. (see b_bending_angles.py for the explanation)\n    for i1 in range(graph.num_vertices):\n        n = list(graph.neighbors[i1])\n        for index, i0 in enumerate(n):\n            for i2 in n[:index]:\n                ics.append(BendingAngle(i0, i1, i2))\n    # C) Collect all dihedrals.\n    for i1, i2 in graph.edges:\n        for i0 in graph.neighbors[i1]:\n            if i0==i2:\n                # All four indexes must be different.\n                continue\n            for i3 in graph.neighbors[i2]:\n                if i3==i1 or i3==i0:\n                    # All four indexes must be different.\n                    continue\n                ics.append(DihedralAngle(i0, i1, i2, i3))\n    return ics", "category": "Python"}, {"instruction": "def _login(self, csrf_token):\n        \"\"\"Attempt to login session on easyname.\"\"\"\n", "input": "", "output": "        login_response = self.session.post(\n            self.URLS['login'],\n            data={\n                'username': self._get_provider_option('auth_username') or '',\n                'password': self._get_provider_option('auth_password') or '',\n                'submit': '',\n                'loginxtoken': csrf_token,\n            }\n        )\n        self._log('Login', login_response)\n        assert login_response.status_code == 200, \\\n            'Could not login due to a network error.'\n        assert login_response.url == self.URLS['overview'], \\\n            'Easyname login failed, bad EASYNAME_USER or EASYNAME_PASS.'", "category": "Python"}, {"instruction": "def get_stp_brief_info_output_spanning_tree_info_spanning_tree_mode_stp_stp_port_interface_type(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_stp_brief_info = ET.Element(\"get_stp_brief_info\")\n        config = get_stp_brief_info\n        output = ET.SubElement(get_stp_brief_info, \"output\")\n        spanning_tree_info = ET.SubElement(output, \"spanning-tree-info\")\n        spanning_tree_mode = ET.SubElement(spanning_tree_info, \"spanning-tree-mode\")\n        stp = ET.SubElement(spanning_tree_mode, \"stp\")\n        stp = ET.SubElement(stp, \"stp\")\n        port = ET.SubElement(stp, \"port\")\n        interface_type = ET.SubElement(port, \"interface-type\")\n        interface_type.text = kwargs.pop('interface_type')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def str2actfunc(act_func):\n    \"\"\"Convert activation function name to tf function.\"\"\"\n", "input": "", "output": "    if act_func == 'sigmoid':\n        return tf.nn.sigmoid\n\n    elif act_func == 'tanh':\n        return tf.nn.tanh\n\n    elif act_func == 'relu':\n        return tf.nn.relu", "category": "Python"}, {"instruction": "def at(self, year, month, day):\n        \"\"\" time entries by year, month and day. \"\"\"\n", "input": "", "output": "        path = partial(_path, self.adapter)\n        path = partial(path, int(year))\n        path = partial(path, int(month))\n        path = path(int(day))\n        return self._get(path)", "category": "Python"}, {"instruction": "def train(self, data, **kwargs):\n        \"\"\"\n        Calculate the standard deviations and means in the training data\n        \"\"\"\n", "input": "", "output": "        self.data = data\n        for i in xrange(0,data.shape[1]):\n            column_mean = np.mean(data.icol(i))\n            column_stdev = np.std(data.icol(i))\n\n            #Have to do += or \"list\" type will fail (ie with append)\n            self.column_means += [column_mean]\n            self.column_stdevs += [column_stdev]\n\n        self.data = self.predict(data)", "category": "Python"}, {"instruction": "def remove_envelop(self, begin, end):\n        \"\"\"\n        Removes all intervals completely enveloped in the given range.\n\n        Completes in O((r+m)*log n) time, where:\n          * n = size of the tree\n          * m = number of matches\n          * r = size of the search range\n        \"\"\"\n", "input": "", "output": "        hitlist = self.envelop(begin, end)\n        for iv in hitlist:\n            self.remove(iv)", "category": "Python"}, {"instruction": "def fletcher_checksum(data, offset):\n    \"\"\"\n    Fletcher Checksum -- Refer to RFC1008\n\n    calling with offset == _FLETCHER_CHECKSUM_VALIDATE will validate the\n    checksum without modifying the buffer; a valid checksum returns 0.\n    \"\"\"\n", "input": "", "output": "    c0 = 0\n    c1 = 0\n    pos = 0\n    length = len(data)\n    data = bytearray(data)\n    data[offset:offset + 2] = [0] * 2\n\n    while pos < length:\n        tlen = min(length - pos, _MODX)\n        for d in data[pos:pos + tlen]:\n            c0 += d\n            c1 += c0\n        c0 %= 255\n        c1 %= 255\n        pos += tlen\n\n    x = ((length - offset - 1) * c0 - c1) % 255\n    if x <= 0:\n        x += 255\n    y = 510 - c0 - x\n    if y > 255:\n        y -= 255\n\n    data[offset] = x\n    data[offset + 1] = y\n    return (x << 8) | (y & 0xff)", "category": "Python"}, {"instruction": "def find_detrend_keyword(header, type):\n    \"\"\"Search through header and find\n    the elixir formated string(s) that match the the\n    input 'type'.\n\n    header is a FITS HDU.\n    Elixir formated strings are crunid.type.filter/exptime.chipid.version.\n    \"\"\"\n", "input": "", "output": "    import re, string\n    value='NULL'\n    #print type\n    for h in header:\n        g = str(h)\n        if ( string.find(g,'.'+type+'.')!= -1 ):\n            result=re.search('[^\\s]*\\.'+type+'\\.[^\\s]*\\.\\d\\d\\.\\d\\d',g)\n            if result:\n                return result.group(0)", "category": "Python"}, {"instruction": "def range_to_numeric(ranges):\n    \"\"\"Converts a sequence of string ranges to a sequence of floats.\n\n    E.g.::\n\n        >>> range_to_numeric(['1 uV', '2 mV', '1 V'])\n        [1E-6, 0.002, 1.0]\n\n    \"\"\"\n", "input": "", "output": "    values, units = zip(*(r.split() for r in ranges))\n    # Detect common unit.\n    unit = os.path.commonprefix([u[::-1] for u in units])\n\n    # Strip unit to get just the SI prefix.\n    prefixes = (u[:-len(unit)] for u in units)\n\n    # Convert string value and scale with prefix.\n    values = [float(v) * SI_PREFIX[p] for v, p in zip(values, prefixes)]\n    return values", "category": "Python"}, {"instruction": "def download_deviation(self, deviationid):\n\n        \"\"\"Get the original file download (if allowed)\n\n        :param deviationid: The deviationid you want download info for\n        \"\"\"\n", "input": "", "output": "\n        response = self._req('/deviation/download/{}'.format(deviationid))\n\n        return {\n            'src' : response['src'],\n            'width' : response['width'],\n            'height' : response['height'],\n            'filesize' : response['filesize']\n        }", "category": "Python"}, {"instruction": "def _norm_slices(self, fsls):\n        \"\"\"Convert slices to a normalized tuple of int/slice/farray.\"\"\"\n", "input": "", "output": "        # Normalize indices, and fill empty slice entries\n        nsls = list()\n        for i, fsl in enumerate(fsls):\n            fsl_type = type(fsl)\n            if fsl_type is int:\n                nsls.append(_norm_index(i, fsl, *self.shape[i]))\n            elif fsl_type is slice:\n                nsls.append(_norm_slice(fsl, *self.shape[i]))\n            # farray\n            else:\n                nsls.append(fsl)\n\n        return nsls", "category": "Python"}, {"instruction": "def vfr_hud_send(self, airspeed, groundspeed, heading, throttle, alt, climb, force_mavlink1=False):\n                '''\n                Metrics typically displayed on a HUD for fixed wing aircraft\n\n                airspeed                  : Current airspeed in m/s (float)\n                groundspeed               : Current ground speed in m/s (float)\n                heading                   : Current heading in degrees, in compass units (0..360, 0=north) (int16_t)\n                throttle                  : Current throttle setting in integer percent, 0 to 100 (uint16_t)\n                alt                       : Current altitude (MSL), in meters (float)\n                climb                     : Current climb rate in meters/second (float)\n\n                '''\n", "input": "", "output": "                return self.send(self.vfr_hud_encode(airspeed, groundspeed, heading, throttle, alt, climb), force_mavlink1=force_mavlink1)", "category": "Python"}, {"instruction": "def evaluate(self,scope,local_vars,block=None):\n\t\t''' Execute the compiled template and return the result string. Template\n\t\t\tevaluation is guaranteed to be performed in the scope object with the\n\t\t\tlocals specified and with support for yielding to the block.\n\t\t\t   \n\t\t\t   This method is only used by source generating templates. Subclasses that\n\t\t\toverride render() may not support all features.\n\t\t'''\n", "input": "", "output": "\t\tmethod = self.compiled_method(local_vars.keys())\n\t\tsetattr(scope\t,'compiled',method)\n\t\t\n\t\treturn scope.compiled(local_vars,block=block)", "category": "Python"}, {"instruction": "def on_right_align_toggled(self, chk):\n        \"\"\"set the horizontal alignment setting.\n        \"\"\"\n", "input": "", "output": "        v = chk.get_active()\n        self.settings.general.set_int('window-halignment', 1 if v else 0)", "category": "Python"}, {"instruction": "def get_laboratory_formatted_email(self):\n        \"\"\"Returns the laboratory email formatted\n        \"\"\"\n", "input": "", "output": "        lab = api.get_bika_setup().laboratory\n        return self.get_formatted_email((lab.getName(), lab.getEmailAddress()))", "category": "Python"}, {"instruction": "def _cdf(self, xloc, left, right, cache):\n        \"\"\"\n        Cumulative distribution function.\n\n        Example:\n            >>> print(chaospy.Uniform().fwd([-0.5, 0.5, 1.5, 2.5]))\n            [0.  0.5 1.  1. ]\n            >>> print(chaospy.Add(chaospy.Uniform(), 1).fwd([-0.5, 0.5, 1.5, 2.5]))\n            [0.  0.  0.5 1. ]\n            >>> print(chaospy.Add(1, chaospy.Uniform()).fwd([-0.5, 0.5, 1.5, 2.5]))\n            [0.  0.  0.5 1. ]\n            >>> print(chaospy.Add(1, 1).fwd([-0.5, 0.5, 1.5, 2.5]))\n            [0. 0. 0. 1.]\n        \"\"\"\n", "input": "", "output": "        left = evaluation.get_forward_cache(left, cache)\n        right = evaluation.get_forward_cache(right, cache)\n\n        if isinstance(left, Dist):\n            if isinstance(right, Dist):\n                raise evaluation.DependencyError(\n                    \"under-defined distribution {} or {}\".format(left, right))\n        elif not isinstance(right, Dist):\n            return numpy.asfarray(left+right <= xloc)\n        else:\n            left, right = right, left\n        xloc = (xloc.T-numpy.asfarray(right).T).T\n        output = evaluation.evaluate_forward(left, xloc, cache=cache)\n        assert output.shape == xloc.shape\n        return output", "category": "Python"}, {"instruction": "def get_real(_bytearray, byte_index):\n    \"\"\"\n    Get real value. create float from 4 bytes\n    \"\"\"\n", "input": "", "output": "    x = _bytearray[byte_index:byte_index + 4]\n    real = struct.unpack('>f', struct.pack('4B', *x))[0]\n    return real", "category": "Python"}, {"instruction": "def summary_stats(self, id): # pylint: disable=invalid-name,redefined-builtin\n        \"\"\"Compute summary stats for a result.\n\n        :param id: Result ID as an int.\n        :return: :class:`results.SummaryStats <results.SummaryStats>` object\n        :rtype: results.SummaryStats\n        \"\"\"\n", "input": "", "output": "        schema = SummaryStatsSchema()\n        resp = self.service.get(self.base+str(id)+'/', params={'stats': 'summary'})\n        return self.service.decode(schema, resp)", "category": "Python"}, {"instruction": "def _matches(o, pattern):\n    \"\"\"Match a pattern of types in a sequence.\"\"\"\n", "input": "", "output": "    if not len(o) == len(pattern):\n        return False\n    comps = zip(o,pattern)\n    return all(isinstance(obj,kind) for obj,kind in comps)", "category": "Python"}, {"instruction": "def parse_timespan_value(s):\n    \"\"\"Parse a string that contains a time span, optionally with a unit like s.\n    @return the number of seconds encoded by the string\n    \"\"\"\n", "input": "", "output": "    number, unit = split_number_and_unit(s)\n    if not unit or unit == \"s\":\n        return number\n    elif unit == \"min\":\n        return number * 60\n    elif unit == \"h\":\n        return number * 60 * 60\n    elif unit == \"d\":\n        return number * 24 * 60 * 60\n    else:\n        raise ValueError('unknown unit: {} (allowed are s, min, h, and d)'.format(unit))", "category": "Python"}, {"instruction": "def create_parent_folder(filename):\n    \"\"\" Create parent folder for input filename recursively\n\n    :param filename: input filename\n    :type filename: str\n    :raises: error if folder cannot be created\n    \"\"\"\n", "input": "", "output": "    path = os.path.dirname(filename)\n    if path != '':\n        make_folder(path)", "category": "Python"}, {"instruction": "def percent_point(self, U):\n        \"\"\"Given a cdf value, returns a value in original space.\n\n        Args:\n            U: `int` or `float` cdf value in [0,1]\n\n        Returns:\n            float: value in original space\n        \"\"\"\n", "input": "", "output": "        self.check_fit()\n\n        if not 0 < U < 1:\n            raise ValueError('cdf value must be in [0,1]')\n\n        return scipy.optimize.brentq(self.cumulative_distribution, -1000.0, 1000.0, args=(U))", "category": "Python"}, {"instruction": "def unsubscribe(self, topics):\n        \"\"\"\n            Unsubscribe from some topics.\n\n            Send a MQTT `UNSUBSCRIBE <http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718072>`_ message and wait for broker `UNSUBACK <http://docs.oasis-open.org/mqtt/mqtt/v3.1.1/os/mqtt-v3.1.1-os.html#_Toc398718077>`_ message.\n\n            This method is a *coroutine*.\n\n            :param topics: array of topics to unsubscribe from.\n\n            Example of ``topics`` argument expected structure:\n            ::\n\n                ['$SYS/broker/uptime', '$SYS/broker/load/#']\n        \"\"\"\n", "input": "", "output": "        yield from self._handler.mqtt_unsubscribe(topics, self.session.next_packet_id)", "category": "Python"}, {"instruction": "def from_filename(self, filename):\n        '''\n        Convert an absolute filename into key.\n        '''\n", "input": "", "output": "        i = len(self.base_directory)\n        if filename[:i] != self.base_directory:\n            raise ValueError('Filename needs to start with \"%s\";\\nyou passed \"%s\".' % (self.base_directory, filename))\n\n        if filename.endswith(self.extension):\n            if len(self.extension) > 0:\n                j = -len(self.extension)\n            else:\n                j = None\n            return self.key_transformer.from_path(tuple(filename[i:j].strip('/').split('/')))", "category": "Python"}, {"instruction": "def pmg_serialize(method):\n    \"\"\"\n    Decorator for methods that add MSON serializations keys\n    to the dictionary. See documentation of MSON for more details\n    \"\"\"\n", "input": "", "output": "\n    @functools.wraps(method)\n    def wrapper(*args, **kwargs):\n        self = args[0]\n        d = method(*args, **kwargs)\n        # Add @module and @class\n        d[\"@module\"] = self.__class__.__module__\n        d[\"@class\"] = self.__class__.__name__\n        return d\n\n    return wrapper", "category": "Python"}, {"instruction": "def add_user_to_group(self, username, groupname, raise_on_error=False):\n        \"\"\"Add a user to a group\n        :param username: The username to assign to the group\n        :param groupname: The group name into which to assign the user\n        :return: True on success, False on failure.\n        \"\"\"\n", "input": "", "output": "        data = {\n                'name': groupname,\n        }\n        response = self._post(self.rest_url + \"/user/group/direct\",\n                              params={\"username\": username,},\n                              data=json.dumps(data))\n\n        if response.status_code == 201:\n            return True\n\n        if raise_on_error:\n            raise RuntimeError(response.json()['message'])\n\n        return False", "category": "Python"}, {"instruction": "def filter_int(n: Node, query: str) -> int:\n    \"\"\"\n    Filter and ensure that the returned value is of type int.\n    \"\"\"\n", "input": "", "output": "    return _scalariter2item(n, query, int)", "category": "Python"}, {"instruction": "def POST_AUTH(self, courseid, taskid):  # pylint: disable=arguments-differ\n        \"\"\" Upload or modify a file \"\"\"\n", "input": "", "output": "        if not id_checker(taskid):\n            raise Exception(\"Invalid task id\")\n\n        self.get_course_and_check_rights(courseid, allow_all_staff=False)\n\n        request = web.input(file={})\n        if request.get(\"action\") == \"upload\" and request.get('path') is not None and request.get('file') is not None:\n            return self.action_upload(courseid, taskid, request.get('path'), request.get('file'))\n        elif request.get(\"action\") == \"edit_save\" and request.get('path') is not None and request.get('content') is not None:\n            return self.action_edit_save(courseid, taskid, request.get('path'), request.get('content'))\n        else:\n            return self.show_tab_file(courseid, taskid)", "category": "Python"}, {"instruction": "def get_request_data():\n    \"\"\" extract and convert the json data from the request\n\n    returns a list of dictionaries\n    \"\"\"\n", "input": "", "output": "    request = get_request()\n    data = request.get(\"BODY\", \"{}\")\n    if not is_json_deserializable(data):\n        from plone.jsonapi.routes.exceptions import APIError\n        raise APIError(400, \"Request Data is not JSON deserializable \u2013 Check JSON Syntax!\")\n    return _.convert(json.loads(data), _.to_list)", "category": "Python"}, {"instruction": "def loop(self, *tags):\n        \"\"\" Iterates over the tags in the entire Sentence,\n            For example, Sentence.loop(POS, LEMMA) yields tuples of the part-of-speech tags and lemmata. \n            Possible tags: WORD, LEMMA, POS, CHUNK, PNP, RELATION, ROLE, ANCHOR or a custom word tag.\n            Any order or combination of tags can be supplied.\n        \"\"\"\n", "input": "", "output": "        for i in range(len(self.words)):\n            yield tuple([self.get(i, tag=tag) for tag in tags])", "category": "Python"}, {"instruction": "def upload_hub(hub, host, remote_dir, user=None, port=22, rsync_options=RSYNC_OPTIONS, staging=None):\n    \"\"\"\n    Renders, stages, and uploads a hub.\n    \"\"\"\n", "input": "", "output": "    hub.render()\n    if staging is None:\n        staging = tempfile.mkdtemp()\n    staging, linknames = stage_hub(hub, staging=staging)\n    local_dir = os.path.join(staging)\n    upload(host, user, local_dir=local_dir, remote_dir=remote_dir, rsync_options=rsync_options)\n    return linknames", "category": "Python"}, {"instruction": "def cache(self):\n        \"\"\"Cache the Zotero data.\"\"\"\n", "input": "", "output": "        with open(self.cache_path, \"wb\") as f:\n            cache = {self.CACHE_REFERENCE_LIST: self._references,\n                     self.CACHE_REFERENCE_TYPES: self.reference_types,\n                     self.CACHE_REFERENCE_TEMPLATES: self.reference_templates}\n            pickle.dump(cache, f)", "category": "Python"}, {"instruction": "def _trim_base64(s):\n    \"\"\"Trim and hash base64 strings\"\"\"\n", "input": "", "output": "    if len(s) > 64 and _base64.match(s.replace('\\n', '')):\n        h = hash_string(s)\n        s = '%s...<snip base64, md5=%s...>' % (s[:8], h[:16])\n    return s", "category": "Python"}, {"instruction": "def get_device(self, device_id):\n        \"\"\"\n        Return specified device.\n\n        Returns a Command.\n        \"\"\"\n", "input": "", "output": "        def process_result(result):\n            return Device(result)\n\n        return Command('get', [ROOT_DEVICES, device_id],\n                       process_result=process_result)", "category": "Python"}, {"instruction": "def partial_steps_data(self, start=0):\n        \"\"\"\n        Iterates 5 steps from start position and\n        provides tuple for packing into buffer.\n\n        returns (0, 0) if stpe doesn't exist.\n\n        :param start: Position to start from (typically 0 or 5)\n        :yield: (setting, duration)\n        \"\"\"\n", "input": "", "output": "        cnt = 0\n        if len(self._prog_steps) >= start:\n            # yields actual steps for encoding\n            for step in self._prog_steps[start:start+5]:\n                yield((step.raw_data))\n                cnt += 1\n        while cnt < 5:\n            yield((0, 0))\n            cnt += 1", "category": "Python"}, {"instruction": "def gen_smul(src1, src2, dst):\n        \"\"\"Return a SMUL instruction.\n        \"\"\"\n", "input": "", "output": "        assert src1.size == src2.size\n\n        return ReilBuilder.build(ReilMnemonic.SMUL, src1, src2, dst)", "category": "Python"}, {"instruction": "def secret_exponent(self, s):\n        \"\"\"\n        Parse an integer secret exponent.\n        Return a :class:`Key <pycoin.key.Key>` or None.\n        \"\"\"\n", "input": "", "output": "        v = self.as_number(s)\n        if v:\n            try:\n                return self._network.keys.private(v)\n            except ValueError:\n                pass", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"Increment current term, vote for herself & send vote requests\"\"\"\n", "input": "", "output": "        self.storage.update({\n            'term': self.storage.term + 1,\n            'voted_for': self.id\n        })\n\n        self.vote_count = 1\n        self.request_vote()\n        self.election_timer.start()", "category": "Python"}, {"instruction": "def connect(self, interface=None):\n        \"\"\"Connect to the USB for the hottop.\n\n        Attempt to discover the USB port used for the Hottop and then form a\n        connection using the serial library.\n\n        :returns: bool\n        :raises SerialConnectionError:\n        \"\"\"\n", "input": "", "output": "        if self._simulate:\n            return True\n        if not interface:\n            match = self._autodiscover_usb()\n            self._log.debug(\"Auto-discovered USB port: %s\" % match)\n        else:\n            self.USB_PORT = interface\n\n        try:\n            self._conn = serial.Serial(self.USB_PORT, baudrate=self.BAUDRATE,\n                                       bytesize=self.BYTE_SIZE,\n                                       parity=self.PARITY,\n                                       stopbits=self.STOPBITS,\n                                       timeout=self.TIMEOUT)\n        except serial.serialutil.SerialException as e:\n            raise SerialConnectionError(str(e))\n\n        self._log.debug(\"Serial connection set\")\n        if not self._conn.isOpen():\n            self._conn.open()\n            self._log.debug(\"Serial connection opened\")\n        return True", "category": "Python"}, {"instruction": "def get_blen(self):\n        \"\"\"Get the BPF buffer length\"\"\"\n", "input": "", "output": "\n        try:\n            ret = fcntl.ioctl(self.ins, BIOCGBLEN, struct.pack(\"I\", 0))\n            return struct.unpack(\"I\", ret)[0]\n        except IOError:\n            warning(\"Unable to get the BPF buffer length\")\n            return", "category": "Python"}, {"instruction": "def get_functions_by_search(self, function_query, function_search):\n        \"\"\"Pass through to provider FunctionSearchSession.get_functions_by_search\"\"\"\n", "input": "", "output": "        # Implemented from azosid template for -\n        # osid.resource.ResourceSearchSession.get_resources_by_search_template\n        if not self._can('search'):\n            raise PermissionDenied()\n        return self._provider_session.get_functions_by_search(function_query, function_search)", "category": "Python"}, {"instruction": "def execute(self, *args, **options):\n        '''Placing this in execute because then subclass handle() don't have to call super'''\n", "input": "", "output": "        if options['verbose']:\n            options['verbosity'] = 3\n        if options['quiet']:\n            options['verbosity'] = 0\n        self.verbosity = options.get('verbosity', 1)\n        super().execute(*args, **options)", "category": "Python"}, {"instruction": "def DisplayTree(node, children, level=0):\n  \"\"\"Recursively display a node and each of its children.\n\n  Args:\n    node: The node we're displaying the children of.\n    children: Children of the parent node.\n    level: How deep in the tree we are.\n  \"\"\"\n", "input": "", "output": "  value = ''\n  node_type = ''\n\n  if 'caseValue' in node:\n    case_value = node['caseValue']\n    node_type = case_value['ProductDimension.Type']\n\n    if node_type == 'ProductCanonicalCondition':\n      value = (case_value['condition'] if 'condition' in case_value\n               else 'OTHER')\n    elif node_type == 'ProductBiddingCategory':\n      value = '%s(%s)' % (case_value['type'], case_value['value']\n                          if 'value' in case_value else 'OTHER')\n    else:\n      value = (case_value['value'] if 'value' in case_value else 'OTHER')\n\n  print ('%sid: %s, node_type: %s, value: %s\\n'\n         % (' ' * level, node['id'], node_type, value))\n\n  for child_node in children[node['id']]:\n    DisplayTree(child_node, children, level + 1)", "category": "Python"}, {"instruction": "def _get_list_select(self, column, key=None):\n        \"\"\"\n        Get the columns that should be used in a list\n\n        :param column: The column to get the values for\n        :type column: str\n\n        :param key: The key\n        :type key: str\n\n        :return: The list of values\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        if key is None:\n            elements = [column]\n        else:\n            elements = [column, key]\n\n        select = []\n        for elem in elements:\n            dot = elem.find('.')\n\n            if dot >= 0:\n                select.append(column[dot + 1:])\n            else:\n                select.append(elem)\n\n        return select", "category": "Python"}, {"instruction": "def verify_submit(\n    session, queue_url, log_url, job_ids, timeout=_DEFAULT_TIMEOUT, delay=_DEFAULT_DELAY, **kwargs\n):\n    \"\"\"Verifies that the results were successfully submitted.\"\"\"\n", "input": "", "output": "    verification_queue = get_queue_obj(session=session, queue_url=queue_url, log_url=log_url)\n    return verification_queue.verify_submit(job_ids, timeout, delay, **kwargs)", "category": "Python"}, {"instruction": "def clause_tokenize(sentence):\n    \"\"\"\n    Split on comma or parenthesis, if there are more then three words for each clause\n\n    >>> context = 'While I was walking home, this bird fell down in front of me.'\n    >>> clause_tokenize(context)\n    ['While I was walking home,', ' this bird fell down in front of me.']\n\n    \"\"\"\n", "input": "", "output": "    clause_re = re.compile(r'((?:\\S+\\s){2,}\\S+,|(?:\\S+\\s){3,}(?=\\((?:\\S+\\s){2,}\\S+\\)))')\n    clause_stem = clause_re.sub(r'\\1###clausebreak###', sentence)\n    return [c for c in clause_stem.split('###clausebreak###') if c != '']", "category": "Python"}, {"instruction": "def tokenize(string, language=None, escape='___'):\n    \"\"\"\n    Given a string, return a list of math symbol tokens\n    \"\"\"\n", "input": "", "output": "    # Set all words to lowercase\n    string = string.lower()\n\n    # Ignore punctuation\n    if len(string) and not string[-1].isalnum():\n        character = string[-1]\n        string = string[:-1] + ' ' + character\n\n    # Parenthesis must have space around them to be tokenized properly\n    string = string.replace('(', ' ( ')\n    string = string.replace(')', ' ) ')\n\n    if language:\n        words = mathwords.words_for_language(language)\n\n        for phrase in words:\n            escaped_phrase = phrase.replace(' ', escape)\n            string = string.replace(phrase, escaped_phrase)\n\n    tokens = string.split()\n\n    for index, token in enumerate(tokens):\n        tokens[index] = token.replace(escape, ' ')\n\n    return tokens", "category": "Python"}, {"instruction": "def pause_writing(self):\n        '''Transport calls when the send buffer is full.'''\n", "input": "", "output": "        if not self.is_closing():\n            self._can_send.clear()\n            self.transport.pause_reading()", "category": "Python"}, {"instruction": "def unschedule(identifier):\n    '''Unschedule a periodical harvest job'''\n", "input": "", "output": "    source = actions.unschedule(identifier)\n    log.info('Unscheduled harvest source \"%s\"', source.name)", "category": "Python"}, {"instruction": "def _fasta_slice(fasta, seqid, start, stop, strand):\n    \"\"\"\n    Return slice of fasta, given (seqid, start, stop, strand)\n    \"\"\"\n", "input": "", "output": "    _strand = 1 if strand == '+' else -1\n    return fasta.sequence({'chr': seqid, 'start': start, 'stop': stop, \\\n        'strand': _strand})", "category": "Python"}, {"instruction": "def get_skydir_lthist(self, skydir, cth_bins):\n        \"\"\"Get the livetime distribution (observing profile) for a given sky\n        direction with binning in incidence angle defined by\n        ``cth_bins``.\n\n        Parameters\n        ----------\n        skydir : `~astropy.coordinates.SkyCoord`\n            Sky coordinate for which the observing profile will be\n            computed.\n\n        cth_bins : `~numpy.ndarray`\n            Bin edges in cosine of the incidence angle.\n\n        \"\"\"\n", "input": "", "output": "        ra = skydir.ra.deg\n        dec = skydir.dec.deg\n\n        npts = 1\n        bins = utils.split_bin_edges(cth_bins, npts)\n\n        center = edge_to_center(bins)\n        width = edge_to_width(bins)\n        ipix = hp.ang2pix(self.hpx.nside, np.pi / 2. - np.radians(dec),\n                          np.radians(ra), nest=self.hpx.nest)\n        lt = np.histogram(self._cth_center,\n                          weights=self.data[:, ipix], bins=bins)[0]\n        lt = np.sum(lt.reshape(-1, npts), axis=1)\n        return lt", "category": "Python"}, {"instruction": "def get_source_folders(self):\n        \"\"\"Returns project source folders\"\"\"\n", "input": "", "output": "        if self.root is None:\n            return []\n        result = list(self._custom_source_folders)\n        result.extend(self.pycore._find_source_folders(self.root))\n        return result", "category": "Python"}, {"instruction": "def give_us_somethin_to_talk_about(self, message):\n        \"\"\"new topic: set the room topic to a random conversation starter.\"\"\"\n", "input": "", "output": "        r = requests.get(\"http://www.chatoms.com/chatom.json?Normal=1&Fun=2&Philosophy=3&Out+There=4\")\n        data = r.json()\n        self.set_topic(data[\"text\"], message=message)", "category": "Python"}, {"instruction": "def imshow(self, name):\r\n        \"\"\"Show item's image\"\"\"\n", "input": "", "output": "        sw = self.shellwidget\r\n        if sw._reading:\r\n            sw.dbg_exec_magic('varexp', '--imshow %s' % name)\r\n        else:\r\n            sw.execute(\"%%varexp --imshow %s\" % name)", "category": "Python"}, {"instruction": "def is_coordinate_variable(ds, variable):\n    '''\n    Returns True if the variable is a coordinate variable\n\n    :param netCDF4.Dataset nc: An open netCDF dataset\n    :param str variable: Variable name\n    '''\n", "input": "", "output": "    if variable not in ds.variables:\n        return False\n    return ds.variables[variable].dimensions == (variable,)", "category": "Python"}, {"instruction": "def apply_to_type(input_, input_type, func):\n    \"\"\"Apply a function on a object of `input_type` or mapping, or sequence of objects of `input_type`.\n    \"\"\"\n", "input": "", "output": "    if isinstance(input_, input_type):\n        return func(input_)\n    elif isinstance(input_, string_classes):\n        return input_\n    elif isinstance(input_, collections.Mapping):\n        return {k: apply_to_type(sample, input_type, func) for k, sample in input_.items()}\n    elif isinstance(input_, collections.Sequence):\n        return [apply_to_type(sample, input_type, func) for sample in input_]\n    else:\n        raise TypeError((\"input must contain {}, dicts or lists; found {}\"\n                         .format(input_type, type(input_))))", "category": "Python"}, {"instruction": "def set_properties(self, properties):\n        \"\"\"Sets properties and text of this info from a dictionary\"\"\"\n", "input": "", "output": "        if isinstance(properties, dict):\n            self.properties = properties\n            self.sync_properties()\n        else:\n            self.text = properties", "category": "Python"}, {"instruction": "def format(self, link_resolver, output, extensions):\n        \"\"\"Banana banana\n        \"\"\"\n", "input": "", "output": "        info('Formatting documentation tree', 'formatting')\n        self.__setup_folder(output)\n\n        link_resolver.get_link_signal.connect(self.__get_link_cb)\n        # Page.formatting_signal.connect(self.__formatting_page_cb)\n        # Link.resolving_link_signal.connect(self.__link_referenced_cb)\n\n        self.__extensions = extensions\n\n        for page in self.walk():\n            self.format_page(page, link_resolver, output, extensions)\n\n        self.__extensions = None\n        link_resolver.get_link_signal.disconnect(self.__get_link_cb)", "category": "Python"}, {"instruction": "def new_code_block(self, **kwargs):\n        \"\"\"Create a new code block.\"\"\"\n", "input": "", "output": "        proto = {'content': '',\n                 'type': self.code,\n                 'IO': '',\n                 'attributes': ''}\n        proto.update(**kwargs)\n        return proto", "category": "Python"}, {"instruction": "def get_global_shelf_fpath(appname='default', ensure=False):\n    \"\"\" Returns the filepath to the global shelf \"\"\"\n", "input": "", "output": "    global_cache_dir = get_global_cache_dir(appname, ensure=ensure)\n    shelf_fpath = join(global_cache_dir, meta_util_constants.global_cache_fname)\n    return shelf_fpath", "category": "Python"}, {"instruction": "def _unpack_header(self, data):\n        \"\"\"\n        Unpacks the header of given byte string.\n        \"\"\"\n", "input": "", "output": "        return struct.unpack(self._struct_header,\n                             data[:self._struct_header_size])", "category": "Python"}, {"instruction": "def _gen_addr(entry):\n        \"\"\"Generates a vCard Address object\"\"\"\n", "input": "", "output": "        return Address(street=entry.get('address', ''),\n                       extended=entry.get('address2', ''),\n                       city=entry.get('city', ''),\n                       region=entry.get('state', ''),\n                       code=entry.get('zip', ''),\n                       country=entry.get('country', ''))", "category": "Python"}, {"instruction": "def img2img_transformer_tiny():\n  \"\"\"Tiny params.\"\"\"\n", "input": "", "output": "  hparams = img2img_transformer2d_base()\n  hparams.num_hidden_layers = 2\n  hparams.hidden_size = 128\n  hparams.batch_size = 4\n  hparams.max_length = 128\n  hparams.attention_key_channels = hparams.attention_value_channels = 0\n  hparams.filter_size = 128\n  hparams.num_heads = 1\n  hparams.pos = \"timing\"\n  return hparams", "category": "Python"}, {"instruction": "def from_file(cls, filepath):\n        \"\"\"Alternative constructor to get Torrent object from file.\n\n        :param str filepath:\n        :rtype: Torrent\n        \"\"\"\n", "input": "", "output": "        torrent = cls(Bencode.read_file(filepath))\n        torrent._filepath = filepath\n        return torrent", "category": "Python"}, {"instruction": "def _native_size(self):\n        \"\"\"\n        A (width, height) 2-tuple representing the native dimensions of the\n        image in EMU, calculated based on the image DPI value, if present,\n        assuming 72 dpi as a default.\n        \"\"\"\n", "input": "", "output": "        EMU_PER_INCH = 914400\n        horz_dpi, vert_dpi = self._dpi\n        width_px, height_px = self._px_size\n\n        width = EMU_PER_INCH * width_px / horz_dpi\n        height = EMU_PER_INCH * height_px / vert_dpi\n\n        return width, height", "category": "Python"}, {"instruction": "def _rename_objects_fast(self):\n        \"\"\" Rename all objects quickly to guaranteed-unique names using the\n        id() of each object.\n\n        This produces mostly unreadable GLSL, but is about 10x faster to\n        compile.\n        \"\"\"\n", "input": "", "output": "        for shader_name, deps in self._shader_deps.items():\n            for dep in deps:\n                name = dep.name\n                if name != 'main':\n                    ext = '_%x' % id(dep)\n                    name = name[:32-len(ext)] + ext\n                self._object_names[dep] = name", "category": "Python"}, {"instruction": "def get_service_status(self, name):\n        \"\"\"Returns the status for the given service name along with the output\n        of the query command\n        \"\"\"\n", "input": "", "output": "        svc = self._query_service(name)\n        if svc is not None:\n            return {'name': name,\n                    'status': self.parse_query(svc['output']),\n                    'output': svc['output']\n                    }\n        else:\n            return {'name': name,\n                    'status': 'missing',\n                    'output': ''\n                    }", "category": "Python"}, {"instruction": "def base_version(self):\n        \"\"\"Returns the actual upstream version (without dev info)\"\"\"\n", "input": "", "output": "        components = [self.xyz_version]\n        if self.ver_extra:\n            components.append(self.ver_extra)\n        return ''.join(components)", "category": "Python"}, {"instruction": "def serve(self, app, conf):\n        \"\"\"\n        A very simple approach for a WSGI server.\n        \"\"\"\n", "input": "", "output": "\n        if self.args.reload:\n            try:\n                self.watch_and_spawn(conf)\n            except ImportError:\n                print('The `--reload` option requires `watchdog` to be '\n                      'installed.')\n                print('   $ pip install watchdog')\n        else:\n            self._serve(app, conf)", "category": "Python"}, {"instruction": "def get(self, key, index=None):\n        \"\"\"Retrieves a value associated with a key from the database\n\n        Args:\n            key (str): The key to retrieve\n        \"\"\"\n", "input": "", "output": "        records = self.get_multi([key], index=index)\n\n        try:\n            return records[0][1]  # return the value from the key/value tuple\n        except IndexError:\n            return None", "category": "Python"}, {"instruction": "def multi_iter(iterable, count=2):\n    \"\"\"Return `count` independent, thread-safe iterators for `iterable`\"\"\"\n", "input": "", "output": "    # no need to special-case re-usable, container-like iterables\n    if not isinstance(\n            iterable,\n            (\n                    list, tuple, set,\n                    FutureChainResults,\n                    collections.Sequence, collections.Set, collections.Mapping, collections.MappingView\n            )):\n        iterable = SafeTee(iterable, n=count)\n    return (iter(iterable) for _ in range(count))", "category": "Python"}, {"instruction": "def decode_escapes(s):\n    '''Unescape libconfig string literals'''\n", "input": "", "output": "    def decode_match(match):\n        return codecs.decode(match.group(0), 'unicode-escape')\n\n    return ESCAPE_SEQUENCE_RE.sub(decode_match, s)", "category": "Python"}, {"instruction": "def bell_set(self, collection, ordinal=False):\n        \"\"\"\n        Calculates the Bell set\n        \"\"\"\n", "input": "", "output": "        if len(collection) == 1:\n            yield [ collection ]\n            return\n\n        first = collection[0]\n        for smaller in self.bell_set(collection[1:]):\n            for n, subset in enumerate(smaller):\n                if not ordinal or (ordinal and is_sorted(smaller[:n] + [[ first ] + subset] + smaller[n+1:], self._nan)):\n                    yield smaller[:n] + [[ first ] + subset] + smaller[n+1:]\n\n            if not ordinal or (ordinal and is_sorted([ [ first ] ] + smaller, self._nan)):\n                yield [ [ first ] ] + smaller", "category": "Python"}, {"instruction": "def set_target_temperture_by_name(self, zone_name, target_temperature):\n        \"\"\"\n        Set the target temperature for a zone by name\n        \"\"\"\n", "input": "", "output": "        zone = self.get_zone(zone_name)\n\n        if zone is None:\n            raise RuntimeError(\"Unknown zone\")\n\n        return self.set_target_temperature_by_id(zone[\"zoneId\"],\n                                                 target_temperature)", "category": "Python"}, {"instruction": "def python_job(self, function, parameters=None):\n        \"\"\"\n        Run python function\n\n        function    :   Python callable to execute\n        name        :   Name of function (if not given, will used function.__name__)\n        parameters  :   Parameters to parse to function\n        label       :   Function label; for logging purposes\n        \"\"\"\n", "input": "", "output": "        \n        if not callable(function):\n            raise utils.StimelaCabRuntimeError('Object given as function is not callable')\n        \n        if self.name is None:\n            self.name = function.__name__\n\n        self.job = {\n                        'function'  :   function,\n                        'parameters':   parameters,\n                   }\n\n        return 0", "category": "Python"}, {"instruction": "def get_option_choices(opt_name, opt_value, default_value, all_choices):\n    \"\"\"\n    Generate possible choices for the option `opt_name`\n    limited to `opt_value` value with default value\n    as `default_value`\n    \"\"\"\n", "input": "", "output": "\n    choices = []\n    if isinstance(opt_value, six.string_types):\n        choices = [opt_value]\n    elif isinstance(opt_value, (list, tuple)):\n        choices = list(opt_value)\n    elif opt_value is None:\n        choices = default_value\n    else:\n        raise InvalidOption('Option %s has invalid'\n                            ' value: %s' % (opt_name, opt_value))\n    if 'all' in choices:\n        choices = all_choices\n    for item in choices:\n        if item not in all_choices:\n            raise InvalidOption('Choices of option %s contains invalid'\n                                ' item: %s' % (opt_name, item))\n    return choices", "category": "Python"}, {"instruction": "def collapseuser(path):\n    \"\"\"If path begins with the home directory, replaces the start of the path with \"~/\". Essentially the reverse of os.path.expanduser()\"\"\"\n", "input": "", "output": "    home = os.path.join(os.path.expanduser(\"~\"), \"\")\n    if path.startswith(home):\n        path = os.path.join(\"~\", path[len(home):])\n    return path", "category": "Python"}, {"instruction": "def create_storage_policy(profile_manager, policy_spec):\n    '''\n    Creates a storage policy.\n\n    profile_manager\n        Reference to the profile manager.\n\n    policy_spec\n        Policy update spec.\n    '''\n", "input": "", "output": "    try:\n        profile_manager.Create(policy_spec)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise VMwareApiError('Not enough permissions. Required privilege: '\n                             '{0}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise VMwareRuntimeError(exc.msg)", "category": "Python"}, {"instruction": "def authorize_url(self):\n        \"\"\"\n        authorization url\n        request weibo authorization url\n        :return:\n        code    string    \u7528\u4e8e\u7b2c\u4e8c\u6b65\u8c03\u7528oauth2/access_token\u63a5\u53e3\uff0c\u83b7\u53d6\u6388\u6743\u540e\u7684access token\u3002\n        state    string    \u5982\u679c\u4f20\u9012\u53c2\u6570\uff0c\u4f1a\u56de\u4f20\u8be5\u53c2\u6570\n        \"\"\"\n", "input": "", "output": "\n        if self.oauth2_params and self.oauth2_params.get(\"display\") == \"mobile\":\n            auth_url = self.mobile_url + \"authorize\"\n        else:\n            auth_url = self.site_url + \"authorize\"\n\n        params = {\n            'client_id': self.client_id,\n            'redirect_uri': self.redirect_url,\n        }\n        params.update(self.oauth2_params)\n\n        params = filter_params(params)\n\n        return \"{auth_url}?{params}\".format(auth_url=auth_url, params=urlencode(params))", "category": "Python"}, {"instruction": "def inverse(self):\n        \"\"\"Return the inverse operator.\n\n        Examples\n        --------\n        >>> r3 = odl.rn(3)\n        >>> vec = r3.element([1, 2, 3])\n        >>> op = ScalingOperator(r3, 2.0)\n        >>> inv = op.inverse\n        >>> inv(op(vec)) == vec\n        True\n        >>> op(inv(vec)) == vec\n        True\n        \"\"\"\n", "input": "", "output": "        if self.scalar == 0.0:\n            raise ZeroDivisionError('scaling operator not invertible for '\n                                    'scalar==0')\n        return ScalingOperator(self.domain, 1.0 / self.scalar)", "category": "Python"}, {"instruction": "def topickle(table, source=None, protocol=-1, write_header=True):\n    \"\"\"\n    Write the table to a pickle file. E.g.::\n\n        >>> import petl as etl\n        >>> table1 = [['foo', 'bar'],\n        ...           ['a', 1],\n        ...           ['b', 2],\n        ...           ['c', 2]]\n        >>> etl.topickle(table1, 'example.p')\n        >>> # look what it did\n        ... table2 = etl.frompickle('example.p')\n        >>> table2\n        +-----+-----+\n        | foo | bar |\n        +=====+=====+\n        | 'a' |   1 |\n        +-----+-----+\n        | 'b' |   2 |\n        +-----+-----+\n        | 'c' |   2 |\n        +-----+-----+\n\n    Note that if a file already exists at the given location, it will be\n    overwritten.\n\n    The pickle file format preserves type information, i.e., reading and writing\n    is round-trippable for tables with non-string data values.\n\n    \"\"\"\n", "input": "", "output": "\n    _writepickle(table, source=source, mode='wb', protocol=protocol,\n                 write_header=write_header)", "category": "Python"}, {"instruction": "def is_valid_poes(self):\n        \"\"\"\n        When computing hazard maps and/or uniform hazard spectra,\n        the poes list must be non-empty.\n        \"\"\"\n", "input": "", "output": "        if self.hazard_maps or self.uniform_hazard_spectra:\n            return bool(self.poes)\n        else:\n            return True", "category": "Python"}, {"instruction": "def _media(self):\n        \"\"\"\n        Returns a forms.Media instance with the basic editor media and media\n        from all registered extensions.\n        \"\"\"\n", "input": "", "output": "        css = ['markymark/css/markdown-editor.css']\n        iconlibrary_css = getattr(\n            settings,\n            'MARKYMARK_FONTAWESOME_CSS',\n            'markymark/fontawesome/fontawesome.min.css'\n        )\n        if iconlibrary_css:\n            css.append(iconlibrary_css)\n\n        media = forms.Media(\n            css={'all': css},\n            js=('markymark/js/markdown-editor.js',)\n        )\n\n        # Use official extension loading to initialize all extensions\n        # and hook in extension-defined media files.\n        renderer = initialize_renderer()\n\n        for extension in renderer.registeredExtensions:\n            if hasattr(extension, 'media'):\n                media += extension.media\n        return media", "category": "Python"}, {"instruction": "def show_frequencies(vid_data, fps, bounds=None):\n    \"\"\"Graph the average value of the video as well as the frequency strength\"\"\"\n", "input": "", "output": "    averages = []\n\n    if bounds:\n        for x in range(1, vid_data.shape[0] - 1):\n            averages.append(vid_data[x, bounds[2]:bounds[3], bounds[0]:bounds[1], :].sum())\n    else:\n        for x in range(1, vid_data.shape[0] - 1):\n            averages.append(vid_data[x, :, :, :].sum())\n\n    averages = averages - min(averages)\n\n    charts_x = 1\n    charts_y = 2\n    pyplot.figure(figsize=(20, 10))\n    pyplot.subplots_adjust(hspace=.7)\n\n    pyplot.subplot(charts_y, charts_x, 1)\n    pyplot.title(\"Pixel Average\")\n    pyplot.xlabel(\"Time\")\n    pyplot.ylabel(\"Brightness\")\n    pyplot.plot(averages)\n\n    freqs = scipy.fftpack.fftfreq(len(averages), d=1.0 / fps)\n    fft = abs(scipy.fftpack.fft(averages))\n    idx = np.argsort(freqs)\n\n    pyplot.subplot(charts_y, charts_x, 2)\n    pyplot.title(\"FFT\")\n    pyplot.xlabel(\"Freq (Hz)\")\n    freqs = freqs[idx]\n    fft = fft[idx]\n\n    freqs = freqs[len(freqs) // 2 + 1:]\n    fft = fft[len(fft) // 2 + 1:]\n    pyplot.plot(freqs, abs(fft))\n\n    pyplot.show()", "category": "Python"}, {"instruction": "def show_linkinfo_output_show_link_info_linkinfo_version(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        show_linkinfo = ET.Element(\"show_linkinfo\")\n        config = show_linkinfo\n        output = ET.SubElement(show_linkinfo, \"output\")\n        show_link_info = ET.SubElement(output, \"show-link-info\")\n        linkinfo_rbridgeid_key = ET.SubElement(show_link_info, \"linkinfo-rbridgeid\")\n        linkinfo_rbridgeid_key.text = kwargs.pop('linkinfo_rbridgeid')\n        linkinfo_version = ET.SubElement(show_link_info, \"linkinfo-version\")\n        linkinfo_version.text = kwargs.pop('linkinfo_version')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def on_datastream(self, datastream):\n        \"\"\"\n        To turn on datastream\n        :param datastream: string\n        \"\"\"\n", "input": "", "output": "        url = '/datastream/' + str(datastream) + '/on'\n        response = self.http.post(url,\"\")\n        return response", "category": "Python"}, {"instruction": "def transformer_base_single_gpu():\n  \"\"\"HParams for transformer base model for single GPU.\"\"\"\n", "input": "", "output": "  hparams = transformer_base()\n  hparams.batch_size = 1024\n  hparams.learning_rate_schedule = \"constant*linear_warmup*rsqrt_decay\"\n  hparams.learning_rate_constant = 0.1\n  hparams.learning_rate_warmup_steps = 16000\n  return hparams", "category": "Python"}, {"instruction": "def compute_arxiv_re(report_pattern, report_number):\n    \"\"\"Compute arXiv report-number.\"\"\"\n", "input": "", "output": "    if report_number is None:\n        report_number = r\"\\g<name>\"\n    report_re = re.compile(r\"(?<!<cds\\.REPORTNUMBER>)(?<!\\w)\" +\n                           \"(?P<name>\" + report_pattern + \")\" +\n                           old_arxiv_numbers, re.U | re.I)\n    return report_re, report_number", "category": "Python"}, {"instruction": "def key_from_dict(**kwargs):\n    \"\"\"\n    Return a unique string representation of a dict as quickly as possible.\n    Used to generated deduplication keys from a request.\n    \"\"\"\n", "input": "", "output": "    out = []\n    stack = [kwargs]\n    while stack:\n        obj = stack.pop()\n        if isinstance(obj, dict):\n            stack.extend(sorted(obj.items()))\n        elif isinstance(obj, (list, tuple)):\n            stack.extend(obj)\n        else:\n            out.append(str(obj))\n    return ''.join(out)", "category": "Python"}, {"instruction": "def _startDPDrag(self):\n        \"\"\"Callback for item menu.\"\"\"\n", "input": "", "output": "        dp = self._dp_menu_on\n        if dp and dp.archived:\n            drag = QDrag(self)\n            mimedata = QMimeData()\n            mimedata.setUrls([QUrl.fromLocalFile(dp.fullpath)])\n            drag.setMimeData(mimedata)\n            drag.exec_(Qt.CopyAction | Qt.LinkAction)", "category": "Python"}, {"instruction": "def register(self):\n        \"\"\"Register the persistent identifier with the provider.\n\n        :raises invenio_pidstore.errors.PIDInvalidAction: If the PID is not\n            already registered or is deleted or is a redirection to another\n            PID.\n        :returns: `True` if the PID is successfully register.\n        \"\"\"\n", "input": "", "output": "        if self.is_registered() or self.is_deleted() or self.is_redirected():\n            raise PIDInvalidAction(\n                \"Persistent identifier has already been registered\"\n                \" or is deleted.\")\n\n        try:\n            with db.session.begin_nested():\n                self.status = PIDStatus.REGISTERED\n                db.session.add(self)\n        except SQLAlchemyError:\n            logger.exception(\"Failed to register PID.\", extra=dict(pid=self))\n            raise\n        logger.info(\"Registered PID.\", extra=dict(pid=self))\n        return True", "category": "Python"}, {"instruction": "def get_tree_size(path):\n    \"\"\"Return total size of all files in directory tree at path.\"\"\"\n", "input": "", "output": "    size = 0\n    try:\n        for entry in scandir.scandir(path):\n            if entry.is_symlink():\n                pass\n            elif entry.is_dir():\n                size += get_tree_size(os.path.join(path, entry.name))\n            else:\n                size += entry.stat().st_size\n    except OSError:\n        pass\n    return size", "category": "Python"}, {"instruction": "def score_x_of_a_kind_yatzy(dice: List[int], min_same_faces: int) -> int:\n    \"\"\"Similar to yahtzee, but only return the sum of the dice that satisfy min_same_faces\n    \"\"\"\n", "input": "", "output": "    for die, count in Counter(dice).most_common(1):\n        if count >= min_same_faces:\n            return die * min_same_faces\n    return 0", "category": "Python"}, {"instruction": "def create_event_permission(self, lambda_name, principal, source_arn):\n        \"\"\"\n        Create permissions to link to an event.\n\n        Related: http://docs.aws.amazon.com/lambda/latest/dg/with-s3-example-configure-event-source.html\n        \"\"\"\n", "input": "", "output": "        logger.debug('Adding new permission to invoke Lambda function: {}'.format(lambda_name))\n        permission_response = self.lambda_client.add_permission(\n            FunctionName=lambda_name,\n            StatementId=''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(8)),\n            Action='lambda:InvokeFunction',\n            Principal=principal,\n            SourceArn=source_arn,\n        )\n\n        if permission_response['ResponseMetadata']['HTTPStatusCode'] != 201:\n            print('Problem creating permission to invoke Lambda function')\n            return None  # XXX: Raise?\n\n        return permission_response", "category": "Python"}, {"instruction": "def recordset(method):\n        \"\"\"Use this to decorate methods that expect a record set.\"\"\"\n", "input": "", "output": "        def wrapper(self, *args, **kwargs):\n            if self.__records__ is None:\n                raise ValidationError(\n                    'There are no records in the set.',\n                )\n            return method(self, *args, **kwargs)\n        return Api.model(wrapper)", "category": "Python"}, {"instruction": "def use_propsfs(self, folder=None, front=False):\n        \"\"\"\n        Args:\n            folder (str | unicode | None): Optional custom mount folder (defaults to /mnt/props on Linux, and /Volumes/props on OSX)\n            front (bool): If True, add provider to front of list\n        \"\"\"\n", "input": "", "output": "        if folder is None:\n            folder = \"/%s/props\" % (\"Volumes\" if platform.system().lower() == \"darwin\" else \"mnt\")\n        self.add(PropsfsProvider(folder), front=front)", "category": "Python"}, {"instruction": "def is_indexed(self, identifier):\n        \"\"\" Returns True if identifier is already indexed. Otherwise returns False. \"\"\"\n", "input": "", "output": "        query = text(", "category": "Python"}, {"instruction": "def is_export_interface(instrument_interface):\n    \"\"\"Returns whether the instrument interface passed in is for results export\n    \"\"\"\n", "input": "", "output": "    if IInstrumentExportInterface.providedBy(instrument_interface):\n        return True\n\n    # TODO Remove this once classic instrument interface migrated\n    if hasattr(instrument_interface, '__name__'):\n        obj_name = instrument_interface.__name__.replace(__name__, \"\")\n        if obj_name[1:] in __all__ and hasattr(instrument_interface, \"Export\"):\n            return True\n    return False", "category": "Python"}, {"instruction": "def eval_expression(self, t):\n        \"\"\"\n        Evaluates a C preprocessor expression.\n\n        This is done by converting it to a Python equivalent and\n        eval()ing it in the C preprocessor namespace we use to\n        track #define values.\n        \"\"\"\n", "input": "", "output": "        t = CPP_to_Python(' '.join(t[1:]))\n        try: return eval(t, self.cpp_namespace)\n        except (NameError, TypeError): return 0", "category": "Python"}, {"instruction": "def add_key_value(self, key, value):\n        \"\"\"Add custom field to Group object.\n\n        .. note:: The key must be the exact name required by the batch schema.\n\n        Example::\n\n            document = tcex.batch.group('Document', 'My Document')\n            document.add_key_value('fileName', 'something.pdf')\n\n        Args:\n            key (str): The field key to add to the JSON batch data.\n            value (str): The field value to add to the JSON batch data.\n        \"\"\"\n", "input": "", "output": "        key = self._metadata_map.get(key, key)\n        if key in ['dateAdded', 'eventDate', 'firstSeen', 'publishDate']:\n            self._group_data[key] = self._utils.format_datetime(\n                value, date_format='%Y-%m-%dT%H:%M:%SZ'\n            )\n        elif key == 'file_content':\n            # file content arg is not part of Group JSON\n            pass\n        else:\n            self._group_data[key] = value", "category": "Python"}, {"instruction": "def cancel(self, force=False):\n        \"\"\"Cancel this job.\n\n        Args:\n            force (bool, optional): Forcefully cancel this job.\n\n        Returns:\n            bool: True if the job was cancelled, otherwise False if an error occurred.\n        \"\"\"\n", "input": "", "output": "        return self.rest_client._sc._delegator._cancel_job(self, force)", "category": "Python"}, {"instruction": "def verify_scores(scores):\n    \"\"\"Ensures that scores is stored as a numpy array and checks that all\n    values are finite.\n    \"\"\"\n", "input": "", "output": "    scores = np.array(scores, copy=False)\n    if np.any(~np.isfinite(scores)):\n        raise ValueError(\"scores contains invalid values. \" +\n                         \"Please check that all values are finite.\")\n    if scores.ndim == 1:\n        scores = scores[:,np.newaxis]\n    return scores", "category": "Python"}, {"instruction": "def facilities(self):\n        \"\"\"\n        This method returns the properties facilities.\n        :return:\n        \"\"\"\n", "input": "", "output": "        facilities = []\n        try:\n            list_items = self._ad_page_content.select(\"#facilities li\")\n        except Exception as e:\n            if self._debug:\n                logging.error(\n                    \"Error getting facilities. Error message: \" + e.args[0])\n            return\n\n        for li in list_items:\n            facilities.append(li.text)\n        return facilities", "category": "Python"}, {"instruction": "def print_table(*args, **kwargs):\n    \"\"\"\n    if csv:\n        import csv\n        t = csv.writer(sys.stdout, delimiter=\";\")\n        t.writerow(header)\n    else:\n        t = PrettyTable(header)\n        t.align = \"r\"\n        t.align[\"details\"] = \"l\"\n    \"\"\"\n", "input": "", "output": "    t = format_table(*args, **kwargs)\n    click.echo(t)", "category": "Python"}, {"instruction": "def on_electrode_states_updated(self, states):\n        '''\n        .. versionchanged:: 0.12\n            Refactor to use :meth:`on_electrode_states_set`.\n        '''\n", "input": "", "output": "        states['electrode_states'] = \\\n            states['electrode_states'].combine_first(self.canvas_slave\n                                                     .electrode_states)\n        self.on_electrode_states_set(states)", "category": "Python"}, {"instruction": "def _check_update_(self):\n        \"\"\"Check if the current version of the library is outdated.\"\"\"\n", "input": "", "output": "        try:\n            data = requests.get(\"https://pypi.python.org/pypi/jira/json\", timeout=2.001).json()\n\n            released_version = data['info']['version']\n            if parse_version(released_version) > parse_version(__version__):\n                warnings.warn(\n                    \"You are running an outdated version of JIRA Python %s. Current version is %s. Do not file any bugs against older versions.\" % (\n                        __version__, released_version))\n        except requests.RequestException:\n            pass\n        except Exception as e:\n            logging.warning(e)", "category": "Python"}, {"instruction": "def value(self):\n        \"\"\"Get parameter value.\n\n        If this cached value is None and this serialized value is not None,\n        calculate the new value from the serialized one.\n\n        :return: parameter value.\n        :raises: TypeError if serialized value is not an instance of self ptype\n            . ParserError if parsing step raised an error.\n        \"\"\"\n", "input": "", "output": "\n        result = self._value\n\n        if result is None and self._svalue is not None:\n\n            try:\n                result = self._value = self.resolve()\n\n            except Exception as e:\n                reraise(\n                    Parameter.Error,\n                    Parameter.Error('Call the method \"resolve\" first.')\n                )\n\n        return result", "category": "Python"}, {"instruction": "def hasOption(self, name):\n    \"\"\"\n    Check whether this CLI has an option with a given name.\n\n    :param name: the name of the option to check; can be short or long name.\n    :return: true if an option exists in the UI for the given name\n    \"\"\"\n", "input": "", "output": "    name = name.strip()\n    for o in self.options:\n      if o.short == name or o.long == name:\n        return True\n    return False", "category": "Python"}, {"instruction": "def save_json(py_obj, json_path):\n    \"\"\"Serialize a native object to JSON and save it normalized, pretty printed to a\n    file.\n\n    The JSON string is normalized by sorting any dictionary keys.\n\n    Args:\n      py_obj: object\n        Any object that can be represented in JSON. Some types, such as datetimes are\n        automatically converted to strings.\n\n      json_path: str\n        File path to which to write the JSON file. E.g.: The path must exist. The\n        filename will normally end with \".json\".\n\n    See Also:\n      ToJsonCompatibleTypes()\n\n    \"\"\"\n", "input": "", "output": "    with open(json_path, 'w', encoding='utf-8') as f:\n        f.write(serialize_to_normalized_pretty_json(py_obj))", "category": "Python"}, {"instruction": "def create(cls, zmq_context, endpoint):\n        \"\"\"Create new server transport.\n\n        Instead of creating the socket yourself, you can call this function and\n        merely pass the :py:class:`zmq.core.context.Context` instance.\n\n        By passing a context imported from :py:mod:`zmq.green`, you can use\n        green (gevent) 0mq sockets as well.\n\n        :param zmq_context: A 0mq context.\n        :param endpoint: The endpoint clients will connect to.\n        \"\"\"\n", "input": "", "output": "        socket = zmq_context.socket(zmq.ROUTER)\n        socket.bind(endpoint)\n        return cls(socket)", "category": "Python"}, {"instruction": "def _convert_angle_to_pypot(angle, joint, **kwargs):\n    \"\"\"Converts an angle to a PyPot-compatible format\"\"\"\n", "input": "", "output": "    angle_deg = (angle * 180 / np.pi)\n\n    if joint[\"orientation-convention\"] == \"indirect\":\n        angle_deg = -1 * angle_deg\n\n    # UGLY\n    if joint[\"name\"].startswith(\"l_shoulder_x\"):\n        angle_deg = -1 * angle_deg\n\n    angle_pypot = angle_deg - joint[\"offset\"]\n\n    return angle_pypot", "category": "Python"}, {"instruction": "def to_profile_info(self, serialize_credentials=False):\n        \"\"\"Unlike to_project_config, this dict is not a mirror of any existing\n        on-disk data structure. It's used when creating a new profile from an\n        existing one.\n\n        :param serialize_credentials bool: If True, serialize the credentials.\n            Otherwise, the Credentials object will be copied.\n        :returns dict: The serialized profile.\n        \"\"\"\n", "input": "", "output": "        result = {\n            'profile_name': self.profile_name,\n            'target_name': self.target_name,\n            'config': self.config.to_dict(),\n            'threads': self.threads,\n            'credentials': self.credentials.incorporate(),\n        }\n        if serialize_credentials:\n            result['credentials'] = result['credentials'].serialize()\n        return result", "category": "Python"}, {"instruction": "def add_comment(self, page_id, text):\n        \"\"\"\n        Add comment into page\n        :param page_id\n        :param text\n        \"\"\"\n", "input": "", "output": "        data = {'type': 'comment',\n                'container': {'id': page_id, 'type': 'page', 'status': 'current'},\n                'body': {'storage': {'value': text, 'representation': 'storage'}}}\n        return self.post('rest/api/content/', data=data)", "category": "Python"}, {"instruction": "def add_type(self, new_name, orig_names):\n        \"\"\"Record the typedefd name for orig_names. Resolve orig_names\n        to their core names and save those.\n\n        :new_name: TODO\n        :orig_names: TODO\n        :returns: TODO\n\n        \"\"\"\n", "input": "", "output": "        self._dlog(\"adding a type '{}'\".format(new_name))\n        # TODO do we allow clobbering of types???\n        res = copy.copy(orig_names)\n        resolved_names = self._resolve_name(res[-1])\n        if resolved_names is not None:\n            res.pop()\n            res += resolved_names\n\n        self._curr_scope[\"types\"][new_name] = res", "category": "Python"}, {"instruction": "def create_list(client, title):\n    ''' Creates a new list with the given title '''\n", "input": "", "output": "    _check_title_length(title, client.api)\n    data = {\n            'title' : title,\n            }\n    response = client.authenticated_request(client.api.Endpoints.LISTS, method='POST', data=data)\n    return response.json()", "category": "Python"}, {"instruction": "def node_from_dict(dic, nodefactory=Node):\n    \"\"\"\n    Convert a (nested) dictionary with attributes tag, attrib, text, nodes\n    into a Node object.\n    \"\"\"\n", "input": "", "output": "    tag = dic['tag']\n    text = dic.get('text')\n    attrib = dic.get('attrib', {})\n    nodes = dic.get('nodes', [])\n    if not nodes:\n        return nodefactory(tag, attrib, text)\n    return nodefactory(tag, attrib, nodes=list(map(node_from_dict, nodes)))", "category": "Python"}, {"instruction": "def cli(env, account_id, content_url):\n    \"\"\"Purge cached files from all edge nodes.\n\n    Examples:\n         slcli cdn purge 97794 http://example.com/cdn/file.txt\n         slcli cdn purge 97794 http://example.com/cdn/file.txt https://dal01.example.softlayer.net/image.png\n    \"\"\"\n", "input": "", "output": "\n    manager = SoftLayer.CDNManager(env.client)\n    content_list = manager.purge_content(account_id, content_url)\n\n    table = formatting.Table(['url', 'status'])\n\n    for content in content_list:\n        table.add_row([\n            content['url'],\n            content['statusCode']\n        ])\n\n    env.fout(table)", "category": "Python"}, {"instruction": "def splitter(iterable, chunksize=60):\n    \"\"\"Split an iterable that supports indexing into chunks of 'chunksize'.\"\"\"\n", "input": "", "output": "    return (iterable[0+i:chunksize+i]\n            for i in range(0, len(iterable), chunksize))", "category": "Python"}, {"instruction": "def stop_at(iterable, idx):\n    \"\"\"Stops iterating before yielding the specified idx.\"\"\"\n", "input": "", "output": "    for i, item in enumerate(iterable):\n        if i == idx: return\n        yield item", "category": "Python"}, {"instruction": "def last_modified(self, name: str = None) -> str:\n\t\t\"\"\"\n\t\tReturn a compact ISO8601 timestamp (UTC timezone) indicating when the layer was last modified\n\n\t\tNote: if name is None, the modification time of the most recently modified layer is returned\n\t\t\"\"\"\n", "input": "", "output": "\t\tif name is not None:\n\t\t\treturn self[name].last_modified()\n\t\tts = \"\"\n\t\tfor name in self.keys():\n\t\t\tif ts is None:\n\t\t\t\tts = self[name].last_modified()\n\t\t\telse:\n\t\t\t\tif self[name].last_modified() > ts:\n\t\t\t\t\tts = self[name].last_modified()\n\t\treturn ts", "category": "Python"}, {"instruction": "def from_secret_type(secret_type, settings):\n    \"\"\"\n    Note: Only called from audit.py\n\n    :type secret_type: str\n    :param secret_type: unique identifier for plugin type\n\n    :type settings: list\n    :param settings: output of \"plugins_used\" in baseline. e.g.\n        >>> [\n        ...     {\n        ...         'name': 'Base64HighEntropyString',\n        ...         'base64_limit': 4.5,\n        ...     },\n        ... ]\n    \"\"\"\n", "input": "", "output": "    mapping = _get_mapping_from_secret_type_to_class_name()\n    try:\n        classname = mapping[secret_type]\n    except KeyError:\n        return None\n\n    for plugin in settings:\n        if plugin['name'] == classname:\n            plugin_init_vars = plugin.copy()\n            plugin_init_vars.pop('name')\n\n            return from_plugin_classname(\n                classname,\n                **plugin_init_vars\n            )", "category": "Python"}, {"instruction": "def get_repos(self, type=github.GithubObject.NotSet):\n        \"\"\"\n        :calls: `GET /orgs/:org/repos <http://developer.github.com/v3/repos>`_\n        :param type: string ('all', 'public', 'private', 'forks', 'sources', 'member')\n        :rtype: :class:`github.PaginatedList.PaginatedList` of :class:`github.Repository.Repository`\n        \"\"\"\n", "input": "", "output": "        assert type is github.GithubObject.NotSet or isinstance(type, (str, unicode)), type\n        url_parameters = dict()\n        if type is not github.GithubObject.NotSet:\n            url_parameters[\"type\"] = type\n        return github.PaginatedList.PaginatedList(\n            github.Repository.Repository,\n            self._requester,\n            self.url + \"/repos\",\n            url_parameters\n        )", "category": "Python"}, {"instruction": "def shell(command, *args):\n\t'''Pass a command into the shell.'''\n", "input": "", "output": "\tif args:\n\t\tcommand = command.format(*args)\n\n\tprint LOCALE['shell'].format(command)\n\n\ttry:\n\t\treturn subprocess.check_output(command, shell=True)\n\texcept subprocess.CalledProcessError, ex:\n\t\treturn ex", "category": "Python"}, {"instruction": "def irc(self, *args, **kwargs):\n        \"\"\"\n        Post IRC Message\n\n        Post a message on IRC to a specific channel or user, or a specific user\n        on a specific channel.\n\n        Success of this API method does not imply the message was successfully\n        posted. This API method merely inserts the IRC message into a queue\n        that will be processed by a background process.\n        This allows us to re-send the message in face of connection issues.\n\n        However, if the user isn't online the message will be dropped without\n        error. We maybe improve this behavior in the future. For now just keep\n        in mind that IRC is a best-effort service.\n\n        This method takes input: ``v1/irc-request.json#``\n\n        This method is ``experimental``\n        \"\"\"\n", "input": "", "output": "\n        return self._makeApiCall(self.funcinfo[\"irc\"], *args, **kwargs)", "category": "Python"}, {"instruction": "def merge(self, other, merge_body=True):\n        \"\"\"\n        Default merge method.\n\n        Args:\n            other: another MujocoXML instance\n                raises XML error if @other is not a MujocoXML instance.\n                merges <worldbody/>, <actuator/> and <asset/> of @other into @self\n            merge_body: True if merging child bodies of @other. Defaults to True.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(other, MujocoXML):\n            raise XMLError(\"{} is not a MujocoXML instance.\".format(type(other)))\n        if merge_body:\n            for body in other.worldbody:\n                self.worldbody.append(body)\n        self.merge_asset(other)\n        for one_actuator in other.actuator:\n            self.actuator.append(one_actuator)\n        for one_equality in other.equality:\n            self.equality.append(one_equality)\n        for one_contact in other.contact:\n            self.contact.append(one_contact)\n        for one_default in other.default:\n            self.default.append(one_default)", "category": "Python"}, {"instruction": "def create_folder(cls, name, parent=None, project=None,\n                      api=None):\n        \"\"\"Create a new folder\n        :param name: Folder name\n        :param parent: Parent folder\n        :param project: Project to create folder in\n        :param api: Api instance\n        :return: New folder\n        \"\"\"\n", "input": "", "output": "        api = api or cls._API\n\n        data = {\n            'name': name,\n            'type': cls.FOLDER_TYPE\n        }\n\n        if not parent and not project:\n            raise SbgError('Parent or project must be provided')\n\n        if parent and project:\n            raise SbgError(\n                'Providing both \"parent\" and \"project\" is not allowed'\n            )\n\n        if parent:\n            data['parent'] = Transform.to_file(file_=parent)\n\n        if project:\n            data['project'] = Transform.to_project(project=project)\n\n        response = api.post(url=cls._URL['create_folder'], data=data).json()\n        return cls(api=api, **response)", "category": "Python"}, {"instruction": "def update(self, resource, timeout=-1):\n        \"\"\"\n        Updates only name for the Artifact Bundle.\n\n        Args:\n            resource (dict): Object to update.\n            timeout:\n                Timeout in seconds. Waits for task completion by default. The timeout does not abort the operation\n                in OneView, it just stops waiting for its completion.\n\n        Returns:\n            dict: Updated resource.\n        \"\"\"\n", "input": "", "output": "        return self._client.update(resource, timeout=timeout, default_values=self.DEFAULT_VALUES)", "category": "Python"}, {"instruction": "def GET_AUTH(self, courseid, taskid):  # pylint: disable=arguments-differ\n        \"\"\" GET request \"\"\"\n", "input": "", "output": "        course, task = self.get_course_and_check_rights(courseid, taskid)\n        return self.page(course, task)", "category": "Python"}, {"instruction": "def ruamelindex(self, strictindex):\n        \"\"\"\n        Get the ruamel equivalent of a strict parsed index.\n\n        E.g. 0 -> 0, 1 -> 2, parsed-via-slugify -> Parsed via slugify\n        \"\"\"\n", "input": "", "output": "        return (\n            self.key_association.get(strictindex, strictindex)\n            if self.is_mapping()\n            else strictindex\n        )", "category": "Python"}, {"instruction": "def Title(self):\n        \"\"\"Return the Batch ID if title is not defined\n        \"\"\"\n", "input": "", "output": "        titlefield = self.Schema().getField('title')\n        if titlefield.widget.visible:\n            return safe_unicode(self.title).encode('utf-8')\n        else:\n            return safe_unicode(self.id).encode('utf-8')", "category": "Python"}, {"instruction": "def get_properties(self, instance, fields):\n        \"\"\"\n        Get the feature metadata which will be used for the GeoJSON\n        \"properties\" key.\n\n        By default it returns all serializer fields excluding those used for\n        the ID, the geometry and the bounding box.\n\n        :param instance: The current Django model instance\n        :param fields: The list of fields to process (fields already processed have been removed)\n        :return: OrderedDict containing the properties of the current feature\n        :rtype: OrderedDict\n        \"\"\"\n", "input": "", "output": "        properties = OrderedDict()\n\n        for field in fields:\n            if field.write_only:\n                continue\n            value = field.get_attribute(instance)\n            representation = None\n            if value is not None:\n                representation = field.to_representation(value)\n            properties[field.field_name] = representation\n\n        return properties", "category": "Python"}, {"instruction": "def resolveFilenameConflicts(self):\n        \"\"\"Goes through list of DPs to make sure that their destination names\n        do not clash. Adjust names as needed. Returns True if some conflicts were resolved.\n        \"\"\"\n", "input": "", "output": "        taken_names = set()\n        resolved = False\n        # iterate through items\n        for item, dp in self.getItemDPList():\n            # only apply this to saved DPs\n            if dp.policy not in [\"remove\", \"ignore\", \"banish\"]:\n                name0 = str(item.text(self.ColRename))\n                name = _makeUniqueFilename(taken_names, name0)\n                if name != name0:\n                    item.setText(self.ColRename, name)\n                    resolved = True\n                    self.emit(SIGNAL(\"updated\"))\n        return resolved", "category": "Python"}, {"instruction": "def update(self, redraw=True):\n        \"\"\"redraw interface\"\"\"\n", "input": "", "output": "        # get the main urwid.Frame widget\n        mainframe = self.root_widget.original_widget\n\n        # body\n        if self.current_buffer:\n            mainframe.set_body(self.current_buffer)\n\n        # footer\n        lines = []\n        if self._notificationbar:  # .get_text()[0] != ' ':\n            lines.append(self._notificationbar)\n        if self._show_statusbar:\n            lines.append(self.build_statusbar())\n\n        if lines:\n            mainframe.set_footer(urwid.Pile(lines))\n        else:\n            mainframe.set_footer(None)\n        # force a screen redraw\n        if self.mainloop.screen.started and redraw:\n            self.mainloop.draw_screen()", "category": "Python"}, {"instruction": "def get_flash_info(self):\n        \"\"\"!\n        @brief Get info about the flash.\n\n        Override this method to return different values.\n        \"\"\"\n", "input": "", "output": "        assert self.region is not None\n\n        info = FlashInfo()\n        info.rom_start = self.region.start\n        info.erase_weight = self.region.erase_all_weight\n        info.crc_supported = self.use_analyzer\n        return info", "category": "Python"}, {"instruction": "def _compute_anom_data_decay_all(self):\n        \"\"\"\n        Compute anomaly scores using a lagging window covering all the data points before.\n        \"\"\"\n", "input": "", "output": "        anom_scores = {}\n        values = self.time_series.values\n        ema = utils.compute_ema(self.smoothing_factor, values)\n        stdev = numpy.std(values)\n        for i, (timestamp, value) in enumerate(self.time_series_items):\n            anom_score = abs((value - ema[i]) / stdev) if stdev else value - ema[i]\n            anom_scores[timestamp] = anom_score\n        self.anom_scores = TimeSeries(self._denoise_scores(anom_scores))", "category": "Python"}, {"instruction": "def convert_argmax(node, **kwargs):\n    \"\"\"Map MXNet's argmax operator attributes to onnx's ArgMax operator\n    and return the created node.\n    \"\"\"\n", "input": "", "output": "    name, input_nodes, attrs = get_inputs(node, kwargs)\n\n    axis = int(attrs.get(\"axis\"))\n    keepdims = get_boolean_attribute_value(attrs, \"keepdims\")\n\n    node = onnx.helper.make_node(\n        'ArgMax',\n        inputs=input_nodes,\n        axis=axis,\n        keepdims=keepdims,\n        outputs=[name],\n        name=name\n    )\n    return [node]", "category": "Python"}, {"instruction": "def reverse_tree(tree):\n    \"\"\"Reverse the dependency tree.\n\n    ie. the keys of the resulting dict are objects of type\n    ReqPackage and the values are lists of DistPackage objects.\n\n    :param dict tree: the pkg dependency tree obtained by calling\n                      `construct_tree` function\n    :returns: reversed tree\n    :rtype: dict\n\n    \"\"\"\n", "input": "", "output": "    rtree = defaultdict(list)\n    child_keys = set(c.key for c in flatten(tree.values()))\n    for k, vs in tree.items():\n        for v in vs:\n            node = find_tree_root(rtree, v.key) or v\n            rtree[node].append(k.as_required_by(v))\n        if k.key not in child_keys:\n            rtree[k.as_requirement()] = []\n    return rtree", "category": "Python"}, {"instruction": "def model_length(gene, domain):\n    \"\"\"\n    get length of model\n    \"\"\"\n", "input": "", "output": "    if gene == '16S':\n        domain2max = {'E_coli_K12': int(1538), 'bacteria': int(1689), 'archaea': int(1563), 'eukarya': int(2652)}\n        return domain2max[domain]\n    elif gene == '23S':\n        domain2max = {'E_coli_K12': int(2903), 'bacteria': int(3146), 'archaea': int(3774), 'eukarya': int(9079)}\n        return domain2max[domain]\n    else:\n        print(sys.stderr, '# length unknown for gene: %s, domain: %s' % (gene, domain))\n        exit()", "category": "Python"}, {"instruction": "def set_setting(key, value, qsettings=None):\n    \"\"\"Set value to QSettings based on key in InaSAFE scope.\n\n    :param key: Unique key for setting.\n    :type key: basestring\n\n    :param value: Value to be saved.\n    :type value: QVariant\n\n    :param qsettings: A custom QSettings to use. If it's not defined, it will\n        use the default one.\n    :type qsettings: qgis.PyQt.QtCore.QSettings\n    \"\"\"\n", "input": "", "output": "    full_key = '%s/%s' % (APPLICATION_NAME, key)\n    set_general_setting(full_key, value, qsettings)", "category": "Python"}, {"instruction": "def duplicate_files(self):\n        '''\n        Search for duplicates of submission file uploads for this assignment.\n        This includes the search in other course, whether inactive or not.\n        Returns a list of lists, where each latter is a set of duplicate submissions\n        with at least on of them for this assignment\n        '''\n", "input": "", "output": "        result=list()\n        files = SubmissionFile.valid_ones.order_by('md5')\n\n        for key, dup_group in groupby(files, lambda f: f.md5):\n            file_list=[entry for entry in dup_group]\n            if len(file_list)>1:\n                for entry in file_list:\n                    if entry.submissions.filter(assignment=self).count()>0:\n                        result.append([key, file_list])\n                        break\n        return result", "category": "Python"}, {"instruction": "def headerData(self, section, orientation, role=Qt.DisplayRole):\r\n        \"\"\"Set header data\"\"\"\n", "input": "", "output": "        if role != Qt.DisplayRole:\r\n            return to_qvariant()\r\n        labels = self.xlabels if orientation == Qt.Horizontal else self.ylabels\r\n        if labels is None:\r\n            return to_qvariant(int(section))\r\n        else:\r\n            return to_qvariant(labels[section])", "category": "Python"}, {"instruction": "def equiv_graph(self):\n        \"\"\"\n        Returns\n        -------\n        graph\n            bidirectional networkx graph of all equivalency relations\n        \"\"\"\n", "input": "", "output": "        eg = nx.Graph()\n        for (u,v,d) in self.get_graph().edges(data=True):\n            if d['pred'] == 'equivalentTo':\n                eg.add_edge(u,v)\n        return eg", "category": "Python"}, {"instruction": "def blob(self, request, pk=None):\n        \"\"\"\n        fetch large object from pg and gives it back to user via HTTP 1.1\n        request\n\n        :param request: django request instance\n        :param pk: requested resource primary key\n        :rtype: django.http.HttpResponse\n        :rtype: HttpResponse\n        :return: file with its filename stored in database\n        \"\"\"\n", "input": "", "output": "        obj = self.get_object_or_none()\n        if obj:\n            blob = obj.get_blob_data()\n            content_type = 'octet/stream'\n            response = HttpResponse(blob, content_type=content_type,\n                                    status=status.HTTP_200_OK)\n            response['Content-Disposition'] = (\n                'attachment; filename=\"%s\"' % obj.name\n            )\n            return response\n        return HttpResponse('404', status=status.HTTP_404_NOT_FOUND,\n                            content_type='application/json')", "category": "Python"}, {"instruction": "def save_image(self, image_file):\n        \"\"\"\n        Saves the image file to disk.\n        \"\"\"\n", "input": "", "output": "        self.ensure_pyplot()\n        command = 'plt.gcf().savefig(\"%s\")'%image_file\n        #print 'SAVEFIG', command  # dbg\n        self.process_input_line('bookmark ipy_thisdir', store_history=False)\n        self.process_input_line('cd -b ipy_savedir', store_history=False)\n        self.process_input_line(command, store_history=False)\n        self.process_input_line('cd -b ipy_thisdir', store_history=False)\n        self.process_input_line('bookmark -d ipy_thisdir', store_history=False)\n        self.clear_cout()", "category": "Python"}, {"instruction": "def make_query(catalog):\n    \"\"\"A function to prepare a query\n    \"\"\"\n", "input": "", "output": "    query = {}\n    request = api.get_request()\n    index = get_search_index_for(catalog)\n    limit = request.form.get(\"limit\")\n\n    q = request.form.get(\"q\")\n    if len(q) > 0:\n        query[index] = q + \"*\"\n    else:\n        return None\n\n    portal_type = request.form.get(\"portal_type\")\n    if portal_type:\n        if not isinstance(portal_type, list):\n            portal_type = [portal_type]\n        query[\"portal_type\"] = portal_type\n\n    if limit and limit.isdigit():\n        query[\"sort_limit\"] = int(limit)\n\n    return query", "category": "Python"}, {"instruction": "def get(_class, api, rt):\n        \"\"\"\n        Return a Route object for route `rt` using API instance `api`.\n        \"\"\"\n", "input": "", "output": "             \n        if not _class.all_routes:\n            _class.all_routes = _class.update_list(api, api.routes()['route'])\n\n        return _class.all_routes[str(rt)]", "category": "Python"}, {"instruction": "def wrap (text, width, **kwargs):\n    \"\"\"Adjust lines of text to be not longer than width. The text will be\n    returned unmodified if width <= 0.\n    See textwrap.wrap() for a list of supported kwargs.\n    Returns text with lines no longer than given width.\"\"\"\n", "input": "", "output": "    if width <= 0 or not text:\n        return text\n    ret = []\n    for para in get_paragraphs(text):\n        text = \" \".join(para.strip().split())\n        ret.extend(textwrap.wrap(text, width, **kwargs))\n    return os.linesep.join(ret)", "category": "Python"}, {"instruction": "async def role(self, *args, **kwargs):\n        \"\"\"\n        Get Role\n\n        Get information about a single role, including the set of scopes that the\n        role expands to.\n\n        This method gives output: ``v1/get-role-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n", "input": "", "output": "\n        return await self._makeApiCall(self.funcinfo[\"role\"], *args, **kwargs)", "category": "Python"}, {"instruction": "def get_drake_data(steps):\n    \"\"\"\n    Returns: a dictionary of outputs mapped to inputs\n    Note that an output is either a target or a leaf node in the\n        step tree\n    \"\"\"\n", "input": "", "output": "    output_inputs = {}\n    if len(steps) == 0:\n        return output_inputs\n\n    for step in steps:\n        output_inputs[step] = get_inputs(step, target=True)\n\n    # recursively do the same for all the inputs\n    inputs = set(itertools.chain(*output_inputs.values()))\n    o = get_drake_data(inputs)\n    output_inputs.update(o)\n\n    return output_inputs", "category": "Python"}, {"instruction": "def node_copy(node, nodefactory=Node):\n    \"\"\"Make a deep copy of the node\"\"\"\n", "input": "", "output": "    return nodefactory(node.tag, node.attrib.copy(), node.text,\n                       [node_copy(n, nodefactory) for n in node])", "category": "Python"}, {"instruction": "def _load(self):\n        \"\"\"Read through the entire archive file and look for readable\n           members.\n        \"\"\"\n", "input": "", "output": "        while True:\n            tarinfo = self.next()\n            if tarinfo is None:\n                break\n        self._loaded = True", "category": "Python"}, {"instruction": "def _search_path(self, directory_name, filename):\n        \"\"\"Searches for a given file in the specified directory.\"\"\"\n", "input": "", "output": "        full_path = path.join(directory_name, filename)\n        if path.exists(full_path):\n            return full_path\n\n        # Could not find the requested file in any of the directories\n        return None", "category": "Python"}, {"instruction": "def get_curve(self, mnemonic, alias=None):\n        \"\"\"\n        Wraps get_mnemonic.\n\n        Instead of picking curves by name directly from the data dict, you\n        can pick them up with this method, which takes account of the alias\n        dict you pass it. If you do not pass an alias dict, then you get the\n        curve you asked for, if it exists, or None. NB Wells do not have alias\n        dicts, but Projects do.\n\n        Args:\n            mnemonic (str): the name of the curve you want.\n            alias (dict): an alias dictionary, mapping mnemonics to lists of\n                mnemonics.\n\n        Returns:\n            Curve.\n        \"\"\"\n", "input": "", "output": "        return self.data.get(self.get_mnemonic(mnemonic, alias=alias), None)", "category": "Python"}, {"instruction": "def tr(self, subdomain: str, string_to_translate: str = \"\") -> str:\n        \"\"\"Returns translation of string passed.\n\n        :param str subdomain: subpart of strings dictionary.\n         Must be one of self.translations.keys() i.e. 'restrictions'\n        :param str string_to_translate: string you want to translate\n        \"\"\"\n", "input": "", "output": "        if subdomain not in self.translations.keys():\n            raise ValueError(\n                \"'{}' is not a correct subdomain.\"\n                \" Must be one of {}\".format(subdomain, self.translations.keys())\n            )\n        else:\n            pass\n        # translate\n        str_translated = self.translations.get(\n            subdomain, {\"error\": \"Subdomain not found: {}\".format(subdomain)}\n        ).get(string_to_translate, \"String not found\")\n\n        # end of method\n        return str_translated", "category": "Python"}, {"instruction": "def read(self, size=None):\n        \"\"\"Read a chunk from rfile buffer and return it.\n\n        Args:\n            size (int): amount of data to read\n\n        Returns:\n            bytes: Chunk from rfile, limited by size if specified.\n\n        \"\"\"\n", "input": "", "output": "        data = EMPTY\n\n        if size == 0:\n            return data\n\n        while True:\n            if size and len(data) >= size:\n                return data\n\n            if not self.buffer:\n                self._fetch()\n                if not self.buffer:\n                    # EOF\n                    return data\n\n            if size:\n                remaining = size - len(data)\n                data += self.buffer[:remaining]\n                self.buffer = self.buffer[remaining:]\n            else:\n                data += self.buffer\n                self.buffer = EMPTY", "category": "Python"}, {"instruction": "def __Languages_comboBox_set_default_view_state(self):\n        \"\"\"\n        Sets the **Languages_comboBox** Widget default View state.\n        \"\"\"\n", "input": "", "output": "\n        if not self.__container.has_editor_tab():\n            return\n\n        editor = self.__container.get_current_editor()\n        index = self.Languages_comboBox.findText(editor.language.name)\n\n        self.Languages_comboBox.setCurrentIndex(index)", "category": "Python"}, {"instruction": "def delete(self, request, bot_id, id, format=None):\n        \"\"\"\n        Delete existing handler\n        ---\n        responseMessages:\n            - code: 401\n              message: Not authenticated\n        \"\"\"\n", "input": "", "output": "        return super(HandlerDetail, self).delete(request, bot_id, id, format)", "category": "Python"}, {"instruction": "def get_queue_name(queue_name):\n  \"\"\"Determine which queue MR should run on.\n\n  How to choose the queue:\n  1. If user provided one, use that.\n  2. If we are starting a mr from taskqueue, inherit that queue.\n     If it's a special queue, fall back to the default queue.\n  3. Default queue.\n\n  If user is using any MR pipeline interface, pipeline.start takes a\n  \"queue_name\" argument. The pipeline will run on that queue and MR will\n  simply inherit the queue_name.\n\n  Args:\n    queue_name: queue_name from user. Maybe None.\n\n  Returns:\n    The queue name to run on.\n  \"\"\"\n", "input": "", "output": "  if queue_name:\n    return queue_name\n  queue_name = os.environ.get(\"HTTP_X_APPENGINE_QUEUENAME\",\n                              parameters.config.QUEUE_NAME)\n  if len(queue_name) > 1 and queue_name[0:2] == \"__\":\n    # We are currently in some special queue. E.g. __cron.\n    return parameters.config.QUEUE_NAME\n  else:\n    return queue_name", "category": "Python"}, {"instruction": "def log_user_in(app_id, token, ticket, response, cookie_name='user',\n                url_detail='https://pswdless.appspot.com/rest/detail'):\n    '''\n    Log the user in setting the user data dictionary in cookie\n    Returns a command that execute the logic\n    '''\n", "input": "", "output": "    return LogUserIn(app_id, token, ticket, response, cookie_name, url_detail)", "category": "Python"}, {"instruction": "def get_contents(self, origin):\n        \"\"\"\n        Try to load the origin.\n        \"\"\"\n", "input": "", "output": "        try:\n            path = self.get_app_template_path(\n                origin.app_name, origin.template_name)\n            with io.open(path, encoding=self.engine.file_charset) as fp:\n                return fp.read()\n        except KeyError:\n            raise TemplateDoesNotExist(origin)\n        except IOError as error:\n            if error.errno == errno.ENOENT:\n                raise TemplateDoesNotExist(origin)\n            raise", "category": "Python"}, {"instruction": "def name_without_zeroes(name):\n    \"\"\"\n    Return a human-readable name without LSDJ's trailing zeroes.\n\n    :param name: the name from which to strip zeroes\n    :rtype: the name, without trailing zeroes\n    \"\"\"\n", "input": "", "output": "    first_zero = name.find(b'\\0')\n\n    if first_zero == -1:\n        return name\n    else:\n        return str(name[:first_zero])", "category": "Python"}, {"instruction": "def rollbackBlockUser(self, userId, chatroomId):\n        \"\"\"\n        \u79fb\u9664\u5c01\u7981\u804a\u5929\u5ba4\u6210\u5458\u65b9\u6cd5 \u65b9\u6cd5\n        @param  userId:\u7528\u6237 Id\u3002\uff08\u5fc5\u4f20\uff09\n        @param  chatroomId:\u804a\u5929\u5ba4 Id\u3002\uff08\u5fc5\u4f20\uff09\n\t \n        @return code:\u8fd4\u56de\u7801\uff0c200 \u4e3a\u6b63\u5e38\u3002\n        @return errorMessage:\u9519\u8bef\u4fe1\u606f\u3002\n\t    \"\"\"\n", "input": "", "output": "\n        desc = {\n            \"name\": \"CodeSuccessReslut\",\n            \"desc\": \" http \u6210\u529f\u8fd4\u56de\u7ed3\u679c\",\n            \"fields\": [{\n                \"name\": \"code\",\n                \"type\": \"Integer\",\n                \"desc\": \"\u8fd4\u56de\u7801\uff0c200 \u4e3a\u6b63\u5e38\u3002\"\n            }, {\n                \"name\": \"errorMessage\",\n                \"type\": \"String\",\n                \"desc\": \"\u9519\u8bef\u4fe1\u606f\u3002\"\n            }]\n        }\n        r = self.call_api(\n            method=('API', 'POST', 'application/x-www-form-urlencoded'),\n            action='/chatroom/user/block/rollback.json',\n            params={\"userId\": userId,\n                    \"chatroomId\": chatroomId})\n        return Response(r, desc)", "category": "Python"}, {"instruction": "def _split_url_string(query_string):\n        \"\"\"\n        Turns a `query_string` into a Python dictionary with unquoted values\n        \"\"\"\n", "input": "", "output": "        parameters = parse_qs(to_utf8(query_string), keep_blank_values=True)\n        for k, v in parameters.iteritems():\n            parameters[k] = urllib.unquote(v[0])\n        return parameters", "category": "Python"}, {"instruction": "def run_process(self, slug, inputs):\n        \"\"\"Run a new process from a running process.\"\"\"\n", "input": "", "output": "        def export_files(value):\n            ", "category": "Python"}, {"instruction": "def deepcopy_strip(item):  # type: (Any) -> Any\n    \"\"\"\n    Make a deep copy of list and dict objects.\n\n    Intentionally do not copy attributes.  This is to discard CommentedMap and\n    CommentedSeq metadata which is very expensive with regular copy.deepcopy.\n    \"\"\"\n", "input": "", "output": "\n    if isinstance(item, MutableMapping):\n        return {k: deepcopy_strip(v) for k, v in iteritems(item)}\n    if isinstance(item, MutableSequence):\n        return [deepcopy_strip(k) for k in item]\n    return item", "category": "Python"}, {"instruction": "def loads(s):\n    \"\"\"\n    xdis.marshal.load() but with its dispatch load_code() function replaced\n    with our decoding version.\n    \"\"\"\n", "input": "", "output": "    um = xmarshal._FastUnmarshaller(s)\n    um.dispatch[xmarshal.TYPE_CODE] = load_code\n    return um.load()", "category": "Python"}, {"instruction": "def all(self, data={}, **kwargs):\n        \"\"\"\"\n        Fetch all Subscription entities\n\n        Returns:\n            Dictionary of Subscription data\n        \"\"\"\n", "input": "", "output": "        return super(Subscription, self).all(data, **kwargs)", "category": "Python"}, {"instruction": "def persist(self, name):\n        \"\"\"\n        clear any expiration TTL set on the object\n\n        :param name: str     the name of the redis key\n        :return: Future()\n        \"\"\"\n", "input": "", "output": "        with self.pipe as pipe:\n            return pipe.persist(self.redis_key(name))", "category": "Python"}, {"instruction": "def rows(self):\n        \"\"\"\n        Returns a list of dicts.\n        \"\"\"\n", "input": "", "output": "        rows = []\n        for rowName in self.rowNames:\n            row = {columnName: self[rowName, columnName] for columnName in self.columnNames}\n            row[\"_\"] = rowName\n            rows.append(row)\n        return rows", "category": "Python"}, {"instruction": "def Units(uname):\n    \"\"\"Generate a unit object.\n\n    Parameters\n    ----------\n    uname : str\n        Wavelength or flux unit name.\n\n    Returns\n    -------\n    unit : `BaseUnit` or `None`\n        Unit object. `None` means unitless.\n\n    Raises\n    ------\n    ValueError\n        Unknown unit name.\n\n    \"\"\"\n", "input": "", "output": "    if isinstance(uname,BaseUnit):\n        return uname\n    else:\n        try:\n            if issubclass(uname,BaseUnit):\n                return uname()\n        except TypeError:\n\n            try:\n                return factory(uname)\n            except KeyError:\n                if uname == str(None):\n                    return None\n                else:\n                    raise ValueError(\"Unknown units %s\"%uname)", "category": "Python"}, {"instruction": "def select_area(self, area_uuid):\n        \"\"\"\n        Update the \"current area\" to be the area with this UUID.\n\n        :param str area_uuid: The RFC4122-compliant UUID of the Upload Area.\n        \"\"\"\n", "input": "", "output": "\n        self._config.upload.current_area = area_uuid\n        self.save()", "category": "Python"}, {"instruction": "def sdiffstore(self, destkey, key, *keys):\n        \"\"\"Subtract multiple sets and store the resulting set in a key.\"\"\"\n", "input": "", "output": "        return self.execute(b'SDIFFSTORE', destkey, key, *keys)", "category": "Python"}, {"instruction": "def normalize(self, dt):\n        \"\"\"\n        Clamp dt to every Nth :py:attr:`~_DatetimeParameterBase.interval` starting at\n        :py:attr:`~_DatetimeParameterBase.start`.\n        \"\"\"\n", "input": "", "output": "        if dt is None:\n            return None\n\n        dt = self._convert_to_dt(dt)\n\n        dt = dt.replace(microsecond=0)  # remove microseconds, to avoid float rounding issues.\n        delta = (dt - self.start).total_seconds()\n        granularity = (self._timedelta * self.interval).total_seconds()\n        return dt - datetime.timedelta(seconds=delta % granularity)", "category": "Python"}, {"instruction": "def annotate_variant(variant, var_obj=None):\n    \"\"\"Annotate a cyvcf variant with observations\n    \n    Args:\n        variant(cyvcf2.variant)\n        var_obj(dict)\n    \n    Returns:\n        variant(cyvcf2.variant): Annotated variant\n    \"\"\"\n", "input": "", "output": "    if var_obj:\n    \n        variant.INFO['Obs'] = var_obj['observations']\n        if var_obj.get('homozygote'):\n            variant.INFO['Hom'] = var_obj['homozygote']\n        if var_obj.get('hemizygote'):\n            variant.INFO['Hem'] = var_obj['hemizygote']\n    \n    return variant", "category": "Python"}, {"instruction": "def set_default_by_index(self, index):\n        \"\"\" Set the default dataset by its index.\n\n        After changing the default dataset, all calls without explicitly specifying the\n        dataset by index or alias will be redirected to this dataset.\n\n        Args:\n            index (int): The index of the dataset that should be made the default.\n\n        Raises:\n            DataInvalidIndex: If the index does not represent a valid dataset.\n        \"\"\"\n", "input": "", "output": "        if index >= len(self._datasets):\n            raise DataInvalidIndex('A dataset with index {} does not exist'.format(index))\n\n        self._default_index = index", "category": "Python"}, {"instruction": "def load(raw_bytes):\n        \"\"\"\n        given a bytes object, should return a base python data\n        structure that represents the object.\n        \"\"\"\n", "input": "", "output": "        try:\n            if not isinstance(raw_bytes, string_type):\n                raw_bytes = raw_bytes.decode()\n            return json.loads(raw_bytes)\n        except ValueError as e:\n            raise SerializationException(str(e))", "category": "Python"}, {"instruction": "def _load_types(root):\n    \"\"\"Returns {name: Type}\"\"\"\n", "input": "", "output": "    def text(t):\n        if t.tag == 'name':\n            return '{name}'\n        elif t.tag == 'apientry':\n            return '{apientry}'\n        out = []\n        if t.text:\n            out.append(_escape_tpl_str(t.text))\n        for x in t:\n            out.append(text(x))\n            if x.tail:\n                out.append(_escape_tpl_str(x.tail))\n        return ''.join(out)\n    out_dict = collections.OrderedDict()\n    for elem in root.findall('types/type'):\n        name = elem.get('name') or elem.find('name').text\n        template = text(elem)\n        api = elem.get('api')\n        if 'requires' in elem.attrib:\n            required_types = set((elem.attrib['requires'],))\n        else:\n            required_types = set()\n        comment = elem.get('comment')\n        if api:\n            k = (name, api)\n        else:\n            k = (name, None)\n        out_dict[k] = Type(name, template, required_types, api, comment)\n    return out_dict", "category": "Python"}, {"instruction": "def parse_node_response(self, response):\n        \"\"\"\n        Update the object with the remote node object\n        \"\"\"\n", "input": "", "output": "        for key, value in response.items():\n            if key == \"console\":\n                self._console = value\n            elif key == \"node_directory\":\n                self._node_directory = value\n            elif key == \"command_line\":\n                self._command_line = value\n            elif key == \"status\":\n                self._status = value\n            elif key == \"console_type\":\n                self._console_type = value\n            elif key == \"name\":\n                self.name = value\n            elif key in [\"node_id\", \"project_id\", \"console_host\",\n                         \"startup_config_content\",\n                         \"private_config_content\",\n                         \"startup_script\"]:\n                if key in self._properties:\n                    del self._properties[key]\n            else:\n                self._properties[key] = value\n        self._list_ports()\n        for link in self._links:\n            yield from link.node_updated(self)", "category": "Python"}, {"instruction": "def register_adapter(from_classes, to_classes, func):\n    \"\"\"\n    Register a function that can handle adapting from `from_classes` to `to_classes`.\n    \"\"\"\n", "input": "", "output": "    assert from_classes, 'Must supply classes to adapt from'\n    assert to_classes, 'Must supply classes to adapt to'\n    assert func, 'Must supply adapter function'\n\n    if not isinstance(from_classes, (tuple, list)):\n        from_classes = [from_classes]\n    if not isinstance(to_classes, (tuple, list)):\n        to_classes = [to_classes]\n\n    for key in itertools.product(from_classes, to_classes):\n        if key in __adapters__:\n            raise AdapterExists('%r to %r already exists.' % key)\n        __adapters__[key] = func", "category": "Python"}, {"instruction": "def _calc_relative_path_lengths(self, x, y):\n        \"\"\"Determine the relative path length at each x,y position.\"\"\"\n", "input": "", "output": "\n        path_lengths = np.sqrt(np.diff(x) ** 2 + np.diff(y) ** 2)\n        total_length = np.sum(path_lengths)\n        cummulative_lengths = np.cumsum(path_lengths)\n        relative_path_lengths = cummulative_lengths / total_length\n        return relative_path_lengths", "category": "Python"}, {"instruction": "def get_snmp_service(self):\n        \"\"\"\n        Enable/Disable snmp\n        :param snmp_parameters:\n        :return:\n        :rtype: SnmpContextManager\n        \"\"\"\n", "input": "", "output": "        return SnmpContextManager(self.enable_flow, self.disable_flow, self._snmp_parameters, self._logger)", "category": "Python"}, {"instruction": "def is_never_accessible(self):\n        \"\"\" Returns true if the course/task is never accessible \"\"\"\n", "input": "", "output": "        return self._val[0] == datetime.max and self._val[1] == datetime.max", "category": "Python"}, {"instruction": "def collect(self):\n        \"\"\"\n        Return a list that contains all of the elements in this RDD.\n\n        .. note:: This method should only be used if the resulting array is expected\n            to be small, as all the data is loaded into the driver's memory.\n        \"\"\"\n", "input": "", "output": "        with SCCallSiteSync(self.context) as css:\n            sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n        return list(_load_from_socket(sock_info, self._jrdd_deserializer))", "category": "Python"}, {"instruction": "def get_logs(self, request):\r\n        \"\"\" Get logs from log service.\r\n        Unsuccessful opertaion will cause an LogException.\r\n        Note: for larger volume of data (e.g. > 1 million logs), use get_log_all\r\n\r\n        :type request: GetLogsRequest\r\n        :param request: the GetLogs request parameters class.\r\n        \r\n        :return: GetLogsResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\n", "input": "", "output": "        project = request.get_project()\r\n        logstore = request.get_logstore()\r\n        from_time = request.get_from()\r\n        to_time = request.get_to()\r\n        topic = request.get_topic()\r\n        query = request.get_query()\r\n        reverse = request.get_reverse()\r\n        offset = request.get_offset()\r\n        size = request.get_line()\r\n\r\n        return self.get_log(project, logstore, from_time, to_time, topic,\r\n                            query, reverse, offset, size)", "category": "Python"}, {"instruction": "def tryForwarding(self, request: Request):\n        \"\"\"\n        Try to forward the request if the required conditions are met.\n        See the method `canForward` for the conditions to check before\n        forwarding a request.\n        \"\"\"\n", "input": "", "output": "        cannot_reason_msg = self.canForward(request)\n        if cannot_reason_msg is None:\n            # If haven't got the client request(REQUEST) for the corresponding\n            # propagate request(PROPAGATE) but have enough propagate requests\n            # to move ahead\n            self.forward(request)\n        else:\n            logger.trace(\"{} not forwarding request {} to its replicas \"\n                         \"since {}\".format(self, request, cannot_reason_msg))", "category": "Python"}, {"instruction": "def optimize(model,cand,obj):\n    \"\"\"optimize: function for solving the model, updating candidate solutions' list\n    Parameters:\n        - model: Gurobi model object\n        - cand: list of pairs of objective functions (for appending more solutions)\n        - obj: name of a model's variable to setup as objective\n    Returns the solver's exit status\n    \"\"\"\n", "input": "", "output": "    # model.Params.OutputFlag = 0 # silent mode\n    model.setObjective(obj,\"minimize\")\n\n    model.optimize()\n    x,y,C,U = model.data\n    status = model.getStatus()\n    if status == \"optimal\" or status == \"bestsollimit\": # todo GRB.Status.SUBOPTIMAL:\n        sols = model.getSols()\n        for sol in sols:\n            cand.append((model.getVal(var=U,solution=sol),model.getVal(var=C,solution=sol)))\n\n     #   for k in range(model.SolCount):\n     #       model.Params.SolutionNumber = k\n     #       cand.append(model.getVal(U),model.getVal(C))\n    return status", "category": "Python"}, {"instruction": "def delete(self, revoke: bool = True):\n        \"\"\"Bound method *delete* of :obj:`Message <pyrogram.Message>`.\n\n        Use as a shortcut for:\n\n        .. code-block:: python\n\n            client.delete_messages(\n                chat_id=chat_id,\n                message_ids=message.message_id\n            )\n\n        Example:\n            .. code-block:: python\n\n                message.delete()\n\n        Args:\n            revoke (``bool``, *optional*):\n                Deletes messages on both parts.\n                This is only for private cloud chats and normal groups, messages on\n                channels and supergroups are always revoked (i.e.: deleted for everyone).\n                Defaults to True.\n\n        Returns:\n            True on success, False otherwise.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>`\n        \"\"\"\n", "input": "", "output": "        return self._client.delete_messages(\n            chat_id=self.chat.id,\n            message_ids=self.message_id,\n            revoke=revoke\n        )", "category": "Python"}, {"instruction": "def plot_bargraph (self, data, cats=None, pconfig=None):\n        \"\"\" Depreciated function. Forwards to new location. \"\"\"\n", "input": "", "output": "        from multiqc.plots import bargraph\n        if pconfig is None:\n            pconfig = {}\n        return bargraph.plot(data, cats, pconfig)", "category": "Python"}, {"instruction": "def read_fmt(fmt, fp):\n    \"\"\"\n    Reads data from ``fp`` according to ``fmt``.\n    \"\"\"\n", "input": "", "output": "    fmt = str(\">\" + fmt)\n    fmt_size = struct.calcsize(fmt)\n    data = fp.read(fmt_size)\n    assert len(data) == fmt_size, 'read=%d, expected=%d' % (\n        len(data), fmt_size\n    )\n    return struct.unpack(fmt, data)", "category": "Python"}, {"instruction": "def coarsen_all_traces(level=2, exponential=False, axes=\"all\", figure=None):\n    \"\"\"\n    This function does nearest-neighbor coarsening of the data. See \n    spinmob.fun.coarsen_data for more information.\n    \n    Parameters\n    ----------\n    level=2\n        How strongly to coarsen.\n    exponential=False\n        If True, use the exponential method (great for log-x plots).\n    axes=\"all\"\n        Which axes to coarsen.\n    figure=None\n        Which figure to use.\n    \n    \"\"\"\n", "input": "", "output": "    if axes==\"gca\": axes=_pylab.gca()\n    if axes==\"all\":\n        if not figure: f = _pylab.gcf()\n        axes = f.axes\n\n    if not _fun.is_iterable(axes): axes = [axes]\n\n    for a in axes:\n        # get the lines from the plot\n        lines = a.get_lines()\n\n        # loop over the lines and trim the data\n        for line in lines:\n            if isinstance(line, _mpl.lines.Line2D):\n                coarsen_line(line, level, exponential, draw=False)\n    _pylab.draw()", "category": "Python"}, {"instruction": "def _temp_bool_prop(propname, doc=\"\", default=False):\n    \"\"\"Creates a property that uses the :class:`_TempBool` class\n\n    Parameters\n    ----------\n    propname: str\n        The attribute name to use. The _TempBool instance will be stored in the\n        ``'_' + propname`` attribute of the corresponding instance\n    doc: str\n        The documentation of the property\n    default: bool\n        The default value of the _TempBool class\"\"\"\n", "input": "", "output": "    def getx(self):\n        if getattr(self, '_' + propname, None) is not None:\n            return getattr(self, '_' + propname)\n        else:\n            setattr(self, '_' + propname, _TempBool(default))\n        return getattr(self, '_' + propname)\n\n    def setx(self, value):\n        getattr(self, propname).value = bool(value)\n\n    def delx(self):\n        getattr(self, propname).value = default\n\n    return property(getx, setx, delx, doc)", "category": "Python"}, {"instruction": "def getWidget(self):\n        \"\"\"Some ideas for your widget:\n        - Textual information (alert, license place number)\n        - Check boxes : if checked, send e-mail to your mom when the analyzer spots something\n        - .. or send an sms to yourself\n        - You can include the cv2.imshow window to the widget to see how the analyzer proceeds\n        \"\"\"\n", "input": "", "output": "        widget = QtWidgets.QLabel(\"NO MOVEMENT YET\")\n        widget.setStyleSheet(style.detector_test)\n        self.signals.start_move.connect(lambda : widget.setText(\"MOVEMENT START\"))\n        self.signals.stop_move. connect(lambda : widget.setText(\"MOVEMENT STOP\"))\n        return widget", "category": "Python"}, {"instruction": "def declare(self, symbol):\n        \"\"\"\n        Nothing gets declared here - it's the parents problem, except\n        for the case where the symbol is the one we have here.\n        \"\"\"\n", "input": "", "output": "\n        if symbol != self.catch_symbol:\n            self.parent.declare(symbol)", "category": "Python"}, {"instruction": "def nickserv_identify(self, password, use_nick=None):\n        \"\"\"Identify to NickServ (legacy).\"\"\"\n", "input": "", "output": "        if self.ready:\n            if use_nick:\n                self.msg(use_nick, 'IDENTIFY {}'.format(password))\n            else:\n                self.send('NICKSERV', params=['IDENTIFY', password])\n        else:\n            self.connect_info['nickserv'] = {\n                'password': password,\n                'use_nick': use_nick,\n            }", "category": "Python"}, {"instruction": "async def find(self, **kwargs):\n\t\t\"\"\"Find all entries with given search key.\n\t\tAccepts named parameter key and arbitrary values.\n\t\tReturns list of entry id`s.\n\n\t\tfind(**kwargs) => document (if exist)\n\t\tfind(**kwargs) => {\"error\":404,\"reason\":\"Not found\"} (if does not exist)\n\t\tfind() => {\"error\":400, \"reason\":\"Missed required fields\"}\n\t\t\"\"\"\n", "input": "", "output": "\t\tif not isinstance(kwargs, dict) and len(kwargs) != 1:\n\t\t\treturn {\"error\":400, \n\t\t\t\t\t\"reason\":\"Bad request\"}\n\t\tdocument = await self.collection.find_one(kwargs)\n\t\tif document:\n\t\t\treturn document\n\t\telse:\n\t\t\treturn {\"error\":404, \"reason\":\"Not found\"}", "category": "Python"}, {"instruction": "def get(cls, community_id, record_uuid):\n        \"\"\"Get an inclusion request.\"\"\"\n", "input": "", "output": "        return cls.query.filter_by(\n            id_record=record_uuid, id_community=community_id\n        ).one_or_none()", "category": "Python"}, {"instruction": "def transform_array(rot_mtx,vec_array):\n\n    '''transform_array( matrix, vector_array ) -> vector_array\n\n    '''\n", "input": "", "output": "\n    return map( lambda x,m=rot_mtx:transform(m,x), vec_array )", "category": "Python"}, {"instruction": "def _adjust_overlap(positions_list, index, direction):\n    '''Increase overlap to the right or left of an index.\n\n    :param positions_list: list of overlap positions\n    :type positions_list: list\n    :param index: index of the overlap to increase.\n    :type index: int\n    :param direction: which side of the overlap to increase - left or right.\n    :type direction: str\n    :returns: A list of overlap positions (2-element lists)\n    :rtype: list\n    :raises: ValueError if direction isn't \\'left\\' or \\'right\\'.\n\n    '''\n", "input": "", "output": "    if direction == 'left':\n        positions_list[index + 1] -= 1\n    elif direction == 'right':\n        positions_list[index] += 1\n    else:\n        raise ValueError('direction must be \\'left\\' or \\'right\\'.')\n    return positions_list", "category": "Python"}, {"instruction": "def get_asset_ddos():\n    \"\"\"Get DDO of all assets.\n    ---\n    tags:\n      - ddo\n    responses:\n      200:\n        description: successful action\n    \"\"\"\n", "input": "", "output": "    args = []\n    query = dict()\n    args.append(query)\n    assets_with_id = dao.get_all_listed_assets()\n    assets_metadata = {a['id']: a for a in assets_with_id}\n    for i in assets_metadata:\n        _sanitize_record(i)\n    return Response(json.dumps(assets_metadata, default=_my_converter), 200,\n                    content_type='application/json')", "category": "Python"}, {"instruction": "def _get_key_location(self, key) -> (int, int):\n        \"\"\"\n        Return chunk no and 1-based offset of key\n        :param key:\n        :return:\n        \"\"\"\n", "input": "", "output": "        key = int(key)\n        if key == 0:\n            return 1, 0\n        remainder = key % self.chunkSize\n        addend = ChunkedFileStore.firstChunkIndex\n        chunk_no = key - remainder + addend if remainder \\\n            else key - self.chunkSize + addend\n        offset = remainder or self.chunkSize\n        return chunk_no, offset", "category": "Python"}, {"instruction": "def plot(self, bins=250, **kwargs):\n        \"\"\"An example plot function. You have to subclass this method.\"\"\"\n", "input": "", "output": "        # Data #\n        counts = [sum(map(len, b.contigs)) for b in self.parent.bins]\n        # Linear bins in logarithmic space #\n        if 'log' in kwargs.get('x_scale', ''):\n            start, stop = numpy.log10(1), numpy.log10(max(counts))\n            bins = list(numpy.logspace(start=start, stop=stop, num=bins))\n            bins.insert(0, 0)\n        # Plot #\n        fig = pyplot.figure()\n        pyplot.hist(counts, bins=bins, color='gray')\n        axes = pyplot.gca()\n        # Information #\n        title = 'Distribution of the total nucleotide count in the bins'\n        axes.set_title(title)\n        axes.set_xlabel('Number of nucleotides in a bin')\n        axes.set_ylabel('Number of bins with that many nucleotides in them')\n        # Save it #\n        self.save_plot(fig, axes, **kwargs)\n        pyplot.close(fig)\n        # For convenience #\n        return self", "category": "Python"}, {"instruction": "def is_pyclustering_instance(model):\n        \"\"\"\n        Checks if the clustering.rst algorithm belongs to pyclustering\n\n        :param model: the clustering.rst algorithm model\n        :return: the truth value (Boolean)\n        \"\"\"\n", "input": "", "output": "        return any(isinstance(model, i) for i in [xmeans, clarans, rock, optics])", "category": "Python"}, {"instruction": "def _format_dates(self, start, end):\n        \"\"\"Format start and end dates.\"\"\"\n", "input": "", "output": "        start = self._split_date(start)\n        end = self._split_date(end)\n        return start, end", "category": "Python"}, {"instruction": "def _cast_boolean(value):\n    \"\"\"\n    Helper to convert config values to boolean as ConfigParser do.\n    \"\"\"\n", "input": "", "output": "    _BOOLEANS = {'1': True, 'yes': True, 'true': True, 'on': True,\n                 '0': False, 'no': False, 'false': False, 'off': False, '': False}\n    value = str(value)\n    if value.lower() not in _BOOLEANS:\n        raise ValueError('Not a boolean: %s' % value)\n\n    return _BOOLEANS[value.lower()]", "category": "Python"}, {"instruction": "def check_next_match(self, match, new_relations, subject_graph, one_match):\n        \"\"\"Check if the (onset for a) match can be a valid (part of a) ring\"\"\"\n", "input": "", "output": "        # avoid duplicate rings (order of traversal)\n        if len(match) == 3:\n            if match.forward[1] < match.forward[2]:\n                #print \"RingPattern.check_next_match: duplicate order\", match.forward[1], match.forward[2]\n                return False\n        # avoid duplicate rings (starting point)\n        for vertex1 in new_relations.values():\n            if vertex1 < match.forward[0]:\n                #print \"RingPattern.check_next_match: duplicate start\", vertex1, match.forward[0]\n                return False\n        # can this ever become a strong ring?\n        for vertex1 in new_relations.values():\n            paths = list(subject_graph.iter_shortest_paths(vertex1, match.forward[0]))\n            if len(paths) != 1:\n                #print \"RingPattern.check_next_match: not strong 1\"\n                return False\n            if len(paths[0]) != (len(match)+1)//2:\n                #print \"RingPattern.check_next_match: not strong 2\"\n                return False\n        return True", "category": "Python"}, {"instruction": "def significant_format(number, decimal_sep='.', thousand_sep=',', n=3):\r\n    \"\"\"Format a number according to a given number of significant figures.\r\n    \"\"\"\n", "input": "", "output": "    str_number = significant(number, n)\r\n\r\n    # sign\r\n    if float(number) < 0:\r\n        sign = '-'\r\n    else:\r\n        sign = ''\r\n\r\n    if str_number[0] == '-':\r\n        str_number = str_number[1:]\r\n    if '.' in str_number:\r\n        int_part, dec_part = str_number.split('.')\r\n    else:\r\n        int_part, dec_part = str_number, ''\r\n    if dec_part:\r\n        dec_part = decimal_sep + dec_part\r\n    if thousand_sep:\r\n        int_part_gd = ''\r\n        for cnt, digit in enumerate(int_part[::-1]):\r\n            if cnt and not cnt % 3:\r\n                int_part_gd += thousand_sep\r\n            int_part_gd += digit\r\n        int_part = int_part_gd[::-1]\r\n    return sign + int_part + dec_part", "category": "Python"}, {"instruction": "def release_eip_address(public_ip=None, allocation_id=None, region=None, key=None,\n                        keyid=None, profile=None):\n    '''\n    Free an Elastic IP address.  Pass either a public IP address to release an\n    EC2 Classic EIP, or an AllocationId to release a VPC EIP.\n\n    public_ip\n        (string) - The public IP address - for EC2 elastic IPs.\n    allocation_id\n        (string) - The Allocation ID - for VPC elastic IPs.\n\n    returns\n        (bool) - True on success, False on failure\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_ec2.release_eip_address allocation_id=eipalloc-ef382c8a\n\n    .. versionadded:: 2016.3.0\n    '''\n", "input": "", "output": "    if not salt.utils.data.exactly_one((public_ip, allocation_id)):\n        raise SaltInvocationError(\"Exactly one of 'public_ip' OR \"\n                                  \"'allocation_id' must be provided\")\n\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n\n    try:\n        return conn.release_address(public_ip, allocation_id)\n    except boto.exception.BotoServerError as e:\n        log.error(e)\n        return False", "category": "Python"}, {"instruction": "def discoverTokenEndpoints(domain, content=None, look_in={'name': 'link'}, test_urls=True, validateCerts=True):\n    \"\"\"Find the token for the given domain.\n    Only scan html element matching all criteria in look_in.\n\n    optionally the content to be scanned can be given as an argument.\n\n    :param domain: the URL of the domain to handle\n    :param content: the content to be scanned for the endpoint\n    :param look_in: dictionary with name, id and class_. only element matching all of these will be scanned\n    :param test_urls: optional flag to test URLs for validation\n    :param validateCerts: optional flag to enforce HTTPS certificates if present\n    :rtype: list of endpoints\n    \"\"\"\n", "input": "", "output": "    return discoverEndpoint(domain, ('token_endpoint',), content, look_in, test_urls, validateCerts)", "category": "Python"}, {"instruction": "def fix_linkdate(self, entry):\n        \"\"\"\n        Give a date for the entry, depending on feed.sync_by_date\n        Save it as feed.linkdate\n        \"\"\"\n", "input": "", "output": "        if self.sync_by_date:\n            try:\n                entry.linkdate = list(entry.published_parsed)\n                self.linkdate = list(entry.published_parsed)\n            except (AttributeError, TypeError):\n                try:\n                    entry.linkdate = list(entry.updated_parsed)\n                    self.linkdate = list(entry.updated_parsed)\n                except (AttributeError, TypeError):\n                    print((\"This entry doesn't seem to have a parseable date. \"\n                           \"I will use your local time instead.\"),\n                          file=sys.stderr, flush=True)\n                    entry.linkdate = list(time.localtime())\n                    self.linkdate = list(time.localtime())\n        else:\n            entry.linkdate = list(time.localtime())", "category": "Python"}, {"instruction": "def tx_for_tx_hash(self, tx_hash):\n        \"\"\"\n        Get a Tx by its hash.\n        \"\"\"\n", "input": "", "output": "        url = \"%s/rawtx/%s\" % (self.url, b2h_rev(tx_hash))\n        d = urlopen(url).read()\n        j = json.loads(d.decode(\"utf8\"))\n        tx = Tx.from_hex(j.get(\"rawtx\", \"\"))\n        if tx.hash() == tx_hash:\n            return tx", "category": "Python"}, {"instruction": "def segment_common_items(segment_id):\n    \"\"\"\n    Returns all the common items in a segment.\n    \"\"\"\n", "input": "", "output": "    df = data.common_items\n    return (\n        df\n        .loc[str(segment_id)]\n        .reset_index(drop=1)\n        .drop(columns=[\"itemOccurrences\"])\n    )", "category": "Python"}, {"instruction": "def contains_info(self, key, value):\n        \"\"\"\n        Returns how many cards in the deck have the specified value under the\n        specified key in their info data.\n\n        This method requires a library to be stored in the deck instance and\n        will return `None` if there is no library.\n        \"\"\"\n", "input": "", "output": "        if self.library is None:\n            return 0\n\n        load = self.library.load_card\n        matches = 0\n        for code in self.cards:\n            card = load(code)\n            if card.get_info(key) == value:\n                matches += 1\n        return matches", "category": "Python"}, {"instruction": "def unmount_volume_groups(self):\n        \"\"\"Unmounts all volume groups and related loopback devices as identified by :func:`find_volume_groups`\"\"\"\n", "input": "", "output": "\n        for vgname, pvname in self.find_volume_groups():\n            _util.check_output_(['lvchange', '-a', 'n', vgname])\n            _util.check_output_(['losetup', '-d', pvname])", "category": "Python"}, {"instruction": "async def async_discovery(session):\n    \"\"\"Find bridges allowing gateway discovery.\"\"\"\n", "input": "", "output": "    bridges = []\n    response = await async_request(session.get, URL_DISCOVER)\n\n    if not response:\n        _LOGGER.info(\"No discoverable bridges available.\")\n        return bridges\n\n    for bridge in response:\n        bridges.append({'bridgeid': bridge['id'],\n                        'host': bridge['internalipaddress'],\n                        'port': bridge['internalport']})\n\n    _LOGGER.info(\"Discovered the following bridges: %s.\", bridges)\n\n    return bridges", "category": "Python"}, {"instruction": "def send(self, request):\n        \"\"\"\n        Send a SOAP method call\n\n        :param request: :class:`suds.transport.Request <suds.transport.Request>` object\n        :return: :class:`suds.transport.Reply <suds.transport.Reply>` object\n        :rtype: suds.transport.Reply\n        \"\"\"\n", "input": "", "output": "        url = request.url\n        msg = request.message\n        headers = request.headers\n        logger.debug('Sending SOAP request: %s' % url)\n        statsd.incr('soap.send')\n        with statsd.timer('soap.send'):\n            resp = requests.post(url,\n                proxies=self.proxies(url),\n                timeout=self.send_timeout,\n                data=msg,\n                headers=headers)\n        resp.raise_for_status()\n        reply = Reply(requests.codes.OK, resp.headers, resp.content)\n        return reply", "category": "Python"}, {"instruction": "def get_labels(labels):\n    \"\"\"Create unique labels.\"\"\"\n", "input": "", "output": "    label_u = unique_labels(labels)\n    label_u_line = [i + \"_line\" for i in label_u]\n    return label_u, label_u_line", "category": "Python"}, {"instruction": "def putch(self, text, x, y, colour=7, attr=0, bg=0, transparent=False):\n        \"\"\"\n        Print text at the specified location.  This method is deprecated.  Use\n        :py:meth:`.print_at` instead.\n\n        :param text: The (single line) text to be printed.\n        :param x: The column (x coord) for the start of the text.\n        :param y: The line (y coord) for the start of the text.\n        :param colour: The colour of the text to be displayed.\n        :param attr: The cell attribute of the text to be displayed.\n        :param bg: The background colour of the text to be displayed.\n        :param transparent: Whether to print spaces or not, thus giving a\n            transparent effect.\n        \"\"\"\n", "input": "", "output": "        self.print_at(text, x, y, colour, attr, bg, transparent)", "category": "Python"}, {"instruction": "def fromfile(cls, path):\n        \"\"\"\n        Creates a METS by parsing a file.\n\n        :param str path: Path to a METS document.\n        \"\"\"\n", "input": "", "output": "        parser = etree.XMLParser(remove_blank_text=True)\n\n        return cls.fromtree(etree.parse(path, parser=parser))", "category": "Python"}, {"instruction": "def build_mxnet(app):\n    \"\"\"Build mxnet .so lib\"\"\"\n", "input": "", "output": "    if not os.path.exists(os.path.join(app.builder.srcdir, '..', 'config.mk')):\n        _run_cmd(\"cd %s/.. && cp make/config.mk config.mk && make -j$(nproc) USE_MKLDNN=0 USE_CPP_PACKAGE=1 \" %\n                app.builder.srcdir)\n    else:\n        _run_cmd(\"cd %s/.. && make -j$(nproc) USE_MKLDNN=0 USE_CPP_PACKAGE=1 \" %\n                app.builder.srcdir)", "category": "Python"}, {"instruction": "def generate_json_docs(module, pretty_print=False, user=None):\n    \"\"\"Return a JSON string format of a Pale module's documentation.\n\n    This string can either be printed out, written to a file, or piped to some\n    other tool.\n\n    This method is a shorthand for calling `generate_doc_dict` and passing\n    it into a json serializer.\n\n    The user argument is optional. If included, it expects the user to be an object with an \"is_admin\"\n    boolean attribute. Any endpoint protected with a \"@requires_permission\" decorator will require\n    user.is_admin == True to display documentation on that endpoint.\n    \"\"\"\n", "input": "", "output": "    indent = None\n    separators = (',', ':')\n    if pretty_print:\n        indent = 4\n        separators = (',', ': ')\n\n    module_doc_dict = generate_doc_dict(module, user)\n    json_str = json.dumps(module_doc_dict,\n            indent=indent,\n            separators=separators)\n    return json_str", "category": "Python"}, {"instruction": "def set_close_callback(self, cb):\n        \"\"\"Specify a function to be called when this connection is closed.\n\n        :param cb:\n            A callable that takes no arguments. This callable will be called\n            when this connection is closed.\n        \"\"\"\n", "input": "", "output": "        assert self._close_cb is None, (\n            'A close_callback has already been set for this connection.'\n        )\n        self._close_cb = stack_context.wrap(cb)\n        if self.closed:\n            self._close_cb()", "category": "Python"}, {"instruction": "def bcdc_package_show(package):\n    \"\"\"Query DataBC Catalogue API about given package\n    \"\"\"\n", "input": "", "output": "    params = {\"id\": package}\n    r = requests.get(bcdata.BCDC_API_URL + \"package_show\", params=params)\n    if r.status_code != 200:\n        raise ValueError(\"{d} is not present in DataBC API list\".format(d=package))\n    return r.json()[\"result\"]", "category": "Python"}, {"instruction": "def query(self, s=None, p=None, o=None):\n        \"\"\"\n        Return all triples that satisfy the given expression. You may specify\n        all or none of the fields (s, p, and o). For instance, if I wanted\n        to query for all the people who live in Kansas, I might write:\n\n        .. code-block:: python\n\n            for triple in graph.query(p='lives', o='Kansas'):\n                print triple['s'], 'lives in Kansas!'\n        \"\"\"\n", "input": "", "output": "        start, end = self.keys_for_query(s, p, o)\n        if end is None:\n            if start in self._z:\n                yield {'s': s, 'p': p, 'o': o}\n            else:\n                raise StopIteration\n        else:\n            for key in self._z.range_by_lex('[' + start, '[' + end):\n                keys, p1, p2, p3 = decode(key).split('::')\n                yield dict(zip(keys, (p1, p2, p3)))", "category": "Python"}, {"instruction": "def airspeed_ratio(VFR_HUD):\n    '''recompute airspeed with a different ARSPD_RATIO'''\n", "input": "", "output": "    import mavutil\n    mav = mavutil.mavfile_global\n    airspeed_pressure = (VFR_HUD.airspeed**2) / ratio\n    airspeed = sqrt(airspeed_pressure * ratio)\n    return airspeed", "category": "Python"}, {"instruction": "def _format_name_map(self, lon, lat):\n        ''' Return the name of the map in the good format '''\n", "input": "", "output": "\n        if self.ppd in [4, 16, 64, 128]:\n            lolaname = '_'.join(['LDEM', str(self.ppd)])\n        elif self.ppd in [512]:\n            lolaname = '_'.join(\n                ['LDEM', str(self.ppd), lat[0], lat[1], lon[0], lon[1]])\n        return lolaname", "category": "Python"}, {"instruction": "def get_html(grafs):\n    \"\"\"\n    Renders the grafs provided in HTML by wrapping them in <p> tags.\n\n    Linebreaks are replaced with <br> tags.\n    \"\"\"\n", "input": "", "output": "    html = [format_html('<p>{}</p>', p) for p in grafs]\n    html = [p.replace(\"\\n\", \"<br>\") for p in html]\n    return format_html(six.text_type('\\n\\n'.join(html)))", "category": "Python"}, {"instruction": "def updateColumnName(self, networkId, tableType, body, verbose=None):\n        \"\"\"\n        Renames an existing column in the table specified by the `tableType` and `networkId` parameters.\n\n        :param networkId: SUID of the network containing the table\n        :param tableType: Table Type\n        :param body: Old and new column name\n        :param verbose: print more\n\n        :returns: default: successful operation\n        \"\"\"\n", "input": "", "output": "\n        response=api(url=self.___url+'networks/'+str(networkId)+'/tables/'+str(tableType)+'/columns', method=\"PUT\", body=body, verbose=verbose)\n        return response", "category": "Python"}, {"instruction": "def qualified_name(self):\n        \"\"\"Get the qualified name of the package.\n\n        Returns:\n            str: Name of the package with version, eg \"maya-2016.1\".\n        \"\"\"\n", "input": "", "output": "        o = VersionedObject.construct(self.name, self.version)\n        return str(o)", "category": "Python"}, {"instruction": "def step(self, actions, step_mul=None):\n    \"\"\"Apply actions, step the world forward, and return observations.\n\n    Args:\n      actions: A list of actions meeting the action spec, one per agent.\n      step_mul: If specified, use this rather than the environment's default.\n\n    Returns:\n      A tuple of TimeStep namedtuples, one per agent.\n    \"\"\"\n", "input": "", "output": "    if self._state == environment.StepType.LAST:\n      return self.reset()\n\n    skip = not self._ensure_available_actions\n    self._parallel.run(\n        (c.act, f.transform_action(o.observation, a, skip_available=skip))\n        for c, f, o, a in zip(\n            self._controllers, self._features, self._obs, actions))\n\n    self._state = environment.StepType.MID\n    return self._step(step_mul)", "category": "Python"}, {"instruction": "def add_filtered_folder(self, path, regex, depth=None, source_type=DefaultSourceType):\n        \"\"\"\n        Add a folder source to scan recursively, with a regex filter on directories.\n\n        :param regex: regex string to filter folders by.\n        :param depth: if provided will be depth limit. 0 = first level only.\n        :param source_type: what to return; files only, folders only, or both.\n        \"\"\"\n", "input": "", "output": "        self.add_source(FilteredFolderSource(path, regex, depth, **source_type))\n        return self", "category": "Python"}, {"instruction": "def Realization(M, C, *args, **kwargs):\n    \"\"\"\n    f = Realization(M, C[, init_mesh, init_vals, check_repeats = True, regularize = True])\n\n\n    Returns a realization from a Gaussian process.\n\n\n    :Arguments:\n\n        -   `M`: A Gaussian process mean function.\n\n        -   `C`: A Covariance instance.\n\n        -   `init_mesh`: An optional ndarray giving mesh at which f's initial value will be specified.\n\n        -   `init_vals`: An ndarray giving the value of f over init_mesh.\n\n        -   `regularize`: If init_mesh is not shaped as (n, ndim), where ndim is the dimension of\n            the space, regularize should be True.\n\n        -   `check_repeats: Determines whether calls to the GP realization will be checked against\n            previous calls before evaluation.\n\n    :SeeAlso: Mean, Covariance, BasisCovariance, observe, GP\n    \"\"\"\n", "input": "", "output": "    if isinstance(C, BasisCovariance):\n        return BasisRealization(M, C, *args, **kwargs)\n    else:\n        return StandardRealization(M, C, *args, **kwargs)", "category": "Python"}, {"instruction": "def volumes(self):\n    \"\"\"generator(Volume): volumes generator.\"\"\"\n", "input": "", "output": "    if not self._is_parsed:\n      self._Parse()\n      self._is_parsed = True\n\n    return iter(self._volumes.values())", "category": "Python"}, {"instruction": "def raise_exception(self, node, exc=None, msg='', expr=None,\n                        lineno=None):\n        \"\"\"Add an exception.\"\"\"\n", "input": "", "output": "        if self.error is None:\n            self.error = []\n        if expr is None:\n            expr = self.expr\n        if len(self.error) > 0 and not isinstance(node, ast.Module):\n            msg = '%s' % msg\n        err = ExceptionHolder(node, exc=exc, msg=msg, expr=expr, lineno=lineno)\n        self._interrupt = ast.Break()\n        self.error.append(err)\n        if self.error_msg is None:\n            self.error_msg = \"at expr='%s'\" % (self.expr)\n        elif len(msg) > 0:\n            self.error_msg = msg\n        if exc is None:\n            try:\n                exc = self.error[0].exc\n            except:\n                exc = RuntimeError\n        raise exc(self.error_msg)", "category": "Python"}, {"instruction": "def fcoe_get_interface_output_fcoe_intf_list_fcoe_intf_rx_errors(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        fcoe_get_interface = ET.Element(\"fcoe_get_interface\")\n        config = fcoe_get_interface\n        output = ET.SubElement(fcoe_get_interface, \"output\")\n        fcoe_intf_list = ET.SubElement(output, \"fcoe-intf-list\")\n        fcoe_intf_fcoe_port_id_key = ET.SubElement(fcoe_intf_list, \"fcoe-intf-fcoe-port-id\")\n        fcoe_intf_fcoe_port_id_key.text = kwargs.pop('fcoe_intf_fcoe_port_id')\n        fcoe_intf_rx_errors = ET.SubElement(fcoe_intf_list, \"fcoe-intf-rx-errors\")\n        fcoe_intf_rx_errors.text = kwargs.pop('fcoe_intf_rx_errors')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def get_nexusport_binding(port_id, vlan_id, switch_ip, instance_id):\n    \"\"\"Lists a nexusport binding.\"\"\"\n", "input": "", "output": "    LOG.debug(\"get_nexusport_binding() called\")\n    return _lookup_all_nexus_bindings(port_id=port_id,\n                                      vlan_id=vlan_id,\n                                      switch_ip=switch_ip,\n                                      instance_id=instance_id)", "category": "Python"}, {"instruction": "def reset(self):\n        \"\"\"\n        Kills old session and creates a new one with no proxies or headers\n        \"\"\"\n", "input": "", "output": "        # Kill old connection\n        self.quit()\n        # Clear proxy data\n        self.driver_args['service_args'] = self.default_service_args\n        # Clear headers\n        self.dcap = dict(webdriver.DesiredCapabilities.PHANTOMJS)\n        # Create new web driver\n        self._create_session()", "category": "Python"}, {"instruction": "def write_models(model, data, field):\n    \"\"\"\n    :param model: a Django model class\n    :param data: a list of hashes to build models from\n    :param field: a field name to match models on, or None\n    :returns: a list of models written\n\n    Create or update models for each data hash.\n\n    `field` is the field that is used to get the existing models out of\n    the database to update them; otherwise, if ``field=None``, new models are\n    created.\n\n    Useful when registering custom tests with :func:`writes_models`.\n    \"\"\"\n", "input": "", "output": "    written = []\n\n    for hash_ in data:\n        if field:\n            if field not in hash_:\n                raise KeyError((\"The \\\"%s\\\" field is required for all update \"\n                                \"operations\") % field)\n\n            model_kwargs = {field: hash_[field]}\n            model_obj = model.objects.get(**model_kwargs)\n\n            for to_set, val in hash_.items():\n                setattr(model_obj, to_set, val)\n\n            model_obj.save()\n\n        else:\n            model_obj = model.objects.create(**hash_)\n\n        written.append(model_obj)\n\n    reset_sequence(model)\n    return written", "category": "Python"}, {"instruction": "def register_actions(self, shortcut_manager):\n        \"\"\"Register callback methods for triggered actions\n\n        :param rafcon.gui.shortcut_manager.ShortcutManager shortcut_manager: Shortcut Manager Object holding mappings\n            between shortcuts and actions.\n        \"\"\"\n", "input": "", "output": "        super(DescriptionEditorController, self).register_actions(shortcut_manager)\n        shortcut_manager.add_callback_for_action(\"abort\", self._abort)", "category": "Python"}, {"instruction": "def indicator_associations(self, params=None):\n        \"\"\"\n         Gets the indicator association from a Indicator/Group/Victim\n\n         Yields: Indicator Association\n\n         \"\"\"\n", "input": "", "output": "        if not self.can_update():\n            self._tcex.handle_error(910, [self.type])\n\n        if params is None:\n            params = {}\n\n        for ia in self.tc_requests.indicator_associations(\n            self.api_type, self.api_sub_type, self.unique_id, owner=self.owner, params=params\n        ):\n            yield ia", "category": "Python"}, {"instruction": "def save(self):\n        \"\"\"Saves pypirc file with new configuration information.\"\"\"\n", "input": "", "output": "        for server, conf in self.servers.iteritems():\n            self._add_index_server()\n            for conf_k, conf_v in conf.iteritems():\n                if not self.conf.has_section(server):\n                    self.conf.add_section(server)\n                self.conf.set(server, conf_k, conf_v)\n\n        with open(self.rc_file, 'wb') as configfile:\n            self.conf.write(configfile)\n        self.conf.read(self.rc_file)", "category": "Python"}, {"instruction": "def main(self, x, y, phase):\n        \"\"\"\n        Runs one step of pipelined CORDIC\n        Returned phase is in 1 to -1 range\n        \"\"\"\n", "input": "", "output": "        self.initial_step(phase, x, y)\n\n        # pipelined CORDIC\n        for i in range(self.ITERATIONS - 1):\n            if self.MODE == CordicMode.ROTATION:\n                direction = self.phase[i] > 0\n            elif self.MODE == CordicMode.VECTORING:\n                direction = self.y[i] < 0\n\n            if direction:\n                self.x[i + 1] = self.x[i] - (self.y[i] >> i)\n                self.y[i + 1] = self.y[i] + (self.x[i] >> i)\n                self.phase[i + 1] = self.phase[i] - self.PHASE_LUT[i]\n            else:\n                self.x[i + 1] = self.x[i] + (self.y[i] >> i)\n                self.y[i + 1] = self.y[i] - (self.x[i] >> i)\n                self.phase[i + 1] = self.phase[i] + self.PHASE_LUT[i]\n\n        return self.x[-1], self.y[-1], self.phase[-1]", "category": "Python"}, {"instruction": "def epoch(args):\n    \"\"\"\n    %prog epoch\n\n    Illustrate the methods used in Maggie's epoch paper, in particular, how to\n    classifiy S/G/F/FB/FN for the genes.\n    \"\"\"\n", "input": "", "output": "    p = OptionParser(__doc__)\n    opts, args = p.parse_args()\n\n    fig = plt.figure(1, (6, 4))\n    root = fig.add_axes([0, 0, 1, 1])\n\n    # Separators\n    linestyle = dict(lw=2, color=\"b\", alpha=.2, zorder=2)\n    root.plot((0, 1), (.5, .5), \"--\", **linestyle)\n    for i in (1./3, 2./3):\n        root.plot((i, i), (.5, 1), \"--\", **linestyle)\n    for i in (1./6, 3./6, 5./6):\n        root.plot((i, i), (0, .5), \"--\", **linestyle)\n\n    # Diagrams\n    plot_diagram(root, 1./6, 3./4, \"S\", \"syntenic\")\n    plot_diagram(root, 3./6, 3./4, \"F\", \"missing, with both flankers\")\n    plot_diagram(root, 5./6, 3./4, \"G\", \"missing, with one flanker\")\n    plot_diagram(root, 2./6, 1./4, \"FB\", \"has non-coding matches\")\n    plot_diagram(root, 4./6, 1./4, \"FN\", \"syntenic region has gap\")\n\n    root.set_xlim(0, 1)\n    root.set_ylim(0, 1)\n    root.set_axis_off()\n\n    figname = fname() + \".pdf\"\n    savefig(figname, dpi=300)", "category": "Python"}, {"instruction": "def add_slice_db(self, fid, slice_end, md5):\n        '''\u5728\u6570\u636e\u5e93\u4e2d\u52a0\u5165\u4e0a\u4f20\u4efb\u52a1\u5206\u7247\u4fe1\u606f'''\n", "input": "", "output": "        sql = 'INSERT INTO slice VALUES(?, ?, ?)'\n        self.cursor.execute(sql, (fid, slice_end, md5))\n        self.check_commit()", "category": "Python"}, {"instruction": "def Luv_to_LCHuv(cobj, *args, **kwargs):\n    \"\"\"\n    Convert from CIE Luv to LCH(uv).\n    \"\"\"\n", "input": "", "output": "    lch_l = cobj.luv_l\n    lch_c = math.sqrt(math.pow(cobj.luv_u, 2.0) + math.pow(cobj.luv_v, 2.0))\n    lch_h = math.atan2(float(cobj.luv_v), float(cobj.luv_u))\n\n    if lch_h > 0:\n        lch_h = (lch_h / math.pi) * 180\n    else:\n        lch_h = 360 - (math.fabs(lch_h) / math.pi) * 180\n    return LCHuvColor(\n        lch_l, lch_c, lch_h, observer=cobj.observer, illuminant=cobj.illuminant)", "category": "Python"}, {"instruction": "def generate_anchors(\n    stride=16, sizes=(32, 64, 128, 256, 512), aspect_ratios=(0.5, 1, 2)\n):\n    \"\"\"Generates a matrix of anchor boxes in (x1, y1, x2, y2) format. Anchors\n    are centered on stride / 2, have (approximate) sqrt areas of the specified\n    sizes, and aspect ratios as given.\n    \"\"\"\n", "input": "", "output": "    return _generate_anchors(\n        stride,\n        np.array(sizes, dtype=np.float) / stride,\n        np.array(aspect_ratios, dtype=np.float),\n    )", "category": "Python"}, {"instruction": "def _strip_key(dictionary, keyword):\n    '''\n    look for a certain key within a dictionary and nullify ti's contents, check within nested\n    dictionaries and lists as well.  Certain attributes such as \"generation\" will change even\n    when there were no changes made to the entity.\n    '''\n", "input": "", "output": "\n    for key, value in six.iteritems(dictionary):\n        if key == keyword:\n            dictionary[key] = None\n        elif isinstance(value, dict):\n            _strip_key(value, keyword)\n        elif isinstance(value, list):\n            for item in value:\n                if isinstance(item, dict):\n                    _strip_key(item, keyword)\n\n    return dictionary", "category": "Python"}, {"instruction": "def load_data(filename):\n        \"\"\"Loads a data matrix from a given file.\n\n        Parameters\n        ----------\n        filename : :obj:`str`\n            The file to load the data from. Must be one of .png, .jpg,\n            .npy, or .npz.\n\n        Returns\n        -------\n        :obj:`numpy.ndarray`\n            The data array read from the file.\n        \"\"\"\n", "input": "", "output": "        file_root, file_ext = os.path.splitext(filename)\n        data = None\n        if file_ext.lower() in COLOR_IMAGE_EXTS:\n            data = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n        elif file_ext == '.npy':\n            data = np.load(filename)\n        elif file_ext == '.npz':\n            data = np.load(filename)['arr_0']\n        else:\n            raise ValueError('Extension %s not supported' % (file_ext))\n        return data", "category": "Python"}, {"instruction": "def remove_team_membership(self, auth, team_id, username):\n        \"\"\"\n        Remove user from team.\n\n        :param auth.Authentication auth: authentication object, must be admin-level\n        :param str team_id: Team's id\n        :param str username: Username of the user to be removed from the team\n        :raises NetworkFailure: if there is an error communicating with the server\n        :raises ApiFailure: if the request cannot be serviced\n        \"\"\"\n", "input": "", "output": "        url = \"/admin/teams/{t}/members/{u}\".format(t=team_id, u=username)\n        self.delete(url, auth=auth)", "category": "Python"}, {"instruction": "def get_country_name_from_iso3(cls, iso3, use_live=True, exception=None):\n        # type: (str, bool, Optional[ExceptionUpperBound]) -> Optional[str]\n        \"\"\"Get country name from ISO3 code\n\n        Args:\n            iso3 (str): ISO3 code for which to get country name\n            use_live (bool): Try to get use latest data from web rather than file in package. Defaults to True.\n            exception (Optional[ExceptionUpperBound]): An exception to raise if country not found. Defaults to None.\n\n        Returns:\n            Optional[str]: Country name\n        \"\"\"\n", "input": "", "output": "        countryinfo = cls.get_country_info_from_iso3(iso3, use_live=use_live, exception=exception)\n        if countryinfo is not None:\n            return countryinfo.get('#country+name+preferred')\n        return None", "category": "Python"}, {"instruction": "def FromString(self, string):\n    \"\"\"Parse a bool from a string.\"\"\"\n", "input": "", "output": "    if string.lower() in (\"false\", \"no\", \"n\"):\n      return False\n\n    if string.lower() in (\"true\", \"yes\", \"y\"):\n      return True\n\n    raise TypeValueError(\"%s is not recognized as a boolean value.\" % string)", "category": "Python"}, {"instruction": "def save(self, collection):\n        \"\"\"\n        Save an asset collection to the service.\n        \"\"\"\n", "input": "", "output": "        assert isinstance(collection, predix.data.asset.AssetCollection), \"Expected AssetCollection\"\n        collection.validate()\n        self.put_collection(collection.uri, collection.__dict__)", "category": "Python"}, {"instruction": "def form_valid(self, form):\n        \"\"\"After the form is valid lets let people know\"\"\"\n", "input": "", "output": "\n        ret = super(ProjectCreate, self).form_valid(form)\n\n        # Good to make note of that\n        messages.add_message(self.request, messages.SUCCESS, 'Project %s created' % self.object.name)\n\n        return ret", "category": "Python"}, {"instruction": "def get_backend_name(self, location):\n        \"\"\"\n        Return the name of the version control backend if found at given\n        location, e.g. vcs.get_backend_name('/path/to/vcs/checkout')\n        \"\"\"\n", "input": "", "output": "        for vc_type in self._registry.values():\n            path = os.path.join(location, vc_type.dirname)\n            if os.path.exists(path):\n                return vc_type.name\n        return None", "category": "Python"}, {"instruction": "def full_address(self):\n        \"\"\"\n        Print the address in a human readable format\n        \"\"\"\n", "input": "", "output": "        addr = \"\"\n        # if self.building:\n        #     addr = addr + \"(\" + self.building + \") \"\n        if self.house_number:\n            addr = addr + self.house_number\n        if self.street_prefix:\n            addr = addr + \" \" + self.street_prefix\n        if self.street:\n            addr = addr + \" \" + self.street\n        if self.street_suffix:\n            addr = addr + \" \" + self.street_suffix\n        if self.apartment:\n            addr = addr + \" \" + self.apartment\n        if self.city:\n            addr = addr + \", \" + self.city\n        if self.state:\n            addr = addr + \", \" + self.state\n        if self.zip:\n            addr = addr + \" \" + self.zip\n        return addr", "category": "Python"}, {"instruction": "def get(self):\n        \"\"\"Returns the system configuration abstraction\n\n        The System resource returns the following:\n\n            * hostname (str): The hostname value\n\n        Returns:\n            dict: Represents the node's system configuration\n        \"\"\"\n", "input": "", "output": "        resource = dict()\n        resource.update(self._parse_hostname())\n        resource.update(self._parse_iprouting())\n        resource.update(self._parse_banners())\n\n        return resource", "category": "Python"}, {"instruction": "def _delete_forever_keys(self):\n        \"\"\"\n        Delete all of the items that were stored forever.\n        \"\"\"\n", "input": "", "output": "        for segment in self._tags.get_namespace().split('|'):\n            segment = self._forever_key(segment)\n            self._delete_forever_values(segment)\n\n            self._store.connection().delete(segment)", "category": "Python"}, {"instruction": "def load_nb(cls, inline=True):\n        \"\"\"\n        Loads the bokeh notebook resources.\n        \"\"\"\n", "input": "", "output": "        LOAD_MIME_TYPE = bokeh.io.notebook.LOAD_MIME_TYPE\n        bokeh.io.notebook.LOAD_MIME_TYPE = MIME_TYPES['jlab-hv-load']\n        load_notebook(hide_banner=True, resources=INLINE if inline else CDN)\n        bokeh.io.notebook.LOAD_MIME_TYPE = LOAD_MIME_TYPE\n        bokeh.io.notebook.curstate().output_notebook()", "category": "Python"}, {"instruction": "def get(self, url, **kwargs):\n        \"\"\"Send a GET request to the specified URL.\n\n        Method directly wraps around `Session.get` and updates browser\n        attributes.\n        <http://docs.python-requests.org/en/master/api/#requests.get>\n\n        Args:\n            url: URL for the new `Request` object.\n            **kwargs: Optional arguments that `Request` takes.\n\n        Returns:\n            `Response` object of a successful request.\n        \"\"\"\n", "input": "", "output": "        response = self.session.get(url, **kwargs)\n        self._url = response.url\n        self._response = response\n        return response", "category": "Python"}, {"instruction": "def use_np_compat(func):\n    \"\"\"Wraps a function with an activated NumPy-compatibility scope. This ensures\n    that the execution of the function is guaranteed with NumPy compatible semantics,\n    such as zero-dim and zero size tensors.\n\n    Example::\n        import mxnet as mx\n        @mx.use_np_compat\n        def scalar_one():\n            return mx.nd.ones(())\n        print(scalar_one())\n\n    Parameters\n    ----------\n    func : a user-provided callable function to be scoped by the NumPy compatibility state.\n\n    Returns\n    -------\n    Function\n        A function for wrapping the user functions in the NumPy compatibility scope.\n    \"\"\"\n", "input": "", "output": "    @wraps(func)\n    def _with_np_compat(*args, **kwargs):\n        with np_compat(active=True):\n            return func(*args, **kwargs)\n\n    return _with_np_compat", "category": "Python"}, {"instruction": "def parse_style(self, style):\n        '''\n        style : string, eg:\n            fill:#ff2a2a;fill-rule:evenodd;stroke:none;stroke-width:1px;\n            stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1\n        returns color as a triple of unsigned bytes: (r, g, b), or None\n        '''\n", "input": "", "output": "        style_elements = style.split(';')\n        while style_elements:\n            element = style_elements.pop()\n            if element.startswith('fill:'):\n                return self.parse_color(element[5:])\n        return None", "category": "Python"}, {"instruction": "def _permission_trees(permissions):\n        \"\"\"Get the cached permission tree, or build a new one if necessary.\"\"\"\n", "input": "", "output": "        treecache = PermissionTreeCache()\n        cached = treecache.get()\n        if not cached:\n            tree = PermissionTreeBuilder()\n            for permission in permissions:\n                tree.insert(permission)\n            result = tree.serialize()\n            treecache.set(result)\n            return result\n        return cached", "category": "Python"}, {"instruction": "def _get_primary_address(self, mac_address, node_list):\n        \"\"\"\n        Uses the _get_aggregated_node_list structure to find\n        the primary mac address associated to a secondary one,\n        if none is found returns itself.\n        \"\"\"\n", "input": "", "output": "        for local_addresses in node_list:\n            if mac_address in local_addresses:\n                return local_addresses[0]\n        return mac_address", "category": "Python"}, {"instruction": "def _generate(self):\n        \"\"\"Generates a particle using the creator function.\n\n        Notes\n        -----\n        Position and speed are uniformly randomly seeded within\n        allowed bounds. The particle also has speed limit settings\n        taken from global values.\n\n        Returns\n        -------\n        part : particle object\n            A particle used during optimisation.\n        \"\"\"\n", "input": "", "output": "        part = creator.Particle(\n            [random.uniform(-1, 1)\n             for _ in range(len(self.value_means))])\n        part.speed = [\n            random.uniform(-self.max_speed, self.max_speed)\n            for _ in range(len(self.value_means))]\n        part.smin = -self.max_speed\n        part.smax = self.max_speed\n        part.ident = None\n        part.neighbours = None\n        return part", "category": "Python"}, {"instruction": "def draw(self, mode, selection):\n        \"\"\" Draw program in given mode, with given selection (IndexBuffer or\n        first, count).\n        \"\"\"\n", "input": "", "output": "        if not self._linked:\n            raise RuntimeError('Cannot draw program if code has not been set')\n        # Init\n        gl.check_error('Check before draw')\n        mode = as_enum(mode)\n        # Draw\n        if len(selection) == 3:\n            # Selection based on indices\n            id_, gtype, count = selection\n            if count:\n                self._pre_draw()\n                ibuf = self._parser.get_object(id_)\n                ibuf.activate()\n                gl.glDrawElements(mode, count, as_enum(gtype), None)\n                ibuf.deactivate()\n        else:\n            # Selection based on start and count\n            first, count = selection\n            if count:\n                self._pre_draw()\n                gl.glDrawArrays(mode, first, count)\n        # Wrap up\n        gl.check_error('Check after draw')\n        self._post_draw()", "category": "Python"}, {"instruction": "def _configuration(self, *args, **kwargs):\n        '''\n        Return configuration files.\n        '''\n", "input": "", "output": "\n        data = dict()\n        self.db.open()\n        for pkg in self.db.get(Package):\n            configs = list()\n            for pkg_cfg in self.db.get(PackageCfgFile, eq={'pkgid': pkg.id}):\n                configs.append(pkg_cfg.path)\n            data[pkg.name] = configs\n\n        if not data:\n            raise InspectorQueryException(\"No inspected configuration yet available.\")\n\n        return data", "category": "Python"}, {"instruction": "def prep_regex(patterns):\n    \"\"\"Compile regex patterns.\"\"\"\n", "input": "", "output": "\n    flags = 0 if Config.options.case_sensitive else re.I\n\n    return [re.compile(pattern, flags) for pattern in patterns]", "category": "Python"}, {"instruction": "def clear(self):\n        \"\"\"\n        Clears grid to be EMPTY\n        \"\"\"\n", "input": "", "output": "        self.grid = [[EMPTY for dummy_col in range(self.grid_width)] for dummy_row in range(self.grid_height)]", "category": "Python"}, {"instruction": "def kill(path):\n    \"\"\"\n    Kills the process, if it still exists.\n\n    :type  path: str\n    :param path: The name of the pidfile.\n    \"\"\"\n", "input": "", "output": "    # try to read the pid from the pidfile\n    pid = read(path)\n    if pid is None:\n        return\n\n    # Try to kill the process.\n    logging.info(\"Killing PID %s\", pid)\n    try:\n        os.kill(pid, 9)\n    except OSError as xxx_todo_changeme2:\n        # re-raise if the error wasn't \"No such process\"\n        (code, text) = xxx_todo_changeme2.args\n        # re-raise if the error wasn't \"No such process\"\n        if code != errno.ESRCH:\n            raise", "category": "Python"}, {"instruction": "def handle_units(changeset):\n    \"\"\"Populate the change set with addUnit changes.\"\"\"\n", "input": "", "output": "    units, records = {}, {}\n    for service_name, service in sorted(changeset.bundle['services'].items()):\n        for i in range(service.get('num_units', 0)):\n            record_id = 'addUnit-{}'.format(changeset.next_action())\n            unit_name = '{}/{}'.format(service_name, i)\n            records[record_id] = {\n                'id': record_id,\n                'method': 'addUnit',\n                'args': [\n                    '${}'.format(changeset.services_added[service_name]),\n                    None,\n                ],\n                'requires': [changeset.services_added[service_name]],\n            }\n            units[unit_name] = {\n                'record': record_id,\n                'service': service_name,\n                'unit': i,\n            }\n    _handle_units_placement(changeset, units, records)", "category": "Python"}, {"instruction": "def log_config(self):\n        ''' Log the current logging configuration. '''\n", "input": "", "output": "        level = self.level\n        debug = self.debug\n        debug('Logging config:')\n        debug('/ name: {}, id: {}', self.name, id(self))\n        debug('  .level: %s (%s)', level_map_int[level], level)\n        debug('  .default_level: %s (%s)',\n                   level_map_int[self.default_level], self.default_level)\n\n        for i, handler in enumerate(self.handlers):\n            fmtr = handler.formatter\n            debug('  + Handler: %s %r', i, handler)\n            debug('    + Formatter: %r', fmtr)\n            debug('      .datefmt: %r', fmtr.datefmt)\n            debug('      .msgfmt: %r', fmtr._fmt)\n            debug('      fmt_style: %r', fmtr._style)\n            try:\n                debug('      theme styles: %r', fmtr._theme_style)\n                debug('      theme icons:\\n%r', fmtr._theme_icons)\n                debug('      lexer: %r\\n', fmtr._lexer)\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def minify(path):\n    \"\"\"\n    Load a javascript file and minify.\n\n    Parameters\n    ------------\n    path: str, path of resource\n    \"\"\"\n", "input": "", "output": "\n    if 'http' in path:\n        data = requests.get(path).content.decode(\n            'ascii', errors='ignore')\n    else:\n        with open(path, 'rb') as f:\n            # some of these assholes use unicode spaces -_-\n            data = f.read().decode('ascii',\n                                   errors='ignore')\n    # don't re- minify\n    if '.min.' in path:\n        return data\n\n    try:\n        return jsmin.jsmin(data)\n    except BaseException:\n        return data", "category": "Python"}, {"instruction": "def _loader(self, tags):\n        \"\"\"Create a yaml Loader.\"\"\"\n", "input": "", "output": "        class ConfigLoader(SafeLoader):\n            pass\n        ConfigLoader.add_multi_constructor(\"\", lambda loader, prefix, node: TaggedValue(node.value, node.tag, *tags))\n        return ConfigLoader", "category": "Python"}, {"instruction": "def predict(self, data):\n        \"\"\"Predict samples on actual data.\n\n        The result of this function is used for calculating the residuals.\n\n        Parameters\n        ----------\n        data : array, shape (trials, channels, samples) or (channels, samples)\n            Epoched or continuous data set.\n\n        Returns\n        -------\n        predicted : array, shape `data`.shape\n            Data as predicted by the VAR model.\n\n        Notes\n        -----\n        Residuals are obtained by r = x - var.predict(x)\n        \"\"\"\n", "input": "", "output": "        data = atleast_3d(data)\n        t, m, l = data.shape\n\n        p = int(np.shape(self.coef)[1] / m)\n\n        y = np.zeros(data.shape)\n        if t > l - p:  # which takes less loop iterations\n            for k in range(1, p + 1):\n                bp = self.coef[:, (k - 1)::p]\n                for n in range(p, l):\n                    y[:, :, n] += np.dot(data[:, :, n - k], bp.T)\n        else:\n            for k in range(1, p + 1):\n                bp = self.coef[:, (k - 1)::p]\n                for s in range(t):\n                    y[s, :, p:] += np.dot(bp, data[s, :, (p - k):(l - k)])\n\n        return y", "category": "Python"}, {"instruction": "def check(self, namespace, level, explicit=False):\n        \"\"\"\n        Checks if the permset has permission to the specified namespace\n        at the specified level\n\n        Arguments:\n\n        namespace -- permissioning namespace (str)\n        level -- permissioning level (int) (PERM_READ for example)\n        explicit -- require explicitly set permissions to the provided namespace\n\n        Returns:\n\n        bool\n        \"\"\"\n", "input": "", "output": "\n        return (self.get_permissions(namespace, explicit=explicit) & level) != 0", "category": "Python"}, {"instruction": "def to_bam(in_file, out_file, data):\n    \"\"\"Convert CRAM file into BAM.\n    \"\"\"\n", "input": "", "output": "    if not utils.file_uptodate(out_file, in_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = [\"samtools\", \"view\", \"-O\", \"BAM\", \"-o\", tx_out_file, in_file]\n            do.run(cmd, \"Convert CRAM to BAM\")\n    bam.index(out_file, data[\"config\"])\n    return out_file", "category": "Python"}, {"instruction": "def key_bytes(self):\n        \"\"\"Returns the raw signing key.\n\n        :rtype: bytes\n        \"\"\"\n", "input": "", "output": "        return self.key.private_bytes(\n            encoding=serialization.Encoding.DER,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption(),\n        )", "category": "Python"}, {"instruction": "def set_generator(self):\n        \"\"\"Parses feed generator and sets value\"\"\"\n", "input": "", "output": "        try:\n            self.generator = self.soup.find('generator').string\n        except AttributeError:\n            self.generator = None", "category": "Python"}, {"instruction": "def is_fp_closed(obj):\n    \"\"\"\n    Checks whether a given file-like object is closed.\n\n    :param obj:\n        The file-like object to check.\n    \"\"\"\n", "input": "", "output": "\n    try:\n        # Check `isclosed()` first, in case Python3 doesn't set `closed`.\n        # GH Issue #928\n        return obj.isclosed()\n    except AttributeError:\n        pass\n\n    try:\n        # Check via the official file-like-object way.\n        return obj.closed\n    except AttributeError:\n        pass\n\n    try:\n        # Check if the object is a container for another file-like object that\n        # gets released on exhaustion (e.g. HTTPResponse).\n        return obj.fp is None\n    except AttributeError:\n        pass\n\n    raise ValueError(\"Unable to determine whether fp is closed.\")", "category": "Python"}, {"instruction": "def update_tool_tip(self):\n        \"\"\"\n        Updates the node tooltip.\n\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        self.roles[Qt.ToolTipRole] = self.__tool_tip_text.format(self.component.name,\n                                                                 self.component.author,\n                                                                 self.component.category,\n                                                                 \", \".join(self.component.require),\n                                                                 self.component.version,\n                                                                 self.component.description)\n        return True", "category": "Python"}, {"instruction": "def getInstalledThemes(self, store):\n        \"\"\"\n        Collect themes from all offerings installed on this store, or (if called\n        multiple times) return the previously collected list.\n        \"\"\"\n", "input": "", "output": "        if not store in self._getInstalledThemesCache:\n            self._getInstalledThemesCache[store] = (self.\n                                                 _realGetInstalledThemes(store))\n        return self._getInstalledThemesCache[store]", "category": "Python"}, {"instruction": "def redirect_warnings(capture=True, logger='py.warnings'):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n", "input": "", "output": "    global _warnings_showwarning\n    if capture:\n        assert _warnings_showwarning is None\n        _warnings_showwarning = warnings.showwarning\n        # `warnings.showwarning` must be a function, a generic\n        # callable object is not accepted ...\n        warnings.showwarning = _WarningsLogger(logger, format_warning_oneline).__call__\n    else:\n        assert _warnings_showwarning is not None\n        warnings.showwarning = _warnings_showwarning\n        _warnings_showwarning = None", "category": "Python"}, {"instruction": "def getResultsRangeDict(self):\n        \"\"\"Return a dictionary with the specification fields for each\n           service. The keys of the dictionary are the keywords of each\n           analysis service. Each service contains a dictionary in which\n           each key is the name of the spec field:\n           specs['keyword'] = {'min': value,\n                               'max': value,\n                               'warnmin': value,\n                               ... }\n        \"\"\"\n", "input": "", "output": "        specs = {}\n        subfields = self.Schema()['ResultsRange'].subfields\n        for spec in self.getResultsRange():\n            keyword = spec['keyword']\n            specs[keyword] = {}\n            for key in subfields:\n                if key not in ['uid', 'keyword']:\n                    specs[keyword][key] = spec.get(key, '')\n        return specs", "category": "Python"}, {"instruction": "def _datetime_key_for_merge(self, logevent):\n        \"\"\"Helper method for ordering log lines correctly during merge.\"\"\"\n", "input": "", "output": "        if not logevent:\n            # if logfile end is reached, return max datetime to never\n            # pick this line\n            return datetime(MAXYEAR, 12, 31, 23, 59, 59, 999999, tzutc())\n\n        # if no datetime present (line doesn't have one) return mindate\n        # to pick this line immediately\n        return logevent.datetime or datetime(MINYEAR, 1, 1, 0, 0, 0, 0,\n                                             tzutc())", "category": "Python"}, {"instruction": "def save_anim(self, fig, animate, init, bitrate=10000, fps=30):\n        \"\"\"Not functional -- TODO\"\"\"\n", "input": "", "output": "        from matplotlib import animation\n        anim = animation.FuncAnimation(fig, animate, init_func=init, frames=360, interval=20)\n        FFMpegWriter = animation.writers['ffmpeg']\n        writer = FFMpegWriter(bitrate= bitrate, fps=fps)\n        # Save #\n        self.avi_path = self.base_dir + self.short_name + '.avi'\n        anim.save(self.avi_path, writer=writer, codec='x264')", "category": "Python"}, {"instruction": "def _get_params(func, *args, **kwargs):\n    \"\"\"Turn an argument list into a dictionary.\n\n    :arg function func: A function.\n    :arg list *args: Positional arguments of `func`.\n    :arg dict **kwargs: Keyword arguments of `func`.\n\n    :returns dict: Dictionary representation of `args` and `kwargs`.\n    \"\"\"\n", "input": "", "output": "    params = dict(zip(func.func_code.co_varnames[:len(args)], args))\n    if func.func_defaults:\n        params.update(dict(zip(\n            func.func_code.co_varnames[-len(func.func_defaults):],\n            func.func_defaults)))\n    params.update(kwargs)\n\n    return params", "category": "Python"}, {"instruction": "def get_authorization_user(self, **kwargs):\n        \"\"\"Gets the user the authorization object is for.\"\"\"\n", "input": "", "output": "        if self.authorization_user is not None:\n            return self.authorization_user\n\n        self.authorization_user = self.request.user\n        return self.request.user", "category": "Python"}, {"instruction": "def add_atype(self, ):\n        \"\"\"Add a atype and store it in the self.atypes\n\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        i = self.atype_tablev.currentIndex()\n        item = i.internalPointer()\n        if item:\n            atype = item.internal_data()\n            atype.projects.add(self._project)\n            self.atypes.append(atype)\n            item.set_parent(None)", "category": "Python"}, {"instruction": "def dependencies(self, user=None, napp=None):\n        \"\"\"Get napp_dependencies from install NApp.\n\n        Args:\n            user(string)  A Username.\n            napp(string): A NApp name.\n        Returns:\n            napps(list): List with tuples with Username and NApp name.\n                         e.g. [('kytos'/'of_core'), ('kytos/of_l2ls')]\n\n        \"\"\"\n", "input": "", "output": "        napps = self._get_napp_key('napp_dependencies', user, napp)\n        return [tuple(napp.split('/')) for napp in napps]", "category": "Python"}, {"instruction": "def variable_declaration(self):\n        \"\"\"\n        variable_declaration: 'let' assignment ';'\n        \"\"\"\n", "input": "", "output": "        self._process(Nature.LET)\n        node = VariableDeclaration(assignment=self.assignment())\n        self._process(Nature.SEMI)\n        return node", "category": "Python"}, {"instruction": "def _build_zmat(self, construction_table):\n        \"\"\"Create the Zmatrix from a construction table.\n\n        Args:\n            Construction table (pd.DataFrame):\n\n        Returns:\n            Zmat: A new instance of :class:`Zmat`.\n        \"\"\"\n", "input": "", "output": "        c_table = construction_table\n        default_cols = ['atom', 'b', 'bond', 'a', 'angle', 'd', 'dihedral']\n        optional_cols = list(set(self.columns) - {'atom', 'x', 'y', 'z'})\n\n        zmat_frame = pd.DataFrame(columns=default_cols + optional_cols,\n                                  dtype='float', index=c_table.index)\n\n        zmat_frame.loc[:, optional_cols] = self.loc[c_table.index,\n                                                    optional_cols]\n\n        zmat_frame.loc[:, 'atom'] = self.loc[c_table.index, 'atom']\n        zmat_frame.loc[:, ['b', 'a', 'd']] = c_table\n\n        zmat_values = self._calculate_zmat_values(c_table)\n        zmat_frame.loc[:, ['bond', 'angle', 'dihedral']] = zmat_values\n\n        zmatrix = Zmat(zmat_frame, metadata=self.metadata,\n                       _metadata={'last_valid_cartesian': self.copy()})\n        return zmatrix", "category": "Python"}, {"instruction": "def pid(self):\n        \"\"\"The pid of the process associated to the scheduler.\"\"\"\n", "input": "", "output": "        try:\n            return self._pid\n        except AttributeError:\n            self._pid = os.getpid()\n            return self._pid", "category": "Python"}, {"instruction": "def valid_hostname(host):\n    \"\"\"\n    Returns whether the specified string is a valid hostname.\n    \"\"\"\n", "input": "", "output": "    if len(host) > 255:\n        return False\n\n    if host[-1:] == '.':\n        host = host[:-1]\n\n    return all(_hostname_re.match(c) for c in host.split('.'))", "category": "Python"}, {"instruction": "def send(sms_to, sms_body, **kwargs):\n    \"\"\"\n    Site: http://smsaero.ru/\n    API: http://smsaero.ru/api/\n    \"\"\"\n", "input": "", "output": "    headers = {\n        \"User-Agent\": \"DBMail/%s\" % get_version(),\n    }\n\n    kwargs.update({\n        'user': settings.SMSAERO_LOGIN,\n        'password': settings.SMSAERO_MD5_PASSWORD,\n        'from': kwargs.pop('sms_from', settings.SMSAERO_FROM),\n        'to': sms_to.replace('+', ''),\n        'text': from_unicode(sms_body),\n        'answer': 'json',\n    })\n\n    http = HTTPConnection(kwargs.pop(\"api_url\", \"gate.smsaero.ru\"))\n    http.request(\"GET\", \"/send/?\" + urlencode(kwargs), headers=headers)\n    response = http.getresponse()\n\n    if response.status != 200:\n        raise AeroSmsError(response.reason)\n\n    read = response.read().decode(response.headers.get_content_charset())\n    data = json.loads(read)\n\n    status = None\n    if 'result' in data:\n        status = data['result']\n\n    sms_id = None\n    if 'id' in data:\n        sms_id = data['id']\n\n    if sms_id and status == 'accepted':\n        return True\n    return False", "category": "Python"}, {"instruction": "def logpdf(self, mu):\n        \"\"\"\n        Log PDF for t prior\n\n        Parameters\n        ----------\n        mu : float\n            Latent variable for which the prior is being formed over\n\n        Returns\n        ----------\n        - log(p(mu))\n        \"\"\"\n", "input": "", "output": "        if self.transform is not None:\n            mu = self.transform(mu)    \n        return ss.t.logpdf(mu, df=self.df0, loc=self.loc0, scale=self.scale0)", "category": "Python"}, {"instruction": "def query(self):\n        \"\"\"\n        De-serialize, decode and return an ORM query stored in b64_query.\n        \"\"\"\n", "input": "", "output": "        if not self.b64_query:\n            return None\n        s = QSerializer(base64=True)\n        return s.loads(self.b64_query)", "category": "Python"}, {"instruction": "def hr_dp010(self, value=None):\n        \"\"\"  Corresponds to IDD Field `hr_dp010`\n        humidity ratio corresponding to\n        Dew-point temperature corresponding to 1.0,% annual cumulative frequency of occurrence\n        calculated at the standard atmospheric pressure at elevation of station\n\n        Args:\n            value (float): value for IDD Field `hr_dp010`\n                if `value` is None it will not be checked against the\n                specification and is assumed to be a missing value\n\n        Raises:\n            ValueError: if `value` is not a valid value\n        \"\"\"\n", "input": "", "output": "        if value is not None:\n            try:\n                value = float(value)\n            except ValueError:\n                raise ValueError('value {} need to be of type float '\n                                 'for field `hr_dp010`'.format(value))\n\n        self._hr_dp010 = value", "category": "Python"}, {"instruction": "def size(self, table=None):\n        \"\"\"\n        Return the size, in bytes, of the profile or *table*.\n\n        If *table* is `None`, this function returns the size of the\n        whole profile (i.e. the sum of the table sizes). Otherwise, it\n        returns the size of *table*.\n\n        Note: if the file is gzipped, it returns the compressed size.\n        \"\"\"\n", "input": "", "output": "        size = 0\n        if table is None:\n            for table in self.relations:\n                size += self.size(table)\n        else:\n            try:\n                fn = _table_filename(os.path.join(self.root, table))\n                size += os.stat(fn).st_size\n            except ItsdbError:\n                pass\n        return size", "category": "Python"}, {"instruction": "def target_id(self):\n        \"\"\"Returns the id the target, to which this post has to be syndicated.\n\n        :returns: string\"\"\"\n", "input": "", "output": "        # already set?\n        if self._target_id:\n            return self._target_id\n        # post already exists?\n        if self._existing:\n            self._target_id = self._existing.get(\"target_id\")\n        return self._target_id", "category": "Python"}, {"instruction": "def get_authorization(self, **kwargs):\n        \"\"\"Gets the authorization object for the view.\"\"\"\n", "input": "", "output": "        if self.authorization is not None:\n            return self.authorization\n\n        auth_class = self.get_authorization_class()\n        auth_user = self.get_authorization_user()\n        auth_kwargs = {\n            'token': self.get_authorization_token(**kwargs)\n        }\n\n        if auth_user and auth_user.is_authenticated():\n            auth_kwargs['created_user'] = self.get_authorization_user()\n\n        self.authorization = auth_class.objects.get_by_token_or_404(\n            **auth_kwargs\n        )\n        return self.authorization", "category": "Python"}, {"instruction": "def get_feature(self, feature_id):\n        \"\"\"GetFeature.\n        [Preview API] Get a specific feature by its id\n        :param str feature_id: The contribution id of the feature\n        :rtype: :class:`<ContributedFeature> <azure.devops.v5_0.feature_management.models.ContributedFeature>`\n        \"\"\"\n", "input": "", "output": "        route_values = {}\n        if feature_id is not None:\n            route_values['featureId'] = self._serialize.url('feature_id', feature_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='c4209f25-7a27-41dd-9f04-06080c7b6afd',\n                              version='5.0-preview.1',\n                              route_values=route_values)\n        return self._deserialize('ContributedFeature', response)", "category": "Python"}, {"instruction": "def done(p_queue, host=None):\n    if host is not None:\n        return _path(_c.FSQ_DONE, root=_path(host, root=hosts(p_queue)))\n    '''Construct a path to the done dir for a queue'''\n", "input": "", "output": "    return _path(p_queue, _c.FSQ_DONE)", "category": "Python"}, {"instruction": "def order_stop(backend, order_id):\n    \"\"\"\n    Stop an order - Turn off the serving generation ability of an order.  Stop any running jobs.  Keep all state around.\n    \"\"\"\n", "input": "", "output": "    if order_id is None:\n        raise click.ClickException('invalid order id %s' % order_id)\n    click.secho('%s - Stop order id %s' % (get_datetime(), order_id), fg='green')\n    check_and_print(DKCloudCommandRunner.stop_order(backend.dki, order_id))", "category": "Python"}, {"instruction": "def importAll(self, gs):\n        '''\n        Import all the enumerate values from this enumerate to *gs*\n        :param gs: usually globals(), a dictionary. At lease __setitem__ should be implemented if not a dictionary.\n        '''\n", "input": "", "output": "        for k,v in self._values.items():\n            gs[k] = v", "category": "Python"}, {"instruction": "def p_arglist(self, tree):\r\n        ''' V ::= arglist ( V , V )'''\n", "input": "", "output": "        if type(tree[0].value) == type([]):\r\n            tree.value = tree[0].value + [tree[2].value]\r\n        else:\r\n            tree.value = [tree[0].value, tree[2].value]\r\n        try:\r\n            tree.svalue = \"%s,%s\"%(tree[0].svalue,tree[2].svalue)\r\n        except AttributeError:\r\n            pass", "category": "Python"}, {"instruction": "async def send_script(self, client_id, conn_string, script):\n        \"\"\"Send a script to a device on behalf of a client.\n\n        See :meth:`AbstractDeviceAdapter.send_script`.\n\n        Args:\n            client_id (str): The client we are working for.\n            conn_string (str): A connection string that will be\n                passed to the underlying device adapter.\n            script (bytes): The script that we wish to send.\n\n        Raises:\n            DeviceServerError: There is an issue with your client_id such\n                as not being connected to the device.\n            DeviceAdapterError: The adapter had a protocol issue sending the script.\n        \"\"\"\n", "input": "", "output": "\n        conn_id = self._client_connection(client_id, conn_string)\n        await self.adapter.send_script(conn_id, script)", "category": "Python"}, {"instruction": "def _bfs_subgraph(self, start_id, forward=True):\n        \"\"\"\n        Private method creates a subgraph in a bfs order.\n\n        The forward parameter specifies whether it is a forward or backward\n        traversal.\n        \"\"\"\n", "input": "", "output": "        if forward:\n            get_bfs  = self.forw_bfs\n            get_nbrs = self.out_nbrs\n        else:\n            get_bfs  = self.back_bfs\n            get_nbrs = self.inc_nbrs\n\n        g = Graph()\n        bfs_list = get_bfs(start_id)\n        for node in bfs_list:\n            g.add_node(node)\n\n        for node in bfs_list:\n            for nbr_id in get_nbrs(node):\n                g.add_edge(node, nbr_id)\n\n        return g", "category": "Python"}, {"instruction": "def publish(self, channel, message, pipeline=False):\n        \"\"\"Post a message to a given channel.\n\n        Args:\n            channel (str): Channel where the message will be published\n            message (str): Message to publish\n            pipeline (bool): True, start a transaction block. Default false.\n\n        \"\"\"\n", "input": "", "output": "        if pipeline:\n            self._pipeline.publish(channel, message)\n        else:\n            self._db.publish(channel, message)", "category": "Python"}, {"instruction": "def exceptionbox(msg=None, title=None):\n    \"\"\"\n    Display a box that gives information about\n    an exception that has just been raised.\n\n    The caller may optionally pass in a title for the window, or a\n    msg to accompany the error information.\n\n    Note that you do not need to (and cannot) pass an exception object\n    as an argument.  The latest exception will automatically be used.\n\n    :param str msg: the msg to be displayed\n    :param str title: the window title\n    :return: None\n\n    \"\"\"\n", "input": "", "output": "    if title is None:\n        title = \"Error Report\"\n    if msg is None:\n        msg = \"An error (exception) has occurred in the program.\"\n\n    codebox(msg, title, ut.exception_format())", "category": "Python"}, {"instruction": "def url_host(url: str) -> str:\n    \"\"\"\n    Parses hostname from URL.\n    :param url: URL\n    :return: hostname\n    \"\"\"\n", "input": "", "output": "    from urllib.parse import urlparse\n    res = urlparse(url)\n    return res.netloc.split(':')[0] if res.netloc else ''", "category": "Python"}, {"instruction": "def _strip_datetime(self, sub_line):\n        \"\"\"Strip datetime and other parts so that there is no redundancy.\"\"\"\n", "input": "", "output": "        try:\n            begin = sub_line.index(']')\n        except ValueError:\n            return sub_line\n        else:\n            # create a \"\" in place character for the beginnings..\n            # needed when interleaving the lists\n            sub = sub_line[begin + 1:]\n            return sub", "category": "Python"}, {"instruction": "def from_file(cls, filepath):\n        \"\"\"Build a :class:`Product` instance from a filepath.\"\"\"\n", "input": "", "output": "        # Find the abinit extension.\n        for i in range(len(filepath)):\n            if filepath[i:] in abi_extensions():\n                ext = filepath[i:]\n                break\n        else:\n            raise ValueError(\"Cannot detect abinit extension in %s\" % filepath)\n\n        return cls(ext, filepath)", "category": "Python"}, {"instruction": "def publish(self, topic, payload, qos, retain):\n        \"\"\"Publish an MQTT message.\"\"\"\n", "input": "", "output": "        self._mqttc.publish(topic, payload, qos, retain)", "category": "Python"}, {"instruction": "def is_correct(self):\n        \"\"\"\n        Check if a group is valid.\n        Valid mean all members exists, so list of unknown_members is empty\n\n        :return: True if group is correct, otherwise False\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        state = True\n\n        # Make members unique, remove duplicates\n        if self.members:\n            self.members = list(set(self.members))\n\n        if self.unknown_members:\n            for member in self.unknown_members:\n                msg = \"[%s::%s] as %s, got unknown member '%s'\" % (\n                    self.my_type, self.get_name(), self.__class__.my_type, member\n                )\n                self.add_error(msg)\n            state = False\n\n        return super(Itemgroup, self).is_correct() and state", "category": "Python"}, {"instruction": "async def rank(self):\n        \"\"\"\n        Try to find a choice in what the user did:\n\n        - If there is a quick reply, then use its payload as choice slug\n        - Otherwise, try to match each choice with its intent\n        \"\"\"\n", "input": "", "output": "        from bernard.platforms.facebook import layers as fbl\n\n        choices = self.request.get_trans_reg('choices')\n\n        if not choices:\n            return\n\n        if self.request.has_layer(fbl.QuickReply):\n            return self._rank_qr(choices)\n        elif self.request.has_layer(l.RawText):\n            return await self._rank_text(choices)", "category": "Python"}, {"instruction": "async def turn_on(self, switch=None):\n        \"\"\"Turn on relay.\"\"\"\n", "input": "", "output": "        if switch is not None:\n            switch = codecs.decode(switch.rjust(2, '0'), 'hex')\n            packet = self.protocol.format_packet(b\"\\x10\" + switch + b\"\\x01\")\n        else:\n            packet = self.protocol.format_packet(b\"\\x0a\")\n        states = await self._send(packet)\n        return states", "category": "Python"}, {"instruction": "def super_kls(self):\n        \"\"\"\n            Determine what kls this group inherits from\n            If default kls should be used, then None is returned\n        \"\"\"\n", "input": "", "output": "        if not self.kls and self.parent and self.parent.name:\n            return self.parent.kls_name\n        return self.kls", "category": "Python"}, {"instruction": "def to_list(obj):\n    \"\"\"\n    Converts an object into a list if it not an iterable, forcing tuples into lists.\n    Nones are returned unchanged.\n    \"\"\"\n", "input": "", "output": "    obj = to_iter(obj)\n\n    if isinstance(obj, type(None)):\n        return None\n    else:\n        return list(obj)", "category": "Python"}, {"instruction": "def save_vlen(self, key, data):\n        \"\"\"\n        Save a sequence of variable-length arrays\n\n        :param key: name of the dataset\n        :param data: data to store as a list of arrays\n        \"\"\"\n", "input": "", "output": "        shape = (None,) + data[0].shape[:-1]\n        try:\n            dset = self[key]\n        except KeyError:\n            vdt = h5py.special_dtype(vlen=data[0].dtype)\n            dset = create(self, key, vdt, shape, fillvalue=None)\n        nbytes = dset.attrs.get('nbytes', 0)\n        totlen = dset.attrs.get('totlen', 0)\n        for i, val in enumerate(data):\n            nbytes += val.nbytes\n            totlen += len(val)\n        length = len(dset)\n        dset.resize((length + len(data),) + shape[1:])\n        for i, arr in enumerate(data):\n            dset[length + i] = arr\n        dset.attrs['nbytes'] = nbytes\n        dset.attrs['totlen'] = totlen", "category": "Python"}, {"instruction": "def arraymax(X,Y):\n    \"\"\"\n    Fast \"vectorized\" max function for element-wise comparison of two numpy arrays.\n\n    For two numpy arrays `X` and `Y` of equal length,\n    return numpy array `Z` such that::\n\n            Z[i] = max(X[i],Y[i])\n\n    **Parameters**\n\n            **X** :  numpy array\n\n                    Numpy array; `len(X) = len(Y)`.\n\n            **Y** :  numpy array\n\n                    Numpy array; `len(Y) = len(X)`.\n\n    **Returns**\n\n            **Z** :  numpy array\n\n                    Numpy array such that `Z[i] = max(X[i],Y[i])`.\n\n    **See Also**\n\n            :func:`tabular.fast.arraymin`\n\n    \"\"\"\n", "input": "", "output": "    Z = np.zeros((len(X),), int)\n    A = X <= Y\n    B = Y < X\n    Z[A] = Y[A]\n    Z[B] = X[B]\n    return Z", "category": "Python"}, {"instruction": "def to_dict(self) -> Dict[str, List[int]]:\n        \"\"\"\n        Returns the contents of this set as a JSON/YAML-ready dictionary.\n        \"\"\"\n", "input": "", "output": "        return {fn: list(lines)\n                for (fn, lines) in self.__contents.items()}", "category": "Python"}, {"instruction": "def measurements(self):\n        \"\"\"Return the measurements associated with this instance.\n\n        if measurements are not present, check if we can model, and then\n        run CRMod to load the measurements.\n        \"\"\"\n", "input": "", "output": "        # check if we have measurements\n        mid = self.assignments.get('measurements', None)\n        if mid is None:\n            return_value = self.model(\n                voltages=True,\n                sensitivities=False,\n                potentials=False,\n            )\n            if return_value is None:\n                print('cannot model')\n                return\n\n        # retrieve measurements\n        cids = self.assignments['measurements']\n        measurements = np.vstack((\n            self.configs.measurements[cids[0]],\n            self.configs.measurements[cids[1]],\n        )).T\n        return measurements", "category": "Python"}, {"instruction": "def bucket_lister(bucket, prefix='', delimiter='', marker='', headers=None):\n    \"\"\"\n    A generator function for listing keys in a bucket.\n    \"\"\"\n", "input": "", "output": "    more_results = True\n    k = None\n    while more_results:\n        rs = bucket.get_all_keys(prefix=prefix, marker=marker,\n                                 delimiter=delimiter, headers=headers)\n        for k in rs:\n            yield k\n        if k:\n            marker = rs.next_marker or k.name\n        more_results= rs.is_truncated", "category": "Python"}, {"instruction": "def warn_sf(messages, response, verbs=None, klass=SalesforceWarning):\n    \"\"\"Issue a warning SalesforceWarning, with message combined from message and data from SFDC response\"\"\"\n", "input": "", "output": "    warnings.warn(klass(messages, response, verbs), stacklevel=2)", "category": "Python"}, {"instruction": "def format_text(text):\n    \"\"\"Remove newlines, but preserve paragraphs\"\"\"\n", "input": "", "output": "    result = \"\"\n    for paragraph in text.split(\"\\n\\n\"):\n        result += \" \".join(paragraph.split()) + \"\\n\\n\"\n\n    result = result.rstrip(\"\\n\")  # Remove last newlines\n\n    # converting links to HTML\n    pattern = r\"(https?:\\/\\/(?:w{1,3}.)?[^\\s]*?(?:\\.[a-z]+)+)\"\n    pattern += r\"(?![^<]*?(?:<\\/\\w+>|\\/?>))\"\n    if re.search(pattern, result):\n        html = r\"<a href='\\1'><font color='FF00CC'>\\1</font></a>\"\n        result = re.sub(pattern, html, result)\n\n    return result", "category": "Python"}, {"instruction": "def encode_uvarint(n, data):\n    '''encodes integer into variable-length format into data.'''\n", "input": "", "output": "    if n < 0:\n        raise ValueError('only support positive integer')\n    while True:\n        this_byte = n & 127\n        n >>= 7\n        if n == 0:\n            data.append(this_byte)\n            break\n        data.append(this_byte | 128)", "category": "Python"}, {"instruction": "def garbage_cycle(index):\n    \"\"\"Get reference cycle details.\"\"\"\n", "input": "", "output": "    graph = _compute_garbage_graphs()[int(index)]\n    graph.reduce_to_cycles()\n    objects = graph.metadata\n    objects.sort(key=lambda x: -x.size)\n    return dict(objects=objects, index=index)", "category": "Python"}, {"instruction": "def handle_404(request, exception):\n    '''Handle 404 Not Found\n    This handler should be used to handle error http 404 not found for all\n    endpoints or if resource not available.\n    '''\n", "input": "", "output": "    error = format_error(title='Resource not found', detail=str(exception))\n    return json(return_an_error(error), status=HTTPStatus.NOT_FOUND)", "category": "Python"}, {"instruction": "def _register_attribute(self, did, checksum, value, account, providers):\n        \"\"\"Register an DID attribute as an event on the block chain.\n\n        :param did: 32 byte string/hex of the DID\n        :param checksum: checksum of the ddo, hex str\n        :param value: url for resolve the did, str\n        :param account: account owner of this DID registration record\n        :param providers: list of providers addresses\n        \"\"\"\n", "input": "", "output": "        assert isinstance(providers, list), ''\n        return self.send_transaction(\n            'registerAttribute',\n            (did,\n             checksum,\n             providers,\n             value),\n            transact={'from': account.address,\n                      'passphrase': account.password}\n        )", "category": "Python"}, {"instruction": "def process_children(self, node, elem, module, path, omit=[]):\n        \"\"\"Proceed with all children of `node`.\"\"\"\n", "input": "", "output": "        for ch in node.i_children:\n            if ch not in omit and (ch.i_config or self.doctype == \"data\"):\n                self.node_handler.get(ch.keyword, self.ignore)(\n                    ch, elem, module, path)", "category": "Python"}, {"instruction": "def create(self, relpath, langs, category):\n    \"\"\"Return a source root at the given `relpath` for the given `langs` and `category`.\n\n    :returns: :class:`SourceRoot`.\n    \"\"\"\n", "input": "", "output": "    return SourceRoot(relpath, tuple(self._canonicalize_langs(langs)), category)", "category": "Python"}, {"instruction": "def reads_to_dataframe(variants_and_allele_reads):\n    \"\"\"\n    Parameters\n    ----------\n    variants_and_allele_reads : sequence\n        List or generator of pairs whose first element is a Variant and\n        whose second element is a sequence of AlleleRead objects.\n    \"\"\"\n", "input": "", "output": "    df_builder = DataFrameBuilder(\n        AlleleRead,\n        extra_column_fns={\n            \"gene\": lambda variant, _: \";\".join(variant.gene_names),\n        })\n    for variant, allele_reads in variants_and_allele_reads:\n        df_builder.add_many(variant, allele_reads)\n    return df_builder.to_dataframe()", "category": "Python"}, {"instruction": "def _get_term_by_id(self, id):\n        '''Simple utility function to load a term.\n        '''\n", "input": "", "output": "        url = (self.url + '/%s.json') % id\n        r = self.session.get(url)\n        return r.json()", "category": "Python"}, {"instruction": "def cxx(source, libraries=[]):\n    r\"\"\"\n    >>> cxx('extern \"C\" { int add(int a, int b) {return a + b;} }').add(40, 2)\n    42\n    \"\"\"\n", "input": "", "output": "    path = _cc_build_shared_lib(source, '.cc', libraries)\n    return ctypes.cdll.LoadLibrary(path)", "category": "Python"}, {"instruction": "def update_user_label(self):\n        \"\"\"\n        finds this parameter and gets the least_unique_twig from the bundle\n\n        \"\"\"\n", "input": "", "output": "        self._user_label = _uniqueid_to_uniquetwig(self._bundle, self.unique_label)\n        self._set_curly_label()", "category": "Python"}, {"instruction": "def help_text(self):\n    \"\"\"Return a string with all config keys and their descriptions.\"\"\"\n", "input": "", "output": "    result = []\n    for name in sorted(self._declarations.keys()):\n      result.append(name)\n      result.append('-' * len(name))\n      decl = self._declarations[name]\n      if decl.description:\n        result.append(decl.description.strip())\n      else:\n        result.append('(no description found)')\n      if decl.has_default:\n        result.append('')\n        quotes = '\"' if type(decl.default_value) is str else ''\n        result.append('  default_value={quotes}{val}{quotes}'.format(\n            quotes=quotes, val=decl.default_value))\n      result.append('')\n      result.append('')\n    return '\\n'.join(result)", "category": "Python"}, {"instruction": "def clear_context(self, app=None):\n        \"\"\"Clear the component's context.\n\n        Keyword Args:\n            app (flask.Flask, optional): The app to clear this component's\n                context for. If omitted, the value from ``Component.app`` is\n                used.\n        \"\"\"\n", "input": "", "output": "        if (app is None and self._context is _CONTEXT_MISSING\n                and not in_app_context()):\n            raise RuntimeError(\"Attempted to clear component context without\"\n                               \" a bound app context or eager app set! Please\"\n                               \" pass the related app you want to update the\"\n                               \" context for!\")\n\n        if self._context is not _CONTEXT_MISSING:\n            self._context = DEFAULT_DICT\n        else:\n            key = self._get_context_name(app=app)\n            setattr(_CONTEXT_LOCALS, key, DEFAULT_DICT)", "category": "Python"}, {"instruction": "def config_sanity_check(self):\n        \"\"\"\n        Base configuration sanity checks\n        \"\"\"\n", "input": "", "output": "        if 'name' not in self.config:\n            raise EventifyConfigError(\n                ", "category": "Python"}, {"instruction": "def delete_queue(name, region, opts=None, user=None):\n    '''\n    Deletes a queue in the region.\n\n    name\n        Name of the SQS queue to deletes\n    region\n        Name of the region to delete the queue from\n\n    opts : None\n        Any additional options to add to the command line\n\n    user : None\n        Run hg as a user other than what the minion runs as\n\n    CLI Example:\n\n        salt '*' aws_sqs.delete_queue <sqs queue> <region>\n\n    '''\n", "input": "", "output": "    queues = list_queues(region, opts, user)\n    url_map = _parse_queue_list(queues)\n\n    log.debug('map %s', url_map)\n    if name in url_map:\n        delete = {'queue-url': url_map[name]}\n\n        rtn = _run_aws(\n            'delete-queue',\n            region=region,\n            opts=opts,\n            user=user,\n            **delete)\n        success = True\n        err = ''\n        out = '{0} deleted'.format(name)\n\n    else:\n        out = ''\n        err = \"Delete failed\"\n        success = False\n\n    ret = {\n        'retcode': 0 if success else 1,\n        'stdout': out,\n        'stderr': err,\n    }\n    return ret", "category": "Python"}, {"instruction": "def get(**kwargs):\n    '''\n    Return system rc configuration variables\n\n    CLI Example:\n\n     .. code-block:: bash\n\n         salt '*' sysrc.get includeDefaults=True\n    '''\n", "input": "", "output": "\n    cmd = 'sysrc -v'\n\n    if 'file' in kwargs:\n        cmd += ' -f '+kwargs['file']\n\n    if 'jail' in kwargs:\n        cmd += ' -j '+kwargs['jail']\n\n    if 'name' in kwargs:\n        cmd += ' '+kwargs['name']\n    elif kwargs.get('includeDefaults', False):\n        cmd += ' -A'\n    else:\n        cmd += ' -a'\n\n    sysrcs = __salt__['cmd.run'](cmd)\n    if \"sysrc: unknown variable\" in sysrcs:\n        # raise CommandExecutionError(sysrcs)\n        return None\n\n    ret = {}\n    for sysrc in sysrcs.split(\"\\n\"):\n        line_components = sysrc.split(': ')\n        rcfile = line_components[0]\n        if len(line_components) > 2:\n            var = line_components[1]\n            val = line_components[2]\n        else:\n            var = line_components[1].rstrip(':')\n            val = ''\n        if rcfile not in ret:\n            ret[rcfile] = {}\n        ret[rcfile][var] = val\n    return ret", "category": "Python"}, {"instruction": "def transpose(self, *dims) -> 'Variable':\n        \"\"\"Return a new Variable object with transposed dimensions.\n\n        Parameters\n        ----------\n        *dims : str, optional\n            By default, reverse the dimensions. Otherwise, reorder the\n            dimensions to this order.\n\n        Returns\n        -------\n        transposed : Variable\n            The returned object has transposed data and dimensions with the\n            same attributes as the original.\n\n        Notes\n        -----\n        This operation returns a view of this variable's data. It is\n        lazy for dask-backed Variables but not for numpy-backed Variables.\n\n        See Also\n        --------\n        numpy.transpose\n        \"\"\"\n", "input": "", "output": "        if len(dims) == 0:\n            dims = self.dims[::-1]\n        axes = self.get_axis_num(dims)\n        if len(dims) < 2:  # no need to transpose if only one dimension\n            return self.copy(deep=False)\n\n        data = as_indexable(self._data).transpose(axes)\n        return type(self)(dims, data, self._attrs, self._encoding,\n                          fastpath=True)", "category": "Python"}, {"instruction": "async def disconnect(self, requested=True):\n        \"\"\"\n        Disconnects this player from it's voice channel.\n        \"\"\"\n", "input": "", "output": "        if self.state == PlayerState.DISCONNECTING:\n            return\n\n        await self.update_state(PlayerState.DISCONNECTING)\n        if not requested:\n            log.debug(\n                f\"Forcing player disconnect for guild {self.channel.guild.id}\"\n                f\" due to player manager request.\"\n            )\n\n        guild_id = self.channel.guild.id\n        voice_ws = self.node.get_voice_ws(guild_id)\n\n        if not voice_ws.closed:\n            await voice_ws.voice_state(guild_id, None)\n\n        await self.node.destroy_guild(guild_id)\n        await self.close()\n\n        self.manager.remove_player(self)", "category": "Python"}, {"instruction": "def padding(self, image, geometry, options):\n        \"\"\"\n        Wrapper for ``_padding``\n        \"\"\"\n", "input": "", "output": "        if options.get('padding') and self.get_image_size(image) != geometry:\n            return self._padding(image, geometry, options)\n        return image", "category": "Python"}, {"instruction": "def players_from_fantasypros_cheatsheet(file):\n    \"\"\"\n    :param File file: path to the file containing the player values in the standard\n    CSV format used for fantasypros cheatsheets\n    :return list(Player): a list of Players based on the info in the cheatsheet\n    \"\"\"\n", "input": "", "output": "\n    with open(file, newline='') as csvfile:\n        # Note - considered using a generator for this, but I don't want to hold the file open\n        # for a long time and it's pretty small anyway.\n        players = []\n        playerindex = 0\n        playerreader = reader(csvfile, delimiter=',')\n        linecount = 0\n        for row in playerreader:\n            if linecount > 1:\n                raw_name = row[0]\n                name = raw_name[:raw_name.find('(') - 1]\n                position_text = raw_name[raw_name.find('(') + 1:raw_name.find('-') - 1]\n                position = Position[position_text]\n                value = int(row[2].replace('$', ''))\n                players.append(Player(name, position, value, linecount-2))\n                playerindex += 1\n            linecount += 1\n    return players", "category": "Python"}, {"instruction": "def detect_regions(bam_in, bed_file, out_dir, prefix):\n    \"\"\"\n    Detect regions using first CoRaL module\n    \"\"\"\n", "input": "", "output": "    bed_file = _reorder_columns(bed_file)\n    counts_reads_cmd = (\"coverageBed -s -counts -b {bam_in} \"\n                        \"-a {bed_file} | sort -k4,4 \"\n                        \"> {out_dir}/loci.cov\")\n    # with tx_tmpdir() as temp_dir:\n    with utils.chdir(out_dir):\n        run(counts_reads_cmd.format(min_trimmed_read_len=min_trimmed_read_len, max_trimmed_read_len=max_trimmed_read_len, **locals()), \"Run counts_reads\")\n        loci_file = _fix_score_column(op.join(out_dir, \"loci.cov\"))\n        return loci_file", "category": "Python"}, {"instruction": "def items(self):\n        \"\"\"\n        Schema or a list of schemas describing particular elements of the object.\n\n        A single schema applies to all the elements. Each element of the object\n        must match that schema. A list of schemas describes particular elements\n        of the object.\n        \"\"\"\n", "input": "", "output": "        value = self._schema.get(\"items\", {})\n        if not isinstance(value, (list, dict)):\n            raise SchemaError(\n                \"items value {0!r} is neither a list nor an object\".\n                format(value))\n        return value", "category": "Python"}, {"instruction": "def loc_info(text, index):\n        '''Location of `index` in source code `text`.'''\n", "input": "", "output": "        if index > len(text):\n            raise ValueError('Invalid index.')\n        line, last_ln = text.count('\\n', 0, index), text.rfind('\\n', 0, index)\n        col = index - (last_ln + 1)\n        return (line, col)", "category": "Python"}, {"instruction": "def from_content_type(cls, content_type):\n\t\t\"\"\"\n\t\tBuild a serializer object from a MIME Content-Type string.\n\n\t\t:param str content_type: The Content-Type string to parse.\n\t\t:return: A new serializer instance.\n\t\t:rtype: :py:class:`.Serializer`\n\t\t\"\"\"\n", "input": "", "output": "\t\tname = content_type\n\t\toptions = {}\n\t\tif ';' in content_type:\n\t\t\tname, options_str = content_type.split(';', 1)\n\t\t\tfor part in options_str.split(';'):\n\t\t\t\tpart = part.strip()\n\t\t\t\tif '=' in part:\n\t\t\t\t\tkey, value = part.split('=')\n\t\t\t\telse:\n\t\t\t\t\tkey, value = (part, None)\n\t\t\t\toptions[key] = value\n\t\t# old style compatibility\n\t\tif name.endswith('+zlib'):\n\t\t\toptions['compression'] = 'zlib'\n\t\t\tname = name[:-5]\n\t\treturn cls(name, charset=options.get('charset', 'UTF-8'), compression=options.get('compression'))", "category": "Python"}, {"instruction": "def _validate_x(self, x, z):\n        \"\"\"Validates x (column), raising error if invalid.\"\"\"\n", "input": "", "output": "\n        if (x < 0) or (x > ((2**z) - 1)):\n            raise InvalidColumnError(\n                \"{} is not a valid value for x (column)\".format(x)\n            )\n\n        return x", "category": "Python"}, {"instruction": "def _get_bios_mappings_resource(self, data):\n        \"\"\"Get the Mappings resource.\n\n        :param data: Existing Bios settings of the server.\n        :returns: mappings settings.\n        :raises: IloCommandNotSupportedError, if resource is not found.\n        :raises: IloError, on an error from iLO.\n        \"\"\"\n", "input": "", "output": "        try:\n            map_uri = data['links']['Mappings']['href']\n        except KeyError:\n            msg = ('Mappings resource not found.')\n            raise exception.IloCommandNotSupportedError(msg)\n\n        status, headers, map_settings = self._rest_get(map_uri)\n        if status != 200:\n            msg = self._get_extended_error(map_settings)\n            raise exception.IloError(msg)\n\n        return map_settings", "category": "Python"}, {"instruction": "def format_table(columns, rows):\n    \"\"\"Formats an ascii table for given columns and rows.\n\n    Parameters\n    ----------\n    columns : list\n        The column names\n    rows : list of tuples\n        The rows in the table. Each tuple must be the same length as\n        ``columns``.\n    \"\"\"\n", "input": "", "output": "    rows = [tuple(str(i) for i in r) for r in rows]\n    columns = tuple(str(i).upper() for i in columns)\n    if rows:\n        widths = tuple(max(max(map(len, x)), len(c))\n                       for x, c in zip(zip(*rows), columns))\n    else:\n        widths = tuple(map(len, columns))\n    row_template = ('    '.join('%%-%ds' for _ in columns)) % widths\n    header = (row_template % tuple(columns)).strip()\n    if rows:\n        data = '\\n'.join((row_template % r).strip() for r in rows)\n        return '\\n'.join([header, data])\n    else:\n        return header", "category": "Python"}, {"instruction": "def update(self, **kwargs):\n        \"\"\"Customize the lazy field\"\"\"\n", "input": "", "output": "        assert not self.called\n        self.kw.update(kwargs)\n        return self", "category": "Python"}, {"instruction": "def on_output_path_textChanged(self):\n        \"\"\"Action when output file name is changed.\"\"\"\n", "input": "", "output": "        output_path = self.output_path.text()\n        output_not_xml_msg = tr('output file is not .tif')\n        if output_path and not output_path.endswith('.tif'):\n            self.warning_text.add(output_not_xml_msg)\n        elif output_path and output_not_xml_msg in self.warning_text:\n            self.warning_text.remove(output_not_xml_msg)\n        self.update_warning()", "category": "Python"}, {"instruction": "def decrypt_file(cls, key, in_filename, out_filename=None, chunksize=24 * 1024):\n        \"\"\" Decrypts a file using AES (CBC mode) with the\n            given key. Parameters are similar to encrypt_file,\n            with one difference: out_filename, if not supplied\n            will be in_filename without its last extension\n            (i.e. if in_filename is 'aaa.zip.enc' then\n            out_filename will be 'aaa.zip')\n        \"\"\"\n", "input": "", "output": "        if not out_filename:\n            out_filename = os.path.splitext(in_filename)[0]\n\n        with open(in_filename, 'rb') as infile:\n            origsize = struct.unpack('<Q', infile.read(struct.calcsize('Q')))[0]\n            iv = infile.read(16)\n            decryptor = AES.new(key, AES.MODE_CBC, iv)\n\n            with open(out_filename, 'wb') as outfile:\n                while True:\n                    chunk = infile.read(chunksize)\n                    if len(chunk) == 0:\n                        break\n                    outfile.write(decryptor.decrypt(chunk))\n\n                outfile.truncate(origsize)", "category": "Python"}, {"instruction": "def get_client(self):\n        \"\"\"Get cache client.\n        \"\"\"\n", "input": "", "output": "        backend_class = import_string(current_app.config.get('CACHE_BACKEND'))\n        backend = backend_class(**current_app.config.get('CACHE_BACKEND_OPTIONS'))\n        return backend", "category": "Python"}, {"instruction": "def to_json(self):\n    \"\"\"Outputs the entire graph.\"\"\"\n", "input": "", "output": "    res_dict = {}\n\n    def gen_dep_edge(node, edge, dep_tgt, aliases):\n      return {\n        'target': dep_tgt.address.spec,\n        'dependency_type': self._edge_type(node.concrete_target, edge, dep_tgt),\n        'products_used': len(edge.products_used),\n        'products_used_ratio': self._used_ratio(dep_tgt, edge),\n        'aliases': [alias.address.spec for alias in aliases],\n      }\n\n    for node in self._nodes.values():\n      res_dict[node.concrete_target.address.spec] = {\n        'cost': self._cost(node.concrete_target),\n        'cost_transitive': self._trans_cost(node.concrete_target),\n        'products_total': node.products_total,\n        'dependencies': [gen_dep_edge(node, edge, dep_tgt, node.dep_aliases.get(dep_tgt, {}))\n                         for dep_tgt, edge in node.dep_edges.items()],\n      }\n    yield str(json.dumps(res_dict, indent=2, sort_keys=True))", "category": "Python"}, {"instruction": "def default_value(self):\n        \"\"\" The default category when making a query\n        \"\"\"\n", "input": "", "output": "        if not hasattr(self, \"_default_value\"):\n            if self.elem_type == \"select\":\n                try:\n                    # Get option marked \"selected\"\n                    def_value = get_option_value(self.elem.select_one(\"[selected]\"))\n                except AttributeError:\n                    # ...or if that one doesen't exist get the first option\n                    def_value = get_option_value(self.elem.select_one(\"option\"))\n\n            elif self.elem_type == \"checkbox\":\n                def_value = self.elem.get(\"value\")\n\n            elif self.elem_type == \"radio\":\n                def_value = [x for x in self.elem if x.has_attr(\"checked\")][0].get(\"value\")\n\n            self._default_value = def_value\n\n            assert def_value is not None\n\n        return self._default_value", "category": "Python"}, {"instruction": "def value(self) -> float:\n        \"\"\"\n        Get a centre-compensated, scaled, value for the axis, taking any dead-zone into account. The value will\n        scale from 0.0 at the edge of the dead-zone to 1.0 (positive) or -1.0 (negative) at the extreme position of\n        the controller or the edge of the hot zone, if defined as other than 1.0. The axis will auto-calibrate for\n        maximum value, initially it will behave as if the highest possible value from the hardware is 0.9 in each\n        direction, and will expand this as higher values are observed. This is scaled by this function and should\n        always return 1.0 or -1.0 at the extreme ends of the axis.\n\n        :return: a float value, negative to the left or down and ranging from -1.0 to 1.0\n        \"\"\"\n", "input": "", "output": "        mapped_value = map_dual_axis(self.min, self.max, self.centre, self.dead_zone, self.hot_zone, self.__value)\n        if self.invert:\n            return -mapped_value\n        else:\n            return mapped_value", "category": "Python"}, {"instruction": "def setClosable(self, state):\n        \"\"\"\n        Sets whether or not the user should be able to close this overlay widget.\n\n        :param      state | <bool>\n        \"\"\"\n", "input": "", "output": "        self._closable = state\n        if state:\n            self._closeButton.show()\n        else:\n            self._closeButton.hide()", "category": "Python"}, {"instruction": "def prepare(self, context):\n\t\t\"\"\"Add the usual suspects to the context.\n\t\t\n\t\tThis adds `request`, `response`, and `path` to the `RequestContext` instance.\n\t\t\"\"\"\n", "input": "", "output": "\t\t\n\t\tif __debug__:\n\t\t\tlog.debug(\"Preparing request context.\", extra=dict(request=id(context)))\n\t\t\n\t\t# Bridge in WebOb `Request` and `Response` objects.\n\t\t# Extensions shouldn't rely on these, using `environ` where possible instead.\n\t\tcontext.request = Request(context.environ)\n\t\tcontext.response = Response(request=context.request)\n\t\t\n\t\t# Record the initial path representing the point where a front-end web server bridged to us.\n\t\tcontext.environ['web.base'] = context.request.script_name\n\t\t\n\t\t# Track the remaining (unprocessed) path elements.\n\t\tcontext.request.remainder = context.request.path_info.split('/')\n\t\tif context.request.remainder and not context.request.remainder[0]:\n\t\t\tdel context.request.remainder[0]\n\t\t\n\t\t# Track the \"breadcrumb list\" of dispatch through distinct controllers.\n\t\tcontext.path = Bread()", "category": "Python"}, {"instruction": "def execute_watch(self, id=None, body=None, params=None):\n        \"\"\"\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api-execute-watch.html>`_\n\n        :arg id: Watch ID\n        :arg body: Execution control\n        :arg debug: indicates whether the watch should execute in debug mode\n        \"\"\"\n", "input": "", "output": "        return self.transport.perform_request(\n            \"PUT\",\n            _make_path(\"_watcher\", \"watch\", id, \"_execute\"),\n            params=params,\n            body=body,\n        )", "category": "Python"}, {"instruction": "def register_method(self, method, fn):\n        \"\"\"Register an HTTP method and handler function.\n\n        :param method: string, HTTP verb\n        :param fn: python function handling the request\n        :raises: RouteAlreadyRegisteredError if the route is already registered\n        :returns: n/a\n        \"\"\"\n", "input": "", "output": "\n        # ensure the HTTP verb is not already registered\n        if method not in self.methods.keys():\n            logger.debug('Service Router ({0} - {1}): Adding method {2} on '\n                         'route {3}'\n                         .format(id(self),\n                                 self.service_name,\n                                 method,\n                                 self.uri))\n            self.methods[method] = fn\n\n        else:\n            raise RouteAlreadyRegisteredError(\n                'Service Router ({0} - {1}): Method {2} already registered '\n                'on Route {3}'\n                .format(id(self),\n                        self.service_name,\n                        method,\n                        self.uri))", "category": "Python"}, {"instruction": "def parse_uri_path(self, path):\n        \"\"\"\n        Given a uri path, return the Redis specific configuration\n        options in that path string according to iana definition\n        http://www.iana.org/assignments/uri-schemes/prov/redis\n\n        :param path: string containing the path. Example: \"/0\"\n        :return: mapping containing the options. Example: {\"db\": \"0\"}\n        \"\"\"\n", "input": "", "output": "        options = {}\n        db, *_ = path[1:].split(\"/\")\n        if db:\n            options[\"db\"] = db\n        return options", "category": "Python"}, {"instruction": "def invoke_with_usage (self, args, **kwargs):\n        \"\"\"Invoke the command with standardized usage-help processing. Same calling\n        convention as `Command.invoke()`.\n\n        \"\"\"\n", "input": "", "output": "        argv0 = kwargs['argv0']\n        usage = self._usage (argv0)\n        argv = [argv0] + args\n        uina = 'long' if self.help_if_no_args else False\n        check_usage (usage, argv, usageifnoargs=uina)\n\n        try:\n            return self.invoke (args, **kwargs)\n        except UsageError as e:\n            wrong_usage (usage, str (e))", "category": "Python"}, {"instruction": "async def _async_connect(self):  # pragma: no cover\n        \"\"\" connect and authenticate to the XMPP server. Async mode. \"\"\"\n", "input": "", "output": "        try:\n            self.conn_coro = self.client.connected()\n            aenter = type(self.conn_coro).__aenter__(self.conn_coro)\n            self.stream = await aenter\n            logger.info(f\"Agent {str(self.jid)} connected and authenticated.\")\n        except aiosasl.AuthenticationFailure:\n            raise AuthenticationFailure(\n                \"Could not authenticate the agent. Check user and password or use auto_register=True\")", "category": "Python"}, {"instruction": "def wait(self, timeout=None):\n        \"\"\"\n        Calls following snippet so you don't have to remember what import. See\n        :py:obj:`WebDriverWait <selenium.webdriver.support.wait.WebDriverWait>` for more\n        information. Detault timeout is `~.default_wait_timeout`.\n\n        .. code-block:: python\n\n            selenium.webdriver.support.wait.WebDriverWait(driver, timeout)\n\n        Example:\n\n        .. code-block:: python\n\n            driver.wait().until(lambda driver: len(driver.find_element_by_id('elm')) > 10)\n\n        If you need to wait for element, consider using\n        :py:meth:`~._WebdriverWrapper.wait_for_element` instead.\n        \"\"\"\n", "input": "", "output": "        if not timeout:\n            timeout = self.default_wait_timeout\n        return WebDriverWait(self, timeout)", "category": "Python"}, {"instruction": "def preproc_data(data, gene_subset=False, **kwargs):\n    \"\"\"\n    basic data preprocessing before running gap score\n\n    Assumes that data is a matrix of shape (genes, cells).\n\n    Returns a matrix of shape (cells, 8), using the first 8 SVD\n    components. Why 8? It's an arbitrary selection...\n    \"\"\"\n", "input": "", "output": "    import uncurl\n    from uncurl.preprocessing import log1p, cell_normalize\n    from sklearn.decomposition import TruncatedSVD\n    data_subset = data\n    if gene_subset:\n        gene_subset = uncurl.max_variance_genes(data)\n        data_subset = data[gene_subset, :]\n    tsvd = TruncatedSVD(min(8, data_subset.shape[0] - 1))\n    data_tsvd = tsvd.fit_transform(log1p(cell_normalize(data_subset)).T)\n    return data_tsvd", "category": "Python"}, {"instruction": "def main_only_quicksetup_rootlogger(level: int = logging.DEBUG,\n                                    with_process_id: bool = False,\n                                    with_thread_id: bool = False) -> None:\n    \"\"\"\n    Quick function to set up the root logger for colour.\n\n    Should ONLY be called from the ``if __name__ == 'main'`` script;\n    see https://docs.python.org/3.4/howto/logging.html#library-config.\n\n    Args:\n        level: log level to set\n        with_process_id: include the process ID in the logger's name?\n        with_thread_id: include the thread ID in the logger's name?\n    \"\"\"\n", "input": "", "output": "    # Nasty. Only call from \"if __name__ == '__main__'\" clauses!\n    rootlogger = logging.getLogger()\n    configure_logger_for_colour(rootlogger, level, remove_existing=True,\n                                with_process_id=with_process_id,\n                                with_thread_id=with_thread_id)", "category": "Python"}, {"instruction": "def OnPadIntCtrl(self, event):\n        \"\"\"Pad IntCtrl event handler\"\"\"\n", "input": "", "output": "\n        self.attrs[\"pad\"] = event.GetValue()\n\n        post_command_event(self, self.DrawChartMsg)", "category": "Python"}, {"instruction": "def stack_suffix(self, context_sensitivity_level):\n        \"\"\"\n        Generate the stack suffix. A stack suffix can be used as the key to a SimRun in CFG recovery.\n\n        :param int context_sensitivity_level: Level of context sensitivity.\n        :return: A tuple of stack suffix.\n        :rtype: tuple\n        \"\"\"\n", "input": "", "output": "\n        ret = ()\n\n        for frame in self:\n            if len(ret) >= context_sensitivity_level*2:\n                break\n            ret = (frame.call_site_addr, frame.func_addr) + ret\n\n        while len(ret) < context_sensitivity_level*2:\n            ret = (None, None) + ret\n\n        return ret", "category": "Python"}, {"instruction": "def device_configuration(self, pending=False, use_included=False):\n        \"\"\"Get a specific device configuration.\n\n        A device can have at most one loaded and one pending device\n        configuration. This returns that device_configuration based on\n        a given flag.\n\n        Keyword Args:\n\n            pending(bool): Fetch the pending configuration or return\n                the loaded one.\n\n            use_included(bool): Use included resources in this device\n                configuration.\n\n        Returns:\n\n            The requested loaded or pending configuration or None if\n            no device configuration is found.\n\n        \"\"\"\n", "input": "", "output": "        device_configs = self.device_configurations(use_included=use_included)\n        for device_config in device_configs:\n            if device_config.is_loaded() is not pending:\n                return device_config\n        return None", "category": "Python"}, {"instruction": "def _start_loop(self, websocket, event_handler):\n\t\t\"\"\"\n\t\tWe will listen for websockets events, sending a heartbeat/pong everytime\n\t\twe react a TimeoutError. If we don't the webserver would close the idle connection,\n\t\tforcing us to reconnect.\n\t\t\"\"\"\n", "input": "", "output": "\t\tlog.debug('Starting websocket loop')\n\t\twhile True:\n\t\t\ttry:\n\t\t\t\tyield from asyncio.wait_for(\n\t\t\t\t\tself._wait_for_message(websocket, event_handler),\n\t\t\t\t\ttimeout=self.options['timeout']\n\t\t\t\t)\n\t\t\texcept asyncio.TimeoutError:\n\t\t\t\tyield from websocket.pong()\n\t\t\t\tlog.debug(\"Sending heartbeat...\")\n\t\t\t\tcontinue", "category": "Python"}, {"instruction": "def _should_include_member(self, name, member):\n    \"\"\"Returns True if this member should be included in the document.\"\"\"\n", "input": "", "output": "    # Always exclude symbols matching _always_drop_symbol_re.\n    if _always_drop_symbol_re.match(name):\n      return False\n    # Finally, exclude any specifically-excluded symbols.\n    if name in self._exclude_symbols:\n      return False\n    return True", "category": "Python"}, {"instruction": "def applyLM(parentBeam, childBeam, classes, lm):\n    \"\"\"\n    calculate LM score of child beam by taking score from parent beam and bigram probability of last two chars\n    \"\"\"\n", "input": "", "output": "    if lm and not childBeam.lmApplied:\n        c1 = classes[parentBeam.labeling[-1] if parentBeam.labeling else classes.index(' ')] # first char\n        c2 = classes[childBeam.labeling[-1]] # second char\n        lmFactor = 0.01 # influence of language model\n        bigramProb = lm.getCharBigram(c1, c2) ** lmFactor # probability of seeing first and second char next to each other\n        childBeam.prText = parentBeam.prText * bigramProb # probability of char sequence\n        childBeam.lmApplied = True", "category": "Python"}, {"instruction": "def check_link_status():\n    '''check status of master links'''\n", "input": "", "output": "    tnow = time.time()\n    if mpstate.status.last_message != 0 and tnow > mpstate.status.last_message + 5:\n        say(\"no link\")\n        mpstate.status.heartbeat_error = True\n    for master in mpstate.mav_master:\n        if not master.linkerror and (tnow > master.last_message + 5 or master.portdead):\n            say(\"link %u down\" % (master.linknum+1))\n            master.linkerror = True", "category": "Python"}, {"instruction": "def get_usage(self, program_name=None):\n        \"\"\"Get the commandline usage string for this experiment.\"\"\"\n", "input": "", "output": "        program_name = os.path.relpath(program_name or sys.argv[0] or 'Dummy',\n                                       self.base_dir)\n        commands = OrderedDict(self.gather_commands())\n        options = gather_command_line_options()\n        long_usage = format_usage(program_name, self.doc, commands, options)\n        # internal usage is a workaround because docopt cannot handle spaces\n        # in program names. So for parsing we use 'dummy' as the program name.\n        # for printing help etc. we want to use the actual program name.\n        internal_usage = format_usage('dummy', self.doc, commands, options)\n        short_usage = printable_usage(long_usage)\n        return short_usage, long_usage, internal_usage", "category": "Python"}, {"instruction": "def open_notification(self):\n        \"\"\"\n        Open notification\n\n        Built in support for Android 4.3 (API level 18)\n\n        Using swipe action as a workaround for API level lower than 18\n\n        \"\"\"\n", "input": "", "output": "        sdk_version = self.device.info['sdkInt']\n        if sdk_version < 18:\n            height = self.device.info['displayHeight']\n            self.device.swipe(1, 1, 1, height - 1, 1)\n        else:\n            self.device.open.notification()", "category": "Python"}, {"instruction": "def delete_files():\n    \"\"\" Delete one or more files from the server \"\"\"\n", "input": "", "output": "\n    session_token = request.headers['session_token']\n    repository    = request.headers['repository']\n\n    #===\n    current_user = have_authenticated_user(request.environ['REMOTE_ADDR'], repository, session_token)\n    if current_user is False: return fail(user_auth_fail_msg)\n\n    #===\n    repository_path = config['repositories'][repository]['path']\n    body_data = request.get_json()\n\n    def with_exclusive_lock():\n        if not varify_user_lock(repository_path, session_token): return fail(lock_fail_msg)\n\n        try:\n            data_store = versioned_storage(repository_path)\n            if not data_store.have_active_commit(): return fail(no_active_commit_msg)\n\n            #-------------\n            for fle in json.loads(body_data['files']):\n                data_store.fs_delete(fle)\n\n            # updates the user lock expiry\n            update_user_lock(repository_path, session_token)\n            return success()\n        except Exception: return fail() # pylint: disable=broad-except\n    return lock_access(repository_path, with_exclusive_lock)", "category": "Python"}, {"instruction": "def move_top_cards(self, other, number=1):\n        \"\"\"\n        Move the top `number` of cards to the top of some `other` deck.\n\n        By default only one card will be moved if `number` is not specified.\n        \"\"\"\n", "input": "", "output": "        other.cards.append(reversed(self.cards[-number:]))", "category": "Python"}, {"instruction": "def execute(self, sensor_graph, scope_stack):\n        \"\"\"Execute this statement on the sensor_graph given the current scope tree.\n\n        This function will likely modify the sensor_graph and will possibly\n        also add to or remove from the scope_tree.  If there are children nodes\n        they will be called after execute_before and before execute_after,\n        allowing block statements to sandwich their children in setup and teardown\n        functions.\n\n        Args:\n            sensor_graph (SensorGraph): The sensor graph that we are building or\n                modifying\n            scope_stack (list(Scope)): A stack of nested scopes that may influence\n                how this statement allocates clocks or other stream resources.\n        \"\"\"\n", "input": "", "output": "\n        self.execute_before(sensor_graph, scope_stack)\n\n        for child in self.children:\n            child.execute(sensor_graph, scope_stack)\n\n        self.execute_after(sensor_graph, scope_stack)", "category": "Python"}, {"instruction": "def iterator(self):\n        \"\"\"\n        An iterator over the results from applying this QuerySet to the\n        database.\n        \"\"\"\n", "input": "", "output": "        if not self._result_cache:\n            len(self)\n        for r in self._result_cache:\n            yield r", "category": "Python"}, {"instruction": "def autoconfigure(filename=None, config=None, force=False):\n    \"\"\"Run ``file_config`` and ``env_config`` if the module has not\n    been initialized.\n    \"\"\"\n", "input": "", "output": "    if not force and is_configured():\n        logger.debug('System already configured, skipping autoconfiguration')\n        return\n\n    # start with the current configuration\n    newconfig = bigchaindb.config\n\n    # update configuration from file\n    try:\n        newconfig = update(newconfig, file_config(filename=filename))\n    except FileNotFoundError as e:\n        if filename:\n            raise\n        else:\n            logger.info('Cannot find config file `%s`.' % e.filename)\n\n    # override configuration with env variables\n    newconfig = env_config(newconfig)\n    if config:\n        newconfig = update(newconfig, config)\n    set_config(newconfig)", "category": "Python"}, {"instruction": "def slice_pdf(input_filename: str, output_filename: str,\n              slice_horiz: int, slice_vert: int) -> str:\n    \"\"\"\n    Slice each page of the original, to convert to \"one real page per PDF\n    page\". Return the output filename.\n    \"\"\"\n", "input": "", "output": "    if slice_horiz == 1 and slice_vert == 1:\n        log.debug(\"No slicing required\")\n        return input_filename  # nothing to do\n    log.info(\"Slicing each source page mv into {} horizontally x {} vertically\",\n             slice_horiz, slice_vert)\n    log.debug(\"... from {!r} to {!r}\", input_filename, output_filename)\n    require(MUTOOL, HELP_MISSING_MUTOOL)\n    run([\n        MUTOOL,\n        \"poster\",\n        \"-x\", str(slice_horiz),\n        \"-y\", str(slice_vert),\n        input_filename,\n        output_filename\n    ])\n    return output_filename", "category": "Python"}, {"instruction": "def can_view(self, user):\n        \"\"\"\n        Returns True if user has permission to render this view.\n\n        At minimum this requires an active staff user. If the required_groups\n        attribute is not empty then the user must be a member of at least one\n        of those groups. If there are no required groups set for the view but\n        required groups are set for the bundle then the user must be a member\n        of at least one of those groups. If there are no groups to check this\n        will return True.\n        \"\"\"\n", "input": "", "output": "\n        if user.is_staff and user.is_active:\n            if user.is_superuser:\n                return True\n            elif self.required_groups:\n                return self._user_in_groups(user, self.required_groups)\n            elif self.bundle.required_groups:\n                return self._user_in_groups(user, self.bundle.required_groups)\n            else:\n                return True\n\n        return False", "category": "Python"}, {"instruction": "def object_types():\n        \"\"\"\n        Show all available 'entry points' available for searching. An entry\n        point defines a uri that provides unfiltered access to all elements\n        of the entry point type.\n\n        :return: list of entry points by name\n        :rtype: list(str)\n        \"\"\"\n", "input": "", "output": "        # Return all elements from the root of the API nested under elements URI\n        #element_uri = str(\n        types = [element.rel\n                 for element in entry_point()]\n        types.extend(list(CONTEXTS))\n        return types", "category": "Python"}, {"instruction": "def createURLParserCtxt(filename, options):\n    \"\"\"Create a parser context for a file or URL content.\n      Automatic support for ZLIB/Compress compressed document is\n      provided by default if found at compile-time and for file\n       accesses \"\"\"\n", "input": "", "output": "    ret = libxml2mod.xmlCreateURLParserCtxt(filename, options)\n    if ret is None:raise parserError('xmlCreateURLParserCtxt() failed')\n    return parserCtxt(_obj=ret)", "category": "Python"}, {"instruction": "def training_env():  # type: () -> _env.TrainingEnv\n    \"\"\"Create a TrainingEnv.\n\n    Returns:\n        TrainingEnv: an instance of TrainingEnv\n    \"\"\"\n", "input": "", "output": "\n    from sagemaker_containers import _env\n\n    return _env.TrainingEnv(\n        resource_config=_env.read_resource_config(),\n        input_data_config=_env.read_input_data_config(),\n        hyperparameters=_env.read_hyperparameters())", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Close the notification.\"\"\"\n", "input": "", "output": "        with self.selenium.context(self.selenium.CONTEXT_CHROME):\n            if self.window.firefox_version > 63:\n                self.find_primary_button().click()\n                self.window.wait_for_notification(None)\n            else:\n                BaseNotification.close(self)", "category": "Python"}, {"instruction": "def GetUnscannedSubNode(self):\n    \"\"\"Retrieves the first unscanned sub node.\n\n    Returns:\n      SourceScanNode: sub scan node or None if not available.\n    \"\"\"\n", "input": "", "output": "    if not self.sub_nodes and not self.scanned:\n      return self\n\n    for sub_node in self.sub_nodes:\n      result = sub_node.GetUnscannedSubNode()\n      if result:\n        return result\n\n    return None", "category": "Python"}, {"instruction": "def data(cls, cube, weighted, prune):\n        \"\"\"Return ndarray representing table index by margin.\"\"\"\n", "input": "", "output": "        return cls()._data(cube, weighted, prune)", "category": "Python"}, {"instruction": "def _run_on_completion(self, func, *args, **kwargs):\n        \"\"\"Function to call when request has finished, after having been _set(). The first argument passed to func will\n        be the request itself. Additional parameters are NOT validated. If the request is already finished, the given\n        function will be run immediately (in same thread).\n        \"\"\"\n", "input": "", "output": "        if self._complete_func is not None:\n            raise ValueError('Completion function already set for %s: %s' % (self.id_, self._complete_func))\n\n        if not self.__event.is_set():\n            self._complete_func = partial(func, self, *args, **kwargs)\n        else:\n            self.__run_completion_func(partial(func, self, *args, **kwargs), self.id_)", "category": "Python"}, {"instruction": "def goal(self, goal_name, count=1):\n        \"\"\"Record that this user has performed a particular goal\n\n        This will update the goal stats for all experiments the user is enrolled in.\"\"\"\n", "input": "", "output": "        for enrollment in self._get_all_enrollments():\n            if enrollment.experiment.is_displaying_alternatives():\n                self._experiment_goal(enrollment.experiment, enrollment.alternative, goal_name, count)", "category": "Python"}, {"instruction": "def register_signal_handler(self, signal_name, handler_function):\n        \"\"\"register `handler_function` to receive `signal_name`.\n\n        Uses class's dbus interface self.IFACE, objects name self.name\n        and objects path self.OBJ_PATH to match signal.\n\n        :param str signal_name: The signal name;\n                                None(default) matches all names.\n        :param function handler_function: The function to be called.\n        \"\"\"\n", "input": "", "output": "        self.bus.add_signal_receiver(signal_wrapper(handler_function),\n                                     signal_name=signal_name,\n                                     dbus_interface=self.IFACE,\n                                     bus_name=self.name,\n                                     path=self.OBJ_PATH)", "category": "Python"}, {"instruction": "def getPageContent(url, session, max_content_bytes=MaxContentBytes):\n    \"\"\"Get text content of given URL.\"\"\"\n", "input": "", "output": "    check_robotstxt(url, session)\n    # read page data\n    try:\n        page = urlopen(url, session, max_content_bytes=max_content_bytes)\n    except IOError:\n        page = urlopen(url, session, max_content_bytes=max_content_bytes)\n    data = page.text\n    tries = MaxRetries\n    while not isValidPageContent(data) and tries > 0:\n        time.sleep(RetryPauseSeconds)\n        page = urlopen(url, session, max_content_bytes=max_content_bytes)\n        data = page.text\n        tries -= 1\n    if not isValidPageContent(data):\n        raise ValueError(\"Got invalid page content from %s: %r\" % (url, data))\n    out.debug(u\"Got page content %r\" % data, level=3)\n    return data", "category": "Python"}, {"instruction": "def common_prefix(s1, s2):\n\t\t\"\"\"\n\t\tReturn the common prefix of two lines.\n\t\t\"\"\"\n", "input": "", "output": "\t\tindex = min(len(s1), len(s2))\n\t\twhile s1[:index] != s2[:index]:\n\t\t\tindex -= 1\n\t\treturn s1[:index]", "category": "Python"}, {"instruction": "def derive_rad39_corr(self, bt11, bt13, method='rosenfeld'):\n        \"\"\"Derive the 3.9 radiance correction factor to account for the\n        attenuation of the emitted 3.9 radiance by CO2\n        absorption. Requires the 11 micron window band and the 13.4\n        CO2 absorption band, as e.g. available on SEVIRI. Currently\n        only supports the Rosenfeld method\n        \"\"\"\n", "input": "", "output": "\n        if method != 'rosenfeld':\n            raise AttributeError(\"Only CO2 correction for SEVIRI using \"\n                                 \"the Rosenfeld equation is supported!\")\n\n        LOG.debug(\"Derive the 3.9 micron radiance CO2 correction coefficent\")\n        self._rad3x_correction = (bt11 - 0.25 * (bt11 - bt13)) ** 4 / bt11 ** 4", "category": "Python"}, {"instruction": "def is_valid_imaging_dicom(dicom_header):\n    \"\"\"\n    Function will do some basic checks to see if this is a valid imaging dicom\n    \"\"\"\n", "input": "", "output": "    # if it is philips and multiframe dicom then we assume it is ok\n    try:\n        if is_philips([dicom_header]):\n            if is_multiframe_dicom([dicom_header]):\n                return True\n\n        if \"SeriesInstanceUID\" not in dicom_header:\n            return False\n\n        if \"InstanceNumber\" not in dicom_header:\n            return False\n\n        if \"ImageOrientationPatient\" not in dicom_header or len(dicom_header.ImageOrientationPatient) < 6:\n            return False\n\n        if \"ImagePositionPatient\" not in dicom_header or len(dicom_header.ImagePositionPatient) < 3:\n            return False\n\n        # for all others if there is image position patient we assume it is ok\n        if Tag(0x0020, 0x0037) not in dicom_header:\n            return False\n\n        return True\n    except (KeyError, AttributeError):\n        return False", "category": "Python"}, {"instruction": "def wrap(lower, upper, x):\n    \"\"\"\n    Circularly alias the numeric value x into the range [lower,upper).\n\n    Valid for cyclic quantities like orientations or hues.\n    \"\"\"\n", "input": "", "output": "    #I have no idea how I came up with this algorithm; it should be simplified.\n    #\n    # Note that Python's % operator works on floats and arrays;\n    # usually one can simply use that instead.  E.g. to wrap array or\n    # scalar x into 0,2*pi, just use \"x % (2*pi)\".\n    range_=upper-lower\n    return lower + np.fmod(x-lower + 2*range_*(1-np.floor(x/(2*range_))), range_)", "category": "Python"}, {"instruction": "def disapprovewitness(ctx, witnesses, account):\n    \"\"\" Disapprove witness(es)\n    \"\"\"\n", "input": "", "output": "    pprint(ctx.peerplays.disapprovewitness(witnesses, account=account))", "category": "Python"}, {"instruction": "def runSearchReadGroupSets(self, request):\n        \"\"\"\n        Runs the specified SearchReadGroupSetsRequest.\n        \"\"\"\n", "input": "", "output": "        return self.runSearchRequest(\n            request, protocol.SearchReadGroupSetsRequest,\n            protocol.SearchReadGroupSetsResponse,\n            self.readGroupSetsGenerator)", "category": "Python"}, {"instruction": "def _scale_mesh(self, scale):\n        \"\"\"\n        TODO: add documentation\n        \"\"\"\n", "input": "", "output": "        pos_ks = ['vertices', 'centers']\n\n        # TODO: scale velocities???\n\n        # handle scale\n        self.update_columns_dict({k: self[k]*scale for k in pos_ks})\n\n        self.update_columns(areas=self.areas*(scale**2))\n        self._volume *= scale**3\n        if self._area is not None:\n            # self._area is None for wd meshes\n            self._area += scale**2", "category": "Python"}, {"instruction": "def generate_public_key(self):\n        \"\"\"\n        Generates public key.\n\n        :return: void\n        :rtype: void\n        \"\"\"\n", "input": "", "output": "        self.public_key = pow(self.generator,\n                              self.__private_key,\n                              self.prime)", "category": "Python"}, {"instruction": "def get_dataset(self, key, info):\n        \"\"\"Load a dataset\"\"\"\n", "input": "", "output": "        with h5py.File(self.filename, 'r') as fid:\n            LOGGER.debug('Reading %s.', key.name)\n            if key.name in DSET_NAMES:\n                m_data = read_dataset(fid, key)\n            else:\n                m_data = read_geo(fid, key)\n        m_data.attrs.update(info)\n        m_data.attrs['sensor'] = self.sensor\n\n        return m_data", "category": "Python"}, {"instruction": "def run(self, *args, **kwargs):\n    \"\"\"Runs command on every job in the run.\"\"\"\n", "input": "", "output": "\n    for job in self.jobs:\n      job.run(*args, **kwargs)", "category": "Python"}, {"instruction": "def verify(value, msg):\n    \"\"\"\n    C-style validator\n\n    Keyword arguments:\n    value -- dictionary to validate (required)\n    msg -- the protobuf schema to validate against (required)\n\n    Returns:\n        True: If valid input\n        False: If invalid input\n    \"\"\"\n", "input": "", "output": "    return bool(value) and \\\n           converts_to_proto(value, msg) and \\\n           successfuly_encodes(msg) and \\\n           special_typechecking(value, msg)", "category": "Python"}, {"instruction": "def fiscal_code(self, gender: Optional[Gender] = None) -> str:\n        \"\"\"Return a random fiscal code.\n\n        :param gender: Gender's enum object.\n        :return: Fiscal code.\n\n        Example:\n            RSSMRA66R05D612U\n        \"\"\"\n", "input": "", "output": "        code = ''.join(self.random.choices(string.ascii_uppercase, k=6))\n\n        code += self.random.custom_code(mask='##')\n\n        month_codes = self._data['fiscal_code']['month_codes']\n        code += self.random.choice(month_codes)\n\n        birth_day = self.random.randint(101, 131)\n        self._validate_enum(gender, Gender)\n        if gender == Gender.FEMALE:\n            birth_day += 40\n        code += str(birth_day)[1:]\n\n        city_letters = self._data['fiscal_code']['city_letters']\n        code += self.random.choice(city_letters)\n        code += self.random.custom_code(mask='###@')\n\n        return code", "category": "Python"}, {"instruction": "def is_sh(executable):\n    \"\"\"Determine if the specified executable is a .sh (contains a #! line)\"\"\"\n", "input": "", "output": "    try:\n        with io.open(executable, encoding='latin-1') as fp:\n            magic = fp.read(2)\n    except (OSError, IOError):\n        return executable\n    return magic == '#!'", "category": "Python"}, {"instruction": "def color_string(self, x):\r\n        \"\"\"Return a string formatted delta for the values in x.\r\n\r\n        Args:\r\n            x: 2-item list of integers (representing number of calls) or\r\n               2-item list of floats (representing seconds of runtime).\r\n\r\n        Returns:\r\n            A list with [formatted x[0], [color, formatted delta]], where\r\n            color reflects whether x[1] is lower, greater, or the same as\r\n            x[0].\r\n        \"\"\"\n", "input": "", "output": "        diff_str = \"\"\r\n        color = \"black\"\r\n\r\n        if len(x) == 2 and self.compare_file is not None:\r\n            difference = x[0] - x[1]\r\n            if difference:\r\n                color, sign = ('green', '-') if difference < 0 else ('red', '+')\r\n                diff_str = '{}{}'.format(sign, self.format_measure(difference))\r\n        return [self.format_measure(x[0]), [diff_str, color]]", "category": "Python"}, {"instruction": "def set_default_proxy(cls, value):\n        \"\"\"\n        Default: None (no proxy)\n\n        A string that will be used to tell each request must be sent through this proxy server.\n        Use the scheme://hostname:port form.\n        If you need to use a proxy, you can configure individual requests with the proxies argument to any request\n        method.\n        \"\"\"\n", "input": "", "output": "        if value is None:\n            cls.DEFAULT_PROXY = None\n        else:\n            scheme, host, port = get_hostname_parameters_from_url(value)\n            cls.DEFAULT_PROXY = \"%s://%s:%s\" % (scheme, host, port)", "category": "Python"}, {"instruction": "def hungarian(A, B):\n    \"\"\"\n    Hungarian reordering.\n\n    Assume A and B are coordinates for atoms of SAME type only\n    \"\"\"\n", "input": "", "output": "\n    # should be kabasch here i think\n    distances = cdist(A, B, 'euclidean')\n\n    # Perform Hungarian analysis on distance matrix between atoms of 1st\n    # structure and trial structure\n    indices_a, indices_b = linear_sum_assignment(distances)\n\n    return indices_b", "category": "Python"}, {"instruction": "def focusInEvent(self, event):\r\n        \"\"\"\r\n        Processes when this widget recieves focus.\r\n        \r\n        :param      event | <QFocusEvent>\r\n        \"\"\"\n", "input": "", "output": "        if not self.signalsBlocked():\r\n            self.focusChanged.emit(True)\r\n            self.focusEntered.emit()\r\n        \r\n        return super(XTextEdit, self).focusInEvent(event)", "category": "Python"}, {"instruction": "def addObserver(self, observer):\n        \"\"\"Add an observer.\"\"\"\n", "input": "", "output": "        Observable.addObserver(self, observer)\n\n        # If self.startOnDemand is True, the reader monitoring\n        # thread only runs when there are observers.\n        if self.startOnDemand:\n            if 0 < self.countObservers():\n                if not self.rmthread:\n                    self.rmthread = ReaderMonitoringThread(\n                        self,\n                        self.readerProc, self.period)\n\n                    # start reader monitoring thread in another thread to\n                    # avoid a deadlock; addObserver and notifyObservers called\n                    # in the ReaderMonitoringThread run() method are\n                    # synchronized\n                    try:\n                        # Python 3.x\n                        import _thread\n                        _thread.start_new_thread(self.rmthread.start, ())\n                    except:\n                        # Python 2.x\n                        import thread\n                        thread.start_new_thread(self.rmthread.start, ())\n        else:\n            observer.update(self, (self.rmthread.readers, []))", "category": "Python"}, {"instruction": "def set_time(self, time):\n        \"\"\"\n        \u65f6\u95f4\u72b6\u6001\n        \"\"\"\n", "input": "", "output": "        rest_time = int(self.song_total_time) - self.time - 1\n        minute = int(rest_time) / 60\n        sec = int(rest_time) % 60\n\n        return str(minute).zfill(2) + ':' + str(sec).zfill(2)", "category": "Python"}, {"instruction": "def to_json(self):\n        \"\"\"Convert the Design Day to a dictionary.\"\"\"\n", "input": "", "output": "        return {\n            'name': self.name,\n            'day_type': self.day_type,\n            'location': self.location.to_json(),\n            'dry_bulb_condition': self.dry_bulb_condition.to_json(),\n            'humidity_condition': self.humidity_condition.to_json(),\n            'wind_condition': self.wind_condition.to_json(),\n            'sky_condition': self.sky_condition.to_json()\n        }", "category": "Python"}, {"instruction": "def row(self, fields):\n        \"\"\"Return a row for fields, for CSV files, pretty printing, etc, give a set of fields to return\"\"\"\n", "input": "", "output": "\n        d = self.dict\n\n        row = [None] * len(fields)\n\n        for i, f in enumerate(fields):\n            if f in d:\n                row[i] = d[f]\n\n        return row", "category": "Python"}, {"instruction": "def change_grid_shape(self, shape):\n        \"\"\"Grid shape change event handler, marks content as changed\"\"\"\n", "input": "", "output": "\n        # Mark content as changed\n        post_command_event(self.main_window, self.ContentChangedMsg)\n\n        self.code_array.shape = shape\n\n        # Update TableChoiceIntCtrl\n        post_command_event(self.main_window, self.ResizeGridMsg, shape=shape)\n\n        # Change grid table dimensions\n        self.grid.GetTable().ResetView()\n\n        # Clear caches\n        self.code_array.result_cache.clear()", "category": "Python"}, {"instruction": "def getImageDescriptor(self, im, xy=None):\n        \"\"\" getImageDescriptor(im, xy=None)\n\n        Used for the local color table properties per image.\n        Otherwise global color table applies to all frames irrespective of\n        whether additional colors comes in play that require a redefined\n        palette. Still a maximum of 256 color per frame, obviously.\n\n        Written by Ant1 on 2010-08-22\n        Modified by Alex Robinson in Janurari 2011 to implement subrectangles.\n\n        \"\"\"\n", "input": "", "output": "\n        # Defaule use full image and place at upper left\n        if xy is None:\n            xy = (0, 0)\n\n        # Image separator,\n        bb = '\\x2C'\n\n        # Image position and size\n        bb += intToBin(xy[0])  # Left position\n        bb += intToBin(xy[1])  # Top position\n        bb += intToBin(im.size[0])  # image width\n        bb += intToBin(im.size[1])  # image height\n\n        # packed field: local color table flag1, interlace0, sorted table0,\n        # reserved00, lct size111=7=2^(7+1)=256.\n\n        bb += '\\x87'\n\n        # LZW minimum size code now comes later, begining of [image data] blocks\n        return bb", "category": "Python"}, {"instruction": "def wrap_json(cls, json, viewers=None, channels=None):\n        \"\"\"Create a Game instance for the given json\n\n        :param json: the dict with the information of the game\n        :type json: :class:`dict`\n        :param viewers: The viewer count\n        :type viewers: :class:`int`\n        :param channels: The viewer count\n        :type channels: :class:`int`\n        :returns: the new game instance\n        :rtype: :class:`Game`\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        g = Game(name=json.get('name'),\n                 box=json.get('box'),\n                 logo=json.get('logo'),\n                 twitchid=json.get('_id'),\n                 viewers=viewers,\n                 channels=channels)\n        return g", "category": "Python"}, {"instruction": "def _inner(self, x, y):\n        \"\"\"Return ``self.inner(x, y)``.\"\"\"\n", "input": "", "output": "        if self.is_uniform and not self.is_uniformly_weighted:\n            # TODO: implement without copying x\n            bdry_fracs = self.partition.boundary_cell_fractions\n            func_list = _scaling_func_list(bdry_fracs, exponent=1.0)\n            x_arr = apply_on_boundary(x, func=func_list, only_once=False)\n            return super(DiscreteLp, self)._inner(self.element(x_arr), y)\n        else:\n            return super(DiscreteLp, self)._inner(x, y)", "category": "Python"}, {"instruction": "def normalizeLibValue(value):\n    \"\"\"\n    Normalizes lib value.\n\n    * **value** must not be ``None``.\n    * Returned value is the same type as the input value.\n    \"\"\"\n", "input": "", "output": "    if value is None:\n        raise ValueError(\"Lib value must not be None.\")\n    if isinstance(value, (list, tuple)):\n        for v in value:\n            normalizeLibValue(v)\n    elif isinstance(value, dict):\n        for k, v in value.items():\n            normalizeLibKey(k)\n            normalizeLibValue(v)\n    elif isinstance(value, basestring):\n        value = unicode(value)\n    return value", "category": "Python"}, {"instruction": "def connect_mturk(aws_access_key_id=None, aws_secret_access_key=None, **kwargs):\n    \"\"\"\n    :type aws_access_key_id: string\n    :param aws_access_key_id: Your AWS Access Key ID\n\n    :type aws_secret_access_key: string\n    :param aws_secret_access_key: Your AWS Secret Access Key\n\n    :rtype: :class:`boto.mturk.connection.MTurkConnection`\n    :return: A connection to MTurk\n    \"\"\"\n", "input": "", "output": "    from boto.mturk.connection import MTurkConnection\n    return MTurkConnection(aws_access_key_id, aws_secret_access_key, **kwargs)", "category": "Python"}, {"instruction": "def update_recurring_payments_profile(self, profileid, **kwargs):\n        \"\"\"Shortcut to the UpdateRecurringPaymentsProfile method.\n\n        ``profileid`` is the same profile id used for getting profile details.\n\n        The keyed arguments are data in the payment profile which you wish to\n        change. The profileid does not change. Anything else will take the new\n        value. Most of, though not all of, the fields available are shared\n        with creating a profile, but for the complete list of parameters, you\n        can visit the following URI:\n        https://www.x.com/docs/DOC-1212\n        \"\"\"\n", "input": "", "output": "        kwargs.update(self._sanitize_locals(locals()))\n        return self._call('UpdateRecurringPaymentsProfile', **kwargs)", "category": "Python"}, {"instruction": "def stopped(name, connection=None, username=None, password=None):\n    '''\n    Stops a VM by shutting it down nicely.\n\n    .. versionadded:: 2016.3.0\n\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    .. code-block:: yaml\n\n        domain_name:\n          virt.stopped\n    '''\n", "input": "", "output": "\n    return _virt_call(name, 'shutdown', 'stopped', \"Machine has been shut down\",\n                      connection=connection, username=username, password=password)", "category": "Python"}, {"instruction": "def on_episode_end(self, episode, logs):\n        \"\"\" Compute and print metrics at the end of each episode \"\"\"\n", "input": "", "output": "        duration = timeit.default_timer() - self.starts[episode]\n\n        metrics = self.metrics[episode]\n        if np.isnan(metrics).all():\n            mean_metrics = np.array([np.nan for _ in self.metrics_names])\n        else:\n            mean_metrics = np.nanmean(metrics, axis=0)\n        assert len(mean_metrics) == len(self.metrics_names)\n\n        data = list(zip(self.metrics_names, mean_metrics))\n        data += list(logs.items())\n        data += [('episode', episode), ('duration', duration)]\n        for key, value in data:\n            if key not in self.data:\n                self.data[key] = []\n            self.data[key].append(value)\n\n        if self.interval is not None and episode % self.interval == 0:\n            self.save_data()\n\n        # Clean up.\n        del self.metrics[episode]\n        del self.starts[episode]", "category": "Python"}, {"instruction": "def filename(cls, tag, schemas, ext='.rnc'):\n        \"\"\"given a tag and a list of schemas, return the filename of the schema.\n        If schemas is a string, treat it as a comma-separated list.\n        \"\"\"\n", "input": "", "output": "        if type(schemas)==str: \n            schemas = re.split(\"\\s*,\\s*\", schemas)\n        for schema in schemas:\n            fn = os.path.join(schema, cls.dirname(tag), cls.basename(tag, ext=ext))\n            if os.path.exists(fn):\n                return fn", "category": "Python"}, {"instruction": "def decyear2dt(t):\n    \"\"\"Convert decimal year to datetime\n    \"\"\"\n", "input": "", "output": "    year = int(t)\n    rem = t - year \n    base = datetime(year, 1, 1)\n    dt = base + timedelta(seconds=(base.replace(year=base.year+1) - base).total_seconds() * rem)\n    #This works for np array input\n    #year = t.astype(int)\n    #rem = t - year \n    #base = np.array([datetime(y, 1, 1) for y in year])\n    return dt", "category": "Python"}, {"instruction": "def url_for(self, operation, _external=True, **kwargs):\n        \"\"\"\n        Construct a URL for an operation against a resource.\n\n        :param kwargs: additional arguments for URL path expansion,\n            which are passed to flask.url_for.\n            In particular, _external=True produces absolute url.\n\n        \"\"\"\n", "input": "", "output": "        return url_for(self.endpoint_for(operation), _external=_external, **kwargs)", "category": "Python"}, {"instruction": "def init_app(kls):\n    \"\"\"\n    To bind middlewares, plugins that needs the 'app' object to init\n    Bound middlewares will be assigned on cls.init()\n    \"\"\"\n", "input": "", "output": "    if not hasattr(kls, \"__call__\"):\n        raise exceptions.MochaError(\"init_app: '%s' is not callable\" % kls)\n    Mocha._init_apps.add(kls)\n    return kls", "category": "Python"}, {"instruction": "def print_classifications(self, cls, data):\n        \"\"\"\n        Prints the classifications to the buffer.\n\n        :param cls: the classifier\n        :type cls: Classifier\n        :param data: the test data\n        :type data: Instances\n        \"\"\"\n", "input": "", "output": "        javabridge.call(\n            self.jobject, \"printClassifications\", \"(Lweka/classifiers/Classifier;Lweka/core/Instances;)V\",\n            cls.jobject, data.jobject)", "category": "Python"}, {"instruction": "def _set_config(xpath, element):\n    '''\n    Sends a set request to the device.\n\n    '''\n", "input": "", "output": "    query = {'type': 'config',\n             'action': 'set',\n             'xpath': xpath,\n             'element': element}\n\n    response = __proxy__['panos.call'](query)\n\n    return _validate_response(response)", "category": "Python"}, {"instruction": "def post_process_model(self, input_model, add_line_refs):\n        \"\"\"\n        This function defines the order and execution logic for actions\n        that are performed in the model post-processing pipeline.\n        :param input_model: The YANG model to be processed in the pipeline\n        :param add_line_refs: Flag that controls whether line number\n            references should be added to the model.\n        :return: List of strings that constitute the final YANG model to\n            be written to its module file.\n        \"\"\"\n", "input": "", "output": "        intermediate_model = self.remove_leading_spaces(input_model)\n        intermediate_model = self.remove_extra_empty_lines(intermediate_model)\n        if add_line_refs:\n            intermediate_model = self.add_line_references(intermediate_model)\n        return finalize_model(intermediate_model)", "category": "Python"}, {"instruction": "def get_parent_bank_ids(self, bank_id):\n        \"\"\"Gets the parent ``Ids`` of the given bank.\n\n        arg:    bank_id (osid.id.Id): a bank ``Id``\n        return: (osid.id.IdList) - the parent ``Ids`` of the bank\n        raise:  NotFound - ``bank_id`` is not found\n        raise:  NullArgument - ``bank_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure occurred\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.BinHierarchySession.get_parent_bin_ids\n        if self._catalog_session is not None:\n            return self._catalog_session.get_parent_catalog_ids(catalog_id=bank_id)\n        return self._hierarchy_session.get_parents(id_=bank_id)", "category": "Python"}, {"instruction": "def update(self):\n        \"\"\"\n        Get virtual and resident size of current process via 'ps'.\n        This should work for MacOS X, Solaris, Linux. Returns true if it was\n        successful.\n        \"\"\"\n", "input": "", "output": "        try:\n            p = Popen(['/bin/ps', '-p%s' % self.pid, '-o', 'rss,vsz'],\n                      stdout=PIPE, stderr=PIPE)\n        except OSError: # pragma: no cover\n            pass\n        else:\n            s = p.communicate()[0].split()\n            if p.returncode == 0 and len(s) >= 2: # pragma: no branch\n                self.vsz = int(s[-1]) * 1024\n                self.rss = int(s[-2]) * 1024\n                return True\n        return False", "category": "Python"}, {"instruction": "def get_mail_addresses(message, header_name):\n    \"\"\"\n    Retrieve all email addresses from one message header.\n    \"\"\"\n", "input": "", "output": "    headers = [h for h in message.get_all(header_name, [])]\n    addresses = email.utils.getaddresses(headers)\n\n    for index, (address_name, address_email) in enumerate(addresses):\n        addresses[index] = {'name': decode_mail_header(address_name),\n                            'email': address_email}\n        logger.debug(\"{} Mail address in message: <{}> {}\".format(\n            header_name.upper(), address_name, address_email))\n    return addresses", "category": "Python"}, {"instruction": "def get_raw_not_managed(self, data):\n        \"\"\"Get elements not managed. They can be used as is.\n\n        :param data: the data to proceed\n\n        \"\"\"\n", "input": "", "output": "        keys = ['also', 'ref', 'note', 'other', 'example', 'method', 'attr']\n        elems = [self.opt[k] for k in self.opt if k in keys]\n        data = data.splitlines()\n        start = 0\n        init = 0\n        raw = ''\n        spaces = None\n        while start != -1:\n            start, end = self.get_next_section_lines(data[init:])\n            if start != -1:\n                init += start\n                if isin_alone(elems, data[init]) and \\\n                        not isin_alone([self.opt[e] for e in self.excluded_sections], data[init]):\n                    spaces = get_leading_spaces(data[init])\n                    if end != -1:\n                        section = [d.replace(spaces, '', 1).rstrip() for d in data[init:init + end]]\n                    else:\n                        section = [d.replace(spaces, '', 1).rstrip() for d in data[init:]]\n                    raw += '\\n'.join(section) + '\\n'\n                init += 2\n        return raw", "category": "Python"}, {"instruction": "def nginx(ctx, hostname):\n    \"\"\"Install nginx configuration\"\"\"\n", "input": "", "output": "\n    install_nginx(ctx.obj['dbhost'], ctx.obj['dbname'], ctx.obj['port'], hostname)", "category": "Python"}, {"instruction": "def plotallanvar(data, dt, tmax=10, ax=None, **kwargs):\n    \"\"\"Plot Allan variance.\n\n    Args:\n        data (np.ndarray): Input data.\n        dt (float): Time between each data.\n        tmax (float): Maximum time.\n        ax (matplotlib.axes): Axis the figure is plotted on.\n        kwargs (optional): Plot options passed to ax.plot().\n    \"\"\"\n", "input": "", "output": "    if ax is None:\n        ax = plt.gca()\n    tk, allanvar = allan_variance(data, dt, tmax)\n    ax.loglog(tk, allanvar, **kwargs)\n    ax.set_xlabel('Time [s]')\n    ax.set_ylabel('Allan Variance')\n    ax.legend()", "category": "Python"}, {"instruction": "def _construct_unique_id(self, id_prefix, lines):\n        \"\"\"Constructs a unique ID for a particular prompt in this case,\n        based on the id_prefix and the lines in the prompt.\n        \"\"\"\n", "input": "", "output": "        text = []\n        for line in lines:\n            if isinstance(line, str):\n                text.append(line)\n            elif isinstance(line, CodeAnswer):\n                text.append(line.dump())\n        return id_prefix + '\\n' + '\\n'.join(text)", "category": "Python"}, {"instruction": "def _normalize_projection(projection):\n        \"\"\"Helper:  convert field paths to message.\"\"\"\n", "input": "", "output": "        if projection is not None:\n\n            fields = list(projection.fields)\n\n            if not fields:\n                field_ref = query_pb2.StructuredQuery.FieldReference(\n                    field_path=\"__name__\"\n                )\n                return query_pb2.StructuredQuery.Projection(fields=[field_ref])\n\n        return projection", "category": "Python"}, {"instruction": "def tags(self, extra_params=None):\n        \"\"\"\n        All Tags in this Ticket\n        \"\"\"\n", "input": "", "output": "\n        # Default params\n        params = {\n            'per_page': settings.MAX_PER_PAGE,\n        }\n\n        if extra_params:\n            params.update(extra_params)\n\n        return self.api._get_json(\n            Tag,\n            space=self,\n            rel_path=self.space._build_rel_path(\n                'tickets/%s/tags' % self['number']\n            ),\n            extra_params=params,\n            get_all=True,  # Retrieve all tags in the ticket\n        )", "category": "Python"}, {"instruction": "def trunc_str(s: str) -> str:\n    \"\"\"Truncate strings to maximum length.\"\"\"\n", "input": "", "output": "    if len(s) > max_str_size:\n        i = max(0, (max_str_size - 3) // 2)\n        j = max(0, max_str_size - 3 - i)\n        s = s[:i] + \"...\" + s[-j:]\n    return s", "category": "Python"}, {"instruction": "def requestMarketDepth(self, contracts=None, num_rows=10):\n        \"\"\"\n        Register to streaming market data updates\n        https://www.interactivebrokers.com/en/software/api/apiguide/java/reqmktdepth.htm\n        \"\"\"\n", "input": "", "output": "\n        if num_rows > 10:\n            num_rows = 10\n\n        if contracts == None:\n            contracts = list(self.contracts.values())\n        elif not isinstance(contracts, list):\n            contracts = [contracts]\n\n        for contract in contracts:\n            tickerId = self.tickerId(self.contractString(contract))\n            self.ibConn.reqMktDepth(\n                tickerId, contract, num_rows)", "category": "Python"}, {"instruction": "def option(\n        value,\n        default='',\n        omit_opts=False,\n        omit_master=False,\n        omit_pillar=False):\n    '''\n    Pass in a generic option and receive the value that will be assigned\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' config.option redis.host\n    '''\n", "input": "", "output": "    if not omit_opts:\n        if value in __opts__:\n            return __opts__[value]\n    if not omit_master:\n        if value in __pillar__.get('master', {}):\n            return __pillar__['master'][value]\n    if not omit_pillar:\n        if value in __pillar__:\n            return __pillar__[value]\n    if value in DEFAULTS:\n        return DEFAULTS[value]\n    return default", "category": "Python"}, {"instruction": "def _interpolate(self, factor, minLayer, maxLayer, round=True,\n                     suppressError=True):\n        \"\"\"\n        This is the environment implementation of\n        :meth:`BaseLayer.interpolate`.\n\n        Subclasses may override this method.\n        \"\"\"\n", "input": "", "output": "        for glyphName in self.keys():\n            del self[glyphName]\n        for glyphName in minLayer.keys():\n            if glyphName not in maxLayer:\n                continue\n            minGlyph = minLayer[glyphName]\n            maxGlyph = maxLayer[glyphName]\n            dstGlyph = self.newGlyph(glyphName)\n            dstGlyph.interpolate(factor, minGlyph, maxGlyph,\n                                 round=round, suppressError=suppressError)", "category": "Python"}, {"instruction": "def initialize_workflow(self, workflow):\n        \"\"\"\n        Create a workflow\n        workflow - a workflow class\n        \"\"\"\n", "input": "", "output": "        self.workflow = workflow()\n        self.workflow.tasks = self.tasks\n\n        self.workflow.input_file = self.input_file\n        self.workflow.input_format = self.input_format\n        self.workflow.target_file = self.target_file\n        self.workflow.target_format = self.target_format\n        self.workflow.run_id = self.run_id\n\n        self.workflow.setup()", "category": "Python"}, {"instruction": "def read_gist_file(self, gist):\n        \"\"\"\n        Returns the contents of file hosted inside a gist at GitHub.\n        :param gist: (dict) gist parsed by GitHubTools._parse()\n        :return: (bytes) content of a gist loaded from GitHub\n        \"\"\"\n", "input": "", "output": "        url = False\n        files = gist.get(\"files\")\n        for gist_file in files:\n            if gist_file.get(\"filename\") == self.filename:\n                url = gist_file.get(\"raw_url\")\n                break\n        if url:\n            self.output(\"Reading {}\".format(url))\n            response = self.requests.get(url)\n            return response.content", "category": "Python"}, {"instruction": "def same_tech(self, other: Union[UnitTypeId, Set[UnitTypeId], List[UnitTypeId], Dict[UnitTypeId, Any]]) -> \"Units\":\n        \"\"\" Usage:\n        'self.units.same_tech(UnitTypeId.COMMANDCENTER)' or 'self.units.same_tech(UnitTypeId.ORBITALCOMMAND)'\n        returns all CommandCenter, CommandCenterFlying, OrbitalCommand, OrbitalCommandFlying, PlanetaryFortress\n        This also works with a set/list/dict parameter, e.g. 'self.units.same_tech({UnitTypeId.COMMANDCENTER, UnitTypeId.SUPPLYDEPOT})'\n        Untested: This should return the equivalents for Hatchery, WarpPrism, Observer, Overseer, SupplyDepot and others\n        \"\"\"\n", "input": "", "output": "        if isinstance(other, UnitTypeId):\n            other = {other}\n        tech_alias_types = set(other)\n        for unitType in other:\n            tech_alias = self.game_data.units[unitType.value].tech_alias\n            if tech_alias:\n                for same in tech_alias:\n                    tech_alias_types.add(same)\n        return self.filter(\n            lambda unit: unit.type_id in tech_alias_types\n            or unit._type_data.tech_alias is not None\n            and any(same in tech_alias_types for same in unit._type_data.tech_alias)\n        )", "category": "Python"}, {"instruction": "def _proxy(self):\n        \"\"\"\n        Generate an instance context for the instance, the context is capable of\n        performing various actions.  All instance actions are proxied to the context\n\n        :returns: ParticipantContext for this ParticipantInstance\n        :rtype: twilio.rest.video.v1.room.room_participant.ParticipantContext\n        \"\"\"\n", "input": "", "output": "        if self._context is None:\n            self._context = ParticipantContext(\n                self._version,\n                room_sid=self._solution['room_sid'],\n                sid=self._solution['sid'],\n            )\n        return self._context", "category": "Python"}, {"instruction": "def setter(self, fset):\n        \"\"\"\n        To be used as a decorator. Will define the decorated method\n        as a write pipe method to be called when client writes to the pipe\n        \"\"\"\n", "input": "", "output": "        self.fset = fset\n        self.pipe_write = PipeWriteType.PIPE_READ_WRITE\n        return self", "category": "Python"}, {"instruction": "def searchable_object_types(self):\n        \"\"\"List of (object_types, friendly name) present in the index.\"\"\"\n", "input": "", "output": "        try:\n            idx = self.index()\n        except KeyError:\n            # index does not exists: service never started, may happens during\n            # tests\n            return []\n\n        with idx.reader() as r:\n            indexed = sorted(set(r.field_terms(\"object_type\")))\n        app_indexed = self.app_state.indexed_fqcn\n\n        return [(name, friendly_fqcn(name)) for name in indexed if name in app_indexed]", "category": "Python"}, {"instruction": "def min(self):\n        \"\"\"\n        Compute the min across records.\n        \"\"\"\n", "input": "", "output": "        return self._constructor(self.values.min(axis=self.baseaxes, keepdims=True))", "category": "Python"}, {"instruction": "def deepcopy_bound(name):\n    '''\n    Compatibility helper function to allow copy.deepcopy copy bound methods\n    which is broken on Python 2.6, due to the following bug:\n    https://bugs.python.org/issue1515\n\n    Warnings:\n        - This method will mutate the global deepcopy dispatcher, which means that\n        this function is NOT threadsafe!\n\n        - Not Py3 compatible. The intended use case is deepcopy compat for Py2.6\n\n    '''\n", "input": "", "output": "    def _deepcopy_method(x, memo):\n        return type(x)(x.im_func, copy.deepcopy(x.im_self, memo), x.im_class)  # pylint: disable=incompatible-py3-code\n    try:\n        pre_dispatch = copy._deepcopy_dispatch\n        copy._deepcopy_dispatch[types.MethodType] = _deepcopy_method\n        ret = copy.deepcopy(name)\n    finally:\n        copy._deepcopy_dispatch = pre_dispatch\n    return ret", "category": "Python"}, {"instruction": "def status_unfavourite(self, id):\n        \"\"\"\n        Un-favourite a status.\n\n        Returns a `toot dict`_ with the un-favourited status.\n        \"\"\"\n", "input": "", "output": "        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/unfavourite'.format(str(id))\n        return self.__api_request('POST', url)", "category": "Python"}, {"instruction": "def lats(self, degrees=True):\n        \"\"\"\n        Return the latitudes of each row of the gridded data.\n\n        Usage\n        -----\n        lats = x.lats([degrees])\n\n        Returns\n        -------\n        lats : ndarray, shape (nlat)\n            1-D numpy array of size nlat containing the latitude of each row\n            of the gridded data.\n\n        Parameters\n        -------\n        degrees : bool, optional, default = True\n            If True, the output will be in degrees. If False, the output will\n            be in radians.\n        \"\"\"\n", "input": "", "output": "        if degrees is False:\n            return _np.radians(self._lats())\n        else:\n            return self._lats()", "category": "Python"}, {"instruction": "def estimate(coll, filter={}, sample=1):\n    \"\"\"\n    Estimate the number of documents in the collection\n    matching the filter.\n\n    Sample may be a fixed number of documents to sample\n    or a percentage of the total collection size.\n\n    >>> coll = getfixture('bulky_collection')\n    >>> estimate(coll)\n    100\n    >>> query = {\"val\": {\"$gte\": 50}}\n    >>> val = estimate(coll, filter=query)\n    >>> val > 0\n    True\n    >>> val = estimate(coll, filter=query, sample=10)\n    >>> val > 0\n    True\n    >>> val = estimate(coll, filter=query, sample=.1)\n    >>> val > 0\n    True\n    \"\"\"\n", "input": "", "output": "    total = coll.estimated_document_count()\n    if not filter and sample == 1:\n        return total\n    if sample <= 1:\n        sample *= total\n    pipeline = list(builtins.filter(None, [\n        {'$sample': {'size': sample}} if sample < total else {},\n        {'$match': filter},\n        {'$count': 'matched'},\n    ]))\n    docs = next(coll.aggregate(pipeline))\n    ratio = docs['matched'] / sample\n    return int(total * ratio)", "category": "Python"}, {"instruction": "def getConf(self, conftype):\n        '''\n        conftype must be a Zooborg constant\n        '''\n", "input": "", "output": "        if conftype not in [ZooConst.CLIENT, ZooConst.WORKER, ZooConst.BROKER]:\n            raise Exception('Zooborg.getConf: invalid type')\n\n        zooconf={}\n\n        #TODO: specialconf entries for the mock\n\n        if conftype == ZooConst.CLIENT:\n            zooconf['broker'] = {}\n            zooconf['broker']['connectionstr'] = b\"tcp://localhost:5555\"\n\n        elif conftype == ZooConst.BROKER:\n            zooconf['bindstr']=b\"tcp://*:5555\"\n\n        elif conftype == ZooConst.WORKER:\n            zooconf['broker'] = {}\n            zooconf['broker']['connectionstr'] = b\"tcp://localhost:5555\"\n\n        else:\n            raise Exception(\"ZooBorgconftype unknown\")\n\n\n        return zooconf", "category": "Python"}, {"instruction": "def _create_table():\n    '''\n    Create table if needed\n    '''\n    # Explicitely check if the table already exists as the library logs a\n    # warning on CREATE TABLE\n    query = \"\"\"SELECT COUNT(TABLE_NAME) FROM information_schema.tables\n        WHERE table_schema = '{0}' AND table_name = '{1}'\"\"\"\n", "input": "", "output": "            _mysql_kwargs['db'],\n            _table_name,\n        )\n    cur, _ = run_query(client, query)\n    r = cur.fetchone()\n    cur.close()\n    if r[0] == 1:\n        return\n\n    query = ", "category": "Python"}, {"instruction": "def savgol_filter(x, window_length, polyorder, deriv=0, delta=1.0, axis=-1, mode='interp', cval=0.0):\n    '''\n    Wrapper for the scipy.signal.savgol_filter function that handles Nan values.\n\n    See: https://github.com/wheeler-microfluidics/dmf-control-board-firmware/issues/3\n\n    Returns\n    -------\n    y : ndarray, same shape as `x`\n        The filtered data.\n    '''\n", "input": "", "output": "    # linearly interpolate missing values before filtering\n    x = np.ma.masked_invalid(pd.Series(x).interpolate())\n\n    try:\n        # start filtering from the first non-zero value since these won't be addressed by\n        # the interpolation above\n        ind = np.isfinite(x).nonzero()[0][0]\n        x[ind:] = signal.savgol_filter(x[ind:], window_length, polyorder, deriv,\n                                       delta, axis, mode, cval)\n    except IndexError:\n        pass\n    return np.ma.masked_invalid(x)", "category": "Python"}, {"instruction": "def get_byte(self, i):\n        \"\"\"Get byte.\"\"\"\n", "input": "", "output": "\n        value = []\n        for x in range(2):\n            c = next(i)\n            if c.lower() in _HEX:\n                value.append(c)\n            else:  # pragma: no cover\n                raise SyntaxError('Invalid byte character at %d!' % (i.index - 1))\n        return ''.join(value)", "category": "Python"}, {"instruction": "def pull(self):\n        \"\"\"Enable or disable internal pull-up resistors for this pin.  A\n        value of digitalio.Pull.UP will enable a pull-up resistor, and None will\n        disable it.  Pull-down resistors are NOT supported!\n        \"\"\"\n", "input": "", "output": "        if _get_bit(self._mcp.gppu, self._pin):\n            return digitalio.Pull.UP\n        return None", "category": "Python"}, {"instruction": "def _set_matplotlib_default_backend():\n    \"\"\"\n    matplotlib will try to print to a display if it is available, but don't want\n    to run it in interactive mode. we tried setting the backend to 'Agg'' before\n    importing, but it was still resulting in issues. we replace the existing\n    backend with 'agg' in the default matplotlibrc. This is a hack until we can\n    find a better solution\n    \"\"\"\n", "input": "", "output": "    if _matplotlib_installed():\n        import matplotlib\n        matplotlib.use('Agg', force=True)\n        config = matplotlib.matplotlib_fname()\n        if os.access(config, os.W_OK):\n            with file_transaction(config) as tx_out_file:\n                with open(config) as in_file, open(tx_out_file, \"w\") as out_file:\n                    for line in in_file:\n                        if line.split(\":\")[0].strip() == \"backend\":\n                            out_file.write(\"backend: agg\\n\")\n                        else:\n                            out_file.write(line)", "category": "Python"}, {"instruction": "def tAx(mt, x, t):\n    \"\"\" n/Ax : Returns the EPV (net single premium) of a deferred whole life insurance. \"\"\"\n", "input": "", "output": "    return mt.Mx[x + t] / mt.Dx[x]", "category": "Python"}, {"instruction": "def pp_xml(body):\n    \"\"\"Pretty print format some XML so it's readable.\"\"\"\n", "input": "", "output": "    pretty = xml.dom.minidom.parseString(body)\n    return pretty.toprettyxml(indent=\"  \")", "category": "Python"}, {"instruction": "def _decode_signature(self, signature):\n        \"\"\"\n            Decode the internal fields of the base64-encoded signature.\n        \"\"\"\n", "input": "", "output": "\n        sig = a2b_base64(signature)\n        if len(sig) != 65:\n            raise EncodingError(\"Wrong length, expected 65\")\n\n        # split into the parts.\n        first = byte2int(sig)\n        r = from_bytes_32(sig[1:33])\n        s = from_bytes_32(sig[33:33+32])\n\n        # first byte encodes a bits we need to know about the point used in signature\n        if not (27 <= first < 35):\n            raise EncodingError(\"First byte out of range\")\n\n        # NOTE: The first byte encodes the \"recovery id\", or \"recid\" which is a 3-bit values\n        # which selects compressed/not-compressed and one of 4 possible public pairs.\n        #\n        first -= 27\n        is_compressed = bool(first & 0x4)\n\n        return is_compressed, (first & 0x3), r, s", "category": "Python"}, {"instruction": "def get_dimension_array(array):\n    \"\"\"\n    Get dimension of an array getting the number of rows and the max num of\n    columns.\n    \"\"\"\n", "input": "", "output": "    if all(isinstance(el, list) for el in array):\n        result = [len(array), len(max([x for x in array], key=len,))]\n\n    # elif array and isinstance(array, list):\n    else:\n        result = [len(array), 1]\n\n    return result", "category": "Python"}, {"instruction": "def define_component(self, body, type_token, def_name, param_defs):\n        \"\"\"\n        Given component definition, recurse to another ComponentVisitor\n        to define a new component\n        \"\"\"\n", "input": "", "output": "        for subclass in ComponentVisitor.__subclasses__():\n            if subclass.comp_type == self._CompType_Map[type_token.type]:\n                visitor = subclass(self.compiler, def_name, param_defs)\n                return visitor.visit(body)\n        raise RuntimeError", "category": "Python"}, {"instruction": "def getmembers(self):\n        \"\"\"\n        :return: list of members as name, type tuples\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        return filter(\n            lambda m: not m[0].startswith(\"__\") and not inspect.isfunction(m[1]) and not inspect.ismethod(m[1]),\n            inspect.getmembers(self.__class__)\n        )", "category": "Python"}, {"instruction": "def replace(self, key, value, time=0, compress_level=-1):\n        \"\"\"\n        Replace a key/value to server ony if it does exist.\n\n        :param key: Key's name\n        :type key: six.string_types\n        :param value: A value to be stored on server.\n        :type value: object\n        :param time: Time in seconds that your key will expire.\n        :type time: int\n        :param compress_level: How much to compress.\n            0 = no compression, 1 = fastest, 9 = slowest but best,\n            -1 = default compression level.\n        :type compress_level: int\n        :return: True if key is replace False if key does not exists\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        server = self._get_server(key)\n        return server.replace(key, value, time, compress_level)", "category": "Python"}, {"instruction": "def _summarize_object_type(model):\n    \"\"\"\n        This function returns the summary for a given model\n    \"\"\"\n", "input": "", "output": "    # the fields for the service's model\n    model_fields = {field.name: field for field in list(model.fields())}\n    # summarize the model\n    return {\n        'fields': [{\n            'name': key,\n            'type': type(convert_peewee_field(value)).__name__\n            } for key, value in model_fields.items()\n        ]\n    }", "category": "Python"}, {"instruction": "def printer(data, depth=0):\n    \"\"\"\n    Prepare data for printing.\n\n    :param data: a data value that will be processed by method\n    :param int depth: recurrency indicator, to maintain proper indent\n\n    :returns: string with formatted config\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    indent = _INDENT * depth\n    config_string = '' if not depth else ':\\n'\n    if isinstance(data, dict):\n        for key, val in data.items():\n            line = '{0}{1}'.format(indent, key)\n            values = printer(val, depth + 1)\n            if not values.count('\\n'):\n                values = ': {0}'.format(values.lstrip())\n\n            line = '{line}{values}'.format(line=line, values=values)\n            config_string += '{0}\\n'.format(line)\n\n    elif isinstance(data, list):\n        for elem in data:\n            config_string += '{0} - {1}\\n'.format(indent, elem)\n    else:\n        config_string = '{0}{1} ({2})'.format(\n            indent, data, data.__class__.__name__\n        )\n\n    return config_string.rstrip('\\n')", "category": "Python"}, {"instruction": "def _buildGraph(self):\n        \"\"\"\n        transforms the triples list into a proper rdflib graph\n        (which can be used later for querying)\n        \"\"\"\n", "input": "", "output": "        for n in self.namespaces:\n            self.rdflib_graph.bind(n[0], rdflib.Namespace(n[1]))\n        if self.triples:\n            for terzetto in self.triples:\n                self.rdflib_graph.add(terzetto)", "category": "Python"}, {"instruction": "def query_target(target_chembl_id):\n    \"\"\"Query ChEMBL API target by id\n\n    Parameters\n    ----------\n    target_chembl_id : str\n\n    Returns\n    -------\n    target : dict\n        dict parsed from json that is unique for the target\n    \"\"\"\n", "input": "", "output": "    query_dict = {'query': 'target',\n                  'params': {'target_chembl_id': target_chembl_id,\n                             'limit': 1}}\n    res = send_query(query_dict)\n    target = res['targets'][0]\n    return target", "category": "Python"}, {"instruction": "def _path_condition_name(self, api_id, path):\n        \"\"\"\n        Generate valid condition logical id from the given API logical id and swagger resource path.\n        \"\"\"\n", "input": "", "output": "        # only valid characters for CloudFormation logical id are [A-Za-z0-9], but swagger paths can contain\n        # slashes and curly braces for templated params, e.g., /foo/{customerId}. So we'll replace\n        # non-alphanumeric characters.\n        path_logical_id = path.replace('/', 'SLASH').replace('{', 'OB').replace('}', 'CB')\n        return '{}{}PathCondition'.format(api_id, path_logical_id)", "category": "Python"}, {"instruction": "def Cache(fn):\n   \"\"\" Function cache decorator \"\"\"\n", "input": "", "output": "   def fnCache(*args, **kwargs):\n      ", "category": "Python"}, {"instruction": "def _split_row(self, row, border):\r\n        \"\"\" split a row of text into list of cells. \"\"\"\n", "input": "", "output": "        if border:\r\n            if row.startswith('|'):\r\n                row = row[1:]\r\n            if row.endswith('|'):\r\n                row = row[:-1]\r\n        return row.split('|')", "category": "Python"}, {"instruction": "def format_props(props, prop_template=\"{{k}} = { {{v}} }\", delim=\"\\n\"):\n    \"\"\" Formats props for the React template.\n\n        Args:\n            props (dict): properties to be written to the template.\n\n        Returns:\n            Two lists, one containing variable names and the other\n            containing a list of props to be fed to the React template.\n\n    \"\"\"\n", "input": "", "output": "    vars_ = []\n    props_ = []\n    for k, v in list(props.items()):\n        vars_.append(Template(\"var {{k}} = {{v}};\").render(k=k,v=json.dumps(v)))\n        props_.append(Template(prop_template).render(k=k, v=k))\n    return \"\\n\".join(vars_), delim.join(props_)", "category": "Python"}, {"instruction": "def get(self, queue, no_ack=False):\n        \"\"\"Receive a message from a declared queue by name.\n\n        :returns: A :class:`Message` object if a message was received,\n            ``None`` otherwise. If ``None`` was returned, it probably means\n            there was no messages waiting on the queue.\n\n        \"\"\"\n", "input": "", "output": "        raw_message = self.channel.basic_get(queue, no_ack=no_ack)\n        if not raw_message:\n            return None\n        return self.message_to_python(raw_message)", "category": "Python"}, {"instruction": "def d2logpdf_dlink2_dvar(self, inv_link_f, y, Y_metadata=None):\n        \"\"\"\n        Gradient of the hessian (d2logpdf_dlink2) w.r.t variance parameter (t_noise)\n\n        .. math::\n            \\\\frac{d}{d\\\\sigma^{2}}(\\\\frac{d^{2} \\\\ln p(y_{i}|\\lambda(f_{i}))}{d^{2}f}) = \\\\frac{v(v+1)(\\\\sigma^{2}v - 3(y_{i} - \\lambda(f_{i}))^{2})}{(\\\\sigma^{2}v + (y_{i} - \\lambda(f_{i}))^{2})^{3}}\n\n        :param inv_link_f: latent variables link(f)\n        :type inv_link_f: Nx1 array\n        :param y: data\n        :type y: Nx1 array\n        :param Y_metadata: Y_metadata which is not used in student t distribution\n        :returns: derivative of hessian evaluated at points f and f_j w.r.t variance parameter\n        :rtype: Nx1 array\n        \"\"\"\n", "input": "", "output": "        e = y - inv_link_f\n        d2logpdf_dlink2_dvar = ( (self.v*(self.v+1)*(self.sigma2*self.v - 3*(e**2)))\n                              / ((self.sigma2*self.v + (e**2))**3)\n                           )\n        return d2logpdf_dlink2_dvar", "category": "Python"}, {"instruction": "def AddArguments(cls, argument_group):\n    \"\"\"Adds command line arguments the helper supports to an argument group.\n\n    This function takes an argument parser or an argument group object and adds\n    to it all the command line arguments this helper supports.\n\n    Args:\n      argument_group (argparse._ArgumentGroup|argparse.ArgumentParser):\n          argparse group.\n    \"\"\"\n", "input": "", "output": "    argument_group.add_argument(\n        '--server', dest='server', type=str, action='store',\n        default=cls._DEFAULT_SERVER, metavar='HOSTNAME',\n        help='The hostname or server IP address of the server.')\n    argument_group.add_argument(\n        '--port', dest='port', type=int, action='store',\n        default=cls._DEFAULT_PORT, metavar='PORT',\n        help='The port number of the server.')", "category": "Python"}, {"instruction": "def dir_list(self, saltenv='base', prefix=''):\n        '''\n        List the dirs on the master\n        '''\n", "input": "", "output": "        load = {'saltenv': saltenv,\n                'prefix': prefix,\n                'cmd': '_dir_list'}\n        return salt.utils.data.decode(self.channel.send(load)) if six.PY2 \\\n            else self.channel.send(load)", "category": "Python"}, {"instruction": "def install_twisted():\n    \"\"\"\n    If twisted is available, make `emit' return a DeferredList\n\n    This has been successfully tested with Twisted 14.0 and later.\n    \"\"\"\n", "input": "", "output": "    global emit, _call_partial\n    try:\n        from twisted.internet import defer\n        emit = _emit_twisted\n        _call_partial = defer.maybeDeferred\n        return True\n    except ImportError:\n        _call_partial = lambda fn, *a, **kw: fn(*a, **kw)\n        return False", "category": "Python"}, {"instruction": "def defer_function(self, callable):\n        \"\"\"Schedule a function handler to be called just before completion.\n\n        This is used for handling function bodies, which must be deferred because code later in the file might modify\n        the global scope. When 'callable' is called, the scope at the time this is called will be restored, however it\n        will contain any new bindings added to it.\n\n        \"\"\"\n", "input": "", "output": "        self._deferred_functions.append((callable, self.scope_stack[:], self.offset))", "category": "Python"}, {"instruction": "def verify(self, obj):\n        \"\"\"Verify that the object conforms to this verifier's schema\n\n        Args:\n            obj (object): A python object to verify\n\n        Raises:\n            ValidationError: If there is a problem verifying the dictionary, a\n                ValidationError is thrown with at least the reason key set indicating\n                the reason for the lack of validation.\n        \"\"\"\n", "input": "", "output": "\n        if obj is not None:\n            raise ValidationError(\"Object is not None\",\n                                  reason='%s is not None' % str(obj), object=obj)\n\n        return obj", "category": "Python"}, {"instruction": "def get_object(self):\n        \"\"\"Return contents in object form, an AttrDict\"\"\"\n", "input": "", "output": "\n        from ..util import AttrDict\n\n        c = self.record.unpacked_contents\n\n        if not c:\n            c = yaml.safe_load(self.default)\n\n        return AttrDict(c)", "category": "Python"}, {"instruction": "def on_basic_return(self, _channel, method, properties, body):\n        \"\"\"Invoke a registered callback or log the returned message.\n\n        :param _channel: The channel the message was sent on\n        :type _channel: pika.channel.Channel\n        :param pika.spec.Basic.Return method: The method object\n        :param pika.spec.BasicProperties properties: The message properties\n        :param str, unicode, bytes body: The message body\n\n        \"\"\"\n", "input": "", "output": "        if self.on_return:\n            self.on_return(method, properties, body)\n        else:\n            LOGGER.critical(\n                '%s message %s published to %s (CID %s) returned: %s',\n                method.exchange, properties.message_id,\n                method.routing_key, properties.correlation_id,\n                method.reply_text)", "category": "Python"}, {"instruction": "def irreducible_causes(self):\n        \"\"\"The set of irreducible causes in this |Account|.\"\"\"\n", "input": "", "output": "        return tuple(link for link in self\n                     if link.direction is Direction.CAUSE)", "category": "Python"}, {"instruction": "def load_private_key(pem_path, passphrase_bytes=None):\n    \"\"\"Load private key from PEM encoded file\"\"\"\n", "input": "", "output": "    with open(pem_path, \"rb\") as f:\n        return cryptography.hazmat.primitives.serialization.load_pem_private_key(\n            data=f.read(),\n            password=passphrase_bytes,\n            backend=cryptography.hazmat.backends.default_backend(),\n        )", "category": "Python"}, {"instruction": "def name(self):\n        \"\"\"Instance name.\"\"\"\n", "input": "", "output": "        return ffi.string(lib.EnvGetInstanceName(self._env, self._ist)).decode()", "category": "Python"}, {"instruction": "def _filter_if(self, node):\n        \"\"\"\n            Check if the node is a condtional node where\n            there is an external call checked\n            Heuristic:\n                - The call is a IF node\n                - It contains a, external call\n                - The condition is the negation (!)\n\n            This will work only on naive implementation\n        \"\"\"\n", "input": "", "output": "        return isinstance(node.expression, UnaryOperation)\\\n            and node.expression.type == UnaryOperationType.BANG", "category": "Python"}, {"instruction": "def image(self, image):\n        \"\"\"Set buffer to value of Python Imaging Library image.  The image should\n        be in 1 bit mode and a size equal to the display size.\n        \"\"\"\n", "input": "", "output": "        if image.mode != '1':\n            raise ValueError('Image must be in mode 1.')\n        imwidth, imheight = image.size\n        if imwidth != self.width or imheight != self.height:\n            raise ValueError('Image must be same dimensions as display ({0}x{1}).' \\\n                .format(self.width, self.height))\n        print(\"bitmap display: image\")\n        image.save(\"dummydisplay.bmp\")", "category": "Python"}, {"instruction": "def remove_existing_container(engine_obj, service_name, remove_volumes=False):\n    \"\"\"\n    Remove a container for an existing service. Handy for removing an existing conductor.\n    \"\"\"\n", "input": "", "output": "    conductor_container_id = engine_obj.get_container_id_for_service(service_name)\n    if engine_obj.service_is_running(service_name):\n        engine_obj.stop_container(conductor_container_id, forcefully=True)\n    if conductor_container_id:\n        engine_obj.delete_container(conductor_container_id, remove_volumes=remove_volumes)", "category": "Python"}, {"instruction": "def list_(runas=None):\n    '''\n    List all rvm-installed rubies\n\n    runas\n        The user under which to run rvm. If not specified, then rvm will be run\n        as the user under which Salt is running.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' rvm.list\n    '''\n", "input": "", "output": "    rubies = []\n    output = _rvm(['list'], runas=runas)\n    if output:\n        regex = re.compile(r'^[= ]([*> ]) ([^- ]+)-([^ ]+) \\[ (.*) \\]')\n        for line in output.splitlines():\n            match = regex.match(line)\n            if match:\n                rubies.append([\n                    match.group(2), match.group(3), match.group(1) == '*'\n                ])\n    return rubies", "category": "Python"}, {"instruction": "def main_target_default_build (self, specification, project):\n        \"\"\" Return the default build value to use when declaring a main target,\n            which is obtained by using specified value if not empty and parent's\n            default build attribute otherwise.\n            specification:  Default build explicitly specified for a main target\n            project:        Project where the main target is to be declared\n        \"\"\"\n", "input": "", "output": "        assert is_iterable_typed(specification, basestring)\n        assert isinstance(project, ProjectTarget)\n        if specification:\n            return property_set.create_with_validation(specification)\n        else:\n            return project.get ('default-build')", "category": "Python"}, {"instruction": "def remove_widget(self, widget):\n        \"\"\"\n        Remove a Widget as a managed child of this Widget.\n\n        Parameters\n        ----------\n        widget : instance of Widget\n            The widget to remove.\n        \"\"\"\n", "input": "", "output": "        self._widgets.remove(widget)\n        widget.parent = None\n        self._update_child_widgets()", "category": "Python"}, {"instruction": "def module_name(self, jamfile_location):\n        \"\"\"Returns the name of module corresponding to 'jamfile-location'.\n        If no module corresponds to location yet, associates default\n        module name with that location.\"\"\"\n", "input": "", "output": "        assert isinstance(jamfile_location, basestring)\n        module = self.location2module.get(jamfile_location)\n        if not module:\n            # Root the path, so that locations are always umbiguious.\n            # Without this, we can't decide if '../../exe/program1' and '.'\n            # are the same paths, or not.\n            jamfile_location = os.path.realpath(\n                os.path.join(os.getcwd(), jamfile_location))\n            module = \"Jamfile<%s>\" % jamfile_location\n            self.location2module[jamfile_location] = module\n        return module", "category": "Python"}, {"instruction": "def stop(vm, force=False, key='uuid'):\n    '''\n    Stop a vm\n\n    vm : string\n        vm to be stopped\n    force : boolean\n        force stop of vm if true\n    key : string [uuid|alias|hostname]\n        value type of 'vm' parameter\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' vmadm.stop 186da9ab-7392-4f55-91a5-b8f1fe770543\n        salt '*' vmadm.stop 186da9ab-7392-4f55-91a5-b8f1fe770543 True\n        salt '*' vmadm.stop vm=nacl key=alias\n        salt '*' vmadm.stop vm=nina.example.org key=hostname\n    '''\n", "input": "", "output": "    ret = {}\n    if key not in ['uuid', 'alias', 'hostname']:\n        ret['Error'] = 'Key must be either uuid, alias or hostname'\n        return ret\n    vm = lookup('{0}={1}'.format(key, vm), one=True)\n    if 'Error' in vm:\n        return vm\n    # vmadm stop <uuid> [-F]\n    cmd = 'vmadm stop {force} {uuid}'.format(\n        force='-F' if force else '',\n        uuid=vm\n    )\n    res = __salt__['cmd.run_all'](cmd)\n    retcode = res['retcode']\n    if retcode != 0:\n        ret['Error'] = _exit_status(retcode)\n        return ret\n    return True", "category": "Python"}, {"instruction": "def execution_engine_model_changed(self, model, prop_name, info):\n        \"\"\"High light active state machine. \"\"\"\n", "input": "", "output": "\n        notebook = self.view['notebook']\n        active_state_machine_id = self.model.state_machine_manager.active_state_machine_id\n\n        if active_state_machine_id is None:\n            # un-mark all state machine that are marked with execution-running style class\n            for tab in self.tabs.values():\n                label = notebook.get_tab_label(tab['page']).get_child().get_children()[0]\n                if label.get_style_context().has_class(constants.execution_running_style_class):\n                    label.get_style_context().remove_class(constants.execution_running_style_class)\n        else:\n            # mark active state machine with execution-running style class\n            page = self.get_page_for_state_machine_id(active_state_machine_id)\n            if page:\n                label = notebook.get_tab_label(page).get_child().get_children()[0]\n                label.get_style_context().add_class(constants.execution_running_style_class)", "category": "Python"}, {"instruction": "def abstracts(soup):\n    \"\"\"\n    Find the article abstract and format it\n    \"\"\"\n", "input": "", "output": "\n    abstracts = []\n\n    abstract_tags = raw_parser.abstract(soup)\n\n    for tag in abstract_tags:\n        abstract = {}\n\n        abstract[\"abstract_type\"] = tag.get(\"abstract-type\")\n        title_tag = raw_parser.title(tag)\n        if title_tag:\n            abstract[\"title\"] = node_text(title_tag)\n\n        abstract[\"content\"] = None\n        if raw_parser.paragraph(tag):\n            abstract[\"content\"] = \"\"\n            abstract[\"full_content\"] = \"\"\n\n            good_paragraphs = remove_doi_paragraph(raw_parser.paragraph(tag))\n\n            # Plain text content\n            glue = \"\"\n            for p_tag in good_paragraphs:\n                abstract[\"content\"] += glue + node_text(p_tag)\n                glue = \" \"\n\n            # Content including markup tags\n            # When more than one paragraph, wrap each in a <p> tag\n            for p_tag in good_paragraphs:\n                abstract[\"full_content\"] += '<p>' + node_contents_str(p_tag) + '</p>'\n\n        abstracts.append(abstract)\n\n    return abstracts", "category": "Python"}, {"instruction": "def python_cardinality(self, subject: str, all_are_optional: bool = False) -> str:\n        \"\"\"Add the appropriate python typing to subject (e.g. Optional, List, ...)\n\n        :param subject: Subject to be decorated\n        :param all_are_optional: Force everything to be optional\n        :return: Typed subject\n        \"\"\"\n", "input": "", "output": "        if self.multiple_elements:\n            rval = f\"typing.List[{subject}]\"\n        elif self.one_optional_element:\n            rval = subject if subject.startswith(\"typing.Optional[\") else f\"typing.Optional[{subject}]\"\n        elif self.max == 0:\n            rval = \"type(None)\"\n        else:\n            rval = subject\n        if all_are_optional and not self.one_optional_element:\n            rval = f\"typing.Optional[{rval}]\"\n        return rval", "category": "Python"}, {"instruction": "def delete(self, photo_id, album_id=0):\n        \"\"\"Delete a photo from the logged in users account.\n\n        :param photo_id: The okcupid id of the photo to delete.\n        :param album_id: The album from which to delete the photo.\n        \"\"\"\n", "input": "", "output": "        if isinstance(photo_id, Info):\n            photo_id = photo_id.id\n        return self._session.okc_post('photoupload', data={\n            'albumid': album_id,\n            'picid': photo_id,\n            'authcode': self._authcode,\n            'picture.delete_ajax': 1\n        })", "category": "Python"}, {"instruction": "def is_admin(name):\n    '''\n    Is the passed user a member of the Administrators group\n\n    Args:\n        name (str): The name to check\n\n    Returns:\n        bool: True if user is a member of the Administrators group, False\n        otherwise\n    '''\n", "input": "", "output": "    groups = get_user_groups(name, True)\n\n    for group in groups:\n        if group in ('S-1-5-32-544', 'S-1-5-18'):\n            return True\n\n    return False", "category": "Python"}, {"instruction": "def pickColor( self ):\n        \"\"\"\n        Prompts the user to select a color for this button.\n        \"\"\"\n", "input": "", "output": "        color = QColorDialog.getColor( self.color(), self )\n        \n        if ( color.isValid() ):\n            self.setColor(color)", "category": "Python"}, {"instruction": "def get_instance(self, payload):\n        \"\"\"\n        Build an instance of MessageInteractionInstance\n\n        :param dict payload: Payload response from the API\n\n        :returns: twilio.rest.proxy.v1.service.session.participant.message_interaction.MessageInteractionInstance\n        :rtype: twilio.rest.proxy.v1.service.session.participant.message_interaction.MessageInteractionInstance\n        \"\"\"\n", "input": "", "output": "        return MessageInteractionInstance(\n            self._version,\n            payload,\n            service_sid=self._solution['service_sid'],\n            session_sid=self._solution['session_sid'],\n            participant_sid=self._solution['participant_sid'],\n        )", "category": "Python"}, {"instruction": "def get_response(wsgi_request):\n    '''\n        Given a WSGI request, makes a call to a corresponding view\n        function and returns the response.\n    '''\n", "input": "", "output": "    service_start_time = datetime.now()\n    # Get the view / handler for this request\n    view, args, kwargs = resolve(wsgi_request.path_info)\n\n    kwargs.update({\"request\": wsgi_request})\n\n    # Let the view do his task.\n    try:\n        resp = view(*args, **kwargs)\n    except Exception as exc:\n        resp = HttpResponseServerError(content=exc.message)\n\n    headers = dict(resp._headers.values())\n    # Convert HTTP response into simple dict type.\n    d_resp = {\"status_code\": resp.status_code, \"reason_phrase\": resp.reason_phrase,\n              \"headers\": headers}\n    try:\n        d_resp.update({\"body\": resp.content})\n    except ContentNotRenderedError:\n        resp.render()\n        d_resp.update({\"body\": resp.content})\n\n    # Check if we need to send across the duration header.\n    if _settings.ADD_DURATION_HEADER:\n        d_resp['headers'].update({_settings.DURATION_HEADER_NAME: (datetime.now() - service_start_time).seconds})\n\n    return d_resp", "category": "Python"}, {"instruction": "def schedule_from_proto_dicts(\n        device: 'xmon_device.XmonDevice',\n        ops: Iterable[Dict],\n) -> Schedule:\n    \"\"\"Convert proto dictionaries into a Schedule for the given device.\"\"\"\n", "input": "", "output": "    scheduled_ops = []\n    last_time_picos = 0\n    for op in ops:\n        delay_picos = 0\n        if 'incremental_delay_picoseconds' in op:\n            delay_picos = op['incremental_delay_picoseconds']\n        time_picos = last_time_picos + delay_picos\n        last_time_picos = time_picos\n        xmon_op = xmon_op_from_proto_dict(op)\n        scheduled_ops.append(ScheduledOperation.op_at_on(\n            operation=xmon_op,\n            time=Timestamp(picos=time_picos),\n            device=device,\n        ))\n    return Schedule(device, scheduled_ops)", "category": "Python"}, {"instruction": "def check_candidate_exists(self, basepath, candidates):\n        \"\"\"\n        Check that at least one candidate exist into a directory.\n\n        Args:\n            basepath (str): Directory path where to search for candidate.\n            candidates (list): List of candidate file paths.\n\n        Returns:\n            list: List of existing candidates.\n        \"\"\"\n", "input": "", "output": "        checked = []\n        for item in candidates:\n            abspath = os.path.join(basepath, item)\n            if os.path.exists(abspath):\n                checked.append(abspath)\n\n        return checked", "category": "Python"}, {"instruction": "def _restart():\n    '''Schedule the restart; returning True if cancelled, False otherwise.'''\n", "input": "", "output": "    if _restart_cb:\n        # https://github.com/formwork-io/lazarus/issues/2\n        if _restart_cb() is not None:\n            # restart cancelled\n            return True\n\n    def down_watchdog():\n        _observer.stop()\n        _observer.join()\n\n        if _close_fds:\n            # close all fds...\n            _util.close_fds()\n\n        # declare a mulligan ;)\n        if _restart_func:\n            _restart_func()\n            _deactivate()\n        else:\n            _util.do_over()\n\n    _util.defer(down_watchdog)\n    return False", "category": "Python"}, {"instruction": "def mock_attr(self, *args, **kwargs):\n        \"\"\"\n        Empty method to call to slurp up args and kwargs.\n\n        `args` get pushed onto the url path.\n        `kwargs` are converted to a query string and appended to the URL.\n        \"\"\"\n", "input": "", "output": "        self.path.extend(args)\n        self.qs.update(kwargs)\n        return self", "category": "Python"}, {"instruction": "def search_by_name(cls, query, name):\n        \"\"\"\n        Make a search\n        :param query:\n        :param name:\n        :return:\n        \"\"\"\n", "input": "", "output": "        query = query.filter(db.or_(cls.first_name.contains(name),\n                                    cls.last_name.contains(name)))\n        return query", "category": "Python"}, {"instruction": "def _reset(self):\n        '''\n            _reset - reset this object. Assigned to .reset after __init__ call.\n        '''\n", "input": "", "output": "        HTMLParser.reset(self)\n\n        self.root = None\n        self.doctype = None\n        self._inTag = []", "category": "Python"}, {"instruction": "def remove_umis(adj_list, cluster, nodes):\n    '''removes the specified nodes from the cluster and returns\n    the remaining nodes '''\n", "input": "", "output": "\n    # list incomprehension: for x in nodes: for node in adj_list[x]: yield node\n    nodes_to_remove = set([node\n                           for x in nodes\n                           for node in adj_list[x]] + nodes)\n\n    return cluster - nodes_to_remove", "category": "Python"}, {"instruction": "def debug_tag(self, tag):\n        \"\"\"Setter for the debug tag.\n\n        By default, the tag is the serial of the device, but sometimes it may\n        be more descriptive to use a different tag of the user's choice.\n\n        Changing debug tag changes part of the prefix of debug info emitted by\n        this object, like log lines and the message of DeviceError.\n\n        Example:\n            By default, the device's serial number is used:\n                'INFO [AndroidDevice|abcdefg12345] One pending call ringing.'\n            The tag can be customized with `ad.debug_tag = 'Caller'`:\n                'INFO [AndroidDevice|Caller] One pending call ringing.'\n        \"\"\"\n", "input": "", "output": "        self.log.info('Logging debug tag set to \"%s\"', tag)\n        self._debug_tag = tag\n        self.log.extra['tag'] = tag", "category": "Python"}, {"instruction": "def run(self, bundle,\n              container_id=None,\n              log_path=None,\n              pid_file=None,\n              log_format=\"kubernetes\"):\n\n    ''' run is a wrapper to create, start, attach, and delete a container.\n\n        Equivalent command line example:      \n          singularity oci run -b ~/bundle mycontainer\n\n        Parameters\n        ==========\n        bundle: the full path to the bundle folder\n        container_id: an optional container_id. If not provided, use same\n                      container_id used to generate OciImage instance\n        log_path: the path to store the log.\n        pid_file: specify the pid file path to use\n        log_format: defaults to kubernetes. Can also be \"basic\" or \"json\"\n    '''\n", "input": "", "output": "    return self._run(bundle,\n                     container_id=container_id,\n                     log_path=log_path,\n                     pid_file=pid_file,\n                     command=\"run\",\n                     log_format=log_format)", "category": "Python"}, {"instruction": "def ls(ctx, name, list_instances):\n    \"\"\"List ELB instances\"\"\"\n", "input": "", "output": "    session = create_session(ctx.obj['AWS_PROFILE_NAME'])\n\n    client = session.client('elb')\n    inst = {'LoadBalancerDescriptions': []}\n    if name == '*':\n        inst = client.describe_load_balancers()\n    else:\n        try:\n            inst = client.describe_load_balancers(LoadBalancerNames=[name])\n        except ClientError as e:\n            click.echo(e, err=True)\n\n    for i in inst['LoadBalancerDescriptions']:\n        click.echo(i['LoadBalancerName'])\n        if list_instances:\n            for ec2 in i['Instances']:\n                health = client.describe_instance_health(\n                    LoadBalancerName=name,\n                    Instances=[ec2]\n                )\n                click.echo('{0}\\t{1}'.format(ec2['InstanceId'], health['InstanceStates'][0]['State']))", "category": "Python"}, {"instruction": "def url(match, handler=None, methods=None, defaults=None,\n        redirect_to=None, build_only=False, name=None, **kwargs):\n    \"\"\"Simple helper for build a url, and return anillo\n    url spec hash map (dictionary)\n\n    It can be used in this way:\n\n    urls = [\n        url(\"/<int:year>\", index, methods=[\"get\", \"post\"]),\n        url(\"/<int:year>\", index, methods=[\"get\", \"post\"])\n    ]\n\n    This is a prefered way to define one url.\n\n    :return: The anillo url spec\n    :rtype: dict\n    \"\"\"\n", "input": "", "output": "    assert isinstance(match, str), \"match parameter should be string.\"\n    assert handler or redirect_to, \"you should specify handler or redirect_to for the url\"\n\n    if isinstance(methods, str):\n        methods = [methods.upper()]\n    elif isinstance(methods, (list, tuple)):\n        methods = [x.upper() for x in methods]\n\n    rule = {\"match\": match,\n            \"handler\": handler,\n            \"methods\": methods,\n            \"defaults\": defaults,\n            \"redirect_to\": redirect_to,\n            \"build_only\": build_only,\n            \"name\": name,\n            \"extra_data\": kwargs}\n    return rule", "category": "Python"}, {"instruction": "def namedb_namespace_fields_check( namespace_rec ):\n    \"\"\"\n    Given a namespace record, make sure the following fields are present:\n    * namespace_id\n    * buckets\n\n    Makes the record suitable for insertion/update.\n    NOTE: MODIFIES namespace_rec\n    \"\"\"\n", "input": "", "output": "\n    assert namespace_rec.has_key('namespace_id'), \"BUG: namespace record has no ID\"\n    assert namespace_rec.has_key('buckets'), 'BUG: missing price buckets'\n    assert isinstance(namespace_rec['buckets'], str), 'BUG: namespace data is not in canonical form'\n\n    return namespace_rec", "category": "Python"}, {"instruction": "def preview_processed_html(self, group_id, html=None):\r\n        \"\"\"\r\n        Preview processed html.\r\n\r\n        Preview html content processed for this group\r\n        \"\"\"\n", "input": "", "output": "        path = {}\r\n        data = {}\r\n        params = {}\r\n\r\n        # REQUIRED - PATH - group_id\r\n        ", "category": "Python"}, {"instruction": "def circDiff(length, ary1, ary2):\n    \"\"\"calculate the circular difference between two paired arrays.\n    This function will return the difference between pairs of numbers; however\n    the difference that is output will be minimal in the sense that if we\n    assume an array with length = 4: [0, 1, 2, 3], the difference between\n    0 and 3 will not be 3, but 1 (i.e. circular difference)\"\"\"\n", "input": "", "output": "    x = np.arange(length)\n    mod = length % 2\n    if mod == 0:\n        temp = np.ones(length)\n        temp[length/2:] = -1\n    else:\n        x = x - np.floor(length/2)\n        temp = np.copy(x)\n        temp[np.less(x, 0)] = 1\n        temp[np.greater(x, 0)] = -1\n    x = np.cumsum(temp)\n\n    diagDiffmat = np.empty((length, length))\n    for idx in np.arange(length):\n        x = np.roll(x, 1)\n        diagDiffmat[idx, :] = x\n    # return diagDiffmat[ary1][ary2]\n    flat = diagDiffmat.flatten()\n    ind = ary1*diagDiffmat.shape[0] + ary2\n    ind = ind.astype('int')\n    return flat[ind]", "category": "Python"}, {"instruction": "def is_cnpj(numero, estrito=False):\n    \"\"\"Uma vers\u00e3o conveniente para usar em testes condicionais. Apenas retorna\n    verdadeiro ou falso, conforme o argumento \u00e9 validado.\n\n    :param bool estrito: Padr\u00e3o ``False``, indica se apenas os d\u00edgitos do\n        n\u00famero dever\u00e3o ser considerados. Se verdadeiro, potenciais caracteres\n        que formam a m\u00e1scara ser\u00e3o removidos antes da valida\u00e7\u00e3o ser realizada.\n\n    \"\"\"\n", "input": "", "output": "    try:\n        cnpj(digitos(numero) if not estrito else numero)\n        return True\n    except NumeroCNPJError:\n        pass\n    return False", "category": "Python"}, {"instruction": "def complete_node(arg):\n    \"\"\" Complete node hostname\n\n        This function is currently a bit special as it looks in the config file\n        for a command to use to complete a node hostname from an external\n        system.\n\n        It is configured by setting the config attribute \"complete_node_cmd\" to\n        a shell command. The string \"%search_string%\" in the command will be\n        replaced by the current search string.\n    \"\"\"\n", "input": "", "output": "\n    # get complete command from config\n    try:\n        cmd = cfg.get('global', 'complete_node_cmd')\n    except configparser.NoOptionError:\n        return [ '', ]\n\n    cmd = re.sub('%search_string%', pipes.quote(arg), cmd)\n\n    args = shlex.split(cmd)\n    p = subprocess.Popen(args, stdout=subprocess.PIPE)\n    res, err = p.communicate()\n\n    nodes = res.split('\\n')\n    return nodes", "category": "Python"}, {"instruction": "async def wait_nodes(self, timeout, check_ready=True):\n        \"\"\"Wait until all nodes are online (their managers accept connections)\n        or timeout expires. Should be called after :meth:`spawn_nodes`.\n\n        This is an alias for :meth:`~creamas.mp.MultiEnvironment.wait_slaves`.\n        \"\"\"\n", "input": "", "output": "        return await self.wait_slaves(timeout, check_ready=check_ready)", "category": "Python"}, {"instruction": "def get_caption_formatted(self, field_formats = app_settings.MEDIA_TREE_METADATA_FORMATS, escape=True):\n        \"\"\"Returns object metadata that has been selected to be displayed to\n        users, compiled as a string including default formatting, for example\n        bold titles.\n\n        You can use this method in templates where you want to output image\n        captions.\n        \"\"\"\n", "input": "", "output": "        if self.override_caption != '':\n            return self.override_caption\n        else:\n            return mark_safe(self.get_metadata_display(field_formats, escape=escape))", "category": "Python"}, {"instruction": "def init_i18n (loc=None):\n    \"\"\"Initialize i18n with the configured locale dir. The environment\n    variable LOCPATH can also specify a locale dir.\n\n    @return: None\n    \"\"\"\n", "input": "", "output": "    if 'LOCPATH' in os.environ:\n        locdir = os.environ['LOCPATH']\n    else:\n        locdir = os.path.join(get_install_data(), 'share', 'locale')\n    i18n.init(configdata.name.lower(), locdir, loc=loc)\n    # install translated log level names\n    import logging\n    logging.addLevelName(logging.CRITICAL, _('CRITICAL'))\n    logging.addLevelName(logging.ERROR, _('ERROR'))\n    logging.addLevelName(logging.WARN, _('WARN'))\n    logging.addLevelName(logging.WARNING, _('WARNING'))\n    logging.addLevelName(logging.INFO, _('INFO'))\n    logging.addLevelName(logging.DEBUG, _('DEBUG'))\n    logging.addLevelName(logging.NOTSET, _('NOTSET'))", "category": "Python"}, {"instruction": "def __parameter_default(self, field):\n    \"\"\"Returns default value of field if it has one.\n\n    Args:\n      field: A simple field.\n\n    Returns:\n      The default value of the field, if any exists, with the exception of an\n          enum field, which will have its value cast to a string.\n    \"\"\"\n", "input": "", "output": "    if field.default:\n      if isinstance(field, messages.EnumField):\n        return field.default.name\n      elif isinstance(field, messages.BooleanField):\n        # The Python standard representation of a boolean value causes problems\n        # when generating client code.\n        return 'true' if field.default else 'false'\n      else:\n        return str(field.default)", "category": "Python"}, {"instruction": "def index():\n    \"\"\"Show the index with all posts.\n\n    :param int all: Whether or not should show all posts\n    \"\"\"\n", "input": "", "output": "    context = {'postform': NewPostForm(),\n               'pageform': NewPageForm(),\n               'delform': DeleteForm()}\n\n    n = request.args.get('all')\n    if n is None:\n        wants_now = None\n    else:\n        wants_now = n == '1'\n\n    if wants_now is None and current_user.wants_all_posts:\n        wants = True\n    else:\n        wants = wants_now\n\n    if current_user.can_edit_all_posts and wants:\n        posts = site.all_posts\n        pages = site.pages\n    else:\n        wants = False\n        posts = []\n        pages = []\n        for p in site.timeline:\n            if (p.meta('author.uid') and\n                    p.meta('author.uid') != str(current_user.uid)):\n                continue\n            if p.is_post:\n                posts.append(p)\n            else:\n                pages.append(p)\n\n    context['posts'] = posts\n    context['pages'] = pages\n    context['title'] = 'Posts & Pages'\n    context['wants'] = wants\n    return render('coil_index.tmpl', context)", "category": "Python"}, {"instruction": "def kill_cursors(cursor_ids):\n    \"\"\"Get a **killCursors** message.\n    \"\"\"\n", "input": "", "output": "    num_cursors = len(cursor_ids)\n    pack = struct.Struct(\"<ii\" + (\"q\" * num_cursors)).pack\n    op_kill_cursors = pack(0, num_cursors, *cursor_ids)\n    return __pack_message(2007, op_kill_cursors)", "category": "Python"}, {"instruction": "def pretty_str(self, indent=0):\n        \"\"\"Return a human-readable string representation of this object.\n\n        Kwargs:\n            indent (int): The amount of spaces to use as indentation.\n        \"\"\"\n", "input": "", "output": "        spaces = ' ' * indent\n        condition = pretty_str(self.condition)\n        v = self.declarations.pretty_str() if self.declarations else ''\n        i = self.increment.pretty_str(indent=1) if self.increment else ''\n        pretty = '{}for ({}; {}; {}):\\n'.format(spaces, v, condition, i)\n        pretty += self.body.pretty_str(indent=indent + 2)\n        return pretty", "category": "Python"}, {"instruction": "def match(self, row):\n        \"\"\"\n        Returns True if the field is in the list of conditions. Returns False otherwise.\n\n        :param dict row: The row.\n\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        if row[self._field] in self._values:\n            return True\n\n        for condition in self._conditions:\n            if condition.match(row):\n                return True\n\n        return False", "category": "Python"}, {"instruction": "def eventFilter(self, obj, event):\n        \"\"\"Filter mouse press events.\n\n        Events that are captured and not propagated return True. Events that\n        are not captured and are propagated return False.\n        \"\"\"\n", "input": "", "output": "        event_type = event.type()\n        if event_type == QEvent.MouseButtonPress:\n            self.tab_pressed(event)\n            return False\n        return False", "category": "Python"}, {"instruction": "def assoc(cls, ops, kwargs):\n    \"\"\"Associatively expand out nested arguments of the flat class.\n    E.g.::\n\n        >>> class Plus(Operation):\n        ...     simplifications = [assoc, ]\n        >>> Plus.create(1,Plus(2,3))\n        Plus(1, 2, 3)\n    \"\"\"\n", "input": "", "output": "    expanded = [(o,) if not isinstance(o, cls) else o.operands for o in ops]\n    return sum(expanded, ()), kwargs", "category": "Python"}, {"instruction": "def angToDisc(nside, lon, lat, radius, inclusive=False, fact=4, nest=False):\n    \"\"\"\n    Wrap `query_disc` to use lon, lat, and radius in degrees.\n    \"\"\"\n", "input": "", "output": "    vec = angToVec(lon,lat)\n    return query_disc(nside,vec,radius,inclusive,fact,nest)", "category": "Python"}, {"instruction": "def print_config_value(self, name, prefix='- ', separator=': '):\n        \"\"\"print a single configuration value, based on a prefix and separator\n\n           Parameters\n           ==========\n           name: the key of the config valur in self.config_values to print\n           prefix: the prefix to print\n           separator: the separator to use (default is : )\n        \"\"\"\n", "input": "", "output": "\n        value_out = 'None'\n        if name in self.config_values and self.config_values[name] is not None:\n            value_out = self.config_values[name]\n        print(prefix + name + separator + value_out)", "category": "Python"}, {"instruction": "def prepare(self, left_tree, right_tree):\n        \"\"\"prepare() is run on the trees before diffing\n\n        This is so the formatter can apply magic before diffing.\"\"\"\n", "input": "", "output": "        # We don't want to diff comments:\n        self._remove_comments(left_tree)\n        self._remove_comments(right_tree)\n\n        self.placeholderer.do_tree(left_tree)\n        self.placeholderer.do_tree(right_tree)", "category": "Python"}, {"instruction": "def visit_simple_value_boolean_query(self, node):\n        \"\"\"\n        Visits only the children of :class:`SimpleValueBooleanQuery` without substituting the actual node type.\n\n        Notes:\n            Defer conversion from :class:`SimpleValueBooleanQuery` to AndOp or OrOp.\n            This transformation needs to occur higher in the tree, so that we don't lose the information that this is a\n            boolean query among terminals and thus the associative rule needs to be applied if we reached here from a\n            keyword query, or a conversion from :class:`SimpleValueBooleanQuery` to :class:`AndOp` or :class:`OrOp`,\n            otherwise.\n        \"\"\"\n", "input": "", "output": "        node.left, node.right = node.left.accept(self), node.right.accept(self)\n        return node", "category": "Python"}, {"instruction": "def get_object_methods(obj):\n    \"\"\"\n    Returns all methods belonging to an object instance specified in by the\n    __dir__ function\n\n    Example:\n        >>> # DISABLE_DOCTEST\n        >>> from utool.util_inspect import *  # NOQA\n        >>> import utool as ut\n        >>> obj = ut.NiceRepr()\n        >>> methods1 = ut.get_object_methods()\n        >>> ut.inject_func_as_method(obj, ut.get_object_methods)\n        >>> methods2 = ut.get_object_methods()\n        >>> assert ut.get_object_methods in methods2\n    \"\"\"\n", "input": "", "output": "    import utool as ut\n    attr_list = (getattr(obj, attrname) for attrname in dir(obj))\n    methods = [attr for attr in attr_list if ut.is_method(attr)]\n    return methods", "category": "Python"}, {"instruction": "def const(const):\n    '''Convenience wrapper to yield the value of a constant'''\n", "input": "", "output": "    try:\n        return getattr(_c, const)\n    except AttributeError:\n        raise FSQEnvError(errno.EINVAL, u'No such constant:'\\\n                               u' {0}'.format(const))\n    except TypeError:\n        raise TypeError(errno.EINVAL, u'const name must be a string or'\\\n                        u' unicode object, not:'\\\n                        u' {0}'.format(const.__class__.__name__))", "category": "Python"}, {"instruction": "def json_load_object_hook(dct):\n    \"\"\" Hook for json.parse(...) to parse Xero date formats.\n    \"\"\"\n", "input": "", "output": "    for key, value in dct.items():\n        if isinstance(value, six.string_types):\n            value = parse_date(value)\n            if value:\n                dct[key] = value\n\n    return dct", "category": "Python"}, {"instruction": "def _do_update_callback(self, msg):\n        \"\"\"Call registered callback functions.\"\"\"\n", "input": "", "output": "        for callback, device in self._update_callbacks:\n            if device == msg:\n                _LOGGER.debug('Update callback %s for device %s by %s',\n                              callback, device, msg)\n                self._event_loop.call_soon(callback, msg)", "category": "Python"}, {"instruction": "def add_xmlid(ctx, record, xmlid, noupdate=False):\n    \"\"\" Add a XMLID on an existing record \"\"\"\n", "input": "", "output": "    try:\n        ref_id, __, __ = ctx.env['ir.model.data'].xmlid_lookup(xmlid)\n    except ValueError:\n        pass  # does not exist, we'll create a new one\n    else:\n        return ctx.env['ir.model.data'].browse(ref_id)\n    if '.' in xmlid:\n        module, name = xmlid.split('.')\n    else:\n        module = ''\n        name = xmlid\n    return ctx.env['ir.model.data'].create({\n        'name': name,\n        'module': module,\n        'model': record._name,\n        'res_id': record.id,\n        'noupdate': noupdate,\n    })", "category": "Python"}, {"instruction": "def listen(cls, event, func):\n        \"\"\"Add a callback for a signal against the class\"\"\"\n", "input": "", "output": "        signal(event).connect(func, sender=cls)", "category": "Python"}, {"instruction": "def _fields_list_to_dict(fields, option_name):\n    \"\"\"Takes a sequence of field names and returns a matching dictionary.\n\n    [\"a\", \"b\"] becomes {\"a\": 1, \"b\": 1}\n\n    and\n\n    [\"a.b.c\", \"d\", \"a.c\"] becomes {\"a.b.c\": 1, \"d\": 1, \"a.c\": 1}\n    \"\"\"\n", "input": "", "output": "    if isinstance(fields, collections.Mapping):\n        return fields\n\n    if isinstance(fields, collections.Sequence):\n        if not all(isinstance(field, string_type) for field in fields):\n            raise TypeError(\"%s must be a list of key names, each an \"\n                            \"instance of %s\" % (option_name,\n                                                string_type.__name__))\n        return dict.fromkeys(fields, 1)\n\n    raise TypeError(\"%s must be a mapping or \"\n                    \"list of key names\" % (option_name,))", "category": "Python"}]