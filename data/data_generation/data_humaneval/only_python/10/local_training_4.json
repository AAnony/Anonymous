[{"instruction": "def RunShellWithReturnCode(command, print_output=False,\n\t\tuniversal_newlines=True, env=os.environ):\n\t\"\"\"Executes a command and returns the output from stdout and the return code.\n\n\tArgs:\n\t\tcommand: Command to execute.\n\t\tprint_output: If True, the output is printed to stdout.\n\t\t\tIf False, both stdout and stderr are ignored.\n\t\tuniversal_newlines: Use universal_newlines flag (default: True).\n\n\tReturns:\n\t\tTuple (output, return code)\n\t\"\"\"\n", "input": "", "output": "\tlogging.info(\"Running %s\", command)\n\tp = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n\t\tshell=use_shell, universal_newlines=universal_newlines, env=env)\n\tif print_output:\n\t\toutput_array = []\n\t\twhile True:\n\t\t\tline = p.stdout.readline()\n\t\t\tif not line:\n\t\t\t\tbreak\n\t\t\tprint line.strip(\"\\n\")\n\t\t\toutput_array.append(line)\n\t\toutput = \"\".join(output_array)\n\telse:\n\t\toutput = p.stdout.read()\n\tp.wait()\n\terrout = p.stderr.read()\n\tif print_output and errout:\n\t\tprint >>sys.stderr, errout\n\tp.stdout.close()\n\tp.stderr.close()\n\treturn output, p.returncode", "category": "Python"}, {"instruction": "def get_authentication_statement(self, subject, ticket):\n        \"\"\"\n        Build an AuthenticationStatement XML block for a SAML 1.1\n        Assertion.\n        \"\"\"\n", "input": "", "output": "        authentication_statement = etree.Element('AuthenticationStatement')\n        authentication_statement.set('AuthenticationInstant',\n                                     self.instant(instant=ticket.consumed))\n        authentication_statement.set('AuthenticationMethod',\n                                     self.authn_method_password)\n        authentication_statement.append(subject)\n        return authentication_statement", "category": "Python"}, {"instruction": "def _filter_fields(self, filter_function):\n        \"\"\"\n        Utility to iterate through all fields (super types first) of a type.\n\n        :param filter: A function that takes in a Field object. If it returns\n            True, the field is part of the generated output. If False, it is\n            omitted.\n        \"\"\"\n", "input": "", "output": "        fields = []\n        if self.parent_type:\n            fields.extend(self.parent_type._filter_fields(filter_function))\n        fields.extend(filter(filter_function, self.fields))\n        return fields", "category": "Python"}, {"instruction": "def _dict_raise_on_duplicates(ordered_pairs):\n    \"\"\"\n    Reject duplicate keys.\n    \"\"\"\n", "input": "", "output": "    d = {}\n    for k, v in ordered_pairs:\n        if k in d:\n           raise ValueError(\"duplicate key: %r\" % (k,))\n        else:\n           d[k] = v\n    return d", "category": "Python"}, {"instruction": "def getChild(self, name, ns=None, default=None):\n        \"\"\"\n        Get a child by (optional) name and/or (optional) namespace.\n        @param name: The name of a child element (may contain prefix).\n        @type name: basestring\n        @param ns: An optional namespace used to match the child.\n        @type ns: (I{prefix}, I{name})\n        @param default: Returned when child not-found.\n        @type default: L{Element}\n        @return: The requested child, or I{default} when not-found.\n        @rtype: L{Element}\n        \"\"\"\n", "input": "", "output": "        if self.__root is None:\n            return default\n        if ns is None:\n            prefix, name = splitPrefix(name)\n            if prefix is None:\n                ns = None\n            else:\n                ns = self.__root.resolvePrefix(prefix)\n        if self.__root.match(name, ns):\n            return self.__root\n        else:\n            return default", "category": "Python"}, {"instruction": "def on_connect_button__clicked(self, event):\n        '''\n        Connect to Zero MQ plugin hub (`zmq_plugin.hub.Hub`) using the settings\n        from the text entry fields (e.g., hub URI, plugin name).\n\n        Emit `plugin-connected` signal with the new plugin instance after hub\n        connection has been established.\n        '''\n", "input": "", "output": "        hub_uri = self.plugin_uri.get_text()\n        ui_plugin_name = self.ui_plugin_name.get_text()\n\n        plugin = self.create_plugin(ui_plugin_name, hub_uri)\n        self.init_plugin(plugin)\n\n        self.connect_button.set_sensitive(False)\n        self.emit('plugin-connected', plugin)", "category": "Python"}, {"instruction": "def serialize_compound(self, tag):\n        \"\"\"Return the literal representation of a compound tag.\"\"\"\n", "input": "", "output": "        separator, fmt = self.comma, '{{{}}}'\n\n        with self.depth():\n            if self.should_expand(tag):\n                separator, fmt = self.expand(separator, fmt)\n\n            return fmt.format(separator.join(\n                f'{self.stringify_compound_key(key)}{self.colon}{self.serialize(value)}'\n                for key, value in tag.items()\n            ))", "category": "Python"}, {"instruction": "def str_filesize(size):\n    \"\"\"\n    >>> print str_filesize(0)\n    0\n    >>> print str_filesize(1023) \n    1023\n    >>> print str_filesize(1024)\n    1K\n    >>> print str_filesize(1024*2)\n    2K\n    >>> print str_filesize(1024**2-1)\n    1023K\n    >>> print str_filesize(1024**2)\n    1M\n    \"\"\"\n", "input": "", "output": "    import bisect\n    \n    d = [(1024-1,'K'), (1024**2-1,'M'), (1024**3-1,'G'), (1024**4-1,'T')]\n    s = [x[0] for x in d]\n    \n    index = bisect.bisect_left(s, size) - 1\n    if index == -1:\n        return str(size)\n    else:\n        b, u = d[index]\n    return str(size / (b+1)) + u", "category": "Python"}, {"instruction": "def get_song_type(self, cache=True):\n        \"\"\"Get the types of a song.\n        \n        Args:\n            cache (boolean): A boolean indicating whether or not the cached value should be used\n            (if available). Defaults to True.\n        \n        Returns:\n            A list of strings, each representing a song type:  'christmas', for example.\n        \n        Example:\n            >>> s = song.Song('SOQKVPH12A58A7AF4D')\n            >>> s.song_type\n            [u'christmas']\n            >>> \n\n        \"\"\"\n", "input": "", "output": "        if not (cache and ('song_type' in self.cache)):\n            response = self.get_attribute('profile', bucket='song_type')\n            if response['songs'][0].has_key('song_type'):\n                self.cache['song_type'] = response['songs'][0]['song_type']\n            else:\n                self.cache['song_type'] = []\n        return self.cache['song_type']", "category": "Python"}, {"instruction": "def get_field_setup_query(query, model, column_name):\n    \"\"\"\n        Help function for SQLA filters, checks for dot notation on column names.\n        If it exists, will join the query with the model\n        from the first part of the field name.\n\n        example:\n            Contact.created_by: if created_by is a User model,\n            it will be joined to the query.\n    \"\"\"\n", "input": "", "output": "    if not hasattr(model, column_name):\n        # it's an inner obj attr\n        rel_model = getattr(model, column_name.split(\".\")[0]).mapper.class_\n        query = query.join(rel_model)\n        return query, getattr(rel_model, column_name.split(\".\")[1])\n    else:\n        return query, getattr(model, column_name)", "category": "Python"}, {"instruction": "def distances(self, word, words):\n    \"\"\"Calculate eucledean pairwise distances between `word` and `words`.\n\n    Args:\n      word (string): single word.\n      words (list): list of strings.\n\n    Returns:\n      numpy array of the distances.\n\n    Note:\n      L2 metric is used to calculate distances.\n    \"\"\"\n", "input": "", "output": "\n    point = self[word]\n    vectors = np.asarray([self[w] for w in words])\n    diff = vectors - point\n    distances = np.linalg.norm(diff, axis=1)\n    return distances", "category": "Python"}, {"instruction": "def generate_template(self, channeldir, filename, header):\n        \"\"\"\n        Create empty template .csv file called `filename` as siblings of the\n        directory `channeldir` with header fields specified in `header`.\n        \"\"\"\n", "input": "", "output": "        file_path = get_metadata_file_path(channeldir, filename)\n        if not os.path.exists(file_path):\n            with open(file_path, 'w') as csv_file:\n                csvwriter = csv.DictWriter(csv_file, header)\n                csvwriter.writeheader()", "category": "Python"}, {"instruction": "def _create_minimum_needs_action(self):\n        \"\"\"Create action for minimum needs dialog.\"\"\"\n", "input": "", "output": "        icon = resources_path('img', 'icons', 'show-minimum-needs.svg')\n        self.action_minimum_needs = QAction(\n            QIcon(icon),\n            self.tr('Minimum Needs Calculator'), self.iface.mainWindow())\n        self.action_minimum_needs.setStatusTip(self.tr(\n            'Open InaSAFE minimum needs calculator'))\n        self.action_minimum_needs.setWhatsThis(self.tr(\n            'Open InaSAFE minimum needs calculator'))\n        self.action_minimum_needs.triggered.connect(self.show_minimum_needs)\n        self.add_action(\n            self.action_minimum_needs, add_to_toolbar=self.full_toolbar)", "category": "Python"}, {"instruction": "def _encode_ndef_text_params(self, data):\n        \"\"\"\n        Prepend language and enconding information to data, according to\n        nfcforum-ts-rtd-text-1-0.pdf\n        \"\"\"\n", "input": "", "output": "        status = len(self.ndef_text_lang)\n        if self.ndef_text_enc == 'UTF16':\n            status = status & 0b10000000\n        return yubico_util.chr_byte(status) + self.ndef_text_lang + data", "category": "Python"}, {"instruction": "def provides_member_resource(obj):\n    \"\"\"\n    Checks if the given type or instance provides the\n    :class:`everest.resources.interfaces.IMemberResource` interface.\n    \"\"\"\n", "input": "", "output": "    if isinstance(obj, type):\n        obj = object.__new__(obj)\n    return IMemberResource in provided_by(obj)", "category": "Python"}, {"instruction": "def parsePowerTable(uhfbandcap):\n        \"\"\"Parse the transmit power table\n\n        @param uhfbandcap: Capability dictionary from\n            self.capabilities['RegulatoryCapabilities']['UHFBandCapabilities']\n        @return: a list of [0, dBm value, dBm value, ...]\n\n        >>> LLRPClient.parsePowerTable({'TransmitPowerLevelTableEntry1': \\\n            {'Index': 1, 'TransmitPowerValue': 3225}})\n        [0, 32.25]\n        >>> LLRPClient.parsePowerTable({})\n        [0]\n        \"\"\"\n", "input": "", "output": "        bandtbl = {k: v for k, v in uhfbandcap.items()\n                   if k.startswith('TransmitPowerLevelTableEntry')}\n        tx_power_table = [0] * (len(bandtbl) + 1)\n        for k, v in bandtbl.items():\n            idx = v['Index']\n            tx_power_table[idx] = int(v['TransmitPowerValue']) / 100.0\n\n        return tx_power_table", "category": "Python"}, {"instruction": "def _get_version():\n    '''\n    Get the xbps version\n    '''\n", "input": "", "output": "    version_string = __salt__['cmd.run'](\n        [_check_xbps(), '--version'],\n        output_loglevel='trace')\n    if version_string is None:\n        # Dunno why it would, but...\n        return False\n\n    VERSION_MATCH = re.compile(r'(?:XBPS:[\\s]+)([\\d.]+)(?:[\\s]+.*)')\n    version_match = VERSION_MATCH.search(version_string)\n    if not version_match:\n        return False\n\n    return version_match.group(1).split('.')", "category": "Python"}, {"instruction": "def stdout():\n    \"\"\"\n    Returns the stdout as a byte stream in a Py2/PY3 compatible manner\n\n    Returns\n    -------\n    io.BytesIO\n        Byte stream of Stdout\n    \"\"\"\n", "input": "", "output": "\n    # We write all of the data to stdout with bytes, typically io.BytesIO. stdout in Python2\n    # accepts bytes but Python3 does not. This is due to a type change on the attribute. To keep\n    # this consistent, we leave Python2 the same and get the .buffer attribute on stdout in Python3\n    byte_stdout = sys.stdout\n\n    if sys.version_info.major > 2:\n        byte_stdout = sys.stdout.buffer  # pylint: disable=no-member\n\n    return byte_stdout", "category": "Python"}, {"instruction": "def web_hook_receiver(sender, **kwargs):\n    \"\"\"Generic receiver for the web hook firing piece.\"\"\"\n", "input": "", "output": "\n    deployment = Deployment.objects.get(pk=kwargs.get('deployment_id'))\n\n    hooks = deployment.web_hooks\n\n    if not hooks:\n        return\n\n    for hook in hooks:\n\n        data = payload_generator(deployment)\n\n        deliver_hook(deployment, hook.url, data)", "category": "Python"}, {"instruction": "def base64_user_pass(self):\n        \"\"\"\n        Composes a basic http auth string, suitable for use with the\n        _replicator database, and other places that need it.\n\n        :returns: Basic http authentication string\n        \"\"\"\n", "input": "", "output": "        if self._username is None or self._password is None:\n            return None\n\n        hash_ = base64.urlsafe_b64encode(bytes_(\"{username}:{password}\".format(\n            username=self._username,\n            password=self._password\n        )))\n        return \"Basic {0}\".format(unicode_(hash_))", "category": "Python"}, {"instruction": "def definition(self, name):\n        \"\"\"\n        Get the definition for the property I{name}.\n        @param name: The property I{name} to find the definition for.\n        @type name: str\n        @return: The property definition\n        @rtype: L{Definition}\n        @raise AttributeError: On not found.\n        \"\"\"\n", "input": "", "output": "        d = self.definitions.get(name)\n        if d is None:\n            raise AttributeError(name)\n        return d", "category": "Python"}, {"instruction": "def environ(on=os, **kw):\n    \"\"\"Update one or more environment variables.\n    \n    Preserves the previous environment variable (if available) and can be\n    applied to remote connections that offer an @environ@ attribute using the\n    @on@ argument.\n    \"\"\"\n", "input": "", "output": "    \n    originals = list()\n    \n    for key in kw:\n        originals.append((key, on.environ.get(key, None)))\n        on.environ[key] = kw[key]\n    \n    yield\n    \n    for key, value in originals:\n        if not value:\n            del on.environ[key]\n            continue\n        \n        on.environ[key] = value", "category": "Python"}, {"instruction": "def match(self, pe, ep_only=True, section_start_only=False):\n        \"\"\"Matches and returns the exact match(es).\n\n        If ep_only is True the result will be a string with\n        the packer name. Otherwise it will be a list of the\n        form (file_offset, packer_name) specifying where\n        in the file the signature was found.\n        \"\"\"\n", "input": "", "output": "\n        matches = self.__match(pe, ep_only, section_start_only)\n\n        # The last match (the most precise) from the\n        # list of matches (if any) is returned\n        #\n        if matches:\n            if ep_only == False:\n                # Get the most exact match for each list of matches\n                # at a given offset\n                #\n                return [(match[0], match[1][-1]) for match in matches]\n\n            return matches[1][-1]\n\n        return None", "category": "Python"}, {"instruction": "def wait(self, timeout=None):\n        \"\"\"Wait until the result is available or until roughly timeout seconds\n        pass.\"\"\"\n", "input": "", "output": "        logger = logging.getLogger(__name__)\n        if int(self.max_sleep_interval) < int(self._min_sleep_interval):\n            self.max_sleep_interval = int(self._min_sleep_interval)\n        t0 = time.time()\n        sleep_seconds = min(5, self.max_sleep_interval)\n        status = self.status\n        prev_status = status\n        while status < COMPLETED:\n            logger.debug(\"sleep for %d seconds\", sleep_seconds)\n            time.sleep(sleep_seconds)\n            if 2*sleep_seconds <= self.max_sleep_interval:\n                sleep_seconds *= 2\n            if timeout is not None:\n                if int(time.time() - t0) > int(timeout):\n                    return\n            status = self.status\n            if status != prev_status:\n                sleep_seconds = min(5, self.max_sleep_interval)\n                prev_status = status", "category": "Python"}, {"instruction": "def _get_urls(self, version, cluster_stats):\n        \"\"\"\n        Compute the URLs we need to hit depending on the running ES version\n        \"\"\"\n", "input": "", "output": "        pshard_stats_url = \"/_stats\"\n        health_url = \"/_cluster/health\"\n\n        if version >= [0, 90, 10]:\n            pending_tasks_url = \"/_cluster/pending_tasks\"\n            stats_url = \"/_nodes/stats\" if cluster_stats else \"/_nodes/_local/stats\"\n            if version < [5, 0, 0]:\n                # version 5 errors out if the `all` parameter is set\n                stats_url += \"?all=true\"\n        else:\n            # legacy\n            pending_tasks_url = None\n            stats_url = \"/_cluster/nodes/stats?all=true\" if cluster_stats else \"/_cluster/nodes/_local/stats?all=true\"\n\n        return health_url, stats_url, pshard_stats_url, pending_tasks_url", "category": "Python"}, {"instruction": "def email(email_address):\n    \"\"\"Display / set / update the user email.\"\"\"\n", "input": "", "output": "    if not email_address:\n        click.secho(dtool_config.utils.get_user_email(CONFIG_PATH))\n    else:\n        click.secho(dtool_config.utils.set_user_email(\n            CONFIG_PATH,\n            email_address\n        ))", "category": "Python"}, {"instruction": "def register(self, src: type, tar: type) -> Callable[[T], B]:\n        \"\"\"doufo.ConverterDict().register(): A decorator factory to define typing converting decorator\n            Attributes:  \n                `self`\n                `src` (`type`): source `type`,\n                `tar` (`type`): target `type`,\n            Returns:\n                `f` (`Callable[[T], B]`): a decorater that defines a converter                \n        \"\"\"\n", "input": "", "output": "        def deco(f):\n            self.converters[(src, tar)] = f\n            self.converters = self.sorted_converters_keys()\n            return f\n        return deco", "category": "Python"}, {"instruction": "def add_user_to_group(user_name, group_name, region=None, key=None, keyid=None,\n                      profile=None):\n    '''\n    Add user to group.\n\n    .. versionadded:: 2015.8.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iam.add_user_to_group myuser mygroup\n    '''\n", "input": "", "output": "    user = get_user(user_name, region, key, keyid, profile)\n    if not user:\n        log.error('Username : %s does not exist.', user_name)\n        return False\n    if user_exists_in_group(user_name, group_name, region=region, key=key,\n                            keyid=keyid, profile=profile):\n        return True\n    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    try:\n        info = conn.add_user_to_group(group_name, user_name)\n        if not info:\n            return False\n        return info\n    except boto.exception.BotoServerError as e:\n        log.debug(e)\n        log.error('Failed to add IAM user %s to group %s.', user_name, group_name)\n        return False", "category": "Python"}, {"instruction": "def tree_token_generator(el, indentation_level=0):\n    \"\"\"\n    Internal generator that yields tokens for the given HTML element as\n    follows:\n\n    - A tuple (LXML element, BEGIN, indentation_level)\n    - Text right after the start of the tag, or None.\n    - Recursively calls the token generator for all child objects\n    - A tuple (LXML element, END, indentation_level)\n    - Text right after the end of the tag, or None.\n    \"\"\"\n", "input": "", "output": "\n    if not isinstance(el.tag, string_class):\n        return\n\n    tag_name = el.tag.lower()\n\n    is_indentation = is_indentation_element(el)\n\n    if is_indentation:\n        indentation_level += 1\n\n    yield (el, BEGIN, indentation_level)\n\n    yield el.text\n\n    for child in el.iterchildren():\n        for token in tree_token_generator(child, indentation_level):\n            yield token\n\n    if is_indentation:\n        indentation_level -= 1\n\n    yield (el, END, indentation_level)\n\n    yield el.tail", "category": "Python"}, {"instruction": "def get_next_non_summer_term(term):\n    \"\"\"\n    Return the Term object for the quarter after\n    as the given term (skip the summer quarter)\n    \"\"\"\n", "input": "", "output": "    next_term = get_term_after(term)\n    if next_term.is_summer_quarter():\n        return get_next_autumn_term(next_term)\n    return next_term", "category": "Python"}, {"instruction": "def convex_conj(self):\n        \"\"\"The convex conjugate\"\"\"\n", "input": "", "output": "        if isinstance(self.domain, ProductSpace):\n            norm = GroupL1Norm(self.domain, 2)\n        else:\n            norm = L1Norm(self.domain)\n\n        return FunctionalQuadraticPerturb(norm.convex_conj,\n                                          quadratic_coeff=self.gamma / 2)", "category": "Python"}, {"instruction": "def blocks_from_ops(ops):\n    \"\"\"\n    Group a list of :class:`Op` and :class:`Label` instances by label.\n\n    Everytime a label is found, a new :class:`Block` is created. The resulting\n    blocks are returned as a dictionary to easily access the target block of a\n    jump operation. The keys of this dictionary will be the labels, the values\n    will be the :class:`Block` instances. The initial block can be accessed\n    by getting the ``None`` item from the dictionary.\n\n    Arguments:\n        ops(list): The list of :class:`Op` and :class:`Label` instances (as\n            returned by :func:`disassemble`.\n\n    Returns:\n        dict: The resulting dictionary of blocks grouped by label.\n    \"\"\"\n", "input": "", "output": "\n    blocks = {}\n    current_block = blocks[None] = Block()\n    for op in ops:\n        if isinstance(op, Label):\n            next_block = blocks[op] = Block(op)\n            current_block.next = next_block\n            current_block = next_block\n            continue\n        current_block.ops.append(op)\n    return blocks", "category": "Python"}, {"instruction": "def loadInstance(self):\n        \"\"\"\n        Loads the plugin from the proxy information that was created from the\n        registry file.\n        \"\"\"\n", "input": "", "output": "        if self._loaded:\n            return\n\n        self._loaded = True\n        module_path = self.modulePath()\n\n        package = projex.packageFromPath(module_path)\n        path = os.path.normpath(projex.packageRootPath(module_path))\n\n        if path in sys.path:\n            sys.path.remove(path)\n\n        sys.path.insert(0, path)\n\n        try:\n            __import__(package)\n\n        except Exception, e:\n            err = Plugin(self.name(), self.version())\n            err.setError(e)\n            err.setFilepath(module_path)\n\n            self._instance = err\n\n            self.setError(e)\n\n            msg = \"%s.plugin('%s') errored loading instance from %s\"\n            opts = (self.proxyClass().__name__, self.name(), module_path)\n            logger.warning(msg % opts)\n            logger.error(e)", "category": "Python"}, {"instruction": "def handle_bad_update(operation, ret):\n    '''report error for bad update'''\n", "input": "", "output": "    print(\"Error \" + operation)\n    sys.exit('Return code: ' + str(ret.status_code) + ' Error: ' + ret.text)", "category": "Python"}, {"instruction": "def update(self, environments):\n        \"\"\"\n        Method to update environments\n\n        :param environments: List containing environments desired to updated\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        data = {'environments': environments}\n        environments_ids = [str(env.get('id')) for env in environments]\n\n        return super(ApiEnvironment, self).put('api/v3/environment/%s/' %\n                                               ';'.join(environments_ids), data)", "category": "Python"}, {"instruction": "def get_labels(term_ids: list) -> dict:\n    \"\"\"Get term labels given term ids\n\n    This only takes the first term returned for a term_id so use the\n    unique term_id for a term not an alternate id that might not be unique.\n    \"\"\"\n", "input": "", "output": "    term_labels = {}\n    for term_id in term_ids:\n        term = get_terms(term_id)\n        term_labels[term_id] = term[0].get(\"label\", \"\")\n\n    return term_labels", "category": "Python"}, {"instruction": "def find_ident(self, name):\n        \"\"\"\n        Searches this module and **all** of its sub-modules for an\n        identifier with name `name` in its list of exported\n        identifiers according to `pydoc`. Note that unexported\n        sub-modules are searched.\n\n        A bare identifier (without `.` separators) will only be checked\n        for in this module.\n\n        The documentation object corresponding to the identifier is\n        returned. If one cannot be found, then an instance of\n        `External` is returned populated with the given identifier.\n        \"\"\"\n", "input": "", "output": "        if name in self.refdoc:\n            return self.refdoc[name]\n        for module in self.submodules():\n            o = module.find_ident(name)\n            if not isinstance(o, External):\n                return o\n        return External(name)", "category": "Python"}, {"instruction": "def deprecated_name(name):\n    \"\"\"Allow old method names for backwards compatability. \"\"\"\n", "input": "", "output": "    def decorator(func):\n        ", "category": "Python"}, {"instruction": "def _MaybeWriteIndex(self, i, ts, mutation_pool):\n    \"\"\"Write index marker i.\"\"\"\n", "input": "", "output": "    if i > self._max_indexed and i % self.INDEX_SPACING == 0:\n      # We only write the index if the timestamp is more than 5 minutes in the\n      # past: hacky defense against a late write changing the count.\n      if ts[0] < (rdfvalue.RDFDatetime.Now() -\n                  self.INDEX_WRITE_DELAY).AsMicrosecondsSinceEpoch():\n        mutation_pool.CollectionAddIndex(self.collection_id, i, ts[0], ts[1])\n        self._index[i] = ts\n        self._max_indexed = max(i, self._max_indexed)", "category": "Python"}, {"instruction": "def AddWatchOnly(self, script_hash):\n        \"\"\"\n        Add a watch only address to the wallet.\n\n        Args:\n            script_hash (UInt160): a bytearray (len 20) representing the public key.\n\n        Note:\n            Prints a warning to the console if the address already exists in the wallet.\n        \"\"\"\n", "input": "", "output": "        if script_hash in self._contracts:\n            logger.error(\"Address already in contracts\")\n            return\n\n        self._watch_only.append(script_hash)", "category": "Python"}, {"instruction": "def _get_db():\n    \"\"\"\n        Get database from server\n    \"\"\"\n", "input": "", "output": "    with cd(env.remote_path):\n        file_path = '/tmp/' + _sql_paths('remote', str(base64.urlsafe_b64encode(uuid.uuid4().bytes)).replace('=', ''))\n        run(env.python + ' manage.py dump_database | gzip > ' + file_path)\n        local_file_path = './backups/' + _sql_paths('remote', datetime.now())\n    get(file_path, local_file_path)\n    run('rm ' + file_path)\n    return local_file_path", "category": "Python"}, {"instruction": "def transformFromNative(cls, obj):\n        \"\"\"Replace the datetime in obj.value with an ISO 8601 string.\"\"\"\n", "input": "", "output": "        # print('transforming from native')\n        if obj.isNative:\n            obj.isNative = False\n            tzid = TimezoneComponent.registerTzinfo(obj.value.tzinfo)\n            obj.value = dateTimeToString(obj.value, cls.forceUTC)\n            if not cls.forceUTC and tzid is not None:\n                obj.tzid_param = tzid\n            if obj.params.get('X-VOBJ-ORIGINAL-TZID'):\n                if not hasattr(obj, 'tzid_param'):\n                    obj.tzid_param = obj.x_vobj_original_tzid_param\n                del obj.params['X-VOBJ-ORIGINAL-TZID']\n\n        return obj", "category": "Python"}, {"instruction": "def one_phase_dP_gravitational(angle, rho, L=1.0, g=g):\n    r'''This function handles calculation of one-phase liquid-gas pressure drop\n    due to gravitation for flow inside channels. This is either a differential \n    calculation for a segment with an infinitesimal difference in elevation (if\n    `L`=1 or a discrete calculation.\n    \n    .. math::\n        -\\left(\\frac{dP}{dz} \\right)_{grav} =  \\rho g \\sin \\theta\n    \n    .. math::\n        -\\left(\\Delta P \\right)_{grav} =  L \\rho g \\sin \\theta\n    \n    Parameters\n    ----------\n    angle : float\n        The angle of the pipe with respect to the horizontal, [degrees]\n    rho : float\n        Fluid density, [kg/m^3]\n    L : float, optional\n        Length of pipe, [m]\n    g : float, optional\n        Acceleration due to gravity, [m/s^2]\n\n    Returns\n    -------\n    dP : float\n        Gravitational component of pressure drop for one-phase flow, [Pa/m] or\n        [Pa]\n        \n    Notes\n    -----\n        \n    Examples\n    --------    \n    >>> one_phase_dP_gravitational(angle=90, rho=2.6)\n    25.49729\n    >>> one_phase_dP_gravitational(angle=90, rho=2.6, L=4)\n    101.98916\n    '''\n", "input": "", "output": "    angle = radians(angle)\n    return L*g*sin(angle)*rho", "category": "Python"}, {"instruction": "def plot_predictions_histogram(Y_ph, Y, title=None):\n    \"\"\"Plot a histogram comparing int predictions vs true labels by class\n\n    Args:\n        Y_ph: An [n] or [n, 1] np.ndarray of predicted int labels\n        Y: An [n] or [n, 1] np.ndarray of gold labels\n    \"\"\"\n", "input": "", "output": "    labels = list(set(Y).union(set(Y_ph)))\n    edges = [x - 0.5 for x in range(min(labels), max(labels) + 2)]\n\n    plt.hist([Y_ph, Y], bins=edges, label=[\"Predicted\", \"Gold\"])\n    ax = plt.gca()\n    ax.set_xticks(labels)\n    plt.xlabel(\"Label\")\n    plt.ylabel(\"# Predictions\")\n    plt.legend(loc=\"upper right\")\n    if isinstance(title, str):\n        plt.title(title)\n    plt.show()", "category": "Python"}, {"instruction": "def _get_path(entity_id):\n    '''Get the entity_id as a string if it is a Reference.\n\n    @param entity_id The ID either a reference or a string of the entity\n          to get.\n    @return entity_id as a string\n    '''\n", "input": "", "output": "    try:\n        path = entity_id.path()\n    except AttributeError:\n        path = entity_id\n    if path.startswith('cs:'):\n        path = path[3:]\n    return path", "category": "Python"}, {"instruction": "def _get_store_by_name(self, name):\n        \"\"\"Return an instance of the correct DiskRepository based on the *first* file that matches the standard syntax for repository files\"\"\"\n", "input": "", "output": "        for cls in self.storage_type_map.values():\n            cluster_files = glob.glob(\n                '%s/%s.%s' % (self.storage_path, name, cls.file_ending))\n            if cluster_files:\n                try:\n                    return cls(self.storage_path)\n                except:\n                    continue\n        raise ClusterNotFound(\"No cluster %s was found\" % name)", "category": "Python"}, {"instruction": "def check(text):\n    \"\"\"Check the text.\"\"\"\n", "input": "", "output": "    err = \"lexical_illusions.misc\"\n    msg = u\"There's a lexical illusion here: a word is repeated.\"\n\n    list = [\n        \"the\\sthe\",\n        \"am\\sam\",\n        \"has\\shas\"\n    ]\n\n    return existence_check(text, list, err, msg)", "category": "Python"}, {"instruction": "def from_dict(cls, raw_data, **kwargs):\n        \"\"\"\n        This factory for :class:`Model` creates a Model from a dict object.\n        \"\"\"\n", "input": "", "output": "        instance = cls()\n        instance.populate(raw_data, **kwargs)\n        instance.validate(**kwargs)\n        return instance", "category": "Python"}, {"instruction": "def set_last_position(self, last_position):\n        \"\"\"\n        Called from the manager, it is in charge of updating the last position of data commited\n        by the writer, in order to have resume support\n        \"\"\"\n", "input": "", "output": "        if last_position is None:\n            self.last_position = {}\n            for partition in self.partitions:\n                self.last_position[partition] = 0\n            self.consumer.offsets = self.last_position.copy()\n            self.consumer.fetch_offsets = self.consumer.offsets.copy()\n        else:\n            self.last_position = last_position\n            self.consumer.offsets = last_position.copy()\n            self.consumer.fetch_offsets = self.consumer.offsets.copy()", "category": "Python"}, {"instruction": "def bind(context, block=False):\n    \"\"\"\n    Given the context, returns a decorator wrapper;\n    the binder replaces the wrapped func with the\n    value from the context OR puts this function in\n    the context with the name.\n    \"\"\"\n", "input": "", "output": "\n    if block:\n        def decorate(func):\n            name = func.__name__.replace('__TK__block__', '')\n            if name not in context:\n                context[name] = func\n            return context[name]\n\n        return decorate\n\n    def decorate(func):\n        name = func.__name__\n        if name not in context:\n            context[name] = func\n        return context[name]\n\n    return decorate", "category": "Python"}, {"instruction": "def on_mouse_release(self, x: int, y: int, button, mods):\r\n        \"\"\"\r\n        Handle mouse release events and forward to example window\r\n        \"\"\"\n", "input": "", "output": "        if button in [1, 4]:\r\n            self.example.mouse_release_event(\r\n                x, self.buffer_height - y,\r\n                1 if button == 1 else 2,\r\n            )", "category": "Python"}, {"instruction": "def save_avatar(strategy, details, user=None, *args, **kwargs):\n    \"\"\"Get user avatar from social provider.\"\"\"\n", "input": "", "output": "    if user:\n        backend_name = kwargs['backend'].__class__.__name__.lower()\n        response = kwargs.get('response', {})\n        social_thumb = None\n        if 'facebook' in backend_name:\n            if 'id' in response:\n                social_thumb = (\n                    'http://graph.facebook.com/{0}/picture?type=normal'\n                ).format(response['id'])\n        elif 'twitter' in backend_name and response.get('profile_image_url'):\n            social_thumb = response['profile_image_url']\n        elif 'googleoauth2' in backend_name and response.get('image', {}).get('url'):\n            social_thumb = response['image']['url'].split('?')[0]\n        else:\n            social_thumb = 'http://www.gravatar.com/avatar/'\n            social_thumb += hashlib.md5(user.email.lower().encode('utf8')).hexdigest()\n            social_thumb += '?size=100'\n        if social_thumb and user.social_thumb != social_thumb:\n            user.social_thumb = social_thumb\n            strategy.storage.user.changed(user)", "category": "Python"}, {"instruction": "def mapping_args(parser):\n    \"\"\"Add various variable mapping command line options to the parser\"\"\"\n", "input": "", "output": "    parser.add_argument('--add-prefix',\n                        dest='add_prefix',\n                        help='Specify a prefix to use when '\n                        'generating secret key names')\n    parser.add_argument('--add-suffix',\n                        dest='add_suffix',\n                        help='Specify a suffix to use when '\n                        'generating secret key names')\n    parser.add_argument('--merge-path',\n                        dest='merge_path',\n                        action='store_true',\n                        default=True,\n                        help='merge vault path and key name')\n    parser.add_argument('--no-merge-path',\n                        dest='merge_path',\n                        action='store_false',\n                        default=True,\n                        help='do not merge vault path and key name')\n    parser.add_argument('--key-map',\n                        dest='key_map',\n                        action='append',\n                        type=str,\n                        default=[])", "category": "Python"}, {"instruction": "def add_tcp_callback(self, port, callback, threaded_callback=False):\n        \"\"\"\n        Adds a unix socket server callback, which will be invoked when values\n        arrive from a connected socket client. The callback must accept two\n        parameters, eg. ``def callback(socket, msg)``.\n        \"\"\"\n", "input": "", "output": "        if not callback:\n            raise AttributeError(\"No callback\")\n\n        serversocket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        serversocket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        serversocket.bind((_TCP_SOCKET_HOST, port))\n        serversocket.listen(1)\n        serversocket.setblocking(0)\n        self._epoll.register(serversocket.fileno(), select.EPOLLIN)\n\n        # Prepare the callback (wrap in Thread if needed)\n        cb = callback if not threaded_callback else \\\n                partial(_threaded_callback, callback)\n\n        self._tcp_server_sockets[serversocket.fileno()] = (serversocket, cb)\n        debug(\"Socket server started at port %s and callback added.\" % port)", "category": "Python"}, {"instruction": "def validate(self):\n        \"\"\"\n        Validates all field values for the model.\n        \"\"\"\n", "input": "", "output": "\n        errors = {}\n\n        for name, field in self.fields:\n            try:\n                field.validate(self._state.get(name))\n            except Exception as e:\n                errors[name] = e\n\n        if len(errors) is not 0:\n            raise Exception(errors)\n\n        return True", "category": "Python"}, {"instruction": "def insert(path, value, create_parents=False, **kwargs):\n    \"\"\"\n    Create a new path in the document. The final path element points to a\n    dictionary key that should be created. Valid only in :cb_bmeth:`mutate_in`\n\n    :param path: The path to create\n    :param value: Value for the path\n    :param create_parents: Whether intermediate parents should be created\n    \"\"\"\n", "input": "", "output": "    return _gen_4spec(LCB_SDCMD_DICT_ADD, path, value,\n                      create_path=create_parents, **kwargs)", "category": "Python"}, {"instruction": "def _enqueue_init_updates(self):\n        \"\"\"Enqueues current routes to be shared with this peer.\"\"\"\n", "input": "", "output": "        assert self.state.bgp_state == const.BGP_FSM_ESTABLISHED\n        if self.is_mbgp_cap_valid(RF_RTC_UC):\n            # Enqueues all best-RTC_NLRIs to be sent as initial update to this\n            # peer.\n            self._peer_manager.comm_all_rt_nlris(self)\n            self._schedule_sending_init_updates()\n        else:\n            # Enqueues all best-path to be sent as initial update to this peer\n            # expect for RTC route-family.\n            tm = self._core_service.table_manager\n            self.comm_all_best_paths(tm.global_tables)", "category": "Python"}, {"instruction": "def xpathNextAttribute(self, ctxt):\n        \"\"\"Traversal function for the \"attribute\" direction TODO:\n           support DTD inherited default attributes \"\"\"\n", "input": "", "output": "        if ctxt is None: ctxt__o = None\n        else: ctxt__o = ctxt._o\n        ret = libxml2mod.xmlXPathNextAttribute(ctxt__o, self._o)\n        if ret is None:raise xpathError('xmlXPathNextAttribute() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "category": "Python"}, {"instruction": "def _get_args_to_parse(args, sys_argv):\n    \"\"\"Return the given arguments if it is not None else sys.argv if it contains\n        something, an empty list otherwise.\n\n    Args:\n        args: argument to be parsed\n        sys_argv: arguments of the command line i.e. sys.argv\n    \"\"\"\n", "input": "", "output": "    arguments = args if args is not None else sys_argv[1:]\n    _LOG.debug(\"Parsing arguments: %s\", arguments)\n    return arguments", "category": "Python"}, {"instruction": "def dump_migration_session_state(raw):\n    \"\"\"\n    Serialize a migration session state to yaml using nicer formatting\n\n    Args:\n        raw: object to serialize\n    Returns: string (of yaml)\n\n    Specifically, this forces the \"output\" member of state step dicts (e.g.\n    state[0]['output']) to use block formatting. For example, rather than this:\n\n    - migration: [app, migration_name]\n      output: \"line 1\\nline2\\nline3\"\n\n    You get this:\n\n    - migration: [app, migration_name]\n      output: |\n        line 1\n        line 2\n        line 3\n    \"\"\"\n", "input": "", "output": "    class BlockStyle(str): pass\n    class SessionDumper(yaml.SafeDumper): pass\n    def str_block_formatter(dumper, data):\n        return dumper.represent_scalar(u'tag:yaml.org,2002:str', data, style='|')\n    SessionDumper.add_representer(BlockStyle, str_block_formatter)\n\n    raw = deepcopy(raw)\n    for step in raw:\n        step['output'] = BlockStyle(step['output'])\n        step['traceback'] = BlockStyle(step['traceback'])\n    return yaml.dump(raw, Dumper=SessionDumper)", "category": "Python"}, {"instruction": "def post_revert(self, post_id, version_id):\n        \"\"\"Function to reverts a post to a previous version (Requires login).\n\n        Parameters:\n            post_id (int):\n            version_id (int): The post version id to revert to.\n        \"\"\"\n", "input": "", "output": "        return self._get('posts/{0}/revert.json'.format(post_id),\n                         {'version_id': version_id}, 'PUT', auth=True)", "category": "Python"}, {"instruction": "def bitbucket(branch: str):\n    \"\"\"\n    Performs necessary checks to ensure that the bitbucket build is one\n    that should create releases.\n\n    :param branch: The branch the environment should be running against.\n    \"\"\"\n", "input": "", "output": "    assert os.environ.get('BITBUCKET_BRANCH') == branch\n    assert not os.environ.get('BITBUCKET_PR_ID')", "category": "Python"}, {"instruction": "def video_pixel_noise_bottom(x, model_hparams, vocab_size):\n  \"\"\"Bottom transformation for video.\"\"\"\n", "input": "", "output": "  input_noise = getattr(model_hparams, \"video_modality_input_noise\", 0.25)\n  inputs = x\n  if model_hparams.mode == tf.estimator.ModeKeys.TRAIN:\n    background = tfp.stats.percentile(inputs, 50., axis=[0, 1, 2, 3])\n    input_shape = common_layers.shape_list(inputs)\n    input_size = tf.reduce_prod(input_shape[:-1])\n    input_mask = tf.multinomial(\n        tf.log([[input_noise, 1.-input_noise]]), input_size)\n    input_mask = tf.reshape(tf.cast(input_mask, tf.int32),\n                            input_shape[:-1]+[1])\n    inputs = inputs * input_mask + background * (1 - input_mask)\n  return video_bottom(inputs, model_hparams, vocab_size)", "category": "Python"}, {"instruction": "def get_network_by_name(self, nwk_name):\n        \"\"\"Search for a openstack network by name. \"\"\"\n", "input": "", "output": "        ret_net_lst = []\n        try:\n            body = {}\n            net_list = self.neutronclient.list_networks(body=body)\n            net_list = net_list.get('networks')\n            for net in net_list:\n                if net.get('name') == nwk_name:\n                    ret_net_lst.append(net)\n        except Exception as exc:\n            LOG.error(\"Failed to get network by name %(name)s, \"\n                      \"Exc %(exc)s\",\n                      {'name': nwk_name, 'exc': str(exc)})\n        return ret_net_lst", "category": "Python"}, {"instruction": "def get_number_unit(number):\n    \"\"\"get the unit of number\"\"\"\n", "input": "", "output": "    n = str(float(number))\n    mult, submult = n.split('.')\n    if float(submult) != 0:\n        unit = '0.' + (len(submult)-1)*'0' + '1'\n        return float(unit)\n    else:\n        return float(1)", "category": "Python"}, {"instruction": "def fromcsv(args):\n    \"\"\"\n    %prog fromcsv contigs.fasta map.csv map.agp\n\n    Convert csv which contains list of scaffolds/contigs to AGP file.\n    \"\"\"\n", "input": "", "output": "    import csv\n    from jcvi.formats.sizes import Sizes\n\n    p = OptionParser(fromcsv.__doc__)\n    p.add_option(\"--evidence\", default=\"map\",\n                 help=\"Linkage evidence to add in AGP\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 3:\n        sys.exit(not p.print_help())\n\n    contigsfasta, mapcsv, mapagp = args\n    reader = csv.reader(open(mapcsv))\n    sizes = Sizes(contigsfasta).mapping\n    next(reader)   # Header\n    fwagp = must_open(mapagp, \"w\")\n    o = OO()\n    for row in reader:\n        if len(row) == 2:\n            object, ctg = row\n            strand = '?'\n        elif len(row) == 3:\n            object, ctg, strand = row\n        size = sizes[ctg]\n        o.add(object, ctg, size, strand)\n\n    o.write_AGP(fwagp, gapsize=100, gaptype=\"scaffold\",\n                phases={}, evidence=opts.evidence)", "category": "Python"}, {"instruction": "def _req_lixian_task_lists(self, page=1):\n        \"\"\"\n        This request will cause the system to create a default downloads\n        directory if it does not exist\n        \"\"\"\n", "input": "", "output": "        url = 'http://115.com/lixian/'\n        params = {'ct': 'lixian', 'ac': 'task_lists'}\n        self._load_signatures()\n        data = {\n            'page': page,\n            'uid': self.user_id,\n            'sign': self._signatures['offline_space'],\n            'time': self._lixian_timestamp,\n        }\n        req = Request(method='POST', url=url, params=params, data=data)\n        res = self.http.send(req)\n        if res.state:\n            self._task_count = res.content['count']\n            self._task_quota = res.content['quota']\n            return res.content['tasks']\n        else:\n            msg = 'Failed to get tasks.'\n            raise RequestFailure(msg)", "category": "Python"}, {"instruction": "def phone_text_subs():\n  \"\"\"\n  Gets a dictionary of dictionaries that each contain alphabetic number manifestations mapped to their actual\n  Number value.\n\n  Returns:\n    dictionary of dictionaries containing Strings mapped to Numbers\n\n  \"\"\"\n", "input": "", "output": "\n  Small = {\n    'zero': 0,\n    'zer0': 0,\n    'one': 1,\n    'two': 2,\n    'three': 3,\n    'four': 4,\n    'fuor': 4,\n    'five': 5,\n    'fith': 5,\n    'six': 6,\n    'seven': 7,\n    'sven': 7,\n    'eight': 8,\n    'nine': 9,\n    'ten': 10,\n    'eleven': 11,\n    'twelve': 12,\n    'thirteen': 13,\n    'fourteen': 14,\n    'fifteen': 15,\n    'sixteen': 16,\n    'seventeen': 17,\n    'eighteen': 18,\n    'nineteen': 19,\n    'twenty': 20,\n    'thirty': 30,\n    'forty': 40,\n    'fifty': 50,\n    'sixty': 60,\n    'seventy': 70,\n    'eighty': 80,\n    'ninety': 90,\n    'oh': 0\n  }\n\n  Magnitude = {\n    'thousand': 000,\n    'million': 000000,\n  }\n\n  Others = {\n    '!': 1,\n    'o': 0,\n    'l': 1,\n    'i': 1\n  }\n\n  output = {}\n  output['Small'] = Small\n  output['Magnitude'] = Magnitude\n  output['Others'] = Others\n\n  return output", "category": "Python"}, {"instruction": "def _plot(self):\n        \"\"\"Draw all the serie slices\"\"\"\n", "input": "", "output": "        squares = self._squares()\n        sq_dimensions = self.add_squares(squares)\n\n        for index, serie in enumerate(self.series):\n            current_square = self._current_square(squares, index)\n            self.gaugify(serie, squares, sq_dimensions, current_square)", "category": "Python"}, {"instruction": "def _metadata(image, datapath):\n    \"\"\"Function which returns metadata dict.\n\n    :param image: image to get spacing from\n    :param datapath: path to data\n    :return: {'series_number': '', 'datadir': '', 'voxelsize_mm': ''}\n    \"\"\"\n", "input": "", "output": "    metadata = {'series_number': 0, 'datadir': datapath}\n    spacing = image.GetSpacing()\n    metadata['voxelsize_mm'] = [\n        spacing[2],\n        spacing[0],\n        spacing[1],\n    ]\n    return metadata", "category": "Python"}, {"instruction": "def dump(self, file_name, page_size=10, vtimeout=10, sep='\\n'):\n        \"\"\"Utility function to dump the messages in a queue to a file\n        NOTE: Page size must be < 10 else SQS errors\"\"\"\n", "input": "", "output": "        fp = open(file_name, 'wb')\n        n = 0\n        l = self.get_messages(page_size, vtimeout)\n        while l:\n            for m in l:\n                fp.write(m.get_body())\n                if sep:\n                    fp.write(sep)\n                n += 1\n            l = self.get_messages(page_size, vtimeout)\n        fp.close()\n        return n", "category": "Python"}, {"instruction": "def create_template_context(self, element, **kwargs):\n        \"\"\"Code generation context, specific to template and current element.\"\"\"\n", "input": "", "output": "        context = dict(element=element, **kwargs)\n        if self.global_context:\n            context.update(**self.global_context)\n        return context", "category": "Python"}, {"instruction": "def _remove_bdb_context(evalue):\n        \"\"\"Remove exception context from Pdb from the exception.\n\n        E.g. \"AttributeError: 'Pdb' object has no attribute 'do_foo'\",\n        when trying to look up commands (bpo-36494).\n        \"\"\"\n", "input": "", "output": "        removed_bdb_context = evalue\n        while removed_bdb_context.__context__:\n            ctx = removed_bdb_context.__context__\n            if (\n                isinstance(ctx, AttributeError)\n                and ctx.__traceback__.tb_frame.f_code.co_name == \"onecmd\"\n            ):\n                removed_bdb_context.__context__ = None\n                break\n            removed_bdb_context = removed_bdb_context.__context__", "category": "Python"}, {"instruction": "def get_energy(self, y_true, input_energy, mask):\n        \"\"\"Energy = a1' y1 + u1' y1 + y1' U y2 + u2' y2 + y2' U y3 + u3' y3 + an' y3\n        \"\"\"\n", "input": "", "output": "        input_energy = K.sum(input_energy * y_true, 2)  # (B, T)\n        chain_energy = K.sum(K.dot(y_true[:, :-1, :], self.chain_kernel) * y_true[:, 1:, :], 2)  # (B, T-1)\n\n        if mask is not None:\n            mask = K.cast(mask, K.floatx())\n            chain_mask = mask[:, :-1] * mask[:, 1:]  # (B, T-1), mask[:,:-1]*mask[:,1:] makes it work with any padding\n            input_energy = input_energy * mask\n            chain_energy = chain_energy * chain_mask\n        total_energy = K.sum(input_energy, -1) + K.sum(chain_energy, -1)  # (B, )\n\n        return total_energy", "category": "Python"}, {"instruction": "def get(self, sid):\n        \"\"\"\n        Constructs a DocumentContext\n\n        :param sid: The sid\n\n        :returns: twilio.rest.preview.sync.service.document.DocumentContext\n        :rtype: twilio.rest.preview.sync.service.document.DocumentContext\n        \"\"\"\n", "input": "", "output": "        return DocumentContext(self._version, service_sid=self._solution['service_sid'], sid=sid, )", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"Render and display Python package documentation.\n        \"\"\"\n", "input": "", "output": "        os.environ['JARN_RUN'] = '1'\n        self.python.check_valid_python()\n\n        args = self.parse_options(self.args)\n        if args:\n            arg = args[0]\n        else:\n            arg = os.curdir\n        if arg:\n            arg = expanduser(arg)\n\n        if isfile(arg):\n            outfile = self.render_file(arg)\n        elif isdir(arg):\n            outfile = self.render_long_description(arg)\n        else:\n            err_exit('No such file or directory: %s' % arg)\n\n        self.open_in_browser(outfile)", "category": "Python"}, {"instruction": "def set_up_network(\n            self,\n            genes: List[Gene],\n            gene_filter: bool = False,\n            disease_associations: Optional[Dict] = None\n    ) -> None:\n        \"\"\"Set up the network.\n\n         Filter genes out if requested and add attributes to the vertices.\n\n        :param genes: A list of Gene objects.\n        :param gene_filter: Removes all genes that are not in list <genes> if True.\n        :param disease_associations: Diseases associated with genes.\n        \"\"\"\n", "input": "", "output": "        if gene_filter:\n            self.filter_genes([gene.entrez_id for gene in genes])\n        self._add_vertex_attributes(genes, disease_associations)\n        self.print_summary(\"Graph of all genes\")", "category": "Python"}, {"instruction": "def AddBatchJob(client):\n  \"\"\"Add a new BatchJob to upload operations to.\n\n  Args:\n    client: an instantiated AdWordsClient used to retrieve the BatchJob.\n\n  Returns:\n    The new BatchJob created by the request.\n  \"\"\"\n", "input": "", "output": "  # Initialize appropriate service.\n  batch_job_service = client.GetService('BatchJobService', version='v201809')\n  # Create a BatchJob.\n  batch_job_operations = [{\n      'operand': {},\n      'operator': 'ADD'\n  }]\n  return batch_job_service.mutate(batch_job_operations)['value'][0]", "category": "Python"}, {"instruction": "def imag(self):\n        \"\"\"Element-wise imaginary part\n\n        Raises:\n            NoConjugateMatrix: if entries have no `conjugate` method and no\n                other way to determine the imaginary part\n\n        Note:\n            A mathematically equivalent way to obtain an imaginary matrix from\n            a complex matrix ``M`` is::\n\n                (M.conjugate() - M) / (I * 2)\n\n            with same same caveats as :attr:`real`.\n        \"\"\"\n", "input": "", "output": "\n        def im(val):\n            if hasattr(val, 'imag'):\n                return val.imag\n            elif hasattr(val, 'as_real_imag'):\n                return val.as_real_imag()[1]\n            elif hasattr(val, 'conjugate'):\n                return (val.conjugate() - val) / (2 * I)\n            else:\n                raise NoConjugateMatrix(\n                    \"Matrix entry %s contains has no defined \"\n                    \"conjugate\" % str(val))\n\n        # Note: Do NOT use self.matrix.real! This will give wrong results, as\n        # numpy thinks of objects (Operators) as real, even if they have no\n        # defined real part\n        return self.element_wise(im)", "category": "Python"}, {"instruction": "def encode_zarr_variable(var, needs_copy=True, name=None):\n    \"\"\"\n    Converts an Variable into an Variable which follows some\n    of the CF conventions:\n\n        - Nans are masked using _FillValue (or the deprecated missing_value)\n        - Rescaling via: scale_factor and add_offset\n        - datetimes are converted to the CF 'units since time' format\n        - dtype encodings are enforced.\n\n    Parameters\n    ----------\n    var : xarray.Variable\n        A variable holding un-encoded data.\n\n    Returns\n    -------\n    out : xarray.Variable\n        A variable which has been encoded as described above.\n    \"\"\"\n", "input": "", "output": "\n    var = conventions.encode_cf_variable(var, name=name)\n\n    # zarr allows unicode, but not variable-length strings, so it's both\n    # simpler and more compact to always encode as UTF-8 explicitly.\n    # TODO: allow toggling this explicitly via dtype in encoding.\n    coder = coding.strings.EncodedStringCoder(allows_unicode=False)\n    var = coder.encode(var, name=name)\n    var = coding.strings.ensure_fixed_length_bytes(var)\n\n    return var", "category": "Python"}, {"instruction": "def __write_k_v(self, k, v, top=False, bot=False, multi=False, indent=False):\n        \"\"\"\n        Write a key value pair to the output file. If v is a list, write multiple lines.\n        :param k: Key\n        :param v: Value\n        :param bool top: Write preceding empty line\n        :param bool bot: Write following empty line\n        :param bool multi: v is a list\n        :return none:\n        \"\"\"\n", "input": "", "output": "        if top:\n            self.noaa_txt += \"\\n#\"\n        if multi:\n            for item in v:\n                if indent:\n                    self.noaa_txt += \"\\n#     {}: {}\".format(str(k), str(item))\n                else:\n                    self.noaa_txt += \"\\n# {}: {}\".format(str(k), str(item))\n        else:\n            if indent:\n                self.noaa_txt += \"\\n#     {}: {}\".format(str(k), str(v))\n            else:\n                self.noaa_txt += \"\\n# {}: {}\".format(str(k), str(v))\n        if bot:\n            self.noaa_txt += \"\\n#\"\n        return", "category": "Python"}, {"instruction": "def __get_user_env_vars(self):\r\n        \"\"\"Return the user defined environment variables\"\"\"\n", "input": "", "output": "        return (os.environ.get(self.GP_URL_ENV_VAR),\r\n            os.environ.get(self.GP_INSTANCE_ID_ENV_VAR),\r\n            os.environ.get(self.GP_USER_ID_ENV_VAR),\r\n            os.environ.get(self.GP_PASSWORD_ENV_VAR),\r\n            os.environ.get(self.GP_IAM_API_KEY_ENV_VAR))", "category": "Python"}, {"instruction": "def _merge_associative_list(alist, path, value):\n    \"\"\"\n    Merge a value into an associative list at the given path, maintaining\n    insertion order. Examples will explain it::\n\n        >>> alist = []\n        >>> _merge_associative_list(alist, [\"foo\", \"bar\"], \"barvalue\")\n        >>> _merge_associative_list(alist, [\"foo\", \"baz\"], \"bazvalue\")\n        >>> alist == [(\"foo\", [(\"bar\", \"barvalue\"), (\"baz\", \"bazvalue\")])]\n\n    @param alist: An associative list of names to values.\n    @param path: A path through sub-alists which we ultimately want to point to\n    C{value}.\n    @param value: The value to set.\n    @return: None. This operation mutates the associative list in place.\n    \"\"\"\n", "input": "", "output": "    for key in path[:-1]:\n        for item in alist:\n            if item[0] == key:\n                alist = item[1]\n                break\n        else:\n            subalist = []\n            alist.append((key, subalist))\n            alist = subalist\n    alist.append((path[-1], value))", "category": "Python"}, {"instruction": "def rect_helper(x0, y0, x1, y1):\n        \"\"\"Rectangle helper\"\"\"\n", "input": "", "output": "        x0, y0, x1, y1 = force_int(x0, y0, x1, y1)\n        if x0 > x1:\n            x0, x1 = x1, x0\n        if y0 > y1:\n            y0, y1 = y1, y0\n        return x0, y0, x1, y1", "category": "Python"}, {"instruction": "def _raise_if_annotated(self, func):\n        \"\"\"Raise TypeError if a function is decorated with Annotate, as such\n        functions cause visual bugs when decorated with Animate.\n\n        Animate should be wrapped by Annotate instead.\n\n        Args:\n            func (function): Any callable.\n        Raises:\n            TypeError\n        \"\"\"\n", "input": "", "output": "        if hasattr(func, ANNOTATED) and getattr(func, ANNOTATED):\n            msg = ('Functions decorated with {!r} '\n                   'should not be decorated with {!r}.\\n'\n                   'Please reverse the order of the decorators!'.format(\n                       self.__class__.__name__, Annotate.__name__))\n            raise TypeError(msg)", "category": "Python"}, {"instruction": "def init(ciprcfg, env, console):\n    \"\"\"\n    Initialize a Corona project directory.\n    \"\"\"\n", "input": "", "output": "    ciprcfg.create()\n\n    templ_dir = path.join(env.skel_dir, 'default')\n\n    console.quiet('Copying files from %s' % templ_dir)\n    for src, dst in util.sync_dir_to(templ_dir, env.project_directory, ignore_existing=True):\n        console.quiet('  %s -> %s' % (src, dst))\n\n    src = path.join(env.code_dir, 'cipr.dev.lua')\n    dst = path.join(env.project_directory, 'cipr.lua')\n    console.quiet('  %s -> %s' % (src, dst))\n\n    shutil.copy(src, dst)", "category": "Python"}, {"instruction": "def ensure_text(str_or_bytes, encoding='utf-8'):\n    \"\"\"Ensures an input is a string, decoding if it is bytes.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(str_or_bytes, six.text_type):\n        return str_or_bytes.decode(encoding)\n    return str_or_bytes", "category": "Python"}, {"instruction": "def get_preorder_burn_info( outputs ):\n    \"\"\"\n    Given the set of outputs, find the fee sent \n    to our burn address.  This is always the third output.\n    \n    Return the fee and burn address on success as {'op_fee': ..., 'burn_address': ...}\n    Return None if not found\n    \"\"\"\n", "input": "", "output": "    \n    if len(outputs) != 3:\n        # not a well-formed preorder \n        return None \n   \n    op_fee = outputs[2]['value']\n    burn_address = None\n\n    try:\n        burn_address = virtualchain.script_hex_to_address(outputs[2]['script'])\n        assert burn_address\n    except:\n        log.error(\"Not a well-formed preorder burn: {}\".format(outputs[2]['script']))\n        return None\n\n    return {'op_fee': op_fee, 'burn_address': burn_address}", "category": "Python"}, {"instruction": "def migrateDown(self):\n        \"\"\"\n        Remove the components in the site store for this SubScheduler.\n        \"\"\"\n", "input": "", "output": "        subStore = self.store.parent.getItemByID(self.store.idInParent)\n        ssph = self.store.parent.findUnique(\n            _SubSchedulerParentHook,\n            _SubSchedulerParentHook.subStore == subStore,\n            default=None)\n        if ssph is not None:\n            te = self.store.parent.findUnique(TimedEvent,\n                                              TimedEvent.runnable == ssph,\n                                              default=None)\n            if te is not None:\n                te.deleteFromStore()\n            ssph.deleteFromStore()", "category": "Python"}, {"instruction": "def chisqprob(x, df):\n    \"\"\"\n    Probability value (1-tail) for the Chi^2 probability distribution.\n\n    Broadcasting rules apply.\n\n    Parameters\n    ----------\n    x : array_like or float > 0\n\n    df : array_like or float, probably int >= 1\n\n    Returns\n    -------\n    chisqprob : ndarray\n        The area from `chisq` to infinity under the Chi^2 probability\n        distribution with degrees of freedom `df`.\n\n    \"\"\"\n", "input": "", "output": "    if x <= 0:\n        return 1.0\n    if x == 0:\n        return 0.0\n    if df <= 0:\n        raise ValueError(\"Domain error.\")\n    if x < 1.0 or x < df:\n        return 1.0 - _igam(0.5*df, 0.5*x)\n    return _igamc(0.5*df, 0.5*x)", "category": "Python"}, {"instruction": "def package_locations(self, package_keyname):\n        \"\"\"List datacenter locations for a package keyname\n\n        :param str package_keyname: The package for which to get the items.\n        :returns: List of locations a package is orderable in\n        \"\"\"\n", "input": "", "output": "        mask = \"mask[description, keyname, locations]\"\n\n        package = self.get_package_by_key(package_keyname, mask='id')\n\n        regions = self.package_svc.getRegions(id=package['id'], mask=mask)\n        return regions", "category": "Python"}, {"instruction": "def dctii(x, axes=None):\n    \"\"\"\n    Compute a multi-dimensional DCT-II over specified array axes. This\n    function is implemented by calling the one-dimensional DCT-II\n    :func:`scipy.fftpack.dct` with normalization mode 'ortho' for each\n    of the specified axes.\n\n    Parameters\n    ----------\n    a : array_like\n      Input array\n    axes : sequence of ints, optional (default None)\n      Axes over which to compute the DCT-II.\n\n    Returns\n    -------\n    y : ndarray\n      DCT-II of input array\n    \"\"\"\n", "input": "", "output": "\n    if axes is None:\n        axes = list(range(x.ndim))\n    for ax in axes:\n        x = fftpack.dct(x, type=2, axis=ax, norm='ortho')\n    return x", "category": "Python"}, {"instruction": "def _ensure_frames(cls, documents):\n        \"\"\"\n        Ensure all items in a list are frames by converting those that aren't.\n        \"\"\"\n", "input": "", "output": "        frames = []\n        for document in documents:\n            if not isinstance(document, Frame):\n                frames.append(cls(document))\n            else:\n                frames.append(document)\n        return frames", "category": "Python"}, {"instruction": "def _load_github_hooks(github_url='https://api.github.com'):\n    \"\"\"Request GitHub's IP block from their API.\n\n    Return the IP network.\n\n    If we detect a rate-limit error, raise an error message stating when\n    the rate limit will reset.\n\n    If something else goes wrong, raise a generic 503.\n    \"\"\"\n", "input": "", "output": "    try:\n        resp = requests.get(github_url + '/meta')\n        if resp.status_code == 200:\n            return resp.json()['hooks']\n        else:\n            if resp.headers.get('X-RateLimit-Remaining') == '0':\n                reset_ts = int(resp.headers['X-RateLimit-Reset'])\n                reset_string = time.strftime('%a, %d %b %Y %H:%M:%S GMT',\n                                             time.gmtime(reset_ts))\n                raise ServiceUnavailable('Rate limited from GitHub until ' +\n                                         reset_string)\n            else:\n                raise ServiceUnavailable('Error reaching GitHub')\n    except (KeyError, ValueError, requests.exceptions.ConnectionError):\n        raise ServiceUnavailable('Error reaching GitHub')", "category": "Python"}, {"instruction": "def add_check(self, check_item):\n        \"\"\"Adds a check universally.\"\"\"\n", "input": "", "output": "        self.checks.append(check_item)\n        for other in self.others:\n            other.add_check(check_item)", "category": "Python"}, {"instruction": "def validate(**kwargs):\n    \"\"\"Defines a decorator to register a validator with a name for look-up.\n\n    If name is not provided we use function name as name of the validator.\n    \"\"\"\n", "input": "", "output": "    def decorator(func):\n        _VALIDATORS[kwargs.pop('name', func.__name__)] = func\n        return func\n\n    return decorator", "category": "Python"}, {"instruction": "def go_standby(self, comment=None):\n        \"\"\"\n        Executes a Go-Standby operation on the specified node.\n        To get the status of the current node/s, run :func:`status`\n\n        :param str comment: optional comment to audit\n        :raises NodeCommandFailed: engine cannot go standby\n        :return: None\n        \"\"\"\n", "input": "", "output": "        self.make_request(\n            NodeCommandFailed,\n            method='update',\n            resource='go_standby',\n            params={'comment': comment})", "category": "Python"}, {"instruction": "def retrieve_secret_id(url, token):\n    \"\"\"Retrieve a response-wrapped secret_id from Vault\n\n    :param url: URL to Vault Server\n    :ptype url: str\n    :param token: One shot Token to use\n    :ptype token: str\n    :returns: secret_id to use for Vault Access\n    :rtype: str\"\"\"\n", "input": "", "output": "    import hvac\n    client = hvac.Client(url=url, token=token)\n    response = client._post('/v1/sys/wrapping/unwrap')\n    if response.status_code == 200:\n        data = response.json()\n        return data['data']['secret_id']", "category": "Python"}, {"instruction": "def _record(self):\n        # type: () -> bytes\n        '''\n        An internal method to generate a string representing this El Torito\n        Validation Entry.\n\n        Parameters:\n         None.\n        Returns:\n         String representing this El Torito Validation Entry.\n        '''\n", "input": "", "output": "        return struct.pack(self.FMT, 1, self.platform_id, 0, self.id_string,\n                           self.checksum, 0x55, 0xaa)", "category": "Python"}, {"instruction": "def data_period_name_or_description(self, value=None):\n        \"\"\"Corresponds to IDD Field `data_period_name_or_description`\n\n        Args:\n            value (str): value for IDD Field `data_period_name_or_description`\n                if `value` is None it will not be checked against the\n                specification and is assumed to be a missing value\n\n        Raises:\n            ValueError: if `value` is not a valid value\n\n        \"\"\"\n", "input": "", "output": "        if value is not None:\n            try:\n                value = str(value)\n            except ValueError:\n                raise ValueError(\n                    'value {} need to be of type str '\n                    'for field `data_period_name_or_description`'.format(value))\n            if ',' in value:\n                raise ValueError('value should not contain a comma '\n                                 'for field `data_period_name_or_description`')\n\n        self._data_period_name_or_description = value", "category": "Python"}, {"instruction": "def handle_data(self, ts, msg):\n        \"\"\"\n        Passes msg to responding data handler, determined by its channel id,\n        which is expected at index 0.\n        :param ts: timestamp, declares when data was received by the client\n        :param msg: list or dict of websocket data\n        :return:\n        \"\"\"\n", "input": "", "output": "        try:\n            chan_id, *data = msg\n        except ValueError as e:\n            # Too many or too few values\n            raise FaultyPayloadError(\"handle_data(): %s - %s\" % (msg, e))\n        self._heartbeats[chan_id] = ts\n        if data[0] == 'hb':\n            self._handle_hearbeat(ts, chan_id)\n            return\n        try:\n            self.channels[chan_id](ts, chan_id, data)\n        except KeyError:\n            raise NotRegisteredError(\"handle_data: %s not registered - \"\n                                     \"Payload: %s\" % (chan_id, msg))", "category": "Python"}, {"instruction": "def is_component(w, ids):\n    \"\"\"Check if the set of ids form a single connected component\n\n    Parameters\n    ----------\n\n    w   : spatial weights boject\n\n    ids : list\n          identifiers of units that are tested to be a single connected\n          component\n\n\n    Returns\n    -------\n\n    True    : if the list of ids represents a single connected component\n\n    False   : if the list of ids forms more than a single connected component\n\n    \"\"\"\n", "input": "", "output": "\n    components = 0\n    marks = dict([(node, 0) for node in ids])\n    q = []\n    for node in ids:\n        if marks[node] == 0:\n            components += 1\n            q.append(node)\n            if components > 1:\n                return False\n        while q:\n            node = q.pop()\n            marks[node] = components\n            others = [neighbor for neighbor in w.neighbors[node]\n                      if neighbor in ids]\n            for other in others:\n                if marks[other] == 0 and other not in q:\n                    q.append(other)\n    return True", "category": "Python"}, {"instruction": "def _toState(self, state, *args, **kwargs):\n        \"\"\"\n        Transition to the next state.\n\n        @param state: Name of the next state.\n        \"\"\"\n", "input": "", "output": "        try:\n            method = getattr(self, '_state_%s' % state)\n        except AttributeError:\n            raise ValueError(\"No such state %r\" % state)\n\n        log.msg(\"%s: to state %r\" % (self.__class__.__name__, state))\n        self._state = state\n        method(*args, **kwargs)", "category": "Python"}, {"instruction": "def private_download_url(self, url, expires=3600):\n        \"\"\"\u751f\u6210\u79c1\u6709\u8d44\u6e90\u4e0b\u8f7d\u94fe\u63a5\n\n        Args:\n            url:     \u79c1\u6709\u7a7a\u95f4\u8d44\u6e90\u7684\u539f\u59cbURL\n            expires: \u4e0b\u8f7d\u51ed\u8bc1\u6709\u6548\u671f\uff0c\u9ed8\u8ba4\u4e3a3600s\n\n        Returns:\n            \u79c1\u6709\u8d44\u6e90\u7684\u4e0b\u8f7d\u94fe\u63a5\n        \"\"\"\n", "input": "", "output": "        deadline = int(time.time()) + expires\n        if '?' in url:\n            url += '&'\n        else:\n            url += '?'\n        url = '{0}e={1}'.format(url, str(deadline))\n\n        token = self.token(url)\n        return '{0}&token={1}'.format(url, token)", "category": "Python"}, {"instruction": "def result(cls, ab, pa, pitch_list):\n        \"\"\"\n        At Bat Result\n        :param ab: at bat object(type:Beautifulsoup)\n        :param pa: atbat data for plate appearance\n        :param pitch_list: Pitching data\n        :return: pa result value(dict)\n        \"\"\"\n", "input": "", "output": "        atbat = OrderedDict()\n        atbat['ball_ct'] = MlbamUtil.get_attribute_stats(ab, 'b', int, None)\n        atbat['strike_ct'] = MlbamUtil.get_attribute_stats(ab, 's', int, None)\n        atbat['pitch_seq'] = ''.join([pitch['pitch_res'] for pitch in pitch_list])\n        atbat['pitch_type_seq'] = '|'.join([pitch['pitch_type'] for pitch in pitch_list])\n        atbat['battedball_cd'] = RetroSheet.battedball_cd(pa['event_cd'], pa['event_tx'], pa['ab_des'])\n        return atbat", "category": "Python"}, {"instruction": "def example_exc_handler(tries_remaining, exception, delay):\n    \"\"\"Example exception handler; prints a warning to stderr.\n\n    tries_remaining: The number of tries remaining.\n    exception: The exception instance which was raised.\n    \"\"\"\n", "input": "", "output": "    print >> sys.stderr, \"Caught '%s', %d tries remaining, sleeping for %s seconds\" % (\n        exception, tries_remaining, delay)", "category": "Python"}, {"instruction": "def pys_width2xls_width(self, pys_width):\n        \"\"\"Returns xls width from given pyspread width\"\"\"\n", "input": "", "output": "\n        width_0 = get_default_text_extent(\"0\")[0]\n        # Scale relative to 12 point font instead of 10 point\n        width_0_char = pys_width * 1.2 / width_0\n        return int(width_0_char * 256.0)", "category": "Python"}, {"instruction": "def unhexlify(blob):\n    \"\"\"\n    Takes a hexlified script and turns it back into a string of Python code.\n    \"\"\"\n", "input": "", "output": "    lines = blob.split('\\n')[1:]\n    output = []\n    for line in lines:\n        # Discard the address, length etc. and reverse the hexlification\n        output.append(binascii.unhexlify(line[9:-2]))\n    # Check the header is correct (\"MP<size>\")\n    if (output[0][0:2].decode('utf-8') != u'MP'):\n        return ''\n    # Strip off header\n    output[0] = output[0][4:]\n    # and strip any null bytes from the end\n    output[-1] = output[-1].strip(b'\\x00')\n    script = b''.join(output)\n    try:\n        result = script.decode('utf-8')\n        return result\n    except UnicodeDecodeError:\n        # Return an empty string because in certain rare circumstances (where\n        # the source hex doesn't include any embedded Python code) this\n        # function may be passed in \"raw\" bytes from MicroPython.\n        return ''", "category": "Python"}, {"instruction": "def commit_changeset(self, changeset_id: uuid.UUID) -> Dict[bytes, Union[bytes, DeletedEntry]]:\n        \"\"\"\n        Collapses all changes for the given changeset into the previous\n        changesets if it exists.\n        \"\"\"\n", "input": "", "output": "        does_clear = self.has_clear(changeset_id)\n        changeset_data = self.pop_changeset(changeset_id)\n        if not self.is_empty():\n            # we only have to assign changeset data into the latest changeset if\n            # there is one.\n            if does_clear:\n                # if there was a clear and more changesets underneath then clear the latest\n                # changeset, and replace with a new clear changeset\n                self.latest = {}\n                self._clears_at.add(self.latest_id)\n                self.record_changeset()\n                self.latest = changeset_data\n            else:\n                # otherwise, merge in all the current data\n                self.latest = merge(\n                    self.latest,\n                    changeset_data,\n                )\n        return changeset_data", "category": "Python"}, {"instruction": "def fileParameter(self, comp):\n        \"\"\"Returns the row which component *comp* can be found in the \n        selections of, and is also a filename parameter\n\n        :returns: int -- the index of the (filename) parameter *comp* is a member of \n        \"\"\"\n", "input": "", "output": "        for row in range(self.nrows()):\n            p = self._parameters[row]\n            if p['parameter'] == 'filename':\n                # ASSUMES COMPONENT IN ONE SELECTION\n                if comp in p['selection']:\n                    return row", "category": "Python"}, {"instruction": "def simple_settings(cls):\n    \"\"\"\n    Create sane default persistence settings for learning pipelines\n    :param cls: The class to decorate\n    \"\"\"\n", "input": "", "output": "\n    class Settings(ff.PersistenceSettings):\n        _id = cls.__name__\n        id_provider = ff.StaticIdProvider(_id)\n        key_builder = ff.StringDelimitedKeyBuilder()\n        database = ff.FileSystemDatabase(\n            path=_id, key_builder=key_builder, createdirs=True)\n\n    class Model(cls, Settings):\n        pass\n\n    Model.__name__ = cls.__name__\n    Model.__module__ = cls.__module__\n    return Model", "category": "Python"}, {"instruction": "def reboot(at_time=None):\n    '''\n    Reboot the system\n\n    at_time\n        The wait time in minutes before the system will be rebooted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.reboot\n    '''\n", "input": "", "output": "    cmd = ['shutdown', '-r', ('{0}'.format(at_time) if at_time else 'now')]\n    ret = __salt__['cmd.run'](cmd, python_shell=False)\n    return ret", "category": "Python"}, {"instruction": "def get_next_action(self, request, application, label, roles):\n        \"\"\" Django get_next_action method. \"\"\"\n", "input": "", "output": "        actions = self.get_actions(request, application, roles)\n        if label is None and \\\n                'is_applicant' in roles and 'is_admin' not in roles:\n            for action in actions:\n                if action in request.POST:\n                    return action\n            link, is_secret = base.get_email_link(application)\n            return render(\n                template_name='kgapplications/software_introduction.html',\n                context={\n                    'actions': actions,\n                    'application': application,\n                    'roles': roles,\n                    'link': link,\n                    'is_secret': is_secret\n                },\n                request=request)\n        return super(StateIntroduction, self).get_next_action(\n            request, application, label, roles)", "category": "Python"}, {"instruction": "def pubkey(self, identity, ecdh=False):\n        \"\"\"Return public key as VerifyingKey object.\"\"\"\n", "input": "", "output": "        with self.device:\n            pubkey = self.device.pubkey(ecdh=ecdh, identity=identity)\n        return formats.decompress_pubkey(\n            pubkey=pubkey, curve_name=identity.curve_name)", "category": "Python"}, {"instruction": "def _get_host(self, region):\n        \"\"\"\n        Returns correctly formatted host. Accepted formats:\n\n            * simple region name, eg 'us-west-1' (see list in AWS_REGIONS)\n            * full host name, eg 's3-us-west-1.amazonaws.com'.\n        \"\"\"\n", "input": "", "output": "        if 'us-east-1' in region:\n            return 's3.amazonaws.com'\n        elif region in AWS_REGIONS:\n            return 's3-%s.amazonaws.com' % region\n        elif region and not REGION_RE.findall(region):\n            raise ImproperlyConfigured('AWS_REGION improperly configured!')\n        # can be full host or empty string, default region\n        return  region", "category": "Python"}, {"instruction": "def has_group_perms(self, perm, obj, approved):\n        \"\"\"\n        Check if group has the permission for the given object\n        \"\"\"\n", "input": "", "output": "        if not self.group:\n            return False\n\n        if self.use_smart_cache:\n            content_type_pk = Permission.objects.get_content_type(obj).pk\n\n            def _group_has_perms(cached_perms):\n                # Check to see if the permission is in the cache.\n                return cached_perms.get((\n                    obj.pk,\n                    content_type_pk,\n                    perm,\n                    approved,\n                ))\n\n            # Check to see if the permission is in the cache.\n            return _group_has_perms(self._group_perm_cache)\n\n        # Actually hit the DB, no smart cache used.\n        return Permission.objects.group_permissions(\n            self.group,\n            perm, obj,\n            approved,\n        ).filter(\n            object_id=obj.pk,\n        ).exists()", "category": "Python"}, {"instruction": "def continuous_binary_search(f, lo, hi, gap=1e-4):\n    \"\"\"Binary search for a function\n\n    :param f: boolean monotone function with f(hi) = True\n    :param int lo:\n    :param int hi: with hi >= lo\n    :param float gap:\n    :returns: first value x in [lo,hi] such that f(x),\n             x is computed up to some precision\n    :complexity: `O(log((hi-lo)/gap))`\n    \"\"\"\n", "input": "", "output": "    while hi - lo > gap:\n        # in other languages you can force floating division by using 2.0\n        mid = (lo + hi) / 2.\n        if f(mid):\n            hi = mid\n        else:\n            lo = mid\n    return lo", "category": "Python"}, {"instruction": "def parent_chain(self):\n        \"\"\"\n        Return the list of parents starting from this node. The chain ends\n        at the first node with no parents.\n        \"\"\"\n", "input": "", "output": "        chain = [self]\n        while True:\n            try:\n                parent = chain[-1].parent\n            except Exception:\n                break\n            if parent is None:\n                break\n            chain.append(parent)\n        return chain", "category": "Python"}, {"instruction": "def get_year(self):\n        \"\"\"\n        Return the year from the database in the format expected by the URL.\n        \"\"\"\n", "input": "", "output": "        year = super(BuildableDayArchiveView, self).get_year()\n        fmt = self.get_year_format()\n        dt = date(int(year), 1, 1)\n        return dt.strftime(fmt)", "category": "Python"}, {"instruction": "def fast_stats(data):\n    \"\"\" Compute base statistics about submissions \"\"\"\n", "input": "", "output": "    \n    total_submission = len(data)\n    total_submission_best = 0\n    total_submission_best_succeeded = 0\n        \n    for submission in data:\n        if \"best\" in submission and submission[\"best\"]:\n            total_submission_best = total_submission_best + 1\n            if \"result\" in submission and submission[\"result\"] == \"success\":\n                total_submission_best_succeeded += 1\n        \n    statistics = [\n        (_(\"Number of submissions\"), total_submission),\n        (_(\"Evaluation submissions (Total)\"), total_submission_best),\n        (_(\"Evaluation submissions (Succeeded)\"), total_submission_best_succeeded),\n        (_(\"Evaluation submissions (Failed)\"), total_submission_best - total_submission_best_succeeded),\n        # add here new common statistics\n        ]\n    \n    return statistics", "category": "Python"}, {"instruction": "def frac_vol_floc_initial(ConcAluminum, ConcClay, coag, material):\n    \"\"\"Return the volume fraction of flocs initially present, accounting for both suspended particles and coagulant precipitates.\n\n    :param ConcAluminum: Concentration of aluminum in solution\n    :type ConcAluminum: float\n    :param ConcClay: Concentration of particle in suspension\n    :type ConcClay: float\n    :param coag: Type of coagulant in solution\n    :type coag: float\n    :param material: Type of particles in suspension, e.g. floc_model.Clay\n    :type material: floc_model.Material\n\n    :return: Volume fraction of particles initially present\n    :rtype: float\n    \"\"\"\n", "input": "", "output": "    return ((conc_precipitate(ConcAluminum, coag).magnitude/coag.PrecipDensity)\n            + (ConcClay / material.Density))", "category": "Python"}, {"instruction": "def super_pipe(self):\n        \"\"\"\n        Creates a mechanism for us to internally bind two different\n        operations together in a shared pipeline on the class.\n        This will temporarily set self._pipe to be this new pipeline,\n        during this context and then when it leaves the context\n        reset self._pipe to its original value.\n\n        Example:\n            def get_set(self, key, val)\n                with self.super_pipe as pipe:\n                    res = self.get(key)\n                    self.set(key, val)\n                    return res\n\n        This will have the effect of using only one network round trip if no\n        pipeline was passed to the constructor.\n\n        This method is still considered experimental and we are working out\n        the details, so don't use it unless you feel confident you have a\n        legitimate use-case for using this.\n        \"\"\"\n", "input": "", "output": "        orig_pipe = self._pipe\n\n        def exit_handler():\n            self._pipe = orig_pipe\n\n        self._pipe = autoexec(orig_pipe, name=self.connection,\n                              exit_handler=exit_handler)\n\n        return self._pipe", "category": "Python"}, {"instruction": "def handle_get_version_command(self):\n        \"\"\" Handles <get_version> command.\n\n        @return: Response string for <get_version> command.\n        \"\"\"\n", "input": "", "output": "        protocol = Element('protocol')\n        for name, value in [('name', 'OSP'), ('version', self.get_protocol_version())]:\n            elem = SubElement(protocol, name)\n            elem.text = value\n\n        daemon = Element('daemon')\n        for name, value in [('name', self.get_daemon_name()), ('version', self.get_daemon_version())]:\n            elem = SubElement(daemon, name)\n            elem.text = value\n\n        scanner = Element('scanner')\n        for name, value in [('name', self.get_scanner_name()), ('version', self.get_scanner_version())]:\n            elem = SubElement(scanner, name)\n            elem.text = value\n\n        content = [protocol, daemon, scanner]\n\n        if self.get_vts_version():\n            vts = Element('vts')\n            elem = SubElement(vts, 'version')\n            elem.text = self.get_vts_version()\n            content.append(vts)\n\n        return simple_response_str('get_version', 200, 'OK', content)", "category": "Python"}, {"instruction": "def _set_default_headers(self):\n        \"\"\"\n        Create some default headers that should be sent along with every HTTP\n        response\n        \"\"\"\n", "input": "", "output": "        self.headers.setdefault('Date', self.get_current_time)\n        self.headers.setdefault('Server', self.SERVER_INFO)\n        self.headers.setdefault('Content-Length', \"%d\" % len(self.message))\n        if self.app.enabled('x-powered-by'):\n            self.headers.setdefault('X-Powered-By', 'Growler')", "category": "Python"}, {"instruction": "def hwstatus_send(self, Vcc, I2Cerr, force_mavlink1=False):\n                '''\n                Status of key hardware\n\n                Vcc                       : board voltage (mV) (uint16_t)\n                I2Cerr                    : I2C error count (uint8_t)\n\n                '''\n", "input": "", "output": "                return self.send(self.hwstatus_encode(Vcc, I2Cerr), force_mavlink1=force_mavlink1)", "category": "Python"}, {"instruction": "def to_JSON(self):\n        \"\"\"Dumps object fields into a JSON formatted string\n\n        :returns: the JSON string\n\n        \"\"\"\n", "input": "", "output": "        last = None\n        if self._last_weather:\n            last = self._last_weather.to_JSON()\n        return json.dumps({'name': self._name,\n                           'station_ID': self._station_ID,\n                           'station_type': self._station_type,\n                           'status': self._status,\n                           'lat': self._lat,\n                           'lon': self._lon,\n                           'distance': self._distance,\n                           'weather': json.loads(last),\n                           })", "category": "Python"}, {"instruction": "def get_method_by_name(self, class_name, method_name, method_descriptor):\n        \"\"\"\n        Search for a :class:`EncodedMethod` in all classes in this analysis\n\n        :param class_name: name of the class, for example 'Ljava/lang/Object;'\n        :param method_name: name of the method, for example 'onCreate'\n        :param method_descriptor: descriptor, for example '(I I Ljava/lang/String)V\n        :return: :class:`EncodedMethod` or None if method was not found\n        \"\"\"\n", "input": "", "output": "        if class_name in self.classes:\n            for method in self.classes[class_name].get_vm_class().get_methods():\n                if method.get_name() == method_name and method.get_descriptor() == method_descriptor:\n                    return method\n        return None", "category": "Python"}, {"instruction": "def create_page(name, parent_id, space, content):\n    \"\"\"Create a page in Confluence.\n    Parameters:\n    - name: name of the Confluence page to create.\n    - parent_id: ID of the intended parent of the page.\n    - space: key of the space where the page will be created.\n    - content: XHTML content to be written to the page.\n    Notes: the page id can be obtained by getting [\"id\"] from the returned JSON.\n    \"\"\"\n", "input": "", "output": "    data = {}\n    data[\"type\"] = \"page\"\n    data[\"title\"] = name\n    data[\"ancestors\"] = [{\"id\": str(parent_id)}]\n    data[\"space\"] = {\"key\": space}\n    data[\"body\"] = {\"storage\": {\"value\": content, \"representation\": \"storage\"}}\n    return _api.rest(\"/\", \"POST\", _json.dumps(data))", "category": "Python"}, {"instruction": "def add_size(self, n):\n        \"\"\"\n        Add an integer to the stream.\n        \n        :param int n: integer to add\n        \"\"\"\n", "input": "", "output": "        self.packet.write(struct.pack('>I', n))\n        return self", "category": "Python"}, {"instruction": "def get_instructor_term_list_name(instructor_netid, year, quarter):\n    \"\"\"\n    Return the list address of UW instructor email list for\n    the given year and quarter\n    \"\"\"\n", "input": "", "output": "    return \"{uwnetid}_{quarter}{year}\".format(\n        uwnetid=instructor_netid,\n        quarter=quarter.lower()[:2],\n        year=str(year)[-2:])", "category": "Python"}, {"instruction": "def getAvg(self,varname,**kwds):\n        '''\n        Calculates the average of an attribute of this instance.  Returns NaN if no such attribute.\n\n        Parameters\n        ----------\n        varname : string\n            The name of the attribute whose average is to be calculated.  This attribute must be an\n            np.array or other class compatible with np.mean.\n\n        Returns\n        -------\n        avg : float or np.array\n            The average of this attribute.  Might be an array if the axis keyword is passed.\n        '''\n", "input": "", "output": "        if hasattr(self,varname):\n            return np.mean(getattr(self,varname),**kwds)\n        else:\n            return np.nan", "category": "Python"}, {"instruction": "def species_list(what_list):\n    '''\n    provide default lists of elements to plot.\n\n    what_list : string\n        String name of species lists provided.\n\n        If what_list is \"CNONe\", then C, N, O and some other light\n        elements.\n\n        If what_list is \"s-process\", then s-process indicators.\n\n    '''\n", "input": "", "output": "    if what_list is \"CNONe\":\n        list_to_print = ['H-1','He-4','C-12','N-14','O-16','Ne-20']\n    elif what_list is \"sprocess\":\n        list_to_print = ['Fe-56','Ge-70','Zn-70','Se-76','Kr-80','Kr-82','Kr-86','Sr-88','Ba-138','Pb-208']\n    elif what_list is \"burn_stages\":\n        list_to_print = ['H-1','He-4','C-12','O-16','Ne-20','Si-28']\n    elif what_list is \"list_marco_1\":\n        list_to_print = ['C-12','O-16','Ne-20','Ne-22','Na-23','Fe-54','Fe-56','Zn-70','Ge-70','Se-76','Kr-80','Kr-82','Sr-88','Y-89','Zr-96','Te-124','Xe-130','Xe-134','Ba-138']\n\n    return list_to_print", "category": "Python"}, {"instruction": "def purge_metadata_by_name(self, name):\n    \"\"\"Purge a processes metadata directory.\n\n    :raises: `ProcessManager.MetadataError` when OSError is encountered on metadata dir removal.\n    \"\"\"\n", "input": "", "output": "    meta_dir = self._get_metadata_dir_by_name(name, self._metadata_base_dir)\n    logger.debug('purging metadata directory: {}'.format(meta_dir))\n    try:\n      rm_rf(meta_dir)\n    except OSError as e:\n      raise ProcessMetadataManager.MetadataError('failed to purge metadata directory {}: {!r}'.format(meta_dir, e))", "category": "Python"}, {"instruction": "def _make_fn_text(self):\r\n        \"\"\"Makes filename text\"\"\"\n", "input": "", "output": "        if not self._f:\r\n            text = \"(not loaded)\"\r\n        elif self._f.filename:\r\n            text = os.path.relpath(self._f.filename, \".\")\r\n        else:\r\n            text = \"(filename not set)\"\r\n        return text", "category": "Python"}, {"instruction": "def set_model(self, model):\n        \"\"\"Set the model the item belongs to\n\n        A TreeItem can only belong to one model.\n\n        :param model: the model the item belongs to\n        :type model: :class:`Treemodel`\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        self._model = model\n        for c in self.childItems:\n            c.set_model(model)", "category": "Python"}, {"instruction": "def add_isoquant_data(proteins, quantproteins, quantacc, quantfields):\n    \"\"\"Runs through a protein table and adds quant data from ANOTHER protein\n    table that contains that data.\"\"\"\n", "input": "", "output": "    for protein in base_add_isoquant_data(proteins, quantproteins,\n                                          prottabledata.HEADER_PROTEIN,\n                                          quantacc, quantfields):\n        yield protein", "category": "Python"}, {"instruction": "def update_info(self, custom=None):\n        \"\"\"Updates the figure's suptitle.\n\n        Calls self.info_string() unless custom is provided.\n\n        Args:\n            custom: Overwrite it with this string, unless None.\n        \"\"\"\n", "input": "", "output": "        self.figure.suptitle(self.info_string() if custom is None else custom)", "category": "Python"}, {"instruction": "def per_from_id_except(s, flavors=chat_flavors+inline_flavors):\n    \"\"\"\n    :param s:\n        a list or set of from id\n\n    :param flavors:\n        ``all`` or a list of flavors\n\n    :return:\n        a seeder function that returns the from id only if the from id is *not* in ``s``\n        and message flavor is in ``flavors``.\n    \"\"\"\n", "input": "", "output": "    return _wrap_none(lambda msg:\n                          msg['from']['id']\n                          if (flavors == 'all' or flavor(msg) in flavors) and msg['from']['id'] not in s\n                          else None)", "category": "Python"}, {"instruction": "def url_to_attrs_dict(url, url_attr):\n    \"\"\"\n    Sanitize url dict as used in django-bootstrap3 settings.\n    \"\"\"\n", "input": "", "output": "    result = dict()\n    # If url is not a string, it should be a dict\n    if isinstance(url, six.string_types):\n        url_value = url\n    else:\n        try:\n            url_value = url[\"url\"]\n        except TypeError:\n            raise BootstrapError(\n                'Function \"url_to_attrs_dict\" expects a string or a dict with key \"url\".'\n            )\n        crossorigin = url.get(\"crossorigin\", None)\n        integrity = url.get(\"integrity\", None)\n        if crossorigin:\n            result[\"crossorigin\"] = crossorigin\n        if integrity:\n            result[\"integrity\"] = integrity\n    result[url_attr] = url_value\n    return result", "category": "Python"}, {"instruction": "def scheduled(self) -> ScheduledTxsAggregate:\n        \"\"\" Scheduled Transactions \"\"\"\n", "input": "", "output": "        if not self.__scheduled_tx_aggregate:\n            self.__scheduled_tx_aggregate = ScheduledTxsAggregate(self.book)\n        return self.__scheduled_tx_aggregate", "category": "Python"}, {"instruction": "def get_rows(self, request, context):\n        \"\"\"\n        Get all rows as HTML\n        \"\"\"\n", "input": "", "output": "        from staff_toolbar.loading import load_toolbar_item\n        rows = []\n        for i, hook in enumerate(self.children):\n            # Allow dotted paths in groups too, loads on demand (get import errors otherwise).\n            if isinstance(hook, (basestring, tuple, list)):\n                hook = load_toolbar_item(hook)\n                self.children[i] = hook\n\n            html = hook(request, context)\n            if not html:\n                continue\n\n            rows.append(html)\n        return rows", "category": "Python"}, {"instruction": "def load_reader_options():\n    \"\"\"\n    Retrieve Pandoc Reader options from the environment\n    \"\"\"\n", "input": "", "output": "    options = os.environ['PANDOC_READER_OPTIONS']\n    options = json.loads(options, object_pairs_hook=OrderedDict)\n    return options", "category": "Python"}, {"instruction": "def reset(self):\n        \"\"\"\n        Clears all entries.\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        for i in range(len(self.values)):\n            self.values[i].delete(0, tk.END)\n\n            if self.defaults[i] is not None:\n                self.values[i].insert(0, self.defaults[i])", "category": "Python"}, {"instruction": "def proj(vec, vec_onto):\n    \"\"\" Vector projection.\n\n    Calculated as:\n\n    .. math::\n\n         \\\\mathsf{vec\\\\_onto} * \\\\frac{\\\\mathsf{vec}\\\\cdot\\\\mathsf{vec\\\\_onto}}\n         {\\\\mathsf{vec\\\\_onto}\\\\cdot\\\\mathsf{vec\\\\_onto}}\n\n    Parameters\n    ----------\n    vec\n        length-R |npfloat_| --\n        Vector to project\n\n    vec_onto\n        length-R |npfloat_| --\n        Vector onto which `vec` is to be projected\n\n    Returns\n    -------\n    proj_vec\n        length-R |npfloat_| --\n        Projection of `vec` onto `vec_onto`\n\n    \"\"\"\n", "input": "", "output": "\n    # Imports\n    import numpy as np\n\n    # Ensure vectors\n    if not len(vec.shape) == 1:\n        raise ValueError(\"'vec' is not a vector\")\n    ## end if\n    if not len(vec_onto.shape) == 1:\n        raise ValueError(\"'vec_onto' is not a vector\")\n    ## end if\n    if not vec.shape[0] == vec_onto.shape[0]:\n        raise ValueError(\"Shape mismatch between vectors\")\n    ## end if\n\n    # Calculate the projection and return\n    proj_vec = np.float_(np.asscalar(np.dot(vec.T, vec_onto))) / \\\n                np.float_(np.asscalar(np.dot(vec_onto.T, vec_onto))) * vec_onto\n    return proj_vec", "category": "Python"}, {"instruction": "def next_population(self, population, fitnesses):\n        \"\"\"Make a new population after each optimization iteration.\n\n        Args:\n            population: The population current population of solutions.\n            fitnesses: The fitness associated with each solution in the population\n        Returns:\n            list; a list of solutions.\n        \"\"\"\n", "input": "", "output": "        return common.make_population(self._population_size,\n                                      self._generate_solution)", "category": "Python"}, {"instruction": "def get_context_object_name(self, object_list):\n        \"\"\"Get the name of the item to be used in the context.\n\n        See original in ``django.views.generic.list.MultipleObjectMixin``.\n        \"\"\"\n", "input": "", "output": "        if self.context_object_name:\n            return self.context_object_name\n        elif hasattr(object_list, 'model'):\n            object_name = object_list.model._meta.object_name.lower()\n            return smart_str('{0}_list'.format(object_name))\n        else:\n            return None", "category": "Python"}, {"instruction": "def timestamp_localize(value):\n        \"\"\"\n        Save timestamp as utc\n\n        :param value: Timestamp (in UTC or with tz_info)\n        :type value: float | datetime.datetime\n        :return: Localized timestamp\n        :rtype: float\n        \"\"\"\n", "input": "", "output": "        if isinstance(value, datetime.datetime):\n            if not value.tzinfo:\n                value = pytz.UTC.localize(value)\n            else:\n                value = value.astimezone(pytz.UTC)\n            # Assumes utc (and add the microsecond part)\n            value = calendar.timegm(value.timetuple()) + \\\n                value.microsecond / 1e6\n        return value", "category": "Python"}, {"instruction": "def exec(self, args):\n        \"\"\"todo: Docstring for exec\n\n        :param args: arg description\n        :type args: type description\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "\n        logger.debug(\"install %s, location %s\", args.install, args.location)\n        location = args.location and os.path.abspath(args.location)\n\n        if location and len(args) > 1:\n            raise Exception((\"You cannot specify multiple install packages when \"\n                             \"using the --location option.\"\n                            ))\n\n        for repo in args.install:\n            self.install_repo(repo, location)", "category": "Python"}, {"instruction": "def save_gradebook(self, gradebook_form, *args, **kwargs):\n        \"\"\"Pass through to provider GradebookAdminSession.update_gradebook\"\"\"\n", "input": "", "output": "        # Implemented from kitosid template for -\n        # osid.resource.BinAdminSession.update_bin\n        if gradebook_form.is_for_update():\n            return self.update_gradebook(gradebook_form, *args, **kwargs)\n        else:\n            return self.create_gradebook(gradebook_form, *args, **kwargs)", "category": "Python"}, {"instruction": "def join_all(self, *parts):\n        \"\"\"\n        Join all parts with domain. Example domain: https://www.python.org\n\n        :param parts: Other parts, example: \"/doc\", \"/py27\"\n        :return: url\n        \"\"\"\n", "input": "", "output": "        url = util.join_all(self.domain, *parts)\n        return url", "category": "Python"}, {"instruction": "def from_json(cls, json_string=None, filename=None,\n                  encoding=\"utf-8\", errors=\"strict\", **kwargs):\n        \"\"\"\n        Transform a json object string into a Box object. If the incoming\n        json is a list, you must use BoxList.from_json.\n\n        :param json_string: string to pass to `json.loads`\n        :param filename: filename to open and pass to `json.load`\n        :param encoding: File encoding\n        :param errors: How to handle encoding errors\n        :param kwargs: parameters to pass to `Box()` or `json.loads`\n        :return: Box object from json data\n        \"\"\"\n", "input": "", "output": "        bx_args = {}\n        for arg in kwargs.copy():\n            if arg in BOX_PARAMETERS:\n                bx_args[arg] = kwargs.pop(arg)\n\n        data = _from_json(json_string, filename=filename,\n                          encoding=encoding, errors=errors, **kwargs)\n\n        if not isinstance(data, dict):\n            raise BoxError('json data not returned as a dictionary, '\n                           'but rather a {0}'.format(type(data).__name__))\n        return cls(data, **bx_args)", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"Stop/Close this session\n\n        Close the socket associated with this session and puts Session\n        into a state such that it can be re-established later.\n        \"\"\"\n", "input": "", "output": "        if self.socket is not None:\n            self.socket.close()\n            self.socket = None\n            self.data = None", "category": "Python"}, {"instruction": "def get_contract(firma, pravni_forma, sidlo, ic, dic, zastoupen):\n    \"\"\"\n    Compose contract and create PDF.\n\n    Args:\n        firma (str): firma\n        pravni_forma (str): pravni_forma\n        sidlo (str): sidlo\n        ic (str): ic\n        dic (str): dic\n        zastoupen (str): zastoupen\n\n    Returns:\n        obj: StringIO file instance containing PDF file.\n    \"\"\"\n", "input": "", "output": "    contract_fn = _resource_context(\n        \"Licencni_smlouva_o_dodavani_elektronickych_publikaci\"\n        \"_a_jejich_uziti.rst\"\n    )\n\n    # load contract\n    with open(contract_fn) as f:\n        contract = f.read()#.decode(\"utf-8\").encode(\"utf-8\")\n\n    # make sure that `firma` has its heading mark\n    firma = firma.strip()\n    firma = firma + \"\\n\" + ((len(firma) + 1) * \"-\")\n\n    # patch template\n    contract = Template(contract).substitute(\n        firma=firma,\n        pravni_forma=pravni_forma.strip(),\n        sidlo=sidlo.strip(),\n        ic=ic.strip(),\n        dic=dic.strip(),\n        zastoupen=zastoupen.strip(),\n        resources_path=RES_PATH\n    )\n\n    return gen_pdf(\n        contract,\n        open(_resource_context(\"style.json\")).read(),\n    )", "category": "Python"}, {"instruction": "def is_prefix(pre_path, path):\n    \"\"\"Return True if pre_path is a path-prefix of path.\"\"\"\n", "input": "", "output": "    pre_path = pre_path.strip('.')\n    path = path.strip('.')\n    return not pre_path or path.startswith(pre_path + '.')", "category": "Python"}, {"instruction": "def _unstack(self, unstacker_func, new_columns, n_rows, fill_value):\n        \"\"\"Return a list of unstacked blocks of self\n\n        Parameters\n        ----------\n        unstacker_func : callable\n            Partially applied unstacker.\n        new_columns : Index\n            All columns of the unstacked BlockManager.\n        n_rows : int\n            Only used in ExtensionBlock.unstack\n        fill_value : int\n            Only used in ExtensionBlock.unstack\n\n        Returns\n        -------\n        blocks : list of Block\n            New blocks of unstacked values.\n        mask : array_like of bool\n            The mask of columns of `blocks` we should keep.\n        \"\"\"\n", "input": "", "output": "        unstacker = unstacker_func(self.values.T)\n        new_items = unstacker.get_new_columns()\n        new_placement = new_columns.get_indexer(new_items)\n        new_values, mask = unstacker.get_new_values()\n\n        mask = mask.any(0)\n        new_values = new_values.T[mask]\n        new_placement = new_placement[mask]\n\n        blocks = [make_block(new_values, placement=new_placement)]\n        return blocks, mask", "category": "Python"}, {"instruction": "def parse(self, **kwargs):\n        \"\"\"Parse the contents of the output files retrieved in the `FolderData`.\"\"\"\n", "input": "", "output": "        try:\n            output_folder = self.retrieved\n        except exceptions.NotExistent:\n            return self.exit_codes.ERROR_NO_RETRIEVED_FOLDER\n\n        filename_stdout = self.node.get_attribute('output_filename')\n        filename_stderr = self.node.get_attribute('error_filename')\n\n        try:\n            with output_folder.open(filename_stderr, 'r') as handle:\n                exit_code = self.parse_stderr(handle)\n        except (OSError, IOError):\n            self.logger.exception('Failed to read the stderr file\\n%s', traceback.format_exc())\n            return self.exit_codes.ERROR_READING_ERROR_FILE\n\n        if exit_code:\n            return exit_code\n\n        try:\n            with output_folder.open(filename_stdout, 'r') as handle:\n                handle.seek(0)\n                exit_code = self.parse_stdout(handle)\n        except (OSError, IOError):\n            self.logger.exception('Failed to read the stdout file\\n%s', traceback.format_exc())\n            return self.exit_codes.ERROR_READING_OUTPUT_FILE\n\n        if exit_code:\n            return exit_code", "category": "Python"}, {"instruction": "def get_ip_on_network(self, network_name):\n        \"\"\"Given a network name, returns the IP address\n\n        :param network_name: (str) Name of the network to search for\n        :return: (str) IP address on the specified network or None\n        \"\"\"\n", "input": "", "output": "        return self.get_scenario_host_ip_on_network(\n            scenario_role_name=self.cons3rt_role_name,\n            network_name=network_name\n        )", "category": "Python"}, {"instruction": "def ReadMostRecentClientGraphSeries(\n      self,\n      client_label,\n      report_type,\n      cursor=None):\n    \"\"\"Fetches the latest graph series for a client-label from the DB.\"\"\"\n", "input": "", "output": "    query = ", "category": "Python"}, {"instruction": "def load_envars(self, items):\n    '''load a tuple of environment variables, to add to the user settings\n    \n        Example:\n \n        items = [('HELPME_DISCOURSE_BOARD', 'user_prompt_board'),\n                 ('HELPME_DISCOURSE_CATEGORY', 'user_prompt_category')]\n\n        # Note that it's added to the client with an underscore:\n        self._load_envars()\n    '''\n", "input": "", "output": "    for item in items:\n        envar = item[0]\n        key = item[1]\n        value = self._get_and_update_setting(envar)\n\n        if value != None:\n            self.data[key] = value\n            self.config.remove_option(self.name, key)", "category": "Python"}, {"instruction": "def _timestamp():\n    \"\"\"Return a timestamp with microsecond precision.\"\"\"\n", "input": "", "output": "    moment = time.time()\n    moment_us = repr(moment).split('.')[1]\n    return time.strftime(\"%Y-%m-%d-%H-%M-%S-{}\".format(moment_us), time.gmtime(moment))", "category": "Python"}, {"instruction": "def _convert_appengine_app_assertion_credentials(credentials):\n    \"\"\"Converts to :class:`google.auth.app_engine.Credentials`.\n\n    Args:\n        credentials (oauth2client.contrib.app_engine.AppAssertionCredentials):\n            The credentials to convert.\n\n    Returns:\n        google.oauth2.service_account.Credentials: The converted credentials.\n    \"\"\"\n", "input": "", "output": "    # pylint: disable=invalid-name\n    return google.auth.app_engine.Credentials(\n        scopes=_helpers.string_to_scopes(credentials.scope),\n        service_account_id=credentials.service_account_id)", "category": "Python"}, {"instruction": "def cast_callback(value):\n        \"\"\"Override `cast_callback` method.\n        \"\"\"\n", "input": "", "output": "        # Postgresql / MySQL drivers change the format on 'TIMESTAMP' columns;\n\n        if 'T' in value:\n            value = value.replace('T', ' ')\n\n        return datetime.strptime(value.split('.')[0], '%Y-%m-%d %H:%M:%S')", "category": "Python"}, {"instruction": "def get_identities(self, item):\n        \"\"\"Return the identities from an item\"\"\"\n", "input": "", "output": "\n        category = item['category']\n        item = item['data']\n\n        if category == \"issue\":\n            identity_types = ['user', 'assignee']\n        elif category == \"pull_request\":\n            identity_types = ['user', 'merged_by']\n        else:\n            identity_types = []\n\n        for identity in identity_types:\n            if item[identity]:\n                # In user_data we have the full user data\n                user = self.get_sh_identity(item[identity + \"_data\"])\n                if user:\n                    yield user", "category": "Python"}, {"instruction": "def validate(self):\n    \"\"\"Validate this measurement and update its 'outcome' field.\"\"\"\n", "input": "", "output": "    # PASS if all our validators return True, otherwise FAIL.\n    try:\n      if all(v(self.measured_value.value) for v in self.validators):\n        self.outcome = Outcome.PASS\n      else:\n        self.outcome = Outcome.FAIL\n      return self\n    except Exception as e:  # pylint: disable=bare-except\n      _LOG.error('Validation for measurement %s raised an exception %s.',\n                 self.name, e)\n      self.outcome = Outcome.FAIL\n      raise\n    finally:\n      if self._cached:\n        self._cached['outcome'] = self.outcome.name", "category": "Python"}, {"instruction": "def _datetime_to_iso8601(self, query_dict):\n        \"\"\"Encode any datetime query parameters to ISO8601.\"\"\"\n", "input": "", "output": "        return {\n            k: v if not isinstance(v, datetime.datetime) else v.isoformat()\n            for k, v in list(query_dict.items())\n        }", "category": "Python"}, {"instruction": "def set_timeout(self, visibility_timeout, callback=None):\n        \"\"\"\n        Set the visibility timeout for the queue.\n\n        :type visibility_timeout: int\n        :param visibility_timeout: The desired timeout in seconds\n        \"\"\"\n", "input": "", "output": "        def _set_timeout(retval):\n            if retval:\n              self.visibility_timeout = visibility_timeout\n            if callable(callback):\n                callback(retval)\n        retval = self.set_attribute('VisibilityTimeout', visibility_timeout, callback=_set_timeout)", "category": "Python"}, {"instruction": "def intersection(*argv):\n        \"\"\"Returns the intersection of multiple sets.\n        Items are ordered by set1, set2, ...\n\n        **\u4e2d\u6587\u6587\u6863**\n        \n        \u6c42\u591a\u4e2a\u6709\u5e8f\u96c6\u5408\u7684\u4ea4\u96c6, \u6309\u7167\u7b2c\u4e00\u4e2a\u96c6\u5408, \u7b2c\u4e8c\u4e2a, ..., \u8fd9\u6837\u7684\u987a\u5e8f\u3002\n        \"\"\"\n", "input": "", "output": "        res = OrderedSet(argv[0])\n        for ods in argv:\n            res = ods & res\n        return res", "category": "Python"}, {"instruction": "def emit(self, record):\n        # pylint: disable=W0212\n        \"\"\"\n        Handle a message logged with the logger\n\n        :param record: A log record\n        \"\"\"\n", "input": "", "output": "        # Get the bundle\n        bundle = self._bundle_from_module(record.module)\n\n        # Convert to a LogEntry\n        entry = LogEntry(\n            record.levelno, record.getMessage(), None, bundle, None\n        )\n        self._reader._store_entry(entry)", "category": "Python"}, {"instruction": "def has_data_for_dates(series_or_df, first_date, last_date):\n    \"\"\"\n    Does `series_or_df` have data on or before first_date and on or after\n    last_date?\n    \"\"\"\n", "input": "", "output": "    dts = series_or_df.index\n    if not isinstance(dts, pd.DatetimeIndex):\n        raise TypeError(\"Expected a DatetimeIndex, but got %s.\" % type(dts))\n    first, last = dts[[0, -1]]\n    return (first <= first_date) and (last >= last_date)", "category": "Python"}, {"instruction": "def Convert(self, metadata, stat_entry, token=None):\n    \"\"\"Converts StatEntry to ExportedFile.\n\n    Does nothing if StatEntry corresponds to a registry entry and not to a file.\n\n    Args:\n      metadata: ExportedMetadata to be used for conversion.\n      stat_entry: StatEntry to be converted.\n      token: Security token.\n\n    Returns:\n      List or generator with resulting RDFValues. Empty list if StatEntry\n      corresponds to a registry entry and not to a file.\n    \"\"\"\n", "input": "", "output": "    return self.BatchConvert([(metadata, stat_entry)], token=token)", "category": "Python"}, {"instruction": "def userinfo_claims(self, access_token, scope_request, claims_request):\n        \"\"\" Return the claims for the requested parameters. \"\"\"\n", "input": "", "output": "        id_token = oidc.userinfo(access_token, scope_request, claims_request)\n        return id_token.claims", "category": "Python"}, {"instruction": "def all_nodes_that_run_in_env(service, env, service_configuration=None):\n    \"\"\" Returns all nodes that run in an environment. This needs\n    to be specified in field named 'env_runs_on' one level under services\n    in the configuration, and needs to contain an object which maps strings\n    to lists (environments to nodes).\n\n    :param service: A string specifying which service to look up nodes for\n    :param env: A string specifying which environment's nodes should be returned\n    :param service_configuration: A service_configuration dict to look in or None to\n                                  use the default dict.\n\n    :returns: list of all nodes running in a certain environment\n    \"\"\"\n", "input": "", "output": "\n    if service_configuration is None:\n        service_configuration = read_services_configuration()\n    env_runs_on = service_configuration[service]['env_runs_on']\n    if env in env_runs_on:\n        return list(sorted(env_runs_on[env]))\n    else:\n        return []", "category": "Python"}, {"instruction": "def _tree_to_labels(X, single_linkage_tree, min_cluster_size=10,\n                    cluster_selection_method='eom',\n                    allow_single_cluster=False,\n                    match_reference_implementation=False):\n    \"\"\"Converts a pretrained tree and cluster size into a\n    set of labels and probabilities.\n    \"\"\"\n", "input": "", "output": "    condensed_tree = condense_tree(single_linkage_tree,\n                                   min_cluster_size)\n    stability_dict = compute_stability(condensed_tree)\n    labels, probabilities, stabilities = get_clusters(condensed_tree,\n                                                      stability_dict,\n                                                      cluster_selection_method,\n                                                      allow_single_cluster,\n                                                      match_reference_implementation)\n\n    return (labels, probabilities, stabilities, condensed_tree,\n            single_linkage_tree)", "category": "Python"}, {"instruction": "def seek(self, offset, whence=os.SEEK_SET):\n    \"\"\"Seeks to an offset within the file-like object.\n\n    Args:\n      offset (int): offset to seek to.\n      whence (Optional(int)): value that indicates whether offset is an absolute\n          or relative position within the file.\n\n    Raises:\n      IOError: if the seek failed or the file has not been opened.\n      OSError: if the seek failed or the file has not been opened.\n    \"\"\"\n", "input": "", "output": "    if not self._gzip_file_object:\n      raise IOError('Not opened.')\n\n    if whence == os.SEEK_CUR:\n      offset += self._current_offset\n    elif whence == os.SEEK_END:\n      offset += self.uncompressed_data_size\n    elif whence != os.SEEK_SET:\n      raise IOError('Unsupported whence.')\n\n    if offset < 0:\n      raise IOError('Invalid offset value less than zero.')\n\n    self._current_offset = offset", "category": "Python"}, {"instruction": "def WriteClientCrashInfo(self, client_id, crash_info, cursor=None):\n    \"\"\"Writes a new client crash record.\"\"\"\n", "input": "", "output": "    query = ", "category": "Python"}, {"instruction": "def get_flipped_ext(file_id,ccd):\n    \"\"\"Given a list of exposure numbers and CCD, get them from the DB\"\"\"\n", "input": "", "output": "    \n    import MOPfits\n    import os, shutil\n    \n    filename=MOPfits.adGet(file_id,extno=int(ccd))\n    if int(ccd)<18:\n        tfname=filename+\"F\"\n\tshutil.move(filename, tfname)\n        os.system(\"imcopy %s[-*,-*] %s\" % (tfname, filename))\n\tos.unlink(tfname)\n    if not os.access(filename,os.R_OK):\n        return(None)\n    return(filename)", "category": "Python"}, {"instruction": "def __perform_unsolicited_callbacks(self, msg):\n        \"\"\"Callbacks for which a client reference is either optional or does not apply at all\"\"\"\n", "input": "", "output": "        type_ = msg[M_TYPE]\n        payload = msg[M_PAYLOAD]\n\n        # callbacks for responses which might be unsolicited (e.g. created or deleted)\n        if type_ in _RSP_PAYLOAD_CB_MAPPING:\n            self.__fire_callback(_RSP_PAYLOAD_CB_MAPPING[type_], msg)\n\n        # Perform callbacks for feed data\n        elif type_ == E_FEEDDATA:\n            self.__simulate_feeddata(payload[P_FEED_ID], *self.__decode_data_time(payload))\n\n        # Perform callbacks for unsolicited subscriber message\n        elif type_ == E_SUBSCRIBED:\n            self.__fire_callback(_CB_SUBSCRIPTION, payload)\n\n        else:\n            logger.error('Unexpected message type for unsolicited callback %s', type_)", "category": "Python"}, {"instruction": "def _spawn(self):\n        \"\"\" Initialize the queue and the threads. \"\"\"\n", "input": "", "output": "        self.queue = Queue(maxsize=self.num_threads * 10)\n        for i in range(self.num_threads):\n            t = Thread(target=self._consume)\n            t.daemon = True\n            t.start()", "category": "Python"}, {"instruction": "def grid(self, grid):\n        \"\"\"Turns all gridlines on or off\n\n        :param bool grid: turns the gridlines on if ``True``, off if ``False``\"\"\"\n", "input": "", "output": "\n        if not isinstance(grid, bool):\n            raise TypeError(\"grid must be boolean, not '%s'\" % grid)\n        self._x_grid = self._y_grid = grid", "category": "Python"}, {"instruction": "def extract_original_links(base_url, bs4):\n    \"\"\"Extracting links that contains specific url from BeautifulSoup object\n\n    :param base_url: `str` specific url that matched with the links\n    :param bs4: `BeautifulSoup`\n    :return: `list` List of links\n    \"\"\"\n", "input": "", "output": "    valid_url = convert_invalid_url(base_url)\n\n    url = urlparse(valid_url)\n\n    base_url = '{}://{}'.format(url.scheme, url.netloc)\n\n    base_url_with_www = '{}://www.{}'.format(url.scheme, url.netloc)\n\n    links = extract_links(bs4)\n\n    result_links = [anchor for anchor in links if anchor.startswith(base_url)]\n\n    result_links_www = [anchor for anchor in links if anchor.startswith(base_url_with_www)]\n\n    return list(set(result_links + result_links_www))", "category": "Python"}, {"instruction": "def string_to_file(path, input):\n    \"\"\"\n    Write a file from a given string.\n    \"\"\"\n", "input": "", "output": "    mkdir_p(os.path.dirname(path))\n\n    with codecs.open(path, \"w+\", \"UTF-8\") as file:\n        file.write(input)", "category": "Python"}, {"instruction": "def verify_document(self, document: Document) -> bool:\n        \"\"\"\n        Check specified document\n        :param duniterpy.documents.Document document:\n        :return:\n        \"\"\"\n", "input": "", "output": "        signature = base64.b64decode(document.signatures[0])\n        prepended = signature + bytes(document.raw(), 'ascii')\n\n        try:\n            self.verify(prepended)\n            return True\n        except ValueError:\n            return False", "category": "Python"}, {"instruction": "def get_first_lang():\n    \"\"\"Get the first lang of Accept-Language Header.\n    \"\"\"\n", "input": "", "output": "    request_lang = request.headers.get('Accept-Language').split(',')\n    if request_lang:\n        lang = locale.normalize(request_lang[0]).split('.')[0]\n    else:\n        lang = False\n    return lang", "category": "Python"}, {"instruction": "def from_file(filename, file_format=\"xyz\"):\n        \"\"\"\n        Uses OpenBabel to read a molecule from a file in all supported formats.\n\n        Args:\n            filename: Filename of input file\n            file_format: String specifying any OpenBabel supported formats.\n\n        Returns:\n            BabelMolAdaptor object\n        \"\"\"\n", "input": "", "output": "        mols = list(pb.readfile(str(file_format), str(filename)))\n        return BabelMolAdaptor(mols[0].OBMol)", "category": "Python"}, {"instruction": "def process_account(self, message):\n        \"\"\" This is used for processing of account Updates. It will\n            return instances of :class:bitshares.account.AccountUpdate`\n        \"\"\"\n", "input": "", "output": "        self.on_account(AccountUpdate(message, blockchain_instance=self.blockchain))", "category": "Python"}, {"instruction": "def _get_local_groups(self):\n        '''\n        Return all known local groups to the system.\n        '''\n", "input": "", "output": "        groups = dict()\n        path = '/etc/group'\n        with salt.utils.files.fopen(path, 'r') as fp_:\n            for line in fp_:\n                line = line.strip()\n                if ':' not in line:\n                    continue\n                name, password, gid, users = line.split(':')\n                groups[name] = {\n                    'gid': gid,\n                }\n\n                if users:\n                    groups[name]['users'] = users.split(',')\n\n        return groups", "category": "Python"}, {"instruction": "def type(filename):\n    \"\"\" Returns file type given it's name. If there are several dots in filename,\n        tries each suffix. E.g. for name of \"file.so.1.2\" suffixes \"2\", \"1\", and\n        \"so\"  will be tried.\n    \"\"\"\n", "input": "", "output": "    assert isinstance(filename, basestring)\n    while 1:\n        filename, suffix = os.path.splitext (filename)\n        if not suffix: return None\n        suffix = suffix[1:]\n\n        if suffix in __suffixes_to_types:\n            return __suffixes_to_types[suffix]", "category": "Python"}, {"instruction": "def buy(self, index):\n        \"\"\" Attempts to buy indexed item, returns result\n           \n        Parameters:\n           index (int) -- The item index\n           \n        Returns\n           bool - True if item was bought, false otherwise\n        \"\"\"\n", "input": "", "output": "        item = self.items[index]\n        us = UserShopFront(self.usr, item.owner, item.id, str(item.price))\n        us.load()\n        \n        if not item.name in us.inventory:\n            return False\n        \n        if not us.inventory[item.name].buy():\n            return False\n                \n        return True", "category": "Python"}, {"instruction": "def state_machine_for(self, process_name):\n        \"\"\" :return: state machine for the given process name \"\"\"\n", "input": "", "output": "        process_entry = self.managed_handlers[process_name].process_entry\n        return self.timetable.state_machines[process_entry.state_machine_name]", "category": "Python"}, {"instruction": "def get_vib_energies(self):\n        \"\"\"Returns a list of vibration in energy units eV.\n\n        Returns\n        -------\n        vibs : list of vibrations in eV\n        \"\"\"\n", "input": "", "output": "        vibs = self.molecule_dict[self.name]['vibrations']\n        vibs = np.array(vibs) * cm2ev\n        return (vibs)", "category": "Python"}, {"instruction": "def is_serializable_type(type_):\n    \"\"\"Return `True` if the given type's instances conform to the Serializable protocol.\n\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "    if not inspect.isclass(type_):\n      return Serializable.is_serializable(type_)\n    return issubclass(type_, Serializable) or hasattr(type_, '_asdict')", "category": "Python"}, {"instruction": "def set_metadata_value(self, key: str, value: typing.Any) -> None:\n        \"\"\"Set the metadata value for the given key.\n\n        There are a set of predefined keys that, when used, will be type checked and be interoperable with other\n        applications. Please consult reference documentation for valid keys.\n\n        If using a custom key, we recommend structuring your keys in the '<group>.<attribute>' format followed\n        by the predefined keys. e.g. 'session.instrument' or 'camera.binning'.\n\n        Also note that some predefined keys map to the metadata ``dict`` but others do not. For this reason, prefer\n        using the ``metadata_value`` methods over directly accessing ``metadata``.\n\n        .. versionadded:: 1.0\n\n        Scriptable: Yes\n        \"\"\"\n", "input": "", "output": "        self._data_item.set_metadata_value(key, value)", "category": "Python"}, {"instruction": "def _run(self):  # pylint: disable=method-hidden\n        \"\"\" Runnable main method, perform wait on long-running subtasks \"\"\"\n", "input": "", "output": "        try:\n            self.event_stop.wait()\n        except gevent.GreenletExit:  # killed without exception\n            self.event_stop.set()\n            gevent.killall(self.greenlets)  # kill children\n            raise  # re-raise to keep killed status\n        except Exception:\n            self.stop()  # ensure cleanup and wait on subtasks\n            raise", "category": "Python"}, {"instruction": "def _compute_mixing_probabilities(self):\n        \"\"\"\n        Compute the mixing probability for each filter.\n        \"\"\"\n", "input": "", "output": "\n        self.cbar = dot(self.mu, self.M)\n        for i in range(self.N):\n            for j in range(self.N):\n                self.omega[i, j] = (self.M[i, j]*self.mu[i]) / self.cbar[j]", "category": "Python"}, {"instruction": "def GenerateDSW(dswfile, source, env):\n    \"\"\"Generates a Solution/Workspace file based on the version of MSVS that is being used\"\"\"\n", "input": "", "output": "\n    version_num = 6.0\n    if 'MSVS_VERSION' in env:\n        version_num, suite = msvs_parse_version(env['MSVS_VERSION'])\n    if version_num >= 7.0:\n        g = _GenerateV7DSW(dswfile, source, env)\n        g.Build()\n    else:\n        g = _GenerateV6DSW(dswfile, source, env)\n        g.Build()", "category": "Python"}, {"instruction": "def _get_consumed(self, time):\n        \"\"\" How many consumables were (or will be) used by resource until given time. \"\"\"\n", "input": "", "output": "        minutes_from_last_update = self._get_minutes_from_last_update(time)\n        if minutes_from_last_update < 0:\n            raise ConsumptionDetailCalculateError('Cannot calculate consumption if time < last modification date.')\n        _consumed = {}\n        for consumable_item in set(list(self.configuration.keys()) + list(self.consumed_before_update.keys())):\n            after_update = self.configuration.get(consumable_item, 0) * minutes_from_last_update\n            before_update = self.consumed_before_update.get(consumable_item, 0)\n            _consumed[consumable_item] = after_update + before_update\n        return _consumed", "category": "Python"}, {"instruction": "def delete(self, item):\n        \"\"\"Deletes the specified item.\"\"\"\n", "input": "", "output": "        uri = \"/%s/%s\" % (self.uri_base, utils.get_id(item))\n        return self._delete(uri)", "category": "Python"}, {"instruction": "def dialing_permissions(self):\n        \"\"\"\n        :rtype: twilio.rest.voice.v1.dialing_permissions.DialingPermissionsList\n        \"\"\"\n", "input": "", "output": "        if self._dialing_permissions is None:\n            self._dialing_permissions = DialingPermissionsList(self)\n        return self._dialing_permissions", "category": "Python"}, {"instruction": "def lock_packages(self, deps_file_path=None, depslock_file_path=None, packages=None):\n        \"\"\"\n        Lock packages. Downloader search packages\n        \"\"\"\n", "input": "", "output": "        if deps_file_path is None:\n            deps_file_path = self._deps_path\n        if depslock_file_path is None:\n            depslock_file_path = self._depslock_path\n        if deps_file_path == depslock_file_path:\n            depslock_file_path += '.lock'\n            # raise CrosspmException(\n            #     CROSSPM_ERRORCODE_WRONG_ARGS,\n            #     'Dependencies and Lock files are same: \"{}\".'.format(deps_file_path),\n            # )\n\n        if packages is None:\n            self.search_dependencies(deps_file_path)\n        else:\n            self._root_package.packages = packages\n\n        self._log.info('Writing lock file [{}]'.format(depslock_file_path))\n\n        output_params = {\n            'out_format': 'lock',\n            'output': depslock_file_path,\n        }\n        Output(config=self._config).write_output(output_params, self._root_package.packages)\n        self._log.info('Done!')", "category": "Python"}, {"instruction": "def toString(self):\n        \"\"\"\n            Return a printable view of the dictionary\n        \"\"\"\n", "input": "", "output": "        result = []\n        k, v = self.optimalRepr()\n        longest = reduce(lambda x, y: x if x > len(y) else len(y), k, 0)\n        for ind in range(len(k)):\n            result.append(\"%s : %s\" % (k[ind].ljust(longest), v[ind]))\n        return \"\\n\".join(result)", "category": "Python"}, {"instruction": "def set_grade(self, grade_id):\n        \"\"\"Sets the grade.\n\n        arg:    grade_id (osid.id.Id): the new grade\n        raise:  InvalidArgument - ``grade_id`` is invalid or\n                ``GradebookColumn.getGradeSystem().isBasedOnGrades()``\n                is ``false``\n        raise:  NoAccess - ``grade_id`` cannot be modified\n        raise:  NullArgument - ``grade_id`` is ``null``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        if not self._grade_system.is_based_on_grades():\n            raise errors.InvalidArgument()\n        if self.get_grade_metadata().is_read_only():\n            raise errors.NoAccess()\n        if not self._is_valid_id(grade_id):\n            raise errors.InvalidArgument()\n        if not self._is_in_set(grade_id, self.get_grade_metadata()):\n            raise errors.InvalidArgument('Grade ID not in the acceptable set.')\n        self._my_map['gradeId'] = str(grade_id)\n        self._my_map['gradingAgentId'] = str(self._effective_agent_id)\n        self._my_map['timeGraded'] = DateTime.utcnow()", "category": "Python"}, {"instruction": "def cursor(self, autocommit=True):\n        '''\n        When a connection exits the with block,\n            - the tx is committed if no errors were encountered\n            - the tx is rolled back if errors\n\n        When a cursor exits its with block it is closed, without affecting\n        the state of the transaction.\n\n        http://initd.org/psycopg/docs/usage.html\n\n        '''\n", "input": "", "output": "\n        with self.connection(autocommit) as conn:\n            with conn.cursor() as cursor:\n                yield cursor", "category": "Python"}, {"instruction": "def timeout_exponential_backoff(\n        retries: int,\n        timeout: int,\n        maximum: int,\n) -> Iterator[int]:\n    \"\"\" Timeouts generator with an exponential backoff strategy.\n\n    Timeouts start spaced by `timeout`, after `retries` exponentially increase\n    the retry delays until `maximum`, then maximum is returned indefinitely.\n    \"\"\"\n", "input": "", "output": "    yield timeout\n\n    tries = 1\n    while tries < retries:\n        tries += 1\n        yield timeout\n\n    while timeout < maximum:\n        timeout = min(timeout * 2, maximum)\n        yield timeout\n\n    while True:\n        yield maximum", "category": "Python"}, {"instruction": "def get_parties(self, obj):\n        \"\"\"All parties.\"\"\"\n", "input": "", "output": "        return PartySerializer(Party.objects.all(), many=True).data", "category": "Python"}, {"instruction": "def adet(z, x):\n  \"\"\"d|A|/dA = adj(A).T\n\n  See  Jacobi's formula: https://en.wikipedia.org/wiki/Jacobi%27s_formula\n  \"\"\"\n", "input": "", "output": "  adjugate = numpy.linalg.det(x) * numpy.linalg.pinv(x)\n  d[x] = d[z] * numpy.transpose(adjugate)", "category": "Python"}, {"instruction": "async def rows(self, offs, size=None, iden=None):\n        '''\n        Yield a number of raw items from the CryoTank starting at a given offset.\n\n        Args:\n            offs (int): The index of the desired datum (starts at 0)\n            size (int): The max number of items to yield.\n\n        Yields:\n            ((indx, bytes)): Index and msgpacked bytes.\n        '''\n", "input": "", "output": "        if iden is not None:\n            self.setOffset(iden, offs)\n\n        for i, (indx, byts) in enumerate(self._items.rows(offs)):\n\n            if size is not None and i >= size:\n                return\n\n            yield indx, byts", "category": "Python"}, {"instruction": "def IncrementCounter(self, metric_name, delta=1, fields=None):\n    \"\"\"See base class.\"\"\"\n", "input": "", "output": "    if delta < 0:\n      raise ValueError(\"Invalid increment for counter: %d.\" % delta)\n    self._counter_metrics[metric_name].Increment(delta, fields)", "category": "Python"}, {"instruction": "def normalize_release_properties(ensembl_release, species):\n    \"\"\"\n    Make sure a given release is valid, normalize it to be an integer,\n    normalize the species name, and get its associated reference.\n    \"\"\"\n", "input": "", "output": "    ensembl_release = check_release_number(ensembl_release)\n    if not isinstance(species, Species):\n        species = find_species_by_name(species)\n    reference_name = species.which_reference(ensembl_release)\n    return ensembl_release, species.latin_name, reference_name", "category": "Python"}, {"instruction": "def _init_io(self):\n        \"\"\"!\n        GPIO initialization.\n        Set GPIO into BCM mode and init other IOs mode\n        \"\"\"\n", "input": "", "output": "        GPIO.setwarnings(False)\n        GPIO.setmode( GPIO.BCM )\n        pins = [ self._spi_dc ]\n        for pin in pins:\n            GPIO.setup( pin, GPIO.OUT )", "category": "Python"}, {"instruction": "def write(self, feature, value, *args, **kwargs):\n        \"\"\"Same as  :py:meth:`~chemlab.io.iohandler.IOHandler.read`. You have to pass\n        also a *value* to write and you may pass any additional \n        arguments.\n        \n        **Example**\n        \n        ::\n        \n            class XyzIO(IOHandler):\n                can_write = ['molecule']\n        \n                def write(self, feature, value, *args, **kwargs):\n                    self.check_feature(feature, \"write\")\n                    if feature == 'molecule':\n                       # Do stuff\n                       return geom\n        \n        \"\"\"\n", "input": "", "output": "        if 'w' not in self.fd.mode and 'x' not in self.fd.mode:\n            raise Exception(\"The file is not opened in writing mode. If you're using datafile, add the 'w' option.\\ndatafile(filename, 'w')\")\n        \n        self.check_feature(feature, \"write\")", "category": "Python"}, {"instruction": "def get_data_point(self, n):\n        \"\"\"\n        Returns the n'th data point (starting at 0) from all columns.\n\n        Parameters\n        ----------\n        n       \n            Index of data point to return.\n        \"\"\"\n", "input": "", "output": "        # loop over the columns and pop the data\n        point = []\n        for k in self.ckeys: point.append(self[k][n])\n        return point", "category": "Python"}, {"instruction": "def write_warning (self, url_data):\n        \"\"\"Write url_data.warning.\"\"\"\n", "input": "", "output": "        self.write(self.part(\"warning\") + self.spaces(\"warning\"))\n        warning_msgs = [u\"[%s] %s\" % x for x in url_data.warnings]\n        self.writeln(self.wrap(warning_msgs, 65), color=self.colorwarning)", "category": "Python"}, {"instruction": "def is40(msg):\n    \"\"\"Check if a message is likely to be BDS code 4,0\n\n    Args:\n        msg (String): 28 bytes hexadecimal message string\n\n    Returns:\n        bool: True or False\n    \"\"\"\n", "input": "", "output": "\n    if allzeros(msg):\n        return False\n\n    d = hex2bin(data(msg))\n\n    # status bit 1, 14, and 27\n\n    if wrongstatus(d, 1, 2, 13):\n        return False\n\n    if wrongstatus(d, 14, 15, 26):\n        return False\n\n    if wrongstatus(d, 27, 28, 39):\n        return False\n\n    if wrongstatus(d, 48, 49, 51):\n        return False\n\n    if wrongstatus(d, 54, 55, 56):\n        return False\n\n    # bits 40-47 and 52-53 shall all be zero\n\n    if bin2int(d[39:47]) != 0:\n        return False\n\n    if bin2int(d[51:53]) != 0:\n        return False\n\n    return True", "category": "Python"}, {"instruction": "def update_progress(cls, progress, starttime):\n        \"\"\" Display an ascii progress bar while processing operation. \"\"\"\n", "input": "", "output": "        width, _height = click.get_terminal_size()\n        if not width:\n            return\n\n        duration = datetime.utcnow() - starttime\n        hours, remainder = divmod(duration.seconds, 3600)\n        minutes, seconds = divmod(remainder, 60)\n\n        size = int(width * .6)\n        status = \"\"\n        if isinstance(progress, int):\n            progress = float(progress)\n        if not isinstance(progress, float):\n            progress = 0\n            status = 'error: progress var must be float\\n'\n            cls.echo(type(progress))\n        if progress < 0:\n            progress = 0\n            status = 'Halt...\\n'\n        if progress >= 1:\n            progress = 1\n            # status = 'Done...\\n'\n        block = int(round(size * progress))\n        text = ('\\rProgress: [{0}] {1:.2%} {2} {3:0>2}:{4:0>2}:{5:0>2}  '\n                ''.format('#' * block + '-' * (size - block), progress,\n                          status, hours, minutes, seconds))\n        sys.stdout.write(text)\n        sys.stdout.flush()", "category": "Python"}, {"instruction": "def get_int(self):\n        \"\"\"Read the next token and interpret it as an integer.\n\n        @raises dns.exception.SyntaxError:\n        @rtype: int\n        \"\"\"\n", "input": "", "output": "\n        token = self.get().unescape()\n        if not token.is_identifier():\n            raise dns.exception.SyntaxError('expecting an identifier')\n        if not token.value.isdigit():\n            raise dns.exception.SyntaxError('expecting an integer')\n        return int(token.value)", "category": "Python"}, {"instruction": "def channels_unarchive(self, room_id, **kwargs):\n        \"\"\"Unarchives a channel.\"\"\"\n", "input": "", "output": "        return self.__call_api_post('channels.unarchive', roomId=room_id, kwargs=kwargs)", "category": "Python"}, {"instruction": "def fix_ilx(self, ilx_id: str) -> str:\n        ''' Database only excepts lower case and underscore version of ID '''\n", "input": "", "output": "        ilx_id = ilx_id.replace('http://uri.interlex.org/base/', '')\n        if ilx_id[:4] not in ['TMP:', 'tmp_', 'ILX:', 'ilx_']:\n            raise ValueError(\n                'Need to provide ilx ID with format ilx_# or ILX:# for given ID ' + ilx_id)\n        return ilx_id.replace('ILX:', 'ilx_').replace('TMP:', 'tmp_')", "category": "Python"}, {"instruction": "def parse_endpoint(endpoint, identifier_type=None):\n        \"\"\"\n        Convert an endpoint name into an (operation, ns) tuple.\n\n        \"\"\"\n", "input": "", "output": "        # compute the operation\n        parts = endpoint.split(\".\")\n        operation = Operation.from_name(parts[1])\n\n        # extract its parts\n        matcher = match(operation.endpoint_pattern, endpoint)\n        if not matcher:\n            raise InternalServerError(\"Malformed operation endpoint: {}\".format(endpoint))\n        kwargs = matcher.groupdict()\n        del kwargs[\"operation\"]\n        if identifier_type is not None:\n            kwargs[\"identifier_type\"] = identifier_type\n        return operation, Namespace(**kwargs)", "category": "Python"}, {"instruction": "def get_user_orders(self, id, **data):\n        \"\"\"\n        GET /users/:id/orders/\n        Returns a :ref:`paginated <pagination>` response of :format:`orders <order>`, under the key ``orders``, of all orders the user has placed (i.e. where the user was the person buying the tickets).\n        :param int id: The id assigned to a user.\n        :param datetime changed_since: (optional) Only return attendees changed on or after the time given.\n        .. note:: A datetime represented as a string in ISO8601 combined date and time format, always in UTC.\n        \"\"\"\n", "input": "", "output": "        \n        return self.get(\"/users/{0}/orders/\".format(id), data=data)", "category": "Python"}, {"instruction": "def merge_pmag_recs(self, old_recs):\n        \"\"\"\n        Takes in a list of dictionaries old_recs and returns a list of\n        dictionaries where every dictionary in the returned list has the\n        same keys as all the others.\n\n        Parameters\n        ----------\n        old_recs : list of dictionaries to fix\n\n        Returns\n        -------\n        recs : list of dictionaries with same keys\n        \"\"\"\n", "input": "", "output": "        recs = {}\n        recs = deepcopy(old_recs)\n        headers = []\n        for rec in recs:\n            for key in list(rec.keys()):\n                if key not in headers:\n                    headers.append(key)\n        for rec in recs:\n            for header in headers:\n                if header not in list(rec.keys()):\n                    rec[header] = \"\"\n        return recs", "category": "Python"}, {"instruction": "def _compute_page_url(self):\n        \"\"\"Compute the page url.\"\"\"\n", "input": "", "output": "        for page in self:\n            base_url = self.env['ir.config_parameter'].sudo().get_param(\n                'web.base.url',\n                default='http://localhost:8069'\n            )\n\n            page.page_url = (\n                '{}/web#db={}&id={}&view_type=form&'\n                'model=document.page.history').format(\n                    base_url,\n                    self.env.cr.dbname,\n                    page.id\n                )", "category": "Python"}, {"instruction": "def expect_and_reraise_internal_error(modID='SDK'):\n    \"\"\"Catch all kinds of zvm client request failure and reraise.\n\n    modID: the moduleID that the internal error happens in.\n    \"\"\"\n", "input": "", "output": "    try:\n        yield\n    except exception.SDKInternalError as err:\n        msg = err.format_message()\n        raise exception.SDKInternalError(msg, modID=modID)", "category": "Python"}, {"instruction": "async def expire(self, name, time):\n        \"\"\"\n        Set an expire flag on key ``name`` for ``time`` seconds. ``time``\n        can be represented by an integer or a Python timedelta object.\n        \"\"\"\n", "input": "", "output": "        if isinstance(time, datetime.timedelta):\n            time = time.seconds + time.days * 24 * 3600\n        return await self.execute_command('EXPIRE', name, time)", "category": "Python"}, {"instruction": "def set_since_id(self, twid):\n        \"\"\" Sets 'since_id' parameter used to return only results \\\n        with an ID greater than (that is, more recent than) the specified ID\n\n        :param twid: A valid tweet ID in either long (Py2k) \\\n        or integer (Py2k + Py3k) format\n        :raises: TwitterSearchException\n        \"\"\"\n", "input": "", "output": "\n        if py3k:\n            if not isinstance(twid, int):\n                raise TwitterSearchException(1004)\n        else:\n            if not isinstance(twid, (int, long)):\n                raise TwitterSearchException(1004)\n\n        if twid > 0:\n            self.arguments.update({'since_id': '%s' % twid})\n        else:\n            raise TwitterSearchException(1004)", "category": "Python"}, {"instruction": "def decimal_year(year, month, day):\n    \"\"\"\n    Allows to calculate the decimal year for a vector of dates\n    (TODO this is legacy code kept to maintain comparability with previous\n    declustering algorithms!)\n\n    :param year: year column from catalogue matrix\n    :type year: numpy.ndarray\n    :param month: month column from catalogue matrix\n    :type month: numpy.ndarray\n    :param day: day column from catalogue matrix\n    :type day: numpy.ndarray\n    :returns: decimal year column\n    :rtype: numpy.ndarray\n    \"\"\"\n", "input": "", "output": "    marker = np.array([0., 31., 59., 90., 120., 151., 181.,\n                       212., 243., 273., 304., 334.])\n    tmonth = (month - 1).astype(int)\n    day_count = marker[tmonth] + day - 1.\n    dec_year = year + (day_count / 365.)\n\n    return dec_year", "category": "Python"}, {"instruction": "def items(self, raw = False):\n        \"\"\"Like `items` for dicts but with a `raw` option\n\n        # Parameters\n\n        _raw_ : `optional [bool]`\n\n        > Default `False`, if `True` the `KeysView` contains the raw values as the values\n\n        # Returns\n\n        `KeysView`\n\n        > The key-value pairs of the record\n        \"\"\"\n", "input": "", "output": "        if raw:\n            return self._fieldDict.items()\n        else:\n            return collections.abc.Mapping.items(self)", "category": "Python"}, {"instruction": "def ExportModelOperationsMixin(model_name):\n    \"\"\"Returns a mixin for models to export counters for lifecycle operations.\n\n    Usage:\n      class User(ExportModelOperationsMixin('user'), Model):\n          ...\n    \"\"\"\n", "input": "", "output": "    # Force create the labels for this model in the counters. This\n    # is not necessary but it avoids gaps in the aggregated data.\n    model_inserts.labels(model_name)\n    model_updates.labels(model_name)\n    model_deletes.labels(model_name)\n\n    class Mixin(object):\n        def _do_insert(self, *args, **kwargs):\n            model_inserts.labels(model_name).inc()\n            return super(Mixin, self)._do_insert(*args, **kwargs)\n\n        def _do_update(self, *args, **kwargs):\n            model_updates.labels(model_name).inc()\n            return super(Mixin, self)._do_update(*args, **kwargs)\n\n        def delete(self, *args, **kwargs):\n            model_deletes.labels(model_name).inc()\n            return super(Mixin, self).delete(*args, **kwargs)\n    return Mixin", "category": "Python"}, {"instruction": "def page(self, course, msg=\"\", error=False):\n        \"\"\" Get all data and display the page \"\"\"\n", "input": "", "output": "        thehash = hashlib.sha512(str(random.getrandbits(256)).encode(\"utf-8\")).hexdigest()\n        self.user_manager.set_session_token(thehash)\n\n        backups = self.get_backup_list(course)\n\n        return self.template_helper.get_renderer().course_admin.danger_zone(course, thehash, backups, msg, error)", "category": "Python"}, {"instruction": "def checkpoint(self, interval):\n        \"\"\"\n        Enable periodic checkpointing of RDDs of this DStream\n\n        @param interval: time in seconds, after each period of that, generated\n                         RDD will be checkpointed\n        \"\"\"\n", "input": "", "output": "        self.is_checkpointed = True\n        self._jdstream.checkpoint(self._ssc._jduration(interval))\n        return self", "category": "Python"}, {"instruction": "def getoptT(X, W, Y, Z, S, M_E, E, m0, rho):\r\n    ''' Perform line search\r\n    '''\n", "input": "", "output": "    iter_max = 20\r\n    norm2WZ = np.linalg.norm(W, ord='fro')**2 + np.linalg.norm(Z, ord='fro')**2\r\n    f = np.zeros(iter_max + 1)\r\n    f[0] = F_t(X, Y, S, M_E, E, m0, rho)\r\n\r\n    t = -1e-1\r\n    for i in range(iter_max):\r\n        f[i + 1] = F_t(X + t * W, Y + t * Z, S, M_E, E, m0, rho)\r\n\r\n        if f[i + 1] - f[0] <= 0.5 * t * norm2WZ:\r\n            return t\r\n        t /= 2\r\n    return t", "category": "Python"}, {"instruction": "def compute_territory(self):\n        \"\"\" Compute territory reachable by the piece from its current position.\n\n        Returns a list of boolean flags of squares indexed linearly, for which\n        a True means the square is reachable.\n        \"\"\"\n", "input": "", "output": "        # Initialize the square occupancy vector of the board.\n        vector = self.board.new_vector()\n\n        # Mark current position as reachable.\n        vector[self.index] = True\n\n        # List all places reacheable by the piece from its current position.\n        for x_shift, y_shift in self.movements:\n            # Mark side positions as reachable if in the limit of the board.\n            try:\n                reachable_index = self.board.coordinates_to_index(\n                    self.x, self.y, x_shift, y_shift)\n            except ForbiddenCoordinates:\n                continue\n            vector[reachable_index] = True\n\n        return vector", "category": "Python"}, {"instruction": "def confusion_matrix(self):\n        \"\"\"\n        Returns the normalised confusion matrix\n        \"\"\"\n", "input": "", "output": "        confusion_matrix = self.pixel_classification_sum.astype(np.float)\n        confusion_matrix = np.divide(confusion_matrix.T, self.pixel_truth_sum.T).T\n\n        return confusion_matrix * 100.0", "category": "Python"}, {"instruction": "def rm(self, path):\n        \"\"\"\n        Delete the path and anything under the path.\n\n        Example\n        -------\n            >>> s3utils.rm(\"path/to/file_or_folder\")\n        \"\"\"\n", "input": "", "output": "\n        list_of_files = list(self.ls(path))\n\n        if list_of_files:\n            if len(list_of_files) == 1:\n                self.bucket.delete_key(list_of_files[0])\n            else:\n                self.bucket.delete_keys(list_of_files)\n            self.printv(\"Deleted: %s\" % list_of_files)\n        else:\n            logger.error(\"There was nothing to remove under %s\", path)", "category": "Python"}, {"instruction": "def get_suitable_app(cls, given_app):\n        u\"\"\"\n        Return a clone of given_app with ChordableDjangoBackend, if needed.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(getattr(given_app, 'backend', None), ChordableDjangoBackend):\n            return_app = deepcopy(given_app)\n            return_app.backend = ChordableDjangoBackend(return_app)\n            return return_app\n        else:\n            return given_app", "category": "Python"}, {"instruction": "def dumpJSON(self):\n        \"\"\"\n        Return dictionary of data for FITS headers.\n        \"\"\"\n", "input": "", "output": "        g = get_root(self).globals\n        return dict(\n            RA=self.ra['text'],\n            DEC=self.dec['text'],\n            tel=g.cpars['telins_name'],\n            alt=self._getVal(self.alt),\n            az=self._getVal(self.az),\n            secz=self._getVal(self.airmass),\n            pa=self._getVal(self.pa),\n            foc=self._getVal(self.focus),\n            mdist=self._getVal(self.mdist)\n        )", "category": "Python"}, {"instruction": "def save(self):\n        \"\"\"Serialize this mesh to a string appropriate for disk storage\n\n        Returns\n        -------\n        state : dict\n            The state.\n        \"\"\"\n", "input": "", "output": "        import pickle\n        if self._faces is not None:\n            names = ['_vertices', '_faces']\n        else:\n            names = ['_vertices_indexed_by_faces']\n\n        if self._vertex_colors is not None:\n            names.append('_vertex_colors')\n        elif self._vertex_colors_indexed_by_faces is not None:\n            names.append('_vertex_colors_indexed_by_faces')\n\n        if self._face_colors is not None:\n            names.append('_face_colors')\n        elif self._face_colors_indexed_by_faces is not None:\n            names.append('_face_colors_indexed_by_faces')\n\n        state = dict([(n, getattr(self, n)) for n in names])\n        return pickle.dumps(state)", "category": "Python"}, {"instruction": "def connect_channels(self, channels):\n        \"\"\"Connect the provided channels\"\"\"\n", "input": "", "output": "        self.log.info(f\"Connecting to channels...\")\n        for chan in channels:\n            chan.connect(self.sock)\n            self.log.info(f\"\\t{chan.channel}\")", "category": "Python"}, {"instruction": "def editColor( self, item, index ):\n        \"\"\"\n        Prompts the user to pick a new color for the inputed item/column.\n        \n        :param      item  | <XColorTreeWidgetItem>\n                    index | <int>\n        \"\"\"\n", "input": "", "output": "        if ( not index ):\n            return\n            \n        newcolor = QColorDialog.getColor(item.colorAt(index-1), self)\n        \n        if ( not newcolor.isValid() ):\n            return\n            \n        endIndex = index + 1\n        if ( self.propogateRight() ):\n            endIndex = self.columnCount()\n        \n        for i in range(index, endIndex):\n            item.setColorAt(i-1, newcolor)", "category": "Python"}, {"instruction": "def htmlReadFile(filename, encoding, options):\n    \"\"\"parse an XML file from the filesystem or the network. \"\"\"\n", "input": "", "output": "    ret = libxml2mod.htmlReadFile(filename, encoding, options)\n    if ret is None:raise treeError('htmlReadFile() failed')\n    return xmlDoc(_obj=ret)", "category": "Python"}, {"instruction": "def looks_like_python(name):\n    # type: (str) -> bool\n    \"\"\"\n    Determine whether the supplied filename looks like a possible name of python.\n\n    :param str name: The name of the provided file.\n    :return: Whether the provided name looks like python.\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "\n    if not any(name.lower().startswith(py_name) for py_name in PYTHON_IMPLEMENTATIONS):\n        return False\n    match = RE_MATCHER.match(name)\n    if match:\n        return any(fnmatch(name, rule) for rule in MATCH_RULES)\n    return False", "category": "Python"}, {"instruction": "def write(fname, data):\n\t\t\"\"\"\n\t\tWrites a Json file\n\n\t\tin: fname - file name\n\t\t\tdata - dictionary of data to put into the file\n\n\t\tout: nothing, everything is written to a file\n\t\t\"\"\"\n", "input": "", "output": "\t\ttry:\n\t\t\twith open(fname, 'w') as f:\n\t\t\t\tjson.dump(data, f)\n\n\t\texcept IOError:\n\t\t\traise Exception('Could not open {0!s} for writing'.format((fname)))", "category": "Python"}, {"instruction": "def requestSchema(self, nym, name, version, sender):\n        \"\"\"\n        Used to get a schema from Sovrin\n        :param nym: nym that schema is attached to\n        :param name: name of schema\n        :param version: version of schema\n        :return: req object\n        \"\"\"\n", "input": "", "output": "        operation = { TARGET_NYM: nym,\n                      TXN_TYPE: GET_SCHEMA,\n                      DATA: {NAME : name,\n                             VERSION: version}\n        }\n\n        req = Request(sender, operation=operation)\n        return self.prepReq(req)", "category": "Python"}, {"instruction": "def normaliseWV(wV, normFac=1.0):\n    \"\"\"\n    make char probs divisible by one\n    \"\"\"\n", "input": "", "output": "    f = sum(wV) / normFac\n    return [ i/f for i in wV ]", "category": "Python"}, {"instruction": "async def redis(self) -> aioredis.Redis:\n        \"\"\"\n        Get Redis connection\n\n        This property is awaitable.\n        \"\"\"\n", "input": "", "output": "        # Use thread-safe asyncio Lock because this method without that is not safe\n        async with self._connection_lock:\n            if self._redis is None:\n                self._redis = await aioredis.create_redis_pool((self._host, self._port),\n                                                               db=self._db, password=self._password, ssl=self._ssl,\n                                                               minsize=1, maxsize=self._pool_size,\n                                                               loop=self._loop, **self._kwargs)\n        return self._redis", "category": "Python"}, {"instruction": "def elements(self) -> Iterator[devicetools.Element]:\n        \"\"\"Yield all |Element| objects returned by |XMLInterface.selections|\n        and |XMLInterface.devices| without duplicates.\n\n        >>> from hydpy.core.examples import prepare_full_example_1\n        >>> prepare_full_example_1()\n\n        >>> from hydpy import HydPy, TestIO, XMLInterface\n        >>> hp = HydPy('LahnH')\n        >>> with TestIO():\n        ...     hp.prepare_network()\n        ...     interface = XMLInterface('single_run.xml')\n        >>> interface.find('selections').text = 'headwaters streams'\n        >>> for element in interface.elements:\n        ...      print(element.name)\n        land_dill\n        land_lahn_1\n        stream_dill_lahn_2\n        stream_lahn_1_lahn_2\n        stream_lahn_2_lahn_3\n        \"\"\"\n", "input": "", "output": "        selections = copy.copy(self.selections)\n        selections += self.devices\n        elements = set()\n        for selection in selections:\n            for element in selection.elements:\n                if element not in elements:\n                    elements.add(element)\n                    yield element", "category": "Python"}, {"instruction": "def wrap_inference_results(inference_result_proto):\n  \"\"\"Returns packaged inference results from the provided proto.\n\n  Args:\n    inference_result_proto: The classification or regression response proto.\n\n  Returns:\n    An InferenceResult proto with the result from the response.\n  \"\"\"\n", "input": "", "output": "  inference_proto = inference_pb2.InferenceResult()\n  if isinstance(inference_result_proto,\n                classification_pb2.ClassificationResponse):\n    inference_proto.classification_result.CopyFrom(\n        inference_result_proto.result)\n  elif isinstance(inference_result_proto, regression_pb2.RegressionResponse):\n    inference_proto.regression_result.CopyFrom(inference_result_proto.result)\n  return inference_proto", "category": "Python"}, {"instruction": "def rate_limit(limit: int, key=None):\n    \"\"\"\n    Decorator for configuring rate limit and key in different functions.\n\n    :param limit:\n    :param key:\n    :return:\n    \"\"\"\n", "input": "", "output": "\n    def decorator(func):\n        setattr(func, 'throttling_rate_limit', limit)\n        if key:\n            setattr(func, 'throttling_key', key)\n        return func\n\n    return decorator", "category": "Python"}, {"instruction": "def destroy(self):\n        \"\"\" Destroy the underlying QWidget object.\n\n        \"\"\"\n", "input": "", "output": "        self._teardown_features()\n        focus_registry.unregister(self.widget)\n        widget = self.widget\n        if widget is not None:\n            del self.widget\n        super(QtGraphicsItem, self).destroy()\n        # If a QWidgetAction was created for this widget, then it has\n        # taken ownership of the widget and the widget will be deleted\n        # when the QWidgetAction is garbage collected. This means the\n        # superclass destroy() method must run before the reference to\n        # the QWidgetAction is dropped.\n        del self._widget_action", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\" For each state in states file build url and download file \"\"\"\n", "input": "", "output": "        states = open(self.states, 'r').read().splitlines()\n        for state in states:\n            url = self.build_url(state)\n            log = \"Downloading State < {0} > from < {1} >\"\n            logging.info(log.format(state, url))\n            tmp = self.download(self.output, url, self.overwrite)\n            self.s3.store(self.extract(tmp, self.tmp2poi(tmp)))", "category": "Python"}, {"instruction": "def get_comparable_values(self):\n        \"\"\"Return a tupple of values representing the unicity of the object\n        \"\"\"\n", "input": "", "output": "        return (str(self.name), str(self.description), str(self.type), bool(self.optional), str(self.constraints) if isinstance(self, Constraintable) else \"\")", "category": "Python"}, {"instruction": "def cmd_fw_download(self, args):\n        '''cmd handler for downloading firmware'''\n", "input": "", "output": "        stuff = self.filtered_rows_from_args(args)\n        if stuff is None:\n            return\n        (filtered, remainder) = stuff\n        if len(filtered) == 0:\n            print(\"fw: No firmware specified\")\n            return\n        if len(filtered) > 1:\n            print(\"fw: No single firmware specified\")\n            return\n\n        firmware = filtered[0][\"_firmware\"]\n        url = firmware[\"url\"]\n\n        try:\n            print(\"fw: URL: %s\"  % (url,))\n            filename=os.path.basename(url)\n            files = []\n            files.append((url,filename))\n            child = multiproc.Process(target=mp_util.download_files, args=(files,))\n            child.start()\n        except Exception as e:\n            print(\"fw: download failed\")\n            print(e)", "category": "Python"}, {"instruction": "def switch(self, idx, u):\n        \"\"\"switch the status of Line idx\"\"\"\n", "input": "", "output": "        self.u[self.uid[idx]] = u\n        self.rebuild = True\n        self.system.dae.factorize = True\n        logger.debug('<Line> Status switch to {} on idx {}.'.format(u, idx))", "category": "Python"}, {"instruction": "def findall(self, expr):\n        \"\"\"list of all matching (sub-)expressions in `expr`\n\n        See also:\n            :meth:`finditer` yields the matches (:class:`MatchDict` instances)\n            for the matched expressions.\n        \"\"\"\n", "input": "", "output": "        result = []\n        try:\n            for arg in expr.args:\n                result.extend(self.findall(arg))\n            for arg in expr.kwargs.values():\n                result.extend(self.findall(arg))\n        except AttributeError:\n            pass\n        if self.match(expr):\n            result.append(expr)\n        return result", "category": "Python"}, {"instruction": "def color(self):\n        \"\"\" Returns the color image. \"\"\"\n", "input": "", "output": "        return ColorImage(self.raw_data[:, :, :3].astype(\n            np.uint8), frame=self.frame)", "category": "Python"}, {"instruction": "def after(self, *nodes: Union[AbstractNode, str]) -> None:\n        \"\"\"Append nodes after this node.\n\n        If nodes contains ``str``, it will be converted to Text node.\n        \"\"\"\n", "input": "", "output": "        if self.parentNode:\n            node = _to_node_list(nodes)\n            _next_node = self.nextSibling\n            if _next_node is None:\n                self.parentNode.appendChild(node)\n            else:\n                self.parentNode.insertBefore(node, _next_node)", "category": "Python"}, {"instruction": "def getrepositorytree(self, project_id, **kwargs):\n        \"\"\"\n        Get a list of repository files and directories in a project.\n\n        :param project_id: The ID of a project\n        :param path: The path inside repository. Used to get contend of subdirectories\n        :param ref_name: The name of a repository branch or tag or if not given the default branch\n        :return: dict with the tree\n        \"\"\"\n", "input": "", "output": "        data = {}\n\n        if kwargs:\n            data.update(kwargs)\n\n        request = requests.get(\n            '{0}/{1}/repository/tree'.format(self.projects_url, project_id), params=data,\n            verify=self.verify_ssl, auth=self.auth, headers=self.headers, timeout=self.timeout)\n\n        if request.status_code == 200:\n            return request.json()\n        else:\n            return False", "category": "Python"}, {"instruction": "def pack_container(in_container, out_file):\n    \"\"\"\n    Pack a container image into a .tar.bz2 archive.\n\n    Args:\n        in_container (str): Path string to the container image.\n        out_file (str): Output file name.\n    \"\"\"\n", "input": "", "output": "    container_filename = local.path(out_file).basename\n    out_container = local.cwd / \"container-out\" / container_filename\n    out_dir = out_container.dirname\n\n    # Pack the results to: container-out\n    with local.cwd(in_container):\n        tar(\"cjf\", out_container, \".\")\n    c_hash = download.update_hash(out_container)\n    if out_dir.exists():\n        mkdir(\"-p\", out_dir)\n    mv(out_container, out_file)\n    mv(out_container + \".hash\", out_file + \".hash\")\n\n    new_container = {\"path\": out_file, \"hash\": str(c_hash)}\n    CFG[\"container\"][\"known\"] += new_container", "category": "Python"}, {"instruction": "def _paload32(ins):\n    ''' Load a 32 bit value from a memory address\n    If 2nd arg. start with '*', it is always treated as\n    an indirect value.\n    '''\n", "input": "", "output": "    output = _paddr(ins.quad[2])\n\n    output.append('call __ILOAD32')\n    output.append('push de')\n    output.append('push hl')\n\n    REQUIRES.add('iload32.asm')\n\n    return output", "category": "Python"}, {"instruction": "def Validate(self, value):\n    \"\"\"Validate a potential list.\"\"\"\n", "input": "", "output": "    if isinstance(value, string_types):\n      raise TypeValueError(\"Value must be an iterable not a string.\")\n\n    elif not isinstance(value, (list, tuple)):\n      raise TypeValueError(\"%r not a valid List\" % value)\n\n    # Validate each value in the list validates against our type.\n    return [self.validator.Validate(val) for val in value]", "category": "Python"}, {"instruction": "def fo_pct(self):\n        \"\"\"\n        Get the by team overall face-off win %.\n        \n        :returns: dict, ``{ 'home': %, 'away': % }``\n        \"\"\"\n", "input": "", "output": "        tots = self.team_totals\n        return {\n            t: tots[t]['won']/(1.0*tots[t]['total']) if tots[t]['total'] else 0.0\n            for t in [ 'home', 'away' ]\n        }", "category": "Python"}, {"instruction": "def dateAt( self, point ):\r\n        \"\"\"\r\n        Returns the date at the given point.\r\n        \r\n        :param      point | <QPoint>\r\n        \"\"\"\n", "input": "", "output": "        for date, data in self._dateGrid.items():\r\n            if ( data[1].contains(point) ):\r\n                return QDate.fromJulianDay(date)\r\n        return QDate()", "category": "Python"}, {"instruction": "def install_trigger(connection: connection, table: str, schema: str='public', overwrite: bool=False) -> None:\n    \"\"\"Install a psycopg2-pgevents trigger against a table.\n\n    Parameters\n    ----------\n    connection: psycopg2.extensions.connection\n        Active connection to a PostGreSQL database.\n    table: str\n        Table for which the trigger should be installed.\n    schema: str\n        Schema to which the table belongs.\n    overwrite: bool\n        Whether or not to overwrite existing installation of trigger for the\n        given table, if existing installation is found.\n\n    Returns\n    -------\n    None\n\n    \"\"\"\n", "input": "", "output": "    prior_install = False\n\n    if not overwrite:\n        prior_install = trigger_installed(connection, table, schema)\n\n    if not prior_install:\n        log('Installing {}.{} trigger...'.format(schema, table), logger_name=_LOGGER_NAME)\n\n        statement = INSTALL_TRIGGER_STATEMENT.format(\n            schema=schema,\n            table=table\n        )\n        execute(connection, statement)\n    else:\n        log('{}.{} trigger already installed; skipping...'.format(schema, table), logger_name=_LOGGER_NAME)", "category": "Python"}, {"instruction": "def serialized(self, prepend_date=True):\n        \"\"\"Return a string fully representing the fact.\"\"\"\n", "input": "", "output": "        name = self.serialized_name()\n        datetime = self.serialized_time(prepend_date)\n        return \"%s %s\" % (datetime, name)", "category": "Python"}, {"instruction": "def default(self, line):\n        ''' Commands with no do_* method fall to here. '''\n", "input": "", "output": "        # Commands that fall through to this method are interpreted as FORTH,\n        # and must be in upper case.\n        if not line.isupper():\n            print(self.style.error('Error: ', 'Unrecognized command.'))\n            return\n\n        if self.arm.is_connected():\n            if self.wrapper:\n                line = self.wrapper.wrap_input(line)\n            self.arm.write(line)\n            try:\n                res = self.arm.read()\n            except KeyboardInterrupt:\n                # NOTE interrupts aren't recursively handled.\n                self.arm.write('STOP')\n                res = self.arm.read()\n            if self.wrapper:\n                res = self.wrapper.wrap_output(res)\n            print(res)\n        else:\n            print(self.style.error('Error: ', 'Arm is not connected.'))", "category": "Python"}, {"instruction": "def check_features(self, features):\n        \"\"\"\n        Method to ensure data to be added is not empty and vectorized.\n\n        Parameters\n        ----------\n        features : iterable\n            Any data that can be converted to a numpy array.\n\n        Returns\n        -------\n        features : numpy array\n            Flattened non-empty numpy array.\n\n        Raises\n        ------\n        ValueError\n            If input data is empty.\n        \"\"\"\n", "input": "", "output": "\n        if not isinstance(features, np.ndarray):\n            features = np.asarray(features)\n\n        if features.size <= 0:\n            raise ValueError('provided features are empty.')\n\n        if features.ndim > 1:\n            features = np.ravel(features)\n\n        return features", "category": "Python"}, {"instruction": "def setPenColor(self, color):\n        \"\"\"\n        Sets the pen for this node.\n        \n        :param      color     <QColor> || None\n        \"\"\"\n", "input": "", "output": "        color = QColor(color)\n        if self._palette is None:\n            self._palette = XNodePalette(self._scenePalette)\n        \n        self._palette.setColor(self._palette.NodeForeground, color)\n        self.setDirty()", "category": "Python"}, {"instruction": "def iter_contributor_statistics(self, number=-1, etag=None):\n        \"\"\"Iterate over the contributors list.\n\n        See also: http://developer.github.com/v3/repos/statistics/\n\n        :param int number: (optional), number of weeks to return. Default -1\n            will return all of the weeks.\n        :param str etag: (optional), ETag from a previous request to the same\n            endpoint\n        :returns: generator of\n            :class:`ContributorStats <github3.repos.stats.ContributorStats>`\n\n        .. note:: All statistics methods may return a 202. On those occasions,\n                  you will not receive any objects. You should store your\n                  iterator and check the new ``last_status`` attribute. If it\n                  is a 202 you should wait before re-requesting.\n\n        .. versionadded:: 0.7\n\n        \"\"\"\n", "input": "", "output": "        url = self._build_url('stats', 'contributors', base_url=self._api)\n        return self._iter(int(number), url, ContributorStats, etag=etag)", "category": "Python"}, {"instruction": "def _memoize_cache_key(args, kwargs):\n    \"\"\"Turn args tuple and kwargs dictionary into a hashable key.\n\n    Expects that all arguments to a memoized function are either hashable\n    or can be uniquely identified from type(arg) and repr(arg).\n    \"\"\"\n", "input": "", "output": "    cache_key_list = []\n\n    # hack to get around the unhashability of lists,\n    # add a special case to convert them to tuples\n    for arg in args:\n        if type(arg) is list:\n            cache_key_list.append(tuple(arg))\n        else:\n            cache_key_list.append(arg)\n    for (k, v) in sorted(kwargs.items()):\n        if type(v) is list:\n            cache_key_list.append((k, tuple(v)))\n        else:\n            cache_key_list.append((k, v))\n    return tuple(cache_key_list)", "category": "Python"}, {"instruction": "def add(modname, features, required_version, installed_version=None,\r\n        optional=False):\r\n    \"\"\"Add Spyder dependency\"\"\"\n", "input": "", "output": "    global DEPENDENCIES\r\n    for dependency in DEPENDENCIES:\r\n        if dependency.modname == modname:\r\n            raise ValueError(\"Dependency has already been registered: %s\"\\\r\n                             % modname)\r\n    DEPENDENCIES += [Dependency(modname, features, required_version,\r\n                                installed_version, optional)]", "category": "Python"}, {"instruction": "def keyPressEvent(self, event):\r\n        \"\"\"\r\n        Qt override.\r\n        \"\"\"\n", "input": "", "output": "        QToolTip.hideText()\r\n        ctrl = event.modifiers() & Qt.ControlModifier\r\n\r\n        if event.key() in [Qt.Key_Enter, Qt.Key_Return]:\r\n            if ctrl:\r\n                self.process_text(array=False)\r\n            else:\r\n                self.process_text(array=True)\r\n            self.accept()\r\n        else:\r\n            QDialog.keyPressEvent(self, event)", "category": "Python"}, {"instruction": "def fit(self, X, y):\n        \"\"\"Fits the given model to the data and labels provided.\n        \n        Parameters:\n        -----------\n        X : matrix, shape (n_samples, n_features)\n        The samples, the train data.\n\n        y : vector, shape (n_samples,)\n        The target labels.\n        \n        Returns:\n        --------\n        self : instance of the model itself (`self`)\n        \n        \"\"\"\n", "input": "", "output": "\n        X = np.array(X, dtype=np.float32)\n        y = np.array(y, dtype=np.float32)\n        assert X.shape[0] == y.shape[0]\n\n        return X, y", "category": "Python"}, {"instruction": "def _run_cnvkit_single(data, background=None):\n    \"\"\"Process a single input file with BAM or uniform background.\n    \"\"\"\n", "input": "", "output": "    if not background:\n        background = []\n    ckouts = _run_cnvkit_shared([data], background)\n    if not ckouts:\n        return [data]\n    else:\n        assert len(ckouts) == 1\n        return _associate_cnvkit_out(ckouts, [data])", "category": "Python"}, {"instruction": "def add_input(cmd, immediate=False):\n    '''add some command input to be processed'''\n", "input": "", "output": "    if immediate:\n        process_stdin(cmd)\n    else:\n        mpstate.input_queue.put(cmd)", "category": "Python"}, {"instruction": "def getStatus(rh):\n    \"\"\"\n    Get the power (logon/off) status of a virtual machine.\n\n    Input:\n       Request Handle with the following properties:\n          function    - 'POWERVM'\n          subfunction - 'STATUS'\n          userid      - userid of the virtual machine\n\n    Output:\n       Request Handle updated with the results.\n       results['overallRC'] - 0: ok, non-zero: error\n       if ok:\n          results['rc'] - 0: for both on and off cases\n          results['rs'] - 0: powered on\n          results['rs'] - 1: powered off\n    \"\"\"\n", "input": "", "output": "\n    rh.printSysLog(\"Enter powerVM.getStatus, userid: \" +\n        rh.userid)\n\n    results = isLoggedOn(rh, rh.userid)\n    if results['overallRC'] != 0:\n        # Unexpected error\n        pass\n    elif results['rs'] == 0:\n        rh.printLn(\"N\", rh.userid + \": on\")\n    else:\n        rh.printLn(\"N\", rh.userid + \": off\")\n\n    rh.updateResults(results)\n\n    rh.printSysLog(\"Exit powerVM.getStatus, rc: \" +\n        str(rh.results['overallRC']))\n    return rh.results['overallRC']", "category": "Python"}, {"instruction": "def differing_blocks(self):\n        \"\"\"\n        :returns: A list of block matches that appear to differ\n        \"\"\"\n", "input": "", "output": "        differing_blocks = []\n        for (func_a, func_b) in self.function_matches:\n            differing_blocks.extend(self.get_function_diff(func_a, func_b).differing_blocks)\n        return differing_blocks", "category": "Python"}, {"instruction": "def network_exists(project_id, network_name,**kwargs):\n    \"\"\"\n    Return a whole network as a complex model.\n    \"\"\"\n", "input": "", "output": "    try:\n        db.DBSession.query(Network.id).filter(func.lower(Network.name).like(network_name.lower()), Network.project_id == project_id).one()\n        return 'Y'\n    except NoResultFound:\n        return 'N'", "category": "Python"}, {"instruction": "def post_check_request(self, check, subscribers):\n        \"\"\"\n        Issues a check execution request.\n        \"\"\"\n", "input": "", "output": "        data = {\n            'check': check,\n            'subscribers': [subscribers]\n        }\n        self._request('POST', '/request', data=json.dumps(data))\n        return True", "category": "Python"}, {"instruction": "def remove_root_repository(self, repository_id):\n        \"\"\"Removes a root repository.\n\n        arg:    repository_id (osid.id.Id): the ``Id`` of a repository\n        raise:  NotFound - ``repository_id`` not a root\n        raise:  NullArgument - ``repository_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.BinHierarchyDesignSession.remove_root_bin_template\n        if self._catalog_session is not None:\n            return self._catalog_session.remove_root_catalog(catalog_id=repository_id)\n        return self._hierarchy_session.remove_root(id_=repository_id)", "category": "Python"}, {"instruction": "def restore_descriptor(self, dataframe):\n        \"\"\"Restore descriptor from Pandas\n        \"\"\"\n", "input": "", "output": "\n        # Prepare\n        fields = []\n        primary_key = None\n\n        # Primary key\n        if dataframe.index.name:\n            field_type = self.restore_type(dataframe.index.dtype)\n            field = {\n                'name': dataframe.index.name,\n                'type': field_type,\n                'constraints': {'required': True},\n            }\n            fields.append(field)\n            primary_key = dataframe.index.name\n\n        # Fields\n        for column, dtype in dataframe.dtypes.iteritems():\n            sample = dataframe[column].iloc[0] if len(dataframe) else None\n            field_type = self.restore_type(dtype, sample=sample)\n            field = {'name': column, 'type': field_type}\n            # TODO: provide better required indication\n            # if dataframe[column].isnull().sum() == 0:\n            #     field['constraints'] = {'required': True}\n            fields.append(field)\n\n        # Descriptor\n        descriptor = {}\n        descriptor['fields'] = fields\n        if primary_key:\n            descriptor['primaryKey'] = primary_key\n\n        return descriptor", "category": "Python"}, {"instruction": "def video_get_size(self, num=0):\n        \"\"\"Get the video size in pixels as 2-tuple (width, height).\n\n        @param num: video number (default 0).\n        \"\"\"\n", "input": "", "output": "        r = libvlc_video_get_size(self, num)\n        if isinstance(r, tuple) and len(r) == 2:\n            return r\n        else:\n            raise VLCException('invalid video number (%s)' % (num,))", "category": "Python"}, {"instruction": "def _txins_data(self):\n        \"\"\"\n        :return: a couple (segwit, txins) where `segwit` is a boolean\n        telling whether the transaction has a segwit format\n        \"\"\"\n", "input": "", "output": "        ntxins = self.parse_varint()\n        if ntxins == 0:\n            self.segwit = True\n            flag = next(self)\n            if flag != 1:\n                raise ValueError('Wrong flag in SegWit transaction: {}'.format(flag))\n            ntxins = self.parse_varint()\n        else:\n            self.segwit = False\n        self.txins = ntxins\n        return self.segwit, [self._txin_data() for _ in range(ntxins)]", "category": "Python"}, {"instruction": "def _shape_repr(shape):\n    \"\"\"Return a platform independent reprensentation of an array shape\n    Under Python 2, the `long` type introduces an 'L' suffix when using the\n    default %r format for tuples of integers (typically used to store the shape\n    of an array).\n    Under Windows 64 bit (and Python 2), the `long` type is used by default\n    in numpy shapes even when the integer dimensions are well below 32 bit.\n    The platform specific type causes string messages or doctests to change\n    from one platform to another which is not desirable.\n    Under Python 3, there is no more `long` type so the `L` suffix is never\n    introduced in string representation.\n    >>> _shape_repr((1, 2))\n    '(1, 2)'\n    >>> one = 2 ** 64 / 2 ** 64  # force an upcast to `long` under Python 2\n    >>> _shape_repr((one, 2 * one))\n    '(1, 2)'\n    >>> _shape_repr((1,))\n    '(1,)'\n    >>> _shape_repr(())\n    '()'\n    \"\"\"\n", "input": "", "output": "    if len(shape) == 0:\n        return \"()\"\n    joined = \", \".join(\"%d\" % e for e in shape)\n    if len(shape) == 1:\n        # special notation for singleton tuples\n        joined += ','\n    return \"(%s)\" % joined", "category": "Python"}, {"instruction": "def delete_types(self, base_key, out_key, *types):\n        \"\"\"\n        Method to delete a parameter from a parameter documentation.\n\n        This method deletes the given `param` from the `base_key` item in the\n        :attr:`params` dictionary and creates a new item with the original\n        documentation without the description of the param. This method works\n        for ``'Results'`` like sections.\n\n\n        See the :meth:`keep_types` method for an example.\n\n        Parameters\n        ----------\n        base_key: str\n            key in the :attr:`params` dictionary\n        out_key: str\n            Extension for the base key (the final key will be like\n            ``'%s.%s' % (base_key, out_key)``\n        ``*types``\n            str. The type identifier of which the documentations shall deleted\n\n        See Also\n        --------\n        delete_params\"\"\"\n", "input": "", "output": "        self.params['%s.%s' % (base_key, out_key)] = self.delete_types_s(\n            self.params[base_key], types)", "category": "Python"}, {"instruction": "def get_url_preview(self, url, ts=None):\n        \"\"\"Get preview for URL.\n\n        Args:\n            url (str): URL to get a preview\n            ts (double): The preferred point in time to return\n                 a preview for. The server may return a newer\n                 version if it does not have the requested\n                 version available.\n        \"\"\"\n", "input": "", "output": "        params = {'url': url}\n        if ts:\n            params['ts'] = ts\n        return self._send(\n            \"GET\", \"\",\n            query_params=params,\n            api_path=\"/_matrix/media/r0/preview_url\"\n        )", "category": "Python"}, {"instruction": "def imagetransformer_cifar10_base_dmol():\n  \"\"\"Best config for 2.90 bits/dim on CIFAR10 using DMOL.\"\"\"\n", "input": "", "output": "  hparams = image_transformer_base()\n  hparams.likelihood = cia.DistributionType.DMOL\n  hparams.num_channels = 1\n  hparams.bottom[\"targets\"] = modalities.image_channel_compress_targets_bottom\n  hparams.top[\"targets\"] = modalities.identity_top\n  hparams.num_heads = 8\n  hparams.batch_size = 8\n  hparams.sampling_method = \"random\"\n  hparams.layer_preprocess_sequence = \"n\"\n  hparams.layer_postprocess_sequence = \"da\"\n  hparams.summarize_grads = True\n  hparams.hidden_size = 256\n  hparams.filter_size = 512\n  hparams.attention_key_channels = 512\n  hparams.attention_value_channels = 512\n  hparams.num_decoder_layers = 12\n  hparams.layer_prepostprocess_dropout = 0.1\n  hparams.learning_rate = 0.1\n  hparams.layer_preprocess_sequence = \"none\"\n  hparams.layer_postprocess_sequence = \"dan\"\n  hparams.pos = \"emb\"\n  hparams.unconditional = True\n  return hparams", "category": "Python"}, {"instruction": "def updateFontPicker(self):\r\n        \"\"\"\r\n        Updates the font picker widget to the current font settings.\r\n        \"\"\"\n", "input": "", "output": "        font = self.currentFont()\r\n        self._fontPickerWidget.setPointSize(font.pointSize())\r\n        self._fontPickerWidget.setCurrentFamily(font.family())", "category": "Python"}, {"instruction": "def _2ndDerivInt(x,y,z,dens,densDeriv,b2,c2,i,j,glx=None,glw=None):\n    \"\"\"Integral that gives the 2nd derivative of the potential in x,y,z\"\"\"\n", "input": "", "output": "    def integrand(s):\n        t= 1/s**2.-1.\n        m= numpy.sqrt(x**2./(1.+t)+y**2./(b2+t)+z**2./(c2+t))\n        return (densDeriv(m)\n                *(x/(1.+t)*(i==0)+y/(b2+t)*(i==1)+z/(c2+t)*(i==2))\n                *(x/(1.+t)*(j==0)+y/(b2+t)*(j==1)+z/(c2+t)*(j==2))/m\\\n                    +dens(m)*(i==j)*((1./(1.+t)*(i==0)+1./(b2+t)*(i==1)+1./(c2+t)*(i==2))))\\\n                    /numpy.sqrt((1.+(b2-1.)*s**2.)*(1.+(c2-1.)*s**2.))\n    if glx is None:\n        return integrate.quad(integrand,0.,1.)[0]\n    else:\n        return numpy.sum(glw*integrand(glx))", "category": "Python"}, {"instruction": "def gen_url_option(\n    str_opt, \n    set_site=set_site, \n    set_runcontrol=set_runcontrol, \n    set_initcond=set_initcond, \n    source='docs'):\n    '''construct a URL for option based on source \n\n    :param str_opt: option name, defaults to ''\n    :param str_opt: str, optional\n    :param source: URL source: 'docs' for readthedocs.org; 'github' for github repo, defaults to 'docs'\n    :param source: str, optional\n    :return: a valid URL pointing to the option related resources\n    :rtype: urlpath.URL\n    '''\n", "input": "", "output": "    dict_base = {\n        'docs': URL('https://suews-docs.readthedocs.io/en/latest/input_files/'),\n        'github': URL('https://github.com/Urban-Meteorology-Reading/SUEWS-Docs/raw/master/docs/source/input_files/'),\n    }\n    url_base = dict_base[source]\n\n    url_page = choose_page(\n        str_opt, set_site, set_runcontrol, set_initcond, source=source)\n    # print('str_opt', str_opt, url_base, url_page)\n    str_opt_x = form_option(str_opt)\n    url_opt = url_base/(url_page+str_opt_x)\n    return url_opt", "category": "Python"}, {"instruction": "def lesson_nums(self):\n        \"\"\"A dict from brain name to the brain's curriculum's lesson number.\"\"\"\n", "input": "", "output": "        lesson_nums = {}\n        for brain_name, curriculum in self.brains_to_curriculums.items():\n            lesson_nums[brain_name] = curriculum.lesson_num\n\n        return lesson_nums", "category": "Python"}, {"instruction": "def all_elements_by_type(name):\n    \"\"\" Get specified elements based on the entry point verb from SMC api\n    To get the entry points available, you can get these from the session::\n\n        session.cache.entry_points\n\n    Execution will get the entry point for the element type, then get all\n    elements that match.\n\n    For example::\n\n        search.all_elements_by_type('host')\n\n    :param name: top level entry point name\n    :raises: `smc.api.exceptions.UnsupportedEntryPoint`\n    :return: list with json representation of name match, else None\n    \"\"\"\n", "input": "", "output": "    if name:\n        entry = element_entry_point(name)\n        if entry:  # in case an invalid entry point is specified\n            result = element_by_href_as_json(entry)\n            return result", "category": "Python"}, {"instruction": "def parse_localinstancepath(self, tup_tree):\n        \"\"\"\n        Parse a LOCALINSTANCEPATH element and return the instance path it\n        represents as a CIMInstanceName object.\n\n          ::\n\n            <!ELEMENT LOCALINSTANCEPATH (LOCALNAMESPACEPATH, INSTANCENAME)>\n        \"\"\"\n", "input": "", "output": "\n        self.check_node(tup_tree, 'LOCALINSTANCEPATH')\n\n        k = kids(tup_tree)\n\n        if len(k) != 2:\n            raise CIMXMLParseError(\n                _format(\"Element {0!A} has invalid number of child elements \"\n                        \"{1!A} (expecting two child elements \"\n                        \"(LOCALNAMESPACEPATH, INSTANCENAME))\",\n                        name(tup_tree), k),\n                conn_id=self.conn_id)\n\n        namespace = self.parse_localnamespacepath(k[0])\n        inst_path = self.parse_instancename(k[1])\n        inst_path.namespace = namespace\n\n        return inst_path", "category": "Python"}, {"instruction": "def set_state(self, onoff, channel=None):\n        \"\"\"Turn state on/off\"\"\"\n", "input": "", "output": "        try:\n            onoff = bool(onoff)\n        except Exception as err:\n            LOG.debug(\"HelperActorState.set_state: Exception %s\" % (err,))\n            return False\n\n        self.writeNodeData(\"STATE\", onoff, channel)", "category": "Python"}, {"instruction": "def enable_pointer_type(self):\n        \"\"\"\n        If a type is a pointer, a platform-independent POINTER_T type needs\n        to be in the generated code.\n        \"\"\"\n", "input": "", "output": "        # 2015-01 reactivating header templates\n        #log.warning('enable_pointer_type deprecated - replaced by generate_headers')\n        # return # FIXME ignore\n        self.enable_pointer_type = lambda: True\n        import pkgutil\n        headers = pkgutil.get_data('ctypeslib', 'data/pointer_type.tpl').decode()\n        import ctypes\n        from clang.cindex import TypeKind\n        # assuming a LONG also has the same sizeof than a pointer.\n        word_size = self.parser.get_ctypes_size(TypeKind.POINTER) // 8\n        word_type = self.parser.get_ctypes_name(TypeKind.ULONG)\n        # pylint: disable=protected-access\n        word_char = getattr(ctypes, word_type)._type_\n        # replacing template values\n        headers = headers.replace('__POINTER_SIZE__', str(word_size))\n        headers = headers.replace('__REPLACEMENT_TYPE__', word_type)\n        headers = headers.replace('__REPLACEMENT_TYPE_CHAR__', word_char)\n        print(headers, file=self.imports)\n        return", "category": "Python"}, {"instruction": "def get_subpackages(name):\n    \"\"\"Return subpackages of package *name*\"\"\"\n", "input": "", "output": "    splist = []\n    for dirpath, _dirnames, _filenames in os.walk(name):\n        if osp.isfile(osp.join(dirpath, '__init__.py')):\n            splist.append(\".\".join(dirpath.split(os.sep)))\n    return splist", "category": "Python"}, {"instruction": "def keys_values(data, *keys):\n    \"\"\"Get an entry as a list from a dict. Provide a fallback key.\"\"\"\n", "input": "", "output": "    values = []\n    if is_mapping(data):\n        for key in keys:\n            if key in data:\n                values.extend(ensure_list(data[key]))\n    return values", "category": "Python"}, {"instruction": "def off(self, evnt, func):\n        '''\n        Remove a previously registered event handler function.\n\n        Example:\n\n            base.off( 'foo', onFooFunc )\n\n        '''\n", "input": "", "output": "        funcs = self._syn_funcs.get(evnt)\n        if funcs is None:\n            return\n\n        try:\n            funcs.remove(func)\n        except ValueError:\n            pass", "category": "Python"}, {"instruction": "def _hexvalue_to_rgb(hexvalue):\n        \"\"\"\n        Converts the hexvalue used by tuya for colour representation into\n        an RGB value.\n        \n        Args:\n            hexvalue(string): The hex representation generated by BulbDevice._rgb_to_hexvalue()\n        \"\"\"\n", "input": "", "output": "        r = int(hexvalue[0:2], 16)\n        g = int(hexvalue[2:4], 16)\n        b = int(hexvalue[4:6], 16)\n\n        return (r, g, b)", "category": "Python"}, {"instruction": "def on_site(self):\n        \"\"\"\n        Return entries published on current site.\n        \"\"\"\n", "input": "", "output": "        return super(EntryPublishedManager, self).get_queryset().filter(\n            sites=Site.objects.get_current())", "category": "Python"}, {"instruction": "def check_bed_contigs(in_file, data):\n    \"\"\"Ensure BED file contigs match the reference genome.\n    \"\"\"\n", "input": "", "output": "    if not dd.get_ref_file(data):\n        return\n    contigs = set([])\n    with utils.open_gzipsafe(in_file) as in_handle:\n        for line in in_handle:\n            if not line.startswith((\"#\", \"track\", \"browser\", \"@\")) and line.strip():\n                contigs.add(line.split()[0])\n    ref_contigs = set([x.name for x in ref.file_contigs(dd.get_ref_file(data))])\n    if contigs and len(contigs - ref_contigs) / float(len(contigs)) > 0.25:\n        raise ValueError(\"Contigs in BED file %s not in reference genome:\\n %s\\n\"\n                         % (in_file, list(contigs - ref_contigs)) +\n                         \"This is typically due to chr1 versus 1 differences in BED file and reference.\")", "category": "Python"}, {"instruction": "def run():\n    \"\"\"Command line entrypoint for the ``refresh-lsst-bib`` program.\n    \"\"\"\n", "input": "", "output": "    args = parse_args()\n\n    if args.verbose:\n        log_level = logging.DEBUG\n    else:\n        log_level = logging.INFO\n    logging.basicConfig(\n        level=log_level,\n        format='%(asctime)s %(levelname)s %(name)s: %(message)s')\n    if not args.verbose:\n        # Manage third-party loggers\n        req_logger = logging.getLogger('requests')\n        req_logger.setLevel(logging.WARNING)\n\n    logger = logging.getLogger(__name__)\n\n    logger.info('refresh-lsst-bib version {}'.format(__version__))\n\n    error_count = process_bib_files(args.dir)\n\n    sys.exit(error_count)", "category": "Python"}, {"instruction": "def center_loss(embedding, label, num_classes, alpha=0.1, scope=\"center_loss\"):\n    r\"\"\"Center-Loss as described in the paper\n    `A Discriminative Feature Learning Approach for Deep Face Recognition`\n    <http://ydwen.github.io/papers/WenECCV16.pdf> by Wen et al.\n\n    Args:\n        embedding (tf.Tensor): features produced by the network\n        label (tf.Tensor): ground-truth label for each feature\n        num_classes (int): number of different classes\n        alpha (float): learning rate for updating the centers\n\n    Returns:\n        tf.Tensor: center loss\n    \"\"\"\n", "input": "", "output": "    nrof_features = embedding.get_shape()[1]\n    centers = tf.get_variable('centers', [num_classes, nrof_features], dtype=tf.float32,\n                              initializer=tf.constant_initializer(0), trainable=False)\n    label = tf.reshape(label, [-1])\n    centers_batch = tf.gather(centers, label)\n    diff = (1 - alpha) * (centers_batch - embedding)\n    centers = tf.scatter_sub(centers, label, diff)\n    loss = tf.reduce_mean(tf.square(embedding - centers_batch), name=scope)\n    return loss", "category": "Python"}, {"instruction": "def provider_parser(subparser):\n    \"\"\"Configure provider parser for Rackspace\"\"\"\n", "input": "", "output": "    subparser.add_argument(\n        \"--auth-account\", help=\"specify account number for authentication\")\n    subparser.add_argument(\n        \"--auth-username\",\n        help=\"specify username for authentication. Only used if --auth-token is empty.\")\n    subparser.add_argument(\n        \"--auth-api-key\",\n        help=\"specify api key for authentication. Only used if --auth-token is empty.\")\n    subparser.add_argument(\n        \"--auth-token\",\n        help=(\"specify token for authentication. \"\n              \"If empty, the username and api key will be used to create a token.\"))\n    subparser.add_argument(\"--sleep-time\", type=float, default=1,\n                           help=\"number of seconds to wait between update requests.\")", "category": "Python"}, {"instruction": "def optimize_spot_bid(ctx, instance_type, spot_bid):\n    \"\"\"\n    Check whether the bid is sane and makes an effort to place the instance in a sensible zone.\n    \"\"\"\n", "input": "", "output": "    spot_history = _get_spot_history(ctx, instance_type)\n    if spot_history:\n        _check_spot_bid(spot_bid, spot_history)\n    zones = ctx.ec2.get_all_zones()\n    most_stable_zone = choose_spot_zone(zones, spot_bid, spot_history)\n    logger.debug(\"Placing spot instances in zone %s.\", most_stable_zone)\n    return most_stable_zone", "category": "Python"}, {"instruction": "def is_correct(self):\n        \"\"\"Check if the Daterange is correct : weekdays are valid\n\n        :return: True if weekdays are valid, False otherwise\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        valid = True\n        valid &= self.swday in range(7)\n        if not valid:\n            logger.error(\"Error: %s is not a valid day\", self.swday)\n\n        valid &= self.ewday in range(7)\n        if not valid:\n            logger.error(\"Error: %s is not a valid day\", self.ewday)\n\n        return valid", "category": "Python"}, {"instruction": "def parse_datetime(s, **kwargs):\n    \"\"\" Converts a time-string into a valid\n    :py:class:`~datetime.datetime.DateTime` object.\n\n        Args:\n            s (str): string to be formatted.\n\n        ``**kwargs`` is passed directly to :func:`.dateutil_parser`.\n\n        Returns:\n            :py:class:`~datetime.datetime.DateTime`\n    \"\"\"\n", "input": "", "output": "    if not s:\n        return None\n    try:\n        ret = dateutil_parser(s, **kwargs)\n    except (OverflowError, TypeError, ValueError) as e:\n        logger.exception(e, exc_info=True)\n        reraise('datetime parsing error from %s' % s, e)\n    return ret", "category": "Python"}, {"instruction": "def increment(self, name, value):\n        \"\"\"\n        Increments counter by given value.\n\n        :param name: a counter name of Increment type.\n\n        :param value: a value to add to the counter.\n        \"\"\"\n", "input": "", "output": "        counter = self.get(name, CounterType.Increment)\n        counter.count = counter.count + value if counter.count != None else value\n        self._update()", "category": "Python"}, {"instruction": "def stop_instance(self, instance_id):\n        \"\"\"Stops the instance gracefully.\n\n        :param str instance_id: instance identifier\n        \"\"\"\n", "input": "", "output": "        instance = self._load_instance(instance_id)\n        instance.terminate()\n        del self._instances[instance_id]", "category": "Python"}, {"instruction": "def merge(self, po_file, source_files):\n        \"\"\"\u4ece\u6e90\u7801\u4e2d\u83b7\u53d6\u6240\u6709\u6761\u76ee\uff0c\u5408\u5e76\u5230 po_file \u4e2d\u3002\n\n        :param string po_file: \u5f85\u5199\u5165\u7684 po \u6587\u4ef6\u8def\u5f84\u3002\n        :param list source_files :  \u6240\u6709\u5f85\u5904\u7406\u7684\u539f\u6587\u4ef6\u8def\u5f84 list\u3002\n\n        \"\"\"\n", "input": "", "output": "        # Create a temporary file to write pot file\n        pot_file = tempfile.NamedTemporaryFile(mode='wb', prefix='rookout_', delete=False)\n        pot_filename = pot_file.name\n        slog.info('Create POT file [%s].', pot_filename)\n        xargs = [self._xgettext,\n                \"--package-name=main\",\n                \"--package-version=0.1\",\n                \"--default-domain=main\",\n                \"--from-code=UTF-8\",\n                \"-C\", \"-k_\",\n                \"--output\", pot_filename]\n        txt = subprocess.check_output(xargs+source_files, \n                stderr=subprocess.STDOUT, \n                universal_newlines=True)\n        if len(txt) > 0:\n            raise(ChildProcessError(txt))\n        slog.info('Start merge [%s] to [%s].', pot_filename, po_file)\n        xargs = [self._msgmerge, \"-U\", po_file, pot_filename]\n        txt = subprocess.check_output(xargs, universal_newlines=True)\n        slog.info(txt)\n        pot_file.close()\n        os.remove(pot_filename)", "category": "Python"}, {"instruction": "def do_transition_for(brain_or_object, transition):\n    \"\"\"Performs a workflow transition for the passed in object.\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: The object where the transtion was performed\n    \"\"\"\n", "input": "", "output": "    if not isinstance(transition, basestring):\n        fail(\"Transition type needs to be string, got '%s'\" % type(transition))\n    obj = get_object(brain_or_object)\n    try:\n        ploneapi.content.transition(obj, transition)\n    except ploneapi.exc.InvalidParameterError as e:\n        fail(\"Failed to perform transition '{}' on {}: {}\".format(\n             transition, obj, str(e)))\n    return obj", "category": "Python"}, {"instruction": "def get(package_str, classname):\n    '''Retrieve from the internal cache a class instance. All arguments are case-insensitive'''\n", "input": "", "output": "    if (package_str in _dynamo_cache) and (classname in _dynamo_cache[package_str]):\n        return _dynamo_cache[package_str][classname]\n    return None", "category": "Python"}, {"instruction": "def _getCleanupCommand(self):\n        \"\"\"also used by tests for expectations\"\"\"\n", "input": "", "output": "        return textwrap.dedent(", "category": "Python"}, {"instruction": "def custom_background_code():\n    \"\"\" Custom code run in a background thread. Prints the current block height.\n\n    This function is run in a daemonized thread, which means it can be instantly killed at any\n    moment, whenever the main thread quits. If you need more safety, don't use a  daemonized\n    thread and handle exiting this thread in another way (eg. with signals and events).\n    \"\"\"\n", "input": "", "output": "    while True:\n        logger.info(\"Block %s / %s\", str(Blockchain.Default().Height), str(Blockchain.Default().HeaderHeight))\n        sleep(15)", "category": "Python"}, {"instruction": "def applies(self, host, method, path):\n        \"\"\"\n        Determines whether or not this sampling rule applies to\n        the incoming request based on some of the request's parameters.\n        Any None parameters provided will be considered an implicit match.\n        \"\"\"\n", "input": "", "output": "        return (not host or wildcard_match(self.host, host)) \\\n            and (not method or wildcard_match(self.method, method)) \\\n            and (not path or wildcard_match(self.path, path))", "category": "Python"}, {"instruction": "def _glyph_for_complex_pattern(self, pattern):\n        \"\"\"Add glyph and member glyphs for a PySB ComplexPattern.\"\"\"\n", "input": "", "output": "        # Make the main glyph for the agent\n        monomer_glyphs = []\n        for monomer_pattern in pattern.monomer_patterns:\n            glyph = self._glyph_for_monomer_pattern(monomer_pattern)\n            monomer_glyphs.append(glyph)\n\n        if len(monomer_glyphs) > 1:\n            pattern.matches_key = lambda: str(pattern)\n            agent_id = self._make_agent_id(pattern)\n            complex_glyph = \\\n                emaker.glyph(emaker.bbox(**self.complex_style),\n                             class_('complex'), id=agent_id)\n            for glyph in monomer_glyphs:\n                glyph.attrib['id'] = agent_id + glyph.attrib['id']\n                complex_glyph.append(glyph)\n            return complex_glyph\n        return monomer_glyphs[0]", "category": "Python"}, {"instruction": "def to_device(self, device, non_blocking=True):\n        \"\"\" Move a rollout to a selected device \"\"\"\n", "input": "", "output": "        return Trajectories(\n            num_steps=self.num_steps,\n            num_envs=self.num_envs,\n            environment_information=self.environment_information,\n            transition_tensors={k: v.to(device, non_blocking=non_blocking) for k, v in self.transition_tensors.items()},\n            rollout_tensors={k: v.to(device, non_blocking=non_blocking) for k, v in self.rollout_tensors.items()},\n            extra_data=self.extra_data\n        )", "category": "Python"}, {"instruction": "def exception(self):\n        '''Return an instance of the corresponding exception'''\n", "input": "", "output": "        code, _, message = self.data.partition(' ')\n        return self.find(code)(message)", "category": "Python"}, {"instruction": "def _append_national_number(self, national_number):\n        \"\"\"Combines the national number with any prefix (IDD/+ and country\n        code or national prefix) that was collected. A space will be inserted\n        between them if the current formatting template indicates this to be\n        suitable.\n        \"\"\"\n", "input": "", "output": "        prefix_before_nn_len = len(self._prefix_before_national_number)\n        if (self._should_add_space_after_national_prefix and prefix_before_nn_len > 0 and\n            self._prefix_before_national_number[-1] != _SEPARATOR_BEFORE_NATIONAL_NUMBER):\n            # We want to add a space after the national prefix if the national\n            # prefix formatting rule indicates that this would normally be\n            # done, with the exception of the case where we already appended a\n            # space because the NDD was surprisingly long.\n            return self._prefix_before_national_number + _SEPARATOR_BEFORE_NATIONAL_NUMBER + national_number\n        else:\n            return self._prefix_before_national_number + national_number", "category": "Python"}, {"instruction": "def parse_args():\n    \"\"\"Parses the arguments and options.\"\"\"\n", "input": "", "output": "    parser = argparse.ArgumentParser(\n        prog=\"geneparse-indexer\",\n        description=\"Genotype file indexer.\"\n    )\n\n    # IMPUTE2 files\n    group = parser.add_argument_group(\"IMPUTE2 index\")\n    group.add_argument(\n        \"--impute2\", metavar=\"IMPUTE2\", type=str, nargs=\"+\",\n        help=\"Index an IMPUTE2 genotype file format. The file can be plain \"\n             \"text or bgzipped.\",\n    )\n\n    # BGEN files\n    group = parser.add_argument_group(\"BGEN index\")\n    group.add_argument(\n        \"--bgen\", metavar=\"BGEN\", type=str, nargs=\"+\",\n        help=\"Index a BGEN genotype file. This requires 'bgenix' to be in the \"\n             \"PATH.\",\n    )\n    group.add_argument(\n        \"--legacy\", action=\"store_true\",\n        help=\"Index the file using the '-with-rowid' option. This flag \"\n             \"enables compatibility with SQLITE prior to version 3.8.2. See \"\n             \"https://bitbucket.org/gavinband/bgen/wiki/bgenix for more \"\n             \"information.\",\n    )\n\n    return parser.parse_args()", "category": "Python"}, {"instruction": "def name(self):\n    \"\"\"str: name.\"\"\"\n", "input": "", "output": "    if self._tsk_attribute:\n      # The value of the attribute name will be None for the default\n      # data stream.\n      attribute_name = getattr(self._tsk_attribute.info, 'name', None)\n      if attribute_name:\n        try:\n          # pytsk3 returns an UTF-8 encoded byte string.\n          return attribute_name.decode('utf8')\n        except UnicodeError:\n          pass\n\n    return ''", "category": "Python"}, {"instruction": "def language(self, lang):\n        \"\"\" Set the language to use; attempts to change the API URL \"\"\"\n", "input": "", "output": "        lang = lang.lower()\n        if self._lang == lang:\n            return\n\n        url = self._api_url\n        tmp = url.replace(\"/{0}.\".format(self._lang), \"/{0}.\".format(lang))\n\n        self._api_url = tmp\n        self._lang = lang\n        self.clear_memoized()", "category": "Python"}, {"instruction": "def object(self):\n        \"\"\"Return the changed object.\"\"\"\n", "input": "", "output": "        if self.type == EntryType.category:\n            return self.category\n        elif self.type == EntryType.event:\n            return self.event\n        elif self.type == EntryType.session:\n            return self.session\n        elif self.type == EntryType.contribution:\n            return self.contribution\n        elif self.type == EntryType.subcontribution:\n            return self.subcontribution", "category": "Python"}, {"instruction": "def flock(path):\n    \"\"\"Attempt to acquire a POSIX file lock.\n    \"\"\"\n", "input": "", "output": "    with open(path, \"w+\") as lf:\n        try:\n            fcntl.flock(lf, fcntl.LOCK_EX | fcntl.LOCK_NB)\n            acquired = True\n            yield acquired\n\n        except OSError:\n            acquired = False\n            yield acquired\n\n        finally:\n            if acquired:\n                fcntl.flock(lf, fcntl.LOCK_UN)", "category": "Python"}, {"instruction": "def get_grade_system_query_session(self):\n        \"\"\"Gets the ``OsidSession`` associated with the grade system query service.\n\n        return: (osid.grading.GradeSystemQuerySession) - a\n                ``GradeSystemQuerySession``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented - ``supports_grade_system_query()`` is\n                ``false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_grade_system_query()`` is ``true``.*\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_grade_system_query():\n            raise errors.Unimplemented()\n        # pylint: disable=no-member\n        return sessions.GradeSystemQuerySession(runtime=self._runtime)", "category": "Python"}, {"instruction": "def render_from_string(content, context=None, globals=None):\n    \"\"\"\n    Renders a templated yaml document from a string.\n\n    :param content: The yaml string to evaluate.\n    :param context: A context to overlay on the yaml file.  This will override any yaml values.\n    :param globals: A dictionary of globally-accessible objects within the rendered template.\n    :return: A dict with the final overlayed configuration.\n    \"\"\"\n", "input": "", "output": "    yaml_resolver = resolver.TYamlResolver.new_from_string(content)\n\n    return yaml_resolver.resolve(Context(context), globals)._data", "category": "Python"}, {"instruction": "def nodeSatisfiesValues(cntxt: Context, n: Node, nc: ShExJ.NodeConstraint, _c: DebugContext) -> bool:\n    \"\"\" `5.4.5 Values Constraint <http://shex.io/shex-semantics/#values>`_\n\n     For a node n and constraint value v, nodeSatisfies(n, v) if n matches some valueSetValue vsv in v.\n    \"\"\"\n", "input": "", "output": "    if nc.values is None:\n        return True\n    else:\n        if any(_nodeSatisfiesValue(cntxt, n, vsv) for vsv in nc.values):\n            return True\n        else:\n            cntxt.fail_reason = f\"Node: {cntxt.n3_mapper.n3(n)} not in value set:\\n\\t \" \\\n                f\"{as_json(cntxt.type_last(nc), indent=None)[:60]}...\"\n            return False", "category": "Python"}, {"instruction": "def symmetries(self):\n        \"\"\"Graph symmetries (permutations) that map the graph onto itself.\"\"\"\n", "input": "", "output": "\n        symmetry_cycles = set([])\n        symmetries = set([])\n        for match in GraphSearch(EqualPattern(self))(self):\n            match.cycles = match.get_closed_cycles()\n            if match.cycles in symmetry_cycles:\n                raise RuntimeError(\"Duplicates in EqualMatch\")\n            symmetry_cycles.add(match.cycles)\n            symmetries.add(match)\n        return symmetries", "category": "Python"}, {"instruction": "def remove_all_callbacks(self):\n        \"\"\" Removes all registered callbacks.\"\"\"\n", "input": "", "output": "        for cb_id in list(self._next_tick_callback_removers.keys()):\n            self.remove_next_tick_callback(cb_id)\n        for cb_id in list(self._timeout_callback_removers.keys()):\n            self.remove_timeout_callback(cb_id)\n        for cb_id in list(self._periodic_callback_removers.keys()):\n            self.remove_periodic_callback(cb_id)", "category": "Python"}, {"instruction": "def addVariable(self, variable, domain):\n        \"\"\"\n        Add a variable to the problem\n\n        Example:\n\n        >>> problem = Problem()\n        >>> problem.addVariable(\"a\", [1, 2])\n        >>> problem.getSolution() in ({'a': 1}, {'a': 2})\n        True\n\n        @param variable: Object representing a problem variable\n        @type  variable: hashable object\n        @param domain: Set of items defining the possible values that\n                       the given variable may assume\n        @type  domain: list, tuple, or instance of C{Domain}\n        \"\"\"\n", "input": "", "output": "        if variable in self._variables:\n            msg = \"Tried to insert duplicated variable %s\" % repr(variable)\n            raise ValueError(msg)\n        if isinstance(domain, Domain):\n            domain = copy.deepcopy(domain)\n        elif hasattr(domain, \"__getitem__\"):\n            domain = Domain(domain)\n        else:\n            msg = \"Domains must be instances of subclasses of the Domain class\"\n            raise TypeError(msg)\n        if not domain:\n            raise ValueError(\"Domain is empty\")\n        self._variables[variable] = domain", "category": "Python"}, {"instruction": "def uncomment(path,\n              regex,\n              char='#',\n              backup='.bak'):\n    '''\n    .. deprecated:: 0.17.0\n       Use :py:func:`~salt.modules.file.replace` instead.\n\n    Uncomment specified commented lines in a file\n\n    path\n        The full path to the file to be edited\n    regex\n        A regular expression used to find the lines that are to be uncommented.\n        This regex should not include the comment character. A leading ``^``\n        character will be stripped for convenience (for easily switching\n        between comment() and uncomment()).\n    char : ``#``\n        The character to remove in order to uncomment a line\n    backup : ``.bak``\n        The file will be backed up before edit with this file extension;\n        **WARNING:** each time ``sed``/``comment``/``uncomment`` is called will\n        overwrite this backup\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.uncomment /etc/hosts.deny 'ALL: PARANOID'\n    '''\n", "input": "", "output": "    return comment_line(path=path,\n                        regex=regex,\n                        char=char,\n                        cmnt=False,\n                        backup=backup)", "category": "Python"}, {"instruction": "def unblock_username(username, pipe=None):\n    \"\"\" unblock the given Username \"\"\"\n", "input": "", "output": "    do_commit = False\n    if not pipe:\n        pipe = REDIS_SERVER.pipeline()\n        do_commit = True\n    if username:\n        pipe.delete(get_username_attempt_cache_key(username))\n        pipe.delete(get_username_blocked_cache_key(username))\n        if do_commit:\n            pipe.execute()", "category": "Python"}, {"instruction": "def checkin(self, package, no_partitions=False, force=False,  cb=None):\n        \"\"\"\n        Check in a bundle package to the remote.\n\n        :param package: A Database, referencing a sqlite database holding the bundle\n        :param cb: a two argument progress callback: cb(message, num_records)\n        :return:\n        \"\"\"\n", "input": "", "output": "        from ambry.orm.exc import NotFoundError\n\n        if not os.path.exists(package.path):\n            raise NotFoundError(\"Package path does not exist: '{}' \".format(package.path))\n\n        if self.is_api:\n            return self._checkin_api(package, no_partitions=no_partitions, force=force, cb=cb)\n        else:\n            return self._checkin_fs(package, no_partitions=no_partitions, force=force, cb=cb)", "category": "Python"}, {"instruction": "def handle(self, env, data):\n        '''\n        Calls application and handles following cases:\n            * catches `webob.HTTPException` errors.\n            * catches unhandled exceptions, calls `handle_error` method\n              and returns 500.\n            * returns 404 if the app has returned None`.\n        '''\n", "input": "", "output": "        try:\n            response = self.handler(env, data)\n            if response is None:\n                logger.debug('Application returned None '\n                             'instead of Response object')\n                response = HTTPNotFound()\n        except HTTPException as e:\n            response = e\n        except Exception as e:\n            self.handle_error(env)\n            response = HTTPInternalServerError()\n        return response", "category": "Python"}, {"instruction": "def get_version():\n    \"\"\"Obtain the packge version from a python file e.g. pkg/__init__.py\n    See <https://packaging.python.org/en/latest/single_source_version.html>.\n    \"\"\"\n", "input": "", "output": "    file_dir = os.path.realpath(os.path.dirname(__file__))\n    with open(\n            os.path.join(file_dir, '..', 'behold', 'version.py')) as f:\n        txt = f.read()\n    version_match = re.search(\n        r", "category": "Python"}, {"instruction": "def interrupt(self):\n\t\t\"\"\"\n\t\tInvoked on a write operation into the IR of the RendererDevice.\n\t\t\"\"\"\n", "input": "", "output": "\t\tif(self.device.read(9) & 0x01):\n\t\t\tself.handle_request()\n\t\tself.device.clear_IR()", "category": "Python"}, {"instruction": "def GetLocation(session=None):\n        \"\"\"Return specified location or if none the default location associated with the provided credentials and alias.\n\n        >>> clc.v2.Account.GetLocation()\n        u'WA1'\n        \"\"\"\n", "input": "", "output": "        if session is not None:\n            return session['location']\n\n        if not clc.LOCATION:  clc.v2.API._Login()\n        return(clc.LOCATION)", "category": "Python"}, {"instruction": "def peek(contents):\n    \"\"\"\n    Parses a byte string of ASN.1 BER/DER-encoded data to find the length\n\n    This is typically used to look into an encoded value to see how long the\n    next chunk of ASN.1-encoded data is. Primarily it is useful when a\n    value is a concatenation of multiple values.\n\n    :param contents:\n        A byte string of BER/DER-encoded data\n\n    :raises:\n        ValueError - when the contents do not contain an ASN.1 header or are truncated in some way\n        TypeError - when contents is not a byte string\n\n    :return:\n        An integer with the number of bytes occupied by the ASN.1 value\n    \"\"\"\n", "input": "", "output": "\n    if not isinstance(contents, byte_cls):\n        raise TypeError('contents must be a byte string, not %s' % type_name(contents))\n\n    info, consumed = _parse(contents, len(contents))\n    return consumed", "category": "Python"}, {"instruction": "def privateKeyToAccount(self, private_key):\n        '''\n        Returns a convenient object for working with the given private key.\n\n        :param private_key: The raw private key\n        :type private_key: hex str, bytes, int or :class:`eth_keys.datatypes.PrivateKey`\n        :return: object with methods for signing and encrypting\n        :rtype: LocalAccount\n\n        .. code-block:: python\n\n            >>> acct = Account.privateKeyToAccount(\n              0xb25c7db31feed9122727bf0939dc769a96564b2de4c4726d035b36ecf1e5b364)\n            >>> acct.address\n            '0x5ce9454909639D2D17A3F753ce7d93fa0b9aB12E'\n            >>> acct.privateKey\n            b\"\\\\xb2\\\\}\\\\xb3\\\\x1f\\\\xee\\\\xd9\\\\x12''\\\\xbf\\\\t9\\\\xdcv\\\\x9a\\\\x96VK-\\\\xe4\\\\xc4rm\\\\x03[6\\\\xec\\\\xf1\\\\xe5\\\\xb3d\"\n\n            # These methods are also available: signHash(), signTransaction(), encrypt()\n            # They correspond to the same-named methods in Account.*\n            # but without the private key argument\n        '''\n", "input": "", "output": "        key = self._parsePrivateKey(private_key)\n        return LocalAccount(key, self)", "category": "Python"}, {"instruction": "def iter_(obj):\n    \"\"\"A custom replacement for iter(), dispatching a few custom picklable\n    iterators for known types.\n    \"\"\"\n", "input": "", "output": "    if six.PY2:\n        file_types = file,  # noqa\n    if six.PY3:\n        file_types = io.IOBase,\n        dict_items = {}.items().__class__\n        dict_values = {}.values().__class__\n        dict_keys = {}.keys().__class__\n        dict_view = (dict_items, dict_values, dict_keys)\n\n    if isinstance(obj, dict):\n        return ordered_sequence_iterator(list(obj.keys()))\n    if isinstance(obj, file_types):\n        return file_iterator(obj)\n    if six.PY2:\n        if isinstance(obj, (list, tuple)):\n            return ordered_sequence_iterator(obj)\n        if isinstance(obj, xrange):  # noqa\n            return range_iterator(obj)\n        if NUMPY_AVAILABLE and isinstance(obj, numpy.ndarray):\n            return ordered_sequence_iterator(obj)\n    if six.PY3 and isinstance(obj, dict_view):\n        return ordered_sequence_iterator(list(obj))\n    return iter(obj)", "category": "Python"}, {"instruction": "def haversine(lon1, lat1, lon2, lat2):\n    \"\"\"\n    Distance between geodesic coordinates http://www.movable-type.co.uk/scripts/latlong.html\n\n    :param lon1: point 1 latitude\n    :param lat1: point 1 longitude\n    :param lon2: point 1 latitude\n    :param lat2: point 2 longitude\n    :return: distance in meters between points 1 and 2\n    \"\"\"\n", "input": "", "output": "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1\n    dlat = lat2 - lat1\n    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n    c = 2 * asin(sqrt(a))\n    m = 6367000 * c\n    return m", "category": "Python"}, {"instruction": "def on_finish(self):\n        \"\"\" Called regardless of success or failure \"\"\"\n", "input": "", "output": "        r = self.response\n        r.request_time = time.time() - self.start_time\n        if self.callback:\n            self.callback(r)", "category": "Python"}, {"instruction": "def _func_filters(self, filters):\n        '''Build post query filters\n        '''\n", "input": "", "output": "        if not isinstance(filters, (list,tuple)):\n            raise TypeError('func_filters must be a <type list> or <type tuple>')\n\n        for i, func in enumerate(filters) :\n            if isinstance(func, str) and func == 'reverse':\n                filters[i] = 'reverse()'\n            elif isinstance(func, tuple) and func[0] in YQL.FUNC_FILTERS:\n                filters[i] = '{:s}(count={:d})'.format(*func)\n            elif isinstance(func, dict) :\n                func_stmt = ''\n                func_name = list(func.keys())[0] # Because of Py3\n                values = [ \"{0}='{1}'\".format(v[0], v[1]) for v in func[func_name] ]\n                func_stmt = ','.join(values)\n                func_stmt = '{0}({1})'.format(func_name, func_stmt)\n                filters[i] = func_stmt\n            else:\n                raise TypeError('{0} is neither a <str>, a <tuple> or a <dict>'.format(func))\n        return '| '.join(filters)", "category": "Python"}, {"instruction": "def init_app(self, app, client_id=None):\n        \"\"\"Initialize the Micropub extension if it was not given app\n        in the constructor.\n\n        Args:\n          app (flask.Flask): the flask application to extend.\n          client_id (string, optional): the IndieAuth client id, will be\n            displayed when the user is asked to authorize this client. If not\n            provided, the app name will be used.\n        \"\"\"\n", "input": "", "output": "        if not self.client_id:\n            if client_id:\n                self.client_id = client_id\n            else:\n                self.client_id = app.name", "category": "Python"}, {"instruction": "def gotoNext( self ):\r\n        \"\"\"\r\n        Goes to the next date based on the current mode and date.\r\n        \"\"\"\n", "input": "", "output": "        scene = self.scene()\r\n        date  = scene.currentDate()\r\n        \r\n        # go forward a day\r\n        if ( scene.currentMode() == scene.Mode.Day ):\r\n            scene.setCurrentDate(date.addDays(1))\r\n        \r\n        # go forward a week\r\n        elif ( scene.currentMode() == scene.Mode.Week ):\r\n            scene.setCurrentDate(date.addDays(7))\r\n        \r\n        # go forward a month\r\n        elif ( scene.currentMode() == scene.Mode.Month ):\r\n            scene.setCurrentDate(date.addMonths(1))", "category": "Python"}, {"instruction": "def _get_batting_order_starting_flg(cls, batter):\n        \"\"\"\n        get batting order and starting member flg\n        :param batter: Beautifulsoup object(batter element)\n        :return: batting order(1-9), starting member flg(True or False)\n        \"\"\"\n", "input": "", "output": "        bo = batter.get('bo', None)\n        if not bo or len(bo) != 3:\n            return False, False\n        batting_order = bo[:1]\n        starting = True if bo[1:3] == '00' else False\n        return batting_order, starting", "category": "Python"}, {"instruction": "def load_logos(filename):\n    '''\n    Load logos from a geologos archive from <filename>\n\n    <filename> can be either a local path or a remote URL.\n    '''\n", "input": "", "output": "    if filename.startswith('http'):\n        log.info('Downloading GeoLogos bundle: %s', filename)\n        filename, _ = urlretrieve(filename, tmp.path('geologos.tar.xz'))\n\n    log.info('Extracting GeoLogos bundle')\n    with contextlib.closing(lzma.LZMAFile(filename)) as xz:\n        with tarfile.open(fileobj=xz) as f:\n            f.extractall(tmp.root)\n\n    log.info('Moving to the final location and cleaning up')\n    if os.path.exists(logos.root):\n        shutil.rmtree(logos.root)\n    shutil.move(tmp.path('logos'), logos.root)\n    log.info('Done')", "category": "Python"}, {"instruction": "def MakeZip(self, xar_file, output_file):\n    \"\"\"Add a zip to the end of the .xar containing build.yaml.\n\n    The build.yaml is already inside the .xar file, but we can't easily open\n    this on linux. To make repacking easier we add a zip to the end of the .xar\n    and add in the build.yaml. The repack step will then look at the build.yaml\n    and insert the config.yaml. We end up storing the build.yaml twice but it is\n    tiny, so this doesn't matter.\n\n    Args:\n      xar_file: the name of the xar file.\n      output_file: the name of the output ZIP archive.\n    \"\"\"\n", "input": "", "output": "    logging.info(\"Generating zip template file at %s\", output_file)\n    with zipfile.ZipFile(output_file, mode=\"a\") as zf:\n      # Get the build yaml\n      # TODO(hanuszczak): YAML, consider using `StringIO` instead.\n      build_yaml = io.BytesIO()\n      self.WriteBuildYaml(build_yaml)\n      build_yaml.seek(0)\n      zf.writestr(\"build.yaml\", build_yaml.read())", "category": "Python"}, {"instruction": "def get_resource_area_by_host(self, area_id, host_id):\n        \"\"\"GetResourceAreaByHost.\n        [Preview API]\n        :param str area_id:\n        :param str host_id:\n        :rtype: :class:`<ResourceAreaInfo> <azure.devops.v5_0.location.models.ResourceAreaInfo>`\n        \"\"\"\n", "input": "", "output": "        route_values = {}\n        if area_id is not None:\n            route_values['areaId'] = self._serialize.url('area_id', area_id, 'str')\n        query_parameters = {}\n        if host_id is not None:\n            query_parameters['hostId'] = self._serialize.query('host_id', host_id, 'str')\n        response = self._send(http_method='GET',\n                              location_id='e81700f7-3be2-46de-8624-2eb35882fcaa',\n                              version='5.0-preview.1',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('ResourceAreaInfo', response)", "category": "Python"}, {"instruction": "def makeImages(self):\n        \"\"\"Make spiral images in sectors and steps.\n\n        Plain, reversed,\n        sectorialized, negative sectorialized\n        outline, outline reversed, lonely\n        only nodes, only edges, both\n        \"\"\"\n", "input": "", "output": "        # make layout\n        self.makeLayout()\n        self.setAgraph()\n        # make function that accepts a mode, a sector\n        # and nodes and edges True and False\n        self.plotGraph()\n        self.plotGraph(\"reversed\",filename=\"tgraphR.png\")\n        agents=n.concatenate(self.np.sectorialized_agents__)\n        for i, sector in enumerate(self.np.sectorialized_agents__):\n            self.plotGraph(\"plain\",   sector,\"sector{:02}.png\".format(i))\n            self.plotGraph(\"reversed\",sector,\"sector{:02}R.png\".format(i))\n            self.plotGraph(\"plain\", n.setdiff1d(agents,sector),\"sector{:02}N.png\".format(i))\n            self.plotGraph(\"reversed\",n.setdiff1d(agents,sector),\"sector{:02}RN.png\".format(i))\n        self.plotGraph(\"plain\",   [],\"BLANK.png\")", "category": "Python"}, {"instruction": "def runSearchReferenceSets(self, request):\n        \"\"\"\n        Runs the specified SearchReferenceSetsRequest.\n        \"\"\"\n", "input": "", "output": "        return self.runSearchRequest(\n            request, protocol.SearchReferenceSetsRequest,\n            protocol.SearchReferenceSetsResponse,\n            self.referenceSetsGenerator)", "category": "Python"}, {"instruction": "def _group_dict_set(iterator):\n    \"\"\"Make a dict that accumulates the values for each key in an iterator of doubles.\n\n    :param iter[tuple[A,B]] iterator: An iterator\n    :rtype: dict[A,set[B]]\n    \"\"\"\n", "input": "", "output": "    d = defaultdict(set)\n    for key, value in iterator:\n        d[key].add(value)\n    return dict(d)", "category": "Python"}, {"instruction": "def get_resource(self):\n        \"\"\"Gets the ``Resource`` for this authorization.\n\n        return: (osid.resource.Resource) - the ``Resource``\n        raise:  IllegalState - ``has_resource()`` is ``false``\n        raise:  OperationFailed - unable to complete request\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for osid.resource.Resource.get_avatar_template\n        if not bool(self._my_map['resourceId']):\n            raise errors.IllegalState('this Authorization has no resource')\n        mgr = self._get_provider_manager('RESOURCE')\n        if not mgr.supports_resource_lookup():\n            raise errors.OperationFailed('Resource does not support Resource lookup')\n        lookup_session = mgr.get_resource_lookup_session(proxy=getattr(self, \"_proxy\", None))\n        lookup_session.use_federated_bin_view()\n        osid_object = lookup_session.get_resource(self.get_resource_id())\n        return osid_object", "category": "Python"}, {"instruction": "def _get_evidence(self, evidence_dict, time_slice, shift):\n        \"\"\"\n        Getting the evidence belonging to a particular timeslice.\n\n        Parameters:\n        ----------\n        evidence: dict\n            a dict key, value pair as {var: state_of_var_observed}\n            None if no evidence\n\n        time: int\n            the evidence corresponding to the time slice\n\n        shift: int\n            shifting the evidence corresponding to the given time slice.\n        \"\"\"\n", "input": "", "output": "        if evidence_dict:\n            return {(node[0], shift): evidence_dict[node] for node in evidence_dict if node[1] == time_slice}", "category": "Python"}, {"instruction": "async def create_session(self, **kwargs):\n        \"\"\"Creates an :class:`aiohttp.ClientSession`\n\n        Override this or call it with ``kwargs`` to use other :mod:`aiohttp`\n        functionality not covered by :class:`~.InfluxDBClient.__init__`\n        \"\"\"\n", "input": "", "output": "        self.opts.update(kwargs)\n        self._session = aiohttp.ClientSession(**self.opts, loop=self._loop)\n        if self.redis_opts:\n            if aioredis:\n                self._redis = await aioredis.create_redis(**self.redis_opts,\n                                                          loop=self._loop)\n            else:\n                warnings.warn(no_redis_warning)", "category": "Python"}, {"instruction": "def export_data( self ):\n        \"\"\"Export data to a byte array.\"\"\"\n", "input": "", "output": "        klass = self.__class__\n\n        output = bytearray( b'\\x00'*self.get_size() )\n\n        # prevalidate all data before export.\n        # this is important to ensure that any dependent fields\n        # are updated beforehand, e.g. a count referenced\n        # in a BlockField\n        queue = []\n        for name in klass._fields:\n            self.scrub_field( name )\n            self.validate_field( name )\n\n        self.update_deps()\n\n        for name in klass._fields:\n            klass._fields[name].update_buffer_with_value(\n                self._field_data[name], output, parent=self\n            )\n\n        for name, check in klass._checks.items():\n            check.update_buffer( output, parent=self )\n        return output", "category": "Python"}, {"instruction": "def get_celery_info():\n    \"\"\"\n    Check celery availability\n    \"\"\"\n", "input": "", "output": "    import celery\n    if not getattr(settings, 'USE_CELERY', False):\n        log.error(\"No celery config found. Set USE_CELERY in settings to enable.\")\n        return {\"status\": NO_CONFIG}\n    start = datetime.now()\n    try:\n        # pylint: disable=no-member\n        app = celery.Celery('tasks')\n        app.config_from_object('django.conf:settings', namespace='CELERY')\n        # Make sure celery is connected with max_retries=1\n        # and not the default of max_retries=None if the connection\n        # is made lazily\n        app.connection().ensure_connection(max_retries=1)\n\n        celery_stats = celery.task.control.inspect().stats()\n        if not celery_stats:\n            log.error(\"No running Celery workers were found.\")\n            return {\"status\": DOWN, \"message\": \"No running Celery workers\"}\n    except Exception as exp:  # pylint: disable=broad-except\n        log.error(\"Error connecting to the backend: %s\", exp)\n        return {\"status\": DOWN, \"message\": \"Error connecting to the backend\"}\n    return {\"status\": UP, \"response_microseconds\": (datetime.now() - start).microseconds}", "category": "Python"}, {"instruction": "def remove_child_vaults(self, vault_id):\n        \"\"\"Removes all children from a vault.\n\n        arg:    vault_id (osid.id.Id): the ``Id`` of a vault\n        raise:  NotFound - ``vault_id`` is not in hierarchy\n        raise:  NullArgument - ``vault_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.BinHierarchyDesignSession.remove_child_bin_template\n        if self._catalog_session is not None:\n            return self._catalog_session.remove_child_catalogs(catalog_id=vault_id)\n        return self._hierarchy_session.remove_children(id_=vault_id)", "category": "Python"}, {"instruction": "def specAutoRange(self):\n        \"\"\"Auto adjusts the visible range of the spectrogram\"\"\"\n", "input": "", "output": "        trace_range = self.responsePlots.values()[0].viewRange()[0]\n        vb = self.specPlot.getViewBox()\n        vb.autoRange(padding=0)\n        self.specPlot.setXlim(trace_range)", "category": "Python"}, {"instruction": "def import_class(import_str):\n    \"\"\"Returns a class from a string including module and class.\"\"\"\n", "input": "", "output": "    mod_str, _sep, class_str = import_str.rpartition('.')\n    try:\n        __import__(mod_str)\n        return getattr(sys.modules[mod_str], class_str)\n    except (ValueError, AttributeError):\n        raise ImportError('Class %s cannot be found (%s)' %\n                          (class_str,\n                           traceback.format_exception(*sys.exc_info())))", "category": "Python"}, {"instruction": "def _vertex_list_to_sframe(ls, id_column_name):\n    \"\"\"\n    Convert a list of vertices into an SFrame.\n    \"\"\"\n", "input": "", "output": "    sf = SFrame()\n\n    if type(ls) == list:\n        cols = reduce(set.union, (set(v.attr.keys()) for v in ls))\n        sf[id_column_name] = [v.vid for v in ls]\n        for c in cols:\n            sf[c] = [v.attr.get(c) for v in ls]\n\n    elif type(ls) == Vertex:\n        sf[id_column_name] = [ls.vid]\n        for col, val in ls.attr.iteritems():\n            sf[col] = [val]\n\n    else:\n        raise TypeError('Vertices type {} is Not supported.'.format(type(ls)))\n\n    return sf", "category": "Python"}, {"instruction": "def is_installed(self, bug: Bug) -> bool:\n        \"\"\"\n        Determines whether the Docker image for a given bug has been installed\n        on the server.\n        \"\"\"\n", "input": "", "output": "        r = self.__api.get('bugs/{}/installed'.format(bug.name))\n\n        if r.status_code == 200:\n            answer = r.json()\n            assert isinstance(answer, bool)\n            return answer\n\n        # TODO bug not registered on server\n        if r.status_code == 404:\n            raise KeyError(\"no bug found with given name: {}\".format(bug.name))\n\n        self.__api.handle_erroneous_response(r)", "category": "Python"}, {"instruction": "def write_alf_params_(self):\n        \"\"\" DEPRECATED \"\"\"\n", "input": "", "output": "        if not hasattr(self, 'alf_dirs'):\n            self.make_alf_dirs()\n\n        if not hasattr(self, 'class_trees'):\n            self.generate_class_trees()\n\n        alf_params = {}\n        for k in range(self.num_classes):\n            alfdir = self.alf_dirs[k + 1]\n            tree = self.class_trees[k + 1]\n            datatype = self.datatype\n            name = 'class{0}'.format(k + 1)\n            num_genes = self.class_list[k]\n            seqlength = self.gene_length_min\n            gene_length_kappa = self.gene_length_kappa\n            gene_length_theta = self.gene_length_theta\n            alf_obj = ALF(tree=tree,\n                          datatype=datatype, num_genes=num_genes,\n                          seqlength=seqlength, gene_length_kappa=gene_length_kappa,\n                          gene_length_theta=gene_length_theta, name=name, tmpdir=alfdir)\n            if datatype == 'protein':\n                alf_obj.params.one_word_model('WAG')\n            else:\n                alf_obj.params.jc_model()\n            alf_params[k + 1] = alf_obj\n\n        self.alf_params = alf_params", "category": "Python"}, {"instruction": "def _add_rank_score(self, variant_obj, info_dict):\n        \"\"\"Add the rank score if found\n        \n        Args:\n            variant_obj (puzzle.models.Variant)\n            info_dict (dict): A info dictionary\n        \n        \"\"\"\n", "input": "", "output": "        rank_score_entry = info_dict.get('RankScore')\n        if rank_score_entry:\n            for family_annotation in rank_score_entry.split(','):\n                rank_score = family_annotation.split(':')[-1]\n            logger.debug(\"Updating rank_score to: {0}\".format(\n                rank_score))\n            variant_obj.rank_score = float(rank_score)", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"\n        Banana banana\n        \"\"\"\n", "input": "", "output": "        res = 0\n\n        self.project.setup()\n        self.__retrieve_all_projects(self.project)\n\n        self.link_resolver.get_link_signal.connect_after(self.__get_link_cb)\n        self.project.format(self.link_resolver, self.output)\n        self.project.write_out(self.output)\n\n        # Generating an XML sitemap makes no sense without a hostname\n        if self.hostname:\n            self.project.write_seo_sitemap(self.hostname, self.output)\n\n        self.link_resolver.get_link_signal.disconnect(self.__get_link_cb)\n\n        self.formatted_signal(self)\n        self.__persist()\n\n        return res", "category": "Python"}, {"instruction": "def get_token_network_events(\n        chain: BlockChainService,\n        token_network_address: Address,\n        contract_manager: ContractManager,\n        events: Optional[List[str]] = ALL_EVENTS,\n        from_block: BlockSpecification = GENESIS_BLOCK_NUMBER,\n        to_block: BlockSpecification = 'latest',\n) -> List[Dict]:\n    \"\"\" Helper to get all events of the ChannelManagerContract at `token_address`. \"\"\"\n", "input": "", "output": "\n    return get_contract_events(\n        chain,\n        contract_manager.get_contract_abi(CONTRACT_TOKEN_NETWORK),\n        token_network_address,\n        events,\n        from_block,\n        to_block,\n    )", "category": "Python"}, {"instruction": "def version():\n    '''\n    Shows installed version of dnsmasq.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' dnsmasq.version\n    '''\n", "input": "", "output": "    cmd = 'dnsmasq -v'\n    out = __salt__['cmd.run'](cmd).splitlines()\n    comps = out[0].split()\n    return comps[2]", "category": "Python"}, {"instruction": "def createWPPointer(self):\n        '''Creates the waypoint pointer relative to current heading.'''\n", "input": "", "output": "        self.headingWPTri = patches.RegularPolygon((0.0,0.55),3,0.05,facecolor='lime',zorder=4,ec='k')\n        self.axes.add_patch(self.headingWPTri)\n        self.headingWPText = self.axes.text(0.0,0.45,'1',color='lime',size=self.fontSize,horizontalalignment='center',verticalalignment='center',zorder=4)   \n        self.headingWPText.set_path_effects([PathEffects.withStroke(linewidth=1,foreground='k')])", "category": "Python"}, {"instruction": "def _set_properties(self):\n        \"\"\"Setup title, size and tooltips\"\"\"\n", "input": "", "output": "\n        self.codetext_ctrl.SetToolTipString(_(\"Enter python code here.\"))\n        self.apply_button.SetToolTipString(_(\"Apply changes to current macro\"))\n        self.splitter.SetBackgroundStyle(wx.BG_STYLE_COLOUR)\n        self.result_ctrl.SetMinSize((10, 10))", "category": "Python"}, {"instruction": "def _new_render(response):\n    \"\"\"\n    Decorator for the TemplateResponse.render() function\n    \"\"\"\n", "input": "", "output": "    orig_render = response.__class__.render\n\n    # No arguments, is used as bound method.\n    def _inner_render():\n        try:\n            return orig_render(response)\n        except HttpRedirectRequest as e:\n            return HttpResponseRedirect(e.url, status=e.status)\n\n    return _inner_render", "category": "Python"}, {"instruction": "def hash(*cols):\n    \"\"\"Calculates the hash code of given columns, and returns the result as an int column.\n\n    >>> spark.createDataFrame([('ABC',)], ['a']).select(hash('a').alias('hash')).collect()\n    [Row(hash=-757602832)]\n    \"\"\"\n", "input": "", "output": "    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.hash(_to_seq(sc, cols, _to_java_column))\n    return Column(jc)", "category": "Python"}, {"instruction": "def save_config(config):\n    \"\"\"Save configuration file to users data location.\n\n     - Linux: ~/.local/share/pyfilemail\n     - OSX: ~/Library/Application Support/pyfilemail\n     - Windows: C:\\\\\\Users\\\\\\{username}\\\\\\AppData\\\\\\Local\\\\\\pyfilemail\n\n     :rtype: str\n    \"\"\"\n", "input": "", "output": "\n    configfile = get_configfile()\n\n    if not os.path.exists(configfile):\n        configdir = os.path.dirname(configfile)\n\n        if not os.path.exists(configdir):\n            os.makedirs(configdir)\n\n    data = config\n\n    with open(configfile, 'wb') as f:\n        json.dump(data, f, indent=2)", "category": "Python"}, {"instruction": "def check(self, url_data):\n        \"\"\"Check content for invalid anchors.\"\"\"\n", "input": "", "output": "        log.debug(LOG_PLUGIN, \"checking content for invalid anchors\")\n        # list of parsed anchors\n        self.anchors = []\n        find_links(url_data, self.add_anchor, linkparse.AnchorTags)\n        self.check_anchor(url_data)", "category": "Python"}, {"instruction": "def alloc_vpcid(nexus_ips):\n    \"\"\"Allocate a vpc id for the given list of switch_ips.\"\"\"\n", "input": "", "output": "\n    LOG.debug(\"alloc_vpc() called\")\n\n    vpc_id = 0\n    intersect = _get_free_vpcids_on_switches(nexus_ips)\n    for intersect_tuple in intersect:\n        try:\n            update_vpc_entry(nexus_ips, intersect_tuple.vpc_id,\n                             False, True)\n            vpc_id = intersect_tuple.vpc_id\n            break\n        except Exception:\n            LOG.exception(\n                \"This exception is expected if another controller \"\n                \"beat us to vpcid %(vpcid)s for nexus %(ip)s\",\n                {'vpcid': intersect_tuple.vpc_id,\n                 'ip': ', '.join(map(str, nexus_ips))})\n\n    return vpc_id", "category": "Python"}, {"instruction": "def create_requests(self, request_to_create):\n        \"\"\"CreateRequests.\n        [Preview API] Create a new symbol request.\n        :param :class:`<Request> <azure.devops.v5_0.symbol.models.Request>` request_to_create: The symbol request to create.\n        :rtype: :class:`<Request> <azure.devops.v5_0.symbol.models.Request>`\n        \"\"\"\n", "input": "", "output": "        content = self._serialize.body(request_to_create, 'Request')\n        response = self._send(http_method='POST',\n                              location_id='ebc09fe3-1b20-4667-abc5-f2b60fe8de52',\n                              version='5.0-preview.1',\n                              content=content)\n        return self._deserialize('Request', response)", "category": "Python"}, {"instruction": "def register_cache(self, cache_group):\n        \"\"\"\n        Register a cache_group with this manager.\n\n        Use this method to register more complicated\n        groups that you create yourself. Such as if you\n        need to register several models each with different\n        parameters.\n\n        :param cache_group: The group to register. \\\n        The group is registered with the cache_group key attribute. \\\n        Raises an exception if the key is already registered.\n        \"\"\"\n", "input": "", "output": "\n        if cache_group.key in self._registry:\n            raise Exception(\"%s is already registered\" % cache_group.key)\n        self._registry[cache_group.key] = cache_group", "category": "Python"}, {"instruction": "def new_eventtype(self, test_type_str=None):\n        \"\"\"Action: create dialog to add new event type.\"\"\"\n", "input": "", "output": "        if self.annot is None:  # remove if buttons are disabled\n            self.parent.statusBar().showMessage('No score file loaded')\n            return\n\n        if test_type_str:\n            answer = test_type_str, True\n        else:\n            answer = QInputDialog.getText(self, 'New Event Type',\n                                          'Enter new event\\'s name')\n        if answer[1]:\n            self.annot.add_event_type(answer[0])\n            self.display_eventtype()\n            n_eventtype = self.idx_eventtype.count()\n            self.idx_eventtype.setCurrentIndex(n_eventtype - 1)\n            \n        return answer", "category": "Python"}, {"instruction": "def save(self, path):\n        ''' Save source video to file.\n\n        Args:\n            path (str): Filename to save to.\n\n        Notes: Saves entire source video to file, not just currently selected\n            frames.\n        '''\n", "input": "", "output": "        # IMPORTANT WARNING: saves entire source video\n        self.clip.write_videofile(path, audio_fps=self.clip.audio.fps)", "category": "Python"}, {"instruction": "def length(self, chain=-1):\n        \"\"\"Return the length of the trace.\n\n        :Parameters:\n        chain : int or None\n          The chain index. If None, returns the combined length of all chains.\n        \"\"\"\n", "input": "", "output": "        if chain is not None:\n            if chain < 0:\n                chain = range(self.db.chains)[chain]\n            return self._trace[chain].shape[0]\n        else:\n            return sum([t.shape[0] for t in self._trace.values()])", "category": "Python"}, {"instruction": "def get_prefix(self, key_prefix, **kwargs):\n        \"\"\"\n        Get a range of keys with a prefix.\n\n        :param key_prefix: first key in range\n        :param keys_only: if True, retrieve only the keys, not the values\n\n        :returns: sequence of (value, metadata) tuples\n        \"\"\"\n", "input": "", "output": "        range_response = self.get_prefix_response(key_prefix, **kwargs)\n        for kv in range_response.kvs:\n            yield (kv.value, KVMetadata(kv, range_response.header))", "category": "Python"}, {"instruction": "def enabled(self):\n        \"\"\"Returns True if this notification is enabled based on the value\n        of Notification model instance.\n\n        Note: Notification names/display_names are persisted in the\n        \"Notification\" model where each mode instance can be flagged\n        as enabled or not, and are selected/subscribed to by\n        each user in their user profile.\n\n        See also: `site_notifications.update_notification_list`\n        \"\"\"\n", "input": "", "output": "        if not self._notification_enabled:\n            self._notification_enabled = self.notification_model.enabled\n        return self._notification_enabled", "category": "Python"}, {"instruction": "def node_to_ini(node, output=sys.stdout):\n    \"\"\"\n    Convert a Node object with the right structure into a .ini file.\n\n    :params node: a Node object\n    :params output: a file-like object opened in write mode\n    \"\"\"\n", "input": "", "output": "    for subnode in node:\n        output.write(u'\\n[%s]\\n' % subnode.tag)\n        for name, value in sorted(subnode.attrib.items()):\n            output.write(u'%s=%s\\n' % (name, value))\n    output.flush()", "category": "Python"}, {"instruction": "def create_edge(self, from_doc, to_doc, edge_data={}):\n        \"\"\"\n            Creates edge document.\n\n            :param from_doc Document from which the edge comes\n            :param to_doc Document to which the edge goes\n            :param edge_data Extra data for the edge\n\n            :returns Document\n        \"\"\"\n", "input": "", "output": "\n        return Edge.create(\n            collection=self,\n            from_doc=from_doc,\n            to_doc=to_doc,\n            edge_data=edge_data\n        )", "category": "Python"}, {"instruction": "def zDDEClose(self):\n        \"\"\"Close the DDE link with Zemax server\"\"\"\n", "input": "", "output": "        if _PyZDDE.server and not _PyZDDE.liveCh:\n            _PyZDDE.server.Shutdown(self.conversation)\n            _PyZDDE.server = 0\n        elif _PyZDDE.server and self.connection and _PyZDDE.liveCh == 1:\n            _PyZDDE.server.Shutdown(self.conversation)\n            self.connection = False\n            self.appName = ''\n            _PyZDDE.liveCh -= 1  \n            _PyZDDE.server = 0  \n        elif self.connection:  \n            _PyZDDE.server.Shutdown(self.conversation)\n            self.connection = False\n            self.appName = ''\n            _PyZDDE.liveCh -= 1\n        return 0", "category": "Python"}, {"instruction": "def parse_inline(self, text):\n        \"\"\"Parses text into inline elements.\n        RawText is not considered in parsing but created as a wrapper of holes\n        that don't match any other elements.\n\n        :param text: the text to be parsed.\n        :returns: a list of inline elements.\n        \"\"\"\n", "input": "", "output": "        element_list = self._build_inline_element_list()\n        return inline_parser.parse(\n            text, element_list, fallback=self.inline_elements['RawText']\n        )", "category": "Python"}, {"instruction": "def _increment_current_byte(self):\n        \"\"\"\n        Increments the value of the current byte at the pointer. If the result is over 255,\n        then it will overflow to 0\n        \"\"\"\n", "input": "", "output": "        # If the current byte is uninitialized, then incrementing it will make it 1\n        if self.tape[self.pointer] is None:\n            self.tape[self.pointer] = 1\n        elif self.tape[self.pointer] == self.MAX_CELL_SIZE:  # If the current byte is already at the max, then overflow\n            self.tape[self.pointer] = self.MIN_CELL_SIZE\n        else:  # increment it\n            self.tape[self.pointer] += 1", "category": "Python"}, {"instruction": "def memoize(function):\n    \"\"\"A very simple memoize decorator to optimize pure-ish functions\n\n    Don't use this unless you've examined the code and see the\n    potential risks.\n    \"\"\"\n", "input": "", "output": "    cache = {}\n    @functools.wraps(function)\n    def _memoize(*args):\n        if args in cache:\n            return cache[args]\n        result = function(*args)\n        cache[args] = result\n        return result\n    return function", "category": "Python"}, {"instruction": "def reset(self):\n        \"\"\"\n        Reset the progressbar to 0, hide it and set original text message at\n        background.\n        \"\"\"\n", "input": "", "output": "        self.hide()\n        self.tag.class_name = \"progress-bar progress-bar-striped active\"\n        self.tag.aria_valuemin = 0\n        self.tag.style.width = \"{}%\".format(0)\n        self.tag.text = self.original_message", "category": "Python"}, {"instruction": "def build_ebound_table(self):\n        \"\"\" Build and return an EBOUNDS table with the encapsulated data.\n        \"\"\"\n", "input": "", "output": "        cols = [\n            Column(name=\"E_MIN\", dtype=float, data=self._emin, unit='MeV'),\n            Column(name=\"E_MAX\", dtype=float, data=self._emax, unit='MeV'),\n            Column(name=\"E_REF\", dtype=float, data=self._eref, unit='MeV'),\n            Column(name=\"REF_DNDE\", dtype=float, data=self._ref_dnde,\n                   unit='ph / (MeV cm2 s)'),\n            Column(name=\"REF_FLUX\", dtype=float, data=self._ref_flux,\n                   unit='ph / (cm2 s)'),\n            Column(name=\"REF_EFLUX\", dtype=float, data=self._ref_eflux,\n                   unit='MeV / (cm2 s)'),\n            Column(name=\"REF_NPRED\", dtype=float, data=self._ref_npred,\n                   unit='ph')\n        ]\n        tab = Table(data=cols)\n        return tab", "category": "Python"}, {"instruction": "def gen_table_key(self, table, db='default'):\n        \"\"\"\n        Returns a key that is standard for a given table name and database\n        alias. Total length up to 212 (max for memcache is 250).\n        \"\"\"\n", "input": "", "output": "        table = force_text(table)\n        db = force_text(settings.DB_CACHE_KEYS[db])\n        if len(table) > 100:\n            table = table[0:68] + self.gen_key(table[68:])\n        if db and len(db) > 100:\n            db = db[0:68] + self.gen_key(db[68:])\n        return '%s_%s_table_%s' % (self.prefix, db, table)", "category": "Python"}, {"instruction": "def icon_cache_key(method, self, brain_or_object):\n    \"\"\"Generates a cache key for the icon lookup\n\n    Includes the virtual URL to handle multiple HTTP/HTTPS domains\n    Example: http://senaite.local/clients?modified=1512033263370\n    \"\"\"\n", "input": "", "output": "    url = api.get_url(brain_or_object)\n    modified = api.get_modification_date(brain_or_object).millis()\n    key = \"{}?modified={}\".format(url, modified)\n    logger.debug(\"Generated Cache Key: {}\".format(key))\n    return key", "category": "Python"}, {"instruction": "def __gather_avail(self):\n        '''\n        Gather the lists of available sls data from the master\n        '''\n", "input": "", "output": "        avail = {}\n        for saltenv in self._get_envs():\n            avail[saltenv] = self.client.list_states(saltenv)\n        return avail", "category": "Python"}, {"instruction": "def fitted_function(self):\n        \"\"\"\n        A function of the single independent variable after\n        partially evaluating `scipy_data_fitting.Fit.function` at\n        the `scipy_data_fitting.Fit.fitted_parameters`.\n        \"\"\"\n", "input": "", "output": "        function = self.function\n        fitted_parameters = self.fitted_parameters\n        return lambda x: function(x, *fitted_parameters)", "category": "Python"}, {"instruction": "def _initialize_context(self, trace_header):\n        \"\"\"\n        Create a facade segment based on environment variables\n        set by AWS Lambda and initialize storage for subsegments.\n        \"\"\"\n", "input": "", "output": "        sampled = None\n        if not global_sdk_config.sdk_enabled():\n            # Force subsequent subsegments to be disabled and turned into DummySegments.\n            sampled = False\n        elif trace_header.sampled == 0:\n            sampled = False\n        elif trace_header.sampled == 1:\n            sampled = True\n\n        segment = FacadeSegment(\n            name='facade',\n            traceid=trace_header.root,\n            entityid=trace_header.parent,\n            sampled=sampled,\n        )\n        setattr(self._local, 'segment', segment)\n        setattr(self._local, 'entities', [])", "category": "Python"}, {"instruction": "def avg_rate(self):\n        \"\"\"Average receiving rate in MB/s over the entire run.\n\n        If the result is not from a success run, this property is None.\n        \"\"\"\n", "input": "", "output": "        if not self._has_data or 'sum' not in self.result['end']:\n            return None\n        bps = self.result['end']['sum']['bits_per_second']\n        return bps / 8 / 1024 / 1024", "category": "Python"}, {"instruction": "def get_provider(name, creds):\n    \"\"\"\n    Generates and memoizes a :class:`~bang.providers.provider.Provider` object\n    for the given name.\n\n    :param str name:  The provider name, as given in the config stanza.  This\n        token is used to find the\n        appropriate :class:`~bang.providers.provider.Provider`.\n\n    :param dict creds:  The credentials dictionary that is appropriate for the\n        desired provider.  Typically, a sub-dict from the main stack config.\n\n    :rtype:  :class:`~bang.providers.provider.Provider`\n\n    \"\"\"\n", "input": "", "output": "    p = _PROVIDERS.get(name)\n    if not p:\n        provider = PROVIDER_MAP.get(name)\n        if not provider:\n            if name == 'hpcloud':\n                print \"## Warning - 'hpcloud' is not currently supported as\" \\\n                    \"a provider; use hpcloud_v12 or hpcloud_v13. See \" \\\n                    \"release notes.\"\n            raise Exception(\"No provider matches %s; check imports\" % name)\n        p = provider(creds)\n        _PROVIDERS[name] = p\n    return p", "category": "Python"}, {"instruction": "def recon_all(subj_id,anatomies):\n    '''Run the ``recon_all`` script'''\n", "input": "", "output": "    if not environ_setup:\n        setup_freesurfer()\n    if isinstance(anatomies,basestring):\n        anatomies = [anatomies]\n    nl.run([os.path.join(freesurfer_home,'bin','recon-all'),'-all','-subjid',subj_id] + [['-i',anat] for anat in anatomies])", "category": "Python"}, {"instruction": "def device_characteristics_str(self, indent):\n        \"\"\"Convenience to string method.\n        \"\"\"\n", "input": "", "output": "        s = \"{}\\n\".format(self.label)\n        s += indent + \"MAC Address: {}\\n\".format(self.mac_addr)\n        s += indent + \"IP Address: {}\\n\".format(self.ip_addr)\n        s += indent + \"Port: {}\\n\".format(self.port)\n        s += indent + \"Power: {}\\n\".format(str_map(self.power_level))\n        s += indent + \"Location: {}\\n\".format(self.location)\n        s += indent + \"Group: {}\\n\".format(self.group)\n        return s", "category": "Python"}, {"instruction": "def range_is_obj(rng, rdfclass):\n    \"\"\" Test to see if range for the class should be an object\n    or a litteral\n    \"\"\"\n", "input": "", "output": "    if rng == 'rdfs_Literal':\n        return False\n    if hasattr(rdfclass, rng):\n        mod_class = getattr(rdfclass, rng)\n        for item in mod_class.cls_defs['rdf_type']:\n            try:\n                if issubclass(getattr(rdfclass, item),\n                              rdfclass.rdfs_Literal):\n                    return False\n            except AttributeError:\n                pass\n        if isinstance(mod_class, rdfclass.RdfClassMeta):\n            return True\n    return False", "category": "Python"}, {"instruction": "def _get_file_alignment_for_new_binary_file(self, file: File) -> int:\n        \"\"\"Detects alignment requirements for binary files with new nn::util::BinaryFileHeader.\"\"\"\n", "input": "", "output": "        if len(file.data) <= 0x20:\n            return 0\n        bom = file.data[0xc:0xc+2]\n        if bom != b'\\xff\\xfe' and bom != b'\\xfe\\xff':\n            return 0\n\n        be = bom == b'\\xfe\\xff'\n        file_size: int = struct.unpack_from(_get_unpack_endian_character(be) + 'I', file.data, 0x1c)[0]\n        if len(file.data) != file_size:\n            return 0\n        return 1 << file.data[0xe]", "category": "Python"}, {"instruction": "def _get_replica_set_members(self, selector):\n        \"\"\"Return set of replica set member addresses.\"\"\"\n", "input": "", "output": "        # Implemented here in Topology instead of MongoClient, so it can lock.\n        with self._lock:\n            topology_type = self._description.topology_type\n            if topology_type not in (TOPOLOGY_TYPE.ReplicaSetWithPrimary,\n                                     TOPOLOGY_TYPE.ReplicaSetNoPrimary):\n                return set()\n\n            return set([sd.address for sd in selector(self._new_selection())])", "category": "Python"}, {"instruction": "def approvecommittee(ctx, members, account):\n    \"\"\" Approve committee member(s)\n    \"\"\"\n", "input": "", "output": "    print_tx(ctx.bitshares.approvecommittee(members, account=account))", "category": "Python"}, {"instruction": "def applet_get_details(object_id, input_params={}, always_retry=True, **kwargs):\n    \"\"\"\n    Invokes the /applet-xxxx/getDetails API method.\n\n    For more info, see: https://wiki.dnanexus.com/API-Specification-v1.0.0/Details-and-Links#API-method%3A-%2Fclass-xxxx%2FgetDetails\n    \"\"\"\n", "input": "", "output": "    return DXHTTPRequest('/%s/getDetails' % object_id, input_params, always_retry=always_retry, **kwargs)", "category": "Python"}, {"instruction": "def watch(self, *keys):\n        \"\"\"\n        Put the pipeline into immediate execution mode.\n        Does not actually watch any keys.\n        \"\"\"\n", "input": "", "output": "        if self.explicit_transaction:\n            raise RedisError(\"Cannot issue a WATCH after a MULTI\")\n        self.watching = True\n        for key in keys:\n            self._watched_keys[key] = deepcopy(self.mock_redis.redis.get(self.mock_redis._encode(key)))", "category": "Python"}, {"instruction": "def _present_perm_set_for_subj(self, perm_dict, subj_str):\n        \"\"\"Return a set containing only the permissions that are present in the\n        ``perm_dict`` for ``subj_str``\"\"\"\n", "input": "", "output": "        return {p for p, s in list(perm_dict.items()) if subj_str in s}", "category": "Python"}, {"instruction": "def main():\n    \"\"\"Build and parse command line.\"\"\"\n", "input": "", "output": "    parser = argument_parser(version=__version__)\n    args = parser.parse_args()\n\n    if args.debug:\n        log_level = logging.DEBUG\n    elif args.verbose:\n        log_level = logging.INFO\n    else:\n        log_level = logging.WARNING\n\n    logging.basicConfig(format='%(levelname)s: %(message)s', level=log_level)\n\n    max_retries = args.retries\n    attempts = 0\n    ret = args_download(args)\n    while ret == 75 and attempts < max_retries:\n        attempts += 1\n        logging.error(\n            'Downloading from NCBI failed due to a connection error, retrying. Retries so far: %s',\n            attempts)\n        ret = args_download(args)\n\n    return ret", "category": "Python"}, {"instruction": "def call_cc(fn: Callable) -> 'Cont':\n        r\"\"\"call-with-current-continuation.\n\n        Haskell: callCC f = Cont $ \\c -> runCont (f (\\a -> Cont $ \\_ -> c a )) c\n        \"\"\"\n", "input": "", "output": "        return Cont(lambda c: fn(lambda a: Cont(lambda _: c(a))).run(c))", "category": "Python"}, {"instruction": "def sky_to_offset(skydir, lon, lat, coordsys='CEL', projection='AIT'):\n    \"\"\"Convert sky coordinates to a projected offset.  This function\n    is the inverse of offset_to_sky.\"\"\"\n", "input": "", "output": "\n    w = create_wcs(skydir, coordsys, projection)\n    skycrd = np.vstack((lon, lat)).T\n\n    if len(skycrd) == 0:\n        return skycrd\n\n    return w.wcs_world2pix(skycrd, 0)", "category": "Python"}, {"instruction": "def height(poly):\n    \"\"\"height\"\"\"\n", "input": "", "output": "    num = len(poly)\n    hgt = 0.0\n    for i in range(num):\n        hgt += (poly[i][2])\n    return hgt/num", "category": "Python"}, {"instruction": "def get_all_parcels(self, view = None):\n    \"\"\"\n    Get all parcels in this cluster.\n\n    @return: A list of ApiParcel objects.\n    \"\"\"\n", "input": "", "output": "    return parcels.get_all_parcels(self._get_resource_root(), self.name, view)", "category": "Python"}, {"instruction": "def rectangle_geo_array(rectangle, map_canvas):\n    \"\"\"Obtain the rectangle in EPSG:4326.\n\n    :param rectangle: A rectangle instance.\n    :type rectangle: QgsRectangle\n\n    :param map_canvas: A map canvas instance.\n    :type map_canvas: QgsMapCanvas\n\n    :returns: A list in the form [xmin, ymin, xmax, ymax] where all\n        coordinates provided are in Geographic / EPSG:4326.\n    :rtype: list\n\n    .. note:: Delegates to extent_to_array()\n    \"\"\"\n", "input": "", "output": "\n    destination_crs = QgsCoordinateReferenceSystem()\n    destination_crs.createFromSrid(4326)\n\n    source_crs = map_canvas.mapSettings().destinationCrs()\n\n    return extent_to_array(rectangle, source_crs, destination_crs)", "category": "Python"}, {"instruction": "def b58encode(v: bytes) -> str:\n    '''Encode a string using Base58'''\n", "input": "", "output": "\n    origlen = len(v)\n    v = v.lstrip(b'\\0')\n    newlen = len(v)\n\n    p, acc = 1, 0\n    for c in iseq(v[::-1]):\n        acc += p * c\n        p = p << 8\n\n    result = ''\n    while acc > 0:\n        acc, mod = divmod(acc, 58)\n        result += alphabet[mod]\n\n    return (result + alphabet[0] * (origlen - newlen))[::-1]", "category": "Python"}, {"instruction": "def calc_rho_and_rho_bar_squared(final_log_likelihood,\n                                 null_log_likelihood,\n                                 num_est_parameters):\n    \"\"\"\n    Calculates McFadden's rho-squared and rho-bar squared for the given model.\n\n    Parameters\n    ----------\n    final_log_likelihood : float.\n        The final log-likelihood of the model whose rho-squared and rho-bar\n        squared are being calculated for.\n    null_log_likelihood : float.\n        The log-likelihood of the model in question, when all parameters are\n        zero or their 'base' values.\n    num_est_parameters : int.\n        The number of parameters estimated in this model.\n\n    Returns\n    -------\n    `(rho_squared, rho_bar_squared)` : tuple of floats.\n        The rho-squared and rho-bar-squared for the model.\n    \"\"\"\n", "input": "", "output": "    rho_squared = 1.0 - final_log_likelihood / null_log_likelihood\n    rho_bar_squared = 1.0 - ((final_log_likelihood - num_est_parameters) /\n                             null_log_likelihood)\n\n    return rho_squared, rho_bar_squared", "category": "Python"}, {"instruction": "def rank_all(self,roots,optimize=False):\n        \"\"\"Computes rank of all vertices.\n        add provided roots to rank 0 vertices,\n        otherwise update ranking from provided roots.\n        The initial rank is based on precedence relationships,\n        optimal ranking may be derived from network flow (simplex).\n        \"\"\"\n", "input": "", "output": "        self._edge_inverter()\n        r = [x for x in self.g.sV if (len(x.e_in())==0 and x not in roots)]\n        self._rank_init(roots+r)\n        if optimize: self._rank_optimize()\n        self._edge_inverter()", "category": "Python"}, {"instruction": "def generate_id(self):\n        \"\"\"Generate a fresh id\"\"\"\n", "input": "", "output": "        if self.use_repeatable_ids:\n            self.repeatable_id_counter += 1\n            return 'autobaked-{}'.format(self.repeatable_id_counter)\n        else:\n            return str(uuid4())", "category": "Python"}, {"instruction": "def AddRow(self, values):\n    \"\"\"Adds a row of values.\n\n    Args:\n      values (list[object]): values.\n\n    Raises:\n      ValueError: if the number of values is out of bounds.\n    \"\"\"\n", "input": "", "output": "    if self._number_of_columns and len(values) != self._number_of_columns:\n      raise ValueError('Number of values is out of bounds.')\n\n    self._rows.append(values)\n\n    if not self._number_of_columns:\n      self._number_of_columns = len(values)", "category": "Python"}, {"instruction": "def get(self, key, sort_key):\n        \"\"\" Get an element in dictionary \"\"\"\n", "input": "", "output": "        key = self.prefixed('{}:{}'.format(key, sort_key))\n        self.logger.debug('Storage - get {}'.format(key))\n        if key not in self.cache.keys():\n            return None\n        return self.cache[key]", "category": "Python"}, {"instruction": "def get_vmpolicy_macaddr_output_vmpolicy_macaddr_mac(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_vmpolicy_macaddr = ET.Element(\"get_vmpolicy_macaddr\")\n        config = get_vmpolicy_macaddr\n        output = ET.SubElement(get_vmpolicy_macaddr, \"output\")\n        vmpolicy_macaddr = ET.SubElement(output, \"vmpolicy-macaddr\")\n        mac = ET.SubElement(vmpolicy_macaddr, \"mac\")\n        mac.text = kwargs.pop('mac')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def set_bracket_matcher_color_scheme(self, color_scheme):\n        \"\"\"Set color scheme for matched parentheses.\"\"\"\n", "input": "", "output": "        bsh = sh.BaseSH(parent=self, color_scheme=color_scheme)\n        mpcolor = bsh.get_matched_p_color()\n        self._bracket_matcher.format.setBackground(mpcolor)", "category": "Python"}, {"instruction": "def deleteMember(self, address, id, headers=None, query_params=None, content_type=\"application/json\"):\n        \"\"\"\n        Delete member from network\n        It is method for DELETE /network/{id}/member/{address}\n        \"\"\"\n", "input": "", "output": "        uri = self.client.base_url + \"/network/\"+id+\"/member/\"+address\n        return self.client.delete(uri, None, headers, query_params, content_type)", "category": "Python"}, {"instruction": "def prep_for_graph(data_frame, series=None, delta_series=None, smoothing=None,\n                   outlier_stddev=None):\n    \"\"\"Prepare a dataframe for graphing by calculating deltas for\n    series that need them, resampling, and removing outliers.\n    \"\"\"\n", "input": "", "output": "    series = series or []\n    delta_series = delta_series or []\n    graph = calc_deltas(data_frame, delta_series)\n\n    for s in series + delta_series:\n        if smoothing:\n            graph[s] = graph[s].resample(smoothing)\n        if outlier_stddev:\n            graph[s] = remove_outliers(graph[s], outlier_stddev)\n\n    return graph[series + delta_series]", "category": "Python"}, {"instruction": "def pre_save(cls, sender, instance, *args, **kwargs):\n        \"\"\"Pull constant_contact_id out of data.\n        \"\"\"\n", "input": "", "output": "        instance.constant_contact_id = str(instance.data['id'])", "category": "Python"}, {"instruction": "def verify(self, obj):\n        \"\"\"Verify that the object conforms to this verifier's schema\n\n        Args:\n            obj (object): A python object to verify\n\n        Raises:\n            ValidationError: If there is a problem verifying the dictionary, a\n                ValidationError is thrown with at least the reason key set indicating\n                the reason for the lack of validation.\n        \"\"\"\n", "input": "", "output": "\n        if len(self._options) == 0:\n            raise ValidationError(\"No options\", reason='no options given in options verifier, matching not possible',\n                                  object=obj)\n\n        exceptions = {}\n\n        for i, option in enumerate(self._options):\n            try:\n                obj = option.verify(obj)\n                return obj\n            except ValidationError as exc:\n                exceptions['option_%d' % (i+1)] = exc.params['reason']\n\n        raise ValidationError(\"Object did not match any of a set of options\",\n                              reason=\"object did not match any given option (first failure = '%s')\"\n                                     % exceptions['option_1'], **exceptions)", "category": "Python"}, {"instruction": "def minimum_image_dr( self, r1, r2, cutoff=None ):\n        \"\"\"\n        Calculate the shortest distance between two points in the cell, \n        accounting for periodic boundary conditions.\n\n        Args:\n            r1 (np.array): fractional coordinates of point r1.\n            r2 (np.array): fractional coordinates of point r2.\n            cutoff (:obj: `float`, optional): if set, return zero if the minimum distance is greater than `cutoff`. Defaults to None.\n\n        Returns:\n            (float): The distance between r1 and r2.\n        \"\"\"\n", "input": "", "output": "        delta_r_vector = self.minimum_image( r1, r2 )\n        return( self.dr( np.zeros( 3 ), delta_r_vector, cutoff ) )", "category": "Python"}, {"instruction": "def update_qa(quietly=False):\n    \"\"\"\n        Merge code from develop to qa\n    \"\"\"\n", "input": "", "output": "    switch('dev')\n    switch('qa')\n    local('git merge --no-edit develop')\n    local('git push')\n    if not quietly:\n        print(red('PLEASE DEPLOY CODE: fab deploy:all'))", "category": "Python"}, {"instruction": "def find_file(pattern, path):\n    \"\"\"\n    A bit like grep. Finds a pattern, looking in path. Returns the filename.\n    \"\"\"\n", "input": "", "output": "    for fname in glob.iglob(path):\n        with open(fname) as f:\n            if re.search(pattern, f.read()):\n                return fname\n    return", "category": "Python"}, {"instruction": "def _encoded_routing_key(self):\n        \"\"\"The encoded routing key used to publish the message on the broker.\"\"\"\n", "input": "", "output": "        topic = self.topic\n        if config.conf[\"topic_prefix\"]:\n            topic = \".\".join((config.conf[\"topic_prefix\"].rstrip(\".\"), topic))\n        return topic.encode(\"utf-8\")", "category": "Python"}, {"instruction": "def _match_type(self, i):\n        \"\"\"Looks at line 'i' to see if the line matches a module user type def.\"\"\"\n", "input": "", "output": "        self.col_match = self.RE_TYPE.match(self._source[i])\n        if self.col_match is not None:\n            self.section = \"types\"\n            self.el_type = CustomType\n            self.el_name = self.col_match.group(\"name\")\n           \n            return True\n        else:\n            return False", "category": "Python"}, {"instruction": "def select(self, neurons):\n        \"\"\"\n        Select spike trains.\n\n\n        Parameters\n        ----------\n        neurons : numpy.ndarray or list\n            Array of list of neurons.\n\n\n        Returns\n        -------\n        list\n            List of numpy.ndarray objects containing spike times.\n\n\n        See also\n        --------\n        sqlite3.connect.cursor\n        \n        \"\"\"\n", "input": "", "output": "        s = []\n        for neuron in neurons:\n            self.cursor.execute('SELECT time FROM spikes where neuron = %d' % neuron)\n            sel = self.cursor.fetchall()\n            spikes = np.array(sel).flatten()\n            s.append(spikes)\n        return s", "category": "Python"}, {"instruction": "def useFahlmanActivationFunction(self):\n        \"\"\"\n        Change the network to use Fahlman's default activation function for all layers.\n        Must be called after all layers have been added.\n        \"\"\"\n", "input": "", "output": "        self.activationFunction = self.activationFunctionFahlman\n        self.ACTPRIME = self.ACTPRIME_Fahlman\n        self.actDeriv = self.actDerivFahlman\n        for layer in self:\n            layer.minTarget, layer.minActivation = -0.5, -0.5 \n            layer.maxTarget, layer.maxActivation = 0.5, 0.5", "category": "Python"}, {"instruction": "def normalize_cell_value(value):\n    \"\"\"Process value for writing into a cell.\n\n    Args:\n        value: any type of variable\n\n    Returns:\n        json serialized value if value is list or dict, else value\n    \"\"\"\n", "input": "", "output": "    if isinstance(value, dict) or isinstance(value, list):\n        return json.dumps(value)\n    return value", "category": "Python"}, {"instruction": "def no_moves(position):\n    \"\"\"\n    Finds if the game is over.\n\n    :type: position: Board\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "    return position.no_moves(color.white) \\\n        or position.no_moves(color.black)", "category": "Python"}, {"instruction": "def is_mention_line(cls, word):\n        \"\"\" Detects links and mentions\n\n            :param word: Token to be evaluated\n        \"\"\"\n", "input": "", "output": "        if word.startswith('@'):\n            return True\n        elif word.startswith('http://'):\n            return True\n        elif word.startswith('https://'):\n            return True\n        else:\n            return False", "category": "Python"}, {"instruction": "def use_isolated_vault_view(self):\n        \"\"\"Pass through to provider AuthorizationLookupSession.use_isolated_vault_view\"\"\"\n", "input": "", "output": "        self._vault_view = ISOLATED\n        # self._get_provider_session('authorization_lookup_session') # To make sure the session is tracked\n        for session in self._get_provider_sessions():\n            try:\n                session.use_isolated_vault_view()\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def rect(self):\n        \"\"\"rect(self) -> PyObject *\"\"\"\n", "input": "", "output": "        val = _fitz.DisplayList_rect(self)\n        val = Rect(val)\n\n        return val", "category": "Python"}, {"instruction": "def set_column_prop(self, prop, values, repeat=\"up\"):\n        \"\"\"\n        Specify the properties of the columns\n\n        :param values:\n        :param repeat: if 'up' then duplicate up the structure\n        :return:\n        \"\"\"\n", "input": "", "output": "        values = np.array(values)\n        if repeat == \"up\":\n            assert len(values.shape) == 1\n            values = [values for ss in range(self.n_storeys)]\n        else:\n            assert len(values.shape) == 2\n        if len(values[0]) != self.n_cols:\n            raise ModelError(\"column props does not match n_cols (%i).\" % self.n_cols)\n        for ss in range(self.n_storeys):\n            for i in range(self.n_cols):\n                self._columns[ss][i].set_section_prop(prop, values[0][i])", "category": "Python"}, {"instruction": "def list_of_list(self):\n        \"\"\"\n        This will convert the data from a list of dict to a list of list\n        :return: list of dict\n        \"\"\"\n", "input": "", "output": "        ret = [[row.get(key, '') for key in self._col_names] for row in self]\n        return ReprListList(ret, col_names=self._col_names,\n                            col_types=self._col_types,\n                            width_limit=self._width_limit,\n                            digits=self._digits,\n                            convert_unicode=self._convert_unicode)", "category": "Python"}, {"instruction": "def vMh2_to_m2Lambda(v, Mh2, C, scale_high):\n    \"\"\"Function to numerically determine the parameters of the Higgs potential\n    given the physical Higgs VEV and mass.\"\"\"\n", "input": "", "output": "    if C['phi'] == 0 and C['phiBox'] == 0 and C['phiD'] == 0:\n        return _vMh2_to_m2Lambda_SM(v, Mh2)\n    else:\n        def f0(x):  # we want the root of this function\n            m2, Lambda = x\n            d = m2Lambda_to_vMh2(m2=m2.real, Lambda=Lambda.real,\n                                 C=C, scale_high=scale_high)\n            return np.array([d['v'] - v, d['Mh2'] - Mh2])\n        dSM = _vMh2_to_m2Lambda_SM(v, Mh2)\n        x0 = np.array([dSM['m2'], dSM['Lambda']])\n        try:\n            xres = scipy.optimize.newton_krylov(f0, x0)\n        except scipy.optimize.nonlin.NoConvergence:\n            raise ValueError(\"No solution for m^2 and Lambda found\")\n        return {'m2': xres[0], 'Lambda': xres[1]}", "category": "Python"}, {"instruction": "def sector_size(self):\n        \"\"\"\n        Property with current sector size. CFB file can store normal sectors\n        and smaller ones.\n        \"\"\"\n", "input": "", "output": "        header = self.source.header\n        return header.mini_sector_size if self._is_mini else header.sector_size", "category": "Python"}, {"instruction": "def inside_brain(stat_dset,atlas=None,p=0.001):\n    '''calculates the percentage of voxels above a statistical threshold inside a brain mask vs. outside it\n    \n    if ``atlas`` is ``None``, it will try to find ``TT_N27``'''\n", "input": "", "output": "    atlas = find_atlas(atlas)\n    if atlas==None:\n        return None\n    mask_dset = nl.suffix(stat_dset,'_atlasfrac')\n    nl.run(['3dfractionize','-template',nl.strip_subbrick(stat_dset),'-input',nl.calc([atlas],'1+step(a-100)',datum='short'),'-preserve','-clip','0.2','-prefix',mask_dset],products=mask_dset,quiet=True,stderr=None)\n    s = nl.roi_stats(mask_dset,nl.thresh(stat_dset,p))\n    return 100.0 * s[2]['nzvoxels'] / (s[1]['nzvoxels'] + s[2]['nzvoxels'])", "category": "Python"}, {"instruction": "def copy_from(self, src, dest):\n        \"\"\"\n        copy a file or a directory from container or image to host system. If you are copying\n        directories, the target directory must not exist (this function is using `shutil.copytree`\n        to copy directories and that's a requirement of the function). In case the directory exists,\n        OSError on python 2 or FileExistsError on python 3 are raised.\n\n        :param src: str, path to a file or a directory within container or image\n        :param dest: str, path to a file or a directory on host system\n        :return: None\n        \"\"\"\n", "input": "", "output": "        p = self.p(src)\n        if os.path.isfile(p):\n            logger.info(\"copying file %s to %s\", p, dest)\n            shutil.copy2(p, dest)\n        else:\n            logger.info(\"copying directory %s to %s\", p, dest)\n            shutil.copytree(p, dest)", "category": "Python"}, {"instruction": "def cast_request_param(param_type, param_name, param_value):\n    \"\"\"Try to cast a request param (e.g. query arg, POST data) from a string to\n    its specified type in the schema. This allows validating non-string params.\n\n    :param param_type: name of the type to be casted to\n    :type  param_type: string\n    :param param_name: param name\n    :type  param_name: string\n    :param param_value: param value\n    :type  param_value: string\n    \"\"\"\n", "input": "", "output": "    try:\n        return CAST_TYPE_TO_FUNC.get(param_type, lambda x: x)(param_value)\n    except ValueError:\n        log.warn(\"Failed to cast %s value of %s to %s\",\n                 param_name, param_value, param_type)\n        # Ignore type error, let jsonschema validation handle incorrect types\n        return param_value", "category": "Python"}, {"instruction": "def eval_grad(self):\n        \"\"\"Compute gradient in Fourier domain.\"\"\"\n", "input": "", "output": "\n        # Compute D X - S\n        self.Ryf[:] = self.eval_Rf(self.Yf)\n\n        # Map to spatial domain to multiply by mask\n        Ry = sl.irfftn(self.Ryf, self.cri.Nv, self.cri.axisN)\n        # Multiply by mask\n        self.WRy[:] = (self.W**2) * Ry\n        # Map back to frequency domain\n        WRyf = sl.rfftn(self.WRy, self.cri.Nv, self.cri.axisN)\n\n        gradf = np.conj(self.Df) * WRyf\n\n        # Multiple channel signal, multiple channel dictionary\n        if self.cri.Cd > 1:\n            gradf = np.sum(gradf, axis=self.cri.axisC, keepdims=True)\n\n        return gradf", "category": "Python"}, {"instruction": "def start(self, sockets=None, **kwargs):\n        \"\"\"\n        start execution\n        \"\"\"\n", "input": "", "output": "        stream = sockets or self.sockets()\n        pumps = []\n\n        if self.interactive:\n            pumps.append(io.Pump(io.Stream(self.stdin), stream, wait_for_output=False))\n\n        pumps.append(io.Pump(stream, io.Stream(self.stdout), propagate_close=False))\n        # FIXME: since exec_start returns a single socket, how do we\n        #        distinguish between stdout and stderr?\n        # pumps.append(io.Pump(stream, io.Stream(self.stderr), propagate_close=False))\n\n        return pumps", "category": "Python"}, {"instruction": "def is_password(self, is_password):\n        \"\"\" Setter for is_identifier \"\"\"\n", "input": "", "output": "\n        if is_password:\n            self.is_forgetable = True\n\n        self._is_password = is_password", "category": "Python"}, {"instruction": "def create_update_event(self):\n        \"\"\"Parse the update messages DSL to insert the data into the Event.\n\n        Returns:\n            list[fleaker.peewee.EventStorageMixin]:\n                All the events that were created for the update.\n        \"\"\"\n", "input": "", "output": "        events = []\n\n        for fields, rules in iteritems(self._meta.update_messages):\n            if not isinstance(fields, (list, tuple, set)):\n                fields = (fields,)\n\n            changed = any([\n                getattr(self, field) != getattr(self.get_original(), field)\n                for field in fields\n            ])\n\n            if changed:\n                event = self.create_audit_event(code=rules['code'])\n                event.body = rules['message']\n                event.meta = self.parse_meta(rules['meta'])\n                events.append(event)\n\n        self.update_event_callback(events)\n\n        with db.database.atomic():\n            for event in events:\n                event.save()\n\n        return events", "category": "Python"}, {"instruction": "def convert_response(self):\n        \"\"\"Finish filling the instance's response object so it's ready to be\n        served to the client. This includes converting the body_raw property\n        to the content type requested by the user if necessary.\n        \"\"\"\n", "input": "", "output": "        if hasattr(self.response, 'body_raw'):\n            if self.response.body_raw is not None:\n                to_type = re.sub('[^a-zA-Z_]', '_', self.type)\n                to_type_method = 'to_' + to_type\n                if hasattr(self, to_type_method):\n                    self.response.body = getattr(self, to_type_method)(\n                        self.response.body_raw)\n            del self.response.body_raw", "category": "Python"}, {"instruction": "def save(self, key, data):\n        \"\"\" Save data associated with key.\n        \"\"\"\n", "input": "", "output": "        self._db[key] = json.dumps(data)\n        self._db.sync()", "category": "Python"}, {"instruction": "def parse_date(dateString):\n    '''Parses a variety of date formats into a 9-tuple in GMT'''\n", "input": "", "output": "    if not dateString:\n        return None\n    for handler in _date_handlers:\n        try:\n            date9tuple = handler(dateString)\n        except (KeyError, OverflowError, ValueError):\n            continue\n        if not date9tuple:\n            continue\n        if len(date9tuple) != 9:\n            continue\n        return date9tuple\n    return None", "category": "Python"}, {"instruction": "def cross_product_compare(start, candidate1, candidate2):\n    \"\"\"Compare two relative changes by their cross-product.\n\n    This is meant to be a way to determine which vector is more \"inside\"\n    relative to ``start``.\n\n    .. note::\n\n       This is a helper for :func:`_simple_convex_hull`.\n\n    Args:\n        start (numpy.ndarray): The start vector (as 1D NumPy array with\n            2 elements).\n        candidate1 (numpy.ndarray): The first candidate vector (as 1D\n            NumPy array with 2 elements).\n        candidate2 (numpy.ndarray): The second candidate vector (as 1D\n            NumPy array with 2 elements).\n\n    Returns:\n        float: The cross product of the two differences.\n    \"\"\"\n", "input": "", "output": "    delta1 = candidate1 - start\n    delta2 = candidate2 - start\n    return cross_product(delta1, delta2)", "category": "Python"}, {"instruction": "def _preamble_cell(path):\n    \"\"\"\n    This cell is inserted at the start of each notebook to set the working\n    directory to the correct folder.\n    \"\"\"\n", "input": "", "output": "    code = dedent(\n        '''\n    # HIDDEN\n    # Clear previously defined variables\n    %reset -f\n    '''.format(path)\n    )\n    return nbformat.v4.new_code_cell(source=code)", "category": "Python"}, {"instruction": "def fast_compare(tree1, tree2):\n    \"\"\" This is optimized to compare two AST trees for equality.\n        It makes several assumptions that are currently true for\n        AST trees used by rtrip, and it doesn't examine the _attributes.\n    \"\"\"\n", "input": "", "output": "\n    geta = ast.AST.__getattribute__\n\n    work = [(tree1, tree2)]\n    pop = work.pop\n    extend = work.extend\n    # TypeError in cPython, AttributeError in PyPy\n    exception = TypeError, AttributeError\n    zipl = zip_longest\n    type_ = type\n    list_ = list\n    while work:\n        n1, n2 = pop()\n        try:\n            f1 = geta(n1, '_fields')\n            f2 = geta(n2, '_fields')\n        except exception:\n            if type_(n1) is list_:\n                extend(zipl(n1, n2))\n                continue\n            if n1 == n2:\n                continue\n            return False\n        else:\n            f1 = [x for x in f1 if x != 'ctx']\n            if f1 != [x for x in f2 if x != 'ctx']:\n                return False\n            extend((geta(n1, fname), geta(n2, fname)) for fname in f1)\n\n    return True", "category": "Python"}, {"instruction": "def get_images_layer(self, image_id):\n        \"\"\"GET /v1/images/{image_id}/layer\"\"\"\n", "input": "", "output": "        return self._http_call(self.IMAGE_LAYER, get, image_id=image_id)", "category": "Python"}, {"instruction": "def process(name, path, data, module=True):\n    \"\"\"\n    Process data to create a screenshot which will be saved in\n    docs/screenshots/<name>.png\n    If module is True the screenshot will include the name and py3status.\n    \"\"\"\n", "input": "", "output": "    # create dir if not exists\n    try:\n        os.makedirs(path)\n    except OSError:\n        pass\n\n    global font, glyph_data\n    if font is None:\n        font = ImageFont.truetype(FONT, FONT_SIZE * SCALE)\n    if glyph_data is None:\n        glyph_data = TTFont(font.path)\n\n    # make sure that the data is in list form\n    if not isinstance(data, list):\n        data = [data]\n\n    if contains_bad_glyph(glyph_data, data):\n        print(\"** %s has characters not in %s **\" % (name, font.getname()[0]))\n    else:\n        create_screenshot(name, data, path, font=font, is_module=module)", "category": "Python"}, {"instruction": "def match_handle(loc, tokens):\n    \"\"\"Process match blocks.\"\"\"\n", "input": "", "output": "    if len(tokens) == 4:\n        matches, match_type, item, stmts = tokens\n        cond = None\n    elif len(tokens) == 5:\n        matches, match_type, item, cond, stmts = tokens\n    else:\n        raise CoconutInternalException(\"invalid match statement tokens\", tokens)\n\n    if match_type == \"in\":\n        invert = False\n    elif match_type == \"not in\":\n        invert = True\n    else:\n        raise CoconutInternalException(\"invalid match type\", match_type)\n\n    matching = Matcher(loc, match_check_var)\n    matching.match(matches, match_to_var)\n    if cond:\n        matching.add_guard(cond)\n    return (\n        match_to_var + \" = \" + item + \"\\n\"\n        + matching.build(stmts, invert=invert)\n    )", "category": "Python"}, {"instruction": "def save_model(self, request, obj, form, change):\n        \"\"\"Set special model attribute to user for reference after save\"\"\"\n", "input": "", "output": "        obj._history_user = request.user\n        super(SimpleHistoryAdmin, self).save_model(request, obj, form, change)", "category": "Python"}, {"instruction": "def display_window(self):\n        \"\"\"Read the channel name from QComboBox and plot its spectrum.\n\n        This function is necessary it reads the data and it sends it to\n        self.display. When the user selects a smaller chunk of data from the\n        visible traces, then we don't need to call this function.\n        \"\"\"\n", "input": "", "output": "        if self.idx_chan.count() == 0:\n            self.update()\n\n        chan_name = self.idx_chan.currentText()\n        lg.debug('Power spectrum for channel ' + chan_name)\n\n        if chan_name:\n            trial = 0\n            data = self.parent.traces.data(trial=trial, chan=chan_name)\n            self.display(data)\n        else:\n            self.scene.clear()", "category": "Python"}, {"instruction": "def fprob(dfnum, dfden, F):\n    \"\"\"\nReturns the (1-tailed) significance level (p-value) of an F\nstatistic given the degrees of freedom for the numerator (dfR-dfF) and\nthe degrees of freedom for the denominator (dfF).\n\nUsage:   lfprob(dfnum, dfden, F)   where usually dfnum=dfbn, dfden=dfwn\n\"\"\"\n", "input": "", "output": "    p = betai(0.5 * dfden, 0.5 * dfnum, dfden / float(dfden + dfnum * F))\n    return p", "category": "Python"}, {"instruction": "def clone_from(repo_url, repo_dir):\n    \"\"\"Clone a remote git repo into a local directory.\"\"\"\n", "input": "", "output": "    repo_url = _fix_repo_url(repo_url)\n    LOG.info(\"Cloning %s into %s.\" % (repo_url, repo_dir))\n    cmd = GIT_CLONE_CMD.format(repo_url, repo_dir)\n    resp = envoy.run(cmd)\n    if resp.status_code != 0:\n        LOG.error(\"Cloned failed: %s\" % resp.std_err)\n        raise GitException(resp.std_err)\n    LOG.info(\"Clone successful.\")", "category": "Python"}, {"instruction": "def GetHashersInformation(cls):\n    \"\"\"Retrieves the hashers information.\n\n    Returns:\n      list[tuple]: containing:\n\n          str: hasher name.\n          str: hasher description.\n    \"\"\"\n", "input": "", "output": "    hashers_information = []\n    for _, hasher_class in cls.GetHasherClasses():\n      description = getattr(hasher_class, 'DESCRIPTION', '')\n      hashers_information.append((hasher_class.NAME, description))\n\n    return hashers_information", "category": "Python"}, {"instruction": "def load(path):\n    \"\"\"\n    Load fixmat at path.\n    \n    Parameters:\n        path : string\n            Absolute path of the file to load from.\n    \"\"\"\n", "input": "", "output": "    f = h5py.File(path,'r')\n    if 'Fixmat' in f:\n      fm_group = f['Fixmat']\n    else:\n      fm_group = f['Datamat']\n    fields = {}\n    params = {}\n    for field, value in list(fm_group.items()):\n        fields[field] = np.array(value)\n    for param, value in list(fm_group.attrs.items()):\n        params[param] = value\n    f.close()\n    return VectorFixmatFactory(fields, params)", "category": "Python"}, {"instruction": "def delete_user(self, uid, delete_views=True):\n        \n        '''\n            a method to retrieve the account details of a user in the bucket\n            \n        :param uid: string with id of user in bucket\n        :param delete_views: boolean to remove indices attached to user\n        :return: integer with status of delete operation\n        '''\n", "input": "", "output": "    \n        title = '%s.delete_user' % self.__class__.__name__\n        \n    # validate inputs\n        input_fields = {\n            'uid': uid\n        }\n        for key, value in input_fields.items():\n            if value:\n                object_title = '%s(%s=%s)' % (title, key, str(value))\n                self.fields.validate(value, '.%s' % key, object_title)\n    \n    # delete any existing sessions\n        self.delete_sessions(uid)\n    \n    # delete any existing views\n        if delete_views:\n            self.delete_view(uid=uid)\n            \n    # construct url\n        url = self.bucket_url + '/_user/%s' % uid\n\n    # send request\n        response = requests.delete(url)\n    \n    # report outcome\n        self.printer('User \"%s\" removed from bucket \"%s\"' % (uid, self.bucket_name))\n        \n        return response.status_code", "category": "Python"}, {"instruction": "def synthesize(vers, opts):\n    \"\"\" Synthesize a capability enabled version response\n        This is a very limited emulation for relatively recent feature sets\n    \"\"\"\n", "input": "", "output": "    parsed_version = parse_version(vers[\"version\"])\n    vers[\"capabilities\"] = {}\n    for name in opts[\"optional\"]:\n        vers[\"capabilities\"][name] = check(parsed_version, name)\n    failed = False  # noqa: F841 T25377293 Grandfathered in\n    for name in opts[\"required\"]:\n        have = check(parsed_version, name)\n        vers[\"capabilities\"][name] = have\n        if not have:\n            vers[\"error\"] = (\n                \"client required capability `\"\n                + name\n                + \"` is not supported by this server\"\n            )\n    return vers", "category": "Python"}, {"instruction": "def qgis_expressions():\n    \"\"\"Retrieve all QGIS Expressions provided by InaSAFE.\n\n    :return: Dictionary of expression name and the expression itself.\n    :rtype: dict\n    \"\"\"\n", "input": "", "output": "    all_expressions = {\n        fct[0]: fct[1] for fct in getmembers(generic_expressions)\n        if fct[1].__class__.__name__ == 'QgsExpressionFunction'}\n    all_expressions.update({\n        fct[0]: fct[1] for fct in getmembers(infographic)\n        if fct[1].__class__.__name__ == 'QgsExpressionFunction'})\n    all_expressions.update({\n        fct[0]: fct[1] for fct in getmembers(map_report)\n        if fct[1].__class__.__name__ == 'QgsExpressionFunction'})\n    all_expressions.update({\n        fct[0]: fct[1] for fct in getmembers(html_report)\n        if fct[1].__class__.__name__ == 'QgsExpressionFunction'})\n    return all_expressions", "category": "Python"}, {"instruction": "def summary(self):\n        \"\"\"\n        Gets summary (e.g. cluster assignments, cluster sizes) of the model trained on the\n        training set. An exception is thrown if no summary exists.\n        \"\"\"\n", "input": "", "output": "        if self.hasSummary:\n            return KMeansSummary(super(KMeansModel, self).summary)\n        else:\n            raise RuntimeError(\"No training summary available for this %s\" %\n                               self.__class__.__name__)", "category": "Python"}, {"instruction": "def get_port_channel_detail_output_lacp_actor_system_id(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_port_channel_detail = ET.Element(\"get_port_channel_detail\")\n        config = get_port_channel_detail\n        output = ET.SubElement(get_port_channel_detail, \"output\")\n        lacp = ET.SubElement(output, \"lacp\")\n        actor_system_id = ET.SubElement(lacp, \"actor-system-id\")\n        actor_system_id.text = kwargs.pop('actor_system_id')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def _sync_params_from_devices(self):\n        \"\"\"Synchronizes parameters from devices to CPU. This function should be called after\n        calling `update` that updates the parameters on the devices, before one can read the\n        latest parameters from ``self._arg_params`` and ``self._aux_params``.\n\n        For row_sparse parameters on devices, ther are pulled from KVStore with all row ids.\n\n        \"\"\"\n", "input": "", "output": "        self._exec_group.get_params(self._arg_params, self._aux_params)\n        if self._kvstore and self._update_on_kvstore:\n            for param_name, param_val in sorted(self._arg_params.items()):\n                if param_val.stype == 'row_sparse':\n                    row_ids = nd.arange(0, param_val.shape[0], dtype='int64')\n                    self._kvstore.row_sparse_pull(param_name, param_val, row_ids=row_ids)\n        self._params_dirty = False", "category": "Python"}, {"instruction": "def _sanitize_column_names(data):\n    \"\"\"Replace illegal characters with underscore\"\"\"\n", "input": "", "output": "    new_names = {}\n    for name in data.columns:\n        new_names[name] = _ILLEGAL_CHARACTER_PAT.sub(\"_\", name)\n    return new_names", "category": "Python"}, {"instruction": "def chunked_join(iterable, int1, int2, str1, str2, func):\n    \"\"\"Chunk and join.\"\"\"\n", "input": "", "output": "    chunks = list(chunked(iterable, int1))\n    logging.debug(chunks)\n    groups = [list(chunked(chunk, int2)) for chunk in chunks]\n    logging.debug(groups)\n    return str1.join([\n        str2.join([func(''.join(chunk)) for chunk in chunks])\n        for chunks in groups\n    ])", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"\n        Close inotify's instance (close its file descriptor).\n        It destroys all existing watches, pending events,...\n        This method is automatically called at the end of loop().\n        Afterward it is invalid to access this instance.\n        \"\"\"\n", "input": "", "output": "        if self._fd is not None:\n            self._pollobj.unregister(self._fd)\n            os.close(self._fd)\n            self._fd = None\n        self._sys_proc_fun = None", "category": "Python"}, {"instruction": "def fulltext(search, lang=Lang.English, ignore_case=True):\n        \"\"\"Full text search.\n\n        Example::\n\n            filters = Text.fulltext(\"python pymongo_mate\")\n\n        .. note::\n\n            This field doesn't need to specify field.\n        \"\"\"\n", "input": "", "output": "        return {\n            \"$text\": {\n                \"$search\": search,\n                \"$language\": lang,\n                \"$caseSensitive\": not ignore_case,\n                \"$diacriticSensitive\": False,\n            }\n        }", "category": "Python"}, {"instruction": "def folder_cls_from_folder_name(cls, folder_name, locale):\n        \"\"\"Returns the folder class that matches a localized folder name.\n\n        locale is a string, e.g. 'da_DK'\n        \"\"\"\n", "input": "", "output": "        for folder_cls in cls.WELLKNOWN_FOLDERS + NON_DELETEABLE_FOLDERS:\n            if folder_name.lower() in folder_cls.localized_names(locale):\n                return folder_cls\n        raise KeyError()", "category": "Python"}, {"instruction": "def parse_xml(self, key_xml):\n        '''\n            Parse a VocabularyKey from an Xml as per Healthvault\n            schema.\n\n            :param key_xml: lxml.etree.Element representing a single VocabularyKey\n        '''\n", "input": "", "output": "        xmlutils = XmlUtils(key_xml)\n        self.name = xmlutils.get_string_by_xpath('name')\n        self.family = xmlutils.get_string_by_xpath('family')\n        self.version = xmlutils.get_string_by_xpath('version')\n        self.description = xmlutils.get_string_by_xpath('description')\n\n        self.language = xmlutils.get_lang()", "category": "Python"}, {"instruction": "def is_root(self, el):\n        \"\"\"\n        Return whether element is a root element.\n\n        We check that the element is the root of the tree (which we have already pre-calculated),\n        and we check if it is the root element under an `iframe`.\n        \"\"\"\n", "input": "", "output": "\n        root = self.root and self.root is el\n        if not root:\n            parent = self.get_parent(el)\n            root = parent is not None and self.is_html and self.is_iframe(parent)\n        return root", "category": "Python"}, {"instruction": "def _type_to_str(self, type):\n        \"\"\"\n        :param type:\n        :type type: int\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        for key, val in self.TYPE_NAMES.items():\n            if key == type:\n                return val", "category": "Python"}, {"instruction": "def get(self, name_or_klass):\n        \"\"\"\n        Gets a specific panel instance.\n\n        :param name_or_klass: Name or class of the panel to retrieve.\n        :return: The specified panel instance.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(name_or_klass, str):\n            name_or_klass = name_or_klass.__name__\n        for zone in range(4):\n            try:\n                panel = self._panels[zone][name_or_klass]\n            except KeyError:\n                pass\n            else:\n                return panel\n        raise KeyError(name_or_klass)", "category": "Python"}, {"instruction": "def xdr(self):\n        \"\"\"Packs and base64 encodes this :class:`Transaction` as an XDR\n        string.\n\n        \"\"\"\n", "input": "", "output": "        tx = Xdr.StellarXDRPacker()\n        tx.pack_Transaction(self.to_xdr_object())\n        return base64.b64encode(tx.get_buffer())", "category": "Python"}, {"instruction": "def set_uint_info(self, field, data):\n        \"\"\"Set uint type property into the DMatrix.\n\n        Parameters\n        ----------\n        field: str\n            The field name of the information\n\n        data: numpy array\n            The array ofdata to be set\n        \"\"\"\n", "input": "", "output": "        _check_call(_LIB.XGDMatrixSetUIntInfo(self.handle,\n                                              c_str(field),\n                                              c_array(ctypes.c_uint, data),\n                                              len(data)))", "category": "Python"}, {"instruction": "def set_var(self, vardef):\n        \"\"\" Set variable to global stylesheet context.\n        \"\"\"\n", "input": "", "output": "        if not(vardef.default and self.cache['ctx'].get(vardef.name)):\n            self.cache['ctx'][vardef.name] = vardef.expression.value", "category": "Python"}, {"instruction": "def validate_int(datum, **kwargs):\n    \"\"\"\n    Check that the data value is a non floating\n    point number with size less that Int32.\n    Also support for logicalType timestamp validation with datetime.\n\n    Int32 = -2147483648<=datum<=2147483647\n\n    conditional python types\n    (int, long, numbers.Integral,\n    datetime.time, datetime.datetime, datetime.date)\n\n    Parameters\n    ----------\n    datum: Any\n        Data being validated\n    kwargs: Any\n        Unused kwargs\n    \"\"\"\n", "input": "", "output": "    return (\n            (isinstance(datum, (int, long, numbers.Integral))\n             and INT_MIN_VALUE <= datum <= INT_MAX_VALUE\n             and not isinstance(datum, bool))\n            or isinstance(\n                datum, (datetime.time, datetime.datetime, datetime.date)\n            )\n    )", "category": "Python"}, {"instruction": "def cli(env, columns, sortby, volume_id):\n    \"\"\"List existing replicant volumes for a file volume.\"\"\"\n", "input": "", "output": "    file_storage_manager = SoftLayer.FileStorageManager(env.client)\n\n    legal_volumes = file_storage_manager.get_replication_partners(\n        volume_id\n    )\n\n    if not legal_volumes:\n        click.echo(\"There are no replication partners for the given volume.\")\n    else:\n        table = formatting.Table(columns.columns)\n        table.sortby = sortby\n\n        for legal_volume in legal_volumes:\n            table.add_row([value or formatting.blank()\n                           for value in columns.row(legal_volume)])\n\n        env.fout(table)", "category": "Python"}, {"instruction": "def find_slots(self, wanted, slots=None):\n        \"\"\"\n        Yields all slots containing the item.\n        Searches the given slots or, if not given,\n        active hotbar slot, hotbar, inventory, open window in this order.\n\n        Args:\n            wanted: function(Slot) or Slot or itemID or (itemID, metadata)\n        \"\"\"\n", "input": "", "output": "        if slots is None:\n            slots = self.inv_slots_preferred + self.window.window_slots\n        wanted = make_slot_check(wanted)\n\n        for slot in slots:\n            if wanted(slot):\n                yield slot", "category": "Python"}, {"instruction": "def load_k40_coincidences_from_hdf5(filename, dom_id):\n    \"\"\"Load k40 coincidences from hdf5 file\n\n    Parameters\n    ----------\n    filename: filename of hdf5 file\n    dom_id: DOM ID\n\n    Returns\n    -------\n    data: numpy array of coincidences\n    livetime: duration of data-taking\n    \"\"\"\n", "input": "", "output": "\n    with h5py.File(filename, 'r') as h5f:\n        data = h5f['/k40counts/{0}'.format(dom_id)]\n        livetime = data.attrs['livetime']\n        data = np.array(data)\n\n    return data, livetime", "category": "Python"}, {"instruction": "def _get_contents_between(string, opener, closer):\n    \"\"\"\n    Get the contents of a string between two characters\n    \"\"\"\n", "input": "", "output": "    opener_location = string.index(opener)\n    closer_location = string.index(closer)\n    content = string[opener_location + 1:closer_location]\n    return content", "category": "Python"}, {"instruction": "def output_results(results, metric, options):\n    \"\"\"\n    Output the results to stdout.\n\n    TODO: add AMPQ support for efficiency\n    \"\"\"\n", "input": "", "output": "    formatter = options['Formatter']\n    context = metric.copy()  # XXX might need to sanitize this\n    try:\n        context['dimension'] = list(metric['Dimensions'].values())[0]\n    except AttributeError:\n        context['dimension'] = ''\n    for result in results:\n        stat_keys = metric['Statistics']\n        if not isinstance(stat_keys, list):\n            stat_keys = [stat_keys]\n        for statistic in stat_keys:\n            context['statistic'] = statistic\n            # get and then sanitize metric name, first copy the unit name from the\n            # result to the context to keep the default format happy\n            context['Unit'] = result['Unit']\n            metric_name = (formatter % context).replace('/', '.').lower()\n            line = '{0} {1} {2}\\n'.format(\n                metric_name,\n                result[statistic],\n                timegm(result['Timestamp'].timetuple()),\n            )\n            sys.stdout.write(line)", "category": "Python"}, {"instruction": "def is_owner():\n    \"\"\"A :func:`.check` that checks if the person invoking this command is the\n    owner of the bot.\n\n    This is powered by :meth:`.Bot.is_owner`.\n\n    This check raises a special exception, :exc:`.NotOwner` that is derived\n    from :exc:`.CheckFailure`.\n    \"\"\"\n", "input": "", "output": "\n    async def predicate(ctx):\n        if not await ctx.bot.is_owner(ctx.author):\n            raise NotOwner('You do not own this bot.')\n        return True\n\n    return check(predicate)", "category": "Python"}, {"instruction": "def calc_fitness(self,X,labels,fit_choice,sel):\n        \"\"\"computes fitness of individual output yhat.\n        yhat: output of a program.\n        labels: correct outputs\n        fit_choice: choice of fitness function\n        \"\"\"\n", "input": "", "output": "\n        if 'lexicase' in sel:\n            # return list(map(lambda yhat: self.f_vec[fit_choice](labels,yhat),X))\n            return np.asarray(\n                              [self.proper(self.f_vec[fit_choice](labels,\n                                                        yhat)) for yhat in X],\n                                                        order='F')\n            # return list(Parallel(n_jobs=-1)(delayed(self.f_vec[fit_choice])(labels,yhat) for yhat in X))\n        else:\n            # return list(map(lambda yhat: self.f[fit_choice](labels,yhat),X))\n            return np.asarray([self.f[fit_choice](labels,yhat) for yhat in X],\n                            order='F').reshape(-1)", "category": "Python"}, {"instruction": "def hacking_no_author_tags(physical_line):\n    \"\"\"Check that no author tags are used.\n\n    H105 don't use author tags\n    \"\"\"\n", "input": "", "output": "    for regex in AUTHOR_TAG_RE:\n        if regex.match(physical_line):\n            physical_line = physical_line.lower()\n            pos = physical_line.find('moduleauthor')\n            if pos < 0:\n                pos = physical_line.find('author')\n            return (pos, \"H105: Don't use author tags\")", "category": "Python"}, {"instruction": "def parameter(self, parser):\n        \"\"\"\n        Return a parser the parses parameters.\n        \"\"\"\n", "input": "", "output": "        return (Suppress(self.syntax.params_start).leaveWhitespace() +\n                Group(parser) + Suppress(self.syntax.params_stop))", "category": "Python"}, {"instruction": "def get_linkrect_idx(self, pos):\n        \"\"\" Determine if cursor is inside one of the link hot spots.\"\"\"\n", "input": "", "output": "        for i, r in enumerate(self.link_rects):\n            if r.Contains(pos):\n                return i\n        return -1", "category": "Python"}, {"instruction": "def convert_reshape(net, node, module, builder):\n    \"\"\"Converts a reshape layer from mxnet to coreml.\n\n    This doesn't currently handle the deprecated parameters for the reshape layer.\n\n    Parameters\n    ----------\n    network: net\n        An mxnet network object.\n\n    layer: node\n        Node to convert.\n\n    module: module\n        A module for MXNet\n\n    builder: NeuralNetworkBuilder\n        A neural network builder object.\n    \"\"\"\n", "input": "", "output": "    input_name, output_name = _get_input_output_name(net, node)\n    name = node['name']\n    target_shape = node['shape']\n\n    if any(item <= 0 for item in target_shape):\n        raise NotImplementedError('Special dimensional values less than or equal to 0 are not supported yet.'\n                                  'Feel free to file an issue here: https://github.com/dmlc/mxnet/issues.')\n\n    if 'reverse' in node and node['reverse'] == 'True':\n        raise NotImplementedError('\"reverse\" parameter is not supported by yet.'\n                                  'Feel free to file an issue here: https://github.com/dmlc/mxnet/issues.')\n\n    mode = 0 # CHANNEL_FIRST\n    builder.add_reshape(name, input_name, output_name, target_shape, mode)", "category": "Python"}, {"instruction": "def getMaximinScores(profile):\n    \"\"\"\n    Returns a dictionary that associates integer representations of each candidate with their\n    Copeland score.\n\n    :ivar Profile profile: A Profile object that represents an election profile.\n    \"\"\"\n", "input": "", "output": "\n    # Currently, we expect the profile to contain complete ordering over candidates. Ties are\n    # allowed however.\n    elecType = profile.getElecType()\n    if elecType != \"soc\" and elecType != \"toc\":\n        print(\"ERROR: unsupported election type\")\n        exit()\n\n    wmgMap = profile.getWmg()\n    # Initialize each Copeland score as infinity.\n    maximinscores = {}\n    for cand in wmgMap.keys():\n        maximinscores[cand] = float(\"inf\")\n\n    # For each pair of candidates, calculate the number of votes in which one beat the other.\n\n    # For each pair of candidates, calculate the number of times each beats the other.\n    for cand1, cand2 in itertools.combinations(wmgMap.keys(), 2):\n        if cand2 in wmgMap[cand1].keys():\n            maximinscores[cand1] = min(maximinscores[cand1], wmgMap[cand1][cand2])\n            maximinscores[cand2] = min(maximinscores[cand2], wmgMap[cand2][cand1])\n\n    return maximinscores", "category": "Python"}, {"instruction": "def refreshFromTarget(self, level=0):\n        \"\"\" Refreshes the configuration tree from the target it monitors (if present).\n            Recursively call _refreshNodeFromTarget for itself and all children. Subclasses should\n            typically override _refreshNodeFromTarget instead of this function.\n            During updateTarget's execution refreshFromTarget is blocked to avoid loops.\n        \"\"\"\n", "input": "", "output": "        if self.getRefreshBlocked():\n            logger.debug(\"_refreshNodeFromTarget blocked\")\n            return\n\n        if False and level == 0:\n            logger.debug(\"refreshFromTarget: {}\".format(self.nodePath))\n\n        self._refreshNodeFromTarget()\n        for child in self.childItems:\n            child.refreshFromTarget(level=level + 1)", "category": "Python"}, {"instruction": "def cosbetas(self):\n        \"\"\"\n        TODO: add documentation\n\n        (ComputedColumn)\n        \"\"\"\n", "input": "", "output": "        coords = self.coords_for_computations\n        norms = self.normals_for_computations\n\n        # TODO: ditch the list comprehension... I know I figured out how to do\n        # this (ie along an axis) with np.dot somewhere else\n        # cosbetas = np.array([np.dot(c,n) / (np.linalg.norm(c)*np.linalg.norm(n)) for c,n in zip(coords, norms)])\n\n        cosbetas = libphoebe.scalproj_cosangle(\n          np.ascontiguousarray(coords),\n          np.ascontiguousarray(norms)\n        )\n\n        return ComputedColumn(self, cosbetas)", "category": "Python"}, {"instruction": "def seek(self, offset, whence=0):\n        \"\"\"\n        seek(offset[, whence]) -> None.  Move to new file position.\n\n        Argument offset is a byte count.  Optional argument whence defaults to\n        0 (offset from start of file, offset should be >= 0); other values are 1\n        (move relative to current position, positive or negative), and 2 (move\n        relative to end of file, usually negative, although many platforms allow\n        seeking beyond the end of a file).  If the file is opened in text mode,\n        only offsets returned by tell() are legal.  Use of other offsets causes\n        undefined behavior.\n        Note that not all file objects are seekable.\n        \"\"\"\n", "input": "", "output": "        assert offset == 0\n        self.linepos = 0\n        return self.stream.seek(offset, whence)", "category": "Python"}, {"instruction": "def quote_code(self, key):\n        \"\"\"Returns string quoted code \"\"\"\n", "input": "", "output": "\n        code = self.grid.code_array(key)\n        quoted_code = quote(code)\n\n        if quoted_code is not None:\n            self.set_code(key, quoted_code)", "category": "Python"}, {"instruction": "def is_valid_abi_type(type_name):\n    \"\"\"\n    This function is used to make sure that the ``type_name`` is a valid ABI Type.\n\n    Please note that this is a temporary function and should be replaced by the corresponding\n    ABI function, once the following issue has been resolved.\n    https://github.com/ethereum/eth-abi/issues/125\n    \"\"\"\n", "input": "", "output": "    valid_abi_types = {\"address\", \"bool\", \"bytes\", \"int\", \"string\", \"uint\"}\n    is_bytesN = type_name.startswith(\"bytes\") and 1 <= int(type_name[5:]) <= 32\n    is_intN = (\n        type_name.startswith(\"int\") and\n        8 <= int(type_name[3:]) <= 256 and\n        int(type_name[3:]) % 8 == 0\n    )\n    is_uintN = (\n        type_name.startswith(\"uint\") and\n        8 <= int(type_name[4:]) <= 256 and\n        int(type_name[4:]) % 8 == 0\n    )\n\n    if type_name in valid_abi_types:\n        return True\n    elif is_bytesN:\n        # bytes1 to bytes32\n        return True\n    elif is_intN:\n        # int8 to int256\n        return True\n    elif is_uintN:\n        # uint8 to uint256\n        return True\n\n    return False", "category": "Python"}, {"instruction": "def xy_round(data,x0,y0,skymode,ker2d,xsigsq,ysigsq,datamin=None,datamax=None):\n    \"\"\" Compute center of source\n    Original code from IRAF.noao.digiphot.daofind.apfind ap_xy_round()\n    \"\"\"\n", "input": "", "output": "    nyk,nxk = ker2d.shape\n    if datamin is None:\n        datamin = data.min()\n    if datamax is None:\n        datamax = data.max()\n\n    # call C function for speed now...\n    xy_val = cdriz.arrxyround(data,x0,y0,skymode,ker2d,xsigsq,ysigsq,datamin,datamax)\n    if xy_val is None:\n        x = None\n        y = None\n        round = None\n    else:\n        x = xy_val[0]\n        y = xy_val[1]\n        round = xy_val[2]\n\n    return x,y,round", "category": "Python"}, {"instruction": "def gradients_X_X2(self, dL_dK, X, X2):\n        \"\"\"Derivative of the covariance matrix with respect to X\"\"\"\n", "input": "", "output": "        return self._comp_grads(dL_dK, X, X2)[3:]", "category": "Python"}, {"instruction": "def imported_classifiers_package(p: ecore.EPackage):\n        \"\"\"Determines which classifiers have to be imported into given package.\"\"\"\n", "input": "", "output": "        classes = {c for c in p.eClassifiers if isinstance(c, ecore.EClass)}\n\n        references = itertools.chain(*(c.eAllReferences() for c in classes))\n        references_types = (r.eType for r in references)\n        imported = {c for c in references_types if getattr(c, 'ePackage', p) is not p}\n\n        imported_dict = {}\n        for classifier in imported:\n            imported_dict.setdefault(classifier.ePackage, set()).add(classifier)\n\n        return imported_dict", "category": "Python"}, {"instruction": "def LifoQueue(self, name, initial=None, maxsize=None):\n        \"\"\"The LIFO queue datatype.\n\n        :param name: The name of the queue.\n        :keyword initial: Initial items in the queue.\n\n        See :class:`redish.types.LifoQueue`.\n\n        \"\"\"\n", "input": "", "output": "        return types.LifoQueue(name, self.api,\n                               initial=initial, maxsize=maxsize)", "category": "Python"}, {"instruction": "def polygons(self):\n        \"\"\"Return a list of polygons describing the region.\n\n        - Each polygon is a list of linear rings, where the first describes the\n          exterior and the rest describe interior holes.\n        - Each linear ring is a list of positions where the last is a repeat of\n          the first.\n        - Each position is a (lat, lon) pair.\n        \"\"\"\n", "input": "", "output": "        if self.type == 'Polygon':\n            polygons = [self._geojson['geometry']['coordinates']]\n        elif self.type == 'MultiPolygon':\n            polygons = self._geojson['geometry']['coordinates']\n        return [   [   [_lat_lons_from_geojson(s) for\n                        s in ring  ]              for\n                    ring in polygon]              for\n                polygon in polygons]", "category": "Python"}, {"instruction": "def _poll_vq_single(self, dname, use_devmode, ddresp):\n        \"\"\"\n        Initiate a view query for a view located in a design document\n        :param ddresp: The design document to poll (as JSON)\n        :return: True if successful, False if no views.\n        \"\"\"\n", "input": "", "output": "        vname = None\n        query = None\n        v_mr = ddresp.get('views', {})\n        v_spatial = ddresp.get('spatial', {})\n        if v_mr:\n            vname = single_dict_key(v_mr)\n            query = Query()\n        elif v_spatial:\n            vname = single_dict_key(v_spatial)\n            query = SpatialQuery()\n\n        if not vname:\n            return False\n\n        query.stale = STALE_OK\n        query.limit = 1\n\n        for r in self._cb.query(dname, vname, use_devmode=use_devmode,\n                                query=query):\n            pass\n        return True", "category": "Python"}, {"instruction": "def df(self, qname_predicates:bool=False, keep_variable_type:bool=True) -> pd.DataFrame:\n        ''' Multi funcitonal DataFrame with settings '''\n", "input": "", "output": "        local_df = self.df.copy()\n        if qname_predicates:\n            for col in self.columns:\n                local_df.rename({col: self.g.qname(col)})\n        if not keep_variable_type:\n            pass\n            # convert all to strings, watch out for lists\n        return local_df", "category": "Python"}, {"instruction": "def svd(a, i=None) -> tuple:\n    \"\"\"Singular Value Decomposition.\n\n    Factors the matrix `a` as ``u * np.diag(s) * v``, where `u` and `v`\n    are unitary and `s` is a 1D array of `a`'s singular values.\n\n    Parameters\n    ----------\n    a : array_like\n        Input array.\n    i : int or slice (optional)\n        What singular value \"slice\" to return.\n        Default is None which returns unitary 2D arrays.\n\n    Returns\n    -------\n    tuple\n        Decomposed arrays in order `u`, `v`, `s`\n    \"\"\"\n", "input": "", "output": "    u, s, v = np.linalg.svd(a, full_matrices=False, compute_uv=True)\n    u = u.T\n    if i is None:\n        return u, v, s\n    else:\n        return u[i], v[i], s[i]", "category": "Python"}, {"instruction": "def set_flow_element_list(self, value):\n        \"\"\"\n        Setter for 'flow_element_list' field.\n        :param value - a new value of 'flow_element_list' field. Must be a list\n        \"\"\"\n", "input": "", "output": "        if value is None or not isinstance(value, list):\n            raise TypeError(\"FlowElementList new value must be a list\")\n        else:\n            for element in value:\n                if not isinstance(element, flow_element.FlowElement):\n                    raise TypeError(\"FlowElementList elements in variable must be of FlowElement class\")\n            self.__flow_element_list = value", "category": "Python"}, {"instruction": "def set_server(self, pos, key, value):\n        \"\"\"Set the key to the value for the pos (position in the list).\"\"\"\n", "input": "", "output": "        self._ports_list[pos][key] = value", "category": "Python"}, {"instruction": "def script(klass, args, interval):\n        \"\"\"\n        Run the script *args* every *interval* (e.g. \"10s\") to peform health\n        check\n        \"\"\"\n", "input": "", "output": "        if isinstance(args, six.string_types) \\\n                or isinstance(args, six.binary_type):\n            warnings.warn(\n                \"Check.script should take a list of args\", DeprecationWarning)\n            args = [\"sh\", \"-c\", args]\n        return {'args': args, 'interval': interval}", "category": "Python"}, {"instruction": "def create_tag(self, tags):\n        \"\"\"Create tags for a Point in the language you specify. Tags can only contain alphanumeric (unicode) characters\n        and the underscore. Tags will be stored lower-cased.\n\n        Raises [IOTException](./Exceptions.m.html#IoticAgent.IOT.Exceptions.IOTException)\n        containing the error if the infrastructure detects a problem\n\n        Raises [LinkException](../Core/AmqpLink.m.html#IoticAgent.Core.AmqpLink.LinkException)\n        if there is a communications problem between you and the infrastructure\n\n        tags (mandatory) (list) - the list of tags you want to add to your Point, e.g.\n        [\"garden\", \"soil\"]\n        \"\"\"\n", "input": "", "output": "        if isinstance(tags, str):\n            tags = [tags]\n\n        evt = self._client._request_point_tag_update(self._type, self.__lid, self.__pid, tags, delete=False)\n        self._client._wait_and_except_if_failed(evt)", "category": "Python"}, {"instruction": "def addSeqsToAlignment(aln1,seqs,outfile,WorkingDir=None,SuppressStderr=None,\\\n        SuppressStdout=None):\n    \"\"\"Aligns sequences from second profile against first profile\n\n    aln1: string, name of file containing the alignment\n    seqs: string, name of file containing the sequences that should be added\n        to the alignment.\n    opoutfile: string, name of the output file (the new alignment)\n    \"\"\"\n", "input": "", "output": "    app = Clustalw({'-sequences':None,'-profile1':aln1,\\\n        '-profile2':seqs,'-outfile':outfile},SuppressStderr=\\\n        SuppressStderr,WorkingDir=WorkingDir, SuppressStdout=SuppressStdout)\n\n    app.Parameters['-align'].off()\n    return app()", "category": "Python"}, {"instruction": "def _build_kernel(self):\n        \"\"\"Private method to build kernel matrix\n\n        Runs public method to build kernel matrix and runs\n        additional checks to ensure that the result is okay\n\n        Returns\n        -------\n        Kernel matrix, shape=[n_samples, n_samples]\n\n        Raises\n        ------\n        RuntimeWarning : if K is not symmetric\n        \"\"\"\n", "input": "", "output": "        kernel = self.build_kernel()\n        kernel = self.symmetrize_kernel(kernel)\n        kernel = self.apply_anisotropy(kernel)\n        if (kernel - kernel.T).max() > 1e-5:\n            warnings.warn(\"K should be symmetric\", RuntimeWarning)\n        if np.any(kernel.diagonal == 0):\n            warnings.warn(\"K should have a non-zero diagonal\", RuntimeWarning)\n        return kernel", "category": "Python"}, {"instruction": "def fit_all(xy,uv,mode='rscale',center=None,verbose=True):\n    \"\"\" Performs an 'rscale' fit between matched lists of pixel positions xy and uv\"\"\"\n", "input": "", "output": "    if mode not in ['general', 'shift', 'rscale']:\n        mode = 'rscale'\n    if not isinstance(xy,np.ndarray):\n        # cast input list as numpy ndarray for fitting\n        xy = np.array(xy)\n    if not isinstance(uv,np.ndarray):\n        # cast input list as numpy ndarray for fitting\n        uv = np.array(uv)\n\n    if mode == 'shift':\n        logstr = 'Performing \"shift\" fit'\n        if verbose:\n            print(logstr)\n        else:\n            log.info(logstr)\n        result = fit_shifts(xy, uv)\n\n    elif mode == 'general':\n        logstr = 'Performing \"general\" fit'\n        if verbose:\n            print(logstr)\n        else:\n            log.info(logstr)\n        result = fit_general(xy, uv)\n\n    else:\n        logstr = 'Performing \"rscale\" fit'\n        if verbose:\n            print(logstr)\n        else:\n            log.info(logstr)\n        result = geomap_rscale(xy, uv, center=center)\n\n    return result", "category": "Python"}, {"instruction": "def _resolve_dut_count(self):\n        \"\"\"\n        Calculates total amount of resources required and their types.\n\n        :return: Nothing, modifies _dut_count, _hardware_count and\n        _process_count\n        :raises: ValueError if total count does not match counts of types separately.\n        \"\"\"\n", "input": "", "output": "        self._dut_count = len(self._dut_requirements)\n        self._resolve_process_count()\n        self._resolve_hardware_count()\n        if self._dut_count != self._hardware_count + self._process_count:\n            raise ValueError(\"Missing or invalid type fields in dut configuration!\")", "category": "Python"}, {"instruction": "def match_spec(element, specification):\n    \"\"\"\n    Matches the group.label specification of the supplied\n    element against the supplied specification dictionary\n    returning the value of the best match.\n    \"\"\"\n", "input": "", "output": "    match_tuple = ()\n    match = specification.get((), {})\n    for spec in [type(element).__name__,\n                 group_sanitizer(element.group, escape=False),\n                 label_sanitizer(element.label, escape=False)]:\n        match_tuple += (spec,)\n        if match_tuple in specification:\n            match = specification[match_tuple]\n    return match", "category": "Python"}, {"instruction": "def check_all_group_permissions(sender, **kwargs):\n    \"\"\"\n    Checks that all the permissions specified in our settings.py are set for our groups.\n    \"\"\"\n", "input": "", "output": "    if not is_permissions_app(sender):\n        return\n\n    config = getattr(settings, 'GROUP_PERMISSIONS', dict())\n\n    # for each of our items\n    for name, permissions in config.items():\n        # get or create the group\n        (group, created) = Group.objects.get_or_create(name=name)\n        if created:\n            pass\n\n        check_role_permissions(group, permissions, group.permissions.all())", "category": "Python"}, {"instruction": "def AraiCurvature(x=x,y=y):\n    \"\"\"\n    input: list of x points, list of y points\n    output: k, a, b, SSE.  curvature, circle center, and SSE\n    Function for calculating the radius of the best fit circle to a set of \n    x-y coordinates.\n    Paterson, G. A., (2011), A simple test for the presence of multidomain\n    behaviour during paleointensity experiments, J. Geophys. Res., in press,\n    doi: 10.1029/2011JB008369\n\n    \"\"\"\n", "input": "", "output": "    # makes sure all values are floats, then norms them by largest value\n    X = numpy.array(list(map(float, x)))\n    X = old_div(X, max(X))\n    Y = numpy.array(list(map(float, y)))\n    Y = old_div(Y, max(Y))\n    XY = numpy.array(list(zip(X, Y)))\n                  \n    #Provide the intitial estimate\n    E1=TaubinSVD(XY);\n\n    #Determine the iterative solution\n    E2=LMA(XY, E1);\n\n    estimates=[E2[2], E2[0], E2[1]];\n    \n    best_a = E2[0]\n    best_b = E2[1]\n    best_r = E2[2]\n\n    if best_a <= numpy.mean(X) and best_b <= numpy.mean(Y):\n        k = old_div(-1.,best_r)\n    else:\n        k = old_div(1.,best_r)\n\n    SSE = get_SSE(best_a, best_b, best_r, X, Y)\n    return k, best_a, best_b, SSE", "category": "Python"}, {"instruction": "def add(self, *nodes):\n        \"\"\" Adds nodes as siblings\n        :param nodes: GraphNode(s)\n        \"\"\"\n", "input": "", "output": "        for node in nodes:\n            node.set_parent(self)\n            self.add_sibling(node)", "category": "Python"}, {"instruction": "def dense_message_pass(node_states, edge_matrices):\n  \"\"\"Computes a_t from h_{t-1}, see bottom of page 3 in the paper.\n\n  Args:\n    node_states: [B, L, D] tensor (h_{t-1})\n    edge_matrices (tf.float32): [B, L*D, L*D]\n\n  Returns:\n    messages (tf.float32): [B, L, D] For each pair\n      of nodes in the graph a message is sent along both the incoming and\n      outgoing edge.\n  \"\"\"\n", "input": "", "output": "  batch_size, num_nodes, node_dim = common_layers.shape_list(node_states)\n\n  # Stack the nodes as a big column vector.\n  h_flat = tf.reshape(\n      node_states, [batch_size, num_nodes * node_dim, 1], name=\"h_flat\")\n\n  messages = tf.reshape(\n      tf.matmul(edge_matrices, h_flat), [batch_size * num_nodes, node_dim],\n      name=\"messages_matmul\")\n\n  message_bias = tf.get_variable(\"message_bias\", shape=node_dim)\n  messages = messages + message_bias\n  messages = tf.reshape(messages, [batch_size, num_nodes, node_dim])\n  return messages", "category": "Python"}, {"instruction": "def __load_settings_from_file(self):\n        \"\"\"\n        Loads settings info from the settings json file\n\n        :returns: True if the settings info is valid\n        :rtype: boolean\n        \"\"\"\n", "input": "", "output": "        filename = self.get_base_path() + 'settings.json'\n\n        if not exists(filename):\n            raise OneLogin_Saml2_Error(\n                'Settings file not found: %s',\n                OneLogin_Saml2_Error.SETTINGS_FILE_NOT_FOUND,\n                filename\n            )\n\n        # In the php toolkit instead of being a json file it is a php file and\n        # it is directly included\n        with open(filename, 'r') as json_data:\n            settings = json.loads(json_data.read())\n\n        advanced_filename = self.get_base_path() + 'advanced_settings.json'\n        if exists(advanced_filename):\n            with open(advanced_filename, 'r') as json_data:\n                settings.update(json.loads(json_data.read()))  # Merge settings\n\n        return self.__load_settings_from_dict(settings)", "category": "Python"}, {"instruction": "def process_directory_statements_sorted_by_pmid(directory_name):\n    \"\"\"Processes a directory filled with CSXML files, first normalizing the\n    character encoding to utf-8, and then processing into INDRA statements\n    sorted by pmid.\n\n    Parameters\n    ----------\n    directory_name : str\n        The name of a directory filled with csxml files to process\n\n    Returns\n    -------\n    pmid_dict : dict\n        A dictionary mapping pmids to a list of statements corresponding to\n        that pmid\n    \"\"\"\n", "input": "", "output": "    s_dict = defaultdict(list)\n    mp = process_directory(directory_name, lazy=True)\n\n    for statement in mp.iter_statements():\n        s_dict[statement.evidence[0].pmid].append(statement)\n    return s_dict", "category": "Python"}, {"instruction": "def decref_dependencies(self, term, refcounts):\n        \"\"\"\n        Decrement in-edges for ``term`` after computation.\n\n        Parameters\n        ----------\n        term : zipline.pipeline.Term\n            The term whose parents should be decref'ed.\n        refcounts : dict[Term -> int]\n            Dictionary of refcounts.\n\n        Return\n        ------\n        garbage : set[Term]\n            Terms whose refcounts hit zero after decrefing.\n        \"\"\"\n", "input": "", "output": "        garbage = set()\n        # Edges are tuple of (from, to).\n        for parent, _ in self.graph.in_edges([term]):\n            refcounts[parent] -= 1\n            # No one else depends on this term. Remove it from the\n            # workspace to conserve memory.\n            if refcounts[parent] == 0:\n                garbage.add(parent)\n        return garbage", "category": "Python"}, {"instruction": "def get_shop(self, shop_id=0):\n        \"\"\"\n        \u67e5\u8be2\u95e8\u5e97\u7684WiFi\u4fe1\u606f\n        http://mp.weixin.qq.com/wiki/15/bcfb5d4578ea818b89913472cf2bbf8f.html\n\n        :param shop_id: \u95e8\u5e97 ID\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n", "input": "", "output": "        res = self._post(\n            'shop/get',\n            data={\n                'shop_id': shop_id,\n            },\n            result_processor=lambda x: x['data']\n        )\n        return res", "category": "Python"}, {"instruction": "def _find_conda():\n    \"\"\"Find the conda executable robustly across conda versions.\n\n    Returns\n    -------\n    conda : str\n        Path to the conda executable.\n\n    Raises\n    ------\n    IOError\n        If the executable cannot be found in either the CONDA_EXE environment\n        variable or in the PATH.\n\n    Notes\n    -----\n    In POSIX platforms in conda >= 4.4, conda can be set up as a bash function\n    rather than an executable. (This is to enable the syntax\n    ``conda activate env-name``.) In this case, the environment variable\n    ``CONDA_EXE`` contains the path to the conda executable. In other cases,\n    we use standard search for the appropriate name in the PATH.\n\n    See https://github.com/airspeed-velocity/asv/issues/645 for more details.\n    \"\"\"\n", "input": "", "output": "    if 'CONDA_EXE' in os.environ:\n        conda = os.environ['CONDA_EXE']\n    else:\n        conda = util.which('conda')\n    return conda", "category": "Python"}, {"instruction": "def count_siblings(self, partitions):\n        \"\"\"Count siblings of partition in given partition-list.\n\n        :key-term:\n        sibling:    partitions with same topic\n        \"\"\"\n", "input": "", "output": "        count = sum(\n            int(self.topic == partition.topic)\n            for partition in partitions\n        )\n        return count", "category": "Python"}, {"instruction": "def get_boxes_and_lines(ax, labels):\n    \"\"\"Get boxes and lines using labels as id.\"\"\"\n", "input": "", "output": "    labels_u, labels_u_line = get_labels(labels)\n    boxes = ax.findobj(mpl.text.Annotation)\n    lines = ax.findobj(mpl.lines.Line2D)\n    lineid_boxes = []\n    lineid_lines = []\n\n    for box in boxes:\n        l = box.get_label()\n        try:\n            loc = labels_u.index(l)\n        except ValueError:\n            # this box is either one not added by lineidplot or has no label.\n            continue\n        lineid_boxes.append(box)\n\n    for line in lines:\n        l = line.get_label()\n        try:\n            loc = labels_u_line.index(l)\n        except ValueError:\n            # this line is either one not added by lineidplot or has no label.\n            continue\n        lineid_lines.append(line)\n\n    return lineid_boxes, lineid_lines", "category": "Python"}, {"instruction": "def mouseMoveEvent(self, event):\n        \"\"\"Override Qt method.\n\n        Draw semitransparent breakpoint hint.\n        \"\"\"\n", "input": "", "output": "        self.line_number_hint = self.editor.get_linenumber_from_mouse_event(\n            event)\n        self.update()", "category": "Python"}, {"instruction": "def ping(config_file, profile, solver_def, json_output, request_timeout, polling_timeout):\n    \"\"\"Ping the QPU by submitting a single-qubit problem.\"\"\"\n", "input": "", "output": "\n    now = utcnow()\n    info = dict(datetime=now.isoformat(), timestamp=datetime_to_timestamp(now), code=0)\n\n    def output(fmt, **kwargs):\n        info.update(kwargs)\n        if not json_output:\n            click.echo(fmt.format(**kwargs))\n\n    def flush():\n        if json_output:\n            click.echo(json.dumps(info))\n\n    try:\n        _ping(config_file, profile, solver_def, request_timeout, polling_timeout, output)\n    except CLIError as error:\n        output(\"Error: {error} (code: {code})\", error=str(error), code=error.code)\n        sys.exit(error.code)\n    except Exception as error:\n        output(\"Unhandled error: {error}\", error=str(error))\n        sys.exit(127)\n    finally:\n        flush()", "category": "Python"}, {"instruction": "def save(self, commit=True, **kwargs):\n        \"\"\" Saves the considered instances. \"\"\"\n", "input": "", "output": "        if self.post:\n            for form in self.forms:\n                form.instance.post = self.post\n        super().save(commit)", "category": "Python"}, {"instruction": "def create_user(name, groups=None, key_file=None):\n    \"\"\"Create a user. Adds a key file to authorized_keys if given.\"\"\"\n", "input": "", "output": "\n    groups = groups or []\n    if not user_exists(name):\n        for group in groups:\n            if not group_exists(group):\n                sudo(u\"addgroup %s\" % group)\n        groups = groups and u'-G %s' % u','.join(groups) or ''\n        sudo(u\"useradd -m %s -s /bin/bash %s\" % (groups, name))\n        sudo(u\"passwd -d %s\" % name)\n    if key_file:\n        sudo(u\"mkdir -p /home/%s/.ssh\" % name)\n        put(key_file, u\"/home/%s/.ssh/authorized_keys\" % name, use_sudo=True)\n        sudo(u\"chown -R %(name)s:%(name)s /home/%(name)s/.ssh\" % {'name': name})", "category": "Python"}, {"instruction": "def _clean_ctx(self):\n        \"\"\"\n        Clears and deallocates context\n        \"\"\"\n", "input": "", "output": "        try:\n            if self.ctx is not None:\n                libcrypto.EVP_MD_CTX_free(self.ctx)\n                del self.ctx\n        except AttributeError:\n            pass\n        self.digest_out = None\n        self.digest_finalized = False", "category": "Python"}, {"instruction": "def parse_labels(result):\n    '''fix up the labels, meaning parse to json if needed, and return\n       original updated object\n\n       Parameters\n       ==========\n       result: the json object to parse from inspect\n    '''\n", "input": "", "output": "\n    if \"data\" in result:\n        labels = result['data']['attributes'].get('labels') or {}\n\n    elif 'attributes' in result:\n        labels = result['attributes'].get('labels') or {}\n\n    # If labels included, try parsing to json\n\n    try:\n        labels = jsonp.loads(labels)\n    except:\n        pass\n\n    if \"data\" in result:\n        result['data']['attributes']['labels'] = labels\n    else:\n        result['attributes']['labels'] = labels\n\n    return result", "category": "Python"}, {"instruction": "def greedy_merge(\n        variant_sequences,\n        min_overlap_size=MIN_VARIANT_SEQUENCE_ASSEMBLY_OVERLAP_SIZE):\n    \"\"\"\n    Greedily merge overlapping sequences into longer sequences.\n\n    Accepts a collection of VariantSequence objects and returns another\n    collection of elongated variant sequences. The reads field of the\n    returned VariantSequence object will contain reads which\n    only partially overlap the full sequence.\n    \"\"\"\n", "input": "", "output": "    merged_any = True\n    while merged_any:\n        variant_sequences, merged_any = greedy_merge_helper(\n            variant_sequences,\n            min_overlap_size=min_overlap_size)\n    return variant_sequences", "category": "Python"}, {"instruction": "def verbosity(verbosity):\n    \"\"\"\n    Convert the number of times the user specified '-v' on the command-line \n    into a log level.\n    \"\"\"\n", "input": "", "output": "    verbosity = int(verbosity)\n\n    if verbosity == 0:\n        return logging.WARNING\n    if verbosity == 1:\n        return logging.INFO\n    if verbosity == 2:\n        return logging.DEBUG\n    if verbosity >= 3:\n        return 0\n    else:\n        raise ValueError", "category": "Python"}, {"instruction": "def beta_r(self, r, kwargs):\n        \"\"\"\n        returns the anisotorpy parameter at a given radius\n        :param r:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if self._type == 'const':\n            return self.const_beta(kwargs)\n        elif self._type == 'OsipkovMerritt':\n            return self.ospikov_meritt(r, kwargs)\n        elif self._type == 'Colin':\n            return self.colin(r, kwargs)\n        elif self._type == 'isotropic':\n            return self.isotropic()\n        elif self._type == 'radial':\n            return self.radial()\n        else:\n            raise ValueError('anisotropy type %s not supported!' % self._type)", "category": "Python"}, {"instruction": "def netbsd_interfaces():\n    '''\n    Obtain interface information for NetBSD >= 8 where the ifconfig\n    output diverged from other BSD variants (Netmask is now part of the\n    address)\n    '''\n", "input": "", "output": "    # NetBSD versions prior to 8.0 can still use linux_interfaces()\n    if LooseVersion(os.uname()[2]) < LooseVersion('8.0'):\n        return linux_interfaces()\n\n    ifconfig_path = salt.utils.path.which('ifconfig')\n    cmd = subprocess.Popen(\n        '{0} -a'.format(ifconfig_path),\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT).communicate()[0]\n    return _netbsd_interfaces_ifconfig(salt.utils.stringutils.to_str(cmd))", "category": "Python"}, {"instruction": "def pass_obj(f):\n    \"\"\"Similar to :func:`pass_context`, but only pass the object on the\n    context onwards (:attr:`Context.obj`).  This is useful if that object\n    represents the state of a nested system.\n    \"\"\"\n", "input": "", "output": "    @pass_context\n    def new_func(*args, **kwargs):\n        ctx = args[0]\n        return ctx.invoke(f, ctx.obj, *args[1:], **kwargs)\n    return update_wrapper(new_func, f)", "category": "Python"}, {"instruction": "def get(self, sid):\n        \"\"\"\n        Constructs a CredentialListContext\n\n        :param sid: The unique string that identifies the resource\n\n        :returns: twilio.rest.trunking.v1.trunk.credential_list.CredentialListContext\n        :rtype: twilio.rest.trunking.v1.trunk.credential_list.CredentialListContext\n        \"\"\"\n", "input": "", "output": "        return CredentialListContext(self._version, trunk_sid=self._solution['trunk_sid'], sid=sid, )", "category": "Python"}, {"instruction": "def SingleStaticServe(file_path):\n    \"\"\"\n    Meta program for serving a single file. Useful for favicon.ico and robots.txt\n    \"\"\"\n", "input": "", "output": "    def get_file():\n        mime, encoding = mimetypes.guess_type(file_path)\n        fullpath = os.path.join(get_config('project_path'), file_path)\n        return open(fullpath, 'rb'), mime or 'application/octet-stream'\n\n    class SingleStaticServe(Program):\n        controllers = ['http-get']\n        model = [get_file]\n        view = FileView()\n\n    return SingleStaticServe()", "category": "Python"}, {"instruction": "def add_escape(self, idx, char):\n        \"\"\"\n        Translates and adds the escape sequence.\n\n        :param idx: Provides the ending index of the escape sequence.\n        :param char: The actual character that was escaped.\n        \"\"\"\n", "input": "", "output": "\n        self.fmt.append_text(self.fmt._unescape.get(\n            self.format[self.str_begin:idx], char))", "category": "Python"}, {"instruction": "def start_event(self):\n        \"\"\"Called by the event loop when it is started.\n\n        Creates the output frame pools (if used) then calls\n        :py:meth:`on_start`. Creating the output frame pools now allows\n        their size to be configured before starting the component.\n\n        \"\"\"\n", "input": "", "output": "        # create object pool for each output\n        if self.with_outframe_pool:\n            self.update_config()\n            for name in self.outputs:\n                self.outframe_pool[name] = ObjectPool(\n                    Frame, self.new_frame, self.config['outframe_pool_len'])\n        try:\n            self.on_start()\n        except Exception as ex:\n            self.logger.exception(ex)\n            raise StopIteration()", "category": "Python"}, {"instruction": "def handle_exception(self, message, exc):\n        # type: (Text, Exception) -> Any\n        \"\"\"\n        Handles an uncaught exception.\n        \"\"\"\n", "input": "", "output": "        return self.handle_invalid_value(\n            message     = message,\n            exc_info    = True,\n            context     = getattr(exc, 'context', {}),\n        )", "category": "Python"}, {"instruction": "def Coerce(type, message=\"Not a valid {} value\"):\n    \"\"\"\n    Creates a validator that attempts to coerce the given value to the\n    specified ``type``. Will raise an error if the coercion fails.\n\n    A custom message can be specified with ``message``.\n    \"\"\"\n", "input": "", "output": "    @wraps(Coerce)\n    def built(value):\n        try:\n            return type(value)\n        except (TypeError, ValueError) as e:\n            raise Error(message.format(type.__name__))\n    return built", "category": "Python"}, {"instruction": "def _verify_support(identity):\n    \"\"\"Make sure the device supports given configuration.\"\"\"\n", "input": "", "output": "    if identity.curve_name not in {formats.CURVE_NIST256}:\n        raise NotImplementedError(\n            'Unsupported elliptic curve: {}'.format(identity.curve_name))", "category": "Python"}, {"instruction": "def page_not_found(request, template_name='404.html'):\n    \"\"\"\n    Default 404 handler.\n\n    Templates: :template:`404.html`\n    Context:\n        request_path\n            The path of the requested URL (e.g., '/app/pages/bad_page/')\n    \"\"\"\n", "input": "", "output": "    response = render_in_page(request, template_name)\n\n    if response:\n        return response\n\n    template = Template(\n        '<h1>Not Found</h1>'\n        '<p>The requested URL {{ request_path }} was not found on this server.</p>')\n    body = template.render(RequestContext(\n        request, {'request_path': request.path}))\n    return http.HttpResponseNotFound(body, content_type=CONTENT_TYPE)", "category": "Python"}, {"instruction": "def zip(self, *items):\n        \"\"\"\n        Zip the collection together with one or more arrays.\n\n        :param items: The items to zip\n        :type items: list\n\n        :rtype: Collection\n        \"\"\"\n", "input": "", "output": "        return self.__class__(list(zip(self.items, *items)))", "category": "Python"}, {"instruction": "def modify_service(self, service_id, type):\n        '''\n        modify_service(self, service_id, type)\n\n        | Modifies a service type (action, container, etc.)\n\n        :Parameters:\n        * *service_id* (`string`) -- Identifier of an existing service\n        * *type* (`string`) -- service type\n\n        :return: Service modification metadata (service id, type, modified date, versions\n\n        :Example:\n        .. code-block:: python\n\n           service_modification_metadata = opereto_client.modify_service ('myService', 'container')\n           if service_modification_metadata['type'] == 'container'\n              print 'service type of {} changed to container'.format('myService')\n\n        '''\n", "input": "", "output": "        request_data = {'id': service_id, 'type': type}\n        return self._call_rest_api('post', '/services', data=request_data, error='Failed to modify service [%s]'%service_id)", "category": "Python"}, {"instruction": "def remove_connection(self, id_interface, back_or_front):\n        \"\"\"\n        Remove a connection between two interfaces\n\n        :param id_interface: One side of relation\n        :param back_or_front: This side of relation is back(0) or front(1)\n\n        :return: None\n\n        :raise InterfaceInvalidBackFrontError: Front or Back of interfaces not match to remove connection\n        :raise InvalidParameterError: Interface id or back or front indicator is none or invalid.\n        :raise DataBaseError: Networkapi failed to access the database.\n        :raise XMLError: Networkapi failed to generate the XML response.\n        \"\"\"\n", "input": "", "output": "\n        msg_err = u'Parameter %s is invalid. Value: %s.'\n\n        if not is_valid_0_1(back_or_front):\n            raise InvalidParameterError(\n                msg_err %\n                ('back_or_front', back_or_front))\n\n        if not is_valid_int_param(id_interface):\n            raise InvalidParameterError(\n                msg_err %\n                ('id_interface', id_interface))\n\n        url = 'interface/%s/%s/' % (str(id_interface), str(back_or_front))\n\n        code, xml = self.submit(None, 'DELETE', url)\n\n        return self.response(code, xml)", "category": "Python"}, {"instruction": "def get_ftp(ftp_conf, debug=0):\n    \"\"\"\u5f97\u5230\u4e00\u4e2a \u5df2\u7ecf\u6253\u5f00\u7684FTP \u5b9e\u4f8b\uff0c\u548c\u4e00\u4e2a ftp \u8def\u5f84\u3002\n\n    :param dict ftp_conf: ftp\u914d\u7f6e\u6587\u4ef6\uff0c\u683c\u5f0f\u5982\u4e0b\uff1a\n    \n        >>> {\n        >>>     'server':'127.0.0.1',\n        >>>     'start_path':None,\n        >>>     'user':'admin',\n        >>>     'password':'123456',\n        >>> }\n\n    :returns: ftp, ftpserverstr\n    :rtype: :class:`ftplib.FTP` , str\n\n    \"\"\"\n", "input": "", "output": "    server = ftp_conf.get('server')\n    user = ftp_conf.get('user')\n    password = ftp_conf.get('password')\n    start_path = ftp_conf.get('start_path')\n    slog.info(\"Connecting FTP server %s ......\", server)\n    ftpStr = 'ftp://%s/'%server\n    if start_path:\n        ftpStr = ftpStr+start_path\n    ftp = ftplib.FTP(server, user, password)\n    ftp.set_debuglevel(debug)\n    if start_path:\n        ftp.cwd(start_path)\n    serverFiles = ftp.nlst()\n    slog.info('There are some files in %s:\\n[%s]'%(ftpStr, ', '.join(serverFiles)))\n    return ftp, ftpStr", "category": "Python"}, {"instruction": "def get_port_channel_detail_output_lacp_isvlag(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_port_channel_detail = ET.Element(\"get_port_channel_detail\")\n        config = get_port_channel_detail\n        output = ET.SubElement(get_port_channel_detail, \"output\")\n        lacp = ET.SubElement(output, \"lacp\")\n        isvlag = ET.SubElement(lacp, \"isvlag\")\n        isvlag.text = kwargs.pop('isvlag')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def getconfig():\n    '''\n    Return the selinux mode from the config file\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' selinux.getconfig\n    '''\n", "input": "", "output": "    try:\n        config = '/etc/selinux/config'\n        with salt.utils.files.fopen(config, 'r') as _fp:\n            for line in _fp:\n                line = salt.utils.stringutils.to_unicode(line)\n                if line.strip().startswith('SELINUX='):\n                    return line.split('=')[1].capitalize().strip()\n    except (IOError, OSError, AttributeError):\n        return None\n    return None", "category": "Python"}, {"instruction": "def find_tag(match: str, strict: bool, directory: str):\n    \"\"\"Find tag for git repository.\"\"\"\n", "input": "", "output": "    with suppress(CalledProcessError):\n        echo(git.find_tag(match, strict=strict, git_dir=directory))", "category": "Python"}, {"instruction": "def log(self, level, *args, **kwargs):\n        \"\"\"Delegate a log call to the underlying logger.\"\"\"\n", "input": "", "output": "        return self._log_kw(level, args, kwargs)", "category": "Python"}, {"instruction": "def _unique_class_name(namespace: Dict[str, Any], uuid: uuid.UUID) -> str:\n    '''Generate unique to namespace name for a class using uuid.\n\n    **Parameters**\n\n    :``namespace``: the namespace to verify uniqueness against\n    :``uuid``:      the \"unique\" portion of the name\n\n    **Return Value(s)**\n\n    A unique string (in namespace) using uuid.\n\n    '''\n", "input": "", "output": "\n    count = 0\n\n    name = original_name = 'f_' + uuid.hex\n    while name in namespace:\n        count += 1\n        name = original_name + '_' + str(count)\n\n    return name", "category": "Python"}, {"instruction": "def alias(self, name, alias_to, path=KISSmetrics.ALIAS_PATH, resp=False):\n        \"\"\"Map `name` to `alias_to`; actions done by one resolve to other.\n\n        :param name: consider as same individual as ``alias_to``\n        :param alias_to: consider an alias of ``name``\n        :param path: endpoint path; defaults to ``KISSmetrics.ALIAS_PATH``\n        :param resp: indicate whether to return response\n        :type resp: boolean\n\n        :returns: an HTTP response for request if `resp=True`\n        :rtype: `urllib3.response.HTTPResponse`\n\n        :raises: Exception if either `identity` or `key` not set\n\n        \"\"\"\n", "input": "", "output": "        self.check_init()\n        response = self.client.alias(person=name, identity=alias_to, path=path)\n        if resp:\n            return response", "category": "Python"}, {"instruction": "def merge_consecutive_self_time(frame, options):\n    '''\n    Combines consecutive 'self time' frames\n    '''\n", "input": "", "output": "    if frame is None:\n        return None\n\n    previous_self_time_frame = None\n\n    for child in frame.children:\n        if isinstance(child, SelfTimeFrame):\n            if previous_self_time_frame:\n                # merge\n                previous_self_time_frame.self_time += child.self_time\n                child.remove_from_parent()\n            else:\n                # keep a reference, maybe it'll be added to on the next loop\n                previous_self_time_frame = child\n        else:\n            previous_self_time_frame = None\n    \n    for child in frame.children:\n        merge_consecutive_self_time(child, options=options)\n    \n    return frame", "category": "Python"}, {"instruction": "def recursive_copy(source, dest):\n    '''\n    Recursively copy the source directory to the destination,\n    leaving files with the source does not explicitly overwrite.\n\n    (identical to cp -r on a unix machine)\n    '''\n", "input": "", "output": "    for root, _, files in salt.utils.path.os_walk(source):\n        path_from_source = root.replace(source, '').lstrip(os.sep)\n        target_directory = os.path.join(dest, path_from_source)\n        if not os.path.exists(target_directory):\n            os.makedirs(target_directory)\n        for name in files:\n            file_path_from_source = os.path.join(source, path_from_source, name)\n            target_path = os.path.join(target_directory, name)\n            shutil.copyfile(file_path_from_source, target_path)", "category": "Python"}, {"instruction": "def get_work(self, function_arn):\n        \"\"\"\n        Retrieve the next work item for specified :code:`function_arn`.\n\n        :param function_arn: Arn of the Lambda function intended to receive the work for processing.\n        :type function_arn: string\n\n        :returns: Next work item to be processed by the function.\n        :type returns: WorkItem\n        \"\"\"\n", "input": "", "output": "        url = self._get_work_url(function_arn)\n        runtime_logger.info('Getting work for function [{}] from {}'.format(function_arn, url))\n\n        request = Request(url)\n        request.add_header(HEADER_AUTH_TOKEN, self.auth_token)\n\n        response = urlopen(request)\n\n        invocation_id = response.info().get(HEADER_INVOCATION_ID)\n        client_context = response.info().get(HEADER_CLIENT_CONTEXT)\n\n        runtime_logger.info('Got work item with invocation id [{}]'.format(invocation_id))\n        return WorkItem(\n            invocation_id=invocation_id,\n            payload=response.read(),\n            client_context=client_context)", "category": "Python"}, {"instruction": "def _scroll(clicks, x=None, y=None):\n    \"\"\"Send the mouse vertical scroll event to Windows by calling the\n    mouse_event() win32 function.\n\n    Args:\n      clicks (int): The amount of scrolling to do. A positive value is the mouse\n      wheel moving forward (scrolling up), a negative value is backwards (down).\n      x (int): The x position of the mouse event.\n      y (int): The y position of the mouse event.\n\n    Returns:\n      None\n    \"\"\"\n", "input": "", "output": "    startx, starty = _position()\n    width, height = _size()\n\n    if x is None:\n        x = startx\n    else:\n        if x < 0:\n            x = 0\n        elif x >= width:\n            x = width - 1\n    if y is None:\n        y = starty\n    else:\n        if y < 0:\n            y = 0\n        elif y >= height:\n            y = height - 1\n\n    try:\n        _sendMouseEvent(MOUSEEVENTF_WHEEL, x, y, dwData=clicks)\n    except (PermissionError, OSError): # TODO: We need to figure out how to prevent these errors, see https://github.com/asweigart/pyautogui/issues/60\n            pass", "category": "Python"}, {"instruction": "def get_global_rate_limit(self):\n        \"\"\"Get the global rate limit per client.\n\n        :rtype: int\n        :returns: The global rate limit for each client.\n        \"\"\"\n", "input": "", "output": "        r = urllib.request.urlopen('https://archive.org/metadata/iamine-rate-limiter')\n        j = json.loads(r.read().decode('utf-8'))\n        return int(j.get('metadata', {}).get('rate_per_second', 300))", "category": "Python"}, {"instruction": "def as_json_range(self, name):\n        \"\"\"Represent the parameter range as a dictionary suitable for a request to\n        create an Amazon SageMaker hyperparameter tuning job using one of the deep learning frameworks.\n\n        The deep learning framework images require that hyperparameters be serialized as JSON.\n\n        Args:\n            name (str): The name of the hyperparameter.\n\n        Returns:\n            dict[str, list[str]]: A dictionary that contains the name and values of the hyperparameter,\n                where the values are serialized as JSON.\n        \"\"\"\n", "input": "", "output": "        return {'Name': name, 'Values': [json.dumps(v) for v in self.values]}", "category": "Python"}, {"instruction": "def job_is_enabled(self, job_id):\n\t\t\"\"\"\n\t\tCheck if a job is enabled.\n\n\t\t:param job_id: Job identifier to check the status of.\n\t\t:type job_id: :py:class:`uuid.UUID`\n\t\t:rtype: bool\n\t\t\"\"\"\n", "input": "", "output": "\t\tjob_id = normalize_job_id(job_id)\n\t\tjob_desc = self._jobs[job_id]\n\t\treturn job_desc['enabled']", "category": "Python"}, {"instruction": "def set_feed_retention_policies(self, policy, feed_id, project=None):\n        \"\"\"SetFeedRetentionPolicies.\n        [Preview API] Set the retention policy for a feed.\n        :param :class:`<FeedRetentionPolicy> <azure.devops.v5_1.feed.models.FeedRetentionPolicy>` policy: Feed retention policy.\n        :param str feed_id: Name or ID of the feed.\n        :param str project: Project ID or project name\n        :rtype: :class:`<FeedRetentionPolicy> <azure.devops.v5_1.feed.models.FeedRetentionPolicy>`\n        \"\"\"\n", "input": "", "output": "        route_values = {}\n        if project is not None:\n            route_values['project'] = self._serialize.url('project', project, 'str')\n        if feed_id is not None:\n            route_values['feedId'] = self._serialize.url('feed_id', feed_id, 'str')\n        content = self._serialize.body(policy, 'FeedRetentionPolicy')\n        response = self._send(http_method='PUT',\n                              location_id='ed52a011-0112-45b5-9f9e-e14efffb3193',\n                              version='5.1-preview.1',\n                              route_values=route_values,\n                              content=content)\n        return self._deserialize('FeedRetentionPolicy', response)", "category": "Python"}, {"instruction": "async def query_firmware(self):\n        \"\"\"Query the firmware versions.\"\"\"\n", "input": "", "output": "\n        _version = await self.request.get(join_path(self._base_path, \"/fwversion\"))\n        _fw = _version.get(\"firmware\")\n        if _fw:\n            _main = _fw.get(\"mainProcessor\")\n            if _main:\n                self._main_processor_version = self._make_version(_main)\n            _radio = _fw.get(\"radio\")\n            if _radio:\n                self._radio_version = self._make_version(_radio)", "category": "Python"}, {"instruction": "def _non_reducing_slice(slice_):\n    \"\"\"\n    Ensurse that a slice doesn't reduce to a Series or Scalar.\n\n    Any user-paseed `subset` should have this called on it\n    to make sure we're always working with DataFrames.\n    \"\"\"\n", "input": "", "output": "    # default to column slice, like DataFrame\n    # ['A', 'B'] -> IndexSlices[:, ['A', 'B']]\n    kinds = (ABCSeries, np.ndarray, Index, list, str)\n    if isinstance(slice_, kinds):\n        slice_ = IndexSlice[:, slice_]\n\n    def pred(part):\n        # true when slice does *not* reduce, False when part is a tuple,\n        # i.e. MultiIndex slice\n        return ((isinstance(part, slice) or is_list_like(part))\n                and not isinstance(part, tuple))\n\n    if not is_list_like(slice_):\n        if not isinstance(slice_, slice):\n            # a 1-d slice, like df.loc[1]\n            slice_ = [[slice_]]\n        else:\n            # slice(a, b, c)\n            slice_ = [slice_]  # to tuplize later\n    else:\n        slice_ = [part if pred(part) else [part] for part in slice_]\n    return tuple(slice_)", "category": "Python"}, {"instruction": "def export_frame_coordinates(topology, trajectory, nframe, output=None):\n    \"\"\"\n    Extract a single frame structure from a trajectory.\n    \"\"\"\n", "input": "", "output": "    if output is None:\n        basename, ext = os.path.splitext(trajectory)\n        output = '{}.frame{}.inpcrd'.format(basename, nframe)\n\n    # ParmEd sometimes struggles with certain PRMTOP files\n    if os.path.splitext(topology)[1] in ('.top', '.prmtop'):\n        top = AmberPrmtopFile(topology)\n        mdtop = mdtraj.Topology.from_openmm(top.topology)\n        traj = mdtraj.load_frame(trajectory, int(nframe), top=mdtop)\n        structure = parmed.openmm.load_topology(top.topology, system=top.createSystem())\n        structure.box_vectors = top.topology.getPeriodicBoxVectors()\n\n    else:  # standard protocol (the topology is loaded twice, though)\n        traj = mdtraj.load_frame(trajectory, int(nframe), top=topology)\n        structure = parmed.load_file(topology)\n\n    structure.positions = traj.openmm_positions(0)\n\n    if traj.unitcell_vectors is not None:  # if frame provides box vectors, use those\n        structure.box_vectors = traj.openmm_boxes(0)\n\n    structure.save(output, overwrite=True)", "category": "Python"}, {"instruction": "def get_nonce_timestamp(nonce):\n    '''\n    Extract the timestamp from a Nonce. To be sure the timestamp was generated by this site,\n    make sure you validate the nonce using validate_nonce().\n    '''\n", "input": "", "output": "    components = nonce.split(':',2)\n    if not len(components) == 3:\n        return None\n\n    try:\n        return float(components[0])\n    except ValueError:\n        return None", "category": "Python"}, {"instruction": "def bokeh(model, scale: float = 0.7, responsive: bool = True):\n    \"\"\"\n    Adds a Bokeh plot object to the notebook display.\n\n    :param model:\n        The plot object to be added to the notebook display.\n    :param scale:\n        How tall the plot should be in the notebook as a fraction of screen\n        height. A number between 0.1 and 1.0. The default value is 0.7.\n    :param responsive:\n        Whether or not the plot should responsively scale to fill the width\n        of the notebook. The default is True.\n    \"\"\"\n", "input": "", "output": "    r = _get_report()\n\n    if 'bokeh' not in r.library_includes:\n        r.library_includes.append('bokeh')\n\n    r.append_body(render_plots.bokeh_plot(\n        model=model,\n        scale=scale,\n        responsive=responsive\n    ))\n    r.stdout_interceptor.write_source('[ADDED] Bokeh plot\\n')", "category": "Python"}, {"instruction": "def urlretrieve(url, dest, write_mode=\"w\"):\n    \"\"\"save a file to disk from a given url\"\"\"\n", "input": "", "output": "    response = urllib2.urlopen(url)\n    mkdir_recursive(os.path.dirname(dest))\n    with open(dest, write_mode) as f:\n        f.write(response.read())\n        f.close()", "category": "Python"}, {"instruction": "def retrieve_api_token(self):\n        \"\"\"\n        Retrieve the access token from AVS.\n\n        This function is memoized, so the\n        value returned by the function will be remembered and returned by\n        subsequent calls until the memo expires. This is because the access\n        token lasts for one hour, then a new token needs to be requested.\n\n        Decorators:\n            helpers.expiring_memo\n\n        Returns:\n            str -- The access token for communicating with AVS\n\n        \"\"\"\n", "input": "", "output": "\n        payload = self.oauth2_manager.get_access_token_params(\n            refresh_token=self.refresh_token\n        )\n        response = requests.post(\n            self.oauth2_manager.access_token_url, json=payload\n        )\n        response.raise_for_status()\n        response_json = json.loads(response.text)\n        return response_json['access_token']", "category": "Python"}, {"instruction": "def get_sqlite_core(connection_string, *, cursor_factory=None, edit_connection=None):\n  \"\"\"Creates a simple SQLite3 core.\"\"\"\n", "input": "", "output": "  import sqlite3 as sqlite\n\n  def opener():\n    ", "category": "Python"}, {"instruction": "def start(self, segment):\n        \"\"\"Begin transfer for an indicated wal segment.\"\"\"\n", "input": "", "output": "\n        if self.closed:\n            raise UserCritical(msg='attempt to transfer wal after closing',\n                               hint='report a bug')\n\n        g = gevent.Greenlet(self.transferer, segment)\n        g.link(self._complete_execution)\n        self.greenlets.add(g)\n\n        # Increment .expect before starting the greenlet, or else a\n        # very unlucky .join could be fooled as to when pool is\n        # complete.\n        self.expect += 1\n\n        g.start()", "category": "Python"}, {"instruction": "def road_analysis_summary_report(feature, parent):\n    \"\"\"Retrieve an HTML road analysis table report from a multi exposure\n    analysis.\n    \"\"\"\n", "input": "", "output": "    _ = feature, parent  # NOQA\n    analysis_dir = get_analysis_dir(exposure_road['key'])\n    if analysis_dir:\n        return get_impact_report_as_string(analysis_dir)\n    return None", "category": "Python"}, {"instruction": "def _non_idempotent_tasks(self, output):\n        \"\"\"\n        Parses the output to identify the non idempotent tasks.\n\n        :param (str) output: A string containing the output of the ansible run.\n        :return: A list containing the names of the non idempotent tasks.\n        \"\"\"\n", "input": "", "output": "        # Remove blank lines to make regex matches easier.\n        output = re.sub(r'\\n\\s*\\n*', '\\n', output)\n\n        # Remove ansi escape sequences.\n        output = util.strip_ansi_escape(output)\n\n        # Split the output into a list and go through it.\n        output_lines = output.split('\\n')\n        res = []\n        task_line = ''\n        for _, line in enumerate(output_lines):\n            if line.startswith('TASK'):\n                task_line = line\n            elif line.startswith('changed'):\n                host_name = re.search(r'\\[(.*)\\]', line).groups()[0]\n                task_name = re.search(r'\\[(.*)\\]', task_line).groups()[0]\n                res.append(u'* [{}] => {}'.format(host_name, task_name))\n\n        return res", "category": "Python"}, {"instruction": "def _read_last_geometry(self):\n        \"\"\"\n        Parses the last geometry from an optimization trajectory for use in a new input file.\n        \"\"\"\n", "input": "", "output": "        header_pattern = r\"\\s+Optimization\\sCycle:\\s+\" + \\\n            str(len(self.data.get(\"energy_trajectory\"))) + \\\n            r\"\\s+Coordinates \\(Angstroms\\)\\s+ATOM\\s+X\\s+Y\\s+Z\"\n        table_pattern = r\"\\s+\\d+\\s+\\w+\\s+([\\d\\-\\.]+)\\s+([\\d\\-\\.]+)\\s+([\\d\\-\\.]+)\"\n        footer_pattern = r\"\\s+Point Group\\:\\s+[\\d\\w\\*]+\\s+Number of degrees of freedom\\:\\s+\\d+\"\n\n        parsed_last_geometry = read_table_pattern(\n            self.text, header_pattern, table_pattern, footer_pattern)\n        if parsed_last_geometry == [] or None:\n            self.data[\"last_geometry\"] = None\n        else:\n            self.data[\"last_geometry\"] = process_parsed_coords(\n                parsed_last_geometry[0])\n            if self.data.get('charge') != None:\n                self.data[\"molecule_from_last_geometry\"] = Molecule(\n                    species=self.data.get('species'),\n                    coords=self.data.get('last_geometry'),\n                    charge=self.data.get('charge'),\n                    spin_multiplicity=self.data.get('multiplicity'))", "category": "Python"}, {"instruction": "def _transfer_data(self, remote_path, data):\n        \"\"\"\n        Used by the base _execute_module(), and in <2.4 also by the template\n        action module, and probably others.\n        \"\"\"\n", "input": "", "output": "        if isinstance(data, dict):\n            data = jsonify(data)\n        if not isinstance(data, bytes):\n            data = to_bytes(data, errors='surrogate_or_strict')\n\n        LOG.debug('_transfer_data(%r, %s ..%d bytes)',\n                  remote_path, type(data), len(data))\n        self._connection.put_data(remote_path, data)\n        return remote_path", "category": "Python"}, {"instruction": "def main(args=None):\n    \"\"\"Main\"\"\"\n", "input": "", "output": "    \n    vs = [(v-100)*0.001 for v in range(200)]\n    \n    for f in ['IM.channel.nml','Kd.channel.nml']:\n        nml_doc = pynml.read_neuroml2_file(f)\n\n        for ct in nml_doc.ComponentType:\n\n            ys = []\n            for v in vs:\n                req_variables = {'v':'%sV'%v,'vShift':'10mV'}\n                vals = pynml.evaluate_component(ct,req_variables=req_variables)\n                print vals\n                if 'x' in vals:\n                    ys.append(vals['x'])\n                if 't' in vals:\n                    ys.append(vals['t'])\n                if 'r' in vals:\n                    ys.append(vals['r'])\n\n            ax = pynml.generate_plot([vs],[ys],          \n                             \"Some traces from %s in %s\"%(ct.name,f),\n                             show_plot_already=False )       \n\n            print vals\n        \n    plt.show()", "category": "Python"}, {"instruction": "def list_locations(self):\n        \"\"\"Print known dist-locations and exit.\n        \"\"\"\n", "input": "", "output": "        known = self.defaults.get_known_locations()\n        for default in self.defaults.distdefault:\n            if default not in known:\n                known.add(default)\n        if not known:\n            err_exit('No locations', 0)\n        for location in sorted(known):\n            if location in self.defaults.distdefault:\n                print(location, '(default)')\n            else:\n                print(location)\n        sys.exit(0)", "category": "Python"}, {"instruction": "def filter_curriculum(curriculum, week, weekday=None):\n    \"\"\"\n    \u7b5b\u9009\u51fa\u6307\u5b9a\u661f\u671f[\u548c\u6307\u5b9a\u661f\u671f\u51e0]\u7684\u8bfe\u7a0b\n\n    :param curriculum: \u8bfe\u7a0b\u8868\u6570\u636e\n    :param week: \u9700\u8981\u7b5b\u9009\u7684\u5468\u6570, \u662f\u4e00\u4e2a\u4ee3\u8868\u5468\u6570\u7684\u6b63\u6574\u6570\n    :param weekday: \u661f\u671f\u51e0, \u662f\u4e00\u4e2a\u4ee3\u8868\u661f\u671f\u7684\u6574\u6570, 1-7 \u5bf9\u5e94\u5468\u4e00\u5230\u5468\u65e5\n    :return: \u5982\u679c weekday \u53c2\u6570\u6ca1\u7ed9\u51fa, \u8fd4\u56de\u7684\u683c\u5f0f\u4e0e\u539f\u8bfe\u8868\u4e00\u81f4, \u4f46\u53ea\u5305\u62ec\u4e86\u5728\u6307\u5b9a\u5468\u6570\u7684\u8bfe\u7a0b, \u5426\u5219\u8fd4\u56de\u6307\u5b9a\u5468\u6570\u548c\u661f\u671f\u51e0\u7684\u5f53\u5929\u8bfe\u7a0b\n    \"\"\"\n", "input": "", "output": "    if weekday:\n        c = [deepcopy(curriculum[weekday - 1])]\n    else:\n        c = deepcopy(curriculum)\n    for d in c:\n        l = len(d)\n        for t_idx in range(l):\n            t = d[t_idx]\n            if t is None:\n                continue\n            # \u4e00\u822c\u540c\u4e00\u65f6\u95f4\u8bfe\u7a0b\u4e0d\u4f1a\u91cd\u590d\uff0c\u91cd\u590d\u65f6\u7ed9\u51fa\u8b66\u544a\n            t = list(filter(lambda k: week in k['\u4e0a\u8bfe\u5468\u6570'], t)) or None\n            if t is not None and len(t) > 1:\n                logger.warning('\u7b2c %d \u5468\u5468 %d \u7b2c %d \u8282\u8bfe\u6709\u51b2\u7a81: %s', week, weekday or c.index(d) + 1, t_idx + 1, t)\n            d[t_idx] = t\n    return c[0] if weekday else c", "category": "Python"}, {"instruction": "def on_delivery(self, name, channel, method, properties, body):\n        \"\"\"Process a message from Rabbit\n\n        :param str name: The connection name\n        :param pika.channel.Channel channel: The message's delivery channel\n        :param pika.frames.MethodFrame method: The method frame\n        :param pika.spec.BasicProperties properties: The message properties\n        :param str body: The message body\n\n        \"\"\"\n", "input": "", "output": "        message = data.Message(name, channel, method, properties, body)\n        if self.is_processing:\n            return self.pending.append(message)\n        self.invoke_consumer(message)", "category": "Python"}, {"instruction": "def append(self, annotation):\n        \"\"\"Add an annotation.\n\n        Args:\n            annotation (gkeepapi.node.Annotation): An Annotation object.\n\n        Returns:\n            gkeepapi.node.Annotation: The Annotation.\n        \"\"\"\n", "input": "", "output": "        self._annotations[annotation.id] = annotation\n        self._dirty = True\n        return annotation", "category": "Python"}, {"instruction": "def compute_residuals(self):\n        \"\"\"Compute residuals and stopping thresholds.\"\"\"\n", "input": "", "output": "\n        r = self.rsdl()\n        adapt_tol = self.opt['RelStopTol']\n\n        if self.opt['AutoStop', 'Enabled']:\n            adapt_tol = self.tau0 / (1. + self.k)\n\n        return r, adapt_tol", "category": "Python"}, {"instruction": "def incr(self, key, incr_by=1):\n        \"\"\"Increment the key by the given amount.\"\"\"\n", "input": "", "output": "        return self.database.hincrby(self.key, key, incr_by)", "category": "Python"}, {"instruction": "def end_of_day(self) -> datetime:\n        \"\"\" End of day \"\"\"\n", "input": "", "output": "        self.value = datetime(self.value.year, self.value.month, self.value.day, 23, 59, 59)\n        return self.value", "category": "Python"}, {"instruction": "def _extract(archive, compression, cmd, format, verbosity, outdir):\n    \"\"\"Extract an LZMA or XZ archive with the lzma Python module.\"\"\"\n", "input": "", "output": "    targetname = util.get_single_outfile(outdir, archive)\n    try:\n        with lzma.LZMAFile(archive, **_get_lzma_options(format)) as lzmafile:\n            with open(targetname, 'wb') as targetfile:\n                data = lzmafile.read(READ_SIZE_BYTES)\n                while data:\n                    targetfile.write(data)\n                    data = lzmafile.read(READ_SIZE_BYTES)\n    except Exception as err:\n        msg = \"error extracting %s to %s: %s\" % (archive, targetname, err)\n        raise util.PatoolError(msg)\n    return None", "category": "Python"}, {"instruction": "def _generate_examples(self, source_file, target_file):\n    \"\"\"This function returns the examples in the raw (text) form.\"\"\"\n", "input": "", "output": "    with tf.io.gfile.GFile(source_file) as f:\n      source_sentences = f.read().split(\"\\n\")\n    with tf.io.gfile.GFile(target_file) as f:\n      target_sentences = f.read().split(\"\\n\")\n\n    assert len(target_sentences) == len(\n        source_sentences), \"Sizes do not match: %d vs %d for %s vs %s.\" % (len(\n            source_sentences), len(target_sentences), source_file, target_file)\n\n    source, target = self.builder_config.language_pair\n    for l1, l2 in zip(source_sentences, target_sentences):\n      result = {source: l1, target: l2}\n      # Make sure that both translations are non-empty.\n      if all(result.values()):\n        yield result", "category": "Python"}, {"instruction": "def _decodeTimestamp(byteIter):\n    \"\"\" Decodes a 7-octet timestamp \"\"\"\n", "input": "", "output": "    dateStr = decodeSemiOctets(byteIter, 7)\n    timeZoneStr = dateStr[-2:]        \n    return datetime.strptime(dateStr[:-2], '%y%m%d%H%M%S').replace(tzinfo=SmsPduTzInfo(timeZoneStr))", "category": "Python"}, {"instruction": "def constrained_to(self, initial_sequence: torch.Tensor, keep_beam_details: bool = True) -> 'BeamSearch':\n        \"\"\"\n        Return a new BeamSearch instance that's like this one but with the specified constraint.\n        \"\"\"\n", "input": "", "output": "        return BeamSearch(self._beam_size, self._per_node_beam_size, initial_sequence, keep_beam_details)", "category": "Python"}, {"instruction": "def security_warnings(request, PROXY_ALLOWED_HOSTS=()):\n    \"\"\" Detects insecure settings and reports them to the client-side context. \"\"\"\n", "input": "", "output": "\n    warnings = []\n\n    PROXY_ALLOWED_HOSTS = PROXY_ALLOWED_HOSTS or getattr(settings, 'PROXY_ALLOWED_HOSTS', ())\n\n    if PROXY_ALLOWED_HOSTS and '*' in PROXY_ALLOWED_HOSTS:\n        warnings.append(dict(title=_('Insecure setting detected.'),\n                             description=_('A wildcard is included in the PROXY_ALLOWED_HOSTS setting.')))\n\n    return dict(warnings=warnings)", "category": "Python"}, {"instruction": "def getGroups(self, proteinId):\n        \"\"\"Return a list of protein groups a protein is associated with.\"\"\"\n", "input": "", "output": "        return [self.groups[gId] for gId in self._proteinToGroupIds[proteinId]]", "category": "Python"}, {"instruction": "def get_segments(segment_id=None):\n    \"\"\"Returns list of all network segments that may be relevant on CVX\"\"\"\n", "input": "", "output": "    session = db.get_reader_session()\n    with session.begin():\n        model = segment_models.NetworkSegment\n        segments = session.query(model).filter_unnecessary_segments()\n        if segment_id:\n            segments = segments.filter(model.id == segment_id)\n    return segments.all()", "category": "Python"}, {"instruction": "def delete_ip_scope(network_address, auth, url):\n    '''Function to delete an entire IP segment from the IMC IP Address management under terminal access\n    :param network_address\n    :param auth\n    :param url\n\n    >>> from pyhpeimc.auth import *\n\n    >>> from pyhpeimc.plat.termaccess import *\n\n    >>> auth = IMCAuth(\"http://\", \"10.101.0.203\", \"8080\", \"admin\", \"admin\")\n\n    >>> new_scope = add_ip_scope('10.50.0.1', '10.50.0.254', 'cyoung', 'test group', auth.creds, auth.url)\n\n    >>> delete_scope = delete_ip_scope('10.50.0.0/24', auth.creds, auth.url)\n\n\n    '''\n", "input": "", "output": "    scope_id = get_scope_id(network_address, auth,url)\n    delete_ip_address_url = ", "category": "Python"}, {"instruction": "def check_keystore_json(jsondata):\n    \"\"\"Check if ``jsondata`` has the structure of a keystore file version 3.\n\n    Note that this test is not complete, e.g. it doesn't check key derivation or cipher parameters.\n\n    :param jsondata: dictionary containing the data from the json file\n    :returns: `True` if the data appears to be valid, otherwise `False`\n    \"\"\"\n", "input": "", "output": "    if 'crypto' not in jsondata and 'Crypto' not in jsondata:\n        return False\n    if 'version' not in jsondata:\n        return False\n    if jsondata['version'] != 3:\n        return False\n    crypto = jsondata.get('crypto', jsondata.get('Crypto'))\n    if 'cipher' not in crypto:\n        return False\n    if 'ciphertext' not in crypto:\n        return False\n    if 'kdf' not in crypto:\n        return False\n    if 'mac' not in crypto:\n        return False\n    return True", "category": "Python"}, {"instruction": "def drop_edge_punct(word: str) -> str:\n    \"\"\"\n    Remove edge punctuation.\n    :param word: a single string\n    >>> drop_edge_punct(\"'fieri\")\n    'fieri'\n    >>> drop_edge_punct('sedes.')\n    'sedes'\n    \"\"\"\n", "input": "", "output": "    if not word:\n        return word\n    try:\n        if not word[0].isalpha():\n            word = word[1:]\n        if not word[-1].isalpha():\n            word = word[:-1]\n    except:\n        pass\n    return word", "category": "Python"}, {"instruction": "def format_header_param(name, value):\n    \"\"\"\n    Helper function to format and quote a single header parameter.\n\n    Particularly useful for header parameters which might contain\n    non-ASCII values, like file names. This follows RFC 2231, as\n    suggested by RFC 2388 Section 4.4.\n\n    :param name:\n        The name of the parameter, a string expected to be ASCII only.\n    :param value:\n        The value of the parameter, provided as a unicode string.\n    \"\"\"\n", "input": "", "output": "    if not any(ch in value for ch in '\"\\\\\\r\\n'):\n        result = '%s=\"%s\"' % (name, value)\n        try:\n            result.encode('ascii')\n        except (UnicodeEncodeError, UnicodeDecodeError):\n            pass\n        else:\n            return result\n    if not six.PY3 and isinstance(value, six.text_type):  # Python 2:\n        value = value.encode('utf-8')\n    value = email.utils.encode_rfc2231(value, 'utf-8')\n    value = '%s*=%s' % (name, value)\n    return value", "category": "Python"}, {"instruction": "def reset(self):\n        \"\"\"Expand to the full scale\"\"\"\n", "input": "", "output": "\n        sun = ephem.Sun()\n        this_time = Time(self.date.get(), scale='utc')\n        sun.compute(this_time.iso)\n        self.sun = Coord((sun.ra, sun.dec))\n\n        self.doplot()\n        self.plot_pointings()", "category": "Python"}, {"instruction": "def relocate(self):\n        \"\"\"Move to the postion of self.SearchVar\"\"\"\n", "input": "", "output": "\n        name=self.SearchVar.get()\n        if kbos.has_key(name):\n\t    import orbfit,ephem,math\n\n\t    jdate=ephem.julian_date(w.date.get())\n \t    try:\n\t        (ra,dec,a,b,ang)=orbfit.predict(kbos[name],jdate,568)\n\t    except:\n\t\treturn\n\t    ra=math.radians(ra)\n\t    dec=math.radians(dec)\n\telif mpc_objs.has_key(name):\n\t    ra=mpc_objs[name].ra\n\t    dec=mpc_objs[name].dec\n\tself.recenter(ra,dec)\n\tself.create_point(ra,dec,color='blue',size=4)", "category": "Python"}, {"instruction": "def _log_agent_name(self, unique_name_file):\n        \"\"\"\n        logs the agent details to logfile\n        unique_name_file (list_agents.txtlist_agents_names.txt) = list of all instances of all agents\n        \"\"\"\n", "input": "", "output": "        agt_list = os.path.join(root_fldr, 'data', unique_name_file)\n        if os.path.exists(agt_list):\n            agents_logged = open(agt_list, 'r').read()\n        else:\n            agents_logged = ''\n        print('agents_logged = ', agents_logged)\n        if self._get_instance() not in agents_logged:\n            with open(agt_list, 'a') as f:\n                f.write(self._get_instance() + '\\n')", "category": "Python"}, {"instruction": "def backward(ctx, grad_output):\n    \"\"\"\n    In the backward pass, we set the gradient to 1 for the winning units, and 0\n    for the others.\n    \"\"\"\n", "input": "", "output": "    indices, = ctx.saved_tensors\n    grad_x = torch.zeros_like(grad_output, requires_grad=True)\n\n    # Probably a better way to do it, but this is not terrible as it only loops\n    # over the batch size.\n    for i in range(grad_output.size(0)):\n      grad_x[i, indices[i]] = grad_output[i, indices[i]]\n\n    return grad_x, None, None, None", "category": "Python"}, {"instruction": "def get_dir_backup():\n    \"\"\"\n    retrieves directory backup\n    \"\"\"\n", "input": "", "output": "    args = parser.parse_args()\n    s3_get_dir_backup(\n        args.aws_access_key_id,\n        args.aws_secret_access_key,\n        args.bucket_name,\n        args.s3_folder,\n        args.zip_backups_dir, args.project)", "category": "Python"}, {"instruction": "def do_powershell_complete(cli, prog_name):\n    \"\"\"Do the powershell completion\n\n    Parameters\n    ----------\n    cli : click.Command\n        The main click Command of the program\n    prog_name : str\n        The program name on the command line\n\n    Returns\n    -------\n    bool\n        True if the completion was successful, False otherwise\n    \"\"\"\n", "input": "", "output": "    commandline = os.environ['COMMANDLINE']\n    args = split_args(commandline)[1:]\n    quote = single_quote\n    incomplete = ''\n    if args and not commandline.endswith(' '):\n        incomplete = args[-1]\n        args = args[:-1]\n        quote_pos = commandline.rfind(incomplete) - 1\n        if quote_pos >= 0 and commandline[quote_pos] == '\"':\n            quote = double_quote\n\n    for item, help in get_choices(cli, prog_name, args, incomplete):\n        echo(quote(item))\n\n    return True", "category": "Python"}, {"instruction": "def get_endpoint_descriptor(self, dev, ep, intf, alt, config):\n        r\"\"\"Return an endpoint descriptor of the given device.\n\n        The object returned is required to have all the Endpoint Descriptor\n        fields acessible as member variables. They must be convertible (but\n        not required to be equal) to the int type.\n\n        The ep parameter is the endpoint logical index (not the bEndpointAddress\n        field) of the endpoint descriptor desired. dev, intf, alt and config are the same\n        values already described in the get_interface_descriptor() method.\n        \"\"\"\n", "input": "", "output": "        _not_implemented(self.get_endpoint_descriptor)", "category": "Python"}, {"instruction": "def find_donor_catchments(self, limit=6, dist_limit=500):\n        \"\"\"\n        Return a suitable donor catchment to improve a QMED estimate based on catchment descriptors alone.\n\n        :param limit: maximum number of catchments to return. Default: 6. Set to `None` to return all available\n                      catchments.\n        :type limit: int\n        :param dist_limit: maximum distance in km. between subject and donor catchment. Default: 500 km. Increasing the\n                           maximum distance will increase computation time!\n        :type dist_limit: float or int\n        :return: list of nearby catchments\n        :rtype: :class:`floodestimation.entities.Catchment`\n        \"\"\"\n", "input": "", "output": "        if self.gauged_catchments:\n            return self.gauged_catchments.nearest_qmed_catchments(self.catchment, limit, dist_limit)\n        else:\n            return []", "category": "Python"}, {"instruction": "def communicates(self,fromstate, tostate, maxlength=999999):\n        \"\"\"See if a node communicates (directly or indirectly) with another. Returns the probability of the *shortest* path (probably, but not necessarily the highest probability)\"\"\"\n", "input": "", "output": "        if (not (fromstate in self.nodes)) or (not (tostate in self.nodes)):\n            return 0\n        assert (fromstate != tostate)\n\n\n        def _test(node,length,prob):\n            if length > maxlength:\n                return 0\n            if node == tostate:\n                prob *= self.edges_out[node][tostate]\n                return True\n\n            for child in self.edges_out[node].keys():\n                if not child in visited:\n                    visited.add(child)\n                    if child == tostate:\n                        return prob * self.edges_out[node][tostate]\n                    else:\n                        r = _test(child, length+1, prob * self.edges_out[node][tostate])\n                        if r:\n                            return r\n            return 0\n\n        visited = set(fromstate)\n        return _test(fromstate,1,1)", "category": "Python"}, {"instruction": "def main(argv=None):\n  \"\"\"Routine to page text or determine window size via command line.\"\"\"\n", "input": "", "output": "\n  if argv is None:\n    argv = sys.argv\n\n  try:\n    opts, args = getopt.getopt(argv[1:], 'dhs', ['nodelay', 'help', 'size'])\n  except getopt.error as msg:\n    raise Usage(msg)\n\n  # Print usage and return, regardless of presence of other args.\n  for opt, _ in opts:\n    if opt in ('-h', '--help'):\n      print(__doc__)\n      print(help_msg)\n      return 0\n\n  isdelay = False\n  for opt, _ in opts:\n    # Prints the size of the terminal and returns.\n    # Mutually exclusive to the paging of text and overrides that behaviour.\n    if opt in ('-s', '--size'):\n      print('Length: %d, Width: %d' % TerminalSize())\n      return 0\n    elif opt in ('-d', '--delay'):\n      isdelay = True\n    else:\n      raise Usage('Invalid arguments.')\n\n  # Page text supplied in either specified file or stdin.\n\n  if len(args) == 1:\n    with open(args[0]) as f:\n      fd = f.read()\n  else:\n    fd = sys.stdin.read()\n  Pager(fd, delay=isdelay).Page()", "category": "Python"}, {"instruction": "def pick_and_display_buffer(self, i):\n        \"\"\"\n        pick i-th buffer from list and display it\n\n        :param i: int\n        :return: None\n        \"\"\"\n", "input": "", "output": "        if len(self.buffers) == 1:\n            # we don't need to display anything\n            # listing is already displayed\n            return\n        else:\n            try:\n                self.display_buffer(self.buffers[i])\n            except IndexError:\n                # i > len\n                self.display_buffer(self.buffers[0])", "category": "Python"}, {"instruction": "async def _query_chunked_post(\n        self, path, method=\"POST\", *, params=None, data=None, headers=None, timeout=None\n    ):\n        \"\"\"\n        A shorthand for uploading data by chunks\n        \"\"\"\n", "input": "", "output": "        if headers is None:\n            headers = {}\n        if headers and \"content-type\" not in headers:\n            headers[\"content-type\"] = \"application/octet-stream\"\n        response = await self._query(\n            path,\n            method,\n            params=params,\n            data=data,\n            headers=headers,\n            timeout=timeout,\n            chunked=True,\n        )\n        return response", "category": "Python"}, {"instruction": "def get_experiment_info(self):\n        \"\"\"Get a dictionary with information about this experiment.\n\n        Contains:\n          * *name*: the name\n          * *sources*: a list of sources (filename, md5)\n          * *dependencies*: a list of package dependencies (name, version)\n\n        :return: experiment information\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        dependencies = set()\n        sources = set()\n        for ing, _ in self.traverse_ingredients():\n            dependencies |= ing.dependencies\n            sources |= ing.sources\n\n        for dep in dependencies:\n            dep.fill_missing_version()\n\n        mainfile = (self.mainfile.to_json(self.base_dir)[0]\n                    if self.mainfile else None)\n\n        def name_lower(d):\n            return d.name.lower()\n\n        return dict(\n            name=self.path,\n            base_dir=self.base_dir,\n            sources=[s.to_json(self.base_dir) for s in sorted(sources)],\n            dependencies=[d.to_json()\n                          for d in sorted(dependencies, key=name_lower)],\n            repositories=collect_repositories(sources),\n            mainfile=mainfile\n        )", "category": "Python"}, {"instruction": "def checkAllTriggers(self, action):\n        \"\"\" Go over all widgets and let them know they have been edited\n            recently and they need to check for any trigger actions.  This\n            would be used right after all the widgets have their values\n            set or forced (e.g. via setAllEntriesFromParList). \"\"\"\n", "input": "", "output": "        for entry in self.entryNo:\n            entry.widgetEdited(action=action, skipDups=False)", "category": "Python"}, {"instruction": "def getRolesForUser(self, username, filter=None, maxCount=None):\n        \"\"\"\n           This operation returns a list of role names that have been\n           assigned to a particular user account.\n           Inputs:\n              username - name of the user for whom the returned roles\n              filter - filter to be applied to the resultant role set.\n              maxCount - maximum number of results to return for this query\n        \"\"\"\n", "input": "", "output": "        uURL = self._url + \"/roles/getRolesForUser\"\n        params = {\n            \"f\" : \"json\",\n            \"username\" : username\n        }\n        if filter is not None:\n            params['filter'] = filter\n\n        if maxCount is not None:\n            params['maxCount'] = maxCount\n        return self._post(url=uURL, param_dict=params,\n                             securityHandler=self._securityHandler,\n                             proxy_url=self._proxy_url,\n                             proxy_port=self._proxy_port)", "category": "Python"}, {"instruction": "def keep_resample(nkeep, X_train, y_train, X_test, y_test, attr_test, model_generator, metric, trained_model, random_state):\n    \"\"\" The model is revaluated for each test sample with the non-important features set to resample background values.\n    \"\"\"\n", "input": "", "output": "\n    X_train, X_test = to_array(X_train, X_test)\n\n    # how many features to mask\n    assert X_train.shape[1] == X_test.shape[1]\n\n    # how many samples to take\n    nsamples = 100\n\n    # keep nkeep top features for each test explanation\n    N,M = X_test.shape\n    X_test_tmp = np.tile(X_test, [1, nsamples]).reshape(nsamples * N, M)\n    tie_breaking_noise = const_rand(M) * 1e-6\n    inds = sklearn.utils.resample(np.arange(N), n_samples=nsamples, random_state=random_state)\n    for i in range(N):\n        if nkeep[i] < M:\n            ordering = np.argsort(-attr_test[i,:] + tie_breaking_noise)\n            X_test_tmp[i*nsamples:(i+1)*nsamples, ordering[nkeep[i]:]] = X_train[inds, :][:, ordering[nkeep[i]:]]\n\n    yp_masked_test = trained_model.predict(X_test_tmp)\n    yp_masked_test = np.reshape(yp_masked_test, (N, nsamples)).mean(1) # take the mean output over all samples\n\n    return metric(y_test, yp_masked_test)", "category": "Python"}, {"instruction": "def _main(self):\n        \"\"\"\n        process\n        \"\"\"\n", "input": "", "output": "        probes = self.config.get('probes', None)\n        if not probes:\n            raise ValueError('no probes specified')\n\n        for probe_config in self.config['probes']:\n            probe = plugin.get_probe(probe_config, self.plugin_context)\n            # FIXME - needs to check for output defined in plugin\n            if 'output' not in probe_config:\n                raise ValueError(\"no output specified\")\n\n            # get all output targets and start / join them\n            for output_name in probe_config['output']:\n                output = plugin.get_output(output_name, self.plugin_context)\n                if not output.started:\n                    output.start()\n                    self.joins.append(output)\n                probe._emit.append(output)\n\n            probe.start()\n            self.joins.append(probe)\n\n        vaping.io.joinall(self.joins)\n        return 0", "category": "Python"}, {"instruction": "def plot(self, data):\n    \"\"\" Plots an overview in a list of dataframes\n\n    Args:\n      data: a dictionary with key the name, and value the dataframe.\n    \"\"\"\n", "input": "", "output": "\n    import IPython\n\n    if not isinstance(data, dict) or not all(isinstance(v, pd.DataFrame) for v in data.values()):\n      raise ValueError('Expect a dictionary where the values are all dataframes.')\n\n    gfsg = GenericFeatureStatisticsGenerator()\n    data = [{'name': k, 'table': self._remove_nonascii(v)} for k, v in six.iteritems(data)]\n    data_proto = gfsg.ProtoFromDataFrames(data)\n    protostr = base64.b64encode(data_proto.SerializeToString()).decode(\"utf-8\")\n    html_id = 'f' + datalab.utils.commands.Html.next_id()\n\n    HTML_TEMPLATE = ", "category": "Python"}, {"instruction": "def order_preserving_single_index_shift(arr, index, new_index):\n    \"\"\"Moves a list element to a new index while preserving order.\n\n    Parameters\n    ---------\n    arr : list\n        The list in which to shift an element.\n    index : int\n        The index of the element to shift.\n    new_index : int\n        The index to which to shift the element.\n\n    Returns\n    -------\n    list\n        The list with the element shifted.\n\n    Example\n    -------\n    >>> arr = ['a', 'b', 'c', 'd']\n    >>> order_preserving_single_index_shift(arr, 2, 0)\n    ['c', 'a', 'b', 'd']\n    >>> order_preserving_single_index_shift(arr, 2, 3)\n    ['a', 'b', 'd', 'c']\n    \"\"\"\n", "input": "", "output": "    if new_index == 0:\n        return [arr[index]] + arr[0:index] + arr[index+1:]\n    if new_index == len(arr) - 1:\n        return arr[0:index] + arr[index+1:] + [arr[index]]\n    if index < new_index:\n        return arr[0:index] + arr[index+1:new_index+1] + [arr[index]] + arr[\n            new_index+1:]\n    if new_index <= index:\n        return arr[0:new_index] + [arr[index]] + arr[new_index:index] + arr[\n            index+1:]", "category": "Python"}, {"instruction": "def _get_items_from_page(self,\n                             item_type,\n                             items_key,\n                             resource,\n                             ):\n        \"\"\"\n        :type item_type: type\n        :type items_key: str\n        :type resource: Dict[str, Any]\n        :rtype: Union[List[Dashboard], List[Issue]]\n        \"\"\"\n", "input": "", "output": "        try:\n            return [item_type(self._options, self._session, raw_issue_json) for raw_issue_json in\n                    (resource[items_key] if items_key else resource)]\n        except KeyError as e:\n            # improving the error text so we know why it happened\n            raise KeyError(str(e) + \" : \" + json.dumps(resource))", "category": "Python"}, {"instruction": "def show(uuid):\n    '''\n    Show manifest of a given image\n\n    uuid : string\n        uuid of image\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' imgadm.show e42f8c84-bbea-11e2-b920-078fab2aab1f\n        salt '*' imgadm.show plexinc/pms-docker:plexpass\n    '''\n", "input": "", "output": "    ret = {}\n\n    if _is_uuid(uuid) or _is_docker_uuid(uuid):\n        cmd = 'imgadm show {0}'.format(uuid)\n        res = __salt__['cmd.run_all'](cmd, python_shell=False)\n        retcode = res['retcode']\n        if retcode != 0:\n            ret['Error'] = _exit_status(retcode, res['stderr'])\n        else:\n            ret = salt.utils.json.loads(res['stdout'])\n    else:\n        ret['Error'] = \"{} is not a valid uuid.\".format(uuid)\n\n    return ret", "category": "Python"}, {"instruction": "def _get_state(self):\n        \"\"\"get state instance which was clicked on\n\n        :return: State that represents the icon which was clicked on\n        :rtype: rafcon.core.states.State\n        \"\"\"\n", "input": "", "output": "\n        selected = self.view.get_selected_items()\n        if not selected:\n            return\n        shorthand, state_class = self.view.states[selected[0][0]]\n        return state_class()", "category": "Python"}, {"instruction": "def members(self):\n        \"\"\"Gets members of current team\n\n        Returns:\n            list of User\n\n        Throws:\n            RTMServiceError when request failed\n        \"\"\"\n", "input": "", "output": "        resp = self._rtm_client.get('v1/current_team.members?all=true')\n        if resp.is_fail():\n            raise RTMServiceError(\n                'Failed to get members of current team',\n                resp\n            )\n        return resp.data['result']", "category": "Python"}, {"instruction": "def update(table, values, where=(), **kwargs):\r\n    \"\"\"Convenience wrapper for database UPDATE.\"\"\"\n", "input": "", "output": "    where = dict(where, **kwargs).items()\r\n    sql, args = makeSQL(\"UPDATE\", table, values=values, where=where)\r\n    return execute(sql, args).rowcount", "category": "Python"}, {"instruction": "def get_label_items(self, label_id, top=None, skip=None):\n        \"\"\"GetLabelItems.\n        Get items under a label.\n        :param str label_id: Unique identifier of label\n        :param int top: Max number of items to return\n        :param int skip: Number of items to skip\n        :rtype: [TfvcItem]\n        \"\"\"\n", "input": "", "output": "        route_values = {}\n        if label_id is not None:\n            route_values['labelId'] = self._serialize.url('label_id', label_id, 'str')\n        query_parameters = {}\n        if top is not None:\n            query_parameters['$top'] = self._serialize.query('top', top, 'int')\n        if skip is not None:\n            query_parameters['$skip'] = self._serialize.query('skip', skip, 'int')\n        response = self._send(http_method='GET',\n                              location_id='06166e34-de17-4b60-8cd1-23182a346fda',\n                              version='5.0',\n                              route_values=route_values,\n                              query_parameters=query_parameters)\n        return self._deserialize('[TfvcItem]', self._unwrap_collection(response))", "category": "Python"}, {"instruction": "def parse(self, text, encoding='utf8', raise_exception=False):\n        \"\"\"Alias of helper.string.serialization.json.parse\"\"\"\n", "input": "", "output": "\n        return self.helper.string.serialization.json.parse(\n            text=text,\n            encoding=encoding,\n            raise_exception=raise_exception)", "category": "Python"}, {"instruction": "def all(self, **kwargs):\n        \"\"\"List all the members, included inherited ones.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of members\n        \"\"\"\n", "input": "", "output": "\n        path = '%s/all' % self.path\n        obj = self.gitlab.http_list(path, **kwargs)\n        return [self._obj_cls(self, item) for item in obj]", "category": "Python"}, {"instruction": "def _getue(self):\n        \"\"\"Return data as unsigned exponential-Golomb code.\n\n        Raises InterpretError if bitstring is not a single exponential-Golomb code.\n\n        \"\"\"\n", "input": "", "output": "        try:\n            value, newpos = self._readue(0)\n            if value is None or newpos != self.len:\n                raise ReadError\n        except ReadError:\n            raise InterpretError(\"Bitstring is not a single exponential-Golomb code.\")\n        return value", "category": "Python"}, {"instruction": "def chunk(iterable, chunk_size=20):\n    \"\"\"chunk an iterable [1,2,3,4,5,6,7,8] -> ([1,2,3], [4,5,6], [7,8])\"\"\"\n", "input": "", "output": "    items = []\n    for value in iterable:\n        items.append(value)\n        if len(items) == chunk_size:\n            yield items\n            items = []\n    if items:\n        yield items", "category": "Python"}, {"instruction": "def ensure_object_is_ordered_dict(item, title):\n    \"\"\"\n    Checks that the item is an OrderedDict. If not, raises ValueError.\n    \"\"\"\n", "input": "", "output": "    assert isinstance(title, str)\n\n    if not isinstance(item, OrderedDict):\n        msg = \"{} must be an OrderedDict. {} passed instead.\"\n        raise TypeError(msg.format(title, type(item)))\n\n    return None", "category": "Python"}, {"instruction": "def get_transcripts(self):\n    \"\"\" a list of the transcripts in the locus\"\"\"\n", "input": "", "output": "    txs = []\n    for pays in [x.payload for x in self.g.get_nodes()]:\n      for pay in pays:\n        txs.append(pay)\n    return txs", "category": "Python"}, {"instruction": "def convert2(self, imtls, sids):\n        \"\"\"\n        Convert a probability map into a composite array of shape (N,)\n        and dtype `imtls.dt`.\n\n        :param imtls:\n            DictArray instance\n        :param sids:\n            the IDs of the sites we are interested in\n        :returns:\n            an array of curves of shape (N,)\n        \"\"\"\n", "input": "", "output": "        assert self.shape_z == 1, self.shape_z\n        curves = numpy.zeros(len(sids), imtls.dt)\n        for imt in curves.dtype.names:\n            curves_by_imt = curves[imt]\n            for i, sid in numpy.ndenumerate(sids):\n                try:\n                    pcurve = self[sid]\n                except KeyError:\n                    pass  # the poes will be zeros\n                else:\n                    curves_by_imt[i] = pcurve.array[imtls(imt), 0]\n        return curves", "category": "Python"}, {"instruction": "def fetch_list(cls, client, ids):\n        \"\"\"\n        fetch instruments by ids\n        \"\"\"\n", "input": "", "output": "        results = []\n        request_url = \"https://api.robinhood.com/options/instruments/\"\n\n        for _ids in chunked_list(ids, 50):\n\n            params = {\"ids\": \",\".join(_ids)}\n            data = client.get(request_url, params=params)\n            partial_results = data[\"results\"]\n\n            while data[\"next\"]:\n                data = client.get(data[\"next\"])\n                partial_results.extend(data[\"results\"])\n            results.extend(partial_results)\n\n        return results", "category": "Python"}, {"instruction": "def DedupVcardFilenames(vcard_dict):\n    \"\"\"Make sure every vCard in the dictionary has a unique filename.\"\"\"\n", "input": "", "output": "    remove_keys = []\n    add_pairs = []\n    for k, v in vcard_dict.items():\n        if not len(v) > 1:\n            continue\n        for idx, vcard in enumerate(v):\n            fname, ext = os.path.splitext(k)\n            fname = '{}-{}'.format(fname, idx + 1)\n            fname = fname + ext\n            assert fname not in vcard_dict\n            add_pairs.append((fname, vcard))\n        remove_keys.append(k)\n\n    for k, v in add_pairs:\n        vcard_dict[k].append(v)\n\n    for k in remove_keys:\n        vcard_dict.pop(k)\n\n    return vcard_dict", "category": "Python"}, {"instruction": "def get_daily_rates(date_req: datetime.datetime = None, lang: str = 'rus') -> Element:\n    \"\"\" Getting currency for current day.\n\n    see example: http://www.cbr.ru/scripts/Root.asp?PrtId=SXML\n\n    :param date_req:\n    :type date_req: datetime.datetime\n    :param lang: language of API response ('eng' || 'rus')\n    :type lang: str\n\n    :return: :class: `Element <Element 'ValCurs'>` object\n    :rtype: ElementTree.Element\n    \"\"\"\n", "input": "", "output": "    if lang not in ['rus', 'eng']:\n        raise ValueError('\"lang\" must be string. \"rus\" or \"eng\"')\n\n    base_url = const.CBRF_API_URLS['daily_rus'] if lang == 'rus' \\\n        else const.CBRF_API_URLS['daily_eng']\n\n    url = base_url + 'date_req=' + utils.date_to_str(date_req) if date_req else base_url\n\n    response = requests.get(url=url)\n\n    return XML(response.text)", "category": "Python"}, {"instruction": "def GetDecoder(cls, encoding_method):\n    \"\"\"Retrieves the decoder object for a specific encoding method.\n\n    Args:\n      encoding_method (str): encoding method identifier.\n\n    Returns:\n      Decoder: decoder or None if the encoding method does not exists.\n    \"\"\"\n", "input": "", "output": "    encoding_method = encoding_method.lower()\n    decoder = cls._decoders.get(encoding_method, None)\n    if not decoder:\n      return None\n\n    return decoder()", "category": "Python"}, {"instruction": "def get_variable_dtype(\n    master_dtype=tf.bfloat16,\n    slice_dtype=tf.float32,\n    activation_dtype=tf.float32):\n  \"\"\"Datatypes to use for the run.\n\n  Args:\n    master_dtype: string, datatype for checkpoints\n      keep this the same between training and eval/inference\n    slice_dtype: string, datatype for variables in memory\n      must be tf.float32 for training\n    activation_dtype: string, datatype for activations\n      less memory usage if tf.bfloat16 but possible numerical issues\n  Returns:\n    a mtf.VariableDtype\n  \"\"\"\n", "input": "", "output": "  return mtf.VariableDType(\n      master_dtype=tf.as_dtype(master_dtype),\n      slice_dtype=tf.as_dtype(slice_dtype),\n      activation_dtype=tf.as_dtype(activation_dtype))", "category": "Python"}, {"instruction": "def request_add(self, req, x, y):\n        \"\"\"Add two numbers\"\"\"\n", "input": "", "output": "        r = x + y\n        self._add_result.set_value(r)\n        return (\"ok\", r)", "category": "Python"}, {"instruction": "def render(self):\n        \"\"\"Render the screen. Here you must draw everything.\"\"\"\n", "input": "", "output": "        self.screen.fill(self.BACKGROUND_COLOR)\n\n        for wid, cond in self._widgets:\n            if cond():\n                wid.render(self.screen)\n\n        if self.BORDER_COLOR is not None:\n            pygame.draw.rect(self.screen, self.BORDER_COLOR, ((0, 0), self.SCREEN_SIZE), 1)\n\n        if self.SHOW_FPS:\n            self.fps.render(self.screen)", "category": "Python"}, {"instruction": "def make_context(self, docker_file=None):\n        \"\"\"Determine the docker lines for this image\"\"\"\n", "input": "", "output": "        kwargs = {\"silent_build\": self.harpoon.silent_build, \"extra_context\": self.commands.extra_context}\n        if docker_file is None:\n            docker_file = self.docker_file\n        with ContextBuilder().make_context(self.context, **kwargs) as ctxt:\n            self.add_docker_file_to_tarfile(docker_file, ctxt.t)\n            yield ctxt", "category": "Python"}, {"instruction": "def pretty_duration(seconds):\n    \"\"\" Returns a user-friendly representation of the provided duration in seconds.\n    For example: 62.8 => \"1m2.8s\", or 129837.8 => \"2d12h4m57.8s\"\n    \"\"\"\n", "input": "", "output": "    if seconds is None:\n        return ''\n    ret = ''\n    if seconds >= 86400:\n        ret += '{:.0f}d'.format(int(seconds / 86400))\n        seconds = seconds % 86400\n    if seconds >= 3600:\n        ret += '{:.0f}h'.format(int(seconds / 3600))\n        seconds = seconds % 3600\n    if seconds >= 60:\n        ret += '{:.0f}m'.format(int(seconds / 60))\n        seconds = seconds % 60\n    if seconds > 0:\n        ret += '{:.1f}s'.format(seconds)\n    return ret", "category": "Python"}, {"instruction": "def multi_lpop(self, queue, number, transaction=False):\r\n        ''' Pops multiple elements from a list '''\n", "input": "", "output": "        try:\r\n            self._multi_lpop_pipeline(self, queue, number)\r\n        except:\r\n            raise", "category": "Python"}, {"instruction": "def cmd():\n    \"\"\"\n    Help to run the command line\n    :return:\n    \"\"\"\n", "input": "", "output": "    if os.path.isfile(os.path.join(os.path.join(CWD, \"juicy.py\"))):\n        import_module(\"juicy\")\n    else:\n        print(\"ERROR: Missing <<'juicy.py'>> @ %s\" % CWD)\n\n    cli()", "category": "Python"}, {"instruction": "def set_layout(self, x, y, offset, w, h):\n        \"\"\"\n        Set the size and position of the Widget.\n\n        This should not be called directly.  It is used by the :py:obj:`.Layout` class to arrange\n        all widgets within the Frame.\n\n        :param x: The x position of the widget.\n        :param y: The y position of the widget.\n        :param offset: The allowed label size for the widget.\n        :param w: The width of the widget.\n        :param h: The height of the widget.\n        \"\"\"\n", "input": "", "output": "        self._x = x\n        self._y = y\n        self._offset = offset\n        self._w = w\n        self._h = h", "category": "Python"}, {"instruction": "def __format_error(self, error_list_tag):\n    \"\"\"Format this error into a JSON response.\n\n    Args:\n      error_list_tag: A string specifying the name of the tag to use for the\n        error list.\n\n    Returns:\n      A dict containing the reformatted JSON error response.\n    \"\"\"\n", "input": "", "output": "    error = {'domain': self.domain(),\n             'reason': self.reason(),\n             'message': self.message()}\n    error.update(self.extra_fields() or {})\n    return {'error': {error_list_tag: [error],\n                      'code': self.status_code(),\n                      'message': self.message()}}", "category": "Python"}, {"instruction": "def create_user(email, password, active, confirmed_at, send_email):\n    \"\"\"\n    Create a new user.\n    \"\"\"\n", "input": "", "output": "    if confirmed_at == 'now':\n        confirmed_at = security.datetime_factory()\n    user = user_manager.create(email=email, password=password, active=active,\n                               confirmed_at=confirmed_at)\n    if click.confirm(f'Are you sure you want to create {user!r}?'):\n        security_service.register_user(user, allow_login=False, send_email=send_email)\n        user_manager.save(user, commit=True)\n        click.echo(f'Successfully created {user!r}')\n    else:\n        click.echo('Cancelled.')", "category": "Python"}, {"instruction": "def request_id(self, request_id):\n        \"\"\"\n        Sets the request_id of this ErrorResponse.\n        Request ID.\n\n        :param request_id: The request_id of this ErrorResponse.\n        :type: str\n        \"\"\"\n", "input": "", "output": "        if request_id is not None and not re.search('^[A-Za-z0-9]{32}', request_id):\n            raise ValueError(\"Invalid value for `request_id`, must be a follow pattern or equal to `/^[A-Za-z0-9]{32}/`\")\n\n        self._request_id = request_id", "category": "Python"}, {"instruction": "def _clear_context():\n    '''\n    Remove context\n    '''\n", "input": "", "output": "    # Using list() here because modifying a dictionary during iteration will\n    # raise a RuntimeError.\n    for key in list(__context__):\n        try:\n            if key.startswith('systemd._systemctl_status.'):\n                __context__.pop(key)\n        except AttributeError:\n            continue", "category": "Python"}, {"instruction": "def convert(outputfile, inputfile, to_format, from_format):\n    \"\"\"\n    Convert pretrained word embedding file in one format to another.\n    \"\"\"\n", "input": "", "output": "    emb = word_embedding.WordEmbedding.load(\n        inputfile, format=_input_choices[from_format][1],\n        binary=_input_choices[from_format][2])\n    emb.save(outputfile, format=_output_choices[to_format][1],\n             binary=_output_choices[to_format][2])", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"\n        Starts the timer from zero\n        \"\"\"\n", "input": "", "output": "        self.startTime = time.time()\n        self.configure(text='{0:<d} s'.format(0))\n        self.update()", "category": "Python"}, {"instruction": "def stop_tcp_server(self):\n        \"\"\"\n        Stops the TCP server.\n\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        if self.__tcp_server.online:\n            if self.__tcp_server.stop():\n                self.__engine.notifications_manager.notify(\n                    \"{0} | TCP Server has stopped!\".format(self.__class__.__name__))\n                return True\n        else:\n            self.__engine.notifications_manager.warnify(\n                \"{0} | TCP Server is not online!\".format(self.__class__.__name__))\n            return False", "category": "Python"}, {"instruction": "def exit(message, code=0):\n    \"\"\" output a message to stdout and terminates the process.\n\n    :param message:\n        Message to be outputed.\n    :type message:\n        String\n    :param code:\n        The termination code. Default is 0\n    :type code:\n        int\n\n    :returns:\n        void\n    \"\"\"\n", "input": "", "output": "\n    v = VerbosityMixin()\n    if code == 0:\n        v.output(message, normal=True, arrow=True)\n        v.output('Done!', normal=True, arrow=True)\n    else:\n        v.output(message, normal=True, error=True)\n    sys.exit(code)", "category": "Python"}, {"instruction": "def construct_kde(samples_array, use_kombine=False):\n    \"\"\"Constructs a KDE from the given samples.\n    \"\"\"\n", "input": "", "output": "    if use_kombine:\n        try:\n            import kombine\n        except ImportError:\n            raise ImportError(\"kombine is not installed.\")\n    # construct the kde\n    if use_kombine:\n        kde = kombine.clustered_kde.KDE(samples_array)\n    else:\n        kde = scipy.stats.gaussian_kde(samples_array.T)\n    return kde", "category": "Python"}, {"instruction": "def execute(self):\n        \"\"\" Execute this generator regarding its current configuration.\n        \"\"\"\n", "input": "", "output": "        if self.direct:\n            if self.file_type == 'pdf':\n                raise IOError(u\"Direct output mode is not available for PDF \"\n                               \"export\")\n            else:\n                print(self.render().encode(self.encoding))\n        else:\n            self.write_and_log()\n\n            if self.watch:\n                from landslide.watcher import watch\n\n                self.log(u\"Watching %s\\n\" % self.watch_dir)\n\n                watch(self.watch_dir, self.write_and_log)", "category": "Python"}, {"instruction": "def register_with_password(self, username, password):\n        \"\"\" Register for a new account on this HS.\n\n        Args:\n            username (str): Account username\n            password (str): Account password\n\n        Returns:\n            str: Access Token\n\n        Raises:\n            MatrixRequestError\n        \"\"\"\n", "input": "", "output": "        response = self.api.register(\n                auth_body={\"type\": \"m.login.dummy\"},\n                kind='user',\n                username=username,\n                password=password,\n        )\n        return self._post_registration(response)", "category": "Python"}, {"instruction": "def set_connections_params(self, harakiri=None, timeout_socket=None, retry_delay=None, retry_max=None):\n        \"\"\"Sets connection-related parameters.\n\n        :param int harakiri: Set gateway harakiri timeout (seconds).\n\n        :param int timeout_socket: Node socket timeout (seconds). Default: 60.\n\n        :param int retry_delay: Retry connections to dead static nodes after the specified\n            amount of seconds. Default: 30.\n\n        :param int retry_max: Maximum number of retries/fallbacks to other nodes. Default: 3.\n\n        \"\"\"\n", "input": "", "output": "        super(RouterSsl, self).set_connections_params(**filter_locals(locals(), ['retry_max']))\n\n        self._set_aliased('max-retries', retry_max)\n\n        return self", "category": "Python"}, {"instruction": "async def peer(client: Client, peer_signed_raw: str) -> ClientResponse:\n    \"\"\"\n    POST a Peer signed raw document\n\n    :param client: Client to connect to the api\n    :param peer_signed_raw: Peer signed raw document\n    :return:\n    \"\"\"\n", "input": "", "output": "    return await client.post(MODULE + '/peering/peers', {'peer': peer_signed_raw}, rtype=RESPONSE_AIOHTTP)", "category": "Python"}, {"instruction": "def _apply_ide_controller_config(ide_controller_label, operation,\n                                 key, bus_number=0):\n    '''\n    Returns a vim.vm.device.VirtualDeviceSpec object specifying to add/edit an\n    IDE controller\n\n    ide_controller_label\n        Controller label of the IDE adapter\n\n    operation\n        Type of operation: add or edit\n\n    key\n        Unique key of the device\n\n    bus_number\n        Device bus number property\n    '''\n", "input": "", "output": "    log.trace('Configuring IDE controller ide_controller_label=%s',\n              ide_controller_label)\n    ide_spec = vim.vm.device.VirtualDeviceSpec()\n    ide_spec.device = vim.vm.device.VirtualIDEController()\n    if operation == 'add':\n        ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.add\n    if operation == 'edit':\n        ide_spec.operation = vim.vm.device.VirtualDeviceSpec.Operation.edit\n    ide_spec.device.key = key\n    ide_spec.device.busNumber = bus_number\n    if ide_controller_label:\n        ide_spec.device.deviceInfo = vim.Description()\n        ide_spec.device.deviceInfo.label = ide_controller_label\n        ide_spec.device.deviceInfo.summary = ide_controller_label\n    return ide_spec", "category": "Python"}, {"instruction": "def plane_errors(axes, covariance_matrix, sheet='upper',**kwargs):\n    \"\"\"\n    kwargs:\n    traditional_layout  boolean [True]\n        Lay the stereonet out traditionally, with north at the pole of\n        the diagram. The default is a more natural and intuitive visualization\n        with vertical at the pole and the compass points of strike around the equator.\n        Thus, longitude at the equator represents strike and latitude represents\n        apparent dip at that azimuth.\n    \"\"\"\n", "input": "", "output": "\n    level = kwargs.pop('level',1)\n    traditional_layout = kwargs.pop('traditional_layout',True)\n\n    d = N.sqrt(covariance_matrix)\n\n    ell = ellipse(**kwargs)\n    bundle = dot(ell, d[:2])\n    res = d[2]*level\n\n    # Switch hemispheres if PCA is upside-down\n    # Normal vector is always correctly fit\n    #if traditional_layout:\n    #if axes[2,2] > 0:\n\n    if axes[2,2] > 0:\n        res *= -1\n\n    if sheet == 'upper':\n        bundle += res\n    elif sheet == 'lower':\n        bundle -= res\n\n    _ = dot(bundle,axes).T\n\n    if traditional_layout:\n        lon,lat = stereonet_math.cart2sph(_[2],_[0],_[1])\n    else:\n        lon,lat = stereonet_math.cart2sph(-_[1],_[0],_[2])\n\n    return list(zip(lon,lat))", "category": "Python"}, {"instruction": "def gen_slot_variable(self, cls: ClassDefinition, slotname: str) -> str:\n        \"\"\" Generate a slot variable for slotname as defined in class\n        \"\"\"\n", "input": "", "output": "        slot = self.schema.slots[slotname]\n\n        # Alias allows re-use of slot names in different contexts\n        if slot.alias:\n            slotname = slot.alias\n\n        range_type = self.range_type_name(slot, cls.name)\n        # Python version < 3.7 -- forward references have to be quoted\n        if slot.inlined and slot.range in self.schema.classes and self.forward_reference(slot.range, cls.name):\n            range_type = f'\"{range_type}\"'\n        slot_range, default_val = self.range_cardinality(range_type, slot, cls)\n        default = f'= {default_val}' if default_val else ''\n        return f'''{underscore(slotname)}: {slot_range} {default}'''", "category": "Python"}, {"instruction": "def build_default_endpoint_prefixes(records_rest_endpoints):\n    \"\"\"Build the default_endpoint_prefixes map.\"\"\"\n", "input": "", "output": "    pid_types = set()\n    guessed = set()\n    endpoint_prefixes = {}\n\n    for key, endpoint in records_rest_endpoints.items():\n        pid_type = endpoint['pid_type']\n        pid_types.add(pid_type)\n        is_guessed = key == pid_type\n        is_default = endpoint.get('default_endpoint_prefix', False)\n\n        if is_default:\n            if pid_type in endpoint_prefixes and pid_type not in guessed:\n                raise ValueError('More than one \"{0}\" defined.'.format(\n                    pid_type\n                ))\n            endpoint_prefixes[pid_type] = key\n            guessed -= {pid_type}\n        elif is_guessed and pid_type not in endpoint_prefixes:\n            endpoint_prefixes[pid_type] = key\n            guessed |= {pid_type}\n\n    not_found = pid_types - set(endpoint_prefixes.keys())\n    if not_found:\n        raise ValueError('No endpoint-prefix for {0}.'.format(\n            ', '.join(not_found)\n        ))\n\n    return endpoint_prefixes", "category": "Python"}, {"instruction": "def amount_object_to_dict(self, amount):\n        \"\"\"Return the dictionary representation of an Amount object.\n\n        Amount object must have amount and currency properties and as_tuple method which will return (currency, amount)\n        and as_quantized method to quantize amount property.\n\n        :param amount: instance of Amount object\n\n        :return: dict with amount and currency keys.\n        \"\"\"\n", "input": "", "output": "        currency, amount = (\n            amount.as_quantized(digits=2).as_tuple()\n            if not isinstance(amount, dict)\n            else (amount[\"currency\"], amount[\"amount\"])\n        )\n        if currency not in self.currencies:\n            raise ValueError(self.err_unknown_currency.format(currency=currency))\n        return {\n            \"amount\": str(amount),\n            \"currency\": str(currency),\n        }", "category": "Python"}, {"instruction": "def tfclasses():\n    \"\"\"\n    A mapping of mimetypes to every class for reading data files.\n    \"\"\"\n", "input": "", "output": "    # automatically find any subclasses of TraceFile in the same\n    # directory as me\n    classes = {}\n    mydir = op.dirname(op.abspath(inspect.getfile(get_mimetype)))\n    tfcls = {\"<class 'aston.tracefile.TraceFile'>\",\n             \"<class 'aston.tracefile.ScanListFile'>\"}\n    for filename in glob(op.join(mydir, '*.py')):\n        name = op.splitext(op.basename(filename))[0]\n        module = import_module('aston.tracefile.' + name)\n        for clsname in dir(module):\n            cls = getattr(module, clsname)\n            if hasattr(cls, '__base__'):\n                if str(cls.__base__) in tfcls:\n                    classes[cls.mime] = cls\n    return classes", "category": "Python"}, {"instruction": "def triangle_colors(tile_x, tile_y, tile_size, pix):\n    \"\"\"Extracts the average color for each triangle in the given tile. Returns\n    a 4-tuple of colors for the triangles in this order: North, East, South,\n    West (clockwise).\n    \"\"\"\n", "input": "", "output": "    quad_size = tile_size / 2\n\n    north = []\n    for y in xrange(tile_y, tile_y + quad_size):\n        x_off = y - tile_y\n        for x in xrange(tile_x + x_off, tile_x + tile_size - x_off):\n            north.append(pix[x, y])\n\n    south = []\n    for y in xrange(tile_y + quad_size, tile_y + tile_size):\n        x_off = tile_y + tile_size - y\n        for x in xrange(tile_x + x_off, tile_x + tile_size - x_off):\n            south.append(pix[x, y])\n\n    east = []\n    for x in xrange(tile_x, tile_x + quad_size):\n        y_off = x - tile_x\n        for y in xrange(tile_y + y_off, tile_y + tile_size - y_off):\n            east.append(pix[x, y])\n\n    west = []\n    for x in xrange(tile_x + quad_size, tile_x + tile_size):\n        y_off = tile_x + tile_size - x\n        for y in xrange(tile_y + y_off, tile_y + tile_size - y_off):\n            west.append(pix[x, y])\n\n    return map(get_average_color, [north, east, south, west])", "category": "Python"}, {"instruction": "def _close_list(self):\n        \"\"\"\n        Add an close list tag corresponding to the currently open\n        list found in current_parent_element.\n        \"\"\"\n", "input": "", "output": "        list_type = self.current_parent_element['attrs']['class']\n        tag = LIST_TYPES[list_type]\n\n        html = '</{t}>'.format(\n            t=tag\n        )\n        self.cleaned_html += html\n        self.current_parent_element['tag'] = ''\n        self.current_parent_element['attrs'] = {}", "category": "Python"}, {"instruction": "def configure(self, endpoint=None, **kwargs):\n        \"\"\"Configure a previously initialized instance of the class.\"\"\"\n", "input": "", "output": "        if endpoint:\n            kwargs['endpoint'] = endpoint\n        keywords = self._keywords.copy()\n        keywords.update(kwargs)\n        if 'endpoint' in kwargs:\n            # Then we need to correctly format the endpoint.\n            endpoint = kwargs['endpoint']\n            keywords['endpoint'] = self._configure_endpoint(endpoint)\n        self.api_key = keywords['api_key'] or self._global_api_key()\n        self.endpoint = keywords['endpoint']\n        self.format = keywords['format'] or 'json'\n        self.jurisdiction = keywords['jurisdiction']\n        self.proxy = keywords['proxy']\n        self.discovery_url = keywords['discovery'] or None\n\n        # Use a custom requests session and set the correct SSL version if\n        # specified.\n        self.session = requests.Session()\n        if 'ssl_version' in keywords:\n            self.session.mount('https://', SSLAdapter(keywords['ssl_version']))", "category": "Python"}, {"instruction": "def _trim_ser_count_by(self, plotArea, count):\n        \"\"\"\n        Remove the last *count* ser elements from *plotArea*. Any xChart\n        elements having no ser child elements after trimming are also\n        removed.\n        \"\"\"\n", "input": "", "output": "        extra_sers = plotArea.sers[-count:]\n        for ser in extra_sers:\n            parent = ser.getparent()\n            parent.remove(ser)\n        extra_xCharts = [\n            xChart for xChart in plotArea.iter_xCharts()\n            if len(xChart.sers) == 0\n        ]\n        for xChart in extra_xCharts:\n            parent = xChart.getparent()\n            parent.remove(xChart)", "category": "Python"}, {"instruction": "def AppendENVPath(self, name, newpath, envname = 'ENV',\n                      sep = os.pathsep, delete_existing=1):\n        \"\"\"Append path elements to the path 'name' in the 'ENV'\n        dictionary for this environment.  Will only add any particular\n        path once, and will normpath and normcase all paths to help\n        assure this.  This can also handle the case where the env\n        variable is a list instead of a string.\n\n        If delete_existing is 0, a newpath which is already in the path\n        will not be moved to the end (it will be left where it is).\n        \"\"\"\n", "input": "", "output": "\n        orig = ''\n        if envname in self._dict and name in self._dict[envname]:\n            orig = self._dict[envname][name]\n\n        nv = SCons.Util.AppendPath(orig, newpath, sep, delete_existing,\n                                   canonicalize=self._canonicalize)\n\n        if envname not in self._dict:\n            self._dict[envname] = {}\n\n        self._dict[envname][name] = nv", "category": "Python"}, {"instruction": "def _read_precursor(precursor, sps):\n    \"\"\"\n    Load precursor file for that species\n    \"\"\"\n", "input": "", "output": "    hairpin = defaultdict(str)\n    name = None\n    with open(precursor) as in_handle:\n        for line in in_handle:\n            if line.startswith(\">\"):\n                if hairpin[name]:\n                    hairpin[name] = hairpin[name] + \"NNNNNNNNNNNN\"\n                name = line.strip().replace(\">\", \" \").split()[0]\n            else:\n                hairpin[name] += line.strip()\n        hairpin[name] = hairpin[name] + \"NNNNNNNNNNNN\"\n    return hairpin", "category": "Python"}, {"instruction": "def _validate(item):\n        \"\"\"Validate (instance, prop name) tuple\"\"\"\n", "input": "", "output": "        if not isinstance(item, tuple) or len(item) != 2:\n            raise ValueError('Linked items must be instance/prop-name tuple')\n        if not isinstance(item[0], tuple(LINK_OBSERVERS)):\n            raise ValueError('Only {} instances may be linked'.format(\n                ', '.join([link_cls.__name__ for link_cls in LINK_OBSERVERS])\n            ))\n        if not isinstance(item[1], string_types):\n            raise ValueError('Properties must be specified as string names')\n        if not hasattr(item[0], item[1]):\n            raise ValueError('Invalid property {} for {} instance'.format(\n                item[1], item[0].__class__.__name__\n            ))", "category": "Python"}, {"instruction": "def parse_individual(self, individual):\n        \"\"\"Converts a deap individual into a full list of parameters.\n\n        Parameters\n        ----------\n        individual: deap individual from optimization\n            Details vary according to type of optimization, but\n            parameters within deap individual are always between -1\n            and 1. This function converts them into the values used to\n            actually build the model\n\n        Returns\n        -------\n        fullpars: list\n            Full parameter list for model building.\n        \"\"\"\n", "input": "", "output": "        scaled_ind = []\n        for i in range(len(self.value_means)):\n            scaled_ind.append(self.value_means[i] + (\n                individual[i] * self.value_ranges[i]))\n        fullpars = list(self.arrangement)\n        for k in range(len(self.variable_parameters)):\n            for j in range(len(fullpars)):\n                if fullpars[j] == self.variable_parameters[k]:\n                    fullpars[j] = scaled_ind[k]\n        return fullpars", "category": "Python"}, {"instruction": "def mine_patterns(self, threshold):\n        \"\"\"\n        Mine the constructed FP tree for frequent patterns.\n        \"\"\"\n", "input": "", "output": "        if self.tree_has_single_path(self.root):\n            return self.generate_pattern_list()\n        else:\n            return self.zip_patterns(self.mine_sub_trees(threshold))", "category": "Python"}, {"instruction": "def _find_schema(data_path, schema_name):\n    \"\"\" Checks if `schema_name` is a valid file, if not\n    searches in `data_path` for it. \"\"\"\n", "input": "", "output": "\n    path = glob.glob(schema_name)\n    for p in path:\n        if os.path.isfile(p):\n            return p\n\n    return _find_data_path_schema(data_path, schema_name)", "category": "Python"}, {"instruction": "def delete(self, key, **opts):\n        \"\"\"Remove a key from the cache.\"\"\"\n", "input": "", "output": "        key, store = self._expand_opts(key, opts)\n        try:\n            del store[key]\n        except KeyError:\n            pass", "category": "Python"}, {"instruction": "def get_version(self):\n        \"\"\"Get blink(1) firmware version\n        \"\"\"\n", "input": "", "output": "        if ( self.dev == None ): return ''\n        buf = [REPORT_ID, ord('v'), 0, 0, 0, 0, 0, 0, 0]\n        self.write(buf)\n        time.sleep(.05)\n        version_raw = self.read()\n        version = (version_raw[3] - ord('0')) * 100 + (version_raw[4] - ord('0'))\n        return str(version)", "category": "Python"}, {"instruction": "def rn_boundary(af, b_hi):\n    \"\"\"\n    R(n) ratio boundary for selecting between [b_hi-1, b_hi]\n    alpha = b + 2\n    \n    \"\"\"\n", "input": "", "output": "    return np.sqrt( rn_theory(af, b)*rn_theory(af, b-1) )", "category": "Python"}, {"instruction": "def _next_unused_label(self):\n        \"\"\"Helper method to move to the next unused label. Not intended for external use.\n\n        This method will shade in any missing labels if desired, and will\n        increment the label_count attribute once a suitable label position has\n        been found.\n\n        \"\"\"\n", "input": "", "output": "        self._next_label()\n\n        # This label may be missing.\n        if self.page_count in self._used:\n            # Keep try while the label is missing.\n            missing = self._used.get(self.page_count, set())\n            while tuple(self._position) in missing:\n                # Throw the missing information away now we have used it. This\n                # allows the _shade_remaining_missing method to work.\n                missing.discard(tuple(self._position))\n\n                # Shade the missing label if desired.\n                if self.shade_missing:\n                    self._shade_missing_label()\n\n                # Try our luck with the next label.\n                self._next_label()\n                missing = self._used.get(self.page_count, set())\n\n        # Increment the count now we have found a suitable position.\n        self.label_count += 1", "category": "Python"}, {"instruction": "def rl(self, r):\n        \"\"\" Like the above, bus uses carry\n        \"\"\"\n", "input": "", "output": "        if self.C is None or not is_number(self.regs[r]):\n            self.set(r, None)\n            self.set_flag(None)\n            return\n\n        self.rlc(r)\n        tmp = self.C\n        v_ = self.getv(self.regs[r])\n        self.C = v_ & 1\n        self.regs[r] = str((v_ & 0xFE) | tmp)", "category": "Python"}, {"instruction": "def get_dataset(self, key, info):\n        \"\"\"Get the data  from the files.\"\"\"\n", "input": "", "output": "        logger.debug(\"Getting raw data\")\n        res = super(HRITGOESFileHandler, self).get_dataset(key, info)\n\n        self.mda['calibration_parameters'] = self._get_calibration_params()\n\n        res = self.calibrate(res, key.calibration)\n        new_attrs = info.copy()\n        new_attrs.update(res.attrs)\n        res.attrs = new_attrs\n        res.attrs['platform_name'] = self.platform_name\n        res.attrs['sensor'] = 'goes_imager'\n        return res", "category": "Python"}, {"instruction": "def set_signed_in(self, me):\n        \"\"\"\n        Configures the application as \"signed in\" (displays user's\n        name and disables the entry to input phone/bot token/code).\n        \"\"\"\n", "input": "", "output": "        self.me = me\n        self.sign_in_label.configure(text='Signed in')\n        self.sign_in_entry.configure(state=tkinter.NORMAL)\n        self.sign_in_entry.delete(0, tkinter.END)\n        self.sign_in_entry.insert(tkinter.INSERT, utils.get_display_name(me))\n        self.sign_in_entry.configure(state=tkinter.DISABLED)\n        self.sign_in_button.configure(text='Log out')\n        self.chat.focus()", "category": "Python"}, {"instruction": "def add_comment(self, app_id, record_id, field_id, message):\n        \"\"\"Directly add a comment to a record without retrieving the app or record first\n\n        Warnings:\n            Does not perform any app, record, or field ID validation\n\n        Args:\n            app_id (str): Full App ID string\n            record_id (str): Full parent Record ID string\n            field_id (str): Full field ID to target reference field on parent Record string\n            message (str): New comment message body\n        \"\"\"\n", "input": "", "output": "\n        self._swimlane.request(\n            'post',\n            'app/{0}/record/{1}/{2}/comment'.format(\n                app_id,\n                record_id,\n                field_id\n            ),\n            json={\n                'message': message,\n                'createdDate': pendulum.now().to_rfc3339_string()\n            }\n        )", "category": "Python"}, {"instruction": "def _get_memcache_timeout(self, key, options=None):\n    \"\"\"Return the memcache timeout (expiration) for this key.\"\"\"\n", "input": "", "output": "    timeout = ContextOptions.memcache_timeout(options)\n    if timeout is None:\n      timeout = self._memcache_timeout_policy(key)\n    if timeout is None:\n      timeout = ContextOptions.memcache_timeout(self._conn.config)\n    if timeout is None:\n      timeout = 0\n    return timeout", "category": "Python"}, {"instruction": "async def zrangebylex(self, name, min, max, start=None, num=None):\n        \"\"\"\n        Return the lexicographical range of values from sorted set ``name``\n        between ``min`` and ``max``.\n\n        If ``start`` and ``num`` are specified, then return a slice of the\n        range.\n        \"\"\"\n", "input": "", "output": "        if (start is not None and num is None) or \\\n                (num is not None and start is None):\n            raise RedisError(\"``start`` and ``num`` must both be specified\")\n        pieces = ['ZRANGEBYLEX', name, min, max]\n        if start is not None and num is not None:\n            pieces.extend([b('LIMIT'), start, num])\n        return await self.execute_command(*pieces)", "category": "Python"}, {"instruction": "def create_label(self, name, color):\n        \"\"\"Create a label for this repository.\n\n        :param str name: (required), name to give to the label\n        :param str color: (required), value of the color to assign to the\n            label, e.g., '#fafafa' or 'fafafa' (the latter is what is sent)\n        :returns: :class:`Label <github3.issues.label.Label>` if successful,\n            else None\n        \"\"\"\n", "input": "", "output": "        json = None\n        if name and color:\n            data = {'name': name, 'color': color.strip('#')}\n            url = self._build_url('labels', base_url=self._api)\n            json = self._json(self._post(url, data=data), 201)\n        return Label(json, self) if json else None", "category": "Python"}, {"instruction": "def colorize(text, color):\n    \"\"\"\n    Colorizes given text using given color.\n\n    :param text: Text to colorize.\n    :type text: unicode\n    :param color: *ANSI* escape code name.\n    :type color: unicode\n    :return: Colorized text.\n    :rtype: unicode\n    \"\"\"\n", "input": "", "output": "\n    escape_code = getattr(AnsiEscapeCodes, color, None)\n    if escape_code is None:\n        raise foundations.exceptions.AnsiEscapeCodeExistsError(\n            \"'{0}' | '{1}' 'ANSI' escape code name doesn't exists!\".format(\n                inspect.getmodulename(__file__), color))\n\n    return \"{0}{1}{2}\".format(escape_code, text, AnsiEscapeCodes.reset)", "category": "Python"}, {"instruction": "def remove(self, node, dirty=True):\n        \"\"\"Remove the given child node.\n\n        Args:\n            node (gkeepapi.Node): Node to remove.\n            dirty (bool): Whether this node should be marked dirty.\n        \"\"\"\n", "input": "", "output": "        if node.id in self._children:\n            self._children[node.id].parent = None\n            del self._children[node.id]\n        if dirty:\n            self.touch()", "category": "Python"}, {"instruction": "def find_version_by_regex(\n    file_source, version_token=\"__version__\"\n):  # type: (str,str)->Optional[str]\n    \"\"\"\n    Regex for dunder version\n    \"\"\"\n", "input": "", "output": "    if not file_source:\n        return None\n    version_match = re.search(\n        r\"^\" + version_token + r\" = ['\\\"]([^'\\\"]*)['\\\"]\", file_source, re.M\n    )\n    if version_match:\n        candidate = version_match.group(1)\n        if candidate == \"\" or candidate == \".\":  # yes, it will match to a .\n            return None\n        return candidate\n    return None", "category": "Python"}, {"instruction": "def stop(self, *args, **kwargs):\n        \"\"\"Deprecated function to |remove| an existing handler.\n\n        Warnings\n        --------\n        .. deprecated:: 0.2.2\n          ``stop()`` will be removed in Loguru 1.0.0, it is replaced by ``remove()`` which is a less\n          confusing name.\n        \"\"\"\n", "input": "", "output": "        warnings.warn(\n            \"The 'stop()' method is deprecated, please use 'remove()' instead\", DeprecationWarning\n        )\n        return self.remove(*args, **kwargs)", "category": "Python"}, {"instruction": "def sort_key_for_numeric_suffixes(path, sep='.', suffix_index=-2):\n    \"\"\"\n    Sort files taking into account potentially absent suffixes like\n        somefile.dcd\n        somefile.1000.dcd\n        somefile.2000.dcd\n\n    To be used with sorted(..., key=callable).\n    \"\"\"\n", "input": "", "output": "    chunks = path.split(sep)\n    # Remove suffix from path and convert to int\n    if chunks[suffix_index].isdigit():\n        return sep.join(chunks[:suffix_index] + chunks[suffix_index+1:]), int(chunks[suffix_index])\n    return path, 0", "category": "Python"}, {"instruction": "async def _get_annotations(entity_tag, connection):\n    \"\"\"Get annotations for the specified entity\n\n    :return dict: The annotations for the entity\n    \"\"\"\n", "input": "", "output": "    facade = client.AnnotationsFacade.from_connection(connection)\n    result = (await facade.Get([{\"tag\": entity_tag}])).results[0]\n    if result.error is not None:\n        raise JujuError(result.error)\n    return result.annotations", "category": "Python"}, {"instruction": "def has_permission(self, request, view):\n        \"\"\"If method is GET, user can access, if method is PUT or POST user must be a superuser.\n        \"\"\"\n", "input": "", "output": "        has_permission = False\n        if request.method == \"GET\" \\\n                or request.method in [\"PUT\", \"POST\", \"DELETE\"] \\\n                and request.user and request.user.is_superuser:\n            has_permission = True\n        return has_permission", "category": "Python"}, {"instruction": "def homogeneity(transition_matrices, regime_names=[], class_names=[],\n                title=\"Markov Homogeneity Test\"):\n    \"\"\"\n    Test for homogeneity of Markov transition probabilities across regimes.\n\n    Parameters\n    ----------\n    transition_matrices : list\n                          of transition matrices for regimes, all matrices must\n                          have same size (r, c). r is the number of rows in the\n                          transition matrix and c is the number of columns in\n                          the transition matrix.\n    regime_names        : sequence\n                          Labels for the regimes.\n    class_names         : sequence\n                          Labels for the classes/states of the Markov chain.\n    title               : string\n                          name of test.\n\n    Returns\n    -------\n                        : implicit\n                          an instance of Homogeneity_Results.\n    \"\"\"\n", "input": "", "output": "\n    return Homogeneity_Results(transition_matrices, regime_names=regime_names,\n                               class_names=class_names, title=title)", "category": "Python"}, {"instruction": "def _planck_deriv(self, lam, Teff):\n        \"\"\"\n        Computes the derivative of the monochromatic blackbody intensity using\n        the Planck function.\n\n        @lam: wavelength in m\n        @Teff: effective temperature in K\n\n        Returns: the derivative of monochromatic blackbody intensity\n        \"\"\"\n", "input": "", "output": "\n        expterm = np.exp(self.h*self.c/lam/self.k/Teff)\n        return 2*self.h*self.c*self.c/self.k/Teff/lam**7 * (expterm-1)**-2 * (self.h*self.c*expterm-5*lam*self.k*Teff*(expterm-1))", "category": "Python"}, {"instruction": "def split_path(path):\n    '''\n    Splits the argument into its constituent directories and returns them as\n    a list.\n    '''\n", "input": "", "output": "    def recurse_path(path,retlist):\n        if len(retlist) > 100:\n            fullpath = os.path.join(*([ path, ] + retlist))\n            print(\"Directory '{}' contains too many levels\".format(fullpath))\n            exit(1)\n        head, tail = os.path.split(path)\n        if len(tail) > 0:\n            retlist.insert(0,tail)\n            recurse_path(head,retlist)\n        elif len(head) > 1:\n            recurse_path(head,retlist)\n        else:\n            return\n\n    retlist = []\n    path = os.path.realpath(os.path.normpath(path))\n    drive, path = os.path.splitdrive(path)\n    if len(drive) > 0: retlist.append(drive)\n    recurse_path(path,retlist)\n    return retlist", "category": "Python"}, {"instruction": "def get_wallet_height(self, id=None, endpoint=None):\n        \"\"\"\n        Get the current wallet index height.\n        Args:\n            id: (int, optional) id to use for response tracking\n            endpoint: (RPCEndpoint, optional) endpoint to specify to use\n        Returns:\n            json object of the result or the error encountered in the RPC call\n        \"\"\"\n", "input": "", "output": "        return self._call_endpoint(GET_WALLET_HEIGHT, id=id, endpoint=endpoint)", "category": "Python"}, {"instruction": "def update_dict(obj, dict, attributes):\n    \"\"\"Update dict with fields from obj.attributes.\n\n    :param obj: the object updated into dict\n    :param dict: the result dictionary\n    :param attributes: a list of attributes belonging to obj\n    \"\"\"\n", "input": "", "output": "    for attribute in attributes:\n        if hasattr(obj, attribute) and getattr(obj, attribute) is not None:\n            dict[attribute] = getattr(obj, attribute)", "category": "Python"}, {"instruction": "def get_attribute(self, attribute):\n        \"\"\"Get an attribute from the session.\n\n        :param attribute:\n        :return: attribute value, status code\n        :rtype: object, constants.StatusCode\n        \"\"\"\n", "input": "", "output": "\n        # Check that the attribute exists.\n        try:\n            attr = attributes.AttributesByID[attribute]\n        except KeyError:\n            return 0, constants.StatusCode.error_nonsupported_attribute\n\n        # Check that the attribute is valid for this session type.\n        if not attr.in_resource(self.session_type):\n            return 0, constants.StatusCode.error_nonsupported_attribute\n\n        # Check that the attribute is readable.\n        if not attr.read:\n            raise Exception('Do not now how to handle write only attributes.')\n\n        # Return the current value of the default according the VISA spec\n        return self.attrs.setdefault(attribute, attr.default), constants.StatusCode.success", "category": "Python"}, {"instruction": "def authenticate(self, token):\n        \"\"\" Authenticate a token\n\n        :param token:\n        \"\"\"\n", "input": "", "output": "        if self.verify_token_callback:\n            # Specified verify function overrides below\n            return self.verify_token_callback(token)\n\n        if not token:\n            return False\n\n        name = self.token_manager.verify(token)\n        if not name:\n            return False\n\n        return True", "category": "Python"}, {"instruction": "def check_exists_repositories(repo):\n    \"\"\"Checking if repositories exists by PACKAGES.TXT file\n    \"\"\"\n", "input": "", "output": "    pkg_list = \"PACKAGES.TXT\"\n    if repo == \"sbo\":\n        pkg_list = \"SLACKBUILDS.TXT\"\n    if check_for_local_repos(repo) is True:\n        pkg_list = \"PACKAGES.TXT\"\n        return \"\"\n    if not os.path.isfile(\"{0}{1}{2}\".format(\n            _meta_.lib_path, repo, \"_repo/{0}\".format(pkg_list))):\n        return repo\n    return \"\"", "category": "Python"}, {"instruction": "def query(sql, format='df'):\n    '''\n    Submit an `sql` query (string) to treasury.io and return a pandas DataFrame.\n\n    For example::\n\n        print('Operating cash balances for May 22, 2013')\n        print(treasuryio.query('SELECT * FROM \"t1\" WHERE \"date\" = \\'2013-05-22\\';'))\n\n    Return a dict::\n\n        treasuryio.query('SELECT * FROM \"t1\" WHERE \"date\" = \\'2013-05-22\\';', format='dict')\n\n    '''\n", "input": "", "output": "    url = 'http://api.treasury.io/cc7znvq/47d80ae900e04f2/sql/'\n    query_string = urlencode({'q':sql})\n    handle = urlopen(url + '?' + query_string)\n    if handle.code == 200:\n        d = load(handle)\n        if format == 'df':\n            return DataFrame(d)\n        elif format == 'dict':\n            return d\n        else:\n            raise ValueError('format must equal \"df\" or \"dict\"')\n    else:\n        raise ValueError(handle.read())", "category": "Python"}, {"instruction": "def _initLayerCtors(self):\n        '''\n        Registration for built-in Layer ctors\n        '''\n", "input": "", "output": "        ctors = {\n            'lmdb': s_lmdblayer.LmdbLayer,\n            'remote': s_remotelayer.RemoteLayer,\n        }\n        self.layrctors.update(**ctors)", "category": "Python"}, {"instruction": "def producer(cfg_uri, queue, logger=None):\n    \"\"\"\n    \u5206\u5e03\u5f0f\u722c\u866b\u7684\u4efb\u52a1\u7aef\uff08\u5c06\u4efb\u52a1\u52a0\u5165Queue\uff09\n\n    \u6ce8\u610f\uff1a\n        \u88ab\u5305\u88c5\u7684\u51fd\u6570\u9700\u8981\u8fd4\u56de\u4e00\u4e2a\u53ef\u8fed\u4ee3\u7684\u5bf9\u8c61\n\n    :param cfg_uri: \u8bfb\u53d6\u4efb\u52a1\u7684\u8def\u5f84\n    :param queue: Queue\u7684\u540d\u5b57\n    :param logger: \u65e5\u5fd7\u8bb0\u5f55\u5de5\u5177\n    \"\"\"\n", "input": "", "output": "    _info, _exception = _deal_logger(logger)\n    cfg_uri = _deal_uri(cfg_uri)\n\n    def decorator(function):\n        @wraps(function)\n        def wapper(*args, **kwargs):\n            client = _conn(cfg_uri, queue, _info)\n            for item in function(*args, **kwargs):\n                try:\n                    if not isinstance(item, dict):\n                        item = {'data': item}\n                    data = dumps(item, ensure_ascii=False)\n                    client.send(queue, data, headers={'persistent': 'true'})\n                    _info('Producer insert %s - %s' % (queue, item))\n                except ProducerFatalError, e:\n                    _exception(e)\n                    break\n                except Exception, e:\n                    _exception(e)\n            client.disconnect()\n            _info('disconnected %s' % cfg_uri)\n        return wapper\n    return decorator", "category": "Python"}, {"instruction": "def clear(self):\n        \"\"\"Clears the Merkle Tree by releasing the Merkle root and each leaf's references, the rest\n        should be garbage collected.  This may be useful for situations where you want to take an existing\n        tree, make changes to the leaves, but leave it uncalculated for some time, without node\n        references that are no longer correct still hanging around. Usually it is better just to make\n        a new tree.\n        \"\"\"\n", "input": "", "output": "        self.root = None\n        for leaf in self.leaves:\n            leaf.p, leaf.sib, leaf.side = (None, ) * 3", "category": "Python"}, {"instruction": "def _to_numpy(Z):\n        \"\"\"Converts a None, list, np.ndarray, or torch.Tensor to np.ndarray\"\"\"\n", "input": "", "output": "        if isinstance(Z, list):\n            return [Classifier._to_numpy(z) for z in Z]\n        else:\n            return Classifier._to_numpy(Z)", "category": "Python"}, {"instruction": "def main():\n    \"\"\"\n    Main method.\n    \"\"\"\n", "input": "", "output": "    run_config = _parse_args(sys.argv[1:])\n    if run_config.debug:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n    gitlab_config = GitLabConfig(run_config.url, run_config.token)\n\n    project_updater_builder = FileBasedProjectVariablesUpdaterBuilder(\n        setting_repositories=run_config.setting_repositories,\n        default_setting_extensions=run_config.default_setting_extensions)\n\n    updater = FileBasedProjectsVariablesUpdater(config_location=run_config.config_location, gitlab_config=gitlab_config,\n                                                project_variables_updater_builder=project_updater_builder)\n    updater.update()", "category": "Python"}, {"instruction": "def day_interval(year, month, day, milliseconds=False, return_string=False):\n    \"\"\"\n    Return a start datetime and end datetime of a day.\n\n    :param milliseconds: Minimum time resolution.\n    :param return_string: If you want string instead of datetime, set True\n\n    Usage Example::\n\n        >>> start, end = rolex.day_interval(2014, 6, 17)\n        >>> start\n        datetime(2014, 6, 17, 0, 0, 0)\n\n        >>> end\n        datetime(2014, 6, 17, 23, 59, 59)\n    \"\"\"\n", "input": "", "output": "    if milliseconds:  # pragma: no cover\n        delta = timedelta(milliseconds=1)\n    else:\n        delta = timedelta(seconds=1)\n\n    start = datetime(year, month, day)\n    end = datetime(year, month, day) + timedelta(days=1) - delta\n\n    if not return_string:\n        return start, end\n    else:\n        return str(start), str(end)", "category": "Python"}, {"instruction": "def __ordering_deprecated(self):\n        \"\"\"Deprecated warning for pywbem CIM Objects\"\"\"\n", "input": "", "output": "        msg = _format(\"Ordering comparisons involving {0} objects are \"\n                      \"deprecated.\", self.__class__.__name__)\n        if DEBUG_WARNING_ORIGIN:\n            msg += \"\\nTraceback:\\n\" + ''.join(traceback.format_stack())\n        warnings.warn(msg, DeprecationWarning, stacklevel=3)", "category": "Python"}, {"instruction": "def getnodefieldname(idfobject, endswith, fluid=None, startswith=None):\n    \"\"\"return the field name of the node\n    fluid is only needed if there are air and water nodes\n    fluid is Air or Water or ''.\n    if the fluid is Steam, use Water\"\"\"\n", "input": "", "output": "    if startswith is None:\n        startswith = ''\n    if fluid is None:\n        fluid = ''\n    nodenames = getfieldnamesendswith(idfobject, endswith)\n    nodenames = [name for name in nodenames if name.startswith(startswith)]\n    fnodenames = [nd for nd in nodenames if nd.find(fluid) != -1]\n    fnodenames = [name for name in fnodenames if name.startswith(startswith)]\n    if len(fnodenames) == 0:\n        nodename = nodenames[0]\n    else:\n        nodename = fnodenames[0]\n    return nodename", "category": "Python"}, {"instruction": "def iterShapes(self):\r\n        \"\"\"Serves up shapes in a shapefile as an iterator. Useful\r\n        for handling large shapefiles.\"\"\"\n", "input": "", "output": "        shp = self.__getFileObj(self.shp)\r\n        shp.seek(0,2)\r\n        self.shpLength = shp.tell()\r\n        shp.seek(100)\r\n        while shp.tell() < self.shpLength:\r\n            yield self.__shape()", "category": "Python"}, {"instruction": "def color_range(startcolor, goalcolor, steps):\n    \"\"\"\n    wrapper for interpolate_tuple that accepts colors as html (\"#CCCCC\" and such)\n    \"\"\"\n", "input": "", "output": "    start_tuple = make_color_tuple(startcolor)\n    goal_tuple = make_color_tuple(goalcolor)\n\n    return interpolate_tuple(start_tuple, goal_tuple, steps)", "category": "Python"}, {"instruction": "def _load_upgrades(self, remove_applied=True):\n        \"\"\"Load upgrade modules.\n\n        :param remove_applied: if True, already applied upgrades will not\n            be included, if False the entire upgrade graph will be\n            returned.\n        \"\"\"\n", "input": "", "output": "        if remove_applied:\n            self.load_history()\n\n        for entry_point in iter_entry_points('invenio_upgrader.upgrades'):\n            upgrade = entry_point.load()()\n            self.__class__._upgrades[upgrade.name] = upgrade\n\n        return self.__class__._upgrades", "category": "Python"}, {"instruction": "def _read_models(self, graph):\n        \"\"\"\n        Read graph and add models to document\n        \"\"\"\n", "input": "", "output": "        for e in self._get_elements(graph, SBOL.Model):\n            identity = e[0]\n            m = self._get_rdf_identified(graph, identity)\n            m['source'] = self._get_triplet_value(graph, identity, SBOL.source)\n            m['language'] = self._get_triplet_value(graph, identity, SBOL.language)\n            m['framework'] = self._get_triplet_value(graph, identity, SBOL.framework)\n            obj = Model(**m)\n            self._models[identity.toPython()] = obj\n            self._collection_store[identity.toPython()] = obj", "category": "Python"}, {"instruction": "def icanhazascii(client, channel, nick, message, found):\n    \"\"\"\n    A plugin for generating showing ascii artz\n    \"\"\"\n", "input": "", "output": "    global FLOOD_RATE, LAST_USED\n    now = time.time()\n\n    if channel in LAST_USED and (now - LAST_USED[channel]) < FLOOD_RATE:\n        return\n\n    LAST_USED[channel] = now\n    return found", "category": "Python"}, {"instruction": "def parse_path(path):\n    \"\"\"Parse path string.\"\"\"\n", "input": "", "output": "    version, project = path[1:].split('/')\n    return dict(version=int(version), project=project)", "category": "Python"}, {"instruction": "def write_byte(self, address, value):\n        \"\"\"Writes the byte to unaddressed register in a device. \"\"\"\n", "input": "", "output": "        LOGGER.debug(\"Writing byte %s to device %s!\", bin(value), hex(address))\n        return self.driver.write_byte(address, value)", "category": "Python"}, {"instruction": "def copy(self, memo):\n        \"\"\"\n        Make a copy of this SimAbstractMemory object\n        :return:\n        \"\"\"\n", "input": "", "output": "        am = SimAbstractMemory(\n            memory_id=self._memory_id,\n            endness=self.endness,\n            stack_region_map=self._stack_region_map,\n            generic_region_map=self._generic_region_map\n        )\n        for region_id, region in self._regions.items():\n            am._regions[region_id] = region.copy(memo)\n        am._stack_size = self._stack_size\n        return am", "category": "Python"}, {"instruction": "def getTraceIdsByAnnotation(self, service_name, annotation, value, end_ts, limit, order):\n    \"\"\"\n    Fetch trace ids with a particular annotation.\n    Gets \"limit\" number of entries from before the \"end_ts\".\n\n    When requesting based on time based annotations only pass in the first parameter, \"annotation\" and leave out\n    the second \"value\". If looking for a key-value binary annotation provide both, \"annotation\" is then the\n    key in the key-value.\n\n    Timestamps are in microseconds.\n\n    Parameters:\n     - service_name\n     - annotation\n     - value\n     - end_ts\n     - limit\n     - order\n    \"\"\"\n", "input": "", "output": "    self.send_getTraceIdsByAnnotation(service_name, annotation, value, end_ts, limit, order)\n    return self.recv_getTraceIdsByAnnotation()", "category": "Python"}, {"instruction": "def get_user_tracking_beacons(self, user_id, **data):\n        \"\"\"\n        GET /users/:user_id/tracking_beacons/\n        Returns the list of :format:`tracking_beacon` for the user :user_id\n        \"\"\"\n", "input": "", "output": "        \n        return self.get(\"/users/{0}/tracking_beacons/\".format(user_id), data=data)", "category": "Python"}, {"instruction": "def get_banks(self):\n        \"\"\"Pass through to provider BankLookupSession.get_banks\"\"\"\n", "input": "", "output": "        # Implemented from kitosid template for -\n        # osid.resource.BinLookupSession.get_bins_template\n        catalogs = self._get_provider_session('bank_lookup_session').get_banks()\n        cat_list = []\n        for cat in catalogs:\n            cat_list.append(Bank(self._provider_manager, cat, self._runtime, self._proxy))\n        return BankList(cat_list)", "category": "Python"}, {"instruction": "def initialize(self):\n        \"\"\" A reimplemented initializer.\n\n        This method will add the include objects to the parent of the\n        include and ensure that they are initialized.\n\n        \"\"\"\n", "input": "", "output": "        super(Block, self).initialize()\n\n        block = self.block\n\n        if block: #: This block is setting the content of another block\n            #: Remove the existing blocks children\n            if self.mode == 'replace':\n                #: Clear the blocks children\n                for c in block.children:\n                    c.destroy()\n            #: Add this blocks children to the other block\n            block.insert_children(None, self.children)\n\n        else: #: This block is inserting it's children into it's parent\n            self.parent.insert_children(self, self.children)", "category": "Python"}, {"instruction": "def model_import(self):\n        \"\"\"\n        Import and instantiate the non-JIT models and the JIT models.\n\n        Models defined in ``jits`` and ``non_jits`` in ``models/__init__.py``\n        will be imported and instantiated accordingly.\n\n        Returns\n        -------\n        None\n        \"\"\"\n", "input": "", "output": "        # non-JIT models\n        for file, pair in non_jits.items():\n            for cls, name in pair.items():\n                themodel = importlib.import_module('andes.models.' + file)\n                theclass = getattr(themodel, cls)\n                self.__dict__[name] = theclass(self, name)\n\n                group = self.__dict__[name]._group\n                self.group_add(group)\n                self.__dict__[group].register_model(name)\n\n                self.devman.register_device(name)\n\n        # import JIT models\n        for file, pair in jits.items():\n            for cls, name in pair.items():\n                self.__dict__[name] = JIT(self, file, cls, name)", "category": "Python"}, {"instruction": "def compute_md5(self):\n        \"\"\"Compute and erturn MD5 hash value.\"\"\"\n", "input": "", "output": "        import hashlib\n        with open(self.path, \"rt\") as fh:\n            text = fh.read()\n            m = hashlib.md5(text.encode(\"utf-8\"))\n            return m.hexdigest()", "category": "Python"}, {"instruction": "def get_subprocess_output(cls, command, ignore_stderr=True, **kwargs):\n    \"\"\"Get the output of an executed command.\n\n    :param command: An iterable representing the command to execute (e.g. ['ls', '-al']).\n    :param ignore_stderr: Whether or not to ignore stderr output vs interleave it with stdout.\n    :raises: `ProcessManager.ExecutionError` on `OSError` or `CalledProcessError`.\n    :returns: The output of the command.\n    \"\"\"\n", "input": "", "output": "    if ignore_stderr is False:\n      kwargs.setdefault('stderr', subprocess.STDOUT)\n\n    try:\n      return subprocess.check_output(command, **kwargs).decode('utf-8').strip()\n    except (OSError, subprocess.CalledProcessError) as e:\n      subprocess_output = getattr(e, 'output', '').strip()\n      raise cls.ExecutionError(str(e), subprocess_output)", "category": "Python"}, {"instruction": "def hex_digit(coord, digit=1):\n    \"\"\"\n    Returns either the first or second digit of the hexadecimal representation of the given coordinate.\n    :param coord: hexadecimal coordinate, int\n    :param digit: 1 or 2, meaning either the first or second digit of the hexadecimal\n    :return: int, either the first or second digit\n    \"\"\"\n", "input": "", "output": "    if digit not in [1,2]:\n        raise ValueError('hex_digit can only get the first or second digit of a hex number, was passed digit={}'.format(\n            digit\n        ))\n    return int(hex(coord)[1+digit], 16)", "category": "Python"}, {"instruction": "async def get_zones(self) -> List[Zone]:\n        \"\"\"Return list of available zones.\"\"\"\n", "input": "", "output": "        res = await self.services[\"avContent\"][\"getCurrentExternalTerminalsStatus\"]()\n        zones = [Zone.make(services=self.services, **x) for x in res if 'meta:zone:output' in x['meta']]\n        if not zones:\n            raise SongpalException(\"Device has no zones\")\n        return zones", "category": "Python"}, {"instruction": "def _process_event(self, event):\n        \"\"\" Extend event object with User and Channel objects \"\"\"\n", "input": "", "output": "        if event.get('user'):\n            event.user = self.lookup_user(event.get('user'))\n\n        if event.get('channel'):\n            event.channel = self.lookup_channel(event.get('channel'))\n\n        if self.user.id in event.mentions:\n            event.mentions_me = True\n\n        event.mentions = [ self.lookup_user(uid) for uid in event.mentions ]\n\n        return event", "category": "Python"}, {"instruction": "def set_person(self, what, rep):\n        \"\"\"Set a person substitution.\n\n        Equivalent to ``! person`` in RiveScript code.\n\n        :param str what: The original text to replace.\n        :param str rep: The text to replace it with.\n            Set this to ``None`` to delete the substitution.\n        \"\"\"\n", "input": "", "output": "        if rep is None:\n            # Unset the variable.\n            if what in self._person:\n                del self._person[what]\n        self._person[what] = rep", "category": "Python"}, {"instruction": "def _handle_adapter(self, app):\n        \"\"\"\n        Handle storage _adapter configuration\n        :param app: Flask application\n        \"\"\"\n", "input": "", "output": "        if 'IMAGINE_ADAPTER' in app.config \\\n                and 'name' in app.config['IMAGINE_ADAPTER'] \\\n                and app.config['IMAGINE_ADAPTER']['name'] in self._adapters.keys():\n            self._adapter = self._adapters[app.config['IMAGINE_ADAPTER']['name']](\n                **app.config['IMAGINE_ADAPTER']\n            )\n        else:\n            raise ValueError('Unknown _adapter: %s' % str(app.config['IMAGINE_ADAPTER']))", "category": "Python"}, {"instruction": "def team_scores(self, team_scores, time):\n        \"\"\"Store output of team scores to a CSV file\"\"\"\n", "input": "", "output": "        headers = ['Date', 'Home Team Name', 'Home Team Goals',\n                   'Away Team Goals', 'Away Team Name']\n        result = [headers]\n        result.extend([score[\"utcDate\"].split('T')[0],\n                       score['homeTeam']['name'],\n                       score['score']['fullTime']['homeTeam'],\n                       score['score']['fullTime']['awayTeam'],\n                       score['awayTeam']['name']]\n                      for score in team_scores['matches']\n                      if score['status'] == 'FINISHED')\n        self.generate_output(result)", "category": "Python"}, {"instruction": "def fetchall_syns(ont):\n    \"\"\"\n    fetch all synonyms for an ontology\n    \"\"\"\n", "input": "", "output": "    logging.info(\"fetching syns for: \"+ont)\n    namedGraph = get_named_graph(ont)\n    queryBody = querybody_syns()\n    query = ", "category": "Python"}, {"instruction": "def add_record(self, orcid_id, token, request_type, data,\n                   content_type='application/orcid+json'):\n        \"\"\"Add a record to a profile.\n\n        Parameters\n        ----------\n        :param orcid_id: string\n            Id of the author.\n        :param token: string\n            Token received from OAuth 2 3-legged authorization.\n        :param request_type: string\n            One of 'activities', 'education', 'employment', 'funding',\n            'peer-review', 'work'.\n        :param data: dict | lxml.etree._Element\n            The record in Python-friendly format, as either JSON-compatible\n            dictionary (content_type == 'application/orcid+json') or\n            XML (content_type == 'application/orcid+xml')\n        :param content_type: string\n            MIME type of the passed record.\n\n        Returns\n        -------\n        :returns: string\n            Put-code of the new work.\n        \"\"\"\n", "input": "", "output": "        return self._update_activities(orcid_id, token, requests.post,\n                                       request_type, data,\n                                       content_type=content_type)", "category": "Python"}, {"instruction": "def create(self, qname):\n        '''\n        Create RackSpace Queue.\n        '''\n", "input": "", "output": "        try:\n            if self.exists(qname):\n                log.error('Queues \"%s\" already exists. Nothing done.', qname)\n                return True\n\n            self.conn.create(qname)\n\n            return True\n        except pyrax.exceptions as err_msg:\n            log.error('RackSpace API got some problems during creation: %s',\n                      err_msg)\n        return False", "category": "Python"}, {"instruction": "def simxReadCollision(clientID, collisionObjectHandle, operationMode):\n    '''\n    Please have a look at the function description/documentation in the V-REP user manual\n    '''\n", "input": "", "output": "    collisionState = ct.c_ubyte()\n    return c_ReadCollision(clientID, collisionObjectHandle, ct.byref(collisionState), operationMode), bool(collisionState.value!=0)", "category": "Python"}, {"instruction": "def _paths_equal(lhs, rhs):\n    \"\"\"If one object path doesn't inlcude a host, don't include the hosts\n    in the comparison\n\n    \"\"\"\n", "input": "", "output": "\n    if lhs is rhs:\n        return True\n    if lhs.host is not None and rhs.host is not None and lhs.host != rhs.host:\n        return False\n    # need to make sure this stays in sync with CIMInstanceName.__cmp__()\n    return not (pywbem.cmpname(rhs.classname, lhs.classname) or\n                cmp(rhs.keybindings, lhs.keybindings) or\n                pywbem.cmpname(rhs.namespace, lhs.namespace))", "category": "Python"}, {"instruction": "def updateSquareFees(paymentRecord):\r\n    '''\r\n    The Square Checkout API does not calculate fees immediately, so this task is\r\n    called to be asynchronously run 1 minute after the initial transaction, so that\r\n    any Invoice or ExpenseItem associated with this transaction also remains accurate.\r\n    '''\n", "input": "", "output": "\r\n    fees = paymentRecord.netFees\r\n    invoice = paymentRecord.invoice\r\n    invoice.fees = fees\r\n    invoice.save()\r\n    invoice.allocateFees()\r\n    return fees", "category": "Python"}, {"instruction": "def _get_cached_response(self):\n        \"\"\"Returns a file object of the cached response.\"\"\"\n", "input": "", "output": "\n        if not self._is_cached():\n            response = self._download_response()\n            self.cache.set_xml(self._get_cache_key(), response)\n\n        return self.cache.get_xml(self._get_cache_key())", "category": "Python"}, {"instruction": "def get_catchup_block_ids(self, last_known_block_id):\n        '''\n        Raises:\n            PossibleForkDetectedError\n        '''\n", "input": "", "output": "        # If latest known block is not the current chain head, catch up\n        catchup_up_blocks = []\n        chain_head = self._block_store.chain_head\n        if chain_head and last_known_block_id != chain_head.identifier:\n            # Start from the chain head and get blocks until we reach the\n            # known block\n            for block in self._block_store.get_predecessor_iter():\n                # All the blocks if NULL_BLOCK_IDENTIFIER\n                if last_known_block_id != NULL_BLOCK_IDENTIFIER:\n                    if block.identifier == last_known_block_id:\n                        break\n                catchup_up_blocks.append(block.identifier)\n\n        return list(reversed(catchup_up_blocks))", "category": "Python"}, {"instruction": "def ret_dump(self, d_ret, **kwargs):\n        \"\"\"\n        JSON print results to console (or caller)\n        \"\"\"\n", "input": "", "output": "        b_print     = True\n        for k, v in kwargs.items():\n            if k == 'JSONprint':    b_print     = bool(v)\n        if b_print:\n            print(\n                json.dumps(   \n                    d_ret, \n                    indent      = 4,\n                    sort_keys   = True\n                )\n        )", "category": "Python"}, {"instruction": "def stonith_create(stonith_id, stonith_device_type, stonith_device_options=None, cibfile=None):\n    '''\n    Create a stonith resource via pcs command\n\n    stonith_id\n        name for the stonith resource\n    stonith_device_type\n        name of the stonith agent fence_eps, fence_xvm f.e.\n    stonith_device_options\n        additional options for creating the stonith resource\n    cibfile\n        use cibfile instead of the live CIB for manipulation\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' pcs.stonith_create stonith_id='eps_fence' stonith_device_type='fence_eps'\n                                    stonith_device_options=\"['pcmk_host_map=node1.example.org:01;node2.example.org:02', 'ipaddr=myepsdevice.example.org', 'action=reboot', 'power_wait=5', 'verbose=1', 'debug=/var/log/pcsd/eps_fence.log', 'login=hidden', 'passwd=hoonetorg']\" cibfile='/tmp/cib_for_stonith.cib'\n    '''\n", "input": "", "output": "    return item_create(item='stonith',\n                       item_id=stonith_id,\n                       item_type=stonith_device_type,\n                       extra_args=stonith_device_options,\n                       cibfile=cibfile)", "category": "Python"}, {"instruction": "def get_ext(self, obj=None):\n        \"\"\"Return the file extension\n\n        :param obj: the fileinfo with information. If None, this will use the stored object of JB_File\n        :type obj: :class:`FileInfo`\n        :returns: the file extension\n        :rtype: str\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        if obj is None:\n            obj = self._obj\n        return self._extel.get_ext(obj)", "category": "Python"}, {"instruction": "def format_as_dataframes(explanation):\n    # type: (Explanation) -> Dict[str, pd.DataFrame]\n    \"\"\" Export an explanation to a dictionary with ``pandas.DataFrame`` values\n    and string keys that correspond to explanation attributes.\n    Use this method if several dataframes can be exported from a single\n    explanation (e.g. for CRF explanation with has both feature weights\n    and transition matrix).\n    Note that :func:`eli5.explain_weights` limits number of features\n    by default. If you need all features, pass ``top=None`` to\n    :func:`eli5.explain_weights`, or use\n    :func:`explain_weights_dfs`.\n    \"\"\"\n", "input": "", "output": "    result = {}\n    for attr in _EXPORTED_ATTRIBUTES:\n        value = getattr(explanation, attr)\n        if value:\n            result[attr] = format_as_dataframe(value)\n    return result", "category": "Python"}, {"instruction": "def get_callbacks(service):\n    \"\"\"Get configured callbacks list for a given service identifier.\"\"\"\n", "input": "", "output": "    callbacks = list(getattr(settings, 'MAMA_CAS_ATTRIBUTE_CALLBACKS', []))\n    if callbacks:\n        warnings.warn(\n            'The MAMA_CAS_ATTRIBUTE_CALLBACKS setting is deprecated. Service callbacks '\n            'should be configured using MAMA_CAS_SERVICES.', DeprecationWarning)\n\n    for backend in _get_backends():\n        try:\n            callbacks.extend(backend.get_callbacks(service))\n        except AttributeError:\n            raise NotImplementedError(\"%s.%s.get_callbacks() not implemented\" % (\n                backend.__class__.__module__, backend.__class__.__name__)\n            )\n    return callbacks", "category": "Python"}, {"instruction": "def to_df(self) -> pd.DataFrame:\n        \"\"\"Generate shallow :class:`~pandas.DataFrame`.\n\n        The data matrix :attr:`X` is returned as\n        :class:`~pandas.DataFrame`, where :attr:`obs_names` initializes the\n        index, and :attr:`var_names` the columns.\n\n        * No annotations are maintained in the returned object.\n        * The data matrix is densified in case it is sparse.\n        \"\"\"\n", "input": "", "output": "        if issparse(self._X):\n            X = self._X.toarray()\n        else:\n            X = self._X\n        return pd.DataFrame(X, index=self.obs_names, columns=self.var_names)", "category": "Python"}, {"instruction": "def validated_value(self, raw_value):\n        \"\"\"Return parsed parameter value and run validation handlers.\n\n        Error message included in exception will be included in http error\n        response\n\n        Args:\n            value: raw parameter value to parse validate\n\n        Returns:\n            None\n\n        Note:\n            Concept of validation for params is understood here as a process\n            of checking if data of valid type (successfully parsed/processed by\n            ``.value()`` handler) does meet some other constraints\n            (lenght, bounds, uniqueness, etc.). It will internally call its\n            ``value()`` handler.\n\n        \"\"\"\n", "input": "", "output": "        value = self.value(raw_value)\n        try:\n            for validator in self.validators:\n                validator(value)\n        except:\n            raise\n        else:\n            return value", "category": "Python"}, {"instruction": "def password_args(subparsers):\n    \"\"\"Add command line options for the set_password operation\"\"\"\n", "input": "", "output": "    password_parser = subparsers.add_parser('set_password')\n    password_parser.add_argument('vault_path',\n                                 help='Path which contains password'\n                                 'secret to be udpated')\n    base_args(password_parser)", "category": "Python"}, {"instruction": "def _authorize_credentials(self):\n        \"\"\"\n        Fetches an access/refresh token by authorizing OAuth 2.0 credentials.\n\n        Returns: True, if the authorization was successful.\n                 False, if a ServerNotFoundError is thrown.\n        \"\"\"\n", "input": "", "output": "        try:\n            http = self.credentials.authorize(httplib2.Http())\n            self.service = discovery.build(\"calendar\", \"v3\", http=http)\n            return True\n        except ServerNotFoundError:\n            return False", "category": "Python"}, {"instruction": "def horizon_dashboard_nav(context):\n    \"\"\"Generates sub-navigation entries for the current dashboard.\"\"\"\n", "input": "", "output": "    if 'request' not in context:\n        return {}\n    dashboard = context['request'].horizon['dashboard']\n    panel_groups = dashboard.get_panel_groups()\n    non_empty_groups = []\n\n    for group in panel_groups.values():\n        allowed_panels = []\n        for panel in group:\n            if (callable(panel.nav) and panel.nav(context) and\n                    panel.can_access(context)):\n                allowed_panels.append(panel)\n            elif (not callable(panel.nav) and panel.nav and\n                    panel.can_access(context)):\n                allowed_panels.append(panel)\n        if allowed_panels:\n            if group.name is None:\n                non_empty_groups.append((dashboard.name, allowed_panels))\n            else:\n                non_empty_groups.append((group.name, allowed_panels))\n\n    return {'components': OrderedDict(non_empty_groups),\n            'user': context['request'].user,\n            'current': context['request'].horizon['panel'].slug,\n            'request': context['request']}", "category": "Python"}, {"instruction": "def DebyeChain(q, Rg):\n    \"\"\"Scattering form-factor intensity of a Gaussian chain (Debye)\n\n    Inputs:\n    -------\n        ``q``: independent variable\n        ``Rg``: radius of gyration\n\n    Formula:\n    --------\n        ``2*(exp(-a)-1+a)/a^2`` where ``a=(q*Rg)^2``\n    \"\"\"\n", "input": "", "output": "    a = (q * Rg) ** 2\n    return 2 * (np.exp(-a) - 1 + a) / a ** 2", "category": "Python"}, {"instruction": "def get_mac(self):\n        \"\"\"\n        :return: the machine's mac address\n        \"\"\"\n", "input": "", "output": "        command = const.CMD_OPTIONS_RRQ\n        command_string = b'MAC\\x00'\n        response_size = 1024\n\n        cmd_response = self.__send_command(command, command_string, response_size)\n        if cmd_response.get('status'):\n            mac = self.__data.split(b'=', 1)[-1].split(b'\\x00')[0]\n            return mac.decode()\n        else:\n            raise ZKErrorResponse(\"can't read mac address\")", "category": "Python"}, {"instruction": "def get_pwhash_bits(params):\n    \"\"\" Extract bits for password hash validation from params. \"\"\"\n", "input": "", "output": "    if not \"pwhash\" in params or \\\n            not \"nonce\" in params or \\\n            not \"aead\" in params or \\\n            not \"kh\" in params:\n        raise Exception(\"Missing required parameter in request (pwhash, nonce, aead or kh)\")\n    pwhash = params[\"pwhash\"][0]\n    nonce = params[\"nonce\"][0]\n    aead = params[\"aead\"][0]\n    key_handle = pyhsm.util.key_handle_to_int(params[\"kh\"][0])\n    return pwhash, nonce, aead, key_handle", "category": "Python"}, {"instruction": "def delete(self, name):\n        \"\"\"Delete object on remote\"\"\"\n", "input": "", "output": "        obj = self._get_object(name)\n        if obj:\n            return self.driver.delete_object(obj)", "category": "Python"}, {"instruction": "def kv_depth(d, depth, _counter=1):\n        \"\"\"Iterate items on specific depth.\n        depth has to be greater equal than 0.\n    \n        Usage::\n            \n            >>> for key, node in DictTree.kv_depth(d, 2):\n            >>>     print(key, DictTree.getattr(node, \"population\"))   \n            bethesta 5800\n            germentown 1400\n            vienna 1500\n            arlington 5000\n        \"\"\"\n", "input": "", "output": "        if depth == 0:\n            yield d[_meta][\"_rootname\"], d\n        else:\n            if _counter == depth:\n                for key, node in DictTree.kv(d):\n                    yield key, node\n            else:\n                _counter += 1\n                for node in DictTree.v(d):\n                    for key, node in DictTree.kv_depth(node, depth, _counter):\n                        yield key, node", "category": "Python"}, {"instruction": "def parse_function_signature(code):\n    \"\"\"\n    Return the name, arguments, and return type of the first function\n    definition found in *code*. Arguments are returned as [(type, name), ...].\n    \"\"\"\n", "input": "", "output": "    m = re.search(\"^\\s*\" + re_func_decl + \"\\s*{\", code, re.M)\n    if m is None:\n        print(code)\n        raise Exception(\"Failed to parse function signature. \"\n                        \"Full code is printed above.\")\n    rtype, name, args = m.groups()[:3]\n    if args == 'void' or args.strip() == '':\n        args = []\n    else:\n        args = [tuple(arg.strip().split(' ')) for arg in args.split(',')]\n    return name, args, rtype", "category": "Python"}, {"instruction": "def add_file_to_archive(self, name: str) -> None:\n        \"\"\"\n        Any class in its ``from_params`` method can request that some of its\n        input files be added to the archive by calling this method.\n\n        For example, if some class ``A`` had an ``input_file`` parameter, it could call\n\n        ```\n        params.add_file_to_archive(\"input_file\")\n        ```\n\n        which would store the supplied value for ``input_file`` at the key\n        ``previous.history.and.then.input_file``. The ``files_to_archive`` dict\n        is shared with child instances via the ``_check_is_dict`` method, so that\n        the final mapping can be retrieved from the top-level ``Params`` object.\n\n        NOTE: You must call ``add_file_to_archive`` before you ``pop()``\n        the parameter, because the ``Params`` instance looks up the value\n        of the filename inside itself.\n\n        If the ``loading_from_archive`` flag is True, this will be a no-op.\n        \"\"\"\n", "input": "", "output": "        if not self.loading_from_archive:\n            self.files_to_archive[f\"{self.history}{name}\"] = cached_path(self.get(name))", "category": "Python"}, {"instruction": "def find(self, chrom, start, end):\n        \"\"\"find the intersecting elements\n\n        :param chrom: chromosome\n        :param start: start\n        :param end: end\n        :return: a list of intersecting elements\"\"\"\n", "input": "", "output": "\n        tree = self._trees.get( chrom, None )\n        if tree:\n            return tree.find( start, end )\n        #return always a list\n        return []", "category": "Python"}, {"instruction": "def get_raise_brok(self, host_name, service_name=''):\n        \"\"\"Get a start downtime brok\n\n        :param host_name: host concerned by the downtime\n        :type host_name\n        :param service_name: service concerned by the downtime\n        :type service_name\n        :return: brok with wanted data\n        :rtype: alignak.brok.Brok\n        \"\"\"\n", "input": "", "output": "        data = self.serialize()\n        data['host'] = host_name\n        if service_name != '':\n            data['service'] = service_name\n\n        return Brok({'type': 'downtime_raise', 'data': data})", "category": "Python"}, {"instruction": "def encode(self, obj):\n        \"\"\"Returns ``obj`` serialized as JSON formatted bytes.\n\n        Raises\n        ------\n        ~ipfsapi.exceptions.EncodingError\n\n        Parameters\n        ----------\n        obj : str | list | dict | int\n            JSON serializable Python object\n\n        Returns\n        -------\n            bytes\n        \"\"\"\n", "input": "", "output": "        try:\n            result = json.dumps(obj, sort_keys=True, indent=None,\n                                separators=(',', ':'), ensure_ascii=False)\n            if isinstance(result, six.text_type):\n                return result.encode(\"utf-8\")\n            else:\n                return result\n        except (UnicodeEncodeError, TypeError) as error:\n            raise exceptions.EncodingError('json', error)", "category": "Python"}, {"instruction": "def get_metadata(engine_name=None):\n    \"\"\"\n    get metadata according used for alembic\n    It'll import all tables\n    \"\"\"\n", "input": "", "output": "    dispatch.get(None, 'load_models')\n\n    engine = engine_manager[engine_name]\n    \n    for tablename, m in engine.models.items():\n        get_model(tablename, engine_name, signal=False)\n        if hasattr(m, '__dynamic__') and getattr(m, '__dynamic__'):\n            m.table.__mapping_only__ = True\n    return engine.metadata", "category": "Python"}, {"instruction": "def tag_structure(tag, site):\n    \"\"\"\n    A tag structure.\n    \"\"\"\n", "input": "", "output": "    return {'tag_id': tag.pk,\n            'name': tag.name,\n            'count': tag.count,\n            'slug': tag.name,\n            'html_url': '%s://%s%s' % (\n                PROTOCOL, site.domain,\n                reverse('zinnia:tag_detail', args=[tag.name])),\n            'rss_url': '%s://%s%s' % (\n                PROTOCOL, site.domain,\n                reverse('zinnia:tag_feed', args=[tag.name]))\n            }", "category": "Python"}, {"instruction": "def register(self, name, function, description=None):\n        \"\"\"\n        Register a new thread.\n\n        :param function: Function, which gets called for the new thread\n        :type function: function\n        :param name: Unique name of the thread for documentation purposes.\n        :param description: Short description of the thread\n        \"\"\"\n", "input": "", "output": "        return self.__app.threads.register(name, function, self._plugin, description)", "category": "Python"}, {"instruction": "def _validate_positional_arguments(args):\n    \"\"\"\n    To validate the positional argument feature - https://github.com/Azure/azure-cli/pull/6055.\n    Assuming that unknown commands are positional arguments immediately\n    led by words that only appear at the end of the commands\n\n    Slight modification of\n    https://github.com/Azure/azure-cli/blob/dev/src/azure-cli-core/azure/cli/core/commands/__init__.py#L356-L373\n\n    Args:\n        args: The arguments that the user inputs in the terminal.\n\n    Returns:\n        Rudimentary parsed arguments.\n    \"\"\"\n", "input": "", "output": "    nouns = []\n    for arg in args:\n        if not arg.startswith('-') or not arg.startswith('{{'):\n            nouns.append(arg)\n        else:\n            break\n\n    while nouns:\n        search = ' '.join(nouns)\n        # Since the command name may be immediately followed by a positional arg, strip those off\n        if not next((x for x in azext_alias.cached_reserved_commands if x.endswith(search)), False):\n            del nouns[-1]\n        else:\n            return\n\n    raise CLIError(INVALID_ALIAS_COMMAND_ERROR.format(' '.join(args)))", "category": "Python"}, {"instruction": "def from_dict(cls, dict_in):\n        \"\"\"Initialize the class from a dict.\n\n        :param dict_in: The dictionary that contains the item content. Required\n            fields are listed class variable by that name\n        :type dict_in: dict\n        \"\"\"\n", "input": "", "output": "        kwargs = dict_in.copy()\n        args = [kwargs.pop(key) for key in cls.required_fields]\n        return cls(*args, **kwargs)", "category": "Python"}, {"instruction": "def add_row(self, data: list):\n        \"\"\"\n        Add a row of data to the current widget\n\n        :param data: a row of data\n        :return: None\n        \"\"\"\n", "input": "", "output": "        # validation\n        if self.headers:\n            if len(self.headers) != len(data):\n                raise ValueError\n\n        if len(data) != self.num_of_columns:\n            raise ValueError\n\n        offset = 0 if not self.headers else 1\n        row = list()\n        for i, element in enumerate(data):\n            label = ttk.Label(self, text=str(element), relief=tk.GROOVE,\n                              padding=self.padding)\n            label.grid(row=len(self._rows) + offset, column=i, sticky='E,W')\n            row.append(label)\n\n        self._rows.append(row)", "category": "Python"}, {"instruction": "def has_manifest(app, filename='manifest.json'):\n    '''Verify the existance of a JSON assets manifest'''\n", "input": "", "output": "    try:\n        return pkg_resources.resource_exists(app, filename)\n    except ImportError:\n        return os.path.isabs(filename) and os.path.exists(filename)", "category": "Python"}, {"instruction": "def _convert_to_unicode(string):\n    \"\"\"This method should work with both Python 2 and 3 with the caveat\n    that they need to be compiled with wide unicode character support.\n\n    If there isn't wide unicode character support it'll blow up with a\n    warning.\n\n    \"\"\"\n", "input": "", "output": "    codepoints = []\n    for character in string.split('-'):\n        if character in BLACKLIST_UNICODE:\n            next\n\n        codepoints.append(\n            '\\U{0:0>8}'.format(character).decode('unicode-escape')\n        )\n\n    return codepoints", "category": "Python"}, {"instruction": "def remove_user(self, group, username):\n        \"\"\"\n        Remove a user from the specified LDAP group.\n\n        Args:\n            group: Name of group to update\n            username: Username of user to remove\n\n        Raises:\n            ldap_tools.exceptions.InvalidResult:\n                Results of the query were invalid.  The actual exception raised\n                inherits from InvalidResult.  See #lookup_id for more info.\n\n        \"\"\"\n", "input": "", "output": "        try:\n            self.lookup_id(group)\n        except ldap_tools.exceptions.InvalidResult as err:  # pragma: no cover\n            raise err from None\n\n        operation = {'memberUid': [(ldap3.MODIFY_DELETE, [username])]}\n        self.client.modify(self.__distinguished_name(group), operation)", "category": "Python"}, {"instruction": "def register_resources(klass, registry, resource_class):\n        \"\"\" meta model subscriber on resource registration.\n\n        We watch for PHD event that provides affected entities and register\n        the health-event filter to the resources.\n        \"\"\"\n", "input": "", "output": "        services = {'acm-certificate', 'directconnect', 'dms-instance', 'directory', 'ec2',\n                    'dynamodb-table', 'cache-cluster', 'efs', 'app-elb', 'elb', 'emr', 'rds',\n                    'storage-gateway'}\n        if resource_class.type in services:\n            resource_class.filter_registry.register('health-event', klass)", "category": "Python"}, {"instruction": "def _domain_event_migration_iteration_cb(conn, domain, iteration, opaque):\n    '''\n    Domain migration iteration events handler\n    '''\n", "input": "", "output": "    _salt_send_domain_event(opaque, conn, domain, opaque['event'], {\n        'iteration': iteration\n    })", "category": "Python"}, {"instruction": "def setup(interactive, not_relative, dry_run, reset, root_dir, testuser):\n    \"\"\"This will help you to setup cellpy.\"\"\"\n", "input": "", "output": "\n    click.echo(\"[cellpy] (setup)\")\n\n    # generate variables\n    init_filename = create_custom_init_filename()\n    userdir, dst_file = get_user_dir_and_dst(init_filename)\n\n    if testuser:\n        if not root_dir:\n            root_dir = os.getcwd()\n\n        click.echo(f\"[cellpy] (setup) DEV-MODE testuser: {testuser}\")\n        init_filename = create_custom_init_filename(testuser)\n        userdir = root_dir\n        dst_file = get_dst_file(userdir, init_filename)\n        click.echo(f\"[cellpy] (setup) DEV-MODE userdir: {userdir}\")\n        click.echo(f\"[cellpy] (setup) DEV-MODE dst_file: {dst_file}\")\n\n    if not pathlib.Path(dst_file).is_file():\n        reset = True\n\n    if interactive:\n        click.echo(\" interactive mode \".center(80, \"-\"))\n        _update_paths(root_dir, not not_relative, dry_run=dry_run, reset=reset)\n        _write_config_file(\n            userdir, dst_file,\n            init_filename, dry_run,\n        )\n        _check()\n\n    else:\n        _write_config_file(userdir, dst_file, init_filename, dry_run)\n        _check()", "category": "Python"}, {"instruction": "def do_GET(self):\n        \"\"\" Handle a HTTP GET request. \"\"\"\n", "input": "", "output": "        # Example session:\n        # in  : GET /wsapi/decrypt?otp=ftftftccccdvvbfcfduvvcubikngtchlubtutucrld HTTP/1.0\n        # out : OK counter=0004 low=f585 high=3e use=03\n        if self.path.startswith(self.serve_url):\n            from_key = self.path[len(self.serve_url):]\n\n            val_res = self.decrypt_yubikey_otp(from_key)\n\n            self.send_response(200)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n            self.wfile.write(val_res)\n            self.wfile.write(\"\\n\")\n        elif self.stats_url and self.path == self.stats_url:\n            self.send_response(200)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n            for key in stats:\n                self.wfile.write(\"%s %d\\n\" % (key, stats[key]))\n        else:\n            self.log_error(\"Bad URL '%s' - I'm serving '%s' (responding 403)\" % (self.path, self.serve_url))\n            self.send_response(403, 'Forbidden')\n            self.end_headers()", "category": "Python"}, {"instruction": "def release(self):\n        \"\"\" Get rid of the lock by deleting the lockfile.\n            When working in a `with` statement, this gets automatically\n            called at the end.\n        \"\"\"\n", "input": "", "output": "        if self.is_locked:\n            os.close(self.fd)\n            os.unlink(self.lockfile)\n            self.is_locked = False", "category": "Python"}, {"instruction": "def get_instance(self, payload):\n        \"\"\"\n        Build an instance of MessageInstance\n\n        :param dict payload: Payload response from the API\n\n        :returns: twilio.rest.messaging.v1.session.message.MessageInstance\n        :rtype: twilio.rest.messaging.v1.session.message.MessageInstance\n        \"\"\"\n", "input": "", "output": "        return MessageInstance(self._version, payload, session_sid=self._solution['session_sid'], )", "category": "Python"}, {"instruction": "def warmup(f):\n    \"\"\" Decorator to run warmup before running a command \"\"\"\n", "input": "", "output": "\n    @wraps(f)\n    def wrapped(self, *args, **kwargs):\n        if not self.warmed_up:\n            self.warmup()\n        return f(self, *args, **kwargs)\n    return wrapped", "category": "Python"}, {"instruction": "def is_list_of_list(item):\n    \"\"\"\n    check whether the item is list (tuple)\n    and consist of list (tuple) elements\n    \"\"\"\n", "input": "", "output": "    if (\n        type(item) in (list, tuple)\n        and len(item)\n        and isinstance(item[0], (list, tuple))\n    ):\n        return True\n    return False", "category": "Python"}, {"instruction": "def enhance_dark_holes(image, min_radius, max_radius, mask=None):\n    '''Enhance dark holes using a rolling ball filter\n\n    image - grayscale 2-d image\n    radii - a vector of radii: we enhance holes at each given radius\n    '''\n", "input": "", "output": "    #\n    # Do 4-connected erosion\n    #\n    se = np.array([[False, True, False],\n                   [True, True, True],\n                   [False, True, False]])\n    #\n    # Invert the intensities\n    #\n    inverted_image = image.max() - image\n    previous_reconstructed_image = inverted_image\n    eroded_image = inverted_image\n    smoothed_image = np.zeros(image.shape)\n    for i in range(max_radius+1):\n        eroded_image = grey_erosion(eroded_image, mask=mask, footprint = se)\n        reconstructed_image = grey_reconstruction(eroded_image, inverted_image,\n                                                  footprint = se)\n        output_image = previous_reconstructed_image - reconstructed_image\n        if i >= min_radius:\n            smoothed_image = np.maximum(smoothed_image,output_image)\n        previous_reconstructed_image = reconstructed_image\n    return smoothed_image", "category": "Python"}, {"instruction": "def create(self, locator):\n        \"\"\"\n        Creates a component identified by given locator.\n\n        :param locator: a locator to identify component to be created.\n\n        :return: the created component.\n        \"\"\"\n", "input": "", "output": "        for registration in self._registrations:\n            this_locator = registration.locator\n\n            if this_locator == locator:\n                try:\n                    return registration.factory(locator)\n                except Exception as ex:\n                    if isinstance(ex, CreateException):\n                        raise ex\n                    \n                    raise CreateException(\n                        None,\n                        \"Failed to create object for \" + str(locator)\n                    ).with_cause(ex)", "category": "Python"}, {"instruction": "def host_chairs(self):\n        '''Returns a list of members that chair the host committee,\n        including \"co-chair\" and \"chairperson.\" This could concievalby\n        yield a false positive if the person's title is 'dunce chair'.\n        '''\n", "input": "", "output": "        chairs = []\n        # Host is guaranteed to be a committe or none.\n        host = self.host()\n        if host is None:\n            return\n        for member, full_member in host.members_objects:\n            if 'chair' in member.get('role', '').lower():\n                chairs.append((member, full_member))\n        return chairs", "category": "Python"}, {"instruction": "def escapejson_filter(value):\n    \"\"\"\n    Escape `value` to prevent </script> and unicode whitespace attacks. If\n    `value` is not a string, JSON-encode it first.\n    \"\"\"\n", "input": "", "output": "    if isinstance(value, six.string_types):\n        string = value\n    else:\n        string = json.dumps(value, cls=DjangoJSONEncoder)\n    return mark_safe(escapejson(string))", "category": "Python"}, {"instruction": "def _get_envs(self):\n        '''\n        Pull the file server environments out of the master options\n        '''\n", "input": "", "output": "        envs = ['base']\n        if 'file_roots' in self.opts:\n            envs.extend([x for x in list(self.opts['file_roots'])\n                         if x not in envs])\n        env_order = self.opts.get('env_order', [])\n        # Remove duplicates while preserving the order\n        members = set()\n        env_order = [env for env in env_order if not (env in members or members.add(env))]\n        client_envs = self.client.envs()\n        if env_order and client_envs:\n            return [env for env in env_order if env in client_envs]\n\n        elif env_order:\n            return env_order\n        else:\n            envs.extend([env for env in client_envs if env not in envs])\n            return envs", "category": "Python"}, {"instruction": "def _values(self):\n        \"\"\"Getter for series values (flattened)\"\"\"\n", "input": "", "output": "        if self.interpolate:\n            return [\n                val[0] for serie in self.series for val in serie.interpolated\n            ]\n        else:\n            return super(Line, self)._values", "category": "Python"}, {"instruction": "def _dump_date(d, delim):\n    \"\"\"Used for `http_date` and `cookie_date`.\"\"\"\n", "input": "", "output": "    if d is None:\n        d = gmtime()\n    elif isinstance(d, datetime):\n        d = d.utctimetuple()\n    elif isinstance(d, (integer_types, float)):\n        d = gmtime(d)\n    return \"%s, %02d%s%s%s%s %02d:%02d:%02d GMT\" % (\n        (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")[d.tm_wday],\n        d.tm_mday,\n        delim,\n        (\n            \"Jan\",\n            \"Feb\",\n            \"Mar\",\n            \"Apr\",\n            \"May\",\n            \"Jun\",\n            \"Jul\",\n            \"Aug\",\n            \"Sep\",\n            \"Oct\",\n            \"Nov\",\n            \"Dec\",\n        )[d.tm_mon - 1],\n        delim,\n        str(d.tm_year),\n        d.tm_hour,\n        d.tm_min,\n        d.tm_sec,\n    )", "category": "Python"}, {"instruction": "def set_quota(self, quota_bytes):\n        \"\"\"\n        Set a quota (in bytes) on this user library.  The quota is 'best effort',\n        and should be set conservatively.\n\n        A quota of 0 is 'unlimited'\n        \"\"\"\n", "input": "", "output": "        self.set_library_metadata(ArcticLibraryBinding.QUOTA, quota_bytes)\n        self.quota = quota_bytes\n        self.quota_countdown = 0", "category": "Python"}, {"instruction": "def get_plugins_dists(app, name=None):\n    '''Return a list of Distributions with enabled udata plugins'''\n", "input": "", "output": "    if name:\n        plugins = set(e.name for e in iter_all(name) if e.name in app.config['PLUGINS'])\n    else:\n        plugins = set(app.config['PLUGINS'])\n    return [\n        d for d in known_dists()\n        if any(set(v.keys()) & plugins for v in d.get_entry_map().values())\n    ]", "category": "Python"}, {"instruction": "def parse_specs(cls, target_specs, build_root=None, exclude_patterns=None, tags=None):\n    \"\"\"Parse string specs into unique `Spec` objects.\n\n    :param iterable target_specs: An iterable of string specs.\n    :param string build_root: The path to the build root.\n    :returns: A `Specs` object.\n    \"\"\"\n", "input": "", "output": "    build_root = build_root or get_buildroot()\n    spec_parser = CmdLineSpecParser(build_root)\n\n    dependencies = tuple(OrderedSet(spec_parser.parse_spec(spec_str) for spec_str in target_specs))\n    return Specs(\n      dependencies=dependencies,\n      exclude_patterns=exclude_patterns if exclude_patterns else tuple(),\n      tags=tags)", "category": "Python"}, {"instruction": "def _warn_if_deprecated_setting_value_requested(\n        self, setting_name, warn_only_if_overridden, suppress_warnings,\n        warning_stacklevel,\n    ):\n        \"\"\"\n        get(), get_object(), get_model() and get_module() must all check\n        whether a requested app setting is deprecated. This method allows\n        the helper to do that in a DRY/consistent way.\n        \"\"\"\n", "input": "", "output": "        if(\n            not suppress_warnings and\n            not warn_only_if_overridden and\n            setting_name in self._deprecated_settings\n        ):\n            depr = self._deprecated_settings[setting_name]\n            depr.warn_if_deprecated_setting_value_requested(warning_stacklevel + 1)", "category": "Python"}, {"instruction": "def delticks(fig):\n    \"\"\"\n     deletes half the x-axis tick marks\n    Parameters\n    ___________\n    fig : matplotlib figure number\n\n    \"\"\"\n", "input": "", "output": "    locs = fig.xaxis.get_ticklocs()\n    nlocs = np.delete(locs, list(range(0, len(locs), 2)))\n    fig.set_xticks(nlocs)", "category": "Python"}, {"instruction": "def _add_interval(all_intervals, new_interval):\n        \"\"\"\n        Adds a new interval to a set of none overlapping intervals.\n\n        :param set[(int,int)] all_intervals: The set of distinct intervals.\n        :param (int,int) new_interval: The new interval.\n        \"\"\"\n", "input": "", "output": "        intervals = None\n        old_interval = None\n        for old_interval in all_intervals:\n            intervals = Type2CondenseHelper._distinct(new_interval, old_interval)\n            if intervals:\n                break\n\n        if intervals is None:\n            all_intervals.add(new_interval)\n        else:\n            if old_interval:\n                all_intervals.remove(old_interval)\n            for distinct_interval in intervals:\n                Type2CondenseHelper._add_interval(all_intervals, distinct_interval)", "category": "Python"}, {"instruction": "def put(self, key, value, minutes):\n        \"\"\"\n        Store an item in the cache for a given number of minutes.\n\n        :param key: The cache key\n        :type key: str\n\n        :param value: The cache value\n        :type value: mixed\n\n        :param minutes: The lifetime in minutes of the cached value\n        :type minutes: int\n        \"\"\"\n", "input": "", "output": "        value = encode(str(self._expiration(minutes))) + encode(self.serialize(value))\n\n        path = self._path(key)\n        self._create_cache_directory(path)\n\n        with open(path, 'wb') as fh:\n            fh.write(value)", "category": "Python"}, {"instruction": "def expert_to_gates(self):\n    \"\"\"Gate values corresponding to the examples in the per-expert `Tensor`s.\n\n    Returns:\n      a list of `num_experts` one-dimensional `Tensor`s of type `tf.float32`.\n    \"\"\"\n", "input": "", "output": "    return self._ep(\n        tf.concat,\n        transpose_list_of_lists(\n            self._dp(lambda d: d.expert_to_gates(), self._dispatchers)), 0)", "category": "Python"}, {"instruction": "def _load_significant_pathways_file(path_to_file):\n    \"\"\"Read in the significant pathways file as a\n    pandas.DataFrame.\n    \"\"\"\n", "input": "", "output": "    feature_pathway_df = pd.read_table(\n        path_to_file, header=0,\n        usecols=[\"feature\", \"side\", \"pathway\"])\n    feature_pathway_df = feature_pathway_df.sort_values(\n        by=[\"feature\", \"side\"])\n    return feature_pathway_df", "category": "Python"}, {"instruction": "async def get_my_did_with_meta(wallet_handle: int, did: str) -> str:\n    \"\"\"\n    Get DID metadata and verkey stored in the wallet.\n\n    :param wallet_handle: wallet handler (created by open_wallet).\n    :param did: The DID to retrieve metadata.\n    :return: DID with verkey and metadata.\n    \"\"\"\n", "input": "", "output": "\n    logger = logging.getLogger(__name__)\n    logger.debug(\"get_my_did_with_meta: >>> wallet_handle: %r, did: %r\",\n                 wallet_handle,\n                 did)\n\n    if not hasattr(get_my_did_with_meta, \"cb\"):\n        logger.debug(\"get_my_did_with_meta: Creating callback\")\n        get_my_did_with_meta.cb = create_cb(CFUNCTYPE(None, c_int32, c_int32, c_char_p))\n\n    c_wallet_handle = c_int32(wallet_handle)\n    c_did = c_char_p(did.encode('utf-8'))\n\n    did_with_meta = await do_call('indy_get_my_did_with_meta',\n                                  c_wallet_handle,\n                                  c_did,\n                                  get_my_did_with_meta.cb)\n\n    res = did_with_meta.decode()\n\n    logger.debug(\"get_my_did_with_meta: <<< res: %r\", res)\n    return res", "category": "Python"}, {"instruction": "def _create_security_group(self, ingress):\n        \"\"\"Send a POST to spinnaker to create a new security group.\n\n        Returns:\n            boolean: True if created successfully\n\n        \"\"\"\n", "input": "", "output": "        template_kwargs = {\n            'app': self.app_name,\n            'env': self.env,\n            'region': self.region,\n            'vpc': get_vpc_id(self.env, self.region),\n            'description': self.properties['security_group']['description'],\n            'ingress': ingress,\n        }\n\n        secgroup_json = get_template(\n            template_file='infrastructure/securitygroup_data.json.j2', formats=self.generated, **template_kwargs)\n\n        wait_for_task(secgroup_json)\n        return True", "category": "Python"}, {"instruction": "def read_hdf5_array(source, path=None, array_type=Array):\n    \"\"\"Read an `Array` from the given HDF5 object\n\n    Parameters\n    ----------\n    source : `str`, :class:`h5py.HLObject`\n        path to HDF file on disk, or open `h5py.HLObject`.\n\n    path : `str`\n        path in HDF hierarchy of dataset.\n\n    array_type : `type`\n        desired return type\n    \"\"\"\n", "input": "", "output": "    dataset = io_hdf5.find_dataset(source, path=path)\n    attrs = dict(dataset.attrs)\n    # unpickle channel object\n    try:\n        attrs['channel'] = _unpickle_channel(attrs['channel'])\n    except KeyError:  # no channel stored\n        pass\n    # unpack byte strings for python3\n    for key in attrs:\n        if isinstance(attrs[key], bytes):\n            attrs[key] = attrs[key].decode('utf-8')\n    return array_type(dataset[()], **attrs)", "category": "Python"}, {"instruction": "def set_value(self, index, value):\r\n        \"\"\"Set value\"\"\"\n", "input": "", "output": "        self._data[ self.keys[index.row()] ] = value\r\n        self.showndata[ self.keys[index.row()] ] = value\r\n        self.sizes[index.row()] = get_size(value)\r\n        self.types[index.row()] = get_human_readable_type(value)\r\n        self.sig_setting_data.emit()", "category": "Python"}, {"instruction": "def _clean_inheritance_tokens(self):\n        \"\"\"create a new copy of this :class:`.Context`. with\n        tokens related to inheritance state removed.\"\"\"\n", "input": "", "output": "\n        c = self._copy()\n        x = c._data\n        x.pop('self', None)\n        x.pop('parent', None)\n        x.pop('next', None)\n        return c", "category": "Python"}, {"instruction": "def set_rich_menu_image(self, rich_menu_id, content_type, content, timeout=None):\n        \"\"\"Call upload rich menu image API.\n\n        https://developers.line.me/en/docs/messaging-api/reference/#upload-rich-menu-image\n\n        Uploads and attaches an image to a rich menu.\n\n        :param str rich_menu_id: IDs of the richmenu\n        :param str content_type: image/jpeg or image/png\n        :param content: image content as bytes, or file-like object\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        \"\"\"\n", "input": "", "output": "        self._post(\n            '/v2/bot/richmenu/{rich_menu_id}/content'.format(rich_menu_id=rich_menu_id),\n            data=content,\n            headers={'Content-Type': content_type},\n            timeout=timeout\n        )", "category": "Python"}, {"instruction": "def insert(self, context):\n\t\n\t\t\"\"\"\n\t\tCreate mail session.\n\t\t\n\t\t:param resort.engine.execution.Context context:\n\t\t   Current execution context.\n\t\t\"\"\"\n", "input": "", "output": "\t\t\n\t\tstatus_code, msg = self.__endpoint.post(\n\t\t\t\"/resources/mail-resource\",\n\t\t\tdata={\n\t\t\t\t\"id\": self.__name,\n\t\t\t\t\"host\": self.__host,\n\t\t\t\t\"user\": self.__username,\n\t\t\t\t\"from\": self.__mail_from,\n\t\t\t\t\"property\": props_value(self.__props)\n\t\t\t}\n\t\t)\n\t\tself.__available = True", "category": "Python"}, {"instruction": "def cnormpath (path):\n    \"\"\"Norm a path name to platform specific notation and make it absolute.\"\"\"\n", "input": "", "output": "    path = normpath(path)\n    if os.name == 'nt':\n        # replace slashes with backslashes\n        path = path.replace(\"/\", \"\\\\\")\n    if not os.path.isabs(path):\n        path = normpath(os.path.join(sys.prefix, path))\n    return path", "category": "Python"}, {"instruction": "def clear(self, name):\n        \"\"\"\n        Clears (resets) a counter specified by its name.\n\n        :param name: a counter name to clear.\n        \"\"\"\n", "input": "", "output": "        self._lock.acquire()\n        try:\n            del self._cache[name]\n        finally:\n            self._lock.release()", "category": "Python"}, {"instruction": "def rabin_karp_matching(s, t):\n    \"\"\"Find a substring by Rabin-Karp\n\n    :param s: the haystack string\n    :param t: the needle string\n\n    :returns: index i such that s[i: i + len(t)] == t, or -1\n    :complexity: O(len(s) + len(t)) in expected time,\n                and O(len(s) * len(t)) in worst case\n    \"\"\"\n", "input": "", "output": "    hash_s = 0\n    hash_t = 0\n    len_s = len(s)\n    len_t = len(t)\n    last_pos = pow(DOMAIN, len_t - 1) % PRIME\n    if len_s < len_t:\n        return -1\n    for i in range(len_t):         # preprocessing\n        hash_s = (DOMAIN * hash_s + ord(s[i])) % PRIME\n        hash_t = (DOMAIN * hash_t + ord(t[i])) % PRIME\n    for i in range(len_s - len_t + 1):\n        if hash_s == hash_t:       # check character by character\n            if matches(s, t, i, 0, len_t):\n                return i\n        if i < len_s - len_t:\n            hash_s = roll_hash(hash_s, ord(s[i]), ord(s[i + len_t]),\n                               last_pos)\n    return -1", "category": "Python"}, {"instruction": "def _handle_invalid_tag_start(self):\n        \"\"\"Handle the (possible) start of an implicitly closing single tag.\"\"\"\n", "input": "", "output": "        reset = self._head + 1\n        self._head += 2\n        try:\n            if not is_single_only(self.tag_splitter.split(self._read())[0]):\n                raise BadRoute()\n            tag = self._really_parse_tag()\n        except BadRoute:\n            self._head = reset\n            self._emit_text(\"</\")\n        else:\n            tag[0].invalid = True  # Set flag of TagOpenOpen\n            self._emit_all(tag)", "category": "Python"}, {"instruction": "def splitRangeByTablets(self, login, tableName, range, maxSplits):\n    \"\"\"\n    Parameters:\n     - login\n     - tableName\n     - range\n     - maxSplits\n    \"\"\"\n", "input": "", "output": "    self.send_splitRangeByTablets(login, tableName, range, maxSplits)\n    return self.recv_splitRangeByTablets()", "category": "Python"}, {"instruction": "def __configure_client(self, config):\n        \"\"\" write the perforce client \"\"\"\n", "input": "", "output": "        self.logger.info(\"Configuring p4 client...\")\n        client_dict = config.to_dict()\n        client_dict['root_path'] = os.path.expanduser(config.get('root_path'))\n        os.chdir(client_dict['root_path'])\n        client_dict['hostname'] = system.NODE\n        client_dict['p4view'] = config['p4view'] % self.environment.target.get_context_dict()\n        client = re.sub('//depot', '    //depot', p4client_template % client_dict)\n        self.logger.info(lib.call(\"%s client -i\" % self.p4_command,\n                                  stdin=client,\n                                  env=self.p4environ,\n                                  cwd=client_dict['root_path']))", "category": "Python"}, {"instruction": "def virtual_host(self):\n\t\t\"\"\" Return request virtual host (\"Host\" header value)\n\n\t\t:return: None or str\n\t\t\"\"\"\n", "input": "", "output": "\t\tif self.headers() is not None:\n\t\t\thost_value = self.headers()['Host']\n\t\t\treturn host_value[0].lower() if host_value is not None else None", "category": "Python"}, {"instruction": "def initialize(self, client, initial_response, deserialization_callback):\n        \"\"\"Set the initial status of this LRO.\n\n        :param initial_response: The initial response of the poller\n        :raises: CloudError if initial status is incorrect LRO state\n        \"\"\"\n", "input": "", "output": "        self._client = client\n        self._response = initial_response\n        self._operation = LongRunningOperation(initial_response, deserialization_callback, self._lro_options)\n        try:\n            self._operation.set_initial_status(initial_response)\n        except BadStatus:\n            self._operation.status = 'Failed'\n            raise CloudError(initial_response)\n        except BadResponse as err:\n            self._operation.status = 'Failed'\n            raise CloudError(initial_response, str(err))\n        except OperationFailed:\n            raise CloudError(initial_response)", "category": "Python"}, {"instruction": "def session_info(self, session):\n        \"\"\"Return all session unique ids recorded in prepare phase.\n\n        :param ts: timestamp, default to current timestamp\n        :return: set of session unique ids\n        \"\"\"\n", "input": "", "output": "        _, sp_hkey = self._keygen(session)\n        picked_event = self.r.hgetall(sp_hkey)\n        event = {s(k): pickle.loads(v) for k, v in picked_event.items()}\n        return event", "category": "Python"}, {"instruction": "def ProcessFingerprint(self, responses):\n    \"\"\"Store the fingerprint response.\"\"\"\n", "input": "", "output": "    if not responses.success:\n      # Its better to raise rather than merely logging since it will make it to\n      # the flow's protobuf and users can inspect the reason this flow failed.\n      raise flow.FlowError(\"Could not fingerprint file: %s\" % responses.status)\n\n    response = responses.First()\n    if response.pathspec.path:\n      pathspec = response.pathspec\n    else:\n      pathspec = self.args.pathspec\n\n    self.state.urn = pathspec.AFF4Path(self.client_urn)\n\n    hash_obj = response.hash\n\n    if data_store.AFF4Enabled():\n      with aff4.FACTORY.Create(\n          self.state.urn, aff4_grr.VFSFile, mode=\"w\", token=self.token) as fd:\n        fd.Set(fd.Schema.HASH, hash_obj)\n\n    if data_store.RelationalDBEnabled():\n      path_info = rdf_objects.PathInfo.FromPathSpec(pathspec)\n      path_info.hash_entry = response.hash\n\n      data_store.REL_DB.WritePathInfos(self.client_id, [path_info])\n\n    self.ReceiveFileFingerprint(\n        self.state.urn, hash_obj, request_data=responses.request_data)", "category": "Python"}, {"instruction": "def get_local_ep(*args, **kwargs):\n    \"\"\"\n    Warning:\n        DEPRECATED: Use ``globus_sdk.LocalGlobusConnectPersonal().endpoint_id`` instead.\n    \"\"\"\n", "input": "", "output": "    if kwargs.get(\"warn\", True):\n        raise DeprecationWarning(\"'get_local_ep()' has been deprecated in favor of \"\n                                 \"'globus_sdk.LocalGlobusConnectPersonal().endpoint_id'. \"\n                                 \"To override, pass in 'warn=False'.\")\n    else:\n        import warnings\n        warnings.warn(\"'get_local_ep()' has been deprecated in favor of \"\n                      \"'globus_sdk.LocalGlobusConnectPersonal().endpoint_id'.\")\n    return globus_sdk.LocalGlobusConnectPersonal().endpoint_id", "category": "Python"}, {"instruction": "def clean_file(self):\n        \"\"\"Analyse the uploaded file, and return the parsed lines.\n\n        Returns:\n            tuple of tuples of cells content (as text).\n        \"\"\"\n", "input": "", "output": "        data = self.cleaned_data['file']\n\n        available_parsers = self.get_parsers()\n\n        for parser in available_parsers:\n            try:\n                return parser.parse_file(data)\n            except parsers.ParserError:\n                pass\n\n        raise forms.ValidationError(\n            \"No parser could read the file. Tried with parsers %s.\" %\n            (\", \" % (force_text(p) for p in available_parsers)))", "category": "Python"}, {"instruction": "def reset(self):\n        \"\"\"Set `iter` to 0.\"\"\"\n", "input": "", "output": "        import tqdm\n        self.iter = 0\n        self.pbar = tqdm.tqdm(total=self.niter, **self.kwargs)", "category": "Python"}, {"instruction": "def add_hook(self, name, callback, prio=0):\n        \"\"\" Add a new hook that can be called with the call_hook function.\n            `prio` is the priority. Higher priority hooks are called before lower priority ones.\n            This function does not enforce a particular order between hooks with the same priorities.\n        \"\"\"\n", "input": "", "output": "        hook_list = self._hooks.get(name, [])\n\n        add = (lambda *args, **kwargs: self._exception_free_callback(callback, *args, **kwargs)), -prio\n        pos = bisect.bisect_right(list(x[1] for x in hook_list), -prio)\n        hook_list[pos:pos] = [add]\n\n        self._hooks[name] = hook_list", "category": "Python"}, {"instruction": "def smilin(smiles, transforms=[figueras.sssr, aromaticity.aromatize]):\r\n    \"\"\"(smiles)->molecule\r\n    Convert a smiles string into a molecule representation\"\"\"\n", "input": "", "output": "    builder = BuildMol()\r\n    tokenize(smiles, builder)\r\n    mol = builder.mol\r\n\r\n        \r\n    for transform in transforms:\r\n        mol = transform(mol)\r\n\r\n    ## implicit hcount doesn't make any sense anymore...\r\n    for atom in mol.atoms:\r\n        if not atom.has_explicit_hcount:\r\n            atom.imp_hcount = atom.hcount - atom.explicit_hcount\r\n\r\n    return mol", "category": "Python"}, {"instruction": "def is_indel(reference_bases, alternate_bases):\n    \"\"\" Return whether or not the variant is an INDEL \"\"\"\n", "input": "", "output": "    if len(reference_bases) > 1:\n        return True\n    for alt in alternate_bases:\n        if alt is None:\n            return True\n        elif len(alt) != len(reference_bases):\n            return True\n    return False", "category": "Python"}, {"instruction": "def _z2deriv(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n            _z2deriv\n        PURPOSE:\n            evaluate the second vertical derivative for this potential\n        INPUT:\n            R - Galactocentric cylindrical radius\n            z - vertical height\n            phi - azimuth\n            t- time\n        OUTPUT:\n            the second vertical derivative\n        HISTORY:\n            2015-02-13 - Written - Trick (MPIA)\n        \"\"\"\n", "input": "", "output": "        l,n    = bovy_coords.Rz_to_lambdanu    (R,z,ac=self._ac,Delta=self._Delta)\n        jac    = bovy_coords.Rz_to_lambdanu_jac(R,z,            Delta=self._Delta)\n        hess   = bovy_coords.Rz_to_lambdanu_hess(R,z,            Delta=self._Delta)\n        dldz = jac[0,1]\n        dndz = jac[1,1]\n        d2ldz2 = hess[0,1,1]\n        d2ndz2 = hess[1,1,1]\n        return d2ldz2       * self._lderiv(l,n)  + \\\n               d2ndz2       * self._nderiv(l,n)  + \\\n               (dldz)**2    * self._l2deriv(l,n) + \\\n               (dndz)**2    * self._n2deriv(l,n) + \\\n               2.*dldz*dndz * self._lnderiv(l,n)", "category": "Python"}, {"instruction": "def pathFromHere_explore(self, astr_startPath = '/'):\n            \"\"\"\n            Return a list of paths from \"here\" in the stree, using the\n            child explore access.\n\n            :param astr_startPath: path from which to start\n            :return: a list of paths from \"here\"\n            \"\"\"\n", "input": "", "output": "\n            self.l_lwd  = []\n            self.treeExplore(startPath = astr_startPath, f=self.lwd)\n            return self.l_lwd", "category": "Python"}, {"instruction": "def get_as_array(self, key):\n        \"\"\"\n        Converts map element into an AnyValueMap or returns empty AnyValueMap if conversion is not possible.\n\n        :param key: an index of element to get.\n\n        :return: AnyValueMap value of the element or empty AnyValueMap if conversion is not supported.\n        \"\"\"\n", "input": "", "output": "        value = self.get(key)\n        return AnyValueMap.from_value(value)", "category": "Python"}, {"instruction": "def mutate(self, info_in):\n        \"\"\"Replicate an info + mutation.\n\n        To mutate an info, that info must have a method called\n        ``_mutated_contents``.\n\n        \"\"\"\n", "input": "", "output": "        # check self is not failed\n        if self.failed:\n            raise ValueError(\"{} cannot mutate as it has failed.\".format(self))\n\n        from transformations import Mutation\n        info_out = type(info_in)(origin=self,\n                                 contents=info_in._mutated_contents())\n        Mutation(info_in=info_in, info_out=info_out)", "category": "Python"}, {"instruction": "def dumps(self, param):\n        '''\n        Checks the parameters generating new proxy instances to avoid\n        query concurrences from shared proxies and creating proxies for\n        actors from another host.\n        '''\n", "input": "", "output": "        if isinstance(param, Proxy):\n            module_name = param.actor.klass.__module__\n            filename = sys.modules[module_name].__file__\n            return ProxyRef(param.actor.url, param.actor.klass.__name__,\n                            module_name)\n        elif isinstance(param, list):\n            return [self.dumps(elem) for elem in param]\n        elif isinstance(param, dict):\n            new_dict = param\n            for key in new_dict.keys():\n                new_dict[key] = self.dumps(new_dict[key])\n            return new_dict\n        elif isinstance(param, tuple):\n            return tuple([self.dumps(elem) for elem in param])\n        else:\n            return param", "category": "Python"}, {"instruction": "def set_current_language(self, language_code, initialize=False):\n        \"\"\"\n        Switch the currently activate language of the object.\n        \"\"\"\n", "input": "", "output": "        self._current_language = normalize_language_code(language_code or get_language())\n\n        # Ensure the translation is present for __get__ queries.\n        if initialize:\n            self._get_translated_model(use_fallback=False, auto_create=True)", "category": "Python"}, {"instruction": "def verify(signature: Signature, message: bytes, ver_key: VerKey, gen: Generator) -> bool:\n        \"\"\"\n        Verifies the message signature and returns true - if signature valid or false otherwise.\n\n        :param: signature - Signature to verify\n        :param: message - Message to verify\n        :param: ver_key - Verification key\n        :param: gen - Generator point\n        :return: true if signature valid\n        \"\"\"\n", "input": "", "output": "\n        logger = logging.getLogger(__name__)\n        logger.debug(\"Bls::verify: >>> signature: %r, message: %r, ver_key: %r, gen: %r\", signature, message, ver_key,\n                     gen)\n\n        valid = c_bool()\n        do_call('indy_crypto_bsl_verify',\n                signature.c_instance,\n                message, len(message),\n                ver_key.c_instance,\n                gen.c_instance,\n                byref(valid))\n\n        res = valid\n        logger.debug(\"Bls::verify: <<< res: %r\", res)\n        return res", "category": "Python"}, {"instruction": "def cast_to_list(emoticons_list):\n    \"\"\"\n    Fix list of emoticons with a single name\n    to a list for easier future iterations,\n    and cast iterables to list.\n    \"\"\"\n", "input": "", "output": "    emoticons_tuple = []\n    for emoticons, image in emoticons_list:\n        if isinstance(emoticons, basestring):\n            emoticons = [emoticons]\n        else:\n            emoticons = list(emoticons)\n        emoticons_tuple.append((emoticons, image))\n    return emoticons_tuple", "category": "Python"}, {"instruction": "def geom(self):\n        \"\"\"Provide :shapely:`Shapely LineString object<linestrings>` geometry of\n        :class:`Line`\"\"\"\n", "input": "", "output": "        adj_nodes = self._grid._graph.nodes_from_line(self)\n\n        return LineString([adj_nodes[0].geom, adj_nodes[1].geom])", "category": "Python"}, {"instruction": "def convert_old_command_expansions(command):\n    \"\"\"Convert expansions from !OLD! style to {new}.\"\"\"\n", "input": "", "output": "    command = command.replace(\"!VERSION!\",       \"{version}\")\n    command = command.replace(\"!MAJOR_VERSION!\", \"{version.major}\")\n    command = command.replace(\"!MINOR_VERSION!\", \"{version.minor}\")\n    command = command.replace(\"!BASE!\",          \"{base}\")\n    command = command.replace(\"!ROOT!\",          \"{root}\")\n    command = command.replace(\"!USER!\",          \"{system.user}\")\n    return command", "category": "Python"}, {"instruction": "def GetMetaData(self, request):\n    \"\"\"Get metadata from local metadata server.\n\n    Any failed URL check will fail the whole action since our bios/service\n    checks may not always correctly identify cloud machines. We don't want to\n    wait on multiple DNS timeouts.\n\n    Args:\n      request: CloudMetadataRequest object\n    Returns:\n      rdf_cloud.CloudMetadataResponse object\n    Raises:\n      ValueError: if request has a timeout of 0. This is a defensive\n      check (we pass 1.0) because the requests library just times out and it's\n      not obvious why.\n    \"\"\"\n", "input": "", "output": "    if request.timeout == 0:\n      raise ValueError(\"Requests library can't handle timeout of 0\")\n    result = requests.request(\n        \"GET\", request.url, headers=request.headers, timeout=request.timeout)\n    # By default requests doesn't raise on HTTP error codes.\n    result.raise_for_status()\n\n    # Requests does not always raise an exception when an incorrect response\n    # is received. This fixes that behaviour.\n    if not result.ok:\n      raise requests.RequestException(response=result)\n\n    return rdf_cloud.CloudMetadataResponse(\n        label=request.label or request.url, text=result.text)", "category": "Python"}, {"instruction": "def check_raw_string(self, string, is_bstring=True):\n        \"\"\"\n        Check whether the given string\n        is properly UTF-8 encoded (if ``is_bytes`` is ``True``),\n        it is not empty, and\n        it does not contain reserved characters.\n\n        :param string string: the byte string or Unicode string to be checked\n        :param bool is_bstring: if True, string is a byte string\n        :rtype: :class:`~aeneas.validator.ValidatorResult`\n        \"\"\"\n", "input": "", "output": "        self.log(u\"Checking the given byte string\")\n        self.result = ValidatorResult()\n        if self._are_safety_checks_disabled(u\"check_raw_string\"):\n            return self.result\n        if is_bstring:\n            self._check_utf8_encoding(string)\n            if not self.result.passed:\n                return self.result\n            string = gf.safe_unicode(string)\n        self._check_not_empty(string)\n        if not self.result.passed:\n            return self.result\n        self._check_reserved_characters(string)\n        return self.result", "category": "Python"}, {"instruction": "def push(self, request):\n        \"\"\"Push a request\"\"\"\n", "input": "", "output": "        self.server.basic_publish(\n            exchange='',\n            routing_key=self.key,\n            body=self._encode_request(request)\n        )", "category": "Python"}, {"instruction": "def loggray(x, a, b):\n    \"\"\"Auxiliary function that specifies the logarithmic gray scale.\n    a and b are the cutoffs.\"\"\"\n", "input": "", "output": "    linval = 10.0 + 990.0 * (x-float(a))/(b-a)\n    return (np.log10(linval)-1.0)*0.5 * 255.0", "category": "Python"}, {"instruction": "def doflip(dec, inc):\n    \"\"\"\n    flips lower hemisphere data to upper hemisphere\n    \"\"\"\n", "input": "", "output": "    if inc < 0:\n        inc = -inc\n        dec = (dec + 180.) % 360.\n    return dec, inc", "category": "Python"}, {"instruction": "def add_result(self, scan_id, result_type, host='', name='', value='',\n                   port='', test_id='', severity='', qod=''):\n        \"\"\" Add a result to a scan in the table. \"\"\"\n", "input": "", "output": "\n        assert scan_id\n        assert len(name) or len(value)\n        result = dict()\n        result['type'] = result_type\n        result['name'] = name\n        result['severity'] = severity\n        result['test_id'] = test_id\n        result['value'] = value\n        result['host'] = host\n        result['port'] = port\n        result['qod'] = qod\n        results = self.scans_table[scan_id]['results']\n        results.append(result)\n        # Set scan_info's results to propagate results to parent process.\n        self.scans_table[scan_id]['results'] = results", "category": "Python"}, {"instruction": "def checksum(digits):\n    \"\"\"\n    Returns the checksum of CPF digits.\n    References to the algorithm:\n    https://pt.wikipedia.org/wiki/Cadastro_de_pessoas_f%C3%ADsicas#Algoritmo\n    https://metacpan.org/source/MAMAWE/Algorithm-CheckDigits-v1.3.0/lib/Algorithm/CheckDigits/M11_004.pm\n    \"\"\"\n", "input": "", "output": "    s = 0\n    p = len(digits) + 1\n    for i in range(0, len(digits)):\n        s += digits[i] * p\n        p -= 1\n\n    reminder = s % 11\n    if reminder == 0 or reminder == 1:\n        return 0\n    else:\n        return 11 - reminder", "category": "Python"}, {"instruction": "def imagetransformer_base_10l_16h_big_uncond_dr01_imgnet():\n  \"\"\"big 1d model for conditional image generation.\"\"\"\n", "input": "", "output": "  hparams = imagetransformer_base_14l_8h_big_dr01()\n  # num_hidden_layers\n  hparams.num_decoder_layers = 10\n  hparams.num_heads = 16\n  hparams.hidden_size = 1024\n  hparams.filter_size = 4096\n  hparams.batch_size = 1\n  hparams.layer_prepostprocess_dropout = 0.1\n  return hparams", "category": "Python"}, {"instruction": "def mouseMove(self, PSRML=None, dy=0):\n        \"\"\" Low-level mouse actions \"\"\"\n", "input": "", "output": "        if PSRML is None:\n            PSRML = self._lastMatch or self # Whichever one is not None\n        if isinstance(PSRML, Pattern):\n            move_location = self.find(PSRML).getTarget()\n        elif isinstance(PSRML, basestring):\n            move_location = self.find(PSRML).getTarget()\n        elif isinstance(PSRML, Match):\n            move_location = PSRML.getTarget()\n        elif isinstance(PSRML, Region):\n            move_location = PSRML.getCenter()\n        elif isinstance(PSRML, Location):\n            move_location = PSRML\n        elif isinstance(PSRML, int):\n            # Assume called as mouseMove(dx, dy)\n            offset = Location(PSRML, dy)\n            move_location = Mouse.getPos().offset(offset)\n        else:\n            raise TypeError(\"doubleClick expected Pattern, String, Match, Region, or Location object\")\n        Mouse.moveSpeed(move_location)", "category": "Python"}, {"instruction": "def strip_byte_order_mark(cls, data):\n        \"\"\"If a byte-order mark is present, strip it and return the encoding it implies.\"\"\"\n", "input": "", "output": "        encoding = None\n        if (len(data) >= 4) and (data[:2] == b'\\xfe\\xff') \\\n               and (data[2:4] != '\\x00\\x00'):\n            encoding = 'utf-16be'\n            data = data[2:]\n        elif (len(data) >= 4) and (data[:2] == b'\\xff\\xfe') \\\n                 and (data[2:4] != '\\x00\\x00'):\n            encoding = 'utf-16le'\n            data = data[2:]\n        elif data[:3] == b'\\xef\\xbb\\xbf':\n            encoding = 'utf-8'\n            data = data[3:]\n        elif data[:4] == b'\\x00\\x00\\xfe\\xff':\n            encoding = 'utf-32be'\n            data = data[4:]\n        elif data[:4] == b'\\xff\\xfe\\x00\\x00':\n            encoding = 'utf-32le'\n            data = data[4:]\n        return data, encoding", "category": "Python"}, {"instruction": "def world_coords(self):\n        \"\"\"\n        :return: world coordinates of mate.\n        :rtype: :class:`CoordSystem <cqparts.utils.geometry.CoordSystem>`\n        :raises ValueError: if ``.component`` does not have valid world coordinates.\n\n        If ``.component`` is ``None``, then the ``.local_coords`` are returned.\n        \"\"\"\n", "input": "", "output": "        if self.component is None:\n            # no component, world == local\n            return copy(self.local_coords)\n        else:\n            cmp_origin = self.component.world_coords\n            if cmp_origin is None:\n                raise ValueError(\n                    \"mate's component does not have world coordinates; \"\n                    \"cannot get mate's world coordinates\"\n                )\n\n            return cmp_origin + self.local_coords", "category": "Python"}, {"instruction": "def truncate_database(self, database=None):\n        \"\"\"Drop all tables in a database.\"\"\"\n", "input": "", "output": "        # Change database if needed\n        if database in self.databases and database is not self.database:\n            self.change_db(database)\n\n        # Get list of tables\n        tables = self.tables if isinstance(self.tables, list) else [self.tables]\n        if len(tables) > 0:\n            self.drop(tables)\n            self._printer('\\t' + str(len(tables)), 'tables truncated from', database)\n        return tables", "category": "Python"}, {"instruction": "def links(self, base_link, current_page) -> dict:\r\n        \"\"\" Return JSON paginate links \"\"\"\n", "input": "", "output": "        max_pages = self.max_pages - 1 if \\\r\n            self.max_pages > 0 else self.max_pages\r\n        base_link = '/%s' % (base_link.strip(\"/\"))\r\n        self_page = current_page\r\n        prev = current_page - 1 if current_page is not 0 else None\r\n        prev_link = '%s/page/%s/%s' % (base_link, prev, self.limit) if \\\r\n            prev is not None else None\r\n        next = current_page + 1 if current_page < max_pages else None\r\n        next_link = '%s/page/%s/%s' % (base_link, next, self.limit) if \\\r\n            next is not None else None\r\n        first = 0\r\n        last = max_pages\r\n        return {\r\n            'self': '%s/page/%s/%s' % (base_link, self_page, self.limit),\r\n            'prev': prev_link,\r\n            'next': next_link,\r\n            'first': '%s/page/%s/%s' % (base_link, first, self.limit),\r\n            'last': '%s/page/%s/%s' % (base_link, last, self.limit),\r\n        }", "category": "Python"}, {"instruction": "def create_legend(info, c, option=\"clean\", use_index=False):\n    \"\"\"creating more informative legends\"\"\"\n", "input": "", "output": "\n    logging.debug(\"    - creating legends\")\n    mass, loading, label = info.loc[c, [\"masses\", \"loadings\", \"labels\"]]\n\n    if use_index or not label:\n        label = c.split(\"_\")\n        label = \"_\".join(label[1:])\n\n    if option == \"clean\":\n        return label\n\n    if option == \"mass\":\n        label = f\"{label} ({mass:.2f} mg)\"\n    elif option == \"loading\":\n        label = f\"{label} ({loading:.2f} mg/cm2)\"\n    elif option == \"all\":\n        label = f\"{label} ({mass:.2f} mg) ({loading:.2f} mg/cm2)\"\n\n    return label", "category": "Python"}, {"instruction": "def add_startup_handler(self, handler):\n        \"\"\"Adds a startup handler to the hug api\"\"\"\n", "input": "", "output": "        if not self.startup_handlers:\n            self._startup_handlers = []\n\n        self.startup_handlers.append(handler)", "category": "Python"}, {"instruction": "def autodetect():\n    \"\"\"\n    Auto-detects and use the first available QT_API by importing them in the\n    following order:\n\n    1) PyQt5\n    2) PyQt4\n    3) PySide\n    \"\"\"\n", "input": "", "output": "    logging.getLogger(__name__).debug('auto-detecting QT_API')\n    try:\n        logging.getLogger(__name__).debug('trying PyQt5')\n        import PyQt5\n        os.environ[QT_API] = PYQT5_API[0]\n        logging.getLogger(__name__).debug('imported PyQt5')\n    except ImportError:\n        try:\n            logging.getLogger(__name__).debug('trying PyQt4')\n            setup_apiv2()\n            import PyQt4\n            os.environ[QT_API] = PYQT4_API[0]\n            logging.getLogger(__name__).debug('imported PyQt4')\n        except ImportError:\n            try:\n                logging.getLogger(__name__).debug('trying PySide')\n                import PySide\n                os.environ[QT_API] = PYSIDE_API[0]\n                logging.getLogger(__name__).debug('imported PySide')\n            except ImportError:\n                raise PythonQtError('No Qt bindings could be found')", "category": "Python"}, {"instruction": "def get_datastrip_name(self, datastrip):\n        \"\"\"\n        :param datastrip: name of datastrip\n        :type datastrip: str\n        :return: name of datastrip folder\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        if self.safe_type == EsaSafeType.OLD_TYPE:\n            return datastrip\n        return '_'.join(datastrip.split('_')[4:-1])", "category": "Python"}, {"instruction": "def onCMapSave(self, event=None, col='int'):\n        \"\"\"save color table image\"\"\"\n", "input": "", "output": "        file_choices = 'PNG (*.png)|*.png'\n        ofile = 'Colormap.png'\n        dlg = wx.FileDialog(self, message='Save Colormap as...',\n                            defaultDir=os.getcwd(),\n                            defaultFile=ofile,\n                            wildcard=file_choices,\n                            style=wx.FD_SAVE|wx.FD_CHANGE_DIR)\n\n        if dlg.ShowModal() == wx.ID_OK:\n            self.cmap_panels[0].cmap_canvas.print_figure(dlg.GetPath(), dpi=600)", "category": "Python"}, {"instruction": "def color_format():\n    \"\"\"\n    Main entry point to get a colored formatter, it will use the\n    BASE_FORMAT by default and fall back to no colors if the system\n    does not support it\n    \"\"\"\n", "input": "", "output": "    str_format = BASE_COLOR_FORMAT if supports_color() else BASE_FORMAT\n    color_format = color_message(str_format)\n    return ColoredFormatter(color_format)", "category": "Python"}, {"instruction": "def commit(self):  # pylint: disable=no-self-use\n        \"\"\"Will commit the changes on the device\"\"\"\n", "input": "", "output": "        if self._config_changed:\n            self._last_working_config = self._download_running_config()\n            self._config_history.append(self._last_working_config)\n            self._committed = True  # comfiguration was committed\n            self._config_changed = False  # no changes since last commit :)\n            return True  # this will be always true\n            # since the changes are automatically applied\n        self._committed = False  # make sure the _committed attribute is not True by any chance\n        return False", "category": "Python"}, {"instruction": "def nodebalancer_create(self, region, **kwargs):\n        \"\"\"\n        Creates a new NodeBalancer in the given Region.\n\n        :param region: The Region in which to create the NodeBalancer.\n        :type region: Region or str\n\n        :returns: The new NodeBalancer\n        :rtype: NodeBalancer\n        \"\"\"\n", "input": "", "output": "        params = {\n            \"region\": region.id if isinstance(region, Base) else region,\n        }\n        params.update(kwargs)\n\n        result = self.post('/nodebalancers', data=params)\n\n        if not 'id' in result:\n            raise UnexpectedResponseError('Unexpected response when creating Nodebalaner!', json=result)\n\n        n = NodeBalancer(self, result['id'], result)\n        return n", "category": "Python"}, {"instruction": "def do_debug(self, args, arguments):\n        \"\"\"\n        ::\n\n        Usage:\n              debug on\n              debug off\n\n              Turns the debug log level on and off.\n        \"\"\"\n", "input": "", "output": "        filename = path_expand(\"~/.cloudmesh/cmd3.yaml\")\n\n        config = ConfigDict(filename=filename)\n        if arguments['on']:\n            self.set_debug(True)\n        elif arguments['off']:\n            self.set_debug(False)", "category": "Python"}, {"instruction": "def data(self, data):\n        \"\"\"Use a length prefixed protocol to give the length of a pickled\n        message.\n\n        \"\"\"\n", "input": "", "output": "        self._buffer = self._buffer + data\n\n        while self._data_handler():\n            pass", "category": "Python"}, {"instruction": "def get_etree_layout_as_dict(layout_tree):\n    \"\"\"\n    Convert something that looks like this:\n    <layout>\n        <item>\n            <name>color</name>\n            <value>red</value>\n        </item>\n        <item>\n            <name>shapefile</name>\n            <value>blah.shp</value>\n        </item>\n    </layout>\n    Into something that looks like this:\n    {\n        'color' : ['red'],\n        'shapefile' : ['blah.shp']\n    }\n    \"\"\"\n", "input": "", "output": "    layout_dict = dict()\n\n    for item in layout_tree.findall('item'):\n        name  = item.find('name').text\n        val_element = item.find('value')\n        value = val_element.text.strip()\n        if value == '':\n            children = val_element.getchildren()\n            value = etree.tostring(children[0], pretty_print=True, encoding=\"unicode\")\n        layout_dict[name] = value\n    return layout_dict", "category": "Python"}, {"instruction": "def add_symbol(name, address):\n    \"\"\"\n    Register the *address* of global symbol *name*.  This will make\n    it usable (e.g. callable) from LLVM-compiled functions.\n    \"\"\"\n", "input": "", "output": "    ffi.lib.LLVMPY_AddSymbol(_encode_string(name), c_void_p(address))", "category": "Python"}, {"instruction": "def identify_raw_files(task: Task, test_mode: bool = False) -> List[str]:\n    \"\"\"\n    Identify raw files that need to be downloaded for a given task.\n\n    :param task: Sequence-to-sequence task.\n    :param test_mode: Run in test mode, only downloading test data.\n    :return: List of raw file names.\n    \"\"\"\n", "input": "", "output": "    raw_files = set()\n    all_sets = [task.test,] if test_mode else [task.train, task.dev, task.test]\n    for file_sets in all_sets:\n        for file_set in file_sets:\n            for fname in file_set[:2]:\n                raw_file = fname.split(\"/\", 1)[0]\n                if raw_file not in RAW_FILES:\n                    raise RuntimeError(\"Unknown raw file %s found in path %s\" % (raw_file, fname))\n                raw_files.add(raw_file)\n    return sorted(raw_files)", "category": "Python"}, {"instruction": "def register(self, model_or_iterable, moderation_class):\n        \"\"\"\n        Register a model or a list of models for comment moderation,\n        using a particular moderation class.\n\n        Raise ``AlreadyModerated`` if any of the models are already\n        registered.\n\n        \"\"\"\n", "input": "", "output": "        if isinstance(model_or_iterable, ModelBase):\n            model_or_iterable = [model_or_iterable]\n        for model in model_or_iterable:\n            if model in self._registry:\n                raise AlreadyModerated(\n                    \"The model '%s' is already being moderated\" % model._meta.verbose_name\n                )\n            self._registry[model] = moderation_class(model)", "category": "Python"}, {"instruction": "def ParseObjs(self, objs, expression):\n    \"\"\"Parse one or more objects by testing if it has matching stat results.\n\n    Args:\n      objs: An iterable of objects that should be checked.\n      expression: A StatFilter expression, e.g.:\n        \"uid:>0 gid:=0 file_type:link\"\n\n    Yields:\n      matching objects.\n    \"\"\"\n", "input": "", "output": "    self.Validate(expression)\n    for obj in objs:\n      if not isinstance(obj, rdf_client_fs.StatEntry):\n        continue\n      # If all match conditions pass, yield the object.\n      for match in self.matchers:\n        if not match(obj):\n          break\n      else:\n        yield obj", "category": "Python"}, {"instruction": "def get_pending_reboot():\n    '''\n    Determine whether there is a reboot pending.\n\n    .. versionadded:: 2016.11.0\n\n    Returns:\n        bool: ``True`` if the system is pending reboot, otherwise ``False``\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' system.get_pending_reboot\n    '''\n", "input": "", "output": "\n    # Order the checks for reboot pending in most to least likely.\n    checks = (get_pending_update,\n              get_pending_file_rename,\n              get_pending_servermanager,\n              get_pending_component_servicing,\n              get_reboot_required_witnessed,\n              get_pending_computer_name,\n              get_pending_domain_join)\n\n    for check in checks:\n        if check():\n            return True\n\n    return False", "category": "Python"}, {"instruction": "def find_comments_by_me(self, access_token, page=1, count=20):\n        \"\"\"doc: http://open.youku.com/docs/doc?id=37\n        \"\"\"\n", "input": "", "output": "        url = 'https://openapi.youku.com/v2/comments/by_me.json'\n        data = {\n            'client_id': self.client_id,\n            'access_token': access_token,\n            'page': page,\n            'count': count\n        }\n        r = requests.post(url, data=data)\n        check_error(r)\n        return r.json()", "category": "Python"}, {"instruction": "def _x_reduced(self, x, y, n_sersic, r_eff, center_x, center_y):\n        \"\"\"\n        coordinate transform to normalized radius\n        :param x:\n        :param y:\n        :param center_x:\n        :param center_y:\n        :return:\n        \"\"\"\n", "input": "", "output": "        x_ = x - center_x\n        y_ = y - center_y\n        r = np.sqrt(x_**2 + y_**2)\n        if isinstance(r, int) or isinstance(r, float):\n            r = max(self._s, r)\n        else:\n            r[r < self._s] = self._s\n        x_reduced = (r/r_eff)**(1./n_sersic)\n        return x_reduced", "category": "Python"}, {"instruction": "def visit_extslice(self, node):\n        \"\"\"return an astroid.ExtSlice node as string\"\"\"\n", "input": "", "output": "        return \", \".join(dim.accept(self) for dim in node.dims)", "category": "Python"}, {"instruction": "def create(self, parties):\n        \"\"\"Create the barrier for the given number of parties.\n\n        Parameters:\n          parties(int): The number of parties to wait for.\n\n        Returns:\n          bool: Whether or not the new barrier was successfully created.\n        \"\"\"\n", "input": "", "output": "        assert parties > 0, \"parties must be a positive integer.\"\n        return self.backend.add(self.key, parties, self.ttl)", "category": "Python"}, {"instruction": "def command(self, cmd, expected_retcode=0):  # pylint: disable=invalid-name\n        # expected_retcode kwd argument is used in many test cases, we cannot rename it.\n        \"\"\"\n        Shortcut for sending a command to this node specifically.\n        :param cmd: Command to send\n        :param expected_retcode: Expected return code as int, default is 0\n        :return: CliResponse\n        \"\"\"\n", "input": "", "output": "        return self.bench.execute_command(self.endpoint_id, cmd, expected_retcode=expected_retcode)", "category": "Python"}, {"instruction": "def _wait_for_connection_state(self, state=Stateful.OPEN, rpc_timeout=30):\n        \"\"\"Wait for a Connection state.\n\n        :param int state: State that we expect\n\n        :raises AMQPConnectionError: Raises if we are unable to establish\n                                     a connection to RabbitMQ.\n\n        :return:\n        \"\"\"\n", "input": "", "output": "        start_time = time.time()\n        while self.current_state != state:\n            self.check_for_errors()\n            if time.time() - start_time > rpc_timeout:\n                raise AMQPConnectionError('Connection timed out')\n            sleep(IDLE_WAIT)", "category": "Python"}, {"instruction": "def L2S(lunarD, lunarM, lunarY, lunarLeap, tZ=7):\n    '''def L2S(lunarD, lunarM, lunarY, lunarLeap, tZ = 7): Convert a lunar date\n    to the corresponding solar date.'''\n", "input": "", "output": "    if (lunarM < 11):\n        a11 = getLunarMonth11(lunarY - 1, tZ)\n        b11 = getLunarMonth11(lunarY, tZ)\n    else:\n        a11 = getLunarMonth11(lunarY, tZ)\n        b11 = getLunarMonth11(lunarY + 1, tZ)\n    k = int(0.5 +\n            (a11 - 2415021.076998695) / 29.530588853)\n    off = lunarM - 11\n    if (off < 0):\n        off += 12\n    if (b11 - a11 > 365):\n        leapOff = getLeapMonthOffset(a11, tZ)\n        leapM = leapOff - 2\n        if (leapM < 0):\n            leapM += 12\n        if (lunarLeap != 0 and lunarM != leapM):\n            return [0, 0, 0]\n        elif (lunarLeap != 0 or off >= leapOff):\n            off += 1\n    monthStart = getNewMoonDay(k + off, tZ)\n    return jdToDate(monthStart + lunarD - 1)", "category": "Python"}, {"instruction": "async def i2c_config(self, read_delay_time=0):\n        \"\"\"\n        NOTE: THIS METHOD MUST BE CALLED BEFORE ANY I2C REQUEST IS MADE\n        This method initializes Firmata for I2c operations.\n\n        :param read_delay_time (in microseconds): an optional parameter,\n                                                  default is 0\n\n        :returns: No Return Value\n        \"\"\"\n", "input": "", "output": "        data = [read_delay_time & 0x7f, (read_delay_time >> 7) & 0x7f]\n        await self._send_sysex(PrivateConstants.I2C_CONFIG, data)", "category": "Python"}, {"instruction": "def update_compliance_all(self, information, timeout=-1):\n        \"\"\"\n        Returns SAS Logical Interconnects to a consistent state. The current SAS Logical Interconnect state is\n        compared to the associated SAS Logical Interconnect group.\n\n        Args:\n            information: Can be either the resource ID or URI.\n            timeout: Timeout in seconds. Wait for task completion by default. The timeout does not abort the operation\n                in OneView; it just stops waiting for its completion.\n\n        Returns:\n            dict: SAS Logical Interconnect.\n        \"\"\"\n", "input": "", "output": "\n        uri = self.URI + \"/compliance\"\n        result = self._helper.update(information, uri, timeout=timeout)\n\n        return result", "category": "Python"}, {"instruction": "def _pull_query_results(resultset):\n    '''\n    Parses a ResultSet returned from InfluxDB into a dictionary of results,\n    grouped by series names and optional JSON-encoded grouping tags.\n    '''\n", "input": "", "output": "    _results = collections.defaultdict(lambda: {})\n    for _header, _values in resultset.items():\n        _header, _group_tags = _header\n        if _group_tags:\n            _results[_header][salt.utils.json.dumps(_group_tags)] = [_value for _value in _values]\n        else:\n            _results[_header] = [_value for _value in _values]\n    return dict(sorted(_results.items()))", "category": "Python"}, {"instruction": "def get_known_periods(self):\n        \"\"\"\n            Get the list of periods the variable value is known for.\n        \"\"\"\n", "input": "", "output": "\n        return list(self._memory_storage.get_known_periods()) + list((\n            self._disk_storage.get_known_periods() if self._disk_storage else []))", "category": "Python"}, {"instruction": "def scale_and_shift_cmap(self, scale_pct, shift_pct):\n        \"\"\"Stretch and/or shrink the color map.\n        See :meth:`ginga.RGBMap.RGBMapper.scale_and_shift`.\n\n        \"\"\"\n", "input": "", "output": "        rgbmap = self.get_rgbmap()\n        rgbmap.scale_and_shift(scale_pct, shift_pct)", "category": "Python"}, {"instruction": "def _load(self, filename=None):\n        \"\"\"Read the AATSR rsr data\"\"\"\n", "input": "", "output": "        if not filename:\n            filename = self.aatsr_path\n\n        wb_ = open_workbook(filename)\n\n        for sheet in wb_.sheets():\n            ch_name = sheet.name.strip()\n            if ch_name == 'aatsr_' + self.bandname:\n\n                data = np.array([s.split() for s in\n                                 sheet.col_values(0,\n                                                  start_rowx=3, end_rowx=258)])\n                data = data.astype('f')\n                wvl = data[:, 0]\n                resp = data[:, 1]\n\n                self.rsr = {'wavelength': wvl, 'response': resp}", "category": "Python"}, {"instruction": "def samples_multidimensional_uniform(bounds,num_data):\n    '''\n    Generates a multidimensional grid uniformly distributed.\n    :param bounds: tuple defining the box constraints.\n    :num_data: number of data points to generate.\n\n    '''\n", "input": "", "output": "    dim = len(bounds)\n    Z_rand = np.zeros(shape=(num_data,dim))\n    for k in range(0,dim): Z_rand[:,k] = np.random.uniform(low=bounds[k][0],high=bounds[k][1],size=num_data)\n    return Z_rand", "category": "Python"}, {"instruction": "def rindex(self, sub, start=None, end=None):\n        \"\"\"Like .rfind() but raise ValueError when the substring is not found.\n\n        :param str sub: Substring to search.\n        :param int start: Beginning position.\n        :param int end: Stop comparison at this position.\n        \"\"\"\n", "input": "", "output": "        return self.value_no_colors.rindex(sub, start, end)", "category": "Python"}, {"instruction": "def get_color_zones(self, start_index, end_index=None, callb=None):\n        \"\"\"Convenience method to request the state of colour by zones from the device\n\n        This method will request the information from the device and request that callb\n        be executed when a response is received.\n\n            :param start_index: Index of the start of the zone of interest\n            :type start_index: int\n            :param end_index: Index of the end of the zone of interest. By default start_index+7\n            :type end_index: int\n            :param callb: Callable to be used when the response is received. If not set,\n                        self.resp_set_label will be used.\n            :type callb: callable\n            :returns: None\n            :rtype: None\n        \"\"\"\n", "input": "", "output": "        if end_index is None:\n            end_index = start_index + 7\n        args = {\n            \"start_index\": start_index,\n            \"end_index\": end_index,\n        }\n        self.req_with_resp(MultiZoneGetColorZones, MultiZoneStateMultiZone, payload=args, callb=callb)", "category": "Python"}, {"instruction": "def get_or_create_head(root):\n    \"\"\"Ensures that `root` contains a <head> element and returns it.\n    \"\"\"\n", "input": "", "output": "    head = _create_cssselector(\"head\")(root)\n    if not head:\n        head = etree.Element(\"head\")\n        body = _create_cssselector(\"body\")(root)[0]\n        body.getparent().insert(0, head)\n        return head\n    else:\n        return head[0]", "category": "Python"}, {"instruction": "def bibitem_as_plaintext(bibitem):\n    \"\"\"\n    Return a plaintext representation of a bibitem from the ``.bbl`` file.\n\n    .. note::\n\n        This plaintext representation can be super ugly, contain URLs and so \\\n        on.\n\n    .. note::\n\n        You need to have ``delatex`` installed system-wide, or to build it in \\\n                this repo, according to the ``README.md`` before using this \\\n                function.\n\n    :param bibitem: The text content of the bibitem.\n    :returns: A cleaned plaintext citation from the bibitem.\n    \"\"\"\n", "input": "", "output": "    try:\n        output = subprocess.check_output([\"delatex\",\n                                          \"-s\"],\n                                         input=bibitem.encode(\"utf-8\"))\n    except FileNotFoundError:\n        script_dir = os.path.dirname(os.path.abspath(__file__))\n        output = subprocess.check_output([\"%s/../external/opendetex/delatex\" %\n                                          (script_dir,),\n                                          \"-s\"],\n                                         input=bibitem.encode(\"utf-8\"))\n    output = output.decode(\"utf-8\")\n    output = tools.clean_whitespaces(output)\n    return output", "category": "Python"}, {"instruction": "def get_with_boundaries(self, name, boundaries):\n        r\"\"\"Get a color table from the registry with a corresponding norm.\n\n        Builds a `matplotlib.colors.BoundaryNorm` using `boundaries`.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n        boundaries : array_like\n            The list of boundaries for the norm\n\n        Returns\n        -------\n        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`\n            The boundary norm based on `boundaries`, and the color table itself.\n\n        \"\"\"\n", "input": "", "output": "        cmap = self.get_colortable(name)\n        return mcolors.BoundaryNorm(boundaries, cmap.N), cmap", "category": "Python"}, {"instruction": "def get_content_version(cls, abspath):\n        \"\"\"Returns a version string for the resource at the given path.\n\n        This class method may be overridden by subclasses.  The\n        default implementation is a hash of the file's contents.\n\n        .. versionadded:: 3.1\n        \"\"\"\n", "input": "", "output": "        data = cls.get_content(abspath)\n        hasher = hashlib.md5()\n\n        mtime_data = format(cls.get_content_modified_time(abspath), \"%Y-%m-%d %H:%M:%S\")\n\n        hasher.update(mtime_data.encode())\n\n        if isinstance(data, bytes):\n            hasher.update(data)\n        else:\n            for chunk in data:\n                hasher.update(chunk)\n        return hasher.hexdigest()", "category": "Python"}, {"instruction": "def virtualenv(self, virtualenv):\n        '''\n        Sets the virtual environment for the lambda package\n\n        If this is not set then package_dependencies will create a new one.\n\n        Takes a path to a virtualenv or a boolean if the virtualenv creation\n        should be skipped.\n        '''\n", "input": "", "output": "        # If a boolean is passed then set the internal _skip_virtualenv flag\n        if isinstance(virtualenv, bool):\n            self._skip_virtualenv = virtualenv\n        else:\n            self._virtualenv = virtualenv\n            if not os.path.isdir(self._virtualenv):\n                raise Exception(\"virtualenv %s not found\" % self._virtualenv)\n            LOG.info(\"Using existing virtualenv at %s\" % self._virtualenv)\n            # use supplied virtualenv path\n            self._pkg_venv = self._virtualenv\n            self._skip_virtualenv = True", "category": "Python"}, {"instruction": "def flatmap(source, func, *more_sources, task_limit=None):\n    \"\"\"Apply a given function that creates a sequence from the elements of one\n    or several asynchronous sequences, and generate the elements of the created\n    sequences as soon as they arrive.\n\n    The function is applied as described in `map`, and must return an\n    asynchronous sequence. The returned sequences are awaited concurrently,\n    although it's possible to limit the amount of running sequences using\n    the `task_limit` argument.\n\n    Errors raised in a source or output sequence are propagated.\n    \"\"\"\n", "input": "", "output": "    return flatten.raw(\n        combine.smap.raw(source, func, *more_sources), task_limit=task_limit)", "category": "Python"}, {"instruction": "def synced(func):\n    '''\n    Decorator for functions that should be called synchronously from another thread\n\n    :param func: function to call\n    '''\n", "input": "", "output": "\n    def wrapper(self, *args, **kwargs):\n        ", "category": "Python"}, {"instruction": "def abort(self):\n        \"\"\"Abort the processing_block.\"\"\"\n", "input": "", "output": "        LOG.debug('Aborting PB %s', self._id)\n        self.set_status('aborted')\n        pb_type = DB.get_hash_value(self.key, 'type')\n        key = '{}:active'.format(self._type)\n        DB.remove_from_list(key, self._id)\n        key = '{}:active:{}'.format(self._type, pb_type)\n        DB.remove_from_list(key, self._id)\n        key = '{}:aborted'.format(self._type)\n        DB.append_to_list(key, self._id)\n        key = '{}:aborted:{}'.format(self._type, pb_type)\n        DB.append_to_list(key, self._id)\n        self._mark_updated()", "category": "Python"}, {"instruction": "def _check_location_part(cls, val, regexp):\n        \"\"\"Deprecated. See CourseLocator._check_location_part\"\"\"\n", "input": "", "output": "        cls._deprecation_warning()\n        return CourseLocator._check_location_part(val, regexp)", "category": "Python"}, {"instruction": "def List(self):\n        \"\"\"\n        Lists the keys and values.\n        \"\"\"\n", "input": "", "output": "        print()\n        for key in list(self.keys()):\n            print(key,'=',self[key])\n        print()", "category": "Python"}, {"instruction": "def human_size_to_bytes(human_size):\n    '''\n    Convert human-readable units to bytes\n    '''\n", "input": "", "output": "    size_exp_map = {'K': 1, 'M': 2, 'G': 3, 'T': 4, 'P': 5}\n    human_size_str = six.text_type(human_size)\n    match = re.match(r'^(\\d+)([KMGTP])?$', human_size_str)\n    if not match:\n        raise ValueError(\n            'Size must be all digits, with an optional unit type '\n            '(K, M, G, T, or P)'\n        )\n    size_num = int(match.group(1))\n    unit_multiplier = 1024 ** size_exp_map.get(match.group(2), 0)\n    return size_num * unit_multiplier", "category": "Python"}, {"instruction": "def start(self, transaction_context=None):\n        \"\"\"Publish start message to all turrets\n        \"\"\"\n", "input": "", "output": "        transaction_context = transaction_context or {}\n        context_cmd = {'command': 'set_transaction_context',\n                       'msg': transaction_context}\n        self.publish(context_cmd)\n        self.publish(self.START)", "category": "Python"}, {"instruction": "def _archive_single_dir(archive):\n    \"\"\"\n    Check if all members of the archive are in a single top-level directory\n\n    :param archive:\n        An archive from _open_archive()\n\n    :return:\n        None if not a single top level directory in archive, otherwise a\n        unicode string of the top level directory name\n    \"\"\"\n", "input": "", "output": "\n    common_root = None\n    for info in _list_archive_members(archive):\n        fn = _info_name(info)\n        if fn in set(['.', '/']):\n            continue\n        sep = None\n        if '/' in fn:\n            sep = '/'\n        elif '\\\\' in fn:\n            sep = '\\\\'\n        if sep is None:\n            root_dir = fn\n        else:\n            root_dir, _ = fn.split(sep, 1)\n        if common_root is None:\n            common_root = root_dir\n        else:\n            if common_root != root_dir:\n                return None\n    return common_root", "category": "Python"}, {"instruction": "def _make_attr_tuple_class(cls_name, attr_names):\n    \"\"\"\n    Create a tuple subclass to hold `Attribute`s for an `attrs` class.\n\n    The subclass is a bare tuple with properties for names.\n\n    class MyClassAttributes(tuple):\n        __slots__ = ()\n        x = property(itemgetter(0))\n    \"\"\"\n", "input": "", "output": "    attr_class_name = \"{}Attributes\".format(cls_name)\n    attr_class_template = [\n        \"class {}(tuple):\".format(attr_class_name),\n        \"    __slots__ = ()\",\n    ]\n    if attr_names:\n        for i, attr_name in enumerate(attr_names):\n            attr_class_template.append(\n                _tuple_property_pat.format(index=i, attr_name=attr_name)\n            )\n    else:\n        attr_class_template.append(\"    pass\")\n    globs = {\"_attrs_itemgetter\": itemgetter, \"_attrs_property\": property}\n    eval(compile(\"\\n\".join(attr_class_template), \"\", \"exec\"), globs)\n\n    return globs[attr_class_name]", "category": "Python"}, {"instruction": "def read_file(fname, *args, **kwargs):\n    \"\"\"Read data from a file saved in the standard IAMC format\n    or a table with year/value columns\n    \"\"\"\n", "input": "", "output": "    if not isstr(fname):\n        raise ValueError('reading multiple files not supported, '\n                         'please use `pyam.IamDataFrame.append()`')\n    logger().info('Reading `{}`'.format(fname))\n    format_kwargs = {}\n    # extract kwargs that are intended for `format_data`\n    for c in [i for i in IAMC_IDX + ['year', 'time', 'value'] if i in kwargs]:\n        format_kwargs[c] = kwargs.pop(c)\n    return format_data(read_pandas(fname, *args, **kwargs), **format_kwargs)", "category": "Python"}, {"instruction": "def labels(self):\n        \"\"\"Retrieve or set labels assigned to this bucket.\n\n        See\n        https://cloud.google.com/storage/docs/json_api/v1/buckets#labels\n\n        .. note::\n\n           The getter for this property returns a dict which is a *copy*\n           of the bucket's labels.  Mutating that dict has no effect unless\n           you then re-assign the dict via the setter.  E.g.:\n\n           >>> labels = bucket.labels\n           >>> labels['new_key'] = 'some-label'\n           >>> del labels['old_key']\n           >>> bucket.labels = labels\n           >>> bucket.update()\n\n        :setter: Set labels for this bucket.\n        :getter: Gets the labels for this bucket.\n\n        :rtype: :class:`dict`\n        :returns: Name-value pairs (string->string) labelling the bucket.\n        \"\"\"\n", "input": "", "output": "        labels = self._properties.get(\"labels\")\n        if labels is None:\n            return {}\n        return copy.deepcopy(labels)", "category": "Python"}, {"instruction": "def pack_bitstring(length, is_float, value, signed):\n    \"\"\"\n    returns a value in bits\n    :param length: length of signal in bits\n    :param is_float: value is float\n    :param value: value to encode\n    :param signed: value is signed\n    :return:\n    \"\"\"\n", "input": "", "output": "    if is_float:\n        types = {\n            32: '>f',\n            64: '>d'\n        }\n\n        float_type = types[length]\n        x = bytearray(struct.pack(float_type, value))\n        bitstring = ''.join('{:08b}'.format(b) for b in x)\n    else:\n        b = '{:0{}b}'.format(int((2<<length )+ value), length)\n        bitstring = b[-length:]\n\n    return bitstring", "category": "Python"}, {"instruction": "def _PSat_T(T):\n    \"\"\"Define the saturated line, P=f(T)\n\n    Parameters\n    ----------\n    T : float\n        Temperature, [K]\n\n    Returns\n    -------\n    P : float\n        Pressure, [MPa]\n\n    Notes\n    ------\n    Raise :class:`NotImplementedError` if input isn't in limit:\n\n        * 273.15 \u2264 T \u2264 647.096\n\n    References\n    ----------\n    IAPWS, Revised Release on the IAPWS Industrial Formulation 1997 for the\n    Thermodynamic Properties of Water and Steam August 2007,\n    http://www.iapws.org/relguide/IF97-Rev.html, Eq 30\n\n    Examples\n    --------\n    >>> _PSat_T(500)\n    2.63889776\n    \"\"\"\n", "input": "", "output": "    # Check input parameters\n    if T < 273.15 or T > Tc:\n        raise NotImplementedError(\"Incoming out of bound\")\n\n    n = [0, 0.11670521452767E+04, -0.72421316703206E+06, -0.17073846940092E+02,\n         0.12020824702470E+05, -0.32325550322333E+07, 0.14915108613530E+02,\n         -0.48232657361591E+04, 0.40511340542057E+06, -0.23855557567849E+00,\n         0.65017534844798E+03]\n    tita = T+n[9]/(T-n[10])\n    A = tita**2+n[1]*tita+n[2]\n    B = n[3]*tita**2+n[4]*tita+n[5]\n    C = n[6]*tita**2+n[7]*tita+n[8]\n    return (2*C/(-B+(B**2-4*A*C)**0.5))**4", "category": "Python"}, {"instruction": "def order_by(self, *args):\n        \"\"\"\n        Applies query ordering.\n\n        Args:\n            **args: Order by fields names.\n            Defaults to ascending, prepend with hypen (-) for desecending ordering.\n\n        Returns:\n            Self. Queryset object.\n\n        Examples:\n            >>> Person.objects.order_by('-name', 'join_date')\n        \"\"\"\n", "input": "", "output": "        clone = copy.deepcopy(self)\n        clone.adapter.ordered = True\n        if args:\n            clone.adapter.order_by(*args)\n        return clone", "category": "Python"}, {"instruction": "def predict(self, X, with_noise=True):\n        \"\"\"\n        Predictions with the model. Returns posterior means and standard deviations at X. Note that this is different in GPy where the variances are given.\n\n        Parameters:\n            X (np.ndarray) - points to run the prediction for.\n            with_noise (bool) - whether to add noise to the prediction. Default is True.\n        \"\"\"\n", "input": "", "output": "        m, v = self._predict(X, False, with_noise)\n        # We can take the square root because v is just a diagonal matrix of variances\n        return m, np.sqrt(v)", "category": "Python"}, {"instruction": "def command_line(self):\n        '''Return a string to open read files with'''\n        file_format=self.guess_sequence_input_file_format(self.read_file)\n        logging.debug(\"Detected file format %s\" % file_format)\n        if file_format == self.FORMAT_FASTA:\n            cmd=\"\"\"cat %s\"\"\"\n", "input": "", "output": "        elif file_format == self.FORMAT_FASTQ_GZ:\n            cmd=", "category": "Python"}, {"instruction": "def get_result(self, decorated_function, *args, **kwargs):\n\t\t\"\"\" :meth:`WCacheStorage.get_result` method implementation\n\t\t\"\"\"\n", "input": "", "output": "\t\ttry:\n\t\t\treturn self._storage[decorated_function]\n\t\texcept KeyError as e:\n\t\t\traise WCacheStorage.CacheMissedException('No cache record found') from e", "category": "Python"}, {"instruction": "def delete_project(self, project_name):\r\n        \"\"\" delete project\r\n        Unsuccessful opertaion will cause an LogException.\r\n\r\n        :type project_name: string\r\n        :param project_name: the Project name \r\n\r\n        :return: DeleteProjectResponse \r\n\r\n        :raise: LogException\r\n        \"\"\"\n", "input": "", "output": "        headers = {}\r\n        params = {}\r\n        resource = \"/\"\r\n\r\n        (resp, header) = self._send(\"DELETE\", project_name, None, resource, params, headers)\r\n        return DeleteProjectResponse(header, resp)", "category": "Python"}, {"instruction": "def rewind(self, count):\n        \"\"\"Rewind index.\"\"\"\n", "input": "", "output": "\n        if count > self._index:  # pragma: no cover\n            raise ValueError(\"Can't rewind past beginning!\")\n\n        self._index -= count", "category": "Python"}, {"instruction": "def _get_master_address(self, instance):\n        \"\"\"\n        Get the master address from the instance configuration\n        \"\"\"\n", "input": "", "output": "\n        master_address = instance.get(MASTER_ADDRESS)\n        if master_address is None:\n            master_address = instance.get(DEPRECATED_MASTER_ADDRESS)\n\n            if master_address:\n                self.log.warning(\n                    'The use of `%s` is deprecated. Please use `%s` instead.'\n                    % (DEPRECATED_MASTER_ADDRESS, MASTER_ADDRESS)\n                )\n            else:\n                raise Exception('URL for `%s` must be specified in the instance configuration' % MASTER_ADDRESS)\n\n        return master_address", "category": "Python"}, {"instruction": "def extract_attributes(element, prefix_key_char='@', dict_constructor=dict):\n    \"\"\"\n    Given an XML node, extract a dictionary of attribute key-value pairs.\n    Optional arguments:\n\n    - prefix_key_char: if a character is given, the attributes keys\n      in the resulting dictionary are prefixed with that character.\n\n    - dict_constructor: the class used to create the resulting dictionary.\n      Default is 'dict', but in DINGO, also DingoObjDict may be used.\n    \"\"\"\n", "input": "", "output": "    result = dict_constructor()\n    if element.properties:\n        for prop in element.properties:\n            if not prop:\n                break\n            if prop.type == 'attribute':\n                try:\n                    # First try with namespace. If no namespace exists,\n                    # an exception is raised\n                    result[\"%s%s:%s\" % (prefix_key_char, prop.ns().name, prop.name)] = prop.content\n                except:\n                    result[\"%s%s\" % (prefix_key_char, prop.name)] = prop.content\n    return result", "category": "Python"}, {"instruction": "def search_user_group_entities(self, **kwargs):  # noqa: E501\n        \"\"\"Search over a customer's user groups  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.search_user_group_entities(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param SortableSearchRequest body:\n        :return: ResponseContainerPagedUserGroup\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.search_user_group_entities_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.search_user_group_entities_with_http_info(**kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def get_scanner (type, prop_set):\n    \"\"\" Returns a scanner instance appropriate to 'type' and 'property_set'.\n    \"\"\"\n", "input": "", "output": "    if __debug__:\n        from .property_set import PropertySet\n        assert isinstance(type, basestring)\n        assert isinstance(prop_set, PropertySet)\n    if registered (type):\n        scanner_type = __types [type]['scanner']\n        if scanner_type:\n            return scanner.get (scanner_type, prop_set.raw ())\n            pass\n\n    return None", "category": "Python"}, {"instruction": "def is_locator(self, path, relative=False):\n        \"\"\"\n        Returns True if path refer to a locator.\n\n        Depending the storage, locator may be a bucket or container name,\n        a hostname, ...\n\n        args:\n            path (str): path or URL.\n            relative (bool): Path is relative to current root.\n\n        Returns:\n            bool: True if locator.\n        \"\"\"\n", "input": "", "output": "        if not relative:\n            path = self.relpath(path)\n        # Bucket is the main directory\n        return path and '/' not in path.rstrip('/')", "category": "Python"}, {"instruction": "def regions():\n    \"\"\"\n    Get all available regions for the CloudWatch service.\n\n    :rtype: list\n    :return: A list of :class:`boto.RegionInfo` instances\n    \"\"\"\n", "input": "", "output": "    regions = []\n    for region_name in RegionData:\n        region = RegionInfo(name=region_name,\n                            endpoint=RegionData[region_name],\n                            connection_cls=CloudWatchConnection)\n        regions.append(region)\n    return regions", "category": "Python"}, {"instruction": "def _filter_to_info(in_file, data):\n    \"\"\"Move DKFZ filter information into INFO field.\n    \"\"\"\n", "input": "", "output": "    header = (", "category": "Python"}, {"instruction": "def threadid(self):\n        \"\"\"\n        Current thread ident. If current thread is main thread then it returns ``None``.\n\n        :type: int or None\n        \"\"\"\n", "input": "", "output": "        current = self.thread.ident\n        main = get_main_thread()\n        if main is None:\n            return current\n        else:\n            return current if current != main.ident else None", "category": "Python"}, {"instruction": "def haab_monthcalendar(baktun=None, katun=None, tun=None, uinal=None, kin=None, jdc=None):\n    '''For a given long count, return a calender of the current haab month, divided into tzolkin \"weeks\"'''\n", "input": "", "output": "    if not jdc:\n        jdc = to_jd(baktun, katun, tun, uinal, kin)\n\n    haab_number, haab_month = to_haab(jdc)\n    first_j = jdc - haab_number + 1\n\n    tzolkin_start_number, tzolkin_start_name = to_tzolkin(first_j)\n\n    gen_longcount = longcount_generator(*from_jd(first_j))\n    gen_tzolkin = tzolkin_generator(tzolkin_start_number, tzolkin_start_name)\n\n    # 13 day long tzolkin 'weeks'\n    lpad = tzolkin_start_number - 1\n    rpad = 13 - (tzolkin_start_number + 19 % 13)\n\n    monlen = month_length(haab_month)\n\n    days = [None] * lpad + list(range(1, monlen + 1)) + rpad * [None]\n\n    def g(x, generate):\n        if x is None:\n            return None\n        return next(generate)\n\n    return [[(k, g(k, gen_tzolkin), g(k, gen_longcount)) for k in days[i:i + 13]] for i in range(0, len(days), 13)]", "category": "Python"}, {"instruction": "def get_subnets(self, vlan):\n        \"\"\"Return the subnets for the `vlan`.\"\"\"\n", "input": "", "output": "        return vlan._origin.Subnets([\n            subnet\n            for subnet in self.subnets\n            if subnet.vlan.id == vlan.id\n        ])", "category": "Python"}, {"instruction": "def _getImpliedEdges(roots):\n        \"\"\"\n        Gets the set of implied edges. See Job.checkJobGraphAcylic\n        \"\"\"\n", "input": "", "output": "        #Get nodes in job graph\n        nodes = set()\n        for root in roots:\n            root._dfs(nodes)\n\n        ##For each follow-on edge calculate the extra implied edges\n        #Adjacency list of implied edges, i.e. map of jobs to lists of jobs\n        #connected by an implied edge\n        extraEdges = dict([(n, []) for n in nodes])\n        for job in nodes:\n            if len(job._followOns) > 0:\n                #Get set of jobs connected by a directed path to job, starting\n                #with a child edge\n                reacheable = set()\n                for child in job._children:\n                    child._dfs(reacheable)\n                #Now add extra edges\n                for descendant in reacheable:\n                    extraEdges[descendant] += job._followOns[:]\n        return extraEdges", "category": "Python"}, {"instruction": "def fake_run(self):\n        '''Doesn't actually run cd-hit. Instead, puts each input sequence into its own cluster. So it's as if cdhit was run, but didn't cluster anything'''\n", "input": "", "output": "        clusters = {}\n        used_names = set()\n        seq_reader = pyfastaq.sequences.file_reader(self.infile)\n\n        for seq in seq_reader:\n            if seq.id in used_names:\n                raise Error('Sequence name \"' + seq.id + '\" not unique. Cannot continue')\n\n            clusters[str(len(clusters) + self.min_cluster_number)] = {seq.id}\n            used_names.add(seq.id)\n\n        return clusters", "category": "Python"}, {"instruction": "def _handle_msg(self, msg):\n        \"\"\"Called when a msg is received from the front-end\"\"\"\n", "input": "", "output": "        data = msg['content']['data']\n        method = data['method']\n\n        if method == 'update':\n            if 'state' in data:\n                state = data['state']\n                if 'buffer_paths' in data:\n                    _put_buffers(state, data['buffer_paths'], msg['buffers'])\n                self.set_state(state)\n\n        # Handle a state request.\n        elif method == 'request_state':\n            self.send_state()\n\n        # Handle a custom msg from the front-end.\n        elif method == 'custom':\n            if 'content' in data:\n                self._handle_custom_msg(data['content'], msg['buffers'])\n\n        # Catch remainder.\n        else:\n            self.log.error('Unknown front-end to back-end widget msg with method \"%s\"' % method)", "category": "Python"}, {"instruction": "def add_arguments(self, parser):\n        '''Add generic command-line arguments to a top-level argparse parser.\n\n        After running this, the results from ``argparse.parse_args()``\n        can be passed to :meth:`main`.\n\n        '''\n", "input": "", "output": "        commands = set(name[3:] for name in dir(self) if name.startswith('do_'))\n        parser.add_argument('action', help='action to run', nargs='?',\n                            choices=list(commands))\n        parser.add_argument('arguments', help='arguments specific to ACTION',\n                            nargs=argparse.REMAINDER)", "category": "Python"}, {"instruction": "def graph_val_dump(self):\n        \"\"\"Yield the entire contents of the graph_val table.\"\"\"\n", "input": "", "output": "        self._flush_graph_val()\n        for (graph, key, branch, turn, tick, value) in self.sql('graph_val_dump'):\n            yield (\n                self.unpack(graph),\n                self.unpack(key),\n                branch,\n                turn,\n                tick,\n                self.unpack(value)\n            )", "category": "Python"}, {"instruction": "def get_debug_info():\n        \"\"\"Return a list of lines with backend info.\n        \"\"\"\n", "input": "", "output": "        from . import __version__\n        d = OrderedDict()\n        d['Version'] = '%s' % __version__\n\n        for key, val in PyVisaLibrary.get_session_classes().items():\n            key_name = '%s %s' % (key[0].name.upper(), key[1])\n            try:\n                d[key_name] = getattr(val, 'session_issue').split('\\n')\n            except AttributeError:\n                d[key_name] = 'Available ' + val.get_low_level_info()\n\n        return d", "category": "Python"}, {"instruction": "def add(self, name, description):\n        \"\"\"Inserts a new Filter and returns its identifier.\n\n        :param name: Name. String with a maximum of 100 characters and respect [a-zA-Z\\_-]\n        :param description: Description. String with a maximum of 200 characters and respect [a-zA-Z\\_-]\n\n        :return: Following dictionary:\n\n        ::\n\n            {'filter': {'id': < id >}}\n\n        :raise InvalidParameterError: The value of name or description is invalid.\n        :raise FilterDuplicateError: A filter named by name already exists.\n        :raise DataBaseError: Networkapi failed to access the database.\n        :raise XMLError: Networkapi failed to generate the XML response.\n        \"\"\"\n", "input": "", "output": "        filter_map = dict()\n        filter_map['name'] = name\n        filter_map['description'] = description\n\n        code, xml = self.submit({'filter': filter_map}, 'POST', 'filter/')\n\n        return self.response(code, xml)", "category": "Python"}, {"instruction": "def gettree(self, key, create = False):\n        \"\"\"\n        Get a subtree node from the key (path relative to this node)\n        \"\"\"\n", "input": "", "output": "        tree, _ = self._getsubitem(key + '.tmp', create)\n        return tree", "category": "Python"}, {"instruction": "def tag_verb_chains(self):\n        \"\"\"Create ``verb_chains`` layer.\n           Depends on ``clauses`` layer.\n        \"\"\"\n", "input": "", "output": "        if not self.is_tagged(CLAUSES):\n            self.tag_clauses()\n        if self.__verbchain_detector is None:\n            self.__verbchain_detector = load_default_verbchain_detector()\n        sentences = self.divide()\n        verbchains = []\n        for sentence in sentences:\n            chains = self.__verbchain_detector.detectVerbChainsFromSent( sentence )\n            for chain in chains:\n                # 1) Get spans for all words of the phrase\n                word_spans = [ ( sentence[idx][START], sentence[idx][END] ) \\\n                                 for idx in sorted( chain[PHRASE] ) ]\n                # 2) Assign to the chain\n                chain[START] = [ span[0] for span in word_spans ]\n                chain[END]   = [ span[1] for span in word_spans ]\n            verbchains.extend(chains)\n        self[VERB_CHAINS] = verbchains\n        return self", "category": "Python"}, {"instruction": "def initialize_ui(self):\n        \"\"\"\n        Initializes the Component ui.\n\n        :return: Method success.\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        LOGGER.debug(\"> Initializing '{0}' Component ui.\".format(self.__class__.__name__))\n\n        self.__Port_spinBox_set_ui()\n        self.__Autostart_TCP_Server_checkBox_set_ui()\n\n        # Signals / Slots.\n        self.Port_spinBox.valueChanged.connect(self.__Port_spinBox__valueChanged)\n        self.Autostart_TCP_Server_checkBox.stateChanged.connect(\n            self.__Autostart_TCP_Server_checkBox__stateChanged)\n        self.Start_TCP_Server_pushButton.clicked.connect(self.__Start_TCP_Server_pushButton__clicked)\n        self.Stop_TCP_Server_pushButton.clicked.connect(self.__Stop_TCP_Server_pushButton__clicked)\n\n        self.initialized_ui = True\n        return True", "category": "Python"}, {"instruction": "def build_package_data(self):\n        \"\"\"Copy data files into build directory\"\"\"\n", "input": "", "output": "        for package, src_dir, build_dir, filenames in self.data_files:\n            for filename in filenames:\n                target = os.path.join(build_dir, filename)\n                self.mkpath(os.path.dirname(target))\n                srcfile = os.path.join(src_dir, filename)\n                outf, copied = self.copy_file(srcfile, target)\n                srcfile = os.path.abspath(srcfile)\n                if (copied and\n                        srcfile in self.distribution.convert_2to3_doctests):\n                    self.__doctests_2to3.append(outf)", "category": "Python"}, {"instruction": "def ProcessContent(self, strip_expansion=False):\n    \"\"\"Processes the file contents.\"\"\"\n", "input": "", "output": "    self._ParseFile()\n    if strip_expansion:\n      # Without a collection the expansions become blank, removing them.\n      collection = None\n    else:\n      collection = MacroCollection()\n    for section in self._sections:\n      section.BindMacroCollection(collection)\n    result = ''\n    for section in self._sections:\n      result += section.text\n    self._processed_content = result", "category": "Python"}, {"instruction": "def convolved_1d(iterable, kernel_size=1, stride=1, padding=0, default_value=None):\n    \"\"\"1D Iterable to get every convolution window per loop iteration.\n\n    For more information, refer to:\n    - https://github.com/guillaume-chevalier/python-conv-lib/blob/master/conv/conv.py\n    - https://github.com/guillaume-chevalier/python-conv-lib\n    - MIT License, Copyright (c) 2018 Guillaume Chevalier\n    \"\"\"\n", "input": "", "output": "    return convolved(iterable, kernel_size, stride, padding, default_value)", "category": "Python"}, {"instruction": "def colorize(text, color=\"BLUE\", close=True):\n    \"\"\" Colorizes text for terminal outputs\n\n        @text: #str to colorize\n        @color: #str color from :mod:colors\n        @close: #bool whether or not to reset the color\n\n        -> #str colorized @text\n        ..\n            from vital.debug import colorize\n\n            colorize(\"Hello world\", \"blue\")\n            # -> '\\x1b[0;34mHello world\\x1b[1;m'\n\n            colorize(\"Hello world\", \"blue\", close=False)\n            # -> '\\x1b[0;34mHello world'\n        ..\n    \"\"\"\n", "input": "", "output": "    if color:\n        color = getattr(colors, color.upper())\n        return color + uncolorize(str(text)) + (colors.RESET if close else \"\")\n    return text", "category": "Python"}, {"instruction": "def dist(self, x1, x2):\n        \"\"\"Return the weighted distance between ``x1`` and ``x2``.\n\n        Parameters\n        ----------\n        x1, x2 : `NumpyTensor`\n            Tensors whose mutual distance is calculated.\n\n        Returns\n        -------\n        dist : float\n            The distance between the tensors.\n        \"\"\"\n", "input": "", "output": "        if self.exponent == 2.0:\n            return float(np.sqrt(self.const) * _norm_default(x1 - x2))\n        elif self.exponent == float('inf'):\n            return float(self.const * _pnorm_default(x1 - x2, self.exponent))\n        else:\n            return float((self.const ** (1 / self.exponent) *\n                          _pnorm_default(x1 - x2, self.exponent)))", "category": "Python"}, {"instruction": "def positions_to_contigs(positions):\n    \"\"\"Label contigs according to relative positions\n\n    Given a list of positions, return an ordered list\n    of labels reflecting where the positions array started\n    over (and presumably a new contig began).\n\n    Parameters\n    ----------\n    positions : list or array_like\n        A piece-wise ordered list of integers representing\n        positions\n\n    Returns\n    -------\n    contig_labels : numpy.ndarray\n        The list of contig labels\n\n    \"\"\"\n", "input": "", "output": "\n    contig_labels = np.zeros_like(positions)\n\n    contig_index = 0\n    for i, p in enumerate(positions):\n        if p == 0:\n            contig_index += 1\n        contig_labels[i] = contig_index\n\n    return contig_labels", "category": "Python"}, {"instruction": "def calc_tmean_v1(self):\n    \"\"\"Calculate the areal mean temperature of the subbasin.\n\n    Required derived parameter:\n      |RelZoneArea|\n\n    Required flux sequence:\n      |TC|\n\n    Calculated flux sequences:\n      |TMean|\n\n    Examples:\n\n        Prepare two zones, the first one being twice as large\n        as the second one:\n\n        >>> from hydpy.models.hland import *\n        >>> parameterstep('1d')\n        >>> nmbzones(2)\n        >>> derived.relzonearea(2.0/3.0, 1.0/3.0)\n\n        With temperature values of 5\u00b0C and 8\u00b0C  of the respective zones,\n        the mean temperature is 6\u00b0C:\n\n        >>> fluxes.tc = 5.0, 8.0\n        >>> model.calc_tmean_v1()\n        >>> fluxes.tmean\n        tmean(6.0)\n    \"\"\"\n", "input": "", "output": "    con = self.parameters.control.fastaccess\n    der = self.parameters.derived.fastaccess\n    flu = self.sequences.fluxes.fastaccess\n    flu.tmean = 0.\n    for k in range(con.nmbzones):\n        flu.tmean += der.relzonearea[k]*flu.tc[k]", "category": "Python"}, {"instruction": "def cmd_speed(self, args):\n        '''enable/disable speed report'''\n", "input": "", "output": "        self.settings.set('speedreporting', not self.settings.speedreporting)\n        if self.settings.speedreporting:\n            self.console.writeln(\"Speed reporting enabled\", bg='yellow')\n        else:\n            self.console.writeln(\"Speed reporting disabled\", bg='yellow')", "category": "Python"}, {"instruction": "def available(self):\n        \"\"\" True if any of the supported modules from ``packages`` is available for use.\n\n        :return: True if any modules from ``packages`` exist\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        for module_name in self.packages:\n            if importlib.util.find_spec(module_name):\n                return True\n        return False", "category": "Python"}, {"instruction": "def do_response(self, response_args=None, request=None, **kwargs):\n        \"\"\"\n        **Placeholder for the time being**\n\n        :param response_args:\n        :param request:\n        :param kwargs: request arguments\n        :return: Response information\n        \"\"\"\n", "input": "", "output": "\n        links = [Link(href=h, rel=OIC_ISSUER) for h in kwargs['hrefs']]\n\n        _response = JRD(subject=kwargs['subject'], links=links)\n\n        info = {\n            'response': _response.to_json(),\n            'http_headers': [('Content-type', 'application/json')]\n        }\n\n        return info", "category": "Python"}, {"instruction": "def synonyms(self):\n        \"\"\"A ranked list of all the names associated with this Compound.\n\n        Requires an extra request. Result is cached.\n        \"\"\"\n", "input": "", "output": "        if self.cid:\n            results = get_json(self.cid, operation='synonyms')\n            return results['InformationList']['Information'][0]['Synonym'] if results else []", "category": "Python"}, {"instruction": "def _import_class(self, module2cls):\n        \"\"\"Import class by module dot split string\"\"\"\n", "input": "", "output": "        d = module2cls.rfind(\".\")\n        classname = module2cls[d + 1: len(module2cls)]\n        m = __import__(module2cls[0:d], globals(), locals(), [classname])\n        return getattr(m, classname)", "category": "Python"}, {"instruction": "def get_axes(x, y):\n    \"\"\"\n    computes the axis x and y of a given 2d grid\n    :param x:\n    :param y:\n    :return:\n    \"\"\"\n", "input": "", "output": "    n=int(np.sqrt(len(x)))\n    if n**2 != len(x):\n        raise ValueError(\"lenght of input array given as %s is not square of integer number!\" % (len(x)))\n    x_image = x.reshape(n,n)\n    y_image = y.reshape(n,n)\n    x_axes = x_image[0,:]\n    y_axes = y_image[:,0]\n    return x_axes, y_axes", "category": "Python"}, {"instruction": "def get_direct_band_gap(self):\n        \"\"\"\n        Returns the direct band gap.\n\n        Returns:\n             the value of the direct band gap\n        \"\"\"\n", "input": "", "output": "        if self.is_metal():\n            return 0.0\n        dg = self.get_direct_band_gap_dict()\n        return min(v['value'] for v in dg.values())", "category": "Python"}, {"instruction": "def tanh(x):\n    \"\"\"\n    Hyperbolic tangent\n    \"\"\"\n", "input": "", "output": "    if isinstance(x, UncertainFunction):\n        mcpts = np.tanh(x._mcpts)\n        return UncertainFunction(mcpts)\n    else:\n        return np.tanh(x)", "category": "Python"}, {"instruction": "def downsample_trace(mesh, tol=1.0):\n    \"\"\"\n    Downsamples the upper edge of a fault within a rectangular mesh, retaining\n    node points only if changes in direction on the order of tol are found\n\n    :returns:\n        Downsampled edge as a numpy array of [long, lat, depth]\n    \"\"\"\n", "input": "", "output": "    idx = _find_turning_points(mesh, tol)\n    if mesh.depths is not None:\n        return numpy.column_stack([mesh.lons[0, idx],\n                                   mesh.lats[0, idx],\n                                   mesh.depths[0, idx]])\n    else:\n        return numpy.column_stack([mesh.lons[0, idx], mesh.lats[0, idx]])", "category": "Python"}, {"instruction": "def is_valid_intensity_measure_levels(self):\n        \"\"\"\n        In order to compute hazard curves, `intensity_measure_types_and_levels`\n        must be set or extracted from the risk models.\n        \"\"\"\n", "input": "", "output": "        invalid = self.no_imls() and not self.risk_files and (\n            self.hazard_curves_from_gmfs or self.calculation_mode in\n            ('classical', 'disaggregation'))\n        return not invalid", "category": "Python"}, {"instruction": "def protocol_names(self):\n    \"\"\"Returns all registered protocol names\"\"\"\n", "input": "", "output": "\n    l = self.protocols()\n    retval = [str(k.name) for k in l]\n    return retval", "category": "Python"}, {"instruction": "def _add_device(self, scs_id, ha_id, name):\n        \"\"\" Add device to the list of known ones \"\"\"\n", "input": "", "output": "        if scs_id in self._devices:\n            return\n\n        self._devices[scs_id] = {\n            'name': name,\n            'ha_id': ha_id\n        }", "category": "Python"}, {"instruction": "def get_exif_data(self, image):\n        \"\"\"Returns a dictionary from the exif data of an PIL Image item. Also converts the GPS Tags\"\"\"\n", "input": "", "output": "        exif_data = {}\n        info = image._getexif()\n        if info:\n            for tag, value in info.items():\n                decoded = TAGS.get(tag, tag)\n                if decoded == \"GPSInfo\":\n                    gps_data = {}\n                    for t in value:\n                        sub_decoded = GPSTAGS.get(t, t)\n                        gps_data[sub_decoded] = value[t]\n\n                    exif_data[decoded] = gps_data\n                else:\n                    exif_data[decoded] = value\n\n        return exif_data", "category": "Python"}, {"instruction": "def proper_repr(value: Any) -> str:\n    \"\"\"Overrides sympy and numpy returning repr strings that don't parse.\"\"\"\n", "input": "", "output": "\n    if isinstance(value, sympy.Basic):\n        result = sympy.srepr(value)\n\n        # HACK: work around https://github.com/sympy/sympy/issues/16074\n        # (only handles a few cases)\n        fixed_tokens = [\n            'Symbol', 'pi', 'Mul', 'Add', 'Mod', 'Integer', 'Float', 'Rational'\n        ]\n        for token in fixed_tokens:\n            result = result.replace(token, 'sympy.' + token)\n\n        return result\n\n    if isinstance(value, np.ndarray):\n        return 'np.array({!r})'.format(value.tolist())\n    return repr(value)", "category": "Python"}, {"instruction": "def _find_value(ret_dict, key, path=None):\n    '''\n    PRIVATE METHOD\n    Traverses a dictionary of dictionaries/lists to find key\n    and return the value stored.\n    TODO:// this method doesn't really work very well, and it's not really\n            very useful in its current state. The purpose for this method is\n            to simplify parsing the JSON output so you can just pass the key\n            you want to find and have it return the value.\n    ret : dict<str,obj>\n        The dictionary to search through. Typically this will be a dict\n        returned from solr.\n    key : str\n        The key (str) to find in the dictionary\n\n    Return: list<dict<str,obj>>::\n\n        [{path:path, value:value}]\n    '''\n", "input": "", "output": "    if path is None:\n        path = key\n    else:\n        path = \"{0}:{1}\".format(path, key)\n\n    ret = []\n    for ikey, val in six.iteritems(ret_dict):\n        if ikey == key:\n            ret.append({path: val})\n        if isinstance(val, list):\n            for item in val:\n                if isinstance(item, dict):\n                    ret = ret + _find_value(item, key, path)\n        if isinstance(val, dict):\n            ret = ret + _find_value(val, key, path)\n    return ret", "category": "Python"}, {"instruction": "def request_info(self, request_id):\n        \"\"\"\n        Get information of a single pull request.\n        :param request_id: the id of the request\n        :return:\n        \"\"\"\n", "input": "", "output": "        request_url = \"{}pull-request/{}\".format(self.create_basic_url(),\n                                                 request_id)\n\n        return_value = self._call_api(request_url)\n        return return_value", "category": "Python"}, {"instruction": "def returns(*checkers_args):\n    \"\"\" Create a decorator for validating function return values.\n\n    Parameters\n    ----------\n    checkers_args: positional arguments\n        A single functions to apply to the output of the decorated function. If a tuple is returned \n        by the decorated function, multiple function can be listed and are assumed to match by \n        possition to the elements in the returned tuple.   \n\n    Examples\n    --------\n    @returns(df_checker)\n    def do_something_with_df(df, args*, kw**):\n        print(df.head())\n        return df\n\n    @returns(df_checker1, df_checker2)\n    def do_something_with_dfs(df1, df2, args*, kw**):\n        # Do somethign with both dfs\n        return (df1, df2)\n    \"\"\"\n", "input": "", "output": "    \n    @decorator\n    def run_checkers(func, *args, **kwargs):\n        ret = func(*args, **kwargs)\n        \n        if type(ret) != tuple:\n            ret = (ret, )\n        assert len(ret) == len(checkers_args)\n        \n        if checkers_args:\n            for idx, checker_function in enumerate(checkers_args):\n                if callable(checker_function):\n                    result = checker_function(ret[idx])\n        return ret\n    return run_checkers", "category": "Python"}, {"instruction": "def create_data_types(self):\n        \"\"\"Map of standard playbook variable types to create method.\"\"\"\n", "input": "", "output": "        return {\n            'Binary': self.create_binary,\n            'BinaryArray': self.create_binary_array,\n            'KeyValue': self.create_key_value,\n            'KeyValueArray': self.create_key_value_array,\n            'String': self.create_string,\n            'StringArray': self.create_string_array,\n            'TCEntity': self.create_tc_entity,\n            'TCEntityArray': self.create_tc_entity_array,\n        }", "category": "Python"}, {"instruction": "def _fetch_url_data(self, url, username, password, verify, custom_headers):\n        ''' Hit a given http url and return the stats lines '''\n", "input": "", "output": "        # Try to fetch data from the stats URL\n\n        auth = (username, password)\n        url = \"%s%s\" % (url, STATS_URL)\n        custom_headers.update(headers(self.agentConfig))\n\n        self.log.debug(\"Fetching haproxy stats from url: %s\" % url)\n\n        response = requests.get(\n            url, auth=auth, headers=custom_headers, verify=verify, timeout=self.default_integration_http_timeout\n        )\n        response.raise_for_status()\n\n        # it only needs additional decoding in py3, so skip it if it's py2\n        if PY2:\n            return response.content.splitlines()\n        else:\n            content = response.content\n\n            # If the content is a string, it can't be decoded again\n            # But if it's bytes, it can be decoded.\n            # So, check if it has the decode method\n            decode_fn = getattr(content, \"decode\", None)\n            if callable(decode_fn):\n                content = content.decode('utf-8')\n\n            return content.splitlines()", "category": "Python"}, {"instruction": "def add_words(self):\n        \"\"\"The data block must fill the entire data capacity of the QR code.\n        If we fall short, then we must add bytes to the end of the encoded\n        data field. The value of these bytes are specified in the standard.\n        \"\"\"\n", "input": "", "output": "\n        data_blocks = len(self.buffer.getvalue()) // 8\n        total_blocks = tables.data_capacity[self.version][self.error][0] // 8\n        needed_blocks = total_blocks - data_blocks\n\n        if needed_blocks == 0:\n            return None\n\n        #This will return item1, item2, item1, item2, etc.\n        block = itertools.cycle(['11101100', '00010001'])\n\n        #Create a string of the needed blocks\n        return ''.join([next(block) for x in range(needed_blocks)])", "category": "Python"}, {"instruction": "def _check_last_arg_pattern(self, current_arg_pattern, last_arg_pattern):\n        \"\"\"Given a \"current\" arg pattern (that was used to match the last\n        actual argument of an expression), and another (\"last\") argument\n        pattern, raise a ValueError, unless the \"last\" argument pattern is a\n        \"zero or more\" wildcard. In that case, return a dict that maps the\n        wildcard name to an empty list\n        \"\"\"\n", "input": "", "output": "        try:\n            if last_arg_pattern.mode == self.single:\n                raise ValueError(\"insufficient number of arguments\")\n            elif last_arg_pattern.mode == self.zero_or_more:\n                if last_arg_pattern.wc_name is not None:\n                    if last_arg_pattern != current_arg_pattern:\n                        # we have to record an empty match\n                        return {last_arg_pattern.wc_name: []}\n            elif last_arg_pattern.mode == self.one_or_more:\n                if last_arg_pattern != current_arg_pattern:\n                    raise ValueError(\"insufficient number of arguments\")\n        except AttributeError:\n            raise ValueError(\"insufficient number of arguments\")\n        return {}", "category": "Python"}, {"instruction": "def check_cv(cv, X=None, y=None, classifier=False):\n    \"\"\"Input checker utility for building a CV in a user friendly way.\n\n    Parameters\n    ----------\n    cv : int, a cv generator instance, or None\n        The input specifying which cv generator to use. It can be an\n        integer, in which case it is the number of folds in a KFold,\n        None, in which case 3 fold is used, or another object, that\n        will then be used as a cv generator.\n\n    X : array-like\n        The data the cross-val object will be applied on.\n\n    y : array-like\n        The target variable for a supervised learning problem.\n\n    classifier : boolean optional\n        Whether the task is a classification task, in which case\n        stratified KFold will be used.\n\n    Returns\n    -------\n    checked_cv: a cross-validation generator instance.\n        The return value is guaranteed to be a cv generator instance, whatever\n        the input type.\n    \"\"\"\n", "input": "", "output": "    return _check_cv(cv, X=X, y=y, classifier=classifier, warn_mask=True)", "category": "Python"}, {"instruction": "def drawPoints(self, pen, contours=True, components=True):\n        \"\"\"\n        Draw the glyph's outline data (contours and components) to\n        the given :ref:`type-pointpen`.\n\n            >>> glyph.drawPoints(pointPen)\n\n        If ``contours`` is set to ``False``, the glyph's\n        contours will not be drawn.\n\n            >>> glyph.drawPoints(pointPen, contours=False)\n\n        If ``components`` is set to ``False``, the glyph's\n        components will not be drawn.\n\n            >>> glyph.drawPoints(pointPen, components=False)\n        \"\"\"\n", "input": "", "output": "        if contours:\n            for contour in self:\n                contour.drawPoints(pen)\n        if components:\n            for component in self.components:\n                component.drawPoints(pen)", "category": "Python"}, {"instruction": "def getdict(source):\n    \"\"\"Returns a standard python Dict with computed values\n    from the DynDict\n    :param source: (DynDict) input\n    :return: (dict) Containing computed values\n    \"\"\"\n", "input": "", "output": "    std_dict = {}\n    for var, val in source.iteritems():\n        std_dict[var] = source[var]\n    return std_dict", "category": "Python"}, {"instruction": "def log_request_fail(self, method, full_url, body, duration, status_code=None, exception=None):\n        \"\"\"\n        Log an unsuccessful API call.\n        \"\"\"\n", "input": "", "output": "        logger.warning(\n            '%s %s [status:%s request:%.3fs]', method, full_url,\n            status_code or 'N/A', duration, exc_info=exception is not None\n        )\n\n        if body and not isinstance(body, dict):\n            body = body.decode('utf-8')\n\n        logger.debug('> %s', body)", "category": "Python"}, {"instruction": "def build_wheel(wheel_directory, config_settings, metadata_directory=None):\n    \"\"\"Invoke the mandatory build_wheel hook.\n\n    If a wheel was already built in the\n    prepare_metadata_for_build_wheel fallback, this\n    will copy it rather than rebuilding the wheel.\n    \"\"\"\n", "input": "", "output": "    prebuilt_whl = _find_already_built_wheel(metadata_directory)\n    if prebuilt_whl:\n        shutil.copy2(prebuilt_whl, wheel_directory)\n        return os.path.basename(prebuilt_whl)\n\n    return _build_backend().build_wheel(wheel_directory, config_settings,\n                                        metadata_directory)", "category": "Python"}, {"instruction": "def count_quota_handler_factory(count_quota_field):\n    \"\"\" Creates handler that will recalculate count_quota on creation/deletion \"\"\"\n", "input": "", "output": "\n    def recalculate_count_quota(sender, instance, **kwargs):\n        signal = kwargs['signal']\n        if signal == signals.post_save and kwargs.get('created'):\n            count_quota_field.add_usage(instance, delta=1)\n        elif signal == signals.post_delete:\n            count_quota_field.add_usage(instance, delta=-1, fail_silently=True)\n\n    return recalculate_count_quota", "category": "Python"}, {"instruction": "def process_nested_tags(self, node, tag = ''):\n        \"\"\"\n        Process child tags.\n\n        @param node: Current node being parsed.\n        @type node: xml.etree.Element\n\n        @raise ParseError: Raised when an unexpected nested tag is found.\n        \"\"\"\n", "input": "", "output": "        ##print(\"---------Processing: %s, %s\"%(node.tag,tag))\n\n        if tag == '':\n            t = node.ltag\n        else:\n            t = tag.lower()\n        \n        for child in node.children:\n            self.xml_node_stack = [child] + self.xml_node_stack\n\n            ctagl = child.ltag\n\n            if ctagl in self.tag_parse_table and ctagl in self.valid_children[t]:\n                #print(\"Processing known type: %s\"%ctagl)\n                self.tag_parse_table[ctagl](child)\n            else:\n                #print(\"Processing unknown type: %s\"%ctagl)\n                self.parse_component_by_typename(child, child.tag)\n\n            self.xml_node_stack = self.xml_node_stack[1:]", "category": "Python"}, {"instruction": "def listen(self):\n        \"\"\"Starts the client listener to listen for server responses.\n\n        Args:\n          None\n\n        Returns:\n          None\n\n        \"\"\"\n", "input": "", "output": "\n        logger.info(\"Listening on port \" + str(self.listener.listen_port))\n        self.listener.listen()", "category": "Python"}, {"instruction": "def update_anchor(self):\n        \"\"\" \u5982\u679c\u662f\u4f7f\u7528set_anchor_rate\u6765\u8bbe\u5b9a\u951a\u70b9\uff0c\u90a3\u4e48\u5c31\u9700\u8981\u4e0d\u505c\u7684\u66f4\u65b0\u951a\u70b9\u7684\u4f4d\u7f6e \"\"\"\n", "input": "", "output": "        t = self.transform\n        self.update_collision_rect()\n        if t.anchor_x_r and t.anchor_y_r:\n            t.anchor_x = self.min_x + (self.max_x - self.min_x) * t.anchor_x_r\n            t.anchor_y = self.min_y + (self.max_y - self.min_y) * t.anchor_y_r", "category": "Python"}, {"instruction": "async def _unsubscribe(self, channels, is_mask):\n        \"\"\"Unsubscribe from given channel.\"\"\"\n", "input": "", "output": "        vanished = []\n        if channels:\n            for channel in channels:\n                key = channel, is_mask\n                self._channels.remove(key)\n                self._plugin._subscriptions[key].remove(self._queue)\n                if not self._plugin._subscriptions[key]:  # we were last sub?\n                    vanished.append(channel)\n                    del self._plugin._subscriptions[key]\n        else:\n            while self._channels:\n                channel, is_mask = key = self._channels.pop()\n                self._plugin._subscriptions[key].remove(self._queue)\n                if not self._plugin._subscriptions[key]:\n                    vanished.append(channel)\n                    del self._plugin._subscriptions[key]\n        if vanished:\n            await getattr(self._sub, 'punsubscribe' if is_mask else 'unsubscribe')(vanished)", "category": "Python"}, {"instruction": "def validate_argmin_with_skipna(skipna, args, kwargs):\n    \"\"\"\n    If 'Series.argmin' is called via the 'numpy' library,\n    the third parameter in its signature is 'out', which\n    takes either an ndarray or 'None', so check if the\n    'skipna' parameter is either an instance of ndarray or\n    is None, since 'skipna' itself should be a boolean\n    \"\"\"\n", "input": "", "output": "\n    skipna, args = process_skipna(skipna, args)\n    validate_argmin(args, kwargs)\n    return skipna", "category": "Python"}, {"instruction": "def check_output_data_type(self):\n        \"\"\"Check the output data types of the state\n\n        Checks all output data ports if the handed data is not of the specified type and generate an error logger\n        message with details of the found type conflict.\n        \"\"\"\n", "input": "", "output": "        for data_port in self.output_data_ports.values():\n            if data_port.name in self.output_data and self.output_data[data_port.name] is not None:\n                # check for class\n                if not isinstance(self.output_data[data_port.name], data_port.data_type):\n                    logger.error(\"{0} had an data port error: Output of execute function must be of type '{1}' not \"\n                                 \"'{2}' as current value {3}\".format(self, data_port.data_type.__name__,\n                                                               type(self.output_data[data_port.name]).__name__,\n                                                               self.output_data[data_port.name]))", "category": "Python"}, {"instruction": "def cli(env, group_id, name, description):\n    \"\"\"Edit details of a security group.\"\"\"\n", "input": "", "output": "    mgr = SoftLayer.NetworkManager(env.client)\n    data = {}\n    if name:\n        data['name'] = name\n    if description:\n        data['description'] = description\n\n    if not mgr.edit_securitygroup(group_id, **data):\n        raise exceptions.CLIAbort(\"Failed to edit security group\")", "category": "Python"}, {"instruction": "def on_save(self, event):\n        '''called on save button'''\n", "input": "", "output": "        dlg = wx.FileDialog(None, self.settings.get_title(), '', \"\", '*.*',\n                            wx.FD_SAVE | wx.FD_OVERWRITE_PROMPT)\n        if dlg.ShowModal() == wx.ID_OK:\n            self.settings.save(dlg.GetPath())", "category": "Python"}, {"instruction": "def NS(s, o):\n    \"\"\"\n        Nash Sutcliffe efficiency coefficient\n        input:\n        s: simulated\n        o: observed\n        output:\n        ns: Nash Sutcliffe efficient coefficient\n        \"\"\"\n", "input": "", "output": "    # s,o = filter_nan(s,o)\n    return 1 - np.sum((s-o)**2)/np.sum((o-np.mean(o))**2)", "category": "Python"}, {"instruction": "def proportional_weights(self, fraction_stdev=1.0, wmax=100.0,\n                             leave_zero=True):\n        \"\"\"setup  weights inversely proportional to the observation value\n\n        Parameters\n        ----------\n        fraction_stdev : float\n            the fraction portion of the observation\n            val to treat as the standard deviation.  set to 1.0 for\n            inversely proportional\n        wmax : float\n            maximum weight to allow\n        leave_zero : bool\n            flag to leave existing zero weights\n\n        \"\"\"\n", "input": "", "output": "        new_weights = []\n        for oval, ow in zip(self.observation_data.obsval,\n                            self.observation_data.weight):\n            if leave_zero and ow == 0.0:\n                ow = 0.0\n            elif oval == 0.0:\n                ow = wmax\n            else:\n                nw = 1.0 / (np.abs(oval) * fraction_stdev)\n                ow = min(wmax, nw)\n            new_weights.append(ow)\n        self.observation_data.weight = new_weights", "category": "Python"}, {"instruction": "def cartesian_to_spherical(cartesian):\n    \"\"\"Convert cartesian to spherical coordinates passed as (N,3) shaped arrays.\"\"\"\n", "input": "", "output": "    xyz = cartesian\n    xy = xyz[:,0]**2 + xyz[:,1]**2\n    r = np.sqrt(xy + xyz[:,2]**2)\n    phi = np.arctan2(np.sqrt(xy), xyz[:,2]) # for elevation angle defined from Z-axis down\n    #ptsnew[:,4] = np.arctan2(xyz[:,2], np.sqrt(xy)) # for elevation angle defined from XY-plane up\n    theta = np.arctan2(xyz[:,1], xyz[:,0])\n    return np.array([r, theta, phi]).T", "category": "Python"}, {"instruction": "def _kml_default_colors(x):\n    \"\"\" flight mode to color conversion \"\"\"\n", "input": "", "output": "    x = max([x, 0])\n    colors_arr = [simplekml.Color.red, simplekml.Color.green, simplekml.Color.blue,\n                  simplekml.Color.violet, simplekml.Color.yellow, simplekml.Color.orange,\n                  simplekml.Color.burlywood, simplekml.Color.azure, simplekml.Color.lightblue,\n                  simplekml.Color.lawngreen, simplekml.Color.indianred, simplekml.Color.hotpink]\n    return colors_arr[x]", "category": "Python"}, {"instruction": "def import_object_from_string_code(code, object):\n    \"\"\"Used to import an object from arbitrary passed code.\n\n    Passed in code is treated as a module and is imported and added\n    to `sys.modules` with its SHA256 hash as key.\n\n    Args:\n        code (string): Python code to import as module\n\n        object (string): Name of object to extract from imported module\n    \"\"\"\n", "input": "", "output": "    sha256 = hashlib.sha256(code.encode('UTF-8')).hexdigest()\n    module = imp.new_module(sha256)\n    try:\n        exec_(code, module.__dict__)\n    except Exception as e:\n        raise exceptions.UserError('User code exception', exception_message=str(e))\n    sys.modules[sha256] = module\n    try:\n        return getattr(module, object)\n    except AttributeError:\n        raise exceptions.UserError(\"{} not found in code\".format(object))", "category": "Python"}, {"instruction": "def deserializeG2(x, compressed=True):\n    \"\"\"\n    Deserializes an array of bytes, @x, into a G2 element.\n    \"\"\"\n", "input": "", "output": "    return _deserialize(x, G2Element, compressed, librelic.g2_read_bin_abi)", "category": "Python"}, {"instruction": "def add_customers(self, service_desk_id, list_of_usernames):\n        \"\"\"\n        Adds one or more existing customers to the given service desk.\n        If you need to create a customer, see Create customer method.\n\n        Administer project permission is required, or agents if public signups\n        and invites are enabled for the Service Desk project.\n\n        :param service_desk_id: str\n        :param list_of_usernames: list\n        :return: the customers added to the service desk\n        \"\"\"\n", "input": "", "output": "        url = 'rest/servicedeskapi/servicedesk/{}/customer'.format(service_desk_id)\n        data = {'usernames': list_of_usernames}\n\n        return self.post(url, headers=self.experimental_headers, data=data)", "category": "Python"}, {"instruction": "def _process_request(self, request, client_address):\n        \"\"\"Actually processes the request.\"\"\"\n", "input": "", "output": "        try:\n            self.finish_request(request, client_address)\n        except Exception:\n            self.handle_error(request, client_address)\n        finally:\n            self.shutdown_request(request)", "category": "Python"}, {"instruction": "def _handle_call(self, actual_call, stubbed_call):\n        \"\"\"Extends Stub call handling behavior to be callable by default.\"\"\"\n", "input": "", "output": "        self._actual_calls.append(actual_call)\n        use_call = stubbed_call or actual_call\n        return use_call.return_value", "category": "Python"}, {"instruction": "def loadSharedResource(self, pchResourceName, pchBuffer, unBufferLen):\n        \"\"\"\n        Loads the specified resource into the provided buffer if large enough.\n        Returns the size in bytes of the buffer required to hold the specified resource.\n        \"\"\"\n", "input": "", "output": "\n        fn = self.function_table.loadSharedResource\n        result = fn(pchResourceName, pchBuffer, unBufferLen)\n        return result", "category": "Python"}, {"instruction": "def is_all_field_none(self):\n        \"\"\"\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "\n        if self._MonetaryAccountBank is not None:\n            return False\n\n        if self._MonetaryAccountJoint is not None:\n            return False\n\n        if self._MonetaryAccountLight is not None:\n            return False\n\n        if self._MonetaryAccountSavings is not None:\n            return False\n\n        return True", "category": "Python"}, {"instruction": "def _gen_code(self):\n        \"\"\"Generate code for the list of expressions provided using the common sub-expression eliminator to separate out portions that are computed multiple times.\"\"\"\n", "input": "", "output": "        # This is the dictionary that stores all the generated code.\n\n        self.code = {}\n        def match_key(expr):\n            if type(expr) is dict:\n                code = {}\n                for key in expr.keys():\n                    code[key] = match_key(expr[key])\n            else:\n                arg_list = [e for e in expr.atoms() if e.is_Symbol]\n                code = self._expr2code(arg_list, expr)\n            return code\n\n        self.code = match_key(self.expressions)", "category": "Python"}, {"instruction": "def do_rnn_checkpoint(cells, prefix, period=1):\n    \"\"\"Make a callback to checkpoint Module to prefix every epoch.\n    unpacks weights used by cells before saving.\n\n    Parameters\n    ----------\n    cells : mxnet.rnn.RNNCell or list of RNNCells\n        The RNN cells used by this symbol.\n    prefix : str\n        The file prefix to checkpoint to\n    period : int\n        How many epochs to wait before checkpointing. Default is 1.\n\n    Returns\n    -------\n    callback : function\n        The callback function that can be passed as iter_end_callback to fit.\n    \"\"\"\n", "input": "", "output": "    period = int(max(1, period))\n    # pylint: disable=unused-argument\n    def _callback(iter_no, sym=None, arg=None, aux=None):\n        ", "category": "Python"}, {"instruction": "def empty(self, duration):\n        '''Empty vector annotations.\n\n        This returns an annotation with a single observation\n        vector consisting of all-zeroes.\n\n        Parameters\n        ----------\n        duration : number >0\n            Length of the track\n\n        Returns\n        -------\n        ann : jams.Annotation\n            The empty annotation\n        '''\n", "input": "", "output": "        ann = super(VectorTransformer, self).empty(duration)\n\n        ann.append(time=0, duration=duration, confidence=0,\n                   value=np.zeros(self.dimension, dtype=np.float32))\n        return ann", "category": "Python"}, {"instruction": "def nvc2pl(normal, constant):\n    \"\"\"\n    Make a plane from a normal vector and a constant.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/nvc2pl_c.html\n\n    :param normal: A normal vector defining a plane.\n    :type normal: 3-Element Array of floats\n    :param constant: A constant defining a plane.\n    :type constant: float\n    :return: plane\n    :rtype: spiceypy.utils.support_types.Plane\n    \"\"\"\n", "input": "", "output": "    plane = stypes.Plane()\n    normal = stypes.toDoubleVector(normal)\n    constant = ctypes.c_double(constant)\n    libspice.nvc2pl_c(normal, constant, ctypes.byref(plane))\n    return plane", "category": "Python"}, {"instruction": "def im_files(self, room_id=None, user_name=None, **kwargs):\n        \"\"\"Retrieves the files from a direct message.\"\"\"\n", "input": "", "output": "        if room_id:\n            return self.__call_api_get('im.files', roomId=room_id, kwargs=kwargs)\n        elif user_name:\n            return self.__call_api_get('im.files', username=user_name, kwargs=kwargs)\n        else:\n            raise RocketMissingParamException('roomId or username required')", "category": "Python"}, {"instruction": "def priority_color(self, p_priority):\n        \"\"\"\n        Returns a dict with priorities as keys and color numbers as value.\n        \"\"\"\n", "input": "", "output": "        def _str_to_dict(p_string):\n            pri_colors_dict = dict()\n            for pri_color in p_string.split(','):\n                pri, color = pri_color.split(':')\n                pri_colors_dict[pri] = Color(color)\n\n            return pri_colors_dict\n\n        try:\n            pri_colors_str = self.cp.get('colorscheme', 'priority_colors')\n\n            if pri_colors_str == '':\n                pri_colors_dict = _str_to_dict('A:-1,B:-1,C:-1')\n            else:\n                pri_colors_dict = _str_to_dict(pri_colors_str)\n        except ValueError:\n            pri_colors_dict = _str_to_dict(self.defaults['colorscheme']['priority_colors'])\n\n        return pri_colors_dict[p_priority] if p_priority in pri_colors_dict else Color('NEUTRAL')", "category": "Python"}, {"instruction": "def transform_annotation(self, ann, duration):\n        '''Transform an annotation to static label encoding.\n\n        Parameters\n        ----------\n        ann : jams.Annotation\n            The annotation to convert\n\n        duration : number > 0\n            The duration of the track\n\n        Returns\n        -------\n        data : dict\n            data['tags'] : np.ndarray, shape=(n_labels,)\n                A static binary encoding of the labels\n        '''\n", "input": "", "output": "        intervals = np.asarray([[0, 1]])\n        values = list([obs.value for obs in ann])\n        intervals = np.tile(intervals, [len(values), 1])\n\n        # Suppress all intervals not in the encoder\n        tags = [v for v in values if v in self._classes]\n        if len(tags):\n            target = self.encoder.transform([tags]).astype(np.bool).max(axis=0)\n        else:\n            target = np.zeros(len(self._classes), dtype=np.bool)\n\n        return {'tags': target}", "category": "Python"}, {"instruction": "def change_column_length(table: Table, column: Column, length: int, engine: Engine) -> None:\n    \"\"\" Change the column length in the supplied table\n    \"\"\"\n", "input": "", "output": "    if column.type.length < length:\n        print(\"Changing length of {} from {} to {}\".format(column, column.type.length, length))\n        column.type.length = length\n        column_name = column.name\n        column_type = column.type.compile(engine.dialect)\n        engine.execute('ALTER TABLE {table} ALTER COLUMN {column_name} TYPE {column_type}'.format(**locals()))", "category": "Python"}, {"instruction": "def get_command_line(self):\n        \"\"\"\n        Retrieves the command line with wich the program was started.\n\n        @rtype:  str\n        @return: Command line string.\n\n        @raise WindowsError: On error an exception is raised.\n        \"\"\"\n", "input": "", "output": "        (Buffer, MaximumLength) = self.get_command_line_block()\n        CommandLine = self.peek_string(Buffer, dwMaxSize=MaximumLength,\n                                                            fUnicode=True)\n        gst = win32.GuessStringType\n        if gst.t_default == gst.t_ansi:\n            CommandLine = CommandLine.encode('cp1252')\n        return CommandLine", "category": "Python"}, {"instruction": "def get_suffix(id, regex=\"-[A-Z]{1}[0-9]{1,2}$\"):\n    \"\"\"Get the suffix of the ID, e.g. '-R01' or '-P05'\n\n    The current regex determines a pattern of a single uppercase character with\n    at most 2 numbers following at the end of the ID as the suffix.\n    \"\"\"\n", "input": "", "output": "    parts = re.findall(regex, id)\n    if not parts:\n        return \"\"\n    return parts[0]", "category": "Python"}, {"instruction": "def _init_study_items_max(self):\n        \"\"\"User can limit the number of genes printed in a GO term.\"\"\"\n", "input": "", "output": "        if self.study_items is None:\n            return None\n        if self.study_items is True:\n            return None\n        if isinstance(self.study_items, int):\n            return self.study_items\n        return None", "category": "Python"}, {"instruction": "def _text_attr(self, attr):\n        \"\"\"\n        Given a text attribute, set the current cursor appropriately.\n        \"\"\"\n", "input": "", "output": "        attr = text[attr]\n        if attr == \"reset\":\n            self.cursor_attributes = self.default_attributes\n        elif attr == \"underline-off\":\n            self.cursor_attributes = self._remove_text_attr(\"underline\")\n        elif attr == \"blink-off\":\n            self.cursor_attributes = self._remove_text_attr(\"blink\")\n        elif attr == \"reverse-off\":\n            self.cursor_attributes = self._remove_text_attr(\"reverse\")\n        else:\n            self.cursor_attributes = self._add_text_attr(attr)", "category": "Python"}, {"instruction": "def metaclass(self):\n        \"\"\"Get a metaclass configured to use this registry.\"\"\"\n", "input": "", "output": "        if '_metaclass' not in self.__dict__:\n            self._metaclass = type('PermissionsMeta', (PermissionsMeta,), {'registry': self})\n        return self._metaclass", "category": "Python"}, {"instruction": "def __parts_and_divisions(self):\n        \"\"\"\n        The parts and divisions directly part of this element.\n        \"\"\"\n", "input": "", "output": "\n        from .division import Division\n        from .part import Part\n        from .placeholder_part import PlaceholderPart\n\n        text = self.node.text\n        if text:\n            stripped_text = text.replace('\\n', '')\n            if stripped_text.strip():\n                yield PlaceholderPart(stripped_text)\n\n        for item in self.node:\n            if item.tag == 'part':\n                yield Part(item)\n            elif item.tag == 'div':\n                yield Division(item)\n            \n            if item.tail:\n                stripped_tail = item.tail.replace('\\n', '')\n                if stripped_tail.strip():\n                    yield PlaceholderPart(stripped_tail)", "category": "Python"}, {"instruction": "def hex_is_dark(hexx, percent=50):\n    \"\"\"\n    Function to decide if a hex colour is dark.\n\n    Args:\n        hexx (str): A hexadecimal colour, starting with '#'.\n\n    Returns:\n        bool: The colour's brightness is less than the given percent.\n    \"\"\"\n", "input": "", "output": "    r, g, b = hex_to_rgb(hexx)\n    luma = (0.2126 * r + 0.7152 * g + 0.0722 * b) / 2.55  # per ITU-R BT.709\n\n    return (luma < percent)", "category": "Python"}, {"instruction": "def _seqtk_fastq_prep_cl(data, in_file=None, read_num=0):\n    \"\"\"Provide a commandline for prep of fastq inputs with seqtk.\n\n    Handles fast conversion of fastq quality scores and trimming.\n    \"\"\"\n", "input": "", "output": "    needs_convert = dd.get_quality_format(data).lower() == \"illumina\"\n    trim_ends = dd.get_trim_ends(data)\n    seqtk = config_utils.get_program(\"seqtk\", data[\"config\"])\n    if in_file:\n        in_file = objectstore.cl_input(in_file)\n    else:\n        in_file = \"/dev/stdin\"\n    cmd = \"\"\n    if needs_convert:\n        cmd += \"{seqtk} seq -Q64 -V {in_file}\".format(**locals())\n    if trim_ends:\n        left_trim, right_trim = trim_ends[0:2] if data.get(\"read_num\", read_num) == 0 else trim_ends[2:4]\n        if left_trim or right_trim:\n            trim_infile = \"/dev/stdin\" if needs_convert else in_file\n            pipe = \" | \" if needs_convert else \"\"\n            cmd += \"{pipe}{seqtk} trimfq -b {left_trim} -e {right_trim} {trim_infile}\".format(**locals())\n    return cmd", "category": "Python"}, {"instruction": "def logical_chassis_fwdl_status_output_cluster_fwdl_entries_fwdl_entries_index(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        logical_chassis_fwdl_status = ET.Element(\"logical_chassis_fwdl_status\")\n        config = logical_chassis_fwdl_status\n        output = ET.SubElement(logical_chassis_fwdl_status, \"output\")\n        cluster_fwdl_entries = ET.SubElement(output, \"cluster-fwdl-entries\")\n        fwdl_entries = ET.SubElement(cluster_fwdl_entries, \"fwdl-entries\")\n        index = ET.SubElement(fwdl_entries, \"index\")\n        index.text = kwargs.pop('index')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def ucnstring_to_python(ucn_string):\n    \"\"\"\n    Return string with Unicode UCN (e.g. \"U+4E00\") to native Python Unicode\n    (u'\\\\u4e00').\n    \"\"\"\n", "input": "", "output": "    res = re.findall(\"U\\+[0-9a-fA-F]*\", ucn_string)\n    for r in res:\n        ucn_string = ucn_string.replace(text_type(r), text_type(ucn_to_unicode(r)))\n\n    ucn_string = ucn_string.encode('utf-8')\n\n    assert isinstance(ucn_string, bytes)\n    return ucn_string", "category": "Python"}, {"instruction": "def _find_template(parameters, index, required=False, notfoundmsg=None):\n    \"\"\"\n    Generate ``.find()`` call for HTMLElement.\n\n    Args:\n        parameters (list): List of parameters for ``.find()``.\n        index (int): Index of the item you want to get from ``.find()`` call.\n        required (bool, default False): Use :func:`_required_idiom` to returned\n                 data.\n        notfoundmsg (str, default None): Message which will be used for\n                    :func:`_required_idiom` if the item is not found.\n\n    Returns:\n        str: Python code.\n\n    Live example::\n        >>> print g._find_template([\"<xex>\"], 3)\n            el = dom.find('<xex>')\n            # pick element from list\n            el = el[3] if len(el) - 1 >= 3 else None\n    \"\"\"\n", "input": "", "output": "    output = IND + \"el = dom.find(%s)\\n\\n\" % repr(parameters)[1:-1]\n\n    if required:\n        return output + _required_idiom(parameters[0], index, notfoundmsg)\n\n    return output + _index_idiom(\"el\", index)", "category": "Python"}, {"instruction": "def p_values(self, p):\n        \"\"\"values :\n                  | values value VALUE_SEPARATOR\n                  | values value\"\"\"\n", "input": "", "output": "        if len(p) == 1:\n            p[0] = list()\n        else:\n            p[1].append(p[2])\n            p[0] = p[1]", "category": "Python"}, {"instruction": "def _get_site(self, url, headers, cookies, timeout, driver_args, driver_kwargs):\n        \"\"\"\n        Try and return page content in the requested format using requests\n        \"\"\"\n", "input": "", "output": "        try:\n            # Headers and cookies are combined to the ones stored in the requests session\n            #  Ones passed in here will override the ones in the session if they are the same key\n            response = self.driver.get(url,\n                                       *driver_args,\n                                       headers=headers,\n                                       cookies=cookies,\n                                       timeout=timeout,\n                                       **driver_kwargs)\n\n            # Set data to access from script\n            self.status_code = response.status_code\n            self.url = response.url\n            self.response = response\n\n            if response.status_code == requests.codes.ok:\n                # Return the correct format\n                return response.text\n\n            response.raise_for_status()\n\n        except Exception as e:\n            raise e.with_traceback(sys.exc_info()[2])", "category": "Python"}, {"instruction": "def widgetSubCheckBoxRect(widget, option):\n    \"\"\" Returns the rectangle of a check box drawn as a sub element of widget\n    \"\"\"\n", "input": "", "output": "    opt = QtWidgets.QStyleOption()\n    opt.initFrom(widget)\n    style = widget.style()\n    return style.subElementRect(QtWidgets.QStyle.SE_ViewItemCheckIndicator, opt, widget)", "category": "Python"}, {"instruction": "def write_human(self, buffer_):\n        \"\"\" Emulates human typing speed \"\"\"\n", "input": "", "output": "\n        if self.IAC in buffer_:\n            buffer_ = buffer_.replace(self.IAC, self.IAC + self.IAC)\n        self.msg(\"send %r\", buffer_)\n        for char in buffer_:\n            delta = random.gauss(80, 20)\n            self.sock.sendall(char)\n            time.sleep(delta / 1000.0)", "category": "Python"}, {"instruction": "def simplify_path(path: str, path_prefixes: list = None) -> str:\n    \"\"\"\n    Simplifies package paths by replacing path prefixes with values specified\n    in the replacements list\n\n    :param path:\n    :param path_prefixes:\n    :return:\n    \"\"\"\n", "input": "", "output": "\n    test_path = '{}'.format(path if path else '')\n    replacements = (path_prefixes if path_prefixes else []).copy()\n    replacements.append(('~', os.path.expanduser('~')))\n\n    for key, value in replacements:\n        if test_path.startswith(value):\n            return '{}{}'.format(key, test_path[len(value):])\n\n    return test_path", "category": "Python"}, {"instruction": "def __load_child_classes(self, ac: AssetClass):\n        \"\"\" Loads child classes/stocks \"\"\"\n", "input": "", "output": "        # load child classes for ac\n        db = self.__get_session()\n        entities = (\n            db.query(dal.AssetClass)\n            .filter(dal.AssetClass.parentid == ac.id)\n            .order_by(dal.AssetClass.sortorder)\n            .all()\n        )\n        # map\n        for entity in entities:\n            child_ac = self.__map_entity(entity)\n            # depth\n            child_ac.depth = ac.depth + 1\n            ac.classes.append(child_ac)\n            # Add to index\n            self.model.asset_classes.append(child_ac)\n\n            self.__load_child_classes(child_ac)", "category": "Python"}, {"instruction": "def set_font(font, section='appearance', option='font'):\n    \"\"\"Set font\"\"\"\n", "input": "", "output": "    CONF.set(section, option+'/family', to_text_string(font.family()))\n    CONF.set(section, option+'/size', float(font.pointSize()))\n    CONF.set(section, option+'/italic', int(font.italic()))\n    CONF.set(section, option+'/bold', int(font.bold()))\n    FONT_CACHE[(section, option)] = font", "category": "Python"}, {"instruction": "def move_before(self, node_id):\n        \"\"\" Moving one node of tree before another\n\n        For example see:\n\n        * :mod:`sqlalchemy_mptt.tests.cases.move_node.test_move_before_function`\n        * :mod:`sqlalchemy_mptt.tests.cases.move_node.test_move_before_to_other_tree`\n        * :mod:`sqlalchemy_mptt.tests.cases.move_node.test_move_before_to_top_level`\n        \"\"\"\n", "input": "", "output": "        session = Session.object_session(self)\n        table = _get_tree_table(self.__mapper__)\n        pk = getattr(table.c, self.get_pk_column().name)\n        node = session.query(table).filter(pk == node_id).one()\n        self.parent_id = node.parent_id\n        self.mptt_move_before = node_id\n        session.add(self)", "category": "Python"}, {"instruction": "def get_empty_dimension(**kwargs):\n    \"\"\"\n        Returns a dimension object initialized with empty values\n    \"\"\"\n", "input": "", "output": "    dimension = JSONObject(Dimension())\n    dimension.id = None\n    dimension.name = ''\n    dimension.description = ''\n    dimension.project_id = None\n    dimension.units = []\n    return dimension", "category": "Python"}, {"instruction": "def filter_params(self, value):\n        \"\"\" return filtering params \"\"\"\n", "input": "", "output": "        if value is None:\n            return {}\n        target = \".\".join(self.field.source_attrs)\n        return { '__raw__': { target + \".$id\": value } }", "category": "Python"}, {"instruction": "def update(self, story, params={}, **options): \n        \"\"\"Updates the story and returns the full record for the updated story.\n        Only comment stories can have their text updated, and only comment stories and\n        attachment stories can be pinned. Only one of `text` and `html_text` can be specified.\n\n        Parameters\n        ----------\n        story : {Id} Globally unique identifier for the story.\n        [data] : {Object} Data for the request\n          - [text] : {String} The plain text with which to update the comment.\n          - [html_text] : {String} The rich text with which to update the comment.\n          - [is_pinned] : {Boolean} Whether the story should be pinned on the resource.\n        \"\"\"\n", "input": "", "output": "        path = \"/stories/%s\" % (story)\n        return self.client.put(path, params, **options)", "category": "Python"}, {"instruction": "def _get_data_from_list_of_dicts(source, fields='*', first_row=0, count=-1, schema=None):\n  \"\"\" Helper function for _get_data that handles lists of dicts. \"\"\"\n", "input": "", "output": "  if schema is None:\n    schema = google.datalab.bigquery.Schema.from_data(source)\n  fields = get_field_list(fields, schema)\n  gen = source[first_row:first_row + count] if count >= 0 else source\n  rows = [{'c': [{'v': row[c]} if c in row else {} for c in fields]} for row in gen]\n  return {'cols': _get_cols(fields, schema), 'rows': rows}, len(source)", "category": "Python"}, {"instruction": "def file_name(self, value):\n        \"\"\"The file_name property.\n        \n        Args:\n            value (string). the property value.\n        \"\"\"\n", "input": "", "output": "        if value == self._defaults['fileName'] and 'fileName' in self._values:\n            del self._values['fileName']\n        else:\n            self._values['fileName'] = value", "category": "Python"}, {"instruction": "def trim_dense(M, n_std=3, s_min=None, s_max=None):\n    \"\"\"By default, return a matrix stripped of component\n    vectors whose sparsity (i.e. total contact count on a\n    single column or row) deviates more than specified number\n    of standard deviations from the mean. Boolean variables\n    s_min and s_max act as absolute fixed values which override\n    such behaviour when specified.\n    \"\"\"\n", "input": "", "output": "\n    M = np.array(M)\n    sparsity = M.sum(axis=1)\n    mean = np.mean(sparsity)\n    std = np.std(sparsity)\n    if s_min is None:\n        s_min = mean - n_std * std\n    if s_max is None:\n        s_max = mean + n_std * std\n    elif s_max == 0:\n        s_max = np.amax(M)\n    f = (sparsity > s_min) * (sparsity < s_max)\n    N = M[f][:, f]\n    return N", "category": "Python"}, {"instruction": "def _is_bhyve_hyper():\n    '''\n    Returns a bool whether or not this node is a bhyve hypervisor\n    '''\n", "input": "", "output": "    sysctl_cmd = 'sysctl hw.vmm.create'\n    vmm_enabled = False\n    try:\n        stdout = subprocess.Popen(sysctl_cmd,\n                                  shell=True,\n                                  stdout=subprocess.PIPE).communicate()[0]\n        vmm_enabled = len(salt.utils.stringutils.to_str(stdout).split('\"')[1]) != 0\n    except IndexError:\n        pass\n    return vmm_enabled", "category": "Python"}, {"instruction": "def iter_graph(cur):\n    \"\"\"Iterate over all graphs in the cache.\n\n    Args:\n        cur (:class:`sqlite3.Cursor`): An sqlite3 cursor. This function\n            is meant to be run within a :obj:`with` statement.\n\n    Yields:\n        tuple: A 2-tuple containing:\n\n            list: The nodelist for a graph in the cache.\n\n            list: the edgelist for a graph in the cache.\n\n    Examples:\n        >>> nodelist = [0, 1, 2]\n        >>> edgelist = [(0, 1), (1, 2)]\n        >>> with pmc.cache_connect(':memory:') as cur:\n        ...     pmc.insert_graph(cur, nodelist, edgelist)\n        ...     list(pmc.iter_graph(cur))\n        [([0, 1, 2], [[0, 1], [1, 2]])]\n\n    \"\"\"\n", "input": "", "output": "    select = ", "category": "Python"}, {"instruction": "def touch(ctx, key, policy, admin_pin, force):\n    \"\"\"\n    Manage touch policy for OpenPGP keys.\n\n    \\b\n    KEY     Key slot to set (sig, enc or aut).\n    POLICY  Touch policy to set (on, off or fixed).\n    \"\"\"\n", "input": "", "output": "    controller = ctx.obj['controller']\n    old_policy = controller.get_touch(key)\n\n    if old_policy == TOUCH_MODE.FIXED:\n        ctx.fail('A FIXED policy cannot be changed!')\n\n    force or click.confirm('Set touch policy of {.name} key to {.name}?'.format(\n        key, policy), abort=True, err=True)\n    if admin_pin is None:\n        admin_pin = click.prompt('Enter admin PIN', hide_input=True, err=True)\n    controller.set_touch(key, policy, admin_pin.encode('utf8'))", "category": "Python"}]