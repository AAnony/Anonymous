[{"instruction": "def update_cb(self, context, t, idx, userdata):\n        \"\"\"A sink property changed, calls request_update\"\"\"\n", "input": "", "output": "\n        if t & PA_SUBSCRIPTION_EVENT_FACILITY_MASK == PA_SUBSCRIPTION_EVENT_SERVER:\n            pa_operation_unref(\n                pa_context_get_server_info(context, self._server_info_cb, None))\n\n        self.request_update(context)", "category": "Python"}, {"instruction": "def do_printActivities(self,args):\n        \"\"\"Print scaling activities\"\"\"\n", "input": "", "output": "        parser = CommandArgumentParser(\"printActivities\")\n        parser.add_argument('-r','--refresh',action='store_true',dest='refresh',help='refresh');\n        args = vars(parser.parse_args(args))\n        refresh = args['refresh'] or not self.activities\n\n        if refresh:\n            response = self.client.describe_scaling_activities(AutoScalingGroupName=self.scalingGroup)\n            self.activities = response['Activities']\n        \n        index = 0\n        for activity in self.activities:\n            print \"{}: {} -> {} {}: {}\".format(index,activity['StartTime'],stdplus.defaultifyDict(activity,'EndTime',''),activity['StatusCode'],activity['Description'])\n            index = index + 1", "category": "Python"}, {"instruction": "def remove_link_button(self):\n        \"\"\"\n        Function removes link button from Run Window\n        \"\"\"\n", "input": "", "output": "        if self.link is not None:\n            self.info_box.remove(self.link)\n            self.link.destroy()\n            self.link = None", "category": "Python"}, {"instruction": "def run():\n    \"\"\"This client pushes a file into Workbench.\"\"\"\n", "input": "", "output": "    \n    # Grab server args\n    args = client_helper.grab_server_args()\n\n    # Start up workbench connection\n    workbench = zerorpc.Client(timeout=300, heartbeat=60)\n    workbench.connect('tcp://'+args['server']+':'+args['port'])\n\n    # Upload the file into workbench\n    my_file = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                           '../data/pe/bad/033d91aae8ad29ed9fbb858179271232')\n    with open(my_file,'rb') as f:\n\n        # Throw file into workbench\n        filename = os.path.basename(my_file)\n        raw_bytes = f.read()\n        md5 = workbench.store_sample(raw_bytes, filename, 'exe')\n        results = workbench.work_request('view', md5)\n        print 'Filename: %s' % filename\n        pprint.pprint(results)", "category": "Python"}, {"instruction": "def getWidth(self):\n        \"\"\"\n        Get the width of the text output for the table.\n\n        @rtype:  int\n        @return: Width in characters for the text output,\n            including the newline character.\n        \"\"\"\n", "input": "", "output": "        width = 0\n        if self.__width:\n            width = sum( abs(x) for x in self.__width )\n            width = width + len(self.__width) * len(self.__sep) + 1\n        return width", "category": "Python"}, {"instruction": "def run_after_async(seconds, func, *args, **kwargs):\n    \"\"\"Run the function after seconds asynchronously.\"\"\"\n", "input": "", "output": "    t = Timer(seconds, func, args, kwargs)\n    t.daemon = True\n    t.start()\n    return t", "category": "Python"}, {"instruction": "def build_distribution():\n    \"\"\"Build distributions of the code.\"\"\"\n", "input": "", "output": "    result = invoke.run('python setup.py sdist bdist_egg bdist_wheel',\n                        warn=True, hide=True)\n    if result.ok:\n        print(\"[{}GOOD{}] Distribution built without errors.\"\n              .format(GOOD_COLOR, RESET_COLOR))\n    else:\n        print('[{}ERROR{}] Something broke trying to package your '\n              'code...'.format(ERROR_COLOR, RESET_COLOR))\n        print(result.stderr)\n        sys.exit(1)", "category": "Python"}, {"instruction": "def contains(self, key):\n    \"\"\"Checks if the specified item exists.\n\n    Args:\n      key: the key of the item to lookup.\n    Returns:\n      True if the item exists; False otherwise.\n    Raises:\n      Exception if there was an error requesting information about the item.\n    \"\"\"\n", "input": "", "output": "    try:\n      self._api.objects_get(self._bucket, key)\n    except datalab.utils.RequestException as e:\n      if e.status == 404:\n        return False\n      raise e\n    except Exception as e:\n      raise e\n    return True", "category": "Python"}, {"instruction": "def make_instance(self, command):\n        \"\"\"Create and initialize an instance of a user-defined class.\n\n        command must be a string in the form:\n\n        (<instance-name> of <class-name> <slot-override>*)\n        <slot-override> :== (<slot-name> <constant>*)\n\n        Python equivalent of the CLIPS make-instance command.\n\n        \"\"\"\n", "input": "", "output": "        ist = lib.EnvMakeInstance(self._env, command.encode())\n        if ist == ffi.NULL:\n            raise CLIPSError(self._env)\n\n        return Instance(self._env, ist)", "category": "Python"}, {"instruction": "def get_xmldoc_path(self, filepath):\n        \"\"\"Returns the full path to a possible XML documentation file for the\n        specified code filepath.\"\"\"\n", "input": "", "output": "        segs = filepath.split(\".\")\n        segs.pop()\n        return \".\".join(segs) + \".xml\"", "category": "Python"}, {"instruction": "def delete_row(self, key, value):\n        \"\"\"\n            Deletes the rows where key = value.\n        \"\"\"\n", "input": "", "output": "        self.rows = filter(lambda x: x.get(key) != value, self.rows)", "category": "Python"}, {"instruction": "def histogram_bin_edges_mincount(data, min_count, bins):\n  r'''\nMerge bins with right-neighbour until each bin has a minimum number of data-points.\n\n:arguments:\n\n  **data** (``<array_like>``)\n    Input data. The histogram is computed over the flattened array.\n\n  **bins** (``<array_like>`` | ``<int>``)\n    The bin-edges (or the number of bins, automatically converted to equal-sized bins).\n\n  **min_count** (``<int>``)\n    The minimum number of data-points per bin.\n  '''\n", "input": "", "output": "\n  # escape\n  if min_count is None : return bins\n  if min_count is False: return bins\n\n  # check\n  if type(min_count) != int: raise IOError('\"min_count\" must be an integer number')\n\n  # keep removing where needed\n  while True:\n\n    P, _ = np.histogram(data, bins=bins, density=False)\n\n    idx = np.where(P < min_count)[0]\n\n    if len(idx) == 0: return bins\n\n    idx = idx[0]\n\n    if idx+1 == len(P): bins = np.hstack(( bins[:(idx)  ], bins[-1]       ))\n    else              : bins = np.hstack(( bins[:(idx+1)], bins[(idx+2):] ))", "category": "Python"}, {"instruction": "def assign(self, partitions):\n        \"\"\"Manually assign a list of TopicPartitions to this consumer.\n\n        Arguments:\n            partitions (list of TopicPartition): Assignment for this instance.\n\n        Raises:\n            IllegalStateError: If consumer has already called\n            :meth:`~kafka.KafkaConsumer.subscribe`.\n\n        Warning:\n            It is not possible to use both manual partition assignment with\n            :meth:`~kafka.KafkaConsumer.assign` and group assignment with\n            :meth:`~kafka.KafkaConsumer.subscribe`.\n\n        Note:\n            This interface does not support incremental assignment and will\n            replace the previous assignment (if there was one).\n\n        Note:\n            Manual topic assignment through this method does not use the\n            consumer's group management functionality. As such, there will be\n            no rebalance operation triggered when group membership or cluster\n            and topic metadata change.\n        \"\"\"\n", "input": "", "output": "        self._subscription.assign_from_user(partitions)\n        self._client.set_topics([tp.topic for tp in partitions])", "category": "Python"}, {"instruction": "def get_int(byte_array, signed=True):\n    \"\"\"\n    Gets the specified integer from its byte array.\n    This should be used by this module alone, as it works with big endian.\n\n    :param byte_array: the byte array representing th integer.\n    :param signed: whether the number is signed or not.\n    :return: the integer representing the given byte array.\n    \"\"\"\n", "input": "", "output": "    return int.from_bytes(byte_array, byteorder='big', signed=signed)", "category": "Python"}, {"instruction": "def unbind(self, exchange, source, routing_key='', nowait=True,\n               arguments={}, ticket=None, cb=None):\n        '''\n        Unbind an exchange from another.\n        '''\n", "input": "", "output": "        nowait = nowait and self.allow_nowait() and not cb\n\n        args = Writer()\n        args.write_short(ticket or self.default_ticket).\\\n            write_shortstr(exchange).\\\n            write_shortstr(source).\\\n            write_shortstr(routing_key).\\\n            write_bit(nowait).\\\n            write_table(arguments or {})\n        self.send_frame(MethodFrame(self.channel_id, 40, 40, args))\n\n        if not nowait:\n            self._unbind_cb.append(cb)\n            self.channel.add_synchronous_cb(self._recv_unbind_ok)", "category": "Python"}, {"instruction": "def match_difficulty(self, value):\n        \"\"\"stub\"\"\"\n", "input": "", "output": "        self._my_osid_query._add_match('texts.difficulty', str(value).lower(), True)", "category": "Python"}, {"instruction": "def RgbToHsv(r, g, b):\n    '''Convert the color from RGB coordinates to HSV.\n\n    Parameters:\n      :r:\n        The Red component value [0...1]\n      :g:\n        The Green component value [0...1]\n      :b:\n        The Blue component value [0...1]\n\n    Returns:\n      The color as an (h, s, v) tuple in the range:\n      h[0...360],\n      s[0...1],\n      v[0...1]\n\n    >>> Color.RgbToHsv(1, 0.5, 0)\n    (30.0, 1.0, 1.0)\n\n    '''\n", "input": "", "output": "    v = float(max(r, g, b))\n    d = v - min(r, g, b)\n    if d==0: return (0.0, 0.0, v)\n    s = d / v\n\n    dr, dg, db = [(v - val) / d for val in (r, g, b)]\n\n    if r==v:\n      h = db - dg             # between yellow & magenta\n    elif g==v:\n      h = 2.0 + dr - db       # between cyan & yellow\n    else: # b==v\n      h = 4.0 + dg - dr       # between magenta & cyan\n\n    h = (h*60.0) % 360.0\n    return (h, s, v)", "category": "Python"}, {"instruction": "def tokenize(self):\n\n        \"\"\"\n        Tokenize the text.\n        \"\"\"\n", "input": "", "output": "\n        self.tokens = []\n        self.terms = OrderedDict()\n\n        # Generate tokens.\n        for token in utils.tokenize(self.text):\n\n            # Ignore stopwords.\n            if token['unstemmed'] in self.stopwords:\n                self.tokens.append(None)\n\n            else:\n\n                # Token:\n                self.tokens.append(token)\n\n                # Term:\n                offsets = self.terms.setdefault(token['stemmed'], [])\n                offsets.append(token['offset'])", "category": "Python"}, {"instruction": "def calc_shape_step(self, stat_names, time):\n        \"\"\"\n        Calculate shape statistics for a single time step\n\n        Args:\n            stat_names: List of shape statistics calculated from region props\n            time: Time being investigated\n\n        Returns:\n            List of shape statistics\n\n        \"\"\"\n", "input": "", "output": "        ti = np.where(self.times == time)[0][0]\n        props = regionprops(self.masks[ti], self.timesteps[ti])[0]\n        shape_stats = []\n        for stat_name in stat_names:\n            if \"moments_hu\" in stat_name:\n                hu_index = int(stat_name.split(\"_\")[-1])\n                hu_name = \"_\".join(stat_name.split(\"_\")[:-1])\n                hu_val = np.log(props[hu_name][hu_index])\n                if np.isnan(hu_val):\n                    shape_stats.append(0)\n                else:\n                    shape_stats.append(hu_val)\n            else:\n                shape_stats.append(props[stat_name])\n        return shape_stats", "category": "Python"}, {"instruction": "def move_red_left(self):\n        \"\"\"\n        Shuffle red to the left of a tree.\n        \"\"\"\n", "input": "", "output": "\n        self = self.flip()\n        if self.right is not NULL and self.right.left.red:\n            self = self._replace(right=self.right.rotate_right())\n            self = self.rotate_left().flip()\n\n        return self", "category": "Python"}, {"instruction": "def draw(self):\n        \"\"\"Draw the figures and those that are shared and have been changed\"\"\"\n", "input": "", "output": "        for fig in self.figs2draw:\n            fig.canvas.draw()\n        self._figs2draw.clear()", "category": "Python"}, {"instruction": "def _set_values_on_model(self, model, values, fields=None):\n        \"\"\"\n        Updates the values with the specified values.\n\n        :param Model model: The sqlalchemy model instance\n        :param dict values: The dictionary of attributes and\n            the values to set.\n        :param list fields: A list of strings indicating\n            the valid fields. Defaults to self.fields.\n        :return: The model with the updated\n        :rtype: Model\n        \"\"\"\n", "input": "", "output": "        fields = fields or self.fields\n        for name, val in six.iteritems(values):\n            if name not in fields:\n                continue\n            setattr(model, name, val)\n        return model", "category": "Python"}, {"instruction": "def _get_url(url):\n    \"\"\"Retrieve requested URL\"\"\"\n", "input": "", "output": "    try:\n        data = HTTP_SESSION.get(url, stream=True)\n        data.raise_for_status()\n    except requests.exceptions.RequestException as exc:\n        raise FetcherException(exc)\n\n    return data", "category": "Python"}, {"instruction": "def _set_concurrent_future_state(concurrent, source):\n    \"\"\"Copy state from a future to a concurrent.futures.Future.\"\"\"\n", "input": "", "output": "    assert source.done()\n    if source.cancelled():\n        concurrent.cancel()\n    if not concurrent.set_running_or_notify_cancel():\n        return\n    exception = source.exception()\n    if exception is not None:\n        concurrent.set_exception(exception)\n    else:\n        result = source.result()\n        concurrent.set_result(result)", "category": "Python"}, {"instruction": "def process_results(self, results=None, **value):\n        \"\"\"take results list of all events and put them in a dict\"\"\"\n", "input": "", "output": "        channels = []\n        for res in results:\n            channels.extend(res.pop('channels', '').split())\n            value.update(res)\n        value['channels'] = channels\n        value['success'] = value.get('retcode') == '318'\n        return value", "category": "Python"}, {"instruction": "def peek(self, size: int) -> memoryview:\n        \"\"\"\n        Get a view over at most ``size`` bytes (possibly fewer) at the\n        current buffer position.\n        \"\"\"\n", "input": "", "output": "        assert size > 0\n        try:\n            is_memview, b = self._buffers[0]\n        except IndexError:\n            return memoryview(b\"\")\n\n        pos = self._first_pos\n        if is_memview:\n            return typing.cast(memoryview, b[pos : pos + size])\n        else:\n            return memoryview(b)[pos : pos + size]", "category": "Python"}, {"instruction": "def addPhenotypeSearchOptions(parser):\n    \"\"\"\n    Adds options to a phenotype searches command line parser.\n    \"\"\"\n", "input": "", "output": "    parser.add_argument(\n        \"--phenotype_association_set_id\", \"-s\", default=None,\n        help=\"Only return phenotypes from this phenotype_association_set.\")\n    parser.add_argument(\n        \"--phenotype_id\", \"-p\", default=None,\n        help=\"Only return this phenotype.\")\n    parser.add_argument(\n        \"--description\", \"-d\", default=None,\n        help=\"Only return phenotypes matching this description.\")\n    parser.add_argument(\n        \"--age_of_onset\", \"-a\", default=None,\n        help=\"Only return phenotypes with this age_of_onset.\")\n    parser.add_argument(\n        \"--type\", \"-T\", default=None,\n        help=\"Only return phenotypes with this type.\")", "category": "Python"}, {"instruction": "def render_to_png(self, filename=None, dpi=72, **kwargs):\n        \"\"\"Render the graph, convert it to png and write it to filename\"\"\"\n", "input": "", "output": "        import cairosvg\n        return cairosvg.svg2png(\n            bytestring=self.render(**kwargs), write_to=filename, dpi=dpi\n        )", "category": "Python"}, {"instruction": "def get_duration(self, matrix_name):\n        \"\"\"\n        Get duration for a concrete matrix.\n\n        Args:\n            matrix_name (str): name of the Matrix.\n\n        Returns:\n            float: duration of concrete matrix in seconds.\n        \"\"\"\n", "input": "", "output": "        duration = 0.0\n        if matrix_name in self.data:\n            duration = sum([stage.duration() for stage in self.data[matrix_name]])\n        return duration", "category": "Python"}, {"instruction": "def send_margin_order(self, acc_id, amount, symbol, _type, price=0, _async=False):\n        \"\"\"\n        \u521b\u5efa\u5e76\u6267\u884c\u501f\u8d37\u8ba2\u5355\n        :param amount:\n        :param symbol:\n        :param _type: \u53ef\u9009\u503c {buy-market\uff1a\u5e02\u4ef7\u4e70, sell-market\uff1a\u5e02\u4ef7\u5356, buy-limit\uff1a\u9650\u4ef7\u4e70, sell-limit\uff1a\u9650\u4ef7\u5356}\n        :param price:\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        params = {\n            'account-id': acc_id,\n            'amount': amount,\n            'symbol': symbol,\n            'type': _type,\n            'source': 'margin-api'\n        }\n        if price:\n            params['price'] = price\n\n        path = '/v1/order/orders/place'\n        return api_key_post(params, path, _async=_async)", "category": "Python"}, {"instruction": "def disassemble(self, code, lasti=-1, file=None):\n        \"\"\"Disassemble a code object.\"\"\"\n", "input": "", "output": "        return self.disco(code, lasti, file)", "category": "Python"}, {"instruction": "def entry_modification_time(self):\n    \"\"\"dfdatetime.Filetime: entry modification time or None if not set.\"\"\"\n", "input": "", "output": "    timestamp = self._fsntfs_attribute.get_entry_modification_time_as_integer()\n    return dfdatetime_filetime.Filetime(timestamp=timestamp)", "category": "Python"}, {"instruction": "def _sample_names(files, kwargs):\n    \"\"\"\n    Make sample (or other) names.\n\n    Parameters:\n    -----------\n\n    files : list of string\n        Typically a list of file paths although could be any list of strings\n        that you want to make names for. If neither names nor define_sample_name\n        are provided, then files is returned as is.\n\n    kwargs : dict\n        kwargs from another function. Can include the following keys with\n        appropriate arguments.\n\n    names : list of strings\n        Names to use. Overrides define_sample_name if provided.\n\n    define_sample_name : function that takes string as input\n        Function mapping string to name. For instance, you may have a sample\n        name in a file path and use a regex to extract it.\n\n    \"\"\"\n", "input": "", "output": "    if 'define_sample_name' not in kwargs.keys():\n        define_sample_name = lambda x: x\n    else:\n        define_sample_name = kwargs['define_sample_name']\n    \n    if 'names' in kwargs.keys():\n        names = kwargs['names']\n    else:\n        names = [define_sample_name(f) for f in files]\n    \n    assert len(names) == len(files)\n    return names", "category": "Python"}, {"instruction": "def state_schema(module=''):\n    '''\n    Return a JSON Schema for the given state function(s)\n\n    .. versionadded:: 2016.3.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' sys.state_schema\n        salt '*' sys.state_schema pkg.installed\n    '''\n", "input": "", "output": "    specs = state_argspec(module)\n\n    schemas = []\n    for state_mod, state_spec in specs.items():\n        schemas.append(_argspec_to_schema(state_mod, state_spec))\n\n    return schemas", "category": "Python"}, {"instruction": "def _get_sm_scale_in(self, scale_sm=91.1876):\n        \"\"\"Get an estimate of the SM parameters at the input scale by running\n        them from the EW scale using constant values for the Wilson coefficients\n        (corresponding to their leading log approximated values at the EW\n        scale).\n\n        Note that this is not guaranteed to work and will fail if some of the\n        Wilson coefficients (the ones affecting the extraction of SM parameters)\n        are large.\"\"\"\n", "input": "", "output": "        # intialize a copy of ourselves\n        _smeft = SMEFT()\n        _smeft.set_initial(self.C_in, self.scale_in, self.scale_high)\n        # Step 1: run the SM up, using the WCs at scale_input as (constant) estimate\n        _smeft.C_in.update(self._run_sm_scale_in(self.C_in, scale_sm=scale_sm))\n        # Step 2: run the WCs down in LL approximation\n        C_out = _smeft.rgevolve_leadinglog(scale_sm)\n        # Step 3: run the SM up again, this time using the WCs at scale_sm as (constant) estimate\n        return self._run_sm_scale_in(C_out, scale_sm=scale_sm)", "category": "Python"}, {"instruction": "def find_usage(self):\n        \"\"\"\n        Determine the current usage for each limit of this service,\n        and update corresponding Limit via\n        :py:meth:`~.AwsLimit._add_current_usage`.\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect_resource()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._find_usage_dynamodb()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "category": "Python"}, {"instruction": "def send_change_notification(hub, topic_url, updated_content=None):\n    \"\"\"7. Content Distribution\"\"\"\n", "input": "", "output": "\n    if updated_content:\n        body = base64.b64decode(updated_content['content'])\n    else:\n        body, updated_content = get_new_content(hub.config, topic_url)\n    b64_body = updated_content['content']\n\n    headers = updated_content['headers']\n    link_header = headers.get('Link', '')\n    if 'rel=\"hub\"' not in link_header or 'rel=\"self\"' not in link_header:\n        raise NotificationError(INVALID_LINK)\n\n    for callback_url, secret in hub.storage.get_callbacks(topic_url):\n        schedule_request(hub, topic_url, callback_url, secret, body, b64_body,\n                         headers)", "category": "Python"}, {"instruction": "def get_es(**overrides):\n    \"\"\"Return a elasticsearch Elasticsearch object using settings\n    from ``settings.py``.\n\n    :arg overrides: Allows you to override defaults to create the\n        ElasticSearch object. You can override any of the arguments\n        isted in :py:func:`elasticutils.get_es`.\n\n    For example, if you wanted to create an ElasticSearch with a\n    longer timeout to a different cluster, you'd do:\n\n    >>> from elasticutils.contrib.django import get_es\n    >>> es = get_es(urls=['http://some_other_cluster:9200'], timeout=30)\n\n    \"\"\"\n", "input": "", "output": "    defaults = {\n        'urls': settings.ES_URLS,\n        'timeout': getattr(settings, 'ES_TIMEOUT', 5)\n        }\n\n    defaults.update(overrides)\n    return base_get_es(**defaults)", "category": "Python"}, {"instruction": "def alias_assessment(self, assessment_id, alias_id):\n        \"\"\"Adds an ``Id`` to an ``Assessment`` for the purpose of creating compatibility.\n\n        The primary ``Id`` of the ``Assessment`` is determined by the\n        provider. The new ``Id`` is an alias to the primary ``Id``. If\n        the alias is a pointer to another assessment, it is reassigned\n        to the given assessment ``Id``.\n\n        arg:    assessment_id (osid.id.Id): the ``Id`` of an\n                ``Assessment``\n        arg:    alias_id (osid.id.Id): the alias ``Id``\n        raise:  AlreadyExists - ``alias_id`` is in use as a primary\n                ``Id``\n        raise:  NotFound - ``assessment_id`` not found\n        raise:  NullArgument - ``assessment_id`` or ``alias_id`` is\n                ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure occurred\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.ResourceAdminSession.alias_resources_template\n        self._alias_id(primary_id=assessment_id, equivalent_id=alias_id)", "category": "Python"}, {"instruction": "def visit_DictComp(self, node: ast.DictComp) -> Any:\n        \"\"\"Compile the dictionary comprehension as a function and call it.\"\"\"\n", "input": "", "output": "        result = self._execute_comprehension(node=node)\n\n        for generator in node.generators:\n            self.visit(generator.iter)\n\n        self.recomputed_values[node] = result\n        return result", "category": "Python"}, {"instruction": "def datetime(\n        self, year, month, day, hour=0, minute=0, second=0, microsecond=0\n    ):  # type: (int, int, int, int, int, int, int) -> datetime\n        \"\"\"\n        Return a normalized datetime for the current timezone.\n        \"\"\"\n", "input": "", "output": "        if _HAS_FOLD:\n            return self.convert(\n                datetime(year, month, day, hour, minute, second, microsecond, fold=1)\n            )\n\n        return self.convert(\n            datetime(year, month, day, hour, minute, second, microsecond),\n            dst_rule=POST_TRANSITION,\n        )", "category": "Python"}, {"instruction": "def get_rows(self, match, fields=None, limit=10, sampling=None):\n        \"\"\"\n        Returns raw rows that matches given query\n\n        :arg match: query to be run against Kibana log messages (ex. {\"@message\": \"Foo Bar DB queries\"})\n        :type fields list[str] or None\n        :arg limit: the number of results (defaults to 10)\n        :type sampling int or None\n        :arg sampling: Percentage of results to be returned (0,100)\n        \"\"\"\n", "input": "", "output": "        query = {\n            \"match\": match,\n        }\n\n        return self._search(query, fields, limit, sampling)", "category": "Python"}, {"instruction": "async def disconnect(self):\n        \"\"\"\n        Disconnects all players.\n        \"\"\"\n", "input": "", "output": "        for p in tuple(self.players):\n            await p.disconnect(requested=False)\n        log.debug(\"Disconnected players.\")", "category": "Python"}, {"instruction": "def update_config(self, cluster_config, login_config):\n        \"\"\"Update current configuration.\n\n        This method is usually called after loading a `Cluster`\n        instance from a persistent storage. Note that not all fields\n        are actually updated, but only those that can be safely\n        updated.\n        \"\"\"\n", "input": "", "output": "\n        oldvalue = self.__update_option(cluster_config, 'ssh_to', 'ssh_to')\n        if oldvalue:\n            log.debug(\"Attribute 'ssh_to' updated: %s -> %s\", oldvalue, self.ssh_to)", "category": "Python"}, {"instruction": "def successors(self, *args, **kwargs):\n        \"\"\"\n        Perform execution using any applicable engine. Enumerate the current engines and use the\n        first one that works. Return a SimSuccessors object classifying the results of the run.\n\n        :param state:           The state to analyze\n        :param addr:            optional, an address to execute at instead of the state's ip\n        :param jumpkind:        optional, the jumpkind of the previous exit\n        :param inline:          This is an inline execution. Do not bother copying the state.\n\n        Additional keyword arguments will be passed directly into each engine's process method.\n        \"\"\"\n", "input": "", "output": "\n        return self.project.engines.successors(*args, **kwargs)", "category": "Python"}, {"instruction": "def press(self):\n    \"\"\"Triggers a simulated button press to the Keypad.\"\"\"\n", "input": "", "output": "    self._lutron.send(Lutron.OP_EXECUTE, Keypad._CMD_TYPE, self._keypad.id,\n                      self.component_number, Button._ACTION_PRESS)", "category": "Python"}, {"instruction": "def ints_to_string(ints):\n    \"\"\"Convert a list of integers to a *|* separated string.\n\n    Args:\n        ints (list[int]|int): List of integer items to convert or single\n            integer to convert.\n\n    Returns:\n        str: Formatted string\n    \"\"\"\n", "input": "", "output": "    if not isinstance(ints, list):\n        return six.u(str(ints))\n\n    return '|'.join(six.u(str(l)) for l in ints)", "category": "Python"}, {"instruction": "def fit(self, labels, samples, pstates):\n        \"\"\"\n        Fit the classifier with labels y and observations X\n        \"\"\"\n", "input": "", "output": "        assert len(labels) == len(samples) == len(pstates)\n\n        for label in set(labels):\n            label_samples = [s for l,s in zip(labels, samples) if l == label]\n            label_pstates = [p for l,p in zip(labels, pstates) if l == label]\n\n            pohmm = self.pohmm_factory()\n            pohmm.fit(label_samples, label_pstates)\n            self.pohmms[label] = pohmm\n\n        return self", "category": "Python"}, {"instruction": "def get_offset(name):\n    \"\"\"\n    Return DateOffset object associated with rule name\n\n    Examples\n    --------\n    get_offset('EOM') --> BMonthEnd(1)\n    \"\"\"\n", "input": "", "output": "    if name not in libfreqs._dont_uppercase:\n        name = name.upper()\n        name = libfreqs._lite_rule_alias.get(name, name)\n        name = libfreqs._lite_rule_alias.get(name.lower(), name)\n    else:\n        name = libfreqs._lite_rule_alias.get(name, name)\n\n    if name not in _offset_map:\n        try:\n            split = name.split('-')\n            klass = prefix_mapping[split[0]]\n            # handles case where there's no suffix (and will TypeError if too\n            # many '-')\n            offset = klass._from_name(*split[1:])\n        except (ValueError, TypeError, KeyError):\n            # bad prefix or suffix\n            raise ValueError(libfreqs.INVALID_FREQ_ERR_MSG.format(name))\n        # cache\n        _offset_map[name] = offset\n\n    return _offset_map[name]", "category": "Python"}, {"instruction": "def pop_all(self):\n        \"\"\" Preserve the context stack by transferring it to a new instance \"\"\"\n", "input": "", "output": "        ret = ExitStack()\n        ret._context_stack.append(self._context_stack.pop())\n        self._context_stack.append([])", "category": "Python"}, {"instruction": "def done(self, on_success=None, on_failure=None):\n        \"\"\"Attaches some callbacks to the promise and returns the promise.\"\"\"\n", "input": "", "output": "        if on_success is not None:\n            if self._state == 'pending':\n                self._callbacks.append(on_success)\n            elif self._state == 'resolved':\n                on_success(self.value)\n        if on_failure is not None:\n            if self._state == 'pending':\n                self._errbacks.append(on_failure)\n            elif self._state == 'rejected':\n                on_failure(self.reason)\n        return self", "category": "Python"}, {"instruction": "def get_database(self, name):\n        \"\"\"\n        Finds the database in this instance with the specified name, and\n        returns a CloudDatabaseDatabase object. If no match is found, a\n        NoSuchDatabase exception is raised.\n        \"\"\"\n", "input": "", "output": "        try:\n            return [db for db in self.list_databases()\n                    if db.name == name][0]\n        except IndexError:\n            raise exc.NoSuchDatabase(\"No database by the name '%s' exists.\" %\n                    name)", "category": "Python"}, {"instruction": "def coastal_edges(tile_id):\n    \"\"\"\n    Returns a list of coastal edge coordinate.\n\n    An edge is coastal if it is on the grid's border.\n    :return: list(int)\n    \"\"\"\n", "input": "", "output": "    edges = list()\n    tile_coord = tile_id_to_coord(tile_id)\n    for edge_coord in edges_touching_tile(tile_id):\n        dirn = tile_edge_offset_to_direction(edge_coord - tile_coord)\n        if tile_id_in_direction(tile_id, dirn) is None:\n            edges.append(edge_coord)\n    return edges", "category": "Python"}, {"instruction": "def max_width(*args, **kwargs):\n    \"\"\"Returns formatted text or context manager for textui:puts.\n\n        >>> from clint.textui import puts, max_width\n        >>> max_width('123 5678', 8)\n        '123 5678'\n        >>> max_width('123 5678', 7)\n        '123 \\n5678'\n        >>> with max_width(7):\n        ...     puts('123 5678')\n        '123 \\n5678'\n    \"\"\"\n", "input": "", "output": "    args = list(args)\n\n    if not args:\n        args.append(kwargs.get('string'))\n        args.append(kwargs.get('cols'))\n        args.append(kwargs.get('separator'))\n    elif len(args) == 1:\n        args.append(kwargs.get('cols'))\n        args.append(kwargs.get('separator'))\n    elif len(args) == 2:\n        args.append(kwargs.get('separator'))\n\n    string, cols, separator = args\n    if separator is None:\n        separator = '\\n'  # default value\n    if cols is None:\n        # cols should be specified vitally\n        # because string can be specified at textui:puts function\n        string, cols = cols, string\n\n    if string is None:\n        MAX_WIDTHS.append((cols, separator))\n        return _max_width_context()\n    else:\n        return _max_width_formatter(string, cols, separator)", "category": "Python"}, {"instruction": "def modelstandardfunctions(self):\n        \"\"\"Standard functions of the model class.\"\"\"\n", "input": "", "output": "        lines = Lines()\n        lines.extend(self.doit)\n        lines.extend(self.iofunctions)\n        lines.extend(self.new2old)\n        lines.extend(self.run)\n        lines.extend(self.update_inlets)\n        lines.extend(self.update_outlets)\n        lines.extend(self.update_receivers)\n        lines.extend(self.update_senders)\n        return lines", "category": "Python"}, {"instruction": "def _minimum_one_is_missing(self, **kwargs):\n        \"\"\"Helper function to do operation on sets\n\n        Verify if at least one of the elements\n        is present in **kwargs. If no items of rqset\n        are contained in **kwargs  the function\n        raises exception.\n\n        This check will only trigger if rqset is not empty.\n\n        Raises:\n             MissingRequiredCreationParameter\n        \"\"\"\n", "input": "", "output": "        rqset = self._meta_data['minimum_additional_parameters']\n        if rqset:\n            kwarg_set = set(iterkeys(kwargs))\n            if kwarg_set.isdisjoint(rqset):\n                args = sorted(rqset)\n                error_message = 'This resource requires at least one of the ' \\\n                                'mandatory additional ' \\\n                                'parameters to be provided: %s' % ', '.join(args)\n                raise MissingRequiredCreationParameter(error_message)", "category": "Python"}, {"instruction": "def deliver_hook(instance, target, payload_override=None):\n    \"\"\"\n    Deliver the payload to the target URL.\n\n    By default it serializes to JSON and POSTs.\n    \"\"\"\n", "input": "", "output": "    payload = payload_override or serialize_hook(instance)\n    if hasattr(settings, 'HOOK_DELIVERER'):\n        deliverer = get_module(settings.HOOK_DELIVERER)\n        deliverer(target, payload, instance=instance)\n    else:\n        client.post(\n            url=target,\n            data=json.dumps(payload, cls=serializers.json.DjangoJSONEncoder),\n            headers={'Content-Type': 'application/json'}\n        )\n\n    return None", "category": "Python"}, {"instruction": "def normalize_mesh(mesh):\n  '''Scale mesh to fit into -1..1 cube'''\n", "input": "", "output": "  mesh = dict(mesh)\n  pos = mesh['position'][:,:3].copy()\n  pos -= (pos.max(0)+pos.min(0)) / 2.0\n  pos /= np.abs(pos).max()\n  mesh['position'] = pos\n  return mesh", "category": "Python"}, {"instruction": "def rename_datastore(datastore_ref, new_datastore_name):\n    '''\n    Renames a datastore\n\n    datastore_ref\n        vim.Datastore reference to the datastore object to be changed\n\n    new_datastore_name\n        New datastore name\n    '''\n", "input": "", "output": "    ds_name = get_managed_object_name(datastore_ref)\n    log.trace(\"Renaming datastore '%s' to '%s'\", ds_name, new_datastore_name)\n    try:\n        datastore_ref.RenameDatastore(new_datastore_name)\n    except vim.fault.NoPermission as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(\n            'Not enough permissions. Required privilege: '\n            '{}'.format(exc.privilegeId))\n    except vim.fault.VimFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareApiError(exc.msg)\n    except vmodl.RuntimeFault as exc:\n        log.exception(exc)\n        raise salt.exceptions.VMwareRuntimeError(exc.msg)", "category": "Python"}, {"instruction": "def syncItems(self, client=None, clientId=None):\n        \"\"\" Returns an instance of :class:`plexapi.sync.SyncList` for specified client.\n\n            Parameters:\n                client (:class:`~plexapi.myplex.MyPlexDevice`): a client to query SyncItems for.\n                clientId (str): an identifier of a client to query SyncItems for.\n\n            If both `client` and `clientId` provided the client would be preferred.\n            If neither `client` nor `clientId` provided the clientId would be set to current clients`s identifier.\n        \"\"\"\n", "input": "", "output": "        if client:\n            clientId = client.clientIdentifier\n        elif clientId is None:\n            clientId = X_PLEX_IDENTIFIER\n\n        data = self.query(SyncList.key.format(clientId=clientId))\n\n        return SyncList(self, data)", "category": "Python"}, {"instruction": "def df(self):\n        \"\"\"Return dict with size of Ya.Disk. Keys: 'available', 'used'.\"\"\"\n", "input": "", "output": "\n        def parseContent(content):\n            root = ET.fromstring(content)\n            return {\n                'available': root.find(\".//d:quota-available-bytes\", namespaces=self.namespaces).text, \n                'used': root.find(\".//d:quota-used-bytes\", namespaces=self.namespaces).text\n            }\n\n        data = ", "category": "Python"}, {"instruction": "def format_error(status=None, title=None, detail=None, code=None):\n    '''Formatting JSON API Error Object\n    Constructing an error object based on JSON API standard\n    ref: http://jsonapi.org/format/#error-objects\n    Args:\n        status: Can be a http status codes\n        title: A summary of error\n        detail: A descriptive error message\n        code: Application error codes (if any)\n    Returns:\n        A dictionary contains of status, title, detail and code\n    '''\n", "input": "", "output": "    error = {}\n    error.update({ 'title': title })\n\n    if status is not None:\n        error.update({ 'status': status })\n\n    if detail is not None:\n        error.update({ 'detail': detail })\n\n    if code is not None:\n        error.update({ 'code': code })\n\n    return error", "category": "Python"}, {"instruction": "def insertPreviousCommand(self):\r\n        \"\"\"\r\n        Inserts the previous command from history into the line.\r\n        \"\"\"\n", "input": "", "output": "        self._currentHistoryIndex -= 1\r\n        if 0 <= self._currentHistoryIndex < len(self._history):\r\n            cmd = self._history[self._currentHistoryIndex]\r\n        else:\r\n            cmd = '>>> '\r\n            self._currentHistoryIndex = len(self._history)\r\n        \r\n        self.replaceCommand(cmd)", "category": "Python"}, {"instruction": "def _set_optimal_area(self, data):\n        \"\"\"\n        Reduce the zone to reduce the size of fetched data on refresh\n        \"\"\"\n", "input": "", "output": "        lats = [station[\"latitude\"] for station in data.values()]\n        longs = [station[\"longitude\"] for station in data.values()]\n        self.gps.update(\n            {\n                \"gpsTopLatitude\": max(lats),\n                \"gpsTopLongitude\": max(longs),\n                \"gpsBotLatitude\": min(lats),\n                \"gpsBotLongitude\": min(longs),\n            }\n        )", "category": "Python"}, {"instruction": "def _create_worker(self, method, *args, **kwargs):\n        \"\"\"Create a new worker instance.\"\"\"\n", "input": "", "output": "        thread = QThread()\n        worker = RequestsDownloadWorker(method, args, kwargs)\n        worker.moveToThread(thread)\n        worker.sig_finished.connect(self._start)\n        self._sig_download_finished.connect(worker.sig_download_finished)\n        self._sig_download_progress.connect(worker.sig_download_progress)\n        worker.sig_finished.connect(thread.quit)\n        thread.started.connect(worker.start)\n        self._queue.append(thread)\n        self._threads.append(thread)\n        self._workers.append(worker)\n        self._start()\n        return worker", "category": "Python"}, {"instruction": "def read(self, include_deleted=False):\n        \"\"\"Return only read items in the current queryset\"\"\"\n", "input": "", "output": "        if is_soft_delete() and not include_deleted:\n            return self.filter(unread=False, deleted=False)\n\n        # When SOFT_DELETE=False, developers are supposed NOT to touch 'deleted' field.\n        # In this case, to improve query performance, don't filter by 'deleted' field\n        return self.filter(unread=False)", "category": "Python"}, {"instruction": "def draw_no_data(self):\n        \"\"\"Write the no data text to the svg\"\"\"\n", "input": "", "output": "        no_data = self.node(\n            self.graph.nodes['text_overlay'],\n            'text',\n            x=self.graph.view.width / 2,\n            y=self.graph.view.height / 2,\n            class_='no_data'\n        )\n        no_data.text = self.graph.no_data_text", "category": "Python"}, {"instruction": "def register(cls, use_admin=True):\n    \"\"\"Register with the API a :class:`sandman.model.Model` class and\n    associated endpoint.\n\n    :param cls: User-defined class derived from :class:`sandman.model.Model` to\n                be registered with the endpoint returned by :func:`endpoint()`\n    :type cls: :class:`sandman.model.Model` or tuple\n\n    \"\"\"\n", "input": "", "output": "    with app.app_context():\n        if getattr(current_app, 'class_references', None) is None:\n            current_app.class_references = {}\n        if isinstance(cls, (list, tuple)):\n            for entry in cls:\n                register_internal_data(entry)\n                entry.use_admin = use_admin\n        else:\n            register_internal_data(cls)\n            cls.use_admin = use_admin", "category": "Python"}, {"instruction": "def set_map_alpha(alpha):\n    \"\"\"\n    Alpha color of the map tiles\n\n    :param alpha: int between 0 and 255. 0 is completely dark, 255 is full brightness\n    \"\"\"\n", "input": "", "output": "    if alpha < 0 or alpha > 255:\n        raise Exception('invalid alpha '  + str(alpha))\n    _global_config.map_alpha = alpha", "category": "Python"}, {"instruction": "def set_attribute_label(span, resource_type, resource_labels, attribute_key,\n                        canonical_key=None, label_value_prefix=''):\n    \"\"\"Set a label to span that can be used for tracing.\n    :param span: Span object\n    :param resource_type: resource type\n    :param resource_labels: collection of labels\n    :param attribute_key: actual label key\n    :param canonical_key: exporter specific label key, Optional\n    :param label_value_prefix: exporter specific label value prefix, Optional\n    \"\"\"\n", "input": "", "output": "\n    if attribute_key in resource_labels:\n        if canonical_key is None:\n            canonical_key = attribute_key\n\n        pair = {RESOURCE_LABEL % (resource_type, canonical_key):\n                label_value_prefix + resource_labels[attribute_key]\n                }\n        pair_attrs = Attributes(pair).format_attributes_json()\\\n            .get('attributeMap')\n\n        _update_attr_map(span, pair_attrs)", "category": "Python"}, {"instruction": "def is_js_date_utc(json):\n        \"\"\"Check if the string contains Date.UTC function \n        and return match group(s) if there is\n        \"\"\"\n", "input": "", "output": "        \n        JS_date_utc_pattern = r'Date\\.UTC\\(([0-9]+,[0-9]+,[0-9]+)(,[0-9]+,[0-9]+,[0-9]+)?(,[0-9]+)?\\)'\n        re_date = re.compile(JS_date_utc_pattern, re.M)\n\n        if re_date.search(json):\n            return re_date.search(json).group(0)\n        else:\n            return False", "category": "Python"}, {"instruction": "def get_stored_files(self):\n        \"\"\"Check which files are in your temporary storage.\"\"\"\n", "input": "", "output": "        method = 'GET'\n        endpoint = '/rest/v1/storage/{}'.format(self.client.sauce_username)\n        return self.client.request(method, endpoint)", "category": "Python"}, {"instruction": "def split_long_sentence(sentence, words_per_line):\n    \"\"\"Takes a sentence and adds a newline every \"words_per_line\" words.\n\n    Parameters\n    ----------\n    sentence: str\n        Sentene to split\n    words_per_line: double\n        Add a newline every this many words\n    \"\"\"\n", "input": "", "output": "    words = sentence.split(' ')\n    split_sentence = ''\n    for i in range(len(words)):\n        split_sentence = split_sentence + words[i]\n        if (i+1) % words_per_line == 0:\n            split_sentence = split_sentence + '\\n'\n        elif i != len(words) - 1:\n            split_sentence = split_sentence + \" \"\n    return split_sentence", "category": "Python"}, {"instruction": "def power_on_vm(name, datacenter=None, service_instance=None):\n    '''\n    Powers on a virtual machine specified by it's name.\n\n    name\n        Name of the virtual machine\n\n    datacenter\n        Datacenter of the virtual machine\n\n    service_instance\n        Service instance (vim.ServiceInstance) of the vCenter.\n        Default is None.\n\n    .. code-block:: bash\n\n        salt '*' vsphere.power_on_vm name=my_vm\n\n    '''\n", "input": "", "output": "    log.trace('Powering on virtual machine %s', name)\n    vm_properties = [\n        'name',\n        'summary.runtime.powerState'\n    ]\n    virtual_machine = salt.utils.vmware.get_vm_by_property(\n        service_instance,\n        name,\n        datacenter=datacenter,\n        vm_properties=vm_properties)\n    if virtual_machine['summary.runtime.powerState'] == 'poweredOn':\n        result = {'comment': 'Virtual machine is already powered on',\n                  'changes': {'power_on': True}}\n        return result\n    salt.utils.vmware.power_cycle_vm(virtual_machine['object'], action='on')\n    result = {'comment': 'Virtual machine power on action succeeded',\n              'changes': {'power_on': True}}\n    return result", "category": "Python"}, {"instruction": "def filter_cat(self, axis, cat_index, cat_name):\n    '''\n    Filter the matrix based on their category. cat_index is the index of the category, the first category has index=1.\n    '''\n", "input": "", "output": "    run_filter.filter_cat(self, axis, cat_index, cat_name)", "category": "Python"}, {"instruction": "def validate_frame_length(frame_length, algorithm):\n    \"\"\"Validates that frame length is within the defined limits and is compatible with the selected algorithm.\n\n    :param int frame_length: Frame size in bytes\n    :param algorithm: Algorithm to use for encryption\n    :type algorithm: aws_encryption_sdk.identifiers.Algorithm\n    :raises SerializationError: if frame size is negative or not a multiple of the algorithm block size\n    :raises SerializationError: if frame size is larger than the maximum allowed frame size\n    \"\"\"\n", "input": "", "output": "    if frame_length < 0 or frame_length % algorithm.encryption_algorithm.block_size != 0:\n        raise SerializationError(\n            \"Frame size must be a non-negative multiple of the block size of the crypto algorithm: {block_size}\".format(\n                block_size=algorithm.encryption_algorithm.block_size\n            )\n        )\n    if frame_length > aws_encryption_sdk.internal.defaults.MAX_FRAME_SIZE:\n        raise SerializationError(\n            \"Frame size too large: {frame} > {max}\".format(\n                frame=frame_length, max=aws_encryption_sdk.internal.defaults.MAX_FRAME_SIZE\n            )\n        )", "category": "Python"}, {"instruction": "def get_last_weekday(self, including_today=False):\n        \"\"\"Gets last week day\n\n        :param including_today: If today is sunday and requesting next sunday\n        :return: Date of last monday, tuesday ..\n        \"\"\"\n", "input": "", "output": "        weekday = self.date_time.weekday()\n        return Weekday.get_last(weekday, including_today=including_today)", "category": "Python"}, {"instruction": "def get_field(self, path, name):\n        \"\"\"\n        Retrieves the value of the field at the specified path.\n\n        :param path: str or Path instance\n        :param name:\n        :type name: str\n        :return:\n        :raises ValueError: A component of path is a field name.\n        :raises KeyError: A component of path doesn't exist.\n        :raises TypeError: The field name is a component of a path.\n        \"\"\"\n", "input": "", "output": "        try:\n            value = self.get(path, name)\n            if not isinstance(value, str):\n                raise TypeError()\n            return value\n        except KeyError:\n            raise KeyError()", "category": "Python"}, {"instruction": "def addIndividual(self):\n        \"\"\"\n        Adds a new individual into this repo\n        \"\"\"\n", "input": "", "output": "        self._openRepo()\n        dataset = self._repo.getDatasetByName(self._args.datasetName)\n        individual = bio_metadata.Individual(\n            dataset, self._args.individualName)\n        individual.populateFromJson(self._args.individual)\n        self._updateRepo(self._repo.insertIndividual, individual)", "category": "Python"}, {"instruction": "def units(cls, scale=1):\n        '''\n        :scale: optional integer scaling factor\n        :return: list of three Point subclass\n\n        Returns three points whose coordinates are the head of a\n        unit vector from the origin ( conventionally i, j and k).\n\n        '''\n", "input": "", "output": "        return [cls(x=scale), cls(y=scale), cls(z=scale)]", "category": "Python"}, {"instruction": "def _cleanupConnections(senderkey, signal):\n    \"\"\"Delete any empty signals for senderkey. Delete senderkey if empty.\"\"\"\n", "input": "", "output": "    try:\n        receivers = connections[senderkey][signal]\n    except:\n        pass\n    else:\n        if not receivers:\n            # No more connected receivers. Therefore, remove the signal.\n            try:\n                signals = connections[senderkey]\n            except KeyError:\n                pass\n            else:\n                del signals[signal]\n                if not signals:\n                    # No more signal connections. Therefore, remove the sender.\n                    _removeSender(senderkey)", "category": "Python"}, {"instruction": "def hangup(self):\n        \"\"\" End the phone call.\n        \n        Does nothing if the call is already inactive.\n        \"\"\"\n", "input": "", "output": "        if self.active:\n            self._gsmModem.write('ATH')\n            self.answered = False\n            self.active = False\n        if self.id in self._gsmModem.activeCalls:\n            del self._gsmModem.activeCalls[self.id]", "category": "Python"}, {"instruction": "def is_depsignal_handler(class_, signal_name, cb, *, defer=False):\n    \"\"\"\n    Return true if `cb` has been decorated with :func:`depsignal` for the given\n    signal, class and connection mode.\n    \"\"\"\n", "input": "", "output": "    try:\n        handlers = get_magic_attr(cb)\n    except AttributeError:\n        return False\n\n    return _depsignal_spec(class_, signal_name, cb, defer) in handlers", "category": "Python"}, {"instruction": "def _execute_callback(async, callback):\n    \"\"\"Execute the given callback or insert the Async callback, or if no\n    callback is given return the async.result.\n    \"\"\"\n", "input": "", "output": "    from furious.async import Async\n\n    if not callback:\n        return async.result.payload\n\n    if isinstance(callback, Async):\n        return callback.start()\n\n    return callback()", "category": "Python"}, {"instruction": "def remove_elements(target, indices):\n    \"\"\"Remove multiple elements from a list and return result.\n    This implementation is faster than the alternative below.\n    Also note the creation of a new list to avoid altering the\n    original. We don't have any current use for the original\n    intact list, but may in the future...\"\"\"\n", "input": "", "output": "\n    copied = list(target)\n\n    for index in reversed(indices):\n        del copied[index]\n    return copied", "category": "Python"}, {"instruction": "def _commit(self, session, errorMessage):\n        \"\"\"\n        Custom commit function for file objects\n        \"\"\"\n", "input": "", "output": "        try:\n            session.commit()\n        except IntegrityError:\n            # Raise special error if the commit fails due to empty files\n            log.error('Commit to database failed. %s' % errorMessage)\n        except:\n            # Raise other errors as normal\n            raise", "category": "Python"}, {"instruction": "def find_all(self, tagtype):\n        ''' Find all token-level tags with the specified tagtype '''\n", "input": "", "output": "        return [t for t in self.__tags if t.tagtype == tagtype]", "category": "Python"}, {"instruction": "def parse(self, values):\n    \"\"\"Override existing hyperparameter values, parsing new values from a string.\n\n    See parse_values for more detail on the allowed format for values.\n\n    Args:\n      values: String.  Comma separated list of `name=value` pairs where 'value'\n        must follow the syntax described above.\n\n    Returns:\n      The `HParams` instance.\n\n    Raises:\n      ValueError: If `values` cannot be parsed or a hyperparameter in `values`\n      doesn't exist.\n    \"\"\"\n", "input": "", "output": "    type_map = {}\n    for name, t in self._hparam_types.items():\n      param_type, _ = t\n      type_map[name] = param_type\n\n    values_map = parse_values(values, type_map)\n    return self.override_from_dict(values_map)", "category": "Python"}, {"instruction": "def parse_host_address(addr):\n    \"\"\"\n    parse host address to get domain name or ipv4/v6 address,\n    cidr prefix and net mask code string if given a subnet address\n\n    :param addr:\n    :type addr: str\n    :return: parsed domain name/ipv4 address/ipv6 address,\n             cidr prefix if there is,\n             net mask code string if there is\n    :rtype: (string, int, string)\n    \"\"\"\n", "input": "", "output": "\n    if addr.startswith('[') and addr.endswith(']'):\n        addr = addr[1:-1]\n\n    parts = addr.split('/')\n    if len(parts) == 1:\n        return parts[0], None, None\n    if len(parts) > 2:\n        raise ValueError(\"Illegal host address\")\n    else:\n        domain_or_ip, prefix = parts\n        prefix = int(prefix)\n        if re.match(r\"^(?:[0-9]{1,3}\\.){3}[0-9]{1,3}$\", domain_or_ip):\n            return domain_or_ip, prefix, ipv4_prefix_to_mask(prefix)\n        elif ':' in domain_or_ip:\n            return domain_or_ip, prefix, ipv6_prefix_to_mask(prefix)\n        else:\n            return domain_or_ip, None, None", "category": "Python"}, {"instruction": "def get_config(self, bot_label, config_name=None, require_ready=True):\n        \"\"\"\n        Return the config matching the given bot_label and config_name.\n        config_name is case-insensitive.\n        Raise LookupError if no bot exists with this label, or no config\n        exists with this name in the bot. Raise ValueError if called with a\n        single argument that doesn't contain exactly one dot.\n        \"\"\"\n", "input": "", "output": "        if require_ready:\n            self.check_configs_ready()\n        else:\n            self.check_bots_ready()\n\n        if config_name is None:\n            config_name = defaults.BOT_CONFIG\n\n        bot = self.get_bot(bot_label)\n\n        if not require_ready and bot.configs is None:\n            bot.import_configs()\n\n        return bot.get_config(config_name, require_ready=require_ready)", "category": "Python"}, {"instruction": "def strand(s1, s2):\n    \"\"\"\n    Returns the binary AND of the 2 provided strings s1 and s2. s1 and s2\n    must be of same length.\n    \"\"\"\n", "input": "", "output": "    return \"\".join(map(lambda x,y:chr(ord(x)&ord(y)), s1, s2))", "category": "Python"}, {"instruction": "def _tag_net_direction(data):\n        \"\"\"Create a tag based on the direction of the traffic\"\"\"\n", "input": "", "output": "\n        # IP or IPv6\n        src = data['packet']['src_domain']\n        dst = data['packet']['dst_domain']\n        if src == 'internal':\n            if dst == 'internal' or 'multicast' in dst or 'broadcast' in dst:\n                return 'internal'\n            else:\n                return 'outgoing'\n        elif dst == 'internal':\n            return 'incoming'\n        else:\n            return None", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self, 'pdf') and self.pdf is not None:\n            _dict['pdf'] = self.pdf._to_dict()\n        if hasattr(self, 'word') and self.word is not None:\n            _dict['word'] = self.word._to_dict()\n        if hasattr(self, 'html') and self.html is not None:\n            _dict['html'] = self.html._to_dict()\n        if hasattr(self, 'segment') and self.segment is not None:\n            _dict['segment'] = self.segment._to_dict()\n        if hasattr(\n                self,\n                'json_normalizations') and self.json_normalizations is not None:\n            _dict['json_normalizations'] = [\n                x._to_dict() for x in self.json_normalizations\n            ]\n        return _dict", "category": "Python"}, {"instruction": "def create_share(ase, containers_created, timeout=None):\n    # type: (blobxfer.models.azure.StorageEntity, dict, int) -> None\n    \"\"\"Create file share\n    :param blobxfer.models.azure.StorageEntity ase: Azure StorageEntity\n    :param dict containers_created: containers already created map\n    :param int timeout: timeout\n    \"\"\"\n", "input": "", "output": "    # check if auth allows create container\n    if not ase.can_create_containers:\n        return\n    key = ase.client.account_name + ':file=' + ase.container\n    if key in containers_created:\n        return\n    if ase.client.create_share(\n            share_name=ase.container,\n            fail_on_exist=False,\n            timeout=timeout):\n        logger.info('created file share {} on storage account {}'.format(\n            ase.container, ase.client.account_name))\n    # always add to set (as it could be pre-existing)\n    containers_created.add(key)", "category": "Python"}, {"instruction": "def _check_avail(cmd):\n    '''\n    Check to see if the given command can be run\n    '''\n", "input": "", "output": "    if isinstance(cmd, list):\n        cmd = ' '.join([six.text_type(x) if not isinstance(x, six.string_types) else x\n                        for x in cmd])\n    bret = True\n    wret = False\n    if __salt__['config.get']('cmd_blacklist_glob'):\n        blist = __salt__['config.get']('cmd_blacklist_glob', [])\n        for comp in blist:\n            if fnmatch.fnmatch(cmd, comp):\n                # BAD! you are blacklisted\n                bret = False\n    if __salt__['config.get']('cmd_whitelist_glob', []):\n        blist = __salt__['config.get']('cmd_whitelist_glob', [])\n        for comp in blist:\n            if fnmatch.fnmatch(cmd, comp):\n                # GOOD! You are whitelisted\n                wret = True\n                break\n    else:\n        # If no whitelist set then alls good!\n        wret = True\n    return bret and wret", "category": "Python"}, {"instruction": "def read_pcap_from_source(self):\n        \"\"\"\n        Return a FileStream of the Pcap from the compute node\n        \"\"\"\n", "input": "", "output": "        if self._capture_node:\n            compute = self._capture_node[\"node\"].compute\n            return compute.stream_file(self._project, \"tmp/captures/\" + self._capture_file_name)", "category": "Python"}, {"instruction": "def traits(args):\n    \"\"\"\n    %prog traits directory\n\n    Make HTML page that reports eye and skin color.\n    \"\"\"\n", "input": "", "output": "    p = OptionParser(traits.__doc__)\n    opts, args = p.parse_args(args)\n\n    if len(args) < 1:\n        sys.exit(not p.print_help())\n\n    samples = []\n    for folder in args:\n        targets = iglob(folder, \"*-traits.json\")\n        if not targets:\n            continue\n        filename = targets[0]\n        js = json.load(open(filename))\n        js[\"skin_rgb\"] = make_rgb(\n            js[\"traits\"][\"skin-color\"][\"L\"],\n            js[\"traits\"][\"skin-color\"][\"A\"],\n            js[\"traits\"][\"skin-color\"][\"B\"])\n        js[\"eye_rgb\"] = make_rgb(\n            js[\"traits\"][\"eye-color\"][\"L\"],\n            js[\"traits\"][\"eye-color\"][\"A\"],\n            js[\"traits\"][\"eye-color\"][\"B\"])\n        samples.append(js)\n\n    template = Template(traits_template)\n    fw = open(\"report.html\", \"w\")\n    print(template.render(samples=samples), file=fw)\n    logging.debug(\"Report written to `{}`\".format(fw.name))\n    fw.close()", "category": "Python"}, {"instruction": "def collectintargz(target, source, env):\n    \"\"\" Puts all source files into a tar.gz file. \"\"\"\n", "input": "", "output": "    # the rpm tool depends on a source package, until this is changed\n    # this hack needs to be here that tries to pack all sources in.\n    sources = env.FindSourceFiles()\n\n    # filter out the target we are building the source list for.\n    sources = [s for s in sources if s not in target]\n\n    # find the .spec file for rpm and add it since it is not necessarily found\n    # by the FindSourceFiles function.\n    sources.extend( [s for s in source if str(s).rfind('.spec')!=-1] )\n    # sort to keep sources from changing order across builds\n    sources.sort()\n\n    # as the source contains the url of the source package this rpm package\n    # is built from, we extract the target name\n    tarball = (str(target[0])+\".tar.gz\").replace('.rpm', '')\n    try:\n        tarball = env['SOURCE_URL'].split('/')[-1]\n    except KeyError as e:\n        raise SCons.Errors.UserError( \"Missing PackageTag '%s' for RPM packager\" % e.args[0] )\n\n    tarball = src_targz.package(env, source=sources, target=tarball,\n                                PACKAGEROOT=env['PACKAGEROOT'], )\n\n    return (target, tarball)", "category": "Python"}, {"instruction": "def gen_salt_and_hash(val=None):\n    \"\"\" Generate a salt & hash\n\n    If no string is provided then a random string will be\n    used to hash & referred to as `val`.\n\n    The salt will always be randomly generated & the hash\n    will be a sha256 hex value of the `val` & the salt as\n    a concatenated string. It follows the guidance here:\n\n        crackstation.net/hashing-security.htm#properhashing\n\n    :param val: str\n    :return: tuple of strings (salt, hash)\n    \"\"\"\n", "input": "", "output": "\n    if not val:\n        val = random_str()\n\n    str_salt = random_str()\n    str_hash = hashlib.sha256(val + str_salt).hexdigest()\n    return str_salt, str_hash", "category": "Python"}, {"instruction": "def get_language_tabs(self, request, obj, available_languages, css_class=None):\n        \"\"\"\n        Determine the language tabs to show.\n        \"\"\"\n", "input": "", "output": "        current_language = self.get_form_language(request, obj)\n        return get_language_tabs(request, current_language, available_languages, css_class=css_class)", "category": "Python"}, {"instruction": "def paint(self, iconic, painter, rect, mode, state, options):\n        \"\"\"Main paint method.\"\"\"\n", "input": "", "output": "        for opt in options:\n            self._paint_icon(iconic, painter, rect, mode, state, opt)", "category": "Python"}, {"instruction": "def pop(self, settings):\n        \"\"\"\n        Args:\n            settings (dict): Dict to pop applicable fields from\n        \"\"\"\n", "input": "", "output": "        if settings:\n            for name in self.__slots__:\n                self._set(name, settings.pop(name, UNSET))", "category": "Python"}, {"instruction": "def DOMDebugger_removeInstrumentationBreakpoint(self, eventName):\n\t\t\"\"\"\n\t\tFunction path: DOMDebugger.removeInstrumentationBreakpoint\n\t\t\tDomain: DOMDebugger\n\t\t\tMethod name: removeInstrumentationBreakpoint\n\t\t\n\t\t\tWARNING: This function is marked 'Experimental'!\n\t\t\n\t\t\tParameters:\n\t\t\t\tRequired arguments:\n\t\t\t\t\t'eventName' (type: string) -> Instrumentation name to stop on.\n\t\t\tNo return value.\n\t\t\n\t\t\tDescription: Removes breakpoint on particular native event.\n\t\t\"\"\"\n", "input": "", "output": "\t\tassert isinstance(eventName, (str,)\n\t\t    ), \"Argument 'eventName' must be of type '['str']'. Received type: '%s'\" % type(\n\t\t    eventName)\n\t\tsubdom_funcs = self.synchronous_command(\n\t\t    'DOMDebugger.removeInstrumentationBreakpoint', eventName=eventName)\n\t\treturn subdom_funcs", "category": "Python"}, {"instruction": "def shard_query_generator(self, query):\n    '''A generator that queries each shard in sequence.'''\n", "input": "", "output": "    shard_query = query.copy()\n\n    for shard in self._stores:\n      # yield all items matching within this shard\n      cursor = shard.query(shard_query)\n      for item in cursor:\n        yield item\n\n      # update query with results of first query\n      shard_query.offset = max(shard_query.offset - cursor.skipped, 0)\n      if shard_query.limit:\n        shard_query.limit = max(shard_query.limit - cursor.returned, 0)\n\n        if shard_query.limit <= 0:\n          break", "category": "Python"}, {"instruction": "def detect_function_style(self, test_record):\n        \"\"\"Returns the index for the function declaration style detected in the given string\n           or None if no function declarations are detected.\"\"\"\n", "input": "", "output": "        index = 0\n        # IMPORTANT: apply regex sequentially and stop on the first match:\n        for regex in FUNCTION_STYLE_REGEX:\n            if re.search(regex, test_record):\n                return index\n            index+=1\n        return None", "category": "Python"}, {"instruction": "def models_get(self, resource_url):\n        \"\"\"Get handle for model resource at given Url.\n\n        Parameters\n        ----------\n        resource_url : string\n            Url for subject resource at SCO-API\n\n        Returns\n        -------\n        models.ModelHandle\n            Handle for local copy of subject resource\n        \"\"\"\n", "input": "", "output": "        # Get resource directory, Json representation, active flag, and cache id\n        obj_dir, obj_json, is_active, cache_id = self.get_object(resource_url)\n        # Create model handle.\n        model = ModelHandle(obj_json)\n        # Add resource to cache if not exists\n        if not cache_id in self.cache:\n            self.cache_add(resource_url, cache_id)\n        # Return subject handle\n        return model", "category": "Python"}, {"instruction": "def to_python(self, data):\n        \"\"\"\n        Convert a data to python format.\n        \"\"\"\n", "input": "", "output": "        if data is None:\n            return ''\n        if isinstance(data, unicode):\n            data = data.encode(DEFAULT_ENCODING)\n        else:\n            data = str(data)\n        return data", "category": "Python"}, {"instruction": "def _GetDirectory(self):\n    \"\"\"Retrieves a directory.\n\n    Returns:\n      VShadowDirectory: a directory None if not available.\n    \"\"\"\n", "input": "", "output": "    if self.entry_type != definitions.FILE_ENTRY_TYPE_DIRECTORY:\n      return None\n    return VShadowDirectory(self._file_system, self.path_spec)", "category": "Python"}, {"instruction": "def publish_minions(self):\n        '''\n        Publishes minions as a list of dicts.\n        '''\n", "input": "", "output": "        log.debug('in publish minions')\n        minions = {}\n\n        log.debug('starting loop')\n        for minion, minion_info in six.iteritems(self.minions):\n            log.debug(minion)\n            # log.debug(minion_info)\n            curr_minion = {}\n            curr_minion.update(minion_info)\n            curr_minion.update({'id': minion})\n            minions[minion] = curr_minion\n        log.debug('ended loop')\n        ret = {'minions': minions}\n        self.handler.write_message(\n            salt.utils.json.dumps(ret) + str('\\n\\n'))", "category": "Python"}, {"instruction": "def update(self, schema, fields, query, **kwargs):\n        \"\"\"\n        Persist the query.fields into the db that match query.fields_where\n\n        schema -- Schema()\n        fields -- dict -- the values to persist\n        query -- Query() -- will be used to create the where clause\n\n        return -- int -- how many rows where updated\n        \"\"\"\n", "input": "", "output": "        with self.connection(**kwargs) as connection:\n            kwargs['connection'] = connection\n            try:\n                with self.transaction(**kwargs):\n                    r = self._update(schema, fields, query, **kwargs)\n\n            except Exception as e:\n                exc_info = sys.exc_info()\n                if self.handle_error(schema, e, **kwargs):\n                    r = self._update(schema, fields, query, **kwargs)\n                else:\n                    self.raise_error(e, exc_info)\n\n        return r", "category": "Python"}, {"instruction": "def _check_descendant(self, item):\n        \"\"\"Check the boxes of item's descendants.\"\"\"\n", "input": "", "output": "        children = self.get_children(item)\n        for iid in children:\n            self.change_state(iid, \"checked\")\n            self._check_descendant(iid)", "category": "Python"}, {"instruction": "def print_trace(self):\n        \"\"\"\n        Prints stack trace for current exceptions chain.\n        \"\"\"\n", "input": "", "output": "        traceback.print_exc()\n        for tb in self.tracebacks:\n            print tb,\n        print ''", "category": "Python"}, {"instruction": "def get_nodedata(self, sort_names=False):\n        \"\"\"\n        get dc node data from solved power flow\n        \"\"\"\n", "input": "", "output": "        if not self.Node.n:\n            return\n        if not self.pflow.solved:\n            logger.error('Power flow not solved when getting bus data.')\n            return tuple([False] * 7)\n        idx = self.Node.idx\n        names = self.Node.name\n        V = [self.dae.y[x] for x in self.Node.v]\n\n        if sort_names:\n            ret = (list(x)\n                   for x in zip(*sorted(zip(idx, names, V), key=itemgetter(0))))\n        else:\n            ret = idx, names, V\n\n        return ret", "category": "Python"}, {"instruction": "def get_item_key(self, item):\n        \"\"\"Return the value of the item 'key'.\"\"\"\n", "input": "", "output": "        try:\n            ret = item[item['key']]\n        except KeyError:\n            logger.error(\"No 'key' available in {}\".format(item))\n        if isinstance(ret, list):\n            return ret[0]\n        else:\n            return ret", "category": "Python"}, {"instruction": "def _find_countour_yaml(start, checked, names=None):\n    \"\"\"Traverse the directory tree identified by start\n    until a directory already in checked is encountered or the path\n    of countour.yaml is found.\n\n    Checked is present both to make the loop termination easy\n    to reason about and so the same directories do not get\n    rechecked\n\n    Args:\n        start: the path to start looking in and work upward from\n        checked: the set of already checked directories\n\n    Returns:\n        the path of the countour.yaml file or None if it is not found\n    \"\"\"\n", "input": "", "output": "    extensions = []\n\n    if names:\n        for name in names:\n            if not os.path.splitext(name)[1]:\n                extensions.append(name + \".yaml\")\n                extensions.append(name + \".yml\")\n\n    yaml_names = (names or []) + CONTOUR_YAML_NAMES + extensions\n    directory = start\n\n    while directory not in checked:\n        checked.add(directory)\n\n        for fs_yaml_name in yaml_names:\n            yaml_path = os.path.join(directory, fs_yaml_name)\n\n            if os.path.exists(yaml_path):\n                return yaml_path\n\n        directory = os.path.dirname(directory)\n\n    return", "category": "Python"}, {"instruction": "def mean(self, values, axis=0, weights=None, dtype=None):\n        \"\"\"compute the mean over each group\n\n        Parameters\n        ----------\n        values : array_like, [keys, ...]\n            values to take average of per group\n        axis : int, optional\n            alternative reduction axis for values\n        weights : ndarray, [keys, ...], optional\n            weight to use for each value\n        dtype : output dtype\n\n        Returns\n        -------\n        unique: ndarray, [groups]\n            unique keys\n        reduced : ndarray, [groups, ...]\n            value array, reduced over groups\n        \"\"\"\n", "input": "", "output": "        values = np.asarray(values)\n        if weights is None:\n            result = self.reduce(values, axis=axis, dtype=dtype)\n            shape = [1] * values.ndim\n            shape[axis] = self.groups\n            weights = self.count.reshape(shape)\n        else:\n            weights = np.asarray(weights)\n            result = self.reduce(values * weights, axis=axis, dtype=dtype)\n            weights = self.reduce(weights, axis=axis, dtype=dtype)\n        return self.unique, result / weights", "category": "Python"}, {"instruction": "def fcoe_fsb_fcoe_fsb_enable(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        fcoe_fsb = ET.SubElement(config, \"fcoe-fsb\", xmlns=\"urn:brocade.com:mgmt:brocade-fcoe\")\n        fcoe_fsb_enable = ET.SubElement(fcoe_fsb, \"fcoe-fsb-enable\")\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def qualified_name(self):\n        '''return the fully qualified name (`<module>.<interface>#<operation>`)'''\n", "input": "", "output": "        return '{0}.{1}#{2}'.format(self.module.name, self.interface.name, self.name)", "category": "Python"}, {"instruction": "def load_dependencies(req, history=None):\n    \"\"\"\n    Load the dependency tree as a Python object tree,\n    suitable for JSON serialization.\n\n    >>> deps = load_dependencies('jaraco.packaging')\n    >>> import json\n    >>> doc = json.dumps(deps)\n    \"\"\"\n", "input": "", "output": "    if history is None:\n        history = set()\n    dist = pkg_resources.get_distribution(req)\n    spec = dict(\n        requirement=str(req),\n        resolved=str(dist),\n    )\n    if req not in history:\n        # traverse into children\n        history.add(req)\n        extras = parse_extras(req)\n        depends = [\n            load_dependencies(dep, history=history)\n            for dep in dist.requires(extras=extras)\n        ]\n        if depends:\n            spec.update(depends=depends)\n    return spec", "category": "Python"}, {"instruction": "def init(config, workdir=None, logfile=None, loglevel=logging.INFO, **kwargs):\n    \"\"\"\n    Initialize the Lago environment\n\n    Args:\n        config(str): Path to LagoInitFile\n        workdir(str): Path to initalize the workdir, defaults to \"$PWD/.lago\"\n        **kwargs(dict): Pass arguments to :func:`~lago.cmd.do_init`\n        logfile(str): A path to setup a log file.\n        loglevel(int): :mod:`logging` log level.\n\n    Returns:\n        :class:`~lago.sdk.SDK`: Initialized Lago enviornment\n\n    Raises:\n       :exc:`~lago.utils.LagoException`: If initialization failed\n    \"\"\"\n", "input": "", "output": "\n    setup_sdk_logging(logfile, loglevel)\n    defaults = lago_config.get_section('init')\n    if workdir is None:\n        workdir = os.path.abspath('.lago')\n    defaults['workdir'] = workdir\n    defaults['virt_config'] = config\n    defaults.update(kwargs)\n    workdir, prefix = cmd.do_init(**defaults)\n    return SDK(workdir, prefix)", "category": "Python"}, {"instruction": "def options(self, *args, **kwargs):\n        \"\"\"XHR cross-domain OPTIONS handler\"\"\"\n", "input": "", "output": "        self.enable_cache()\n        self.handle_session_cookie()\n        self.preflight()\n\n        if self.verify_origin():\n            allowed_methods = getattr(self, 'access_methods', 'OPTIONS, POST')\n            self.set_header('Access-Control-Allow-Methods', allowed_methods)\n            self.set_header('Allow', allowed_methods)\n\n            self.set_status(204)\n        else:\n            # Set forbidden\n            self.set_status(403)\n\n        self.finish()", "category": "Python"}, {"instruction": "def cmd_reload(args):\n    '''reload graphs'''\n", "input": "", "output": "    mestate.console.writeln('Reloading graphs', fg='blue')\n    load_graphs()\n    setup_menus()\n    mestate.console.write(\"Loaded %u graphs\\n\" % len(mestate.graphs))", "category": "Python"}, {"instruction": "def smacof_mds(C, dim, max_iter=3000, eps=1e-9):\r\n    \"\"\"\r\n    Returns an interpolated point cloud following the dissimilarity matrix C\r\n    using SMACOF multidimensional scaling (MDS) in specific dimensionned\r\n    target space\r\n\r\n    Parameters\r\n    ----------\r\n    C : ndarray, shape (ns, ns)\r\n        dissimilarity matrix\r\n    dim : int\r\n          dimension of the targeted space\r\n    max_iter :  int\r\n        Maximum number of iterations of the SMACOF algorithm for a single run\r\n    eps : float\r\n        relative tolerance w.r.t stress to declare converge\r\n\r\n    Returns\r\n    -------\r\n    npos : ndarray, shape (R, dim)\r\n           Embedded coordinates of the interpolated point cloud (defined with\r\n           one isometry)\r\n    \"\"\"\n", "input": "", "output": "\r\n    rng = np.random.RandomState(seed=3)\r\n\r\n    mds = manifold.MDS(\r\n        dim,\r\n        max_iter=max_iter,\r\n        eps=1e-9,\r\n        dissimilarity='precomputed',\r\n        n_init=1)\r\n    pos = mds.fit(C).embedding_\r\n\r\n    nmds = manifold.MDS(\r\n        2,\r\n        max_iter=max_iter,\r\n        eps=1e-9,\r\n        dissimilarity=\"precomputed\",\r\n        random_state=rng,\r\n        n_init=1)\r\n    npos = nmds.fit_transform(C, init=pos)\r\n\r\n    return npos", "category": "Python"}, {"instruction": "def unwrap_querystring_lists(obj):\n    \"\"\"Convert responder querystring params, pulling values out of list\n    if there's only one.\n    \"\"\"\n", "input": "", "output": "    new_dict = {\n        key: (obj[key][0]\n              if len(obj[key]) == 1 else obj[key])\n        for key in obj.keys()\n        }\n    return new_dict", "category": "Python"}, {"instruction": "def plot_sections(self, fout_dir=\".\", **kws_usr):\n        \"\"\"Plot groups of GOs which have been placed in sections.\"\"\"\n", "input": "", "output": "        kws_plt, _ = self._get_kws_plt(None, **kws_usr)\n        PltGroupedGos(self).plot_sections(fout_dir, **kws_plt)", "category": "Python"}, {"instruction": "def _timestamps_eq(a, b):\n    \"\"\"Compares two timestamp operands for equivalence under the Ion data model.\"\"\"\n", "input": "", "output": "    assert isinstance(a, datetime)\n    if not isinstance(b, datetime):\n        return False\n    # Local offsets must be equivalent.\n    if (a.tzinfo is None) ^ (b.tzinfo is None):\n        return False\n    if a.utcoffset() != b.utcoffset():\n        return False\n    for a, b in ((a, b), (b, a)):\n        if isinstance(a, Timestamp):\n            if isinstance(b, Timestamp):\n                # Both operands declare their precisions. They are only equivalent if their precisions are the same.\n                if a.precision is b.precision and a.fractional_precision is b.fractional_precision:\n                    break\n                return False\n            elif a.precision is not TimestampPrecision.SECOND or a.fractional_precision != MICROSECOND_PRECISION:\n                # Only one of the operands declares its precision. It is only equivalent to the other (a naive datetime)\n                # if it has full microseconds precision.\n                return False\n    return a == b", "category": "Python"}, {"instruction": "def get_details(app='groupproject', env='dev', region='us-east-1'):\n    \"\"\"Extract details for Application.\n\n    Args:\n        app (str): Application Name\n        env (str): Environment/account to get details from\n\n    Returns:\n        collections.namedtuple with _group_, _policy_, _profile_, _role_,\n            _user_.\n\n    \"\"\"\n", "input": "", "output": "    url = '{host}/applications/{app}'.format(host=API_URL, app=app)\n\n    request = requests.get(url, verify=GATE_CA_BUNDLE, cert=GATE_CLIENT_CERT)\n\n    if not request.ok:\n        raise SpinnakerAppNotFound('\"{0}\" not found.'.format(app))\n\n    app_details = request.json()\n\n    LOG.debug('App details: %s', app_details)\n    group = app_details['attributes'].get('repoProjectKey')\n    project = app_details['attributes'].get('repoSlug')\n    generated = gogoutils.Generator(group, project, env=env, region=region, formats=APP_FORMATS)\n\n    LOG.debug('Application details: %s', generated)\n    return generated", "category": "Python"}, {"instruction": "def register(self, CorpNum, cashbill, UserID=None):\r\n        \"\"\" \ud604\uae08\uc601\uc218\uc99d \ub4f1\ub85d\r\n            args\r\n                CorpNum : \ud31d\ube4c\ud68c\uc6d0 \uc0ac\uc5c5\uc790\ubc88\ud638\r\n                cashbill : \ub4f1\ub85d\ud560 \ud604\uae08\uc601\uc218\uc99d object. made with Cashbill(...)\r\n                UserID : \ud31d\ube4c\ud68c\uc6d0 \uc544\uc774\ub514\r\n            return\r\n                \ucc98\ub9ac\uacb0\uacfc. consist of code and message\r\n            raise\r\n                PopbillException\r\n        \"\"\"\n", "input": "", "output": "        if cashbill == None:\r\n            raise PopbillException(-99999999, \"\ud604\uae08\uc601\uc218\uc99d \uc815\ubcf4\uac00 \uc785\ub825\ub418\uc9c0 \uc54a\uc558\uc2b5\ub2c8\ub2e4.\")\r\n\r\n        postData = self._stringtify(cashbill)\r\n\r\n        return self._httppost('/Cashbill', postData, CorpNum, UserID)", "category": "Python"}, {"instruction": "def _indent(x):\n    \"\"\"Indent a string by 4 characters.\"\"\"\n", "input": "", "output": "    lines = x.splitlines()\n    for i, line in enumerate(lines):\n        lines[i] = '    ' + line\n    return '\\n'.join(lines)", "category": "Python"}, {"instruction": "def current_dim_changed(self, index):\r\n        \"\"\"\r\n        This change the active axis the array editor is plotting over\r\n        in 3D\r\n        \"\"\"\n", "input": "", "output": "        self.last_dim = index\r\n        string_size = ['%i']*3\r\n        string_size[index] = '<font color=red>%i</font>'\r\n        self.shape_label.setText(('Shape: (' + ', '.join(string_size) +\r\n                                 ')    ') % self.data.shape)\r\n        if self.index_spin.value() != 0:\r\n            self.index_spin.setValue(0)\r\n        else:\r\n            # this is done since if the value is currently 0 it does not emit\r\n            # currentIndexChanged(int)\r\n            self.change_active_widget(0)\r\n        self.index_spin.setRange(-self.data.shape[index],\r\n                                 self.data.shape[index]-1)", "category": "Python"}, {"instruction": "def count_packages(self):\n        \"\"\"Count dependencies and packages\n        \"\"\"\n", "input": "", "output": "        packages = []\n        for pkg in self.dmap.values():\n            packages += pkg\n            self.count_dep += 1\n        self.count_pkg = len(set(packages))", "category": "Python"}, {"instruction": "def get_terminal_size():\n    \"\"\"\n    get size of console: rows x columns\n\n    :return: tuple, (int, int)\n    \"\"\"\n", "input": "", "output": "    try:\n        rows, columns = subprocess.check_output(['stty', 'size']).split()\n    except subprocess.CalledProcessError:\n        # not attached to terminal\n        logger.info(\"not attached to terminal\")\n        return 0, 0\n    logger.debug(\"console size is %s %s\", rows, columns)\n    return int(rows), int(columns)", "category": "Python"}, {"instruction": "def markers_to_events(self, keep_name=False):\n        \"\"\"Copy all markers in dataset to event type. \"\"\"\n", "input": "", "output": "        markers = self.parent.info.markers\n        \n        if markers is None:\n            self.parent.statusBar.showMessage('No markers in dataset.')\n            return\n        \n        if not keep_name:\n            name, ok = self.new_eventtype()            \n            if not ok:\n                return            \n        else:\n            name = None\n                \n        self.annot.add_events(markers, name=name, chan='')\n        \n        if keep_name:\n            self.display_eventtype()\n            n_eventtype = self.idx_eventtype.count()\n            self.idx_eventtype.setCurrentIndex(n_eventtype - 1)\n        \n        self.update_annotations()", "category": "Python"}, {"instruction": "def pendingResultsFor( self, ps ):\n        \"\"\"Retrieve a list of all pending results associated with the given parameters.\n\n        :param ps: the parameters\n        :returns: a list of pending result job ids, which may be empty\"\"\"\n", "input": "", "output": "        k = self._parametersAsIndex(ps)\n        if k in self._results.keys():\n            # filter out pending job ids, which can be anything except dicts\n            return [ j for j in self._results[k] if not isinstance(j, dict) ]\n        else:\n            return []", "category": "Python"}, {"instruction": "def isoformat(self):\n        \"\"\"Generate an ISO 8601 formatted time stamp.\n\n        Returns:\n            str: `ISO 8601`_ formatted time stamp\n\n        .. _ISO 8601: http://www.cl.cam.ac.uk/~mgk25/iso-time.html\n        \"\"\"\n", "input": "", "output": "        text = [self.strftime('%Y-%m-%dT%H:%M:%S'), ]\n        if self.tzinfo:\n            text.append(self.tzinfo.as_timezone())\n        else:\n            text.append('+00:00')\n        return ''.join(text)", "category": "Python"}, {"instruction": "def _call_method_from_namespace(obj, method_name, namespace):\n    \"\"\"Call the method, retrieved from obj, with the correct arguments via\n    the namespace\n\n    Args:\n        obj: any kind of object\n        method_name: method to be called\n        namespace: an argparse.Namespace object containing parsed command\n        line arguments\n    \"\"\"\n", "input": "", "output": "    method = getattr(obj, method_name)\n    method_parser = method.parser\n    arg_names = _get_args_name_from_parser(method_parser)\n    if method_name == \"__init__\":\n        return _call(obj, arg_names, namespace)\n    return _call(method, arg_names, namespace)", "category": "Python"}, {"instruction": "def get_groupnames(self, sgs):\n        \"\"\"Get servicegroups list\n\n        :return: comma separated list of servicegroups\n        :rtype: str\n        \"\"\"\n", "input": "", "output": "        return ','.join([sgs[sg].get_name() for sg in self.servicegroups])", "category": "Python"}, {"instruction": "def enumerate_imports(tokens):\n    \"\"\"\n    Iterates over *tokens* and returns a list of all imported modules.\n\n    .. note:: This ignores imports using the 'as' and 'from' keywords.\n    \"\"\"\n", "input": "", "output": "    imported_modules = []\n    import_line = False\n    from_import = False\n    for index, tok in enumerate(tokens):\n        token_type = tok[0]\n        token_string = tok[1]\n        if token_type == tokenize.NEWLINE:\n            import_line = False\n            from_import = False\n        elif token_string == \"import\":\n            import_line = True\n        elif token_string == \"from\":\n            from_import = True\n        elif import_line:\n            if token_type == tokenize.NAME and tokens[index+1][1] != 'as':\n                if not from_import:\n                    if token_string not in reserved_words:\n                        if token_string not in imported_modules:\n                            imported_modules.append(token_string)\n    return imported_modules", "category": "Python"}, {"instruction": "def submit_coroutine(self, coro, loop):\n        \"\"\"Schedule and await a coroutine on the specified loop\n\n        The coroutine is wrapped and scheduled using\n        :func:`asyncio.run_coroutine_threadsafe`. While the coroutine is\n        \"awaited\", the result is not available as method returns immediately.\n\n        Args:\n            coro: The :term:`coroutine` to schedule\n            loop: The :class:`event loop <asyncio.BaseEventLoop>` on which to\n                schedule the coroutine\n\n        Note:\n            This method is used internally by :meth:`__call__` and is not meant\n            to be called directly.\n        \"\"\"\n", "input": "", "output": "        async def _do_call(_coro):\n            with _IterationGuard(self):\n                await _coro\n        asyncio.run_coroutine_threadsafe(_do_call(coro), loop=loop)", "category": "Python"}, {"instruction": "def absolute_path(user_path):\n    \"\"\"\n    Some paths must be made absolute, this will attempt to convert them.\n    \"\"\"\n", "input": "", "output": "    if os.path.abspath(user_path):\n        return unix_path_coercion(user_path)\n    else:\n        try:\n            openaccess_epub.utils.evaluate_relative_path(relative=user_path)\n        except:\n            raise ValidationError('This path could not be rendered as absolute')", "category": "Python"}, {"instruction": "def cache(self, con):\n        \"\"\"Put a connection back into the pool cache.\"\"\"\n", "input": "", "output": "        try:\n            if self._reset == 2:\n                con.reset()  # reset the connection completely\n            else:\n                if self._reset or con._transaction:\n                    try:\n                        con.rollback()  # rollback a possible transaction\n                    except Exception:\n                        pass\n            self._cache.put(con, 0)  # and then put it back into the cache\n        except Full:\n            con.close()\n        if self._connections:\n            self._connections.release()", "category": "Python"}, {"instruction": "def lsb_release():\n    \"\"\"Return /etc/os-release in a dict.\"\"\"\n", "input": "", "output": "    d = {}\n    with open('/etc/os-release', 'r') as lsb:\n        for l in lsb:\n            s = l.split('=')\n            if len(s) != 2:\n                continue\n            d[s[0].strip()] = s[1].strip()\n    return d", "category": "Python"}, {"instruction": "def hash(self, algorithm: Algorithm = None) -> str:  # noqa: A003\n        \"\"\"Generate random hash.\n\n        To change hashing algorithm, pass parameter ``algorithm``\n        with needed value of the enum object :class:`~mimesis.enums.Algorithm`\n\n        :param algorithm: Enum object :class:`~mimesis.enums.Algorithm`.\n        :return: Hash.\n        :raises NonEnumerableError: if algorithm is not supported.\n        \"\"\"\n", "input": "", "output": "        key = self._validate_enum(algorithm, Algorithm)\n\n        if hasattr(hashlib, key):\n            fn = getattr(hashlib, key)\n            return fn(self.uuid().encode()).hexdigest()", "category": "Python"}, {"instruction": "def wait(self, delay):\n        \"\"\"\n        Wait at the current location for the specified number of iterations.\n\n        :param delay: The time to wait (in animation frames).\n        \"\"\"\n", "input": "", "output": "        for _ in range(0, delay):\n            self._add_step((self._rec_x, self._rec_y))", "category": "Python"}, {"instruction": "def _get_command(self):\n        \"\"\"Return command with options and targets, ready for execution.\"\"\"\n", "input": "", "output": "        targets = ' '.join(self.targets)\n        cmd_str = self._linter.command_with_options + ' ' + targets\n        cmd_shlex = shlex.split(cmd_str)\n        return list(cmd_shlex)", "category": "Python"}, {"instruction": "def _fix_incoming(self, son, collection):\n        \"\"\"Apply manipulators to an incoming SON object before it gets stored.\n\n        :Parameters:\n          - `son`: the son object going into the database\n          - `collection`: the collection the son object is being saved in\n        \"\"\"\n", "input": "", "output": "        son = self._apply_incoming_manipulators(son, collection)\n        son = self._apply_incoming_copying_manipulators(son, collection)\n        return son", "category": "Python"}, {"instruction": "def check(self) -> z3.CheckSatResult:\n        \"\"\"Returns z3 smt check result.\n        Also suppresses the stdout when running z3 library's check() to avoid unnecessary output\n        :return: The evaluated result which is either of sat, unsat or unknown\n        \"\"\"\n", "input": "", "output": "        old_stdout = sys.stdout\n        sys.stdout = open(os.devnull, \"w\")\n        evaluate = self.raw.check()\n        sys.stdout = old_stdout\n        return evaluate", "category": "Python"}, {"instruction": "def area(\n            self):\n        \"\"\"*The mean area of triangles in this mesh in units of square degrees.*\n\n        **Usage:**\n\n            .. code-block:: python\n\n                mesh.area\n        \"\"\"\n", "input": "", "output": "        pi = numpy.pi\n        area0 = 4.0 * pi / 8.0\n        areadiv = 4.0 ** self.depth\n        area = area0 / areadiv * (180.0 / pi) ** 2\n        return area", "category": "Python"}, {"instruction": "def arg_types(**kwargs):\n    \"\"\"\n    Mark the expected types of certain arguments.  Arguments for which\n    no types are provided default to strings.  To specify an argument\n    type, give this decorator a keyword argument, where the argument\n    name is the name of the function argument and the value is a\n    callable taking one argument, which will convert a string to a\n    value of that type.\n\n    Note that the 'bool' type is treated specially.\n    \"\"\"\n", "input": "", "output": "\n    def decorator(func):\n        if not hasattr(func, '_bark_types'):\n            func._bark_types = {}\n        func._bark_types.update(kwargs)\n        return func\n\n    return decorator", "category": "Python"}, {"instruction": "def _merge_asized(base, other, level=0):\n    \"\"\"\n    Merge **Asized** instances `base` and `other` into `base`.\n    \"\"\"\n", "input": "", "output": "    ref2key = lambda ref: ref.name.split(':')[0]\n    base.size += other.size\n    base.flat += other.flat\n    if level > 0:\n        base.name = ref2key(base)\n    # Add refs from other to base. Any new refs are appended.\n    base.refs = list(base.refs) # we may need to append items\n    refs = {}\n    for ref in base.refs:\n        refs[ref2key(ref)] = ref\n    for ref in other.refs:\n        key = ref2key(ref)\n        if key in refs:\n            _merge_asized(refs[key], ref, level=level+1)\n        else:\n            # Don't modify existing Asized instances => deepcopy\n            base.refs.append(deepcopy(ref))\n            base.refs[-1].name = key", "category": "Python"}, {"instruction": "def set_fill_rule(self, fill_rule):\n        \"\"\"Set the current :ref:`FILL_RULE` within the cairo context.\n        The fill rule is used to determine which regions are inside\n        or outside a complex (potentially self-intersecting) path.\n        The current fill rule affects both :meth:`fill` and :meth:`clip`.\n\n        The default fill rule is :obj:`WINDING <FILL_RULE_WINDING>`.\n\n        :param fill_rule: A :ref:`FILL_RULE` string.\n\n        \"\"\"\n", "input": "", "output": "        cairo.cairo_set_fill_rule(self._pointer, fill_rule)\n        self._check_status()", "category": "Python"}, {"instruction": "def _decode_data(self, data, charset):\n        \"\"\" Decode string data.\n\n        :returns: unicode string\n        \"\"\"\n", "input": "", "output": "\n        try:\n            return smart_unicode(data, charset)\n        except UnicodeDecodeError:\n            raise errors.BadRequest('wrong charset')", "category": "Python"}, {"instruction": "def apply_network_settings(**settings):\n    '''\n    Apply global network configuration.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' ip.apply_network_settings\n    '''\n", "input": "", "output": "    if 'require_reboot' not in settings:\n        settings['require_reboot'] = False\n\n    if 'apply_hostname' not in settings:\n        settings['apply_hostname'] = False\n\n    hostname_res = True\n    if settings['apply_hostname'] in _CONFIG_TRUE:\n        if 'hostname' in settings:\n            hostname_res = __salt__['network.mod_hostname'](settings['hostname'])\n        else:\n            log.warning(\n                'The network state sls is trying to apply hostname '\n                'changes but no hostname is defined.'\n            )\n            hostname_res = False\n\n    res = True\n    if settings['require_reboot'] in _CONFIG_TRUE:\n        log.warning(\n            'The network state sls is requiring a reboot of the system to '\n            'properly apply network configuration.'\n        )\n        res = True\n    else:\n        res = __salt__['service.restart']('network')\n\n    return hostname_res and res", "category": "Python"}, {"instruction": "def deprecate_thing_type(thingTypeName, undoDeprecate=False,\n    region=None, key=None, keyid=None, profile=None):\n    '''\n    Given a thing type name, deprecate it when undoDeprecate is False\n    and undeprecate it when undoDeprecate is True.\n\n    Returns {deprecated: true} if the thing type was deprecated and returns\n    {deprecated: false} if the thing type was not deprecated.\n\n    .. versionadded:: 2016.11.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_iot.deprecate_thing_type mythingtype\n\n    '''\n", "input": "", "output": "\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.deprecate_thing_type(\n            thingTypeName=thingTypeName,\n            undoDeprecate=undoDeprecate\n        )\n        deprecated = True if undoDeprecate is False else False\n        return {'deprecated': deprecated}\n    except ClientError as e:\n        return {'deprecated': False, 'error': __utils__['boto3.get_error'](e)}", "category": "Python"}, {"instruction": "async def make_transition_register(self, request: 'Request'):\n        \"\"\"\n        Use all underlying stacks to generate the next transition register.\n        \"\"\"\n", "input": "", "output": "\n        register = {}\n\n        for stack in self._stacks:\n            register = await stack.patch_register(register, request)\n\n        return register", "category": "Python"}, {"instruction": "def get_battery_state(self, prop):\n        \"\"\"\n        Return the first line from the file located at battery_path/prop as a\n        string.\n        \"\"\"\n", "input": "", "output": "        with open(os.path.join(self.options['battery_path'], prop), 'r') as f:\n                return f.readline().strip()", "category": "Python"}, {"instruction": "def get_all_client_tags(self, params=None):\n        \"\"\"\n        Get all client tags\n        This will iterate over all pages until it gets all elements.\n        So if the rate limit exceeded it will throw an Exception and you will get nothing\n\n        :param params: search params\n        :return: list\n        \"\"\"\n", "input": "", "output": "        return self._iterate_through_pages(\n            get_function=self.get_client_tags_per_page,\n            resource=CLIENT_TAGS,\n            **{'params': params}\n        )", "category": "Python"}, {"instruction": "def OLD_printDebug(s, style=None):\n    \"\"\"\n    util for printing in colors to sys.stderr stream\n    \"\"\"\n", "input": "", "output": "    if style == \"comment\":\n        s = Style.DIM + s + Style.RESET_ALL\n    elif style == \"important\":\n        s = Style.BRIGHT + s + Style.RESET_ALL\n    elif style == \"normal\":\n        s = Style.RESET_ALL + s + Style.RESET_ALL\n    elif style == \"red\":\n        s = Fore.RED + s + Style.RESET_ALL\n    elif style == \"green\":\n        s = Fore.GREEN + s + Style.RESET_ALL\n    try:\n        print(s, file=sys.stderr)\n    except:\n        pass", "category": "Python"}, {"instruction": "def unsubscribe(self, destination=None, id=None, headers=None, **keyword_headers):\n        \"\"\"\n        Unsubscribe from a destination by either id or the destination name.\n\n        :param str destination: the name of the topic or queue to unsubscribe from\n        :param str id: the unique identifier of the topic or queue to unsubscribe from\n        :param dict headers: a map of any additional headers the broker requires\n        :param keyword_headers: any additional headers the broker requires\n        \"\"\"\n", "input": "", "output": "        assert id is not None or destination is not None, \"'id' or 'destination' is required\"\n        headers = utils.merge_headers([headers, keyword_headers])\n        if id:\n            headers[HDR_ID] = id\n        if destination:\n            headers[HDR_DESTINATION] = destination\n        self.send_frame(CMD_UNSUBSCRIBE, headers)", "category": "Python"}, {"instruction": "def _kl_divergence(self, other_locs, other_weights, kernel=None, delta=1e-2):\n        \"\"\"\n        Finds the KL divergence between this and another particle\n        distribution by using a kernel density estimator to smooth over the\n        other distribution's particles.\n        \"\"\"\n", "input": "", "output": "        if kernel is None:\n            kernel = st.norm(loc=0, scale=1).pdf\n\n        dist = rescaled_distance_mtx(self, other_locs) / delta\n        K = kernel(dist)\n\n        return -self.est_entropy() - (1 / delta) * np.sum(\n            self.particle_weights *\n            np.log(\n                np.sum(\n                    other_weights * K,\n                    axis=1 # Sum over the particles of ``other``.\n                )\n            ),\n            axis=0  # Sum over the particles of ``self``.\n        )", "category": "Python"}, {"instruction": "def function_trace(function_name):\n    \"\"\"\n    Wraps a chunk of code that we want to appear as a separate, explicit,\n    segment in our monitoring tools.\n    \"\"\"\n", "input": "", "output": "    if newrelic:\n        nr_transaction = newrelic.agent.current_transaction()\n        with newrelic.agent.FunctionTrace(nr_transaction, function_name):\n            yield\n    else:\n        yield", "category": "Python"}, {"instruction": "def extract_shn (archive, compression, cmd, verbosity, interactive, outdir):\n    \"\"\"Decompress a SHN archive to a WAV file.\"\"\"\n", "input": "", "output": "    cmdlist = [util.shell_quote(cmd)]\n    outfile = util.get_single_outfile(outdir, archive, extension=\".wav\")\n    cmdlist.extend(['-x', '-', util.shell_quote(outfile), '<',\n        util.shell_quote(archive)])\n    return (cmdlist, {'shell': True})", "category": "Python"}, {"instruction": "def fit(self, X, y=None):\n        \"\"\"Calls the ctmc.ctmc function\n\n        Parameters\n        ----------\n        X : list of lists\n            (see ctmc function 'data')\n\n        y\n            not used, present for API consistence purpose.\n        \"\"\"\n", "input": "", "output": "        self.transmat, self.genmat, self.transcount, self.statetime = ctmc(\n            X, self.numstates, self.transintv, self.toltime, self.debug)\n        return self", "category": "Python"}, {"instruction": "def parse_func_body(self):\n        \"\"\"If success, return a tuple (args, body)\"\"\"\n", "input": "", "output": "        self.save()\n        self._expected = []\n        if self.next_is_rc(Tokens.OPAR, False):  # do not render right hidden\n            self.handle_hidden_right()  # render hidden after new level\n            args = self.parse_param_list()\n            if args is not None:  # may be an empty table\n                if self.next_is_rc(Tokens.CPAR, False):  # do not render right hidden\n                    self.handle_hidden_right()  # render hidden after new level\n                    body = self.parse_block()\n                    if body:\n                        self._expected = []\n                        token = self.next_is_rc(Tokens.END, False)\n                        if token:\n                            body.stop_char = token.stop\n                            self.success()\n                            return args, body\n                        else:\n                            self.abort()\n                else:\n                    self.abort()\n        return self.failure()", "category": "Python"}, {"instruction": "def splits(cls, fields, root=\".data\", train=\"en-ud-tag.v2.train.txt\",\n               validation=\"en-ud-tag.v2.dev.txt\",\n               test=\"en-ud-tag.v2.test.txt\", **kwargs):\n        \"\"\"Downloads and loads the Universal Dependencies Version 2 POS Tagged\n        data.\n        \"\"\"\n", "input": "", "output": "\n        return super(UDPOS, cls).splits(\n            fields=fields, root=root, train=train, validation=validation,\n            test=test, **kwargs)", "category": "Python"}, {"instruction": "def _get_tag(self, url, tag, print_warn=True):\n        \"\"\"Check if url is available and returns given tag type\n        :param url: url to content relative to base url\n        :param anchor: anchor type to return\n        :param print_warn: if True print warn when url is unavailable\n        \"\"\"\n", "input": "", "output": "        # construct complete url\n        complete_url = urljoin(self.base_url, url)\n        # check if exists\n        if requests.get('http:' + complete_url).ok:\n            # construct tag\n            if tag == 'script':\n                return '<script src=\"{0}\"></script>'.format(complete_url)\n            elif tag == 'stylesheet':\n                return '<link rel=\"stylesheet\" href=\"{0}\">'.format(complete_url)\n            else:\n                warnings.warn('Given tag is not valid')\n        elif print_warn:\n            warnings.warn('Url {0} not valid'.format(complete_url))\n        return None", "category": "Python"}, {"instruction": "def add_filter(self, filter_):\n        \"\"\" Add an image filter for post-processing \"\"\"\n", "input": "", "output": "        assert has_pil, _(\"Cannot add filters without python PIL\")\n        self.cache.basename += filter_.basename\n        self._filters.append(filter_)", "category": "Python"}, {"instruction": "def getRegisterUserInfo(self, svctype = \"Android NDrive App ver\", auth = 0):\n        \"\"\"Retrieve information about useridx\n\n        :param svctype: Information about the platform you are using right now.\n        :param auth: Authentication type\n\n        :return: ``True`` when success or ``False`` when failed\n        \"\"\"\n", "input": "", "output": "        data = {'userid': self.user_id,\n                'svctype': svctype,\n                'auth': auth\n               }\n\n        s, metadata = self.GET('getRegisterUserInfo', data)\n\n        if s is True:\n            self.useridx = metadata['useridx']\n            return True, metadata\n        else:\n            return False, metadata", "category": "Python"}, {"instruction": "def calc_x_from_L(L, y):\n    \"\"\" Calculate the industry output x from L and a y vector\n\n    Parameters\n    ----------\n    L : pandas.DataFrame or numpy.array\n        Symmetric input output Leontief table\n    y : pandas.DataFrame or numpy.array\n        a column vector of the total final demand\n\n    Returns\n    -------\n    pandas.DataFrame or numpy.array\n        Industry output x as column vector\n        The type is determined by the type of L. If DataFrame index as L\n\n    \"\"\"\n", "input": "", "output": "    x = L.dot(y)\n    if type(x) is pd.Series:\n        x = pd.DataFrame(x)\n    if type(x) is pd.DataFrame:\n        x.columns = ['indout']\n    return x", "category": "Python"}, {"instruction": "def isdisjoint(self, other):\n        \"\"\"Return True if the set has no elements in common with other.\"\"\"\n", "input": "", "output": "        return not bool(self.db.sinter([self.key, other.key]))", "category": "Python"}, {"instruction": "def update(self, container, instances=None, map_name=None, **kwargs):\n        \"\"\"\n        Updates instances from a container configuration. Typically this means restarting or recreating containers based\n        on detected changes in the configuration or environment.  Note that not all policy classes necessarily implement\n        this method.\n\n        :param container: Container name.\n        :type container: unicode | str\n        :param instances: Instance names to remove. If not specified, will remove all instances as specified in the\n         configuration (or just one default instance).\n        :type instances: collections.Iterable[unicode | str | NoneType]\n        :param map_name: Container map name. Optional - if not provided the default map is used.\n        :type map_name: unicode | str\n        :param kwargs: Additional kwargs. Only options controlling policy behavior are considered.\n        :return: Return values of actions.\n        :rtype: list[dockermap.map.runner.ActionOutput]\n        \"\"\"\n", "input": "", "output": "        return self.run_actions('update', container, instances=instances, map_name=map_name, **kwargs)", "category": "Python"}, {"instruction": "def activate(self, path=None, replace=False):\n        \"\"\"Ensure distribution is importable on `path` (default=sys.path)\"\"\"\n", "input": "", "output": "        if path is None:\n            path = sys.path\n        self.insert_on(path, replace=replace)\n        if path is sys.path:\n            fixup_namespace_packages(self.location)\n            for pkg in self._get_metadata('namespace_packages.txt'):\n                if pkg in sys.modules:\n                    declare_namespace(pkg)", "category": "Python"}, {"instruction": "def _make_txn(signer, setting_key, payload):\n    \"\"\"Creates and signs a sawtooth_settings transaction with with a payload.\n    \"\"\"\n", "input": "", "output": "    serialized_payload = payload.SerializeToString()\n    header = TransactionHeader(\n        signer_public_key=signer.get_public_key().as_hex(),\n        family_name='sawtooth_settings',\n        family_version='1.0',\n        inputs=_config_inputs(setting_key),\n        outputs=_config_outputs(setting_key),\n        dependencies=[],\n        payload_sha512=hashlib.sha512(serialized_payload).hexdigest(),\n        batcher_public_key=signer.get_public_key().as_hex()\n    ).SerializeToString()\n\n    return Transaction(\n        header=header,\n        header_signature=signer.sign(header),\n        payload=serialized_payload)", "category": "Python"}, {"instruction": "def default_qai(qareport):\n    \"\"\"QAI = 2 * (TP * (PT/PNC) * COV) / (1 + exp(MSE/tau))\n\n    Where:\n        TP: If all tests passes is 1, 0 otherwise.\n        PT: Processors and commands tested.\n        PCN: The number number of processors (Loader, Steps and Alerts)\n             and commands.\n        COV: The code coverage (between 0 and 1).\n        MSE: The Maintainability and Style Errors.\n        tau: Tolerance of style errors per file\n\n    \"\"\"\n", "input": "", "output": "    TP = 1. if qareport.is_test_sucess else 0.\n    PCN = qareport.processors_number + qareport.commands_number\n    PT_div_PCN = float(qareport.pc_tested_number) / PCN\n    COV = qareport.coverage_line_rate\n    tau = get_tau()\n\n    total_tau = float(tau) * len(qareport.project_modules)\n    style = 1 + math.exp(qareport.style_errors / total_tau)\n\n    result = (2 * TP * PT_div_PCN * COV) / style\n    return result", "category": "Python"}, {"instruction": "def find_kernel_specs(self):\n        \"\"\"Returns a dict mapping kernel names to resource directories.\"\"\"\n", "input": "", "output": "        # let real installed kernels overwrite envs with the same name:\n        # this is the same order as the get_kernel_spec way, which also prefers\n        # kernels from the jupyter dir over env kernels.\n        specs = self.find_kernel_specs_for_envs()\n        specs.update(super(EnvironmentKernelSpecManager,\n                           self).find_kernel_specs())\n        return specs", "category": "Python"}, {"instruction": "def color_gen_map(\n    colors: Iterable[Tuple[int, int, int]], indexes: Iterable[int]\n) -> List[Color]:\n    \"\"\"Return a smoothly defined scale of colors.\n\n    If ``indexes`` is [0, 3, 9] for example, the first color from ``colors``\n    will be returned at 0, the 2nd will be at 3, and the 3rd will be at 9.\n    All in-betweens will be filled with a gradient.\n\n    Args:\n        colors (Iterable[Union[Tuple[int, int, int], Sequence[int]]]):\n            Array of colors to be sampled.\n        indexes (Iterable[int]): A list of indexes.\n\n    Returns:\n        List[Color]: A list of Color instances.\n\n    Example:\n        >>> tcod.color_gen_map([(0, 0, 0), (255, 128, 0)], [0, 5])\n        [Color(0, 0, 0), Color(51, 25, 0), Color(102, 51, 0), \\\nColor(153, 76, 0), Color(204, 102, 0), Color(255, 128, 0)]\n    \"\"\"\n", "input": "", "output": "    ccolors = ffi.new(\"TCOD_color_t[]\", colors)\n    cindexes = ffi.new(\"int[]\", indexes)\n    cres = ffi.new(\"TCOD_color_t[]\", max(indexes) + 1)\n    lib.TCOD_color_gen_map(cres, len(ccolors), ccolors, cindexes)\n    return [Color._new_from_cdata(cdata) for cdata in cres]", "category": "Python"}, {"instruction": "def constraint_from_choices(cls, value_type: type, choices: collections.Sequence):\n        \"\"\"\n        Returns a constraint callable based on choices of a given type\n        \"\"\"\n", "input": "", "output": "        choices_str = ', '.join(map(str, choices))\n\n        def constraint(value):\n            value = value_type(value)\n            if value not in choices:\n                raise ParameterError('Argument must be one of %s' % choices_str)\n            return value\n\n        constraint.__name__ = 'choices_%s' % value_type.__name__\n        constraint.__doc__ = 'choice of %s' % choices_str\n        return constraint", "category": "Python"}, {"instruction": "def get_setter(self, oid):\n\t\t\"\"\"\n\t\tRetrieve the nearest parent setter function for an OID\n\t\t\"\"\"\n", "input": "", "output": "\t\tif hasattr(self.setter, oid):\n\t\t\treturn self.setter[oid]\n\t\tparents = [ poid for poid in list(self.setter.keys()) if oid.startswith(poid) ]\n\t\tif parents:\n\t\t\treturn self.setter[max(parents)]\n\t\treturn self.default_setter", "category": "Python"}, {"instruction": "def print_treemap(self, format=None, output=sys.stdout, **kwargs):\n        \"\"\"\n        Print the matrix for self's nodes.\n\n        Args:\n            format (str): output format (csv, json or text).\n            output (file): file descriptor on which to write.\n        \"\"\"\n", "input": "", "output": "        treemap = self.as_treemap()\n        treemap.print(format=format, output=output, **kwargs)", "category": "Python"}, {"instruction": "def is_logged_in(self, name_id):\n        \"\"\" Check if user is in the cache\n\n        :param name_id: The identifier of the subject\n        \"\"\"\n", "input": "", "output": "        identity = self.users.get_identity(name_id)[0]\n        return bool(identity)", "category": "Python"}, {"instruction": "def makePacket(ID, instr, reg=None, params=None):\n\t\"\"\"\n\tThis makes a generic packet.\n\n\tTODO: look a struct ... does that add value using it?\n\n\t0xFF, 0xFF, 0xFD, 0x00, ID, LEN_L, LEN_H, INST, PARAM 1, PARAM 2, ..., PARAM N, CRC_L, CRC_H]\n\tin:\n\t\tID - servo id\n\t\tinstr - instruction\n\t\treg - register\n\t\tparams - instruction parameter values\n\tout: packet\n\t\"\"\"\n", "input": "", "output": "\tpkt = []\n\tpkt += [0xFF, 0xFF, 0xFD]  # header\n\tpkt += [0x00]  # reserved byte\n\tpkt += [ID]\n\tpkt += [0x00, 0x00]  # length placeholder\n\tpkt += [instr]  # instruction\n\tif reg:\n\t\tpkt += le(reg)  # not everything has a register\n\tif params:\n\t\tpkt += params    # not everything has parameters\n\n\tlength = le(len(pkt) - 5)  # length = len(packet) - (header(3), reserve(1), id(1))\n\tpkt[5] = length[0]  # L\n\tpkt[6] = length[1]  # H\n\n\tcrc = crc16(pkt)\n\tpkt += le(crc)\n\n\treturn pkt", "category": "Python"}, {"instruction": "def working_yesterday(self, date_from=None, date_format=None):\n\t\t\"\"\"\n\t\tRetourne la date d'hier depuis maintenant ou depuis une date fournie\n\t\tseulement sur les jours ouvrableq.\n\t\tAinsi lundi devient samedi et samedi devient vendredi\n\n\t\t:param: :date_from date de r\u00e9f\u00e9rence\n\t\t:return datetime\n\t\t\"\"\"\n", "input": "", "output": "\t\t# date d'hier que sur les jours de week-end\n\t\treturn self.delta(days=-1, date_from=date_from, date_format=date_format, days_range=[1, 2, 3, 4, 5, 6])", "category": "Python"}, {"instruction": "def get_sectors(self, start, end=None):\n        \"\"\" Get contiguous sectors\n        \n        :param start: first sector number to read (note: numbering starts from 1)\n        :param end: last sector number to read\n        :returns: bytes\n        \"\"\"\n", "input": "", "output": "        s = self.get_sector_slice(start, end)\n        return self.bytes[s], self.style[s]", "category": "Python"}, {"instruction": "def _molfile(stream):\n    \"\"\"Process ``Molfile``.\n    \n    :param stream: Queue containing lines of text.\n    :type stream: :py:class:`collections.deque`\n    :return: Tuples of data.\n    \"\"\"\n", "input": "", "output": "    yield MolfileStart()\n    yield HeaderBlock(stream.popleft().strip(), stream.popleft().strip(), stream.popleft().strip())\n    # yield from _ctab(stream)\n    for token in _ctab(stream):\n        yield token\n    yield MolfileEnd()", "category": "Python"}, {"instruction": "def on_chain_updated(self, chain_head,\n                         committed_batches=None,\n                         uncommitted_batches=None):\n        \"\"\"\n        The existing chain has been updated, the current head block has\n        changed.\n\n        :param chain_head: the new head of block_chain, can be None if\n        no block publishing is desired.\n        :param committed_batches: the set of batches that were committed\n         as part of the new chain.\n        :param uncommitted_batches: the list of transactions if any that are\n        now de-committed when the new chain was selected.\n        :return: None\n        \"\"\"\n", "input": "", "output": "        try:\n            self._py_call(\n                'on_chain_updated',\n                ctypes.py_object(chain_head),\n                ctypes.py_object(committed_batches),\n                ctypes.py_object(uncommitted_batches))\n\n        # pylint: disable=broad-except\n        except Exception:\n            LOGGER.exception(\n                \"Unhandled exception in BlockPublisher.on_chain_updated\")", "category": "Python"}, {"instruction": "def remove_message(self, message, afterwards=None):\n        \"\"\"\n        Remove a message from the notmuch index\n\n        :param message: message to remove\n        :type message: :class:`Message`\n        :param afterwards: callback to trigger after removing\n        :type afterwards: callable or None\n        \"\"\"\n", "input": "", "output": "        if self.ro:\n            raise DatabaseROError()\n        path = message.get_filename()\n        self.writequeue.append(('remove', afterwards, path))", "category": "Python"}, {"instruction": "def fatal(self, message):\n        \"\"\"Log a fatal messsage and exit.\n\n        :param message:  Log message.\n\n        \"\"\"\n", "input": "", "output": "\n        self.logger.fatal(message)\n        sys.stderr.flush()\n        if self.exit_on_fatal:\n            sys.exit(1)\n        else:\n            raise FatalError(message)", "category": "Python"}, {"instruction": "def emflx(self, area, wavelengths=None):\n        \"\"\"Calculate\n        :ref:`equivalent monochromatic flux <synphot-formula-emflx>`.\n\n        Parameters\n        ----------\n        area, wavelengths\n            See :func:`unit_response`.\n\n        Returns\n        -------\n        em_flux : `~astropy.units.quantity.Quantity`\n            Equivalent monochromatic flux.\n\n        \"\"\"\n", "input": "", "output": "        t_lambda = self.tlambda(wavelengths=wavelengths)\n\n        if t_lambda == 0:  # pragma: no cover\n            em_flux = 0.0 * units.FLAM\n        else:\n            uresp = self.unit_response(area, wavelengths=wavelengths)\n            equvw = self.equivwidth(wavelengths=wavelengths).value\n            em_flux = uresp * equvw / t_lambda\n\n        return em_flux", "category": "Python"}, {"instruction": "def calculate_token(self, text, seed=None):\n        \"\"\" Calculate the request token (`tk`) of a string\n        :param text: str The text to calculate a token for\n        :param seed: str The seed to use. By default this is the number of hours since epoch\n        \"\"\"\n", "input": "", "output": "\n        if seed is None:\n            seed = self._get_token_key()\n\n        [first_seed, second_seed] = seed.split(\".\")\n\n        try:\n            d = bytearray(text.encode('UTF-8'))\n        except UnicodeDecodeError:\n            # This will probably only occur when d is actually a str containing UTF-8 chars, which means we don't need\n            # to encode.\n            d = bytearray(text)\n\n        a = int(first_seed)\n        for value in d:\n            a += value\n            a = self._work_token(a, self.SALT_1)\n        a = self._work_token(a, self.SALT_2)\n        a ^= int(second_seed)\n        if 0 > a:\n            a = (a & 2147483647) + 2147483648\n        a %= 1E6\n        a = int(a)\n        return str(a) + \".\" + str(a ^ int(first_seed))", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"Stop a running SocketIO web server.\n\n        This method must be called from a HTTP or SocketIO handler function.\n        \"\"\"\n", "input": "", "output": "        if self.server.eio.async_mode == 'threading':\n            func = flask.request.environ.get('werkzeug.server.shutdown')\n            if func:\n                func()\n            else:\n                raise RuntimeError('Cannot stop unknown web server')\n        elif self.server.eio.async_mode == 'eventlet':\n            raise SystemExit\n        elif self.server.eio.async_mode == 'gevent':\n            self.wsgi_server.stop()", "category": "Python"}, {"instruction": "def apply_bios_properties_filter(settings, filter_to_be_applied):\n    \"\"\"Applies the filter to return the dict of filtered BIOS properties.\n\n    :param settings: dict of BIOS settings on which filter to be applied.\n    :param filter_to_be_applied: list of keys to be applied as filter.\n    :returns: A dictionary of filtered BIOS settings.\n    \"\"\"\n", "input": "", "output": "    if not settings or not filter_to_be_applied:\n        return settings\n\n    return {k: settings[k] for k in filter_to_be_applied if k in settings}", "category": "Python"}, {"instruction": "def get_property(self):\n        \"\"\"Establishes access of Property values\"\"\"\n", "input": "", "output": "\n        prop = super(File, self).get_property()\n\n        # scope is the Property instance\n        scope = self\n\n        def fdel(self):\n            ", "category": "Python"}, {"instruction": "def rowlenselect(table, n, complement=False):\n    \"\"\"Select rows of length `n`.\"\"\"\n", "input": "", "output": "\n    where = lambda row: len(row) == n\n    return select(table, where, complement=complement)", "category": "Python"}, {"instruction": "def _apply(self, ctx: ExtensionContext) -> AugmentedDict:\n        \"\"\"\n        Replaces any {{env::*}} directives with it's actual environment variable value or a default.\n\n        Args:\n            ctx: The processing context.\n\n        Returns:\n            Returns the altered node key and value.\n        \"\"\"\n", "input": "", "output": "        node_key, node_value = ctx.node\n\n        def process(pattern: Pattern[str], _str: str) -> str:\n            _match = pattern.match(_str)\n            if _match is None:\n                return _str\n            # We got a match\n            # Group 0: Whole match; Group 1: Our placeholder; Group 2: The environment variable\n            placeholder, envvar = _match.group(1), _match.group(2)\n            envvalue = os.environ.get(envvar, None)\n            if envvalue is None and self.fail_on_unset:\n                raise ExtensionError(\"Environment variable '{}' is unset.\".format(envvar))\n\n            return _str.replace(placeholder, envvalue or self.default)\n\n        _pattern = re.compile(self.__pattern__)\n        node_key = process(_pattern, node_key)\n        node_value = process(_pattern, node_value)\n\n        return {node_key: node_value}", "category": "Python"}, {"instruction": "def _osx_platform_data():\n    '''\n    Additional data for macOS systems\n    Returns: A dictionary containing values for the following:\n        - model_name\n        - boot_rom_version\n        - smc_version\n        - system_serialnumber\n    '''\n", "input": "", "output": "    cmd = 'system_profiler SPHardwareDataType'\n    hardware = __salt__['cmd.run'](cmd)\n\n    grains = {}\n    for line in hardware.splitlines():\n        field_name, _, field_val = line.partition(': ')\n        if field_name.strip() == \"Model Name\":\n            key = 'model_name'\n            grains[key] = _clean_value(key, field_val)\n        if field_name.strip() == \"Boot ROM Version\":\n            key = 'boot_rom_version'\n            grains[key] = _clean_value(key, field_val)\n        if field_name.strip() == \"SMC Version (system)\":\n            key = 'smc_version'\n            grains[key] = _clean_value(key, field_val)\n        if field_name.strip() == \"Serial Number (system)\":\n            key = 'system_serialnumber'\n            grains[key] = _clean_value(key, field_val)\n\n    return grains", "category": "Python"}, {"instruction": "def get_defaults(path):\n    '''\n    Reads file for configuration defaults.\n\n    Arguments:\n        - path (str) Absolute filepath (usually ~/.licenser)\n\n    Returns:\n        - (dict) Defaults for name, email, license, .txt extension\n    '''\n", "input": "", "output": "\n    defaults = {}\n\n    if os.path.isfile(path):\n        with open(path) as f:\n\n            for line in f:\n                line = line.strip()\n                if '=' not in line or line.startswith('#'):\n                    continue\n\n                k, v = line.split('=', 1)\n                v = v.strip('\"').strip(\"'\")\n\n                defaults[k] = v\n        return defaults\n    else:\n        return {}", "category": "Python"}, {"instruction": "def _port_postfix(self):\n        \"\"\"\n        Returns empty string for the default port and ':port' otherwise\n        \"\"\"\n", "input": "", "output": "        port = self.real_connection.port\n        default_port = {'https': 443, 'http': 80}[self._protocol]\n        return ':{}'.format(port) if port != default_port else ''", "category": "Python"}, {"instruction": "def set_report_recipients(self, report, recipients):\n        \"\"\"Set recipients to the reports w/o overwriting the old ones\n\n        :param reports: list of ARReports\n        :param recipients: list of name,email strings\n        \"\"\"\n", "input": "", "output": "        to_set = report.getRecipients()\n        for recipient in recipients:\n            if recipient not in to_set:\n                to_set.append(recipient)\n        report.setRecipients(to_set)", "category": "Python"}, {"instruction": "def loss(params, batch, model_predict, rng):\n  \"\"\"Calculate loss.\"\"\"\n", "input": "", "output": "  inputs, targets = batch\n  predictions = model_predict(inputs, params, rng=rng)\n  predictions, targets = _make_list(predictions, targets)\n  xent = []\n  for (pred, target) in zip(predictions, targets):\n    xent.append(np.sum(pred * layers.one_hot(target, pred.shape[-1]), axis=-1))\n  return - masked_mean(xent, targets)", "category": "Python"}, {"instruction": "def __execute_jcc(self, instr):\n        \"\"\"Execute JCC instruction.\n        \"\"\"\n", "input": "", "output": "        op0_val = self.read_operand(instr.operands[0])  # Branch condition.\n        op2_val = self.read_operand(instr.operands[2])  # Target address.\n\n        return op2_val if op0_val != 0 else None", "category": "Python"}, {"instruction": "def get_version(self, is_full: bool = False) -> dict or str:\n        \"\"\"\n        This interface is used to get the version information of the connected node in current network.\n\n        Return:\n            the version information of the connected node.\n        \"\"\"\n", "input": "", "output": "        payload = self.generate_json_rpc_payload(RpcMethod.GET_VERSION)\n        response = self.__post(self.__url, payload)\n        if is_full:\n            return response\n        return response['result']", "category": "Python"}, {"instruction": "def readin_rho(filename, rhofile=True, aniso=False):\n    \"\"\"Read in the values of the resistivity in Ohmm.\n    The format is variable: rho-file or mag-file.\n    \"\"\"\n", "input": "", "output": "    if aniso:\n        a = [[0, 1, 2], [2, 3, 4]]\n    else:\n        a = [0, 2]\n    if rhofile:\n        if filename is None:\n            filename = 'rho/rho.dat'\n        with open(filename, 'r') as fid:\n            mag = np.loadtxt(fid, skiprows=1, usecols=(a[0]))\n\n    else:\n        if filename is None:\n            filename = read_iter()\n        with open(filename, 'r') as fid:\n            mag = np.power(10, np.loadtxt(fid, skiprows=1, usecols=(a[1])))\n\n    return mag", "category": "Python"}, {"instruction": "def load_rel(self, reltype, target, rId, is_external=False):\n        \"\"\"\n        Return newly added |_Relationship| instance of *reltype* between this\n        part and *target* with key *rId*. Target mode is set to\n        ``RTM.EXTERNAL`` if *is_external* is |True|. Intended for use during\n        load from a serialized package, where the rId is well known. Other\n        methods exist for adding a new relationship to the package during\n        processing.\n        \"\"\"\n", "input": "", "output": "        return self.rels.add_relationship(reltype, target, rId, is_external)", "category": "Python"}, {"instruction": "def scan (data, clamconf):\n    \"\"\"Scan data for viruses.\n    @return (infection msgs, errors)\n    @rtype ([], [])\n    \"\"\"\n", "input": "", "output": "    try:\n        scanner = ClamdScanner(clamconf)\n    except socket.error:\n        errmsg = _(\"Could not connect to ClamAV daemon.\")\n        return ([], [errmsg])\n    try:\n        scanner.scan(data)\n    finally:\n        scanner.close()\n    return scanner.infected, scanner.errors", "category": "Python"}, {"instruction": "def set_deltatime(self, delta_time):\n        \"\"\"Set the delta_time.\n\n        Can be an integer or a variable length byte.\n        \"\"\"\n", "input": "", "output": "        if type(delta_time) == int:\n            delta_time = self.int_to_varbyte(delta_time)\n        self.delta_time = delta_time", "category": "Python"}, {"instruction": "def retain_all(self, items):\n        \"\"\"\n        Removes the items which are not contained in the specified collection. In other words, only the items that\n        are contained in the specified collection will be retained.\n\n        :param items: (Collection), collection which includes the elements to be retained in this set.\n        :return: (bool), ``true`` if this queue changed as a result of the call.\n        \"\"\"\n", "input": "", "output": "        check_not_none(items, \"Value can't be None\")\n        data_items = []\n        for item in items:\n            check_not_none(item, \"Value can't be None\")\n            data_items.append(self._to_data(item))\n        return self._encode_invoke(queue_compare_and_retain_all_codec, data_list=data_items)", "category": "Python"}, {"instruction": "def split_into(max_num_chunks, list_to_chunk):\n    \"\"\"\n    Yields the list with a max total size of max_num_chunks\n    \"\"\"\n", "input": "", "output": "    max_chunk_size = math.ceil(len(list_to_chunk) / max_num_chunks)\n    return chunks_of(max_chunk_size, list_to_chunk)", "category": "Python"}, {"instruction": "def shadow_reference(self, dispatcher, node):\n        \"\"\"\n        Only simply make a reference to the value in the current scope,\n        specifically for the FuncBase type.\n        \"\"\"\n", "input": "", "output": "\n        # as opposed to the previous one, only add the value of the\n        # identifier itself to the scope so that it becomes reserved.\n        self.current_scope.reference(node.identifier.value)", "category": "Python"}, {"instruction": "def _send_data(self, data):\n        \"\"\"\n        Try to send all data in buffer.\n        \"\"\"\n", "input": "", "output": "        try:\n            self.socket.sendall(data)\n            self._reset_errors()\n        except:\n            self._close()\n            self._throttle_error(\"GraphiteHandler: Socket error, \"\n                                 \"trying reconnect.\")\n            self._connect()\n            try:\n                self.socket.sendall(data)\n            except:\n                return\n            self._reset_errors()", "category": "Python"}, {"instruction": "def value(self):\n        \"\"\" Field value as an enum name string. Fall back is an unsigned integer number.\"\"\"\n", "input": "", "output": "        if self._enum and issubclass(self._enum, Enumeration):\n            name = self._enum.get_name(self._value)\n            if name:\n                return name\n        return self._value", "category": "Python"}, {"instruction": "def _process_priv_part(perms):\n    '''\n    Process part\n    '''\n", "input": "", "output": "    _tmp = {}\n    previous = None\n    for perm in perms:\n        if previous is None:\n            _tmp[_PRIVILEGES_MAP[perm]] = False\n            previous = _PRIVILEGES_MAP[perm]\n        else:\n            if perm == '*':\n                _tmp[previous] = True\n            else:\n                _tmp[_PRIVILEGES_MAP[perm]] = False\n                previous = _PRIVILEGES_MAP[perm]\n    return _tmp", "category": "Python"}, {"instruction": "def get_paths(cls, packages):\n        \"\"\"Create list of matching packages for translation engine.\"\"\"\n", "input": "", "output": "        allowable_packages = dict((app_config.name, app_config) for app_config in apps.get_app_configs())\n        app_configs = [allowable_packages[p] for p in packages if p in allowable_packages]\n        # paths of requested packages\n        return [os.path.join(app.path, 'locale') for app in app_configs]", "category": "Python"}, {"instruction": "def router(self):\n        \"\"\"\n        Property returning the router at the top of the middleware\n        chain's stack (the last item in the list). If the list is empty\n        OR the item is not an instance of growler.Router, one is created\n        and added to the middleware chain, matching all requests.\n        \"\"\"\n", "input": "", "output": "        if not self.has_root_router:\n            self.middleware.add(HTTPMethod.ALL,\n                                MiddlewareChain.ROOT_PATTERN,\n                                Router())\n        return self.middleware.last().func", "category": "Python"}, {"instruction": "def can_approve(self, user, **data):\n        \"\"\"\n        Only sys admins can approve a service\n        :param user: a User\n        :param data: data that the user wants to update\n        \"\"\"\n", "input": "", "output": "        is_external = data.get('service_type', self.service_type) == 'external'\n        raise Return(user.is_admin() or is_external)", "category": "Python"}, {"instruction": "def append_item(self, item):\n        \"\"\"\n        Add an item to the end of the menu before the exit item.\n\n        Args:\n            item (MenuItem): The item to be added.\n\n        \"\"\"\n", "input": "", "output": "        did_remove = self.remove_exit()\n        item.menu = self\n        self.items.append(item)\n        if did_remove:\n            self.add_exit()", "category": "Python"}, {"instruction": "def tf_name_scope(self):\n        \"\"\"\n        Auxilary method for composing gpflow's tree name scopes. The Parentable pathname\n        can be considered as a set of name scopes. This method grabs `pathname` and\n        returns only name of the node in that path.\n        Leading node name is always replaced with two parts: the name and the index\n        for uniquiness in TensorFlow.\n        \"\"\"\n", "input": "", "output": "        if self.parent is self:\n            leader_name = self.name\n            leader_index = self.index\n            if leader_index is None:\n                return leader_name\n            return \"{name}-{index}\".format(name=leader_name, index=leader_index)\n        return self.pathname.rsplit('/')[-1]", "category": "Python"}, {"instruction": "def __remove_trailing_zeros(self, collection):\n        \"\"\"Removes trailing zeroes from indexable collection of numbers\"\"\"\n", "input": "", "output": "        index = len(collection) - 1\n        while index >= 0 and collection[index] == 0:\n            index -= 1\n\n        return collection[:index + 1]", "category": "Python"}, {"instruction": "def endswith(self, pat):\n        \"\"\"Test if elements end with pat.\n\n        Parameters\n        ----------\n        pat : str\n\n        Returns\n        -------\n        Series\n\n        \"\"\"\n", "input": "", "output": "        check_type(pat, str)\n\n        return _series_bool_result(self, weld_str_endswith, pat=pat)", "category": "Python"}, {"instruction": "def make_dict(fields, fields_kwargs):\n    \"\"\"lot's of methods take a dict or kwargs, this combines those\n\n    Basically, we do a lot of def method(fields, **kwargs) and we want to merge\n    those into one super dict with kwargs taking precedence, this does that\n\n    fields -- dict -- a passed in dict\n    fields_kwargs -- dict -- usually a **kwargs dict from another function\n\n    return -- dict -- a merged fields and fields_kwargs\n    \"\"\"\n", "input": "", "output": "    ret = {}\n    if fields:\n        ret.update(fields)\n\n    if fields_kwargs:\n        ret.update(fields_kwargs)\n\n    return ret", "category": "Python"}, {"instruction": "def set_status(self, name: str = None):\n        \"\"\"Updates the bot's status\n\n        This is used to get the game that the bot is \"playing\" or to clear it.\n        If you want to set a game, pass a name; if you want to clear it, either\n        call this method without the optional ``name`` parameter or explicitly\n        pass ``None``.\n\n        Args:\n            name: the game's name, or None\n        \"\"\"\n", "input": "", "output": "        game = None\n        if name:\n            game = {\n                'name': name\n            }\n        payload = {\n            'op': WebSocketEvent.STATUS_UPDATE.value,\n            'd': {\n                'game': game,\n                'status': 'online',\n                'afk': False,\n                'since': 0.0\n            }\n        }\n        data = json.dumps(payload, indent=2)\n        self.logger.debug(f'Sending status update payload: {data}')\n        self._ws.send(data)", "category": "Python"}, {"instruction": "def load_object_by_name(object_name):\n    \"\"\"Load an object from a module by name\"\"\"\n", "input": "", "output": "    mod_name, attr = object_name.rsplit('.', 1)\n    mod = import_module(mod_name)\n    return getattr(mod, attr)", "category": "Python"}, {"instruction": "def create_qc(self, product_id=None, expected_result='', actual_result='', weight=100, status='success', **kwargs):\n        '''\n        create_qc(self, product_id=None, expected_result='', actual_result='', weight=100, status='success', **kwargs)\n\n        Create Quality Criteria\n\n        :Parameters:\n        * *product_id* (`string`) -- The product (release candidate) identifier\n        * *expected_result* (`string`) -- Text describing the expected result of this criteria\n        * *actual_result* (`string`) --  Text describing the actual result of this criteria\n        * *weight* (`integer`) -- Overall weight of this criteria (integer between 0-100)\n        * *status* (`string`) -- pass/fail/norun\n\n        '''\n", "input": "", "output": "        request_data = {'product_id': product_id, 'expected': expected_result, 'actual': actual_result,'weight': weight, 'exec_status': status}\n        request_data.update(**kwargs)\n        return self._call_rest_api('post', '/qc', data=request_data, error='Failed to create criteria')", "category": "Python"}, {"instruction": "def process(self):\n        \"\"\"Entry point of SelectableSelector\"\"\"\n", "input": "", "output": "        if WINDOWS:\n            select_inputs = []\n            for i in self.inputs:\n                if not isinstance(i, SelectableObject):\n                    warning(\"Unknown ignored object type: %s\", type(i))\n                elif i.__selectable_force_select__:\n                    # Then use select.select\n                    select_inputs.append(i)\n                elif not self.remain and i.check_recv():\n                    self.results.append(i)\n                else:\n                    i.wait_return(self._exit_door)\n            if select_inputs:\n                # Use default select function\n                self.results.extend(select(select_inputs, [], [], self.remain)[0])  # noqa: E501\n            if not self.remain:\n                return self.results\n\n            threading.Thread(target=self._timeout_thread, args=(self.remain,)).start()  # noqa: E501\n            if not self._ended:\n                self.available_lock.acquire()\n            return self.results\n        else:\n            r, _, _ = select(self.inputs, [], [], self.remain)\n            return r", "category": "Python"}, {"instruction": "def Constant(cls, irsb_c, val, ty):\n        \"\"\"\n        Creates a constant as a VexValue\n        :param irsb_c: The IRSBCustomizer to use\n        :param val: The value, as an integer\n        :param ty: The type of the resulting VexValue\n        :return: a VexValue\n        \"\"\"\n", "input": "", "output": "        assert not (isinstance(val, VexValue) or isinstance(val, IRExpr))\n        rdt = irsb_c.mkconst(val, ty)\n        return cls(irsb_c, rdt)", "category": "Python"}, {"instruction": "def sex2dec(ra, dec):\n    '''\n    Convert sexadecimal hours to decimal degrees. Adapted from\n    `PyKE <http://keplergo.arc.nasa.gov/PyKE.shtml>`_.\n\n    :param float ra: The right ascension\n    :param float dec: The declination\n\n    :returns: The same values, but in decimal degrees\n\n    '''\n", "input": "", "output": "\n    ra = re.sub('\\s+', '|', ra.strip())\n    ra = re.sub(':', '|', ra.strip())\n    ra = re.sub(';', '|', ra.strip())\n    ra = re.sub(',', '|', ra.strip())\n    ra = re.sub('-', '|', ra.strip())\n    ra = ra.split('|')\n    outra = (float(ra[0]) + float(ra[1]) / 60. + float(ra[2]) / 3600.) * 15.0\n\n    dec = re.sub('\\s+', '|', dec.strip())\n    dec = re.sub(':', '|', dec.strip())\n    dec = re.sub(';', '|', dec.strip())\n    dec = re.sub(',', '|', dec.strip())\n    dec = dec.split('|')\n\n    if float(dec[0]) > 0.0:\n        outdec = float(dec[0]) + float(dec[1]) / 60. + float(dec[2]) / 3600.\n    else:\n        outdec = float(dec[0]) - float(dec[1]) / 60. - float(dec[2]) / 3600.\n\n    return outra, outdec", "category": "Python"}, {"instruction": "def _cast_to_type(self, value):\n        \"\"\" Convert the value to its string representation\"\"\"\n", "input": "", "output": "        if isinstance(value, str) or value is None:\n            return value\n        return str(value)", "category": "Python"}, {"instruction": "def _get_method_params(self):\n        \"\"\"\n        This method makes reading and filtering each method implemented\n        in this class a more general approach. It reads the previous\n        frame from Python and filters the params passed to the caller\n        of _make_request.\n        :return: a dictionary of caller's parameters and values\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        caller = sys._getframe(2)\n        var_names = list(caller.f_code.co_varnames)\n        caller_locals = caller.f_locals\n\n        var_names.remove('self')\n        kwargs = {key: value for key, value in caller_locals.items()\n                  if key in var_names and value is not None}\n        return kwargs", "category": "Python"}, {"instruction": "def newTextChild(self, parent, name, content):\n        \"\"\"Creation of a new child element, added at the end of\n          @parent children list. @ns and @content parameters are\n          optional (None). If @ns is None, the newly created element\n          inherits the namespace of @parent. If @content is non None,\n          a child TEXT node will be created containing the string\n          @content. NOTE: Use xmlNewChild() if @content will contain\n          entities that need to be preserved. Use this function,\n          xmlNewTextChild(), if you need to ensure that reserved XML\n          chars that might appear in @content, such as the ampersand,\n          greater-than or less-than signs, are automatically replaced\n           by their XML escaped entity representations. \"\"\"\n", "input": "", "output": "        if parent is None: parent__o = None\n        else: parent__o = parent._o\n        ret = libxml2mod.xmlNewTextChild(parent__o, self._o, name, content)\n        if ret is None:raise treeError('xmlNewTextChild() failed')\n        __tmp = xmlNode(_obj=ret)\n        return __tmp", "category": "Python"}, {"instruction": "def _build_url_rewriter(cls, session: AppSession):\n        '''Build URL rewriter if needed.'''\n", "input": "", "output": "        if session.args.escaped_fragment or session.args.strip_session_id:\n            return session.factory.new(\n                'URLRewriter',\n                hash_fragment=session.args.escaped_fragment,\n                session_id=session.args.strip_session_id\n            )", "category": "Python"}, {"instruction": "def settings(self) -> typing.Union[None, SharedCache]:\n        \"\"\"The settings associated with this project.\"\"\"\n", "input": "", "output": "        return self._project.settings if self._project else None", "category": "Python"}, {"instruction": "def unique_ordered(data):\n    \"\"\"\n    Returns the same as np.unique, but ordered as per the\n    first occurrence of the unique value in data.\n\n    Examples\n    ---------\n    In [1]: a = [0, 3, 3, 4, 1, 3, 0, 3, 2, 1]\n\n    In [2]: np.unique(a)\n    Out[2]: array([0, 1, 2, 3, 4])\n\n    In [3]: trimesh.grouping.unique_ordered(a)\n    Out[3]: array([0, 3, 4, 1, 2])\n    \"\"\"\n", "input": "", "output": "    data = np.asanyarray(data)\n    order = np.sort(np.unique(data, return_index=True)[1])\n    result = data[order]\n    return result", "category": "Python"}, {"instruction": "def check_spam(self, ip=None, email=None, name=None, login=None, realname=None,\n                   subject=None, body=None, subject_type='plain', body_type='plain'):\n        \"\"\" http://api.yandex.ru/cleanweb/doc/dg/concepts/check-spam.xml\n            subject_type = plain|html|bbcode\n            body_type = plain|html|bbcode\n        \"\"\"\n", "input": "", "output": "        data = {'ip': ip, 'email': email, 'name': name, 'login': login, 'realname': realname,\n                'body-%s' % body_type: body, 'subject-%s' % subject_type: subject}\n        r = self.request('post', 'http://cleanweb-api.yandex.ru/1.0/check-spam', data=data)\n        root = ET.fromstring(r.content)\n        return {\n            'id': root.findtext('id'),\n            'spam_flag': yesnobool(root.find('text').attrib['spam-flag']),\n            'links': [(link.attrib['href'], yesnobool(link.attrib['spam-flag'])) for link in root.findall('./links/link')]\n        }", "category": "Python"}, {"instruction": "def create_current_pb(self, ):\n        \"\"\"Create a push button and place it in the corner of the tabwidget\n\n        :returns: the created button\n        :rtype: :class:`QtGui.QPushButton`\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        pb = QtGui.QPushButton(\"Select current\")\n        self.selection_tabw.setCornerWidget(pb)\n        return pb", "category": "Python"}, {"instruction": "def _load_vertex_buffers(self):\n        \"\"\"Load each vertex buffer into each material\"\"\"\n", "input": "", "output": "        fd = gzip.open(cache_name(self.file_name), 'rb')\n\n        for buff in self.meta.vertex_buffers:\n\n            mat = self.wavefront.materials.get(buff['material'])\n            if not mat:\n                mat = Material(name=buff['material'], is_default=True)\n                self.wavefront.materials[mat.name] = mat\n\n            mat.vertex_format = buff['vertex_format']\n            self.load_vertex_buffer(fd, mat, buff['byte_length'])\n\n        fd.close()", "category": "Python"}, {"instruction": "def buildIndex(ipFile, ndxFile, append='Y', silent='N', useShortFileName='Y'):\n    \"\"\"\n    this creates an index of a text file specifically for use in AIKIF\n    separates the ontology descriptions highest followed by values and lastly\n    a final pass to get all delimited word parts.\n    \"\"\"\n", "input": "", "output": "    if silent == 'N':\n        pass\n    if append == 'N':\n        try:\n            os.remove(ndxFile)\n        except Exception as ex:\n            print('file already deleted - ignore' + str(ex))\n            \n    delims = [',', chr(31), '\u001f', '$', '&', '\"', '%', '/', '\\\\', '.', ';', ':', '!', '?', '-', '_', ' ', '\\n', '*', '\\'', '(', ')', '[', ']', '{', '}']\n    # 1st pass - index the ontologies, including 2 depths up (later - TODO)\n    #buildIndex(ipFile, ndxFile, ' ', 1, 'Y')\n\n    # 2nd pass - use ALL delims to catch each word as part of hyphenated - eg AI Build py\n    totWords, totLines, uniqueWords = getWordList(ipFile, delims)\n    \n    AppendIndexDictionaryToFile(uniqueWords, ndxFile, ipFile, useShortFileName)\n    if silent == 'N':\n        print(format_op_row(ipFile, totLines, totWords, uniqueWords))\n   \n        show('uniqueWords', uniqueWords, 5)\n    DisplayIndexAsDictionary(uniqueWords)", "category": "Python"}, {"instruction": "def _margtime_mfsnr(template, data):\n        \"\"\"Returns a time series for the matched filter SNR assuming that the\n        template and data have both been normalised and whitened.\n        \"\"\"\n", "input": "", "output": "        snr = matched_filter_core(template, data, h_norm=1, psd=None)\n        hd_i = snr[0].numpy().real\n        return hd_i", "category": "Python"}, {"instruction": "def expand_dims(self, axis):\n        \"\"\"Insert a new axis, at a given position in the array shape\n        Args:\n          axis (int): Position (amongst axes) where new axis is to be inserted.\n        \"\"\"\n", "input": "", "output": "        if axis <= self._distaxis:\n            subaxis = axis\n            new_distaxis = self._distaxis + 1\n        else:\n            subaxis = axis - 1\n            new_distaxis = self._distaxis\n        new_subts = [rts.expand_dims(subaxis) for rts in self._subarrays]\n        if axis == 0:\n            # prepended an axis: no longer a Timeseries\n            return distob.DistArray(new_subts, new_distaxis)\n        else:\n            axislabels = self.labels[self._distaxis]\n            return DistTimeseries(new_subts, new_distaxis, axislabels)", "category": "Python"}, {"instruction": "def date(self, pattern='%Y-%m-%d', end_datetime=None):\n        \"\"\"\n        Get a date string between January 1, 1970 and now\n        :param pattern format\n        :example '2008-11-27'\n        \"\"\"\n", "input": "", "output": "        return self.date_time(end_datetime=end_datetime).strftime(pattern)", "category": "Python"}, {"instruction": "def on_event(self, evt, is_final):\n        \"\"\" this is invoked from in response to COM PumpWaitingMessages - different thread \"\"\"\n", "input": "", "output": "        for msg in XmlHelper.message_iter(evt):\n            # Single security element in historical request\n            node = msg.GetElement('securityData')\n            if node.HasElement('securityError'):\n                secid = XmlHelper.get_child_value(node, 'security')\n                self.security_errors.append(XmlHelper.as_security_error(node.GetElement('securityError'), secid))\n            else:\n                self.on_security_data_node(node)", "category": "Python"}, {"instruction": "def _make_socket(cls, ip, port):\n    \"\"\"Bind to a new socket.\n\n    If LIBPROCESS_PORT or LIBPROCESS_IP are configured in the environment,\n    these will be used for socket connectivity.\n    \"\"\"\n", "input": "", "output": "    bound_socket = bind_sockets(port, address=ip)[0]\n    ip, port = bound_socket.getsockname()\n\n    if not ip or ip == '0.0.0.0':\n      ip = socket.gethostbyname(socket.gethostname())\n\n    return bound_socket, ip, port", "category": "Python"}, {"instruction": "def cmd(send, msg, args):\n    \"\"\"Runs eix with the given arguments.\n\n    Syntax: {command} <package>\n\n    \"\"\"\n", "input": "", "output": "    if not msg:\n        result = subprocess.run(['eix', '-c'], env={'EIX_LIMIT': '0', 'HOME': os.environ['HOME']}, stdout=subprocess.PIPE, universal_newlines=True)\n        if result.returncode:\n            send(\"eix what?\")\n            return\n        send(choice(result.stdout.splitlines()))\n        return\n    args = ['eix', '-c'] + msg.split()\n    result = subprocess.run(args, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, universal_newlines=True)\n    if result.returncode:\n        send(\"%s isn't important enough for Gentoo.\" % msg)\n    else:\n        send(result.stdout.splitlines()[0].strip())", "category": "Python"}, {"instruction": "def send_signal(self, request, response, forum):\n        \"\"\" Sends the signal associated with the view. \"\"\"\n", "input": "", "output": "        self.view_signal.send(\n            sender=self, forum=forum, user=request.user, request=request, response=response,\n        )", "category": "Python"}, {"instruction": "def setmode(self, mode):\n        \"\"\"Set the interface mode. It can be:\n        - 0 or managed: Managed Mode (aka \"Extensible Station Mode\")\n        - 1 or monitor: Monitor Mode (aka \"Network Monitor Mode\")\n        - 2 or master: Master Mode (aka \"Extensible Access Point\")\n              (supported from Windows 7 and later)\n        - 3 or wfd_device: The Wi-Fi Direct Device operation mode\n              (supported from Windows 8 and later)\n        - 4 or wfd_owner: The Wi-Fi Direct Group Owner operation mode\n              (supported from Windows 8 and later)\n        - 5 or wfd_client: The Wi-Fi Direct Client operation mode\n              (supported from Windows 8 and later)\n        Only available with Npcap.\"\"\"\n", "input": "", "output": "        # According to https://nmap.org/npcap/guide/npcap-devguide.html#npcap-feature-dot11  # noqa: E501\n        self._check_npcap_requirement()\n        _modes = {\n            0: \"managed\",\n            1: \"monitor\",\n            2: \"master\",\n            3: \"wfd_device\",\n            4: \"wfd_owner\",\n            5: \"wfd_client\"\n        }\n        m = _modes.get(mode, \"unknown\") if isinstance(mode, int) else mode\n        return self._npcap_set(\"mode\", m)", "category": "Python"}, {"instruction": "def get_type(symbol):\n    '''\n    Uses BeautifulSoup to scrape symbol category from Yahoo! Finance website\n    '''\n", "input": "", "output": "    url = 'http://finance.yahoo.com/q/pr?s=%s+Profile' % symbol\n    soup = BeautifulSoup(urlopen(url).read())\n    if soup.find('span', text='Business Summary'):\n        return 'Stock'\n    elif soup.find('span', text='Fund Summary'):\n        asset_type = 'Fund'\n    elif symbol.find('^') == 0:\n        asset_type = 'Index'\n    else:\n        pass\n    return asset_type", "category": "Python"}, {"instruction": "def get_all(self, page=None, per_page=None, include_totals=False):\n        \"\"\"Retrieves all resource servers\n\n        Args:\n            page (int, optional): The result's page number (zero based).\n\n            per_page (int, optional): The amount of entries per page.\n\n            include_totals (bool, optional): True if the query summary is\n                to be included in the result, False otherwise.\n\n        See: https://auth0.com/docs/api/management/v2#!/Resource_Servers/get_resource_servers\n        \"\"\"\n", "input": "", "output": "\n        params = {\n            'page': page,\n            'per_page': per_page,\n            'include_totals': str(include_totals).lower()\n        }\n\n        return self.client.get(self._url(), params=params)", "category": "Python"}, {"instruction": "def mplcmap_to_palette(cmap, ncolors=None, categorical=False):\n    \"\"\"\n    Converts a matplotlib colormap to palette of RGB hex strings.\"\n    \"\"\"\n", "input": "", "output": "    from matplotlib.colors import Colormap, ListedColormap\n\n    ncolors = ncolors or 256\n    if not isinstance(cmap, Colormap):\n        import matplotlib.cm as cm\n        # Alias bokeh Category cmaps with mpl tab cmaps\n        if cmap.startswith('Category'):\n            cmap = cmap.replace('Category', 'tab')\n        try:\n            cmap = cm.get_cmap(cmap)\n        except:\n            cmap = cm.get_cmap(cmap.lower())\n    if isinstance(cmap, ListedColormap):\n        if categorical:\n            palette = [rgb2hex(cmap.colors[i%cmap.N]) for i in range(ncolors)]\n            return palette\n        elif cmap.N > ncolors:\n            palette = [rgb2hex(c) for c in cmap(np.arange(cmap.N))]\n            if len(palette) != ncolors:\n                palette = [palette[int(v)] for v in np.linspace(0, len(palette)-1, ncolors)]\n            return palette\n    return [rgb2hex(c) for c in cmap(np.linspace(0, 1, ncolors))]", "category": "Python"}, {"instruction": "def _update_rhs(curr_rhs, xCore, zCore, new_rhs):\n    \"\"\" Function to be called from the project()\"\"\"\n", "input": "", "output": "    # TODO: Use intermediate variable to use 5 nested loops instead of 6.\n    r_x, n, r_old_x = xCore.shape\n    num_obj, r_z, n, r_old_z = zCore.shape\n    for idx in range(num_obj):\n        for val in range(n):\n            for alpha_old_z in range(r_old_z):\n                for alpha_z in range(r_z):\n                    for alpha_old_x in range(r_old_x):\n                        for alpha_x in range(r_x):\n                            curr_value = curr_rhs[idx, alpha_old_z, alpha_old_x]\n                            curr_value *= xCore[alpha_x, val, alpha_old_x]\n                            curr_value *= zCore[idx, alpha_z, val, alpha_old_z]\n                            new_rhs[idx, alpha_z, alpha_x] += curr_value", "category": "Python"}, {"instruction": "def download_file(url):\n    \"\"\"Download file\"\"\"\n", "input": "", "output": "\n    response = requests.get(url, stream=True)\n    fp = tempfile.NamedTemporaryFile()\n    for chunk in response.iter_content(chunk_size=1024):\n        if chunk:  # filter out keep-alive new chunks\n            fp.write(chunk)\n\n    # log.info(f'Download file - tmp file: {fp.name}  size: {fp.tell()}')\n    return fp", "category": "Python"}, {"instruction": "def join(self, channel):\n        \"\"\"Add this user to the channel's user list and add the channel to this\n        user's list of joined channels.\n        \"\"\"\n", "input": "", "output": "        \n        if channel not in self.channels:\n            channel.users.add(self.nick)\n            self.channels.append(channel)", "category": "Python"}, {"instruction": "def set_state(self, state, *, index=0):\n        \"\"\"Set state of a light.\"\"\"\n", "input": "", "output": "        return self.set_values({\n            ATTR_DEVICE_STATE: int(state)\n        }, index=index)", "category": "Python"}, {"instruction": "async def run(self, config, *, name=None):\n        \"\"\"\n        Create and start a container.\n\n        If container.start() will raise an error the exception will contain\n        a `container_id` attribute with the id of the container.\n        \"\"\"\n", "input": "", "output": "        try:\n            container = await self.create(config, name=name)\n        except DockerError as err:\n            # image not find, try pull it\n            if err.status == 404 and \"Image\" in config:\n                await self.docker.pull(config[\"Image\"])\n                container = await self.create(config, name=name)\n            else:\n                raise err\n\n        try:\n            await container.start()\n        except DockerError as err:\n            raise DockerContainerError(\n                err.status, {\"message\": err.message}, container[\"id\"]\n            )\n\n        return container", "category": "Python"}, {"instruction": "def remove_item(self, item):\n        \"\"\"Remove item from model\"\"\"\n", "input": "", "output": "        index = self.items.index(item)\n        self.beginRemoveRows(QtCore.QModelIndex(), index, index)\n        self.items.remove(item)\n        self.endRemoveRows()", "category": "Python"}, {"instruction": "def as_dict(self):\n        \"\"\"\n        turns attribute filter object into python dictionary\n        \"\"\"\n", "input": "", "output": "        output_dictionary = dict()\n\n        for key, value in iter(self._key_map.items()):\n            if isinstance(value, bool):\n                output_dictionary[key] = value\n            elif isinstance(value, self.__class__):\n                output_dictionary[key] = value.as_dict()\n\n        return output_dictionary", "category": "Python"}, {"instruction": "def walk(self, root):\r\n        \"\"\"\r\n        Walks the path for a returning the folders and roots for the\r\n        files found, similar to os.walk.\r\n        \r\n        :param      path | <str>\r\n        \"\"\"\n", "input": "", "output": "        files = []\r\n        folders = []\r\n        \r\n        for relpath in self.listdir(root):\r\n            if self.isfile(root + '/' + relpath):\r\n                files.append(relpath)\r\n            else:\r\n                folders.append(relpath)\r\n        \r\n        yield root, folders, files\r\n        \r\n        for folder in folders:\r\n            folderpath = root + '/' + folder\r\n            for sub_root, sub_folders, sub_files in self.walk(folderpath):\r\n                yield sub_root, sub_folders, sub_files", "category": "Python"}, {"instruction": "def attr_list(args):\n    '''Retrieve names of all attributes attached to a given object, either\n       an entity (if entity type+name is provided) or workspace (if not)'''\n", "input": "", "output": "    args.attributes = None\n    result = attr_get(args)\n    names = result.get(\"__header__\",[])\n    if names:\n        names = names[1:]\n    else:\n        names = result.keys()\n    return sorted(names)", "category": "Python"}, {"instruction": "def need_update(a, b):\n    \"\"\"\n    Check if file a is newer than file b and decide whether or not to update\n    file b. Can generalize to two lists.\n    \"\"\"\n", "input": "", "output": "    a = listify(a)\n    b = listify(b)\n\n    return any((not op.exists(x)) for x in b) or \\\n           all((os.stat(x).st_size == 0 for x in b)) or \\\n           any(is_newer_file(x, y) for x in a for y in b)", "category": "Python"}, {"instruction": "def get_all_hits(self):\n        ''' Get all HITs '''\n", "input": "", "output": "        if not self.connect_to_turk():\n            return False\n        try:\n            hits = []\n            paginator = self.mtc.get_paginator('list_hits')\n            for page in paginator.paginate():\n                hits.extend(page['HITs'])\n        except Exception as e:\n            print e\n            return False\n        hits_data = self._hit_xml_to_object(hits)\n        return hits_data", "category": "Python"}, {"instruction": "def calc_checksum(sentence):\n    \"\"\"Calculate a NMEA 0183 checksum for the given sentence.\n\n    NMEA checksums are a simple XOR of all the characters in the sentence\n    between the leading \"$\" symbol, and the \"*\" checksum separator.\n\n    Args:\n        sentence (str): NMEA 0183 formatted sentence\n    \"\"\"\n", "input": "", "output": "    if sentence.startswith('$'):\n        sentence = sentence[1:]\n    sentence = sentence.split('*')[0]\n    return reduce(xor, map(ord, sentence))", "category": "Python"}, {"instruction": "def guessFormat(self):\n        '''return quality score format -\n        might return several if ambiguous.'''\n", "input": "", "output": "\n        c = [ord(x) for x in self.quals]\n        mi, ma = min(c), max(c)\n        r = []\n        for entry_format, v in iteritems(RANGES):\n            m1, m2 = v\n            if mi >= m1 and ma < m2:\n                r.append(entry_format)\n        return r", "category": "Python"}, {"instruction": "def extract_prefix(self, prefix):\n        \"\"\" \n        It extracts an irc msg prefix chunk \n        \"\"\"\n", "input": "", "output": "\n        field = re.match(PREFIX_REG, prefix if prefix else '')\n        \n        return (prefix,) if not field else field.group(1, 2, 3)", "category": "Python"}, {"instruction": "def verify_firebase_token(id_token, request, audience=None):\n    \"\"\"Verifies an ID Token issued by Firebase Authentication.\n\n    Args:\n        id_token (Union[str, bytes]): The encoded token.\n        request (google.auth.transport.Request): The object used to make\n            HTTP requests.\n        audience (str): The audience that this token is intended for. This is\n            typically your Firebase application ID. If None then the audience\n            is not verified.\n\n    Returns:\n        Mapping[str, Any]: The decoded token.\n    \"\"\"\n", "input": "", "output": "    return verify_token(\n        id_token, request, audience=audience, certs_url=_GOOGLE_APIS_CERTS_URL)", "category": "Python"}, {"instruction": "def merge(self, path, outfile, infiles):\n        \"\"\"Merge files\n        \"\"\"\n", "input": "", "output": "        with open(path + outfile, 'w') as out_f:\n            for i in infiles:\n                if os.path.isfile(\"{0}{1}\".format(path, i)):\n                    with open(path + i, \"r\") as in_f:\n                        for line in in_f:\n                            out_f.write(line)", "category": "Python"}, {"instruction": "async def ping(self, conversation_id: uuid.UUID = None) -> float:\n        \"\"\"\n        Send a message to the remote server to check liveness.\n\n        Returns:\n            The round-trip time to receive a Pong message in fractional seconds\n\n        Examples:\n\n            >>> async with connect() as conn:\n            >>>     print(\"Sending a PING to the server\")\n            >>>     time_secs = await conn.ping()\n            >>>     print(\"Received a PONG after {} secs\".format(time_secs))\n\n        \"\"\"\n", "input": "", "output": "        cmd = convo.Ping(conversation_id=conversation_id or uuid.uuid4())\n        result = await self.dispatcher.start_conversation(cmd)\n\n        return await result", "category": "Python"}, {"instruction": "def ensure_writable(self):\n        \"\"\"Make sure the directory exists and is writable.\"\"\"\n", "input": "", "output": "        self.ensure_exists()\n        if not self.context.is_writable(self.directory):\n            if self.context.have_superuser_privileges:\n                msg = \"The directory %s isn't writable!\"\n                raise ValueError(msg % self)\n            else:\n                raise ValueError(compact(", "category": "Python"}, {"instruction": "def _replace_property(property_key, property_value, resource, logical_id):\n        \"\"\"\n        Replace a property with an asset on a given resource\n\n        This method will mutate the template\n\n        Parameters\n        ----------\n        property str\n            The property to replace on the resource\n        property_value str\n            The new value of the property\n        resource dict\n            Dictionary representing the Resource to change\n        logical_id str\n            LogicalId of the Resource\n\n        \"\"\"\n", "input": "", "output": "        if property_key and property_value:\n            resource.get(PROPERTIES_KEY, {})[property_key] = property_value\n        elif property_key or property_value:\n            LOG.info(\"WARNING: Ignoring Metadata for Resource %s. Metadata contains only aws:asset:path or \"\n                     \"aws:assert:property but not both\", logical_id)", "category": "Python"}, {"instruction": "def reference(self):\n    \"\"\"Return the Reference object for this Key.\n\n    This is a entity_pb.Reference instance -- a protocol buffer class\n    used by the lower-level API to the datastore.\n\n    NOTE: The caller should not mutate the return value.\n    \"\"\"\n", "input": "", "output": "    if self.__reference is None:\n      self.__reference = _ConstructReference(self.__class__,\n                                             pairs=self.__pairs,\n                                             app=self.__app,\n                                             namespace=self.__namespace)\n    return self.__reference", "category": "Python"}, {"instruction": "def get_window_size(self, windowHandle='current'):\n        \"\"\"\n        Gets the width and height of the current window.\n\n        :Usage:\n            ::\n\n                driver.get_window_size()\n        \"\"\"\n", "input": "", "output": "        command = Command.GET_WINDOW_SIZE\n        if self.w3c:\n            if windowHandle != 'current':\n                warnings.warn(\"Only 'current' window is supported for W3C compatibile browsers.\")\n            size = self.get_window_rect()\n        else:\n            size = self.execute(command, {'windowHandle': windowHandle})\n\n        if size.get('value', None) is not None:\n            size = size['value']\n\n        return {k: size[k] for k in ('width', 'height')}", "category": "Python"}, {"instruction": "def get_doc(self, doc_id):\n        \"\"\"\n        Get replication document state for a given replication document ID.\n        \"\"\"\n", "input": "", "output": "        resp = self._r_session.get('/'.join([self._scheduler, 'docs', '_replicator', doc_id]))\n        resp.raise_for_status()\n        return response_to_json_dict(resp)", "category": "Python"}, {"instruction": "def get_conf_attr(self, attr, default=None):\n        \"\"\"\n        Get the value of a attribute in the configuration\n\n        :param attr: The attribute\n        :param default: If the attribute doesn't appear in the configuration\n            return this value\n        :return: The value of attribute in the configuration or the default\n            value\n        \"\"\"\n", "input": "", "output": "        if attr in self.conf:\n            return self.conf[attr]\n        else:\n            return default", "category": "Python"}, {"instruction": "def raiseMasterKilled(signum, _stack):\n    \"\"\"\n    When a SIGTERM is received, raise the MasterKilled\n    exception with an appropriate error message.\n\n    :param int signum: the number of the received signal\n    :param _stack: the current frame object, ignored\n    \"\"\"\n", "input": "", "output": "    # Disable further CTRL-C to allow tasks revocation when Celery is used\n    if OQ_DISTRIBUTE.startswith('celery'):\n        signal.signal(signal.SIGINT, inhibitSigInt)\n\n    msg = 'Received a signal %d' % signum\n    if signum in (signal.SIGTERM, signal.SIGINT):\n        msg = 'The openquake master process was killed manually'\n\n    # kill the calculation only if os.getppid() != _PPID, i.e. the controlling\n    # terminal died; in the workers, do nothing\n    # NB: there is no SIGHUP on Windows\n    if hasattr(signal, 'SIGHUP'):\n        if signum == signal.SIGHUP:\n            if os.getppid() == _PPID:\n                return\n            else:\n                msg = 'The openquake master lost its controlling terminal'\n\n    raise MasterKilled(msg)", "category": "Python"}, {"instruction": "def _tab_P(P):\n    \"\"\"Define the boundary between Region 3a-3b, T=f(P)\n\n    Parameters\n    ----------\n    P : float\n        Pressure, [MPa]\n\n    Returns\n    -------\n    T : float\n        Temperature, [K]\n\n    References\n    ----------\n    IAPWS, Revised Supplementary Release on Backward Equations for Specific\n    Volume as a Function of Pressure and Temperature v(p,T) for Region 3 of the\n    IAPWS Industrial Formulation 1997 for the Thermodynamic Properties of Water\n    and Steam, http://www.iapws.org/relguide/Supp-VPT3-2016.pdf, Eq. 2\n\n    Examples\n    --------\n    >>> _tab_P(40)\n    693.0341408\n    \"\"\"\n", "input": "", "output": "    I = [0, 1, 2, -1, -2]\n    n = [0.154793642129415e4, -0.187661219490113e3, 0.213144632222113e2,\n         -0.191887498864292e4, 0.918419702359447e3]\n\n    Pr = P/1\n    T = 0\n    for i, ni in zip(I, n):\n        T += ni * log(Pr)**i\n    return T", "category": "Python"}, {"instruction": "def split_strings_in_list_retain_spaces(orig_list):\n    \"\"\"\n    Function to split every line in a list, and retain spaces for a rejoin\n    :param orig_list: Original list\n    :return:\n        A List with split lines\n\n    \"\"\"\n", "input": "", "output": "    temp_list = list()\n    for line in orig_list:\n        line_split = __re.split(r'(\\s+)', line)\n        temp_list.append(line_split)\n\n    return temp_list", "category": "Python"}, {"instruction": "def split_by_commas(maybe_s: str) -> Tuple[str, ...]:\n    \"\"\"Split a string by commas, but allow escaped commas.\n    - If maybe_s is falsey, returns an empty tuple\n    - Ignore backslashed commas\n    \"\"\"\n", "input": "", "output": "    if not maybe_s:\n        return ()\n    parts: List[str] = []\n    split_by_backslash = maybe_s.split(r'\\,')\n    for split_by_backslash_part in split_by_backslash:\n        splitby_comma = split_by_backslash_part.split(',')\n        if parts:\n            parts[-1] += ',' + splitby_comma[0]\n        else:\n            parts.append(splitby_comma[0])\n        parts.extend(splitby_comma[1:])\n    return tuple(parts)", "category": "Python"}, {"instruction": "def _get_language_with_alpha2_fallback(language_code):\n    \"\"\"\n    Lookup language code `language_code` (string) in the internal language codes,\n    and if that fails, try to map map `language_code` to the internal represention\n    using the `getlang_by_alpha2` helper method.\n    Returns either a le-utils Language object or None if both lookups fail.\n    \"\"\"\n", "input": "", "output": "    # 1. try to lookup `language` using internal representation\n    language_obj = languages.getlang(language_code)\n    # if language_obj not None, we know `language` is a valid language_id in the internal repr.\n    if language_obj is None:\n        # 2. try to match by two-letter ISO code\n        language_obj = languages.getlang_by_alpha2(language_code)\n    return language_obj", "category": "Python"}, {"instruction": "def icon(cls, size):\n        \"\"\"Returns an icon to use for the game.\"\"\"\n", "input": "", "output": "        tile = pygame.Surface((size, size))\n        tile.fill((237, 194, 46))\n        label = load_font(cls.BOLD_NAME, int(size / 3.2)).render(cls.NAME, True, (249, 246, 242))\n        width, height = label.get_size()\n        tile.blit(label, ((size - width) / 2, (size - height) / 2))\n        return tile", "category": "Python"}, {"instruction": "def validate_many(records, schema, raise_errors=True):\n    \"\"\"\n    Validate a list of data!\n\n    Parameters\n    ----------\n    records: iterable\n        List of records to validate\n    schema: dict\n        Schema\n    raise_errors: bool, optional\n        If true, errors are raised for invalid data. If false, a simple\n        True (valid) or False (invalid) result is returned\n\n\n    Example::\n\n        from fastavro.validation import validate_many\n        schema = {...}\n        records = [{...}, {...}, ...]\n        validate_many(records, schema)\n    \"\"\"\n", "input": "", "output": "    errors = []\n    results = []\n    for record in records:\n        try:\n            results.append(validate(record, schema, raise_errors=raise_errors))\n        except ValidationError as e:\n            errors.extend(e.errors)\n    if raise_errors and errors:\n        raise ValidationError(*errors)\n    return all(results)", "category": "Python"}, {"instruction": "def createPolyline(self, points, strokewidth=1, stroke='black'):\n        \"\"\"\n        Creates a Polyline\n        @type  points: string in the form \"x1,y1 x2,y2 x3,y3\"\n        @param points:  all points relevant to the polygon\n        @type  strokewidth: string or int\n        @param strokewidth:  width of the pen used to draw\n        @type  stroke: string (either css constants like \"black\" or numerical values like \"#FFFFFF\")\n        @param stroke:  color with which to draw the outer limits\n        @return:  a polyline object\n        \"\"\"\n", "input": "", "output": "        style_dict = {'fill':'none', 'stroke-width':strokewidth, 'stroke':stroke}\n        myStyle = StyleBuilder(style_dict)\n        p = Polyline(points=points)\n        p.set_style(myStyle.getStyle())\n        return p", "category": "Python"}, {"instruction": "def add_sim_errors(self, qname, sname, value, sym=True):\n        \"\"\"Add a similarity error value to self.similarity_errors.\"\"\"\n", "input": "", "output": "        self.similarity_errors.loc[qname, sname] = value\n        if sym:\n            self.similarity_errors.loc[sname, qname] = value", "category": "Python"}, {"instruction": "def discard_queue_messages(self):\n\t\t\"\"\" Sometimes it is necessary to drop undelivered messages. These messages may be stored in different\n\t\tcaches, for example in a zmq socket queue. With different zmq flags we can tweak zmq sockets and\n\t\tcontexts no to keep those messages. But inside ZMQStream class there is a queue that can not be\n\t\tcleaned other way then the way it does in this method. So yes, it is dirty to access protected\n\t\tmembers, and yes it can be broken at any moment. And yes without correct locking procedure there\n\t\tis a possibility of unpredicted behaviour. But still - there is no other way to drop undelivered\n\t\tmessages\n\n\t\tDiscussion of the problem: https://github.com/zeromq/pyzmq/issues/1095\n\n\t\t:return: None\n\t\t\"\"\"\n", "input": "", "output": "\t\tzmq_stream_queue = self.handler().stream()._send_queue\n\t\twhile not zmq_stream_queue.empty():\n\t\t\ttry:\n\t\t\t\tzmq_stream_queue.get(False)\n\t\t\texcept queue.Empty:\n\t\t\t\tcontinue\n\t\t\tzmq_stream_queue.task_done()", "category": "Python"}, {"instruction": "def azm(self):\n        \"\"\"Corrected azimuth, taking into account backsight, declination, and compass corrections.\"\"\"\n", "input": "", "output": "        azm1 = self.get('BEARING', None)\n        azm2 = self.get('AZM2', None)\n        if azm1 is None and azm2 is None:\n            return None\n        if azm2 is None:\n            return azm1 + self.declination\n        if azm1 is None:\n            return (azm2 + 180) % 360 + self.declination\n        return (azm1 + (azm2 + 180) % 360) / 2.0 + self.declination", "category": "Python"}, {"instruction": "def new_get_angle_diff(v1,v2):\n    \"\"\"returns angular difference in degrees between two vectors.  may be more precise in certain cases.  see SPD\"\"\"\n", "input": "", "output": "    v1 = numpy.array(v1)\n    v2 = numpy.array(v2)\n    angle = numpy.arctan2(numpy.linalg.norm(numpy.cross(v1, v2)), numpy.dot(v1, v2))\n    return math.degrees(angle)", "category": "Python"}, {"instruction": "def status_unpin(self, id):\n        \"\"\"\n        Unpin a pinned status for the logged-in user.\n\n        Returns a `toot dict`_ with the status that used to be pinned.\n        \"\"\"\n", "input": "", "output": "        id = self.__unpack_id(id)\n        url = '/api/v1/statuses/{0}/unpin'.format(str(id))\n        return self.__api_request('POST', url)", "category": "Python"}, {"instruction": "def level(self, level, time=0):\n        \"\"\"(Helper) Set light to specified level\"\"\"\n", "input": "", "output": "        if level <= 0:\n            self._elk.send(pf_encode(self._index))\n        elif level >= 98:\n            self._elk.send(pn_encode(self._index))\n        else:\n            self._elk.send(pc_encode(self._index, 9, level, time))", "category": "Python"}, {"instruction": "def get_content_type(*names):\n    \"\"\"Return the MIME content type for the file with the given name.\"\"\"\n", "input": "", "output": "    for name in names:\n        if name is not None:\n            mimetype, encoding = mimetypes.guess_type(name)\n            if mimetype is not None:\n                if isinstance(mimetype, bytes):\n                    return mimetype.decode(\"ascii\")\n                else:\n                    return mimetype\n    else:\n        return \"application/octet-stream\"", "category": "Python"}, {"instruction": "def validate(cls, job_config):\n    \"\"\"Inherit docs.\"\"\"\n", "input": "", "output": "    super(DatastoreInputReader, cls).validate(job_config)\n    params = job_config.input_reader_params\n    entity_kind = params[cls.ENTITY_KIND_PARAM]\n    # Check for a \".\" in the entity kind.\n    if \".\" in entity_kind:\n      logging.warning(\n          \". detected in entity kind %s specified for reader %s.\"\n          \"Assuming entity kind contains the dot.\",\n          entity_kind, cls.__name__)\n    # Validate the filters parameters.\n    if cls.FILTERS_PARAM in params:\n      filters = params[cls.FILTERS_PARAM]\n      for f in filters:\n        if f[1] != \"=\":\n          raise errors.BadReaderParamsError(\n              \"Only equality filters are supported: %s\", f)", "category": "Python"}, {"instruction": "def __get_descendants(node, dfs_data):\n    \"\"\"Gets the descendants of a node.\"\"\"\n", "input": "", "output": "    list_of_descendants = []\n\n    stack = deque()\n\n    children_lookup = dfs_data['children_lookup']\n\n    current_node = node\n    children = children_lookup[current_node]\n    dfs_current_node = D(current_node, dfs_data)\n    for n in children:\n        dfs_child = D(n, dfs_data)\n        # Validate that the child node is actually a descendant and not an ancestor\n        if dfs_child > dfs_current_node:\n            stack.append(n)\n\n    while len(stack) > 0:\n        current_node = stack.pop()\n        list_of_descendants.append(current_node)\n        children = children_lookup[current_node]\n        dfs_current_node = D(current_node, dfs_data)\n        for n in children:\n            dfs_child = D(n, dfs_data)\n            # Validate that the child node is actually a descendant and not an ancestor\n            if dfs_child > dfs_current_node:\n                stack.append(n)\n\n    return list_of_descendants", "category": "Python"}, {"instruction": "def dim_dm(self, pars):\n        r\"\"\"\n        :math:Add formula\n        \"\"\"\n", "input": "", "output": "        self._set_parameters(pars)\n        num1 = self.otc * np.sin(self.ang)\n        result = -self.sigmai * num1 / self.denom\n\n        return result", "category": "Python"}, {"instruction": "def gather_defaults(self):\n        \"\"\"\n        Return the number of default variables.\n        \"\"\"\n", "input": "", "output": "        total_defaults = 0\n        defaults_lines = []\n\n        if not os.path.exists(self.paths[\"defaults\"]):\n            # reset the defaults if no defaults were found\n            self.defaults = \"\"\n            return 0\n\n        file = open(self.paths[\"defaults\"], \"r\")\n        for line in file:\n            if len(line) > 0:\n                first_char = line[0]\n            else:\n                first_char = \"\"\n\n            defaults_lines.append(line)\n\n            if (first_char != \"#\" and first_char != \"-\" and\n                    first_char != \" \" and\n                    first_char != \"\\r\" and\n                    first_char != \"\\n\" and\n                    first_char != \"\\t\"):\n                total_defaults += 1\n        file.close()\n\n        self.defaults = \"\".join(defaults_lines)\n\n        return total_defaults", "category": "Python"}, {"instruction": "def get_waveset(model):\n    \"\"\"Get optimal wavelengths for sampling a given model.\n\n    Parameters\n    ----------\n    model : `~astropy.modeling.Model`\n        Model.\n\n    Returns\n    -------\n    waveset : array-like or `None`\n        Optimal wavelengths. `None` if undefined.\n\n    Raises\n    ------\n    synphot.exceptions.SynphotError\n        Invalid model.\n\n    \"\"\"\n", "input": "", "output": "    if not isinstance(model, Model):\n        raise SynphotError('{0} is not a model.'.format(model))\n\n    if isinstance(model, _CompoundModel):\n        waveset = model._tree.evaluate(WAVESET_OPERATORS, getter=None)\n    else:\n        waveset = _get_sampleset(model)\n\n    return waveset", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"Start this LXC\"\"\"\n", "input": "", "output": "        if self.status == 'RUNNING':\n            raise LXCAlreadyStarted(self.name)\n        self._service.start(self.name)", "category": "Python"}, {"instruction": "def manage_inventory(self):\n        \"\"\"\n        Manages inventory for Ansible and returns None.\n\n        :returns: None\n        \"\"\"\n", "input": "", "output": "        self._write_inventory()\n        self._remove_vars()\n        if not self.links:\n            self._add_or_update_vars()\n        else:\n            self._link_or_update_vars()", "category": "Python"}, {"instruction": "def _expect(self, expected, times=50):\n        \"\"\"Find the `expected` line within `times` trials.\n\n        Args:\n            expected    str: the expected string\n            times       int: number of trials\n        \"\"\"\n", "input": "", "output": "        logger.debug('[%s] Expecting [%s]', self.port, expected)\n        retry_times = 10\n        while times:\n            if not retry_times:\n                break\n\n            line = self._readline()\n\n            if line == expected:\n                return\n\n            if not line:\n                retry_times -= 1\n                time.sleep(0.1)\n\n            times -= 1\n\n        raise Exception('failed to find expected string[%s]' % expected)", "category": "Python"}, {"instruction": "def acquire(self, **kwargs):\n        \"\"\"\n        Copy the file and return its path\n\n        Returns\n        -------\n        str or None\n            The path of the file or None if it does not exist or if\n            verification failed.\n        \"\"\"\n", "input": "", "output": "        path = path_string(self.path)\n        if os.path.exists(path):\n            if config.verify_file(path, self.sha256):\n                return path\n        return None", "category": "Python"}, {"instruction": "def query_jobs_schedule(repo_name, revision, auth):\n    \"\"\"\n    Query Buildapi for jobs.\n    \"\"\"\n", "input": "", "output": "    url = \"%s/%s/rev/%s?format=json\" % (SELF_SERVE, repo_name, revision)\n    LOG.debug(\"About to fetch %s\" % url)\n    req = requests.get(url, auth=auth, timeout=TCP_TIMEOUT)\n\n    # If the revision doesn't exist on buildapi, that means there are\n    # no buildapi jobs for this revision\n    if req.status_code not in [200]:\n        return []\n\n    return req.json()", "category": "Python"}, {"instruction": "def _inherited_value(self, attr_name):\n        \"\"\"\n        Return the attribute value, e.g. 'width' of the base placeholder this\n        placeholder inherits from.\n        \"\"\"\n", "input": "", "output": "        base_placeholder = self._base_placeholder\n        if base_placeholder is None:\n            return None\n        inherited_value = getattr(base_placeholder, attr_name)\n        return inherited_value", "category": "Python"}, {"instruction": "def _on_selection(self):\n        \"\"\"\n        Internal function to handle directory traversal or bubble notifications up to user of the\n        Widget as needed.\n        \"\"\"\n", "input": "", "output": "        if self.value and os.path.isdir(self.value):\n            self._populate_list(self.value)\n        elif self._external_notification:\n            self._external_notification()", "category": "Python"}, {"instruction": "def union_overlapping(intervals):\n    \"\"\"Union any overlapping intervals in the given set.\"\"\"\n", "input": "", "output": "    disjoint_intervals = []\n\n    for interval in intervals:\n        if disjoint_intervals and disjoint_intervals[-1].overlaps(interval):\n            disjoint_intervals[-1] = disjoint_intervals[-1].union(interval)\n        else:\n            disjoint_intervals.append(interval)\n\n    return disjoint_intervals", "category": "Python"}, {"instruction": "def prepare_params(request):\n    \"\"\"prepare request parameters\"\"\"\n", "input": "", "output": "    return request.replace(\n        params={key: dump_param(val) for key, val in request.params.items()\n                if val is not None})", "category": "Python"}, {"instruction": "def get_asset_address(self, asset: str) -> bytes:\n        \"\"\"\n        This interface is used to get the smart contract address of ONT otr ONG.\n\n        :param asset: a string which is used to indicate which asset's contract address we want to get.\n        :return: the contract address of asset in the form of bytearray.\n        \"\"\"\n", "input": "", "output": "        if asset.upper() == 'ONT':\n            return self.__ont_contract\n        elif asset.upper() == 'ONG':\n            return self.__ong_contract\n        else:\n            raise SDKException(ErrorCode.other_error('asset is not equal to ONT or ONG.'))", "category": "Python"}, {"instruction": "def disabledPenColor(self):\n        \"\"\"\n        Returns the disabled pen color for this node.\n        \n        :return     <QColor>\n        \"\"\"\n", "input": "", "output": "        palette = self.palette()\n        return palette.color(palette.Disabled, palette.NodeForeground)", "category": "Python"}, {"instruction": "def perform_smooth(x_values, y_values, span=None, smoother_cls=None):\n    \"\"\"\n    Convenience function to run the basic smoother.\n\n    Parameters\n    ----------\n    x_values : iterable\n        List of x value observations\n    y_ values : iterable\n        list of y value observations\n    span : float, optional\n        Fraction of data to use as the window\n    smoother_cls : Class\n        The class of smoother to use to smooth the data\n\n    Returns\n    -------\n    smoother : object\n        The smoother object with results stored on it.\n    \"\"\"\n", "input": "", "output": "    if smoother_cls is None:\n        smoother_cls = DEFAULT_BASIC_SMOOTHER\n    smoother = smoother_cls()\n    smoother.specify_data_set(x_values, y_values)\n    smoother.set_span(span)\n    smoother.compute()\n    return smoother", "category": "Python"}, {"instruction": "def _load_meta(self, meta):\n        '''Load data from meta.yaml to a dictionary'''\n", "input": "", "output": "        meta = yaml.load(meta, Loader=Loader)\n\n        # Versions are often specified in a format that is convertible to an\n        # int or a float, so we want to make sure it is interpreted as a str.\n        # Fix for the bug #300.\n        if 'version' in meta:\n            meta['version'] = str(meta['version'])\n\n        return meta", "category": "Python"}, {"instruction": "def _check_device_failover_status(self, device, status):\n        '''Determine if a device has a specific failover status.\n\n        :param status: str -- status to check against\n        :returns: bool -- True is it has status, False otherwise\n        '''\n", "input": "", "output": "\n        sync_status = device.tm.cm.sync_status\n        sync_status.refresh()\n        current_status = (sync_status.entries[self.sync_status_entry]\n                          ['nestedStats']['entries']['status']\n                          ['description'])\n        if status == current_status:\n            return True\n        return False", "category": "Python"}, {"instruction": "def get_genes(genes_str):\n        \"\"\"Given a string containng genes, return a list.\"\"\"\n", "input": "", "output": "        gene_set = genes_str.split(', ')\n        if gene_set and gene_set[0].isdigit():\n            gene_set = set(int(g) for g in gene_set)\n        return gene_set", "category": "Python"}, {"instruction": "def _get_row_sparse(self, arr_list, ctx, row_id):\n        \"\"\" Get row_sparse data from row_sparse parameters based on row_id. \"\"\"\n", "input": "", "output": "        # get row sparse params based on row ids\n        if not isinstance(row_id, ndarray.NDArray):\n            raise TypeError(\"row_id must have NDArray type, but %s is given\"%(type(row_id)))\n        if not self._trainer:\n            raise RuntimeError(\"Cannot get row_sparse data for Parameter '%s' when no \" \\\n                               \"Trainer is created with it.\"%self.name)\n        results = self._check_and_get(arr_list, ctx)\n\n        # fetch row sparse params from the trainer\n        self._trainer._row_sparse_pull(self, results, row_id)\n        return results", "category": "Python"}, {"instruction": "def repo_name(self):\n        \"\"\"\n        Returns a DataFrame of the repo names present in this project directory\n\n        :return: DataFrame\n\n        \"\"\"\n", "input": "", "output": "\n        ds = [[x.repo_name] for x in self.repos]\n        df = pd.DataFrame(ds, columns=['repository'])\n        return df", "category": "Python"}, {"instruction": "def update_from_json(self, json_device):\n        \"\"\"Set all attributes based on API response.\"\"\"\n", "input": "", "output": "        self.identifier = json_device['Id']\n        self.license_plate = json_device['EquipmentHeader']['SerialNumber']\n        self.make = json_device['EquipmentHeader']['Make']\n        self.model = json_device['EquipmentHeader']['Model']\n        self.equipment_id = json_device['EquipmentHeader']['EquipmentID']\n        self.active = json_device['EngineRunning']\n        self.odo = json_device['Odometer']\n        self.latitude = json_device['Location']['Latitude']\n        self.longitude = json_device['Location']['Longitude']\n        self.altitude = json_device['Location']['Altitude']\n        self.speed = json_device['Speed']\n        self.last_seen = json_device['Location']['DateTime']", "category": "Python"}, {"instruction": "def get_asset_content_ids(self):\n        \"\"\"Gets the content ``Ids`` of this asset.\n\n        return: (osid.id.IdList) - the asset content ``Ids``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for osid.repository.Asset.get_asset_content_ids_template\n        id_list = []\n        for asset_content in self.get_asset_contents():\n            id_list.append(asset_content.get_id())\n        return IdList(id_list)", "category": "Python"}, {"instruction": "def _define_jco_args(cmd_parser):\n    \"\"\"\n    Define job configuration arguments.\n    Returns groups defined, currently one.\n    \"\"\"\n", "input": "", "output": "    jo_group = cmd_parser.add_argument_group('Job options', 'Job configuration options')\n\n    jo_group.add_argument('--job-name', help='Job name')\n    jo_group.add_argument('--preload', action='store_true', help='Preload job onto all resources in the instance')\n    jo_group.add_argument('--trace', choices=['error', 'warn', 'info', 'debug', 'trace'], help='Application trace level')\n\n    jo_group.add_argument('--submission-parameters', '-p', nargs='+', action=_SubmitParamArg, help=\"Submission parameters as name=value pairs\")\n\n    jo_group.add_argument('--job-config-overlays', help=\"Path to file containing job configuration overlays JSON. Overrides any job configuration set by the application.\" , metavar='file')\n\n    return jo_group,", "category": "Python"}, {"instruction": "def join(self, timeout_s=None):\n    \"\"\"Joins blocking until the interval ends or until timeout is reached.\n\n    Args:\n      timeout_s: The time in seconds to wait, defaults to forever.\n    Returns:\n      True if the interval is still running and we reached the timeout.\n    \"\"\"\n", "input": "", "output": "    if not self.thread:\n      return False\n    self.thread.join(timeout_s)\n    return self.running", "category": "Python"}, {"instruction": "def rnaseqc_general_stats (self):\n        \"\"\"\n        Add alignment rate to the general stats table\n        \"\"\"\n", "input": "", "output": "        headers = OrderedDict()\n        headers['Expression Profiling Efficiency'] = {\n            'title': '% Expression Efficiency',\n            'description': 'Expression Profiling Efficiency: Ratio of exon reads to total reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'YlGn',\n            'modify': lambda x: float(x) * 100.0\n        }\n        headers['Genes Detected'] = {\n            'title': '# Genes',\n            'description': 'Number of genes detected with at least 5 reads.',\n            'min': 0,\n            'scale': 'Bu',\n            'format': '{:,.0f}'\n        }\n        headers['rRNA rate'] = {\n            'title': '% rRNA Alignment',\n            'description': ' rRNA reads (non-duplicate and duplicate reads) per total reads',\n            'max': 100,\n            'min': 0,\n            'suffix': '%',\n            'scale': 'Reds',\n            'modify': lambda x: float(x) * 100.0\n        }\n\n        self.general_stats_addcols(self.rna_seqc_metrics, headers)", "category": "Python"}, {"instruction": "def emit( self, record ):\n        \"\"\" \n        Throws an error based on the information that the logger reported,\n        given the logging level.\n        \n        :param      record | <logging.LogRecord>\n        \"\"\"\n", "input": "", "output": "        msg   = self._formatter.format(record)\n        align = self._splash.textAlignment()\n        fg    = self._splash.textColor()\n        \n        self._splash.showMessage(msg, align, fg)", "category": "Python"}, {"instruction": "def download_file(url, file_name):\n    \"\"\"\n    Helper for downloading a remote file to disk.\n    \"\"\"\n", "input": "", "output": "\n    logger.info(\"Downloading URL: %s\", url)\n    file_size = 0\n\n    if not os.path.isfile(file_name):\n        response = requests.get(url, stream=True)\n\n        with open(file_name, \"wb\") as fp:\n            if not response.ok:\n                raise Exception(\"Download exception. Will fail.\")\n\n            for block in response.iter_content(1024):\n                if not block:\n                    break\n\n                fp.write(block)\n                file_size += len(block)\n\n        logger.info(\"Download finished, size is %d bytes.\", file_size)\n\n    return file_size", "category": "Python"}, {"instruction": "def ffd(items, targets, **kwargs):\n    \"\"\"First-Fit Decreasing\n\n    This is perhaps the simplest packing heuristic;\n    it simply packs items in the next available bin.\n\n    This algorithm differs only from Next-Fit Decreasing\n    in having a 'sort'; that is, the items are pre-sorted\n    (largest to smallest).\n\n    Complexity O(n^2)\n    \"\"\"\n", "input": "", "output": "    sizes = zip(items, weight(items, **kwargs))\n    sizes = sorted(sizes, key=operator.itemgetter(1), reverse=True)\n    items = map(operator.itemgetter(0), sizes)\n    return ff(items, targets)", "category": "Python"}, {"instruction": "def require_api_auth(allow_anonymous=False):\n    \"\"\"Decorator to require API authentication using OAuth token.\n\n    :param allow_anonymous: Allow access without OAuth token\n        (default: ``False``).\n    \"\"\"\n", "input": "", "output": "    def wrapper(f):\n        ", "category": "Python"}, {"instruction": "def load_window_opener(self, item):\n        \"\"\"Load window opener from JSON.\"\"\"\n", "input": "", "output": "        window = Window.from_config(self.pyvlx, item)\n        self.add(window)", "category": "Python"}, {"instruction": "def needs_refresh(self, source):\n        \"\"\"Has the (persisted) source expired in the store\n\n        Will return True if the source is not in the store at all, if it's\n        TTL is set to None, or if more seconds have passed than the TTL.\n        \"\"\"\n", "input": "", "output": "        now = time.time()\n        if source._tok in self:\n            s0 = self[source._tok]\n            if self[source._tok].metadata.get('ttl', None):\n                then = s0.metadata['timestamp']\n                if s0.metadata['ttl'] < then - now:\n                    return True\n            return False\n        return True", "category": "Python"}, {"instruction": "def search(self, s, stype=1, offset=0, total='true', limit=60):\n        \"\"\"get songs list from search keywords\"\"\"\n", "input": "", "output": "        action = uri + '/search/get'\n        data = {\n            's': s,\n            'type': stype,\n            'offset': offset,\n            'total': total,\n            'limit': 60\n        }\n        resp = self.request('POST', action, data)\n        if resp['code'] == 200:\n            return resp['result']['songs']\n        return []", "category": "Python"}, {"instruction": "def writeParams(rawfilepath, outputpath, isolationWindow, coElute=0):\n    \"\"\"Generate and write a pParse parameter file.\n\n    :param rawfilepath: location of the thermo \".raw\" file\n    :param outputpath: path to the output directory of pParse\n    :param isolationWindow: MSn isolation window that was used for the\n        aquisition of the specified thermo raw file\n    :param coElute:\n\n    :returns: file path of the pParse parameter file\n    \"\"\"\n", "input": "", "output": "    paramText = generateParams(rawfilepath, outputpath, isolationWindow,\n                               coElute)\n    filename, fileext = os.path.splitext(os.path.basename(rawfilepath))\n    paramPath = aux.joinpath(outputpath, filename+'.pparse.para')\n    with open(paramPath, 'wb') as openfile:\n        openfile.write(paramText)\n    return paramPath", "category": "Python"}, {"instruction": "def log_type(args_kw, ret, func, slf=False, prop_getter=False, clss=None, argspecs=None,\n            args_kw_type=None, ret_type = None):\n    \"\"\"Stores information of a function or method call into a cache, so pytypes can\n    create a PEP 484 stubfile from this information later on (see dump_cache).\n    \"\"\"\n", "input": "", "output": "    if args_kw_type is None:\n        args_kw_type = deep_type(args_kw)\n    if ret_type is None:\n        ret_type = deep_type(ret)\n    if argspecs is None:\n        argspecs = getargspecs(func)\n    node = _register_logged_func(func, slf, prop_getter, clss, argspecs)\n    node.add_observation(args_kw_type, ret_type)\n    \n    md = util.getmodule_for_member(func, prop_getter)\n    if not md.__name__ in _module_file_map:\n        _module_file_map[md.__name__] = md.__file__\n\n    if clss is None:\n        try:\n            clss = util.get_class_that_defined_method(func)\n        except ValueError:\n            pass\n    if not clss is None and not clss in _member_line_map:\n        _member_line_map[clss] = findsource(clss)[1]", "category": "Python"}, {"instruction": "def get_user(self, user_id):\n        \"\"\"\n        Details for a specific user.\n        Will pull from cached users first, or get and add to cached users.\n        :param username: the username or userId of the user\n        :return: VoicebaseUser\n        \"\"\"\n", "input": "", "output": "        if user_id in self._users:\n            return self._users.get(user_id)\n        else:\n            # Load user\n            # Save user in cache\n            return", "category": "Python"}, {"instruction": "def _check_raw(self, file_names, abort_on_missing=False):\n        \"\"\"Get the file-ids for the res_files.\"\"\"\n", "input": "", "output": "\n        strip_file_names = True\n        check_on = self.filestatuschecker\n        if not self._is_listtype(file_names):\n            file_names = [file_names, ]\n\n        ids = dict()\n        for f in file_names:\n            self.logger.debug(f\"checking res file {f}\")\n            fid = FileID(f)\n            # self.logger.debug(fid)\n            if fid.name is None:\n                warnings.warn(f\"file does not exist: {f}\")\n                if abort_on_missing:\n                    sys.exit(-1)\n            else:\n                if strip_file_names:\n                    name = os.path.basename(f)\n                else:\n                    name = f\n                if check_on == \"size\":\n                    ids[name] = int(fid.size)\n                elif check_on == \"modified\":\n                    ids[name] = int(fid.last_modified)\n                else:\n                    ids[name] = int(fid.last_accessed)\n        return ids", "category": "Python"}, {"instruction": "def get_pkg_info(\n    package_name, additional=(\"pip\", \"flit\", \"pbr\", \"setuptools\", \"wheel\")\n):\n    \"\"\"Return build and package dependencies as a dict.\"\"\"\n", "input": "", "output": "    dist_index = build_dist_index(pkg_resources.working_set)\n    root = dist_index[package_name]\n    tree = construct_tree(dist_index)\n    dependencies = {pkg.name: pkg.installed_version for pkg in tree[root]}\n    # Add the initial package itself.\n    root = root.as_requirement()\n    dependencies[root.name] = root.installed_version\n    # Retrieve information on additional packages such as build tools.\n    for name in additional:\n        try:\n            pkg = dist_index[name].as_requirement()\n            dependencies[pkg.name] = pkg.installed_version\n        except KeyError:\n            continue\n    return dependencies", "category": "Python"}, {"instruction": "def order_by_on_list(objects, order_field, is_desc=False):\n    \"\"\"\n    Utility function to sort objects django-style even for non-query set collections\n\n    :param objects: list of objects to sort\n    :param order_field: field name, follows django conventions, so \"foo__bar\" means `foo.bar`, can be a callable.\n    :param is_desc: reverse the sorting\n    :return:\n    \"\"\"\n", "input": "", "output": "    if callable(order_field):\n        objects.sort(key=order_field, reverse=is_desc)\n        return\n\n    def order_key(x):\n        v = getattr_path(x, order_field)\n        if v is None:\n            return MIN\n        return v\n\n    objects.sort(key=order_key, reverse=is_desc)", "category": "Python"}, {"instruction": "def import_eit_fzj(self, filename, configfile, correction_file=None,\n                       timestep=None, **kwargs):\n        \"\"\"EIT data import for FZJ Medusa systems\"\"\"\n", "input": "", "output": "        # we get not electrode positions (dummy1) and no topography data\n        # (dummy2)\n        df_emd, dummy1, dummy2 = eit_fzj.read_3p_data(\n            filename,\n            configfile,\n            **kwargs\n        )\n        if correction_file is not None:\n            eit_fzj_utils.apply_correction_factors(df_emd, correction_file)\n\n        if timestep is not None:\n            df_emd['timestep'] = timestep\n\n        self._add_to_container(df_emd)\n\n        print('Summary:')\n        self._describe_data(df_emd)", "category": "Python"}, {"instruction": "def get_certificates_der_v3(self):\n        \"\"\"\n        Return a list of DER coded X.509 certificates from the v3 signature block\n        \"\"\"\n", "input": "", "output": "\n        if self._v3_signing_data == None:\n            self.parse_v3_signing_block()\n\n        certs = []\n        for signed_data in [signer.signed_data for signer in self._v3_signing_data]:\n            for cert in signed_data.certificates:\n                certs.append(cert)\n\n        return certs", "category": "Python"}, {"instruction": "def sents(self, fileids=None) -> Generator[str, str, None]:\n        \"\"\"\n        :param fileids:\n        :return: A generator of sentences\n        \"\"\"\n", "input": "", "output": "        for para in self.paras(fileids):\n            sentences = self._sent_tokenizer.tokenize(para)\n            for sentence in sentences:\n                yield sentence", "category": "Python"}, {"instruction": "def get_my_choices_projects():\n    \"\"\" Retrieves all projects in the system\n        for the project management page\n    \"\"\"\n", "input": "", "output": "\n    proj_list = Project.objects.all()\n    proj_tuple = []\n    counter = 1\n    for proj in proj_list:\n        proj_tuple.append((counter, proj))\n        counter = counter + 1\n    return proj_tuple", "category": "Python"}, {"instruction": "def update_col(input, **params):\n    \"\"\"\n    Updates document with value from another document/collection/constant\n    :param input:\n    :param params:\n    :return:\n    \"\"\"\n", "input": "", "output": "    PARAM_TARGET = 'target'\n    PARAM_UPDATE = 'update'\n    SOURCE_TYPE = 'src.type'\n    SOURCE_COL = 'src.col'\n    SOURCE_FIELD = 'src.field'\n    DEST_FIELD = 'dest.field'\n    SOURCE_TYPE_DOC = 'doc'\n    SOURCE_TYPE_CONST = 'const'\n    CONST_VALUE = 'const.value'\n\n    update_list = params.get(PARAM_UPDATE)\n    res = input[params.get(PARAM_TARGET)] if PARAM_TARGET in params else input\n    for row in res:\n        for update_desc in update_list:\n            if update_desc[SOURCE_TYPE] == SOURCE_TYPE_DOC:\n                row[update_desc[DEST_FIELD]] = input[update_desc[SOURCE_COL]][update_desc[SOURCE_FIELD]]\n            elif update_desc[SOURCE_TYPE] == SOURCE_TYPE_CONST:\n                row[update_desc[DEST_FIELD]] = update_desc[CONST_VALUE]\n    return res", "category": "Python"}, {"instruction": "def setSamplefrequency(self, edfsignal, samplefrequency):\n        \"\"\"\n        Sets the samplefrequency of signal edfsignal.\n\n        Notes\n        -----\n        This function is required for every signal and can be called only after opening a file in writemode and before the first sample write action.\n        \"\"\"\n", "input": "", "output": "        if edfsignal < 0 or edfsignal > self.n_channels:\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['sample_rate'] = samplefrequency\n        self.update_header()", "category": "Python"}, {"instruction": "def remove_tag(self, tag, value, silent_fail=False):\n        \"\"\" we try to remove supplied pair tag -- value, and if does not exist outcome depends on the silent_fail flag \"\"\"\n", "input": "", "output": "        try:\n            self.tags.remove((tag, value))\n        except ValueError as err:\n            if not silent_fail:\n                raise err", "category": "Python"}, {"instruction": "def plot_prep_methods(df, prep, prepi, out_file_base, outtype, title=None,\n                      size=None):\n    \"\"\"Plot comparison between BAM preparation methods.\n    \"\"\"\n", "input": "", "output": "    samples = df[(df[\"bamprep\"] == prep)][\"sample\"].unique()\n    assert len(samples) >= 1, samples\n    out_file = \"%s-%s.%s\" % (out_file_base, samples[0], outtype)\n    df = df[df[\"category\"].isin(cat_labels)]\n    _seaborn(df, prep, prepi, out_file, title, size)\n    return out_file", "category": "Python"}, {"instruction": "def resolve_format(format, path):\n    \"\"\"Looks at a file's extension and format (if any) and returns format.\n    \"\"\"\n", "input": "", "output": "    if format is None:\n        if (re.match(r'.+\\.(yml|yaml)$', path)):\n            return 'yaml'\n        elif (re.match(r'.+\\.tsv$', path)):\n            return 'tsv'\n    else:\n        return format.lower()", "category": "Python"}, {"instruction": "def p_DefaultValue_string(p):\n  \"\"\"DefaultValue : STRING\"\"\"\n", "input": "", "output": "  p[0] = model.Value(type=model.Value.STRING, value=p[1])", "category": "Python"}, {"instruction": "def Exp(input_vertex: vertex_constructor_param_types, label: Optional[str]=None) -> Vertex:\n    \"\"\"\n    Calculates the exponential of an input vertex\n    \n    :param input_vertex: the vertex\n    \"\"\"\n", "input": "", "output": "    return Double(context.jvm_view().ExpVertex, label, cast_to_double_vertex(input_vertex))", "category": "Python"}, {"instruction": "def check(self):\n        \"\"\"\n        Check if pkg has a later version\n        Returns true if later version exists\n        \"\"\"\n", "input": "", "output": "        current = self._get_current()\n        highest = self._get_highest_version()\n        return highest > current", "category": "Python"}, {"instruction": "def _get_token_create_url(config):\n    '''\n    Create Vault url for token creation\n    '''\n", "input": "", "output": "    role_name = config.get('role_name', None)\n    auth_path = '/v1/auth/token/create'\n    base_url = config['url']\n    return '/'.join(x.strip('/') for x in (base_url, auth_path, role_name) if x)", "category": "Python"}, {"instruction": "def pprint(sequence, keys=None):\n    \"\"\"\n    Print sequence as ascii table to stdout.\n\n    Args:\n        sequence (list or tuple): a sequence with a dictionary each entry.\n        keys (list): optional list of keys to order columns as well as to filter for them.\n    \"\"\"\n", "input": "", "output": "    if len(sequence) > 0:\n        columns = calculate_columns(sequence)\n        row_format = calculate_row_format(columns, keys)\n        header = row_format % dict([(key, key.title()) for key in columns])\n        separator = row_format % dict([(key, '-' * columns[key]) for key in columns])\n\n        print(separator)\n        print(header)\n        print(separator)\n\n        for row in sequence:\n            print(row_format % row)\n\n        print(separator)", "category": "Python"}, {"instruction": "def fetcher(date=datetime.today(), url_pattern=URL_PATTERN):\n    \"\"\"\n    Fetch json data from n.pl\n\n    Args:\n        date (date) - default today\n        url_patter (string) - default URL_PATTERN\n\n    Returns:\n        dict - data from api\n    \"\"\"\n", "input": "", "output": "    api_url = url_pattern % date.strftime('%Y-%m-%d')\n\n    headers = {'Referer': 'http://n.pl/program-tv'}\n    raw_result = requests.get(api_url, headers=headers).json()\n    return raw_result", "category": "Python"}, {"instruction": "def encode(self, uuid, pad_length=None):\n        \"\"\"\n        Encode a UUID into a string (LSB first) according to the alphabet\n\n        If leftmost (MSB) bits are 0, the string might be shorter.\n        \"\"\"\n", "input": "", "output": "        if pad_length is None:\n            pad_length = self._length\n        return int_to_string(uuid.int, self._alphabet, padding=pad_length)", "category": "Python"}, {"instruction": "def _pdist(x, exponent=1):\n    \"\"\"\n    Pairwise distance between points in a set.\n\n    As Scipy converts every value to double, this wrapper uses\n    a less efficient implementation if the original dtype\n    can not be converted to double.\n\n    \"\"\"\n", "input": "", "output": "    if _can_be_double(x):\n        return _pdist_scipy(x, exponent)\n    else:\n        return _cdist_naive(x, x, exponent)", "category": "Python"}, {"instruction": "def list(self, department_id, fetch_child=False, status=0, simple=False):\n        \"\"\"\n        \u6279\u91cf\u83b7\u53d6\u90e8\u95e8\u6210\u5458 / \u6279\u91cf\u83b7\u53d6\u90e8\u95e8\u6210\u5458\u8be6\u60c5\n\n        https://work.weixin.qq.com/api/doc#90000/90135/90200\n        https://work.weixin.qq.com/api/doc#90000/90135/90201\n\n        \u6b64\u63a5\u53e3\u548c `WeChatDepartment.get_users` \u662f\u540c\u4e00\u4e2a\u63a5\u53e3\uff0c\u533a\u522b\u4e3a simple \u7684\u9ed8\u8ba4\u503c\u4e0d\u540c\u3002\n        \"\"\"\n", "input": "", "output": "        url = 'user/simplelist' if simple else 'user/list'\n        res = self._get(\n            url,\n            params={\n                'department_id': department_id,\n                'fetch_child': 1 if fetch_child else 0,\n                'status': status\n            }\n        )\n        return res['userlist']", "category": "Python"}, {"instruction": "def main():\n    \"\"\"\n    Parses user input for a package name\n    :return:\n    \"\"\"\n", "input": "", "output": "    parser = argparse.ArgumentParser('Determines the health of a package')\n\n    parser.add_argument(\n        'package_name',\n        help='Name of package listed on pypi.python.org',\n    )\n\n    parser.add_argument(\n        'package_version', nargs='?',\n        help='Version of package to check',\n    )\n\n    parser.add_argument(\n        '-v', '--verbose', required=False,\n        help='Show verbose output - the reasons for the package health score',\n        action='store_true'\n    )\n\n    parser.add_argument(\n        '-n', '--no_output', required=False,\n        help='Show no output - no output will be generated',\n        action='store_true'\n    )\n\n    args = parser.parse_args()\n\n    return calculate_health(args.package_name, args.package_version, args.verbose, args.no_output)", "category": "Python"}, {"instruction": "def getstats(self, names, default=numpy.nan):\n        \"\"\"Get the requested stats as a tuple.\n\n        If a requested stat is not an attribute (implying it hasn't been\n        stored), then the default value is returned for that stat.\n\n        Parameters\n        ----------\n        names : list of str\n            The names of the stats to get.\n        default : float, optional\n            What to return if a requested stat is not an attribute of self.\n            Default is ``numpy.nan``.\n\n        Returns\n        -------\n        tuple\n            A tuple of the requested stats.\n        \"\"\"\n", "input": "", "output": "        return tuple(getattr(self, n, default) for n in names)", "category": "Python"}, {"instruction": "def insert(self, index, text):\n        \"\"\"Insert line to the document\n        \"\"\"\n", "input": "", "output": "        if index < 0 or index > self._doc.blockCount():\n            raise IndexError('Invalid block index', index)\n\n        if index == 0:  # first\n            cursor = QTextCursor(self._doc.firstBlock())\n            cursor.insertText(text)\n            cursor.insertBlock()\n        elif index != self._doc.blockCount():  # not the last\n            cursor = QTextCursor(self._doc.findBlockByNumber(index).previous())\n            cursor.movePosition(QTextCursor.EndOfBlock)\n            cursor.insertBlock()\n            cursor.insertText(text)\n        else:  # last append to the end\n            self.append(text)", "category": "Python"}, {"instruction": "def remove_display_name_by_language(self, language_type):\n        \"\"\"Removes the specified display_name.\n\n        raise:  NoAccess - ``Metadata.isRequired()`` is ``true`` or\n                ``Metadata.isReadOnly()`` is ``true``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        if self.get_display_names_metadata().is_read_only():\n            raise NoAccess()\n        if not isinstance(language_type, Type):\n            raise InvalidArgument('language_type must be instance of Type')\n        self.my_osid_object_form._my_map['displayNames'] = [t\n                                                            for t in self.my_osid_object_form._my_map['displayNames']\n                                                            if t['languageTypeId'] != str(language_type)]", "category": "Python"}, {"instruction": "def get_media_formats(self, media_id):\n        \"\"\"CR doesn't seem to provide the video_format and video_quality params\n        through any of the APIs so we have to scrape the video page\n        \"\"\"\n", "input": "", "output": "        url = (SCRAPER.API_URL + 'media-' + media_id).format(\n            protocol=SCRAPER.PROTOCOL_INSECURE)\n        format_pattern = re.compile(SCRAPER.VIDEO.FORMAT_PATTERN)\n        formats = {}\n\n        for format, param in iteritems(SCRAPER.VIDEO.FORMAT_PARAMS):\n            resp = self._connector.get(url, params={param: '1'})\n            if not resp.ok:\n                continue\n            try:\n                match = format_pattern.search(resp.content)\n            except TypeError:\n                match = format_pattern.search(resp.text)\n            if match:\n                formats[format] = (int(match.group(1)), int(match.group(2)))\n        return formats", "category": "Python"}, {"instruction": "def get_location(dom, location):\n    \"\"\"\n    Get the node at the specified location in the dom.\n    Location is a sequence of child indices, starting at the children of the\n    root element. If there is no node at this location, raise a ValueError.\n    \"\"\"\n", "input": "", "output": "    node = dom.documentElement\n    for i in location:\n        node = get_child(node, i)\n        if not node:\n            raise ValueError('Node at location %s does not exist.' % location) #TODO: line not covered\n    return node", "category": "Python"}, {"instruction": "def str2bool_or_none(string_, default='raise'):\n    \"\"\"\n    Convert a string to a bool or to None.\n\n    Parameters\n    ----------\n    string_ : str\n    default : {'raise', False}\n        Default behaviour if none of the \"true\" or \"none\" strings is detected.\n\n    Returns\n    -------\n    bool_or_none : bool or None\n\n    Examples\n    --------\n    >>> str2bool_or_none('True')\n    True\n    >>> str2bool_or_none('1')\n    True\n    >>> str2bool_or_none('0')\n    False\n    >>> str2bool_or_none('undefined')\n    \"\"\"\n", "input": "", "output": "    if is_none(string_, default=False):\n        return None\n    else:\n        return str2bool(string_, default)", "category": "Python"}, {"instruction": "async def remove(self) -> bool:\n        \"\"\"\n        Remove serialized wallet, best effort, if it exists. Return whether wallet absent after operation\n        (removal successful or else not present a priori).\n\n        Raise WalletState if wallet is open.\n\n        :return: whether wallet gone from persistent storage\n        \"\"\"\n", "input": "", "output": "\n        LOGGER.debug('Wallet.remove >>>')\n\n        if self.handle:\n            LOGGER.debug('Wallet.remove <!< Wallet %s is open', self.name)\n            raise WalletState('Wallet {} is open'.format(self.name))\n\n        rv = True\n        try:\n            LOGGER.info('Attempting to remove wallet: %s', self.name)\n            await wallet.delete_wallet(\n                json.dumps(self.config),\n                json.dumps(self.access_creds))\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletNotFoundError:\n                LOGGER.info('Wallet %s not present; abstaining from removal', self.name)\n            else:\n                LOGGER.info('Failed wallet %s removal; indy-sdk error code %s', self.name, x_indy.error_code)\n                rv = False\n\n        LOGGER.debug('Wallet.remove <<< %s', rv)\n        return rv", "category": "Python"}, {"instruction": "def linkify(self, timeperiods, commands):\n        \"\"\"Replace check_period by real Timeperiod object into each CheckModulation\n        Replace check_command by real Command object into each CheckModulation\n\n        :param timeperiods: timeperiods to link to\n        :type timeperiods: alignak.objects.timeperiod.Timeperiods\n        :param commands: commands to link to\n        :type commands: alignak.objects.command.Commands\n        :return: None\n        \"\"\"\n", "input": "", "output": "        self.linkify_with_timeperiods(timeperiods, 'check_period')\n        self.linkify_one_command_with_commands(commands, 'check_command')", "category": "Python"}, {"instruction": "def check_indexes(self):\n        \"\"\"Check if the indexes exists\"\"\"\n", "input": "", "output": "        for collection_name in INDEXES:\n            existing_indexes = self.indexes(collection_name)\n            indexes = INDEXES[collection_name]\n            for index in indexes:\n                index_name = index.document.get('name')\n                if not index_name in existing_indexes:\n                    logger.warning(\"Index {0} missing. Run command `loqusdb index`\".format(index_name))\n                    return\n        logger.info(\"All indexes exists\")", "category": "Python"}, {"instruction": "def get_term_before(aterm):\n    \"\"\"\n    Returns a uw_sws.models.Term object,\n    for the term before the term given.\n    \"\"\"\n", "input": "", "output": "    prev_year = aterm.year\n    prev_quarter = QUARTER_SEQ[QUARTER_SEQ.index(aterm.quarter) - 1]\n\n    if prev_quarter == \"autumn\":\n        prev_year -= 1\n\n    return get_term_by_year_and_quarter(prev_year, prev_quarter)", "category": "Python"}, {"instruction": "def currency_to_protocol(amount):\n    \"\"\"\n    Convert a string of 'currency units' to 'protocol units'. For instance\n    converts 19.1 bitcoin to 1910000000 satoshis.\n\n    Input is a float, output is an integer that is 1e8 times larger.\n\n    It is hard to do this conversion because multiplying\n    floats causes rounding nubers which will mess up the transactions creation\n    process.\n\n    examples:\n\n    19.1 -> 1910000000\n    0.001 -> 100000\n\n    \"\"\"\n", "input": "", "output": "    if type(amount) in [float, int]:\n        amount = \"%.8f\" % amount\n\n    return int(amount.replace(\".\", ''))", "category": "Python"}, {"instruction": "def paths_from_version(version):\n    \"\"\"Get the EnergyPlus install directory and executable path.\n\n    Parameters\n    ----------\n    version : str, optional\n        EnergyPlus version in the format \"X-X-X\", e.g. \"8-7-0\".\n\n    Returns\n    -------\n    eplus_exe : str\n        Full path to the EnergyPlus executable.\n    eplus_home : str\n        Full path to the EnergyPlus install directory.\n\n    \"\"\"\n", "input": "", "output": "    if platform.system() == 'Windows':\n        eplus_home = \"C:/EnergyPlusV{version}\".format(version=version)\n        eplus_exe = os.path.join(eplus_home, 'energyplus.exe')\n    elif platform.system() == \"Linux\":\n        eplus_home = \"/usr/local/EnergyPlus-{version}\".format(version=version)\n        eplus_exe = os.path.join(eplus_home, 'energyplus')\n    else:\n        eplus_home = \"/Applications/EnergyPlus-{version}\".format(version=version)\n        eplus_exe = os.path.join(eplus_home, 'energyplus')\n    return eplus_exe, eplus_home", "category": "Python"}, {"instruction": "def get_interface_detail_output_interface_ifHCOutBroadcastPkts(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_interface_detail = ET.Element(\"get_interface_detail\")\n        config = get_interface_detail\n        output = ET.SubElement(get_interface_detail, \"output\")\n        interface = ET.SubElement(output, \"interface\")\n        interface_type_key = ET.SubElement(interface, \"interface-type\")\n        interface_type_key.text = kwargs.pop('interface_type')\n        interface_name_key = ET.SubElement(interface, \"interface-name\")\n        interface_name_key.text = kwargs.pop('interface_name')\n        ifHCOutBroadcastPkts = ET.SubElement(interface, \"ifHCOutBroadcastPkts\")\n        ifHCOutBroadcastPkts.text = kwargs.pop('ifHCOutBroadcastPkts')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def StateOfCharge(self):\n        \"\"\" % of Full Charge \"\"\"\n", "input": "", "output": "        return (self.bus.read_byte_data(self.address, 0x02) + self.bus.read_byte_data(self.address, 0x03) * 256)", "category": "Python"}, {"instruction": "def subtract_params(param_list_left, param_list_right):\n    \"\"\"Subtract two lists of parameters\n\n    :param param_list_left: list of numpy arrays\n    :param param_list_right: list of numpy arrays\n    :return: list of numpy arrays\n    \"\"\"\n", "input": "", "output": "    res = []\n    for x, y in zip(param_list_left, param_list_right):\n        res.append(x - y)\n    return res", "category": "Python"}, {"instruction": "def getOxum(dataPath):\n    \"\"\"\n    Calculate the oxum for a given path\n    \"\"\"\n", "input": "", "output": "\n    fileCount = 0L\n    fileSizeTotal = 0L\n    for root, dirs, files in os.walk(dataPath):\n        for fileName in files:\n            fullName = os.path.join(root, fileName)\n            stats = os.stat(fullName)\n            fileSizeTotal += stats.st_size\n            fileCount += 1\n    return \"%s.%s\" % (fileSizeTotal, fileCount)", "category": "Python"}, {"instruction": "def get_resources_nodes(call=None, resFilter=None):\n    '''\n    Retrieve all hypervisors (nodes) available on this environment\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-cloud -f get_resources_nodes my-proxmox-config\n    '''\n", "input": "", "output": "    log.debug('Getting resource: nodes.. (filter: %s)', resFilter)\n    resources = query('get', 'cluster/resources')\n\n    ret = {}\n    for resource in resources:\n        if 'type' in resource and resource['type'] == 'node':\n            name = resource['node']\n            ret[name] = resource\n\n    if resFilter is not None:\n        log.debug('Filter given: %s, returning requested '\n                  'resource: nodes', resFilter)\n        return ret[resFilter]\n\n    log.debug('Filter not given: %s, returning all resource: nodes', ret)\n    return ret", "category": "Python"}, {"instruction": "def instance_ip_grouping_key():\n    \"\"\"Grouping key with instance set to the IP Address of this host.\"\"\"\n", "input": "", "output": "    with closing(socket.socket(socket.AF_INET, socket.SOCK_DGRAM)) as s:\n        s.connect(('localhost', 0))\n        return {'instance': s.getsockname()[0]}", "category": "Python"}, {"instruction": "def _list_global_ips_by_identifier(self, identifier):\n        \"\"\"Returns a list of IDs of the global IP matching the identifier.\n\n        :param string identifier: The identifier to look up\n        :returns: List of matching IDs\n        \"\"\"\n", "input": "", "output": "        results = self.list_global_ips(identifier=identifier, mask='id')\n        return [result['id'] for result in results]", "category": "Python"}, {"instruction": "def build_functional(self, *pattern, **kwargs):\n        \"\"\"\n        Builds a new functional pattern\n\n        :param pattern:\n        :type pattern:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        set_defaults(self._functional_defaults, kwargs)\n        set_defaults(self._defaults, kwargs)\n        return FunctionalPattern(*pattern, **kwargs)", "category": "Python"}, {"instruction": "def trace3D(self, join=True):\n        \"\"\"Give a 3D representation of the traceroute.\n        right button: rotate the scene\n        middle button: zoom\n        shift-left button: move the scene\n        left button on a ball: toggle IP displaying\n        double-click button on a ball: scan ports 21,22,23,25,80 and 443 and display the result\"\"\"\n", "input": "", "output": "        # When not ran from a notebook, vpython pooly closes itself\n        # using os._exit once finished. We pack it into a Process\n        import multiprocessing\n        p = multiprocessing.Process(target=self.trace3D_notebook)\n        p.start()\n        if join:\n            p.join()", "category": "Python"}, {"instruction": "def delete(self, id):\n        \"\"\"\n        \u6839\u636e id \u5220\u9664\u6570\u636e\u3002\n\n        :param id: \u8981\u5220\u9664\u7684\u6570\u636e\u7684 id\n        \"\"\"\n", "input": "", "output": "        self.conn.cursor().execute(\"DELETE FROM WeRoBot WHERE id=%s\", (id, ))\n        self.conn.commit()", "category": "Python"}, {"instruction": "def add_custom_metadata(self, key, value, meta_type=None):\n        \"\"\"\n        Add custom metadata to the Video.  meta_type is required for XML API.\n        \"\"\"\n", "input": "", "output": "        self.metadata.append({'key': key, 'value': value, 'type': meta_type})", "category": "Python"}, {"instruction": "def __initialize_ui(self):\n        \"\"\"\n        Initializes the Widget ui.\n        \"\"\"\n", "input": "", "output": "\n        self.setAutoScroll(True)\n        self.setIndentation(self.__tree_view_indentation)\n        self.setRootIsDecorated(False)\n        self.setDragDropMode(QAbstractItemView.DragOnly)\n\n        self.header().hide()\n\n        self.setSortingEnabled(True)\n        self.sortByColumn(0, Qt.AscendingOrder)\n\n        self.__set_default_ui_state()\n\n        # Signals / Slots.\n        self.model().modelReset.connect(self.__set_default_ui_state)", "category": "Python"}, {"instruction": "def setup_ui(self, ):\n        \"\"\"Setup the general ui\n\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        plus_icon = get_icon('glyphicons_433_plus_bright.png', asicon=True)\n        self.add_tb.setIcon(plus_icon)\n\n        self.shot_browser = ListBrowser(4, parent=self, headers=[\"Project\", \"Sequence\", \"Shot\", \"Type\"])\n        self.asset_browser = ListBrowser(4, parent=self, headers=[\"Project\", \"Assettype\", \"Asset\", \"Type\"])\n\n        self.shotmodel = self.create_shot_model()\n        self.assetmodel = self.create_asset_model()\n\n        self.shot_browser.set_model(self.shotmodel)\n        self.asset_browser.set_model(self.assetmodel)\n\n        self.shot_vbox.addWidget(self.shot_browser)\n        self.asset_vbox.addWidget(self.asset_browser)", "category": "Python"}, {"instruction": "def _get_upserts(queryset, model_objs_updated, model_objs_created, unique_fields):\n    \"\"\"\n    Given a list of model objects that were updated and model objects that were created,\n    return the list of all model objects upserted. Doing this requires fetching all of\n    the models created with bulk create (since django can't return bulk_create pks)\n    \"\"\"\n", "input": "", "output": "    updated, created = _get_upserts_distinct(queryset, model_objs_updated, model_objs_created, unique_fields)\n    return updated + created", "category": "Python"}, {"instruction": "def transformation_matrix(self):\n        \"\"\"Get the 4x4 homogeneous transformation matrix equivalent of the quaternion rotation.\n\n        Returns:\n            A 4x4 homogeneous transformation matrix as a 4x4 Numpy array\n\n        Note:\n            This feature only makes sense when referring to a unit quaternion. Calling this method will implicitly normalise the Quaternion object to a unit quaternion if it is not already one.\n        \"\"\"\n", "input": "", "output": "        t = np.array([[0.0], [0.0], [0.0]])\n        Rt = np.hstack([self.rotation_matrix, t])\n        return np.vstack([Rt, np.array([0.0, 0.0, 0.0, 1.0])])", "category": "Python"}, {"instruction": "def get_k8s_versions(cli_ctx, location):\n    \"\"\"Return a list of Kubernetes versions available for a new cluster.\"\"\"\n", "input": "", "output": "    from ._client_factory import cf_container_services\n    from jmespath import search  # pylint: disable=import-error\n\n    results = cf_container_services(cli_ctx).list_orchestrators(location, resource_type='managedClusters').as_dict()\n    # Flatten all the \"orchestrator_version\" fields into one array\n    return search('orchestrators[*].orchestrator_version', results)", "category": "Python"}, {"instruction": "def parse_create(prs, conn):\n    \"\"\"Create record.\n\n    Arguments:\n\n        prs:  parser object of argparse\n        conn: dictionary of connection information\n    \"\"\"\n", "input": "", "output": "    prs_create = prs.add_parser(\n        'create', help='create record of specific zone')\n    set_option(prs_create, 'domain')\n    conn_options(prs_create, conn)\n    prs_create.set_defaults(func=create)", "category": "Python"}, {"instruction": "def on_action_run(self, task_vars, delegate_to_hostname, loader_basedir):\n        \"\"\"\n        Invoked by ActionModuleMixin to indicate a new task is about to start\n        executing. We use the opportunity to grab relevant bits from the\n        task-specific data.\n\n        :param dict task_vars:\n            Task variable dictionary.\n        :param str delegate_to_hostname:\n            :data:`None`, or the template-expanded inventory hostname this task\n            is being delegated to. A similar variable exists on PlayContext\n            when ``delegate_to:`` is active, however it is unexpanded.\n        :param str loader_basedir:\n            Loader base directory; see :attr:`loader_basedir`.\n        \"\"\"\n", "input": "", "output": "        self.inventory_hostname = task_vars['inventory_hostname']\n        self._task_vars = task_vars\n        self.host_vars = task_vars['hostvars']\n        self.delegate_to_hostname = delegate_to_hostname\n        self.loader_basedir = loader_basedir\n        self._mitogen_reset(mode='put')", "category": "Python"}, {"instruction": "def _action_add(self, ids):\n        \"\"\"Add IDs to the group\n\n        Parameters\n        ----------\n        ids : {list, set, tuple, generator} of str\n            The IDs to add\n\n        Returns\n        -------\n        list of dict\n            The details of the added jobs\n        \"\"\"\n", "input": "", "output": "        return self._action_get((self.listen_to_node(id_) for id_ in ids))", "category": "Python"}, {"instruction": "def find_branches(self):\n        \"\"\"\n        Find the branches in the Mercurial repository.\n\n        :returns: A generator of :class:`.Revision` objects.\n\n        .. note:: Closed branches are not included.\n        \"\"\"\n", "input": "", "output": "        listing = self.context.capture('hg', 'branches')\n        for line in listing.splitlines():\n            tokens = line.split()\n            if len(tokens) >= 2 and ':' in tokens[1]:\n                revision_number, revision_id = tokens[1].split(':')\n                yield Revision(\n                    branch=tokens[0],\n                    repository=self,\n                    revision_id=revision_id,\n                    revision_number=int(revision_number),\n                )", "category": "Python"}, {"instruction": "def maxlen(max_length,\n           strict=False  # type: bool\n           ):\n    \"\"\"\n    'Maximum length' validation_function generator.\n    Returns a validation_function to check that len(x) <= max_length (strict=False, default) or len(x) < max_length (strict=True)\n\n    :param max_length: maximum length for x\n    :param strict: Boolean flag to switch between len(x) <= max_length (strict=False) and len(x) < max_length\n    (strict=True)\n    :return:\n    \"\"\"\n", "input": "", "output": "    if strict:\n        def maxlen_(x):\n            if len(x) < max_length:\n                return True\n            else:\n                # raise Failure('maxlen: len(x) < ' + str(max_length) + ' does not hold for x=' + str(x))\n                raise TooLong(wrong_value=x, max_length=max_length, strict=True)\n    else:\n        def maxlen_(x):\n            if len(x) <= max_length:\n                return True\n            else:\n                # raise Failure('maxlen: len(x) <= ' + str(max_length) + ' does not hold for x=' + str(x))\n                raise TooLong(wrong_value=x, max_length=max_length, strict=False)\n\n    maxlen_.__name__ = 'length_{}lesser_than_{}'.format('strictly_' if strict else '', max_length)\n    return maxlen_", "category": "Python"}, {"instruction": "def once(self, message, *args, **kws):\n    \"\"\"Show a message only once, determined by position in source or identifer.\n\n    This will not work in IPython or Jupyter notebooks if no identifier is\n    specified, since then the determined position in source contains the\n    execution number of the input (cell), which changes every time.\n    Set a unique identifier, otherwise the message will be printed every\n    time.\n    \"\"\"\n", "input": "", "output": "    # TODO: after py2 support drop, put this into\n    # function signature: identifier=None (between *args and **kws)\n    identifier = kws.pop('identifier', None)\n\n    if identifier is None:\n        caller = getframeinfo(stack()[1][0])\n        identifier = \"%s:%d\" % (caller.filename, caller.lineno)\n    if not hasattr(self, 'once_dict'):\n        self.once_dict = {}\n    if identifier in self.once_dict:\n        return\n    self.once_dict[identifier] = True\n    self._log(ONCE, message, args, **kws)", "category": "Python"}, {"instruction": "def draw_spring(self, **kwargs):\n\n        \"\"\"\n        Render a spring layout.\n        \"\"\"\n", "input": "", "output": "\n        nx.draw_spring(\n            self.graph,\n            with_labels=True,\n            font_size=10,\n            edge_color='#dddddd',\n            node_size=0,\n            **kwargs\n        )\n\n        plt.show()", "category": "Python"}, {"instruction": "def getheader(self, name, default=None):\n        \"\"\"\n        Return a given response header.\n        \"\"\"\n", "input": "", "output": "        return self.response.headers.get(name, default)", "category": "Python"}, {"instruction": "def krai_from_raw(self, amount):\n        \"\"\"\n        Divide a raw amount down by the krai ratio.\n\n        :param amount: Amount in raw to convert to krai\n        :type amount: int\n\n        :raises: :py:exc:`nano.rpc.RPCException`\n\n        >>> rpc.krai_from_raw(amount=1000000000000000000000000000)\n        1\n        \"\"\"\n", "input": "", "output": "\n        amount = self._process_value(amount, 'int')\n\n        payload = {\"amount\": amount}\n\n        resp = self.call('krai_from_raw', payload)\n\n        return int(resp['amount'])", "category": "Python"}, {"instruction": "def error(transaction, code):  # pragma: no cover\n        \"\"\"\n        Notifies generic error on blockwise exchange.\n\n        :type transaction: Transaction\n        :param transaction: the transaction that owns the response\n        :rtype : Transaction\n        :return: the edited transaction\n        \"\"\"\n", "input": "", "output": "        transaction.block_transfer = True\n        transaction.response = Response()\n        transaction.response.destination = transaction.request.source\n        transaction.response.type = defines.Types[\"RST\"]\n        transaction.response.token = transaction.request.token\n        transaction.response.code = code\n        return transaction", "category": "Python"}, {"instruction": "def _get_initialized_channels_for_service(self, org_id, service_id):\n        '''return [channel]'''\n", "input": "", "output": "        channels_dict = self._get_initialized_channels_dict_for_service(org_id, service_id)\n        return list(channels_dict.values())", "category": "Python"}, {"instruction": "def set_sequestered(self, sequestered):\n        \"\"\"Sets the sequestered flag.\n\n        arg:    sequestered (boolean): the new sequestered flag\n        raise:  InvalidArgument - ``sequestered`` is invalid\n        raise:  NoAccess - ``Metadata.isReadOnly()`` is ``true``\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        if sequestered is None:\n            raise errors.NullArgument()\n        if self.get_sequestered_metadata().is_read_only():\n            raise errors.NoAccess()\n        if not isinstance(sequestered, bool):\n            raise errors.InvalidArgument()\n        self._my_map['sequestered'] = sequestered", "category": "Python"}, {"instruction": "def set(self, document_id):\n        \"\"\"\n        Associate this document with a ProvStore document without making any calls to the API.\n        :param int document_id: ID of the document on ProvStore\n        :return: self\n        \"\"\"\n", "input": "", "output": "        if not self.abstract:\n            raise ImmutableDocumentException()\n        self._id = document_id\n\n        return self", "category": "Python"}, {"instruction": "def sign(self, secret=None):\n        \"\"\"Sign the generated :class:`TransactionEnvelope\n        <stellar_base.transaction_envelope.TransactionEnvelope>` from the list\n        of this builder's operations.\n\n        :param str secret: The secret seed to use if a key pair or secret was\n            not provided when this class was originaly instantiated, or if\n            another key is being utilized to sign the transaction envelope.\n\n        \"\"\"\n", "input": "", "output": "        keypair = self.keypair if not secret else Keypair.from_seed(secret)\n        self.gen_te()\n        self.te.sign(keypair)", "category": "Python"}, {"instruction": "def get_vlan_brief_output_has_more(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_vlan_brief = ET.Element(\"get_vlan_brief\")\n        config = get_vlan_brief\n        output = ET.SubElement(get_vlan_brief, \"output\")\n        has_more = ET.SubElement(output, \"has-more\")\n        has_more.text = kwargs.pop('has_more')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def write_branch_data(self, file, padding=\"    \"):\n        \"\"\" Writes branch data in Graphviz DOT language.\n        \"\"\"\n", "input": "", "output": "        attrs = ['%s=\"%s\"' % (k,v) for k,v in self.branch_attr.iteritems()]\n        attr_str = \", \".join(attrs)\n\n        for br in self.case.branches:\n            file.write(\"%s%s -> %s [%s];\\n\" % \\\n                (padding, br.from_bus.name, br.to_bus.name, attr_str))", "category": "Python"}, {"instruction": "def handle_type(self, frames):\n        \"\"\"\n        Config video types\n        \"\"\"\n", "input": "", "output": "        if self.vtype == 'mouth':\n            self.process_frames_mouth(frames)\n        elif self.vtype == 'face':\n            self.process_frames_face(frames)\n        else:\n            raise Exception('Video type not found')", "category": "Python"}, {"instruction": "def get_aws_secrets_from_file(credentials_file):  # type: (str) -> Set[str]\n    \"\"\"Extract AWS secrets from configuration files.\n\n    Read an ini-style configuration file and return a set with all found AWS\n    secret access keys.\n    \"\"\"\n", "input": "", "output": "    aws_credentials_file_path = os.path.expanduser(credentials_file)\n    if not os.path.exists(aws_credentials_file_path):\n        return set()\n\n    parser = configparser.ConfigParser()\n    try:\n        parser.read(aws_credentials_file_path)\n    except configparser.MissingSectionHeaderError:\n        return set()\n\n    keys = set()\n    for section in parser.sections():\n        for var in (\n            'aws_secret_access_key', 'aws_security_token',\n            'aws_session_token',\n        ):\n            try:\n                key = parser.get(section, var).strip()\n                if key:\n                    keys.add(key)\n            except configparser.NoOptionError:\n                pass\n    return keys", "category": "Python"}, {"instruction": "def load_from_string(self, content, container, **options):\n        \"\"\"\n        Load configuration data from given string 'content'.\n\n        :param content: Configuration string\n        :param container: callble to make a container object\n        :param options: keyword options passed to '_load_from_string_fn'\n\n        :return: container object holding the configuration data\n        \"\"\"\n", "input": "", "output": "        return load_with_fn(self._load_from_string_fn, content, container,\n                            allow_primitives=self.allow_primitives(),\n                            **options)", "category": "Python"}, {"instruction": "def order_by_json_path(self, json_path, language_code=None, order='asc'):\n        \"\"\"\n        Orders a queryset by the value of the specified `json_path`.\n\n        More about the `#>>` operator and the `json_path` arg syntax:\n        https://www.postgresql.org/docs/current/static/functions-json.html\n\n        More about Raw SQL expressions:\n        https://docs.djangoproject.com/en/dev/ref/models/expressions/#raw-sql-expressions\n\n        Usage example:\n            MyModel.objects.language('en_us').filter(is_active=True).order_by_json_path('title')\n        \"\"\"\n", "input": "", "output": "        language_code = (language_code\n                            or self._language_code\n                            or self.get_language_key(language_code))\n        json_path = '{%s,%s}' % (language_code, json_path)\n        # Our jsonb field is named `translations`.\n        raw_sql_expression = RawSQL(\"translations#>>%s\", (json_path,))\n        if order == 'desc':\n            raw_sql_expression = raw_sql_expression.desc()\n        return self.order_by(raw_sql_expression)", "category": "Python"}, {"instruction": "def create_api_context(self, cls):\n        \"\"\"Create and return an API context\"\"\"\n", "input": "", "output": "        return self.api_context_schema().load({\n            \"name\": cls.name,\n            \"cls\": cls,\n            \"inst\": [],\n            \"conf\": self.conf.get_api_service(cls.name),\n            \"calls\": self.conf.get_api_calls(),\n            \"shared\": {},  # Used per-API to monitor state\n            \"log_level\": self.conf.get_log_level(),\n            \"callback\": self.receive\n            })", "category": "Python"}, {"instruction": "async def start(self, zone_id: int, time: int) -> dict:\n        \"\"\"Start a program.\"\"\"\n", "input": "", "output": "        return await self._request(\n            'post', 'zone/{0}/start'.format(zone_id), json={'time': time})", "category": "Python"}, {"instruction": "def add_roles(self, databaseName, roleNames, collectionName=None):\n        \"\"\"Add multiple roles\n        \n        Args:\n            databaseName (str): Database Name\n            roleNames (list of RoleSpecs): roles\n            \n        Keyword Args:\n            collectionName (str): Collection\n        \n        Raises:\n            ErrRoleException: role not compatible with the databaseName and/or collectionName\n        \"\"\"\n", "input": "", "output": "        for roleName in roleNames:\n            self.add_role(databaseName, roleName, collectionName)", "category": "Python"}, {"instruction": "def complete(self):\n        \"\"\" Return True if the limit has been reached \"\"\"\n", "input": "", "output": "        if self.scan_limit is not None and self.scan_limit == 0:\n            return True\n        if self.item_limit is not None and self.item_limit == 0:\n            return True\n        return False", "category": "Python"}, {"instruction": "def safe_join(directory, *pathnames):\n    \"\"\"Safely join `directory` and one or more untrusted `pathnames`.  If this\n    cannot be done, this function returns ``None``.\n\n    :param directory: the base directory.\n    :param pathnames: the untrusted pathnames relative to that directory.\n    \"\"\"\n", "input": "", "output": "    parts = [directory]\n    for filename in pathnames:\n        if filename != \"\":\n            filename = posixpath.normpath(filename)\n        for sep in _os_alt_seps:\n            if sep in filename:\n                return None\n        if os.path.isabs(filename) or filename == \"..\" or filename.startswith(\"../\"):\n            return None\n        parts.append(filename)\n    return posixpath.join(*parts)", "category": "Python"}, {"instruction": "def websocktunnelToken(self, *args, **kwargs):\n        \"\"\"\n        Get a client token for the Websocktunnel service\n\n        Get a temporary token suitable for use connecting to a\n        [websocktunnel](https://github.com/taskcluster/websocktunnel) server.\n\n        The resulting token will only be accepted by servers with a matching audience\n        value.  Reaching such a server is the callers responsibility.  In general,\n        a server URL or set of URLs should be provided to the caller as configuration\n        along with the audience value.\n\n        The token is valid for a limited time (on the scale of hours). Callers should\n        refresh it before expiration.\n\n        This method gives output: ``v1/websocktunnel-token-response.json#``\n\n        This method is ``stable``\n        \"\"\"\n", "input": "", "output": "\n        return self._makeApiCall(self.funcinfo[\"websocktunnelToken\"], *args, **kwargs)", "category": "Python"}, {"instruction": "def interpol_hist2d(h2d, oversamp_factor=10):\n    \"\"\"Sample the interpolator of a root 2d hist.\n\n    Root's hist2d has a weird internal interpolation routine,\n    also using neighbouring bins.\n    \"\"\"\n", "input": "", "output": "    from rootpy import ROOTError\n\n    xlim = h2d.bins(axis=0)\n    ylim = h2d.bins(axis=1)\n    xn = h2d.nbins(0)\n    yn = h2d.nbins(1)\n    x = np.linspace(xlim[0], xlim[1], xn * oversamp_factor)\n    y = np.linspace(ylim[0], ylim[1], yn * oversamp_factor)\n    mat = np.zeros((xn, yn))\n    for xi in range(xn):\n        for yi in range(yn):\n            try:\n                mat[xi, yi] = h2d.interpolate(x[xi], y[yi])\n            except ROOTError:\n                continue\n    return mat, x, y", "category": "Python"}, {"instruction": "def _prior_headerfooter(self):\n        \"\"\"|_Header| proxy on prior sectPr element or None if this is first section.\"\"\"\n", "input": "", "output": "        preceding_sectPr = self._sectPr.preceding_sectPr\n        return (\n            None if preceding_sectPr is None\n            else _Header(preceding_sectPr, self._document_part, self._hdrftr_index)\n        )", "category": "Python"}, {"instruction": "def unconvert_coord_object(tile):\n    \"\"\"Convert rawr_tiles.tile.Tile -> ModestMaps.Core.Coordinate\"\"\"\n", "input": "", "output": "    assert isinstance(tile, Tile)\n    return Coordinate(zoom=tile.z, column=tile.x, row=tile.y)", "category": "Python"}, {"instruction": "def classifer_metrics(label, pred):\n    \"\"\"\n    computes f1, precision and recall on the entity class\n    \"\"\"\n", "input": "", "output": "    prediction = np.argmax(pred, axis=1)\n    label = label.astype(int)\n\n    pred_is_entity = prediction != not_entity_index\n    label_is_entity = label != not_entity_index\n\n    corr_pred = (prediction == label) == (pred_is_entity == True)\n\n    #how many entities are there?\n    num_entities = np.sum(label_is_entity)\n    entity_preds = np.sum(pred_is_entity)\n\n    #how many times did we correctly predict an entity?\n    correct_entitites = np.sum(corr_pred[pred_is_entity])\n\n    #precision: when we predict entity, how often are we right?\n    precision = correct_entitites/entity_preds\n    if entity_preds == 0:\n        precision = np.nan\n\n    #recall: of the things that were an entity, how many did we catch?\n    recall = correct_entitites / num_entities\n    if num_entities == 0:\n        recall = np.nan\n    f1 = 2 * precision * recall / (precision + recall)\n    return precision, recall, f1", "category": "Python"}, {"instruction": "def set_end(self,time):\n    \"\"\"\n    Set the end time of the datafind query.\n    @param time: GPS end time of query.\n    \"\"\"\n", "input": "", "output": "    self.add_var_opt('gps-end-time', time)\n    self.__end = time\n    self.__set_output()", "category": "Python"}, {"instruction": "def iter_ruptures(self):\n        \"\"\"\n        Yield ruptures for the current set of indices\n        \"\"\"\n", "input": "", "output": "        assert self.orig, '%s is not fully initialized' % self\n        for ridx in range(self.start, self.stop):\n            if self.orig.rate[ridx]:  # ruptures may have have zero rate\n                rup = self.get_ucerf_rupture(ridx, self.src_filter)\n                if rup:\n                    yield rup", "category": "Python"}, {"instruction": "def packet_in_handler(self, req_igmp, msg):\n        \"\"\"the process when the querier received IGMP.\"\"\"\n", "input": "", "output": "        ofproto = msg.datapath.ofproto\n        if ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION:\n            in_port = msg.in_port\n        else:\n            in_port = msg.match['in_port']\n        if (igmp.IGMP_TYPE_REPORT_V1 == req_igmp.msgtype or\n                igmp.IGMP_TYPE_REPORT_V2 == req_igmp.msgtype):\n            self._do_report(req_igmp, in_port, msg)\n        elif igmp.IGMP_TYPE_LEAVE == req_igmp.msgtype:\n            self._do_leave(req_igmp, in_port, msg)", "category": "Python"}, {"instruction": "def getOntology(self, id=None, uri=None, match=None):\n        \"\"\"\n        get the saved-ontology with given ID or via other methods...\n        \"\"\"\n", "input": "", "output": "\n        if not id and not uri and not match:\n            return None\n\n        if type(id) == type(\"string\"):\n            uri = id\n            id = None\n            if not uri.startswith(\"http://\"):\n                match = uri\n                uri = None\n        if match:\n            if type(match) != type(\"string\"):\n                return []\n            res = []\n            for x in self.ontologies:\n                if match.lower() in x.uri.lower():\n                    res += [x]\n            return res\n        else:\n            for x in self.ontologies:\n                if id and x.id == id:\n                    return x\n                if uri and x.uri.lower() == uri.lower():\n                    return x\n            return None", "category": "Python"}, {"instruction": "def put_archive(self, container, path, data):\n        \"\"\"\n        Insert a file or folder in an existing container using a tar archive as\n        source.\n\n        Args:\n            container (str): The container where the file(s) will be extracted\n            path (str): Path inside the container where the file(s) will be\n                extracted. Must exist.\n            data (bytes): tar data to be extracted\n\n        Returns:\n            (bool): True if the call succeeds.\n\n        Raises:\n            :py:class:`docker.errors.APIError`\n                If the server returns an error.\n        \"\"\"\n", "input": "", "output": "        params = {'path': path}\n        url = self._url('/containers/{0}/archive', container)\n        res = self._put(url, params=params, data=data)\n        self._raise_for_status(res)\n        return res.status_code == 200", "category": "Python"}, {"instruction": "def wait(self, timeout=None):\n        # type: (Optional[int]) -> None\n        \"\"\"Wait on the long running operation for a specified length\n        of time. You can check if this call as ended with timeout with the\n        \"done()\" method.\n\n        :param int timeout: Period of time to wait for the long running\n         operation to complete (in seconds).\n        :raises CloudError: Server problem with the query.\n        \"\"\"\n", "input": "", "output": "        if self._thread is None:\n            return\n        self._thread.join(timeout=timeout)\n        try:\n            # Let's handle possible None in forgiveness here\n            raise self._exception  # type: ignore\n        except TypeError: # Was None\n            pass", "category": "Python"}, {"instruction": "def parse_event_record(self, node):\n        \"\"\"\n        Parses <EventRecord>\n\n        @param node: Node containing the <EventRecord> element\n        @type node: xml.etree.Element\n        \"\"\"\n", "input": "", "output": "\n        if self.current_simulation == None:\n            self.raise_error('<EventRecord> must be only be used inside a ' +\n                             'simulation specification')\n\n        if 'quantity' in node.lattrib:\n            quantity = node.lattrib['quantity']\n        else:\n            self.raise_error('<EventRecord> must specify a quantity.')\n\n        if 'eventport' in node.lattrib:\n            eventPort = node.lattrib['eventport']\n        else:\n            self.raise_error('<EventRecord> must specify an eventPort.')\n\n\n        self.current_simulation.add_event_record(EventRecord(quantity, eventPort))", "category": "Python"}, {"instruction": "def minimize(self, loss_fn, x, optim_state):\n    \"\"\"\n    Analogous to tf.Optimizer.minimize\n\n    :param loss_fn: tf Tensor, representing the loss to minimize\n    :param x: list of Tensor, analogous to tf.Optimizer's var_list\n    :param optim_state: A possibly nested dict, containing any optimizer state.\n\n    Returns:\n      new_x: list of Tensor, updated version of `x`\n      new_optim_state: dict, updated version of `optim_state`\n    \"\"\"\n", "input": "", "output": "    grads = self._compute_gradients(loss_fn, x, optim_state)\n    return self._apply_gradients(grads, x, optim_state)", "category": "Python"}, {"instruction": "def sort_files_by_size(filenames, verbose = False, reverse = False):\n\t\"\"\"\n\tReturn a list of the filenames sorted in order from smallest file\n\tto largest file (or largest to smallest if reverse is set to True).\n\tIf a filename in the list is None (used by many pycbc_glue.ligolw based\n\tcodes to indicate stdin), its size is treated as 0.  The filenames\n\tmay be any sequence, including generator expressions.\n\t\"\"\"\n", "input": "", "output": "\tif verbose:\n\t\tif reverse:\n\t\t\tprint >>sys.stderr, \"sorting files from largest to smallest ...\"\n\t\telse:\n\t\t\tprint >>sys.stderr, \"sorting files from smallest to largest ...\"\n\treturn sorted(filenames, key = (lambda filename: os.stat(filename)[stat.ST_SIZE] if filename is not None else 0), reverse = reverse)", "category": "Python"}, {"instruction": "def shell(self, command, *args, environment=None):\n        \"\"\"\n        Runs a shell command\n        \"\"\"\n", "input": "", "output": "        command += ' ' + ' '.join(args)\n        command = command.strip()\n        self.debug(self.yellow_style('$ %s' % command))\n        env = self.env.copy()\n        env.update(environment or {})\n        return subprocess.call(command, shell=True, env=env)", "category": "Python"}, {"instruction": "def calibrate(self, data, calibration):\n        \"\"\"Calibrate the data.\"\"\"\n", "input": "", "output": "        tic = datetime.now()\n        if calibration == 'counts':\n            res = data\n        elif calibration in ['radiance', 'brightness_temperature']:\n            res = self._calibrate(data)\n        else:\n            raise NotImplementedError(\"Don't know how to calibrate to \" +\n                                      str(calibration))\n\n        res.attrs['standard_name'] = calibration\n        res.attrs['calibration'] = calibration\n\n        logger.debug(\"Calibration time \" + str(datetime.now() - tic))\n        return res", "category": "Python"}, {"instruction": "def profile(self, func):\n        \"\"\" Runs and profiles the method to_profile passed in. A profile object is returned. \"\"\"\n", "input": "", "output": "        pr = cProfile.Profile()\n        pr.runcall(func)\n        st = pstats.Stats(pr)\n        st.stream = None  # make it picklable\n        st.strip_dirs()\n\n        # Adds a new profile to the existing accumulated value\n        self._accumulator.add(st)", "category": "Python"}, {"instruction": "def get_login_info(self, auth_code, provider_access_token=None):\n        \"\"\"\n        \u83b7\u53d6\u4f01\u4e1a\u53f7\u767b\u5f55\u7528\u6237\u4fe1\u606f\n\n        \u8be6\u60c5\u8bf7\u53c2\u8003\n        https://qydev.weixin.qq.com/wiki/index.php?title=\u83b7\u53d6\u4f01\u4e1a\u53f7\u767b\u5f55\u7528\u6237\u4fe1\u606f\n\n        :param provider_access_token: \u670d\u52a1\u63d0\u4f9b\u5546\u7684 accesstoken\n        :param auth_code: OAuth 2.0 \u6388\u6743\u4f01\u4e1a\u53f7\u7ba1\u7406\u5458\u767b\u5f55\u4ea7\u751f\u7684 code\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n", "input": "", "output": "        return self._post(\n            'service/get_login_info',\n            params={\n                'provider_access_token': provider_access_token,\n            },\n            data={\n                'auth_code': auth_code,\n            }\n        )", "category": "Python"}, {"instruction": "def get_last_traceback():\n    \"\"\"Retrieve the last traceback as an `unicode` string.\"\"\"\n", "input": "", "output": "    import traceback\n    from StringIO import StringIO\n    tb = StringIO()\n    traceback.print_exc(file=tb)\n    return to_unicode(tb.getvalue())", "category": "Python"}, {"instruction": "def get_all_agile_boards(self, board_name=None, project_key=None, board_type=None, start=0, limit=50):\n        \"\"\"\n        Returns all boards. This only includes boards that the user has permission to view.\n        :param board_name:\n        :param project_key:\n        :param board_type:\n        :param start:\n        :param limit:\n        :return:\n        \"\"\"\n", "input": "", "output": "        url = 'rest/agile/1.0/board'\n        params = {}\n        if board_name:\n            params['name'] = board_name\n        if project_key:\n            params['projectKeyOrId'] = project_key\n        if board_type:\n            params['type'] = board_type\n        if start:\n            params['startAt'] = int(start)\n        if limit:\n            params['maxResults'] = int(limit)\n\n        return self.get(url, params=params)", "category": "Python"}, {"instruction": "def _parse_access_vlan(self, config):\n        \"\"\"Scans the specified config and parse the access-vlan value\n        Args:\n            config (str): The interface configuration block to scan\n\n        Returns:\n            dict: A Python dict object with the value of switchport access\n                value.  The dict returned is intended to be merged into the\n                resource dict\n        \"\"\"\n", "input": "", "output": "        value = re.search(r'switchport access vlan (\\d+)', config)\n        return dict(access_vlan=value.group(1))", "category": "Python"}, {"instruction": "def save_model(self, request, obj, form, change):\n        \"\"\"Updates all metrics with the same name\"\"\"\n", "input": "", "output": "        like_metrics = self.model.objects.filter(name=obj.name)\n        # 2.7+ only :(\n        # = {key: form.cleaned_data[key] for key in form.changed_data}\n        updates = {}\n        for key in form.changed_data:\n            updates[key] = form.cleaned_data[key]\n        like_metrics.update(**updates)", "category": "Python"}, {"instruction": "def get_output_tags(self):\n        \"\"\"\n        Return an escaped string of comma separated tag_name: tag_value pairs\n\n        Tags should be sorted by key before being sent for best performance. The sort should\n        match that from the Go bytes. Compare function (http://golang.org/pkg/bytes/#Compare).\n        \"\"\"\n", "input": "", "output": "\n        # Sort the tags in lexicographically by tag name\n        sorted_tags = sorted(self.tags.items())\n\n        # Finally render, escape and return the tag string\n        return u\",\".join(u\"{0}={1}\".format(format_string(k), format_string(v)) for k, v in sorted_tags)", "category": "Python"}, {"instruction": "def time_remaining(self, zone):\n        \"\"\"\n        Returns the amount of watering time left in seconds.\n\n        :param zone: The zone to check.\n        :type zone: int\n        :returns: If the zone is not running returns 0. If the zone doesn't\n                  exist returns None. Otherwise returns number of seconds left\n                  in the watering cycle.\n        :rtype: None or seconds left in the waterting cycle.\n        \"\"\"\n", "input": "", "output": "\n        self.update_controller_info()\n\n        if zone < 0 or zone > (self.num_relays-1):\n            return None\n\n        if self.is_zone_running(zone):\n            return int(self.running[0]['time_left'])\n\n        return 0", "category": "Python"}, {"instruction": "def truncate_ellipsis(line, length=30):\n    \"\"\"Truncate a line to the specified length followed by ``...`` unless its shorter than length already.\"\"\"\n", "input": "", "output": "\n    l = len(line)\n    return line if l < length else line[:length - 3] + \"...\"", "category": "Python"}, {"instruction": "def create(name, region=None, key=None, keyid=None, profile=None):\n    '''\n    Create an SNS topic.\n\n    CLI example to create a topic::\n\n        salt myminion boto_sns.create mytopic region=us-east-1\n    '''\n", "input": "", "output": "    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    conn.create_topic(name)\n    log.info('Created SNS topic %s', name)\n    _invalidate_cache()\n    return True", "category": "Python"}, {"instruction": "def sigprocmask(self, how, new_mask, sigsetsize, valid_ptr=True):\n        \"\"\"\n        Updates the signal mask.\n\n        :param how: the \"how\" argument of sigprocmask (see manpage)\n        :param new_mask: the mask modification to apply\n        :param sigsetsize: the size (in *bytes* of the sigmask set)\n        :param valid_ptr: is set if the new_mask was not NULL\n        \"\"\"\n", "input": "", "output": "        oldmask = self.sigmask(sigsetsize)\n        self._sigmask = self.state.solver.If(valid_ptr,\n            self.state.solver.If(how == self.SIG_BLOCK,\n                oldmask | new_mask,\n                self.state.solver.If(how == self.SIG_UNBLOCK,\n                    oldmask & (~new_mask),\n                    self.state.solver.If(how == self.SIG_SETMASK,\n                        new_mask,\n                        oldmask\n                     )\n                )\n            ),\n            oldmask\n        )", "category": "Python"}, {"instruction": "def calculate_pore_volume(self):\n        \"\"\"\n        Return the intrinsic pore volume.\n\n        Returns\n        -------\n        :class:`float`\n            The intrinsic pore volume.\n\n        \"\"\"\n", "input": "", "output": "        self.pore_volume = sphere_volume(self.calculate_pore_diameter() / 2)\n        self.properties['pore_volume'] = self.pore_volume\n        return self.pore_volume", "category": "Python"}, {"instruction": "def set_pttl(self, key, ttl):\n        \"\"\" Sets time to live for @key to @ttl milliseconds\n            -> #bool True if the timeout was set\n        \"\"\"\n", "input": "", "output": "        return self._client.pexpire(self.get_key(key), ttl)", "category": "Python"}, {"instruction": "def main(argv=None):\n    \"\"\"Main Block - Configure and run the Bottle Web Server.\"\"\"\n", "input": "", "output": "    cmd_opts = parse_cmdline(argv)[0]\n    if cmd_opts.confpath is not None:\n        if os.path.exists(cmd_opts.confpath):\n            conf_paths = [cmd_opts.confpath,]\n        else:\n            return \"Configuration file not found: %s\" % cmd_opts.confpath\n    else:\n        conf_paths = [os.path.join(path, defaultConfFilename) \n                      for path in ('/etc', '.',)]\n    try:\n        conf.update(parse_conf_files(conf_paths))\n    except ConfigurationError:\n        return(sys.exc_info()[1])\n    if cmd_opts.bindport is not None:\n        conf['bindport'] = cmd_opts.bindport\n    if cmd_opts.bindaddr is not None:\n        conf['bindaddr'] = cmd_opts.bindaddr\n    if cmd_opts.baseurl is not None:\n        conf['baseurl'] = cmd_opts.baseurl\n    if cmd_opts.devel:\n        from bottle import debug\n        debug(True)\n    app = SessionMiddleware(bottle.app(), sessionOpts)\n    bottle.run(app=app, host=conf['bindaddr'], port=conf['bindport'], \n               reloader=cmd_opts.devel)", "category": "Python"}, {"instruction": "def _get_preferred_host(self, result: ResolveResult) -> Tuple[str, str]:\n        '''Get preferred host from DNS results.'''\n", "input": "", "output": "        host_1 = result.first_ipv4.ip_address if result.first_ipv4 else None\n        host_2 = result.first_ipv6.ip_address if result.first_ipv6 else None\n\n        if not host_2:\n            return host_1, None\n        elif not host_1:\n            return host_2, None\n\n        preferred_host = self._happy_eyeballs_table.get_preferred(\n            host_1, host_2)\n\n        if preferred_host:\n            return preferred_host, None\n        else:\n            return host_1, host_2", "category": "Python"}, {"instruction": "def start_workunit(self, workunit):\n    \"\"\"Implementation of Reporter callback.\"\"\"\n", "input": "", "output": "    if not self.is_under_main_root(workunit):\n      return\n\n    label_format = self._get_label_format(workunit)\n\n    if label_format == LabelFormat.FULL:\n      if not WorkUnitLabel.SUPPRESS_LABEL in workunit.labels:\n        self._emit_indented_workunit_label(workunit)\n      # Start output on a new line.\n      tool_output_format = self._get_tool_output_format(workunit)\n      if tool_output_format == ToolOutputFormat.INDENT:\n        self.emit(self._prefix(workunit, '\\n'))\n      elif tool_output_format == ToolOutputFormat.UNINDENTED:\n        self.emit('\\n')\n    elif label_format == LabelFormat.DOT:\n      self.emit('.')\n\n    self.flush()", "category": "Python"}, {"instruction": "def visitInlineShapeOrRef(self, ctx: ShExDocParser.InlineShapeOrRefContext):\n        \"\"\" inlineShapeOrRef: inlineShapeDefinition | shapeRef \"\"\"\n", "input": "", "output": "        if ctx.inlineShapeDefinition():\n            from pyshexc.parser_impl.shex_shape_definition_parser import ShexShapeDefinitionParser\n            shdef_parser = ShexShapeDefinitionParser(self.context, self.label)\n            shdef_parser.visitChildren(ctx)\n            self.expr = shdef_parser.shape\n        else:\n            self.expr = self.context.shapeRef_to_iriref(ctx.shapeRef())", "category": "Python"}, {"instruction": "def grant(self, auth, resource, permissions, ttl=None, defer=False):\n        \"\"\" Grant resources with specific permissions and return a token.\n\n        Args:\n            auth: <cik>\n            resource: Alias or ID of resource.\n            permissions: permissions of resources.\n            ttl: Time To Live.\n        \"\"\"\n", "input": "", "output": "        args = [resource, permissions]\n        if ttl is not None:\n            args.append({\"ttl\": ttl})\n        return self._call('grant', auth, args, defer)", "category": "Python"}, {"instruction": "def clock_resized_cb(self, viewer, width, height):\n        \"\"\"This method is called when an individual clock is resized.\n        It deletes and reconstructs the placement of the text objects\n        in the canvas.\n        \"\"\"\n", "input": "", "output": "        self.logger.info(\"resized canvas to %dx%d\" % (width, height))\n        # add text objects to canvas\n\n        self.canvas.delete_all_objects()\n\n        Text = self.canvas.get_draw_class('text')\n        x, y = 20, int(height * 0.55)\n        # text object for the time\n        self.time_txt = Text(x, y, text='', color=self.color,\n                             font=self.font, fontsize=self.largesize,\n                             coord='window')\n        self.canvas.add(self.time_txt, tag='_time', redraw=False)\n\n        # for supplementary info (date, timezone, etc)\n        self.suppl_txt = Text(x, height - 10, text='', color=self.color,\n                              font=self.font, fontsize=self.smallsize,\n                              coord='window')\n        self.canvas.add(self.suppl_txt, tag='_suppl', redraw=False)\n\n        self.canvas.update_canvas(whence=3)", "category": "Python"}, {"instruction": "def get_samples(self, n):\n        \"\"\"Sample the GMM distribution.\n\n        Arguments\n        ---------\n        n : int\n            Number of samples needed\n\n        Returns\n        -------\n        1D array\n            Samples from the distribution\n        \"\"\"\n", "input": "", "output": "\n        normalized_w = self.weights / np.sum(self.weights)\n        get_rand_index = st.rv_discrete(values=(range(self.N),\n                                        normalized_w)).rvs(size=n)\n        samples = np.zeros(n)\n        k = 0\n        j = 0\n        while (k < n):\n            i = get_rand_index[j]\n            j = j + 1\n            if (j == n):\n                get_rand_index = st.rv_discrete(values=(range(self.N),\n                                                normalized_w)).rvs(size=n)\n                j = 0\n            v = np.random.normal(loc=self.points[i], scale=self.sigma[i])\n            if (v > self.max_limit or v < self.min_limit):\n                continue\n            else:\n                samples[k] = v\n                k = k + 1\n                if (k == n):\n                    break\n        return samples", "category": "Python"}, {"instruction": "def edit_ipv4(self, ip4, descricao, id_ip):\n        \"\"\"\n        Edit a IP4\n\n        :param ip4: An IP4 available to save in format x.x.x.x.\n        :param id_ip: IP identifier. Integer value and greater than zero.\n        :param descricao: IP description.\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        if not is_valid_int_param(id_ip):\n            raise InvalidParameterError(\n                u'Ip identifier is invalid or was not informed.')\n\n        if ip4 is None or ip4 == \"\":\n            raise InvalidParameterError(\n                u'The IP4 is invalid or was not informed.')\n\n        ip_map = dict()\n        ip_map['descricao'] = descricao\n        ip_map['ip4'] = ip4\n        ip_map['id_ip'] = id_ip\n\n        url = \"ip4/edit/\"\n\n        code, xml = self.submit({'ip_map': ip_map}, 'POST', url)\n\n        return self.response(code, xml)", "category": "Python"}, {"instruction": "def comparelist_view(self, request, object_id, extra_context=None):\n        \"\"\"Allow selecting versions to compare.\"\"\"\n", "input": "", "output": "        opts = self.model._meta\n        object_id = unquote(object_id)\n        current = get_object_or_404(self.model, pk=object_id)\n        # As done by reversion's history_view\n        action_list = [\n            {\n                \"revision\": version.revision,\n                \"url\": reverse(\"%s:%s_%s_compare\" % (self.admin_site.name, opts.app_label, opts.model_name), args=(quote(version.object_id), version.id)),\n            } for version in self._reversion_order_version_queryset(Version.objects.get_for_object_reference(\n                self.model,\n                object_id).select_related(\"revision__user\"))]\n        context = {\"action_list\": action_list,\n                   \"opts\": opts,\n                   \"object_id\": quote(object_id),\n                   \"original\": current,\n                  }\n        extra_context = extra_context or {}\n        context.update(extra_context)\n        return render(request, self.compare_list_template or self._get_template_list(\"compare_list.html\"),\n                      context)", "category": "Python"}, {"instruction": "def _send_notification(self, handle, payload):\n        \"\"\"Send a notification over BLE\n        It is executed in the baBLE working thread: should not be blocking.\n\n        Args:\n            handle (int): The handle to notify on\n            payload (bytearray): The value to notify\n        \"\"\"\n", "input": "", "output": "\n        self.bable.notify(\n            connection_handle=self._connection_handle,\n            attribute_handle=handle,\n            value=payload\n        )", "category": "Python"}, {"instruction": "def log_data_encode(self, id, ofs, count, data):\n                '''\n                Reply to LOG_REQUEST_DATA\n\n                id                        : Log id (from LOG_ENTRY reply) (uint16_t)\n                ofs                       : Offset into the log (uint32_t)\n                count                     : Number of bytes (zero for end of log) (uint8_t)\n                data                      : log data (uint8_t)\n\n                '''\n", "input": "", "output": "                return MAVLink_log_data_message(id, ofs, count, data)", "category": "Python"}, {"instruction": "def get_srv_host_filters(self, expr):\n        # pylint: disable=too-many-return-statements\n        \"\"\"Generates service filter list corresponding to the expression ::\n\n        * '*' => any\n        * 'g' => hostgroup filter\n        * 'r' => host regex name filter\n        * 'l' => host bp rule label filter\n        * 't' => tag  filter\n        * '' => none filter\n        * No flag match => host name filter\n\n        :param expr: expression to parse\n        :type expr: str\n        :return: filter list\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        if expr == \"*\":\n            return [filter_any]\n\n        match = re.search(r\"^([%s]+):(.*)\" % self.host_flags, expr)\n        if match is None:\n            return [filter_service_by_host_name(expr)]\n\n        flags, expr = match.groups()\n        if \"g\" in flags:\n            return [filter_service_by_hostgroup_name(expr)]\n        if \"r\" in flags:\n            return [filter_service_by_regex_host_name(expr)]\n        if \"l\" in flags:\n            return [filter_service_by_host_bp_rule_label(expr)]\n        if \"t\" in flags:\n            return [filter_service_by_host_tag_name(expr)]\n\n        return [filter_none]", "category": "Python"}, {"instruction": "def do_status_progress(p, hide=False):\n  \"\"\"\n  :type p ProgressBar\n  :type hide bool\n  \"\"\"\n", "input": "", "output": "  p.start(10)\n\n  for i in range(1, 60):\n    p.progress_inc(new_status=\"count %s\" % i)\n    time.sleep(0.1)\n\n  p.stop(hide_progress=hide, new_status=\"done\")", "category": "Python"}, {"instruction": "def setdefault(msg_or_dict, key, value):\n    \"\"\"Set the key on a protobuf Message or dictionary to a given value if the\n    current value is falsy.\n\n    Because protobuf Messages do not distinguish between unset values and\n    falsy ones particularly well (by design), this method treats any falsy\n    value (e.g. 0, empty list) as a target to be overwritten, on both Messages\n    and dictionaries.\n\n    Args:\n        msg_or_dict (Union[~google.protobuf.message.Message, Mapping]): the\n            object.\n        key (str): The key on the object in question.\n        value (Any): The value to set.\n\n    Raises:\n        TypeError: If ``msg_or_dict`` is not a Message or dictionary.\n    \"\"\"\n", "input": "", "output": "    if not get(msg_or_dict, key, default=None):\n        set(msg_or_dict, key, value)", "category": "Python"}, {"instruction": "def parse_verbosity(self, args):\n    '''parse_verbosity will take an argument object, and return the args\n       passed (from a dictionary) to a list\n\n       Parameters\n       ==========\n       args: the argparse argument objects\n\n    '''\n", "input": "", "output": "\n    flags = []\n\n    if args.silent is True:\n       flags.append('--silent')\n    elif args.quiet is True:\n        flags.append('--quiet')\n    elif args.debug is True:\n       flags.append('--debug')\n    elif args.verbose is True:\n       flags.append('-' + 'v' * args.verbose)\n\n    return flags", "category": "Python"}, {"instruction": "def fetch(url, body=None, headers=None):\n    \"\"\"Invoke the fetch method on the default fetcher. Most users\n    should need only this method.\n\n    @raises Exception: any exceptions that may be raised by the default fetcher\n    \"\"\"\n", "input": "", "output": "    fetcher = getDefaultFetcher()\n    return fetcher.fetch(url, body, headers)", "category": "Python"}, {"instruction": "def add_sub_resource(self, relative_id, sub_resource):\n        \"\"\"Add sub resource\"\"\"\n", "input": "", "output": "        existing_sub_resources = self.resources.get(sub_resource.RELATIVE_PATH_TEMPLATE, defaultdict(list))\n        existing_sub_resources[relative_id].append(sub_resource)\n        self.resources.update({sub_resource.RELATIVE_PATH_TEMPLATE: existing_sub_resources})", "category": "Python"}, {"instruction": "def ProcessCompletedRequests(self, notification, unused_thread_pool=None):\n    \"\"\"Go through the list of requests and process the completed ones.\n\n    We take a snapshot in time of all requests and responses for this flow. We\n    then process as many completed requests as possible. If responses are not\n    quite here we leave it for next time.\n\n    It is safe to call this function as many times as needed. NOTE: We assume\n    that the flow queue is locked so another worker is not processing these\n    messages while we are. It is safe to insert new messages to the flow:state\n    queue.\n\n    Args:\n      notification: The notification object that triggered this processing.\n    \"\"\"\n", "input": "", "output": "    self.ScheduleKillNotification()\n    try:\n      self._ProcessCompletedRequests(notification)\n    finally:\n      self.FinalizeProcessCompletedRequests(notification)", "category": "Python"}, {"instruction": "def _get_columns(h5group):\n    \"\"\"Find valid column names from a PyCBC HDF5 Group\n\n    Returns a `set` of names.\n    \"\"\"\n", "input": "", "output": "    columns = set()\n    for name in sorted(h5group):\n        if (not isinstance(h5group[name], h5py.Dataset) or\n                name == 'template_boundaries'):\n            continue\n        if name.endswith('_template') and name[:-9] in columns:\n            continue\n        columns.add(name)\n    return columns - META_COLUMNS", "category": "Python"}, {"instruction": "def collect_summary_stats(data):\n    \"\"\"Collect summary statisitics from an array\n\n    This creates a dictionry of output arrays of summary\n    statistics, with the input array dimension reducted by one.\n\n    Parameters\n    ----------\n\n    data : `numpy.ndarray`\n        Array with the collected input data\n\n\n    Returns\n    -------\n\n    output : dict\n        Dictionary of `np.ndarray` with the summary data.\n        These include mean, std, median, and 4 quantiles (0.025, 0.16, 0.86, 0.975).\n\n    \"\"\"\n", "input": "", "output": "    mean = np.mean(data, axis=0)\n    std = np.std(data, axis=0)\n    median = np.median(data, axis=0)\n    q02, q16, q84, q97 = np.percentile(data, [2.5, 16, 84, 97.5], axis=0)\n\n    o = dict(mean=mean,\n             std=std,\n             median=median,\n             q02=q02,\n             q16=q16,\n             q84=q84,\n             q97=q97)\n\n    return o", "category": "Python"}, {"instruction": "def attach(self):\n        \"\"\"Attach strategy to its sensor and send initial update.\"\"\"\n", "input": "", "output": "        s = self._sensor\n        self.update(s, s.read())\n        self._sensor.attach(self)", "category": "Python"}, {"instruction": "def render_label(content, label_for=None, label_class=None, label_title=\"\"):\n    \"\"\"\n    Render a label with content\n    \"\"\"\n", "input": "", "output": "    attrs = {}\n    if label_for:\n        attrs[\"for\"] = label_for\n    if label_class:\n        attrs[\"class\"] = label_class\n    if label_title:\n        attrs[\"title\"] = label_title\n    return render_tag(\"label\", attrs=attrs, content=content)", "category": "Python"}, {"instruction": "def homogeneous_poisson(self):\n        \"\"\"\n        returns the homogeneous poisson ratio\n        \"\"\"\n", "input": "", "output": "        return (1. - 2. / 3. * self.g_vrh / self.k_vrh) / \\\n               (2. + 2. / 3. * self.g_vrh / self.k_vrh)", "category": "Python"}, {"instruction": "def _filepath_converter(self, user_path, file_name):\n        \"\"\"Logic for obtaining a file path\n\n        :param user_path: a path as provided by the user. perhaps a file or directory?\n        :param file_name: the name of the remote file\n        :return:\n        \"\"\"\n", "input": "", "output": "        path = user_path or os.path.join(os.getcwd(), 'billing_reports', os.path.sep)\n        dir_specified = path.endswith(os.sep)\n        if dir_specified:\n            path = os.path.join(path, file_name)\n        path = os.path.abspath(path)\n\n        directory = os.path.dirname(path)\n        if not os.path.isdir(directory):\n            os.makedirs(directory)\n\n        if os.path.exists(path):\n            raise IOError('SDK will not write into an existing path: %r' % path)\n\n        return path", "category": "Python"}, {"instruction": "def set_implementation(self, impl):\n        \"\"\"\n        Sets the implementation of this module\n\n        Parameters\n        ----------\n        impl : str\n            One of [\"python\", \"c\"]\n\n        \"\"\"\n", "input": "", "output": "        if impl.lower() == 'python':\n            self.__impl__ = self.__IMPL_PYTHON__\n        elif impl.lower() == 'c':\n            self.__impl__ = self.__IMPL_C__\n        else:\n            import warnings\n            warnings.warn('Implementation '+impl+' is not known. Using the fallback python implementation.')\n            self.__impl__ = self.__IMPL_PYTHON__", "category": "Python"}, {"instruction": "def getbydatatype(data_type, besteffort=True):\n    \"\"\"Get schema class by data type.\n\n    :param type data_type: data type from where get schema class.\n    :param bool besteffort: if True and data_type not registered, parse all\n        registered data_types and stop when once data_type is a subclass of\n        input data_type.\n    :return: sub class of Schema.\n    :rtype: type\n    \"\"\"\n", "input": "", "output": "    return _REGISTRY.getbydatatype(data_type=data_type, besteffort=besteffort)", "category": "Python"}, {"instruction": "def extract_exif(self):\n        '''\n        Extract a list of exif infos\n        '''\n", "input": "", "output": "        width, height = self.extract_image_size()\n        make, model = self.extract_make(), self.extract_model()\n        orientation = self.extract_orientation()\n        geo = self.extract_geo()\n        capture = self.extract_capture_time()\n        direction = self.extract_direction()\n        d = {\n            'width': width,\n            'height': height,\n            'orientation': orientation,\n            'direction': direction,\n            'make': make,\n            'model': model,\n            'capture_time': capture\n        }\n        d['gps'] = geo\n        return d", "category": "Python"}, {"instruction": "def older_message(m, lastm):\n    '''return true if m is older than lastm by timestamp'''\n", "input": "", "output": "    atts = {'time_boot_ms' : 1.0e-3,\n            'time_unix_usec' : 1.0e-6,\n            'time_usec' : 1.0e-6}\n    for a in atts.keys():\n        if hasattr(m, a):\n            mul = atts[a]\n            t1 = m.getattr(a) * mul\n            t2 = lastm.getattr(a) * mul\n            if t2 >= t1 and t2 - t1 < 60:\n                return True\n    return False", "category": "Python"}, {"instruction": "def color_to_tuple(color, opacity=1):\n    \"\"\" convert any color to standard ()\n    \"red\"       ->  'c3B', (255, 125, 0)\n    \"#ffffff\"   ->  'c3B', (255, 255, 255)\n    \"#ffffffff\" ->  'c4B', (255, 255, 255, 255)\n    \"\"\"\n", "input": "", "output": "    if(type(color) == str and color[0] == \"#\"):\n        color = hex_color_to_tuple(color)\n\n    elif type(color) == str:\n        if color in color_dict:\n            color = color_dict[color.lower()]\n        else:\n            print(\"\u65e0\u6cd5\u89e3\u6790\u989c\u8272:\" + color)\n            color = (255, 125, 0, int(255*opacity)) \n\n    while len(color) < 4:\n        color += (int(255*opacity),)\n\n    return color", "category": "Python"}, {"instruction": "def drain_transport(self):\n\t\t'''\n\t\t\"Drain\" the transport connection.\n\n\t\tThis command simply returns all waiting messages sent from the remote chrome\n\t\tinstance. This can be useful when waiting for a specific asynchronous message\n\t\tfrom chrome, but higher level calls are better suited for managing wait-for-message\n\t\ttype needs.\n\n\t\t'''\n", "input": "", "output": "\t\tself.transport.check_process_ded()\n\t\tret = self.transport.drain(tab_key=self.tab_id)\n\t\tself.transport.check_process_ded()\n\t\treturn ret", "category": "Python"}, {"instruction": "def get_all_xml_file_paths(self, raw_data_directory: str) -> List[str]:\n        \"\"\" Loads all XML-files that are located in the folder.\n        :param raw_data_directory: Path to the raw directory, where the MUSCIMA++ dataset was extracted to\n        \"\"\"\n", "input": "", "output": "        raw_data_directory = os.path.join(raw_data_directory, \"v1.0\", \"data\", \"cropobjects_manual\")\n        xml_files = [y for x in os.walk(raw_data_directory) for y in glob(os.path.join(x[0], '*.xml'))]\n        return xml_files", "category": "Python"}, {"instruction": "def prepare_outdir(outdir):\n    \"\"\"\n    Creates the output directory if not existing.\n    If outdir is None or if no output_files are provided nothing happens.\n\n    :param outdir: The output directory to create.\n    \"\"\"\n", "input": "", "output": "    if outdir:\n        outdir = os.path.expanduser(outdir)\n        if not os.path.isdir(outdir):\n            try:\n                os.makedirs(outdir)\n            except os.error as e:\n                raise JobExecutionError('Failed to create outdir \"{}\".\\n{}'.format(outdir, str(e)))", "category": "Python"}, {"instruction": "def list(context, sort, limit, offset, where, verbose):\n    \"\"\"list(context, sort, limit, offset, where, verbose)\n\n    List all users.\n\n    >>> dcictl user-list\n\n    :param string sort: Field to apply sort\n    :param integer limit: Max number of rows to return\n    :param integer offset: Offset associated with the limit\n    :param string where: An optional filter criteria\n    :param boolean verbose: Display verbose output\n    \"\"\"\n", "input": "", "output": "    result = user.list(\n        context,\n        sort=sort,\n        limit=limit,\n        offset=offset,\n        where=where\n    )\n    utils.format_output(result, context.format, verbose=verbose)", "category": "Python"}, {"instruction": "def load(self):\n        \"\"\"\n        Loads session data from DynamoDB, runs it through the session\n        data de-coder (base64->dict), sets ``self.session``.\n\n        :rtype: dict\n        :returns: The de-coded session data, as a dict.\n        \"\"\"\n", "input": "", "output": "\n        response = self.table.get_item(\n            Key={'session_key': self.session_key},\n            ConsistentRead=ALWAYS_CONSISTENT)\n        if 'Item' in response:\n            session_data = response['Item']['data']\n            return self.decode(session_data)\n        else:\n            self.create()\n            return {}", "category": "Python"}, {"instruction": "def add_graph(patterns, G):\n    \"\"\"Add a graph to a set of unique patterns.\"\"\"\n", "input": "", "output": "    if not patterns:\n        patterns.append([G])\n        return\n    for i, graphs in enumerate(patterns):\n        if networkx.is_isomorphic(graphs[0], G, node_match=type_match,\n                                  edge_match=type_match):\n            patterns[i].append(G)\n            return\n    patterns.append([G])", "category": "Python"}, {"instruction": "def delete(self, stream, start_time, end_time, start_id=None, namespace=None):\n    \"\"\"\n    Delete events in the stream with name `stream` that occurred between\n    `start_time` and `end_time` (both inclusive).  An optional `start_id` allows\n    the client to delete events starting from after an ID rather than starting\n    at a timestamp.\n    \"\"\"\n", "input": "", "output": "    if isinstance(start_time, types.StringTypes):\n      start_time = parse(start_time)\n    if isinstance(end_time, types.StringTypes):\n      end_time = parse(end_time)\n    if isinstance(start_time, datetime):\n      start_time = datetime_to_kronos_time(start_time)\n    if isinstance(end_time, datetime):\n      end_time = datetime_to_kronos_time(end_time)\n    request_dict = {\n      'stream': stream,\n      'end_time': end_time\n    }\n    if start_id:\n      request_dict['start_id'] = start_id\n    else:\n      request_dict['start_time'] = start_time\n\n    namespace = namespace or self.namespace\n    if namespace is not None:\n      request_dict['namespace'] = namespace\n\n    return self._make_request(self._delete_url, data=request_dict)", "category": "Python"}, {"instruction": "def execute(self, eopatch):\n        \"\"\"Returns the EOPatch with renamed features.\n\n        :param eopatch: input EOPatch\n        :type eopatch: EOPatch\n        :return: input EOPatch with the renamed features\n        :rtype: EOPatch\n        \"\"\"\n", "input": "", "output": "        for feature_type, feature_name, new_feature_name in self.feature_gen(eopatch):\n            eopatch[feature_type][new_feature_name] = eopatch[feature_type][feature_name]\n            del eopatch[feature_type][feature_name]\n\n        return eopatch", "category": "Python"}, {"instruction": "def Tz(self,**kwargs): #pragma: no cover\n        \"\"\"\n        NAME:\n           Tz\n        PURPOSE:\n           Calculate the vertical period\n        INPUT:\n           +scipy.integrate.quad keywords\n        OUTPUT:\n           T_z(z,vz)*vc/ro + estimate of the error\n        HISTORY:\n           2012-06-01 - Written - Bovy (IAS)\n        \"\"\"\n", "input": "", "output": "        if hasattr(self,'_Tz'):\n            return self._Tz\n        zmax= self.calczmax()\n        Ez= calcEz(self._z,self._vz,self._verticalpot)\n        self._Tz= 4.*integrate.quad(_TzIntegrand,0.,zmax,\n                                    args=(Ez,self._verticalpot),\n                                    **kwargs)[0]\n        return self._Tz", "category": "Python"}, {"instruction": "def get_name_and_checksum(checksums, end):\n    \"\"\"Extract a full filename and checksum from the checksums list for a file ending in given end.\"\"\"\n", "input": "", "output": "    for entry in checksums:\n        if not entry['file'].endswith(end):\n            # wrong file\n            continue\n        # workaround for ..cds_from_genomic.fna.gz and ..rna_from_genomic.fna.gz also\n        # ending in _genomic.fna.gz, causing bogus matches for the plain fasta\n        if '_from_' not in end and '_from_' in entry['file']:\n            # still the wrong file\n            continue\n        filename = entry['file']\n        expected_checksum = entry['checksum']\n        return filename, expected_checksum\n    raise ValueError('No entry for file ending in {!r}'.format(end))", "category": "Python"}, {"instruction": "def ram2disk(self):\n        \"\"\"Move internal data from RAM to disk.\"\"\"\n", "input": "", "output": "        values = self.series\n        self.deactivate_ram()\n        self.diskflag = True\n        self._save_int(values)\n        self.update_fastaccess()", "category": "Python"}, {"instruction": "def endpoint(self, service, binding=None, context=None):\n        \"\"\" Goes through the list of endpoint specifications for the\n        given type of service and returns a list of endpoint that matches\n        the given binding. If no binding is given all endpoints available for\n        that service will be returned.\n\n        :param service: The service the endpoint should support\n        :param binding: The expected binding\n        :return: All the endpoints that matches the given restrictions\n        \"\"\"\n", "input": "", "output": "        spec = []\n        unspec = []\n        endps = self.getattr(\"endpoints\", context)\n        if endps and service in endps:\n            for endpspec in endps[service]:\n                try:\n                    endp, bind = endpspec\n                    if binding is None or bind == binding:\n                        spec.append(endp)\n                except ValueError:\n                    unspec.append(endpspec)\n\n        if spec:\n            return spec\n        else:\n            return unspec", "category": "Python"}, {"instruction": "def ocrd_cli_options(f):\n    \"\"\"\n    Implement MP CLI.\n\n    Usage::\n\n        import ocrd_click_cli from ocrd.utils\n\n        @click.command()\n        @ocrd_click_cli\n        def cli(mets_url):\n            print(mets_url)\n    \"\"\"\n", "input": "", "output": "    params = [\n        click.option('-m', '--mets', help=\"METS URL to validate\"),\n        click.option('-w', '--working-dir', help=\"Working Directory\"),\n        click.option('-I', '--input-file-grp', help='File group(s) used as input.', default='INPUT'),\n        click.option('-O', '--output-file-grp', help='File group(s) used as output.', default='OUTPUT'),\n        click.option('-g', '--page-id', help=\"ID(s) of the pages to process\"),\n        click.option('-p', '--parameter', type=click.Path()),\n        click.option('-J', '--dump-json', help=\"Dump tool description as JSON and exit\", is_flag=True, default=False),\n        loglevel_option,\n        click.option('-V', '--version', help=\"Show version\", is_flag=True, default=False)\n    ]\n    for param in params:\n        param(f)\n    return f", "category": "Python"}, {"instruction": "def get_name(self, plugin):\n        \"\"\" Return name for registered plugin or None if not registered. \"\"\"\n", "input": "", "output": "        for name, val in self._name2plugin.items():\n            if plugin == val:\n                return name", "category": "Python"}, {"instruction": "def _find_controller(self, class_):\n        \"\"\"Find first controller that matches given request class.\n\n        Order is specified by the keys of\n        ``SpaceTrackClient.request_controllers``\n        (:class:`~collections.OrderedDict`)\n        \"\"\"\n", "input": "", "output": "        for controller, classes in self.request_controllers.items():\n            if class_ in classes:\n                return controller\n        else:\n            raise ValueError('Unknown request class {!r}'.format(class_))", "category": "Python"}, {"instruction": "def stSpectralCentroidAndSpread(X, fs):\n    \"\"\"Computes spectral centroid of frame (given abs(FFT))\"\"\"\n", "input": "", "output": "    ind = (numpy.arange(1, len(X) + 1)) * (fs/(2.0 * len(X)))\n\n    Xt = X.copy()\n    Xt = Xt / Xt.max()\n    NUM = numpy.sum(ind * Xt)\n    DEN = numpy.sum(Xt) + eps\n\n    # Centroid:\n    C = (NUM / DEN)\n\n    # Spread:\n    S = numpy.sqrt(numpy.sum(((ind - C) ** 2) * Xt) / DEN)\n\n    # Normalize:\n    C = C / (fs / 2.0)\n    S = S / (fs / 2.0)\n\n    return (C, S)", "category": "Python"}, {"instruction": "def typed_range(type_func, minimum, maximum):\n    \"\"\"\n    Require variables to be of the specified type, between minimum and maximum\n    \"\"\"\n", "input": "", "output": "    @functools.wraps(type_func)\n    def inner(string):\n        result = type_func(string)\n        if not result >= minimum and result <= maximum:\n            raise argparse.ArgumentTypeError(\n                    \"Please provide a value between {0} and {1}\".format(\n                        minimum, maximum))\n        return result\n    return inner", "category": "Python"}, {"instruction": "def _eval_binop(self, node):\n        \"\"\"\n        Evaluate a binary operator node (ie. 2+3, 5*6, 3 ** 4)\n\n        :param node: Node to eval\n        :return: Result of node\n        \"\"\"\n", "input": "", "output": "        return self.operators[type(node.op)](self._eval(node.left),\n                                             self._eval(node.right))", "category": "Python"}, {"instruction": "def compose_post(apikey, resize, rotation, noexif):\n    \"\"\"\n    composes basic post requests\n    \"\"\"\n", "input": "", "output": "    check_rotation(rotation)\n    check_resize(resize)\n\n    post_data = {\n            'formatliste': ('', 'og'),\n            'userdrehung': ('', rotation),\n            'apikey': ('', apikey)\n            }\n\n    if resize and 'x' in resize:\n        width, height = [ x.strip() for x in resize.split('x')]\n        post_data['udefb'] = ('', width)\n        post_data['udefh'] = ('', height)\n    elif resize and '%' in resize:\n        precentage = resize.strip().strip('%')\n        post_data['udefp'] = precentage\n\n    if noexif:\n        post_data['noexif'] = ('', '')\n\n    return post_data", "category": "Python"}, {"instruction": "def CompileReport(self, mediator):\n    \"\"\"Compiles an analysis report.\n\n    Args:\n      mediator (AnalysisMediator): mediates interactions between analysis\n          plugins and other components, such as storage and dfvfs.\n\n    Returns:\n      AnalysisReport: analysis report.\n    \"\"\"\n", "input": "", "output": "    lines_of_text = []\n    for user, extensions in sorted(self._results.items()):\n      lines_of_text.append(' == USER: {0:s} =='.format(user))\n      for extension, extension_identifier in sorted(extensions):\n        lines_of_text.append('  {0:s} [{1:s}]'.format(\n            extension, extension_identifier))\n      lines_of_text.append('')\n\n    lines_of_text.append('')\n    report_text = '\\n'.join(lines_of_text)\n    analysis_report = reports.AnalysisReport(\n        plugin_name=self.NAME, text=report_text)\n    analysis_report.report_dict = self._results\n    return analysis_report", "category": "Python"}, {"instruction": "def add(self, quote_id, product_data, store_view=None):\n        \"\"\"\n        Allows you to add one or more products to the shopping cart (quote).\n\n        :param quote_id: Shopping cart ID (quote ID)\n        :param product_data, list of dicts of product details, example\n            [\n                {\n                    'product_id': 1,\n                    'qty': 2,\n                    'options': {\n                        'option_1': 'value_1',\n                        'option_2': 'value_2',\n                        ...\n                    },\n                    'bundle_option': {},\n                    'bundle_option_qty': {},\n                    'links': [],\n                },\n                {\n                    'sku': 'S0012345',\n                    'qty': 4,\n                },\n            ]\n        :param store_view: Store view ID or code\n        :return: boolean, True on success (if the product is added to the\n                shopping cart)\n        \"\"\"\n", "input": "", "output": "        return bool(\n            self.call('cart_product.add', [quote_id, product_data, store_view])\n        )", "category": "Python"}, {"instruction": "def timezone(self, tz):\n        \"\"\"\n        Set timezone on the audit records. Timezone can be in formats:\n        'US/Eastern', 'PST', 'Europe/Helsinki'\n        \n        See SMC Log Viewer settings for more examples.\n        \n        :param str tz: timezone, i.e. CST\n        \"\"\"\n", "input": "", "output": "        self.data['resolving'].update(\n            timezone=tz,\n            time_show_zone=True)", "category": "Python"}, {"instruction": "def sort_timeseries(self, ascending=True):\n        \"\"\"Sorts the data points within the TimeSeries according to their occurrence inline.\n\n        :param boolean ascending: Determines if the TimeSeries will be ordered ascending or\n            descending. If this is set to descending once, the ordered parameter defined in\n            :py:meth:`TimeSeries.__init__` will be set to False FOREVER.\n\n        :return:    Returns :py:obj:`self` for convenience.\n        :rtype:     TimeSeries\n        \"\"\"\n", "input": "", "output": "        # the time series is sorted by default\n        if ascending and self._sorted:\n            return\n\n        sortorder = 1\n        if not ascending:\n            sortorder = -1\n            self._predefinedSorted = False\n\n        self._timeseriesData.sort(key=lambda i: sortorder * i[0])\n\n        self._sorted = ascending\n\n        return self", "category": "Python"}, {"instruction": "def process_hive(vargs):\n    \"\"\"\n    Main Hive.co path.\n    \"\"\"\n", "input": "", "output": "\n    artist_url = vargs['artist_url']\n\n    if 'hive.co' in artist_url:\n        mc_url = artist_url\n    else:\n        mc_url = 'https://www.hive.co/downloads/download/' + artist_url\n\n    filenames = scrape_hive_url(mc_url, num_tracks=vargs['num_tracks'], folders=vargs['folders'], custom_path=vargs['path'])\n\n    if vargs['open']:\n        open_files(filenames)\n\n    return", "category": "Python"}, {"instruction": "def show_vcs_output_vcs_nodes_vcs_node_info_node_public_ipv6_addresses_node_public_ipv6_address(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        show_vcs = ET.Element(\"show_vcs\")\n        config = show_vcs\n        output = ET.SubElement(show_vcs, \"output\")\n        vcs_nodes = ET.SubElement(output, \"vcs-nodes\")\n        vcs_node_info = ET.SubElement(vcs_nodes, \"vcs-node-info\")\n        node_public_ipv6_addresses = ET.SubElement(vcs_node_info, \"node-public-ipv6-addresses\")\n        node_public_ipv6_address = ET.SubElement(node_public_ipv6_addresses, \"node-public-ipv6-address\")\n        node_public_ipv6_address.text = kwargs.pop('node_public_ipv6_address')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def _vksdesc(d):\n    '''\n        d = {'a':1,'b':2,'c':2,'d':4}\n        desc = _vksdesc(d)\n        pobj(desc)\n    '''\n", "input": "", "output": "    pt = copy.deepcopy(d)\n    seqs_for_del =[]\n    vset = set({})\n    for k in pt:\n        vset.add(pt[k])\n    desc = {}\n    for v in vset:\n        desc[v] = []\n    for k in pt:\n        desc[pt[k]].append(k)\n    return(desc)", "category": "Python"}, {"instruction": "def get_pid(rundir, process_type=PROCESS_TYPE, name=None):\n    \"\"\"\n    Get the pid from the pid file in the run directory, using the given\n    process type and process name for the filename.\n\n    @returns: pid of the process, or None if not running or file not found.\n    \"\"\"\n", "input": "", "output": "    pidPath = get_pidpath(rundir, process_type, name)\n    log.log('run', 'pidfile for %s %s is %s' % (process_type, name, pidPath))\n\n    if not os.path.exists(pidPath):\n        return\n\n    pidFile = open(pidPath, 'r')\n    pid = pidFile.readline()\n    pidFile.close()\n    if not pid or int(pid) == 0:\n        return\n\n    return int(pid)", "category": "Python"}, {"instruction": "def preferred_width(self, cli, max_available_width):\n        \"\"\"\n        Report the width of the longest meta text as the preferred width of this control.\n\n        It could be that we use less width, but this way, we're sure that the\n        layout doesn't change when we select another completion (E.g. that\n        completions are suddenly shown in more or fewer columns.)\n        \"\"\"\n", "input": "", "output": "        if cli.current_buffer.complete_state:\n            state = cli.current_buffer.complete_state\n            return 2 + max(get_cwidth(c.display_meta) for c in state.current_completions)\n        else:\n            return 0", "category": "Python"}, {"instruction": "def get_street_from_xy(self, **kwargs):\n        \"\"\"Obtain a list of streets around the specified point.\n\n        Args:\n            latitude (double): Latitude in decimal degrees.\n            longitude (double): Longitude in decimal degrees.\n            radius (int): Radius (in meters) of the search.\n            lang (str): Language code (*es* or *en*).\n\n        Returns:\n            Status boolean and parsed response (list[Street]), or message string\n            in case of error.\n        \"\"\"\n", "input": "", "output": "        # Endpoint parameters\n        params = {\n            'coordinateX': kwargs.get('longitude'),\n            'coordinateY': kwargs.get('latitude'),\n            'Radius': kwargs.get('radius'),\n            'cultureInfo': util.language_code(kwargs.get('lang'))\n        }\n\n        # Request\n        result = self.make_request('geo', 'get_street_from_xy', **params)\n\n        # Funny endpoint, no status code\n        if not util.check_result(result, 'site'):\n            return False, 'UNKNOWN ERROR'\n\n        # Parse\n        values = util.response_list(result, 'site')\n        return True, [emtype.Street(**a) for a in values]", "category": "Python"}, {"instruction": "def serialize_to_string(self, name, datas):\n        \"\"\"\n        Serialize given datas to a string.\n\n        Simply return the value from required variable``value``.\n\n        Arguments:\n            name (string): Name only used inside possible exception message.\n            datas (dict): Datas to serialize.\n\n        Returns:\n            string: Value.\n        \"\"\"\n", "input": "", "output": "        value = datas.get('value', None)\n\n        if value is None:\n            msg = (\"String reference '{}' lacks of required 'value' variable \"\n                   \"or is empty\")\n            raise SerializerError(msg.format(name))\n\n        return value", "category": "Python"}, {"instruction": "def prepend(cls, d, s, filter=Filter()):\n        \"\"\"\n        Prepend schema object's from B{s}ource list to\n        the B{d}estination list while applying the filter.\n        @param d: The destination list.\n        @type d: list\n        @param s: The source list.\n        @type s: list\n        @param filter: A filter that allows items to be prepended.\n        @type filter: L{Filter}\n        \"\"\"\n", "input": "", "output": "        i = 0\n        for x in s:\n            if x in filter:\n                d.insert(i, x)\n                i += 1", "category": "Python"}, {"instruction": "def add(self, key):\n        \"\"\" Store new key in a new link at the end of the linked list \"\"\"\n", "input": "", "output": "        if key not in self._map:\n            self._map[key] = link = _Link()\n            root = self._root\n            last = root.prev\n            link.prev, link.next, link.key = last, root, key\n            last.next = root.prev = weakref.proxy(link)", "category": "Python"}, {"instruction": "def _SetYaraRules(self, yara_rules_string):\n    \"\"\"Sets the Yara rules.\n\n    Args:\n      yara_rules_string (str): unparsed Yara rule definitions.\n    \"\"\"\n", "input": "", "output": "    if not yara_rules_string:\n      return\n\n    analyzer_object = analyzers_manager.AnalyzersManager.GetAnalyzerInstance(\n        'yara')\n    analyzer_object.SetRules(yara_rules_string)\n    self._analyzers.append(analyzer_object)", "category": "Python"}, {"instruction": "def quote_names(db, names):\n    \"\"\"psycopg2 doesn't know how to quote identifier names, so we ask the server\"\"\"\n", "input": "", "output": "    c = db.cursor()\n    c.execute(\"SELECT pg_catalog.quote_ident(n) FROM pg_catalog.unnest(%s::text[]) n\", [list(names)])\n    return [name for (name,) in c]", "category": "Python"}, {"instruction": "def patch_f90_compiler(f90_compiler):\n    \"\"\"Patch up ``f90_compiler.library_dirs``.\n\n    On macOS, a Homebrew installed ``gfortran`` needs some help. The\n    ``numpy.distutils`` \"default\" constructor for ``Gnu95FCompiler`` only has\n    a single library search path, but there are many library paths included in\n    the full ``gcc`` install.\n\n    Args:\n        f90_compiler (numpy.distutils.fcompiler.FCompiler): A Fortran compiler\n            instance.\n    \"\"\"\n", "input": "", "output": "    if not is_macos_gfortran(f90_compiler):\n        return\n\n    library_dirs = f90_compiler.library_dirs\n    # ``library_dirs`` is a list (i.e. mutable), so we can update in place.\n    library_dirs[:] = setup_helpers.gfortran_search_path(library_dirs)", "category": "Python"}, {"instruction": "def all_arch_srcarch_pairs():\n    \"\"\"\n    Generates all valid (ARCH, SRCARCH) tuples for the kernel, corresponding to\n    different architectures. SRCARCH holds the arch/ subdirectory.\n    \"\"\"\n", "input": "", "output": "    for srcarch in os.listdir(\"arch\"):\n        # Each subdirectory of arch/ containing a Kconfig file corresponds to\n        # an architecture\n        if os.path.exists(os.path.join(\"arch\", srcarch, \"Kconfig\")):\n            yield (srcarch, srcarch)\n\n    # Some architectures define additional ARCH settings with ARCH != SRCARCH\n    # (search for \"Additional ARCH settings for\" in the top-level Makefile)\n\n    yield (\"i386\", \"x86\")\n    yield (\"x86_64\", \"x86\")\n\n    yield (\"sparc32\", \"sparc\")\n    yield (\"sparc64\", \"sparc\")\n\n    yield (\"sh64\", \"sh\")\n\n    yield (\"um\", \"um\")", "category": "Python"}, {"instruction": "def handle_error(self, error):\n        \"\"\"\n        Try to detect repetitive errors and sleep for a while to avoid being marked as spam\n        \"\"\"\n", "input": "", "output": "        logging.exception(\"try to sleep if there are repeating errors.\")\n        error_desc = str(error)\n        now = datetime.datetime.now()\n        if error_desc not in self.error_time_log:\n            self.error_time_log[error_desc] = now\n            return\n\n        time_of_last_encounter = self.error_time_log[str(error)]\n        time_since_last_encounter = now - time_of_last_encounter\n        if time_since_last_encounter.total_seconds() > self.config.get('min_seconds_between_errors'):\n            self.error_time_log[error_desc] = now\n            return\n\n        if error_desc not in self.error_sleep_log:\n            time.sleep(self.config.get('sleep_seconds_on_consecutive_errors'))\n            self.error_sleep_log[error_desc] = 1\n        else:\n            sys.exit()", "category": "Python"}, {"instruction": "def getTypesModuleName(self):\n        '''return module name.\n        '''\n", "input": "", "output": "        assert self.wsdl is not None, 'initialize, call fromWSDL'\n        if self.types_module_name is not None:\n            return self.types_module_name\n\n        wsm = WriteServiceModule(self.wsdl)\n        return wsm.getTypesModuleName()", "category": "Python"}, {"instruction": "def urlunsplit(components):\n    \"\"\"Combine the elements of a tuple as returned by urlsplit() into a\n    complete URL as a string. The data argument can be any five-item iterable.\n    This may result in a slightly different, but equivalent URL, if the URL that\n    was parsed originally had unnecessary delimiters (for example, a ? with an\n    empty query; the RFC states that these are equivalent).\"\"\"\n", "input": "", "output": "    scheme, netloc, url, query, fragment, _coerce_result = (\n                                          _coerce_args(*components))\n    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):\n        if url and url[:1] != '/': url = '/' + url\n        url = '//' + (netloc or '') + url\n    if scheme:\n        url = scheme + ':' + url\n    if query:\n        url = url + '?' + query\n    if fragment:\n        url = url + '#' + fragment\n    return _coerce_result(url)", "category": "Python"}, {"instruction": "def train_eval():\n    \"\"\" train and eval the model\n    \"\"\"\n", "input": "", "output": "\n    global trainloader\n    global testloader\n    global net\n\n    (x_train, y_train) = trainloader\n    (x_test, y_test) = testloader\n\n    # train procedure\n    net.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=args.batch_size,\n        validation_data=(x_test, y_test),\n        epochs=args.epochs,\n        shuffle=True,\n        callbacks=[\n            SendMetrics(),\n            EarlyStopping(min_delta=0.001, patience=10),\n            TensorBoard(log_dir=TENSORBOARD_DIR),\n        ],\n    )\n\n    # trial report final acc to tuner\n    _, acc = net.evaluate(x_test, y_test)\n    logger.debug(\"Final result is: %.3f\", acc)\n    nni.report_final_result(acc)", "category": "Python"}, {"instruction": "def db_parse( block_id, opcode, op_payload, senders, inputs, outputs, fee, db_state=None ):\n   \"\"\"\n   Given the block ID, and information from what looks like \n   an OP_RETURN transaction that is part of the virtual chain, parse the \n   transaction's OP_RETURN nulldata into a dict.\n   \n   Return the dict if this is a valid op.\n   Return None if not.\n   \n   NOTE: the virtual chain indexer reserves all keys that start with 'virtualchain_'\n   \"\"\"\n", "input": "", "output": "   print \"\\nreference implementation of db_parse\\n\"\n   return None", "category": "Python"}, {"instruction": "def install(zone, nodataset=False, brand_opts=None):\n    '''\n    Install the specified zone from the system.\n\n    zone : string\n        name of the zone\n    nodataset : boolean\n        do not create a ZFS file system\n    brand_opts : string\n        brand specific options to pass\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zoneadm.install dolores\n        salt '*' zoneadm.install teddy True\n    '''\n", "input": "", "output": "    ret = {'status': True}\n\n    ## install zone\n    res = __salt__['cmd.run_all']('zoneadm -z {zone} install{nodataset}{brand_opts}'.format(\n        zone=zone,\n        nodataset=' -x nodataset' if nodataset else '',\n        brand_opts=' {0}'.format(brand_opts) if brand_opts else '',\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    ret['message'] = ret['message'].replace('zoneadm: ', '')\n    if ret['message'] == '':\n        del ret['message']\n\n    return ret", "category": "Python"}, {"instruction": "def save(self, model, joining=None, touch=True):\n        \"\"\"\n        Save a new model and attach it to the parent model.\n\n        :type model: eloquent.Model\n        :type joining: dict\n        :type touch: bool\n\n        :rtype: eloquent.Model\n        \"\"\"\n", "input": "", "output": "        if joining is None:\n            joining = {}\n\n        model.save({'touch': False})\n\n        self.attach(model.get_key(), joining, touch)\n\n        return model", "category": "Python"}, {"instruction": "def add_plugin_filepaths(self, filepaths, except_blacklisted=True):\n        \"\"\"\n        Adds `filepaths` to internal state. Recommend passing\n        in absolute filepaths. Method will attempt to convert to\n        absolute paths if they are not already.\n\n        `filepaths` can be a single object or an iterable\n\n        If `except_blacklisted` is `True`, all `filepaths` that\n        have been blacklisted will not be added.\n        \"\"\"\n", "input": "", "output": "        self.file_manager.add_plugin_filepaths(filepaths,\n                                               except_blacklisted)", "category": "Python"}, {"instruction": "def gauss_pdf(x, mu, sigma):\n    \"\"\"Normalized Gaussian\"\"\"\n", "input": "", "output": "    return 1 / np.sqrt(2 * np.pi) / sigma * np.exp(-(x - mu) ** 2 / 2. / sigma ** 2)", "category": "Python"}, {"instruction": "def _parseline(self, line):\n        \"\"\"\n        https://jira.bikalabs.com/browse/LIMS-1818?focusedCommentId=16915&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16915\n        Only first 4 columns are important:\n\n        3/24/2015 7:55 AM BOG 651 (IND) - 16 0.10931925301288605 0.016803081793406938\n\n        Date: 3/24/2015 7:55 AM\n        Sample: BOG 651\n        Carbon: 0.10931925301288605\n        Sulphur: 0.016803081793406938\n        \"\"\"\n", "input": "", "output": "\n        sline = line.split('\\t')\n        if len(sline) < 4:\n            return -1\n        try:\n            raw_dict = {\n                self._analysis1: {\n                    'DefaultResult': 'Result',\n                    'Result': sline[2],\n                    'Date': self.csvDate2BikaDate(sline[0])\n                },\n                self._analysis2: {\n                    'DefaultResult': 'Result',\n                    'Result': sline[3],\n                    'Date': self.csvDate2BikaDate(sline[0])\n                }\n            }\n            self._addRawResult(sline[1], raw_dict)\n            return 0\n        except IndexError:\n            return -1", "category": "Python"}, {"instruction": "def _parseEnv(self, env=None):\n        \"\"\"Private method for parsing through environment variables.\n        \n        Parses for environment variables common to all Munin Plugins:\n            - MUNIN_STATEFILE\n            - MUNIN_CAP_DIRTY_CONFIG\n            - nested_graphs\n        \n        @param env: Dictionary of environment variables.\n                    (Only used for testing. initialized automatically by \n                    constructor.\n        \n        \"\"\"\n", "input": "", "output": "        if not env:\n            env = self._env\n        if env.has_key('MUNIN_STATEFILE'):\n            self._stateFile = env.get('MUNIN_STATEFILE')\n        else:\n            self._stateFile = '/tmp/munin-state-%s' % self.plugin_name\n        if env.has_key('MUNIN_CAP_DIRTY_CONFIG'):\n            self._dirtyConfig = True", "category": "Python"}, {"instruction": "def _flag_transform(flags):\n    \"\"\"Transform flags to glob defaults.\"\"\"\n", "input": "", "output": "\n    # Here we force `PATHNAME`.\n    flags = (flags & FLAG_MASK) | _wcparse.PATHNAME\n    if flags & _wcparse.REALPATH and util.platform() == \"windows\":\n        flags |= _wcparse._FORCEWIN\n        if flags & _wcparse.FORCECASE:\n            flags ^= _wcparse.FORCECASE\n    return flags", "category": "Python"}, {"instruction": "def default_stokes(self, context):\n    \"\"\"\n    Returns [[1, 0], tiled up to other dimensions\n             [0, 0]]\n    \"\"\"\n", "input": "", "output": "    A = np.empty(context.shape, context.dtype)\n    A[:,:,:] = [[[1,0,0,0]]]\n    return A", "category": "Python"}, {"instruction": "def dcyldr(x, y, z):\n    \"\"\"\n    This routine computes the Jacobian of the transformation from\n    rectangular to cylindrical coordinates.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/dcyldr_c.html\n\n    :param x: X-coordinate of point.\n    :type x: float\n    :param y: Y-coordinate of point.\n    :type y: float\n    :param z: Z-coordinate of point.\n    :type z: float\n    :return: Matrix of partial derivatives.\n    :rtype: 3x3-Element Array of floats\n    \"\"\"\n", "input": "", "output": "    x = ctypes.c_double(x)\n    y = ctypes.c_double(y)\n    z = ctypes.c_double(z)\n    jacobi = stypes.emptyDoubleMatrix()\n    libspice.dcyldr_c(x, y, z, jacobi)\n    return stypes.cMatrixToNumpy(jacobi)", "category": "Python"}, {"instruction": "def add(self, selected: 'SelectedMailbox', *,\n            replace: 'SelectedMailbox' = None) -> None:\n        \"\"\"Add a new selected mailbox object to the set, which may then be\n        returned by :meth:`.any_selected`.\n\n        Args:\n            selected: The new selected mailbox object.\n            replace: An existing selected mailbox object that should be removed\n                from the weak set.\n\n        \"\"\"\n", "input": "", "output": "        if replace is not None:\n            self._set.discard(replace)\n        self._set.add(selected)", "category": "Python"}, {"instruction": "def _get_unique_categories(self, df):\n        \"\"\"Get all categories for each categorical columns from training data.\"\"\"\n", "input": "", "output": "\n        categories = []\n        for col in self._categorical_columns:\n            categocial = pd.Categorical(df[col])\n            col_categories = list(map(str, categocial.categories))\n            col_categories.append('_UNKNOWN')\n            categories.append(col_categories)\n        return categories", "category": "Python"}, {"instruction": "def as_coeff_unit(self):\n        \"\"\"Factor the coefficient multiplying a unit\n\n        For units that are multiplied by a constant dimensionless\n        coefficient, returns a tuple containing the coefficient and\n        a new unit object for the unmultiplied unit.\n\n        Example\n        -------\n\n        >>> import unyt as u\n        >>> unit = (u.m**2/u.cm).simplify()\n        >>> unit\n        100*m\n        >>> unit.as_coeff_unit()\n        (100.0, m)\n        \"\"\"\n", "input": "", "output": "        coeff, mul = self.expr.as_coeff_Mul()\n        coeff = float(coeff)\n        ret = Unit(\n            mul,\n            self.base_value / coeff,\n            self.base_offset,\n            self.dimensions,\n            self.registry,\n        )\n        return coeff, ret", "category": "Python"}, {"instruction": "def firstSacDist(fm):\n    \"\"\"\n        Computes the distribution of angle and length\n        combinations that were made as first saccades\n        \n        Parameters:\n            fm : ocupy.fixmat \n                The fixation data to be analysed\n    \n    \"\"\"\n", "input": "", "output": "    ang, leng, ad, ld = anglendiff(fm, return_abs=True)\n    y_arg = leng[0][np.roll(fm.fix == min(fm.fix), 1)]/fm.pixels_per_degree\n    x_arg = reshift(ang[0][np.roll(fm.fix == min(fm.fix), 1)])\n    bins = [list(range(int(ceil(np.nanmax(y_arg)))+1)), np.linspace(-180, 180, 361)]\n    return makeHist(x_arg, y_arg, fit=None, bins = bins)", "category": "Python"}, {"instruction": "def process_forever(self, timeout=0.2):\n        \"\"\"Run an infinite loop, processing data from connections.\n\n        This method repeatedly calls process_once.\n\n        Arguments:\n\n            timeout -- Parameter to pass to process_once.\n        \"\"\"\n", "input": "", "output": "        # This loop should specifically *not* be mutex-locked.\n        # Otherwise no other thread would ever be able to change\n        # the shared state of a Reactor object running this function.\n        log.debug(\"process_forever(timeout=%s)\", timeout)\n        one = functools.partial(self.process_once, timeout=timeout)\n        consume(infinite_call(one))", "category": "Python"}, {"instruction": "def getquerylist(self, sep='&', encoding='utf-8', errors='strict'):\n        \"\"\"Split the query component into individual `name=value` pairs\n        separated by `sep`, and return a list of `(name, value)`\n        tuples.\n\n        \"\"\"\n", "input": "", "output": "        if not self.query:\n            return []\n        elif isinstance(sep, type(self.query)):\n            qsl = self.query.split(sep)\n        elif isinstance(sep, bytes):\n            qsl = self.query.split(sep.decode('ascii'))\n        else:\n            qsl = self.query.split(sep.encode('ascii'))\n        items = []\n        for parts in [qs.partition(self.EQ) for qs in qsl if qs]:\n            name = uridecode(parts[0], encoding, errors)\n            if parts[1]:\n                value = uridecode(parts[2], encoding, errors)\n            else:\n                value = None\n            items.append((name, value))\n        return items", "category": "Python"}, {"instruction": "def reduce_value(value, default=EMPTY_STR):\n    \"\"\"\n    :return: a single value from lists, tuples or sets with one item;\n    otherwise, the value itself if not empty or the default if it is.\n    \"\"\"\n", "input": "", "output": "\n    if hasattr(value, '__len__'):\n        vlen = len(value)\n\n        if vlen == 0:\n            return default\n        elif vlen == 1:\n            if isinstance(value, set):\n                return value.pop()\n            elif isinstance(value, _reduce_types):\n                return value[0]\n\n    return default if value is None else value", "category": "Python"}, {"instruction": "def shutdown(self):\n        \"\"\"\n        Shuts down all CAN interfaces and hardware interface.\n        \"\"\"\n", "input": "", "output": "        try:\n            self._ucan.shutdown()\n        except Exception as ex:\n            log.error(ex)", "category": "Python"}, {"instruction": "def clear_face_values(self):\n        \"\"\"stub\"\"\"\n", "input": "", "output": "        if (self.get_face_values_metadata().is_read_only() or\n                self.get_face_values_metadata().is_required()):\n            raise NoAccess()\n        self.clear_integer_value('frontFaceValue')\n        self.clear_integer_value('sideFaceValue')\n        self.clear_integer_value('topFaceValue')", "category": "Python"}, {"instruction": "def get_branding_metadata(self):\n        \"\"\"Gets the metadata for the asset branding.\n\n        return: (osid.Metadata) - metadata for the asset branding.\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        metadata = dict(self._branding_metadata)\n        metadata.update({'existing_id_values': self.my_osid_object_form._my_map['brandingIds']})\n        return Metadata(**metadata)", "category": "Python"}, {"instruction": "def _get_distance_term(self, C, rhypo, mag):\n        \"\"\"\n        Returns the distance scaling term\n        \"\"\"\n", "input": "", "output": "        h_eff = self._get_effective_distance(mag)\n        r_val = np.sqrt(rhypo ** 2.0 + h_eff ** 2.0)\n        return C[\"c3\"] * np.log10(r_val)", "category": "Python"}, {"instruction": "def info(self, channel_id):\n        \"\"\"Gets channel information by channel id\n\n        Args:\n            channel_id(int): the id of channel\n\n        Returns:\n            Channel\n\n        Throws:\n            RTMServiceError when request failed\n        \"\"\"\n", "input": "", "output": "        resource = 'v1/channel.info?channel_id={}'.format(channel_id)\n        resp = self._rtm_client.get(resource)\n        if resp.is_fail():\n            raise RTMServiceError(\"Failed to get channel information\", resp)\n\n        return resp.data['result']", "category": "Python"}, {"instruction": "def warped_gp_cubic_sine(max_iters=100):\n    \"\"\"\n    A test replicating the cubic sine regression problem from\n    Snelson's paper.\n    \"\"\"\n", "input": "", "output": "    X = (2 * np.pi) * np.random.random(151) - np.pi\n    Y = np.sin(X) + np.random.normal(0,0.2,151)\n    Y = np.array([np.power(abs(y),float(1)/3) * (1,-1)[y<0] for y in Y])\n    X = X[:, None]\n    Y = Y[:, None]\n\n    warp_k = GPy.kern.RBF(1)\n    warp_f = GPy.util.warping_functions.TanhFunction(n_terms=2)\n    warp_m = GPy.models.WarpedGP(X, Y, kernel=warp_k, warping_function=warp_f)\n    warp_m['.*\\.d'].constrain_fixed(1.0)\n    m = GPy.models.GPRegression(X, Y)\n    m.optimize_restarts(parallel=False, robust=True, num_restarts=5, max_iters=max_iters)\n    warp_m.optimize_restarts(parallel=False, robust=True, num_restarts=5, max_iters=max_iters)\n    #m.optimize(max_iters=max_iters)\n    #warp_m.optimize(max_iters=max_iters)\n\n    print(warp_m)\n    print(warp_m['.*warp.*'])\n\n    warp_m.predict_in_warped_space = False\n    warp_m.plot(title=\"Warped GP - Latent space\")\n    warp_m.predict_in_warped_space = True\n    warp_m.plot(title=\"Warped GP - Warped space\")\n    m.plot(title=\"Standard GP\")\n    warp_m.plot_warping()\n    pb.show()", "category": "Python"}, {"instruction": "def truth(val, context):\n    \"\"\" Convert truth value in \"val\" to a boolean.\n    \"\"\"\n", "input": "", "output": "    try:\n        0 + val\n    except TypeError:\n        lower_val = val.lower()\n\n        if lower_val in TRUE:\n            return True\n        elif lower_val in FALSE:\n            return False\n        else:\n            raise FilterError(\"Bad boolean value %r in %r (expected one of '%s', or '%s')\" % (\n                val, context, \"' '\".join(TRUE), \"' '\".join(FALSE)\n            ))\n    else:\n        return bool(val)", "category": "Python"}, {"instruction": "def cli(env):\n    \"\"\"Print environment variables.\"\"\"\n", "input": "", "output": "    filtered_vars = dict([(k, v)\n                          for k, v in env.vars.items()\n                          if not k.startswith('_')])\n    env.fout(formatting.iter_to_table(filtered_vars))", "category": "Python"}, {"instruction": "def _wait_threads(self):\n        \"\"\"\n        Tell all the threads to terminate (by sending a sentinel value) and\n        wait for them to do so.\n        \"\"\"\n", "input": "", "output": "        # Note that you need two loops, since you can't say which\n        # thread will get each sentinel\n        for t in self._threads:\n            self._to_fetch.put(None)    # sentinel\n        for t in self._threads:\n            t.join()\n        self._threads = []", "category": "Python"}, {"instruction": "def list_huisnummers_adapter(obj, request):\n    \"\"\"\n    Adapter for rendering a list of\n    :class:`crabpy.gateway.crab.Huisnummer` to json.\n    \"\"\"\n", "input": "", "output": "    return {\n        'id': obj.id,\n        'status': {\n            'id': obj.status.id,\n            'naam': obj.status.naam,\n            'definitie': obj.status.definitie\n        },\n        'label': obj.huisnummer\n    }", "category": "Python"}, {"instruction": "def datetime_to_ms(dt):\n    \"\"\"\n    Converts a datetime to a millisecond accuracy timestamp\n    \"\"\"\n", "input": "", "output": "    seconds = calendar.timegm(dt.utctimetuple())\n    return seconds * 1000 + int(dt.microsecond / 1000)", "category": "Python"}, {"instruction": "def column( self, name ):\r\n        \"\"\"\r\n        Returns the index of the column at the given name.\r\n        \r\n        :param      name | <str>\r\n        \r\n        :return     <int> (-1 if not found)\r\n        \"\"\"\n", "input": "", "output": "        columns = self.columns()\r\n        if ( name in columns ):\r\n            return columns.index(name)\r\n        return -1", "category": "Python"}, {"instruction": "def asset_taskfile_sel_changed(self, tf):\n        \"\"\"Callback for when the version selection has changed\n\n        :param tf: the selected taskfileinfo\n        :type tf: :class:`TaskFileInfo` | None\n        :returns: None\n        :rtype: None\n        :raises: None\n        \"\"\"\n", "input": "", "output": "        self.asset_open_pb.setEnabled(bool(tf))\n        # only allow new, when there is an asset. if there is an asset, there should always be a task\n        enablenew = bool(self.browser.assetbrws.selected_indexes(1)) and self.browser.get_releasetype() == djadapter.RELEASETYPES['work']\n        self.asset_save_pb.setEnabled(enablenew)\n        self.asset_descriptor_le.setEnabled(enablenew)\n        self.asset_comment_pte.setEnabled(enablenew)\n        self.update_descriptor_le(self.asset_descriptor_le, tf)", "category": "Python"}, {"instruction": "def about(self):\n        \"\"\"\n        Get server info.  Uses GET to /about interface\n\n        :returns: dict - Server information\n        \"\"\"\n", "input": "", "output": "        response = self._get(url.about)\n        self._check_response(response, 200)\n        return self._create_response(response)", "category": "Python"}, {"instruction": "def at(self, p):\n        \"\"\"\n        Returns the set of all intervals that contain p.\n\n        Completes in O(m + log n) time, where:\n          * n = size of the tree\n          * m = number of matches\n        :rtype: set of Interval\n        \"\"\"\n", "input": "", "output": "        root = self.top_node\n        if not root:\n            return set()\n        return root.search_point(p, set())", "category": "Python"}, {"instruction": "def write_release_version(version):\n    \"\"\"Write the release version to ``_version.py``.\"\"\"\n", "input": "", "output": "    dirname = os.path.abspath(os.path.dirname(__file__))\n    f = open(os.path.join(dirname, \"_version.py\"), \"wt\")\n    f.write(\"__version__ = '%s'\\n\" % version)\n    f.close()", "category": "Python"}, {"instruction": "def add_parameter(self, name, value=None):\n\t\t\"\"\" Add new parameter value to this query. New value will be appended to previously added values.\n\n\t\t:param name: parameter name\n\t\t:param value: value to add (None to set null-value)\n\t\t:return: None\n\t\t\"\"\"\n", "input": "", "output": "\t\tif name not in self.__query:\n\t\t\tself.__query[name] = [value]\n\t\telse:\n\t\t\tself.__query[name].append(value)", "category": "Python"}, {"instruction": "def get_environmental_configuration(self):\n        \"\"\"\n        Gets the settings that describe the environmental configuration (supported feature set, calibrated minimum &\n        maximum power, location & dimensions, ...) of the enclosure resource.\n\n        Returns:\n            Settings that describe the environmental configuration.\n        \"\"\"\n", "input": "", "output": "        uri = '{}/environmentalConfiguration'.format(self.data['uri'])\n        return self._helper.do_get(uri)", "category": "Python"}, {"instruction": "def wait_until_page_contains(self, text, timeout=None, error=None):\n        \"\"\"Waits until `text` appears on current page.\n\n        Fails if `timeout` expires before the text appears. See\n        `introduction` for more information about `timeout` and its\n        default value.\n\n        `error` can be used to override the default error message.\n\n        See also `Wait Until Page Does Not Contain`,\n        `Wait Until Page Contains Element`,\n        `Wait Until Page Does Not Contain Element` and\n        BuiltIn keyword `Wait Until Keyword Succeeds`.\n        \"\"\"\n", "input": "", "output": "        if not error:\n            error = \"Text '%s' did not appear in <TIMEOUT>\" % text\n        self._wait_until(timeout, error, self._is_text_present, text)", "category": "Python"}, {"instruction": "def calc_efficiency(self):\n        \"\"\"\n        Solar cell efficiency\n\n        The efficiency is calculated according to Shockley & Queisser's :cite:`10.1063/1.1736034` Eq. 2.8. This method returns a :class:`float`.\n        \"\"\"\n", "input": "", "output": "        cell_power = self.calc_power_density()\n        solar_power = self.calc_blackbody_radiant_power_density()\n        efficiency = cell_power/solar_power\n\n        return efficiency.decompose().value", "category": "Python"}, {"instruction": "def listdir_stat(dirname, show_hidden=True):\n    \"\"\"Returns a list of tuples for each file contained in the named\n       directory, or None if the directory does not exist. Each tuple\n       contains the filename, followed by the tuple returned by\n       calling os.stat on the filename.\n    \"\"\"\n", "input": "", "output": "    import os\n    try:\n        files = os.listdir(dirname)\n    except OSError:\n        return None\n    if dirname == '/':\n        return list((file, stat('/' + file)) for file in files if is_visible(file) or show_hidden)\n    return list((file, stat(dirname + '/' + file)) for file in files if is_visible(file) or show_hidden)", "category": "Python"}, {"instruction": "def verify(password, hash):\n    \"\"\"\n    Verify a password against a passed hash\n    \"\"\"\n", "input": "", "output": "    _, algorithm, cost, salt, password_hash = hash.split(\"$\")\n\n    password = pbkdf2.pbkdf2_hex(password, salt, int(cost) * 500)\n\n    return _safe_str_cmp(password, password_hash)", "category": "Python"}, {"instruction": "def get_vectors(self, indices=None):\n        '''Compute Ritz vectors.'''\n", "input": "", "output": "        H_ = self._deflated_solver.H\n        (n_, n) = H_.shape\n        coeffs = self.coeffs if indices is None else self.coeffs[:, indices]\n        return numpy.c_[self._deflated_solver.V[:, :n],\n                        self._deflated_solver.projection.U].dot(coeffs)", "category": "Python"}, {"instruction": "def _make_interpolation(self):\n        \"\"\"\n        creates an interpolation grid in H_0, omega_m and computes quantities in Dd and Ds_Dds\n        :return:\n        \"\"\"\n", "input": "", "output": "        H0_range = np.linspace(10, 100, 90)\n        omega_m_range = np.linspace(0.05, 1, 95)\n        grid2d = np.dstack(np.meshgrid(H0_range, omega_m_range)).reshape(-1, 2)\n        H0_grid = grid2d[:, 0]\n        omega_m_grid = grid2d[:, 1]\n        Dd_grid = np.zeros_like(H0_grid)\n        Ds_Dds_grid = np.zeros_like(H0_grid)\n        for i in range(len(H0_grid)):\n            Dd, Ds_Dds = self.cosmo2Dd_Ds_Dds(H0_grid[i], omega_m_grid[i])\n            Dd_grid[i] = Dd\n            Ds_Dds_grid[i] = Ds_Dds\n        self._f_H0 = interpolate.interp2d(Dd_grid, Ds_Dds_grid, H0_grid, kind='linear', copy=False, bounds_error=False, fill_value=-1)\n        print(\"H0 interpolation done\")\n        self._f_omega_m = interpolate.interp2d(Dd_grid, Ds_Dds_grid, omega_m_grid, kind='linear', copy=False, bounds_error=False, fill_value=-1)\n        print(\"omega_m interpolation done\")", "category": "Python"}, {"instruction": "def reindex(report):\n    \"\"\"Reindex report so that 'TOTAL' is the last row\"\"\"\n", "input": "", "output": "    index = list(report.index)\n    i = index.index('TOTAL')\n    return report.reindex(index[:i] + index[i+1:] + ['TOTAL'])", "category": "Python"}, {"instruction": "def _error_serializer(req, exc):  # pylint: disable=unused-argument\n        \"\"\" Serializer for native falcon HTTPError exceptions.\n\n        We override the default serializer with our own so we\n        can ensure the errors are serialized in a JSON API\n        compliant format.\n\n        Surprisingly, most falcon error attributes map directly\n        to the JSON API spec. The few that don't can be mapped\n        accordingly:\n\n\n            HTTPError               JSON API\n            ~~~~~~~~~               ~~~~~~~~\n\n            exc.description    ->   error['detail']\n            exc.link['href']   ->   error['links']['about']\n\n\n        Per the falcon docs this function should return a tuple\n        of (MIMETYPE, BODY PAYLOAD)\n        \"\"\"\n", "input": "", "output": "\n        error = {\n            'detail': exc.description,\n            'title': exc.title,\n            'status': exc.status,\n        }\n\n        try:\n            error['links'] = {'about': exc.link['href']}\n        except (TypeError, KeyError):\n            error['links'] = {'about': ''}\n\n        return (\n            goldman.config.JSONAPI_MIMETYPE,\n            json.dumps({'errors': [error]}),\n        )", "category": "Python"}, {"instruction": "def set_locale(request):\n    \"\"\"Return locale from GET lang param or automatically.\"\"\"\n", "input": "", "output": "    return request.query.get('lang', app.ps.babel.select_locale_by_request(request))", "category": "Python"}, {"instruction": "def disable_jt_ha(self, active_name):\n    \"\"\"\n    Disable high availability for a MR JobTracker active-standby pair.\n\n    @param active_name: name of the JobTracker that will be active after\n                        the disable operation. The other JobTracker and\n                        Failover Controllers will be removed.\n    @return: Reference to the submitted command.\n    \"\"\"\n", "input": "", "output": "    args = dict(\n      activeName = active_name,\n    )\n    return self._cmd('disableJtHa', data=args)", "category": "Python"}, {"instruction": "def Exponential(x, a, tau, y0):\n    \"\"\"Exponential function\n\n    Inputs:\n    -------\n        ``x``: independent variable\n        ``a``: scaling factor\n        ``tau``: time constant\n        ``y0``: additive constant\n\n    Formula:\n    --------\n        ``a*exp(x/tau)+y0``\n    \"\"\"\n", "input": "", "output": "    return np.exp(x / tau) * a + y0", "category": "Python"}, {"instruction": "def getLeader(self, vehID, dist=0.):\n        \"\"\"getLeader(string, double) -> (string, double)\n\n        Return the leading vehicle id together with the distance. The distance\n        is measured from the front + minGap to the back of the leader, so it does not include the\n        minGap of the vehicle.\n        The dist parameter defines the maximum lookahead, 0 calculates a lookahead from the brake gap.\n        Note that the returned leader may be farther away than the given dist.\n        \"\"\"\n", "input": "", "output": "        self._connection._beginMessage(\n            tc.CMD_GET_VEHICLE_VARIABLE, tc.VAR_LEADER, vehID, 1 + 8)\n        self._connection._string += struct.pack(\"!Bd\", tc.TYPE_DOUBLE, dist)\n        return _readLeader(self._connection._checkResult(tc.CMD_GET_VEHICLE_VARIABLE, tc.VAR_LEADER, vehID))", "category": "Python"}, {"instruction": "def modpath_pkg_resources(module, entry_point):\n    \"\"\"\n    Goes through pkg_resources for compliance with various PEPs.\n\n    This one accepts a module as argument.\n    \"\"\"\n", "input": "", "output": "\n    result = []\n    try:\n        path = resource_filename_mod_entry_point(module.__name__, entry_point)\n    except ImportError:\n        logger.warning(\"module '%s' could not be imported\", module.__name__)\n    except Exception:\n        logger.warning(\"%r does not appear to be a valid module\", module)\n    else:\n        if path:\n            result.append(path)\n    return result", "category": "Python"}, {"instruction": "def update_ip_rule(self, ip, mac):\n        \"\"\"Update a rule associated with given ip and mac.\"\"\"\n", "input": "", "output": "\n        rule_no = self._find_rule_no(mac)\n        chain = self._find_chain_name(mac)\n        if not rule_no or not chain:\n            LOG.error('Failed to update ip rule for %(ip)s %(mac)s',\n                      {'ip': ip, 'mac': mac})\n            return\n\n        update_cmd = ['iptables', '-R', '%s' % chain, '%s' % rule_no,\n                      '-s', '%s/32' % ip, '-m', 'mac', '--mac-source',\n                      '%s' % mac, '-j', 'RETURN']\n        LOG.debug('Execute command: %s', update_cmd)\n        dsl.execute(update_cmd, self._root_helper, log_output=False)", "category": "Python"}, {"instruction": "def _update_state_from_response(self, response_json):\r\n        \"\"\"\r\n        :param response_json: the json obj returned from query\r\n        :return:\r\n        \"\"\"\n", "input": "", "output": "        if 'data' in response_json and response_json['data']['object_type'] == \"cloud_clock\":\r\n            cloud_clock = response_json.get('data')\r\n            if cloud_clock is None:\r\n                return False\r\n\r\n            alarms = cloud_clock.get('alarms')\r\n            for alarm in alarms:\r\n                if alarm.get('object_id') == self.object_id():\r\n                    self.json_state = alarm\r\n                    return True\r\n            return False\r\n        if 'data' in response_json:\r\n            alarm = response_json.get('data')\r\n            self.json_state = alarm\r\n            return True\r\n        self.json_state = response_json\r\n        return True", "category": "Python"}, {"instruction": "def set_authors(data):\n    \"\"\"Add 'authors' attribute based on repo contributions\n    \"\"\"\n", "input": "", "output": "    if \"authors\" in data:\n        return\n\n    shfile = os.path.join(os.path.dirname(__file__), \"get_committers.sh\")\n\n    p = subprocess.Popen([\"bash\", shfile], stdout=subprocess.PIPE)\n    out, _ = p.communicate()\n    if p.returncode:\n        return\n\n    authors = out.strip().split('\\n')\n    authors = [x.strip() for x in authors]\n\n    data[\"authors\"] = authors", "category": "Python"}, {"instruction": "def process(self, tup):\n        \"\"\"Process steps:\n\n        1. Stream third positional value from input into Kafka topic.\n        \"\"\"\n", "input": "", "output": "        status_seq = self.iter_using_shelf(tup.values[2], self.tweet_shelf)\n        # This could be more efficient by passing the result from twitter\n        # straight through to the producer, instead of deserializing and\n        # reserializing json.\n        self.producer.produce(json.dumps(status) for status in status_seq)", "category": "Python"}, {"instruction": "def gauss_fltr_opencv(dem, size=3, sigma=1):\n    \"\"\"OpenCV Gaussian filter\n    Still propagates NaN values\n    \"\"\"\n", "input": "", "output": "    import cv2\n    dem = malib.checkma(dem)\n    dem_cv = cv2.GaussianBlur(dem.filled(np.nan), (size, size), sigma)\n    out = np.ma.fix_invalid(dem_cv)\n    out.set_fill_value(dem.fill_value)\n    return out", "category": "Python"}, {"instruction": "def _call_watcher(self_, watcher, event):\n        \"\"\"\n        Invoke the given the watcher appropriately given a Event object.\n        \"\"\"\n", "input": "", "output": "        if self_.self_or_cls.param._TRIGGER:\n            pass\n        elif watcher.onlychanged and (not self_._changed(event)):\n            return\n\n        if self_.self_or_cls.param._BATCH_WATCH:\n            self_._events.append(event)\n            if watcher not in self_._watchers:\n                self_._watchers.append(watcher)\n        elif watcher.mode == 'args':\n            with batch_watch(self_.self_or_cls, run=False):\n                watcher.fn(self_._update_event_type(watcher, event, self_.self_or_cls.param._TRIGGER))\n        else:\n            with batch_watch(self_.self_or_cls, run=False):\n                event = self_._update_event_type(watcher, event, self_.self_or_cls.param._TRIGGER)\n                watcher.fn(**{event.name: event.new})", "category": "Python"}, {"instruction": "def SetServerInformation(self, server, port):\n    \"\"\"Sets the server information.\n\n    Args:\n      server (str): hostname or IP address of the database server.\n      port (int): port number of the database server.\n    \"\"\"\n", "input": "", "output": "    self._host = server\n    self._port = port", "category": "Python"}, {"instruction": "def change_cash(self, money):\n        \"\"\"\n        \u5916\u90e8\u64cd\u4f5c|\u9ad8\u5371|\n        \"\"\"\n", "input": "", "output": "        res = self.cash[-1] + money\n        if res >= 0:\n            # \u9ad8\u5371\u64cd\u4f5c\n            self.cash[-1] = res", "category": "Python"}, {"instruction": "def get_option_set_by_id(cls, option_set_id, **kwargs):\n        \"\"\"Find OptionSet\n\n        Return single instance of OptionSet by its ID.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.get_option_set_by_id(option_set_id, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str option_set_id: ID of optionSet to return (required)\n        :return: OptionSet\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._get_option_set_by_id_with_http_info(option_set_id, **kwargs)\n        else:\n            (data) = cls._get_option_set_by_id_with_http_info(option_set_id, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Closes associated resources of this request object.  This\n        closes all file handles explicitly.  You can also use the request\n        object in a with statement with will automatically close it.\n\n        .. versionadded:: 0.9\n        \"\"\"\n", "input": "", "output": "        files = self.__dict__.get('files')\n        for key, value in iter_multi_items(files or ()):\n            value.close()", "category": "Python"}, {"instruction": "def is_subsumed_by(x, y):\n    \"\"\"\n    Returns true if y subsumes x (for example P(x) subsumes P(A) as it is more\n    abstract)\n    \"\"\"\n", "input": "", "output": "    varsX = __split_expression(x)[1]\n    theta = unify(x, y)\n    if theta is problem.FAILURE:\n        return False\n    return all(__is_variable(theta[var]) for var in theta.keys()\n               if var in varsX)", "category": "Python"}, {"instruction": "def get_include_fields(request):\n    \"\"\"Retrieve include_fields values from the request\n    \"\"\"\n", "input": "", "output": "    include_fields = []\n    rif = request.get(\"include_fields\", \"\")\n    if \"include_fields\" in request:\n        include_fields = [x.strip()\n                          for x in rif.split(\",\")\n                          if x.strip()]\n    if \"include_fields[]\" in request:\n        include_fields = request['include_fields[]']\n    return include_fields", "category": "Python"}, {"instruction": "def _load_edflib(filename):\n    \"\"\"load a multi-channel Timeseries from an EDF (European Data Format) file\n    or EDF+ file, using edflib.\n\n    Args:\n      filename: EDF+ file\n\n    Returns:\n      Timeseries\n    \"\"\"\n", "input": "", "output": "    import edflib\n    e = edflib.EdfReader(filename, annotations_mode='all')\n    if np.ptp(e.get_samples_per_signal()) != 0:\n        raise Error('channels have differing numbers of samples')\n    if np.ptp(e.get_signal_freqs()) != 0:\n        raise Error('channels have differing sample rates')\n    n = e.samples_in_file(0)\n    m = e.signals_in_file\n    channelnames = e.get_signal_text_labels()\n    dt = 1.0/e.samplefrequency(0)\n    # EDF files hold <=16 bits of information for each sample. Representing as\n    # double precision (64bit) is unnecessary use of memory. use 32 bit float:\n    ar = np.zeros((n, m), dtype=np.float32)\n    # edflib requires input buffer of float64s\n    buf = np.zeros((n,), dtype=np.float64)\n    for i in range(m):\n        e.read_phys_signal(i, 0, n, buf)\n        ar[:,i] = buf\n    tspan = np.arange(0, (n - 1 + 0.5) * dt, dt, dtype=np.float32)\n    return Timeseries(ar, tspan, labels=[None, channelnames])", "category": "Python"}, {"instruction": "def bearing_to(self, point):\n        '''\n        Return the bearing to another point.\n\n        :param point: Point to measure bearing to\n        :type point: Point\n\n        :returns: The bearing to the other point\n        :rtype: Bearing\n        '''\n", "input": "", "output": "        delta_long = point.long_radians - self.long_radians\n        y = sin(delta_long) * cos(point.lat_radians)\n        x = (\n            cos(self.lat_radians) * sin(point.lat_radians) -\n            sin(self.lat_radians) * cos(point.lat_radians) * cos(delta_long)\n        )\n        radians = math.atan2(y, x)\n        return Bearing.from_radians(radians)", "category": "Python"}, {"instruction": "def set_min_lease(self, min_lease):\n        \"\"\"\n        Set the minimum lease period in months.\n        :param min_lease: int\n        \"\"\"\n", "input": "", "output": "        self._query_params += str(QueryParam.MIN_LEASE) + str(min_lease)", "category": "Python"}, {"instruction": "def retract_vote(\n        self,\n        chat_id: Union[int, str],\n        message_id: id\n    ) -> bool:\n        \"\"\"Use this method to retract your vote in a poll.\n\n        Args:\n            chat_id (``int`` | ``str``):\n                Unique identifier (int) or username (str) of the target chat.\n                For your personal cloud (Saved Messages) you can simply use \"me\" or \"self\".\n                For a contact that exists in your Telegram address book you can use his phone number (str).\n\n            message_id (``int``):\n                Unique poll message identifier inside this chat.\n\n        Returns:\n            On success, True is returned.\n\n        Raises:\n            :class:`RPCError <pyrogram.RPCError>` in case of a Telegram RPC error.\n        \"\"\"\n", "input": "", "output": "        self.send(\n            functions.messages.SendVote(\n                peer=self.resolve_peer(chat_id),\n                msg_id=message_id,\n                options=[]\n            )\n        )\n\n        return True", "category": "Python"}, {"instruction": "def prebuild_arch(self, arch):\n        '''Run any pre-build tasks for the Recipe. By default, this checks if\n        any prebuild_archname methods exist for the archname of the current\n        architecture, and runs them if so.'''\n", "input": "", "output": "        prebuild = \"prebuild_{}\".format(arch.arch.replace('-', '_'))\n        if hasattr(self, prebuild):\n            getattr(self, prebuild)()\n        else:\n            info('{} has no {}, skipping'.format(self.name, prebuild))", "category": "Python"}, {"instruction": "def encrypt(self,\n                encryption_context_key: str,\n                plaintext: str) -> Tuple[bytes, Sequence[str]]:\n        \"\"\"\n        Encrypt a plaintext string value.\n\n        The return value will include *both* the resulting binary ciphertext and the\n        master key ids used for encryption. In the likely case that the encryptor was initialized\n        with master key aliases, these master key ids returned will represent the unaliased key.\n\n        \"\"\"\n", "input": "", "output": "        encryption_context = dict(\n            microcosm=encryption_context_key,\n        )\n\n        cyphertext, header = encrypt(\n            source=plaintext,\n            materials_manager=self.materials_manager,\n            encryption_context=encryption_context,\n        )\n\n        key_ids = [\n            self.unpack_key_id(encrypted_data_key.key_provider)\n            for encrypted_data_key in header.encrypted_data_keys\n        ]\n        return cyphertext, key_ids", "category": "Python"}, {"instruction": "async def send_from_directory(\n        directory: FilePath,\n        file_name: str,\n        *,\n        mimetype: Optional[str]=None,\n        as_attachment: bool=False,\n        attachment_filename: Optional[str]=None,\n        add_etags: bool=True,\n        cache_timeout: Optional[int]=None,\n        conditional: bool=True,\n        last_modified: Optional[datetime]=None,\n) -> Response:\n    \"\"\"Send a file from a given directory.\n\n    Arguments:\n       directory: Directory that when combined with file_name gives\n           the file path.\n       file_name: File name that when combined with directory gives\n           the file path.\n       See :func:`send_file` for the other arguments.\n    \"\"\"\n", "input": "", "output": "    file_path = safe_join(directory, file_name)\n    if not file_path.is_file():\n        raise NotFound()\n    return await send_file(\n        file_path,\n        mimetype=mimetype,\n        as_attachment=as_attachment,\n        attachment_filename=attachment_filename,\n        add_etags=add_etags,\n        cache_timeout=cache_timeout,\n        conditional=conditional,\n        last_modified=last_modified,\n    )", "category": "Python"}, {"instruction": "def use(self, func):\n        \"\"\"\n        Add function (or other callable) to the middleware stack.\n        Takes a single function as an argument and returns it,\n        so this can be used as a decorator.\n\n        func should take two arguments: files and stack\n\n        @stack.use\n        def count_files(files, stack):\n            stack.metadata['count'] = len(files)\n\n        \"\"\"\n", "input": "", "output": "        if not callable(func):\n            raise TypeError('Stack.use requires a callable')\n\n        self.middleware.append(func)\n        return func", "category": "Python"}, {"instruction": "def setCell(self, x, y, v):\n        \"\"\"set the cell value at x,y\"\"\"\n", "input": "", "output": "        self.cells[y][x] = v", "category": "Python"}, {"instruction": "def send_document(self, peer: Peer, document: str, reply: int=None, on_success: callable=None,\n                      reply_markup: botapi.ReplyMarkup=None):\n        \"\"\"\n        Send document to peer.\n        :param peer: Peer to send message to.\n        :param document: File path to document to send.\n        :param reply: Message object or message_id to reply to.\n        :param on_success: Callback to call when call is complete.\n\n        :type reply: int or Message\n        \"\"\"\n", "input": "", "output": "        if isinstance(reply, Message):\n            reply = reply.id\n\n        document = botapi.InputFile('document', botapi.InputFileInfo(document, open(document, 'rb'),\n                                                                     get_mimetype(document)))\n\n        botapi.send_document(chat_id=peer.id, document=document, reply_to_message_id=reply, on_success=on_success,\n                             reply_markup=reply_markup, **self.request_args).run()", "category": "Python"}, {"instruction": "def job_path(cls, project, jobs):\n        \"\"\"Return a fully-qualified job string.\"\"\"\n", "input": "", "output": "        return google.api_core.path_template.expand(\n            \"projects/{project}/jobs/{jobs}\", project=project, jobs=jobs\n        )", "category": "Python"}, {"instruction": "def _validate(self, data: Any, schema: AnyMapping) -> Any:\n        \"\"\"Validate data against given schema.\n\n        :param data: Data to validate.\n        :param schema: Schema to use for validation.\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.validate_func(schema, self._pure_data(data))\n        except self.validation_error_class as err:\n            logger.error(\n                'Schema validation error',\n                exc_info=True,\n                extra={'schema': schema, 'schema_module': self.module})\n            if self.error_class is None:\n                raise\n            raise self.make_error('Validation Error', error=err) from err", "category": "Python"}, {"instruction": "def get_bytesize(self, key, default=None, minimum=None, maximum=None, default_unit=None, base=DEFAULT_BASE):\n        \"\"\"Size in bytes expressed by value configured under 'key'\n\n        Args:\n            key (str | unicode): Key to lookup\n            default (int | str | unicode | None): Default to use if key is not configured\n            minimum (int | str | unicode | None): If specified, result can't be below this minimum\n            maximum (int | str | unicode | None): If specified, result can't be above this maximum\n            default_unit (str | unicode | None): Default unit for unqualified values (see UNITS)\n            base (int): Base to use (usually 1024)\n\n        Returns:\n            (int): Size in bytes\n        \"\"\"\n", "input": "", "output": "        value = to_bytesize(self.get_str(key), default_unit, base)\n        if value is None:\n            return to_bytesize(default, default_unit, base)\n        return capped(value, to_bytesize(minimum, default_unit, base), to_bytesize(maximum, default_unit, base))", "category": "Python"}, {"instruction": "def add_to_matching_blacklist(session, term):\n    \"\"\"Add term to the matching blacklist.\n\n    This function adds a `term` to the matching blacklist.\n    The term to add cannot have a `None` or empty value,\n    on this case an `ValueError` will be raised.\n\n    :param session: database session\n    :param term: term, word or value to blacklist\n\n    :return: a new entry in the blacklist\n\n    :raises ValueError: raised when `term` is `None` or an empty string\n    \"\"\"\n", "input": "", "output": "    if term is None:\n        raise ValueError(\"'term' to blacklist cannot be None\")\n    if term == '':\n        raise ValueError(\"'term' to blacklist cannot be an empty string\")\n\n    mb = MatchingBlacklist(excluded=term)\n    session.add(mb)\n\n    return mb", "category": "Python"}, {"instruction": "def host_names(urls):\n    '''\n    Takes a StringCounter of normalized URL and parses their hostnames\n\n    N.B. this assumes that absolute URLs will begin with\n\n    http://\n\n    in order to accurately resolve the host name.\n    Relative URLs will not have host names.\n    '''\n", "input": "", "output": "    host_names = StringCounter()\n    for url in urls:\n        host_names[urlparse(url).netloc] += urls[url]\n    return host_names", "category": "Python"}, {"instruction": "def sends(self):\n        \"\"\"\n        Returns a table of mbox message data, with \"sender/recipient\" pairs as rows/observations.\n\n        .. note::\n\n            Rows may have a recipient from either \"TO\" or \"CC\". SendType column specifies this for each row.\n\n        .. note::\n\n            drop_collections is not available for this method, since there are no meaningful collections to keep.\n\n        :return: pandas.DataFrame\n        \"\"\"\n", "input": "", "output": "        # Expand on each \"to\" field\n        on_to_df = self.expand_on('From', 'To', rename1='From', rename2='Recipient')\n        on_cc_df = self.expand_on('From', 'Cc', rename1='From', rename2='Recipient')\n\n        # Specify how it was sent\n        on_to_df['SendType'] = 'To'\n        on_cc_df['SendType'] = 'Cc'\n\n        # Combine dataframes\n        output_df = pd.concat([on_to_df, on_cc_df])\n\n        return self._drop_collections(output_df)", "category": "Python"}, {"instruction": "def json_is_exception(resp):\n    \"\"\"\n    Is the given response object\n    an exception traceback?\n\n    Return True if so\n    Return False if not\n    \"\"\"\n", "input": "", "output": "    if not json_is_error(resp):\n        return False\n\n    if 'traceback' not in resp.keys() or 'error' not in resp.keys():\n        return False\n\n    return True", "category": "Python"}, {"instruction": "def create_backend_noexc(log: logging.Logger, name: str=None, git_index: GitIndex=None,\n                         args: str=None) -> Optional[StorageBackend]:\n    \"\"\"Initialize a new Backend, return None if there was a known problem.\"\"\"\n", "input": "", "output": "    try:\n        return create_backend(name, git_index, args)\n    except KeyError:\n        log.critical(\"No such backend: %s (looked in %s)\",\n                     name, list(__registry__.keys()))\n        return None\n    except ValueError:\n        log.critical(\"Invalid backend arguments: %s\", args)\n        return None", "category": "Python"}, {"instruction": "def Open(self):\n    \"\"\"Opens the storage writer.\n\n    Raises:\n      IOError: if the storage writer is already opened.\n      OSError: if the storage writer is already opened.\n    \"\"\"\n", "input": "", "output": "    if self._storage_file:\n      raise IOError('Storage writer already opened.')\n\n    self._storage_file = self._CreateStorageFile()\n\n    if self._serializers_profiler:\n      self._storage_file.SetSerializersProfiler(self._serializers_profiler)\n\n    if self._storage_profiler:\n      self._storage_file.SetStorageProfiler(self._storage_profiler)\n\n    self._storage_file.Open(path=self._output_file, read_only=False)\n\n    self._first_written_event_source_index = (\n        self._storage_file.GetNumberOfEventSources())\n    self._written_event_source_index = self._first_written_event_source_index", "category": "Python"}, {"instruction": "def remove_key(self, store_key):\n        \"\"\"Remove key in the context of the current transaction.\n\n        :param store_key: The key for the document in the store\n        :type store_key: str\n\n        \"\"\"\n", "input": "", "output": "        self._remove_cache[store_key] = True\n        if store_key in self._add_cache:\n            for hash_value in self._add_cache[store_key]:\n                self._reverse_add_cache[hash_value].remove(store_key)\n            del self._add_cache[store_key]\n        if store_key in self._undefined_cache:\n            del self._undefined_cache[store_key]", "category": "Python"}, {"instruction": "def mergeBboxes(bbox1, bbox2):\n    \"\"\"\n    :param bbox1: (top, left, bottom, right)\n    :param bbox2: (top, left, bottom, right)\n    :return: Merge bounding boxes\n    \"\"\"\n", "input": "", "output": "    if isContained(bbox1, bbox2):\n        return bbox2\n    elif isContained(bbox2, bbox1):\n        return bbox1\n    else:\n        return (\n            min(bbox1[0], bbox2[0]),\n            min(bbox1[1], bbox2[1]),\n            max(bbox1[2], bbox2[2]),\n            max(bbox1[3], bbox2[3]),\n        )", "category": "Python"}, {"instruction": "def provision(self,\n                  bug: Bug,\n                  *,\n                  plugins: Optional[List[Tool]] = None\n                  ) -> Container:\n        \"\"\"\n        Provisions a container for a given bug.\n        \"\"\"\n", "input": "", "output": "        if plugins is None:\n            plugins = []\n\n        logger.info(\"provisioning container for bug: %s\", bug.name)\n        endpoint = 'bugs/{}/provision'.format(bug.name)\n        payload = {\n            'plugins': [p.to_dict() for p in plugins]\n        }  # type: Dict[str, Any]\n        r = self.__api.post(endpoint, json=payload)\n\n        if r.status_code == 200:\n            container = Container.from_dict(r.json())\n            logger.info(\"provisioned container (id: %s) for bug: %s\",\n                               container.uid,\n                               bug.name)\n            return container\n\n        if r.status_code == 404:\n            raise KeyError(\"no bug registered with given name: {}\".format(bug.name))\n\n        self.__api.handle_erroneous_response(r)", "category": "Python"}, {"instruction": "def find_files(folder):\n    \"\"\"Discover stereo photos and return them as a pairwise sorted list.\"\"\"\n", "input": "", "output": "    files = [i for i in os.listdir(folder) if i.startswith(\"left\")]\n    files.sort()\n    for i in range(len(files)):\n        insert_string = \"right{}\".format(files[i * 2][4:])\n        files.insert(i * 2 + 1, insert_string)\n    files = [os.path.join(folder, filename) for filename in files]\n    return files", "category": "Python"}, {"instruction": "def fft_spectrum(frames, fft_points=512):\n    \"\"\"This function computes the one-dimensional n-point discrete Fourier\n    Transform (DFT) of a real-valued array by means of an efficient algorithm\n    called the Fast Fourier Transform (FFT). Please refer to\n    https://docs.scipy.org/doc/numpy/reference/generated/numpy.fft.rfft.html\n    for further details.\n\n    Args:\n        frames (array): The frame array in which each row is a frame.\n        fft_points (int): The length of FFT. If fft_length is greater than frame_len, the frames will be zero-padded.\n\n    Returns:\n            array: The fft spectrum.\n            If frames is an num_frames x sample_per_frame matrix, output\n            will be num_frames x FFT_LENGTH.\n    \"\"\"\n", "input": "", "output": "    SPECTRUM_VECTOR = np.fft.rfft(frames, n=fft_points, axis=-1, norm=None)\n    return np.absolute(SPECTRUM_VECTOR)", "category": "Python"}, {"instruction": "def read_handle(self, handle: int) -> bytes:\n        \"\"\"Read a handle from the device.\"\"\"\n", "input": "", "output": "        if not self.is_connected():\n            raise BluetoothBackendException('Not connected to device!')\n        return self._device.char_read_handle(handle)", "category": "Python"}, {"instruction": "def remove(self, node):\n        \"\"\"Remove a node from the list.\"\"\"\n", "input": "", "output": "        if not isinstance(node, Node):\n            raise TypeError('expecting Node instance')\n        if node._list is None:\n            return\n        if node._list is not self:\n            raise RuntimeError('node is not contained in list')\n        if node._next is None:\n            self._last = node._prev  # last node\n        else:\n            node._next._prev = node._prev\n        if node._prev is None:\n            self._first = node._next  # first node\n        else:\n            node._prev._next = node._next\n        node._list = node._prev = node._next = None\n        self._size -= 1", "category": "Python"}, {"instruction": "def items(self):\n        \"\"\" Return all merged items as iterator \"\"\"\n", "input": "", "output": "        if not self.pdata and not self.spills:\n            return iter(self.data.items())\n        return self._external_items()", "category": "Python"}, {"instruction": "def accuracy_score(y, y_pred):\n    \"\"\"Calculates the fraction of the correctly\n    classified samples over all.\n    \n    Parameters:\n    -----------\n    y : vector, shape (n_samples,)\n    The target labels.\n\n    y_pred : vector, shape (n_samples,)\n    The predicted labels.\n    \n    Returns:\n    --------\n    accuracy : float number, the fraction\n    of the correctly classified samples over all\n    \n    \"\"\"\n", "input": "", "output": "\n    y, y_pred = convert_assert(y, y_pred)\n    return np.count_nonzero(y == y_pred) / y.size", "category": "Python"}, {"instruction": "def estimate_column_scales(\n            self,\n            X_centered,\n            row_scales):\n        \"\"\"\n        column_scale[j] ** 2 =\n          mean{i in observed[:, j]}{\n            (X[i, j] - row_center[i] - column_center[j]) ** 2\n            -------------------------------------------------\n                        row_scale[i] ** 2\n        }\n        \"\"\"\n", "input": "", "output": "        n_rows, n_cols = X_centered.shape\n        row_scales = np.asarray(row_scales)\n\n        if len(row_scales) != n_rows:\n            raise ValueError(\"Expected length %s, got shape %s\" % (\n                n_rows, row_scales.shape,))\n\n        column_variances = np.nanmean(\n            X_centered ** 2 / (row_scales ** 2).reshape((n_rows, 1)),\n            axis=0)\n        column_variances[column_variances == 0] = 1.0\n        assert len(column_variances) == n_cols, \"%d != %d\" % (\n            len(column_variances),\n            n_cols)\n        return np.sqrt(column_variances)", "category": "Python"}, {"instruction": "def before_render(self):\n        \"\"\"Before template render hook\n        \"\"\"\n", "input": "", "output": "        super(PricelistsView, self).before_render()\n        # Render the Add button if the user has the AddPricelist permission\n        if check_permission(AddPricelist, self.context):\n            self.context_actions[_(\"Add\")] = {\n                \"url\": \"createObject?type_name=Pricelist\",\n                \"icon\": \"++resource++bika.lims.images/add.png\"\n            }\n        # Don't allow any context actions on the Methods folder\n        self.request.set(\"disable_border\", 1)", "category": "Python"}, {"instruction": "def detect_rowspans(self, use_actual=1):\n        \"\"\"Determine if any row spans are present in the values.\n        :param use_actual:  if True, check actual_values for span. if False, use the formatted_values\n        :return: self\n        \"\"\"\n", "input": "", "output": "        ", "category": "Python"}, {"instruction": "def owned_pre_save(sender, document, **kwargs):\n    '''\n    Owned mongoengine.pre_save signal handler\n    Need to fetch original owner before the new one erase it.\n    '''\n", "input": "", "output": "    if not isinstance(document, Owned):\n        return\n    changed_fields = getattr(document, '_changed_fields', [])\n    if 'organization' in changed_fields:\n        if document.owner:\n            # Change from owner to organization\n            document._previous_owner = document.owner\n            document.owner = None\n        else:\n            # Change from org to another\n            # Need to fetch previous value in base\n            original = sender.objects.only('organization').get(pk=document.pk)\n            document._previous_owner = original.organization\n    elif 'owner' in changed_fields:\n        if document.organization:\n            # Change from organization to owner\n            document._previous_owner = document.organization\n            document.organization = None\n        else:\n            # Change from owner to another\n            # Need to fetch previous value in base\n            original = sender.objects.only('owner').get(pk=document.pk)\n            document._previous_owner = original.owner", "category": "Python"}, {"instruction": "def default_target(self, value):\n        \"\"\"\n        Setter for **self.__default_target** attribute.\n\n        :param value: Attribute value.\n        :type value: unicode\n        \"\"\"\n", "input": "", "output": "\n        if value is not None:\n            assert type(value) is unicode, \"'{0}' attribute: '{1}' type is not 'unicode'!\".format(\n                \"default_target\", value)\n            assert os.path.exists(value), \"'{0}' attribute: '{1}' file doesn't exists!\".format(\"default_target\", value)\n        self.__default_target = value", "category": "Python"}, {"instruction": "def catalog_split_yaml(self, **kwargs):\n        \"\"\" return the name of a catalog split yaml file\n        \"\"\"\n", "input": "", "output": "        kwargs_copy = self.base_dict.copy()\n        kwargs_copy.update(**kwargs)\n        self._replace_none(kwargs_copy)        \n        localpath = NameFactory.catalog_split_yaml_format.format(**kwargs_copy)\n        if kwargs.get('fullpath', False):\n            return self.fullpath(localpath=localpath)\n        return localpath", "category": "Python"}, {"instruction": "def drop_columns(cr, column_spec):\n    \"\"\"\n    Drop columns but perform an additional check if a column exists.\n    This covers the case of function fields that may or may not be stored.\n    Consider that this may not be obvious: an additional module can govern\n    a function fields' store properties.\n\n    :param column_spec: a list of (table, column) tuples\n    \"\"\"\n", "input": "", "output": "    for (table, column) in column_spec:\n        logger.info(\"table %s: drop column %s\",\n                    table, column)\n        if column_exists(cr, table, column):\n            cr.execute('ALTER TABLE \"%s\" DROP COLUMN \"%s\"' %\n                       (table, column))\n        else:\n            logger.warn(\"table %s: column %s did not exist\",\n                        table, column)", "category": "Python"}, {"instruction": "def trade_day(dt, cal='US'):\n    \"\"\"\n    Latest trading day w.r.t given dt\n\n    Args:\n        dt: date of reference\n        cal: trading calendar\n\n    Returns:\n        pd.Timestamp: last trading day\n\n    Examples:\n        >>> trade_day('2018-12-25').strftime('%Y-%m-%d')\n        '2018-12-24'\n    \"\"\"\n", "input": "", "output": "    from xone import calendar\n\n    dt = pd.Timestamp(dt).date()\n    return calendar.trading_dates(start=dt - pd.Timedelta('10D'), end=dt, calendar=cal)[-1]", "category": "Python"}, {"instruction": "def delete(method, hmc, uri, uri_parms, logon_required):\n        \"\"\"Operation: Delete <resource>.\"\"\"\n", "input": "", "output": "        try:\n            resource = hmc.lookup_by_uri(uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        resource.manager.remove(resource.oid)", "category": "Python"}, {"instruction": "async def serviceNodeMsgs(self, limit: int) -> int:\n        \"\"\"\n        Process `limit` number of messages from the nodeInBox.\n\n        :param limit: the maximum number of messages to process\n        :return: the number of messages successfully processed\n        \"\"\"\n", "input": "", "output": "        with self.metrics.measure_time(MetricsName.SERVICE_NODE_STACK_TIME):\n            n = await self.nodestack.service(limit, self.quota_control.node_quota)\n\n        self.metrics.add_event(MetricsName.NODE_STACK_MESSAGES_PROCESSED, n)\n\n        await self.processNodeInBox()\n        return n", "category": "Python"}, {"instruction": "def _get_datapath(self):\n        \"\"\" Get a valid datapath, else raise an exception.\n        \"\"\"\n", "input": "", "output": "        if self._datapath is None:\n            raise OSError(errno.ENOENT, \"You didn't provide any datapath for %r\" % self.filename)\n\n        return self._datapath", "category": "Python"}, {"instruction": "def detectFirefoxOSTablet(self):\n        \"\"\"Return detection of a Firefox OS tablet\n\n        Detects a tablet (probably) running the Firefox OS.\n        \"\"\"\n", "input": "", "output": "        if self.detectIos() \\\n            or self.detectAndroid() \\\n            or self.detectSailfish():\n            return False\n\n        if UAgentInfo.engineFirefox in self.__userAgent \\\n           and UAgentInfo.deviceTablet in self.__userAgent:\n            return True\n\n        return False", "category": "Python"}, {"instruction": "def start(name, quiet=False, path=None):\n    '''\n    Start the named container.\n\n    path\n        path to the container parent\n        default: /var/lib/lxc (system default)\n\n        .. versionadded:: 2015.8.0\n\n    .. code-block:: bash\n\n        salt-run lxc.start name\n    '''\n", "input": "", "output": "    data = _do_names(name, 'start', path=path)\n    if data and not quiet:\n        __jid_event__.fire_event(\n            {'data': data, 'outputter': 'lxc_start'}, 'progress')\n    return data", "category": "Python"}, {"instruction": "def plain(self):\n        \"\"\"\n        Get a string representation of this XML fragment.\n\n        @return: A I{plain} string.\n        @rtype: basestring\n\n        \"\"\"\n", "input": "", "output": "        result = [\"<%s\" % (self.qname(),), self.nsdeclarations()]\n        for a in self.attributes:\n            result.append(\" %s\" % (unicode(a),))\n        if self.isempty():\n            result.append(\"/>\")\n            return \"\".join(result)\n        result.append(\">\")\n        if self.hasText():\n            result.append(self.text.escape())\n        for c in self.children:\n            result.append(c.plain())\n        result.append(\"</%s>\" % (self.qname(),))\n        return \"\".join(result)", "category": "Python"}, {"instruction": "def _filter_repeating_items(download_list):\n        \"\"\" Because of data_filter some requests in download list might be the same. In order not to download them again\n        this method will reduce the list of requests. It will also return a mapping list which can be used to\n        reconstruct the previous list of download requests.\n\n        :param download_list: List of download requests\n        :type download_list: list(sentinelhub.DownloadRequest)\n        :return: reduced download list with unique requests and mapping list\n        :rtype: (list(sentinelhub.DownloadRequest), list(int))\n        \"\"\"\n", "input": "", "output": "        unique_requests_map = {}\n        mapping_list = []\n        unique_download_list = []\n        for download_request in download_list:\n            if download_request not in unique_requests_map:\n                unique_requests_map[download_request] = len(unique_download_list)\n                unique_download_list.append(download_request)\n            mapping_list.append(unique_requests_map[download_request])\n        return unique_download_list, mapping_list", "category": "Python"}, {"instruction": "def classes_in_module(module) -> List:\n        \"\"\"\n        Return all classes with super class ExtractionModule\n\n        Args:\n            module:\n\n        Returns: List of classes\n\n        \"\"\"\n", "input": "", "output": "        md = module.__dict__\n        return [\n            md[c] for c in md if (\n                    isinstance(md[c], type) and\n                    issubclass(md[c], ETKModule\n                               ) and\n                    md[c].__module__ == module.__name__)\n        ]", "category": "Python"}, {"instruction": "def load(self, name, data, context=None):\n        \"\"\"Deserialize data from primitive types. Raises\n        :exc:`~lollipop.errors.ValidationError` if data is invalid.\n\n        :param str name: Name of attribute to deserialize.\n        :param data: Raw data to get value to deserialize from.\n        :param kwargs: Same keyword arguments as for :meth:`Type.load`.\n        :returns: Loaded data.\n        :raises: :exc:`~lollipop.errors.ValidationError`\n        \"\"\"\n", "input": "", "output": "        return self.field_type.load(data.get(name, MISSING), context=context)", "category": "Python"}, {"instruction": "def invert(self, points):\n        \"\"\"Invert the distortion\n\n        Parameters\n        ------------------\n        points : ndarray\n            Input image points\n\n        Returns\n        -----------------\n        ndarray\n            Undistorted points\n        \"\"\"\n", "input": "", "output": "        X = points if not points.ndim == 1 else points.reshape((points.size, 1))\n\n        wx, wy = self.wc\n\n        # Switch to polar coordinates\n        rn = np.sqrt((X[0,:] - wx)**2 + (X[1,:] - wy)**2)\n        phi = np.arctan2(X[1,:] - wy, X[0,:]-wx)\n        # 'atan' method\n        r = np.tan(rn * self.lgamma) / self.lgamma;\n\n        # Switch back to rectangular coordinates\n        Y = np.ones(X.shape)\n        Y[0,:] = wx + r * np.cos(phi)\n        Y[1,:]= wy + r * np.sin(phi)\n        return Y", "category": "Python"}, {"instruction": "def marks(value):\n        \"\"\"list or KeyedList of ``Mark`` : Mark definitions\n\n        Marks are the visual objects (such as lines, bars, etc.) that\n        represent the data in the visualization space. See the :class:`Mark`\n        class for details.\n        \"\"\"\n", "input": "", "output": "        for i, entry in enumerate(value):\n            _assert_is_type('marks[{0}]'.format(i), entry, Mark)", "category": "Python"}, {"instruction": "def _to_dict(self):\n        \"\"\"Return a json dictionary representing this model.\"\"\"\n", "input": "", "output": "        _dict = {}\n        if hasattr(self, 'text') and self.text is not None:\n            _dict['text'] = self.text\n        if hasattr(self, 'tokens') and self.tokens is not None:\n            _dict['tokens'] = self.tokens\n        if hasattr(self, 'readings') and self.readings is not None:\n            _dict['readings'] = self.readings\n        if hasattr(self, 'part_of_speech') and self.part_of_speech is not None:\n            _dict['part_of_speech'] = self.part_of_speech\n        return _dict", "category": "Python"}, {"instruction": "def new(self, file_type):\n        # type: (str) -> None\n        '''\n        A method to create a new UDF ICB Tag.\n\n        Parameters:\n         file_type - What file type this represents, one of 'dir', 'file', or 'symlink'.\n        Returns:\n         Nothing.\n        '''\n", "input": "", "output": "        if self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF ICB Tag already initialized')\n\n        self.prior_num_direct_entries = 0  # FIXME: let the user set this\n        self.strategy_type = 4\n        self.strategy_param = 0  # FIXME: let the user set this\n        self.max_num_entries = 1\n        if file_type == 'dir':\n            self.file_type = 4\n        elif file_type == 'file':\n            self.file_type = 5\n        elif file_type == 'symlink':\n            self.file_type = 12\n        else:\n            raise pycdlibexception.PyCdlibInternalError(\"Invalid file type for ICB; must be one of 'dir', 'file', or 'symlink'\")\n\n        self.parent_icb_log_block_num = 0  # FIXME: let the user set this\n        self.parent_icb_part_ref_num = 0  # FIXME: let the user set this\n        self.flags = 560  # hex 0x230 == binary 0010 0011 0000\n\n        self._initialized = True", "category": "Python"}, {"instruction": "def run(command, raw_output=False):\n    \"\"\"Run a command using subprocess.\n\n    :param command: command line to be run\n    :type command: str\n    :param raw_output: does not attempt to convert the output as unicode\n    :type raw_output: bool\n    :return: error code, output (``stdout``) and error (``stderr``)\n    :rtype: tuple\n\n    \"\"\"\n", "input": "", "output": "    p = Popen(command.split(), stdout=PIPE, stderr=PIPE)\n    (stdout, stderr) = p.communicate()\n    # On python 3, subprocess.Popen returns bytes objects.\n    if not raw_output:\n        return (\n            p.returncode,\n            [line.rstrip() for line in stdout.decode(\"utf-8\").splitlines()],\n            [line.rstrip() for line in stderr.decode(\"utf-8\").splitlines()]\n        )\n    else:\n        return (p.returncode, stdout, stderr)", "category": "Python"}, {"instruction": "def set_value(self, comp_str):\n        \"\"\"\n        Set the value of component.\n\n        :param string comp_str: value of component\n        :returns: None\n        :exception: ValueError - incorrect value of component\n        \"\"\"\n", "input": "", "output": "\n        self._is_negated = False\n        self._encoded_value = comp_str\n        self._standard_value = super(\n            CPEComponent2_3_URI_edpacked, self)._decode()", "category": "Python"}, {"instruction": "def worker(self):\r\n        \"\"\"\r\n        Returns the worker object for loading records for this record box.\r\n        \r\n        :return     <XOrbLookupWorker>\r\n        \"\"\"\n", "input": "", "output": "        if self._worker is None:\r\n            self._worker = XOrbLookupWorker(self.isThreadEnabled())\r\n            self._worker.setBatchSize(self._batchSize)\r\n            self._worker.setBatched(not self.isThreadEnabled())\r\n            \r\n            # connect the worker\r\n            self.loadRequested.connect(self._worker.loadRecords)\r\n            self._worker.loadingStarted.connect(self.markLoadingStarted)\r\n            self._worker.loadingFinished.connect(self.markLoadingFinished)\r\n            self._worker.loadedRecords.connect(self.addRecordsFromThread)\r\n        \r\n        return self._worker", "category": "Python"}, {"instruction": "def get_env(key: str,\n            default: Any = None,\n            clean: Callable[[str], Any] = lambda v: v):\n    '''\n    Retrieves a configuration value from the environment variables.\n    The given *key* is uppercased and prefixed by ``\"BACKEND_\"`` and then\n    ``\"SORNA_\"`` if the former does not exist.\n\n    :param key: The key name.\n    :param default: The default value returned when there is no corresponding\n        environment variable.\n    :param clean: A single-argument function that is applied to the result of lookup\n        (in both successes and the default value for failures).\n        The default is returning the value as-is.\n\n    :returns: The value processed by the *clean* function.\n    '''\n", "input": "", "output": "    key = key.upper()\n    v = os.environ.get('BACKEND_' + key)\n    if v is None:\n        v = os.environ.get('SORNA_' + key)\n    if v is None:\n        if default is None:\n            raise KeyError(key)\n        v = default\n    return clean(v)", "category": "Python"}, {"instruction": "def _extract_comments(self):\n        \"\"\"Retrieve all comments from the file\"\"\"\n", "input": "", "output": "        self._det_file.seek(0, 0)\n        for line in self._det_file.readlines():\n            line = line.strip()\n            if line.startswith('#'):\n                self.add_comment(line[1:])", "category": "Python"}, {"instruction": "def isDescendantOf(self, other):\n    '''Returns whether this Key is a descendant of `other`.\n\n        >>> Key('/Comedy/MontyPython').isDescendantOf(Key('/Comedy'))\n        True\n\n    '''\n", "input": "", "output": "    if isinstance(other, Key):\n      return other.isAncestorOf(self)\n    raise TypeError('%s is not of type %s' % (other, Key))", "category": "Python"}, {"instruction": "def subtask(*args, **kwargs):\n    '''Decorator which prints out the name of the decorated function on\n    execution.\n    '''\n", "input": "", "output": "    depth = kwargs.get('depth', 2)\n    prefix = kwargs.get('prefix', '\\n' + '#' * depth + ' ')\n    tail = kwargs.get('tail', '\\n')\n    doc1 = kwargs.get('doc1', False)\n    color = kwargs.get('color', cyan)\n\n    def real_decorator(func):\n        if doc1:\n            return print_full_name(color=color, prefix=prefix,\n                                   tail=tail)(print_doc1(func))\n        return print_full_name(color=color, prefix=prefix, tail=tail)(func)\n\n    invoked = bool(not args or kwargs)\n    if not invoked:\n        # invoke decorator function which returns the wrapper function\n        return real_decorator(func=args[0])\n    return real_decorator", "category": "Python"}, {"instruction": "def callback_parent(attr, old, new):\n    '''Update data directories drop down with new parent directory'''\n", "input": "", "output": "    import os\n\n    # Remove accidental white space if copy/pasted\n    new = new.strip()\n    parent_input.value = new\n\n    # Verify new parent path exists and update `datadirs_select` widget\n    if os.path.exists(new):\n        # Create sorted list of data directories, ignore files\n        joinisdir = lambda parent, d: os.path.isdir(os.path.join(parent, d))\n        options = sorted([d for d in os.listdir(new) if joinisdir(new, d)])\n\n        # Update dropdown list of available data directories and select first\n        datadirs_select.options = options\n        datadirs_select.value = options[0]\n        callback_datadirs('value', options[0], options[0])\n\n    else:\n        msg = ", "category": "Python"}, {"instruction": "def make_signed_token(self, key):\n        \"\"\"Signs the payload.\n\n        Creates a JWS token with the header as the JWS protected header and\n        the claims as the payload. See (:class:`jwcrypto.jws.JWS`) for\n        details on the exceptions that may be reaised.\n\n        :param key: A (:class:`jwcrypto.jwk.JWK`) key.\n        \"\"\"\n", "input": "", "output": "\n        t = JWS(self.claims)\n        t.add_signature(key, protected=self.header)\n        self.token = t", "category": "Python"}, {"instruction": "def reset(self):\n        '''\n        Resets this agent type to prepare it for a new simulation run.  This\n        includes resetting the random number generator and initializing the style\n        of each agent of this type.\n        '''\n", "input": "", "output": "        self.resetRNG()\n        sNow = np.zeros(self.pop_size)\n        Shk  = self.RNG.rand(self.pop_size)\n        sNow[Shk < self.p_init] = 1\n        self.sNow = sNow", "category": "Python"}, {"instruction": "def get_span_datas(self, span):\n        \"\"\"Extracts a list of SpanData tuples from a span\n\n        :rtype: list of opencensus.trace.span_data.SpanData\n        :return list of SpanData tuples\n        \"\"\"\n", "input": "", "output": "        span_datas = [\n            span_data_module.SpanData(\n                name=ss.name,\n                context=self.span_context,\n                span_id=ss.span_id,\n                parent_span_id=ss.parent_span.span_id if\n                ss.parent_span else None,\n                attributes=ss.attributes,\n                start_time=ss.start_time,\n                end_time=ss.end_time,\n                child_span_count=len(ss.children),\n                stack_trace=ss.stack_trace,\n                time_events=ss.time_events,\n                links=ss.links,\n                status=ss.status,\n                same_process_as_parent_span=ss.same_process_as_parent_span,\n                span_kind=ss.span_kind\n            )\n            for ss in span\n        ]\n\n        return span_datas", "category": "Python"}, {"instruction": "def stn(s, length, encoding, errors):\n    \"\"\"Convert a string to a null-terminated bytes object.\n    \"\"\"\n", "input": "", "output": "    s = s.encode(encoding, errors)\n    return s[:length] + (length - len(s)) * NUL", "category": "Python"}, {"instruction": "def lsattr(path):\n    '''\n    .. versionadded:: 2018.3.0\n    .. versionchanged:: 2018.3.1\n        If ``lsattr`` is not installed on the system, ``None`` is returned.\n    .. versionchanged:: 2018.3.4\n        If on ``AIX``, ``None`` is returned even if in filesystem as lsattr on ``AIX``\n        is not the same thing as the linux version.\n\n    Obtain the modifiable attributes of the given file. If path\n    is to a directory, an empty list is returned.\n\n    path\n        path to file to obtain attributes of. File/directory must exist.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' file.lsattr foo1.txt\n    '''\n", "input": "", "output": "    if not salt.utils.path.which('lsattr') or salt.utils.platform.is_aix():\n        return None\n\n    if not os.path.exists(path):\n        raise SaltInvocationError(\"File or directory does not exist: \" + path)\n\n    cmd = ['lsattr', path]\n    result = __salt__['cmd.run'](cmd, ignore_retcode=True, python_shell=False)\n\n    results = {}\n    for line in result.splitlines():\n        if not line.startswith('lsattr: '):\n            vals = line.split(None, 1)\n            results[vals[1]] = re.findall(r\"[aAcCdDeijPsStTu]\", vals[0])\n\n    return results", "category": "Python"}, {"instruction": "def merge_tasks(runset_results):\n    \"\"\"\n    This function merges the results of all RunSetResult objects.\n    If necessary, it can merge lists of names: [A,C] + [A,B] --> [A,B,C]\n    and add dummy elements to the results.\n    It also ensures the same order of tasks.\n    \"\"\"\n", "input": "", "output": "    task_list = []\n    task_set = set()\n    for runset in runset_results:\n        index = -1\n        currentresult_taskset = set()\n        for task in runset.get_tasks():\n            if task in currentresult_taskset:\n                logging.warning(\"Task '%s' is present twice, skipping it.\", task[0])\n            else:\n                currentresult_taskset.add(task)\n                if task not in task_set:\n                    task_list.insert(index+1, task)\n                    task_set.add(task)\n                    index += 1\n                else:\n                    index = task_list.index(task)\n\n    merge_task_lists(runset_results, task_list)", "category": "Python"}, {"instruction": "def save(self, destination: BinaryIO, buffer_size: int=16384) -> None:\n        \"\"\"Save the file to the destination.\n\n        Arguments:\n            destination: A filename (str) or file object to write to.\n            buffer_size: Buffer size as used as length in\n                :func:`shutil.copyfileobj`.\n        \"\"\"\n", "input": "", "output": "        close_destination = False\n        if isinstance(destination, str):\n            destination = open(destination, 'wb')\n            close_destination = True\n        try:\n            copyfileobj(self.stream, destination, buffer_size)\n        finally:\n            if close_destination:\n                destination.close()", "category": "Python"}, {"instruction": "def compile_column(name: str, data_type: str, nullable: bool) -> str:\n    \"\"\"Create column definition statement.\"\"\"\n", "input": "", "output": "\n    null_str = 'NULL' if nullable else 'NOT NULL'\n\n    return '{name} {data_type} {null},'.format(name=name,\n                                               data_type=data_type,\n                                               null=null_str)", "category": "Python"}, {"instruction": "def expected_values(self, beta):\n        \"\"\" Expected values of the function given the covariance matrix and hyperparameters\n        \n        Parameters\n        ----------\n        beta : np.ndarray\n            Contains untransformed values for latent variables\n        \n        Returns\n        ----------\n        The expected values of the function\n        \"\"\"\n", "input": "", "output": "\n        parm = np.array([self.latent_variables.z_list[k].prior.transform(beta[k]) for k in range(beta.shape[0])])\n        L = self._L(parm)\n        alpha = self._alpha(L)\n        return np.dot(np.transpose(self.kernel.K(parm)), alpha)", "category": "Python"}, {"instruction": "def delete_space(self, space_key, callback=None):\n        \"\"\"\n        Deletes a Space.\n\n        The space is deleted in a long running task, so the space cannot be considered deleted when this method returns.\n        Clients can follow the status link in the response and poll it until the task completes.\n\n        :param space_key (string): The key of the space to delete.\n        :param callback: OPTIONAL: The callback to execute on the resulting data, before the method returns.\n                         Default: None (no callback, raw data returned).\n        :return: A pointer to the longpoll task if successful, or the results of the callback.\n                 Will raise requests.HTTPError on bad input, potentially.\n        \"\"\"\n", "input": "", "output": "        return self._service_delete_request(\"rest/api/space/{key}\".format(key=space_key),\n                                            callback=callback)", "category": "Python"}, {"instruction": "def scale_axes_from_data(self):\n        \"\"\"Restrict data limits for Y-axis based on what you can see\n        \"\"\"\n", "input": "", "output": "        # get tight limits for X-axis\n        if self.args.xmin is None:\n            self.args.xmin = min(ts.xspan[0] for ts in self.timeseries)\n        if self.args.xmax is None:\n            self.args.xmax = max(ts.xspan[1] for ts in self.timeseries)\n\n        # autoscale view for Y-axis\n        cropped = [ts.crop(self.args.xmin, self.args.xmax) for\n                   ts in self.timeseries]\n        ymin = min(ts.value.min() for ts in cropped)\n        ymax = max(ts.value.max() for ts in cropped)\n        self.plot.gca().yaxis.set_data_interval(ymin, ymax, ignore=True)\n        self.plot.gca().autoscale_view(scalex=False)", "category": "Python"}, {"instruction": "def getbit(self, key, offset):\n        \"\"\"Returns the bit value at offset in the string value stored at key.\n\n        :raises TypeError: if offset is not int\n        :raises ValueError: if offset is less than 0\n        \"\"\"\n", "input": "", "output": "        if not isinstance(offset, int):\n            raise TypeError(\"offset argument must be int\")\n        if offset < 0:\n            raise ValueError(\"offset must be greater equal 0\")\n        return self.execute(b'GETBIT', key, offset)", "category": "Python"}, {"instruction": "def _error_handling(self, res):\n        \"\"\"\n        Helper function to do the error handling\n\n        :params res: the response from a request\n        \"\"\"\n", "input": "", "output": "\n        # making the error handling generic so if an status_code starting with 2 doesn't exist, we raise the error\n        if res.status_code // 100 != 2:\n            error = self._get_response_json(res)\n            raise exceptions.KeenApiError(error)", "category": "Python"}, {"instruction": "def defer_entity_syncing(wrapped, instance, args, kwargs):\n    \"\"\"\n    A decorator that can be used to defer the syncing of entities until after the method has been run\n    This is being introduced to help avoid deadlocks in the meantime as we attempt to better understand\n    why they are happening\n    \"\"\"\n", "input": "", "output": "\n    # Defer entity syncing while we run our method\n    sync_entities.defer = True\n\n    # Run the method\n    try:\n        return wrapped(*args, **kwargs)\n\n    # After we run the method disable the deferred syncing\n    # and sync all the entities that have been buffered to be synced\n    finally:\n        # Enable entity syncing again\n        sync_entities.defer = False\n\n        # Get the models that need to be synced\n        model_objs = list(sync_entities.buffer.values())\n\n        # If none is in the model objects we need to sync all\n        if None in sync_entities.buffer:\n            model_objs = list()\n\n        # Sync the entities that were deferred if any\n        if len(sync_entities.buffer):\n            sync_entities(*model_objs)\n\n        # Clear the buffer\n        sync_entities.buffer = {}", "category": "Python"}, {"instruction": "def get_first_song(self):\n        \"\"\"\n        \u521d\u59cb\u83b7\u53d6\u6b4c\u66f2\n\n        :params return: json\n        \"\"\"\n", "input": "", "output": "        count = 3\n        while count > 0:\n            song = self.requests_url('n')\n            if song:\n                return song\n            count -= 1\n            time.sleep(2)\n        raise GenericError('\u83b7\u53d6\u7b2c\u4e00\u9996\u6b4c\u66f2\u5931\u8d25\uff0c\u65e0\u6cd5\u5b8c\u6210\u521d\u59cb\u5316')", "category": "Python"}, {"instruction": "def get_related(self, instance, number):\n        \"\"\"\n        Implement high level cache system for get_related.\n        \"\"\"\n", "input": "", "output": "        cache = self.cache\n        cache_key = '%s:%s' % (instance.pk, number)\n        if cache_key not in cache:\n            related_objects = super(CachedModelVectorBuilder,\n                                    self).get_related(instance, number)\n            cache[cache_key] = related_objects\n            self.cache = cache\n        return cache[cache_key]", "category": "Python"}, {"instruction": "def _pastore16(ins):\n    ''' Stores 2\u00ba operand content into address of 1st operand.\n    store16 a, x =>  *(&a) = x\n    Use '*' for indirect store on 1st operand.\n    '''\n", "input": "", "output": "    output = _paddr(ins.quad[1])\n\n    value = ins.quad[2]\n    if value[0] == '*':\n        value = value[1:]\n        indirect = True\n    else:\n        indirect = False\n\n    try:\n        value = int(ins.quad[2]) & 0xFFFF\n        output.append('ld de, %i' % value)\n        if indirect:\n            output.append('call __LOAD_DE_DE')\n            REQUIRES.add('lddede.asm')\n\n    except ValueError:\n        output.append('pop de')\n\n    output.append('ld (hl), e')\n    output.append('inc hl')\n    output.append('ld (hl), d')\n\n    return output", "category": "Python"}, {"instruction": "def main():\n    \"\"\"CLI entrypoint for scaling policy creation\"\"\"\n", "input": "", "output": "    logging.basicConfig(format=LOGGING_FORMAT)\n    log = logging.getLogger(__name__)\n\n    parser = argparse.ArgumentParser()\n    add_debug(parser)\n    add_app(parser)\n    add_properties(parser)\n    add_env(parser)\n    add_region(parser)\n    args = parser.parse_args()\n\n    logging.getLogger(__package__.split('.')[0]).setLevel(args.debug)\n\n    log.debug('Parsed arguments: %s', args)\n\n    asgpolicy = AutoScalingPolicy(app=args.app, prop_path=args.properties, env=args.env, region=args.region)\n\n    asgpolicy.create_policy()", "category": "Python"}, {"instruction": "def setup_logger():\n    \"\"\"Return a logger with a default ColoredFormatter.\"\"\"\n", "input": "", "output": "    formatter = ColoredFormatter(\n        \"%(log_color)s%(levelname)-8s%(reset)s %(blue)s%(message)s\",\n        datefmt=None,\n        reset=True,\n        log_colors={\n            'DEBUG':    'cyan',\n            'INFO':     'green',\n            'WARNING':  'yellow',\n            'ERROR':    'red',\n            'CRITICAL': 'red',\n        }\n    )\n\n    logger = logging.getLogger('example')\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n\n    return logger", "category": "Python"}, {"instruction": "def get_url_params(end_point: str) -> list:\n        \"\"\"\n        Gets route parameters as dictionary\n        :param end_point str target route\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        var_params = end_point.split('/')\n\n        if len(var_params) == 1 and var_params[0] == '':\n            return []\n\n        elif len(var_params) == 1 and var_params[0] != '':\n            return [var_params[0]]\n        else:\n            params = list()\n            for param in var_params:\n                if len(param) > 0:\n                    params.append(param)\n            return params", "category": "Python"}, {"instruction": "def get_recent_season_matches(self, season_key):\n        \"\"\"\n        Calling specific season recent matches.\n\n        Arg:\n           season_key: key of the season.\n        Return:\n           json date\n        \"\"\"\n", "input": "", "output": "\n        season_recent_matches_url = self.api_path + \"season/\" + season_key + \"/recent_matches/\"\n        response = self.get_response(season_recent_matches_url)\n        return response", "category": "Python"}, {"instruction": "def distill_resnet_32_to_15_cifar20x5():\n  \"\"\"Set of hyperparameters.\"\"\"\n", "input": "", "output": "  hparams = distill_base()\n  hparams.teacher_model = \"resnet\"\n  hparams.teacher_hparams = \"resnet_cifar_32\"\n  hparams.student_model = \"resnet\"\n  hparams.student_hparams = \"resnet_cifar_15\"\n\n  hparams.optimizer_momentum_nesterov = True\n  # (base_lr=0.1) * (batch_size=128*8 (on TPU, or 8 GPUs)=1024) / (256.)\n  hparams.teacher_learning_rate = 0.25 * 128. * 8. / 256.\n  hparams.student_learning_rate = 0.2 * 128. * 8. / 256.\n  hparams.learning_rate_decay_scheme = \"piecewise\"\n  hparams.add_hparam(\"learning_rate_boundaries\", [40000, 60000, 80000])\n  hparams.add_hparam(\"learning_rate_multiples\", [0.1, 0.01, 0.001])\n\n  hparams.task_balance = 0.28\n  hparams.distill_temperature = 2.0\n\n  hparams.num_classes = 20\n\n  return hparams", "category": "Python"}, {"instruction": "def _construct_axes_dict(self, axes=None, **kwargs):\n        \"\"\"Return an axes dictionary for myself.\"\"\"\n", "input": "", "output": "        d = {a: self._get_axis(a) for a in (axes or self._AXIS_ORDERS)}\n        d.update(kwargs)\n        return d", "category": "Python"}, {"instruction": "def metrics(self, *metrics):\n        \"\"\" Add a list of Metric ingredients to the query. These can either be\n        Metric objects or strings representing metrics on the shelf.\n\n        The Metric expression will be added to the query's select statement.\n        The metric value is a property of each row of the result.\n\n        :param metrics: Metrics to add to the recipe. Metrics can\n                         either be keys on the ``shelf`` or\n                         Metric objects\n        :type metrics: list\n        \"\"\"\n", "input": "", "output": "        for m in metrics:\n            self._cauldron.use(self._shelf.find(m, Metric))\n        self.dirty = True\n        return self", "category": "Python"}, {"instruction": "def _add_url_rule(url_or_urls):\n    \"\"\"Register URL rule to application URL map.\"\"\"\n", "input": "", "output": "    old = current_app._got_first_request\n    # This is bit of cheating to overcome @flask.app.setupmethod decorator.\n    current_app._got_first_request = False\n    if isinstance(url_or_urls, six.string_types):\n        url_or_urls = [url_or_urls]\n    map(lambda url: current_app.add_url_rule(url, 'invenio_pages.view', view),\n        url_or_urls)\n    current_app._got_first_request = old", "category": "Python"}, {"instruction": "def noreplynum_get(self, service_staff_id, start_date, end_date, session):\n        '''taobao.wangwang.eservice.noreplynum.get \u5ba2\u670d\u672a\u56de\u590d\u4eba\u6570\n        \n        \u6839\u636e\u64cd\u4f5c\u8005ID\uff0c\u8fd4\u56de\u88ab\u67e5\u8005ID\u6307\u5b9a\u65e5\u671f\u5185\u6bcf\u4e2a\u5e10\u53f7\u6bcf\u65e5\u7684\"\u672a\u56de\u590d\u60c5\u51b5\" \u5907\u6ce8\uff1a\n        \n        - 1\u3001\u5982\u679c\u662f\u64cd\u4f5c\u8005ID=\u88ab\u67e5\u8005ID\uff0c\u8fd4\u56de\u88ab\u67e5\u8005ID\u7684\"\u672a\u56de\u590d\u60c5\u51b5\"\uff08\u672a\u56de\u590d\u4eba\u6570\u3001\u672a\u56de\u590d\u7684ID\uff09\u3002 \n        - 2\u3001\u5982\u679c\u64cd\u4f5c\u8005\u662f\u7ec4\u7ba1\u7406\u5458\uff0c\u4ed6\u53ef\u4ee5\u67e5\u8be2\u4ed6\u7684\u7ec4\u4e2d\u7684\u6240\u6709\u5b50\u5e10\u53f7\u7684\"\u672a\u56de\u590d\u60c5\u51b5\"\u3002 \n        - 3\u3001\u5982\u679c\u64cd\u4f5c\u8005\u662f\u4e3b\u8d26\u6237\uff0c\u4ed6\u53ef\u4ee5\u67e5\u8be2\u6240\u6709\u5b50\u5e10\u53f7\u7684\"\u672a\u56de\u590d\u60c5\u51b5\"\u3002 \n        - 4\u3001\u88ab\u67e5\u8005ID\u53ef\u4ee5\u662f\u591a\u4e2a\uff0c\u7528 \",\" \u9694\u5f00\uff0cid\u6570\u4e0d\u80fd\u8d85\u8fc730\u3002 \n        - 5\u3001\u5f00\u59cb\u65f6\u95f4\u4e0e\u7ed3\u675f\u65f6\u95f4\u4e4b\u95f4\u7684\u95f4\u9694\u4e0d\u80fd\u8d85\u8fc77\u5929 \n        - 6\u3001\u4e0d\u80fd\u67e5\u8be290\u5929\u4ee5\u524d\u7684\u6570\u636e \n        - 7\u3001\u4e0d\u80fd\u67e5\u8be2\u5f53\u5929\u7684\u8bb0\u5f55'''\n", "input": "", "output": "        request = TOPRequest('taobao.wangwang.eservice.noreplynum.get')\n        request['service_staff_id'] = service_staff_id\n        request['start_date'] = start_date\n        request['end_date'] = end_date\n        self.create(self.execute(request, session))\n        return self.non_reply_stat_on_days", "category": "Python"}, {"instruction": "def get_platform():\n    \"\"\"Return our platform name 'win32', 'linux_x86_64'\"\"\"\n", "input": "", "output": "    # XXX remove distutils dependency\n    result = distutils.util.get_platform().replace('.', '_').replace('-', '_')\n    if result == \"linux_x86_64\" and sys.maxsize == 2147483647:\n        # pip pull request #3497\n        result = \"linux_i686\"\n    return result", "category": "Python"}, {"instruction": "def getPointOnLine(x1, y1, x2, y2, n):\n    \"\"\"Returns the (x, y) tuple of the point that has progressed a proportion\n    n along the line defined by the two x, y coordinates.\n\n    Copied from pytweening module.\n    \"\"\"\n", "input": "", "output": "    x = ((x2 - x1) * n) + x1\n    y = ((y2 - y1) * n) + y1\n    return (x, y)", "category": "Python"}, {"instruction": "def read_hector_output(csv_file):\n    \"\"\"\n    Reads a Hector output stream CSV file and returns a wide DataFrame with\n    Hector output data.\n    \"\"\"\n", "input": "", "output": "    # Filter out spin-up values. In Hector 1.x RCP output streams years are\n    # given as end of simulation year.\n    # See https://github.com/JGCRI/hector/issues/177\n    start_year = 1746\n    output_stream = pd.read_csv(csv_file, skiprows=1)\n\n    wide = output_stream[output_stream.year >= start_year].pivot_table(\n        index=\"year\", columns=\"variable\", values=\"value\"\n    )\n\n    return wide", "category": "Python"}, {"instruction": "def any(self, axis=0, *args, **kwargs):\n        \"\"\"\n        Tests whether at least one of elements evaluate True\n\n        Returns\n        -------\n        any : bool\n\n        See Also\n        --------\n        numpy.any\n        \"\"\"\n", "input": "", "output": "        nv.validate_any(args, kwargs)\n\n        values = self.sp_values\n\n        if len(values) != len(self) and np.any(self.fill_value):\n            return True\n\n        return values.any().item()", "category": "Python"}, {"instruction": "def create_or_login(resp):\n    \"\"\"This is called when login with OpenID succeeded and it's not\n    necessary to figure out if this is the users's first login or not.\n    This function has to redirect otherwise the user will be presented\n    with a terrible URL which we certainly don't want.\n    \"\"\"\n", "input": "", "output": "    session['openid'] = resp.identity_url\n    user = User.get_collection().find_one({'openid':resp.identity_url})\n    if user is not None:\n        flash(u'Successfully signed in')\n        g.user = user\n        return redirect(oid.get_next_url())\n    return redirect(url_for('create_profile', next=oid.get_next_url(),\n                            name=resp.fullname or resp.nickname,\n                            email=resp.email))", "category": "Python"}, {"instruction": "def search_datasets(\n        self,\n        license=None,\n        format=None,\n        query=None,\n        featured=None,\n        owner=None,\n        organization=None,\n        badge=None,\n        reuses=None,\n        page_size=20,\n        x_fields=None,\n    ):\n        \"\"\"Search datasets within uData portal.\"\"\"\n", "input": "", "output": "        # handling request parameters\n        payload = {\"badge\": badge, \"size\": page_size, \"X-Fields\": x_fields}\n\n        # search request\n        # head = {\"X-API-KEY\": self.api_key}\n        search_url = \"{}/datasets\".format(\n            self.base_url,\n            # org_id,\n            # page_size\n        )\n\n        search_req = requests.get(\n            search_url,\n            # headers=head,\n            params=payload,\n        )\n\n        # serializing result into dict and storing resources in variables\n        logger.debug(search_req.url)\n        return search_req.json()", "category": "Python"}, {"instruction": "def _fastfood_show(args):\n    \"\"\"Run on `fastfood show`.\"\"\"\n", "input": "", "output": "    template_pack = pack.TemplatePack(args.template_pack)\n    if args.stencil_set:\n        stencil_set = template_pack.load_stencil_set(args.stencil_set)\n        print(\"Stencil Set %s:\" % args.stencil_set)\n        print('  Stencils:')\n        for stencil in stencil_set.stencils:\n            print(\"    %s\" % stencil)\n        print('  Options:')\n        for opt, vals in stencil_set.manifest['options'].items():\n            print(\"    %s - %s\" % (opt, vals['help']))", "category": "Python"}, {"instruction": "def diff_many(models1, models2, migrator=None, reverse=False):\n    \"\"\"Calculate changes for migrations from models2 to models1.\"\"\"\n", "input": "", "output": "    models1 = pw.sort_models(models1)\n    models2 = pw.sort_models(models2)\n\n    if reverse:\n        models1 = reversed(models1)\n        models2 = reversed(models2)\n\n    models1 = OrderedDict([(m._meta.name, m) for m in models1])\n    models2 = OrderedDict([(m._meta.name, m) for m in models2])\n\n    changes = []\n\n    for name, model1 in models1.items():\n        if name not in models2:\n            continue\n        changes += diff_one(model1, models2[name], migrator=migrator)\n\n    # Add models\n    for name in [m for m in models1 if m not in models2]:\n        changes.append(create_model(models1[name], migrator=migrator))\n\n    # Remove models\n    for name in [m for m in models2 if m not in models1]:\n        changes.append(remove_model(models2[name]))\n\n    return changes", "category": "Python"}, {"instruction": "def set_base_initial_condition(model, monomer, value):\n    \"\"\"Set an initial condition for a monomer in its 'default' state.\"\"\"\n", "input": "", "output": "    # Build up monomer pattern dict\n    sites_dict = {}\n    for site in monomer.sites:\n        if site in monomer.site_states:\n            if site == 'loc' and 'cytoplasm' in monomer.site_states['loc']:\n                sites_dict['loc'] = 'cytoplasm'\n            else:\n                sites_dict[site] = monomer.site_states[site][0]\n        else:\n            sites_dict[site] = None\n    mp = monomer(**sites_dict)\n    pname = monomer.name + '_0'\n    try:\n        p = model.parameters[pname]\n        p.value = value\n    except KeyError:\n        p = Parameter(pname, value)\n        model.add_component(p)\n        model.initial(mp, p)", "category": "Python"}, {"instruction": "def execute(self, http_method, path, params=None, data=None, headers=None):\n    \"\"\"\n    Submit an HTTP request.\n    @param http_method: GET, POST, PUT, DELETE\n    @param path: The path of the resource.\n    @param params: Key-value parameter data.\n    @param data: The data to attach to the body of the request.\n    @param headers: The headers to set for this request.\n\n    @return: The result of urllib2.urlopen()\n    \"\"\"\n", "input": "", "output": "    # Prepare URL and params\n    url = self._make_url(path, params)\n    if http_method in (\"GET\", \"DELETE\"):\n      if data is not None:\n        self.logger.warn(\n            \"GET method does not pass any data. Path '%s'\" % (path,))\n        data = None\n\n    # Setup the request\n    request = urllib2.Request(url, data)\n    # Hack/workaround because urllib2 only does GET and POST\n    request.get_method = lambda: http_method\n\n    headers = self._get_headers(headers)\n    for k, v in headers.items():\n      request.add_header(k, v)\n\n    # Call it\n    self.logger.debug(\"%s %s\" % (http_method, url))\n    try:\n      return self._opener.open(request)\n    except urllib2.HTTPError, ex:\n      raise self._exc_class(ex)", "category": "Python"}, {"instruction": "def plus_dora(tile, dora_indicators):\n    \"\"\"\n    :param tile: int 136 tiles format\n    :param dora_indicators: array of 136 tiles format\n    :return: int count of dora\n    \"\"\"\n", "input": "", "output": "    tile_index = tile // 4\n    dora_count = 0\n\n    for dora in dora_indicators:\n        dora //= 4\n\n        # sou, pin, man\n        if tile_index < EAST:\n\n            # with indicator 9, dora will be 1\n            if dora == 8:\n                dora = -1\n            elif dora == 17:\n                dora = 8\n            elif dora == 26:\n                dora = 17\n\n            if tile_index == dora + 1:\n                dora_count += 1\n        else:\n            if dora < EAST:\n                continue\n\n            dora -= 9 * 3\n            tile_index_temp = tile_index - 9 * 3\n\n            # dora indicator is north\n            if dora == 3:\n                dora = -1\n\n            # dora indicator is hatsu\n            if dora == 6:\n                dora = 3\n\n            if tile_index_temp == dora + 1:\n                dora_count += 1\n\n    return dora_count", "category": "Python"}, {"instruction": "def pre_delete_title(instance, **kwargs):\n    ''' Update article.languages\n    '''\n", "input": "", "output": "    if instance.article.languages:\n        languages = instance.article.languages.split(',')\n    else:\n        languages = []\n    if instance.language in languages:\n        languages.remove(instance.language)\n        instance.article.languages = ','.join(languages)\n        instance.article._publisher_keep_state = True\n        instance.article.save(no_signals=True)", "category": "Python"}, {"instruction": "def classify(self, txt):\n        '''\n        Classifies text by language. Uses preferred_languages weighting.\n        '''\n", "input": "", "output": "        ranks = []\n        for lang, score in langid.rank(txt):\n            if lang in self.preferred_languages:\n                score += self.preferred_factor\n            ranks.append((lang, score))\n        ranks.sort(key=lambda x: x[1], reverse=True)\n        return ranks[0][0]", "category": "Python"}, {"instruction": "def get(self, wg_uuid, uuid, tree=False):\n        \"\"\" Get one workgroup member.\"\"\"\n", "input": "", "output": "        url = \"%(base)s/%(wg_uuid)s/nodes/%(uuid)s\" % {\n            'base': self.local_base_url,\n            'wg_uuid': wg_uuid,\n            'uuid': uuid\n        }\n        param = {}\n        if tree:\n            param['tree'] = True\n        encode = urllib.urlencode(param)\n        if encode:\n            url += \"?\"\n            url += encode\n        return self.core.get(url)", "category": "Python"}, {"instruction": "def email_quote_txt(text,\n                    indent_txt='>>',\n                    linebreak_input=\"\\n\",\n                    linebreak_output=\"\\n\"):\n    \"\"\"\n    Takes a text and returns it in a typical mail quoted format, e.g.::\n        C'est un lapin, lapin de bois.\n        >>Quoi?\n        Un cadeau.\n        >>What?\n        A present.\n        >>Oh, un cadeau.\n\n    will return::\n        >>C'est un lapin, lapin de bois.\n        >>>>Quoi?\n        >>Un cadeau.\n        >>>>What?\n        >>A present.\n        >>>>Oh, un cadeau.\n\n    @param text: the string to quote\n    @param indent_txt: the string used for quoting (default: '>>')\n    @param linebreak_input: in the text param, string used for linebreaks\n    @param linebreak_output: linebreak used for output\n    @return: the text as a quoted string\n    \"\"\"\n", "input": "", "output": "    if (text == \"\"):\n        return \"\"\n    lines = text.split(linebreak_input)\n    text = \"\"\n    for line in lines:\n        text += indent_txt + line + linebreak_output\n    return text", "category": "Python"}, {"instruction": "def process(self, envelope):\n\t\t\"\"\" :meth:`.WMessengerOnionSessionProto.process` method implementation.\n\t\t\"\"\"\n", "input": "", "output": "\n\t\tdef process_single_layer(iter_envelope, iter_obj):\n\t\t\tlayer = self.onion().layer(iter_obj.layer_name())\n\t\t\tlayer_args = iter_obj.layer_args()\n\t\t\treturn layer.process(iter_envelope, self, **layer_args)\n\n\t\titerator = self.session_flow().iterator(envelope)\n\t\tif iterator is not None:\n\t\t\twhile iterator is not None:\n\t\t\t\tenvelope = process_single_layer(envelope, iterator)\n\t\t\t\titerator = iterator.next(envelope)\n\t\treturn envelope", "category": "Python"}, {"instruction": "def _getcols(cur, table):\n        \"\"\"\n        Will execute a pg query to get the column names for the given table.\n        :param cur:\n        :param table:\n        :return:\n        \"\"\"\n", "input": "", "output": "        query = ' '.join((\"SELECT * FROM\", table, \"LIMIT 0\"))  # for testing\n\n        cur.execute(query)\n        colnames = [desc[0] for desc in cur.description]\n        LOG.info(\"COLS (%s): %s\", table, colnames)\n\n        return", "category": "Python"}, {"instruction": "def filter_by_missing(max_miss=0.01):\r\n    \"\"\"\r\n    return function that filters by missing values\r\n    (takes maximum fraction of missing values, default is 0.01)\r\n    \"\"\"\n", "input": "", "output": "\r\n    def f(G, bim):\r\n        Isnp = sp.isnan(G).mean(0) < max_miss\r\n        G_out = G[:, Isnp]\r\n        bim_out = bim[Isnp]\r\n        return G_out, bim_out\r\n\r\n    return f", "category": "Python"}, {"instruction": "def is_valid(self, instance):\r\n        '''Perform validation for *instance* and stores serialized data,\r\nindexes and errors into local cache.\r\nReturn ``True`` if the instance is ready to be saved to database.'''\n", "input": "", "output": "        dbdata = instance.dbdata\r\n        data = dbdata['cleaned_data'] = {}\r\n        errors = dbdata['errors'] = {}\r\n        #Loop over scalar fields first\r\n        for field, value in instance.fieldvalue_pairs():\r\n            name = field.attname\r\n            try:\r\n                svalue = field.set_get_value(instance, value)\r\n            except Exception as e:\r\n                errors[name] = str(e)\r\n            else:\r\n                if (svalue is None or svalue is '') and field.required:\r\n                    errors[name] = (\"Field '{0}' is required for '{1}'.\"\r\n                                    .format(name, self))\r\n                else:\r\n                    if isinstance(svalue, dict):\r\n                        data.update(svalue)\r\n                    elif svalue is not None:\r\n                        data[name] = svalue\r\n        return len(errors) == 0", "category": "Python"}, {"instruction": "def accel_toggle_transparency(self, *args):\n        \"\"\"Callback to toggle transparency.\n        \"\"\"\n", "input": "", "output": "        self.transparency_toggled = not self.transparency_toggled\n        self.settings.styleBackground.triggerOnChangedValue(\n            self.settings.styleBackground, 'transparency'\n        )\n        return True", "category": "Python"}, {"instruction": "def rsq_adj(self):\r\n        \"\"\"Adjusted R-squared.\"\"\"\n", "input": "", "output": "        n = self.n\r\n        k = self.k\r\n        return 1.0 - ((1.0 - self.rsq) * (n - 1.0) / (n - k - 1.0))", "category": "Python"}, {"instruction": "def read(self, entity=None, attrs=None, ignore=None, params=None):\n        \"\"\"Ignore ``organization`` field as it's never returned by the server\n        and is only added to entity to be able to use organization path\n        dependent helpers.\n        \"\"\"\n", "input": "", "output": "        if ignore is None:\n            ignore = set()\n        ignore.add('organization')\n        return super(Subscription, self).read(entity, attrs, ignore, params)", "category": "Python"}, {"instruction": "def transitions(network, before_state, after_state):\n    \"\"\"Return a generator of all **possible** transitions of a network.\n    \"\"\"\n", "input": "", "output": "    # TODO: Does not return subsystems that are in an impossible transitions.\n\n    # Elements without inputs are reducibe effects,\n    # elements without outputs are reducible causes.\n    possible_causes = np.where(np.sum(network.cm, 1) > 0)[0]\n    possible_effects = np.where(np.sum(network.cm, 0) > 0)[0]\n\n    for cause_subset in utils.powerset(possible_causes, nonempty=True):\n        for effect_subset in utils.powerset(possible_effects, nonempty=True):\n            try:\n                yield Transition(network, before_state, after_state,\n                                 cause_subset, effect_subset)\n            except exceptions.StateUnreachableError:\n                pass", "category": "Python"}, {"instruction": "def _clean_fields(allowed_fields: dict, fields: FieldsParam) -> Iterable[str]:\n    \"\"\"Clean lookup fields and check for errors.\"\"\"\n", "input": "", "output": "    if fields == ALL:\n        fields = allowed_fields.keys()\n    else:\n        fields = tuple(fields)\n        unknown_fields = set(fields) - allowed_fields.keys()\n        if unknown_fields:\n            raise ValueError('Unknown fields: {}'.format(unknown_fields))\n    return fields", "category": "Python"}, {"instruction": "def _pre_md5_skip_on_check(self, lpath, rfile):\n        # type: (Downloader, pathlib.Path,\n        #        blobxfer.models.azure.StorageEntity) -> None\n        \"\"\"Perform pre MD5 skip on check\n        :param Downloader self: this\n        :param pathlib.Path lpath: local path\n        :param blobxfer.models.azure.StorageEntity rfile: remote file\n        \"\"\"\n", "input": "", "output": "        md5 = blobxfer.models.metadata.get_md5_from_metadata(rfile)\n        key = blobxfer.operations.download.Downloader.\\\n            create_unique_transfer_operation_id(rfile)\n        with self._md5_meta_lock:\n            self._md5_map[key] = rfile\n        slpath = str(lpath)\n        # temporarily create a download descriptor view for vectored io\n        if rfile.vectored_io is not None:\n            view, _ = blobxfer.models.download.Descriptor.generate_view(rfile)\n            fpath = str(\n                blobxfer.models.download.Descriptor.\n                convert_vectored_io_slice_to_final_path_name(lpath, rfile)\n            )\n        else:\n            view = None\n            fpath = slpath\n        self._md5_offload.add_localfile_for_md5_check(\n            key, slpath, fpath, md5, rfile.mode, view)", "category": "Python"}, {"instruction": "def save_cards(cards, filename=None):\n    \"\"\"\n    Save the given cards, in plain text, to a txt file.\n\n    :arg cards:\n        The cards to save. Can be a ``Stack``, ``Deck``, or ``list``.\n    :arg str filename:\n        The filename to use for the cards file. If no filename given,\n        defaults to \"cards-YYYYMMDD.txt\", where \"YYYYMMDD\" is the year, month,\n        and day. For example, \"cards-20140711.txt\".\n\n    \"\"\"\n", "input": "", "output": "    filename = filename or \"cards-%s.txt\" % (time.strftime(\"%Y%m%d\"))\n\n    with open(filename, \"w\") as deck_file:\n        card_reprs = [\"%s %s\\n\" % (card.value, card.suit) for card in cards]\n        card_reprs[-1] = card_reprs[-1].rstrip(\"\\n\")\n        for card in card_reprs:\n            deck_file.write(card)", "category": "Python"}, {"instruction": "def refresh(self):\r\n        \"\"\"Remove editors that are not longer open.\"\"\"\n", "input": "", "output": "        self._update_id_list()\r\n        for _id in self.history[:]:\r\n            if _id not in self.id_list:\r\n                self.history.remove(_id)", "category": "Python"}, {"instruction": "def create(max_kl, cg_iters, line_search_iters, cg_damping, entropy_coef, vf_iters, discount_factor,\n           gae_lambda=1.0, improvement_acceptance_ratio=0.1, max_grad_norm=0.5):\n    \"\"\" Vel factory function \"\"\"\n", "input": "", "output": "    return TrpoPolicyGradient(\n        max_kl, int(cg_iters), int(line_search_iters), cg_damping, entropy_coef, vf_iters,\n        discount_factor=discount_factor,\n        gae_lambda=gae_lambda,\n        improvement_acceptance_ratio=improvement_acceptance_ratio,\n        max_grad_norm=max_grad_norm\n    )", "category": "Python"}, {"instruction": "def _update_field(self, natvalue):\n        \"\"\"\n        Update this NATValue if values are different\n        \n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        updated = False\n        if natvalue.element and natvalue.element != self.element:\n            self.update(element=natvalue.element)\n            self.pop('ip_descriptor', None)\n            updated = True\n        elif natvalue.ip_descriptor and self.ip_descriptor and \\\n            natvalue.ip_descriptor != self.ip_descriptor:\n            self.update(ip_descriptor=natvalue.ip_descriptor)\n            self.pop('element', None)\n            updated = True\n\n        for port in ('min_port', 'max_port'):\n            _port = getattr(natvalue, port, None)\n            if _port is not None and getattr(self, port, None) != _port:\n                self[port] = _port\n                updated = True\n    \n        return updated", "category": "Python"}, {"instruction": "def guess_title(basename):\n    \"\"\" Attempt to guess the title from the filename \"\"\"\n", "input": "", "output": "\n    base, _ = os.path.splitext(basename)\n    return re.sub(r'[ _-]+', r' ', base).title()", "category": "Python"}, {"instruction": "def update_file(self, file_id, upload_id):\n        \"\"\"\n        Send PUT request to /files/{file_id} to update the file contents to upload_id and sets a label.\n        :param file_id: str uuid of file\n        :param upload_id: str uuid of the upload where all the file chunks where uploaded\n        :param label: str short display label for the file\n        :return: requests.Response containing the successful result\n        \"\"\"\n", "input": "", "output": "        put_data = {\n            \"upload[id]\": upload_id,\n        }\n        return self._put(\"/files/\" + file_id, put_data, content_type=ContentType.form)", "category": "Python"}, {"instruction": "def get_string(self, key, default=UndefinedKey):\n        \"\"\"Return string representation of value found at key\n\n        :param key: key to use (dot separated). E.g., a.b.c\n        :type key: basestring\n        :param default: default value if key not found\n        :type default: basestring\n        :return: string value\n        :type return: basestring\n        \"\"\"\n", "input": "", "output": "        value = self.get(key, default)\n        if value is None:\n            return None\n\n        string_value = unicode(value)\n        if isinstance(value, bool):\n            string_value = string_value.lower()\n        return string_value", "category": "Python"}, {"instruction": "def _get_row(self, id):\n        \"\"\"Create a row dictionary for a given object id.\"\"\"\n", "input": "", "output": "        return {name: d['func'](id) for (name, d) in self._columns.items()}", "category": "Python"}, {"instruction": "def _addSpecfile(self, specfile, path):\n        \"\"\"Adds a new specfile entry to SiiContainer.info. See also\n        :class:`SiiContainer.addSpecfile()`.\n\n        :param specfile: the name of an ms-run file\n        :param path: filedirectory for loading and saving the ``siic`` files\n        \"\"\"\n", "input": "", "output": "        self.info[specfile] = {'path': path, 'qcAttr': None, 'qcCutoff': None,\n                               'qcLargerBetter': None, 'rankAttr': None,\n                               'rankLargerBetter': None\n                               }\n        self.container[specfile] = dict()", "category": "Python"}, {"instruction": "def getsize(self, path):\n        \"\"\"Return the file object size in bytes.\n\n        Args:\n          path:  path to the file object.\n\n        Returns:\n          file size in bytes.\n        \"\"\"\n", "input": "", "output": "        try:\n            file_obj = self.filesystem.resolve(path)\n            if (self.filesystem.ends_with_path_separator(path) and\n                    S_IFMT(file_obj.st_mode) != S_IFDIR):\n                error_nr = (errno.EINVAL if self.filesystem.is_windows_fs\n                            else errno.ENOTDIR)\n                self.filesystem.raise_os_error(error_nr, path)\n            return file_obj.st_size\n        except IOError as exc:\n            raise os.error(exc.errno, exc.strerror)", "category": "Python"}, {"instruction": "def delete_tag_from_job(user, job_id, tag_id):\n    \"\"\"Delete a tag from a job.\"\"\"\n", "input": "", "output": "\n    _JJT = models.JOIN_JOBS_TAGS\n    job = v1_utils.verify_existence_and_get(job_id, _TABLE)\n    if not user.is_in_team(job['team_id']):\n        raise dci_exc.Unauthorized()\n    v1_utils.verify_existence_and_get(tag_id, models.TAGS)\n\n    query = _JJT.delete().where(sql.and_(_JJT.c.tag_id == tag_id,\n                                         _JJT.c.job_id == job_id))\n\n    try:\n        flask.g.db_conn.execute(query)\n    except sa_exc.IntegrityError:\n        raise dci_exc.DCICreationConflict('tag', 'tag_id')\n\n    return flask.Response(None, 204, content_type='application/json')", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"Start the component running.\"\"\"\n", "input": "", "output": "        for name, child in self._compound_children.items():\n            self.logger.debug('start %s (%s)', name, child.__class__.__name__)\n            child.start()", "category": "Python"}, {"instruction": "def smart_query_string(parser, token):\n    \"\"\"\n    Outputs current GET query string with additions appended.\n    Additions are provided in token pairs. \n    \"\"\"\n", "input": "", "output": "    args = token.split_contents()\n    additions = args[1:]\n   \n    addition_pairs = []\n    while additions:\n        addition_pairs.append(additions[0:2])\n        additions = additions[2:]\n\n    return SmartQueryStringNode(addition_pairs)", "category": "Python"}, {"instruction": "def _default_plugins(self):\n        \"\"\" Get entry points to load any plugins installed. \n        The build process should create an \"entry_points.json\" file\n        with all of the data from the installed entry points.\n        \n        \"\"\"\n", "input": "", "output": "        plugins = {}\n        try:\n            with open('entry_points.json') as f:\n                entry_points = json.load(f)\n            for ep, obj in entry_points.items():\n                plugins[ep] = []\n                for name, src in obj.items():\n                    plugins[ep].append(Plugin(name=name, source=src))\n        except Exception as e:\n            print(\"Failed to load entry points {}\".format(e))\n        return plugins", "category": "Python"}, {"instruction": "def rename(self, bucket, key, key_to, force='false'):\n        \"\"\"\u91cd\u547d\u540d\u6587\u4ef6:\n\n        \u7ed9\u8d44\u6e90\u8fdb\u884c\u91cd\u547d\u540d\uff0c\u672c\u8d28\u4e3amove\u64cd\u4f5c\u3002\n\n        Args:\n            bucket: \u5f85\u64cd\u4f5c\u8d44\u6e90\u6240\u5728\u7a7a\u95f4\n            key:    \u5f85\u64cd\u4f5c\u8d44\u6e90\u6587\u4ef6\u540d\n            key_to: \u76ee\u6807\u8d44\u6e90\u6587\u4ef6\u540d\n\n        Returns:\n            \u4e00\u4e2adict\u53d8\u91cf\uff0c\u6210\u529f\u8fd4\u56deNULL\uff0c\u5931\u8d25\u8fd4\u56de{\"error\": \"<errMsg string>\"}\n            \u4e00\u4e2aResponseInfo\u5bf9\u8c61\n        \"\"\"\n", "input": "", "output": "        return self.move(bucket, key, bucket, key_to, force)", "category": "Python"}, {"instruction": "def record_received(self, msg):\n        \"\"\"Handle ALDB record received from device.\"\"\"\n", "input": "", "output": "        release_lock = False\n        userdata = msg.userdata\n        rec = ALDBRecord.create_from_userdata(userdata)\n        self._records[rec.mem_addr] = rec\n\n        _LOGGER.debug('ALDB Record: %s', rec)\n\n        rec_count = self._load_action.rec_count\n        if rec_count == 1 or self._have_all_records():\n            release_lock = True\n\n        if self._is_first_record(rec):\n            self._mem_addr = rec.mem_addr\n\n        if release_lock and self._rec_mgr_lock.locked():\n            _LOGGER.debug('Releasing lock because record received')\n            self._rec_mgr_lock.release()", "category": "Python"}, {"instruction": "def get_value(value):\n    \"\"\" quoted-string / attribute\n\n    \"\"\"\n", "input": "", "output": "    v = Value()\n    if not value:\n        raise errors.HeaderParseError(\"Expected value but found end of string\")\n    leader = None\n    if value[0] in CFWS_LEADER:\n        leader, value = get_cfws(value)\n    if not value:\n        raise errors.HeaderParseError(\"Expected value but found \"\n                                      \"only {}\".format(leader))\n    if value[0] == '\"':\n        token, value = get_quoted_string(value)\n    else:\n        token, value = get_extended_attribute(value)\n    if leader is not None:\n        token[:0] = [leader]\n    v.append(token)\n    return v, value", "category": "Python"}, {"instruction": "def connection(self):\n        \"\"\"The :class:`pika.BlockingConnection` for the current\n        thread.  This property may change without notice.\n        \"\"\"\n", "input": "", "output": "        connection = getattr(self.state, \"connection\", None)\n        if connection is None:\n            connection = self.state.connection = pika.BlockingConnection(\n                parameters=self.parameters)\n            self.connections.add(connection)\n        return connection", "category": "Python"}, {"instruction": "def get_dns_subject_alternative_names(certificate: cryptography.x509.Certificate) -> List[str]:\n        \"\"\"Retrieve all the DNS entries of the Subject Alternative Name extension.\n        \"\"\"\n", "input": "", "output": "        subj_alt_names: List[str] = []\n        try:\n            san_ext = certificate.extensions.get_extension_for_oid(ExtensionOID.SUBJECT_ALTERNATIVE_NAME)\n            subj_alt_names = san_ext.value.get_values_for_type(DNSName)\n        except ExtensionNotFound:\n            pass\n        return subj_alt_names", "category": "Python"}, {"instruction": "def _jzerostr(ins):\n    \"\"\" Jumps if top of the stack contains a NULL pointer\n        or its len is Zero\n    \"\"\"\n", "input": "", "output": "    output = []\n    disposable = False  # True if string must be freed from memory\n\n    if ins.quad[1][0] == '_':  # Variable?\n        output.append('ld hl, (%s)' % ins.quad[1][0])\n    else:\n        output.append('pop hl')\n        output.append('push hl')  # Saves it for later\n        disposable = True\n\n    output.append('call __STRLEN')\n\n    if disposable:\n        output.append('ex (sp), hl')\n        output.append('call __MEM_FREE')\n        output.append('pop hl')\n        REQUIRES.add('alloc.asm')\n\n    output.append('ld a, h')\n    output.append('or l')\n    output.append('jp z, %s' % str(ins.quad[2]))\n    REQUIRES.add('strlen.asm')\n    return output", "category": "Python"}, {"instruction": "def _get_elements(mol, label):\n        \"\"\"\n        The the elements of the atoms in the specified order\n\n        Args:\n            mol: The molecule. OpenBabel OBMol object.\n            label: The atom indices. List of integers.\n\n        Returns:\n            Elements. List of integers.\n        \"\"\"\n", "input": "", "output": "        elements = [int(mol.GetAtom(i).GetAtomicNum()) for i in label]\n        return elements", "category": "Python"}, {"instruction": "def scan(self, match=\"*\", count=1000, cursor=0):\n        \"\"\" Iterates the set of keys in :prop:key_prefix in :prop:_client\n            @match: #str pattern to match after the :prop:key_prefix\n            @count: the user specified the amount of work that should be done\n                at every call in order to retrieve elements from the collection\n            @cursor: the next cursor position\n\n            -> #tuple (#int cursor position in scan, #list of full key names)\n        \"\"\"\n", "input": "", "output": "        cursor, data = self._client.scan(\n            cursor=cursor,\n            match=\"{}:{}\".format(self.key_prefix, match),\n            count=count)\n        return (cursor, list(map(self._decode, data)))", "category": "Python"}, {"instruction": "def name(self, code):\n        \"\"\"\n        Return the name of a country, based on the code.\n\n        If no match is found, returns an empty string.\n        \"\"\"\n", "input": "", "output": "        code = self.alpha2(code)\n        if code not in self.countries:\n            return \"\"\n        return self.translate_pair(code)[1]", "category": "Python"}, {"instruction": "def normalize_array(q):\n        \"\"\"\n        Normalizes the list with len 4 so that it can be used as quaternion\n        :param q: array of len 4\n        :returns: normalized array\n        \"\"\"\n", "input": "", "output": "        assert(len(q) == 4)\n        q = np.array(q)\n        n = QuaternionBase.norm_array(q)\n        return q / n", "category": "Python"}, {"instruction": "def find_home_directory():\n    \"\"\"\n    Look up the home directory of the effective user id.\n\n    :returns: The pathname of the home directory (a string).\n\n    .. note:: On Windows this uses the ``%APPDATA%`` environment variable (if\n              available) and otherwise falls back to ``~/Application Data``.\n    \"\"\"\n", "input": "", "output": "    if WINDOWS:\n        directory = os.environ.get('APPDATA')\n        if not directory:\n            directory = os.path.expanduser(r'~\\Application Data')\n    else:\n        # This module isn't available on Windows so we have to import it here.\n        import pwd\n        # Look up the home directory of the effective user id so we can\n        # generate pathnames relative to the home directory.\n        entry = pwd.getpwuid(os.getuid())\n        directory = entry.pw_dir\n    return directory", "category": "Python"}, {"instruction": "def _dump_multipolygon(obj, big_endian, meta):\n    \"\"\"\n    Dump a GeoJSON-like `dict` to a multipolygon WKB string.\n\n    Input parameters and output are similar to :funct:`_dump_point`.\n    \"\"\"\n", "input": "", "output": "    coords = obj['coordinates']\n    vertex = coords[0][0][0]\n    num_dims = len(vertex)\n\n    wkb_string, byte_fmt, byte_order = _header_bytefmt_byteorder(\n        'MultiPolygon', num_dims, big_endian, meta\n    )\n\n    poly_type = _WKB[_INT_TO_DIM_LABEL.get(num_dims)]['Polygon']\n    if big_endian:\n        poly_type = BIG_ENDIAN + poly_type\n    else:\n        poly_type = LITTLE_ENDIAN + poly_type[::-1]\n\n    # apped the number of polygons\n    wkb_string += struct.pack('%sl' % byte_order, len(coords))\n\n    for polygon in coords:\n        # append polygon header\n        wkb_string += poly_type\n        # append the number of rings in this polygon\n        wkb_string += struct.pack('%sl' % byte_order, len(polygon))\n        for ring in polygon:\n            # append the number of vertices in this ring\n            wkb_string += struct.pack('%sl' % byte_order, len(ring))\n            for vertex in ring:\n                wkb_string += struct.pack(byte_fmt, *vertex)\n\n    return wkb_string", "category": "Python"}, {"instruction": "def clear_choice_texts(self, choice_id):\n        \"\"\"stub\"\"\"\n", "input": "", "output": "        if self.get_choices_metadata().is_read_only():\n            raise NoAccess()\n        updated_choices = []\n        for current_choice in self.my_osid_object_form._my_map['choices']:\n            if current_choice['id'] != choice_id:\n                updated_choices.append(current_choice)\n            else:\n                updated_choices.append({\n                    'id': current_choice['id'],\n                    'texts': [],\n                    'name': current_choice['name']\n                })\n        self.my_osid_object_form._my_map['choices'] = updated_choices", "category": "Python"}, {"instruction": "def reference_of(selector: shapeLabel, cntxt: Union[Context, ShExJ.Schema] ) -> Optional[ShExJ.shapeExpr]:\n    \"\"\" Return the shape expression in the schema referenced by selector, if any\n\n    :param cntxt: Context node or ShEx Schema\n    :param selector: identifier of element to select within the schema\n    :return:\n    \"\"\"\n", "input": "", "output": "    schema = cntxt.schema if isinstance(cntxt, Context) else cntxt\n    if selector is START:\n        return schema.start\n    for expr in schema.shapes:\n        if not isinstance(expr, ShExJ.ShapeExternal) and expr.id == selector:\n            return expr\n    return schema.start if schema.start is not None and schema.start.id == selector else None", "category": "Python"}, {"instruction": "def delete_alert(self, alert):\n        \"\"\"\n        Deletes the specified alert from the Alert API\n        :param alert: the alert to be deleted\n        :type alert: pyowm.alertapi30.alert.Alert`\n        :return: ``None`` if the deletion was successful, an error otherwise\n        \"\"\"\n", "input": "", "output": "        assert alert is not None\n        assert isinstance(alert.id, str), \"Value must be a string\"\n        assert isinstance(alert.trigger_id, str), \"Value must be a string\"\n        status, _ = self.http_client.delete(\n            NAMED_ALERT_URI % (alert.trigger_id, alert.id),\n            params={'appid': self.API_key},\n            headers={'Content-Type': 'application/json'})", "category": "Python"}, {"instruction": "def uuid_to_date(uuid, century='20'):\n    \"\"\"Return a date created from the last 6 digits of a uuid.\n\n    Arguments:\n        uuid The unique identifier to parse.\n        century The first 2 digits to assume in the year. Default is '20'.\n\n    Examples:\n        >>> uuid_to_date('e8820616-1462-49b6-9784-e99a32120201')\n        datetime.date(2012, 2, 1)\n\n        >>> uuid_to_date('e8820616-1462-49b6-9784-e99a32120201', '18')\n        datetime.date(1812, 2, 1)\n\n    \"\"\"\n", "input": "", "output": "    day = int(uuid[-2:])\n    month = int(uuid[-4:-2])\n    year = int('%s%s' % (century, uuid[-6:-4]))\n\n    return datetime.date(year=year, month=month, day=day)", "category": "Python"}, {"instruction": "def learn(self, grad_arr, fix_opt_flag=False):\r\n        '''\r\n        Update this Discriminator by ascending its stochastic gradient.\r\n\r\n        Args:\r\n            grad_arr:       `np.ndarray` of gradients.\r\n            fix_opt_flag:   If `False`, no optimization in this model will be done.\r\n        \r\n        Returns:\r\n            `np.ndarray` of delta or gradients.\r\n        '''\n", "input": "", "output": "        if grad_arr.ndim != 2:\r\n            grad_arr = grad_arr.reshape((grad_arr.shape[0], -1))\r\n        delta_arr = self.__nn.back_propagation(grad_arr)\r\n        if fix_opt_flag is False:\r\n            self.__nn.optimize(self.__learning_rate, 1)\r\n        \r\n        return delta_arr", "category": "Python"}, {"instruction": "def parent(self):\n        \"\"\"Get the parent package.\n\n        Returns:\n            `Package`.\n        \"\"\"\n", "input": "", "output": "        if self._parent is not None:\n            return self._parent\n\n        try:\n            package = self.repository.get_parent_package(self.resource)\n            self._parent = Package(package, context=self.context)\n        except AttributeError as e:\n            reraise(e, ValueError)\n\n        return self._parent", "category": "Python"}, {"instruction": "def virtual_machines_list_all(**kwargs):\n    '''\n    .. versionadded:: 2019.2.0\n\n    List all virtual machines within a subscription.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_compute.virtual_machines_list_all\n\n    '''\n", "input": "", "output": "    result = {}\n    compconn = __utils__['azurearm.get_client']('compute', **kwargs)\n    try:\n        vms = __utils__['azurearm.paged_object_to_list'](\n            compconn.virtual_machines.list_all()\n        )\n        for vm in vms:  # pylint: disable=invalid-name\n            result[vm['name']] = vm\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('compute', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result", "category": "Python"}, {"instruction": "def parse_packet(packet):\n    \"\"\"Parse a beacon advertisement packet.\"\"\"\n", "input": "", "output": "    frame = parse_ltv_packet(packet)\n    if frame is None:\n        frame = parse_ibeacon_packet(packet)\n    return frame", "category": "Python"}, {"instruction": "def _poor_convergence(z, r, f, bn, mvec):\n    \"\"\"\n    Test for poor convergence based on three function evaluations.\n\n    This test evaluates the function at the three points and returns false if\n    the relative error is greater than 1e-3.\n    \"\"\"\n", "input": "", "output": "    check_points = (-0.4 + 0.3j, 0.7 + 0.2j, 0.02 - 0.06j)\n    diffs = []\n    ftests = []\n    for check_point in check_points:\n        rtest = r * check_point\n        ztest = z + rtest\n        ftest = f(ztest)\n        # Evaluate powerseries:\n        comp = np.sum(bn * np.power(check_point, mvec))\n        ftests.append(ftest)\n        diffs.append(comp - ftest)\n\n    max_abs_error = np.max(np.abs(diffs))\n    max_f_value = np.max(np.abs(ftests))\n    return max_abs_error > 1e-3 * max_f_value", "category": "Python"}, {"instruction": "def clone(zone, source, snapshot=None):\n    '''\n    Install a zone by copying an existing installed zone.\n\n    zone : string\n        name of the zone\n    source : string\n        zone to clone from\n    snapshot : string\n        optional name of snapshot to use as source\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zoneadm.clone clementine dolores\n    '''\n", "input": "", "output": "    ret = {'status': True}\n\n    ## install zone\n    res = __salt__['cmd.run_all']('zoneadm -z {zone} clone {snapshot}{source}'.format(\n        zone=zone,\n        source=source,\n        snapshot='-s {0} '.format(snapshot) if snapshot else '',\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    ret['message'] = ret['message'].replace('zoneadm: ', '')\n    if ret['message'] == '':\n        del ret['message']\n\n    return ret", "category": "Python"}, {"instruction": "def q_prior(q, m=1, gamma=0.3, qmin=0.1):\n    \"\"\"Default prior on mass ratio q ~ q^gamma\n    \"\"\"\n", "input": "", "output": "    if q < qmin or q > 1:\n        return 0\n    C = 1/(1/(gamma+1)*(1 - qmin**(gamma+1)))\n    return C*q**gamma", "category": "Python"}, {"instruction": "def simplify_script(self) -> 'Language':\n        \"\"\"\n        Remove the script from some parsed language data, if the script is\n        redundant with the language.\n\n        >>> Language.make(language='en', script='Latn').simplify_script()\n        Language.make(language='en')\n\n        >>> Language.make(language='yi', script='Latn').simplify_script()\n        Language.make(language='yi', script='Latn')\n\n        >>> Language.make(language='yi', script='Hebr').simplify_script()\n        Language.make(language='yi')\n        \"\"\"\n", "input": "", "output": "        if self._simplified is not None:\n            return self._simplified\n\n        if self.language and self.script:\n            if DEFAULT_SCRIPTS.get(self.language) == self.script:\n                result = self.update_dict({'script': None})\n                self._simplified = result\n                return self._simplified\n\n        self._simplified = self\n        return self._simplified", "category": "Python"}, {"instruction": "def value(self):\n    \"\"\"The current value of the mean.\"\"\"\n", "input": "", "output": "    return self._sum / tf.cast(self._count, self._dtype)", "category": "Python"}, {"instruction": "def restore(self, request):\n        \"\"\" Push the request back onto the queue.\n\n        Args:\n            request (Request): Reference to a request object that should be pushed back\n                               onto the request queue.\n        \"\"\"\n", "input": "", "output": "        self._connection.connection.rpush(self._request_key, pickle.dumps(request))", "category": "Python"}, {"instruction": "def get_dict_column(dict_, colx):\n    r\"\"\"\n    Args:\n        dict_ (dict_): a dictionary of lists\n        colx (int):\n\n    CommandLine:\n        python -m utool.util_dict --test-get_dict_column\n\n    Example:\n        >>> # ENABLE_DOCTEST\n        >>> from utool.util_dict import *  # NOQA\n        >>> import utool as ut\n        >>> dict_ = {'a': [0, 1, 2], 'b': [3, 4, 5], 'c': [6, 7, 8]}\n        >>> colx = [2, 0]\n        >>> retdict_ = get_dict_column(dict_, colx)\n        >>> result = ut.repr2(retdict_)\n        >>> print(result)\n        {'a': [2, 0], 'b': [5, 3], 'c': [8, 6]}\n    \"\"\"\n", "input": "", "output": "    retdict_ = {key: util_list.list_take(val, colx)\n                for key, val in six.iteritems(dict_)}\n    return retdict_", "category": "Python"}, {"instruction": "def zoom(self, zoom=None):\n        \"\"\"Zooms to zoom factor\"\"\"\n", "input": "", "output": "\n        status = True\n\n        if zoom is None:\n            zoom = self.grid.grid_renderer.zoom\n            status = False\n\n        # Zoom factor for grid content\n        self.grid.grid_renderer.zoom = zoom\n\n        # Zoom grid labels\n        self._zoom_labels(zoom)\n\n        # Zoom rows and columns\n        self._zoom_rows(zoom)\n        self._zoom_cols(zoom)\n\n        if self.main_window.IsFullScreen():\n            # Do not display labels in fullscreen mode\n            self.main_window.handlers.row_label_size = \\\n                self.grid.GetRowLabelSize()\n            self.main_window.handlers.col_label_size = \\\n                self.grid.GetColLabelSize()\n\n            self.grid.HideRowLabels()\n            self.grid.HideColLabels()\n\n        self.grid.ForceRefresh()\n\n        if status:\n            statustext = _(u\"Zoomed to {0:.2f}.\").format(zoom)\n\n            post_command_event(self.main_window, self.StatusBarMsg,\n                               text=statustext)", "category": "Python"}, {"instruction": "def do_load(self, filename):\n        \"\"\"Load disk image for analysis\"\"\"\n", "input": "", "output": "        try:\n            self.__session.load(filename)\n        except IOError as e:\n            self.logger.error(e.strerror)", "category": "Python"}, {"instruction": "def complete_upload(self):\n        \"\"\"\n        Complete the MultiPart Upload operation.  This method should\n        be called when all parts of the file have been successfully\n        uploaded to S3.\n\n        :rtype: :class:`boto.s3.multipart.CompletedMultiPartUpload`\n        :returns: An object representing the completed upload.\n        \"\"\"\n", "input": "", "output": "        xml = self.to_xml()\n        return self.bucket.complete_multipart_upload(self.key_name,\n                                                     self.id, xml)", "category": "Python"}, {"instruction": "def register_backend(cls: Type[StorageBackend]):\n    \"\"\"Decorator to register another StorageBackend using it's `NAME`.\"\"\"\n", "input": "", "output": "    if not issubclass(cls, StorageBackend):\n        raise TypeError(\"cls must be a subclass of StorageBackend\")\n    __registry__[cls.NAME] = cls\n    return cls", "category": "Python"}, {"instruction": "def money_receipts(pronac, dt):\n    \"\"\"\n    Checks how many items are in a same receipt when payment type is\n    withdraw/money\n        - is_outlier: True if there are any receipts that have more than one\n        - itens_que_compartilham_comprovantes: List of items that share receipt\n    \"\"\"\n", "input": "", "output": "    df = verified_repeated_receipts_for_pronac(pronac)\n    comprovantes_saque = df[df['tpFormaDePagamento'] == 3.0]\n\n    return metric_return(comprovantes_saque)", "category": "Python"}, {"instruction": "def ensure_vm_running(self, vm_location):\n        \"\"\" Gets or creates a Vagrantfile in ``vm_location`` and calls ``vagrant up`` if the VM is not running.\n        \"\"\"\n", "input": "", "output": "        import vagrant\n\n        if self.vagrant is None:\n            vagrant_file = vm_location / \"Vagrantfile\"\n            if not vagrant_file.exists():\n                self.init_vagrant(vagrant_file)\n\n            self.vagrant = vagrant.Vagrant(vm_location,\n                                           quiet_stdout=self.quiet,\n                                           quiet_stderr=self.quiet,\n                                           out_cm=self.log_cm,\n                                           err_cm=self.log_cm)\n\n        status = [x for x in self.vagrant.status() if x.name == \"default\"][0]\n\n        if status.state != \"running\":\n            try:\n                self.vagrant.up()\n            except subprocess.CalledProcessError:\n                raise BuildError(\"Vagrant VM couldn't up launched. See output for details.\")", "category": "Python"}, {"instruction": "def write_banner(title, text='&nbsp;'):\n    \"\"\"\n    Write html <title> tag into markup.page object\n    \"\"\"\n", "input": "", "output": "\n    page = markup.page(mode=\"strict_html\")\n    page._escape = False\n\n    page.div(id=\"header\")\n    page.h1()\n    page.add(title)\n    page.h1.close()\n    page.h3()\n    page.add(text)\n    page.h3.close()\n\n    page.hr(class_=\"short\")\n    page.hr(class_=\"long\")\n\n    page.div.close()\n\n    page.div(id=\"container\")\n\n    return page", "category": "Python"}, {"instruction": "def _prepare_record(record, index, doc_type):\n        \"\"\"Prepare record data for indexing.\n\n        :param record: The record to prepare.\n        :param index: The Elasticsearch index.\n        :param doc_type: The Elasticsearch document type.\n        :returns: The record metadata.\n        \"\"\"\n", "input": "", "output": "        if current_app.config['INDEXER_REPLACE_REFS']:\n            data = copy.deepcopy(record.replace_refs())\n        else:\n            data = record.dumps()\n\n        data['_created'] = pytz.utc.localize(record.created).isoformat() \\\n            if record.created else None\n        data['_updated'] = pytz.utc.localize(record.updated).isoformat() \\\n            if record.updated else None\n\n        # Allow modification of data prior to sending to Elasticsearch.\n        before_record_index.send(\n            current_app._get_current_object(),\n            json=data,\n            record=record,\n            index=index,\n            doc_type=doc_type,\n        )\n\n        return data", "category": "Python"}, {"instruction": "def normalize_select(query):\n    \"\"\"\n    If the query contains the :select operator, we enforce :sys properties.\n    The SDK requires sys.type to function properly, but as other of our\n    SDKs require more parts of the :sys properties, we decided that every\n    SDK should include the complete :sys block to provide consistency\n    accross our SDKs.\n    \"\"\"\n", "input": "", "output": "\n    if 'select' not in query:\n        return\n\n    if isinstance(\n            query['select'],\n            str_type()):\n        query['select'] = [s.strip() for s in query['select'].split(',')]\n\n    query['select'] = [s for s\n                       in query['select']\n                       if not s.startswith('sys.')]\n\n    if 'sys' not in query['select']:\n        query['select'].append('sys')", "category": "Python"}, {"instruction": "def report(self, stream):\n        \"\"\"Writes an Xunit-formatted XML file\n\n        The file includes a report of test errors and failures.\n\n        \"\"\"\n", "input": "", "output": "        from collections import OrderedDict\n        self.stats['total'] = sum(self.stats.values())\n        for group in self.report_data.values():\n            group.stats['total'] = sum(group.stats.values())\n        self.report_file.write(self.jinja.get_template('report.html').render(\n            report=OrderedDict(sorted(self.report_data.items())),\n            stats=self.stats,\n        ))\n        self.report_file.close()\n        if self.config.verbosity > 1:\n            stream.writeln(\"-\" * 70)\n            stream.writeln(\"HTML: %s\" % self.report_file.name)", "category": "Python"}, {"instruction": "def get_current_project(self):\n        \"\"\"\n        Get name of current project using `oc project` command.\n        Raise ConuException in case of an error.\n        :return: str, project name\n        \"\"\"\n", "input": "", "output": "\n        try:\n            command = self._oc_command([\"project\", \"-q\"])\n            output = run_cmd(command, return_output=True)\n        except subprocess.CalledProcessError as ex:\n            raise ConuException(\"Failed to obtain current project name : %s\" % ex)\n\n        try:\n            return output.rstrip()  # remove '\\n'\n        except IndexError:\n            raise ConuException(\"Failed to obtain project name\")", "category": "Python"}, {"instruction": "def get(self, request, uri):\n        \"\"\"\n        List uri revisions.\n\n        JSON Response:\n            [[uri, state], ...]\n        \"\"\"\n", "input": "", "output": "        uri = self.decode_uri(uri)\n        revisions = cio.revisions(uri)\n        revisions = [list(revision) for revision in revisions]  # Convert tuples to lists\n        return self.render_to_json(revisions)", "category": "Python"}, {"instruction": "def transform(self, X):\n        '''\n        Transform the segmented time series data into feature data.\n        If contextual data is included in X, it is returned with the feature data.\n\n        Parameters\n        ----------\n        X : array-like, shape [n_series, ...]\n            Segmented time series data and (optionally) contextual data\n\n        Returns\n        -------\n        X_new : array shape [n_series, ...]\n            Feature representation of segmented time series data and contextual data\n\n        '''\n", "input": "", "output": "        self._check_if_fitted()\n        Xt, Xc = get_ts_data_parts(X)\n        check_array(Xt, dtype='numeric', ensure_2d=False, allow_nd=True)\n\n        fts = np.column_stack([self.features[f](Xt) for f in self.features])\n        if Xc is not None:\n            fts = np.column_stack([fts, Xc])\n        return fts", "category": "Python"}, {"instruction": "def position_after_whitespace(self, body: str, start_position: int) -> int:\n        \"\"\"Go to next position after a whitespace.\n\n        Reads from body starting at start_position until it finds a non-whitespace\n        character, then returns the position of that character for lexing.\n        \"\"\"\n", "input": "", "output": "        body_length = len(body)\n        position = start_position\n        while position < body_length:\n            char = body[position]\n            if char in \" \\t,\\ufeff\":\n                position += 1\n            elif char == \"\\n\":\n                position += 1\n                self.line += 1\n                self.line_start = position\n            elif char == \"\\r\":\n                if body[position + 1 : position + 2] == \"\\n\":\n                    position += 2\n                else:\n                    position += 1\n                self.line += 1\n                self.line_start = position\n            else:\n                break\n        return position", "category": "Python"}, {"instruction": "def get_all_alert(self, **kwargs):  # noqa: E501\n        \"\"\"Get all alerts for a customer  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.get_all_alert(async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param int offset:\n        :param int limit:\n        :return: ResponseContainerPagedAlert\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.get_all_alert_with_http_info(**kwargs)  # noqa: E501\n        else:\n            (data) = self.get_all_alert_with_http_info(**kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def set_regularizers(self, regularizers, clear=True):\n        \"\"\"\n        Adds a set of regularizers\n\n        Parameters\n        ----------\n        regularizers : dict\n            Each key is the name of a corresponding proximal operator, and the\n            value associated with that key is a set of keyword arguments\n\n        clear : boolean, optional\n            Whether or not to clear the existing regularizers. (Default: True)\n\n        \"\"\"\n", "input": "", "output": "\n        # clear existing operators\n        if clear:\n            self.clear()\n\n        # add new regularizers\n        list([self.add_regularizer(proxfun, **regularizers[proxfun])\n              for proxfun in regularizers.keys()])", "category": "Python"}, {"instruction": "def import_string(dotted_path):\n    \"\"\"\n    Import a dotted module path and return the attribute/class designated by\n    the last name in the path. Raise ImportError if the import failed.\n    \"\"\"\n", "input": "", "output": "    try:\n        module_path, class_name = dotted_path.rsplit('.', 1)\n    except ValueError:\n        msg = \"%s doesn't look like a module path\" % dotted_path\n        six.reraise(ImportError, ImportError(msg), sys.exc_info()[2])\n\n    module = import_module(module_path)\n\n    try:\n        return getattr(module, class_name)\n    except AttributeError:\n        msg = 'Module \"%s\" does not define a \"%s\" attribute/class' % (\n            dotted_path, class_name\n        )\n        six.reraise(ImportError, ImportError(msg), sys.exc_info()[2])", "category": "Python"}, {"instruction": "def create(cls, zone_id, record):\n        \"\"\"Create a new zone version for record.\"\"\"\n", "input": "", "output": "        cls.echo('Creating new zone version')\n        new_version_id = Zone.new(zone_id)\n\n        cls.echo('Updating zone version')\n        cls.add(zone_id, new_version_id, record)\n\n        cls.echo('Activation of new zone version')\n        Zone.set(zone_id, new_version_id)\n\n        return new_version_id", "category": "Python"}, {"instruction": "def iter_user_teams(self, number=-1, etag=None):\n        \"\"\"Gets the authenticated user's teams across all of organizations.\n\n        List all of the teams across all of the organizations to which the\n        authenticated user belongs. This method requires user or repo scope\n        when authenticating via OAuth.\n\n        :returns: generator of :class:`Team <github3.orgs.Team>` objects\n        \"\"\"\n", "input": "", "output": "        url = self._build_url('user', 'teams')\n        return self._iter(int(number), url, Team, etag=etag)", "category": "Python"}, {"instruction": "def parse(self, data):\n        \"\"\"Parse bulk cymru data\"\"\"\n", "input": "", "output": "\n        ASNlist = []\n        for line in data.splitlines()[1:]:\n            line = plain_str(line)\n            if \"|\" not in line:\n                continue\n            asn, ip, desc = [elt.strip() for elt in line.split('|')]\n            if asn == \"NA\":\n                continue\n            asn = \"AS%s\" % asn\n            ASNlist.append((ip, asn, desc))\n        return ASNlist", "category": "Python"}, {"instruction": "def runner(fun, arg=None, timeout=5):\n    '''\n    Execute a runner on the master and return the data from the runnr function\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-ssh '*' publish.runner jobs.lookup_jid 20140916125524463507\n    '''\n", "input": "", "output": "    # Form args as list\n    if not isinstance(arg, list):\n        arg = [salt.utils.args.yamlify_arg(arg)]\n    else:\n        arg = [salt.utils.args.yamlify_arg(x) for x in arg]\n    if len(arg) == 1 and arg[0] is None:\n        arg = []\n\n    # Create and run the runner\n    runner = salt.runner.RunnerClient(__opts__['__master_opts__'])\n    return runner.cmd(fun, arg)", "category": "Python"}, {"instruction": "def resample_signal(self, data_frame):\n        \"\"\"\n            Convenience method for frequency conversion and resampling of data frame. \n            Object must have a DatetimeIndex. After re-sampling, this methods interpolate the time magnitude sum \n            acceleration values and the x,y,z values of the data frame acceleration\n\n            :param data_frame: the data frame to resample\n            :type data_frame: pandas.DataFrame\n            :return: the resampled data frame\n            :rtype: pandas.DataFrame\n\n        \"\"\"\n", "input": "", "output": "        df_resampled = data_frame.resample(str(1 / self.sampling_frequency) + 'S').mean()\n\n        f = interpolate.interp1d(data_frame.td, data_frame.mag_sum_acc)\n        new_timestamp = np.arange(data_frame.td[0], data_frame.td[-1], 1.0 / self.sampling_frequency)\n        df_resampled.mag_sum_acc = f(new_timestamp)\n\n        logging.debug(\"resample signal\")\n        return df_resampled.interpolate(method='linear')", "category": "Python"}, {"instruction": "def record_calculation_start(self, variable_name, period, **parameters):\n        \"\"\"\n            Record that OpenFisca started computing a variable.\n\n            :param str variable_name: Name of the variable starting to be computed\n            :param Period period: Period for which the variable is being computed\n            :param list parameters: Parameter with which the variable is being computed\n        \"\"\"\n", "input": "", "output": "        key = self._get_key(variable_name, period, **parameters)\n\n        if self.stack:  # The variable is a dependency of another variable\n            parent = self.stack[-1]\n            self.trace[parent]['dependencies'].append(key)\n        else:  # The variable has been requested by the client\n            self.requested_calculations.add(key)\n\n        if not self.trace.get(key):\n            self.trace[key] = {'dependencies': [], 'parameters': {}}\n\n        self.stack.append(key)\n        self._computation_log.append((key, len(self.stack)))\n        self.usage_stats[variable_name]['nb_requests'] += 1", "category": "Python"}, {"instruction": "def _do_cb(self, task, cb, error_cb, *args, **kw):\n        \"\"\"\n        Called internally by callback(). Does cb and error_cb selection.\n        \"\"\"\n", "input": "", "output": "        try:\n            res = self.work(task, *args, **kw)\n        except BackendProcessingError as e:\n            if error_cb is None:\n                self.log(ERROR, e.__traceback__)\n                show_err()\n            elif error_cb:\n                error_cb(e)\n        else:\n            # Success, let's call away!\n            cb(res)", "category": "Python"}, {"instruction": "def read_dict_from_file(file_path):\n    \"\"\"\n    Read a dictionary of strings from a file\n    \"\"\"\n", "input": "", "output": "    with open(file_path) as file:\n        lines = file.read().splitlines()\n\n    obj = {}\n    for line in lines:\n        key, value = line.split(':', maxsplit=1)\n        obj[key] = eval(value)\n\n    return obj", "category": "Python"}, {"instruction": "def isNXEnabled(self):\n        \"\"\"\n        Determines if the current L{PE} instance has the NXCOMPAT (Compatible with Data Execution Prevention) flag enabled.\n        @see: U{http://msdn.microsoft.com/en-us/library/ms235442.aspx}\n\n        @rtype: bool\n        @return: Returns C{True} if the current L{PE} instance has the NXCOMPAT flag enabled. Otherwise, returns C{False}.\n        \"\"\"\n", "input": "", "output": "        return self.ntHeaders.optionalHeader.dllCharacteristics.value & consts.IMAGE_DLL_CHARACTERISTICS_NX_COMPAT == consts.IMAGE_DLL_CHARACTERISTICS_NX_COMPAT", "category": "Python"}, {"instruction": "def qmed_all_methods(self):\n        \"\"\"\n        Returns a dict of QMED methods using all available methods.\n\n        Available methods are defined in :attr:`qmed_methods`. The returned dict keys contain the method name, e.g.\n        `amax_record` with value representing the corresponding QMED estimate in m\u00b3/s.\n\n        :return: dict of QMED estimates\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        result = {}\n        for method in self.methods:\n            try:\n                result[method] = getattr(self, '_qmed_from_' + method)()\n            except:\n                result[method] = None\n        return result", "category": "Python"}, {"instruction": "def init_widget(self):\n        \"\"\" Initialize the underlying widget.\n\n        \"\"\"\n", "input": "", "output": "        super(AndroidActionMenuView, self).init_widget()\n        d = self.declaration\n        w = self.widget\n\n        #: Kinda hackish, but when we get the menu back, load it\n        w.getMenu().then(self.on_menu)\n        w.setOnMenuItemClickListener(w.getId())\n        w.onMenuItemClick.connect(self.on_menu_item_click)", "category": "Python"}, {"instruction": "def _xml_to_dict(xml_to_convert):\n    \"\"\"\n    Convert RAW XML string to Python dict\n    :param xml_to_convert: XML to convert (string/text)\n    :return: Python dict with all XML data\n    \"\"\"\n", "input": "", "output": "\n    logger.debug(\"Converting to Python dict this XML: \" + str(xml_to_convert))\n    return xmltodict.parse(xml_to_convert, attr_prefix='')", "category": "Python"}, {"instruction": "def parse_single(str_):\n    \"\"\"\n    Very simple parser to parse expressions represent some single values.\n\n    :param str_: a string to parse\n    :return: Int | Bool | String\n\n    >>> parse_single(None)\n    ''\n    >>> parse_single(\"0\")\n    0\n    >>> parse_single(\"123\")\n    123\n    >>> parse_single(\"True\")\n    True\n    >>> parse_single(\"a string\")\n    'a string'\n    >>> parse_single('\"a string\"')\n    'a string'\n    >>> parse_single(\"'a string'\")\n    'a string'\n    >>> parse_single(\"0.1\")\n    '0.1'\n    >>> parse_single(\"    a string contains extra whitespaces     \")\n    'a string contains extra whitespaces'\n    \"\"\"\n", "input": "", "output": "    if str_ is None:\n        return ''\n\n    str_ = str_.strip()\n\n    if not str_:\n        return ''\n\n    if BOOL_PATTERN.match(str_) is not None:\n        return bool(str_)\n\n    if INT_PATTERN.match(str_) is not None:\n        return int(str_)\n\n    if STR_PATTERN.match(str_) is not None:\n        return str_[1:-1]\n\n    return str_", "category": "Python"}, {"instruction": "def run_migrations_online():\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n", "input": "", "output": "    print(\"Running migrations online\")\n\n    project_path = get_project_path()\n    project_config = get_project_config(project_path)\n\n    backend = get_backend(project_path,project_config,initialize_db = False)\n\n    context.configure(\n        connection=backend.connection,\n        target_metadata=backend.metadata,\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()", "category": "Python"}, {"instruction": "def API520_F2(k, P1, P2):\n    r'''Calculates coefficient F2 for subcritical flow for use in API 520\n    subcritical flow relief valve sizing.\n\n    .. math::\n        F_2 = \\sqrt{\\left(\\frac{k}{k-1}\\right)r^\\frac{2}{k}\n        \\left[\\frac{1-r^\\frac{k-1}{k}}{1-r}\\right]}\n\n    .. math::\n        r = \\frac{P_2}{P_1}\n\n    Parameters\n    ----------\n    k : float\n        Isentropic coefficient or ideal gas heat capacity ratio [-]\n    P1 : float\n        Upstream relieving pressure; the set pressure plus the allowable\n        overpressure, plus atmospheric pressure, [Pa]\n    P2 : float\n        Built-up backpressure; the increase in pressure during flow at the\n        outlet of a pressure-relief device after it opens, [Pa]\n\n    Returns\n    -------\n    F2 : float\n        Subcritical flow coefficient `F2` [-]\n\n    Notes\n    -----\n    F2 is completely dimensionless.\n\n    Examples\n    --------\n    From [1]_ example 2, matches.\n\n    >>> API520_F2(1.8, 1E6, 7E5)\n    0.8600724121105563\n\n    References\n    ----------\n    .. [1] API Standard 520, Part 1 - Sizing and Selection.\n    '''\n", "input": "", "output": "    r = P2/P1\n    return ( k/(k-1)*r**(2./k) * ((1-r**((k-1.)/k))/(1.-r)) )**0.5", "category": "Python"}, {"instruction": "def get_grade_entry_form(self, *args, **kwargs):\n        \"\"\"Pass through to provider GradeEntryAdminSession.get_grade_entry_form_for_update\"\"\"\n", "input": "", "output": "        # Implemented from kitosid template for -\n        # osid.resource.ResourceAdminSession.get_resource_form_for_update\n        # This method might be a bit sketchy. Time will tell.\n        if isinstance(args[-1], list) or 'grade_entry_record_types' in kwargs:\n            return self.get_grade_entry_form_for_create(*args, **kwargs)\n        else:\n            return self.get_grade_entry_form_for_update(*args, **kwargs)", "category": "Python"}, {"instruction": "def save(url, destination):\n    \"\"\"\n    This is just the thread target.\n    It's actually responsible for downloading and saving.\n\n    :param str url: which dump to download\n    :param str destination: a file path to save to\n    \"\"\"\n", "input": "", "output": "    r = requests.get(url, stream=True)\n\n    with open(destination, 'wb') as fd:\n        for chunk in r.iter_content(chunk_size=128):\n            fd.write(chunk)", "category": "Python"}, {"instruction": "def tag(self, tokens):\n        \"\"\"Return a list of (token, tag) tuples for a given list of tokens.\"\"\"\n", "input": "", "output": "        tags = []\n        for token in tokens:\n            normalized = self.lexicon[token].normalized\n            for regex, tag in self.regexes:\n                if regex.match(normalized):\n                    tags.append((token, tag))\n                    break\n            else:\n                tags.append((token, None))\n        return tags", "category": "Python"}, {"instruction": "def find_first_file_with_ext(base_paths, prefix, exts):\n    \"\"\"Runs through the given list of file extensions and returns the first file with the given base\n    path and extension combination that actually exists.\n\n    Args:\n        base_paths: The base paths in which to search for files.\n        prefix: The filename prefix of the file for which to search.\n        exts: An ordered list of file extensions for which to search.\n\n    Returns:\n        On success, a 2-tuple containing the base path in which the file was found, and the extension of the file.\n        On failure, returns (None, None).\n    \"\"\"\n", "input": "", "output": "    for base_path in base_paths:\n        for ext in exts:\n            filename = os.path.join(base_path, \"%s%s\" % (prefix, ext))\n            if os.path.exists(filename) and os.path.isfile(filename):\n                logger.debug(\"Found first file with relevant extension: %s\", filename)\n                return base_path, ext\n\n    logger.debug(\"No files found for prefix %s, extensions %s\", prefix, \", \".join(exts))\n    return None, None", "category": "Python"}, {"instruction": "def destroy(self):\r\n        \"\"\"\r\n        Destroyes this item by disconnecting any signals that may exist.  This\r\n        is called when the tree clears itself or is deleted.  If you are\r\n        manually removing an item, you should call the destroy method yourself.\r\n        This is required since Python allows for non-QObject connections, and\r\n        since QTreeWidgetItem's are not QObjects', they do not properly handle\r\n        being destroyed with connections on them.\r\n        \"\"\"\n", "input": "", "output": "        try:\r\n            tree = self.treeWidget()\r\n            tree.destroyed.disconnect(self.destroy)\r\n        except StandardError:\r\n            pass\r\n        \r\n        for movie in set(self._movies.values()):\r\n            try:\r\n                movie.frameChanged.disconnect(self._updateFrame)\r\n            except StandardError:\r\n                pass", "category": "Python"}, {"instruction": "def create_hls_profile(apps, schema_editor):\n    \"\"\" Create hls profile \"\"\"\n", "input": "", "output": "    Profile = apps.get_model(\"edxval\", \"Profile\")\n    Profile.objects.get_or_create(profile_name=HLS_PROFILE)", "category": "Python"}, {"instruction": "def _getDeltas(self, firstSub, secondSub):\n        \"\"\"Arguments must have \"start\" and \"end\" properties which are FrameTimes.\"\"\"\n", "input": "", "output": "        startDelta = max(firstSub.start, secondSub.start) - min(firstSub.start, secondSub.start)\n        endDelta = max(firstSub.end, secondSub.end) - min(firstSub.end, secondSub.end)\n        return (startDelta, endDelta)", "category": "Python"}, {"instruction": "def cosi_posterior(vsini_dist,veq_dist,vgrid=None,npts=100,vgrid_pts=1000):\n    \"\"\"returns posterior of cosI given dists for vsini and veq (incorporates unc. in vsini)\n    \"\"\"\n", "input": "", "output": "    if vgrid is None:\n        vgrid = np.linspace(min(veq_dist.ppf(0.001),vsini_dist.ppf(0.001)),\n                            max(veq_dist.ppf(0.999),vsini_dist.ppf(0.999)),\n                            vgrid_pts)\n        logging.debug('vgrid: {} pts, {} to {}'.format(vgrid_pts,\n                                                       vgrid[0],\n                                                       vgrid[-1]))\n        #vgrid = np.linspace(vsini_dist.ppf(0.005),vsini_dist.ppf(0.995),vgrid_pts)\n    cs = np.linspace(0,1,npts)\n    Ls = cs*0\n    for i,c in enumerate(cs):\n        Ls[i] = like_cosi(c,vsini_dist,veq_dist,vgrid=vgrid)\n    if np.isnan(Ls[-1]): #hack to prevent nan when cos=1\n        Ls[-1] = Ls[-2]\n    Ls /= np.trapz(Ls,cs)\n    return cs,Ls", "category": "Python"}, {"instruction": "def options_policy(skip_invalid, warn_on_skip):\n    \"\"\"\n    Context manager to temporarily set the skip_invalid and warn_on_skip\n    class parameters on Options.\n    \"\"\"\n", "input": "", "output": "    settings = (Options.skip_invalid, Options.warn_on_skip)\n    (Options.skip_invalid, Options.warn_on_skip) = (skip_invalid, warn_on_skip)\n    yield\n    (Options.skip_invalid, Options.warn_on_skip) = settings", "category": "Python"}, {"instruction": "def filter_single_need(need, filter_string=\"\"):\n    \"\"\"\n    Checks if a single need/need_part passes a filter_string\n\n    :param need: need or need_part\n    :param filter_string: string, which is used as input for eval()\n    :return: True, if need as passed the filter_string, else False\n    \"\"\"\n", "input": "", "output": "    filter_context = need.copy()\n    filter_context[\"search\"] = re.search\n    result = False\n    try:\n        result = bool(eval(filter_string, None, filter_context))\n    except Exception as e:\n        raise NeedInvalidFilter(\"Filter {0} not valid: Error: {1}\".format(filter_string, e))\n    return result", "category": "Python"}, {"instruction": "def send(self, url, data, headers):\n        \"\"\"\n        Sends a request to an SQS queue -- to be later popped off\n        later for submission into Sentry.\n        Note: This will simply raise any Boto ClientErrors that are encountered.\n        \"\"\"\n", "input": "", "output": "        if not self.queue_url:\n            self.queue_url = self.sqs_client.get_queue_url(QueueName=self.sqs_name,\n                                                           QueueOwnerAWSAccountId=self.sqs_account)[\"QueueUrl\"]\n\n        payload = {\n            \"url\": url,\n            \"headers\": headers,\n            \"data\": base64.b64encode(data).decode(\"utf-8\")\n        }\n\n        try:\n            self.sqs_client.send_message(QueueUrl=self.queue_url,\n                                         MessageBody=json.dumps(payload))\n        except ClientError as e:\n            # TODO break circular dep for logging\n            print(data)  # not sure if this is the best way to get messages.\n            raise e", "category": "Python"}, {"instruction": "def axes_grid(n, sharex=False, sharey=False, subplot_kw=None, **fig_kw):\n  '''Finds a reasonable arrangement of n axes. Returns (fig, axes) tuple.\n  For keyword arguments descriptions, see matplotlib.pyplot.subplots'''\n", "input": "", "output": "  r = np.floor(np.sqrt(n))\n  r, c = int(r), int(np.ceil(n / r))\n  fig, axes = plt.subplots(nrows=r, ncols=c, figsize=(c*4, r*4), squeeze=False,\n                           sharex=sharex, sharey=sharey,\n                           subplot_kw=subplot_kw, **fig_kw)\n  # Turn off any extra axes\n  for ax in axes.flat[n:]:\n    ax.set_axis_off()\n  return fig, axes", "category": "Python"}, {"instruction": "def delete_image(self, subreddit, name=None, header=False):\n        \"\"\"Delete an image from the subreddit.\n\n        :param name: The name of the image if removing a CSS image.\n        :param header: When true, delete the subreddit header.\n        :returns: The json response from the server.\n\n        \"\"\"\n", "input": "", "output": "        subreddit = six.text_type(subreddit)\n        if name and header:\n            raise TypeError('Both name and header cannot be set.')\n        elif name:\n            data = {'img_name': name}\n            url = self.config['delete_sr_image']\n            self.evict(self.config['stylesheet'].format(subreddit=subreddit))\n        else:\n            data = True\n            url = self.config['delete_sr_header']\n        url = url.format(subreddit=subreddit)\n        return self.request_json(url, data=data)", "category": "Python"}, {"instruction": "def cprint(text, color=None, on_color=None, attrs=None, **kwargs):\n    \"\"\"Print colorize text.\n\n    It accepts arguments of print function.\n    \"\"\"\n", "input": "", "output": "    try:\n        print((colored(text, color, on_color, attrs)), **kwargs)\n    except TypeError:\n        # flush is not supported by py2.7\n        kwargs.pop(\"flush\", None)\n        print((colored(text, color, on_color, attrs)), **kwargs)", "category": "Python"}, {"instruction": "def weekend_tomorrow(self, date_from=None, date_format=None):\n\t\t\"\"\"\n\t\tRetourne la date de demain depuis maintenant ou depuis une date fournie\n\t\tseulement sur les jours de weekend.\n\t\tAinsi dimanche devient samedi et samedi devient dimanche\n\n\t\t:param: :date_from date de r\u00e9f\u00e9rence\n\t\t:return datetime\n\t\t\"\"\"\n", "input": "", "output": "\t\t# date de demain que sur les jours de week-end\n\t\treturn self.delta(days=1, date_from=date_from, date_format=date_format, days_range=[6, 7])", "category": "Python"}, {"instruction": "def qlognormal(mu, sigma, q, random_state):\n    '''\n    mu: float or array_like of floats\n    sigma: float or array_like of floats\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n", "input": "", "output": "    return np.round(lognormal(mu, sigma, random_state) / q) * q", "category": "Python"}, {"instruction": "def parse_substitutions(self, messages):\n        \"\"\"\n        Parse substitutions in a supplied message\n        :param messages: A tuple messages being parsed (normalized, case preserved, raw)\n        :type  messages: tuple of (str, str, str)\n\n        :return: Substituted messages (normalized, case preserved, raw)\n        :rtype : tuple of (str, str, str)\n        \"\"\"\n", "input": "", "output": "        # If no substitutions have been defined, just normalize the message\n        if not self._substitutions:\n            self._log.info('No substitutions to process')\n            return messages\n\n        self._log.info('Processing message substitutions')\n\n        def substitute(sub_group, sub_message):\n            word, substitution = sub_group\n            return word.sub(substitution, sub_message)\n\n        normalized, preserve_case, raw = messages\n        for sub_normalized, sub_preserve_case, sub_raw in self._substitutions:\n            normalized = substitute(sub_normalized, normalized)\n            preserve_case = substitute(sub_preserve_case, preserve_case)\n            raw = substitute(sub_raw, raw)\n\n        return normalized, preserve_case, raw", "category": "Python"}, {"instruction": "def print_tokens(self, tokens, style=None):\n        \"\"\"\n        Print a list of (Token, text) tuples to the output.\n        (When the UI is running, this method has to be called through\n        `run_in_terminal`, otherwise it will destroy the UI.)\n\n        :param style: Style class to use. Defaults to the active style in the CLI.\n        \"\"\"\n", "input": "", "output": "        print_tokens(self.output, tokens, style or self.application.style)", "category": "Python"}, {"instruction": "def flush_pending(function):\n    \"\"\"Attempt to acquire any pending locks.\n    \"\"\"\n", "input": "", "output": "    s = boto3.Session()\n    client = s.client('lambda')\n    results = client.invoke(\n        FunctionName=function,\n        Payload=json.dumps({'detail-type': 'Scheduled Event'})\n    )\n    content = results.pop('Payload').read()\n    pprint.pprint(results)\n    pprint.pprint(json.loads(content))", "category": "Python"}, {"instruction": "def create_model_table(self, model):\n        \"\"\"Creates the table for the given model.\n\n        Args:\n            model: A StatikModel instance.\n\n        Returns:\n            A SQLAlchemy model instance for the table corresponding to this\n            particular model.\n        \"\"\"\n", "input": "", "output": "        try:\n            return db_model_factory(self.Base, model, self.models)\n        except Exception as exc:\n            raise ModelError(\n                model.name,\n                message=\"failed to create in-memory table.\",\n                orig_exc=exc,\n                context=self.error_context\n            )", "category": "Python"}, {"instruction": "def plot_distance_landscape_projection(self, x_axis, y_axis, ax=None, *args, **kwargs):\n        \"\"\"\n        Plots the projection of distance landscape (if it was returned), onto the\n        parameters specified\n\n        :param x_axis: symbol to plot on x axis\n        :param y_axis: symbol to plot on y axis\n        :param ax: axis object to plot onto\n        :param args: arguments to pass to :func:`matplotlib.pyplot.contourf`\n        :param kwargs: keyword arguments to pass to :func:`matplotlib.pyplot.contourf`\n        :return:\n        \"\"\"\n", "input": "", "output": "        x, y, z = self.distance_landscape_as_3d_data(x_axis, y_axis)\n        plot_contour(x, y, z, x_axis, y_axis, ax=ax, *args, **kwargs)", "category": "Python"}, {"instruction": "def RCScan():\n    \"\"\"Return a prototype Scanner instance for scanning RC source files\"\"\"\n", "input": "", "output": "\n    res_re= r'^(?:\\s*#\\s*(?:include)|' \\\n            '.*?\\s+(?:ICON|BITMAP|CURSOR|HTML|FONT|MESSAGETABLE|TYPELIB|REGISTRY|D3DFX)' \\\n            '\\s*.*?)' \\\n            '\\s*(<|\"| )([^>\"\\s]+)(?:[>\"\\s])*$'\n    resScanner = SCons.Scanner.ClassicCPP(\"ResourceScanner\",\n                                          \"$RCSUFFIXES\",\n                                          \"CPPPATH\",\n                                          res_re,\n                                          recursive=no_tlb)\n    \n    return resScanner", "category": "Python"}, {"instruction": "def _render_rate_limit_page(self, exception=None):\n        \"\"\"\n        Renders the rate limit page.\n        \"\"\"\n", "input": "", "output": "        auth = request.args.get('auth')\n        is_auth = auth == '1' if auth else bool(self.auth)\n        return render_template('limit.html', is_authenticated=is_auth), 403", "category": "Python"}, {"instruction": "def _rotate_tr(self):\n        \"\"\"Rotate the transformation matrix based on camera parameters\"\"\"\n", "input": "", "output": "        rot, x, y, z = self._quaternion.get_axis_angle()\n        up, forward, right = self._get_dim_vectors()\n        self.transform.rotate(180 * rot / np.pi, (x, z, y))", "category": "Python"}, {"instruction": "def score_cosine(self, term1, term2, **kwargs):\n\n        \"\"\"\n        Compute a weighting score based on the cosine distance between the\n        kernel density estimates of two terms.\n\n        Args:\n            term1 (str)\n            term2 (str)\n\n        Returns: float\n        \"\"\"\n", "input": "", "output": "\n        t1_kde = self.kde(term1, **kwargs)\n        t2_kde = self.kde(term2, **kwargs)\n\n        return 1-distance.cosine(t1_kde, t2_kde)", "category": "Python"}, {"instruction": "def get_all_mfa_devices(self, user_name, marker=None, max_items=None):\n        \"\"\"\n        Get all MFA devices associated with an account.\n\n        :type user_name: string\n        :param user_name: The username of the user\n\n        :type marker: string\n        :param marker: Use this only when paginating results and only in\n                       follow-up request after you've received a response\n                       where the results are truncated.  Set this to the\n                       value of the Marker element in the response you\n                       just received.\n\n        :type max_items: int\n        :param max_items: Use this only when paginating results to indicate\n                          the maximum number of groups you want in the\n                          response.\n                          \n        \"\"\"\n", "input": "", "output": "        params = {'UserName' : user_name}\n        if marker:\n            params['Marker'] = marker\n        if max_items:\n            params['MaxItems'] = max_items\n        return self.get_response('ListMFADevices',\n                                 params, list_marker='MFADevices')", "category": "Python"}, {"instruction": "def get_track_by_mbid(self, mbid):\n        \"\"\"Looks up a track by its MusicBrainz ID\"\"\"\n", "input": "", "output": "\n        params = {\"mbid\": mbid}\n\n        doc = _Request(self, \"track.getInfo\", params).execute(True)\n\n        return Track(_extract(doc, \"name\", 1), _extract(doc, \"name\"), self)", "category": "Python"}, {"instruction": "def setup_kojiclient(profile):\n    \"\"\"Setup koji client session\n    \"\"\"\n", "input": "", "output": "    opts = koji.read_config(profile)\n    for k, v in opts.iteritems():\n        opts[k] = os.path.expanduser(v) if type(v) is str else v\n    kojiclient = koji.ClientSession(opts['server'], opts=opts)\n    kojiclient.ssl_login(opts['cert'], None, opts['serverca'])\n    return kojiclient", "category": "Python"}, {"instruction": "def merge(self, other):\n        \"\"\"\n        Combine this region with other.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(other, one):\n            other = one(other)\n        new = concatenate((self.coordinates, other.coordinates))\n        unique = set([tuple(x) for x in new.tolist()])\n        final = asarray([list(x) for x in unique])\n        return one(final)", "category": "Python"}, {"instruction": "def _add_err(self, exinfo):\n        \"\"\"\n        Sets the error on this MultiResult. Will be ignored if an error is\n        already set.\n        :param exinfo: Return value from ``sys.exc_info()``\n        \"\"\"\n", "input": "", "output": "        if self._err:\n            return\n        self._err = exinfo\n        self.all_ok = False", "category": "Python"}, {"instruction": "def url(self, name, data, data_type, **kwargs):\n        \"\"\"Serialize data intended for a URL path.\n\n        :param data: The data to be serialized.\n        :param str data_type: The type to be serialized from.\n        :rtype: str\n        :raises: TypeError if serialization fails.\n        :raises: ValueError if data is None\n        \"\"\"\n", "input": "", "output": "        if self.client_side_validation:\n            data = self.validate(data, name, required=True, **kwargs)\n        try:\n            output = self.serialize_data(data, data_type, **kwargs)\n            if data_type == 'bool':\n                output = json.dumps(output)\n\n            if kwargs.get('skip_quote') is True:\n                output = str(output)\n            else:\n                output = quote(str(output), safe='')\n        except SerializationError:\n            raise TypeError(\"{} must be type {}.\".format(name, data_type))\n        else:\n            return output", "category": "Python"}, {"instruction": "def source_sum_err(self):\n        \"\"\"\n        The uncertainty of `~photutils.SourceProperties.source_sum`,\n        propagated from the input ``error`` array.\n\n        ``source_sum_err`` is the quadrature sum of the total errors\n        over the non-masked pixels within the source segment:\n\n        .. math:: \\\\Delta F = \\\\sqrt{\\\\sum_{i \\\\in S}\n                  \\\\sigma_{\\\\mathrm{tot}, i}^2}\n\n        where :math:`\\\\Delta F` is ``source_sum_err``,\n        :math:`\\\\sigma_{\\\\mathrm{tot, i}}` are the pixel-wise total\n        errors, and :math:`S` are the non-masked pixels in the source\n        segment.\n\n        Pixel values that are masked in the input ``data``, including\n        any non-finite pixel values (i.e. NaN, infs) that are\n        automatically masked, are also masked in the error array.\n        \"\"\"\n", "input": "", "output": "\n        if self._error is not None:\n            if self._is_completely_masked:\n                return np.nan * self._error_unit  # table output needs unit\n            else:\n                return np.sqrt(np.sum(self._error_values ** 2))\n        else:\n            return None", "category": "Python"}, {"instruction": "def get_image_dimension(self, url):\n        \"\"\"\n        Return a tuple that contains (width, height)\n        Pass in a url to an image and find out its size without loading the whole file\n        If the image wxh could not be found, the tuple will contain `None` values\n        \"\"\"\n", "input": "", "output": "        w_h = (None, None)\n        try:\n            if url.startswith('//'):\n                url = 'http:' + url\n            data = requests.get(url).content\n            im = Image.open(BytesIO(data))\n\n            w_h = im.size\n        except Exception:\n            logger.warning(\"Error getting image size {}\".format(url), exc_info=True)\n\n        return w_h", "category": "Python"}, {"instruction": "def expected_parameters_from_cli(opts):\n    \"\"\"Parses the --expected-parameters arguments from the `plot_posterior`\n    option group.\n\n    Parameters\n    ----------\n    opts : ArgumentParser\n        The parsed arguments from the command line.\n\n    Returns\n    -------\n    dict\n        Dictionary of parameter name -> expected value. Only parameters that\n        were specified in the --expected-parameters option will be included; if\n        no parameters were provided, will return an empty dictionary.\n    \"\"\"\n", "input": "", "output": "    expected = {}\n    for x in opts.expected_parameters:\n        x = x.split(':')\n        if len(x) != 2:\n            raise ValueError(\"option --expected-paramters not specified \"\n                             \"correctly; see help\")\n        expected[x[0]] = float(x[1])\n    return expected", "category": "Python"}, {"instruction": "def get_host_address(host=None, default_address=DEFAULT_HOST_IP):\n    \"\"\"\n    Returns the given host address.\n\n    :param host: Host to retrieve the address.\n    :type host: unicode\n    :param default_address: Default address if the host is unreachable.\n    :type default_address: unicode\n    :return: Host address.\n    :rtype: unicode\n    \"\"\"\n", "input": "", "output": "\n    try:\n        return unicode(socket.gethostbyname(host or socket.gethostname()),\n                       Constants.default_codec,\n                       Constants.codec_error)\n    except Exception as error:\n        return default_address", "category": "Python"}, {"instruction": "def default_create_thread(callback):\n    \"\"\"\n    Default thread creation - used to create threads when the client doesn't want to provide their\n    own thread creation.\n\n    :param function callback: the callback function provided to threading.Thread\n    \"\"\"\n", "input": "", "output": "    thread = threading.Thread(None, callback)\n    thread.daemon = True  # Don't let thread prevent termination\n    thread.start()\n    return thread", "category": "Python"}, {"instruction": "def validate(self):\n        '''Validate that devices are each trusted by one another\n\n        :param kwargs: dict -- keyword args for devices and partition\n        :raises: DeviceNotTrusted\n        '''\n", "input": "", "output": "\n        self._populate_domain()\n        missing = []\n        for domain_device in self.domain:\n            for truster, trustees in iteritems(self.domain):\n                if domain_device not in trustees:\n                    missing.append((domain_device, truster, trustees))\n\n        if missing:\n            msg = ''\n            for item in missing:\n                msg += '\\n%r is not trusted by %r, which trusts: %r' % \\\n                    (item[0], item[1], item[2])\n            raise DeviceNotTrusted(msg)\n        self.device_group = DeviceGroup(\n            devices=self.devices,\n            device_group_name=self.device_group_name,\n            device_group_type=self.device_group_type,\n            device_group_partition=self.partition\n        )", "category": "Python"}, {"instruction": "def __remove_obsolete_metadata(self):\n        \"\"\"\n        Removes obsolete entries from the metadata of all stored routines.\n        \"\"\"\n", "input": "", "output": "        clean = {}\n        for key, _ in self._source_file_names.items():\n            if key in self._pystratum_metadata:\n                clean[key] = self._pystratum_metadata[key]\n\n        self._pystratum_metadata = clean", "category": "Python"}, {"instruction": "def get_user_classes(self):\n        \"\"\"Get list of the current user's classes. This is a subset of the\n        information returned by the call to ``get_user_status``.\n\n        :returns: Classes of currently authenticated user\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        # Previously getting classes from profile (such a list is incomplete)\n        # raw_classes = self.get_user_profile().get('all_classes').values()\n\n        # Get classes from the user status (includes all classes)\n        status = self.get_user_status()\n        uid = status['id']\n        raw_classes = status.get('networks', [])\n\n        classes = []\n        for rawc in raw_classes:\n            c = {k: rawc[k] for k in ['name', 'term']}\n            c['num'] = rawc.get('course_number', '')\n            c['nid'] = rawc['id']\n            c['is_ta'] = uid in rawc['prof_hash']\n            classes.append(c)\n\n        return classes", "category": "Python"}, {"instruction": "def build_search_request(filter_like, item_types, name=None, interval=None):\n    '''Build a data-api search request body for the specified item_types.\n    If 'filter_like' is a request, item_types will be merged and, if name or\n    interval is provided, will replace any existing values.\n\n    :param dict filter_like: a filter or request with a filter\n    :param sequence(str) item_types: item-types to specify in the request\n    :param str name: optional name\n    :param str interval: optional interval [year, month, week, day]\n    '''\n", "input": "", "output": "    filter_spec = filter_like.get('filter', filter_like)\n    all_items = list(set(filter_like.get('item_types', [])).union(item_types))\n    name = filter_like.get('name', name)\n    interval = filter_like.get('interval', interval)\n    req = {'item_types': all_items, 'filter': filter_spec}\n    if name:\n        req['name'] = name\n    if interval:\n        req['interval'] = interval\n    return req", "category": "Python"}, {"instruction": "def inject(cls, span, obj):\n        \"\"\" Injects the span context into a `carrier` object.\n\n        :param opentracing.span.SpanContext span: the SpanContext instance\n        :param Any obj: Object to use as context\n        \"\"\"\n", "input": "", "output": "        obj.metadata['__parent-span__'] = dict()\n        cls.inject_span(span, obj.metadata['__parent-span__'])", "category": "Python"}, {"instruction": "def download(self, uuid, output_format='gzip'):\n        \"\"\"\n        Download pre-prepared data by UUID\n\n        :type uuid: str\n        :param uuid: Data UUID\n\n        :type output_format: str\n        :param output_format: Output format of the data, either \"gzip\" or \"text\"\n\n        :rtype: str\n        :return: The downloaded content\n        \"\"\"\n", "input": "", "output": "\n        if output_format.lower() not in ('gzip', 'text'):\n            raise Exception(\"output_format must be one of file, text\")\n\n        data = {\n            'format': output_format,\n            'uuid': uuid,\n        }\n        return self.get('download', get_params=data, is_json=False)", "category": "Python"}, {"instruction": "def IsLinearOutputModule(cls, name):\n    \"\"\"Determines if a specific output class is a linear output module.\n\n    Args:\n      name (str): name of the output module.\n\n    Returns:\n      True: if the output module is linear.\n    \"\"\"\n", "input": "", "output": "    name = name.lower()\n\n    output_class = cls._output_classes.get(name, None)\n    if not output_class:\n      output_class = cls._disabled_output_classes.get(name, None)\n\n    if output_class:\n      return issubclass(output_class, interface.LinearOutputModule)\n\n    return False", "category": "Python"}, {"instruction": "def prettify(elem):\n    # from xml.etree.ElementTree import Element, SubElement, Comment, tostring\n    from xml.etree import ElementTree\n    from xml.dom import minidom\n    \"\"\"Return a pretty-printed XML string for the Element.\n    \"\"\"\n", "input": "", "output": "    rough_string = ElementTree.tostring(elem, 'utf-8')\n    reparsed = minidom.parseString(rough_string)\n    return reparsed.toprettyxml(indent=\"  \")", "category": "Python"}, {"instruction": "def create_qgis_template_output(output_path, layout):\n    \"\"\"Produce QGIS Template output.\n\n    :param output_path: The output path.\n    :type output_path: str\n\n    :param composition: QGIS Composition object to get template.\n        values\n    :type composition: qgis.core.QgsLayout\n\n    :return: Generated output path.\n    :rtype: str\n    \"\"\"\n", "input": "", "output": "    # make sure directory is created\n    dirname = os.path.dirname(output_path)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)\n\n    context = QgsReadWriteContext()\n    context.setPathResolver(QgsProject.instance().pathResolver())\n\n    layout.saveAsTemplate(output_path, context)\n    return output_path", "category": "Python"}, {"instruction": "def generate(env):\n    \"\"\"Add Builders and construction variables for Borland ilink to an\n    Environment.\"\"\"\n", "input": "", "output": "    SCons.Tool.createSharedLibBuilder(env)\n    SCons.Tool.createProgBuilder(env)\n\n    env['LINK']        = '$CC'\n    env['LINKFLAGS']   = SCons.Util.CLVar('')\n    env['LINKCOM']     = '$LINK -q $LINKFLAGS -e$TARGET $SOURCES $LIBS'\n    env['LIBDIRPREFIX']=''\n    env['LIBDIRSUFFIX']=''\n    env['LIBLINKPREFIX']=''\n    env['LIBLINKSUFFIX']='$LIBSUFFIX'", "category": "Python"}, {"instruction": "def juju_version():\n    \"\"\"Full version string (eg. '1.23.3.1-trusty-amd64')\"\"\"\n", "input": "", "output": "    # Per https://bugs.launchpad.net/juju-core/+bug/1455368/comments/1\n    jujud = glob.glob('/var/lib/juju/tools/machine-*/jujud')[0]\n    return subprocess.check_output([jujud, 'version'],\n                                   universal_newlines=True).strip()", "category": "Python"}, {"instruction": "def find_by_uuid(self, uuid):\n        \"\"\"Find an entry by uuid.\n\n        :raise: EntryNotFoundError\n        \"\"\"\n", "input": "", "output": "        for entry in self.entries:\n            if entry.uuid == uuid:\n                return entry\n        raise EntryNotFoundError(\"Entry not found for uuid: %s\" % uuid)", "category": "Python"}, {"instruction": "def delete_order_line_item_by_id(cls, order_line_item_id, **kwargs):\n        \"\"\"Delete OrderLineItem\n\n        Delete an instance of OrderLineItem by its ID.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.delete_order_line_item_by_id(order_line_item_id, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str order_line_item_id: ID of orderLineItem to delete. (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._delete_order_line_item_by_id_with_http_info(order_line_item_id, **kwargs)\n        else:\n            (data) = cls._delete_order_line_item_by_id_with_http_info(order_line_item_id, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def authenticate(self, reauth=False):\n        \"\"\"\n        Authenticate with the API and return an authentication token.\n        \"\"\"\n", "input": "", "output": "        auth_url = BASE_URL + \"/rest/user\"\n        payload = {'email': self.email, 'password': self.password}\n        arequest = requests.get(auth_url, params=payload)\n        status = arequest.status_code\n        if status != 200:\n            if reauth:\n                _LOGGER.error(\"Reauthentication request failed. \" + status)\n            else:\n                _LOGGER.error(\"Authentication request failed, please check credintials. \" + status)\n        self.token = arequest.json().get('usertoken')\n        if reauth:\n            _LOGGER.info(\"Reauthentication was successful, token updated.\")\n        else:\n            _LOGGER.info(\"Authentication was successful, token set.\")", "category": "Python"}, {"instruction": "def update_file(finfo, sample_info, config):\n    \"\"\"\n    Update the file to an iRODS repository.\n    \"\"\"\n", "input": "", "output": "    ffinal = filesystem.update_file(finfo, sample_info, config, pass_uptodate=True)\n\n    _upload_dir_icommands_cli(config.get(\"dir\"), config.get(\"folder\"), config)", "category": "Python"}, {"instruction": "def _read_weights(self):\n        \"\"\"Reads weights from each of the load cells.\n        \"\"\"\n", "input": "", "output": "        weights = []\n\n        grams_per_pound = 453.592\n\n        # Read from each of the sensors\n        for ser in self._serials:\n            ser.write('W\\r')\n            ser.flush()\n        time.sleep(0.02)\n        for ser in self._serials:\n            try:\n                output_str = ser.readline()\n                weight = float(output_str) * grams_per_pound\n                weights.append(weight)\n            except:\n                weights.append(0.0)\n\n        # Log the output\n        log_output = ''\n        for w in weights:\n            log_output +='{:.2f} '.format(w)\n        rospy.loginfo(log_output)\n\n        return weights", "category": "Python"}, {"instruction": "def alternate_helper(x, alt_samps, func=None):\n    \"\"\"Helper function for making fgivenx plots of functions with 2 array\n    arguments of variable lengths.\"\"\"\n", "input": "", "output": "    alt_samps = alt_samps[~np.isnan(alt_samps)]\n    arg1 = alt_samps[::2]\n    arg2 = alt_samps[1::2]\n    return func(x, arg1, arg2)", "category": "Python"}, {"instruction": "def get(self):\n        \"\"\"Returns existing value, or None if deadline has expired.\"\"\"\n", "input": "", "output": "        if self.timer() > self.deadline:\n            self.value = None\n        return self.value", "category": "Python"}, {"instruction": "def visit_lambda(self, node, parent):\n        \"\"\"visit a Lambda node by returning a fresh instance of it\"\"\"\n", "input": "", "output": "        newnode = nodes.Lambda(node.lineno, node.col_offset, parent)\n        newnode.postinit(self.visit(node.args, newnode), self.visit(node.body, newnode))\n        return newnode", "category": "Python"}, {"instruction": "def _graph_add_edge(self, cfg_node, src_node, src_jumpkind, src_ins_addr, src_stmt_idx):\n        \"\"\"\n        Add edge between nodes, or add node if entry point\n\n        :param CFGNode cfg_node: node which is jumped to\n        :param CFGNode src_node: node which is jumped from none if entry point\n        :param str src_jumpkind: what type of jump the edge takes\n        :param int or str src_stmt_idx: source statements ID\n        :return: None\n        \"\"\"\n", "input": "", "output": "\n        if src_node is None:\n            self.graph.add_node(cfg_node)\n        else:\n            self.graph.add_edge(src_node, cfg_node, jumpkind=src_jumpkind, ins_addr=src_ins_addr,\n                                stmt_idx=src_stmt_idx)", "category": "Python"}, {"instruction": "def search_edges_with_evidence(self, evidence: str) -> List[Edge]:\n        \"\"\"Search edges with the given evidence.\n\n        :param evidence: A string to search evidences. Can use wildcard percent symbol (%).\n        \"\"\"\n", "input": "", "output": "        return self.session.query(Edge).join(Evidence).filter(Evidence.text.like(evidence)).all()", "category": "Python"}, {"instruction": "def process_seq(seq, material):\n    '''Validate and process sequence inputs.\n\n    :param seq: input sequence\n    :type seq: str\n    :param material: DNA, RNA, or peptide\n    :type: str\n    :returns: Uppercase version of `seq` with the alphabet checked by\n              check_alphabet().\n    :rtype: str\n\n    '''\n", "input": "", "output": "    check_alphabet(seq, material)\n    seq = seq.upper()\n    return seq", "category": "Python"}, {"instruction": "def robots(self):\n        \"\"\"Return values for robots html meta key\"\"\"\n", "input": "", "output": "        r = 'noindex' if self.is_noindex else 'index'\n        r += ','\n        r += 'nofollow' if self.is_nofollow else 'follow'\n        return r", "category": "Python"}, {"instruction": "def login_exists(login, domain='', **kwargs):\n    '''\n    Find if a login exists in the MS SQL server.\n    domain, if provided, will be prepended to login\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt minion mssql.login_exists 'LOGIN'\n    '''\n", "input": "", "output": "    if domain:\n        login = '{0}\\\\{1}'.format(domain, login)\n    try:\n        # We should get one, and only one row\n        return len(tsql_query(query=\"SELECT name FROM sys.syslogins WHERE name='{0}'\".format(login), **kwargs)) == 1\n\n    except Exception as e:\n        return 'Could not find the login: {0}'.format(e)", "category": "Python"}, {"instruction": "def run(self, i, o):  # type: () -> int\n        \"\"\"\n        Initialize command.\n        \"\"\"\n", "input": "", "output": "        self.input = i\n        self.output = PoetryStyle(i, o)\n\n        for logger in self._loggers:\n            self.register_logger(logging.getLogger(logger))\n\n        return super(BaseCommand, self).run(i, o)", "category": "Python"}, {"instruction": "def pull_tag_dict(data):\n    \"\"\"This will pull out a list of Tag Name-Value objects, and return it as a dictionary.\n\n    :param data: The dict collected from the collector.\n    :returns dict: A dict of the tag names and their corresponding values.\n    \"\"\"\n", "input": "", "output": "    # If there are tags, set them to a normal dict, vs. a list of dicts:\n    tags = data.pop('Tags', {}) or {}\n    if tags:\n        proper_tags = {}\n        for tag in tags:\n            proper_tags[tag['Key']] = tag['Value']\n\n        tags = proper_tags\n\n    return tags", "category": "Python"}, {"instruction": "def addDefaultShareID(store, shareID, priority):\n    \"\"\"\n    Add a default share ID to C{store}, pointing to C{shareID} with a\n    priority C{priority}.  The highest-priority share ID identifies the share\n    that will be retrieved when a user does not explicitly provide a share ID\n    in their URL (e.g. /host/users/username/).\n\n    @param shareID: A share ID.\n    @type shareID: C{unicode}\n\n    @param priority: The priority of this default.  Higher means more\n    important.\n    @type priority: C{int}\n    \"\"\"\n", "input": "", "output": "    _DefaultShareID(store=store, shareID=shareID, priority=priority)", "category": "Python"}, {"instruction": "def query(self, listdelimiter=\"+\", safe=\"\", **kwargs):\n        \"\"\"\n        Url queries\n\n        :param listdelimiter: Specifies what list delimiter should be\n        :param safe: string that includes all the characters that should not be ignored\n\n        Kwargs (Since its a dictionary) are not ordered. You must call the\n        method again if you absolutely need one query\n        after another or vice versa.\n\n        \"\"\"\n", "input": "", "output": "        safe = safe if safe else self.safe\n        for arg in list(kwargs.keys()):\n            if (isinstance(kwargs[arg], list)\n                    or isinstance(kwargs[arg], tuple)\n                    or isinstance(kwargs[arg], set)):\n                items = [quote_plus(str(x), safe=safe) for x in kwargs[arg]]\n                self.__query__.update({arg: listdelimiter.join(items)})\n            else:\n                self.__query__.update({arg: kwargs.get(arg)})\n\n        return self", "category": "Python"}, {"instruction": "def fire_event(self, event, *args, **kwargs):\n        \"\"\"Fires a event.\"\"\"\n", "input": "", "output": "        self.event_queue.append((event, args))\n        self.process_events()", "category": "Python"}, {"instruction": "def _merge_region_trees(self, dst_tree, src_tree, pid):\n        \"\"\"Merge conflicts occur if a folder in one tree is a file in the other.\n\n        As the files are PIDs, this can only happen if a PID matches one of the\n        geographical areas that the dataset covers and should be very rare. In such\n        conflicts, the destination wins.\n\n        \"\"\"\n", "input": "", "output": "        for k, v in list(src_tree.items()):\n            # Prepend an underscore to the administrative area names, to make them\n            # sort separately from the identifiers.\n            # k = '_' + k\n            if k not in dst_tree or dst_tree[k] is None:\n                dst_tree[k] = {}\n            dst_tree[k][pid] = None\n            if v is not None:\n                self._merge_region_trees(dst_tree[k], v, pid)", "category": "Python"}, {"instruction": "def cmd_create(args):\n    \"\"\"Creates a list\"\"\"\n", "input": "", "output": "    name = args.get(0)\n    if name:\n        penStore.createList(name)\n    else:\n        puts(\"not valid\")", "category": "Python"}, {"instruction": "def p_fragment_definition1(self, p):\n        \"\"\"\n        fragment_definition : FRAGMENT fragment_name ON type_condition directives selection_set\n        \"\"\"\n", "input": "", "output": "        p[0] = FragmentDefinition(name=p[2], type_condition=p[4],\n                                  selections=p[6], directives=p[5])", "category": "Python"}, {"instruction": "def drop(self):\n        \"\"\"Drop this database.\n\n        See\n        https://cloud.google.com/spanner/reference/rpc/google.spanner.admin.database.v1#google.spanner.admin.database.v1.DatabaseAdmin.DropDatabase\n        \"\"\"\n", "input": "", "output": "        api = self._instance._client.database_admin_api\n        metadata = _metadata_with_prefix(self.name)\n        api.drop_database(self.name, metadata=metadata)", "category": "Python"}, {"instruction": "def create_gzip_cache(pelican):\n    '''Create a gzip cache file for every file that a webserver would\n    reasonably want to cache (e.g., text type files).\n\n    :param pelican: The Pelican instance\n    '''\n", "input": "", "output": "    for dirpath, _, filenames in os.walk(pelican.settings['OUTPUT_PATH']):\n        for name in filenames:\n            if should_compress(name):\n                filepath = os.path.join(dirpath, name)\n                create_gzip_file(filepath, should_overwrite(pelican.settings))", "category": "Python"}, {"instruction": "def _get_update_method(self):\n        \"\"\"Return the HTTP method to use.\n\n        Returns:\n            object: http_put (default) or http_post\n        \"\"\"\n", "input": "", "output": "        if getattr(self, '_update_uses_post', False):\n            http_method = self.gitlab.http_post\n        else:\n            http_method = self.gitlab.http_put\n        return http_method", "category": "Python"}, {"instruction": "def decode(self, bytes):\n        \"\"\"\n        Decodes the packet off the byte string.\n        \"\"\"\n", "input": "", "output": "\n        self.buffer = bytes\n        self._pos = 0\n\n        Packet = identifier.get_packet_from_id(self._read_variunt())\n\n        # unknown packets will be None from the identifier\n        if Packet is None:\n            return None\n\n        packet = Packet()\n        packet.ParseFromString(self.remaining_bytes())\n        return packet", "category": "Python"}, {"instruction": "def count_per_m(self):\n        \"\"\"\n        Returns the number of tacho counts in one meter of travel of the motor. Tacho\n        counts are used by the position and speed attributes, so you can use this\n        value to convert from distance to tacho counts. (linear motors only)\n        \"\"\"\n", "input": "", "output": "        (self._count_per_m, value) = self.get_cached_attr_int(self._count_per_m, 'count_per_m')\n        return value", "category": "Python"}, {"instruction": "def from_str(value: str) -> ulid.ULID:\n    \"\"\"\n    Create a new :class:`~ulid.ulid.ULID` instance from the given :class:`~str` value.\n\n    :param value: Base32 encoded string\n    :type value: :class:`~str`\n    :return: ULID from string value\n    :rtype: :class:`~ulid.ulid.ULID`\n    :raises ValueError: when the value is not 26 characters or malformed\n    \"\"\"\n", "input": "", "output": "    return ulid.ULID(base32.decode_ulid(value))", "category": "Python"}, {"instruction": "def delete_cancel_operation_by_id(cls, cancel_operation_id, **kwargs):\n        \"\"\"Delete CancelOperation\n\n        Delete an instance of CancelOperation by its ID.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.delete_cancel_operation_by_id(cancel_operation_id, async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param str cancel_operation_id: ID of cancelOperation to delete. (required)\n        :return: None\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._delete_cancel_operation_by_id_with_http_info(cancel_operation_id, **kwargs)\n        else:\n            (data) = cls._delete_cancel_operation_by_id_with_http_info(cancel_operation_id, **kwargs)\n            return data", "category": "Python"}, {"instruction": "def _load_controllers(self):\n        \"\"\"\n        Load all controllers from folder 'controllers'.\n\n        Ignore files with leading underscore (for example: controllers/_blogs.py)\n        \"\"\"\n", "input": "", "output": "        for file_name in os.listdir(os.path.join(self._project_dir, 'controllers')):\n            # ignore disabled controllers\n            if not file_name.startswith('_'):\n                module_name = file_name.split('.', 1)[0]\n                module_path = \"controllers.{}\".format(module_name)\n                module = import_module(module_path)\n                # transform 'blog_articles' file name to 'BlogArticles' class\n                controller_class_name = module_name.title().replace('_', '')\n                controller_class = getattr(module, controller_class_name)\n                controller = controller_class()\n                for action_name in dir(controller):\n                    action = getattr(controller, action_name)\n                    if action_name.startswith('_') or not callable(action):\n                        continue\n                    url_path = \"/\".join([module_name, action_name])\n                    self._controllers[url_path] = action\n        return self._controllers", "category": "Python"}, {"instruction": "def _paired_load_script(work_bams, names, chrom, pairmode, items):\n    \"\"\"Prepare BAMs for assessing CNVs in a paired tumor/normal setup.\n    \"\"\"\n", "input": "", "output": "    paired = vcfutils.get_paired_bams(work_bams, items)\n    bed_file = _get_regional_bed_file(items[0])\n    if bed_file:\n        return _paired_prep_targeted.format(case_file=paired.tumor_bam, case_name=paired.tumor_name,\n                                            ctrl_file=paired.normal_bam, ctrl_name=paired.normal_name,\n                                            num_cores=0, chrom=chrom, pairmode=pairmode, bed_file=bed_file)\n    else:\n        return _paired_prep.format(case_file=paired.tumor_bam, case_name=paired.tumor_name,\n                                   ctrl_file=paired.normal_bam, ctrl_name=paired.normal_name,\n                                   num_cores=0, chrom=chrom, pairmode=pairmode)", "category": "Python"}, {"instruction": "def freeze(self):\n        \"\"\"\n        Freezes this Config object, disallowing modification or addition of any parameters.\n        \"\"\"\n", "input": "", "output": "        if getattr(self, '_frozen'):\n            return\n        object.__setattr__(self, \"_frozen\", True)\n        for k, v in self.__dict__.items():\n            if isinstance(v, Config) and k != \"self\":\n                v.freeze()", "category": "Python"}, {"instruction": "def _init_metadata(self):\n        \"\"\"stub\"\"\"\n", "input": "", "output": "        self._choice_ids_metadata = {\n            'element_id': Id(self.my_osid_object_form._authority,\n                             self.my_osid_object_form._namespace,\n                             'choice_ids'),\n            'element_label': 'response set',\n            'instructions': 'submit correct choice for answer',\n            'required': False,\n            'read_only': False,\n            'linked': False,\n            'array': False,\n            'default_object_values': [[]],\n            'syntax': 'OBJECT',\n        }\n        self._choice_id_metadata = {\n            'element_id': Id(self.my_osid_object_form._authority,\n                             self.my_osid_object_form._namespace,\n                             'choice_id'),\n            'element_label': 'response set',\n            'instructions': 'submit correct choice for answer',\n            'required': True,\n            'read_only': False,\n            'linked': False,\n            'array': False,\n            'default_id_values': [''],\n            'syntax': 'ID',\n            'id_set': []\n        }", "category": "Python"}, {"instruction": "def _get_cache_size(replace=False):\n    \"\"\"Get size of cache.\"\"\"\n", "input": "", "output": "\n    if not replace:\n        size = _cached_search_compile.cache_info().currsize\n    else:\n        size = _cached_replace_compile.cache_info().currsize\n    return size", "category": "Python"}, {"instruction": "def _artifacts(self):\n        \"\"\"Retrieve the artifacts json object\"\"\"\n", "input": "", "output": "        if '_artifacts' not in self._memo:\n            json = _get_url(self._artifacts_url).json()\n            self._memo['_artifacts'] = json['artifacts']\n        return self._memo['_artifacts']", "category": "Python"}, {"instruction": "def get_global(self):\n        \"\"\"Gets the current global evaluation result.\n\n        Returns\n        -------\n        names : list of str\n           Name of the metrics.\n        values : list of float\n           Value of the evaluations.\n        \"\"\"\n", "input": "", "output": "        if self._has_global_stats:\n            if self.global_num_inst == 0:\n                return (self.name, float('nan'))\n            else:\n                return (self.name, self.global_sum_metric / self.global_num_inst)\n        else:\n            return self.get()", "category": "Python"}, {"instruction": "def get_table(self, name='Meta', h5loc='/meta'):\n        \"\"\"Convert metadata to a KM3Pipe Table.\n\n        Returns `None` if there is no data.\n\n        Each column's dtype will be set to a fixed size string (numpy.string_)\n        with the length of the longest entry, since writing variable length\n        strings does not fit the current scheme.\n        \"\"\"\n", "input": "", "output": "        if not self.meta:\n            return None\n\n        data = defaultdict(list)\n        for entry in self.meta:\n            for key, value in entry.items():\n                data[key].append(value)\n        dtypes = []\n        for key, values in data.items():\n            max_len = max(map(len, values))\n            dtype = 'S{}'.format(max_len)\n            dtypes.append((key, dtype))\n        tab = Table(\n            data, dtype=dtypes, h5loc=h5loc, name='Meta', h5singleton=True\n        )\n        return tab", "category": "Python"}, {"instruction": "def run_start_backup(cls):\n        \"\"\"\n        Connects to a server and attempts to start a hot backup\n\n        Yields the WAL information in a dictionary for bookkeeping and\n        recording.\n\n        \"\"\"\n", "input": "", "output": "        def handler(popen):\n            assert popen.returncode != 0\n            raise UserException('Could not start hot backup')\n\n        # The difficulty of getting a timezone-stamped, UTC,\n        # ISO-formatted datetime is downright embarrassing.\n        #\n        # See http://bugs.python.org/issue5094\n        label = 'freeze_start_' + (datetime.datetime.utcnow()\n                                   .replace(tzinfo=UTC()).isoformat())\n\n        return cls._dict_transform(psql_csv_run(\n                \"SELECT file_name, \"\n                \"  lpad(file_offset::text, 8, '0') AS file_offset \"\n                \"FROM pg_{0}file_name_offset(\"\n                \"  pg_start_backup('{1}'))\".format(cls._wal_name(), label),\n                error_handler=handler))", "category": "Python"}, {"instruction": "def visitMember(self, ctx: jsgParser.MemberContext):\n        \"\"\" member: pairDef COMMA? \"\"\"\n", "input": "", "output": "        self._strict = ctx.COMMA() is None\n        self.visitChildren(ctx)", "category": "Python"}, {"instruction": "def minLength( self, orientation ):\r\n        \"\"\"\r\n        Returns the minimum length for this ruler based on its notches and the\r\n        given orientation.\r\n        \r\n        :param      orientation | <Qt.Orientation>\r\n        \r\n        :return     <int>\r\n        \"\"\"\n", "input": "", "output": "        padding         = self.padStart() + self.padEnd()\r\n        count           = len(self.notches())\r\n        notch_padding   = self.notchPadding()\r\n        \r\n        if ( orientation == Qt.Horizontal ):\r\n            section     = self.maxNotchSize(Qt.Vertical)\r\n        else:\r\n            section     = self.maxNotchSize(Qt.Horizontal)\r\n        \r\n        return notch_padding * count + section * count + padding", "category": "Python"}, {"instruction": "def correlator(A, B):\n    \"\"\"Correlators between the probabilities of two parties.\n\n    :param A: Measurements of Alice.\n    :type A: list of list of\n             :class:`sympy.physics.quantum.operator.HermitianOperator`.\n    :param B: Measurements of Bob.\n    :type B: list of list of\n             :class:`sympy.physics.quantum.operator.HermitianOperator`.\n\n    :returns: list of correlators.\n    \"\"\"\n", "input": "", "output": "    correlators = []\n    for i in range(len(A)):\n        correlator_row = []\n        for j in range(len(B)):\n            corr = 0\n            for k in range(len(A[i])):\n                for l in range(len(B[j])):\n                    if k == l:\n                        corr += A[i][k] * B[j][l]\n                    else:\n                        corr -= A[i][k] * B[j][l]\n            correlator_row.append(corr)\n        correlators.append(correlator_row)\n    return correlators", "category": "Python"}, {"instruction": "def save(self, page, language, data, change, extra_data=None):\n        \"\"\"Actually save the placeholder data into the Content object.\"\"\"\n", "input": "", "output": "        # if this placeholder is untranslated, we save everything\n        # in the default language\n        if self.untranslated:\n            language = settings.PAGE_DEFAULT_LANGUAGE\n\n        # the page is being changed\n        if change:\n            # we need create a new content if revision is enabled\n            if(settings.PAGE_CONTENT_REVISION and self.name\n                not in settings.PAGE_CONTENT_REVISION_EXCLUDE_LIST):\n                Content.objects.create_content_if_changed(\n                    page,\n                    language,\n                    self.name,\n                    data\n                )\n            else:\n                Content.objects.set_or_create_content(\n                    page,\n                    language,\n                    self.name,\n                    data\n                )\n        # the page is being added\n        else:\n            Content.objects.set_or_create_content(\n                page,\n                language,\n                self.name,\n                data\n            )", "category": "Python"}, {"instruction": "def create_fork_exec(original_name):\n    \"\"\"\n    _posixsubprocess.fork_exec(args, executable_list, close_fds, ... (13 more))\n    \"\"\"\n", "input": "", "output": "\n    def new_fork_exec(args, *other_args):\n        import _posixsubprocess  # @UnresolvedImport\n        args = patch_args(args)\n        send_process_created_message()\n        return getattr(_posixsubprocess, original_name)(args, *other_args)\n\n    return new_fork_exec", "category": "Python"}, {"instruction": "def geo_distance(a, b):\n    \"\"\"Distance between two geo points in km. (p.x = long, p.y = lat)\"\"\"\n", "input": "", "output": "    a_y = radians(a.y)\n    b_y = radians(b.y)\n    delta_x = radians(a.x - b.x)\n    cos_x = (sin(a_y) * sin(b_y) +\n             cos(a_y) * cos(b_y) * cos(delta_x))\n    return acos(cos_x) * earth_radius_km", "category": "Python"}, {"instruction": "def pdf_merge(inputs: [str], output: str, delete: bool = False):\n    \"\"\"\n    Merge multiple Pdf input files in one output file.\n    :param inputs: input files\n    :param output: output file\n    :param delete: delete input files after completion if true\n\n    \"\"\"\n", "input": "", "output": "    writer = PdfFileWriter()\n    if os.path.isfile(output):\n        ans = input(\n            \"The file '%s' already exists. \"\n            \"Overwrite? Yes/Abort [Y/a]: \" % output\n        ).lower()\n        if ans == \"a\":\n            return\n\n    outputfile = open(output, \"wb\")\n    try:\n        infiles = []\n        for filename in inputs:\n            f = open(filename, \"rb\")\n            reader = PdfFileReader(f)\n            for page in reader.pages:\n                writer.addPage(page)\n            infiles.append(f)\n        writer.write(outputfile)\n    except FileNotFoundError as e:\n        print(e.strerror + \": \" + e.filename)\n    finally:\n        outputfile.close()\n        for f in infiles:\n            f.close()\n    if delete:\n        for filename in inputs:\n            os.remove(filename)", "category": "Python"}, {"instruction": "def tar_to_bigfile(self, fname, outfile):\n        \"\"\"Convert tar of multiple FASTAs to one file.\"\"\"\n", "input": "", "output": "        fnames = []\n        tmpdir = mkdtemp()\n        \n        # Extract files to temporary directory\n        with tarfile.open(fname) as tar:\n            tar.extractall(path=tmpdir)\n        for root, _, files in os.walk(tmpdir):\n            fnames += [os.path.join(root, fname) for fname in files]\n        \n        # Concatenate\n        with open(outfile, \"w\") as out:\n            for infile in fnames:\n                for line in open(infile):\n                    out.write(line)\n                os.unlink(infile)\n        \n        # Remove temp dir\n        shutil.rmtree(tmpdir)", "category": "Python"}, {"instruction": "def short_repr(obj, max_len=40):\n  \"\"\"Returns a short, term-friendly string representation of the object.\n\n  Args:\n    obj: An object for which to return a string representation.\n    max_len: Maximum length of the returned string. Longer reprs will be turned\n        into a brief descriptive string giving the type and length of obj.\n  \"\"\"\n", "input": "", "output": "  obj_repr = repr(obj)\n  if len(obj_repr) <= max_len:\n    return obj_repr\n  return '<{} of length {}>'.format(type(obj).__name__, len(obj_repr))", "category": "Python"}, {"instruction": "def validate(self, value, model_instance, **kwargs):\n        \"\"\"This follows the validate rules for choices_form_class field used.\n        \"\"\"\n", "input": "", "output": "        self.get_choices_form_class().validate(value, model_instance, **kwargs)", "category": "Python"}, {"instruction": "def HEAD(self, rest_path_list, **kwargs):\n        \"\"\"Send a HEAD request. See requests.sessions.request for optional parameters.\n\n        :returns: Response object\n\n        \"\"\"\n", "input": "", "output": "        kwargs.setdefault(\"allow_redirects\", False)\n        return self._request(\"HEAD\", rest_path_list, **kwargs)", "category": "Python"}, {"instruction": "def create_widget(self):\n        \"\"\" Create the underlying widget.\n\n        \"\"\"\n", "input": "", "output": "        d = self.declaration\n        self.widget = RelativeLayout(self.get_context(), None, d.style)", "category": "Python"}, {"instruction": "def unmonitor(self, target):\n        \"\"\" Stop monitoring the online status of a user. Returns whether or not the server supports monitoring. \"\"\"\n", "input": "", "output": "        if 'monitor-notify' in self._capabilities and self.is_monitoring(target):\n            yield from self.rawmsg('MONITOR', '-', target)\n            self._monitoring.remove(target)\n            return True\n        else:\n            return False", "category": "Python"}, {"instruction": "def formatMessage(self, record: logging.LogRecord) -> str:\n        \"\"\"Convert the already filled log record to a string.\"\"\"\n", "input": "", "output": "        level_color = \"0\"\n        text_color = \"0\"\n        fmt = \"\"\n        if record.levelno <= logging.DEBUG:\n            fmt = \"\\033[0;37m\" + logging.BASIC_FORMAT + \"\\033[0m\"\n        elif record.levelno <= logging.INFO:\n            level_color = \"1;36\"\n            lmsg = record.message.lower()\n            if self.GREEN_RE.search(lmsg):\n                text_color = \"1;32\"\n        elif record.levelno <= logging.WARNING:\n            level_color = \"1;33\"\n        elif record.levelno <= logging.CRITICAL:\n            level_color = \"1;31\"\n        if not fmt:\n            fmt = \"\\033[\" + level_color + \\\n                  \"m%(levelname)s\\033[0m:%(rthread)s:%(name)s:\\033[\" + text_color + \\\n                  \"m%(message)s\\033[0m\"\n        fmt = _fest + fmt\n        record.rthread = reduce_thread_id(record.thread)\n        return fmt % record.__dict__", "category": "Python"}, {"instruction": "def decipher(self,string,keep_punct=False):\n        \"\"\"Decipher string using affine cipher according to initialised key.\n\n        Example::\n        \n            plaintext = Affine(a,b).decipher(ciphertext)     \n\n        :param string: The string to decipher.\n        :param keep_punct: if true, punctuation and spacing are retained. If false, it is all removed. Default is False. \n        :returns: The deciphered string.\n        \"\"\"\n", "input": "", "output": "        if not keep_punct: string = self.remove_punctuation(string)    \n        ret = ''\n        for c in string:\n            if c.isalpha(): ret += self.i2a(self.inva*(self.a2i(c) - self.b))\n            else: ret += c\n        return ret", "category": "Python"}, {"instruction": "def args_priority(args, environ):\n    '''\n        priority of token\n        1) as argumment: -t\n        2) as environ variable\n\n        priority of as_user\n        1) as argument: -a\n        2) as environ variable\n    '''\n", "input": "", "output": "\n    arg_token = args.token\n    arg_as_user = args.as_user\n\n    slack_token_var_name = 'SLACK_TOKEN'\n    if slack_token_var_name in environ.keys():\n        token = environ[slack_token_var_name]\n    else:\n        token = None\n\n    if arg_token:\n        token = arg_token\n\n    # slack as_user\n    slack_as_user_var_name = 'SLACK_AS_USER'\n    as_user = bool(environ.get(slack_as_user_var_name))\n\n    if arg_as_user:\n        as_user = True\n\n    return token, as_user, args.channel", "category": "Python"}, {"instruction": "def format_interface_name(intf_type, port, ch_grp=0):\n    \"\"\"Method to format interface name given type, port.\n\n    Given interface type, port, and channel-group, this\n    method formats an interface name.  If channel-group is\n    non-zero, then port-channel is configured.\n\n    :param intf_type: Such as 'ethernet' or 'port-channel'\n    :param port: unique identification -- 1/32 or 1\n    :ch_grp: If non-zero, ignore other params and format\n             port-channel<ch_grp>\n    :returns: the full formatted interface name.\n              ex: ethernet:1/32, port-channel:1\n    \"\"\"\n", "input": "", "output": "    if ch_grp > 0:\n        return 'port-channel:%s' % str(ch_grp)\n\n    return '%s:%s' % (intf_type.lower(), port)", "category": "Python"}, {"instruction": "def run(self):\n\t\t\"\"\" Fonctionnement du thread \"\"\"\n", "input": "", "output": "\t\tif self.debug:\n\t\t\tprint(\"Starting \" + self.name)\n\n\t\t# Lancement du programme du thread\n\t\tif isinstance(self.function, str):\n\t\t\tglobals()[self.function](*self.args, **self.kwargs)\n\t\telse:\n\t\t\tself.function(*self.args, **self.kwargs)\n\n\t\tif self.debug:\n\t\t\tprint(\"Exiting \" + self.name)", "category": "Python"}, {"instruction": "def get(cls, label='default', path=None):\n        \"\"\"Read a server configuration from a configuration file.\n\n        :param label: A string. The configuration identified by ``label`` is\n            read.\n        :param path: A string. The configuration file to be manipulated.\n            Defaults to what is returned by\n            :func:`nailgun.config._get_config_file_path`.\n        :returns: A brand new :class:`nailgun.config.BaseServerConfig` object\n            whose attributes have been populated as appropriate.\n        :rtype: BaseServerConfig\n\n        \"\"\"\n", "input": "", "output": "        if path is None:\n            path = _get_config_file_path(\n                cls._xdg_config_dir,\n                cls._xdg_config_file\n            )\n        with open(path) as config_file:\n            return cls(**json.load(config_file)[label])", "category": "Python"}, {"instruction": "def edf_mtotdev(N, m, alpha):\n    \"\"\" Equivalent degrees of freedom for Modified Total Deviation\n    \n        NIST SP1065 page 41, Table 8\n    \"\"\"\n", "input": "", "output": "    assert(alpha in [2, 1, 0, -1, -2])\n    NIST_SP1065_table8 = [(1.90, 2.1), (1.20, 1.40), (1.10, 1.2), (0.85, 0.50), (0.75, 0.31)]\n    #(b, c) = NIST_SP1065_table8[ abs(alpha-2) ]\n    (b, c) = NIST_SP1065_table8[abs(alpha-2)]\n    edf = b*(float(N)/float(m))-c\n    print(\"mtotdev b,c= \", (b, c), \" edf=\", edf)\n    return edf", "category": "Python"}, {"instruction": "def get_preprocessor(space):\n    \"\"\"Returns an appropriate preprocessor class for the given space.\"\"\"\n", "input": "", "output": "\n    legacy_patch_shapes(space)\n    obs_shape = space.shape\n\n    if isinstance(space, gym.spaces.Discrete):\n        preprocessor = OneHotPreprocessor\n    elif obs_shape == ATARI_OBS_SHAPE:\n        preprocessor = GenericPixelPreprocessor\n    elif obs_shape == ATARI_RAM_OBS_SHAPE:\n        preprocessor = AtariRamPreprocessor\n    elif isinstance(space, gym.spaces.Tuple):\n        preprocessor = TupleFlatteningPreprocessor\n    elif isinstance(space, gym.spaces.Dict):\n        preprocessor = DictFlatteningPreprocessor\n    else:\n        preprocessor = NoPreprocessor\n\n    return preprocessor", "category": "Python"}, {"instruction": "def master(self):\n        '''return the currently chosen mavlink master object'''\n", "input": "", "output": "        if len(self.mav_master) == 0:\n              return None\n        if self.settings.link > len(self.mav_master):\n            self.settings.link = 1\n        # try to use one with no link error\n        if not self.mav_master[self.settings.link-1].linkerror:\n            return self.mav_master[self.settings.link-1]\n        for m in self.mav_master:\n            if not m.linkerror:\n                return m\n        return self.mav_master[self.settings.link-1]", "category": "Python"}, {"instruction": "def create_data_and_metadata_from_data(self, data: numpy.ndarray, intensity_calibration: Calibration.Calibration=None, dimensional_calibrations: typing.List[Calibration.Calibration]=None, metadata: dict=None, timestamp: str=None) -> DataAndMetadata.DataAndMetadata:\n        \"\"\"Create a data_and_metadata object from data.\n\n        .. versionadded:: 1.0\n        .. deprecated:: 1.1\n           Use :py:meth:`~nion.swift.Facade.DataItem.create_data_and_metadata` instead.\n\n        Scriptable: No\n        \"\"\"\n", "input": "", "output": "        ...", "category": "Python"}, {"instruction": "def open(self):\n        \"\"\"!\n        \\~english Trun on device and with all sensors at same time\n        \\~chinese \u5f00\u542f\u5168\u90e8\u4f20\u611f\u5668\n        \"\"\"\n", "input": "", "output": "        self._sendCmd(self.REG_PWR_MGMT_1, 0x00)\n        self._sendCmd(self.REG_PWR_MGMT_2, 0x00)", "category": "Python"}, {"instruction": "def do_view(self):\n        \"\"\"\n        Authenticate user with given credentials.\n        Connects user's queue and exchange\n        \"\"\"\n", "input": "", "output": "        self.current.output['login_process'] = True\n        self.current.task_data['login_successful'] = False\n        if self.current.is_auth:\n            self._do_upgrade()\n        else:\n            try:\n                auth_result = self.current.auth.authenticate(\n                    self.current.input['username'],\n                    self.current.input['password'])\n                self.current.task_data['login_successful'] = auth_result\n                if auth_result:\n                    self._do_upgrade()\n            except ObjectDoesNotExist:\n                self.current.log.exception(\"Wrong username or another error occurred\")\n                pass\n            except:\n                raise\n            if self.current.output.get('cmd') != 'upgrade':\n                self.current.output['status_code'] = 403\n            else:\n                KeepAlive(self.current.user_id).reset()", "category": "Python"}, {"instruction": "def list(self, include_claimed=False, echo=False, marker=None, limit=None):\n        \"\"\"\n        Need to form the URI differently, so we can't use the default list().\n        \"\"\"\n", "input": "", "output": "        return self._iterate_list(include_claimed=include_claimed, echo=echo,\n                marker=marker, limit=limit)", "category": "Python"}, {"instruction": "def put_privileges(self, body, params=None):\n        \"\"\"\n        `<TODO>`_\n\n        :arg body: The privilege(s) to add\n        :arg refresh: If `true` (the default) then refresh the affected shards\n            to make this operation visible to search, if `wait_for` then wait\n            for a refresh to make this operation visible to search, if `false`\n            then do nothing with refreshes., valid choices are: 'true', 'false',\n            'wait_for'\n        \"\"\"\n", "input": "", "output": "        if body in SKIP_IN_PATH:\n            raise ValueError(\"Empty value passed for a required argument 'body'.\")\n        return self.transport.perform_request(\n            \"PUT\", \"/_security/privilege/\", params=params, body=body\n        )", "category": "Python"}, {"instruction": "def matches_querytime(instance, querytime):\n        \"\"\"\n        Checks whether the given instance satisfies the given QueryTime object.\n\n        :param instance: an instance of Versionable\n        :param querytime: QueryTime value to check against\n        \"\"\"\n", "input": "", "output": "        if not querytime.active:\n            return True\n\n        if not querytime.time:\n            return instance.version_end_date is None\n\n        return (instance.version_start_date <= querytime.time and\n                (instance.version_end_date is None or\n                 instance.version_end_date > querytime.time))", "category": "Python"}, {"instruction": "def get_offsets(self, pix):\n        \"\"\"Get offset of the first pixel in each dimension in the\n        global coordinate system.\n\n        Parameters\n        ----------\n        pix : `~numpy.ndarray`\n            Pixel coordinates in global coordinate system.\n        \"\"\"\n", "input": "", "output": "\n        idx = []\n        for i in range(self.ndim):\n\n            if i == 0:\n                idx += [0]\n            else:\n                npix1 = int(self.shape[i])\n                pix0 = int(pix[i - 1]) - npix1 // 2\n                idx += [pix0]\n\n        return idx", "category": "Python"}, {"instruction": "def recipients(self, notification_type, recipients, priority='Low'):\n        \"\"\"Set vars for the passed in data. Used for one or more recipient notification.\n\n        .. code-block:: javascript\n\n            {\n                \"notificationType\": notification_type,\n                \"priority\": priority\n                \"isOrganization\": false,\n                \"recipients\": recipients\n            }\n\n        Args:\n            notification_type (str): The type of notification being sent.\n            recipients (str): A comma delimited string of recipients.\n            priority (str): The priority: Low, Medium, High.\n        \"\"\"\n", "input": "", "output": "        self._notification_type = notification_type\n        self._recipients = recipients\n        self._priority = priority\n        self._is_organization = False", "category": "Python"}, {"instruction": "def item(self, index: int) -> Optional[Attr]:\n        \"\"\"Return ``index``-th attr node.\"\"\"\n", "input": "", "output": "        if 0 <= index < len(self):\n            return self._dict[tuple(self._dict.keys())[index]]\n        return None", "category": "Python"}, {"instruction": "def merge_partition_offsets(*partition_offsets):\n    \"\"\"Merge the partition offsets of a single topic from multiple responses.\n\n    :param partition_offsets: list of dict partition: offset\n    :returns: dict partition: offset\n    \"\"\"\n", "input": "", "output": "    output = dict()\n    for partition_offset in partition_offsets:\n        for partition, offset in six.iteritems(partition_offset):\n            prev_offset = output.get(partition, 0)\n            output[partition] = max(prev_offset, offset)\n    return output", "category": "Python"}, {"instruction": "def filter( names, pat ):\n    \"\"\"Return the subset of the list NAMES that match PAT\"\"\"\n", "input": "", "output": "    import os, posixpath\n\n    result = [ ]\n    pat = os.path.normcase( pat )\n    if not pat in _cache:\n        res = translate( pat )\n        if len( _cache ) >= _MAXCACHE:\n            _cache.clear( )\n        _cache[ pat ] = re.compile( res )\n    match = _cache[ pat ].match\n    if os.path is posixpath:\n        # normcase on posix is NOP. Optimize it away from the loop.\n        for name in names:\n            if match( name ):\n                result.append( name )\n    else:\n        for name in names:\n            if match( os.path.normcase( name ) ):\n                result.append( name )\n    return result", "category": "Python"}, {"instruction": "def _setup_bar(self):\n        \"\"\"\n        Setup the process bar.\n        \"\"\"\n", "input": "", "output": "        bar = u\"\"\n        items_cnt = len(PROGRESS_BAR_ITEMS)\n        bar_val = float(self._time_left) / self._section_time * self.num_progress_bars\n        while bar_val > 0:\n            selector = int(bar_val * items_cnt)\n            selector = min(selector, items_cnt - 1)\n            bar += PROGRESS_BAR_ITEMS[selector]\n            bar_val -= 1\n\n        bar = bar.ljust(self.num_progress_bars)\n        return bar", "category": "Python"}, {"instruction": "def value_to_db(self, value):\n        \"\"\" Returns field's single value prepared for saving into a database. \"\"\"\n", "input": "", "output": "        assert isinstance(value, datetime.datetime)\n\n        try:\n            value = value - datetime.datetime(1970, 1, 1)\n        except OverflowError:\n            raise tldap.exceptions.ValidationError(\"is too big a date\")\n\n        value = value.seconds + value.days * 24 * 3600\n        value = str(value).encode(\"utf_8\")\n\n        return value", "category": "Python"}, {"instruction": "def addFixedEffect(self,F=None,A=None):\n        \"\"\"\n        add fixed effect to the model\n\n        Args:\n            F: fixed effect matrix [N,1]\n            A: design matrix [K,P] (e.g. SP.ones((1,P)) common effect; SP.eye(P) any effect)\n        \"\"\"\n", "input": "", "output": "        if A==None:\n            A = SP.eye(self.P)\n        if F==None:\n            F = SP.ones((self.N,1))\n        \n        assert A.shape[1]==self.P, 'Incompatible shape'\n        assert F.shape[0]==self.N, 'Incompatible shape'\n       \n        if F.shape[1]>1:\n            for m in range(F.shape[1]):\n                self.vd.addFixedEffTerm(A,F[:,m:m+1])\n        else:\n            self.vd.addFixedEffTerm(A,F)\n\n        #TODO: what is this gp object doing, is this initialization correct?\n        self.gp      = None\n        self.init    = False\n        self.fast    = False\n        self.optimum = None\n\n        self.cache['Sigma']   = None\n        self.cache['Hessian'] = None\n        self.cache['Lparams'] = None\n        self.cache['paramsST']= None", "category": "Python"}, {"instruction": "def encode_endpoint_props(ed):\n    \"\"\"\n    Encodes the properties of the given EndpointDescription\n    \"\"\"\n", "input": "", "output": "    props = encode_osgi_props(ed)\n    props[ECF_RSVC_ID] = \"{0}\".format(ed.get_remoteservice_id()[1])\n    props[ECF_ENDPOINT_ID] = \"{0}\".format(ed.get_container_id()[1])\n    props[ECF_ENDPOINT_CONTAINERID_NAMESPACE] = \"{0}\".format(\n        ed.get_container_id()[0]\n    )\n    props[ECF_ENDPOINT_TIMESTAMP] = \"{0}\".format(ed.get_timestamp())\n    ctid = ed.get_connect_target_id()\n    if ctid:\n        props[ECF_ENDPOINT_CONNECTTARGET_ID] = \"{0}\".format(ctid)\n    id_filters = ed.get_id_filters()\n    if id_filters:\n        props[ECF_ENDPOINT_IDFILTER_IDS] = \" \".join([x[1] for x in id_filters])\n    rs_filter = ed.get_remoteservice_filter()\n    if rs_filter:\n        props[ECF_ENDPOINT_REMOTESERVICE_FILTER] = ed.get_remoteservice_filter()\n    async_intfs = ed.get_async_interfaces()\n    if async_intfs:\n        props[ECF_SERVICE_EXPORTED_ASYNC_INTERFACES] = \" \".join(async_intfs)\n\n    all_props = ed.get_properties()\n    other_props = {\n        key: all_props[key]\n        for key in all_props.keys()\n        if not is_reserved_property(key)\n    }\n    return merge_dicts(props, other_props)", "category": "Python"}, {"instruction": "def collapseTs(ts=None):\n    \"\"\"\n    Collapse a time series back into LiPD record form.\n\n    | Example\n    | 1. D = lipd.readLipd()\n    | 2. ts = lipd.extractTs(D)\n    | 3. New_D = lipd.collapseTs(ts)\n\n    _timeseries_data is sorted by time_id, and then by dataSetName\n    _timeseries_data[10103341][\"ODP1098B\"] = {data}\n\n    :param list ts: Time series\n    :return dict: Metadata\n    \"\"\"\n", "input": "", "output": "    # Retrieve the associated raw data according to the \"time_id\" found in each object. Match it in _timeseries_data\n    global _timeseries_data\n    _d = {}\n    if not ts:\n        print(\"Error: Time series data not provided. Pass time series into the function.\")\n    else:\n        # Send time series list through to be collapsed.\n        try:\n            _raw = _timeseries_data[ts[0][\"time_id\"]]\n            print(mode_ts(\"collapse\", mode=\"\", ts=ts))\n            _d = collapse(ts, _raw)\n            _d = rm_empty_fields(_d)\n        except Exception as e:\n            print(\"Error: Unable to collapse the time series: {}\".format(e))\n            logger_start.error(\"collapseTs: unable to collapse the time series: {}\".format(e))\n    return _d", "category": "Python"}, {"instruction": "def _draw_fold_region_background(self, block, painter):\n        \"\"\"\n        Draw the fold region when the mouse is over and non collapsed\n        indicator.\n\n        :param top: Top position\n        :param block: Current block.\n        :param painter: QPainter\n        \"\"\"\n", "input": "", "output": "        r = FoldScope(block)\n        th = TextHelper(self.editor)\n        start, end = r.get_range(ignore_blank_lines=True)\n        if start > 0:\n            top = th.line_pos_from_number(start)\n        else:\n            top = 0\n        bottom = th.line_pos_from_number(end + 1)\n        h = bottom - top\n        if h == 0:\n            h = self.sizeHint().height()\n        w = self.sizeHint().width()\n        self._draw_rect(QRectF(0, top, w, h), painter)", "category": "Python"}, {"instruction": "def init_widget(self):\n        \"\"\" Initialize the underlying widget.\n\n        \"\"\"\n", "input": "", "output": "        super(AndroidTimePicker, self).init_widget()\n        d = self.declaration\n        w = self.widget\n        self.set_hour(d.hour)\n        self.set_minute(d.minute)\n        self.set_hour_mode(d.hour_mode)\n\n        w.setOnTimeChangedListener(w.getId())\n        w.onTimeChanged.connect(self.on_time_changed)", "category": "Python"}, {"instruction": "def get(schema=None, key=None, user=None, **kwargs):\n    '''\n    Get key in a particular GNOME schema\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' gnome.get user=<username> schema=org.gnome.desktop.screensaver key=idle-activation-enabled\n\n    '''\n", "input": "", "output": "    _gsession = _GSettings(user=user, schema=schema, key=key)\n    return _gsession._get()", "category": "Python"}, {"instruction": "def get_rows( self, key ):\n        \"\"\"Get the set of rows for the type-key\"\"\"\n", "input": "", "output": "        if key not in self.roots:\n            self.get_root( key )\n        if key == 'location':\n            return self.location_rows \n        else:\n            return self.rows", "category": "Python"}, {"instruction": "def _AddFieldPaths(node, prefix, field_mask):\n  \"\"\"Adds the field paths descended from node to field_mask.\"\"\"\n", "input": "", "output": "  if not node:\n    field_mask.paths.append(prefix)\n    return\n  for name in sorted(node):\n    if prefix:\n      child_path = prefix + '.' + name\n    else:\n      child_path = name\n    _AddFieldPaths(node[name], child_path, field_mask)", "category": "Python"}, {"instruction": "def euler_angles(self):\n        \"\"\":obj:`tuple` of float: The three euler angles for the rotation.\n        \"\"\"\n", "input": "", "output": "        q_wxyz = self.quaternion\n        q_xyzw = np.roll(q_wxyz, -1)\n        return transformations.euler_from_quaternion(q_xyzw)", "category": "Python"}, {"instruction": "def set_primary_key(self, columns, index_name=False):\n        \"\"\"\n        Set the primary key.\n\n        :type columns: list\n        :type index_name: str or bool\n\n        :rtype: Table\n        \"\"\"\n", "input": "", "output": "        self._add_index(\n            self._create_index(columns, index_name or \"primary\", True, True)\n        )\n\n        for column_name in columns:\n            column = self.get_column(column_name)\n            column.set_notnull(True)\n\n        return self", "category": "Python"}, {"instruction": "def process_apk(self, data, name):\n        \"\"\"\n        Processes Android application\n        :param data:\n        :param name:\n        :return:\n        \"\"\"\n", "input": "", "output": "        try:\n            from apk_parse.apk import APK\n        except Exception as e:\n            logger.warning('Could not import apk_parse, try running: pip install apk_parse_ph4')\n            return [TestResult(fname=name, type='apk-pem-cert', error='cannot-import')]\n\n        ret = []\n        try:\n            from cryptography.x509.base import load_der_x509_certificate\n            apkf = APK(data, process_now=False, process_file_types=False, raw=True,\n                       temp_dir=self.args.tmp_dir)\n            apkf.process()\n            self.num_apk += 1\n\n            pem = apkf.cert_pem\n            aux = {'subtype': 'apk'}\n\n            x509 = load_der_x509_certificate(pem_to_der(pem), self.get_backend())\n\n            sub = self.process_x509(x509, name=name, idx=0, data=data, pem=True, source='apk-pem-cert', aux=aux)\n            ret.append(sub)\n\n        except Exception as e:\n            logger.debug('Exception in processing APK %s : %s' % (name, e))\n            self.trace_logger.log(e)\n        return ret", "category": "Python"}, {"instruction": "def is_url(default_scheme='http', **kwargs):\n    \"\"\"Return a converter that converts a clean string to an URL.\"\"\"\n", "input": "", "output": "    def converter(value):\n        if value is None:\n            return value\n        if '://' not in value and default_scheme:\n            value = '://'.join((default_scheme, value.strip()))\n        try:\n            return uris.validate(value)\n        except uris.ValidationError as e:\n            raise Invalid(e.message)\n    return converter", "category": "Python"}, {"instruction": "def create_blueprint(state):\n    \"\"\"Create blueprint serving JSON schemas.\n\n    :param state: :class:`invenio_jsonschemas.ext.InvenioJSONSchemasState`\n        instance used to retrieve the schemas.\n    \"\"\"\n", "input": "", "output": "    blueprint = Blueprint(\n        'invenio_jsonschemas',\n        __name__,\n    )\n\n    @blueprint.route('/<path:schema_path>')\n    def get_schema(schema_path):\n        ", "category": "Python"}, {"instruction": "def database_root_path(cls, project, database):\n        \"\"\"Return a fully-qualified database_root string.\"\"\"\n", "input": "", "output": "        return google.api_core.path_template.expand(\n            \"projects/{project}/databases/{database}\",\n            project=project,\n            database=database,\n        )", "category": "Python"}, {"instruction": "def delete_connections(self, **kwargs):\n        \"\"\"Remove a single connection to a provider for the specified user.\"\"\"\n", "input": "", "output": "        rv = False\n        for c in self.find_connections(**kwargs):\n            self.delete(c)\n            rv = True\n        return rv", "category": "Python"}, {"instruction": "def msg2processor(msg, **config):\n    \"\"\" For a given message return the text processor that can handle it.\n\n    This will raise a :class:`fedmsg.meta.ProcessorsNotInitialized` exception\n    if :func:`fedmsg.meta.make_processors` hasn't been called yet.\n    \"\"\"\n", "input": "", "output": "    for processor in processors:\n        if processor.handle_msg(msg, **config) is not None:\n            return processor\n    else:\n        return processors[-1]", "category": "Python"}, {"instruction": "def QA_indicator_CCI(DataFrame, N=14):\n    \"\"\"\n    TYP:=(HIGH+LOW+CLOSE)/3;\n    CCI:(TYP-MA(TYP,N))/(0.015*AVEDEV(TYP,N));\n    \"\"\"\n", "input": "", "output": "    typ = (DataFrame['high'] + DataFrame['low'] + DataFrame['close']) / 3\n    cci = ((typ - MA(typ, N)) / (0.015 * AVEDEV(typ, N)))\n    a = 100\n    b = -100\n\n    return pd.DataFrame({\n        'CCI': cci, 'a': a, 'b': b\n    })", "category": "Python"}, {"instruction": "def write_to_screen(self, cli, screen, mouse_handlers, write_position):\n        \"\"\"\n        Render the prompt to a `Screen` instance.\n\n        :param screen: The :class:`~prompt_toolkit.layout.screen.Screen` class\n            to which the output has to be written.\n        \"\"\"\n", "input": "", "output": "        sizes = self._divide_heigths(cli, write_position)\n\n        if self.report_dimensions_callback:\n            self.report_dimensions_callback(cli, sizes)\n\n        if sizes is None:\n            self.window_too_small.write_to_screen(\n                cli, screen, mouse_handlers, write_position)\n        else:\n            # Draw child panes.\n            ypos = write_position.ypos\n            xpos = write_position.xpos\n            width = write_position.width\n\n            for s, c in zip(sizes, self.children):\n                c.write_to_screen(cli, screen, mouse_handlers, WritePosition(xpos, ypos, width, s))\n                ypos += s", "category": "Python"}, {"instruction": "def create_space(self, space_name, add_users=True):\n        \"\"\"\n        Create a new space with the given name in the current target\n        organization.\n        \"\"\"\n", "input": "", "output": "        body = {\n            'name': space_name,\n            'organization_guid': self.api.config.get_organization_guid()\n        }\n\n        # MAINT: may need to do this more generally later\n        if add_users:\n            space_users = []\n            org_users = self.org.get_users()\n            for org_user in org_users['resources']:\n                guid = org_user['metadata']['guid']\n                space_users.append(guid)\n\n            body['manager_guids'] = space_users\n            body['developer_guids'] = space_users\n\n        return self.api.post('/v2/spaces', body)", "category": "Python"}, {"instruction": "def get_instance(self, payload):\n        \"\"\"\n        Build an instance of ConferenceInstance\n\n        :param dict payload: Payload response from the API\n\n        :returns: twilio.rest.api.v2010.account.conference.ConferenceInstance\n        :rtype: twilio.rest.api.v2010.account.conference.ConferenceInstance\n        \"\"\"\n", "input": "", "output": "        return ConferenceInstance(self._version, payload, account_sid=self._solution['account_sid'], )", "category": "Python"}, {"instruction": "def participants(self):\n        \"\"\"\n        Access the participants\n\n        :returns: twilio.rest.messaging.v1.session.participant.ParticipantList\n        :rtype: twilio.rest.messaging.v1.session.participant.ParticipantList\n        \"\"\"\n", "input": "", "output": "        if self._participants is None:\n            self._participants = ParticipantList(self._version, session_sid=self._solution['sid'], )\n        return self._participants", "category": "Python"}, {"instruction": "def branch_rate(self, filename=None):\n        \"\"\"\n        Return the global branch rate of the coverage report. If the\n        `filename` file is given, return the branch rate of the file.\n        \"\"\"\n", "input": "", "output": "        if filename is None:\n            el = self.xml\n        else:\n            el = self._get_class_element_by_filename(filename)\n\n        return float(el.attrib['branch-rate'])", "category": "Python"}, {"instruction": "def get_lib_name(self):\n        \"\"\" Parse Cargo.toml to get the name of the shared library. \"\"\"\n", "input": "", "output": "        # We import in here to make sure the the setup_requires are already installed\n        import toml\n\n        cfg = toml.load(self.path)\n        name = cfg.get(\"lib\", {}).get(\"name\")\n        if name is None:\n            name = cfg.get(\"package\", {}).get(\"name\")\n        if name is None:\n            raise Exception(\n                \"Can not parse library name from Cargo.toml. \"\n                \"Cargo.toml missing value for 'name' key \"\n                \"in both the [package] section and the [lib] section\"\n            )\n        name = re.sub(r\"[./\\\\-]\", \"_\", name)\n        return name", "category": "Python"}, {"instruction": "def i8(self, name, value=None, align=None):\n        \"\"\"Add an 1 byte integer field to template.\n\n        This is an convenience method that simply calls `Int` keyword with predefined length.\"\"\"\n", "input": "", "output": "        self.int(1, name, value, align)", "category": "Python"}, {"instruction": "def _getFirstPathExpression(name):\n    \"\"\"Returns the first metric path in an expression.\"\"\"\n", "input": "", "output": "    tokens = grammar.parseString(name)\n    pathExpression = None\n    while pathExpression is None:\n        if tokens.pathExpression:\n            pathExpression = tokens.pathExpression\n        elif tokens.expression:\n            tokens = tokens.expression\n        elif tokens.call:\n            tokens = tokens.call.args[0]\n        else:\n            break\n    return pathExpression", "category": "Python"}, {"instruction": "def get_args_index(target) -> int:\n    \"\"\"\n    Returns the index of the \"*args\" parameter if such a parameter exists in\n    the function arguments or -1 otherwise.\n\n    :param target:\n        The target function for which the args index should be determined\n    :return:\n        The arguments index if it exists or -1 if not\n    \"\"\"\n", "input": "", "output": "\n    code = target.__code__\n\n    if not bool(code.co_flags & inspect.CO_VARARGS):\n        return -1\n\n    return code.co_argcount + code.co_kwonlyargcount", "category": "Python"}, {"instruction": "def makeductcomponent(idf, dname):\n    \"\"\"make a duct component\n    generate inlet outlet names\"\"\"\n", "input": "", "output": "    aduct = idf.newidfobject(\"duct\".upper(), Name=dname)\n    aduct.Inlet_Node_Name = \"%s_inlet\" % (dname,)\n    aduct.Outlet_Node_Name = \"%s_outlet\" % (dname,)\n    return aduct", "category": "Python"}, {"instruction": "def bvlpdu_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n", "input": "", "output": "        broadcast_distribution_table = []\n        for bdte in self.bvlciBDT:\n            broadcast_distribution_table.append(str(bdte))\n\n        return key_value_contents(use_dict=use_dict, as_class=as_class,\n            key_values=(\n                ('function', 'ReadBroadcastDistributionTableAck'),\n                ('bdt', broadcast_distribution_table),\n            ))", "category": "Python"}, {"instruction": "def build_url(self):\n        \"\"\"Builds the URL for elevations API services based on the data given\n        by the user.\n\n        Returns:\n            url (str): URL for the elevations API services\n        \"\"\"\n", "input": "", "output": "        url = '{protocol}/{url}/{rest}/{version}/{restapi}/{rscpath}/' \\\n              '{query}'.format(protocol=self.schema.protocol,\n                               url=self.schema.main_url,\n                               rest=self.schema.rest,\n                               version=self.schema.version,\n                               restapi=self.schema.restApi,\n                               rscpath=self.schema.resourcePath,\n                               query=self.schema.query)\n        return url.replace('/None/', '/')", "category": "Python"}, {"instruction": "def props(cls):\n    \"\"\"\n    Class method that returns all defined arguments within the class.\n    \n    Returns:\n      A dictionary containing all action defined arguments (if any).\n    \"\"\"\n", "input": "", "output": "    return {k:v for (k, v) in inspect.getmembers(cls) if type(v) is Argument}", "category": "Python"}, {"instruction": "def get_image_set(self):\n        \"\"\"\n        Obtain existing ImageSet if `pk` is specified, otherwise\n        create a new ImageSet for the user.\n        \"\"\"\n", "input": "", "output": "        image_set_pk = self.kwargs.get(\"pk\", None)\n        if image_set_pk is None:\n            return self.request.user.image_sets.create()\n        return get_object_or_404(self.get_queryset(), pk=image_set_pk)", "category": "Python"}, {"instruction": "def _get_api_url(self, secure=None, **formatters):\n        '''Constructs Postmark API url\n\n        :param secure: Use the https Postmark API.\n        :param \\*\\*formatters: :func:`string.format` keyword arguments to\n            format the url with.\n        :rtype: Postmark API url\n        '''\n", "input": "", "output": "        if self.endpoint is None:\n            raise NotImplementedError('endpoint must be defined on a subclass')\n        if secure is None:\n            secure = self.secure\n        if secure:\n            api_url = POSTMARK_API_URL_SECURE\n        else:\n            api_url = POSTMARK_API_URL\n        url = urljoin(api_url, self.endpoint)\n        if formatters:\n            url = url.format(**formatters)\n        return url", "category": "Python"}, {"instruction": "def _convert_to_3(path):  # pragma: no cover\n    \"\"\"Convert python 2 file to python 3.\"\"\"\n", "input": "", "output": "    try:\n        log.warn('##### Trying to convert %s to Python 3. #####', path)\n        subprocess.call(['2to3', '-w', path])\n    except subprocess.SubprocessError:\n        log.exception('Check if 2to3 is installed. https://docs.python.org/2/library/2to3.html')\n        exit(1)", "category": "Python"}, {"instruction": "def end(target):\n    \"\"\"schedule a greenlet to be stopped immediately\n\n    :param target: the greenlet to end\n    :type target: greenlet\n    \"\"\"\n", "input": "", "output": "    if not isinstance(target, compat.greenlet):\n        raise TypeError(\"argument must be a greenlet\")\n    if not target.dead:\n        schedule(target)\n        state.to_raise[target] = compat.GreenletExit()", "category": "Python"}, {"instruction": "def aspects(self, obj):\n        \"\"\" Returns true if this star aspects another object.\n        Fixed stars only aspect by conjunctions. \n        \n        \"\"\"\n", "input": "", "output": "        dist = angle.closestdistance(self.lon, obj.lon)\n        return abs(dist) < self.orb()", "category": "Python"}, {"instruction": "def launch_minecraft(port, installdir=\"MalmoPlatform\", replaceable=False):\n    \"\"\"Launch Minecraft listening for malmoenv connections.\n    Args:\n        port:  the TCP port to listen on.\n        installdir: the install dir name. Defaults to MalmoPlatform.\n        Must be same as given (or defaulted) in download call if used.\n        replaceable: whether or not to automatically restart Minecraft (default is false).\n    \"\"\"\n", "input": "", "output": "    launch_script = './launchClient.sh'\n    if os.name == 'nt':\n        launch_script = 'launchClient.bat'\n    cwd = os.getcwd()\n    os.chdir(installdir)\n    os.chdir(\"Minecraft\")\n    try:\n        cmd = [launch_script, '-port', str(port), '-env']\n        if replaceable:\n            cmd.append('-replaceable')\n        subprocess.check_call(cmd)\n    finally:\n        os.chdir(cwd)", "category": "Python"}, {"instruction": "def extract_forward_and_reverse_complement(\n            self, forward_reads_to_extract, reverse_reads_to_extract, database_fasta_file,\n            output_file):\n        '''As per extract except also reverse complement the sequences.'''\n", "input": "", "output": "        self.extract(forward_reads_to_extract, database_fasta_file, output_file)\n        cmd_rev = \"fxtract -XH -f /dev/stdin '%s'\" % database_fasta_file\n\n        output = extern.run(cmd_rev, stdin='\\n'.join(reverse_reads_to_extract))\n\n        with open(output_file, 'a') as f:\n            for record in SeqIO.parse(StringIO(output), 'fasta'):\n                record.seq = record.reverse_complement().seq\n                SeqIO.write(record, f, 'fasta')", "category": "Python"}, {"instruction": "def insert_paraphrase_information(germanet_db, wiktionary_files):\n    '''\n    Reads in the given GermaNet relation file and inserts its contents\n    into the given MongoDB database.\n\n    Arguments:\n    - `germanet_db`: a pymongo.database.Database object\n    - `wiktionary_files`:\n    '''\n", "input": "", "output": "    num_paraphrases = 0\n    # cache the lexunits while we work on them\n    lexunits = {}\n    for filename in wiktionary_files:\n        paraphrases = read_paraphrase_file(filename)\n        num_paraphrases += len(paraphrases)\n        for paraphrase in paraphrases:\n            if paraphrase['lexUnitId'] not in lexunits:\n                lexunits[paraphrase['lexUnitId']] = \\\n                    germanet_db.lexunits.find_one(\n                    {'id': paraphrase['lexUnitId']})\n            lexunit = lexunits[paraphrase['lexUnitId']]\n            if 'paraphrases' not in lexunit:\n                lexunit['paraphrases'] = []\n            lexunit['paraphrases'].append(paraphrase)\n    for lexunit in lexunits.values():\n        germanet_db.lexunits.save(lexunit)\n\n    print('Inserted {0} wiktionary paraphrases.'.format(num_paraphrases))", "category": "Python"}, {"instruction": "def add_densities(density1, density2):\n    \"\"\"\n    Method to sum two densities.\n\n    Args:\n        density1: First density.\n        density2: Second density.\n\n    Returns:\n        Dict of {spin: density}.\n    \"\"\"\n", "input": "", "output": "    return {spin: np.array(density1[spin]) + np.array(density2[spin])\n            for spin in density1.keys()}", "category": "Python"}, {"instruction": "def _update_data(self, *data_dict, **kwargs):\n        \"\"\"\n        A private method to process and update entity values correctly.\n\n        :param data: A dictionary of values to be updated for the entity\n        :param kwargs: keyword arguments with key-value pairs to be updated\n        \"\"\"\n", "input": "", "output": "\n        # Load each of the fields given in the data dictionary\n        self.errors = {}\n\n        for data in data_dict:\n            if not isinstance(data, dict):\n                raise AssertionError(\n                    f'Positional argument \"{data}\" passed must be a dict.'\n                    f'This argument serves as a template for loading common '\n                    f'values.'\n                )\n            for field_name, val in data.items():\n                setattr(self, field_name, val)\n\n        # Now load against the keyword arguments\n        for field_name, val in kwargs.items():\n            setattr(self, field_name, val)\n\n        # Raise any errors found during update\n        if self.errors:\n            raise ValidationError(self.errors)", "category": "Python"}, {"instruction": "def stop():\n    \"\"\"\n    Stop recording stats.  Call this from a benchmark script when the code you\n    want benchmarked has finished.  Call this exactly the same number of times\n    you call L{start} and only after calling it.\n\n    @raise RuntimeError: Raised if the parent process responds with anything\n    other than an acknowledgement of this message.\n    \"\"\"\n", "input": "", "output": "    os.write(BenchmarkProcess.BACKCHANNEL_OUT, BenchmarkProcess.STOP)\n    response = util.untilConcludes(os.read, BenchmarkProcess.BACKCHANNEL_IN, 1)\n    if response != BenchmarkProcess.STOP:\n        raise RuntimeError(\n            \"Parent process responded with %r instead of STOP\" % (response,))", "category": "Python"}, {"instruction": "def getPositionByName(self, name):\n        \"\"\"Return field position by filed name.\n\n        Parameters\n        ----------\n        name: :py:class:`str`\n            Field name\n\n        Returns\n        -------\n        : :py:class:`int`\n            Field position in fields set\n\n        Raises\n        ------\n        : :class:`~pyasn1.error.PyAsn1Error`\n            If *name* is not present or not unique within callee *NamedTypes*\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.__nameToPosMap[name]\n\n        except KeyError:\n            raise error.PyAsn1Error('Name %s not found' % (name,))", "category": "Python"}, {"instruction": "def matrix_product(mat1, mat2):\n    \"\"\"Compute the product of two Fortran contiguous matrices.\n\n    This is to avoid the overhead of NumPy converting to C-contiguous\n    before computing a matrix product.\n\n    Does so via ``A B = (B^T A^T)^T`` since ``B^T`` and ``A^T`` will be\n    C-contiguous without a copy, then the product ``P = B^T A^T`` will\n    be C-contiguous and we can return the view ``P^T`` without a copy.\n\n    Args:\n        mat1 (numpy.ndarray): The left-hand side matrix.\n        mat2 (numpy.ndarray): The right-hand side matrix.\n\n    Returns:\n        numpy.ndarray: The product of the two matrices.\n    \"\"\"\n", "input": "", "output": "    return np.dot(mat2.T, mat1.T).T", "category": "Python"}, {"instruction": "def countriesdata(cls, use_live=True):\n        # type: (bool) -> List[Dict[Dict]]\n        \"\"\"\n        Read countries data from OCHA countries feed (falling back to file)\n\n        Args:\n            use_live (bool): Try to get use latest data from web rather than file in package. Defaults to True.\n\n        Returns:\n            List[Dict[Dict]]: Countries dictionaries\n        \"\"\"\n", "input": "", "output": "        if cls._countriesdata is None:\n            countries = None\n            if use_live:\n                try:\n                    countries = hxl.data(cls._ochaurl)\n                except IOError:\n                    logger.exception('Download from OCHA feed failed! Falling back to stored file.')\n            if countries is None:\n                countries = hxl.data(\n                    script_dir_plus_file('Countries & Territories Taxonomy MVP - C&T Taxonomy with HXL Tags.csv',\n                                         Country), allow_local=True)\n            cls.set_countriesdata(countries)\n        return cls._countriesdata", "category": "Python"}, {"instruction": "def delete_actions(self, form_id, action_ids):\n        \"\"\"\n        Remove actions from a form\n\n        :param form_id: int\n        :param action_ids: list|tuple\n        :return: dict|str\n        \"\"\"\n", "input": "", "output": "\n        response = self._client.session.delete(\n            '{url}/{form_id}/actions/delete'.format(\n                url=self.endpoint_url, form_id=form_id\n            ),\n            params={'actions': action_ids}\n        )\n        return self.process_response(response)", "category": "Python"}, {"instruction": "def _checkIfClusterExists(self):\n        \"\"\"\n        Try deleting the resource group. This will fail if it exists and raise an exception.\n        \"\"\"\n", "input": "", "output": "        ansibleArgs = {\n            'resgrp': self.clusterName,\n            'region': self._zone\n        }\n        try:\n            self.callPlaybook(self.playbook['check-cluster'], ansibleArgs, wait=True)\n        except RuntimeError:\n            logger.info(\"The cluster could not be created. Try deleting the cluster if it already exits.\")\n            raise", "category": "Python"}, {"instruction": "def detach(self):\n        \"\"\"If alive then mark as dead and return (obj, func, args, kwargs);\n        otherwise return None\"\"\"\n", "input": "", "output": "        info = self._registry.get(self)\n        obj = info and info.weakref()\n        if obj is not None and self._registry.pop(self, None):\n            return (obj, info.func, info.args, info.kwargs or {})", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"\n        A context manager starting up threads to send and receive data from a\n        gateway and handle callbacks. Yields when a connection has been made,\n        and cleans up connections and threads when it's done.\n        \"\"\"\n", "input": "", "output": "        listener_thr = _spawn(self.receiver.run)\n        callback_thr = _spawn(self.callbacks.run)\n        sender_thr = _spawn(self.sender.run)\n        logger_thr = _spawn(self.logger.run)\n\n        self.connect()\n        try:\n            yield\n        finally:\n            self.stop()\n\n            # Wait for the listener to finish.\n            listener_thr.join()\n            self.callbacks.put('shutdown')\n\n            # Tell the other threads to finish, and wait for them.\n            for obj in [self.callbacks, self.sender, self.logger]:\n                obj.stop()\n            for thr in [callback_thr, sender_thr, logger_thr]:\n                thr.join()", "category": "Python"}, {"instruction": "def list_all_states(cls, **kwargs):\n        \"\"\"List States\n\n        Return a list of States\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async=True\n        >>> thread = api.list_all_states(async=True)\n        >>> result = thread.get()\n\n        :param async bool\n        :param int page: page number\n        :param int size: page size\n        :param str sort: page order\n        :return: page[State]\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async'):\n            return cls._list_all_states_with_http_info(**kwargs)\n        else:\n            (data) = cls._list_all_states_with_http_info(**kwargs)\n            return data", "category": "Python"}, {"instruction": "def article_parse(self, response, rss_title=None):\n        \"\"\"\n        Checks any given response on being an article and if positiv,\n        passes the response to the pipeline.\n\n        :param obj response: The scrapy response\n        :param str rss_title: Title extracted from the rss feed\n        \"\"\"\n", "input": "", "output": "        if not self.helper.parse_crawler.content_type(response):\n            return\n\n        yield self.helper.parse_crawler.pass_to_pipeline_if_article(\n            response, self.ignored_allowed_domain, self.original_url,\n            rss_title)", "category": "Python"}, {"instruction": "def updateData(self, state_data, action_data, reward_data):\n        \"\"\" Updates the data used by the renderer.\n        \"\"\"\n", "input": "", "output": "#        self.dataLock.acquire()\n\n        self.state_data[:, self.updates] = state_data\n        self.action_data[:, self.updates] = action_data\n        self.reward_data[0, self.updates] = reward_data\n        self.updates += 1\n        self._render()", "category": "Python"}, {"instruction": "def client_tokens(self):\n        \"\"\"try and get Oauth 2.0 client id and secret first from basic auth header,\n        then from GET or POST parameters\n\n        return -- tuple -- client_id, client_secret\n        \"\"\"\n", "input": "", "output": "        client_id, client_secret = self.get_auth_basic()\n        if not client_id and not client_secret:\n            client_id = self.query_kwargs.get('client_id', '')\n            client_secret = self.query_kwargs.get('client_secret', '')\n            if not client_id and not client_secret:\n                client_id = self.body_kwargs.get('client_id', '')\n                client_secret = self.body_kwargs.get('client_secret', '')\n\n        return client_id, client_secret", "category": "Python"}, {"instruction": "def settings_api():\n    \"\"\"\n    Virtual ``settings.py`` with values transported from real\n    :mod:`settings.py`, so the Brython frontend may be configured same way as\n    backend.\n\n    Some of the critical values are intentionally left out.\n    \"\"\"\n", "input": "", "output": "    variables = [\n        \"%s = %s\" % (var, repr(getattr(settings, var)))\n        for var in sorted(dir(settings))\n        if (not var.startswith(\"_\") and var.upper() == var and\n            not var.startswith(\"SEEDER\"))  # hide the private tokens and url\n    ]\n\n    return PY_HEADER + \"\\n\\n\".join(variables)", "category": "Python"}, {"instruction": "def rbd_exists(service, pool, rbd_img):\n    \"\"\"Check to see if a RADOS block device exists.\"\"\"\n", "input": "", "output": "    try:\n        out = check_output(['rbd', 'list', '--id',\n                            service, '--pool', pool])\n        if six.PY3:\n            out = out.decode('UTF-8')\n    except CalledProcessError:\n        return False\n\n    return rbd_img in out", "category": "Python"}, {"instruction": "def count_children(obj, type=None):\n    \"\"\"Return the number of children of obj, optionally restricting by class\"\"\"\n", "input": "", "output": "    if type is None:\n        return len(obj)\n    else:\n        # there doesn't appear to be any hdf5 function for getting this\n        # information without inspecting each child, which makes this somewhat\n        # slow\n        return sum(1 for x in obj if obj.get(x, getclass=True) is type)", "category": "Python"}, {"instruction": "def get_color_mode(mode):\n    \"\"\"Convert PIL mode to ColorMode.\"\"\"\n", "input": "", "output": "    name = mode.upper()\n    name = name.rstrip('A')  # Trim alpha.\n    name = {'1': 'BITMAP', 'L': 'GRAYSCALE'}.get(name, name)\n    return getattr(ColorMode, name)", "category": "Python"}, {"instruction": "def shift_up_left(self, times=1):\n        \"\"\"\n        Finds Location shifted up left by 1\n\n        :rtype: Location\n        \"\"\"\n", "input": "", "output": "        try:\n            return Location(self._rank + times, self._file - times)\n        except IndexError as e:\n            raise IndexError(e)", "category": "Python"}, {"instruction": "def get_body(name):\n    \"\"\"Retrieve the Body structure of a JPL .bsp file object\n\n    Args:\n        name (str)\n    Return:\n        :py:class:`~beyond.constants.Body`\n    \"\"\"\n", "input": "", "output": "\n    body = Pck()[name]\n    body.propagate = lambda date: get_orbit(name, date)\n    return body", "category": "Python"}, {"instruction": "def arcsin_sqrt(biom_tbl):\n    \"\"\"\n    Applies the arcsine square root transform to the\n    given BIOM-format table\n    \"\"\"\n", "input": "", "output": "    arcsint = lambda data, id_, md: np.arcsin(np.sqrt(data))\n\n    tbl_relabd = relative_abd(biom_tbl)\n    tbl_asin = tbl_relabd.transform(arcsint, inplace=False)\n\n    return tbl_asin", "category": "Python"}, {"instruction": "def tuple(self, r):\n        \"\"\"\n        Converts the linear_index `q` into an chimera_index\n\n        Parameters\n        ----------\n        r : int\n            The linear_index node label    \n\n        Returns\n        -------\n        q : tuple\n            The chimera_index node label corresponding to r\n        \"\"\"\n", "input": "", "output": "\n        m, n, t = self.args\n        r, k = divmod(r, t)\n        r, u = divmod(r, 2)\n        i, j = divmod(r, n)\n        return i, j, u, k", "category": "Python"}, {"instruction": "def create_backup(self, name, description=None):\n        \"\"\"\n        Creates a backup of this instance, giving it the specified name along\n        with an optional description.\n        \"\"\"\n", "input": "", "output": "        return self.manager.create_backup(self, name, description=description)", "category": "Python"}, {"instruction": "def _construct_number_token(self, d: Dict, nlp) -> List[Dict]:\n        \"\"\"\n        Construct a shape token\n        Args:\n            d: Dict\n            nlp\n\n        Returns: List[Dict]\n        \"\"\"\n", "input": "", "output": "\n        result = []\n        if not d[\"numbers\"]:\n            this_token = {attrs.LIKE_NUM: True}\n            result.append(this_token)\n            if d[\"length\"]:\n                result = self._add_length_constrain(result, d[\"length\"])\n        elif len(d[\"numbers\"]) == 1:\n            this_token = {attrs.ORTH: str(d[\"numbers\"][0])}\n            result.append(this_token)\n        else:\n            global FLAG_ID\n            number_set = set(d[\"numbers\"])\n\n            def is_selected_number(x):\n                return x in number_set\n\n            FLAG_DICT[FLAG_ID] = nlp.vocab.add_flag(is_selected_number)\n            this_token = {FLAG_DICT[FLAG_ID]: True}\n            FLAG_ID += 1\n            result.append(this_token)\n        result = self._add_common_constrain(result, d)\n        return result", "category": "Python"}, {"instruction": "def _process_net_mhcii(mhc_file, normal=False):\n    \"\"\"\n    Process the results from running NetMHCIIpan binding predictions into a pandas dataframe.\n\n    :param str mhc_file: Output file containing netmhciipan mhcii:peptide binding predictions\n    :param bool normal: Is this processing the results of a normal?\n    :return: Results in a tabular format\n    :rtype: pandas.DataFrame\n    \"\"\"\n", "input": "", "output": "    results = pandas.DataFrame(columns=['allele', 'pept', 'tumor_pred', 'core', 'peptide_name'])\n    with open(mhc_file, 'r') as mf:\n        peptides = set()\n        # Get the allele from the first line and skip the second line\n        allele = re.sub('-DQB', '/DQB', mf.readline().strip())\n        _ = mf.readline()\n        for line in mf:\n            line = line.strip().split('\\t')\n            pept = line[1]\n            pred = line[5]\n            core = 'NOCORE'\n            peptide_name = line[2]\n            if float(pred) > 5.00 and not normal:\n                continue\n            results.loc[len(results)] = [allele, pept, pred, core, peptide_name]\n    results.drop_duplicates(inplace=True)\n    return results", "category": "Python"}, {"instruction": "def _prt_line_detail(self, prt, values, lnum=\"\"):\n        \"\"\"Print header and field values in a readable format.\"\"\"\n", "input": "", "output": "        #### data = zip(self.req_str, self.ntgafobj._fields, values)\n        data = zip(self.req_str, self.flds, values)\n        txt = [\"{:2}) {:3} {:20} {}\".format(i, req, hdr, val) for i, (req, hdr, val) in enumerate(data)]\n        prt.write(\"{LNUM}\\n{TXT}\\n\".format(LNUM=lnum, TXT=\"\\n\".join(txt)))", "category": "Python"}, {"instruction": "def coal(return_X_y=True):\n    \"\"\"coal-mining accidents dataset\n\n    Parameters\n    ----------\n    return_X_y : bool,\n        if True, returns a model-ready tuple of data (X, y)\n        otherwise, returns a Pandas DataFrame\n\n    Returns\n    -------\n    model-ready tuple of data (X, y)\n        OR\n    Pandas DataFrame\n\n    Notes\n    -----\n    The (X, y) tuple is a processed version of the otherwise raw DataFrame.\n\n    A histogram of 150 bins has been computed describing the number accidents per year.\n\n    X contains the midpoints of histogram bins.\n    y contains the count in each histogram bin.\n\n    Source:\n    https://vincentarelbundock.github.io/Rdatasets/doc/boot/coal.html\n    \"\"\"\n", "input": "", "output": "    # y is counts\n    # recommend PoissonGAM\n    coal = pd.read_csv(PATH + '/coal.csv', index_col=0)\n    if return_X_y:\n        y, x = np.histogram(coal.values, bins=150)\n        X = x[:-1] + np.diff(x)/2 # get midpoints of bins\n        return _clean_X_y(X, y)\n    return coal", "category": "Python"}, {"instruction": "def parse_value(self, sn: \"DataNode\") -> ScalarValue:\n        \"\"\"Let schema node's type parse the receiver's value.\"\"\"\n", "input": "", "output": "        res = sn.type.parse_value(self.value)\n        if res is None:\n            raise InvalidKeyValue(self.value)\n        return res", "category": "Python"}, {"instruction": "def bed_occupied(self):\n        \"\"\"\n        bed_occupied: None -> boolean\n\n        Determines whether the bed is currently occupied by requesting\n        data from the remote XBee and comparing the analog value with\n        a threshold.\n        \"\"\"\n", "input": "", "output": "\n        # Receive samples from the remote device\n        self._set_send_samples(True)\n\n        while True:\n            packet = self.hw.wait_read_frame()\n\n            if 'adc-0' in packet['samples'][0]:\n                # Stop receiving samples from the remote device\n                self._set_send_samples(False)\n                return packet['samples'][0]['adc-0'] > XBeeAlarm.DETECT_THRESH", "category": "Python"}, {"instruction": "def add(self, loan_id, amount):\n        \"\"\"\n        Add a loan and amount you want to invest, to your order.\n        If this loan is already in your order, it's amount will be replaced\n        with the this new amount\n\n        Parameters\n        ----------\n        loan_id : int or dict\n            The ID of the loan you want to add or a dictionary containing a `loan_id` value\n        amount : int % 25\n            The dollar amount you want to invest in this loan, as a multiple of 25.\n        \"\"\"\n", "input": "", "output": "        assert amount > 0 and amount % 25 == 0, 'Amount must be a multiple of 25'\n        assert type(amount) in (float, int), 'Amount must be a number'\n\n        if type(loan_id) is dict:\n            loan = loan_id\n            assert 'loan_id' in loan and type(loan['loan_id']) is int, 'loan_id must be a number or dictionary containing a loan_id value'\n            loan_id = loan['loan_id']\n\n        assert type(loan_id) in [str, unicode, int], 'Loan ID must be an integer number or a string'\n        self.loans[loan_id] = amount", "category": "Python"}, {"instruction": "def get_user_id_from_email(self, email):\n        \"\"\" Uses the get-all-user-accounts Portals API to retrieve the\n        user-id by supplying an email. \"\"\"\n", "input": "", "output": "        accts = self.get_all_user_accounts()\n\n        for acct in accts:\n            if acct['email'] == email:\n                return acct['id']\n        return None", "category": "Python"}, {"instruction": "def collapse_user(fp):\n    \"\"\"\n    Converts a path back to ~/ from expanduser()\n    \"\"\"\n", "input": "", "output": "    home_dir = os.path.expanduser(\"~\")\n    abs_path = os.path.abspath(fp)\n    return abs_path.replace(home_dir, \"~\")", "category": "Python"}, {"instruction": "def any_text_to_fernet_key(self, text):\n        \"\"\"\n        Convert any text to a fernet key for encryption.\n        \"\"\"\n", "input": "", "output": "        md5 = fingerprint.fingerprint.of_text(text)\n        fernet_key = base64.b64encode(md5.encode(\"utf-8\"))\n        return fernet_key", "category": "Python"}, {"instruction": "def kmer_counter(seq, k=4):\n    \"\"\"Return a sequence of all the unique substrings (k-mer or q-gram) within a short (<128 symbol) string\n\n    Used for algorithms like UniqTag for genome unique identifier locality sensitive hashing.\n\n    jellyfish is a C implementation of k-mer counting\n\n    If seq is a string generate a sequence of k-mer string\n    If seq is a sequence of strings then generate a sequence of generators or sequences of k-mer strings\n    If seq is a sequence of sequences of strings generate a sequence of sequence of generators ...\n\n    Default k = 4 because that's the length of a gene base-pair?\n\n    >>> kmer_counter('AGATAGATAGACACAGAAATGGGACCACAC') == Counter({'ACAC': 2, 'ATAG': 2, 'CACA': 2,\n    ...     'TAGA': 2, 'AGAT': 2, 'GATA': 2, 'AGAC': 1, 'ACAG': 1, 'AGAA': 1, 'AAAT': 1, 'TGGG': 1, 'ATGG': 1,\n    ...     'ACCA': 1, 'GGAC': 1, 'CCAC': 1, 'CAGA': 1, 'GAAA': 1, 'GGGA': 1, 'GACA': 1, 'GACC': 1, 'AATG': 1})\n    True\n    \"\"\"\n", "input": "", "output": "    if isinstance(seq, basestring):\n        return Counter(generate_kmers(seq, k))", "category": "Python"}, {"instruction": "def map_new(w: int, h: int) -> tcod.map.Map:\n    \"\"\"Return a :any:`tcod.map.Map` with a width and height.\n\n    .. deprecated:: 4.5\n        Use the :any:`tcod.map` module for working with field-of-view,\n        or :any:`tcod.path` for working with path-finding.\n    \"\"\"\n", "input": "", "output": "    return tcod.map.Map(w, h)", "category": "Python"}, {"instruction": "def execution_order(self, refcounts):\n        \"\"\"\n        Return a topologically-sorted iterator over the terms in ``self`` which\n        need to be computed.\n        \"\"\"\n", "input": "", "output": "        return iter(nx.topological_sort(\n            self.graph.subgraph(\n                {term for term, refcount in refcounts.items() if refcount > 0},\n            ),\n        ))", "category": "Python"}, {"instruction": "def k_means_clustering(data,K):\n    \"\"\"\n    K-means clustering is an algorithm that take a data set and \n    a number of clusters K and returns the labels which represents\n    the clusters of data which are similar to others\n    \n    Parameters    \n    --------------------\n    data: array-like, shape= (m_samples,n_samples)\n    K: integer\n        number of K clusters   \n    Returns\n    -------\n    labels: array-like, shape (1,n_samples)    \n    \"\"\"\n", "input": "", "output": "    N = data.shape[0]\n    centroids, data_norms = orthogonal_initialization(data,K)\n    old_centroids= np.zeros((N,K))\n    labels = []\n    \n    # Run the main k-means algorithm\n    while not _has_converged(centroids, old_centroids):    \n        labels = get_labels(data, centroids,K)                \n        centroids = get_centroids(data,K,labels,centroids,data_norms)\n        old_centroids = centroids\n        \n    return labels", "category": "Python"}, {"instruction": "def _commands(ctx):\n    \"\"\"Prints a list of commands for shell completion hooks.\"\"\"\n", "input": "", "output": "    ctx = ctx.parent\n    ctx.show_hidden_subcommands = False\n    main = ctx.command\n\n    for subcommand in main.list_commands(ctx):\n        cmd = main.get_command(ctx, subcommand)\n        if cmd is None:\n            continue\n        help = cmd.short_help or \"\"\n        click.echo(\"{}:{}\".format(subcommand, help))", "category": "Python"}, {"instruction": "def create_cbz(directory):\n    \"\"\"Creates or updates a CBZ from files in the given comic directory.\"\"\"\n", "input": "", "output": "    if not os.path.isdir(directory):\n        print(\"ERROR: Directory\", directory, \"not found.\")\n        return\n    base = os.path.basename(directory.rstrip(os.path.sep))\n    zipname = '%s.cbz' % base\n    zipname = os.path.join(directory, zipname)\n    d = os.path.join(directory, 'inorder')\n    if os.path.isdir(d):\n        # use directory with ordered symlinks\n        directory = d\n    if os.path.exists(zipname):\n        os.remove(zipname)\n    with zipfile.ZipFile(zipname, 'w') as myzip:\n        for filename in sorted(os.listdir(d)):\n            fullname = os.path.join(d, filename)\n            if is_image(fullname):\n                myzip.write(fullname)\n        myzip.comment = get_cbz_comment()\n    print(\"INFO: Created\", zipname)", "category": "Python"}, {"instruction": "def grant_admin_privileges(self, username):\n        \"\"\"Grant cluster administration privileges to a user.\n\n        :param username: the username to grant privileges to\n        :type username: str\n\n        .. note:: Only a cluster administrator can create/drop databases\n            and manage users.\n        \"\"\"\n", "input": "", "output": "        text = \"GRANT ALL PRIVILEGES TO {0}\".format(quote_ident(username))\n        self.query(text, method=\"POST\")", "category": "Python"}, {"instruction": "def put_many(self, items):  # pragma: no cover\n        \"\"\"Put many key-value pairs.\n\n        This method may take advantage of performance or atomicity\n        features of the underlying store. It does not guarantee that\n        all items will be set in the same transaction, only that\n        transactions may be used for performance.\n\n        :param items: An iterable producing (key, value) tuples.\n\n        \"\"\"\n", "input": "", "output": "        for key, value in items:\n            self.put(key, value)", "category": "Python"}, {"instruction": "def wrap(text, width=70, **kwargs):\n    \"\"\"Wrap multiple paragraphs of text, returning a list of wrapped lines.\n\n    Reformat the multiple paragraphs  'text' so they fit in lines of no\n    more than 'width' columns, and return a list of wrapped lines.  By\n    default, tabs in 'text' are expanded with string.expandtabs(), and\n    all other whitespace characters (including newline) are converted to\n    space.  See ParagraphWrapper class for available keyword args to customize\n    wrapping behaviour.\n    \"\"\"\n", "input": "", "output": "    w = ParagraphWrapper(width=width, **kwargs)\n    return w.wrap(text)", "category": "Python"}, {"instruction": "def write_pkg_to_file(self, name, objects, path='.', filename=None):\n        \"\"\"Write a list of related objs to file\"\"\"\n", "input": "", "output": "        # Kibana uses an array of docs, do the same\n        # as opposed to a dict of docs\n        pkg_objs = []\n        for _, obj in iteritems(objects):\n            pkg_objs.append(obj)\n        sorted_pkg = sorted(pkg_objs, key=lambda k: k['_id'])\n        output = self.json_dumps(sorted_pkg) + '\\n'\n        if filename is None:\n            filename = self.safe_filename('Pkg', name)\n        filename = os.path.join(path, filename)\n        self.pr_inf(\"Writing to file: \" + filename)\n        with open(filename, 'w') as f:\n            f.write(output)\n        return filename", "category": "Python"}, {"instruction": "def RgbToGreyscale(r, g, b):\n    '''Convert the color from RGB to its greyscale equivalent\n\n    Parameters:\n      :r:\n        The Red component value [0...1]\n      :g:\n        The Green component value [0...1]\n      :b:\n        The Blue component value [0...1]\n\n    Returns:\n      The color as an (r, g, b) tuple in the range:\n      the range:\n      r[0...1],\n      g[0...1],\n      b[0...1]\n\n    >>> '(%g, %g, %g)' % Color.RgbToGreyscale(1, 0.8, 0)\n    '(0.6, 0.6, 0.6)'\n\n    '''\n", "input": "", "output": "    v = (r + g + b) / 3.0\n    return (v, v, v)", "category": "Python"}, {"instruction": "def RB_to_OPLS(c0, c1, c2, c3, c4, c5):\n    \"\"\"Converts Ryckaert-Bellemans type dihedrals to OPLS type.\n\n    Parameters\n    ----------\n    c0, c1, c2, c3, c4, c5 : Ryckaert-Belleman coefficients (in kcal/mol)\n\n    Returns\n    -------\n    opls_coeffs : np.array, shape=(4,)\n        Array containing the OPLS dihedrals coeffs f1, f2, f3, and f4\n        (in kcal/mol)\n\n    \"\"\"\n", "input": "", "output": "\n    f1 = (-1.5 * c3) - (2 * c1)\n    f2 = c0 + c1 + c3\n    f3 = -0.5 * c3\n    f4 = -0.25 * c4\n    return np.array([f1, f2, f3, f4])", "category": "Python"}, {"instruction": "def is_a_string(var, allow_none=False):\n    \"\"\" Returns True if var is a string (ascii or unicode)\n\n        Result             py-2  py-3\n        -----------------  ----- -----\n        b'bytes literal'   True  False\n         'string literal'  True  True\n        u'unicode literal' True  True\n\n        Also returns True if the var is a numpy string (numpy.string_, numpy.unicode_).\n    \"\"\"\n", "input": "", "output": "    return isinstance(var, six.string_types) or (var is None and allow_none)", "category": "Python"}, {"instruction": "def _handle_cast(self, node, scope, ctxt, stream):\n        \"\"\"Handle cast nodes\n\n        :node: TODO\n        :scope: TODO\n        :ctxt: TODO\n        :stream: TODO\n        :returns: TODO\n\n        \"\"\"\n", "input": "", "output": "        self._dlog(\"handling cast\")\n        to_type = self._handle_node(node.to_type, scope, ctxt, stream)\n        val_to_cast = self._handle_node(node.expr, scope, ctxt, stream)\n\n        res = to_type()\n        res._pfp__set_value(val_to_cast)\n        return res", "category": "Python"}, {"instruction": "def set_placeholder(self, key, value):\n        \"\"\"Placeholders are custom magic variables defined during configuration\n        time.\n\n        .. note:: These are accessible, like any uWSGI option, in your application code via\n            ``.runtime.environ.uwsgi_env.config``.\n\n        :param str|unicode key:\n\n        :param str|unicode value:\n\n        \"\"\"\n", "input": "", "output": "        self._set('set-placeholder', '%s=%s' % (key, value), multi=True)\n\n        return self", "category": "Python"}, {"instruction": "def maybe_call_closing_deferred(self):\n        \"\"\"\n        Used internally to callback on the _closing_deferred if it\n        exists.\n        \"\"\"\n", "input": "", "output": "\n        if self._closing_deferred:\n            self._closing_deferred.callback(self)\n            self._closing_deferred = None\n        self._when_closed.fire(self)", "category": "Python"}, {"instruction": "def accepts(self, package):  # type: (poetry.packages.Package) -> bool\n        \"\"\"\n        Determines if the given package matches this dependency.\n        \"\"\"\n", "input": "", "output": "        return (\n            self._name == package.name\n            and self._constraint.allows(package.version)\n            and (not package.is_prerelease() or self.allows_prereleases())\n        )", "category": "Python"}, {"instruction": "def get_aligned_abi_inputs(abi, args):\n    \"\"\"\n    Takes a function ABI (``abi``) and a sequence or mapping of args (``args``).\n    Returns a list of type strings for the function's inputs and a list of\n    arguments which have been aligned to the layout of those types.  The args\n    contained in ``args`` may contain nested mappings or sequences corresponding\n    to tuple-encoded values in ``abi``.\n    \"\"\"\n", "input": "", "output": "    input_abis = abi.get('inputs', [])\n\n    if isinstance(args, abc.Mapping):\n        # `args` is mapping.  Align values according to abi order.\n        args = tuple(args[abi['name']] for abi in input_abis)\n\n    return (\n        tuple(collapse_if_tuple(abi) for abi in input_abis),\n        type(args)(\n            _align_abi_input(abi, arg)\n            for abi, arg in zip(input_abis, args)\n        ),\n    )", "category": "Python"}, {"instruction": "def get_signature_request_list(self, page=1, ux_version=None):\n        ''' Get a list of SignatureRequest that you can access\n\n        This includes SignatureRequests you have sent as well as received, but\n        not ones that you have been CCed on.\n\n        Args:\n\n            page (int, optional):   Which page number of the SignatureRequest list to return. Defaults to 1.\n\n            ux_version (int):       UX version, either 1 (default) or 2.\n\n        Returns:\n            A ResourceList object\n\n        '''\n", "input": "", "output": "\n        request = self._get_request()\n        parameters = {\n            \"page\": page\n        }\n\n        if ux_version is not None:\n            parameters['ux_version'] = ux_version\n\n        return request.get(self.SIGNATURE_REQUEST_LIST_URL, parameters=parameters)", "category": "Python"}, {"instruction": "def distinct(self, key):\n        \"\"\"Get a list of distinct values for `key` among all documents\n        in the result set of this query.\n\n        Raises :class:`TypeError` if `key` is not an instance of\n        :class:`basestring` (:class:`str` in python 3).\n\n        The :meth:`distinct` method obeys the\n        :attr:`~pymongo.collection.Collection.read_preference` of the\n        :class:`~pymongo.collection.Collection` instance on which\n        :meth:`~pymongo.collection.Collection.find` was called.\n\n        :Parameters:\n          - `key`: name of key for which we want to get the distinct values\n\n        .. seealso:: :meth:`pymongo.collection.Collection.distinct`\n        \"\"\"\n", "input": "", "output": "        options = {}\n        if self.__spec:\n            options[\"query\"] = self.__spec\n        if self.__max_time_ms is not None:\n            options['maxTimeMS'] = self.__max_time_ms\n        if self.__comment:\n            options['$comment'] = self.__comment\n        if self.__collation is not None:\n            options['collation'] = self.__collation\n\n        return self.__collection.distinct(key, **options)", "category": "Python"}, {"instruction": "def _raise_exception(self, args, kwargs):\n        \"\"\" Raises an ``UnallowedMethodCallError`` with a useful message.\n\n        :raise: ``UnallowedMethodCallError``\n        \"\"\"\n", "input": "", "output": "\n        error_message = (\n            \"Received unexpected call to '{}' on {!r}.  The supplied arguments \"\n            \"{} do not match any available allowances.\"\n        )\n\n        raise UnallowedMethodCallError(\n            error_message.format(\n                self._method_name,\n                self._target.obj,\n                build_argument_repr_string(args, kwargs)\n            )\n        )", "category": "Python"}, {"instruction": "def implicit_includes (self, feature, target_type):\n        \"\"\" Returns the properties which specify implicit include paths to\n            generated headers. This traverses all targets in this subvariant,\n            and subvariants referred by <implcit-dependecy>properties.\n            For all targets which are of type 'target-type' (or for all targets,\n            if 'target_type' is not specified), the result will contain\n            <$(feature)>path-to-that-target.\n        \"\"\"\n", "input": "", "output": "        assert isinstance(feature, basestring)\n        assert isinstance(target_type, basestring)\n        if not target_type:\n            key = feature\n        else:\n            key = feature + \"-\" + target_type\n\n\n        result = self.implicit_includes_cache_.get(key)\n        if not result:\n            target_paths = self.all_target_directories(target_type)\n            target_paths = unique(target_paths)\n            result = [\"<%s>%s\" % (feature, p) for p in target_paths]\n            self.implicit_includes_cache_[key] = result\n\n        return result", "category": "Python"}, {"instruction": "def sample(self, idx):\n        \"\"\" return a tuple of (s,r,a,o),\n            where s is of shape self._output_shape, which is\n            [H, W, (hist_len+1) * channel] if input is (H, W, channel)\"\"\"\n", "input": "", "output": "        idx = (self._curr_pos + idx) % self._curr_size\n        k = self.history_len + 1\n        if idx + k <= self._curr_size:\n            state = self.state[idx: idx + k]\n            reward = self.reward[idx: idx + k]\n            action = self.action[idx: idx + k]\n            isOver = self.isOver[idx: idx + k]\n        else:\n            end = idx + k - self._curr_size\n            state = self._slice(self.state, idx, end)\n            reward = self._slice(self.reward, idx, end)\n            action = self._slice(self.action, idx, end)\n            isOver = self._slice(self.isOver, idx, end)\n        ret = self._pad_sample(state, reward, action, isOver)\n        return ret", "category": "Python"}, {"instruction": "def execd_submodule_paths(command, execd_dir=None):\n    \"\"\"Generate a list of full paths to the specified command within exec_dir.\n    \"\"\"\n", "input": "", "output": "    for module_path in execd_module_paths(execd_dir):\n        path = os.path.join(module_path, command)\n        if os.access(path, os.X_OK) and os.path.isfile(path):\n            yield path", "category": "Python"}, {"instruction": "def get_lexer_for_filename(_fn, code=None, **options):\n    \"\"\"Get a lexer for a filename.\n\n    If multiple lexers match the filename pattern, use ``analyse_text()`` to\n    figure out which one is more appropriate.\n\n    Raises ClassNotFound if not found.\n    \"\"\"\n", "input": "", "output": "    res = find_lexer_class_for_filename(_fn, code)\n    if not res:\n        raise ClassNotFound('no lexer for filename %r found' % _fn)\n    return res(**options)", "category": "Python"}, {"instruction": "def open_id_connect(self, client_id, client_secret):\n        \"\"\"\n        Get OpenID Connect client\n\n        :param str client_id:\n        :param str client_secret:\n        :rtype: keycloak.openid_connect.KeycloakOpenidConnect\n        \"\"\"\n", "input": "", "output": "        return KeycloakOpenidConnect(realm=self, client_id=client_id,\n                                     client_secret=client_secret)", "category": "Python"}, {"instruction": "def save_method_args(method):\n\t\"\"\"\n\tWrap a method such that when it is called, the args and kwargs are\n\tsaved on the method.\n\n\t>>> class MyClass:\n\t...     @save_method_args\n\t...     def method(self, a, b):\n\t...         print(a, b)\n\t>>> my_ob = MyClass()\n\t>>> my_ob.method(1, 2)\n\t1 2\n\t>>> my_ob._saved_method.args\n\t(1, 2)\n\t>>> my_ob._saved_method.kwargs\n\t{}\n\t>>> my_ob.method(a=3, b='foo')\n\t3 foo\n\t>>> my_ob._saved_method.args\n\t()\n\t>>> my_ob._saved_method.kwargs == dict(a=3, b='foo')\n\tTrue\n\n\tThe arguments are stored on the instance, allowing for\n\tdifferent instance to save different args.\n\n\t>>> your_ob = MyClass()\n\t>>> your_ob.method({str('x'): 3}, b=[4])\n\t{'x': 3} [4]\n\t>>> your_ob._saved_method.args\n\t({'x': 3},)\n\t>>> my_ob._saved_method.args\n\t()\n\t\"\"\"\n", "input": "", "output": "\targs_and_kwargs = collections.namedtuple('args_and_kwargs', 'args kwargs')\n\n\t@functools.wraps(method)\n\tdef wrapper(self, *args, **kwargs):\n\t\tattr_name = '_saved_' + method.__name__\n\t\tattr = args_and_kwargs(args, kwargs)\n\t\tsetattr(self, attr_name, attr)\n\t\treturn method(self, *args, **kwargs)\n\treturn wrapper", "category": "Python"}, {"instruction": "def read(self, file_or_path):\n        \"\"\"Read template from cache or file.\"\"\"\n", "input": "", "output": "        if file_or_path in self._cached_templates:\n            return self._cached_templates[file_or_path]\n\n        if is_filelike(file_or_path):\n            template = file_or_path.read()\n            dirname = None\n        else:\n            with open(file_or_path, 'r') as f:\n                template = f.read()\n            dirname = os.path.dirname(file_or_path)\n\n        template = self._engine(template,\n                                dirname=dirname,\n                                tolerant=self._tolerant)\n\n        self._cached_templates[file_or_path] = template\n        return template", "category": "Python"}, {"instruction": "def _render_headers(self):\n        \"\"\"\n        Write the headers row\n        \"\"\"\n", "input": "", "output": "        headers = getattr(self, 'headers', ())\n        for index, col in enumerate(headers):\n            # We write the headers\n            cell = self.worksheet.cell(row=1, column=index + 1)\n            cell.value = col['label']\n\n        index += 1\n\n        extra_headers = getattr(self, 'extra_headers', ())\n        for add_index, col in enumerate(extra_headers):\n            cell = self.worksheet.cell(row=1, column=add_index + index + 1)\n            cell.value = col['label']", "category": "Python"}, {"instruction": "def is_zone_running(self, zone):\n        \"\"\"\n        Returns the state of the specified zone.\n\n        :param zone: The zone to check.\n        :type zone: int\n        :returns: Returns True if the zone is currently running, otherwise\n                  returns False if the zone is not running.\n        :rtype: boolean\n        \"\"\"\n", "input": "", "output": "\n        self.update_controller_info()\n\n        if self.running is None or not self.running:\n            return False\n\n        if int(self.running[0]['relay']) == zone:\n            return True\n\n        return False", "category": "Python"}, {"instruction": "def restriction_dist_chart (self):\n\n        \"\"\" Make the petagRestrictionDistribution plot \"\"\"\n", "input": "", "output": "\n        pconfig = {\n            'id': 'petagRestrictionDistribution',\n            'title': 'Restriction Distribution',\n            'ylab': 'Reads',\n            'xlab': 'Distance from cut site (bp)',\n            'data_labels': [\n                        {'name': 'Number of Tags'},\n                        {'name': 'Percenatge'}\n                            ]\n       }\n        datasets = [\n            self.tagdir_data['restriction'],\n            self.tagdir_data['restriction_norm']\n        ]\n\n        return linegraph.plot(datasets, pconfig)", "category": "Python"}, {"instruction": "def map(requests, prefetch=True, size=None):\n    \"\"\"Concurrently converts a list of Requests to Responses.\n\n    :param requests: a collection of Request objects.\n    :param prefetch: If False, the content will not be downloaded immediately.\n    :param size: Specifies the number of requests to make at a time. If None, no throttling occurs.\n    \"\"\"\n", "input": "", "output": "\n    if size:\n        pool = Pool(size)\n        pool.map(send, requests)\n        pool.join()\n    else:\n        jobs = [gevent.spawn(send, r) for r in requests]\n        gevent.joinall(jobs)\n\n    if prefetch:\n        [r.response.content for r in requests]\n\n    return [r.response for r in requests]", "category": "Python"}, {"instruction": "def open_new_window(self, switch_to=True):\n        \"\"\" Opens a new browser tab/window and switches to it by default. \"\"\"\n", "input": "", "output": "        self.driver.execute_script(\"window.open('');\")\n        time.sleep(0.01)\n        if switch_to:\n            self.switch_to_window(len(self.driver.window_handles) - 1)", "category": "Python"}, {"instruction": "def _write_newname_file(in_file, out_file, mappings):\n    \"\"\"Re-write an input file with contigs matching the correct reference.\n    \"\"\"\n", "input": "", "output": "    with utils.open_gzipsafe(in_file) as in_handle:\n        with open(out_file, \"w\") as out_handle:\n            for line in in_handle:\n                if line.startswith(\"#\"):\n                    out_handle.write(line)\n                else:\n                    parts = line.split(\"\\t\")\n                    new_contig = mappings.get(parts[0])\n                    if new_contig:\n                        parts[0] = new_contig\n                        out_handle.write(\"\\t\".join(parts))", "category": "Python"}, {"instruction": "def next(self):\n        \"\"\"Next point in iteration\n        \"\"\"\n", "input": "", "output": "        x, y = next(self.scan)\n        xr = x + self.tx\n        yr = y + self.ty\n        return xr, yr", "category": "Python"}, {"instruction": "def set_mode (filename, flags):\n    \"\"\"Set mode flags for given filename if not already set.\"\"\"\n", "input": "", "output": "    try:\n        mode = os.lstat(filename).st_mode\n    except OSError:\n        # ignore\n        return\n    if not (mode & flags):\n        try:\n            os.chmod(filename, flags | mode)\n        except OSError as msg:\n            log_error(\"could not set mode flags for `%s': %s\" % (filename, msg))", "category": "Python"}, {"instruction": "def _proxy(self):\n        \"\"\"\n        Generate an instance context for the instance, the context is capable of\n        performing various actions.  All instance actions are proxied to the context\n\n        :returns: AccountContext for this AccountInstance\n        :rtype: twilio.rest.api.v2010.account.AccountContext\n        \"\"\"\n", "input": "", "output": "        if self._context is None:\n            self._context = AccountContext(self._version, sid=self._solution['sid'], )\n        return self._context", "category": "Python"}, {"instruction": "def push_tx(self, crypto, tx_hex):\n        \"\"\"\n        This method is untested.\n        \"\"\"\n", "input": "", "output": "        url = \"%s/pushtx\" % self.base_url\n        return self.post_url(url, {'hex': tx_hex}).content", "category": "Python"}, {"instruction": "def cause_info(self, mechanism, purview):\n        \"\"\"Return the cause information for a mechanism over a purview.\"\"\"\n", "input": "", "output": "        return repertoire_distance(\n            Direction.CAUSE,\n            self.cause_repertoire(mechanism, purview),\n            self.unconstrained_cause_repertoire(purview)\n        )", "category": "Python"}, {"instruction": "def check_path(self, path):\n        \"\"\"\n        turns path into an absolute path and checks that it exists, then\n        returns it as a string.\n        \"\"\"\n", "input": "", "output": "        path = os.path.abspath(path)\n        if os.path.exists(path):\n            return path\n        else:\n            utils.die(\"input file does not exists:\\n  {}\".format(path))", "category": "Python"}, {"instruction": "def screenshot_themes(self, *args):\n        \"\"\"Take a screenshot for all themes available\"\"\"\n", "input": "", "output": "        from time import sleep\n        for theme in THEMES:\n            example.set_theme(theme)\n            example.update()\n            sleep(0.05)\n            self.screenshot()", "category": "Python"}, {"instruction": "def get_name(model_id):\n    \"\"\"\n    Get the name for a model.\n\n    :returns str: The model's name.  If the id has no associated name, then \"id = {ID} (no name)\" is returned.\n    \"\"\"\n", "input": "", "output": "    name = _names.get(model_id)\n    if name is None:\n        name = 'id = %s (no name)' % str(model_id)\n    return name", "category": "Python"}, {"instruction": "def insertReference(self, reference):\n        \"\"\"\n        Inserts the specified reference into this repository.\n        \"\"\"\n", "input": "", "output": "        models.Reference.create(\n            id=reference.getId(),\n            referencesetid=reference.getParentContainer().getId(),\n            name=reference.getLocalId(),\n            length=reference.getLength(),\n            isderived=reference.getIsDerived(),\n            species=json.dumps(reference.getSpecies()),\n            md5checksum=reference.getMd5Checksum(),\n            sourceaccessions=json.dumps(reference.getSourceAccessions()),\n            sourceuri=reference.getSourceUri())", "category": "Python"}, {"instruction": "def checkFuelPosition(obs, agent_host):\n    '''Make sure our coal, if we have any, is in slot 0.'''\n", "input": "", "output": "    # (We need to do this because the furnace crafting commands - cooking the potato and the rabbit -\n    # take the first available item of fuel in the inventory. If this isn't the coal, it could end up burning the wood\n    # that we need for making the bowl.)\n    for i in range(1,39):\n        key = 'InventorySlot_'+str(i)+'_item'\n        if key in obs:\n            item = obs[key]\n            if item == 'coal':\n                agent_host.sendCommand(\"swapInventoryItems 0 \" + str(i))\n                return", "category": "Python"}, {"instruction": "def pipe_createrss(context=None, _INPUT=None, conf=None, **kwargs):\n    \"\"\"An operator that converts a source into an RSS stream. Not loopable.\n\n    \"\"\"\n", "input": "", "output": "    conf = DotDict(conf)\n\n    for item in _INPUT:\n        item = DotDict(item)\n\n        yield {\n            value: item.get(conf.get(key, **kwargs))\n            for key, value in RSS_FIELDS.items()}", "category": "Python"}, {"instruction": "def _check_buffer(self, data, ctype):\n        \"\"\"Convert buffer to cdata and check for valid size.\"\"\"\n", "input": "", "output": "        assert ctype in _ffi_types.values()\n        if not isinstance(data, bytes):\n            data = _ffi.from_buffer(data)\n        frames, remainder = divmod(len(data),\n                                   self.channels * _ffi.sizeof(ctype))\n        if remainder:\n            raise ValueError(\"Data size must be a multiple of frame size\")\n        return data, frames", "category": "Python"}, {"instruction": "def _check_convergence(self, F):\n        \"\"\" Checks if the solution has converged to within the specified\n            tolerance.\n        \"\"\"\n", "input": "", "output": "        normF = linalg.norm(F, Inf)\n\n        if normF < self.tolerance:\n            converged = True\n        else:\n            converged = False\n            if self.verbose:\n                logger.info(\"Difference: %.3f\" % (normF - self.tolerance))\n\n        return converged", "category": "Python"}, {"instruction": "def set_row_names(self, row_names):\n        \"\"\" Setup the feature vector with some column names\n        :param row_names: the column names we want\n        :return:\n        \"\"\"\n", "input": "", "output": "        if self.row_count() != len(row_names) or len(self._row_name_list) > 0 or len(self._row_name_idx) > 0:\n            raise NotImplementedError(\"You can only manually set names once data has been added\")\n\n        for idx, k in enumerate(row_names):\n            self._row_name_idx[k] = idx\n\n        self._row_name_list = row_names", "category": "Python"}, {"instruction": "def lz4f_decode(payload):\n    \"\"\"Decode payload using interoperable LZ4 framing. Requires Kafka >= 0.10\"\"\"\n", "input": "", "output": "    # pylint: disable-msg=no-member\n    ctx = lz4f.createDecompContext()\n    data = lz4f.decompressFrame(payload, ctx)\n    lz4f.freeDecompContext(ctx)\n\n    # lz4f python module does not expose how much of the payload was\n    # actually read if the decompression was only partial.\n    if data['next'] != 0:\n        raise RuntimeError('lz4f unable to decompress full payload')\n    return data['decomp']", "category": "Python"}, {"instruction": "def add_comment(self, case_obj, text, variant_id=None, username=None):\n        \"\"\"Add a comment to a variant or a case\"\"\"\n", "input": "", "output": "\n        comment = Comment(\n            text=text,\n            username=username or 'Anonymous',\n            case=case_obj,\n            # md5 sum of chrom, pos, ref, alt\n            variant_id=variant_id\n        )\n        self.session.add(comment)\n        self.save()\n        return comment", "category": "Python"}, {"instruction": "def uninstall_package_and_wait(\n        package_name,\n        service_name=None,\n        all_instances=False,\n        wait_for_completion=True,\n        timeout_sec=600\n):\n    \"\"\" Uninstall a package via the DC/OS library and wait for completion\n\n        :param package_name: name of the package\n        :type package_name: str\n        :param service_name: unique service name for the package\n        :type service_name: str\n        :param all_instances: uninstall all instances of package\n        :type all_instances: bool\n        :param wait_for_completion: whether or not to wait for task completion before returning\n        :type wait_for_completion: bool\n        :param timeout_sec: number of seconds to wait for task completion\n        :type timeout_sec: int\n\n        :return: True if uninstall was successful, False otherwise\n        :rtype: bool\n    \"\"\"\n", "input": "", "output": "\n    return uninstall_package(\n        package_name,\n        service_name,\n        all_instances,\n        wait_for_completion,\n        timeout_sec\n    )", "category": "Python"}, {"instruction": "def documents(self, full=False):\n        '''Return list of documents owned by user.\n\n        If `full=True`, it'll download all pages returned by the HTTP server'''\n", "input": "", "output": "        url = self.base_url + self.DOCUMENTS_PAGE\n        class_ = Document\n        results = self._retrieve_resources(url, class_, full)\n        return results", "category": "Python"}, {"instruction": "def partition(pred, iterable):\n    \"\"\"Partition an iterable.\n\n    Arguments\n    ---------\n    pred     : function\n               A function that takes an element of the iterable and returns\n               a boolen indicating to which partition it belongs\n    iterable : iterable\n\n    Returns\n    -------\n    A two-tuple of lists with the first list containing the elements on which\n    the predicate indicated False and the second list containing the elements\n    on which the predicate indicated True.\n\n    Note that, unlike the recipe which returns generators, this version\n    returns lists.\n    \"\"\"\n", "input": "", "output": "    pos, neg = [], []\n    pos_append, neg_append = pos.append, neg.append\n    for elem in iterable:\n        if pred(elem):\n            pos_append(elem)\n        else:\n            neg_append(elem)\n    return neg, pos", "category": "Python"}, {"instruction": "def scalar_term(self, st):\n        \"\"\"Return a _ScalarTermS or _ScalarTermU from a string, to perform text and HTML substitutions\"\"\"\n", "input": "", "output": "        if isinstance(st, binary_type):\n            return _ScalarTermS(st, self._jinja_sub)\n        elif isinstance(st, text_type):\n            return _ScalarTermU(st, self._jinja_sub)\n        elif st is None:\n            return _ScalarTermU(u(''), self._jinja_sub)\n        else:\n            return st", "category": "Python"}, {"instruction": "def __run(self):\n\t\t\"\"\" Internal function that is run in a separate thread. Do not call \n\t\tdirectly. \"\"\"\n", "input": "", "output": "\t\tself.interval_start = time.time()\n\t\twhile self.status != STOPPED:\n\t\t\tif self.status == RUNNING:\n\t\t\t\tself.current_interval_duration = time.time() - self.interval_start\n\n\t\t\t# If max_duration is set, stop the clock if it is reached\n\t\t\tif self.max_duration and self.time > self.max_duration:\n\t\t\t\tself.status == STOPPED\n\n\t\t\t# One refresh per millisecond seems enough\n\t\t\ttime.sleep(0.001)", "category": "Python"}, {"instruction": "def read_cf1_config(self):\n        \"\"\"Read a flash page from the specified target\"\"\"\n", "input": "", "output": "        target = self._cload.targets[0xFF]\n        config_page = target.flash_pages - 1\n\n        return self._cload.read_flash(addr=0xFF, page=config_page)", "category": "Python"}, {"instruction": "def getService(self, serviceIdentifier):\n        \"\"\"\n        Return the requested service instance.\n\n        :param serviceIdentifier: <str> service identifier\n        :return: <object> service instance\n        \"\"\"\n", "input": "", "output": "        if serviceIdentifier in self._services:\n            return self._services[serviceIdentifier]\n        else:\n            message = \"Application - getService() - \" \\\n                      \"Service with identifier {} does not exist.\" \\\n                      .format(serviceIdentifier)\n            raise Exception(message)", "category": "Python"}, {"instruction": "def memory_pour(buffer_, *args, **kwargs):\n    \"\"\"Yield data from entries.\"\"\"\n", "input": "", "output": "\n    def opener(archive_res):\n        _LOGGER.debug(\"Opening from (%d) bytes (memory_pour).\", len(buffer_))\n        _archive_read_open_memory(archive_res, buffer_)\n\n    return _pour(opener, *args, flags=0, **kwargs)", "category": "Python"}, {"instruction": "def copy_files(project_vars, project_dir, files):\n    \"\"\"\n    Copies files from the template into their target location. Unicode files\n    get their variables replaced here and files with a shebang are set to be\n    executable.\n    \"\"\"\n", "input": "", "output": "\n    for root, name, content, is_unicode in files:\n        project_name = project_vars['project_name_snake']\n\n        if is_unicode:\n            content = replace_content(content, project_vars)\n\n        file_path = make_file_path(project_dir, project_name, root, name)\n        makedirs(make_dir_path(project_dir, root, project_name), exist_ok=True)\n\n        if is_unicode:\n            with open(file_path, 'w') as f:\n                f.write(content)\n\n            if content.startswith('#!'):\n                chmod(file_path, 0o755)\n        else:\n            with open(file_path, 'wb') as f:\n                f.write(content)", "category": "Python"}, {"instruction": "def save_files(self, selections) -> None:\n        \"\"\"Save the |Selection| objects contained in the given |Selections|\n        instance to separate network files.\"\"\"\n", "input": "", "output": "        try:\n            currentpath = self.currentpath\n            selections = selectiontools.Selections(selections)\n            for selection in selections:\n                if selection.name == 'complete':\n                    continue\n                path = os.path.join(currentpath, selection.name+'.py')\n                selection.save_networkfile(filepath=path)\n        except BaseException:\n            objecttools.augment_excmessage(\n                'While trying to save selections `%s` into network files'\n                % selections)", "category": "Python"}, {"instruction": "def parse_acl(acl_iter):\n    \"\"\"Parse a string, or list of ACE definitions, into usable ACEs.\"\"\"\n", "input": "", "output": "\n    if isinstance(acl_iter, basestring):\n        acl_iter = [acl_iter]\n\n    for chunk in acl_iter:\n\n        if isinstance(chunk, basestring):\n            chunk = chunk.splitlines()\n            chunk = [re.sub(r'#.+', '', line).strip() for line in chunk]\n            chunk = filter(None, chunk)\n        else:\n            chunk = [chunk]\n\n        for ace in chunk:\n\n            # If this was provided as a string, then parse the permission set.\n            # Otherwise, use it as-is, which will result in an equality test.\n            if isinstance(ace, basestring):\n                ace = ace.split(None, 2)\n                state, predicate, permission_set = ace\n                yield parse_state(state), parse_predicate(predicate), parse_permission_set(permission_set)\n            else:\n                state, predicate, permission_set = ace\n                yield parse_state(state), parse_predicate(predicate), permission_set", "category": "Python"}, {"instruction": "def to_pytime(self):\n        \"\"\"\n        Converts sql time object into Python's time object\n        this will truncate nanoseconds to microseconds\n        @return: naive time\n        \"\"\"\n", "input": "", "output": "        nanoseconds = self._nsec\n        hours = nanoseconds // 1000000000 // 60 // 60\n        nanoseconds -= hours * 60 * 60 * 1000000000\n        minutes = nanoseconds // 1000000000 // 60\n        nanoseconds -= minutes * 60 * 1000000000\n        seconds = nanoseconds // 1000000000\n        nanoseconds -= seconds * 1000000000\n        return datetime.time(hours, minutes, seconds, nanoseconds // 1000)", "category": "Python"}, {"instruction": "def memset(self, buffer, value, size):\n        \"\"\"set the memory in allocation to the value in value\n\n        :param allocation: An OpenCL Buffer to fill\n        :type allocation: pyopencl.Buffer\n\n        :param value: The value to set the memory to\n        :type value: a single 32-bit int\n\n        :param size: The size of to the allocation unit in bytes\n        :type size: int\n\n        \"\"\"\n", "input": "", "output": "        if isinstance(buffer, cl.Buffer):\n            try:\n                cl.enqueue_fill_buffer(self.queue, buffer, numpy.uint32(value), 0, size)\n            except AttributeError:\n                src=numpy.zeros(size, dtype='uint8')+numpy.uint8(value)\n                cl.enqueue_copy(self.queue, buffer, src)", "category": "Python"}, {"instruction": "def _assemble_gap(stmt):\n    \"\"\"Assemble Gap statements into text.\"\"\"\n", "input": "", "output": "    subj_str = _assemble_agent_str(stmt.gap)\n    obj_str = _assemble_agent_str(stmt.ras)\n    stmt_str = subj_str + ' is a GAP for ' + obj_str\n    return _make_sentence(stmt_str)", "category": "Python"}, {"instruction": "def get_raw(self):\n        \"\"\"\n        Return the raw buffer of this object\n\n        :rtype: bytearray\n        \"\"\"\n", "input": "", "output": "        buff = bytearray()\n        for i in self.get_instructions():\n            buff += i.get_raw()\n        return buff", "category": "Python"}, {"instruction": "def base_url(klass, space_id, parent_resource_id, resource_url='entries', resource_id=None, environment_id=None):\n        \"\"\"\n        Returns the URI for the snapshot.\n        \"\"\"\n", "input": "", "output": "\n        return \"spaces/{0}{1}/{2}/{3}/snapshots/{4}\".format(\n            space_id,\n            '/environments/{0}'.format(environment_id) if environment_id is not None else '',\n            resource_url,\n            parent_resource_id,\n            resource_id if resource_id is not None else ''\n        )", "category": "Python"}, {"instruction": "def getBlockValue(self, block):\n        \"\"\"Provides the previously set block value respecting the bits range.\n           0 value and not marked block are treated the same way and 0 is\n           provided.\n        \"\"\"\n", "input": "", "output": "        if self._bit_count == 0:\n            raise Exception( \"The margin '\" + self._name +\n                             \"' did not allocate any bits for the values\")\n        val = block.userState()\n        if val in [ 0, -1 ]:\n            return 0\n\n        # Shift the value to the right\n        val >>= self._bitRange[ 0 ]\n\n        # Apply the mask to the value\n        mask = 2 ** self._bit_count - 1\n        val &= mask\n        return val", "category": "Python"}, {"instruction": "def domain_update(auth=None, **kwargs):\n    '''\n    Update a domain\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' keystoneng.domain_update name=domain1 new_name=newdomain\n        salt '*' keystoneng.domain_update name=domain1 enabled=True description='new description'\n    '''\n", "input": "", "output": "    cloud = get_operator_cloud(auth)\n    kwargs = _clean_kwargs(**kwargs)\n    if 'new_name' in kwargs:\n        kwargs['name'] = kwargs.pop('new_name')\n    return cloud.update_domain(**kwargs)", "category": "Python"}, {"instruction": "def matrix_params_from_table(table, transpose=False):\n    \"\"\"\n    Calculate TP,TN,FP,FN from confusion matrix.\n\n    :param table: input matrix\n    :type table : dict\n    :param transpose : transpose flag\n    :type transpose : bool\n    :return: [classes_list,table,TP,TN,FP,FN]\n    \"\"\"\n", "input": "", "output": "    classes = sorted(table.keys())\n    map_dict = {k: 0 for k in classes}\n    TP_dict = map_dict.copy()\n    TN_dict = map_dict.copy()\n    FP_dict = map_dict.copy()\n    FN_dict = map_dict.copy()\n    for i in classes:\n        TP_dict[i] = table[i][i]\n        sum_row = sum(list(table[i].values()))\n        for j in classes:\n            if j != i:\n                FN_dict[i] += table[i][j]\n                FP_dict[j] += table[i][j]\n                TN_dict[j] += sum_row - table[i][j]\n    if transpose:\n        temp = FN_dict\n        FN_dict = FP_dict\n        FP_dict = temp\n        table = transpose_func(classes, table)\n    return [classes, table, TP_dict, TN_dict, FP_dict, FN_dict]", "category": "Python"}]