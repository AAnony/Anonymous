[{"instruction": "def _pad_tensors_to_same_length(x, y):\n  \"\"\"Pad x and y so that the results have the same length (second dimension).\"\"\"\n", "input": "", "output": "  with tf.name_scope(\"pad_to_same_length\"):\n    x_length = tf.shape(x)[1]\n    y_length = tf.shape(y)[1]\n\n    max_length = tf.maximum(x_length, y_length)\n\n    x = tf.pad(x, [[0, 0], [0, max_length - x_length], [0, 0]])\n    y = tf.pad(y, [[0, 0], [0, max_length - y_length]])\n    return x, y", "category": "Python"}, {"instruction": "def _selection_by_callable(self, view, num_slices, non_empty_slices):\n        \"\"\"Returns all the slices selected by the given callable.\"\"\"\n", "input": "", "output": "\n        selected = [sl for sl in non_empty_slices\n                    if self._sampler(self._get_axis(self._image, view, sl))]\n\n        return selected[:num_slices]", "category": "Python"}, {"instruction": "def SADWindowSize(self, value):\n        \"\"\"Set private ``_sad_window_size`` and reset ``_block_matcher``.\"\"\"\n", "input": "", "output": "        if value >= 1 and value <= 11 and value % 2:\n            self._sad_window_size = value\n        else:\n            raise InvalidSADWindowSizeError(\"SADWindowSize must be odd and \"\n                                            \"between 1 and 11.\")\n        self._replace_bm()", "category": "Python"}, {"instruction": "def create_error(msg, cause=None):\n    \"\"\"Creates a ``GaxError`` or subclass.\n\n    Attributes:\n        msg (string): describes the error that occurred.\n        cause (Exception, optional): the exception raised by a lower\n            layer of the RPC stack (for example, gRPC) that caused this\n            exception, or None if this exception originated in GAX.\n\n    Returns:\n        .GaxError: The exception that wraps ``cause``.\n    \"\"\"\n", "input": "", "output": "    status_code = config.exc_to_code(cause)\n    status_name = config.NAME_STATUS_CODES.get(status_code)\n    if status_name == 'INVALID_ARGUMENT':\n        return InvalidArgumentError(msg, cause=cause)\n    else:\n        return GaxError(msg, cause=cause)", "category": "Python"}, {"instruction": "def div(a,b):\n    \"\"\"``div(a,b)`` is like ``a // b`` if ``b`` devides ``a``, otherwise\n    an `ValueError` is raised.\n\n    >>> div(10,2)\n    5\n    >>> div(10,3)\n    Traceback (most recent call last):\n    ...\n    ValueError: 3 does not divide 10\n    \"\"\"\n", "input": "", "output": "    res, fail = divmod(a,b)\n    if fail:\n        raise ValueError(\"%r does not divide %r\" % (b,a))\n    else:\n        return res", "category": "Python"}, {"instruction": "def _err(self, errclass: str=\"error\", *args) -> \"Err\":\n        \"\"\"\n        Creates an error\n        \"\"\"\n", "input": "", "output": "        error = self._new_err(errclass, *args)\n        if self.log_errs is True:\n            sep = \" \"\n            if self.log_format == \"csv\":\n                sep = \",\"\n            msg = str(datetime.now()) + sep + \\\n                self._errmsg(error, msgformat=self.log_format)\n            self.logger.error(msg)\n        print(self._errmsg(error))\n        self._add(error)\n        return error", "category": "Python"}, {"instruction": "def get_resource_dirs(resource):\n    \"\"\"Returns a list of all known resource dirs for a given resource.\n\n    :param str resource:\n        Name of the resource (e.g. \"themes\")\n    :return:\n        A list of resource dirs\n    \"\"\"\n", "input": "", "output": "    dirs = [\n        os.path.join(dir, resource) for dir in\n        itertools.chain(GLib.get_system_data_dirs(), GUAKE_THEME_DIR, GLib.get_user_data_dir())\n    ]\n    dirs += [os.path.join(os.path.expanduser(\"~\"), \".{}\".format(resource))]\n\n    return [Path(dir) for dir in dirs if os.path.isdir(dir)]", "category": "Python"}, {"instruction": "def checksum (data, start = 0, skip_word = None):\n  \"\"\"\n  Calculate standard internet checksum over data starting at start'th byte\n\n  skip_word: If specified, it's the word offset of a word in data to \"skip\"\n             (as if it were zero).  The purpose is when data is received\n             data which contains a computed checksum that you are trying to\n             verify -- you want to skip that word since it was zero when\n             the checksum was initially calculated.\n  \"\"\"\n", "input": "", "output": "  if len(data) % 2 != 0:\n    arr = array.array('H', data[:-1])\n  else:\n    arr = array.array('H', data)\n\n  if skip_word is not None:\n    for i in range(0, len(arr)):\n      if i == skip_word:\n        continue\n      start +=  arr[i]\n  else:\n    for i in range(0, len(arr)):\n      start +=  arr[i]\n\n  if len(data) % 2 != 0:\n    start += struct.unpack('H', data[-1:]+b'\\x00')[0] # Specify order?\n\n  start  = (start >> 16) + (start & 0xffff)\n  start += (start >> 16)\n  #while start >> 16:\n  #  start = (start >> 16) + (start & 0xffff)\n\n  return ntohs(~start & 0xffff)", "category": "Python"}, {"instruction": "def to_list_of_dicts(self, **kwargs):\n        \"\"\"\n        Convert the :class:`ParameterSet` to a list of the dictionary representation\n        of each :class:`Parameter`\n\n        :return: list of dicts\n        \"\"\"\n", "input": "", "output": "        if kwargs:\n            return self.filter(**kwargs).to_list_of_dicts()\n        return [param.to_dict() for param in self._params]", "category": "Python"}, {"instruction": "def _read_composites(self, compositor_nodes):\n        \"\"\"Read (generate) composites.\"\"\"\n", "input": "", "output": "        keepables = set()\n        for item in compositor_nodes:\n            self._generate_composite(item, keepables)\n        return keepables", "category": "Python"}, {"instruction": "def write_collection(self, filename, collection):\n        \"\"\"\n        Writes a collection of stop words into a file.\n        \"\"\"\n", "input": "", "output": "        collection = sorted(list(collection))\n        with open(filename, 'wb+') as fd:\n            fd.truncate()\n            fd.write('\\n'.join(collection).encode('utf-8'))", "category": "Python"}, {"instruction": "def setLength(self, typeID, length):\n        \"\"\"setLength(string, double) -> None\n\n        Sets the length in m of the vehicles of this type.\n        \"\"\"\n", "input": "", "output": "        self._connection._sendDoubleCmd(\n            tc.CMD_SET_VEHICLETYPE_VARIABLE, tc.VAR_LENGTH, typeID, length)", "category": "Python"}, {"instruction": "def add_depth_embedding(x):\n  \"\"\"Add n-dimensional embedding as the depth embedding (timing signal).\n\n  Adds embeddings to represent the position of the step in the recurrent\n  tower.\n\n  Args:\n    x: a tensor with shape [max_step, batch, length, depth]\n\n  Returns:\n    a Tensor the same shape as x.\n  \"\"\"\n", "input": "", "output": "  x_shape = common_layers.shape_list(x)\n  depth = x_shape[-1]\n  num_steps = x_shape[0]\n  shape = [num_steps, 1, 1, depth]\n  depth_embedding = (\n      tf.get_variable(\n          \"depth_embedding\",\n          shape,\n          initializer=tf.random_normal_initializer(0, depth**-0.5)) * (depth**\n                                                                       0.5))\n\n  x += depth_embedding\n  return x", "category": "Python"}, {"instruction": "def build_table(self, table, force=False):\n        \"\"\"Build all of the sources for a table \"\"\"\n", "input": "", "output": "\n        sources = self._resolve_sources(None, [table])\n\n        for source in sources:\n            self.build_source(None, source, force=force)\n\n        self.unify_partitions()", "category": "Python"}, {"instruction": "def value_attr(attr_name):\n    \"\"\"\n    Creates a getter that will retrieve value's attribute\n    with specified name.\n    @param attr_name: the name of an attribute belonging to the value.\n    @type attr_name: str\n    \"\"\"\n", "input": "", "output": "\n    def value_attr(value, context, **_params):\n        value = getattr(value, attr_name)\n        return _attr(value)\n\n    return value_attr", "category": "Python"}, {"instruction": "def time_replacer(match, timestamp):\n        \"\"\"\n        Transforms the timestamp to the format the regex match determines.\n\n        :param str match: the regex match\n        :param time timestamp: the timestamp to format with match.group(1)\n        :return str: the timestamp formated with strftime the way the\n                     regex-match within the first set of braces defines\n        \"\"\"\n", "input": "", "output": "        # match.group(0) = entire match\n        # match.group(1) = match in braces #1\n        return time.strftime(match.group(1), time.gmtime(timestamp))", "category": "Python"}, {"instruction": "def __output_unpaired_vals(d_vals, used_ff_keys, f_f_header, sf_d, s_f_header,\n                           missing_val, out_handler, outfh, delim=\"\\t\"):\n  \"\"\"\n  Use an output handler to output keys that could not be paired.\n\n  Go over the keys in d_vals and for any that were not used (i.e. not in\n  used_ff_keys), build an output line using the values from d_vals,\n  populated the missing columns with missing_val, and output these using the\n  provided output hander.\n  \"\"\"\n", "input": "", "output": "  if missing_val is None:\n    raise MissingValueError(\"Need missing value to output \" +\n                            \" unpaired lines\")\n  for k in d_vals:\n    if k not in used_ff_keys:\n      f_f_flds = d_vals[k]\n      if s_f_header is not None:\n        s_f_flds = [dict(zip(s_f_header, [missing_val] * len(s_f_header)))]\n      else:\n        s_f_num_cols = len(sf_d[d_vals.keys()[0]][0])\n        s_f_flds = [[missing_val] * s_f_num_cols]\n      out_handler.write_output(outfh, delim, s_f_flds, f_f_flds,\n                               s_f_header, f_f_header)", "category": "Python"}, {"instruction": "def cleanup(context):\n    \"\"\"Clean up the work_dir and artifact_dir between task runs, then recreate.\n\n    Args:\n        context (scriptworker.context.Context): the scriptworker context.\n\n    \"\"\"\n", "input": "", "output": "    for name in 'work_dir', 'artifact_dir', 'task_log_dir':\n        path = context.config[name]\n        if os.path.exists(path):\n            log.debug(\"rm({})\".format(path))\n            rm(path)\n        makedirs(path)", "category": "Python"}, {"instruction": "def parse_pagination(headers):\n    \"\"\" Parses headers to create a pagination objects\n\n    :param headers: HTTP Headers\n    :type headers: dict\n    :return: Navigation object for pagination\n    :rtype: _Navigation\n    \"\"\"\n", "input": "", "output": "    links = {\n        link.rel: parse_qs(link.href).get(\"page\", None)\n        for link in link_header.parse(headers.get(\"Link\", \"\")).links\n    }\n    return _Navigation(\n        links.get(\"previous\", [None])[0],\n        links.get(\"next\", [None])[0],\n        links.get(\"last\", [None])[0],\n        links.get(\"current\", [None])[0],\n        links.get(\"first\", [None])[0]\n    )", "category": "Python"}, {"instruction": "def toggle_rich_text(self, checked):\r\n        \"\"\"Toggle between sphinxified docstrings or plain ones\"\"\"\n", "input": "", "output": "        if checked:\r\n            self.docstring = not checked\r\n            self.switch_to_rich_text()\r\n        self.set_option('rich_mode', checked)", "category": "Python"}, {"instruction": "def _reset_py_display() -> None:\n        \"\"\"\n        Resets the dynamic objects in the sys module that the py and ipy consoles fight over.\n        When a Python console starts it adopts certain display settings if they've already been set.\n        If an ipy console has previously been run, then py uses its settings and ends up looking\n        like an ipy console in terms of prompt and exception text. This method forces the Python\n        console to create its own display settings since they won't exist.\n\n        IPython does not have this problem since it always overwrites the display settings when it\n        is run. Therefore this method only needs to be called before creating a Python console.\n        \"\"\"\n", "input": "", "output": "        # Delete any prompts that have been set\n        attributes = ['ps1', 'ps2', 'ps3']\n        for cur_attr in attributes:\n            try:\n                del sys.__dict__[cur_attr]\n            except KeyError:\n                pass\n\n        # Reset functions\n        sys.displayhook = sys.__displayhook__\n        sys.excepthook = sys.__excepthook__", "category": "Python"}, {"instruction": "def set_scope(self, value):\n        \"\"\" narrows the scopes the commands \"\"\"\n", "input": "", "output": "        if self.default_command:\n            self.default_command += ' ' + value\n        else:\n            self.default_command += value\n        return value", "category": "Python"}, {"instruction": "def update_resource(zone, resource_type, resource_selector, **kwargs):\n    '''\n    Add a resource\n\n    zone : string\n        name of zone\n    resource_type : string\n        type of resource\n    resource_selector : string\n        unique resource identifier\n    kwargs : string|int|...\n        resource properties\n\n    .. note::\n        Set resource_selector to None for resource that do not require one.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zonecfg.update_resource tallgeese rctl name name=zone.max-locked-memory value='(priv=privileged,limit=33554432,action=deny)'\n    '''\n", "input": "", "output": "    return _resource('update', zone, resource_type, resource_selector, **kwargs)", "category": "Python"}, {"instruction": "def display(self, mode='dot'):\n        '''\n        Displays the current graph via dotty\n        '''\n", "input": "", "output": "\n        if  mode == 'neato':\n            self.save_dot(self.temp_neo)\n            neato_cmd = \"%s -o %s %s\" % (self.neato, self.temp_dot, self.temp_neo)\n            os.system(neato_cmd)\n        else:\n            self.save_dot(self.temp_dot)\n\n        plot_cmd = \"%s %s\" % (self.dotty, self.temp_dot)\n        os.system(plot_cmd)", "category": "Python"}, {"instruction": "def get_document(self, doc_uri):\n        \"\"\"Return a managed document if-present, else create one pointing at disk.\n\n        See https://github.com/Microsoft/language-server-protocol/issues/177\n        \"\"\"\n", "input": "", "output": "        return self._docs.get(doc_uri) or self._create_document(doc_uri)", "category": "Python"}, {"instruction": "def _export_local_images(project, image, z):\n    \"\"\"\n    Take a project file (.gns3) and export images to the zip\n\n    :param image: Image path\n    :param z: Zipfile instance for the export\n    \"\"\"\n", "input": "", "output": "    from ..compute import MODULES\n\n    for module in MODULES:\n        try:\n            img_directory = module.instance().get_images_directory()\n        except NotImplementedError:\n            # Some modules don't have images\n            continue\n\n        directory = os.path.split(img_directory)[-1:][0]\n        if os.path.exists(image):\n            path = image\n        else:\n            path = os.path.join(img_directory, image)\n\n        if os.path.exists(path):\n            arcname = os.path.join(\"images\", directory, os.path.basename(image))\n            z.write(path, arcname)\n            return", "category": "Python"}, {"instruction": "def _check_unmask(name, unmask, unmask_runtime, root=None):\n    '''\n    Common code for conditionally removing masks before making changes to a\n    service's state.\n    '''\n", "input": "", "output": "    if unmask:\n        unmask_(name, runtime=False, root=root)\n    if unmask_runtime:\n        unmask_(name, runtime=True, root=root)", "category": "Python"}, {"instruction": "def render_template(self, source, **kwargs_context):\n        r\"\"\"Render a template string using sandboxed environment.\n\n        :param source: A string containing the page source.\n        :param \\*\\*kwargs_context: The context associated with the page.\n        :returns: The rendered template.\n        \"\"\"\n", "input": "", "output": "        return self.jinja_env.from_string(source).render(kwargs_context)", "category": "Python"}, {"instruction": "def _opt_call_from_base_type(self, value):\n    \"\"\"Call _from_base_type() if necessary.\n\n    If the value is a _BaseValue instance, unwrap it and call all\n    _from_base_type() methods.  Otherwise, return the value\n    unchanged.\n    \"\"\"\n", "input": "", "output": "    if isinstance(value, _BaseValue):\n      value = self._call_from_base_type(value.b_val)\n    return value", "category": "Python"}, {"instruction": "def make_table_row(contents, tag='td'):\n  \"\"\"Given an iterable of string contents, make a table row.\n\n  Args:\n    contents: An iterable yielding strings.\n    tag: The tag to place contents in. Defaults to 'td', you might want 'th'.\n\n  Returns:\n    A string containing the content strings, organized into a table row.\n\n  Example: make_table_row(['one', 'two', 'three']) == '''\n  <tr>\n  <td>one</td>\n  <td>two</td>\n  <td>three</td>\n  </tr>'''\n  \"\"\"\n", "input": "", "output": "  columns = ('<%s>%s</%s>\\n' % (tag, s, tag) for s in contents)\n  return '<tr>\\n' + ''.join(columns) + '</tr>\\n'", "category": "Python"}, {"instruction": "def get_grid_bbox(self, shape):\n        \"\"\"Returns ((top, left), (bottom, right)) of bounding box\n\n        A bounding box is the smallest rectangle that contains all selections.\n        Non-specified boundaries are filled i from size.\n\n        Parameters\n        ----------\n\n        shape: 3-Tuple of Integer\n        \\tGrid shape\n\n        \"\"\"\n", "input": "", "output": "\n        (bb_top, bb_left), (bb_bottom, bb_right) = self.get_bbox()\n\n        if bb_top is None:\n            bb_top = 0\n        if bb_left is None:\n            bb_left = 0\n        if bb_bottom is None:\n            bb_bottom = shape[0]\n        if bb_right is None:\n            bb_right = shape[1]\n\n        return ((bb_top, bb_left), (bb_bottom, bb_right))", "category": "Python"}, {"instruction": "def _update_seek(self, offset, whence):\n        \"\"\"\n        Update seek value.\n\n        Args:\n            offset (int): Offset.\n            whence (int): Whence.\n\n        Returns:\n            int: Seek position.\n        \"\"\"\n", "input": "", "output": "        with self._seek_lock:\n            if whence == SEEK_SET:\n                self._seek = offset\n            elif whence == SEEK_CUR:\n                self._seek += offset\n            elif whence == SEEK_END:\n                self._seek = offset + self._size\n            else:\n                raise ValueError('whence value %s unsupported' % whence)\n        return self._seek", "category": "Python"}, {"instruction": "def api(self, action, args=None):\n        \"\"\"\n        Sends a request to Solr Collections API.\n        Documentation is here: https://cwiki.apache.org/confluence/display/solr/Collections+API\n\n        :param string action: Name of the collection for the action\n        :param dict args: Dictionary of specific parameters for action\n        \"\"\"\n", "input": "", "output": "        if args is None:\n            args = {}\n        args['action'] = action.upper()\n\n        try:\n            res, con_info = self.solr.transport.send_request(endpoint='admin/collections', params=args)\n        except Exception as e:\n            self.logger.error(\"Error querying SolrCloud Collections API. \")\n            self.logger.exception(e)\n            raise e\n\n        if 'responseHeader' in res and res['responseHeader']['status'] == 0:\n            return res, con_info\n        else:\n            raise SolrError(\"Error Issuing Collections API Call for: {} +\".format(con_info, res))", "category": "Python"}, {"instruction": "def cleanup_handlers(event=None):\n    \"\"\"Remove handlers of a given `event`. If no event is informed, wipe\n    out all events registered.\n\n    Be careful!! This function is intended to help when writing tests\n    and for debugging purposes. If you call it, all handlers associated\n    to an event (or to all of them) will be disassociated. Which means\n    that you'll have to reload all modules that teclare handlers. I'm\n    sure you don't want it.\n    \"\"\"\n", "input": "", "output": "    if event:\n        if event in HANDLER_REGISTRY:\n            del HANDLER_REGISTRY[event]\n        if event in EXTERNAL_HANDLER_REGISTRY:\n            del EXTERNAL_HANDLER_REGISTRY[event]\n    else:\n        HANDLER_REGISTRY.clear()\n        EXTERNAL_HANDLER_REGISTRY.clear()", "category": "Python"}, {"instruction": "def cpc(self):\n        \"\"\"\n        :class:`~zhmcclient.Cpc`: The :term:`CPC` to which this storage group\n        is associated.\n\n        The returned :class:`~zhmcclient.Cpc` has only a minimal set of\n        properties populated.\n        \"\"\"\n", "input": "", "output": "        # We do here some lazy loading.\n        if not self._cpc:\n            cpc_uri = self.get_property('cpc-uri')\n            cpc_mgr = self.manager.console.manager.client.cpcs\n            self._cpc = cpc_mgr.resource_object(cpc_uri)\n        return self._cpc", "category": "Python"}, {"instruction": "def get_item_ids(self):\n        \"\"\"This is out of spec, but required for adaptive assessment parts?\"\"\"\n", "input": "", "output": "        item_ids = []\n        if self.has_items():\n            for idstr in self._my_map['itemIds']:\n                item_ids.append(idstr)\n        return IdList(item_ids)", "category": "Python"}, {"instruction": "def execute(self, i, o):\n        \"\"\"\n        Executes the command.\n\n        :type i: cleo.inputs.input.Input\n        :type o: cleo.outputs.output.Output\n        \"\"\"\n", "input": "", "output": "        super(MigrateMakeCommand, self).execute(i, o)\n\n        creator = MigrationCreator()\n\n        name = i.get_argument('name')\n        table = i.get_option('table')\n        create = bool(i.get_option('create'))\n\n        if not table and create is not False:\n            table = create\n\n        path = i.get_option('path')\n        if path is None:\n            path = self._get_migration_path()\n\n        file_ = self._write_migration(creator, name, table, create, path)\n\n        o.writeln('<info>Create migration: <comment>%s</comment></info>' % file_)", "category": "Python"}, {"instruction": "def _conn_maker(self, *args, **kwargs):\n        \"\"\"\n        This is called to create a connection instance. Normally you'd\n        pass a connection class to do_open, but it doesn't actually check for\n        a class, and just expects a callable. As long as we behave just as a\n        constructor would have, we should be OK. If it ever changes so that\n        we *must* pass a class, we'll create an UnsafeHTTPSConnection class\n        which just sets check_domain to False in the class definition, and\n        choose which one to pass to do_open.\n        \"\"\"\n", "input": "", "output": "        result = HTTPSConnection(*args, **kwargs)\n        if self.ca_certs:\n            result.ca_certs = self.ca_certs\n            result.check_domain = self.check_domain\n        return result", "category": "Python"}, {"instruction": "def build_model_from_xy(self, x_values, y_values):\n        \"\"\"Construct the model and perform regressions based on x, y data.\"\"\"\n", "input": "", "output": "        self.init_ace(x_values, y_values)\n        self.run_ace()\n        self.build_interpolators()", "category": "Python"}, {"instruction": "def L1(layer=\"input\", constant=0, batch=None):\n  \"\"\"L1 norm of layer. Generally used as penalty.\"\"\"\n", "input": "", "output": "  if batch is None:\n    return lambda T: tf.reduce_sum(tf.abs(T(layer) - constant))\n  else:\n    return lambda T: tf.reduce_sum(tf.abs(T(layer)[batch] - constant))", "category": "Python"}, {"instruction": "def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n", "input": "", "output": "        config = super(HAProxyCollector, self).get_default_config()\n        config.update({\n            'method':           'http',\n            'path':             'haproxy',\n            'url':              'http://localhost/haproxy?stats;csv',\n            'user':             'admin',\n            'pass':             'password',\n            'sock':             '/var/run/haproxy.sock',\n            'ignore_servers':   False,\n        })\n        return config", "category": "Python"}, {"instruction": "def send_nest_params_to_sli(p):\n    '''\n    Read parameters and send them to SLI\n    \n    Parameters\n    ----------\n    p : dict\n        sli parameter name and value as dictionary key and value pairs\n    \n    Returns\n    -------\n    None\n    '''\n", "input": "", "output": "    for name in p.keys():\n        value = p[name]\n        if type(value) == np.ndarray:\n            value = value.tolist()\n        if type(value) == dict:\n            value = dict_of_numpyarray_to_dict_of_list(value)\n        if name == 'neuron_model': # special case as neuron_model is a\n                                   # NEST model and not a string\n            try:\n                nest.sli_run('/'+name)\n                nest.sli_push(value)\n                nest.sli_run('eval')\n                nest.sli_run('def')\n            except: \n                print 'Could not put variable %s on SLI stack' % (name)\n                print type(value)\n        else:\n            try:\n                nest.sli_run('/'+name)\n                nest.sli_push(value)\n                nest.sli_run('def')\n            except: \n                print 'Could not put variable %s on SLI stack' % (name)\n                print type(value)\n    return", "category": "Python"}, {"instruction": "def get_authorization(self):\n        \"\"\"\n        Get the username and password for Basic authentication header.\n        :return Authentication: The authentication data or None if it is not present or invalid.\n        \"\"\"\n", "input": "", "output": "        auth = get_authorization_header()\n\n        if not auth:\n            return None\n\n        auth_type, auth_info = auth\n\n        if auth_type != b'basic':\n            return None\n\n        try:\n            username, password = base64.b64decode(auth_info).split(b':', 1)\n        except Exception:\n            return None\n\n        return Authorization('basic', username=bytes_to_wsgi(username), password=bytes_to_wsgi(password))", "category": "Python"}, {"instruction": "def read(self, filename):\n        \"\"\" Reads the file specified and tokenizes the data for parsing.\n            \"\"\"\n", "input": "", "output": "\n        try:\n            with open(filename, 'r') as _file:\n                self._filename = filename\n                self.readstream(_file)\n            return True\n\n        except IOError:\n            self._filename = None\n            return False", "category": "Python"}, {"instruction": "def get_host_info():\n    \"\"\"Collect some information about the machine this experiment runs on.\n\n    Returns\n    -------\n    dict\n        A dictionary with information about the CPU, the OS and the\n        Python version of this machine.\n\n    \"\"\"\n", "input": "", "output": "    host_info = {}\n    for k, v in host_info_gatherers.items():\n        try:\n            host_info[k] = v()\n        except IgnoreHostInfo:\n            pass\n    return host_info", "category": "Python"}, {"instruction": "def log_metrics(metrics, summ_writer, log_prefix, step, history=None):\n  \"\"\"Log metrics to summary writer and history.\"\"\"\n", "input": "", "output": "  rjust_len = max([len(name) for name in metrics])\n  for name, value in six.iteritems(metrics):\n    step_log(step, \"%s %s | % .8f\" % (\n        log_prefix.ljust(5), name.rjust(rjust_len), value))\n    full_name = \"metrics/\" + name\n    if history:\n      history.append(log_prefix, full_name, step, value)\n    if summ_writer:\n      summ_writer.scalar(full_name, value, step)", "category": "Python"}, {"instruction": "def get(self):\n        \"\"\"\n        :return: A string containing the TSV result from SSOS\n        :rtype: str\n        :raise: AssertionError\n        \"\"\"\n", "input": "", "output": "        params = self.param_dict_builder.params\n        logger.debug(pprint.pformat(format(params)))\n        response = requests.post(SSOS_URL,\n                                 data=params,\n                                 headers=self.headers)\n        logger.debug(response.url)\n        assert isinstance(response, requests.Response)\n        assert (response.status_code == requests.codes.ok)\n\n        lines = response.content\n        # note: spelling 'occured' is in SSOIS\n        if len(lines) < 2 or \"An error occured getting the ephemeris\" in lines:\n            print lines\n            print response.url\n            raise IOError(os.errno.EACCES,\n                          \"call to SSOIS failed on format error\")\n\n        if os.access(\"backdoor.tsv\", os.R_OK):\n            lines += open(\"backdoor.tsv\").read()\n        return lines", "category": "Python"}, {"instruction": "def call_orig(self, *args, **kwargs):\n        '''\n        Calls the original function.\n        '''\n", "input": "", "output": "        return self._orig(self._obj, *args, **kwargs)", "category": "Python"}, {"instruction": "def get_url(self, resource, params=None):\n        \"\"\"\n            Generate url for request\n        \"\"\"\n", "input": "", "output": "\n        # replace placeholders\n\n        pattern = r'\\{(.+?)\\}'\n\n        resource = re.sub(pattern, lambda t: str(params.get(t.group(1), '')), resource)\n\n        # build url\n\n        parts = (self.endpoint, '/api/', resource)\n        return '/'.join(map(lambda x: str(x).strip('/'), parts))", "category": "Python"}, {"instruction": "def _seqcluster_stats(data, out_dir):\n    \"\"\"Parse seqcluster output\"\"\"\n", "input": "", "output": "    name = dd.get_sample_name(data)\n    fn = data.get(\"seqcluster\", {}).get(\"stat_file\", None)\n    if not fn:\n        return None\n    out_file = os.path.join(out_dir, \"%s.txt\" % name)\n    df = pd.read_csv(fn, sep=\"\\t\", names = [\"reads\", \"sample\", \"type\"])\n    df_sample = df[df[\"sample\"] == name]\n    df_sample.to_csv(out_file, sep=\"\\t\")\n    return out_file", "category": "Python"}, {"instruction": "def mangle_unicode_to_ascii(s: Any) -> str:\n    \"\"\"\n    Mangle unicode to ASCII, losing accents etc. in the process.\n    \"\"\"\n", "input": "", "output": "    # http://stackoverflow.com/questions/1207457\n    if s is None:\n        return \"\"\n    if not isinstance(s, str):\n        s = str(s)\n    return (\n        unicodedata.normalize('NFKD', s)\n                   .encode('ascii', 'ignore')  # gets rid of accents\n                   .decode('ascii')  # back to a string\n    )", "category": "Python"}, {"instruction": "def add_date_facet(self, *args, **kwargs):\n        \"\"\"Add a date factory facet\"\"\"\n", "input": "", "output": "        self.facets.append(DateHistogramFacet(*args, **kwargs))", "category": "Python"}, {"instruction": "def server_args(self, parsed_args):\n        \"\"\"Return keyword args for Server class.\"\"\"\n", "input": "", "output": "        args = {\n            arg: value\n            for arg, value in vars(parsed_args).items()\n            if not arg.startswith('_') and value is not None\n        }\n        args.update(vars(self))\n        return args", "category": "Python"}, {"instruction": "def VerifyCoARequest(self):\n        \"\"\"Verify request authenticator.\n        :return: True if verification failed else False\n        :rtype: boolean\n        \"\"\"\n", "input": "", "output": "        assert(self.raw_packet)\n        hash = md5_constructor(self.raw_packet[0:4] + 16 * six.b('\\x00') +\n                self.raw_packet[20:] + self.secret).digest()\n        return hash == self.authenticator", "category": "Python"}, {"instruction": "def transaction(self, request):\n        \"\"\"Ideally at this method, you will check the \n        caller reference against a user id or uniquely\n        identifiable attribute (if you are already not \n        using it as the caller reference) and the type \n        of transaction (either pay, reserve etc). For\n        the sake of the example, we assume all the users\n        get charged $100\"\"\"\n", "input": "", "output": "        request_url = request.build_absolute_uri()\n        parsed_url = urlparse.urlparse(request_url)\n        query = parsed_url.query\n        dd = dict(map(lambda x: x.split(\"=\"), query.split(\"&\")))\n        resp = self.purchase(100, dd)\n        return HttpResponseRedirect(\"%s?status=%s\" %(reverse(\"app_offsite_amazon_fps\"),\n                                resp[\"status\"]))", "category": "Python"}, {"instruction": "def reload_manifest(self, manifest):\n        \"\"\"\n        Reloads a manifest from the disk\n        :param manifest: The manifest to reload\n        \"\"\"\n", "input": "", "output": "        self._logger.debug(\"Reloading manifest for {}.\".format(manifest.get(\"name\", \"Unnamed Plugin\")))\n        self._manifests.remove(manifest)\n        self.load_manifest(manifest[\"path\"])\n        self._logger.debug(\"Manifest reloaded.\")", "category": "Python"}, {"instruction": "def get_property_names(self):\n        '''\n        Returns all property names in this collection of signatures\n        '''\n", "input": "", "output": "        property_names = set()\n        for signature_value in self.values():\n            for property_name in signature_value.keys():\n                property_names.add(property_name)\n        return property_names", "category": "Python"}, {"instruction": "def mkstemp(*args, **kwargs):\n  \"\"\"\n  Context manager similar to tempfile.NamedTemporaryFile except the file is not deleted on close, and only the filepath\n  is returned\n  .. warnings:: Unlike tempfile.mkstemp, this is not secure\n  \"\"\"\n", "input": "", "output": "  fd, filename = tempfile.mkstemp(*args, **kwargs)\n  os.close(fd)\n  try:\n    yield filename\n  finally:\n    os.remove(filename)", "category": "Python"}, {"instruction": "def _configure_from_module(self, item):\n        \"\"\"Configure from a module by import path.\n\n        Effectively, you give this an absolute or relative import path, it will\n        import it, and then pass the resulting object to\n        ``_configure_from_object``.\n\n        Args:\n            item (str):\n                A string pointing to a valid import path.\n\n        Returns:\n            fleaker.App:\n                Returns itself.\n        \"\"\"\n", "input": "", "output": "        package = None\n        if item[0] == '.':\n            package = self.import_name\n\n        obj = importlib.import_module(item, package=package)\n\n        self.config.from_object(obj)\n\n        return self", "category": "Python"}, {"instruction": "def get_callbacks_from_message(self, msg):\n        \"\"\"Return the callbacks associated with a message template.\"\"\"\n", "input": "", "output": "        callbacks = []\n        for key in self._find_matching_keys(msg):\n            for callback in self[key]:\n                callbacks.append(callback)\n        return callbacks", "category": "Python"}, {"instruction": "def method_name(func):\n    \"\"\"Method wrapper that adds the name of the method being called to its arguments list in Pascal case\n\n    \"\"\"\n", "input": "", "output": "    @wraps(func)\n    def _method_name(*args, **kwargs):\n        name = to_pascal_case(func.__name__)\n        return func(name=name, *args, **kwargs)\n    return _method_name", "category": "Python"}, {"instruction": "def interconnect_all(self):\n        \"\"\"Propagate dependencies for provided instances\"\"\"\n", "input": "", "output": "        for dep in topologically_sorted(self._provides):\n            if hasattr(dep, '__injections__') and not hasattr(dep, '__injections_source__'):\n                self.inject(dep)", "category": "Python"}, {"instruction": "def process_request(self, req, resp):\n        \"\"\" Process the request before routing it.\n\n        We always enforce the use of SSL.\n        \"\"\"\n", "input": "", "output": "\n        if goldman.config.TLS_REQUIRED and req.protocol != 'https':\n            abort(TLSRequired)", "category": "Python"}, {"instruction": "def sendcontrol(self, char):\n        '''Helper method that wraps send() with mnemonic access for sending control\n        character to the child (such as Ctrl-C or Ctrl-D).  For example, to send\n        Ctrl-G (ASCII 7, bell, '\\a')::\n\n            child.sendcontrol('g')\n\n        See also, sendintr() and sendeof().\n        '''\n", "input": "", "output": "        char = char.lower()\n        a = ord(char)\n        if 97 <= a <= 122:\n            a = a - ord('a') + 1\n            byte = _byte(a)\n            return self._writeb(byte), byte\n        d = {'@': 0, '`': 0,\n            '[': 27, '{': 27,\n            '\\\\': 28, '|': 28,\n            ']': 29, '}': 29,\n            '^': 30, '~': 30,\n            '_': 31,\n            '?': 127}\n        if char not in d:\n            return 0, b''\n\n        byte = _byte(d[char])\n        return self._writeb(byte), byte", "category": "Python"}, {"instruction": "def prepack(self, namedstruct, skip_self=False, skip_sub=False):\n        '''\n        Run prepack\n        '''\n", "input": "", "output": "        if not skip_sub and hasattr(namedstruct, self.name) and hasattr(self.basetypeparser, 'fullprepack'):\n            self.basetypeparser.fullprepack(getattr(namedstruct, self.name))\n        Parser.prepack(self, namedstruct, skip_self, skip_sub)", "category": "Python"}, {"instruction": "def plotsignal(self,fig=None,saveplot=True,folder=None,figformat='png',**kwargs):\n        \"\"\"\n        Plots TransitSignal\n\n        Calls :func:`TransitSignal.plot`, saves to provided folder.\n\n        :param fig: (optional)\n            Argument for :func:`plotutils.setfig`.\n\n        :param saveplot: (optional)\n            Whether to save figure.\n\n        :param folder: (optional)\n            Folder to which to save plot\n\n        :param figformat: (optional)\n            Desired format for figure.\n\n        :param **kwargs:\n            Additional keyword arguments passed to :func:`TransitSignal.plot`.\n        \"\"\"\n", "input": "", "output": "        if folder is None:\n            folder = self.folder\n\n        self.trsig.plot(plot_trap=True,fig=fig,**kwargs)\n        if saveplot:\n            plt.savefig('%s/signal.%s' % (folder,figformat))\n            plt.close()", "category": "Python"}, {"instruction": "def sort_amounts(proteins, sort_index):\n    \"\"\"Generic function for sorting peptides and psms. Assumes a higher\n    number is better for what is passed at sort_index position in protein.\"\"\"\n", "input": "", "output": "    amounts = {}\n    for protein in proteins:\n        amount_x_for_protein = protein[sort_index]\n        try:\n            amounts[amount_x_for_protein].append(protein)\n        except KeyError:\n            amounts[amount_x_for_protein] = [protein]\n    return [v for k, v in sorted(amounts.items(), reverse=True)]", "category": "Python"}, {"instruction": "def get_merged_filenode_list(nodes, filter_media_types=None, exclude_media_types=None, filter=None, ordering=None, processors=None, max_depth=None, max_nodes=None):\n    \"\"\"\n    Almost the same as :func:`get_nested_filenode_list`, but returns a flat (one-dimensional) list.\n    Using the same QuerySet as in the example for `get_nested_filenode_list`, this method would return::\n\n        [\n            <FileNode: Empty folder>,\n            <FileNode: Photo folder>,\n            <FileNode: photo1.jpg>,\n            <FileNode: photo2.jpg>,\n            <FileNode: photo3.jpg>,\n            <FileNode: file.txt>\n        ]\n\n    \"\"\"\n", "input": "", "output": "    return __get_filenode_list(nodes, filter_media_types=filter_media_types, exclude_media_types=exclude_media_types,\n        filter=filter, ordering=ordering, processors=processors, list_method='extend', max_depth=max_depth, max_nodes=max_nodes)", "category": "Python"}, {"instruction": "def _connect_su(spec):\n    \"\"\"\n    Return ContextService arguments for su as a become method.\n    \"\"\"\n", "input": "", "output": "    return {\n        'method': 'su',\n        'enable_lru': True,\n        'kwargs': {\n            'username': spec.become_user(),\n            'password': spec.become_pass(),\n            'python_path': spec.python_path(),\n            'su_path': spec.become_exe(),\n            'connect_timeout': spec.timeout(),\n            'remote_name': get_remote_name(spec),\n        }\n    }", "category": "Python"}, {"instruction": "def _simplify_function(self):\n        \"\"\"\n        Simplify the entire function.\n\n        :return:    None\n        \"\"\"\n", "input": "", "output": "\n        # Computing reaching definitions\n        rd = self.project.analyses.ReachingDefinitions(func=self.function, func_graph=self.graph, observe_all=True)\n\n        simp = self.project.analyses.AILSimplifier(self.function, func_graph=self.graph, reaching_definitions=rd)\n\n        for key in list(self._blocks.keys()):\n            old_block = self._blocks[key]\n            if old_block in simp.blocks:\n                self._blocks[key] = simp.blocks[old_block]\n\n        self._update_graph()", "category": "Python"}, {"instruction": "def get(self, index, id, fields=None, doc_type=EsConst.ALL_VALUES, **query_params):\n        \"\"\"\n        Retrieve specific record by id\n        `<http://www.elastic.co/guide/en/elasticsearch/reference/current/docs-get.html>`_\n        :param index: the index name to query\n        :param id: the id of the record\n        :param fields: the fields you what to fetch from the record (str separated by comma's)\n        :param doc_type: the doc type to search in\n        :param query_params: params\n        :return:\n        \"\"\"\n", "input": "", "output": "        if fields:\n            query_params[EsConst.FIELDS] = fields\n\n        path = self._es_parser.make_path(index, doc_type, id)\n        result = yield self._perform_request(HttpMethod.GET, path, params=query_params)\n        returnValue(result)", "category": "Python"}, {"instruction": "def get_gps(self, image):\n        \"\"\"\u83b7\u53d6\u7ecf\u5ea6\uff0c\u7eac\u5ea6\"\"\"\n", "input": "", "output": "        exif_data = self.get_exif_data(image)\n        return self.get_lat_lon(exif_data)", "category": "Python"}, {"instruction": "def fsplit(pred, objs):\n    \"\"\"Split a list into two classes according to the predicate.\"\"\"\n", "input": "", "output": "    t = []\n    f = []\n    for obj in objs:\n        if pred(obj):\n            t.append(obj)\n        else:\n            f.append(obj)\n    return (t, f)", "category": "Python"}, {"instruction": "def as_unicode(obj, encoding=convert.LOCALE, pretty=False):\n    \"\"\"\n    Representing any object to <unicode> string (python2.7) or <str> string (python3.0).\n\n    :param obj: any object\n    :type encoding: str\n    :param encoding: codec for encoding unicode strings\n                     (locale.getpreferredencoding() by default)\n    :type pretty: bool\n    :param pretty: pretty print\n\n    :rtype: unicode\n    :return: any object as unicode string\n    \"\"\"\n", "input": "", "output": "\n    return convert.convert(obj, encoding, 0 if pretty else None)", "category": "Python"}, {"instruction": "def _is_target_region(self, stat, game_region):\r\n        \"\"\"Returns if the stat matches target game region.\r\n\r\n            :param stat: Json of gameplay stat.\r\n            :type stat: dict\r\n            :param game_region: Target game region.\r\n            :type game_region: str\r\n            :return: return does the stat matches target game region.\r\n            :rtype: bool\r\n        \"\"\"\n", "input": "", "output": "        if game_region == constants.GAME_REGION_WILDCARD:\r\n            return True\r\n        return stat['Region'] == game_region", "category": "Python"}, {"instruction": "def resteem(self, identifier):\n        ''' Waits 20 seconds as that is the required \n        amount of time between resteems\n        '''\n", "input": "", "output": "        for num_of_retries in range(default.max_retry):\n            try:\n                self.steem_instance().resteem(\n                    identifier, self.mainaccount)\n                self.msg.message(\"resteemed \" + identifier)\n                time.sleep(10)\n            except Exception as e:\n                self.util.retry(", "category": "Python"}, {"instruction": "def _ProcessFileSource(self, source):\n    \"\"\"Glob paths and return StatEntry objects.\"\"\"\n", "input": "", "output": "\n    if source.path_type != rdf_paths.PathSpec.PathType.OS:\n      raise ValueError(\"Only supported path type is OS.\")\n\n    paths = artifact_utils.InterpolateListKbAttributes(\n        source.base_source.attributes[\"paths\"], self.knowledge_base,\n        self.ignore_interpolation_errors)\n\n    file_finder_action = rdf_file_finder.FileFinderAction.Stat()\n    request = rdf_file_finder.FileFinderArgs(\n        paths=paths, pathtype=source.path_type, action=file_finder_action)\n    action = file_finder.FileFinderOSFromClient\n\n    yield action, request", "category": "Python"}, {"instruction": "def create(model: ModelFactory, discount_factor: float, target_update_frequency: int,\n           max_grad_norm: float, double_dqn: bool = False):\n    \"\"\" Vel factory function \"\"\"\n", "input": "", "output": "    return DistributionalDeepQLearning(\n        model_factory=model,\n        discount_factor=discount_factor,\n        double_dqn=double_dqn,\n        target_update_frequency=target_update_frequency,\n        max_grad_norm=max_grad_norm\n    )", "category": "Python"}, {"instruction": "def iterateBlocksBackFrom(block):\n        \"\"\"Generator, which iterates QTextBlocks from block until the Start of a document\n        But, yields not more than MAX_SEARCH_OFFSET_LINES\n        \"\"\"\n", "input": "", "output": "        count = 0\n        while block.isValid() and count < MAX_SEARCH_OFFSET_LINES:\n            yield block\n            block = block.previous()\n            count += 1", "category": "Python"}, {"instruction": "def update(self, status):\n        \"\"\"\n        Update the RoomInstance\n\n        :param RoomInstance.RoomStatus status: Set to completed to end the Room.\n\n        :returns: Updated RoomInstance\n        :rtype: twilio.rest.video.v1.room.RoomInstance\n        \"\"\"\n", "input": "", "output": "        data = values.of({'Status': status, })\n\n        payload = self._version.update(\n            'POST',\n            self._uri,\n            data=data,\n        )\n\n        return RoomInstance(self._version, payload, sid=self._solution['sid'], )", "category": "Python"}, {"instruction": "def view(db_name):\n    \"\"\"\n    Register a map function as a view\n\n    Currently, only a single map function can be created for each view\n\n    NOTE: the map function source is saved in CouchDB, so it cannot depend on\n    anything outside the function's scope.\n\n    :param db_name: the database name\n    \"\"\"\n", "input": "", "output": "    def decorator(func):\n        v = View(db_name, func)\n        v.register()\n        return v\n    return decorator", "category": "Python"}, {"instruction": "def _parse_trigger(self, trigger_clause):\n        \"\"\"Parse a named event or explicit stream trigger into a TriggerDefinition.\"\"\"\n", "input": "", "output": "\n        cond = trigger_clause[0]\n\n        named_event = None\n        explicit_stream = None\n        explicit_trigger = None\n\n        # Identifier parse tree is Group(Identifier)\n        if cond.getName() == 'identifier':\n            named_event = cond[0]\n        elif cond.getName() == 'stream_trigger':\n            trigger_type = cond[0]\n            stream = cond[1]\n            oper = cond[2]\n            ref = cond[3]\n\n            trigger = InputTrigger(trigger_type, oper, ref)\n            explicit_stream = stream\n            explicit_trigger = trigger\n        elif cond.getName() == 'stream_always':\n            stream = cond[0]\n            trigger = TrueTrigger()\n            explicit_stream = stream\n            explicit_trigger = trigger\n        else:\n            raise ArgumentError(\"OnBlock created from an invalid ParseResults object\", parse_results=trigger_clause)\n\n        return TriggerDefinition(named_event, explicit_stream, explicit_trigger)", "category": "Python"}, {"instruction": "def do_patch(endpoint, body, access_token):\n    '''Do an HTTP PATCH request and return JSON.\n\n    Args:\n        endpoint (str): Azure Resource Manager management endpoint.\n        body (str): JSON body of information to patch.\n        access_token (str): A valid Azure authentication token.\n\n    Returns:\n        HTTP response. JSON body.\n    '''\n", "input": "", "output": "    headers = {\"content-type\": \"application/json\", \"Authorization\": 'Bearer ' + access_token}\n    headers['User-Agent'] = get_user_agent()\n    return requests.patch(endpoint, data=body, headers=headers)", "category": "Python"}, {"instruction": "def alt_fn(self, i, pre_dl=None, post_dl=None):\n        \"\"\"Press Alt + Fn1 ~ 12 once.\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u6309\u4e0b Alt + Fn1 ~ 12 \u7ec4\u5408\u952e\u3002\n        \"\"\"\n", "input": "", "output": "        self.delay(pre_dl)\n        self.k.press_key(self.k.alt_key)\n        self.k.tap_key(self.k.function_keys[i])\n        self.k.release_key(self.k.alt_key)\n        self.delay(post_dl)", "category": "Python"}, {"instruction": "def _setup_postprocess_hds_timeseries(hds_file, df, config_file, prefix=None, model=None):\n    \"\"\"Dirty function to post process concentrations in inactive/dry cells\"\"\"\n", "input": "", "output": "    warnings.warn(\n        \"Setting up post processing of hds or ucn timeseries obs. \"\n        \"Prepending 'pp' to obs name may cause length to exceed 20 chars\", PyemuWarning)\n    if model is not None:\n        t_str = df.index.map(lambda x: x.strftime(\"%Y%m%d\"))\n    else:\n        t_str = df.index.map(lambda x: \"{0:08.2f}\".format(x))\n    if prefix is not None:\n        prefix = \"pp{0}\".format(prefix)\n    else:\n        prefix = \"pp\"\n    ins_file = hds_file+\"_timeseries.post_processed.ins\"\n    print(\"writing instruction file to {0}\".format(ins_file))\n    with open(ins_file,'w') as f:\n        f.write('pif ~\\n')\n        f.write(\"l1 \\n\")\n        for t in t_str:\n            f.write(\"l1 w \")\n            for site in df.columns:\n                obsnme = \"{0}{1}_{2}\".format(prefix, site, t)\n                f.write(\" !{0}!\".format(obsnme))\n            f.write('\\n')\n    frun_line = \"pyemu.gw_utils._apply_postprocess_hds_timeseries('{0}')\\n\".format(config_file)\n    return frun_line", "category": "Python"}, {"instruction": "def add_badge(self, kind):\n        '''Perform an atomic prepend for a new badge'''\n", "input": "", "output": "        badge = self.get_badge(kind)\n        if badge:\n            return badge\n        if kind not in getattr(self, '__badges__', {}):\n            msg = 'Unknown badge type for {model}: {kind}'\n            raise db.ValidationError(msg.format(model=self.__class__.__name__,\n                                                kind=kind))\n        badge = Badge(kind=kind)\n        if current_user.is_authenticated:\n            badge.created_by = current_user.id\n\n        self.update(__raw__={\n            '$push': {\n                'badges': {\n                    '$each': [badge.to_mongo()],\n                    '$position': 0\n                }\n            }\n        })\n        self.reload()\n        post_save.send(self.__class__, document=self)\n        on_badge_added.send(self, kind=kind)\n        return self.get_badge(kind)", "category": "Python"}, {"instruction": "def recursive_model_update(model, props):\n    \"\"\"\n    Recursively updates attributes on a model including other\n    models. If the type of the new model matches the old model\n    properties are simply updated, otherwise the model is replaced.\n    \"\"\"\n", "input": "", "output": "    updates = {}\n    valid_properties = model.properties_with_values()\n    for k, v in props.items():\n        if isinstance(v, Model):\n            nested_model = getattr(model, k)\n            if type(v) is type(nested_model):\n                nested_props = v.properties_with_values(include_defaults=False)\n                recursive_model_update(nested_model, nested_props)\n            else:\n                setattr(model, k, v)\n        elif k in valid_properties and v != valid_properties[k]:\n            updates[k] = v\n    model.update(**updates)", "category": "Python"}, {"instruction": "def actual_original_query_range(self):\n    \"\"\" This accounts for hard clipped bases\n    and a query sequence that hasnt been reverse complemented\n\n    :return: the range covered on the original query sequence\n    :rtype: GenomicRange\n    \"\"\"\n", "input": "", "output": "    l = self.original_query_sequence_length\n    a = self.alignment_ranges\n    qname = a[0][1].chr\n    qstart = a[0][1].start\n    qend = a[-1][1].end\n    #rng = self.get_query_range()\n    start = qstart\n    end = qend\n    if self.strand == '-':\n      end = l-(qstart-1)\n      start = 1+l-(qend)\n    return GenomicRange(qname,start,end,dir=self.strand)", "category": "Python"}, {"instruction": "def pass_obj(f):\n    \"\"\"Similar to :func:`pass_context`, but only pass the object on the\n    context onwards (:attr:`Context.obj`).  This is useful if that object\n    represents the state of a nested system.\n    \"\"\"\n", "input": "", "output": "    def new_func(*args, **kwargs):\n        return f(get_current_context().obj, *args, **kwargs)\n    return update_wrapper(new_func, f)", "category": "Python"}, {"instruction": "def serialize_list(self, data):\n        \"\"\"\n        Given a collection of data (``objects`` or ``dicts``), serializes them.\n\n        :param data: The collection of items to serialize\n        :type data: list or iterable\n\n        :returns: The serialized body\n        :rtype: string\n        \"\"\"\n", "input": "", "output": "        if data is None:\n            return ''\n\n        # Check for a ``Data``-like object. We should assume ``True`` (all\n        # data gets prepared) unless it's explicitly marked as not.\n        if not getattr(data, 'should_prepare', True):\n            prepped_data = data.value\n        else:\n            prepped_data = [self.prepare(item) for item in data]\n\n        final_data = self.wrap_list_response(prepped_data)\n        return self.serializer.serialize(final_data)", "category": "Python"}, {"instruction": "def unit_is_related(self, location, worksheet):\n        '''\n        Checks for relationship between a unit location and this block.\n\n        Returns:\n            True if the location is related to this block.\n        '''\n", "input": "", "output": "        same_worksheet = worksheet == self.worksheet\n        if isinstance(location, (tuple, list)):\n            return (location[0] >= self.start[0] and location[0] < self.end[0] and\n                    location[1] >= self.start[1] and location[1] < self.end[1] and\n                    same_worksheet)\n        else:\n            return same_worksheet", "category": "Python"}, {"instruction": "def _extract_docs_raises(self):\n        \"\"\"Extract raises description from docstring. The internal computed raises list is\n        composed by tuples (raise, description).\n\n        \"\"\"\n", "input": "", "output": "        if self.dst.style['in'] == 'numpydoc':\n            data = '\\n'.join([d.rstrip().replace(self.docs['out']['spaces'], '', 1) for d in self.docs['in']['raw'].splitlines()])\n            self.docs['in']['raises'] += self.dst.numpydoc.get_raise_list(data)\n        if self.dst.style['in'] == 'google':\n            data = '\\n'.join([d.rstrip().replace(self.docs['out']['spaces'], '', 1) for d in self.docs['in']['raw'].splitlines()])\n            self.docs['in']['raises'] += self.dst.googledoc.get_raise_list(data)\n        elif self.dst.style['in'] == 'groups':\n            self._extract_groupstyle_docs_raises()\n        elif self.dst.style['in'] in ['javadoc', 'reST']:\n            self._extract_tagstyle_docs_raises()", "category": "Python"}, {"instruction": "def traptransit_MCMC(ts,fs,dfs=1e-5,nwalkers=200,nburn=300,niter=1000,\n                     threads=1,p0=[0.1,0.1,3,0],return_sampler=False,\n                     maxslope=MAXSLOPE):\n    \"\"\"\n    Fit trapezoidal model to provided ts, fs, [dfs] using MCMC.\n\n    Standard emcee usage.\n    \"\"\"\n", "input": "", "output": "    model = TraptransitModel(ts,fs,dfs,maxslope=maxslope)\n    sampler = emcee.EnsembleSampler(nwalkers,4,model,threads=threads)\n    T0 = p0[0]*(1+rand.normal(size=nwalkers)*0.1)\n    d0 = p0[1]*(1+rand.normal(size=nwalkers)*0.1)\n    slope0 = p0[2]*(1+rand.normal(size=nwalkers)*0.1)\n    ep0 = p0[3]+rand.normal(size=nwalkers)*0.0001\n\n    p0 = np.array([T0,d0,slope0,ep0]).T\n\n    pos, prob, state = sampler.run_mcmc(p0, nburn)\n    sampler.reset()\n    sampler.run_mcmc(pos, niter, rstate0=state)\n    if return_sampler:\n        return sampler\n    else:\n        return sampler.flatchain[:,0],sampler.flatchain[:,1],sampler.flatchain[:,2],sampler.flatchain[:,3]", "category": "Python"}, {"instruction": "def moothedata(data, key=None):\n    \"\"\"Return an amusing picture containing an item from a dict.\n\n    Parameters\n    ----------\n    data: mapping\n        A mapping, such as a raster dataset's ``meta`` or ``profile``\n        property.\n    key:\n        A key of the ``data`` mapping.\n    \"\"\"\n", "input": "", "output": "    if not key:\n        key = choice(list(data.keys()))\n        logger.debug(\"Using randomly chosen key: %s\", key)\n    msg = cow.Moose().milk(\"{0}: {1}\".format(key.capitalize(), data[key]))\n    return msg", "category": "Python"}, {"instruction": "def _restore_clipboard_text(self, backup: str):\n        \"\"\"Restore the clipboard content.\"\"\"\n", "input": "", "output": "        # Pasting takes some time, so wait a bit before restoring the content. Otherwise the restore is done before\n        # the pasting happens, causing the backup to be pasted instead of the desired clipboard content.\n        time.sleep(0.2)\n        self.clipboard.text = backup if backup is not None else \"\"", "category": "Python"}, {"instruction": "def _doublec(self, word):\n        \"\"\"doublec(word) is TRUE <=> word ends with a double consonant\"\"\"\n", "input": "", "output": "        if len(word) < 2:\n            return False\n        if (word[-1] != word[-2]):\n            return False\n        return self._cons(word, len(word)-1)", "category": "Python"}, {"instruction": "def p_variable_declaration_list(self, p):\n        \"\"\"\n        variable_declaration_list \\\n            : variable_declaration\n            | variable_declaration_list COMMA variable_declaration\n        \"\"\"\n", "input": "", "output": "        if len(p) == 2:\n            p[0] = [p[1]]\n        else:\n            p[1].append(p[3])\n            p[0] = p[1]", "category": "Python"}, {"instruction": "def set_bgcolor(self, color):\n        \"\"\"set color for background of plot\"\"\"\n", "input": "", "output": "        self.bgcolor = color\n        for ax in self.canvas.figure.get_axes():\n            if matplotlib.__version__ < '2.0':\n                ax.set_axis_bgcolor(color)\n            else:\n                ax.set_facecolor(color)\n        if callable(self.theme_color_callback):\n            self.theme_color_callback(color, 'bg')", "category": "Python"}, {"instruction": "def write_word(self, cmd, value):\n        \"\"\"\n        Writes a 16-bit word to the specified command register\n        \"\"\"\n", "input": "", "output": "        self.bus.write_word_data(self.address, cmd, value)\n        self.log.debug(\n            \"write_word: Wrote 0x%04X to command register 0x%02X\" % (\n                value, cmd\n            )\n        )", "category": "Python"}, {"instruction": "def jsonget(self, name, *args):\n        \"\"\"\n        Get the object stored as a JSON value at key ``name``\n        ``args`` is zero or more paths, and defaults to root path\n        \"\"\"\n", "input": "", "output": "        pieces = [name]\n        if len(args) == 0:\n            pieces.append(Path.rootPath())\n        else:\n            for p in args:\n                    pieces.append(str_path(p))\n\n        # Handle case where key doesn't exist. The JSONDecoder would raise a\n        # TypeError exception since it can't decode None\n        try:\n            return self.execute_command('JSON.GET', *pieces)\n        except TypeError:\n            return None", "category": "Python"}, {"instruction": "def timeit(threshold=0):\n    \"\"\"Decorator to log the execution time of a function\n    \"\"\"\n", "input": "", "output": "\n    def inner(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            start = time.time()\n            return_value = func(*args, **kwargs)\n            end = time.time()\n            duration = float(end-start)\n            if duration > threshold:\n                logger.info(\"Execution of '{}{}' took {:2f}s\".format(\n                    func.__name__, args, duration))\n            return return_value\n        return wrapper\n    return inner", "category": "Python"}, {"instruction": "def mark_rewrite(self, *names):\n        \"\"\"Mark import names as needing to be re-written.\n\n        The named module or package as well as any nested modules will\n        be re-written on import.\n        \"\"\"\n", "input": "", "output": "        already_imported = set(names).intersection(set(sys.modules))\n        if already_imported:\n            for name in already_imported:\n                if name not in self._rewritten_names:\n                    self._warn_already_imported(name)\n        self._must_rewrite.update(names)", "category": "Python"}, {"instruction": "def is_serving(self) -> bool:\n        \"\"\"\n        Tell whether the server is accepting new connections or shutting down.\n\n        \"\"\"\n", "input": "", "output": "        try:\n            # Python \u2265 3.7\n            return self.server.is_serving()  # type: ignore\n        except AttributeError:  # pragma: no cover\n            # Python < 3.7\n            return self.server.sockets is not None", "category": "Python"}, {"instruction": "def ResolvePrefix(self, subject, attribute_prefix, timestamp=None,\n                    limit=None):\n    \"\"\"Retrieve a set of value matching for this subject's attribute.\n\n    Args:\n      subject: The subject that we will search.\n      attribute_prefix: The attribute prefix.\n      timestamp: A range of times for consideration (In microseconds). Can be a\n        constant such as ALL_TIMESTAMPS or NEWEST_TIMESTAMP or a tuple of ints\n        (start, end).\n      limit: The number of results to fetch.\n\n    Returns:\n       A list of (attribute, value string, timestamp).\n\n       Values with the same attribute (happens when timestamp is not\n       NEWEST_TIMESTAMP, but ALL_TIMESTAMPS or time range) are guaranteed\n       to be ordered in the decreasing timestamp order.\n\n    Raises:\n      AccessError: if anything goes wrong.\n    \"\"\"\n", "input": "", "output": "    for _, values in self.MultiResolvePrefix([subject],\n                                             attribute_prefix,\n                                             timestamp=timestamp,\n                                             limit=limit):\n      values.sort(key=lambda a: a[0])\n      return values\n\n    return []", "category": "Python"}, {"instruction": "def ack(self):\n        \"\"\"Acknowledge the given message.\n\n        Acknowledging a message in Pub/Sub means that you are done\n        with it, and it will not be delivered to this subscription again.\n        You should avoid acknowledging messages until you have\n        *finished* processing them, so that in the event of a failure,\n        you receive the message again.\n\n        .. warning::\n            Acks in Pub/Sub are best effort. You should always\n            ensure that your processing code is idempotent, as you may\n            receive any given message more than once.\n        \"\"\"\n", "input": "", "output": "        time_to_ack = math.ceil(time.time() - self._received_timestamp)\n        self._request_queue.put(\n            requests.AckRequest(\n                ack_id=self._ack_id, byte_size=self.size, time_to_ack=time_to_ack\n            )\n        )", "category": "Python"}, {"instruction": "def training(self):\n        \"\"\"\n        Returns:\n            A :class:`TowerTensorHandles`, containing only the training towers.\n        \"\"\"\n", "input": "", "output": "        handles = [h for h in self._handles if h.is_training]\n        return TowerTensorHandles(handles)", "category": "Python"}, {"instruction": "def send(self, command, sender, target, args, kwargs):\n        \"\"\"Used by the server to send messages to the client.\n        Returns a future.\n        \"\"\"\n", "input": "", "output": "        command = get_command(command)\n        data = {'command': command.__name__,\n                'id': create_aid(),\n                'sender': actor_identity(sender),\n                'target': actor_identity(target),\n                'args': args if args is not None else (),\n                'kwargs': kwargs if kwargs is not None else {}}\n\n        waiter = self._loop.create_future()\n        ack = None\n        if command.ack:\n            ack = create_aid()\n            data['ack'] = ack\n            self.pending_responses[ack] = waiter\n\n        try:\n            self.write(data)\n        except Exception as exc:\n            waiter.set_exception(exc)\n            if ack:\n                self.pending_responses.pop(ack, None)\n        else:\n            if not ack:\n                waiter.set_result(None)\n        return waiter", "category": "Python"}, {"instruction": "def Kdiag(self, X):\n        \"\"\"Compute the diagonal of the covariance matrix associated to X.\"\"\"\n", "input": "", "output": "        vyt = self.variance_Yt\n        vyx = self.variance_Yx\n\n        lyt = 1./(2*self.lengthscale_Yt)\n        lyx = 1./(2*self.lengthscale_Yx)\n\n        a = self.a\n        b = self.b\n        c = self.c\n\n        ## dk^2/dtdt'\n        k1 = (2*lyt )*vyt*vyx\n        ## dk^2/dx^2\n        k2 = ( - 2*lyx )*vyt*vyx\n        ## dk^4/dx^2dx'^2\n        k3 = ( 4*3*lyx**2 )*vyt*vyx\n\n\n        Kdiag = np.zeros(X.shape[0])\n        slices = index_to_slices(X[:,-1])\n\n        for i, ss1 in enumerate(slices):\n            for s1 in ss1:\n                if i==0:\n                    Kdiag[s1]+= vyt*vyx\n                elif i==1:\n                    #i=1\n                    Kdiag[s1]+= b**2*k1 - 2*a*c*k2 + a**2*k3 + c**2*vyt*vyx\n                    #Kdiag[s1]+= Vu*Vy*(k1+k2+k3)\n                else:\n                    raise ValueError(\"invalid input/output index\")\n\n        return Kdiag", "category": "Python"}, {"instruction": "def name(self, name=None):\n        '''api name, default is module.__name__'''\n", "input": "", "output": "        if name:\n            self._name = name\n            return self\n        return self._name", "category": "Python"}, {"instruction": "def publish_attrs(self, upcount=1):\n        \"\"\"\n        Magic function which inject all attrs into the callers namespace\n        :param upcount     int, how many stack levels we go up\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        frame = inspect.currentframe()\n        i = upcount\n        while True:\n            if frame.f_back is None:\n                break\n            frame = frame.f_back\n            i -= 1\n            if i == 0:\n                break\n\n        for k, v in self.__dict__.items():\n            frame.f_globals[k] = v", "category": "Python"}, {"instruction": "def valid_action_request(method):\n    \"\"\" wraps method with verification for is_request_valid\"\"\"\n", "input": "", "output": "\n    @functools.wraps(method)\n    def _wrapper(self, *args, **kwargs):\n        assert isinstance(self, BaseRequestHandler)\n        if not self.is_request_valid:\n            return self.reply_bad_request()\n\n        try:\n            return method(self, *args, **kwargs)\n        except UserWarning as e:\n            return self.reply_server_error(e)\n        except Exception as e:\n            return self.reply_server_error(e)\n\n    return _wrapper", "category": "Python"}, {"instruction": "def get_language_stemmer(language):\n    \"\"\"Retrieves the SnowballStemmer for a particular language.\n\n    Args:\n        language (str): ISO-639-1 code of the language.\n    \"\"\"\n", "input": "", "output": "    from lunr.languages import SUPPORTED_LANGUAGES\n    from nltk.stem.snowball import SnowballStemmer\n\n    return SnowballStemmer(SUPPORTED_LANGUAGES[language])", "category": "Python"}, {"instruction": "def navigate(self, name, *args):\n        \"\"\"Navigate to a route\n        * \n        * @param  {String} name Route name\n        * @param  {*}      arg  A single argument to pass to the route handler\n        */\n        \"\"\"\n", "input": "", "output": "        if name not in self.routes:\n            raise Exception('invalid route name \\'%s\\'' % name)\n        elif callable(self.routes[name]):\n            return self.routes[name](self, *args)\n        raise Exception('route %s not callable', name)", "category": "Python"}, {"instruction": "def aggr(self, group, *attributes, keep_all_rows=False, **named_attributes):\n        \"\"\"\n        Aggregation/projection operator\n        :param group:  an entity set whose entities will be grouped per entity of `self`\n        :param attributes: attributes of self to include in the result\n        :param keep_all_rows: True = preserve the number of elements in the result (equivalent of LEFT JOIN in SQL)\n        :param named_attributes: renamings and computations on attributes of self and group\n        :return: an entity set representing the result of the aggregation/projection operator of entities from `group`\n        per entity of `self`\n        \"\"\"\n", "input": "", "output": "        return GroupBy.create(self, group, keep_all_rows=keep_all_rows,\n                              attributes=attributes, named_attributes=named_attributes)", "category": "Python"}, {"instruction": "def load_configs(envvar_prefix, path=None):\n    '''Load configuration\n\n    The following steps will be undertake:\n        * It will attempt to load configs from file:\n          if `path` is provided, it will be used, otherwise the path\n          will be taken from envvar `envvar_prefix` + \"SETTINGS\".\n        * all envvars starting with `envvar_prefix` will be loaded.\n\n    '''\n", "input": "", "output": "    conf = {}\n    if path:\n        conf.update(from_file(path))\n    else:\n        conf.update(from_envvar_file(envvar_prefix + \"SETTINGS\"))\n    conf.update(from_envvars(prefix=envvar_prefix))\n    return conf", "category": "Python"}, {"instruction": "def POST_evaluate(self) -> None:\n        \"\"\"Evaluate any valid Python expression with the *HydPy* server\n        process and get its result.\n\n        Method |HydPyServer.POST_evaluate| serves to test and debug, primarily.\n        The main documentation on module |servertools| explains its usage.\n        \"\"\"\n", "input": "", "output": "        for name, value in self._inputs.items():\n            result = eval(value)\n            self._outputs[name] = objecttools.flatten_repr(result)", "category": "Python"}, {"instruction": "def remover(self, id_tipo_acesso, id_equipamento):\n        \"\"\"Removes relationship between equipment and access type.\n\n        :param id_equipamento: Equipment identifier.\n        :param id_tipo_acesso: Access type identifier.\n\n        :return: None\n\n        :raise EquipamentoNaoExisteError: Equipment doesn't exist.\n        :raise EquipamentoAcessoNaoExisteError: Relationship between equipment and access type doesn't exist.\n        :raise InvalidParameterError: Equipment and/or access type id is/are invalid.\n        :raise DataBaseError: Networkapi failed to access the database.\n        :raise XMLError: Networkapi failed to generate the XML response.\n        \"\"\"\n", "input": "", "output": "\n        if not is_valid_int_param(id_tipo_acesso):\n            raise InvalidParameterError(u'Access type id is invalid.')\n\n        if not is_valid_int_param(id_equipamento):\n            raise InvalidParameterError(u'Equipment id is invalid.')\n\n        url = 'equipamentoacesso/' + \\\n            str(id_equipamento) + '/' + str(id_tipo_acesso) + '/'\n\n        code, xml = self.submit(None, 'DELETE', url)\n\n        return self.response(code, xml)", "category": "Python"}, {"instruction": "def get_rms_anonymous(self, struct1, struct2):\n        \"\"\"\n        Performs an anonymous fitting, which allows distinct species in one\n        structure to map to another. E.g., to compare if the Li2O and Na2O\n        structures are similar.\n\n        Args:\n            struct1 (Structure): 1st structure\n            struct2 (Structure): 2nd structure\n\n        Returns:\n            (min_rms, min_mapping)\n            min_rms is the minimum rms distance, and min_mapping is the\n            corresponding minimal species mapping that would map\n            struct1 to struct2. (None, None) is returned if the minimax_rms\n            exceeds the threshold.\n        \"\"\"\n", "input": "", "output": "        struct1, struct2 = self._process_species([struct1, struct2])\n        struct1, struct2, fu, s1_supercell = self._preprocess(struct1, struct2)\n\n        matches = self._anonymous_match(struct1, struct2, fu, s1_supercell,\n                                        use_rms=True, break_on_match=False)\n        if matches:\n            best = sorted(matches, key=lambda x: x[1][0])[0]\n            return best[1][0], best[0]\n        else:\n            return None, None", "category": "Python"}, {"instruction": "def keystroke_toggled(self, settings, key, user_data):\n        \"\"\"If the gconf var scroll_keystroke be changed, this method\n        will be called and will set the scroll_on_keystroke in all\n        terminals open.\n        \"\"\"\n", "input": "", "output": "        for i in self.guake.notebook_manager.iter_terminals():\n            i.set_scroll_on_keystroke(settings.get_boolean(key))", "category": "Python"}, {"instruction": "def _getattr_ (self, name):\n        \"\"\"Internal method.  Used by __getattr__() and __getitem__().\"\"\"\n", "input": "", "output": "        value = self._config.get(name)\n\n        if type(value) is dict:\n            value = AitConfig(self._filename, config=value)\n\n        return value", "category": "Python"}, {"instruction": "def generate_session_key(hmac_secret=b''):\n    \"\"\"\n    :param hmac_secret: optional HMAC\n    :type hmac_secret: :class:`bytes`\n    :return: (session_key, encrypted_session_key) tuple\n    :rtype: :class:`tuple`\n    \"\"\"\n", "input": "", "output": "    session_key = random_bytes(32)\n    encrypted_session_key = PKCS1_OAEP.new(UniverseKey.Public, SHA1)\\\n                                      .encrypt(session_key + hmac_secret)\n\n    return (session_key, encrypted_session_key)", "category": "Python"}, {"instruction": "def parent_id(self, value):\n        \"\"\"The parent_id property.\n        \n        Args:\n            value (string). the property value.\n        \"\"\"\n", "input": "", "output": "        if value == self._defaults['ai.operation.parentId'] and 'ai.operation.parentId' in self._values:\n            del self._values['ai.operation.parentId']\n        else:\n            self._values['ai.operation.parentId'] = value", "category": "Python"}, {"instruction": "def all_input(self):\n        \"\"\"\n        Returns all input files as a dict of {filename: vasp object}\n\n        Returns:\n            dict of {filename: object}, e.g., {'INCAR': Incar object, ...}\n        \"\"\"\n", "input": "", "output": "        return {'INCAR': self.incar,\n                'KPOINTS': self.kpoints,\n                'POSCAR': self.poscar,\n                'POTCAR': self.potcar}", "category": "Python"}, {"instruction": "def update_placeholder_formats(self, format_string, placeholder_formats):\n        \"\"\"\n        Update a format string adding formats if they are not already present.\n        \"\"\"\n", "input": "", "output": "        # Tokenize the format string and process them\n        output = []\n        for token in self.tokens(format_string):\n            if (\n                token.group(\"placeholder\")\n                and (not token.group(\"format\"))\n                and token.group(\"key\") in placeholder_formats\n            ):\n                output.append(\n                    \"{%s%s}\"\n                    % (token.group(\"key\"), placeholder_formats[token.group(\"key\")])\n                )\n                continue\n            value = token.group(0)\n            output.append(value)\n        return u\"\".join(output)", "category": "Python"}, {"instruction": "def render(self, rect, data):\n        \"\"\"Draws the columns.\"\"\"\n", "input": "", "output": "        num_elements = len(self.elements)\n        col_width = (rect.w-self.margin*(num_elements-1)) / float(num_elements)\n        x = rect.x\n        for element in self.elements:\n            if element is not None:\n                element.render(datatypes.Rectangle(\n                        x, rect.y, col_width, rect.h\n                        ), data)\n            x += col_width + self.margin", "category": "Python"}, {"instruction": "def final_mass_from_f0_tau(f0, tau, l=2, m=2):\n    \"\"\"Returns the final mass (in solar masses) based on the given frequency\n    and damping time.\n\n    .. note::\n        Currently, only l = m = 2 is supported. Any other indices will raise\n        a ``KeyError``.\n\n    Parameters\n    ----------\n    f0 : float or array\n        Frequency of the QNM (in Hz).\n    tau : float or array\n        Damping time of the QNM (in seconds).\n    l : int, optional\n        l-index of the harmonic. Default is 2.\n    m : int, optional\n        m-index of the harmonic. Default is 2.\n\n    Returns\n    -------\n    float or array\n        The mass of the final black hole. If the combination of frequency\n        and damping times give an unphysical result, ``numpy.nan`` will be\n        returned.\n    \"\"\"\n", "input": "", "output": "    # from Berti et al. 2006\n    spin = final_spin_from_f0_tau(f0, tau, l=l, m=m)\n    a, b, c = _berti_mass_constants[l,m]\n    return (a + b*(1-spin)**c)/(2*numpy.pi*f0*lal.MTSUN_SI)", "category": "Python"}, {"instruction": "def get_last_date(self, field, filters_=[]):\n        '''\n            :field: field with the data\n            :filters_: additional filters to find the date\n        '''\n", "input": "", "output": "\n        last_date = self.get_last_item_field(field, filters_=filters_)\n\n        return last_date", "category": "Python"}, {"instruction": "def parse(self, message, schema):\n        \"\"\"Parse message according to schema.\n\n        `message` should already be validated against the given schema.\n        See :ref:`schemadef` for more information.\n\n        Args:\n            message (dict): message data to parse.\n            schema (str): valid message schema.\n        Returns:\n            (dict): parsed message\n        \"\"\"\n", "input": "", "output": "        func = {\n            'audit-log': self._parse_audit_log_msg,\n            'event': self._parse_event_msg,\n        }[schema]\n        return func(message)", "category": "Python"}, {"instruction": "def has_value(cls, value: int) -> bool:\n        \"\"\"True if specified value exists in int enum; otherwise, False.\"\"\"\n", "input": "", "output": "        return any(value == item.value for item in cls)", "category": "Python"}, {"instruction": "def addtobase(subject, base_suffix):\n  \"\"\"\n  Adds the string *base_suffix* to the basename of *subject*.\n  \"\"\"\n", "input": "", "output": "\n  if not base_suffix:\n    return subject\n  base, ext = os.path.splitext(subject)\n  return base + base_suffix + ext", "category": "Python"}, {"instruction": "def register_as(self, klass, name, callback):\n        \"\"\"\n        Register a class with a function.\n\n        :param klass: The class\n        :type klass: class\n\n        :param callback: The callable\n        :type callback: callable\n\n        :param name: The short name\n        :type name: str\n        \"\"\"\n", "input": "", "output": "        return self.register(klass, callback, name)", "category": "Python"}, {"instruction": "def normaliseURL(url):\n    \"\"\"Normalising\n    - strips and leading or trailing whitespace,\n    - replaces HTML entities and character references,\n    - removes any leading empty segments to avoid breaking urllib2.\n    \"\"\"\n", "input": "", "output": "    url = unicode_safe(url).strip()\n    # XXX: brutal hack\n    url = unescape(url)\n\n    pu = list(urlparse(url))\n    segments = pu[2].split('/')\n    while segments and segments[0] in ('', '..'):\n        del segments[0]\n    pu[2] = '/' + '/'.join(segments)\n    # remove leading '&' from query\n    if pu[4].startswith('&'):\n        pu[4] = pu[4][1:]\n    # remove anchor\n    pu[5] = \"\"\n    return urlunparse(pu)", "category": "Python"}, {"instruction": "def encode(self, tags, encoding, values_to_sub):\n        \"\"\"\n        reads the encoding type from the event-mapping.json\n        and determines whether a value needs encoding\n\n        Parameters\n        ----------\n        tags: dict\n            the values of a particular event that can be substituted\n            within the event json\n        encoding: string\n            string that helps navigate to the encoding field of the json\n        values_to_sub: dict\n            key/value pairs that will be substituted into the json\n        Returns\n        -------\n        values_to_sub: dict\n            the encoded (if need be) values to substitute into the json.\n        \"\"\"\n", "input": "", "output": "\n        for tag in tags:\n            if tags[tag].get(encoding) != \"None\":\n                if tags[tag].get(encoding) == \"url\":\n                    values_to_sub[tag] = self.url_encode(values_to_sub[tag])\n                if tags[tag].get(encoding) == \"base64\":\n                    values_to_sub[tag] = self.base64_utf_encode(values_to_sub[tag])\n        return values_to_sub", "category": "Python"}, {"instruction": "def mksls(src, dst=None):\n    '''\n    Convert an AutoYAST file to an SLS file\n    '''\n", "input": "", "output": "    with salt.utils.files.fopen(src, 'r') as fh_:\n        ps_opts = xml.to_dict(ET.fromstring(fh_.read()))\n\n    if dst is not None:\n        with salt.utils.files.fopen(dst, 'w') as fh_:\n            salt.utils.yaml.safe_dump(ps_opts, fh_, default_flow_style=False)\n    else:\n        return salt.utils.yaml.safe_dump(ps_opts, default_flow_style=False)", "category": "Python"}, {"instruction": "def get_block_count(self, is_full: bool = False) -> int or dict:\n        \"\"\"\n        This interface is used to get the decimal block number in current network.\n\n        Return:\n            the decimal total number of blocks in current network.\n        \"\"\"\n", "input": "", "output": "        payload = self.generate_json_rpc_payload(RpcMethod.GET_BLOCK_COUNT)\n        response = self.__post(self.__url, payload)\n        if is_full:\n            return response\n        return response['result']", "category": "Python"}, {"instruction": "def current_portfolio_weights(self):\n        \"\"\"\n        Compute each asset's weight in the portfolio by calculating its held\n        value divided by the total value of all positions.\n\n        Each equity's value is its price times the number of shares held. Each\n        futures contract's value is its unit price times number of shares held\n        times the multiplier.\n        \"\"\"\n", "input": "", "output": "        position_values = pd.Series({\n            asset: (\n                    position.last_sale_price *\n                    position.amount *\n                    asset.price_multiplier\n            )\n            for asset, position in self.positions.items()\n        })\n        return position_values / self.portfolio_value", "category": "Python"}, {"instruction": "def _get_single_depth(self, multi_index):\n        '''\n        Helper method for determining how many single index entries there\n        are in a particular multi-index\n        '''\n", "input": "", "output": "        single_depth = 0\n        for subind in multi_index:\n            if is_slice_or_dim_range(subind):\n                break\n            single_depth += 1\n        return single_depth", "category": "Python"}, {"instruction": "def prepare_system_options(cfg, defaults=None):\n    \"\"\"\n    Retrieve and delete (pop) system options from input configuration.\n    \"\"\"\n", "input": "", "output": "    d = {} if defaults is None else defaults.copy()\n    if 'nonbondedMethod' in cfg:\n        d['nonbondedMethod'] = warned_getattr(openmm_app, cfg.pop('nonbondedMethod'), None)\n    if 'nonbondedCutoff' in cfg:\n        d['nonbondedCutoff'] = cfg.pop('nonbondedCutoff') * u.nanometers\n    if 'constraints' in cfg:\n        d['constraints'] = warned_getattr(openmm_app, cfg.pop('constraints'), None)\n    for key in ['rigidWater', 'ewaldErrorTolerance']:\n        if key in cfg:\n            d[key] = cfg.pop(key)\n    if 'extra_system_options' in cfg:\n        if 'implicitSolvent' in cfg['extra_system_options']:\n            implicit_solvent = warned_getattr(\n                openmm_app, cfg['extra_system_options']['implicitSolvent'], None)\n            cfg['extra_system_options']['implicitSolvent'] = implicit_solvent\n        d.update(cfg.pop('extra_system_options'))\n    return d", "category": "Python"}, {"instruction": "def retrieve(customer_id):\n        \"\"\"\n        Retrieve a customer from its id.\n\n        :param customer_id: The customer id\n        :type customer_id: string\n\n        :return: The customer resource\n        :rtype: resources.Customer\n        \"\"\"\n", "input": "", "output": "        http_client = HttpClient()\n        response, __ = http_client.get(routes.url(routes.CUSTOMER_RESOURCE, resource_id=customer_id))\n        return resources.Customer(**response)", "category": "Python"}, {"instruction": "def params(self):\n        \"\"\"\n        :return: A dictionary of SSOS query parameters.\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        params = dict(format=RESPONSE_FORMAT,\n                      verbose=self.verbose,\n                      epoch1=str(self.search_start_date),\n                      epoch2=str(self.search_end_date),\n                      search=self.orbit_method,\n                      eunits=self.error_units,\n                      eellipse=self.error_ellipse,\n                      extres=self.resolve_extension,\n                      xyres=self.resolve_position,\n                      telinst=self.telescope_instrument)\n\n        if self.orbit_method == 'bynameHorizons':\n            params['object'] = NEW_LINE.join((str(target_name) for target_name in self.observations))\n        else:\n            params['obs'] = NEW_LINE.join((str(observation) for observation in self.observations))\n        return params", "category": "Python"}, {"instruction": "def _is_string(cls, arg):\n        \"\"\"\n        Return True if arg is a string value,\n        and False if arg is a logical value (ANY or NA).\n\n        :param string arg: string to check\n        :returns: True if value is a string, False if it is a logical value.\n        :rtype: boolean\n\n        This function is a support function for _compare().\n        \"\"\"\n", "input": "", "output": "\n        isAny = arg == CPEComponent2_3_WFN.VALUE_ANY\n        isNa = arg == CPEComponent2_3_WFN.VALUE_NA\n\n        return not (isAny or isNa)", "category": "Python"}, {"instruction": "def codes_get_string_array(handle, key, size, length=None):\n    # type: (cffi.FFI.CData, bytes, int, int) -> T.List[bytes]\n    \"\"\"\n    Get string array values from a key.\n\n    :param bytes key: the keyword whose value(s) are to be extracted\n\n    :rtype: T.List[bytes]\n    \"\"\"\n", "input": "", "output": "    if length is None:\n        length = codes_get_string_length(handle, key)\n    values_keepalive = [ffi.new('char[]', length) for _ in range(size)]\n    values = ffi.new('char*[]', values_keepalive)\n    size_p = ffi.new('size_t *', size)\n    _codes_get_string_array(handle, key.encode(ENC), values, size_p)\n    return [ffi.string(values[i]).decode(ENC) for i in range(size_p[0])]", "category": "Python"}, {"instruction": "def jx_expression_to_function(expr):\n    \"\"\"\n    RETURN FUNCTION THAT REQUIRES PARAMETERS (row, rownum=None, rows=None):\n    \"\"\"\n", "input": "", "output": "    if is_expression(expr):\n        if is_op(expr, ScriptOp) and not is_text(expr.script):\n            return expr.script\n        else:\n            return compile_expression(Python[expr].to_python())\n    if (\n        expr != None\n        and not is_data(expr)\n        and not is_list(expr)\n        and hasattr(expr, \"__call__\")\n    ):\n        return expr\n    return compile_expression(Python[jx_expression(expr)].to_python())", "category": "Python"}, {"instruction": "def logical_chassis_fwdl_sanity_input_host(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        logical_chassis_fwdl_sanity = ET.Element(\"logical_chassis_fwdl_sanity\")\n        config = logical_chassis_fwdl_sanity\n        input = ET.SubElement(logical_chassis_fwdl_sanity, \"input\")\n        host = ET.SubElement(input, \"host\")\n        host.text = kwargs.pop('host')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def throttle( self, wait=True ):\n        \"\"\"\n        If the wait parameter is True, this method returns True after suspending the current\n        thread as necessary to ensure that no less than the configured minimum interval passed\n        since the most recent time an invocation of this method returned True in any thread.\n\n        If the wait parameter is False, this method immediatly returns True if at least the\n        configured minimum interval has passed since the most recent time this method returned\n        True in any thread, or False otherwise.\n        \"\"\"\n", "input": "", "output": "        # I think there is a race in Thread.start(), hence the lock\n        with self.thread_start_lock:\n            if not self.thread_started:\n                self.thread.start( )\n                self.thread_started = True\n        return self.semaphore.acquire( blocking=wait )", "category": "Python"}, {"instruction": "def foreignKeys(self, *a, **kw):  # nopep8\n        \"\"\"Executes the SQLForeignKeys function and creates a result set\n        of column names that are foreign keys in the specified table (columns\n        in the specified table that refer to primary keys in other tables)\n        or foreign keys in other tables that refer to the primary key in\n        the specified table.\n        \"\"\"\n", "input": "", "output": "        fut = self._run_operation(self._impl.foreignKeys, *a, **kw)\n        return fut", "category": "Python"}, {"instruction": "def cummin(expr, sort=None, ascending=True, unique=False,\n           preceding=None, following=None):\n    \"\"\"\n    Calculate cumulative minimum of a sequence expression.\n\n    :param expr: expression for calculation\n    :param sort: name of the sort column\n    :param ascending: whether to sort in ascending order\n    :param unique: whether to eliminate duplicate entries\n    :param preceding: the start point of a window\n    :param following: the end point of a window\n    :return: calculated column\n    \"\"\"\n", "input": "", "output": "    return _cumulative_op(expr, CumMin, sort=sort, ascending=ascending,\n                          unique=unique, preceding=preceding,\n                          following=following)", "category": "Python"}, {"instruction": "def set_group_conditions(self, group_id, conditions, trigger_mode=None):\n        \"\"\"\n        Set the group conditions.\n\n        This replaces any existing conditions on the group and member conditions for all trigger modes.\n\n        :param group_id: Group to be updated\n        :param conditions: New conditions to replace old ones\n        :param trigger_mode: Optional TriggerMode used\n        :type conditions: GroupConditionsInfo\n        :type trigger_mode: TriggerMode\n        :return: The new Group conditions\n        \"\"\"\n", "input": "", "output": "        data = self._serialize_object(conditions)\n\n        if trigger_mode is not None:\n            url = self._service_url(['triggers', 'groups', group_id, 'conditions', trigger_mode])\n        else:\n            url = self._service_url(['triggers', 'groups', group_id, 'conditions'])\n\n        response = self._put(url, data)\n        return Condition.list_to_object_list(response)", "category": "Python"}, {"instruction": "def create_channel(self, channel):\n        \"\"\"\n        Method to create a channel.\n        :param channel: List containing channel's desired to be created on database.\n        :return: Id.\n        \"\"\"\n", "input": "", "output": "\n        data = {'channels': channel}\n        return super(ApiInterfaceRequest, self).post('api/v3/channel/', data)", "category": "Python"}, {"instruction": "def append(self, page, content, **options):\n        \"\"\"Appends *content* text to *page*.\n\n        Valid *options* are:\n\n            * *sum*: (str) change summary\n            * *minor*: (bool) whether this is a minor change\n        \"\"\"\n", "input": "", "output": "        return self._dokuwiki.send('dokuwiki.appendPage', page, content, options)", "category": "Python"}, {"instruction": "def get_listener_count(self):\n        \"\"\"Returns the number of listeners on the network.\"\"\"\n", "input": "", "output": "\n        if hasattr(self, \"listener_count\"):\n            return self.listener_count\n        else:\n            self.listener_count = _number(\n                _extract(self._request(self.ws_prefix + \".getInfo\", True), \"listeners\")\n            )\n            return self.listener_count", "category": "Python"}, {"instruction": "def cmd_namespace(self):\n        \"\"\"\n            A read-only property that gives the namespace of the system for evaluating commands.\n        \"\"\"\n", "input": "", "output": "        import automate\n        ns = dict(list(automate.__dict__.items()) + list(self.namespace.items()))\n        return ns", "category": "Python"}, {"instruction": "def make_dynamic_prompt(self, prompt):\n        \"\"\"Extend prompt with flexible mode handling regexp.\"\"\"\n", "input": "", "output": "        if prompt:\n            self.prompt_re = self.driver.make_dynamic_prompt(prompt)", "category": "Python"}, {"instruction": "def create_subscriptions(config, profile_name):\n    ''' Adds supported subscriptions '''\n", "input": "", "output": "    if 'kinesis' in config.subscription.keys():\n        data = config.subscription['kinesis']\n        function_name = config.name\n        stream_name = data['stream']\n        batch_size = data['batch_size']\n        starting_position = data['starting_position']\n        starting_position_ts = None\n        if starting_position == 'AT_TIMESTAMP':\n            ts = data.get('starting_position_timestamp')\n            starting_position_ts = datetime.strptime(ts, '%Y-%m-%dT%H:%M:%SZ')\n        s = KinesisSubscriber(config, profile_name,\n                              function_name, stream_name, batch_size,\n                              starting_position,\n                              starting_position_ts=starting_position_ts)\n        s.subscribe()", "category": "Python"}, {"instruction": "def undo(self):\n        \"\"\"\n        Restore to last version\n        \"\"\"\n", "input": "", "output": "        log = getLogger('ocrd.workspace_backup.undo')\n        backups = self.list()\n        if backups:\n            last_backup = backups[0]\n            self.restore(last_backup.chksum, choose_first=True)\n        else:\n            log.info(\"No backups, nothing to undo.\")", "category": "Python"}, {"instruction": "def pop(queue, quantity=1, backend='sqlite', is_runner=False):\n    '''\n    Pop one or more or all items from a queue\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-run queue.pop myqueue\n        salt-run queue.pop myqueue 6\n        salt-run queue.pop myqueue all\n        salt-run queue.pop myqueue 6 backend=sqlite\n        salt-run queue.pop myqueue all backend=sqlite\n    '''\n", "input": "", "output": "    queue_funcs = salt.loader.queues(__opts__)\n    cmd = '{0}.pop'.format(backend)\n    if cmd not in queue_funcs:\n        raise SaltInvocationError('Function \"{0}\" is not available'.format(cmd))\n    ret = queue_funcs[cmd](quantity=quantity, queue=queue, is_runner=is_runner)\n    return ret", "category": "Python"}, {"instruction": "def resolve_relative_rst_links(text: str, base_link: str):\n    \"\"\"Resolve all relative links in a given RST document.\n\n    All links of form `link`_ become `link <base_link/link>`_.\n    \"\"\"\n", "input": "", "output": "    document = parse_rst(text)\n    visitor = SimpleRefCounter(document)\n    document.walk(visitor)\n    for target in visitor.references:\n        name = target.attributes['name']\n        uri = target.attributes['refuri']\n        new_link = '`{} <{}{}>`_'.format(name, base_link, uri)\n        if name == uri:\n            text = text.replace('`<{}>`_'.format(uri), new_link)\n        else:\n            text = text.replace('`{} <{}>`_'.format(name, uri), new_link)\n    return text", "category": "Python"}, {"instruction": "def get_fields(model, include=None):\n    \"\"\"\n    Returns ordered dict in format 'field': 'verbose_name'\n    \"\"\"\n", "input": "", "output": "    fields = OrderedDict()\n    info = model._meta\n    if include:\n        selected = [info.get_field(name) for name in include]\n    else:\n        selected = [field for field in info.fields if field.editable]\n    for field in selected:\n        fields[field.name] = field.verbose_name\n    return fields", "category": "Python"}, {"instruction": "def EnablePlugins(self, plugin_includes):\n    \"\"\"Enables parser plugins.\n\n    Args:\n      plugin_includes (list[str]): names of the plugins to enable, where None\n          or an empty list represents all plugins. Note the default plugin, if\n          it exists, is always enabled and cannot be disabled.\n    \"\"\"\n", "input": "", "output": "    self._plugins = []\n    if not self._plugin_classes:\n      return\n\n    default_plugin_name = '{0:s}_default'.format(self.NAME)\n    for plugin_name, plugin_class in iter(self._plugin_classes.items()):\n      if plugin_name == default_plugin_name:\n        self._default_plugin = plugin_class()\n        continue\n\n      if plugin_includes and plugin_name not in plugin_includes:\n        continue\n\n      plugin_object = plugin_class()\n      self._plugins.append(plugin_object)", "category": "Python"}, {"instruction": "def blockquote(\n            self,\n            text):\n        \"\"\"*convert plain-text to MMD blockquote*\n\n        **Key Arguments:**\n            - ``text`` -- the text to convert to MMD blockquote\n\n        **Return:**\n            - ``blockquote`` -- the MMD blockquote\n\n        **Usage:**\n\n            To convert a text to a MMD blockquote:\n\n            .. code-block:: python\n\n                text = md.quote(\" This is my quote  \")\n                print text\n\n                # OUTPUT:\n                # >  This is my quote\n                #\n        \"\"\"\n", "input": "", "output": "        m = self.reWS.match(text)\n        return \"\\n> \" + (\"\\n> \").join(m.group(2).split(\"\\n\")) + \"\\n\\n\"", "category": "Python"}, {"instruction": "def truncate(self, max_length):\n        \"\"\"Truncate this vector so it's length does not exceed max.\"\"\"\n", "input": "", "output": "\n        if self.length() > max_length:\n\n            # If it's longer than the max_length, scale to the max_length.\n            self.scale(max_length / self.length())", "category": "Python"}, {"instruction": "def remove_external_references_from_srl_layer(self):\n        \"\"\"\n        Removes all external references present in the term layer\n        \"\"\"\n", "input": "", "output": "        if self.srl_layer is not None:\n            for pred in self.srl_layer.get_predicates():\n                pred.remove_external_references()\n                pred.remove_external_references_from_roles()", "category": "Python"}, {"instruction": "def get_rate(self, currency, date):\n        \"\"\"Get the exchange rate for ``currency`` against ``_INTERNAL_CURRENCY``\n\n        If implementing your own backend, you should probably override :meth:`_get_rate()`\n        rather than this.\n        \"\"\"\n", "input": "", "output": "        if str(currency) == defaults.INTERNAL_CURRENCY:\n            return Decimal(1)\n\n        cached = cache.get(_cache_key(currency, date))\n        if cached:\n            return Decimal(cached)\n        else:\n            # Expect self._get_rate() to implement caching\n            return Decimal(self._get_rate(currency, date))", "category": "Python"}, {"instruction": "def cmd_speech(self, args):\n        '''speech commands'''\n", "input": "", "output": "        usage = \"usage: speech <say>\"\n        if len(args) < 1:\n            print(usage)\n            return\n\n        if args[0] == \"say\":\n            if len(args) < 2:\n                print(\"usage: speech say <text to say>\")\n                return\n            self.say(\" \".join(args[1::]))\n        if args[0] == \"list_voices\":\n            self.list_voices()", "category": "Python"}, {"instruction": "def learn(self, features, labels):\n        \"\"\" Fits the classifier\n\n        If it's state is empty, the classifier is fitted, if not\n        the classifier is partially fitted.\n        See sklearn's SGDClassifier fit and partial_fit methods.\n\n        Args:\n            features (:obj:`list` of :obj:`list` of :obj:`float`)\n            labels (:obj:`list` of :obj:`str`): Labels for each set of features.\n                New features are learnt.\n        \"\"\"\n", "input": "", "output": "        labels = np.ravel(labels)\n        self.__learn_labels(labels)\n        if len(labels) == 0:\n            return\n\n        labels = self.labels.transform(labels)\n        if self.feature_length > 0 and hasattr(self.clf, 'partial_fit'):\n            # FIXME? check docs, may need to pass class=[...]\n            self.clf = self.clf.partial_fit(features, labels)\n        else:\n            self.clf = self.clf.fit(features, labels)\n            self.feature_length = len(features[0])", "category": "Python"}, {"instruction": "def _to_power_basis33(nodes1, nodes2):\n    r\"\"\"Compute the coefficients of an **intersection polynomial**.\n\n    Helper for :func:`to_power_basis` in the case that each curve is\n    degree three. In this case, B |eacute| zout's `theorem`_ tells us\n    that the **intersection polynomial** is degree :math:`3 \\cdot 3`\n    hence we return ten coefficients.\n\n    .. note::\n\n       This uses a least-squares fit to the function evaluated at the\n       Chebyshev nodes (scaled and shifted onto ``[0, 1]``). Hence, the\n       coefficients may be less stable than those produced for smaller\n       degrees.\n\n    Args:\n        nodes1 (numpy.ndarray): The nodes in the first curve.\n        nodes2 (numpy.ndarray): The nodes in the second curve.\n\n    Returns:\n        numpy.ndarray: ``10``-array of coefficients.\n    \"\"\"\n", "input": "", "output": "    evaluated = [\n        eval_intersection_polynomial(nodes1, nodes2, t_val)\n        for t_val in _CHEB10\n    ]\n    return polynomial.polyfit(_CHEB10, evaluated, 9)", "category": "Python"}, {"instruction": "def note_list(self, body_matches=None, post_id=None, post_tags_match=None,\n                  creator_name=None, creator_id=None, is_active=None):\n        \"\"\"Return list of notes.\n\n        Parameters:\n            body_matches (str): The note's body matches the given terms.\n            post_id (int): A specific post.\n            post_tags_match (str): The note's post's tags match the given terms.\n            creator_name (str): The creator's name. Exact match.\n            creator_id (int): The creator's user id.\n            is_active (bool): Can be: True, False.\n        \"\"\"\n", "input": "", "output": "        params = {\n            'search[body_matches]': body_matches,\n            'search[post_id]': post_id,\n            'search[post_tags_match]': post_tags_match,\n            'search[creator_name]': creator_name,\n            'search[creator_id]': creator_id,\n            'search[is_active]': is_active\n            }\n        return self._get('notes.json', params)", "category": "Python"}, {"instruction": "def get_fout_base(self, goid, name=None, pre=\"gogrp\"):\n        \"\"\"Get filename for a group of GO IDs under a single header GO ID.\"\"\"\n", "input": "", "output": "        goobj = self.gosubdag.go2obj[goid]\n        if name is None:\n            name = self.grpname.replace(\" \", \"_\")\n        sections = \"_\".join(self.hdrobj.get_sections(goid))\n        return \"{PRE}_{BP}_{NAME}_{SEC}_{DSTR}_{D1s}_{GO}\".format(\n            PRE=pre,\n            BP=Consts.NAMESPACE2NS[goobj.namespace],\n            NAME=self._str_replace(name),\n            SEC=self._str_replace(self._str_replace(sections)),\n            GO=goid.replace(\":\", \"\"),\n            DSTR=self._get_depthsr(goobj),\n            D1s=self.gosubdag.go2nt[goobj.id].D1)", "category": "Python"}, {"instruction": "def dist_factory(path_item, entry, only):\n    \"\"\"\n    Return a dist_factory for a path_item and entry\n    \"\"\"\n", "input": "", "output": "    lower = entry.lower()\n    is_meta = any(map(lower.endswith, ('.egg-info', '.dist-info')))\n    return (\n        distributions_from_metadata\n        if is_meta else\n        find_distributions\n        if not only and _is_egg_path(entry) else\n        resolve_egg_link\n        if not only and lower.endswith('.egg-link') else\n        NoDists()\n    )", "category": "Python"}, {"instruction": "def ensure_directory(path):\n    \"\"\"\n    Ensure directory exists for a given file path.\n    \"\"\"\n", "input": "", "output": "    dirname = os.path.dirname(path)\n    if not os.path.exists(dirname):\n        os.makedirs(dirname)", "category": "Python"}, {"instruction": "def _closest_location(self, location_map, target):\n        \"\"\"Return path of font whose location is closest to target.\"\"\"\n", "input": "", "output": "\n        def dist(a, b):\n            return math.sqrt(sum((a[k] - b[k]) ** 2 for k in a.keys()))\n\n        paths = iter(location_map.keys())\n        closest = next(paths)\n        closest_dist = dist(target, location_map[closest])\n        for path in paths:\n            cur_dist = dist(target, location_map[path])\n            if cur_dist < closest_dist:\n                closest = path\n                closest_dist = cur_dist\n        return closest", "category": "Python"}, {"instruction": "def repair_url(url):\n    \"\"\"\n    Fixes URL.\n    @param url: url to repair.\n    @param out: instance of StandardOutput as defined in this lib.\n    @return: Newline characters are stripped from the URL string.\n        If the url string parameter does not start with http, it prepends http://\n        If the url string parameter does not end with a slash, appends a slash.\n        If the url contains a query string, it gets removed.\n    \"\"\"\n", "input": "", "output": "    url = url.strip('\\n')\n    if not re.match(r\"^http\", url):\n        url = \"http://\" + url\n\n    if \"?\" in url:\n        url, _ = url.split('?')\n\n    if not url.endswith(\"/\"):\n        return url + \"/\"\n    else :\n        return url", "category": "Python"}, {"instruction": "def clear(ctx, schema):\n    \"\"\"Clears an entire database collection irrevocably. Use with caution!\"\"\"\n", "input": "", "output": "\n    response = _ask('Are you sure you want to delete the collection \"%s\"' % (\n        schema), default='N', data_type='bool')\n    if response is True:\n        host, port = ctx.obj['dbhost'].split(':')\n\n        client = pymongo.MongoClient(host=host, port=int(port))\n        database = client[ctx.obj['dbname']]\n\n        log(\"Clearing collection for\", schema, lvl=warn,\n            emitter='MANAGE')\n        result = database.drop_collection(schema)\n        if not result['ok']:\n            log(\"Could not drop collection:\", lvl=error)\n            log(result, pretty=True, lvl=error)\n        else:\n            log(\"Done\")", "category": "Python"}, {"instruction": "def delete_connection(self, **kwargs):\n        \"\"\"Remove a single connection to a provider for the specified user.\"\"\"\n", "input": "", "output": "        conn = self.find_connection(**kwargs)\n        if not conn:\n            return False\n        self.delete(conn)\n        return True", "category": "Python"}, {"instruction": "def payment_methods(self):\n\t\t\"\"\"\n\t\tAn iterable of all of the customer's payment methods (sources, then legacy cards)\n\t\t\"\"\"\n", "input": "", "output": "\t\tfor source in self.sources.iterator():\n\t\t\tyield source\n\n\t\tfor card in self.legacy_cards.iterator():\n\t\t\tyield card", "category": "Python"}, {"instruction": "def compress(self):\n        \"\"\"\n        Compress the new exported image,\n        Block size was taken from virt-builder page\n        \"\"\"\n", "input": "", "output": "        if not self.do_compress:\n            return\n        with LogTask('Compressing disk'):\n            utils.compress(self.dst, 16777216)\n            os.unlink(self.dst)", "category": "Python"}, {"instruction": "def run_in_executor(self, executor=None, func=None, *args):\n        \"\"\"If `kwargs` needed, try like this: func=lambda: foo(*args, **kwargs)\"\"\"\n", "input": "", "output": "        return self.loop.run_in_executor(executor, func, *args)", "category": "Python"}, {"instruction": "def get_klass_children(gi_name):\n    '''\n    Returns a dict of qualified symbols representing\n    the children of the klass-like symbol named gi_name\n    '''\n", "input": "", "output": "    res = {}\n    children = __HIERARCHY_GRAPH.successors(gi_name)\n    for gi_name in children:\n        ctype_name = ALL_GI_TYPES[gi_name]\n        qs = QualifiedSymbol(type_tokens=[Link(None, ctype_name, ctype_name)])\n        qs.add_extension_attribute ('gi-extension', 'type_desc',\n                SymbolTypeDesc([], gi_name, ctype_name, 0))\n        res[ctype_name] = qs\n    return res", "category": "Python"}, {"instruction": "def generate_password(mode, length):\n    \"\"\"generate a random password\"\"\"\n", "input": "", "output": "    # generate random password\n    r = random.SystemRandom()\n    length = length or RANDOM_PASSWORD_DEFAULT_LENGTH\n    password = \"\".join(r.choice(RANDOM_PASSWORD_ALPHABET) for _ in range(length))\n\n    # copy or echo generated password\n    if mode == Mode.ECHO:\n        click.echo(style_password(password))\n    elif mode == Mode.COPY:\n        try:\n            import pyperclip\n\n            pyperclip.copy(password)\n            result = style_success(\"*** PASSWORD COPIED TO CLIPBOARD ***\")\n        except ImportError:\n            result = style_error('*** PYTHON PACKAGE \"PYPERCLIP\" NOT FOUND ***')\n        click.echo(result)\n    elif mode == Mode.RAW:\n        click.echo(password)", "category": "Python"}, {"instruction": "def decorator(func):\n        \"\"\"A function timer decorator.\"\"\"\n", "input": "", "output": "\n        def function_timer(*args, **kwargs):\n            ", "category": "Python"}, {"instruction": "def area_orifice(Height, RatioVCOrifice, FlowRate):\n    \"\"\"Return the area of the orifice.\"\"\"\n", "input": "", "output": "    #Checking input validity\n    ut.check_range([Height, \">0\", \"Height\"], [FlowRate, \">0\", \"Flow rate\"],\n                   [RatioVCOrifice, \"0-1, >0\", \"VC orifice ratio\"])\n    return FlowRate / (RatioVCOrifice * np.sqrt(2 * gravity.magnitude * Height))", "category": "Python"}, {"instruction": "def split(s):\n    \"\"\"\n    Split a string into a list, respecting any quoted strings inside\n    Uses ``shelx.split`` which has a bad habbit of inserting null bytes where they are not wanted\n    \"\"\"\n", "input": "", "output": "    return map(lambda w: filter(lambda c: c != '\\x00', w), lexsplit(s))", "category": "Python"}, {"instruction": "def check_available_aac_encoders():\n    \"\"\"Returns the available AAC encoders\n\n    Returns\n    ----------\n    codecs : list(str)\n        List of available encoder codecs\n    \"\"\"\n", "input": "", "output": "\n    cmd = [\n        'ffmpeg',\n        '-v', 'error',\n        '-codecs'\n    ]\n\n    output = sp.check_output(cmd)\n    aac_codecs = [\n        x for x in\n        output.splitlines() if \"AAC (Advanced Audio Coding)\" in str(x)\n    ][0]\n    hay = aac_codecs.decode('ascii')\n    match = re.findall(r'\\(encoders: ([^\\)]*) \\)', hay)\n    if match:\n        return match[0].split(\" \")\n    else:\n        return None", "category": "Python"}, {"instruction": "async def get_image(\n        self,\n        input_source: str,\n        output_format: str = IMAGE_JPEG,\n        extra_cmd: Optional[str] = None,\n        timeout: int = 15,\n    ) -> Optional[bytes]:\n        \"\"\"Open FFmpeg process as capture 1 frame.\"\"\"\n", "input": "", "output": "        command = [\"-an\", \"-frames:v\", \"1\", \"-c:v\", output_format]\n\n        # open input for capture 1 frame\n        is_open = await self.open(\n            cmd=command,\n            input_source=input_source,\n            output=\"-f image2pipe -\",\n            extra_cmd=extra_cmd,\n        )\n\n        # error after open?\n        if not is_open:\n            _LOGGER.warning(\"Error starting FFmpeg.\")\n            return None\n\n        # read image\n        try:\n            proc_func = functools.partial(self._proc.communicate, timeout=timeout)\n            image, _ = await self._loop.run_in_executor(None, proc_func)\n            return image\n\n        except (subprocess.TimeoutExpired, ValueError):\n            _LOGGER.warning(\"Timeout reading image.\")\n            self.kill()\n            return None", "category": "Python"}, {"instruction": "def mock_server_receive(sock, length):\n    \"\"\"Receive `length` bytes from a socket object.\"\"\"\n", "input": "", "output": "    msg = b''\n    while length:\n        chunk = sock.recv(length)\n        if chunk == b'':\n            raise socket.error(errno.ECONNRESET, 'closed')\n\n        length -= len(chunk)\n        msg += chunk\n\n    return msg", "category": "Python"}, {"instruction": "def _get_page_title(self, page):\n        \"\"\"\n        Open the rst file `page` and extract its title.\n        \"\"\"\n", "input": "", "output": "        fname = os.path.join(SOURCE_PATH, '{}.rst'.format(page))\n        option_parser = docutils.frontend.OptionParser(\n            components=(docutils.parsers.rst.Parser,))\n        doc = docutils.utils.new_document(\n            '<doc>',\n            option_parser.get_default_values())\n        with open(fname) as f:\n            data = f.read()\n\n        parser = docutils.parsers.rst.Parser()\n        # do not generate any warning when parsing the rst\n        with open(os.devnull, 'a') as f:\n            doc.reporter.stream = f\n            parser.parse(data, doc)\n\n        section = next(node for node in doc.children\n                       if isinstance(node, docutils.nodes.section))\n        title = next(node for node in section.children\n                     if isinstance(node, docutils.nodes.title))\n\n        return title.astext()", "category": "Python"}, {"instruction": "def parameter_blocks(self):\n        '''Compute the size (in 512B blocks) of the parameter section.'''\n", "input": "", "output": "        bytes = 4. + sum(g.binary_size() for g in self.groups.values())\n        return int(np.ceil(bytes / 512))", "category": "Python"}, {"instruction": "def bounding_box(positions):\n    '''Computes the bounding box for a list of 3-dimensional points.\n    '''\n", "input": "", "output": "    (x0, y0, z0) = (x1, y1, z1) = positions[0]\n    for x, y, z in positions:\n        x0 = min(x0, x)\n        y0 = min(y0, y)\n        z0 = min(z0, z)\n        x1 = max(x1, x)\n        y1 = max(y1, y)\n        z1 = max(z1, z)\n    return (x0, y0, z0), (x1, y1, z1)", "category": "Python"}, {"instruction": "def reload(request):\n    \"\"\"Reload local requirements file.\"\"\"\n", "input": "", "output": "    refresh_packages.clean()\n    refresh_packages.local()\n    refresh_packages.remote()\n    url = request.META.get('HTTP_REFERER')\n    if url:\n        return HttpResponseRedirect(url)\n    else:\n        return HttpResponse('Local requirements list has been reloaded.')", "category": "Python"}, {"instruction": "def drange(v0, v1, d):\n    \"\"\"Returns a discrete range.\"\"\"\n", "input": "", "output": "    assert v0 < v1\n    return xrange(int(v0)//d, int(v1+d)//d)", "category": "Python"}, {"instruction": "def write_chunks(out, chunks):\n    \"\"\"Create a PNG file by writing out the chunks.\"\"\"\n", "input": "", "output": "\n    out.write(signature)\n    for chunk in chunks:\n        write_chunk(out, *chunk)", "category": "Python"}, {"instruction": "def Run(self, args):\n    \"\"\"Reads a buffer on the client and sends it to the server.\"\"\"\n", "input": "", "output": "    # Make sure we limit the size of our output\n    if args.length > constants.CLIENT_MAX_BUFFER_SIZE:\n      raise RuntimeError(\"Can not read buffers this large.\")\n\n    data = vfs.ReadVFS(args.pathspec, args.offset, args.length)\n\n    digest = hashlib.sha256(data).digest()\n\n    # Now report the hash of this blob to our flow as well as the offset and\n    # length.\n    self.SendReply(\n        rdf_client.BufferReference(\n            offset=args.offset, length=len(data), data=digest))", "category": "Python"}, {"instruction": "def _call_from_base_type(self, value):\n    \"\"\"Call all _from_base_type() methods on the value.\n\n    This calls the methods in the reverse method resolution order of\n    the property's class.\n    \"\"\"\n", "input": "", "output": "    methods = self._find_methods('_from_base_type', reverse=True)\n    call = self._apply_list(methods)\n    return call(value)", "category": "Python"}, {"instruction": "def get_psms(self):\n        \"\"\"Creates iterator to write to new tsv. Contains input tsv\n        lines plus quant data for these.\"\"\"\n", "input": "", "output": "        self.header, isob_header = prep.get_full_and_isobaric_headers(\n            self.oldheader, self.lookup, self.isobaric, self.precursor)\n        self.psms = prep.generate_psms_quanted(self.lookup, self.fn,\n                                               isob_header, self.oldheader,\n                                               self.isobaric, self.precursor)", "category": "Python"}, {"instruction": "def run_gvcftyper(vrn_files, out_file, region, data):\n    \"\"\"Produce joint called variants from input gVCF files.\n    \"\"\"\n", "input": "", "output": "    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            license = license_export(data)\n            ref_file = dd.get_ref_file(data)\n            input_files = \" \".join(vrn_files)\n            region = bamprep.region_to_gatk(region)\n            cmd = (\"{license}sentieon driver -r {ref_file} --interval {region} \"\n                   \"--algo GVCFtyper {tx_out_file} {input_files}\")\n            do.run(cmd.format(**locals()), \"Sentieon GVCFtyper\")\n    return out_file", "category": "Python"}, {"instruction": "def html(self):\n        \"\"\"\n        Build HTML documentation.\n        \"\"\"\n", "input": "", "output": "        ret_code = self._sphinx_build('html')\n        zip_fname = os.path.join(BUILD_PATH, 'html', 'pandas.zip')\n        if os.path.exists(zip_fname):\n            os.remove(zip_fname)\n\n        if self.single_doc_html is not None:\n            self._open_browser(self.single_doc_html)\n        else:\n            self._add_redirects()\n        return ret_code", "category": "Python"}, {"instruction": "def from_filename(filename, require=None):\n    \"\"\"Reads a Google service account JSON file and returns its parsed info.\n\n    Args:\n        filename (str): The path to the service account .json file.\n        require (Sequence[str]): List of keys required to be present in the\n            info.\n\n    Returns:\n        Tuple[ Mapping[str, str], google.auth.crypt.Signer ]: The verified\n            info and a signer instance.\n    \"\"\"\n", "input": "", "output": "    with io.open(filename, 'r', encoding='utf-8') as json_file:\n        data = json.load(json_file)\n        return data, from_dict(data, require=require)", "category": "Python"}, {"instruction": "def handle_rule_versions(self, filename, rule_type, rule):\n        \"\"\"\n        For each version of a rule found in the ruleset, append a new Rule object\n        \"\"\"\n", "input": "", "output": "        if 'versions' in rule:\n            versions = rule.pop('versions')\n            for version_key_suffix in versions:\n                version = versions[version_key_suffix]\n                version['key_suffix'] = version_key_suffix\n                tmp_rule = dict(rule, **version)\n                self.rules[filename].append(Rule(filename, rule_type, tmp_rule))\n        else:\n            self.rules[filename].append(Rule(filename, rule_type, rule))", "category": "Python"}, {"instruction": "def fmt_addr_raw(addr, reverse=True):\n    \"\"\"Given a string containing a xx:xx:xx:xx:xx:xx address, return as a byte sequence.\n\n    Args:\n        addr (str): Bluetooth address in xx:xx:xx:xx:xx:xx format.\n        reverse (bool): True if the byte ordering should be reversed in the output.\n\n    Returns:\n        A bytearray containing the converted address.\n    \"\"\"\n", "input": "", "output": "    addr = addr.replace(':', '')\n    raw_addr = [int(addr[i:i+2], 16) for i in range(0, len(addr), 2)]\n    if reverse:\n        raw_addr.reverse()\n\n    # for Python 2, this needs to be a string instead of a bytearray\n    if sys.version_info[0] == 2:\n        return str(bytearray(raw_addr))\n    return bytearray(raw_addr)", "category": "Python"}, {"instruction": "def all(self, func):\n        \"\"\"\n        :param func:\n        :type func: (K, T) -> bool\n        :rtype: bool\n\n        Usage:\n\n            >>> TDict(k1=1, k2=2, k3=3).all(lambda k, v: v > 0)\n            True\n            >>> TDict(k1=1, k2=2, k3=3).all(lambda k, v: v > 1)\n            False\n        \"\"\"\n", "input": "", "output": "        return all([func(k, v) for k, v in self.items()])", "category": "Python"}, {"instruction": "def editunivset(self):\n        '''\n        .foo = bar\n        '''\n", "input": "", "output": "        self.ignore(whitespace)\n\n        if not self.nextstr('.'):\n            self._raiseSyntaxExpects('.')\n\n        univ = self.univprop()\n        self.ignore(whitespace)\n\n        self.nextmust('=')\n\n        self.ignore(whitespace)\n\n        valu = self.valu()\n        return s_ast.EditPropSet(kids=(univ, valu))", "category": "Python"}, {"instruction": "def to_sif(graph: BELGraph, file: Optional[TextIO] = None, sep: Optional[str] = None) -> None:\n    \"\"\"Write the graph as a tab-separated SIF file.\n\n    The resulting file will contain the following columns:\n\n    1. Source BEL term\n    2. Relation\n    3. Target BEL term\n\n    This format is simple and can be used readily with many applications, but is lossy in that it does not include\n    relation metadata.\n    \"\"\"\n", "input": "", "output": "    if sep is None:\n        sep = '\\t'\n\n    for u, v, data in graph.edges(data=True):\n        print(\n            graph.edge_to_bel(u, v, edge_data=data, sep=sep),\n            file=file,\n        )", "category": "Python"}, {"instruction": "def blob_for(self, pack_uri):\n        \"\"\"\n        Return contents of file corresponding to *pack_uri* in package\n        directory.\n        \"\"\"\n", "input": "", "output": "        path = os.path.join(self._path, pack_uri.membername)\n        with open(path, 'rb') as f:\n            blob = f.read()\n        return blob", "category": "Python"}, {"instruction": "def rev_regs(self) -> list:\n        \"\"\"\n        Return list of revocation registry identifiers for which HolderProver has tails files.\n\n        :return: list of revocation registry identifiers for which HolderProver has tails files\n        \"\"\"\n", "input": "", "output": "\n        LOGGER.debug('HolderProver.rev_regs >>>')\n\n        rv = [basename(f) for f in Tails.links(self._dir_tails)]\n        LOGGER.debug('HolderProver.rev_regs <<< %s', rv)\n        return rv", "category": "Python"}, {"instruction": "def find_heavy_atoms_near_atom(self, source_atom, search_radius, atom_hit_cache = set(), restrict_to_CA = False):\n        '''atom_hit_cache is a set of atom serial numbers which have already been tested. We keep track of these to avoid recalculating the distance.\n        '''\n", "input": "", "output": "        #todo: Benchmark atom_hit_cache to see if it actually speeds up the search\n\n        non_heavy_atoms = self.get_atom_names_by_group(set(['H', 'D', 'T']))\n        return self.find_atoms_near_atom(source_atom, search_radius, atom_names_to_exclude = non_heavy_atoms, atom_hit_cache = atom_hit_cache, restrict_to_CA = restrict_to_CA)", "category": "Python"}, {"instruction": "def cluster_from_file(filename):\n        \"\"\"\n        Parse the feff input file and return the atomic cluster as a Molecule\n        object.\n\n        Args:\n            filename (str): path the feff input file\n\n        Returns:\n            Molecule: the atomic cluster as Molecule object. The absorbing atom\n                is the one at the origin.\n        \"\"\"\n", "input": "", "output": "        atoms_string = Atoms.atoms_string_from_file(filename)\n        line_list = [l.split() for l in atoms_string.splitlines()[3:]]\n        coords = []\n        symbols = []\n        for l in line_list:\n            if l:\n                coords.append([float(i) for i in l[:3]])\n                symbols.append(l[4])\n        return Molecule(symbols, coords)", "category": "Python"}, {"instruction": "def from_dict(settings):\n    \"\"\"Apply settings from dictionary\n\n    Arguments:\n        settings (dict): Settings in the form of a dictionary\n\n    \"\"\"\n", "input": "", "output": "\n    assert isinstance(settings, dict), \"`settings` must be of type dict\"\n    for key, value in settings.items():\n        setattr(self, key, value)", "category": "Python"}, {"instruction": "def get_indented_block(prefix_lines):\n    \"\"\"Returns an integer.\n\n    The return value is the number of lines that belong to block begun\n    on the first line.\n\n    Parameters\n    ----------\n\n      prefix_lines : list of basestring pairs\n        Each pair corresponds to a line of SHPAML source code. The\n        first element of each pair is indentation. The second is the\n        remaining part of the line, except for trailing newline.\n    \"\"\"\n", "input": "", "output": "    prefix, line = prefix_lines[0]\n    len_prefix = len(prefix)\n\n    # Find the first nonempty line with len(prefix) <= len(prefix)\n    i = 1\n    while i < len(prefix_lines):\n        new_prefix, line = prefix_lines[i]\n        if line and len(new_prefix) <= len_prefix:\n            break\n        i += 1\n\n    # Rewind to exclude empty lines\n    while i - 1 > 0 and prefix_lines[i - 1][1] == '':\n        i -= 1\n\n    return i", "category": "Python"}, {"instruction": "def get_gtf_db(gtf, in_memory=False):\n    \"\"\"\n    create a gffutils DB, in memory if we don't have write permissions\n    \"\"\"\n", "input": "", "output": "    db_file = gtf + \".db\"\n    if file_exists(db_file):\n        return gffutils.FeatureDB(db_file)\n    if not os.access(os.path.dirname(db_file), os.W_OK | os.X_OK):\n        in_memory = True\n    db_file = \":memory:\" if in_memory else db_file\n    if in_memory or not file_exists(db_file):\n        infer_extent = guess_infer_extent(gtf)\n        disable_extent = not infer_extent\n        db = gffutils.create_db(gtf, dbfn=db_file,\n                                disable_infer_genes=disable_extent,\n                                disable_infer_transcripts=disable_extent)\n    if in_memory:\n        return db\n    else:\n        return gffutils.FeatureDB(db_file)", "category": "Python"}, {"instruction": "def netconf_capability_change_changed_by_server_or_user_by_user_username(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        netconf_capability_change = ET.SubElement(config, \"netconf-capability-change\", xmlns=\"urn:ietf:params:xml:ns:yang:ietf-netconf-notifications\")\n        changed_by = ET.SubElement(netconf_capability_change, \"changed-by\")\n        server_or_user = ET.SubElement(changed_by, \"server-or-user\")\n        by_user = ET.SubElement(server_or_user, \"by-user\")\n        username = ET.SubElement(by_user, \"username\")\n        username.text = kwargs.pop('username')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def metadata_to_md_options(metadata):\n    \"\"\"Encode {'class':None, 'key':'value'} into 'class key=\"value\"' \"\"\"\n", "input": "", "output": "\n    return ' '.join([\"{}={}\".format(key, dumps(metadata[key]))\n                     if metadata[key] is not None else key for key in metadata])", "category": "Python"}, {"instruction": "def enqueue_job(request, queue_index, job_id):\n    \"\"\" Enqueue deferred jobs\n    \"\"\"\n", "input": "", "output": "    queue_index = int(queue_index)\n    queue = get_queue_by_index(queue_index)\n    job = Job.fetch(job_id, connection=queue.connection)\n\n    if request.method == 'POST':\n        queue.enqueue_job(job)\n\n        # Remove job from correct registry if needed\n        if job.get_status() == JobStatus.DEFERRED:\n            registry = DeferredJobRegistry(queue.name, queue.connection)\n            registry.remove(job)\n        elif job.get_status() == JobStatus.FINISHED:\n            registry = FinishedJobRegistry(queue.name, queue.connection)\n            registry.remove(job)\n\n        messages.info(request, 'You have successfully enqueued %s' % job.id)\n        return redirect('rq_job_detail', queue_index, job_id)\n\n    context_data = {\n        'queue_index': queue_index,\n        'job': job,\n        'queue': queue,\n    }\n    return render(request, 'django_rq/delete_job.html', context_data)", "category": "Python"}, {"instruction": "def pfd(X, D=None):\n    \"\"\"Compute Petrosian Fractal Dimension of a time series from either two\n    cases below:\n        1. X, the time series of type list (default)\n        2. D, the first order differential sequence of X (if D is provided,\n           recommended to speed up)\n\n    In case 1, D is computed using Numpy's difference function.\n\n    To speed up, it is recommended to compute D before calling this function\n    because D may also be used by other functions whereas computing it here\n    again will slow down.\n    \"\"\"\n", "input": "", "output": "    if D is None:\n        D = numpy.diff(X)\n        D = D.tolist()\n    N_delta = 0  # number of sign changes in derivative of the signal\n    for i in range(1, len(D)):\n        if D[i] * D[i - 1] < 0:\n            N_delta += 1\n    n = len(X)\n    return numpy.log10(n) / (\n        numpy.log10(n) + numpy.log10(n / n + 0.4 * N_delta)\n    )", "category": "Python"}, {"instruction": "def ziparchive_opener(path, pattern='', verbose=False):\n    \"\"\"Opener that opens files from zip archive..\n\n    :param str path: Path.\n    :param str pattern: Regular expression pattern.\n    :return: Filehandle(s).\n    \"\"\"\n", "input": "", "output": "    with zipfile.ZipFile(io.BytesIO(urlopen(path).read()), 'r') if is_url(path) else zipfile.ZipFile(path, 'r') as ziparchive:\n        for zipinfo in ziparchive.infolist():\n            if not zipinfo.filename.endswith('/'):\n                source = os.path.join(path, zipinfo.filename)\n\n                if pattern and not re.match(pattern, zipinfo.filename):\n                    logger.verbose('Skipping file: {}, did not match regex pattern \"{}\"'.format(os.path.abspath(zipinfo.filename), pattern))\n                    continue\n\n                logger.verbose('Processing file: {}'.format(source))\n                filehandle = ziparchive.open(zipinfo)\n                yield filehandle", "category": "Python"}, {"instruction": "def tanh_discrete_bottleneck(x, bottleneck_bits, bottleneck_noise,\n                             discretize_warmup_steps, mode):\n  \"\"\"Simple discretization through tanh, flip bottleneck_noise many bits.\"\"\"\n", "input": "", "output": "  x = tf.layers.dense(x, bottleneck_bits, name=\"tanh_discrete_bottleneck\")\n  d0 = tf.stop_gradient(2.0 * tf.to_float(tf.less(0.0, x))) - 1.0\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    x += tf.truncated_normal(\n        common_layers.shape_list(x), mean=0.0, stddev=0.2)\n  x = tf.tanh(x)\n  d = x + tf.stop_gradient(2.0 * tf.to_float(tf.less(0.0, x)) - 1.0 - x)\n  if mode == tf.estimator.ModeKeys.TRAIN:\n    noise = tf.random_uniform(common_layers.shape_list(x))\n    noise = 2.0 * tf.to_float(tf.less(bottleneck_noise, noise)) - 1.0\n    d *= noise\n  d = common_layers.mix(d, x, discretize_warmup_steps,\n                        mode == tf.estimator.ModeKeys.TRAIN)\n  return d, d0", "category": "Python"}, {"instruction": "def by_works(self, kind=None, role_name=None):\n        \"\"\"\n        Get the Creators involved in the most Works.\n\n        kind - If supplied, only Works with that `kind` value will be counted.\n        role_name - If supplied, only Works on which the role is that will be counted.\n\n        e.g. To get all 'movie' Works on which the Creators had the role 'Director':\n\n            Creator.objects.by_works(kind='movie', role_name='Director')\n        \"\"\"\n", "input": "", "output": "        if not spectator_apps.is_enabled('events'):\n            raise ImproperlyConfigured(\"To use the CreatorManager.by_works() method, 'spectator.events' must by in INSTALLED_APPS.\")\n\n        qs = self.get_queryset()\n\n        filter_kwargs = {}\n\n        if kind is not None:\n            filter_kwargs['works__kind'] = kind\n\n        if role_name is not None:\n            filter_kwargs['work_roles__role_name'] = role_name\n\n        if filter_kwargs:\n            qs = qs.filter(**filter_kwargs)\n\n        qs = qs.annotate(num_works=Count('works', distinct=True)) \\\n                .order_by('-num_works', 'name_sort')\n\n        return qs", "category": "Python"}, {"instruction": "def deletecc(self, cclist, comment=None):\n        \"\"\"\n        Removes the given email addresses from the CC list for this bug.\n        \"\"\"\n", "input": "", "output": "        vals = self.bugzilla.build_update(comment=comment,\n                                          cc_remove=cclist)\n        log.debug(\"deletecc: update=%s\", vals)\n\n        return self.bugzilla.update_bugs(self.bug_id, vals)", "category": "Python"}, {"instruction": "def tick_label_position(self):\n        \"\"\"\n        Read/write :ref:`XlTickLabelPosition` value specifying where the tick\n        labels for this axis should appear.\n        \"\"\"\n", "input": "", "output": "        tickLblPos = self._element.tickLblPos\n        if tickLblPos is None:\n            return XL_TICK_LABEL_POSITION.NEXT_TO_AXIS\n        if tickLblPos.val is None:\n            return XL_TICK_LABEL_POSITION.NEXT_TO_AXIS\n        return tickLblPos.val", "category": "Python"}, {"instruction": "def _connected(self, link_uri):\n        \"\"\" This callback is called form the Crazyflie API when a Crazyflie\n        has been connected and the TOCs have been downloaded.\"\"\"\n", "input": "", "output": "        print('Connected to %s' % link_uri)\n\n        mems = self._cf.mem.get_mems(MemoryElement.TYPE_1W)\n        print('Found {} 1-wire memories'.format(len(mems)))\n        if len(mems) > 0:\n            print('Erasing memory {}'.format(mems[0].id))\n            mems[0].erase(self._data_written)", "category": "Python"}, {"instruction": "def learn(self, examples, attributes, parent_examples):\n        \"\"\"\n        A decision tree learner that *strictly* follows the pseudocode given in\n        AIMA. In 3rd edition, see Figure 18.5, page 702.\n        \"\"\"\n", "input": "", "output": "        if not examples:\n            return self.plurality_value(parent_examples)\n        elif len(set(map(self.target, examples))) == 1:\n            return self.plurality_value(examples)\n        elif not attributes:\n            return self.plurality_value(examples)\n        A = max(attributes, key=lambda a: self.importance(a, examples))\n        tree = DecisionTreeNode(attribute=A)\n        for value in set(map(A, examples)):\n            exs = [e for e in examples if A(e) == value]\n            subtree = self.learn(exs, attributes - set([A]), examples)\n            tree.add_branch(value, subtree)\n        return tree", "category": "Python"}, {"instruction": "def get_stops(records, group_dist):\n    \"\"\"\n    Group records arounds stop locations and returns a list of\n    dict(location, records) for each stop.\n\n    Parameters\n    ----------\n    records : list\n        A list of Record objects ordered by non-decreasing datetime\n    group_dist : float\n        Minimum distance (in meters) to switch to a new stop.\n    \"\"\"\n", "input": "", "output": "    def traverse(start, next):\n        position_prev = records[next - 1].position.location\n        position_next = records[next].position.location\n        dist = 1000 * great_circle_distance(position_prev, position_next)\n        return dist <= group_dist\n\n    groups = _groupwhile(records, traverse)\n\n    def median(x):\n        return sorted(x)[len(x) // 2]\n\n    stops = []\n    for g in groups:\n        _lat = median([gv.position.location[0] for gv in g])\n        _lon = median([gv.position.location[1] for gv in g])\n        stops.append({\n            'location': (_lat, _lon),\n            'records': g,\n        })\n\n    return stops", "category": "Python"}, {"instruction": "def get_args(self):\n        \"\"\"\n        Returns the list of encoded job arguments. The order of this list corresponds to the\n        arguments expected by the job wrapper script.\n        \"\"\"\n", "input": "", "output": "        return [\n            self.task_cls.__module__,\n            self.task_cls.__name__,\n            self.encode_list(self.task_params),\n            self.encode_list(self.branches),\n            self.encode_bool(self.auto_retry),\n            self.encode_list(self.dashboard_data),\n        ]", "category": "Python"}, {"instruction": "def _tiles_from_bbox(bbox, zoom_level):\n        \"\"\"\n         * Returns all tiles for the specified bounding box\n        \"\"\"\n", "input": "", "output": "\n        if isinstance(bbox, dict):\n            point_min = Point.from_latitude_longitude(latitude=bbox['tl'], longitude=bbox['tr'])\n            point_max = Point.from_latitude_longitude(latitude=bbox['bl'], longitude=bbox['br'])\n        elif isinstance(bbox, list):\n            point_min = Point.from_latitude_longitude(latitude=bbox[1], longitude=bbox[0])\n            point_max = Point.from_latitude_longitude(latitude=bbox[3], longitude=bbox[2])\n        else:\n            raise RuntimeError(\"bbox must bei either a dict or a list\")\n        tile_min = Tile.for_point(point_min, zoom_level)\n        tile_max = Tile.for_point(point_max, zoom_level)\n        tiles = []\n        for x in range(tile_min.tms_x, tile_max.tms_x + 1):\n            for y in range(tile_min.tms_y, tile_max.tms_y + 1):\n                tiles.append(Tile.from_tms(tms_x=x, tms_y=y, zoom=zoom_level))\n        return tiles", "category": "Python"}, {"instruction": "def parse_unix_perm(text):\n    '''Parse a Unix permission string and return integer value.'''\n", "input": "", "output": "    # Based on ftp-ls.c symperms\n    if len(text) != 9:\n        return 0\n\n    perms = 0\n\n    for triad_index in range(3):\n        string_index = triad_index * 3\n        perms <<= 3\n\n        if text[string_index] == 'r':\n            perms |= 1 << 2\n\n        if text[string_index + 1] == 'w':\n            perms |= 1 << 1\n\n        if text[string_index + 2] in 'xs':\n            perms |= 1\n\n    return perms", "category": "Python"}, {"instruction": "def delete(self, resource, url_prefix, auth, session, send_opts):\n        \"\"\"Deletes the entity described by the given resource.\n\n        Args:\n            resource (intern.resource.boss.BossResource)\n            url_prefix (string): Protocol + host such as https://api.theboss.io\n            auth (string): Token to send in the request header.\n            session (requests.Session): HTTP session to use for request.\n            send_opts (dictionary): Additional arguments to pass to session.send().\n\n        Raises:\n            requests.HTTPError on failure.\n        \"\"\"\n", "input": "", "output": "        req = self.get_request(\n            resource, 'DELETE', 'application/json', url_prefix, auth)\n        prep = session.prepare_request(req)\n        resp = session.send(prep, **send_opts)\n        if resp.status_code == 204:\n            return\n\n        err = ('Delete failed on {}, got HTTP response: ({}) - {}'.format(\n            resource.name, resp.status_code, resp.text))\n        raise HTTPError(err, request = req, response = resp)", "category": "Python"}, {"instruction": "def _create_default_config(self):\n        '''Create and write to disk a default site config file.'''\n", "input": "", "output": "        # maybe I should read the default config from somewhere in the package?\n        cfg = { 'site_title': '',\n                'site_subtitle': '',\n                'default_author': '',\n                'site_url': '',\n                'default_theme': 'blog1',\n                'default_template': 'main.html.tpl',\n                'fixed_frontpage': ''\n              }\n\n\n        file_name = os.path.join(self._dirs['s2'],'config.yml')\n        f = open(file_name,'w')\n        f.write(yaml.dump(cfg,default_flow_style=False))\n        f.close()\n        return cfg", "category": "Python"}, {"instruction": "def entropy_bits_nrange(\n        minimum: Union[int, float], maximum: Union[int, float]\n) -> float:\n    \"\"\"Calculate the number of entropy bits in a range of numbers.\"\"\"\n", "input": "", "output": "    # Shannon:\n    # d = fabs(maximum - minimum)\n    # ent = -(1/d) * log(1/d, 2) * d\n    # Aprox form: log10(digits) * log2(10)\n    if not isinstance(minimum, (int, float)):\n        raise TypeError('minimum can only be int or float')\n    if not isinstance(maximum, (int, float)):\n        raise TypeError('maximum can only be int or float')\n    if minimum < 0:\n        raise ValueError('minimum should be greater than 0')\n    if maximum < 0:\n        raise ValueError('maximum should be greater than 0')\n\n    dif = fabs(maximum - minimum)\n    if dif == 0:\n        return 0.0\n\n    ent = log10(dif) * 3.321928\n    return ent", "category": "Python"}, {"instruction": "def coth(x, context=None):\n    \"\"\"\n    Return the hyperbolic cotangent of x.\n\n    \"\"\"\n", "input": "", "output": "    return _apply_function_in_current_context(\n        BigFloat,\n        mpfr.mpfr_coth,\n        (BigFloat._implicit_convert(x),),\n        context,\n    )", "category": "Python"}, {"instruction": "def remove_framework_listener(self, listener):\n        \"\"\"\n        Unregisters a framework stop listener\n\n        :param listener: The framework listener to unregister\n        :return: True if the listener has been unregistered, else False\n        \"\"\"\n", "input": "", "output": "        with self.__fw_lock:\n            try:\n                self.__fw_listeners.remove(listener)\n                return True\n            except ValueError:\n                return False", "category": "Python"}, {"instruction": "def whole_sequence_accuracy(y_true, y_pred, lengths):\n    \"\"\"Average accuracy measured on whole sequences.\n\n    Returns the fraction of sequences in y_true that occur in y_pred without a\n    single error.\n    \"\"\"\n", "input": "", "output": "    lengths = np.asarray(lengths)\n    end = np.cumsum(lengths)\n    start = end - lengths\n    bounds = np.vstack([start, end]).T\n\n    errors = sum(1. for i, j in bounds\n                 if np.any(y_true[i:j] != y_pred[i:j]))\n    return 1 - errors / len(lengths)", "category": "Python"}, {"instruction": "def bytesIO(value,\n            allow_empty = False,\n            **kwargs):\n    \"\"\"Validate that ``value`` is a :class:`BytesIO <python:io.BytesIO>` object.\n\n    :param value: The value to validate.\n\n    :param allow_empty: If ``True``, returns :obj:`None <python:None>` if\n      ``value`` is empty. If ``False``, raises a\n      :class:`EmptyValueError <validator_collection.errors.EmptyValueError>`\n      if ``value`` is empty. Defaults to ``False``.\n    :type allow_empty: :class:`bool <python:bool>`\n\n    :returns: ``value`` / :obj:`None <python:None>`\n    :rtype: :class:`BytesIO <python:io.BytesIO>` / :obj:`None <python:None>`\n\n    :raises EmptyValueError: if ``value`` is empty and ``allow_empty`` is ``False``\n    :raises NotBytesIOError: if ``value`` is not a :class:`BytesIO <python:io.BytesIO>`\n      object.\n    \"\"\"\n", "input": "", "output": "    if not value and not allow_empty:\n        raise errors.EmptyValueError('value (%s) was empty' % value)\n    elif not value:\n        return None\n\n    if not isinstance(value, io.BytesIO):\n        raise errors.NotBytesIOError('value (%s) is not a BytesIO, '\n                                     'is a %s' % (value, type(value)))\n\n    return value", "category": "Python"}, {"instruction": "def load(self, loc):\n        '''Load a pickled model.'''\n", "input": "", "output": "        try:\n            w_td_c = pickle.load(open(loc, 'rb'))\n        except IOError:\n            msg = (\"Missing trontagger.pickle file.\")\n            raise MissingCorpusError(msg)\n        self.model.weights, self.tagdict, self.classes = w_td_c\n        self.model.classes = self.classes\n        return None", "category": "Python"}, {"instruction": "def missing_inds(x, format_data=True):\n    \"\"\"\n    Returns indices of missing data\n\n    This function is useful to identify rows of your array that contain missing\n    data or nans.  The returned indices can be used to remove the rows with\n    missing data, or label the missing data points that are interpolated\n    using PPCA.\n\n    Parameters\n    ----------\n    x : array or list of arrays\n\n    format_data : bool\n        Whether or not to first call the format_data function (default: True).\n\n    Returns\n    ----------\n    inds : list, or list of lists\n        A list of indices representing rows with missing data. If a list of\n        numpy arrays is passed, a list of lists will be returned.\n\n    \"\"\"\n", "input": "", "output": "\n    if format_data:\n        x = formatter(x, ppca=False)\n\n    inds = []\n    for arr in x:\n        if np.argwhere(np.isnan(arr)).size is 0:\n            inds.append(None)\n        else:\n            inds.append(np.argwhere(np.isnan(arr))[:,0])\n    if len(inds) > 1:\n        return inds\n    else:\n        return inds[0]", "category": "Python"}, {"instruction": "def to_bing_str(self):\n        \"\"\"\n        Convert Viewbox object to a string that can be used by Bing\n        as a query parameter.\n        \"\"\"\n", "input": "", "output": "        vb = self.convert_srs(4326)\n        return '%s,%s,%s,%s' % (vb.bottom, vb.left, vb.top, vb.right)", "category": "Python"}, {"instruction": "def _extract_tls_session_ticket(ssl_session: nassl._nassl.SSL_SESSION) -> str:\n        \"\"\"Extract the TLS session ticket from a SSL session object or raises IndexError if the ticket was not set.\n        \"\"\"\n", "input": "", "output": "        session_string = ((ssl_session.as_text()).split('TLS session ticket:'))[1]\n        session_tls_ticket = (session_string.split('Compression:'))[0]\n        return session_tls_ticket", "category": "Python"}, {"instruction": "def get_properties_from_graph(graph):\n        \"\"\"\n        Wrapper for RDFLib.graph.predicates() that returns a unique set\n        :param graph: RDFLib.graph\n        :return: set, set of properties\n        \"\"\"\n", "input": "", "output": "        # collapse to single list\n        property_set = set()\n        for row in graph.predicates():\n            property_set.add(row)\n\n        return property_set", "category": "Python"}, {"instruction": "def modify(self, modification, parameters):\n        \"\"\"\n        Apply a modification to the underlying point sources, with the\n        same parameters for all sources\n        \"\"\"\n", "input": "", "output": "        for src in self:\n            src.modify(modification, parameters)", "category": "Python"}, {"instruction": "def batch_commit(self, *, do_deletes=False):\n        '''\n        Batch and commit and end of context\n        '''\n", "input": "", "output": "        try:\n            yield\n        except Exception as exc:\n            raise exc\n        else:\n            for key, value in self.cache.items():\n                if value is not DELETED:\n                    self.wrapped_db[key] = value\n                elif do_deletes:\n                    self.wrapped_db.pop(key, None)\n                # if do_deletes is False, ignore deletes to underlying db\n        finally:\n            self.cache = {}", "category": "Python"}, {"instruction": "def write_file(self, filename):\n        \"\"\"\n        Write the PWSCF input file.\n\n        Args:\n            filename (str): The string filename to output to.\n        \"\"\"\n", "input": "", "output": "        with open(filename, \"w\") as f:\n            f.write(self.__str__())", "category": "Python"}, {"instruction": "def _write_file_network(data, filename, create=False):\n    '''\n    Writes a file to disk\n    If file does not exist, only create if create\n    argument is True\n    '''\n", "input": "", "output": "    if not os.path.exists(filename) and not create:\n        msg = '{0} cannot be written. {0} does not exist\\\n                and create is set to False'\n        msg = msg.format(filename)\n        log.error(msg)\n        raise AttributeError(msg)\n    with salt.utils.files.flopen(filename, 'w') as fout:\n        fout.write(salt.utils.stringutils.to_str(data))", "category": "Python"}, {"instruction": "async def set_control_setpoint(self, setpoint,\n                                   timeout=OTGW_DEFAULT_TIMEOUT):\n        \"\"\"\n        Manipulate the control setpoint being sent to the boiler. Set\n        to 0 to pass along the value specified by the thermostat.\n        Return the newly accepted value, or None on failure.\n\n        This method is a coroutine\n        \"\"\"\n", "input": "", "output": "        cmd = OTGW_CMD_CONTROL_SETPOINT\n        status = {}\n        ret = await self._wait_for_cmd(cmd, setpoint, timeout)\n        if ret is None:\n            return\n        ret = float(ret)\n        status[DATA_CONTROL_SETPOINT] = ret\n        self._update_status(status)\n        return ret", "category": "Python"}, {"instruction": "def nominal_step(x=None):\n    \"\"\"Return nominal step\"\"\"\n", "input": "", "output": "    if x is None:\n        return 1.0\n    return np.log1p(np.abs(x)).clip(min=1.0)", "category": "Python"}, {"instruction": "def start_logging(out=_stdout, level='info'):\n    \"\"\"\n    Start logging to the file-like object in ``out``. By default, this\n    is stdout.\n    \"\"\"\n", "input": "", "output": "    global _loggers, _observer, _log_level, _started_logging\n\n    if level not in log_levels:\n        raise RuntimeError(\n            \"Invalid log level '{0}'; valid are: {1}\".format(\n                level, ', '.join(log_levels)\n            )\n        )\n\n    if _started_logging:\n        return\n\n    _started_logging = True\n\n    _log_level = level\n    set_global_log_level(_log_level)\n\n    if out:\n        _observer = _LogObserver(out)\n\n    if _NEW_LOGGER:\n        _observers = []\n        if _observer:\n            _observers.append(_observer)\n        globalLogBeginner.beginLoggingTo(_observers)\n    else:\n        assert out, \"out needs to be given a value if using Twisteds before 15.2\"\n        from twisted.python import log\n        log.startLogging(out)", "category": "Python"}, {"instruction": "def enforce_mask_shape(mask, shape):\n    \"\"\"Reduce a boolean mask to fit a given shape.\n\n    Parameters\n    ----------\n    mask : ndarray with bool dtype\n        The mask which is to be reduced\n    shape : tuple of int\n        Shape which broadcasts to the mask shape.\n\n    Returns\n    -------\n        A boolean mask, collapsed along axes where the shape given has one element.\n    \"\"\"\n", "input": "", "output": "    red = tuple([i for i in range(len(shape)) if shape[i] == 1])\n    return mask.max(axis=red, keepdims=True)", "category": "Python"}, {"instruction": "def submit_and_verify(\n    xml_str=None, xml_file=None, xml_root=None, config=None, session=None, dry_run=None, **kwargs\n):\n    \"\"\"Submits data to the Polarion Importer and checks that it was imported.\"\"\"\n", "input": "", "output": "    try:\n        config = config or configuration.get_config()\n        xml_root = _get_xml_root(xml_root, xml_str, xml_file)\n        submit_config = SubmitConfig(xml_root, config, **kwargs)\n        session = session or utils.get_session(submit_config.credentials, config)\n        submit_response = submit(xml_root, submit_config, session, dry_run=dry_run, **kwargs)\n    except Dump2PolarionException as err:\n        logger.error(err)\n        return None\n\n    valid_response = submit_response.validate_response()\n    if not valid_response or kwargs.get(\"no_verify\"):\n        return submit_response.response\n\n    response = verify_submit(\n        session,\n        submit_config.queue_url,\n        submit_config.log_url,\n        submit_response.job_ids,\n        timeout=kwargs.get(\"verify_timeout\"),\n        log_file=kwargs.get(\"log_file\"),\n    )\n\n    return response", "category": "Python"}, {"instruction": "def setTargets(self, targets):\n        \"\"\"\n        Sets the targets.\n        \"\"\"\n", "input": "", "output": "        if not self.verifyArguments(targets) and not self.patterned:\n            raise NetworkError('setTargets() requires [[...],[...],...] or [{\"layerName\": [...]}, ...].', targets)\n        self.targets = targets", "category": "Python"}, {"instruction": "def _unpack_union(type_: int, union: Any) -> Any:\n    \"\"\"\n        unpack items from parser new_property (value_converter)\n    \"\"\"\n", "input": "", "output": "    if type_ == lib.TCOD_TYPE_BOOL:\n        return bool(union.b)\n    elif type_ == lib.TCOD_TYPE_CHAR:\n        return union.c.decode(\"latin-1\")\n    elif type_ == lib.TCOD_TYPE_INT:\n        return union.i\n    elif type_ == lib.TCOD_TYPE_FLOAT:\n        return union.f\n    elif (\n        type_ == lib.TCOD_TYPE_STRING\n        or lib.TCOD_TYPE_VALUELIST15 >= type_ >= lib.TCOD_TYPE_VALUELIST00\n    ):\n        return _unpack_char_p(union.s)\n    elif type_ == lib.TCOD_TYPE_COLOR:\n        return Color._new_from_cdata(union.col)\n    elif type_ == lib.TCOD_TYPE_DICE:\n        return Dice(union.dice)\n    elif type_ & lib.TCOD_TYPE_LIST:\n        return _convert_TCODList(union.list, type_ & 0xFF)\n    else:\n        raise RuntimeError(\"Unknown libtcod type: %i\" % type_)", "category": "Python"}, {"instruction": "def assignrepr_values2(values, prefix):\n    \"\"\"Return a prefixed and properly aligned string representation\n    of the given 2-dimensional value matrix using function |repr|.\n\n    >>> from hydpy.core.objecttools import assignrepr_values2\n    >>> import numpy\n    >>> print(assignrepr_values2(numpy.eye(3), 'test(') + ')')\n    test(1.0, 0.0, 0.0,\n         0.0, 1.0, 0.0,\n         0.0, 0.0, 1.0)\n\n    Functions |assignrepr_values2| works also on empty iterables:\n\n    >>> print(assignrepr_values2([[]], 'test(') + ')')\n    test()\n    \"\"\"\n", "input": "", "output": "    lines = []\n    blanks = ' '*len(prefix)\n    for (idx, subvalues) in enumerate(values):\n        if idx == 0:\n            lines.append('%s%s,' % (prefix, repr_values(subvalues)))\n        else:\n            lines.append('%s%s,' % (blanks, repr_values(subvalues)))\n    lines[-1] = lines[-1][:-1]\n    return '\\n'.join(lines)", "category": "Python"}, {"instruction": "def display_hook(prompt, session, context, matches, longest_match_len):\n        # type: (str, ShellSession, BundleContext, List[str], int) -> None\n        \"\"\"\n        Displays the available bundle matches and the bundle name\n\n        :param prompt: Shell prompt string\n        :param session: Current shell session (for display)\n        :param context: BundleContext of the shell\n        :param matches: List of words matching the substitution\n        :param longest_match_len: Length of the largest match\n        \"\"\"\n", "input": "", "output": "        # Prepare a line pattern for each match\n        match_pattern = \"{{0: >{}}}: {{1}}\".format(longest_match_len)\n\n        # Sort matching IDs\n        matches = sorted(int(match) for match in matches)\n\n        # Print the match and the associated name\n        session.write_line()\n        for bnd_id in matches:\n            bnd = context.get_bundle(bnd_id)\n            session.write_line(match_pattern, bnd_id, bnd.get_symbolic_name())\n\n        # Print the prompt, then current line\n        session.write(prompt)\n        session.write_line_no_feed(readline.get_line_buffer())\n        readline.redisplay()", "category": "Python"}, {"instruction": "def __render_videoframe(self):\n\t\t\"\"\" Retrieves a new videoframe from the stream.\n\n\t\tSets the frame as the __current_video_frame and passes it on to\n\t\t__videorenderfunc() if it is set. \"\"\"\n", "input": "", "output": "\n\t\tnew_videoframe = self.clip.get_frame(self.clock.time)\n\t\t# Pass it to the callback function if this is set\n\t\tif callable(self.__videorenderfunc):\n\t\t\tself.__videorenderfunc(new_videoframe)\n\t\t# Set current_frame to current frame (...)\n\t\tself.__current_videoframe = new_videoframe", "category": "Python"}, {"instruction": "def activate(self, event):\n        \"\"\"Change the value.\"\"\"\n", "input": "", "output": "        self._index += 1\n        if self._index >= len(self._values):\n            self._index = 0\n\n        self._selection = self._values[self._index]\n        self.ao2.speak(self._selection)", "category": "Python"}, {"instruction": "def id_to_extended_id(item_id, item_class):\n        \"\"\"Return the extended ID from an ID.\n\n        :param item_id: The ID of the music library item\n        :type item_id: str\n        :param cls: The class of the music service item\n        :type cls: Sub-class of\n            :py:class:`soco.data_structures.MusicServiceItem`\n\n        The extended id can be something like 00030020trackid_22757082\n        where the id is just trackid_22757082. For classes where the prefix is\n        not known returns None.\n        \"\"\"\n", "input": "", "output": "        out = ID_PREFIX[item_class]\n        if out:\n            out += item_id\n        return out", "category": "Python"}, {"instruction": "def simple_atmo_opstring(haze, contrast, bias):\n    \"\"\"Make a simple atmospheric correction formula.\"\"\"\n", "input": "", "output": "    gamma_b = 1 - haze\n    gamma_g = 1 - (haze / 3.0)\n    ops = (\n        \"gamma g {gamma_g}, \" \"gamma b {gamma_b}, \" \"sigmoidal rgb {contrast} {bias}\"\n    ).format(gamma_g=gamma_g, gamma_b=gamma_b, contrast=contrast, bias=bias)\n    return ops", "category": "Python"}, {"instruction": "def cybox_defined_object_in_fact_term_handler(self, enrichment, fact, attr_info, add_fact_kargs):\n        \"\"\"\n        From CybOX 1.x to Cybox 2.0.0, there was a structural change in the way\n        observable properties are included in the XML: in CybOX 1.x, they\n        were embedded in an element called 'Defined_Object' -- since CybOX 2.x,\n        we have the 'Properties' element. Here, we rename occurrences of 'Defined_Object'\n        in a fact term with 'Properties'. As a result, fact terms for, e.g.,\n        'Properties/Header/To/Recipient/AddressValue' for an Email object are the same\n        for imports from Cybox 1.x and Cybox 2.x.\n        \"\"\"\n", "input": "", "output": "        add_fact_kargs['fact_term_name'] = self.RE_DEFINED_OBJECT.sub('Properties', fact['term'])\n        return True", "category": "Python"}, {"instruction": "def add_package(pkg_type, pkg_name, working=None):\n    \"\"\"add_package\n\nAdds a package to the existing config.JSON file.  This is a standalone function\nto handle this functionality.  The existing config.JSON is read in and is left\nunchanged.\n    \"\"\"\n", "input": "", "output": "    kvp = {pkg_type: pkg_name}\n    working = os.getcwd() if not working else os.path.abspath(working)\n    if '-dist/scripts' in working:\n        config = working + \"/config.JSON\"\n    elif '-dist' in working:\n        config = working + \"/scripts/config.JSON\"\n    else:\n        config = working + \"/*-dist/scripts/config.JSON\"\n        entropy = glob.glob(config)\n        if entropy:\n            config = entropy[0]  # we'll just assume it's the first one...\n    print(\"config\", config, 'working', working)\n    config = load_config(config)\n    config.update(kvp)\n    export_to_json(config)", "category": "Python"}, {"instruction": "def _get_retrier(self, receiver: Address) -> _RetryQueue:\n        \"\"\" Construct and return a _RetryQueue for receiver \"\"\"\n", "input": "", "output": "        if receiver not in self._address_to_retrier:\n            retrier = _RetryQueue(transport=self, receiver=receiver)\n            self._address_to_retrier[receiver] = retrier\n            # Always start the _RetryQueue, otherwise `stop` will block forever\n            # waiting for the corresponding gevent.Greenlet to complete. This\n            # has no negative side-effects if the transport has stopped because\n            # the retrier itself checks the transport running state.\n            retrier.start()\n        return self._address_to_retrier[receiver]", "category": "Python"}, {"instruction": "def getRoles(self, principal_id):\n        \"\"\"\n        give an Owner who is also a 'selfpublisher', the reviewer role\n        \"\"\"\n", "input": "", "output": "        context = self.context\n        current_roles = list(DefaultLocalRoleAdapter.getRoles(\n            self,\n            principal_id,\n        ))\n\n        # check we are not on the workspace itself\n        if IHasWorkspace.providedBy(context):\n            return current_roles\n        # otherwise we should acquire the workspace and check out roles\n        workspace = getattr(context, 'acquire_workspace', lambda: None)()\n        if workspace is None:\n            return current_roles\n\n        workspace_roles = api.user.get_roles(obj=workspace)\n        if 'SelfPublisher' in workspace_roles and 'Owner' in current_roles:\n            current_roles.append('Reviewer')\n        return current_roles", "category": "Python"}, {"instruction": "def _WriteFileEntry(self, file_entry, data_stream_name, destination_file):\n    \"\"\"Writes the contents of the source file entry to a destination file.\n\n    Note that this function will overwrite an existing file.\n\n    Args:\n      file_entry (dfvfs.FileEntry): file entry whose content is to be written.\n      data_stream_name (str): name of the data stream whose content is to be\n          written.\n      destination_file (str): path of the destination file.\n    \"\"\"\n", "input": "", "output": "    source_file_object = file_entry.GetFileObject(\n        data_stream_name=data_stream_name)\n    if not source_file_object:\n      return\n\n    try:\n      with open(destination_file, 'wb') as destination_file_object:\n        source_file_object.seek(0, os.SEEK_SET)\n\n        data = source_file_object.read(self._COPY_BUFFER_SIZE)\n        while data:\n          destination_file_object.write(data)\n          data = source_file_object.read(self._COPY_BUFFER_SIZE)\n\n    finally:\n      source_file_object.close()", "category": "Python"}, {"instruction": "def get_gains_losses(changes):\n    \"\"\" Categorizes changes into gains and losses\n\n    Args:\n        changes: List of floats of price changes between entries in JSON.\n\n    Returns:\n        Dict of changes with keys 'gains' and 'losses'.\n        All values are positive.\n    \"\"\"\n", "input": "", "output": "    res = {'gains': [], 'losses': []}\n    for change in changes:\n        if change > 0:\n            res['gains'].append(change)\n        else:\n            res['losses'].append(change * -1)\n    logger.debug('Gains: {0}'.format(res['gains']))\n    logger.debug('Losses: {0}'.format(res['losses']))\n    return res", "category": "Python"}, {"instruction": "def _getpwnam(name, root=None):\n    '''\n    Alternative implementation for getpwnam, that use only /etc/passwd\n    '''\n", "input": "", "output": "    root = '/' if not root else root\n    passwd = os.path.join(root, 'etc/passwd')\n    with salt.utils.files.fopen(passwd) as fp_:\n        for line in fp_:\n            line = salt.utils.stringutils.to_unicode(line)\n            comps = line.strip().split(':')\n            if comps[0] == name:\n                # Generate a getpwnam compatible output\n                comps[2], comps[3] = int(comps[2]), int(comps[3])\n                return pwd.struct_passwd(comps)\n    raise KeyError", "category": "Python"}, {"instruction": "def visit_continue(self, node, parent):\n        \"\"\"visit a Continue node by returning a fresh instance of it\"\"\"\n", "input": "", "output": "        return nodes.Continue(\n            getattr(node, \"lineno\", None), getattr(node, \"col_offset\", None), parent\n        )", "category": "Python"}, {"instruction": "def relative_field(field, parent):\n    \"\"\"\n    RETURN field PATH WITH RESPECT TO parent\n    \"\"\"\n", "input": "", "output": "    if parent==\".\":\n        return field\n\n    field_path = split_field(field)\n    parent_path = split_field(parent)\n    common = 0\n    for f, p in _builtin_zip(field_path, parent_path):\n        if f != p:\n            break\n        common += 1\n\n    if len(parent_path) == common:\n        return join_field(field_path[common:])\n    else:\n        dots = \".\" * (len(parent_path) - common)\n        return dots + \".\" + join_field(field_path[common:])", "category": "Python"}, {"instruction": "def subscribe(topic, protocol, endpoint, region=None, key=None, keyid=None, profile=None):\n    '''\n    Subscribe to a Topic.\n\n    CLI example to delete a topic::\n\n        salt myminion boto_sns.subscribe mytopic https https://www.example.com/sns-endpoint region=us-east-1\n    '''\n", "input": "", "output": "    conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n    conn.subscribe(get_arn(topic, region, key, keyid, profile), protocol, endpoint)\n    log.info('Subscribe %s %s to %s topic', protocol, endpoint, topic)\n    try:\n        del __context__[_subscriptions_cache_key(topic)]\n    except KeyError:\n        pass\n    return True", "category": "Python"}, {"instruction": "def get_path(self, key, rel_to_cwd=False, rel_to_conf=False):\n        \"\"\"\n        Retrieve a path from the config, resolving it against\n        the invokation directory or the configuration file directory,\n        depending on whether it was passed through the command-line\n        or the configuration file.\n\n        Args:\n            key: str, the key to lookup the path with\n\n        Returns:\n            str: The path, or `None`\n        \"\"\"\n", "input": "", "output": "        if key in self.__cli:\n            path = self.__cli[key]\n            from_conf = False\n        else:\n            path = self.__config.get(key)\n            from_conf = True\n\n        if not isinstance(path, str):\n            return None\n\n        res = self.__abspath(path, from_conf)\n\n        if rel_to_cwd:\n            return os.path.relpath(res, self.__invoke_dir)\n        if rel_to_conf:\n            return os.path.relpath(res, self.__conf_dir)\n\n        return self.__abspath(path, from_conf)", "category": "Python"}, {"instruction": "def tospark(self, engine=None):\n        \"\"\"\n        Convert to distributed spark mode.\n        \"\"\"\n", "input": "", "output": "        from thunder.images.readers import fromarray\n\n        if self.mode == 'spark':\n            logging.getLogger('thunder').warn('images already in spark mode')\n            pass\n\n        if engine is None:\n            raise ValueError('Must provide a SparkContext')\n\n        return fromarray(self.toarray(), engine=engine)", "category": "Python"}, {"instruction": "def create_message(username, message):\r\n    \"\"\" Creates a standard message from a given user with the message \r\n        Replaces newline with html break \"\"\"\n", "input": "", "output": "    message = message.replace('\\n', '<br/>')\r\n    return '{{\"service\":1, \"data\":{{\"message\":\"{mes}\", \"username\":\"{user}\"}} }}'.format(mes=message, user=username)", "category": "Python"}, {"instruction": "def update(self, *args, **kwargs):\n        \"\"\"Update the dictionary from *other*, overwriting existing keys.\"\"\"\n", "input": "", "output": "        self._extend(args, kwargs, 'update', self._update_items)", "category": "Python"}, {"instruction": "def add_member(self, user):\n        \"\"\"\n        Add member to team\n        :param user: User object or user's username\n        :return: Added user.\n        \"\"\"\n", "input": "", "output": "        user = Transform.to_user(user)\n        data = {\n            'id': user\n        }\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {\n                'id': self.id,\n                'data': data,\n            }\n        }\n        logger.info('Adding team member using id', extra=extra)\n        response = self._api.post(\n            url=self._URL['members_query'].format(id=self.id), data=data)\n        member_data = response.json()\n        return TeamMember(api=self._api, **member_data)", "category": "Python"}, {"instruction": "def add_bid(self, bid):\n        \"\"\"Add a bid to the cache\n\n        :param bid:\n        :return:\n\n        \"\"\"\n", "input": "", "output": "        self._bids[bid[0]] = float(bid[1])\n        if bid[1] == \"0.00000000\":\n            del self._bids[bid[0]]", "category": "Python"}, {"instruction": "def _send_config(self):\n        \"\"\"Send configuration data to the hottop.\n\n        :returns: bool\n        :raises: Generic exceptions if an error is identified.\n        \"\"\"\n", "input": "", "output": "        serialized = self._generate_config()\n        self._log.debug(\"Configuration has been serialized\")\n        try:\n            self._conn.flushInput()\n            self._conn.flushOutput()\n            self._conn.write(serialized)\n            return True\n        except Exception as e:\n            self._log.error(e)\n            raise Exception(e)", "category": "Python"}, {"instruction": "def get_min_instability(self, min_voltage=None, max_voltage=None):\n        \"\"\"\n        The minimum instability along a path for a specific voltage range.\n\n        Args:\n            min_voltage: The minimum allowable voltage.\n            max_voltage: The maximum allowable voltage.\n\n        Returns:\n            Minimum decomposition energy of all compounds along the insertion\n            path (a subset of the path can be chosen by the optional arguments)\n        \"\"\"\n", "input": "", "output": "        data = []\n        for pair in self._select_in_voltage_range(min_voltage, max_voltage):\n            if pair.decomp_e_charge is not None:\n                data.append(pair.decomp_e_charge)\n            if pair.decomp_e_discharge is not None:\n                data.append(pair.decomp_e_discharge)\n        return min(data) if len(data) > 0 else None", "category": "Python"}, {"instruction": "def Handle(self, unused_args, token=None):\n    \"\"\"Build the data structure representing the config.\"\"\"\n", "input": "", "output": "\n    sections = {}\n    for descriptor in config.CONFIG.type_infos:\n      if descriptor.section in sections:\n        continue\n\n      section_data = {}\n      for parameter in self._ListParametersInSection(descriptor.section):\n        section_data[parameter] = ApiConfigOption().InitFromConfigOption(\n            parameter)\n\n      sections[descriptor.section] = section_data\n\n    result = ApiGetConfigResult()\n    for section_name in sorted(sections):\n      section = sections[section_name]\n\n      api_section = ApiConfigSection(name=section_name)\n      api_section.options = []\n      for param_name in sorted(section):\n        api_section.options.append(section[param_name])\n      result.sections.append(api_section)\n\n    return result", "category": "Python"}, {"instruction": "def get_cached(self, link, default=None):\n        '''Retrieves a cached navigator from the id_map.\n\n        Either a Link object or a bare uri string may be passed in.'''\n", "input": "", "output": "        if hasattr(link, 'uri'):\n            return self.id_map.get(link.uri, default)\n        else:\n            return self.id_map.get(link, default)", "category": "Python"}, {"instruction": "def pci_contents(self, use_dict=None, as_class=dict):\n        \"\"\"Return the contents of an object as a dict.\"\"\"\n", "input": "", "output": "        if _debug: PCI._debug(\"pci_contents use_dict=%r as_class=%r\", use_dict, as_class)\n\n        # make/extend the dictionary of content\n        if use_dict is None:\n            use_dict = as_class()\n\n        # call the parent class\n        _PCI.pci_contents(self, use_dict=use_dict, as_class=as_class)\n\n        # save the values\n        use_dict.__setitem__('expectingReply', self.pduExpectingReply)\n        use_dict.__setitem__('networkPriority', self.pduNetworkPriority)\n\n        # return what we built/updated\n        return use_dict", "category": "Python"}, {"instruction": "def QA_util_dict_remove_key(dicts, key):\n    \"\"\"\n    \u8f93\u5165\u4e00\u4e2adict \u8fd4\u56de\u5220\u9664\u540e\u7684\n    \"\"\"\n", "input": "", "output": "\n    if isinstance(key, list):\n        for item in key:\n            try:\n                dicts.pop(item)\n            except:\n                pass\n    else:\n        try:\n            dicts.pop(key)\n        except:\n            pass\n    return dicts", "category": "Python"}, {"instruction": "def total_length(self):\n        \"\"\"Returns the total length of the captions.\"\"\"\n", "input": "", "output": "        if not self._captions:\n            return 0\n        return int(self._captions[-1].end_in_seconds) - int(self._captions[0].start_in_seconds)", "category": "Python"}, {"instruction": "def export_base_dts(cls, graph, obj, nsm):\n        \"\"\" Export the base DTS information in a simple reusable way\n\n        :param graph: Current graph where the information lie\n        :param obj: Object for which we build info\n        :param nsm: Namespace manager\n        :return: Dict\n        \"\"\"\n", "input": "", "output": "\n        o = {\n            \"@id\": str(obj.asNode()),\n            \"@type\": nsm.qname(obj.type),\n            nsm.qname(RDF_NAMESPACES.HYDRA.title): str(obj.get_label()),\n            nsm.qname(RDF_NAMESPACES.HYDRA.totalItems): obj.size\n        }\n\n        for desc in graph.objects(obj.asNode(), RDF_NAMESPACES.HYDRA.description):\n            o[nsm.qname(RDF_NAMESPACES.HYDRA.description)] = str(desc)\n\n        return o", "category": "Python"}, {"instruction": "def fetch_projects(self, **kwargs):\n        \"\"\"\n        List projects owned\n        Fetch projects that the currently authenticated user has access to because he or she is the owner of the project.\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please define a `callback` function\n        to be invoked when receiving the response.\n        >>> def callback_function(response):\n        >>>     pprint(response)\n        >>>\n        >>> thread = api.fetch_projects(callback=callback_function)\n\n        :param callback function: The callback function\n            for asynchronous request. (optional)\n        :return: PaginatedProjectResults\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('callback'):\n            return self.fetch_projects_with_http_info(**kwargs)\n        else:\n            (data) = self.fetch_projects_with_http_info(**kwargs)\n            return data", "category": "Python"}, {"instruction": "def backend_from_mime(mime):\n    \"\"\"Determine backend module object from a mime string.\"\"\"\n", "input": "", "output": "    try:\n        mod_name = MIMETYPE_TO_BACKENDS[mime]\n\n    except KeyError:\n        msg = \"No handler for %r, defaulting to %r\" % (mime, DEFAULT_MIME)\n        if 'FULLTEXT_TESTING' in os.environ:\n            warn(msg)\n        else:\n            LOGGER.debug(msg)\n\n        mod_name = MIMETYPE_TO_BACKENDS[DEFAULT_MIME]\n    mod = import_mod(mod_name)\n    return mod", "category": "Python"}, {"instruction": "def semicovariance(prices, benchmark=0, frequency=252):\n    \"\"\"\n    Estimate the semicovariance matrix, i.e the covariance given that\n    the returns are less than the benchmark.\n\n    .. semicov = E([min(r_i - B, 0)] . [min(r_j - B, 0)])\n\n    :param prices: adjusted closing prices of the asset, each row is a date\n                   and each column is a ticker/id.\n    :type prices: pd.DataFrame\n    :param benchmark: the benchmark return, defaults to 0.\n    :type benchmark: float\n    :param frequency: number of time periods in a year, defaults to 252 (the number\n                      of trading days in a year)\n    :type frequency: int, optional\n    :return: semicovariance matrix\n    :rtype: pd.DataFrame\n    \"\"\"\n", "input": "", "output": "    if not isinstance(prices, pd.DataFrame):\n        warnings.warn(\"prices are not in a dataframe\", RuntimeWarning)\n        prices = pd.DataFrame(prices)\n    daily_returns = daily_price_returns(prices)\n    drops = np.fmin(daily_returns - benchmark, 0)\n    return drops.cov() * frequency", "category": "Python"}, {"instruction": "def recode(self, table: pd.DataFrame, validate=False) -> pd.DataFrame:\n        \"\"\"Pass the appropriate columns through each recoder function sequentially and return the final result.\n\n        Args:\n            table (pd.DataFrame): A dataframe on which to apply recoding logic.\n            validate (bool): If ``True``, recoded table must pass validation tests.\n        \"\"\"\n", "input": "", "output": "        return self._recode_output(self._recode_input(table, validate=validate), validate=validate)", "category": "Python"}, {"instruction": "def _get_mdm(cls, df, windows):\r\n        \"\"\" -DM, negative directional moving accumulation\r\n\r\n        If window is not 1, return the SMA of -DM.\r\n        :param df: data\r\n        :param windows: range\r\n        :return:\r\n        \"\"\"\n", "input": "", "output": "        window = cls.get_only_one_positive_int(windows)\r\n        column_name = 'mdm_{}'.format(window)\r\n        um, dm = df['um'], df['dm']\r\n        df['mdm'] = np.where(dm > um, dm, 0)\r\n        if window > 1:\r\n            mdm = df['mdm_{}_ema'.format(window)]\r\n        else:\r\n            mdm = df['mdm']\r\n        df[column_name] = mdm", "category": "Python"}, {"instruction": "def _model_fit_to_table(fit):\n    \"\"\"\n    Produce a pandas DataFrame of model fit results from a statsmodels\n    fit result object.\n\n    Parameters\n    ----------\n    fit : statsmodels.regression.linear_model.RegressionResults\n\n    Returns\n    -------\n    fit_parameters : pandas.DataFrame\n        Will have columns 'Coefficient', 'Std. Error', and 'T-Score'.\n        Index will be model terms.\n\n        This frame will also have non-standard attributes\n        .rsquared and .rsquared_adj with the same meaning and value\n        as on `fit`.\n\n    \"\"\"\n", "input": "", "output": "    fit_parameters = pd.DataFrame(\n        {'Coefficient': fit.params,\n         'Std. Error': fit.bse,\n         'T-Score': fit.tvalues})\n    fit_parameters.rsquared = fit.rsquared\n    fit_parameters.rsquared_adj = fit.rsquared_adj\n    return fit_parameters", "category": "Python"}, {"instruction": "def to_string(self, buf=None, format_abs_ref_as='string',\n                  upper_triangle=True, header=True, index=True, **kwargs):\n        \"\"\"Render a DataFrame to a console-friendly tabular output.\n\n        Wrapper around the :meth:`pandas.DataFrame.to_string` method.\n        \"\"\"\n", "input": "", "output": "        out = self._sympy_formatter()\n        out = out._abs_ref_formatter(format_as=format_abs_ref_as)\n        if not upper_triangle:\n            out = out._remove_upper_triangle()\n\n        content = out._frame.to_string(buf=buf, header=header, index=index,\n                                       **kwargs)\n        if not index and not header:\n            # NOTE(the following might be removed in the future\n            # introduced because of formatting bug in pandas\n            # See https://github.com/pandas-dev/pandas/issues/13032)\n            space = ' ' * (out.loc[:, 'atom'].str.len().max()\n                           - len(out.iloc[0, 0]))\n            content = space + content\n        return content", "category": "Python"}, {"instruction": "def login_as_bot():\n\t\"\"\"\n\tLogin as the bot account \"octogrid\", if user isn't authenticated on Plotly\n\t\"\"\"\n", "input": "", "output": "\n\tplotly_credentials_file = join(\n    \tjoin(expanduser('~'), PLOTLY_DIRECTORY), PLOTLY_CREDENTIALS_FILENAME)\n\n\tif isfile(plotly_credentials_file):\n\t\twith open(plotly_credentials_file, 'r') as f:\n\t\t\tcredentials = loads(f.read())\n\n\t\tif (credentials['username'] == '' or credentials['api_key'] == ''):\n\t\t\tplotly.sign_in(BOT_USERNAME, BOT_API_KEY)\n\telse:\n\t\tplotly.sign_in(BOT_USERNAME, BOT_API_KEY)", "category": "Python"}, {"instruction": "def make_article_info_correspondences(self, article_info_div):\n        \"\"\"\n        Articles generally provide a first contact, typically an email address\n        for one of the authors. This will supply that content.\n        \"\"\"\n", "input": "", "output": "        corresps = self.article.root.xpath('./front/article-meta/author-notes/corresp')\n        if corresps:\n            corresp_div = etree.SubElement(article_info_div,\n                                           'div',\n                                           {'id': 'correspondence'})\n        for corresp in corresps:\n            sub_div = etree.SubElement(corresp_div,\n                                       'div',\n                                       {'id': corresp.attrib['id']})\n            append_all_below(sub_div, corresp)", "category": "Python"}, {"instruction": "def _dasd_reverse_conversion(cls, val, **kwargs):\n        '''\n        converts DASD String values to the reg_sz value\n        '''\n", "input": "", "output": "        if val is not None:\n            if val.upper() == 'ADMINISTRATORS':\n                # \"\" also shows 'administrators' in the GUI\n                return '0'\n            elif val.upper() == 'ADMINISTRATORS AND POWER USERS':\n                return '1'\n            elif val.upper() == 'ADMINISTRATORS AND INTERACTIVE USERS':\n                return '2'\n            elif val.upper() == 'NOT DEFINED':\n                # a setting of anything other than nothing, 0, 1, 2 or if it\n                # doesn't exist show 'not defined'\n                return '9999'\n            else:\n                return 'Invalid Value'\n        else:\n            return 'Not Defined'", "category": "Python"}, {"instruction": "def getUser(cls, cloud):\n        \"\"\"\n        gets the username for a specified cloud.\n        TODO: works currently only for opensatck.\n        :param cloud: the name of the cloud\n        :return: \n        \"\"\"\n", "input": "", "output": "        try:\n            config = d = ConfigDict(\"cloudmesh.yaml\")\n\n            d = ConfigDict(\"cloudmesh.yaml\")\n\n            #\n            # bug: cloud is none when adding a group\n            #\n\n            config = d[\"cloudmesh\"][\"clouds\"][cloud]\n            credentials = config[\"credentials\"]\n            cloud_type = config[\"cm_type\"]\n\n            if cloud_type == \"openstack\":\n                return credentials[\"OS_USERNAME\"]\n            else:\n                raise ValueError(\"getUser for this cloud type not yet \"\n                                 \"supported: {}\".format(cloud))\n        except Exception as ex:\n            Console.error(\"problem getting user\")", "category": "Python"}, {"instruction": "def replacenode(self, othereplus, node):\n        \"\"\"replace the node here with the node from othereplus\"\"\"\n", "input": "", "output": "        node = node.upper()\n        self.dt[node.upper()] = othereplus.dt[node.upper()]", "category": "Python"}, {"instruction": "def median_grouped(name, num, minimum=0, maximum=0, ref=None):\n    '''\n    Calculates the grouped mean of the ``num`` most recent values. Requires a\n    list.\n\n    USAGE:\n\n    .. code-block:: yaml\n\n        foo:\n          calc.median_grouped:\n            - name: myregentry\n            - num: 5\n    '''\n", "input": "", "output": "    return calc(\n        name=name,\n        num=num,\n        oper='median_grouped',\n        minimum=minimum,\n        maximum=maximum,\n        ref=ref\n    )", "category": "Python"}, {"instruction": "def AuxPlane(s1, d1, r1):\n    \"\"\"\n    Get Strike and dip of second plane.\n\n    Adapted from MATLAB script\n    `bb.m <http://www.ceri.memphis.edu/people/olboyd/Software/Software.html>`_\n    written by Andy Michael and Oliver Boyd.\n    \"\"\"\n", "input": "", "output": "    r2d = 180 / np.pi\n\n    z = (s1 + 90) / r2d\n    z2 = d1 / r2d\n    z3 = r1 / r2d\n    # slick vector in plane 1\n    sl1 = -np.cos(z3) * np.cos(z) - np.sin(z3) * np.sin(z) * np.cos(z2)\n    sl2 = np.cos(z3) * np.sin(z) - np.sin(z3) * np.cos(z) * np.cos(z2)\n    sl3 = np.sin(z3) * np.sin(z2)\n    (strike, dip) = StrikeDip(sl2, sl1, sl3)\n\n    n1 = np.sin(z) * np.sin(z2)  # normal vector to plane 1\n    n2 = np.cos(z) * np.sin(z2)\n    h1 = -sl2  # strike vector of plane 2\n    h2 = sl1\n    # note h3=0 always so we leave it out\n    # n3 = np.cos(z2)\n\n    z = h1 * n1 + h2 * n2\n    z = z / np.sqrt(h1 * h1 + h2 * h2)\n    z = np.arccos(z)\n    rake = 0\n    if sl3 > 0:\n        rake = z * r2d\n    if sl3 <= 0:\n        rake = -z * r2d\n    return (strike, dip, rake)", "category": "Python"}, {"instruction": "def connect(self, (host, port)):\n        '''\n        Connect assuming a host and port tuple. Implemented as non-blocking,\n        and will close the transport if there's an error\n        '''\n", "input": "", "output": "        self._host = \"%s:%s\" % (host, port)\n        self._sock = EventSocket(\n            read_cb=self._sock_read_cb,\n            close_cb=self._sock_close_cb,\n            error_cb=self._sock_error_cb,\n            debug=self.connection.debug,\n            logger=self.connection.logger)\n        if self.connection._sock_opts:\n            for k, v in self.connection._sock_opts.iteritems():\n                family, type = k\n                self._sock.setsockopt(family, type, v)\n        self._sock.setblocking(False)\n        self._sock.connect(\n            (host, port), timeout=self.connection._connect_timeout)\n        self._heartbeat_timeout = None", "category": "Python"}, {"instruction": "def has_parent_logs(self, log_id):\n        \"\"\"Tests if the ``Log`` has any parents.\n\n        arg:    log_id (osid.id.Id): the ``Id`` of a log\n        return: (boolean) - ``true`` if the log has parents, ``false``\n                otherwise\n        raise:  NotFound - ``log_id`` is not found\n        raise:  NullArgument - ``log_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.BinHierarchySession.has_parent_bins\n        if self._catalog_session is not None:\n            return self._catalog_session.has_parent_catalogs(catalog_id=log_id)\n        return self._hierarchy_session.has_parents(id_=log_id)", "category": "Python"}, {"instruction": "def graph_png(self):\n        \"\"\"\n        Export a graph of the data in png format using graphviz/dot.\n        \"\"\"\n", "input": "", "output": "        if not self.out_file:\n            ui.error(c.MESSAGES[\"png_missing_out\"])\n            sys.exit(1)\n\n        cli_flags = \"-Gsize='{0}' -Gdpi='{1}' {2} \".format(self.size, self.dpi,\n                                                           self.flags)\n        cli_flags += \"-o {0}\".format(self.out_file)\n\n        (out, err) = utils.capture_shell(\n            \"ansigenome export -t graph -f dot | dot -Tpng {0}\"\n            .format(cli_flags))\n\n        if err:\n            ui.error(err)", "category": "Python"}, {"instruction": "def siteblock(parser, token):\n    \"\"\"Two notation types are acceptable:\n\n        1. Two arguments:\n           {% siteblock \"myblock\" %}\n           Used to render \"myblock\" site block.\n           \n        2. Four arguments:\n           {% siteblock \"myblock\" as myvar %}\n           Used to put \"myblock\" site block into \"myvar\" template variable.\n           \n    \"\"\"\n", "input": "", "output": "    tokens = token.split_contents()\n    tokens_num = len(tokens)\n\n    if tokens_num not in (2, 4):\n        raise template.TemplateSyntaxError(\n            '%r tag requires two or four arguments. '\n            'E.g.: {%% siteblock \"myblock\" %%} or {%% siteblock \"myblock\" as myvar %%}.' % tokens[0])\n\n    block_alias = parser.compile_filter(tokens[1])\n    as_var = None\n    tokens = tokens[2:]\n    if len(tokens) >= 2 and tokens[-2] == 'as':\n        as_var = tokens[-1]\n\n    return siteblockNode(block_alias, as_var)", "category": "Python"}, {"instruction": "def performFirmwareUpdate(self, unDeviceIndex):\n        \"\"\"\n        Performs the actual firmware update if applicable. \n        The following events will be sent, if VRFirmwareError_None was returned: VREvent_FirmwareUpdateStarted, VREvent_FirmwareUpdateFinished \n        Use the properties Prop_Firmware_UpdateAvailable_Bool, Prop_Firmware_ManualUpdate_Bool, and Prop_Firmware_ManualUpdateURL_String\n        to figure our whether a firmware update is available, and to figure out whether its a manual update \n        Prop_Firmware_ManualUpdateURL_String should point to an URL describing the manual update process\n        \"\"\"\n", "input": "", "output": "\n        fn = self.function_table.performFirmwareUpdate\n        result = fn(unDeviceIndex)\n        return result", "category": "Python"}, {"instruction": "def connect_to_cloud_cdn(region=None):\n    \"\"\"Creates a client for working with cloud loadbalancers.\"\"\"\n", "input": "", "output": "    global default_region\n    # (nicholaskuechler/keekz) 2017-11-30 - Not a very elegant solution...\n    # Cloud CDN only exists in 2 regions: DFW and LON\n    # But this isn't playing nicely with the identity service catalog results.\n    # US auth based regions (DFW, ORD, IAD, SYD, HKG) need to use CDN in DFW\n    # UK auth based regions (LON) need to use CDN in LON\n    if region in ['DFW', 'IAD', 'ORD', 'SYD', 'HKG']:\n        return _create_client(ep_name=\"cdn\", region=\"DFW\")\n    elif region in ['LON']:\n        return _create_client(ep_name=\"cdn\", region=\"LON\")\n    else:\n        if default_region in ['DFW', 'IAD', 'ORD', 'SYD', 'HKG']:\n            return _create_client(ep_name=\"cdn\", region=\"DFW\")\n        elif default_region in ['LON']:\n            return _create_client(ep_name=\"cdn\", region=\"LON\")\n        else:\n            return _create_client(ep_name=\"cdn\", region=region)", "category": "Python"}, {"instruction": "def _py_outvar(parameter, lparams, tab):\n    \"\"\"Returns the code to produce a ctypes output variable for interacting with fortran.\n    \"\"\"\n", "input": "", "output": "    if (\"out\" in parameter.direction and parameter.D > 0 and \":\" in parameter.dimension and\n        (\"allocatable\" in parameter.modifiers or \"pointer\" in parameter.modifiers)):\n        lparams.append(\"byref({}_o)\".format(parameter.lname))\n        blank = True if parameter.direction == \"(inout)\" else False\n        return (\"{0}_o = POINTER({1})()\".format(parameter.lname, _py_ctype(parameter)), blank)", "category": "Python"}, {"instruction": "def _parse_title_url(html_chunk):\n    \"\"\"\n    Parse title/name of the book and URL of the book.\n\n    Args:\n        html_chunk (obj): HTMLElement containing slice of the page with details.\n\n    Returns:\n        tuple: (title, url), both as strings.\n    \"\"\"\n", "input": "", "output": "    title = html_chunk.find(\"div\", {\"class\": \"comment\"})\n\n    if not title:\n        return _parse_alt_title(html_chunk), None\n\n    title = title[0].find(\"h2\")\n    if not title:\n        return _parse_alt_title(html_chunk), None\n\n    # look for the url of the book if present\n    url = None\n    url_tag = title[0].find(\"a\")\n    if url_tag:\n        url = url_tag[0].params.get(\"href\", None)\n        title = url_tag\n\n    return title[0].getContent(), normalize_url(BASE_URL, url)", "category": "Python"}, {"instruction": "def clear_all_breakpoints(self):\r\n        \"\"\"Clear breakpoints in all files\"\"\"\n", "input": "", "output": "        self.switch_to_plugin()\r\n        clear_all_breakpoints()\r\n        self.breakpoints_saved.emit()\r\n        editorstack = self.get_current_editorstack()\r\n        if editorstack is not None:\r\n            for data in editorstack.data:\r\n                data.editor.debugger.clear_breakpoints()\r\n        self.refresh_plugin()", "category": "Python"}, {"instruction": "def redraw_tiles(self, surface):\n        \"\"\" redraw the visible portion of the buffer -- it is slow.\n        \"\"\"\n", "input": "", "output": "        # TODO/BUG: Redraw animated tiles correctly.  They are getting reset here\n        logger.warning('pyscroll buffer redraw')\n        self._clear_surface(self._buffer)\n        self._tile_queue = self.data.get_tile_images_by_rect(self._tile_view)\n        self._flush_tile_queue(surface)", "category": "Python"}, {"instruction": "def list_team_codes():\n    \"\"\"List team names in alphabetical order of team ID, per league.\"\"\"\n", "input": "", "output": "    # Sort teams by league, then alphabetical by code\n    cleanlist = sorted(TEAM_DATA, key=lambda k: (k[\"league\"][\"name\"], k[\"code\"]))\n    # Get league names\n    leaguenames = sorted(list(set([team[\"league\"][\"name\"] for team in cleanlist])))\n    for league in leaguenames:\n        teams = [team for team in cleanlist if team[\"league\"][\"name\"] == league]\n        click.secho(league, fg=\"green\", bold=True)\n        for team in teams:\n            if team[\"code\"] != \"null\":\n                click.secho(u\"{0}: {1}\".format(team[\"code\"], team[\"name\"]), fg=\"yellow\")\n        click.secho(\"\")", "category": "Python"}, {"instruction": "def value(self):\n        \"\"\"Retrieve the data value of this attachment.\n\n        Will show the filename of the attachment if there is an attachment available otherwise None\n        Use save_as in order to download as a file.\n\n        Example\n        -------\n\n        >>> file_attachment_property = project.part('Bike').property('file_attachment')\n        >>> if file_attachment_property.value:\n        ...     file_attachment_property.save_as('file.ext')\n        ... else:\n        ...     print('file attachment not set, its value is None')\n\n        \"\"\"\n", "input": "", "output": "        if 'value' in self._json_data and self._json_data['value']:\n            return \"[Attachment: {}]\".format(self._json_data['value'].split('/')[-1])\n        else:\n            return None", "category": "Python"}, {"instruction": "def _make_object(name):\n    \"\"\"Create a generic object for the tags.\"\"\"\n", "input": "", "output": "    klass = type(name, (SWFObject,),\n                 {'__str__': _str, '__repr__': _repr, 'name': name})\n    return klass()", "category": "Python"}, {"instruction": "def reaction_signature(eq, direction=False, stoichiometry=False):\n    \"\"\"Return unique signature object for :class:`Reaction`.\n\n    Signature objects are hashable, and compare equal only if the reactions\n    are considered the same according to the specified rules.\n\n    Args:\n        direction: Include reaction directionality when considering equality.\n        stoichiometry: Include stoichiometry when considering equality.\n    \"\"\"\n", "input": "", "output": "    def compounds_sig(compounds):\n        if stoichiometry:\n            return tuple(sorted(compounds))\n        else:\n            return tuple(sorted(compound for compound, _ in compounds))\n\n    left = compounds_sig(eq.left)\n    right = compounds_sig(eq.right)\n\n    if left < right:\n        reaction_sig = left, right\n        direction_sig = eq.direction\n    else:\n        reaction_sig = right, left\n        direction_sig = eq.direction.flipped()\n\n    if direction:\n        return reaction_sig, direction_sig\n    return reaction_sig", "category": "Python"}, {"instruction": "def _get_release_params():\n    \"\"\"Gets the release params from the current request.\"\"\"\n", "input": "", "output": "    release_name = request.form.get('release_name')\n    utils.jsonify_assert(release_name, 'release_name required')\n    release_number = request.form.get('release_number', type=int)\n    utils.jsonify_assert(release_number is not None, 'release_number required')\n    return release_name, release_number", "category": "Python"}, {"instruction": "def save_variation_for_experiment(self, experiment_id, variation_id):\n    \"\"\" Helper method to save new experiment/variation as part of the user's profile.\n\n    Args:\n      experiment_id: ID for experiment for which the decision is to be stored.\n      variation_id: ID for variation that the user saw.\n    \"\"\"\n", "input": "", "output": "\n    self.experiment_bucket_map.update({\n      experiment_id: {\n        self.VARIATION_ID_KEY: variation_id\n      }\n    })", "category": "Python"}, {"instruction": "def application_2_json(self):\n        \"\"\"\n        transform ariane_clip3 Application object to Ariane server JSON obj\n        :return: Ariane JSON obj\n        \"\"\"\n", "input": "", "output": "        LOGGER.debug(\"Application.application_2_json\")\n        json_obj = {\n            'applicationID': self.id,\n            'applicationName': self.name,\n            'applicationDescription': self.description,\n            'applicationShortName': self.short_name,\n            'applicationColorCode': self.color_code,\n            'applicationCompanyID': self.company_id,\n            'applicationTeamID': self.team_id,\n            'applicationOSInstancesID': self.osi_ids\n        }\n        return json.dumps(json_obj)", "category": "Python"}, {"instruction": "def first(self):\n        \"\"\"Return a (value, source) pair for the first object found for\n        this view. This amounts to the first element returned by\n        `resolve`. If no values are available, a NotFoundError is\n        raised.\n        \"\"\"\n", "input": "", "output": "        pairs = self.resolve()\n        try:\n            return iter_first(pairs)\n        except ValueError:\n            raise NotFoundError(u\"{0} not found\".format(self.name))", "category": "Python"}, {"instruction": "def _parse_errback(self, error):\n        \"\"\"\n        Parse an error from an XML-RPC call.\n\n        raises: ``IOError`` when the Twisted XML-RPC connection times out.\n        raises: ``KojiException`` if we got a response from the XML-RPC\n                server but it is not one of the ``xmlrpc.Fault``s that\n                we know about.\n        raises: ``Exception`` if it is not one of the above.\n        \"\"\"\n", "input": "", "output": "        if isinstance(error.value, IOError):\n            raise error.value\n        if hasattr(xmlrpc, 'Fault'):  # Python 2:\n            fault = xmlrpc.Fault\n        else:\n            fault = xmlrpc.client.Fault\n        if isinstance(error.value, fault):\n            # TODO: specific errors here, see koji/__init__.py\n            if error.value.faultCode >= 1000 and error.value.faultCode <= 1022:\n                raise KojiException(error.value.faultString)\n            raise KojiException(error.value)\n        # We don't know what this is, so just raise it.\n        raise error", "category": "Python"}, {"instruction": "def objective_to_model(self, x_objective):\n        ''' This function serves as interface between objective input vectors and\n        model input vectors'''\n", "input": "", "output": "\n        x_model = []\n\n        for k in range(self.objective_dimensionality):\n            variable = self.space_expanded[k]\n            new_entry = variable.objective_to_model(x_objective[0,k])\n            x_model += new_entry\n\n        return x_model", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"\n        Run the plugin.\n        \"\"\"\n", "input": "", "output": "\n        workspace = self.workflow.plugin_workspace.get(self.key, {})\n        cf_maps_to_remove = workspace.get('cf_maps_to_remove', [])\n        for cm_key, osbs in cf_maps_to_remove:\n            try:\n                osbs.delete_config_map(cm_key)\n                self.log.debug(\"ConfigMap %s deleted\", cm_key)\n            except OsbsResponseException as ex:\n                self.log.warning(\"Failed to delete ConfigMap %s: %s\", cm_key, ex)", "category": "Python"}, {"instruction": "def delete_subscription(self, subscription_id):\n        \"\"\"DeleteSubscription.\n        Delete a specific service hooks subscription.\n        :param str subscription_id: ID for a subscription.\n        \"\"\"\n", "input": "", "output": "        route_values = {}\n        if subscription_id is not None:\n            route_values['subscriptionId'] = self._serialize.url('subscription_id', subscription_id, 'str')\n        self._send(http_method='DELETE',\n                   location_id='fc50d02a-849f-41fb-8af1-0a5216103269',\n                   version='5.0',\n                   route_values=route_values)", "category": "Python"}, {"instruction": "def mode_interactive(options):\n    \"\"\"Interactive Mode: terminal prompts repeatedly for a url to fetch\"\"\"\n", "input": "", "output": "    articles = set()\n    failures = set()\n\n    url = input('Enter a URL: ')\n    while url != '':\n        article = _get_article(url=url, bodyLines=options.bodyLines, debug=options.debug)\n        if (article):\n            articles.add(article)\n        else:\n            failures.add(url)\n        url = input('Enter a URL (press enter to end): ')\n\n    _output(articles, options.outputFile, failures, options.failureFile)", "category": "Python"}, {"instruction": "def temporary_file(mode):\n    \"\"\"Cross platform temporary file creation.\n\n    This is an alternative to ``tempfile.NamedTemporaryFile`` that\n    also works on windows and avoids the \"file being used by\n    another process\" error.\n    \"\"\"\n", "input": "", "output": "    tempdir = tempfile.gettempdir()\n    basename = 'tmpfile-%s' % (uuid.uuid4())\n    full_filename = os.path.join(tempdir, basename)\n    if 'w' not in mode:\n        # We need to create the file before we can open\n        # it in 'r' mode.\n        open(full_filename, 'w').close()\n    try:\n        with open(full_filename, mode) as f:\n            yield f\n    finally:\n        os.remove(f.name)", "category": "Python"}, {"instruction": "def load_cfg(path, envvar_prefix='LIBREANT_', debug=False):\n    '''wrapper of config_utils.load_configs'''\n", "input": "", "output": "    try:\n        return load_configs(envvar_prefix, path=path)\n    except Exception as e:\n        if debug:\n            raise\n        else:\n            die(str(e))", "category": "Python"}, {"instruction": "def ot_find_tree(arg_dict, exact=True, verbose=False, oti_wrapper=None):\n    \"\"\"Uses a peyotl wrapper around an Open Tree web service to get a list of trees including values `value` for a given property to be searched on `porperty`.\n\n    The oti_wrapper can be None (in which case the default wrapper from peyotl.sugar will be used.\n    All other arguments correspond to the arguments of the web-service call.\n    \"\"\"\n", "input": "", "output": "    if oti_wrapper is None:\n        from peyotl.sugar import oti\n        oti_wrapper = oti\n    return oti_wrapper.find_trees(arg_dict,\n                                  exact=exact,\n                                  verbose=verbose,\n                                  wrap_response=True)", "category": "Python"}, {"instruction": "def isBlockComment(self, block, column):\n        \"\"\"Check if character at column is a block comment\n        \"\"\"\n", "input": "", "output": "        dataObject = block.userData()\n        data = dataObject.data if dataObject is not None else None\n        return self._syntax.isBlockComment(data, column)", "category": "Python"}, {"instruction": "def tcl_list_2_py_list(tcl_list, within_tcl_str=False):\n    \"\"\" Convert Tcl list to Python list using Tcl interpreter.\n\n    :param tcl_list: string representing the Tcl string.\n    :param within_tcl_str: True - Tcl list is embedded within Tcl str. False - native Tcl string.\n    :return: Python list equivalent to the Tcl ist.\n    :rtye: list\n    \"\"\"\n", "input": "", "output": "\n    if not within_tcl_str:\n        tcl_list = tcl_str(tcl_list)\n    return tcl_interp_g.eval('join ' + tcl_list + ' LiStSeP').split('LiStSeP') if tcl_list else []", "category": "Python"}, {"instruction": "def upgrade():\n    '''\n    \"Upgrade\" packages\n    '''\n", "input": "", "output": "    DETAILS = _load_state()\n    pkgs = uptodate()\n    DETAILS['packages'] = pkgs\n    _save_state(DETAILS)\n    return pkgs", "category": "Python"}, {"instruction": "def clear_caches():  # suppress(unused-function)\n    \"\"\"Clear all caches.\"\"\"\n", "input": "", "output": "    for _, reader in _spellchecker_cache.values():\n        reader.close()\n\n    _spellchecker_cache.clear()\n    _valid_words_cache.clear()\n    _user_dictionary_cache.clear()", "category": "Python"}, {"instruction": "def get_solr_date(pydate, is_negative):\n    \"\"\"\n    Returns a date in a valid Solr format from a string.\n    \"\"\"\n", "input": "", "output": "    # check if date is valid and then set it to solr format YYYY-MM-DDThh:mm:ssZ\n    try:\n        if isinstance(pydate, datetime.datetime):\n            solr_date = '%sZ' % pydate.isoformat()[0:19]\n            if is_negative:\n                LOGGER.debug('%s This layer has a negative date' % solr_date)\n                solr_date = '-%s' % solr_date\n            return solr_date\n        else:\n            return None\n    except Exception, e:\n        LOGGER.error(e, exc_info=True)\n        return None", "category": "Python"}, {"instruction": "def _get_existing_logical_drives(raid_adapter):\n    \"\"\"Collect existing logical drives on the server.\n\n    :param raid_adapter: raid adapter info\n    :returns: existing_logical_drives: get logical drive on server\n    \"\"\"\n", "input": "", "output": "    existing_logical_drives = []\n    logical_drives = raid_adapter['Server']['HWConfigurationIrmc'][\n        'Adapters']['RAIDAdapter'][0].get('LogicalDrives')\n    if logical_drives is not None:\n        for drive in logical_drives['LogicalDrive']:\n            existing_logical_drives.append(drive['@Number'])\n\n    return existing_logical_drives", "category": "Python"}, {"instruction": "def submit_form_id(step, id):\n    \"\"\"\n    Submit the form having given id.\n    \"\"\"\n", "input": "", "output": "    form = world.browser.find_element_by_xpath(str('id(\"{id}\")'.format(id=id)))\n    form.submit()", "category": "Python"}, {"instruction": "def load(self, filename=None):\n        \"\"\" load runtime configuration from given filename.\n        If filename is None try to read from default file from\n        default location. \"\"\"\n", "input": "", "output": "        if not filename:\n            filename = self.default_config_file\n\n        files = self._cfgs_to_read()\n        # insert last, so it will override all values,\n        # which have already been set in previous files.\n        files.insert(-1, filename)\n\n        try:\n            config = self.__read_cfg(files)\n        except ReadConfigException as e:\n            print(Config._format_msg('config.load(\"{file}\") failed with {error}'.format(file=filename, error=e)))\n        else:\n            self._conf_values = config\n\n        # notice user?\n        if self.show_config_notification and not self.cfg_dir:\n            print(Config._format_msg(\"no configuration directory set or usable.\"\n                                      \" Falling back to defaults.\"))", "category": "Python"}, {"instruction": "def lookup_command(cmdname, mode):\n    \"\"\"\n    returns commandclass, argparser and forced parameters used to construct\n    a command for `cmdname` when called in `mode`.\n\n    :param cmdname: name of the command to look up\n    :type cmdname: str\n    :param mode: mode identifier\n    :type mode: str\n    :rtype: (:class:`Command`, :class:`~argparse.ArgumentParser`,\n            dict(str->dict))\n    \"\"\"\n", "input": "", "output": "    if cmdname in COMMANDS[mode]:\n        return COMMANDS[mode][cmdname]\n    elif cmdname in COMMANDS['global']:\n        return COMMANDS['global'][cmdname]\n    else:\n        return None, None, None", "category": "Python"}, {"instruction": "def contains(self, value):\n        \"\"\"\n        Checks if this array contains a value.\n        The check uses direct comparison between elements and the specified value.\n\n        :param value: a value to be checked\n\n        :return: true if this array contains the value or false otherwise.\n        \"\"\"\n", "input": "", "output": "        str_value = StringConverter.to_nullable_string(value)\n\n        for element in self:\n            str_element = StringConverter.to_string(element)\n\n            if str_value == None and str_element == None:\n                return True\n            if str_value == None or str_element == None:\n                continue\n            \n            if str_value == str_element:\n                return True\n\n        return False", "category": "Python"}, {"instruction": "def native_path(path):  # pragma: no cover\n    \"\"\"\n    Always return a native path, that is unicode on Python 3 and bytestring on\n    Python 2.\n\n    Taken `from Django <http://bit.ly/1r3gogZ>`_.\n    \"\"\"\n", "input": "", "output": "    if PY2 and not isinstance(path, bytes):\n        return path.encode(fs_encoding)\n    return path", "category": "Python"}, {"instruction": "def move_after(self, node_id):\n        \"\"\" Moving one node of tree after another\n\n        For example see :mod:`sqlalchemy_mptt.tests.cases.move_node.test_move_after_function`\n        \"\"\"\n", "input": "", "output": "        session = Session.object_session(self)\n        self.parent_id = self.parent_id\n        self.mptt_move_after = node_id\n        session.add(self)", "category": "Python"}, {"instruction": "def make_object(cls, data):\n    \"\"\"Creates an API object of class `cls`, setting its `_data` to\n    data. Subclasses of `Object` are required to use this to build a\n    new, empty instance without using their constructor.\n    \"\"\"\n", "input": "", "output": "    if issubclass(cls, Object):\n        self = object.__new__(cls)\n        self._data = data\n    else:\n        self = data\n    return self", "category": "Python"}, {"instruction": "def get_encodings(hint_encoding='utf-8'):\n    \"\"\"Used to try different encoding.\n    Function copied from Odoo 11.0 (odoo.loglevels.get_encodings).\n    This piece of code is licensed under the LGPL-v3 and so it is compatible\n    with the LGPL-v3 license of OdooRPC::\n\n        - https://github.com/odoo/odoo/blob/11.0/LICENSE\n        - https://github.com/odoo/odoo/blob/11.0/COPYRIGHT\n    \"\"\"\n", "input": "", "output": "    fallbacks = {\n        'latin1': 'latin9',\n        'iso-8859-1': 'iso8859-15',\n        'cp1252': '1252',\n    }\n    if hint_encoding:\n        yield hint_encoding\n        if hint_encoding.lower() in fallbacks:\n            yield fallbacks[hint_encoding.lower()]\n\n    # some defaults (also taking care of pure ASCII)\n    for charset in ['utf8', 'latin1', 'ascii']:\n        if not hint_encoding or (charset.lower() != hint_encoding.lower()):\n            yield charset\n\n    from locale import getpreferredencoding\n    prefenc = getpreferredencoding()\n    if prefenc and prefenc.lower() != 'utf-8':\n        yield prefenc\n        prefenc = fallbacks.get(prefenc.lower())\n        if prefenc:\n            yield prefenc", "category": "Python"}, {"instruction": "def vote_total(self):\n        \"\"\"\n        Calculates vote total as total_upvotes - total_downvotes. We are adding a method here instead of relying on django-secretballot's addition since that doesn't work for subclasses.\n        \"\"\"\n", "input": "", "output": "        modelbase_obj = self.modelbase_obj\n        return modelbase_obj.votes.filter(vote=+1).count() - modelbase_obj.votes.filter(vote=-1).count()", "category": "Python"}, {"instruction": "def check_order(current, hit, overlap = 200):\n    \"\"\"\n    determine if hits are sequential on model and on the\n    same strand\n        * if not, they should be split into different groups\n    \"\"\"\n", "input": "", "output": "    prev_model = current[-1][2:4]\n    prev_strand = current[-1][-2]\n    hit_model = hit[2:4]\n    hit_strand = hit[-2]\n    # make sure they are on the same strand\n    if prev_strand != hit_strand:\n        return False\n    # check for sequential hits on + strand\n    if prev_strand == '+' and (prev_model[1] - hit_model[0] >= overlap):\n        return False\n    # check for sequential hits on - strand\n    if prev_strand == '-' and (hit_model[1] - prev_model[0] >= overlap):\n        return False\n    else:\n        return True", "category": "Python"}, {"instruction": "def _pfp__set_value(self, new_val):\n        \"\"\"Set the new value if type checking is passes, potentially\n        (TODO? reevaluate this) casting the value to something else\n\n        :new_val: The new value\n        :returns: TODO\n\n        \"\"\"\n", "input": "", "output": "        if self._pfp__frozen:\n            raise errors.UnmodifiableConst()\n        self._pfp__value = self._pfp__get_root_value(new_val)\n        self._pfp__notify_parent()", "category": "Python"}, {"instruction": "def model(request, content_type_id):\n    \"\"\"\n    ``Actor`` focused activity stream for actor defined by ``content_type_id``,\n    ``object_id``.\n    \"\"\"\n", "input": "", "output": "    ctype = get_object_or_404(ContentType, pk=content_type_id)\n    model_class = ctype.model_class()\n    return render(\n        request,\n        'actstream/actor.html',\n        {\n            'action_list': models.model_stream(model_class),\n            'ctype': ctype,\n            'actor': model_class\n        }\n    )", "category": "Python"}, {"instruction": "def set_attribute(self, element, attribute, value):\n        \"\"\"\n        :Description: Modify the given attribute of the target element.\n        :param element: Element for browser instance to target.\n        :type element: WebElement\n        :param attribute: Attribute of target element to modify.\n        :type attribute: string\n        :param value: Value of target element's attribute to modify.\n        :type value: None, bool, int, float, string\n        \"\"\"\n", "input": "", "output": "        self.browser.execute_script('arguments[0].setAttribute(\"%s\", %s);' % (\n            attribute, self.__type2js(value=value)), element)", "category": "Python"}, {"instruction": "def get(self, include_meta=False, chunk_size=None):\n        \"\"\"\n        Fetches the object from storage.\n\n        If 'include_meta' is False, only the bytes representing the\n        file is returned.\n\n        Note: if 'chunk_size' is defined, you must fully read the object's\n        contents before making another request.\n\n        When 'include_meta' is True, what is returned from this method is a\n        2-tuple:\n            Element 0: a dictionary containing metadata about the file.\n            Element 1: a stream of bytes representing the object's contents.\n        \"\"\"\n", "input": "", "output": "        return self.manager.fetch(obj=self, include_meta=include_meta,\n                chunk_size=chunk_size)", "category": "Python"}, {"instruction": "def fromrandom(shape=(10, 50, 50), npartitions=1, seed=42, engine=None):\n    \"\"\"\n    Generate random image data.\n\n    Parameters\n    ----------\n    shape : tuple, optional, default=(10, 50, 50)\n        Dimensions of images.\n\n    npartitions : int, optional, default=1\n        Number of partitions.\n\n    seed : int, optional, default=42\n        Random seed.\n    \"\"\"\n", "input": "", "output": "    seed = hash(seed)\n\n    def generate(v):\n        random.seed(seed + v)\n        return random.randn(*shape[1:])\n\n    return fromlist(range(shape[0]), accessor=generate, npartitions=npartitions, engine=engine)", "category": "Python"}, {"instruction": "def get_config_path(filename, *search_dirs):\n    \"\"\"Get the appropriate path for a filename, in that order: filename, ., PPP_CONFIG_DIR, package's etc dir.\"\"\"\n", "input": "", "output": "    paths = config_search_paths(filename, *search_dirs)\n\n    for path in paths[::-1]:\n        if os.path.exists(path):\n            return path", "category": "Python"}, {"instruction": "def retrieve_in(self, key, *paths, **kwargs):\n        \"\"\"Atomically fetch one or more paths from a document.\n\n        Convenience method for retrieval operations. This functions\n        identically to :meth:`lookup_in`. As such, the following two\n        forms are equivalent:\n\n        .. code-block:: python\n\n            import couchbase.subdocument as SD\n            rv = cb.lookup_in(key,\n                              SD.get('email'),\n                              SD.get('name'),\n                              SD.get('friends.therock')\n\n            email, name, friend = rv\n\n        .. code-block:: python\n\n            rv = cb.retrieve_in(key, 'email', 'name', 'friends.therock')\n            email, name, friend = rv\n\n        .. seealso:: :meth:`lookup_in`\n        \"\"\"\n", "input": "", "output": "        import couchbase.subdocument as SD\n        return self.lookup_in(key, *tuple(SD.get(x) for x in paths), **kwargs)", "category": "Python"}, {"instruction": "def extract_files(ubifs, out_path, perms=False):\n    \"\"\"Extract UBIFS contents to_path/\n\n    Arguments:\n    Obj:ubifs    -- UBIFS object.\n    Str:out_path  -- Path to extract contents to.\n    \"\"\"\n", "input": "", "output": "    try:\n        inodes = {}\n        bad_blocks = []\n\n        walk.index(ubifs, ubifs.master_node.root_lnum, ubifs.master_node.root_offs, inodes, bad_blocks)\n\n        if len(inodes) < 2:\n            raise Exception('No inodes found')\n\n        for dent in inodes[1]['dent']:\n            extract_dents(ubifs, inodes, dent, out_path, perms)\n\n        if len(bad_blocks):\n            error(extract_files, 'Warning', 'Data may be missing or corrupted, bad blocks, LEB [%s]' % ','.join(map(str, bad_blocks)))\n\n    except Exception as e:\n        error(extract_files, 'Error', '%s' % e)", "category": "Python"}, {"instruction": "def get_admonition(cssclass, title, text):\n    \"\"\" Generates rst admonition block given a bootstrap alert css class, title, and text\"\"\"\n", "input": "", "output": "    rst = (\"\\n\\n.. admonition:: \" + title + \"\\n\") if title else \"\\n\\n.. note:: \\n\"\n    rst += \"\\t:class: alert alert-\" + cssclass + \"\\n\\n\"\n    for line in text.splitlines():\n        rst += \"\\t\" + line + \"\\n\"\n\n    rst += \"\\n\"\n    return rst", "category": "Python"}, {"instruction": "def setDictionary(self, props):\r\n        \"\"\"\r\n        Sets a dictionary of the key/value pairing for the items in\r\n        this widget.\r\n        \r\n        :param      props | {<str> key: <str> value, ..}\r\n        \"\"\"\n", "input": "", "output": "        if not self._initialized:\r\n            self.setColumns(['', 'Property', 'Value'])\r\n            self.setColumnWidth(0, 22)\r\n            \r\n        self._initialized = True\r\n        \r\n        self.clear()\r\n        \r\n        palette = self.palette()\r\n        item = XTreeWidgetItem(self, ['add another item'])\r\n        item.setForeground(0, palette.color(palette.Disabled, palette.Text))\r\n        item.setTextAlignment(0, QtCore.Qt.AlignCenter)\r\n        item.setFlags(QtCore.Qt.ItemFlags(0))\r\n        item.setFixedHeight(22)\r\n        item.setFirstColumnSpanned(True)\r\n        \r\n        for key, text in props.items():\r\n            self.addEntry(key, text)", "category": "Python"}, {"instruction": "def create(cls, *operands, **kwargs):\n        \"\"\"Instantiate the product while applying simplification rules\"\"\"\n", "input": "", "output": "        converted_operands = []\n        for op in operands:\n            if not isinstance(op, Scalar):\n                op = ScalarValue.create(op)\n            converted_operands.append(op)\n        return super().create(*converted_operands, **kwargs)", "category": "Python"}, {"instruction": "def iris(display=False):\n    \"\"\" Return the classic iris data in a nice package. \"\"\"\n", "input": "", "output": "\n    d = sklearn.datasets.load_iris()\n    df = pd.DataFrame(data=d.data, columns=d.feature_names) # pylint: disable=E1101\n    if display:\n        return df, [d.target_names[v] for v in d.target] # pylint: disable=E1101\n    else:\n        return df, d.target", "category": "Python"}, {"instruction": "def write_biom(biomT, output_fp, fmt=\"hdf5\", gzip=False):\n    \"\"\"\n    Write the BIOM table to a file.\n\n    :type biomT: biom.table.Table\n    :param biomT: A BIOM table containing the per-sample OTU counts and metadata\n                  to be written out to file.\n    :type output_fp str\n    :param output_fp: Path to the BIOM-format file that will be written.\n    :type fmt: str\n    :param fmt: One of: hdf5, json, tsv. The BIOM version the table will be\n                output (2.x, 1.0, 'classic').\n    \"\"\"\n", "input": "", "output": "    opener = open\n    mode = 'w'\n    if gzip and fmt != \"hdf5\":\n        if not output_fp.endswith(\".gz\"):\n            output_fp += \".gz\"\n        opener = gzip_open\n        mode = 'wt'\n\n    # HDF5 BIOM files are gzipped by default\n    if fmt == \"hdf5\":\n        opener = h5py.File\n\n    with opener(output_fp, mode) as biom_f:\n        if fmt == \"json\":\n            biomT.to_json(biomT.generated_by, direct_io=biom_f)\n        elif fmt == \"tsv\":\n            biom_f.write(biomT.to_tsv())\n        else:\n            biomT.to_hdf5(biom_f, biomT.generated_by)\n\n    return output_fp", "category": "Python"}, {"instruction": "def delete_api_key(awsclient, api_key):\n    \"\"\"Remove API key.\n\n    :param api_key:\n    \"\"\"\n", "input": "", "output": "    _sleep()\n    client_api = awsclient.get_client('apigateway')\n    print('delete api key: %s' % api_key)\n\n    response = client_api.delete_api_key(\n        apiKey=api_key\n    )\n\n    print(json2table(response))", "category": "Python"}, {"instruction": "def to_binary(self, threshold=0.0):\n        \"\"\"Creates a BinaryImage from the depth image. Points where the depth\n        is greater than threshold are converted to ones, and all other points\n        are zeros.\n\n        Parameters\n        ----------\n        threshold : float\n            The depth threshold.\n\n        Returns\n        -------\n        :obj:`BinaryImage`\n            A BinaryImage where all 1 points had a depth greater than threshold\n            in the DepthImage.\n        \"\"\"\n", "input": "", "output": "        data = BINARY_IM_MAX_VAL * (self._data > threshold)\n        return BinaryImage(data.astype(np.uint8), self._frame)", "category": "Python"}, {"instruction": "def do_shutdown(self, restart):\n        \"\"\"\n        Shut down the app gracefully, saving history.\n        \"\"\"\n", "input": "", "output": "        print(\"in shutdown function\")\n        if self.hist_file:\n            with open(self.hist_file, 'wb') as fid:\n                data = '\\n'.join(self.hist_cache[-self.max_hist_cache:])\n                fid.write(data.encode('utf-8'))\n        if self.mva:\n            self.mva._endsas()\n            self.mva = None\n        if restart:\n            self.Print(\"Restarting kernel...\")\n            self.reload_magics()\n            self.restart_kernel()\n            self.Print(\"Done!\")\n        return {'status': 'ok', 'restart': restart}", "category": "Python"}, {"instruction": "def _get_oauth_token(self):\n        \"\"\"\n        Get Monzo access token via OAuth2 `authorization code` grant type.\n\n        Official docs:\n            https://monzo.com/docs/#acquire-an-access-token\n\n        :returns: OAuth 2 access token\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        url = urljoin(self.api_url, '/oauth2/token')\n\n        oauth = OAuth2Session(\n            client_id=self._client_id,\n            redirect_uri=config.REDIRECT_URI,\n        )\n\n        token = oauth.fetch_token(\n            token_url=url,\n            code=self._auth_code,\n            client_secret=self._client_secret,\n        )\n\n        return token", "category": "Python"}, {"instruction": "def example():\n    \"\"\"creates example data set\"\"\"\n", "input": "", "output": "    proc = [[2,3,1],[4,2,3],[1,4,1]]\n    p = {}\n    for i in range(3):\n        for j in range(3):\n            p[i+1,j+1] = proc[j][i]\n    return p", "category": "Python"}, {"instruction": "def mandelbrot(x, y, params):\n    \"\"\"\n    Computes the number of iterations of the given plane-space coordinates.\n\n    :param x: X coordinate on the plane.\n    :param y: Y coordinate on the plane.\n    :param params: Current application parameters.\n    :type params: params.Params\n    :return: Discrete number of iterations.\n    \"\"\"\n", "input": "", "output": "    mb_x, mb_y = get_coords(x, y, params)\n    mb = mandelbrot_iterate(mb_x + 1j * mb_y, params.max_iterations, params.julia_seed)\n\n    return mb[1]", "category": "Python"}, {"instruction": "def indicator_arrays(tc_entity_array):\n        \"\"\"Convert TCEntityArray to Indicator Type dictionary.\n\n        Args:\n            tc_entity_array (dictionary): The TCEntityArray to convert.\n\n        Returns:\n            (dictionary): Dictionary containing arrays of indicators for each indicator type.\n        \"\"\"\n", "input": "", "output": "        type_dict = {}\n        for ea in tc_entity_array:\n            type_dict.setdefault(ea['type'], []).append(ea['value'])\n        return type_dict", "category": "Python"}, {"instruction": "def clean_all(self, args):\n        \"\"\"Delete all build components; the package cache, package builds,\n        bootstrap builds and distributions.\"\"\"\n", "input": "", "output": "        self.clean_dists(args)\n        self.clean_builds(args)\n        self.clean_download_cache(args)", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"Start watching the socket.\"\"\"\n", "input": "", "output": "        if self.closed:\n            raise ConnectionClosed()\n\n        self.read_watcher.start()\n        if self.write == self.buffered_write:\n            self.write_watcher.start()", "category": "Python"}, {"instruction": "def load_config(name, base='conf'):\n    \"\"\"Load config dict from JSON\"\"\"\n", "input": "", "output": "    fname = pjoin(base, name + '.json')\n    if not os.path.exists(fname):\n        return {}\n    try:\n        with open(fname) as f:\n            cfg = json.load(f)\n    except Exception as e:\n        warn(\"Couldn't load %s: %s\" % (fname, e))\n        cfg = {}\n    return cfg", "category": "Python"}, {"instruction": "def get_data_times_for_job_legacy(self, num_job):\n        \"\"\" Get the data that this job will need to read in. \"\"\"\n", "input": "", "output": "        # Should all be integers, so no rounding needed\n        shift_dur = self.curr_seg[0] + int(self.job_time_shift * num_job)\n        job_data_seg = self.data_chunk.shift(shift_dur)\n\n        # If this is the last job, push the end back\n        if num_job == (self.num_jobs - 1):\n            dataPushBack = job_data_seg[1] - self.curr_seg[1]\n            assert dataPushBack >= 0\n            job_data_seg = segments.segment(job_data_seg[0] - dataPushBack,\n                                                 self.curr_seg[1])\n            assert (abs(job_data_seg) == self.data_length)\n        return job_data_seg", "category": "Python"}, {"instruction": "def get_float(self, key, optional=False):\n        \"\"\"\n        Tries to fetch a variable from the config and expects it to be strictly a float\n        :param key: Variable to look for\n        :param optional: Whether to raise ConfigKeyNotFoundError if key was not found\n        :return: float\n        \"\"\"\n", "input": "", "output": "        return self._get_typed_value(key, float, lambda x: float(x), optional)", "category": "Python"}, {"instruction": "def causally_significant_nodes(cm):\n    \"\"\"Return indices of nodes that have both inputs and outputs.\"\"\"\n", "input": "", "output": "    inputs = cm.sum(0)\n    outputs = cm.sum(1)\n    nodes_with_inputs_and_outputs = np.logical_and(inputs > 0, outputs > 0)\n    return tuple(np.where(nodes_with_inputs_and_outputs)[0])", "category": "Python"}, {"instruction": "def if_match(self):\n        \"\"\"\n        Get the If-Match option of a request.\n\n        :return: the If-Match values or [] if not specified by the request\n        :rtype : list\n        \"\"\"\n", "input": "", "output": "        value = []\n        for option in self.options:\n            if option.number == defines.OptionRegistry.IF_MATCH.number:\n                value.append(option.value)\n        return value", "category": "Python"}, {"instruction": "def save(self) -> None:\n        \"\"\"\n        Save the training trace to :py:attr:`CXF_TRACE_FILE` file under the specified directory.\n\n        :raise ValueError: if no output directory was specified\n        \"\"\"\n", "input": "", "output": "        if self._output_dir is None:\n            raise ValueError('Can not save TrainingTrace without output dir.')\n        yaml_to_file(self._trace, self._output_dir, CXF_TRACE_FILE)", "category": "Python"}, {"instruction": "def is_binary_address(value: Any) -> bool:\n    \"\"\"\n    Checks if the given string is an address in raw bytes form.\n    \"\"\"\n", "input": "", "output": "    if not is_bytes(value):\n        return False\n    elif len(value) != 20:\n        return False\n    else:\n        return True", "category": "Python"}, {"instruction": "def Xnor(*xs, simplify=True):\n    \"\"\"Expression exclusive nor (XNOR) operator\n\n    If *simplify* is ``True``, return a simplified expression.\n    \"\"\"\n", "input": "", "output": "    xs = [Expression.box(x).node for x in xs]\n    y = exprnode.not_(exprnode.xor(*xs))\n    if simplify:\n        y = y.simplify()\n    return _expr(y)", "category": "Python"}, {"instruction": "def parse(cls, element):\n        \"\"\"\n        Create a new AltRecordID by parsing root.\n\n        :param element: Element to be parsed into an AltRecordID.\n        :raises exceptions.ParseError: If element is not a valid altRecordID.\n        \"\"\"\n", "input": "", "output": "        if element.tag != cls.ALT_RECORD_ID_TAG:\n            raise exceptions.ParseError(\n                u\"AltRecordID got unexpected tag {}; expected {}\".format(\n                    element.tag, cls.ALT_RECORD_ID_TAG\n                )\n            )\n\n        return cls(element.text, id=element.get(u\"ID\"), type=element.get(u\"TYPE\"))", "category": "Python"}, {"instruction": "def index():\n    \"\"\"List linked accounts.\"\"\"\n", "input": "", "output": "    oauth = current_app.extensions['oauthlib.client']\n\n    services = []\n    service_map = {}\n    i = 0\n\n    for appid, conf in six.iteritems(\n            current_app.config['OAUTHCLIENT_REMOTE_APPS']):\n        if not conf.get('hide', False):\n            services.append(dict(\n                appid=appid,\n                title=conf['title'],\n                icon=conf.get('icon', None),\n                description=conf.get('description', None),\n                account=None\n            ))\n            service_map[oauth.remote_apps[appid].consumer_key] = i\n            i += 1\n\n    # Fetch already linked accounts\n    accounts = RemoteAccount.query.filter_by(\n        user_id=current_user.get_id()\n    ).all()\n\n    for a in accounts:\n        if a.client_id in service_map:\n            services[service_map[a.client_id]]['account'] = a\n\n    # Sort according to title\n    services.sort(key=itemgetter('title'))\n\n    return render_template(\n        'invenio_oauthclient/settings/index.html',\n        services=services\n    )", "category": "Python"}, {"instruction": "def search(self,\n               id_list: Iterable,\n               negated_ids: Iterable,\n               limit: Optional[int],\n               taxon_filter: Optional,\n               category_filter: Optional,\n               method: Optional)-> SimResult:\n        \"\"\"\n        Given an input iterable of classes or individuals,\n        resolves to target classes (phenotypes, go terms, etc)\n        and provides a ranking of similar profiles\n        \"\"\"\n", "input": "", "output": "        pass", "category": "Python"}, {"instruction": "def parse(self, stream, media_type=None, parser_context=None):\n        \"\"\"\n        Parses the incoming bytestream as a URL encoded form,\n        and returns the resulting QueryDict.\n        \"\"\"\n", "input": "", "output": "        parser_context = parser_context or {}\n        encoding = parser_context.get('encoding', settings.DEFAULT_CHARSET)\n        data = QueryDict(stream.read(), encoding=encoding)\n        return data", "category": "Python"}, {"instruction": "def normalize(X):\n    \"\"\" equivalent to scipy.preprocessing.normalize on sparse matrices\n    , but lets avoid another depedency just for a small utility function \"\"\"\n", "input": "", "output": "    X = coo_matrix(X)\n    X.data = X.data / sqrt(bincount(X.row, X.data ** 2))[X.row]\n    return X", "category": "Python"}, {"instruction": "def add_path(self, path, path_filter=None):\n        \"\"\"\n        Adding all files from given path to the object.\n\n        Args:\n            path (str): valid, existing directory\n        \"\"\"\n", "input": "", "output": "        for root, _, files in os.walk(path):\n            for filename in files:\n                full_path_and_filename = os.path.join(root, filename)\n                if path_filter is None or path_filter(full_path_and_filename):\n                    relative_path_and_filename = full_path_and_filename.replace(path + '/', '')\n                    with open(full_path_and_filename, 'rb') as handle:\n                        self.files[relative_path_and_filename] = b64encode(handle.read()).decode('utf-8')", "category": "Python"}, {"instruction": "def getvariable(name):\n    \"\"\"Get the value of a local variable somewhere in the call stack.\"\"\"\n", "input": "", "output": "    import inspect\n    fr = inspect.currentframe()\n    try:\n        while fr:\n            fr = fr.f_back\n            vars = fr.f_locals\n            if name in vars:\n                return vars[name]\n    except:\n        pass\n    return None", "category": "Python"}, {"instruction": "def get_shift_by_data(temp_hourly, lon, lat, time_zone):\n    '''function to get max temp shift (monthly) by hourly data\n    \n    Parameters\n    ----\n    hourly_data_obs : observed hourly data \n    lat :             latitude in DezDeg\n    lon :             longitude in DezDeg\n    time_zone:        timezone\n    '''\n", "input": "", "output": "    daily_index = temp_hourly.resample('D').mean().index\n    sun_times = melodist.util.get_sun_times(daily_index, lon, lat, time_zone)\n\n    idxmax = temp_hourly.groupby(temp_hourly.index.date).idxmax()\n    idxmax.index = pd.to_datetime(idxmax.index)\n    max_temp_hour_obs = idxmax.dropna().apply(lambda d: d.hour)\n    max_temp_hour_pot = sun_times.sunnoon\n    max_delta = max_temp_hour_obs - max_temp_hour_pot\n    mean_monthly_delta = max_delta.groupby(max_delta.index.month).mean()\n\n    return mean_monthly_delta", "category": "Python"}, {"instruction": "def on_error(self, headers, body):\n        \"\"\"\n        Increment the error count. See :py:meth:`ConnectionListener.on_error`\n\n        :param dict headers: headers in the message\n        :param body: the message content\n        \"\"\"\n", "input": "", "output": "        if log.isEnabledFor(logging.DEBUG):\n            log.debug(\"received an error %s [%s]\", body, headers)\n        else:\n            log.info(\"received an error %s\", body)\n        self.errors += 1", "category": "Python"}, {"instruction": "def bonds(self):\n        \"\"\"\n        iterate other all bonds\n        \"\"\"\n", "input": "", "output": "        seen = set()\n        for n, m_bond in self._adj.items():\n            seen.add(n)\n            for m, bond in m_bond.items():\n                if m not in seen:\n                    yield n, m, bond", "category": "Python"}, {"instruction": "def QA_fetch_index_list_adv(collections=DATABASE.index_list):\n    '''\n    '\u83b7\u53d6\u80a1\u7968\u5217\u8868'\n    :param collections: mongodb \u6570\u636e\u5e93\n    :return: DataFrame\n    '''\n", "input": "", "output": "    index_list_items = QA_fetch_index_list(collections)\n    if len(index_list_items) == 0:\n        print(\"QA Error QA_fetch_index_list_adv call item for item in collections.find() return 0 item, maybe the DATABASE.index_list is empty!\")\n        return None\n    return index_list_items", "category": "Python"}, {"instruction": "def _get_next_or_previous_by_order(self, is_next, **kwargs):\n        \"\"\"\n        Retrieves next or previous object by order. We implement our\n        own version instead of Django's so we can hook into the\n        published manager, concrete subclasses and our custom\n        ``with_respect_to`` method.\n        \"\"\"\n", "input": "", "output": "        lookup = self.with_respect_to()\n        lookup[\"_order\"] = self._order + (1 if is_next else -1)\n        concrete_model = base_concrete_model(Orderable, self)\n        try:\n            queryset = concrete_model.objects.published\n        except AttributeError:\n            queryset = concrete_model.objects.filter\n        try:\n            return queryset(**kwargs).get(**lookup)\n        except concrete_model.DoesNotExist:\n            pass", "category": "Python"}, {"instruction": "def solar_azimuth(self, dateandtime=None):\n        \"\"\"Calculates the solar azimuth angle for a specific date/time.\n\n        :param dateandtime: The date and time for which to calculate the angle.\n        :type dateandtime: :class:`~datetime.datetime`\n\n        :returns: The azimuth angle in degrees clockwise from North.\n        :rtype: float\n        \"\"\"\n", "input": "", "output": "\n        if self.astral is None:\n            self.astral = Astral()\n\n        if dateandtime is None:\n            dateandtime = datetime.datetime.now(self.tz)\n        elif not dateandtime.tzinfo:\n            dateandtime = self.tz.localize(dateandtime)\n\n        dateandtime = dateandtime.astimezone(pytz.UTC)\n\n        return self.astral.solar_azimuth(dateandtime, self.latitude, self.longitude)", "category": "Python"}, {"instruction": "def by_median_household_income(self,\n                                   lower=-1,\n                                   upper=2 ** 31,\n                                   zipcode_type=ZipcodeType.Standard,\n                                   sort_by=SimpleZipcode.median_household_income.name,\n                                   ascending=False,\n                                   returns=DEFAULT_LIMIT):\n        \"\"\"\n        Search zipcode information by median household income.\n        \"\"\"\n", "input": "", "output": "        return self.query(\n            median_household_income_lower=lower,\n            median_household_income_upper=upper,\n            sort_by=sort_by, zipcode_type=zipcode_type,\n            ascending=ascending, returns=returns,\n        )", "category": "Python"}, {"instruction": "def check_new_version_available(this_version):\n    \"\"\"\n    Checks if a newer version of Zappa is available.\n\n    Returns True is updateable, else False.\n\n    \"\"\"\n", "input": "", "output": "    import requests\n\n    pypi_url = 'https://pypi.python.org/pypi/Zappa/json'\n    resp = requests.get(pypi_url, timeout=1.5)\n    top_version = resp.json()['info']['version']\n\n    return this_version != top_version", "category": "Python"}, {"instruction": "def cpos(string, chars, start):\n    \"\"\"\n    Find the first occurrence in a string of a character belonging\n    to a collection of characters, starting at a specified location,\n    searching forward.\n\n    http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/cpos_c.html\n\n    :param string: Any character string.\n    :type string: str\n    :param chars: A collection of characters.\n    :type chars: str\n    :param start: Position to begin looking for one of chars.\n    :type start: int\n    :return:\n            The index of the first character of str at or\n            following index start that is in the collection chars.\n    :rtype: int\n    \"\"\"\n", "input": "", "output": "    string = stypes.stringToCharP(string)\n    chars = stypes.stringToCharP(chars)\n    start = ctypes.c_int(start)\n    return libspice.cpos_c(string, chars, start)", "category": "Python"}, {"instruction": "def cf_safe_name(name):\n    \"\"\"Converts a name to a safe string for a Cloudformation resource.\n\n    Given a string, returns a name that is safe for use as a CloudFormation\n    Resource. (ie: Only alphanumeric characters)\n    \"\"\"\n", "input": "", "output": "    alphanumeric = r\"[a-zA-Z0-9]+\"\n    parts = re.findall(alphanumeric, name)\n    return \"\".join([uppercase_first_letter(part) for part in parts])", "category": "Python"}, {"instruction": "def words_with_votes(self, only_topics=True):\n        \"\"\"\n            returns a list with words ordered by the number of votes\n            annotated with the number of votes in the \"votes\" property.\n        \"\"\"\n", "input": "", "output": "        result = Word.objects.filter(\n            bingofield__board__game__id=self.id).exclude(\n            type=WORD_TYPE_MIDDLE)\n\n        if only_topics:\n            result = result.exclude(bingofield__word__type=WORD_TYPE_META)\n\n        result = result.annotate(\n            votes=Sum(\"bingofield__vote\")).order_by(\"-votes\").values()\n\n        for item in result:\n            item['votes'] = max(0, item['votes'])\n            if result[0]['votes'] != 0:\n                item['percent'] = float(item['votes']) / result[0]['votes'] * 100\n            else:\n                item['percent'] = 0\n        return result", "category": "Python"}, {"instruction": "def energy(self):\n        \"\"\"Calculate state's energy.\"\"\"\n", "input": "", "output": "        arr = self.src.copy()\n        arr = self.apply_color(arr, self.state)\n\n        scores = [histogram_distance(self.ref[i], arr[i]) for i in range(3)]\n\n        # Important: scale by 100 for readability\n        return sum(scores) * 100", "category": "Python"}, {"instruction": "def compile_rename_column(self, blueprint, command, connection):\n        \"\"\"\n        Compile a rename column command.\n\n        :param blueprint: The blueprint\n        :type blueprint: Blueprint\n\n        :param command: The command\n        :type command: Fluent\n\n        :param connection: The connection\n        :type connection: orator.connections.Connection\n\n        :rtype: list\n        \"\"\"\n", "input": "", "output": "        table = self.get_table_prefix() + blueprint.get_table()\n\n        column = self.wrap(command.from_)\n\n        return \"ALTER TABLE %s RENAME COLUMN %s TO %s\" % (\n            table,\n            column,\n            self.wrap(command.to),\n        )", "category": "Python"}, {"instruction": "def within_n_std(df, n=3):\n    \"\"\"\n    Assert that every value is within ``n`` standard\n    deviations of its column's mean.\n\n    Parameters\n    ==========\n    df : DataFame\n    n : int\n      number of standard deviations from the mean\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n", "input": "", "output": "    means = df.mean()\n    stds = df.std()\n    inliers = (np.abs(df[means.index] - means) < n * stds)\n    if not np.all(inliers):\n        msg = generic.bad_locations(~inliers)\n        raise AssertionError(msg)\n    return df", "category": "Python"}, {"instruction": "def set_scenario_role_names(self):\n        \"\"\"Populates the list of scenario role names in this deployment and\n        populates the scenario_master with the master role\n\n        Gets a list of deployment properties containing \"isMaster\" because\n        there is exactly one per scenario host, containing the role name\n\n        :return:\n        \"\"\"\n", "input": "", "output": "        log = logging.getLogger(self.cls_logger + '.set_scenario_role_names')\n        is_master_props = self.get_matching_property_names('isMaster')\n        for is_master_prop in is_master_props:\n            role_name = is_master_prop.split('.')[-1]\n            log.info('Adding scenario host: {n}'.format(n=role_name))\n            self.scenario_role_names.append(role_name)\n\n            # Determine if this is the scenario master\n            is_master = self.get_value(is_master_prop).lower().strip()\n            if is_master == 'true':\n                log.info('Found master scenario host: {r}'.format(r=role_name))\n                self.scenario_master = role_name", "category": "Python"}, {"instruction": "def create_api_environment(self):\n        \"\"\"Get an instance of Api Environment services facade.\"\"\"\n", "input": "", "output": "        return ApiEnvironment(\n            self.networkapi_url,\n            self.user,\n            self.password,\n            self.user_ldap)", "category": "Python"}, {"instruction": "def ip_info(self, vuln_name=None, vuln_id=None, ip_list_only=True, hostfilter=None):\n        \"\"\"\n        List of all IP Addresses with a vulnerability\n\n        :param vuln_name: t_vulndata.f_vulnid\n        :param vuln_id: t_vulndata.id\n        :param ip_list_only: IP List only (default) or rest of t_hosts fields\n        :param hostfilter: Valid hostfilter or none\n        :return: [(ip, hostname) ...] or [(ip, hostname, t_service_vulns.f_proof, t_service_vulns.f_status), ...]\n        \"\"\"\n", "input": "", "output": "        return self.send.vuln_ip_info(vuln_name, vuln_id, ip_list_only, hostfilter)", "category": "Python"}, {"instruction": "def plot_xtb(fignum, XTB, Bs, e, f):\n    \"\"\" function to plot series of chi measurements as a function of temperature, holding frequency constant and varying B\n    \"\"\"\n", "input": "", "output": "    plt.figure(num=fignum)\n    plt.xlabel('Temperature (K)')\n    plt.ylabel('Susceptibility (m^3/kg)')\n    k = 0\n    Blab = []\n    for field in XTB:\n        T, X = [], []\n        for xt in field:\n            X.append(xt[0])\n            T.append(xt[1])\n        plt.plot(T, X)\n        plt.text(T[-1], X[-1], '%8.2e' % (Bs[k]) + ' T')\n#        Blab.append('%8.1e'%(Bs[k])+' T')\n        k += 1\n    plt.title(e + ': f = ' + '%i' % (int(f)) + ' Hz')", "category": "Python"}, {"instruction": "def send(self, msg, error_check=False):\n        \"\"\"\n        Send a raw string with the CR-LF appended to it.\n        Required arguments:\n        * msg - Message to send.\n        Optional arguments:\n        * error_check=False - Check for errors.\n        If an error is found the relevant exception will be raised.\n        \"\"\"\n", "input": "", "output": "        with self.lock:\n            msg = msg.replace('\\r', '\\\\r').replace('\\n', '\\\\n') + self._crlf\n            try:\n                data = msg.encode(self.encoding)\n            except UnicodeEncodeError:\n                data = msg.encode(self.fallback_encoding)\n            if len(data) > 512:\n                raise self.MessageTooLong(\"LurklibError: MessageTooLong\")\n            self._socket.send(data)\n            if error_check and self.readable():\n                self._recv()\n                self.stepback()", "category": "Python"}, {"instruction": "def _initialize_referenced(model_class, attribute):\n    \"\"\"Adds a property to the target of a reference field that\n    returns the list of associated objects.\n    \"\"\"\n", "input": "", "output": "    # this should be a descriptor\n    def _related_objects(self):\n        return (model_class.objects\n                .filter(**{attribute.attname: self.id}))\n\n    klass = attribute._target_type\n    if isinstance(klass, basestring):\n        return (klass, model_class, attribute)\n    else:\n        related_name = (attribute.related_name or\n                model_class.__name__.lower() + '_set')\n        setattr(klass, related_name,\n                property(_related_objects))", "category": "Python"}, {"instruction": "def create_directories(self, create_project_dir=True):\n        \"\"\"\n        Call once for new projects to create the initial project directories.\n        \"\"\"\n", "input": "", "output": "        return task.create_directories(self.datadir, self.sitedir,\n            self.target if create_project_dir else None)", "category": "Python"}, {"instruction": "def peek_all(self, model_class):\n        \"\"\"Return a list of models from the local cache.\n\n        Args:\n            model_class (:class:`cinder_data.model.CinderModel`): A subclass of\n                :class:`cinder_data.model.CinderModel` of your chosen model.\n\n        Returns:\n            list: A list of instances of you model_class or and empty list.\n        \"\"\"\n", "input": "", "output": "        if self._cache:\n            return self._cache.get_records(model_class.__name__)\n        else:\n            return []", "category": "Python"}, {"instruction": "def extension_validator(self, azure_rest_api_request_model):\n        \"\"\"ExtensionValidator.\n        [Preview API]\n        :param :class:`<AzureRestApiRequestModel> <azure.devops.v5_0.gallery.models.AzureRestApiRequestModel>` azure_rest_api_request_model:\n        \"\"\"\n", "input": "", "output": "        content = self._serialize.body(azure_rest_api_request_model, 'AzureRestApiRequestModel')\n        self._send(http_method='POST',\n                   location_id='05e8a5e1-8c59-4c2c-8856-0ff087d1a844',\n                   version='5.0-preview.1',\n                   content=content)", "category": "Python"}, {"instruction": "def combine_chunks(storage, args, prefix=None):\n    '''\n    Combine a chunked file into a whole file again.\n    Goes through each part, in order,\n    and appends that part's bytes to another destination file.\n    Chunks are stored in the chunks storage.\n    '''\n", "input": "", "output": "    uuid = args['uuid']\n    # Normalize filename including extension\n    target = utils.normalize(args['filename'])\n    if prefix:\n        target = os.path.join(prefix, target)\n    with storage.open(target, 'wb') as out:\n        for i in xrange(args['totalparts']):\n            partname = chunk_filename(uuid, i)\n            out.write(chunks.read(partname))\n            chunks.delete(partname)\n    chunks.delete(chunk_filename(uuid, META))\n    return target", "category": "Python"}, {"instruction": "def install(bld):\n\t'''installs the build files'''\n", "input": "", "output": "\tbld=check_configured(bld)\n\tOptions.commands['install']=True\n\tOptions.commands['uninstall']=False\n\tOptions.is_install=True\n\tbld.is_install=INSTALL\n\tbuild_impl(bld)\n\tbld.install()", "category": "Python"}, {"instruction": "def write_paula(docgraph, output_root_dir, human_readable=False):\n    \"\"\"\n    converts a DiscourseDocumentGraph into a set of PAULA XML files\n    representing the same document.\n\n    Parameters\n    ----------\n    docgraph : DiscourseDocumentGraph\n        the document graph to be converted\n    \"\"\"\n", "input": "", "output": "    paula_document = PaulaDocument(docgraph, human_readable=human_readable)\n    error_msg = (\"Please specify an output directory.\\nPaula documents consist\"\n                 \" of multiple files, so we can't just pipe them to STDOUT.\")\n    assert isinstance(output_root_dir, str), error_msg\n    document_dir = os.path.join(output_root_dir, paula_document.name)\n    if not os.path.isdir(document_dir):\n        create_dir(document_dir)\n    for paula_id in paula_document.files:\n        with open(os.path.join(document_dir, paula_id+'.xml'), 'w') as outfile:\n            outfile.write(\n                paula_etree_to_string(paula_document.files[paula_id],\n                                      paula_document.file2dtd[paula_id]))", "category": "Python"}, {"instruction": "def _update_plot(self, _):\n        \"\"\"Callback to redraw the plot to reflect the new parameter values.\"\"\"\n", "input": "", "output": "        # Since all sliders call this same callback without saying who they are\n        # I need to update the values for all parameters. This can be\n        # circumvented by creating a seperate callback function for each\n        # parameter.\n        for param in self.model.params:\n            param.value = self._sliders[param].val\n        for indep_var, dep_var in self._projections:\n            self._update_specific_plot(indep_var, dep_var)", "category": "Python"}, {"instruction": "def get_numeric_data(self, copy=False):\n        \"\"\"\n        Parameters\n        ----------\n        copy : boolean, default False\n            Whether to copy the blocks\n        \"\"\"\n", "input": "", "output": "        self._consolidate_inplace()\n        return self.combine([b for b in self.blocks if b.is_numeric], copy)", "category": "Python"}, {"instruction": "def set_state(self, state, speed=None):\n        \"\"\"\n        :param state: bool\n        :param speed: a string one of [\"lowest\", \"low\",\n            \"medium\", \"high\", \"auto\"] defaults to last speed\n        :return: nothing\n        \"\"\"\n", "input": "", "output": "        desired_state = {\"powered\": state}\n        if state:\n            brightness = self._to_brightness.get(speed or self.current_fan_speed(), 0.33)\n            desired_state.update({'brightness': brightness})\n\n        response = self.api_interface.set_device_state(self, {\n            \"desired_state\": desired_state\n        })\n\n        self._update_state_from_response(response)", "category": "Python"}, {"instruction": "def load_from_cloud(self):\n        \"\"\"Sync - read\"\"\"\n", "input": "", "output": "        self.current = [\n            {k: v for k, v in p.to_dict().items() if v is not None}\n            for p in self.api.list_presubscriptions()\n        ]", "category": "Python"}, {"instruction": "def set_mute(self, mute):\n        \"\"\"Send mute command.\"\"\"\n", "input": "", "output": "        req_url = ENDPOINTS[\"setMute\"].format(self.ip_address, self.zone_id)\n        params = {\"enable\": \"true\" if mute else \"false\"}\n        return request(req_url, params=params)", "category": "Python"}, {"instruction": "def orientality(self):\n        \"\"\" Returns the orientality of the object. \"\"\"\n", "input": "", "output": "        sun = self.chart.getObject(const.SUN)\n        return orientality(self.obj, sun)", "category": "Python"}, {"instruction": "def particles(self):\n        \"\"\"\n        Access the variational particles corresponding to this set of variational equations.\n\n        The function returns a list of particles which are sorted in the same way as those in \n        sim.particles\n\n        The particles are pointers and thus can be modified. \n\n        If there are N real particles, this function will also return a list of N particles (all of which are \n        variational particles). \n        \"\"\"\n", "input": "", "output": "        sim = self._sim.contents\n        ps = []\n        if self.testparticle>=0:\n            N = 1\n        else:\n            N = sim.N-sim.N_var \n        \n        ParticleList = Particle*N\n        ps = ParticleList.from_address(ctypes.addressof(sim._particles.contents)+self.index*ctypes.sizeof(Particle))\n        return ps", "category": "Python"}, {"instruction": "def simple():\n    \"\"\"Simple example using just the Bar class\n\n    This example is intended to show usage of the Bar class at the lowest\n    level.\n    \"\"\"\n", "input": "", "output": "    MAX_VALUE = 100\n\n    # Create our test progress bar\n    bar = Bar(max_value=MAX_VALUE, fallback=True)\n\n    bar.cursor.clear_lines(2)\n     # Before beginning to draw our bars, we save the position\n    #   of our cursor so we can restore back to this position before writing\n    #   the next time.\n    bar.cursor.save()\n    for i in range(MAX_VALUE + 1):\n        sleep(0.1 * random.random())\n        # We restore the cursor to saved position before writing\n        bar.cursor.restore()\n        # Now we draw the bar\n        bar.draw(value=i)", "category": "Python"}, {"instruction": "def run_callback(self):\n        \"\"\"Calls the callback_func, passing in the two positional arguments,\n        conditionally waiting if the callback function hasn't been set yet.\n        Meant to be run in a threadpool owned by the FutureCollection.\n\n        Returns:\n            None\n        \"\"\"\n", "input": "", "output": "\n        if self._callback_func is not None:\n            try:\n                self._callback_func(self._request, self._result)\n            except Exception:  # pylint: disable=broad-except\n                LOGGER.exception('An unhandled error occurred while running '\n                                 'future callback')", "category": "Python"}, {"instruction": "def dist_points(bin_edges, d):\n    \"\"\"Return an array of values according to a distribution\n\n    Points are calculated at the center of each bin\n    \"\"\"\n", "input": "", "output": "    bc = bin_centers(bin_edges)\n    if d is not None:\n        d = DISTS[d['type']](d, bc)\n    return d, bc", "category": "Python"}, {"instruction": "def query(cls, select, qb=None):\n        \"\"\"\n        :param select: QBO SQL query select statement\n        :param qb:\n        :return: Returns list\n        \"\"\"\n", "input": "", "output": "        if not qb:\n            qb = QuickBooks()\n\n        json_data = qb.query(select)\n\n        obj_list = []\n\n        if cls.qbo_object_name in json_data[\"QueryResponse\"]:\n            for item_json in json_data[\"QueryResponse\"][cls.qbo_object_name]:\n                obj_list.append(cls.from_json(item_json))\n\n        return obj_list", "category": "Python"}, {"instruction": "def append_transformed_structures(self, tstructs_or_transmuter):\n        \"\"\"\n        Method is overloaded to accept either a list of transformed structures\n        or transmuter, it which case it appends the second transmuter\"s\n        structures.\n\n        Args:\n            tstructs_or_transmuter: A list of transformed structures or a\n                transmuter.\n        \"\"\"\n", "input": "", "output": "        if isinstance(tstructs_or_transmuter, self.__class__):\n            self.transformed_structures.extend(tstructs_or_transmuter\n                                               .transformed_structures)\n        else:\n            for ts in tstructs_or_transmuter:\n                assert isinstance(ts, TransformedStructure)\n            self.transformed_structures.extend(tstructs_or_transmuter)", "category": "Python"}, {"instruction": "def rolldim(P, n=1):\n    \"\"\"\n    Roll the axes.\n\n    Args:\n        P (Poly) : Input polynomial.\n        n (int) : The axis that after rolling becomes the 0th axis.\n\n    Returns:\n        (Poly) : Polynomial with new axis configuration.\n\n    Examples:\n        >>> x,y,z = variable(3)\n        >>> P = x*x*x + y*y + z\n        >>> print(P)\n        q0^3+q1^2+q2\n        >>> print(rolldim(P))\n        q0^2+q2^3+q1\n    \"\"\"\n", "input": "", "output": "    dim = P.dim\n    shape = P.shape\n    dtype = P.dtype\n    A = dict(((key[n:]+key[:n],P.A[key]) for key in P.keys))\n    return Poly(A, dim, shape, dtype)", "category": "Python"}, {"instruction": "def _parse_alt_title(html_chunk):\n    \"\"\"\n    Parse title from alternative location if not found where it should be.\n\n    Args:\n        html_chunk (obj): HTMLElement containing slice of the page with details.\n\n    Returns:\n        str: Book's title.\n    \"\"\"\n", "input": "", "output": "    title = html_chunk.find(\n        \"input\",\n        {\"src\": \"../images_buttons/objednat_off.gif\"}\n    )\n\n    assert title, \"Can't find alternative title!\"\n\n    title = title[0]\n\n    assert \"title\" in title.params, \"Can't find alternative title source!\"\n\n    # title is stored as Bleh bleh: Title\n    title = title.params[\"title\"].split(\":\", 1)[-1]\n\n    return title.strip()", "category": "Python"}, {"instruction": "def get_local_variable_from_name(self, variable_name):\n        \"\"\"\n            Return a local variable from a name\n        Args:\n            varible_name (str): name of the variable\n        Returns:\n            LocalVariable\n        \"\"\"\n", "input": "", "output": "        return next((v for v in self.variables if v.name == variable_name), None)", "category": "Python"}, {"instruction": "def delete_video(video_id, cascade=False, delete_shares=False,\n        _connection=None):\n        \"\"\"\n        Delete the video represented by the ``video_id`` parameter.\n        \"\"\"\n", "input": "", "output": "        c = _connection\n        if not c:\n            c = connection.APIConnection()\n        c.post('delete_video', video_id=video_id, cascade=cascade,\n            delete_shares=delete_shares)", "category": "Python"}, {"instruction": "def _get_construct_result(ir_blocks):\n    \"\"\"Return the ConstructResult block from a list of IR blocks.\"\"\"\n", "input": "", "output": "    last_block = ir_blocks[-1]\n    if not isinstance(last_block, blocks.ConstructResult):\n        raise AssertionError(\n            u'The last IR block {} for IR blocks {} was unexpectedly not '\n            u'a ConstructResult block.'.format(last_block, ir_blocks))\n    return last_block", "category": "Python"}, {"instruction": "def type_decisioner(marc_xml, mono_callback, multimono_callback,\n                    periodical_callback):\n    \"\"\"\n    Detect type of the `marc_xml`. Call proper callback.\n\n    Args:\n        marc_xml (str): Filename or XML string. Don't use ``\\\\n`` in case of\n                        filename.\n        mono_callback (fn reference): Callback in case of monographic\n                      publications.\n        multimono_callback (fn reference): Callback used in case of\n                           multi-monographic publications.\n        periodical_callback (fn reference): Callback used in case of periodical\n                            publications.\n\n    Returns:\n        obj: Content returned by the callback.\n\n    Raises:\n        ValueError: In case that type couldn't be detected.\n    \"\"\"\n", "input": "", "output": "    marc_xml = _read_content_or_path(marc_xml)\n    record = MARCXMLRecord(marc_xml)\n\n    if record.is_monographic or record.is_single_unit:\n        return mono_callback()\n    elif record.is_multi_mono:\n        return multimono_callback()\n    elif record.is_continuing:\n        return periodical_callback()\n\n    raise ValueError(\"Can't identify type of the `marc_xml`!\")", "category": "Python"}, {"instruction": "def do_imports(self):\n        \"\"\"\n        Import all importable options\n        \"\"\"\n", "input": "", "output": "        self.do_import('worker_class', Worker)\n        self.do_import('queue_model', self.options.worker_class.queue_model)\n        self.do_import('error_model', self.options.worker_class.error_model)\n        self.do_import('callback', self.options.worker_class.callback)", "category": "Python"}, {"instruction": "async def read_frame(self, max_size: int) -> Frame:\n        \"\"\"\n        Read a single frame from the connection.\n\n        \"\"\"\n", "input": "", "output": "        frame = await Frame.read(\n            self.reader.readexactly,\n            mask=not self.is_client,\n            max_size=max_size,\n            extensions=self.extensions,\n        )\n        logger.debug(\"%s < %r\", self.side, frame)\n        return frame", "category": "Python"}, {"instruction": "def compare_config(self):\n        \"\"\"\n        Compare configuration to be merged with the one on the device.\n\n        Compare executed candidate config with the running config and\n        return a diff, assuming the loaded config will be merged with the\n        existing one.\n\n        :return:  Config diff.\n        \"\"\"\n", "input": "", "output": "        _show_merge = self._execute_config_show('show configuration merge')\n        _show_run = self._execute_config_show('show running-config')\n\n        diff = difflib.unified_diff(_show_run.splitlines(1)[2:-2], _show_merge.splitlines(1)[2:-2])\n        return ''.join([x.replace('\\r', '') for x in diff])", "category": "Python"}, {"instruction": "def make_ggnvp(f, g=lambda x: 1./2*np.sum(x**2, axis=-1), f_argnum=0):\n    \"\"\"Builds a function for evaluating generalized-Gauss-Newton-vector products\n    at a point. Slightly more expensive than mixed-mode.\"\"\"\n", "input": "", "output": "    @unary_to_nary\n    def _make_ggnvp(f, x):\n        f_vjp, f_x = _make_vjp(f, x)\n        g_hvp, grad_g_x = _make_vjp(grad(g), f_x)\n        f_jvp, _ = _make_vjp(f_vjp, vspace(grad_g_x).zeros())\n        def ggnvp(v): return f_vjp(g_hvp(f_jvp(v)))\n        return ggnvp\n    return _make_ggnvp(f, f_argnum)", "category": "Python"}, {"instruction": "def snmp_server_host_community(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        snmp_server = ET.SubElement(config, \"snmp-server\", xmlns=\"urn:brocade.com:mgmt:brocade-snmp\")\n        host = ET.SubElement(snmp_server, \"host\")\n        ip_key = ET.SubElement(host, \"ip\")\n        ip_key.text = kwargs.pop('ip')\n        community = ET.SubElement(host, \"community\")\n        community.text = kwargs.pop('community')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def fix_chrome_mac_platform(platform):\n    \"\"\"\n    Chrome on Mac OS adds minor version number and uses underscores instead\n    of dots. E.g. platform for Firefox will be: 'Intel Mac OS X 10.11'\n    but for Chrome it will be 'Intel Mac OS X 10_11_6'.\n\n    :param platform: - string like \"Macintosh; Intel Mac OS X 10.8\"\n    :return: platform with version number including minor number and formatted\n    with underscores, e.g. \"Macintosh; Intel Mac OS X 10_8_2\"\n    \"\"\"\n", "input": "", "output": "    ver = platform.split('OS X ')[1]\n    build_range = range(*MACOSX_CHROME_BUILD_RANGE[ver])\n    build = choice(build_range)\n    mac_ver = ver.replace('.', '_') + '_' + str(build)\n    return 'Macintosh; Intel Mac OS X %s' % mac_ver", "category": "Python"}, {"instruction": "def get(self, instance_name):\n        \"\"\"Get an ObjectRocket instance by name.\n\n        :param str instance_name: The name of the instance to retrieve.\n        :returns: A subclass of :py:class:`bases.BaseInstance`, or None if instance does not exist.\n        :rtype: :py:class:`bases.BaseInstance`\n        \"\"\"\n", "input": "", "output": "        url = self._url + instance_name + '/'\n        response = requests.get(url, **self._default_request_kwargs)\n        data = self._get_response_data(response)\n        return self._concrete_instance(data)", "category": "Python"}, {"instruction": "def _uncached_match(self, text, pos, cache, error):\n        \"\"\"Return length of match, ``None`` if no match.\"\"\"\n", "input": "", "output": "        m = self.re.match(text, pos)\n        if m is not None:\n            span = m.span()\n            node = RegexNode(self, text, pos, pos + span[1] - span[0])\n            node.match = m  # TODO: A terrible idea for cache size?\n            return node", "category": "Python"}, {"instruction": "def getObjectWorkflowStates(self):\n        \"\"\"\n        This method is used as a metacolumn.\n        Returns a dictionary with the workflow id as key and workflow state as\n        value.\n        :returns: {'review_state':'active',...}\n        \"\"\"\n", "input": "", "output": "        workflow = getToolByName(self, 'portal_workflow')\n        states = {}\n        for w in workflow.getWorkflowsFor(self):\n            state = w._getWorkflowStateOf(self).id\n            states[w.state_var] = state\n        return states", "category": "Python"}, {"instruction": "def get(self, block, name):\n        \"\"\"\n        Retrieve the value for the field named `name`.\n\n        If a value is provided for `default`, then it will be\n        returned if no value is set\n        \"\"\"\n", "input": "", "output": "        return self._kvs.get(self._key(block, name))", "category": "Python"}, {"instruction": "def perform_pca(A):\n    \"\"\"\n    Computes eigenvalues and eigenvectors of covariance matrix of A.\n    The rows of a correspond to observations, the columns to variables.\n    \"\"\"\n", "input": "", "output": "    # First subtract the mean\n    M = (A-numpy.mean(A.T, axis=1)).T\n    # Get eigenvectors and values of covariance matrix\n    return numpy.linalg.eig(numpy.cov(M))", "category": "Python"}, {"instruction": "def setter(self, can_set=None):\n        \"\"\"\n        Like `CachedProp.deleter` is for `CachedProp.can_delete`, but for `can_set`\n\n        :param can_set: boolean to change to it, and None to toggle\n        :type can_set: Optional[bool]\n        :return: self, so this can be used as a decorator like a `property`\n        :rtype: CachedProperty\n        \"\"\"\n", "input": "", "output": "        if can_set is None:\n            self._setter = not self._setter\n        else:\n            self._setter = bool(can_set)\n        # For use as decorator\n        return self", "category": "Python"}, {"instruction": "def resize(widthWindow, heightWindow):\n\t\"\"\"Initial settings for the OpenGL state machine, clear color, window size, etc\"\"\"\n", "input": "", "output": "\tglEnable(GL_BLEND)\n\tglEnable(GL_POINT_SMOOTH)\n\tglShadeModel(GL_SMOOTH)# Enables Smooth Shading\n\tglBlendFunc(GL_SRC_ALPHA,GL_ONE)#Type Of Blending To Perform\n\tglHint(GL_PERSPECTIVE_CORRECTION_HINT,GL_NICEST);#Really Nice Perspective Calculations\n\tglHint(GL_POINT_SMOOTH_HINT,GL_NICEST);#Really Nice Point Smoothing\n\tglDisable(GL_DEPTH_TEST)", "category": "Python"}, {"instruction": "def find_nested_meta_first(d, prop_name, version):\n    \"\"\"Returns obj. for badgerfish and val for hbf. Appropriate for nested literals\"\"\"\n", "input": "", "output": "    if _is_badgerfish_version(version):\n        return find_nested_meta_first_bf(d, prop_name)\n    p = '^' + prop_name\n    return d.get(p)", "category": "Python"}, {"instruction": "def stop(self):\n        \"\"\"Stop listening and close stream\"\"\"\n", "input": "", "output": "        if self.thread:\n            self.running = False\n            if isinstance(self.stream, ReadWriteStream):\n                self.stream.write(b'\\0' * self.chunk_size)\n            self.thread.join()\n            self.thread = None\n\n        self.engine.stop()\n\n        if self.pa:\n            self.pa.terminate()\n            self.stream.stop_stream()\n            self.stream = self.pa = None", "category": "Python"}, {"instruction": "def _has_name(soup_obj):\n    \"\"\"checks if soup_obj is really a soup object or just a string\n    If it has a name it is a soup object\"\"\"\n", "input": "", "output": "    try:\n        name = soup_obj.name\n        if name == None:\n            return False\n        return True\n    except AttributeError:\n        return False", "category": "Python"}, {"instruction": "def _load_builtin_xml(xmlpath, parser):\n    \"\"\"Loads the builtin function specifications from the builtin.xml file.\n\n    :arg parser: the DocParser instance for parsing the XML tags.\n    \"\"\"\n", "input": "", "output": "    #First we need to get hold of the fortpy directory so we can locate\n    #the isense/builtin.xml file.\n    result = {}\n\n    el = ET.parse(xmlpath).getroot()\n    if el.tag == \"builtin\":\n        for child in el:\n            anexec = _parse_xml(child, parser)\n            result[anexec.name.lower()] = anexec\n\n    return result", "category": "Python"}, {"instruction": "def _query(self, urls):\n        \"\"\"Test URLs for being listed by the service.\n\n        :param urls: a sequence of URLs  to be tested\n        :returns: a tuple containing chunk of URLs and a response\n        pertaining to them if the code of response was 200, which\n        means at least one of the queried URLs is matched in either\n        the phishing, malware, or unwanted software lists.\n        \"\"\"\n", "input": "", "output": "        urls = list(set(urls))\n        for i in range(0, len(urls), self.max_urls_per_request):\n            chunk = urls[i:i+self.max_urls_per_request]\n            response = self._query_once(chunk)\n            if response.status_code == 200:\n                yield chunk, response", "category": "Python"}, {"instruction": "def copy_extra_files(tile):\n    \"\"\"Copy all files listed in a copy_files and copy_products section.\n\n    Files listed in copy_files will be copied from the specified location\n    in the current component to the specified path under the output\n    folder.\n\n    Files listed in copy_products will be looked up with a ProductResolver\n    and copied copied to the specified path in the output folder.  There\n    is not currently a way to specify what type of product is being resolved.\n    The `short_name` given must be unique across all products from this\n    component and its direct dependencies.\n    \"\"\"\n", "input": "", "output": "\n    env = Environment(tools=[])\n    outputbase = os.path.join('build', 'output')\n\n    for src, dest in tile.settings.get('copy_files', {}).items():\n        outputfile = os.path.join(outputbase, dest)\n        env.Command([outputfile], [src], Copy(\"$TARGET\", \"$SOURCE\"))\n\n    resolver = ProductResolver.Create()\n    for src, dest in tile.settings.get('copy_products', {}).items():\n        prod = resolver.find_unique(None, src)\n        outputfile = os.path.join(outputbase, dest)\n\n        env.Command([outputfile], [prod.full_path], Copy(\"$TARGET\", \"$SOURCE\"))", "category": "Python"}, {"instruction": "def findinfiles_callback(self):\r\n        \"\"\"Find in files callback\"\"\"\n", "input": "", "output": "        widget = QApplication.focusWidget()\r\n        if not self.ismaximized:\r\n            self.dockwidget.setVisible(True)\r\n            self.dockwidget.raise_()\r\n        text = ''\r\n        try:\r\n            if widget.has_selected_text():\r\n                text = widget.get_selected_text()\r\n        except AttributeError:\r\n            # This is not a text widget deriving from TextEditBaseWidget\r\n            pass\r\n        self.findinfiles.set_search_text(text)\r\n        if text:\r\n            self.findinfiles.find()", "category": "Python"}, {"instruction": "def get(cls, dname):\n        \"\"\"\n        Get the requested domain\n        @param  dname:  Domain name\n        @type   dname:  str\n        @rtype: Domain or None\n        \"\"\"\n", "input": "", "output": "        Domain = cls\n        dname = dname.hostname if hasattr(dname, 'hostname') else dname.lower()\n        return Session.query(Domain).filter(Domain.name == dname).first()", "category": "Python"}, {"instruction": "def shortcut(*names):\n    \"\"\"Add an shortcut (alias) to a decorated function, but not to class methods!\n\n    Use aliased/alias decorators for class members!\n\n    Calling the shortcut (alias) will call the decorated function. The shortcut name will be appended\n    to the module's __all__ variable and the shortcut function will inherit the function's docstring\n\n    Examples\n    --------\n    In some module you have defined a function\n    >>> @shortcut('is_tmatrix') # doctest: +SKIP\n    >>> def is_transition_matrix(args): # doctest: +SKIP\n    ...     pass # doctest: +SKIP\n    Now you are able to call the function under its short name\n    >>> is_tmatrix(args) # doctest: +SKIP\n\n    \"\"\"\n", "input": "", "output": "    def wrap(f):\n        globals_ = f.__globals__\n        for name in names:\n            globals_[name] = f\n            if '__all__' in globals_ and name not in globals_['__all__']:\n                globals_['__all__'].append(name)\n        return f\n    return wrap", "category": "Python"}, {"instruction": "def HandleBlockReceived(self, inventory):\n        \"\"\"\n        Process a Block inventory payload.\n\n        Args:\n            inventory (neo.Network.Inventory):\n        \"\"\"\n", "input": "", "output": "        block = IOHelper.AsSerializableWithType(inventory, 'neo.Core.Block.Block')\n        if not block:\n            return\n\n        blockhash = block.Hash.ToBytes()\n        try:\n            if blockhash in BC.Default().BlockRequests:\n                BC.Default().BlockRequests.remove(blockhash)\n        except KeyError:\n            pass\n        try:\n            if blockhash in self.myblockrequests:\n                # logger.debug(f\"{self.prefix} received block: {block.Index}\")\n                self.heart_beat(HEARTBEAT_BLOCKS)\n                self.myblockrequests.remove(blockhash)\n        except KeyError:\n            pass\n        self.leader.InventoryReceived(block)", "category": "Python"}, {"instruction": "def fromaligns(args):\n    \"\"\"\n    %prog fromaligns out.aligns\n\n    Convert aligns file (old MCscan output) to anchors file.\n    \"\"\"\n", "input": "", "output": "    p = OptionParser(fromaligns.__doc__)\n    p.set_outfile()\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    alignsfile, = args\n    fp = must_open(alignsfile)\n    fw = must_open(opts.outfile, \"w\")\n    for row in fp:\n        if row.startswith(\"## Alignment\"):\n            print(\"###\", file=fw)\n            continue\n        if row[0] == '#' or not row.strip():\n            continue\n        atoms = row.split(':')[-1].split()\n        print(\"\\t\".join(atoms[:2]), file=fw)\n    fw.close()", "category": "Python"}, {"instruction": "def sendEmoji(\n        self,\n        emoji=None,\n        size=EmojiSize.SMALL,\n        thread_id=None,\n        thread_type=ThreadType.USER,\n    ):\n        \"\"\"\n        Deprecated. Use :func:`fbchat.Client.send` instead\n        \"\"\"\n", "input": "", "output": "        return self.send(\n            Message(text=emoji, emoji_size=size),\n            thread_id=thread_id,\n            thread_type=thread_type,\n        )", "category": "Python"}, {"instruction": "def init(**kwargs):\n    \"\"\"Initialize the specified names in the specified databases.\n\n    The general process is as follows:\n      - Ensure the database in question exists\n      - Ensure all tables exist in the database.\n    \"\"\"\n", "input": "", "output": "\n    # TODO: Iterate through all engines in name set.\n    database = kwargs.pop('database', False)\n    if database and not database_exists(engine['default'].url):\n        create_database(engine['default'].url, encoding='utf8')\n        clear_cache()\n\n    expression = lambda target, table: table.create(target)\n    test = lambda target, table: table.exists(target)\n    op(expression, test=test, primary='init', secondary='create', **kwargs)", "category": "Python"}, {"instruction": "def is_schema_of_common_names(schema: GraphQLSchema) -> bool:\n    \"\"\"Check whether this schema uses the common naming convention.\n\n    GraphQL schema define root types for each type of operation. These types are the\n    same as any other type and can be named in any manner, however there is a common\n    naming convention:\n\n    schema {\n      query: Query\n      mutation: Mutation\n    }\n\n    When using this naming convention, the schema description can be omitted.\n    \"\"\"\n", "input": "", "output": "    query_type = schema.query_type\n    if query_type and query_type.name != \"Query\":\n        return False\n\n    mutation_type = schema.mutation_type\n    if mutation_type and mutation_type.name != \"Mutation\":\n        return False\n\n    subscription_type = schema.subscription_type\n    if subscription_type and subscription_type.name != \"Subscription\":\n        return False\n\n    return True", "category": "Python"}, {"instruction": "def get_server_info(self):\n        \"\"\"\n        Return general server info.\n\n        :rtype: .ServerInfo\n        \"\"\"\n", "input": "", "output": "        response = self.get_proto(path='')\n        message = rest_pb2.GetApiOverviewResponse()\n        message.ParseFromString(response.content)\n        return ServerInfo(message)", "category": "Python"}, {"instruction": "def convert_to_dataset_file_metadata(self, file_data, path):\n        \"\"\" convert a set of file_data to a metadata file at path\n\n            Parameters\n            ==========\n            file_data: a dictionary of file data to write to file\n            path: the path to write the metadata to\n        \"\"\"\n", "input": "", "output": "        as_metadata = {\n            'path': os.path.join(path, file_data['name']),\n            'description': file_data['description']\n        }\n\n        schema = {}\n        fields = []\n        for column in file_data['columns']:\n            field = {\n                'name': column['name'],\n                'title': column['description'],\n                'type': column['type']\n            }\n            fields.append(field)\n        schema['fields'] = fields\n        as_metadata['schema'] = schema\n\n        return as_metadata", "category": "Python"}, {"instruction": "def duplicate_node(self, source_node_id, destination_node_id):\n        \"\"\"\n        Duplicate a node\n\n        :param source_node_id: Source node identifier\n        :param destination_node_id: Destination node identifier\n        :returns: New node instance\n        \"\"\"\n", "input": "", "output": "        source_node = self.get_node(source_node_id)\n        destination_node = self.get_node(destination_node_id)\n\n        # Some node don't have working dir like switch\n        if not hasattr(destination_node, \"working_dir\"):\n            return destination_node\n\n        destination_dir = destination_node.working_dir\n        try:\n            shutil.rmtree(destination_dir)\n            shutil.copytree(source_node.working_dir, destination_dir)\n        except OSError as e:\n            raise aiohttp.web.HTTPConflict(text=\"Can't duplicate node data: {}\".format(e))\n\n        # We force a refresh of the name. This force the rewrite\n        # of some configuration files\n        node_name = destination_node.name\n        destination_node.name = node_name + str(uuid4())\n        destination_node.name = node_name\n\n        return destination_node", "category": "Python"}, {"instruction": "def remove(self, item):\n        \"\"\"Remove an item from the list\n\n        :param item: The item to remove from the list.\n        :raises ValueError: If the item is not present in the list.\n        \"\"\"\n", "input": "", "output": "        if item not in self:\n            raise ValueError('objectlist.remove(item) failed, item not in list')\n        item_path = self._view_path_for(item)\n        giter = self._iter_for(item)\n        del self[giter]\n        self.emit('item-removed', item, item_path)", "category": "Python"}, {"instruction": "def load_np(self, imname, data_np, imtype, header):\n        \"\"\"Display a numpy image buffer in a remote Ginga reference viewer.\n\n        Parameters\n        ----------\n        imname : str\n            A name to use for the image in the reference viewer.\n\n        data_np : ndarray\n            This should be at least a 2D Numpy array.\n\n        imtype : str\n            Image type--currently ignored.\n\n        header : dict\n            Fits header as a dictionary, or other keyword metadata.\n\n        Returns\n        -------\n        0\n\n        Notes\n        -----\n        * The \"RC\" plugin needs to be started in the viewer for this to work.\n        \"\"\"\n", "input": "", "output": "        # future: handle imtype\n\n        load_buffer = self._client.lookup_attr('load_buffer')\n\n        return load_buffer(imname, self._chname,\n                           Blob(data_np.tobytes()),\n                           data_np.shape, str(data_np.dtype),\n                           header, {}, False)", "category": "Python"}, {"instruction": "def _set_scroll(self, *args):\n        \"\"\"Set horizontal scroll of scroll container and ticks Canvas\"\"\"\n", "input": "", "output": "        self._canvas_scroll.xview(*args)\n        self._canvas_ticks.xview(*args)", "category": "Python"}, {"instruction": "def _ensure_directory(path):\n    \"\"\"Ensure that the parent directory of `path` exists\"\"\"\n", "input": "", "output": "    dirname = os.path.dirname(path)\n    if not os.path.isdir(dirname):\n        os.makedirs(dirname)", "category": "Python"}, {"instruction": "def quote_xml(text):\n    \"\"\"Format a value for display as an XML text node.\n\n    Returns:\n        Unicode string (str on Python 3, unicode on Python 2)\n    \"\"\"\n", "input": "", "output": "    text = _coerce_unicode(text)\n\n    # If it's a CDATA block, return the text as is.\n    if text.startswith(CDATA_START):\n        return text\n\n    # If it's not a CDATA block, escape the XML and return the character\n    # encoded string.\n    return saxutils.escape(text)", "category": "Python"}, {"instruction": "def update_network_gateway(self, gateway_id, body=None):\n        \"\"\"Update a network gateway.\"\"\"\n", "input": "", "output": "        return self.put(self.network_gateway_path % gateway_id, body=body)", "category": "Python"}, {"instruction": "def init_sqlite_db(path, initTime=False):\n    \"\"\"\n    Initialize SQLite Database\n    \n    Args:\n        path(str): Path to database (Ex. '/home/username/my_sqlite.db').\n        initTime(Optional[bool]): If True, it will print the amount of time to generate database.\n\n    Example::\n    \n        from gsshapy.lib.db_tools import init_sqlite_db, create_session\n        \n        sqlite_db_path = '/home/username/my_sqlite.db'   \n        \n        init_postgresql_db(path=sqlite_db_path)\n        \n        sqlalchemy_url = init_sqlite_db(path=sqlite_db_path)\n        \n        db_work_sessionmaker = get_sessionmaker(sqlalchemy_url)\n\n        db_work_session = db_work_sessionmaker()\n        \n        ##DO WORK\n        \n        db_work_session.close()\n    \"\"\"\n", "input": "", "output": "    sqlite_base_url = 'sqlite:///'\n    \n    sqlalchemy_url = sqlite_base_url + path\n\n    init_time = init_db(sqlalchemy_url)\n    \n    if initTime:\n        print('TIME: {0} seconds'.format(init_time))\n        \n    return sqlalchemy_url", "category": "Python"}, {"instruction": "def val_to_css(C, val, factor, unit=CSS.rem, pt_per_em=12., decimals=2):\n        \"\"\"convert the Word val to a CSS unit\n        val     : The raw Word val\n        factor  : The conversion factor. If font sizes, typically factor=1/2., others factor=1/20.\n        unit    : The CSS unit to which we are converting, default CSS.rem\n        pt_per_em : The number of CSS.pt per em. 12. is the default, but 'tain't necessarily so.\n        \"\"\"\n", "input": "", "output": "        return (round(float(val) * factor / pt_per_em, decimals) * CSS.rem).asUnit(unit)", "category": "Python"}, {"instruction": "def doc(inherit=None, **kwargs):\n    \"\"\"Annotate the decorated view function or class with the specified Swagger\n    attributes.\n\n    Usage:\n\n    .. code-block:: python\n\n        @doc(tags=['pet'], description='a pet store')\n        def get_pet(pet_id):\n            return Pet.query.filter(Pet.id == pet_id).one()\n\n    :param inherit: Inherit Swagger documentation from parent classes\n    \"\"\"\n", "input": "", "output": "    def wrapper(func):\n        annotate(func, 'docs', [kwargs], inherit=inherit)\n        return activate(func)\n    return wrapper", "category": "Python"}, {"instruction": "def parse_buffer_to_png(data):\n    \"\"\"\n        Parse PNG file bytes to Pillow Image\n    \"\"\"\n", "input": "", "output": "\n    images = []\n\n    c1 = 0\n    c2 = 0\n    data_len = len(data)\n    while c1 < data_len:\n        # IEND can appear in a PNG without being the actual end\n        if data[c2:c2 + 4] == b'IEND' and (c2 + 8 == data_len or data[c2+9:c2+12] == b'PNG'):\n            images.append(Image.open(BytesIO(data[c1:c2 + 8])))\n            c1 = c2 + 8\n            c2 = c1\n        c2 += 1\n\n    return images", "category": "Python"}, {"instruction": "def run(self):\n        \"\"\"Main entrypoint method.\n\n        Returns\n        -------\n        new_nodes : `list`\n            Nodes to add to the doctree.\n        \"\"\"\n", "input": "", "output": "        self._env = self.state.document.settings.env\n\n        nodes = []\n\n        if 'toctree' in self.options:\n            # Insert a hidden toctree\n            toctree_node = self._build_toctree()\n            nodes.append(toctree_node)\n\n        # Placeholder node rendered in `process_task_topic_list`\n        list_node = task_topic_list()\n        list_node['types'] = self.types\n        list_node['root_namespace'] = self.options['root']\n        nodes.append(list_node)\n\n        return nodes", "category": "Python"}, {"instruction": "def insert(cls, index, interceptor):\n        \"\"\"\n        Add interceptor to the given index in the internal list.\n\n        Note: Raises ``ValueError`` if interceptor\n              does not extend ``OpenTracingInterceptor``\n        \"\"\"\n", "input": "", "output": "        cls._check(interceptor)\n        cls._interceptors.insert(index, interceptor)", "category": "Python"}, {"instruction": "def and_raise(self, exception, *args, **kwargs):\n        \"\"\"Causes the double to raise the provided exception when called.\n\n        If provided, additional arguments (positional and keyword) passed to\n        `and_raise` are used in the exception instantiation.\n\n        :param Exception exception: The exception to raise.\n        \"\"\"\n", "input": "", "output": "        def proxy_exception(*proxy_args, **proxy_kwargs):\n            raise exception\n\n        self._return_value = proxy_exception\n        return self", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"\n        Close this pipe object.  Future calls to `read` after the buffer\n        has been emptied will return immediately with an empty string.\n        \"\"\"\n", "input": "", "output": "        self._lock.acquire()\n        try:\n            self._closed = True\n            self._cv.notifyAll()\n            if self._event is not None:\n                self._event.set()\n        finally:\n            self._lock.release()", "category": "Python"}, {"instruction": "def relative_path(sub_directory='', function_index=1):\n    \"\"\"\n        This will return the file relative to this python script\n    :param subd_irectory:  str of the relative path\n    :param function_index: int of the number of function calls to go back\n    :return:               str of the full path\n    \"\"\"\n", "input": "", "output": "    frm = inspect.currentframe()\n    for i in range(function_index):\n        frm = frm.f_back\n    if frm.f_code.co_name == 'run_code':\n        frm = frm.f_back\n\n    if not isinstance(sub_directory, list):\n        sub_directory = sub_directory.replace('\\\\','/').split('/')\n\n    path = os.path.split(frm.f_code.co_filename)[0]\n    if sub_directory:\n        path = os.path.abspath(os.path.join(path, *sub_directory))\n    return path", "category": "Python"}, {"instruction": "def parse_timedelta(ev):\n    \"\"\"Parse a timedelta string in format [<num><multiplier> ]*\n    e.g. 2h 30m\n\n    :returns: timedelta\n    \"\"\"\n", "input": "", "output": "\n    chunks = ev['event-duration'].split()\n    tdargs = {}\n    for c in chunks:\n        try:\n            m = TIME_MULTIPLIERS[c[-1]]\n            val = int(c[:-1])\n            tdargs[m] = val\n        except KeyError:\n            log.error(", "category": "Python"}, {"instruction": "def get_causal_edge(stmt, activates):\n    \"\"\"Returns the causal, polar edge with the correct \"contact\".\"\"\"\n", "input": "", "output": "    any_contact = any(\n        evidence.epistemics.get('direct', False)\n        for evidence in stmt.evidence\n    )\n    if any_contact:\n        return pc.DIRECTLY_INCREASES if activates else pc.DIRECTLY_DECREASES\n\n    return pc.INCREASES if activates else pc.DECREASES", "category": "Python"}, {"instruction": "def _get_publish_details(actions, app_metadata_template):\n    \"\"\"\n    Get the changed application details after publishing.\n\n    :param actions: Actions taken during publishing\n    :type actions: list of str\n    :param app_metadata_template: Original template definitions of app metadata\n    :type app_metadata_template: dict\n    :return: Updated fields and values of the application\n    :rtype: dict\n    \"\"\"\n", "input": "", "output": "    if actions == [CREATE_APPLICATION]:\n        return {k: v for k, v in app_metadata_template.items() if v}\n\n    include_keys = [\n        ApplicationMetadata.AUTHOR,\n        ApplicationMetadata.DESCRIPTION,\n        ApplicationMetadata.HOME_PAGE_URL,\n        ApplicationMetadata.LABELS,\n        ApplicationMetadata.README_URL\n    ]\n\n    if CREATE_APPLICATION_VERSION in actions:\n        # SemanticVersion and SourceCodeUrl can only be updated by creating a new version\n        additional_keys = [ApplicationMetadata.SEMANTIC_VERSION, ApplicationMetadata.SOURCE_CODE_URL]\n        include_keys.extend(additional_keys)\n    return {k: v for k, v in app_metadata_template.items() if k in include_keys and v}", "category": "Python"}, {"instruction": "def _unsafe_update_server(self, disabled=False):\n        \"\"\"Update server with latest network configuration.\"\"\"\n", "input": "", "output": "        id = self.network.id\n        net = model.Network.from_neutron(self.network)\n        if id not in _networks:\n            if disabled:\n                return\n            _networks[id] = net\n            _networks[id].create()\n        elif disabled:\n            _networks[id].delete()\n            del _networks[id]\n        else:\n            _networks[id].update(net)\n            _networks[id] = net", "category": "Python"}, {"instruction": "def get_last_release_time(name, paths=None):\n    \"\"\"Returns the most recent time this package was released.\n\n    Note that releasing a variant into an already-released package is also\n    considered a package release.\n\n    Returns:\n        int: Epoch time of last package release, or zero if this cannot be\n        determined.\n    \"\"\"\n", "input": "", "output": "    entries = _get_families(name, paths)\n    max_time = 0\n\n    for repo, family_resource in entries:\n        time_ = repo.get_last_release_time(family_resource)\n        if time_ == 0:\n            return 0\n        max_time = max(max_time, time_)\n    return max_time", "category": "Python"}, {"instruction": "def _extract_options(config, options, *args):\n    \"\"\"Extract options values from a configparser, optparse pair.\n\n    Options given on command line take precedence over options read in the\n    configuration file.\n\n    Args:\n        config (dict): option values read from a config file through\n            configparser\n        options (optparse.Options): optparse 'options' object containing options\n            values from the command line\n        *args (str tuple): name of the options to extract\n    \"\"\"\n", "input": "", "output": "    extract = {}\n    for key in args:\n        if key not in args:\n            continue\n        extract[key] = config[key]\n        option = getattr(options, key, None)\n        if option is not None:\n            extract[key] = option\n    return extract", "category": "Python"}, {"instruction": "def send(self, cmd, timeout, wait_for_string, password):\n        \"\"\"Send command to the target device.\"\"\"\n", "input": "", "output": "        return self.target_device.send(cmd, timeout=timeout, wait_for_string=wait_for_string, password=password)", "category": "Python"}, {"instruction": "def nova_services_up(self):\n        \"\"\"Checks if required Nova services are up and running.\n\n        returns: True if all needed Nova services are up, False otherwise\n        \"\"\"\n", "input": "", "output": "        required = set(['nova-conductor', 'nova-cert', 'nova-scheduler',\n                       'nova-compute'])\n        try:\n            services = self._nclient.services.list()\n        # There are several individual Nova client exceptions but they have\n        # no other common base than Exception, hence the long list.\n        except Exception as e:\n            LOG.error('Failure determining running Nova services: %s', e)\n            return False\n        return not bool(required.difference(\n            [service.binary for service in services\n             if service.status == 'enabled' and service.state == 'up']))", "category": "Python"}, {"instruction": "def get_filenames(dirname):\n    \"\"\"Return all model output filenames inside a model output directory,\n    sorted by iteration number.\n\n    Parameters\n    ----------\n    dirname: str\n        A path to a directory.\n\n    Returns\n    -------\n    filenames: list[str]\n        Paths to all output files inside `dirname`, sorted in order of\n        increasing iteration number.\n    \"\"\"\n", "input": "", "output": "    filenames = glob.glob('{}/*.pkl'.format(dirname))\n    return sorted(filenames, key=_f_to_i)", "category": "Python"}, {"instruction": "def _SkipFieldMessage(tokenizer):\n  \"\"\"Skips over a field message.\n\n  Args:\n    tokenizer: A tokenizer to parse the field name and values.\n  \"\"\"\n", "input": "", "output": "\n  if tokenizer.TryConsume('<'):\n    delimiter = '>'\n  else:\n    tokenizer.Consume('{')\n    delimiter = '}'\n\n  while not tokenizer.LookingAt('>') and not tokenizer.LookingAt('}'):\n    _SkipField(tokenizer)\n\n  tokenizer.Consume(delimiter)", "category": "Python"}, {"instruction": "def fmt_dict_vals(dict_vals, shorten=True):\n    \"\"\"Returns list of key=val pairs formatted\n    for inclusion in an informative text string.\n    \"\"\"\n", "input": "", "output": "    items = dict_vals.items()\n    if not items:\n        return [fmt_val(None, shorten=shorten)]\n    return [\"%s=%s\" % (k, fmt_val(v, shorten=shorten)) for k,v in items]", "category": "Python"}, {"instruction": "async def update_from_devices(self):\n        \"\"\"Retrieve a list of &devices and values.\"\"\"\n", "input": "", "output": "        res = await self.get_json(URL_DEVICES.format(self._url))\n        if res:\n            self.devices.update_devices(res)\n            return True\n        return False", "category": "Python"}, {"instruction": "def hmac_hex_key(self, hmac_hex_key):\n        \"\"\"\n        Sets the hmac_hex_key of this CfsslAuthCredentials.\n        The key that is used to compute the HMAC of the request using the HMAC-SHA-256 algorithm. Must contain an even number of hexadecimal characters. \n\n        :param hmac_hex_key: The hmac_hex_key of this CfsslAuthCredentials.\n        :type: str\n        \"\"\"\n", "input": "", "output": "        if hmac_hex_key is None:\n            raise ValueError(\"Invalid value for `hmac_hex_key`, must not be `None`\")\n        if hmac_hex_key is not None and len(hmac_hex_key) > 64:\n            raise ValueError(\"Invalid value for `hmac_hex_key`, length must be less than or equal to `64`\")\n        if hmac_hex_key is not None and not re.search('^([a-fA-F0-9][a-fA-F0-9]){1,32}$', hmac_hex_key):\n            raise ValueError(\"Invalid value for `hmac_hex_key`, must be a follow pattern or equal to `/^([a-fA-F0-9][a-fA-F0-9]){1,32}$/`\")\n\n        self._hmac_hex_key = hmac_hex_key", "category": "Python"}, {"instruction": "def put(self, key, value, minutes):\n        \"\"\"\n        Store an item in the cache for a given number of minutes.\n\n        :param key: The cache key\n        :type key: str\n\n        :param value: The cache value\n        :type value: mixed\n\n        :param minutes: The lifetime in minutes of the cached value\n        :type minutes: int\n        \"\"\"\n", "input": "", "output": "        value = self.serialize(value)\n\n        minutes = max(1, minutes)\n\n        self._redis.setex(self._prefix + key, minutes * 60, value)", "category": "Python"}, {"instruction": "def dmxData(self, data: tuple):\n        \"\"\"\n        For legacy devices and to prevent errors, the length of the DMX data is normalized to 512\n        \"\"\"\n", "input": "", "output": "        newData = [0]*512\n        for i in range(0, min(len(data), 512)):\n            newData[i] = data[i]\n        self._dmxData = tuple(newData)\n        # in theory this class supports dynamic length, so the next line is correcting the length\n        self.length = 126 + len(self._dmxData)", "category": "Python"}, {"instruction": "def get_alert(self, alert):\n        \"\"\"\n        Recieves a day as an argument and returns the prediction for that alert\n        if is available. If not, function will return None.\n        \"\"\"\n", "input": "", "output": "        if alert > self.alerts_count() or self.alerts_count() is None:\n            return None\n        else:\n            return self.get()[alert-1]", "category": "Python"}, {"instruction": "def dir():\n        \"\"\"Return the list of patched function names. Used for patching\n        functions imported from the module.\n        \"\"\"\n", "input": "", "output": "        dir = [\n            'access', 'chdir', 'chmod', 'chown', 'close', 'fstat', 'fsync',\n            'getcwd', 'lchmod', 'link', 'listdir', 'lstat', 'makedirs',\n            'mkdir', 'mknod', 'open', 'read', 'readlink', 'remove',\n            'removedirs', 'rename', 'rmdir', 'stat', 'symlink', 'umask',\n            'unlink', 'utime', 'walk', 'write'\n        ]\n        if IS_PY2:\n            dir += ['getcwdu']\n        else:\n            dir += ['getcwdb', 'replace']\n            if sys.platform.startswith('linux'):\n                dir += [\n                    'fdatasync', 'getxattr', 'listxattr',\n                    'removexattr', 'setxattr'\n                ]\n        if use_scandir:\n            dir += ['scandir']\n        return dir", "category": "Python"}, {"instruction": "def convert(self, values, nan_rep, encoding, errors):\n        \"\"\" set the values from this selection: take = take ownership \"\"\"\n", "input": "", "output": "\n        self.values = Int64Index(np.arange(self.table.nrows))\n        return self", "category": "Python"}, {"instruction": "def do_filesizeformat(value, binary=False):\n    \"\"\"Format the value like a 'human-readable' file size (i.e. 13 kB,\n    4.1 MB, 102 Bytes, etc).  Per default decimal prefixes are used (Mega,\n    Giga, etc.), if the second parameter is set to `True` the binary\n    prefixes are used (Mebi, Gibi).\n    \"\"\"\n", "input": "", "output": "    bytes = float(value)\n    base = binary and 1024 or 1000\n    prefixes = [\n        (binary and 'KiB' or 'kB'),\n        (binary and 'MiB' or 'MB'),\n        (binary and 'GiB' or 'GB'),\n        (binary and 'TiB' or 'TB'),\n        (binary and 'PiB' or 'PB'),\n        (binary and 'EiB' or 'EB'),\n        (binary and 'ZiB' or 'ZB'),\n        (binary and 'YiB' or 'YB')\n    ]\n    if bytes == 1:\n        return '1 Byte'\n    elif bytes < base:\n        return '%d Bytes' % bytes\n    else:\n        for i, prefix in enumerate(prefixes):\n            unit = base ** (i + 2)\n            if bytes < unit:\n                return '%.1f %s' % ((base * bytes / unit), prefix)\n        return '%.1f %s' % ((base * bytes / unit), prefix)", "category": "Python"}, {"instruction": "def binormal_curve_single_list(obj, param_list, normalize):\n    \"\"\" Evaluates the curve binormal vectors at the given list of parameter values.\n\n    :param obj: input curve\n    :type obj: abstract.Curve\n    :param param_list: parameter list\n    :type param_list: list or tuple\n    :param normalize: if True, the returned vector is converted to a unit vector\n    :type normalize: bool\n    :return: a list containing \"point\" and \"vector\" pairs\n    :rtype: tuple\n    \"\"\"\n", "input": "", "output": "    ret_vector = []\n    for param in param_list:\n        temp = binormal_curve_single(obj, param, normalize)\n        ret_vector.append(temp)\n    return tuple(ret_vector)", "category": "Python"}, {"instruction": "def _get_exec_binary(binary, kw):\n    \"\"\"\n    On win32, the subprocess module can only reliably resolve the\n    target binary if it's actually a binary; as for a Node.js script\n    it seems to only work iff shell=True was specified, presenting\n    a security risk.  Resolve the target manually through which will\n    account for that.\n\n    The kw argument is the keyword arguments that will be passed into\n    whatever respective subprocess.Popen family of methods.  The PATH\n    environment variable will be used if available.\n    \"\"\"\n", "input": "", "output": "\n    binary = which(binary, path=kw.get('env', {}).get('PATH'))\n    if binary is None:\n        raise_os_error(errno.ENOENT)\n    return binary", "category": "Python"}, {"instruction": "def hist2d(self, da, **kwargs):\n        \"\"\"Make the two dimensional histogram\n\n        Parameters\n        ----------\n        da: xarray.DataArray\n            The data source\"\"\"\n", "input": "", "output": "        if self.value is None or self.value == 'counts':\n            normed = False\n        else:\n            normed = True\n        y = da.values\n        x = da.coords[da.dims[0]].values\n        counts, xedges, yedges = np.histogram2d(\n            x, y, normed=normed, **kwargs)\n        if self.value == 'counts':\n            counts = counts / counts.sum().astype(float)\n        return counts, xedges, yedges", "category": "Python"}, {"instruction": "def dumps(obj):\n    \"\"\"\n    Dumps a serializable object to JSON. This API maps to the Python built-in\n    json dumps method, with a few differences:\n\n    * The return value is always valid JSON according to RFC 7159.\n    * The input can be any of the following types:\n        - SFrame\n        - SArray\n        - SGraph\n        - single flexible_type (Image, int, long, float, datetime.datetime)\n        - recursive flexible_type (list, dict, array.array)\n        - recursive variant_type (list or dict of all of the above)\n    * Serialized result includes both data and schema. Deserialization requires\n      valid schema information to disambiguate various other wrapped types\n      (like Image) from dict.\n    \"\"\"\n", "input": "", "output": "    (data, schema) = to_serializable(obj)\n    return _json.dumps({'data': data, 'schema': schema})", "category": "Python"}, {"instruction": "def register(self, app, options, first_registration=False):\n        \"\"\"\n        Called by :meth:`~flask.Flask.register_blueprint` to register a blueprint\n        on the application.  This can be overridden to customize the register\n        behavior.  Keyword arguments from\n        :func:`~flask.Flask.register_blueprint` are directly forwarded to this\n        method in the `options` dictionary.\n        \"\"\"\n", "input": "", "output": "        self._got_registered_once = True\n        state = self.make_setup_state(app, options, first_registration)\n        if self.has_static_folder:\n            state.add_url_rule(self.static_url_path + '/<path:filename>',\n                               view_func=self.send_static_file,\n                               endpoint=f'{self.bundle._blueprint_name}.static',\n                               register_with_babel=False)\n\n        for deferred in self.bundle._deferred_functions:\n            deferred(self)\n\n        for deferred in self.deferred_functions:\n            deferred(state)", "category": "Python"}, {"instruction": "def report(data):\n    \"\"\"Create a Rmd report for small RNAseq analysis\"\"\"\n", "input": "", "output": "    work_dir = dd.get_work_dir(data[0][0])\n    out_dir = op.join(work_dir, \"report\")\n    safe_makedir(out_dir)\n    summary_file = op.join(out_dir, \"summary.csv\")\n    with file_transaction(summary_file) as out_tx:\n        with open(out_tx, 'w') as out_handle:\n            out_handle.write(\"sample_id,%s\\n\" % _guess_header(data[0][0]))\n            for sample in data:\n                info = sample[0]\n                group = _guess_group(info)\n                files = info[\"seqbuster\"] if \"seqbuster\" in info else \"None\"\n                out_handle.write(\",\".join([dd.get_sample_name(info),\n                                           group]) + \"\\n\")\n    _modify_report(work_dir, out_dir)\n    return summary_file", "category": "Python"}, {"instruction": "def save_image(image, local_filename):\n    \"\"\"\n    Saves a Docker image as a compressed tarball. This command line client method is a suitable alternative, if the\n    Remove API method is too slow.\n\n    :param image: Image id or tag.\n    :type image: unicode\n    :param local_filename: Local file name to store the image into. If this is a directory, the image will be stored\n      there as a file named ``image_<Image name>.tar.gz``.\n    \"\"\"\n", "input": "", "output": "    r_name, __, i_name = image.rpartition('/')\n    i_name, __, __ = i_name.partition(':')\n    with temp_dir() as remote_tmp:\n        archive = posixpath.join(remote_tmp, 'image_{0}.tar.gz'.format(i_name))\n        run('docker save {0} | gzip --stdout > {1}'.format(image, archive), shell=False)\n        get(archive, local_filename)", "category": "Python"}, {"instruction": "def call(self, method, *args, **kwargs):\n        \"\"\"\n        Calls an RPC function\n        \"\"\"\n", "input": "", "output": "        tried_reconnect = False\n        for _ in range(2):\n            try:\n                self._send_call(self.deluge_version, self.deluge_protocol_version, method, *args, **kwargs)\n                return self._receive_response(self.deluge_version, self.deluge_protocol_version)\n            except (socket.error, ConnectionLostException, CallTimeoutException):\n                if self.automatic_reconnect:\n                    if tried_reconnect:\n                        raise FailedToReconnectException()\n                    else:\n                        try:\n                            self.reconnect()\n                        except (socket.error, ConnectionLostException, CallTimeoutException):\n                            raise FailedToReconnectException()\n\n                    tried_reconnect = True\n                else:\n                    raise", "category": "Python"}, {"instruction": "def add_hours(self, datetimestr, n):\n        \"\"\"Returns a time that n hours after a time.\n\n        :param datetimestr: a datetime object or a datetime str\n        :param n: number of hours, value can be negative\n\n        **\u4e2d\u6587\u6587\u6863**\n\n        \u8fd4\u56de\u7ed9\u5b9a\u65e5\u671fN\u5c0f\u65f6\u4e4b\u540e\u7684\u65f6\u95f4\u3002\n        \"\"\"\n", "input": "", "output": "        a_datetime = self.parse_datetime(datetimestr)\n        return a_datetime + timedelta(seconds=3600 * n)", "category": "Python"}, {"instruction": "def get_predictions_under_consistency(instance):\n    '''\n    Computes the set of signs on edges/vertices that can be cautiously\n    derived from [instance], minus those that are a direct consequence\n    of obs_[ev]label predicates\n    '''\n", "input": "", "output": "    inst   = instance.to_file()\n    prg    = [ prediction_prg, inst, exclude_sol([]) ]\n    solver = GringoClasp(clasp_options='--project --enum-mode cautious')\n    models = solver.run(prg, collapseTerms=True, collapseAtoms=False)\n    os.unlink(inst)\n    os.unlink(prg[2])\n    return whatsnew(instance,models[0])", "category": "Python"}, {"instruction": "def _handle_wrong_field(cls, field_name, field_type):\n        \"\"\"Raise an exception whenever an invalid attribute with\n        the given name was attempted to be set to or retrieved from\n        this model class.\n\n        Assumes that the given field is invalid, without making any checks.\n\n        Also adds an entry to the logs.\n        \"\"\"\n", "input": "", "output": "        if field_type == ATTR_TYPE_READ:\n            field_type = 'readable'\n        elif field_type == ATTR_TYPE_WRITE:\n            field_type = 'writable'\n        elif field_type == ATTR_TYPE_URL:\n            field_type = 'URL'\n        else:\n            raise AttributeError('Invalid attribute type: {}'.format(\n                field_type\n            ))\n\n        msg = '{} has no {} attribute \"{}\"'.format(\n            cls.__name__,\n            field_type,\n            field_name\n        )\n        _logger.error(msg)\n        raise AttributeError(msg)", "category": "Python"}, {"instruction": "def set_ncbi_email():\n    \"\"\"Set contact email for NCBI.\"\"\"\n", "input": "", "output": "    Entrez.email = args.email\n    logger.info(\"Set NCBI contact email to %s\", args.email)\n    Entrez.tool = \"genbank_get_genomes_by_taxon.py\"", "category": "Python"}, {"instruction": "def parse_tablature(lines):\n    ''' Parse a list of lines into a `Tablature`. '''\n", "input": "", "output": "    lines = [parse_line(l) for l in lines]\n    return Tablature(lines=lines)", "category": "Python"}, {"instruction": "def gate(self, name, params, qubits):\n        \"\"\"\n        Add a gate to the program.\n\n        .. note::\n\n            The matrix elements along each axis are ordered by bitstring. For two qubits the order\n            is ``00, 01, 10, 11``, where the the bits **are ordered in reverse** by the qubit index,\n            i.e., for qubits 0 and 1 the bitstring ``01`` indicates that qubit 0 is in the state 1.\n            See also :ref:`the related documentation section in the QVM Overview <basis-ordering>`.\n\n        :param string name: The name of the gate.\n        :param list params: Parameters to send to the gate.\n        :param list qubits: Qubits that the gate operates on.\n        :return: The Program instance\n        :rtype: Program\n        \"\"\"\n", "input": "", "output": "        return self.inst(Gate(name, params, [unpack_qubit(q) for q in qubits]))", "category": "Python"}, {"instruction": "def collate(binder, ruleset=None, includes=None):\n    \"\"\"Given a ``Binder`` as ``binder``, collate the content into a new set\n    of models.\n    Returns the collated binder.\n\n    \"\"\"\n", "input": "", "output": "    html_formatter = SingleHTMLFormatter(binder, includes)\n    raw_html = io.BytesIO(bytes(html_formatter))\n    collated_html = io.BytesIO()\n\n    if ruleset is None:\n        # No ruleset found, so no cooking necessary.\n        return binder\n\n    easybake(ruleset, raw_html, collated_html)\n\n    collated_html.seek(0)\n    collated_binder = reconstitute(collated_html)\n\n    return collated_binder", "category": "Python"}, {"instruction": "def validity_duration(self):\n        \"\"\"\n        How long this parameter value is valid.\n\n        .. note: There is also an option when subscribing to get updated when\n                 the parameter values expire.\n\n        :type: :class:`~datetime.timedelta`\n        \"\"\"\n", "input": "", "output": "        if self._proto.HasField('expireMillis'):\n            return timedelta(milliseconds=self._proto.expireMillis)\n        return None", "category": "Python"}, {"instruction": "def server_list_min(self):\n        '''\n        List minimal information about servers\n        '''\n", "input": "", "output": "        nt_ks = self.compute_conn\n        ret = {}\n        for item in nt_ks.servers.list(detailed=False):\n            try:\n                ret[item.name] = {\n                    'id': item.id,\n                    'state': 'Running'\n                }\n            except TypeError:\n                pass\n        return ret", "category": "Python"}, {"instruction": "def check_version(version: str):\n    \"\"\"\n    Checks given version against code version and determines compatibility.\n    Throws if versions are incompatible.\n\n    :param version: Given version.\n    \"\"\"\n", "input": "", "output": "    code_version = parse_version(__version__)\n    given_version = parse_version(version)\n    check_condition(code_version[0] == given_version[0],\n                    \"Given release version (%s) does not match release code version (%s)\" % (version, __version__))\n    check_condition(code_version[1] == given_version[1],\n                    \"Given major version (%s) does not match major code version (%s)\" % (version, __version__))", "category": "Python"}, {"instruction": "def container_fs_limit_bytes(self, metric, scraper_config):\n        \"\"\"\n        Number of bytes that can be consumed by the container on this filesystem.\n        This method is used by container_fs_usage_bytes, it doesn't report any metric\n        \"\"\"\n", "input": "", "output": "        pct_m_name = scraper_config['namespace'] + '.filesystem.usage_pct'\n        if metric.type not in METRIC_TYPES:\n            self.log.error(\"Metric type %s unsupported for metric %s\" % (metric.type, metric.name))\n            return\n        self._process_limit_metric('', metric, self.fs_usage_bytes, scraper_config, pct_m_name)", "category": "Python"}, {"instruction": "def _ka_decode(self, msg):\n        \"\"\"KA: Keypad areas for all keypads.\"\"\"\n", "input": "", "output": "        return {'keypad_areas': [ord(x)-0x31 for x in msg[4:4+Max.KEYPADS.value]]}", "category": "Python"}, {"instruction": "def colorize(text, messageType=None):\n    \"\"\"\n    Function that colorizes a message.\n\n    Args:\n    -----\n        text: The string to be colorized.\n        messageType: Possible options include \"ERROR\", \"WARNING\", \"SUCCESS\",\n            \"INFO\" or \"BOLD\".\n\n    Returns:\n    --------\n        string: Colorized if the option is correct, including a tag at the end\n            to reset the formatting.\n    \"\"\"\n", "input": "", "output": "    formattedText = str(text)\n    # Set colors\n    if \"ERROR\" in messageType:\n        formattedText = colorama.Fore.RED + formattedText\n    elif \"WARNING\" in messageType:\n        formattedText = colorama.Fore.YELLOW + formattedText\n    elif \"SUCCESS\" in messageType:\n        formattedText = colorama.Fore.GREEN + formattedText\n    elif \"INFO\" in messageType:\n        formattedText = colorama.Fore.BLUE + formattedText\n\n    # Set emphashis mode\n    if \"BOLD\" in messageType:\n        formattedText = colorama.Style.BRIGHT + formattedText\n\n    return formattedText + colorama.Style.RESET_ALL", "category": "Python"}, {"instruction": "def metadata(self):\n        \"\"\"Return dict representation of this cookbook's metadata.rb .\"\"\"\n", "input": "", "output": "        self.metadata_path = os.path.join(self.path, 'metadata.rb')\n        if not os.path.isfile(self.metadata_path):\n            raise ValueError(\"Cookbook needs metadata.rb, %s\"\n                             % self.metadata_path)\n\n        if not self._metadata:\n            self._metadata = MetadataRb(open(self.metadata_path, 'r+'))\n\n        return self._metadata", "category": "Python"}, {"instruction": "def get_instructions(self):\n        \"\"\"\n        Get the instructions\n\n        :rtype: a generator of each :class:`Instruction` (or a cached list of instructions if you have setup instructions)\n        \"\"\"\n", "input": "", "output": "        # it is possible to a cache for instructions (avoid a new disasm)\n        if self.cached_instructions is None:\n            lsa = LinearSweepAlgorithm()\n            ins = lsa.get_instructions(self.CM, self.size, self.insn,\n                                          self.idx)\n            self.cached_instructions = list(ins)\n\n        for i in self.cached_instructions:\n            yield i", "category": "Python"}, {"instruction": "def _proxy(self):\n        \"\"\"\n        Generate an instance context for the instance, the context is capable of\n        performing various actions.  All instance actions are proxied to the context\n\n        :returns: PayloadContext for this PayloadInstance\n        :rtype: twilio.rest.api.v2010.account.recording.add_on_result.payload.PayloadContext\n        \"\"\"\n", "input": "", "output": "        if self._context is None:\n            self._context = PayloadContext(\n                self._version,\n                account_sid=self._solution['account_sid'],\n                reference_sid=self._solution['reference_sid'],\n                add_on_result_sid=self._solution['add_on_result_sid'],\n                sid=self._solution['sid'],\n            )\n        return self._context", "category": "Python"}, {"instruction": "def _get_aliases(parse_info):\n        \"\"\"get aliases from parse info.\n\n        :param parse_info: Parsed info from html soup.\n        \"\"\"\n", "input": "", "output": "        return [\n            div.string.strip()\n            for div in parse_info.find('div', id='editassociated')\n            if div.string is not None\n        ]", "category": "Python"}, {"instruction": "def has(self, name, ignore_empty=False):\n        \"\"\"Return ``True`` if any parameter in the template is named *name*.\n\n        With *ignore_empty*, ``False`` will be returned even if the template\n        contains a parameter with the name *name*, if the parameter's value\n        is empty. Note that a template may have multiple parameters with the\n        same name, but only the last one is read by the MediaWiki parser.\n        \"\"\"\n", "input": "", "output": "        name = str(name).strip()\n        for param in self.params:\n            if param.name.strip() == name:\n                if ignore_empty and not param.value.strip():\n                    continue\n                return True\n        return False", "category": "Python"}, {"instruction": "def synctree(src, dst, onexist=None):\n    \"\"\"Recursively sync files at directory src to dst\n\n    This is more or less equivalent to::\n\n       cp -n -R ${src}/ ${dst}/\n\n    If a file at the same path exists in src and dst, it is NOT overwritten\n    in dst. Pass ``onexist`` in order to raise an error on such conditions.\n\n    Args:\n        src (path-like): source directory\n        dst (path-like): destination directory, does not need to exist\n        onexist (callable): function to call if file exists at destination,\n            takes the full path to destination file as only argument\n    \"\"\"\n", "input": "", "output": "    src = pathlib.Path(src).resolve()\n    dst = pathlib.Path(dst).resolve()\n\n    if not src.is_dir():\n        raise ValueError\n\n    if dst.exists() and not dst.is_dir():\n        raise ValueError\n\n    if onexist is None:\n        def onexist(): pass\n\n    _synctree(src, dst, onexist)", "category": "Python"}, {"instruction": "def download_experiment(self, experiment_id):\n        \"\"\"\n            download_experiment:\n        \"\"\"\n", "input": "", "output": "        req = self.query_records('experiment', self.access_key,\n                                 self.secret_key,\n                                 query='download/'+experiment_id)\n        print req.text\n        return", "category": "Python"}, {"instruction": "def describe_vpc_peering_connection(name,\n                                    region=None,\n                                    key=None,\n                                    keyid=None,\n                                    profile=None):\n    '''\n    Returns any VPC peering connection id(s) for the given VPC\n    peering connection name.\n\n    VPC peering connection ids are only returned for connections that\n    are in the ``active``, ``pending-acceptance`` or ``provisioning``\n    state.\n\n    .. versionadded:: 2016.11.0\n\n    :param name: The string name for this VPC peering connection\n    :param region: The aws region to use\n    :param key: Your aws key\n    :param keyid: The key id associated with this aws account\n    :param profile: The profile to use\n    :return: dict\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_vpc.describe_vpc_peering_connection salt-vpc\n        # Specify a region\n        salt myminion boto_vpc.describe_vpc_peering_connection salt-vpc region=us-west-2\n\n    '''\n", "input": "", "output": "    conn = _get_conn3(region=region, key=key, keyid=keyid,\n                      profile=profile)\n    return {\n        'VPC-Peerings': _get_peering_connection_ids(name, conn)\n    }", "category": "Python"}, {"instruction": "def execute(self):\n        \"\"\"\n        Execute the actions necessary to perform a `molecule idempotence` and\n        returns None.\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        self.print_info()\n        if not self._config.state.converged:\n            msg = 'Instances not converged.  Please converge instances first.'\n            util.sysexit_with_message(msg)\n\n        output = self._config.provisioner.converge(out=None, err=None)\n\n        idempotent = self._is_idempotent(output)\n        if idempotent:\n            msg = 'Idempotence completed successfully.'\n            LOG.success(msg)\n        else:\n            msg = ('Idempotence test failed because of the following tasks:\\n'\n                   u'{}').format('\\n'.join(self._non_idempotent_tasks(output)))\n            util.sysexit_with_message(msg)", "category": "Python"}, {"instruction": "def dragMoveEvent( self, event ):\r\n        \"\"\"\r\n        Processes the drag drop event using the filter set by the \\\r\n        setDragDropFilter\r\n        \r\n        :param      event | <QDragEvent>\r\n        \"\"\"\n", "input": "", "output": "        filt = self.dragDropFilter()\r\n        if ( not filt ):\r\n            super(XCalendarWidget, self).dragMoveEvent(event)\r\n            return\r\n        \r\n        filt(self, event)", "category": "Python"}, {"instruction": "def _getAttr(self, attr):\n        \"\"\"\n        Subclasses may override this method.\n\n        If a subclass does not override this method,\n        it must implement '_get_attributeName' methods\n        for all Info methods.\n        \"\"\"\n", "input": "", "output": "        meth = \"_get_%s\" % attr\n        if not hasattr(self, meth):\n            raise AttributeError(\"No getter for attribute '%s'.\" % attr)\n        meth = getattr(self, meth)\n        value = meth()\n        return value", "category": "Python"}, {"instruction": "def _src_media_url_for_video(self, video):\n        '''Get the url for the video media that we can send to Clarify'''\n", "input": "", "output": "        src_url = None\n        best_height = 0\n        best_source = None\n\n        # TODO: This assumes we have ingested videos. For remote videos, check if the remote flag is True\n        # and if so, use the src url from the Asset endpoint.\n        video_sources = self.bc_client.get_video_sources(video['id'])\n        # Look for codec H264 with good resolution\n        for source in video_sources:\n            height = source.get('height', 0)\n            codec = source.get('codec')\n            if source.get('src') and codec and codec.upper() == 'H264' and height <= 1080 and height > best_height:\n                best_source = source\n\n        if best_source is not None:\n            src_url = best_source['src']\n        return src_url", "category": "Python"}, {"instruction": "def _set_max_value(self, max_value):\n        \"\"\"Sets current maximum allowed value\"\"\"\n", "input": "", "output": "\n        self._external_max_value = max_value\n\n        # Check that the current value of the parameter is still within the boundaries. If not, issue a warning\n\n        if self._external_max_value is not None and self.value > self._external_max_value:\n\n            warnings.warn(\"The current value of the parameter %s (%s) \"\n                          \"was above the new maximum %s.\" % (self.name, self.value, self._external_max_value),\n                          exceptions.RuntimeWarning)\n            self.value = self._external_max_value", "category": "Python"}, {"instruction": "def tan_rand(q, seed=9):\n    \"\"\"Find a random vector in the tangent space of the n sphere\n\n    This function will find a random orthogonal vector to q.\n\n    Parameters\n    ----------\n    q\n        (n+1,) array which is in the n-sphere\n\n    Returns\n    -------\n    qd\n        (n+1,) array which is orthogonal to n-sphere and also random\n\n    \"\"\"\n", "input": "", "output": "    # probably need a check in case we get a parallel vector\n    rs = np.random.RandomState(seed)\n    rvec = rs.rand(q.shape[0])\n\n    qd = np.cross(rvec, q)\n    qd = qd / np.linalg.norm(qd)\n\n    while np.dot(q, qd) > 1e-6:\n        rvec = rs.rand(q.shape[0])\n        qd = np.cross(rvec, q)\n        qd = qd / np.linalg.norm(qd)\n\n    return qd", "category": "Python"}, {"instruction": "def get_fields(self):\n        \"\"\"Dynamically adapt fields based on the current request.\"\"\"\n", "input": "", "output": "        fields = super(DataSerializer, self).get_fields()\n\n        # Hide collections/entities fields on list views as fetching them may be expensive.\n        if self.parent is not None:\n            del fields['collections']\n            del fields['entities']\n\n        return fields", "category": "Python"}, {"instruction": "def check_return(callable_name: str, r: typing.Any,\n                 hints: typing.Mapping[str, type]) -> None:\n    \"\"\"Check return type, raise :class:`TypeError` if return type is not\n    expected type.\n\n    :param str callable_name: callable name of :func:`~.typechecked` checked\n    :param r: returned result\n    :param hints: assumed type of given ``r``\n\n    \"\"\"\n", "input": "", "output": "    correct = True\n    if 'return' not in hints:\n        return\n    _, correct = check_type(r, hints['return'])\n    if not correct:\n        raise TypeError(\n            'Incorrect return type `{}`, expected {}. for: {}'.format(\n                type(r), hints.get('return'), callable_name\n            )\n        )", "category": "Python"}, {"instruction": "def update_recent_file_menu(self):\r\n        \"\"\"Update recent file menu\"\"\"\n", "input": "", "output": "        recent_files = []\r\n        for fname in self.recent_files:\r\n            if self.is_file_opened(fname) is None and osp.isfile(fname):\r\n                recent_files.append(fname)\r\n        self.recent_file_menu.clear()\r\n        if recent_files:\r\n            for fname in recent_files:\r\n                action = create_action(self, fname,\r\n                                       icon=ima.icon('FileIcon'),\r\n                                       triggered=self.load)\r\n                action.setData(to_qvariant(fname))\r\n                self.recent_file_menu.addAction(action)\r\n        self.clear_recent_action.setEnabled(len(recent_files) > 0)\r\n        add_actions(self.recent_file_menu, (None, self.max_recent_action,\r\n                                            self.clear_recent_action))", "category": "Python"}, {"instruction": "def init(self, permits):\n        \"\"\"\n        Try to initialize this Semaphore instance with the given permit count.\n\n        :param permits: (int), the given permit count.\n        :return: (bool), ``true`` if initialization success.\n        \"\"\"\n", "input": "", "output": "        check_not_negative(permits, \"Permits cannot be negative!\")\n        return self._encode_invoke(semaphore_init_codec, permits=permits)", "category": "Python"}, {"instruction": "def add(self, node):\n        \"\"\"Add one node as descendant\n        \"\"\"\n", "input": "", "output": "        self.sons.append(node)\n        node.parent = self", "category": "Python"}, {"instruction": "def _add_graphicFrame_containing_table(self, rows, cols, x, y, cx, cy):\n        \"\"\"\n        Return a newly added ``<p:graphicFrame>`` element containing a table\n        as specified by the parameters.\n        \"\"\"\n", "input": "", "output": "        _id = self._next_shape_id\n        name = 'Table %d' % (_id-1)\n        graphicFrame = self._spTree.add_table(\n            _id, name, rows, cols, x, y, cx, cy\n        )\n        return graphicFrame", "category": "Python"}, {"instruction": "def load_model(ecore_model_path):\n    \"\"\"Load a single Ecore model and return the root package.\"\"\"\n", "input": "", "output": "    rset = pyecore.resources.ResourceSet()\n    uri_implementation = select_uri_implementation(ecore_model_path)\n    resource = rset.get_resource(uri_implementation(ecore_model_path))\n    return resource.contents[0]", "category": "Python"}, {"instruction": "def log_error(self, fmt, *fmt_args):\n        \"\"\" Log to syslog. \"\"\"\n", "input": "", "output": "        msg = self.my_address_string() + \" - - \" + fmt % fmt_args\n        my_log_message(args, syslog.LOG_ERR, msg)", "category": "Python"}, {"instruction": "def add_menu(self, menu):\n        \"\"\"\n        Adds a sub-menu to the editor context menu.\n\n        Menu are put at the bottom of the context menu.\n\n        .. note:: to add a menu in the middle of the context menu, you can\n            always add its menuAction().\n\n        :param menu: menu to add\n        \"\"\"\n", "input": "", "output": "        self._menus.append(menu)\n        self._menus = sorted(list(set(self._menus)), key=lambda x: x.title())\n        for action in menu.actions():\n            action.setShortcutContext(QtCore.Qt.WidgetShortcut)\n        self.addActions(menu.actions())", "category": "Python"}, {"instruction": "def _reset(self):\n        \"\"\"Set the filter attributes to its default values\"\"\"\n", "input": "", "output": "        self._in_declare = False\n        self._is_create = False\n        self._begin_depth = 0\n\n        self.consume_ws = False\n        self.tokens = []\n        self.level = 0", "category": "Python"}, {"instruction": "def act(self, cmd_name, params=None):\n        \"\"\" Run the specified command with its parameters.\"\"\"\n", "input": "", "output": "\n        command = getattr(self, cmd_name)\n        if params:\n            command(params)\n        else:\n            command()", "category": "Python"}, {"instruction": "def to_mixed_case(s):\n    \"\"\"\n    convert upper snake case string to mixed case, e.g. MIXED_CASE becomes\n    MixedCase\n    \"\"\"\n", "input": "", "output": "    out = ''\n    last_c = ''\n    for c in s:\n        if c == '_':\n            pass\n        elif last_c in ('', '_'):\n            out += c.upper()\n        else:\n            out += c.lower()\n        last_c = c\n    return out", "category": "Python"}, {"instruction": "def stepEnabled(self):\n        \"\"\"Virtual function that determines whether stepping up and down is legal at any given time.\n\n        Returns:\n            ored combination of StepUpEnabled | StepDownEnabled\n        \"\"\"\n", "input": "", "output": "        if self.value() > self.minimum() and self.value() < self.maximum():\n            return self.StepUpEnabled | self.StepDownEnabled\n        elif self.value() <= self.minimum():\n            return self.StepUpEnabled\n        elif self.value() >= self.maximum():\n            return self.StepDownEnabled", "category": "Python"}, {"instruction": "def create(self):\n        \"\"\"\n        create a SNAP7 client.\n        \"\"\"\n", "input": "", "output": "        logger.info(\"creating snap7 client\")\n        self.library.Cli_Create.restype = c_void_p\n        self.pointer = S7Object(self.library.Cli_Create())", "category": "Python"}, {"instruction": "def infer(self, limit=100, confidence=0.75):\n        \"\"\"https://github.com/frictionlessdata/tableschema-py#schema\n        \"\"\"\n", "input": "", "output": "        if self.__schema is None or self.__headers is None:\n\n            # Infer (tabulator)\n            if not self.__storage:\n                with self.__stream as stream:\n                    if self.__schema is None:\n                        self.__schema = Schema()\n                        self.__schema.infer(stream.sample[:limit],\n                                            headers=stream.headers,\n                                            confidence=confidence)\n                    if self.__headers is None:\n                        self.__headers = stream.headers\n\n            # Infer (storage)\n            else:\n                descriptor = self.__storage.describe(self.__source)\n                if self.__schema is None:\n                    self.__schema = Schema(descriptor)\n                if self.__headers is None:\n                    self.__headers = self.__schema.field_names\n\n        return self.__schema.descriptor", "category": "Python"}, {"instruction": "def create_node(self, *args, **kwargs):\n        \"\"\"\n        Creates a new IOU VM.\n\n        :returns: IOUVM instance\n        \"\"\"\n", "input": "", "output": "\n        with (yield from self._iou_id_lock):\n            # wait for a node to be completely created before adding a new one\n            # this is important otherwise we allocate the same application ID\n            # when creating multiple IOU node at the same time\n            application_id = get_next_application_id(self.nodes)\n            node = yield from super().create_node(*args, application_id=application_id, **kwargs)\n        return node", "category": "Python"}, {"instruction": "def gather(self):\n        \"\"\"\n        Interact with segy in gather mode\n\n        Returns\n        -------\n        gather : Gather\n        \"\"\"\n", "input": "", "output": "        if self.unstructured:\n            raise ValueError(self._unstructured_errmsg)\n\n        if self._gather is not None:\n            return self._gather\n\n        self._gather = Gather(self.trace, self.iline, self.xline, self.offsets)\n        return self._gather", "category": "Python"}, {"instruction": "def samples(self):\n        \"\"\"\n        Access the samples\n\n        :returns: twilio.rest.autopilot.v1.assistant.task.sample.SampleList\n        :rtype: twilio.rest.autopilot.v1.assistant.task.sample.SampleList\n        \"\"\"\n", "input": "", "output": "        if self._samples is None:\n            self._samples = SampleList(\n                self._version,\n                assistant_sid=self._solution['assistant_sid'],\n                task_sid=self._solution['sid'],\n            )\n        return self._samples", "category": "Python"}, {"instruction": "def _resize(self, init=False):\n        col, row = self._selection_to_col_row(self.selection)\n        if not (self.startPos <= row <= self.startPos + self.list_maxY - 1):\n            while row > self.startPos:\n                self.startPos += 1\n            while row < self.startPos + self.list_maxY - 1:\n                self.startPos -= 1\n        ''' if the selection at the end of the list,\n            try to scroll down '''\n", "input": "", "output": "        if init and row > self.list_maxY:\n            new_startPos = self._num_of_rows - self.list_maxY + 1\n            if row > new_startPos:\n                if logger.isEnabledFor(logging.DEBUG):\n                    logger.debug('setting startPos at {}'.format(new_startPos))\n                self.startPos = new_startPos\n        self.refresh_selection()", "category": "Python"}, {"instruction": "def color_map(cls, group):\n        print(\"Group %s\" % group)\n        \"\"\"\n        Change default color behavior.\n\n        Map certain states always to the same colors (similar to MMS).\n        \"\"\"\n", "input": "", "output": "        try:\n            state_idx = cls.states.index(group)\n        except ValueError:\n            # on any unexpected state, return black\n            state_idx = 5\n        return cls.colors[state_idx], cls.markers[0]", "category": "Python"}, {"instruction": "def delete_privileges(self, application, name, params=None):\n        \"\"\"\n        `<TODO>`_\n\n        :arg application: Application name\n        :arg name: Privilege name\n        :arg refresh: If `true` (the default) then refresh the affected shards\n            to make this operation visible to search, if `wait_for` then wait\n            for a refresh to make this operation visible to search, if `false`\n            then do nothing with refreshes., valid choices are: 'true', 'false',\n            'wait_for'\n        \"\"\"\n", "input": "", "output": "        for param in (application, name):\n            if param in SKIP_IN_PATH:\n                raise ValueError(\"Empty value passed for a required argument.\")\n        return self.transport.perform_request(\n            \"DELETE\",\n            _make_path(\"_security\", \"privilege\", application, name),\n            params=params,\n        )", "category": "Python"}, {"instruction": "def get_groups_by_name(self, name, parent=None):\n        \"\"\"\n        Retrieve all groups matching the given name and optionally filtered by the given parent node.\n        :param name: The name of the group that has to be returned\n        :param parent: A PBXGroup object where the object has to be retrieved from. If None all matching groups are returned\n        :return: An list of all matching groups\n        \"\"\"\n", "input": "", "output": "        groups = self.objects.get_objects_in_section(u'PBXGroup')\n        groups = [group for group in groups if group.get_name() == name]\n\n        if parent:\n            return [group for group in groups if parent.has_child(group)]\n\n        return groups", "category": "Python"}, {"instruction": "def _get_top_features(feature_names, coef, top, x):\n    \"\"\"\n    Return a ``(pos, neg)`` tuple. ``pos`` and ``neg`` are lists of\n    ``(name, value)`` tuples for features with positive and negative\n    coefficients.\n\n    Parameters:\n\n    * ``feature_names`` - a vector of feature names;\n    * ``coef`` - coefficient vector; coef.shape must be equal to\n      feature_names.shape;\n    * ``top`` can be either a number or a ``(num_pos, num_neg)`` tuple.\n      If ``top`` is a number, ``top`` features with largest absolute\n      coefficients are returned. If it is a ``(num_pos, num_neg)`` tuple,\n      the function returns no more than ``num_pos`` positive features and\n      no more than ``num_neg`` negative features. ``None`` value means\n      'no limit'.\n    * ``x`` is a vector of feature values, passed to FeatureWeight.value.\n    \"\"\"\n", "input": "", "output": "    if isinstance(top, (list, tuple)):\n        num_pos, num_neg = list(top)  # \"list\" is just for mypy\n        pos = _get_top_positive_features(feature_names, coef, num_pos, x)\n        neg = _get_top_negative_features(feature_names, coef, num_neg, x)\n    else:\n        pos, neg = _get_top_abs_features(feature_names, coef, top, x)\n    return pos, neg", "category": "Python"}, {"instruction": "def addIcon(self, iconActor, pos=3, size=0.08):\n        \"\"\"Add an inset icon mesh into the same renderer.\n\n        :param pos: icon position in the range [1-4] indicating one of the 4 corners,\n                    or it can be a tuple (x,y) as a fraction of the renderer size.\n        :param float size: size of the square inset.\n\n        .. hint:: |icon| |icon.py|_\n        \"\"\"\n", "input": "", "output": "        return addons.addIcon(iconActor, pos, size)", "category": "Python"}, {"instruction": "def update(self, data):\n        \"\"\"\n        Update this dictionary with th key-value pairs from a given\n        dictionary\n        \"\"\"\n", "input": "", "output": "        if not isinstance(data, dict):\n            raise TypeError('Data to update must be in a dictionary.')\n        for k, v in data.items():\n            arr = np.array(v)\n            try:\n                self[k] = arr\n            except TypeError:\n                logging.warning(\"Values under key ({}) not supported by VTK\".format(k))\n        return", "category": "Python"}, {"instruction": "def send(self, packet_type, bulb, packet_fmt, *packet_args):\n        \"\"\"\n        Builds and sends a packet to one or more bulbs.\n        \"\"\"\n", "input": "", "output": "        packet = build_packet(packet_type, self.gateway.mac, bulb,\n                              packet_fmt, *packet_args)\n        self.logger('>> %s', _bytes(packet))\n        self.sender.put(packet)", "category": "Python"}, {"instruction": "def do_get(self, line):\n        \"\"\"get <peer>\n        eg. get sw1\n        \"\"\"\n", "input": "", "output": "\n        def f(p, args):\n            print(p.get())\n\n        self._request(line, f)", "category": "Python"}, {"instruction": "def encode(data, checksum=True):\n    \"\"\"Convert binary to base58 using BASE58_ALPHABET.\"\"\"\n", "input": "", "output": "\n    if checksum:\n        data = data + utils.hash256(data)[:4]\n    v, prefix = to_long(256, lambda x: x, iter(data))\n    data = from_long(v, prefix, BASE58_BASE, lambda v: BASE58_ALPHABET[v])\n    return data.decode(\"utf8\")", "category": "Python"}, {"instruction": "def memoize(fn):\n    \"\"\"Caches previous calls to the function.\"\"\"\n", "input": "", "output": "    memo = {}\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        if not memoize.disabled:\n            key = pickle.dumps((args, kwargs))\n            if key not in memo:\n                memo[key] = fn(*args, **kwargs)\n            value = memo[key]\n        else:\n            # Memoize is disabled, call the function\n            value = fn(*args, **kwargs)\n\n        return value\n\n    return wrapper", "category": "Python"}, {"instruction": "def handle_update(self, args):\n    \"\"\"Handles an event update for this object, e.g. dimmer level change.\"\"\"\n", "input": "", "output": "    _LOGGER.debug(\"handle_update %d -- %s\" % (self._integration_id, args))\n    state = int(args[0])\n    if state != Output._ACTION_ZONE_LEVEL:\n      return False\n    level = float(args[1])\n    _LOGGER.debug(\"Updating %d(%s): s=%d l=%f\" % (\n        self._integration_id, self._name, state, level))\n    self._level = level\n    self._query_waiters.notify()\n    self._dispatch_event(Output.Event.LEVEL_CHANGED, {'level': self._level})\n    return True", "category": "Python"}, {"instruction": "def add_years(datetime_like_object, n, return_date=False):\n    \"\"\"\n    Returns a time that n years after a time.\n\n    :param datetimestr: a datetime object or a datetime str\n    :param n: number of years, value can be negative\n    :param return_date: returns a date object instead of datetime\n\n    **\u4e2d\u6587\u6587\u6863**\n\n    \u8fd4\u56de\u7ed9\u5b9a\u65e5\u671fN\u5e74\u4e4b\u540e\u7684\u65f6\u95f4\u3002\n    \"\"\"\n", "input": "", "output": "    a_datetime = parser.parse_datetime(datetime_like_object)\n\n    # try assign year, month, day\n    try:\n        a_datetime = datetime(\n            a_datetime.year + n, a_datetime.month, a_datetime.day,\n            a_datetime.hour, a_datetime.minute, a_datetime.second,\n            a_datetime.microsecond, tzinfo=a_datetime.tzinfo,\n        )\n    except ValueError:  # Must be xxxx-02-29\n        a_datetime = datetime(\n            a_datetime.year + n, 2, 28,\n            a_datetime.hour, a_datetime.minute,\n            a_datetime.second, a_datetime.microsecond)\n\n    if return_date:  # pragma: no cover\n        return a_datetime.date()\n    else:\n        return a_datetime", "category": "Python"}, {"instruction": "def get_process(self, dwProcessId):\n        \"\"\"\n        @type  dwProcessId: int\n        @param dwProcessId: Global ID of the process to look for.\n\n        @rtype:  L{Process}\n        @return: Process object with the given global ID.\n        \"\"\"\n", "input": "", "output": "        self.__initialize_snapshot()\n        if dwProcessId not in self.__processDict:\n            msg = \"Unknown process ID %d\" % dwProcessId\n            raise KeyError(msg)\n        return self.__processDict[dwProcessId]", "category": "Python"}, {"instruction": "def shutdown_server(self):\n        \"\"\"Shut down server if it is alive.\"\"\"\n", "input": "", "output": "        self.log.debug('shutdown_server: in')\n        if self.ensime and self.toggle_teardown:\n            self.ensime.stop()", "category": "Python"}, {"instruction": "def invalidate_cache(user, size=None):\n    \"\"\"\n    Function to be called when saving or changing an user's avatars.\n    \"\"\"\n", "input": "", "output": "    sizes = set(settings.AVATAR_AUTO_GENERATE_SIZES)\n    if size is not None:\n        sizes.add(size)\n    for prefix in cached_funcs:\n        for size in sizes:\n            cache.delete(get_cache_key(user, size, prefix))", "category": "Python"}, {"instruction": "def get_atomtrailer_list(r):\n    \"\"\"Capture only the leading dotted name list.\n\n    A full sequence typically includes function calls and parameters.\n    pkga.pkgb.pkgc.one_call(arg1, arg2, arg3=4)\n\n    \"\"\"\n", "input": "", "output": "    dot_set = set()\n    for n in r.find_all((\"atomtrailers\",)):\n        name_list = []\n        for x in n.value:\n            if x.type != \"name\":\n                break\n            name_list.append(x.value)\n        if name_list:\n            dot_set.add(tuple(name_list))\n    return sorted(dot_set)", "category": "Python"}, {"instruction": "def _users_watching(self, **kwargs):\n        \"\"\"Return users watching this instance.\"\"\"\n", "input": "", "output": "        return self._users_watching_by_filter(object_id=self.instance.pk,\n                                              **kwargs)", "category": "Python"}, {"instruction": "def _get_coarse_dataset(self, key, info):\n        \"\"\"Get the coarse dataset refered to by `key` from the XML data.\"\"\"\n", "input": "", "output": "        angles = self.root.find('.//Tile_Angles')\n        if key in ['solar_zenith_angle', 'solar_azimuth_angle']:\n            elts = angles.findall(info['xml_tag'] + '/Values_List/VALUES')\n            return np.array([[val for val in elt.text.split()] for elt in elts],\n                            dtype=np.float)\n\n        elif key in ['satellite_zenith_angle', 'satellite_azimuth_angle']:\n            arrays = []\n            elts = angles.findall(info['xml_tag'] + '[@bandId=\"1\"]')\n            for elt in elts:\n                items = elt.findall(info['xml_item'] + '/Values_List/VALUES')\n                arrays.append(np.array([[val for val in item.text.split()] for item in items],\n                                       dtype=np.float))\n            return np.nanmean(np.dstack(arrays), -1)\n        else:\n            return", "category": "Python"}, {"instruction": "def _set_final_freeness(self, flag):\n        \"\"\"\n        Sets the freedom of the final chunk. Since no proper chunk follows the final chunk, the heap itself manages\n        this. Nonetheless, for now it is implemented as if an additional chunk followed the final chunk.\n        \"\"\"\n", "input": "", "output": "        if flag:\n            self.state.memory.store(self.heap_base + self.heap_size - self._chunk_size_t_size, ~CHUNK_P_MASK)\n        else:\n            self.state.memory.store(self.heap_base + self.heap_size - self._chunk_size_t_size, CHUNK_P_MASK)", "category": "Python"}, {"instruction": "def trim(self):\n        '''Remove items that are expired or exceed the max size.'''\n", "input": "", "output": "        now_time = time.time()\n\n        while self._seq and self._seq[0].expire_time < now_time:\n            item = self._seq.popleft()\n            del self._map[item.key]\n\n        if self._max_items:\n            while self._seq and len(self._seq) > self._max_items:\n                item = self._seq.popleft()\n                del self._map[item.key]", "category": "Python"}, {"instruction": "def slice(cls, *args, **kwargs):\n        \"\"\"Take a slice of a DataSetFamily to produce a dataset\n        indexed by asset and date.\n\n        Parameters\n        ----------\n        *args\n        **kwargs\n            The coordinates to fix along each extra dimension.\n\n        Returns\n        -------\n        dataset : DataSet\n            A regular pipeline dataset indexed by asset and date.\n\n        Notes\n        -----\n        The extra dimensions coords used to produce the result are available\n        under the ``extra_coords`` attribute.\n        \"\"\"\n", "input": "", "output": "        coords, hash_key = cls._canonical_key(args, kwargs)\n        try:\n            return cls._slice_cache[hash_key]\n        except KeyError:\n            pass\n\n        Slice = cls._make_dataset(coords)\n        cls._slice_cache[hash_key] = Slice\n        return Slice", "category": "Python"}, {"instruction": "def _handle_sending(self, data):\n        \"\"\"\n        Handles results of a keypress send.\n\n        :param data: Sending string to parse\n        :type data: string\n        \"\"\"\n", "input": "", "output": "\n        matches = re.match('^!Sending(\\.{1,5})done.*', data)\n        if matches is not None:\n            good_send = False\n            if len(matches.group(1)) < 5:\n                good_send = True\n\n            self.on_sending_received(status=good_send, message=data)", "category": "Python"}, {"instruction": "def build_counter(obj, instance, instance_index, counter):\n        r'''\n        Makes a fully resolved counter path. Counter names are formatted like\n        this:\n\n        ``\\Processor(*)\\% Processor Time``\n\n        The above breaks down like this:\n\n            obj = 'Processor'\n            instance = '*'\n            counter = '% Processor Time'\n\n        Args:\n\n            obj (str):\n                The top level object\n\n            instance (str):\n                The instance of the object\n\n            instance_index (int):\n                The index of the instance. Can usually be 0\n\n            counter (str):\n                The name of the counter\n\n        Returns:\n            Counter: A Counter object with the path if valid\n\n        Raises:\n            CommandExecutionError: If the path is invalid\n        '''\n", "input": "", "output": "        path = win32pdh.MakeCounterPath(\n            (None, obj, instance, None, instance_index, counter), 0)\n        if win32pdh.ValidatePath(path) is 0:\n            return Counter(path, obj, instance, instance_index, counter)\n        raise CommandExecutionError('Invalid counter specified: {0}'.format(path))", "category": "Python"}, {"instruction": "def three_d_effect(img, **kwargs):\n    \"\"\"Create 3D effect using convolution\"\"\"\n", "input": "", "output": "    w = kwargs.get('weight', 1)\n    LOG.debug(\"Applying 3D effect with weight %.2f\", w)\n    kernel = np.array([[-w, 0, w],\n                       [-w, 1, w],\n                       [-w, 0, w]])\n    mode = kwargs.get('convolve_mode', 'same')\n\n    def func(band_data, kernel=kernel, mode=mode, index=None):\n        del index\n\n        delay = dask.delayed(_three_d_effect_delayed)(band_data, kernel, mode)\n        new_data = da.from_delayed(delay, shape=band_data.shape, dtype=band_data.dtype)\n        return new_data\n\n    return apply_enhancement(img.data, func, separate=True, pass_dask=True)", "category": "Python"}, {"instruction": "def pop(self, key, default=None):\n        \"\"\"Remove specified key and return the corresponding value.\n           If key is not found, default is returned if given, otherwise \n           KeyError is raised.\n        \"\"\"\n", "input": "", "output": "        if key not in self:\n            if default is not None:\n                return default\n            raise KeyError(key)\n        for map in [self._pb.IntMap, self._pb.FloatMap, self._pb.StringMap,\n                    self._pb.BoolMap]:\n            if key in map.keys():\n                return map.pop(key)", "category": "Python"}, {"instruction": "def merge_all_models_into_first_model(biop_structure):\n    \"\"\"Merge all existing models into a Structure's first_model attribute.\n\n    This directly modifies the Biopython Structure object. Chains IDs will start from A and increment for each new\n    chain (model that is converted).\n\n    Args:\n        biop_structure (Structure): Structure with multiple models that should be merged\n\n    \"\"\"\n", "input": "", "output": "    from string import ascii_uppercase\n    idx = 1\n    first_model = biop_structure[0]\n\n    for m in biop_structure.get_models():\n        # Don't duplicate the original model\n        if first_model.id == m.id:\n            continue\n        for c in m.get_chains():\n            c.id = ascii_uppercase[idx]\n            first_model.add(c)\n        idx += 1", "category": "Python"}, {"instruction": "def validate_document_class(option, value):\n    \"\"\"Validate the document_class option.\"\"\"\n", "input": "", "output": "    if not issubclass(value, (abc.MutableMapping, RawBSONDocument)):\n        raise TypeError(\"%s must be dict, bson.son.SON, \"\n                        \"bson.raw_bson.RawBSONDocument, or a \"\n                        \"sublass of collections.MutableMapping\" % (option,))\n    return value", "category": "Python"}, {"instruction": "def run_exitfuncs():\n    \"\"\"Function that behaves exactly like Python's atexit, but runs atexit functions\n    in the order in which they were registered, not reversed.\n    \"\"\"\n", "input": "", "output": "    exc_info = None\n    for func, targs, kargs in _exithandlers:\n        try:\n            func(*targs, **kargs)\n        except SystemExit:\n            exc_info = sys.exc_info()\n        except:\n            exc_info = sys.exc_info()\n\n    if exc_info is not None:\n        six.reraise(exc_info[0], exc_info[1], exc_info[2])", "category": "Python"}, {"instruction": "def albedo(self, model='smith'):\n        \"\"\"Finds broad-band surface reflectance (albedo)\n        \n        Smith (2010),  \u201cThe heat budget of the earth\u2019s surface deduced from space\u201d\n        LT5 toa reflectance bands 1, 3, 4, 5, 7\n        # normalized i.e. 0.356 + 0.130 + 0.373 + 0.085 + 0.07 = 1.014\n        \n        Should have option for Liang, 2000; \n        \n        Tasumi (2008), \"At-Surface Reflectance and Albedo from Satellite for\n                        Operational Calculation of Land Surface Energy Balance\"\n                        \n        \n        :return albedo array of floats\n        \"\"\"\n", "input": "", "output": "        if model == 'smith':\n            blue, red, nir, swir1, swir2 = (self.reflectance(1), self.reflectance(3), self.reflectance(4),\n                                            self.reflectance(5), self.reflectance(7))\n            alb = (0.356 * blue + 0.130 * red + 0.373 * nir + 0.085 * swir1 + 0.072 * swir2 - 0.0018) / 1.014\n        elif model == 'tasumi':\n            pass\n        # add tasumi algorithm TODO\n        return alb", "category": "Python"}, {"instruction": "def handle_aggregated_quotas(sender, instance, **kwargs):\n    \"\"\" Call aggregated quotas fields update methods \"\"\"\n", "input": "", "output": "    quota = instance\n    # aggregation is not supported for global quotas.\n    if quota.scope is None:\n        return\n    quota_field = quota.get_field()\n    # usage aggregation should not count another usage aggregator field to avoid calls duplication.\n    if isinstance(quota_field, fields.UsageAggregatorQuotaField) or quota_field is None:\n        return\n    signal = kwargs['signal']\n    for aggregator_quota in quota_field.get_aggregator_quotas(quota):\n        field = aggregator_quota.get_field()\n        if signal == signals.post_save:\n            field.post_child_quota_save(aggregator_quota.scope, child_quota=quota, created=kwargs.get('created'))\n        elif signal == signals.pre_delete:\n            field.pre_child_quota_delete(aggregator_quota.scope, child_quota=quota)", "category": "Python"}, {"instruction": "def get_column_header_for_number(self, column_var_names, header=False):\n        \"\"\"This function subtracts 1 from inputted column number to comply\n        with programmers counting (i.e. from 0, not from 1). For TSV data.\"\"\"\n", "input": "", "output": "        if not header:\n            header = self.oldheader\n        for col in column_var_names:\n            value = getattr(self, col)\n            if not value or value is None:\n                continue\n            setattr(self, col, self.number_to_headerfield(value, header))", "category": "Python"}, {"instruction": "def package(self, vm_name=None, base=None, output=None, vagrantfile=None):\n        '''\n        Packages a running vagrant environment into a box.\n\n        vm_name=None: name of VM.\n        base=None: name of a VM in virtualbox to package as a base box\n        output=None: name of the file to output\n        vagrantfile=None: Vagrantfile to package with this box\n        '''\n", "input": "", "output": "        cmd = ['package', vm_name]\n        if output is not None:\n            cmd += ['--output', output]\n        if vagrantfile is not None:\n            cmd += ['--vagrantfile', vagrantfile]\n\n        self._call_vagrant_command(cmd)", "category": "Python"}, {"instruction": "def get_tablenames(cur):\n    \"\"\" Conveinience: \"\"\"\n", "input": "", "output": "    cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\")\n    tablename_list_ = cur.fetchall()\n    tablename_list = [str(tablename[0]) for tablename in tablename_list_ ]\n    return tablename_list", "category": "Python"}, {"instruction": "def handle_input(self, input_str, place=True, check=False):\n        '''Transfer user input to valid chess position'''\n", "input": "", "output": "        user = self.get_player()\n        pos = self.validate_input(input_str)\n        if pos[0] == 'u':\n            self.undo(pos[1])\n            return pos\n        if place:\n            result = self.set_pos(pos, check)\n            return result\n        else:\n            return pos", "category": "Python"}, {"instruction": "def UV_H(self):\n        \"\"\"\n        UV = all non-trivial (!V\u2282U) implications U->V with UuV closed; in ternary coding (1=V,2=U)\n        K = all closed sets\n\n        This is UV_H function, but the returned implications are respected by all attribute sets of this context.\n        This corresponds to a multiplication or & operation of the Hg sets.\n        \"\"\"\n", "input": "", "output": "        h = reduce(lambda x,y:x&y,(H(g,self.width-1) for g in self))\n        return UV_H(h, self.width)", "category": "Python"}, {"instruction": "def _flatten_fetched_fields(fields_arg):\n    \"\"\" this method takes either a kwargs 'fields', which can be a dict :\n    {\"_id\": False, \"store\": 1, \"url\": 1} or a list : [\"store\", \"flag\", \"url\"]\n    and returns a tuple : (\"store\", \"flag\").\n    it HAS to be a tuple, so that it is not updated by the different instances\n    \"\"\"\n", "input": "", "output": "    if fields_arg is None:\n        return None\n    if isinstance(fields_arg, dict):\n        return tuple(sorted([k for k in list(fields_arg.keys()) if fields_arg[k]]))\n    else:\n        return tuple(sorted(fields_arg))", "category": "Python"}, {"instruction": "def get_crawl_delay(self, user_agent):\n        \"\"\"Returns a float representing the crawl delay specified for this \n        user agent, or None if the crawl delay was unspecified or not a float.\n        \"\"\"\n", "input": "", "output": "        # See is_allowed() comment about the explicit unicode conversion.\n        if (PY_MAJOR_VERSION < 3) and (not isinstance(user_agent, unicode)):\n            user_agent = user_agent.decode()\n    \n        for ruleset in self.__rulesets:\n            if ruleset.does_user_agent_match(user_agent):\n                return ruleset.crawl_delay\n                \n        return None", "category": "Python"}, {"instruction": "def _get_block_sizes(resnet_size):\n  \"\"\"Retrieve the size of each block_layer in the ResNet model.\n\n  The number of block layers used for the Resnet model varies according\n  to the size of the model. This helper grabs the layer set we want, throwing\n  an error if a non-standard size has been selected.\n\n  Args:\n    resnet_size: The number of convolutional layers needed in the model.\n\n  Returns:\n    A list of block sizes to use in building the model.\n\n  Raises:\n    KeyError: if invalid resnet_size is received.\n  \"\"\"\n", "input": "", "output": "  choices = {\n      18: [2, 2, 2, 2],\n      34: [3, 4, 6, 3],\n      50: [3, 4, 6, 3],\n      101: [3, 4, 23, 3],\n      152: [3, 8, 36, 3],\n      200: [3, 24, 36, 3]\n  }\n\n  try:\n    return choices[resnet_size]\n  except KeyError:\n    err = ('Could not find layers for selected Resnet size.\\n'\n           'Size received: {}; sizes allowed: {}.'.format(\n               resnet_size, choices.keys()))\n    raise ValueError(err)", "category": "Python"}, {"instruction": "def split_by(self, layer, sep=' '):\n        \"\"\"Split the text into multiple instances defined by elements of given layer.\n\n        The spans for layer elements are extracted and feed to :py:meth:`~estnltk.text.Text.split_given_spans`\n        method.\n\n        Parameters\n        ----------\n        layer: str\n            String determining the layer that is used to define the start and end positions of resulting splits.\n        sep: str (default: ' ')\n            The separator to use to join texts of multilayer elements.\n\n        Returns\n        -------\n        list of Text\n        \"\"\"\n", "input": "", "output": "        if not self.is_tagged(layer):\n            self.tag(layer)\n        return self.split_given_spans(self.spans(layer), sep=sep)", "category": "Python"}, {"instruction": "def _set_bank_view(self, session):\n        \"\"\"Sets the underlying bank view to match current view\"\"\"\n", "input": "", "output": "        if self._bank_view == FEDERATED:\n            try:\n                session.use_federated_bank_view()\n            except AttributeError:\n                pass\n        else:\n            try:\n                session.use_isolated_bank_view()\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def split_every(parts, iterable):\n    \"\"\"\n    Split an iterable into parts of length parts\n\n    >>> l = iter([1, 2, 3, 4])\n    >>> split_every(2, l)\n    [[1, 2], [3, 4]]\n\n    :param iterable: iterable to split\n    :param parts: number of chunks\n    :return: return the iterable split in parts\n    \"\"\"\n", "input": "", "output": "    return takewhile(bool, (list(islice(iterable, parts)) for _ in count()))", "category": "Python"}, {"instruction": "def template(client, src, dest, paths, opt):\n    \"\"\"Writes a template using variables from a vault path\"\"\"\n", "input": "", "output": "    key_map = cli_hash(opt.key_map)\n    obj = {}\n    for path in paths:\n        response = client.read(path)\n        if not response:\n            raise aomi.exceptions.VaultData(\"Unable to retrieve %s\" % path)\n        if is_aws(response['data']) and 'sts' not in path:\n            renew_secret(client, response, opt)\n\n        for s_k, s_v in response['data'].items():\n            o_key = s_k\n            if s_k in key_map:\n                o_key = key_map[s_k]\n\n            k_name = secret_key_name(path, o_key, opt) \\\n                .lower() \\\n                .replace('-', '_')\n            obj[k_name] = s_v\n\n    template_obj = blend_vars(obj, opt)\n    output = render(grok_template_file(src),\n                    template_obj)\n    write_raw_file(output, abspath(dest))", "category": "Python"}, {"instruction": "def _findindex(az0, el0, az, el):\n    \"\"\"\n    inputs:\n    ------\n    az0, el0: N-D array of azimuth, elevation. May be masked arrays\n    az, el: 1-D vectors of azimuth, elevation points from other camera to find closest angle for joint FOV.\n\n    output:\n    row, col:  index of camera 0 closest to camera 1 FOV for each unmasked pixel\n\n    I think with some minor tweaks this could be numba.jit if too slow.\n    \"\"\"\n", "input": "", "output": "\n    assert az0.size == el0.size  # just for clarity\n    assert az.ndim == el.ndim == 1, 'expect vector of test points'\n    ic = np.empty(az.size, dtype=int)\n\n    for i, (a, e) in enumerate(zip(az, el)):\n        # we do this point by point because we need to know the closest pixel for each point\n        # errang = haver.anglesep(az,el, apt,ept, deg=False)\n        ic[i] = haver.anglesep_meeus(az0, el0, a, e, deg=False).argmin()\n\n    ", "category": "Python"}, {"instruction": "def root_adb(self):\n        \"\"\"Change adb to root mode for this device if allowed.\n\n        If executed on a production build, adb will not be switched to root\n        mode per security restrictions.\n        \"\"\"\n", "input": "", "output": "        self.adb.root()\n        self.adb.wait_for_device(\n            timeout=DEFAULT_TIMEOUT_BOOT_COMPLETION_SECOND)", "category": "Python"}, {"instruction": "def height(poly):\n    \"\"\"Height of a polygon poly\"\"\"\n", "input": "", "output": "    num = len(poly) - 1\n    if abs(poly[num][2] - poly[0][2]) > abs(poly[1][2] - poly[0][2]):\n        return dist(poly[num], poly[0])\n    elif abs(poly[num][2] - poly[0][2]) < abs(poly[1][2] - poly[0][2]):\n        return dist(poly[1], poly[0])\n    else:\n        return min(dist(poly[num], poly[0]), dist(poly[1], poly[0]))", "category": "Python"}, {"instruction": "def set_etag(self, etag, weak=False):\n        \"\"\"Set the etag, and override the old one if there was one.\"\"\"\n", "input": "", "output": "        self.headers[\"ETag\"] = quote_etag(etag, weak)", "category": "Python"}, {"instruction": "def logical_definitions(self, nid):\n        \"\"\"\n        Retrieves logical definitions for a class id\n\n        Arguments\n        ---------\n        nid : str\n            Node identifier for entity to be queried\n\n        Returns\n        -------\n        LogicalDefinition\n        \"\"\"\n", "input": "", "output": "        ldefs = self.all_logical_definitions\n        if ldefs is not None:\n            #print(\"TESTING: {} AGAINST LD: {}\".format(nid, str(ldefs)))\n            return [x for x in ldefs if x.class_id == nid]\n        else:\n            return []", "category": "Python"}, {"instruction": "def select(self, axis, index, force_copy: bool = False):\n        \"\"\"Alias for [] to be compatible with HistogramND.\"\"\"\n", "input": "", "output": "        if axis == 0:\n            if index == slice(None) and not force_copy:\n                return self\n            return self[index]\n        else:\n            raise ValueError(\"In Histogram1D.select(), axis must be 0.\")", "category": "Python"}, {"instruction": "def main():\n    \"\"\"Read two files line-by-line and print edit distances between each pair\n    of lines. Will terminate at the end of the shorter of the two files.\"\"\"\n", "input": "", "output": "\n    if len(sys.argv) != 3:\n        print('Usage: {} <file1> <file2>'.format(sys.argv[0]))\n        exit(-1)\n    file1 = sys.argv[1]\n    file2 = sys.argv[2]\n\n    with open(file1) as f1, open(file2) as f2:\n        for line1, line2 in zip(f1, f2):\n            print(\"Line 1: {}\".format(line1.strip()))\n            print(\"Line 2: {}\".format(line2.strip()))\n            dist, _, _ = edit_distance_backpointer(line1.split(), line2.split())\n            print('Distance: {}'.format(dist))\n            print('=' * 80)", "category": "Python"}, {"instruction": "def insertDatastore(self, index, store):\n    '''Inserts datastore `store` into this collection at `index`.'''\n", "input": "", "output": "    if not isinstance(store, Datastore):\n      raise TypeError(\"stores must be of type %s\" % Datastore)\n\n    self._stores.insert(index, store)", "category": "Python"}, {"instruction": "def _call_rpc(self, address, rpc_id, payload):\n        \"\"\"Call an RPC with the given information and return its response.\n\n        Must raise a hardware error of the appropriate kind if the RPC\n        can not be executed correctly.  Otherwise it should return the binary\n        response payload received from the RPC.\n\n        Args:\n            address (int): The address of the tile we want to call the RPC\n                on\n            rpc_id (int): The id of the RPC that we want to call\n            payload (bytes, bytearray): The data that we want to send as the payload\n        \"\"\"\n", "input": "", "output": "\n        # FIXME: Set a timeout of 1.1 seconds to make sure we fail if the device hangs but\n        #        this should be long enough to accommodate any actual RPCs we need to send.\n\n        status, response = self.hw.stream.send_rpc(address, rpc_id, payload, timeout=1.1)\n        return response", "category": "Python"}, {"instruction": "def get_tuple(self):\n        \"\"\"Return the unsigned integer tuple of the identifier.\"\"\"\n", "input": "", "output": "        objType, objInstance = self.value\n\n        if isinstance(objType, int):\n            pass\n        elif isinstance(objType, str):\n            # turn it back into an integer\n            objType = self.objectTypeClass()[objType]\n        else:\n            raise TypeError(\"invalid datatype for objType\")\n\n        # pack the components together\n        return (objType, objInstance)", "category": "Python"}, {"instruction": "def _create_virtual_network(self, region, resource_group_name, vnet_name):\n        \"\"\"\n        Create a vnet in the given resource group with default address space.\n        \"\"\"\n", "input": "", "output": "        vnet_config = {\n            'location': region,\n            'address_space': {\n                'address_prefixes': ['10.0.0.0/27']\n            }\n        }\n\n        try:\n            vnet_setup = self.network.virtual_networks.create_or_update(\n                resource_group_name, vnet_name, vnet_config\n            )\n        except Exception as error:\n            raise AzureCloudException(\n                'Unable to create vnet: {0}.'.format(error)\n            )\n\n        vnet_setup.wait()", "category": "Python"}, {"instruction": "def get_or_create_by_natural_key(self, *args):\n        \"\"\"\n        get_or_create + get_by_natural_key\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.get_by_natural_key(*args), False\n        except self.model.DoesNotExist:\n            return self.create_by_natural_key(*args), True", "category": "Python"}, {"instruction": "def new(namespace, name, wdl, synopsis,\n            documentation=None, api_url=fapi.PROD_API_ROOT):\n        \"\"\"Create new FireCloud method.\n\n        If the namespace + name already exists, a new snapshot is created.\n\n        Args:\n            namespace (str): Method namespace for this method\n            name (str): Method name\n            wdl (file): WDL description\n            synopsis (str): Short description of task\n            documentation (file): Extra documentation for method\n        \"\"\"\n", "input": "", "output": "        r = fapi.update_workflow(namespace, name, synopsis,\n                                 wdl, documentation, api_url)\n        fapi._check_response_code(r, 201)\n        d = r.json()\n        return Method(namespace, name, d[\"snapshotId\"])", "category": "Python"}, {"instruction": "def get_buckets(min_length, max_length, bucket_count):\n    '''\n    Get bucket by length.\n    '''\n", "input": "", "output": "    if bucket_count <= 0:\n        return [max_length]\n    unit_length = int((max_length - min_length) // (bucket_count))\n    buckets = [min_length + unit_length *\n               (i + 1) for i in range(0, bucket_count)]\n    buckets[-1] = max_length\n    return buckets", "category": "Python"}, {"instruction": "def _separable_approx3(h, N=1):\n    \"\"\" returns the N first approximations to the 3d function h\n    \"\"\"\n", "input": "", "output": "    return np.cumsum([np.einsum(\"i,j,k\", fz, fy, fx) for fz, fy, fx in _separable_series3(h, N)], 0)", "category": "Python"}, {"instruction": "def _expected_condition_find_element_visible(self, element):\n        \"\"\"Tries to find the element and checks that it is visible, but does not thrown an exception if the element is\n            not found\n\n        :param element: PageElement or element locator as a tuple (locator_type, locator_value) to be found\n        :returns: the web element if it is visible or False\n        :rtype: selenium.webdriver.remote.webelement.WebElement or appium.webdriver.webelement.WebElement\n        \"\"\"\n", "input": "", "output": "        web_element = self._expected_condition_find_element(element)\n        try:\n            return web_element if web_element and web_element.is_displayed() else False\n        except StaleElementReferenceException:\n            return False", "category": "Python"}, {"instruction": "def rgb(self):\n        \"\"\"\n        An |RGBColor| value or |None| if no RGB color is specified.\n\n        When :attr:`type` is `MSO_COLOR_TYPE.RGB`, the value of this property\n        will always be an |RGBColor| value. It may also be an |RGBColor|\n        value if :attr:`type` is `MSO_COLOR_TYPE.THEME`, as Word writes the\n        current value of a theme color when one is assigned. In that case,\n        the RGB value should be interpreted as no more than a good guess\n        however, as the theme color takes precedence at rendering time. Its\n        value is |None| whenever :attr:`type` is either |None| or\n        `MSO_COLOR_TYPE.AUTO`.\n\n        Assigning an |RGBColor| value causes :attr:`type` to become\n        `MSO_COLOR_TYPE.RGB` and any theme color is removed. Assigning |None|\n        causes any color to be removed such that the effective color is\n        inherited from the style hierarchy.\n        \"\"\"\n", "input": "", "output": "        color = self._color\n        if color is None:\n            return None\n        if color.val == ST_HexColorAuto.AUTO:\n            return None\n        return color.val", "category": "Python"}, {"instruction": "def to_json(self):\n        \"\"\"\n        Serialize object to json dict\n\n        :return: dict\n        \"\"\"\n", "input": "", "output": "        data = dict()\n        data['name'] = self.name\n        data['language'] = self.language\n        data['fancy_name'] = self.fancy_name\n        data['scenario'] = self.scenario\n        return data", "category": "Python"}, {"instruction": "def select_directory(self):\r\n        \"\"\"Select directory\"\"\"\n", "input": "", "output": "        self.redirect_stdio.emit(False)\r\n        directory = getexistingdirectory(self.main, _(\"Select directory\"),\r\n                                         getcwd_or_home())\r\n        if directory:\r\n            self.chdir(directory)\r\n        self.redirect_stdio.emit(True)", "category": "Python"}, {"instruction": "def wind_shear(shear: str, unit_alt: str = 'ft', unit_wind: str = 'kt', spoken: bool = False) -> str:\n    \"\"\"\n    Translate wind shear into a readable string\n\n    Ex: Wind shear 2000ft from 140 at 30kt\n    \"\"\"\n", "input": "", "output": "    if not shear or 'WS' not in shear or '/' not in shear:\n        return ''\n    shear = shear[2:].rstrip(unit_wind.upper()).split('/')  # type: ignore\n    wdir = core.spoken_number(shear[1][:3]) if spoken else shear[1][:3]\n    return f'Wind shear {int(shear[0])*100}{unit_alt} from {wdir} at {shear[1][3:]}{unit_wind}'", "category": "Python"}, {"instruction": "def xslt_transformation(xml, template):\n    \"\"\"\n    Transform `xml` using XSLT `template`.\n\n    Args:\n        xml (str): Filename or XML string. Don't use ``\\\\n`` in case of\n                   filename.\n        template (str): Filename or XML string. Don't use ``\\\\n`` in case of\n                        filename.\n\n    Returns:\n        str: Transformed `xml` as string.\n    \"\"\"\n", "input": "", "output": "    transformer = ET.XSLT(\n        _read_template(template)\n    )\n    newdom = transformer(\n        _read_marcxml(xml)\n    )\n\n    return ET.tostring(newdom, pretty_print=True, encoding=\"utf-8\")", "category": "Python"}, {"instruction": "def call(self, name, *args, **kwargs):\n        \"\"\"Make a SoftLayer API call\n\n        :param service: the name of the SoftLayer API service\n        :param method: the method to call on the service\n        :param \\\\*args: same optional arguments that ``BaseClient.call`` takes\n        :param \\\\*\\\\*kwargs: same optional keyword arguments that\n                           ``BaseClient.call`` takes\n\n        :param service: the name of the SoftLayer API service\n\n        Usage:\n            >>> import SoftLayer\n            >>> client = SoftLayer.create_client_from_env()\n            >>> client['Account'].getVirtualGuests(mask=\"id\", limit=10)\n            [...]\n\n        \"\"\"\n", "input": "", "output": "        return self.client.call(self.name, name, *args, **kwargs)", "category": "Python"}, {"instruction": "def send_voice_message(self, user_id, media_id, kf_account=None):\n        \"\"\"\n        \u53d1\u9001\u8bed\u97f3\u6d88\u606f\u3002\n\n        :param user_id: \u7528\u6237 ID \u3002 \u5c31\u662f\u4f60\u6536\u5230\u7684 `Message` \u7684 source\n        :param media_id: \u53d1\u9001\u7684\u8bed\u97f3\u7684\u5a92\u4f53ID\u3002 \u53ef\u4ee5\u901a\u8fc7 :func:`upload_media` \u4e0a\u4f20\u3002\n        :param kf_account: \u53d1\u9001\u6d88\u606f\u7684\u5ba2\u670d\u8d26\u6237\uff0c\u9ed8\u8ba4\u503c\u4e3a None\uff0cNone \u4e3a\u4e0d\u6307\u5b9a\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n", "input": "", "output": "        data = {\n            \"touser\": user_id,\n            \"msgtype\": \"voice\",\n            \"voice\": {\n                \"media_id\": media_id\n            }\n        }\n        if kf_account is not None:\n            data['customservice'] = {'kf_account': kf_account}\n        return self.post(\n            url=\"https://api.weixin.qq.com/cgi-bin/message/custom/send\",\n            data=data\n        )", "category": "Python"}, {"instruction": "def get_filenames(directory):\n    \"\"\"Get all the file to copy\"\"\"\n", "input": "", "output": "    for filename in os.listdir(directory):\n        if re.search(r\"cp\\d{2}mu?-manylinux1_\\S+\\.whl\", filename):\n            yield filename", "category": "Python"}, {"instruction": "def registerParser(self, parser):\n        \"\"\"\n        Registers a parser to parse configuration inputs.\n        \"\"\"\n", "input": "", "output": "\n        if not isinstance(parser, Subparser):\n            raise TypeError(\"%s is not an instance of a subparser.\" % parser)\n\n        self.parsers.append(parser)", "category": "Python"}, {"instruction": "def rmtree(self, ignore_errors=False, onerror=None):\n        \"\"\"Remove workdir (same API as shutil.rmtree).\"\"\"\n", "input": "", "output": "        if not os.path.exists(self.workdir): return\n        shutil.rmtree(self.workdir, ignore_errors=ignore_errors, onerror=onerror)", "category": "Python"}, {"instruction": "def get_symbol_at_address(self, address):\n        \"\"\"\n        Tries to find the closest matching symbol for the given address.\n\n        @type  address: int\n        @param address: Memory address to query.\n\n        @rtype: None or tuple( str, int, int )\n        @return: Returns a tuple consisting of:\n             - Name\n             - Address\n             - Size (in bytes)\n            Returns C{None} if no symbol could be matched.\n        \"\"\"\n", "input": "", "output": "        # Any module may have symbols pointing anywhere in memory, so there's\n        # no easy way to optimize this. I guess we're stuck with brute force.\n        found = None\n        for (SymbolName, SymbolAddress, SymbolSize) in self.iter_symbols():\n            if SymbolAddress > address:\n                continue\n\n            if SymbolAddress == address:\n                found = (SymbolName, SymbolAddress, SymbolSize)\n                break\n\n            if SymbolAddress < address:\n                if found and (address - found[1]) < (address - SymbolAddress):\n                    continue\n                else:\n                    found = (SymbolName, SymbolAddress, SymbolSize)\n        return found", "category": "Python"}, {"instruction": "def get_dweets_for(thing_name, key=None, session=None):\n    \"\"\"Read all the dweets for a dweeter\n    \"\"\"\n", "input": "", "output": "    if key is not None:\n        params = {'key': key}\n    else:\n        params = None\n    return _request('get', '/get/dweets/for/{0}'.format(thing_name), params=params, session=None)", "category": "Python"}, {"instruction": "def get_roles(self):\n        \"\"\"Return the m2m relations connecting me to works\"\"\"\n", "input": "", "output": "        work_ids = self.get_works().values_list('id', flat=True)\n        return self.works.through.objects.filter(\n            creator=self.get_draft(),\n            work_id__in=work_ids,\n        ).select_related('role')", "category": "Python"}, {"instruction": "def _unique(x):\n    \"\"\"Faster version of np.unique().\n\n    This version is restricted to 1D arrays of non-negative integers.\n\n    It is only faster if len(x) >> len(unique(x)).\n\n    \"\"\"\n", "input": "", "output": "    if x is None or len(x) == 0:\n        return np.array([], dtype=np.int64)\n    # WARNING: only keep positive values.\n    # cluster=-1 means \"unclustered\".\n    x = _as_array(x)\n    x = x[x >= 0]\n    bc = np.bincount(x)\n    return np.nonzero(bc)[0]", "category": "Python"}, {"instruction": "def render_import_image(self, use_auth=None):\n        \"\"\"\n        Configure the import_image plugin\n        \"\"\"\n", "input": "", "output": "        # import_image is a multi-phase plugin\n        if self.user_params.imagestream_name.value is None:\n            self.pt.remove_plugin('exit_plugins', 'import_image',\n                                  'imagestream not in user parameters')\n        elif self.pt.has_plugin_conf('exit_plugins', 'import_image'):\n            self.pt.set_plugin_arg('exit_plugins', 'import_image', 'imagestream',\n                                   self.user_params.imagestream_name.value)", "category": "Python"}, {"instruction": "def commit(cls, client=None):\n        \"\"\"Commit everything from datapoints via the client.\n\n        :param client: InfluxDBClient instance for writing points to InfluxDB.\n        :attention: any provided client will supersede the class client.\n        :return: result of client.write_points.\n        \"\"\"\n", "input": "", "output": "        if not client:\n            client = cls._client\n        rtn = client.write_points(cls._json_body_())\n        cls._reset_()\n        return rtn", "category": "Python"}, {"instruction": "def __rmfile(path):\n    \"\"\"Delete a file.\n\n    Args:\n        path (str): Path to the file that needs to be deleted.\n\n    Returns:\n        bool: True if the operation is successful, False otherwise.\n    \"\"\"\n", "input": "", "output": "    logger.info(\"rmfile: %s\" % path)\n    try:\n        os.remove(path)\n        return True\n    except Exception as e:\n        logger.error(\"rmfile: %s failed! Error: %s\" % (path, e))\n        return False", "category": "Python"}, {"instruction": "def remove_consumer_tag(self, tag=None):\n        \"\"\"Remove a Consumer tag.\n\n            If no tag is specified, all all tags will be removed.\n\n        :param str|None tag: Consumer tag.\n        :return:\n        \"\"\"\n", "input": "", "output": "        if tag is not None:\n            if tag in self._consumer_tags:\n                self._consumer_tags.remove(tag)\n        else:\n            self._consumer_tags = []", "category": "Python"}, {"instruction": "def build_flags(library, type_, path):\n    \"\"\"Return separated build flags from pkg-config output\"\"\"\n", "input": "", "output": "\n    pkg_config_path = [path]\n    if \"PKG_CONFIG_PATH\" in os.environ:\n        pkg_config_path.append(os.environ['PKG_CONFIG_PATH'])\n    if \"LIB_DIR\" in os.environ:\n        pkg_config_path.append(os.environ['LIB_DIR'])\n        pkg_config_path.append(os.path.join(os.environ['LIB_DIR'], \"pkgconfig\"))\n\n    options = [\n        \"--static\",\n        {\n            'I': \"--cflags-only-I\",\n            'L': \"--libs-only-L\",\n            'l': \"--libs-only-l\"\n        }[type_]\n    ]\n\n    return [\n        flag.strip(\"-{}\".format(type_))\n        for flag\n        in subprocess.check_output(\n            [\"pkg-config\"] + options + [library],\n            env=dict(os.environ, PKG_CONFIG_PATH=\":\".join(pkg_config_path))\n        ).decode(\"UTF-8\").split()\n    ]", "category": "Python"}, {"instruction": "def _get_crmod_abmn(self):\n        \"\"\"return a Nx2 array with the measurement configurations formatted\n        CRTomo style\n        \"\"\"\n", "input": "", "output": "        ABMN = np.vstack((\n            self.configs[:, 0] * 1e4 + self.configs[:, 1],\n            self.configs[:, 2] * 1e4 + self.configs[:, 3],\n        )).T.astype(int)\n        return ABMN", "category": "Python"}, {"instruction": "def set_mpi_procs(self, mpi_procs):\n        \"\"\"Set the number of CPUs used for MPI.\"\"\"\n", "input": "", "output": "        QueueAdapter.set_mpi_procs(self, mpi_procs)\n\n        num_nodes, rest_cores = self.hw.divmod_node(mpi_procs, omp_threads=1)\n        if num_nodes == 0:\n            self.qparams[\"nodes\"] = 1\n            self.qparams[\"ppn\"] = mpi_procs\n        else:\n            if rest_cores != 0:\n                # Pack cores as much as possible.\n                num_nodes += 1\n            self.qparams[\"nodes\"] = num_nodes\n            self.qparams[\"ppn\"] = self.hw.cores_per_node", "category": "Python"}, {"instruction": "def executeOnController(self, clusterId, script, lang):\n        \"\"\"\n        Parameters:\n         - clusterId\n         - script\n         - lang\n        \"\"\"\n", "input": "", "output": "        self.send_executeOnController(clusterId, script, lang)\n        return self.recv_executeOnController()", "category": "Python"}, {"instruction": "def _ostaunicode_zero_pad(src, fulllen):\n    # type: (str, int) -> bytes\n    '''\n    Internal function to create a zero-padded Identifier byte string from a\n    source string.\n\n    Parameters:\n     src - The src string to start from.\n     fulllen - The padded out length of the result.\n    Returns:\n     A full identifier byte string containing the source string.\n    '''\n", "input": "", "output": "    byte_src = _ostaunicode(src)\n    return byte_src + b'\\x00' * (fulllen - 1 - len(byte_src)) + (struct.pack('=B', len(byte_src)))", "category": "Python"}, {"instruction": "def send(self, message):\n        \"\"\"\n        Send a message\n\n        :param message: a message instance\n        :type message: :class:`steam.core.msg.Msg`, :class:`steam.core.msg.MsgProto`\n        \"\"\"\n", "input": "", "output": "        if not isinstance(message, (Msg, MsgProto)):\n            raise ValueError(\"Expected Msg or MsgProto, got %s\" % message)\n\n        if self.steam_id:\n            message.steamID = self.steam_id\n        if self.session_id:\n            message.sessionID = self.session_id\n\n        if self.verbose_debug:\n            self._LOG.debug(\"Outgoing: %s\\n%s\" % (repr(message), str(message)))\n        else:\n            self._LOG.debug(\"Outgoing: %s\", repr(message))\n\n        data = message.serialize()\n\n        if self.channel_key:\n            if self.channel_hmac:\n                data = crypto.symmetric_encrypt_HMAC(data, self.channel_key, self.channel_hmac)\n            else:\n                data = crypto.symmetric_encrypt(data, self.channel_key)\n\n        self.connection.put_message(data)", "category": "Python"}, {"instruction": "def create_graph_rules(address_mapper):\n  \"\"\"Creates tasks used to parse Structs from BUILD files.\n\n  :param address_mapper_key: The subject key for an AddressMapper instance.\n  :param symbol_table: A SymbolTable instance to provide symbols for Address lookups.\n  \"\"\"\n", "input": "", "output": "\n  @rule(AddressMapper, [])\n  def address_mapper_singleton():\n    return address_mapper\n\n  return [\n    address_mapper_singleton,\n    # BUILD file parsing.\n    hydrate_struct,\n    parse_address_family,\n    # Spec handling: locate directories that contain build files, and request\n    # AddressFamilies for each of them.\n    addresses_from_address_families,\n    # Root rules representing parameters that might be provided via root subjects.\n    RootRule(Address),\n    RootRule(BuildFileAddress),\n    RootRule(BuildFileAddresses),\n    RootRule(Specs),\n  ]", "category": "Python"}, {"instruction": "def mimedata2url(source, extlist=None):\r\n    \"\"\"\r\n    Extract url list from MIME data\r\n    extlist: for example ('.py', '.pyw')\r\n    \"\"\"\n", "input": "", "output": "    pathlist = []\r\n    if source.hasUrls():\r\n        for url in source.urls():\r\n            path = _process_mime_path(to_text_string(url.toString()), extlist)\r\n            if path is not None:\r\n                pathlist.append(path)\r\n    elif source.hasText():\r\n        for rawpath in to_text_string(source.text()).splitlines():\r\n            path = _process_mime_path(rawpath, extlist)\r\n            if path is not None:\r\n                pathlist.append(path)\r\n    if pathlist:\r\n        return pathlist", "category": "Python"}, {"instruction": "def anchor(self, value):\n        \"\"\"\n        Setter for **self.__anchor** attribute.\n\n        :param value: Attribute value.\n        :type value: int\n        \"\"\"\n", "input": "", "output": "\n        if value is not None:\n            assert type(value) is int, \"'{0}' attribute: '{1}' type is not 'int'!\".format(\"anchor\", value)\n            assert value in range(\n                0, 9), \"'{0}' attribute: '{1}' need to be in '0' to '8' range!\".format(\"anchor\", value)\n        self.__anchor = value", "category": "Python"}, {"instruction": "def line_counts_as_uncovered(line: str,\n                             is_from_cover_annotation_file: bool) -> bool:\n    \"\"\"\n    Args:\n        line: The line of code (including coverage annotation).\n        is_from_cover_annotation_file: Whether this line has been annotated.\n    Returns:\n        Does the line count as uncovered?\n    \"\"\"\n", "input": "", "output": "\n    # Ignore this line?\n    if is_from_cover_annotation_file:\n        # Already covered, or the tool deemed it not relevant for coverage.\n        if not line.startswith('! '):\n            return False\n\n        content = line[2:]\n    else:\n        content = line\n\n    # Ignore surrounding whitespace.\n    content = content.strip()\n\n    # Ignore end-of-line comments.\n    # TODO: avoid # in strings, etc.\n    if '#' in content:\n        content = content[:content.index('#')].strip()\n\n    # Ignored line pattern?\n    if any(re.search(pat, content) for pat in IGNORED_LINE_PATTERNS):\n        return False\n\n    return (is_from_cover_annotation_file or\n            line_content_counts_as_uncovered_manual(content))", "category": "Python"}, {"instruction": "def register_on_snapshot_changed(self, callback):\n        \"\"\"Set the callback function to consume on snapshot changed events\n        which occur when snapshot properties have been changed.\n\n        Callback receives a ISnapshotChangedEvent object.\n\n        Returns the callback_id\n        \"\"\"\n", "input": "", "output": "        event_type = library.VBoxEventType.on_snapshot_changed\n        return self.event_source.register_callback(callback, event_type)", "category": "Python"}, {"instruction": "def send_group_file(self, sender, receiver, media_id):\n        \"\"\"\n        \u53d1\u9001\u7fa4\u804a\u6587\u4ef6\u6d88\u606f\n\n        :param sender: \u53d1\u9001\u4eba\n        :param receiver: \u4f1a\u8bdd ID\n        :param media_id: \u6587\u4ef6id\uff0c\u53ef\u4ee5\u8c03\u7528\u4e0a\u4f20\u7d20\u6750\u6587\u4ef6\u63a5\u53e3\u83b7\u53d6, \u6587\u4ef6\u987b\u5927\u4e8e4\u5b57\u8282\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n", "input": "", "output": "        return self.send_file(sender, 'group', receiver, media_id)", "category": "Python"}, {"instruction": "def getconf(self, path, conf=None, logger=None):\n        \"\"\"Parse a configuration path with input conf and returns\n        parameters by param name.\n\n        :param str path: conf resource path to parse and from get parameters.\n        :param Configuration conf: conf to fill with path values and\n            conf param names.\n        :param Logger logger: logger to use in order to trace\n            information/error.\n        :rtype: Configuration\n        \"\"\"\n", "input": "", "output": "\n        result = conf\n\n        pathconf = None\n\n        rscpaths = self.rscpaths(path=path)\n\n        for rscpath in rscpaths:\n\n            pathconf = self._getconf(rscpath=rscpath, logger=logger, conf=conf)\n\n            if pathconf is not None:\n\n                if result is None:\n                    result = pathconf\n\n                else:\n                    result.update(pathconf)\n\n        return result", "category": "Python"}, {"instruction": "def _prepare_header_cells(self, table_header):\n        \"\"\"\n        Set the scope of header cells of table header.\n\n        :param table_header: The table header.\n        :type table_header: hatemile.util.html.htmldomelement.HTMLDOMElement\n        \"\"\"\n", "input": "", "output": "\n        cells = self.parser.find(table_header).find_children(\n            'tr'\n        ).find_children('th').list_results()\n        for cell in cells:\n            self.id_generator.generate_id(cell)\n\n            cell.set_attribute('scope', 'col')", "category": "Python"}, {"instruction": "def _augmenting_row_reduction(self):\n        \"\"\"\n        Augmenting row reduction step from LAPJV algorithm\n        \"\"\"\n", "input": "", "output": "        unassigned = np.where(self._x == -1)[0]\n        for i in unassigned:\n            for _ in range(self.c.size):\n                # Time in this loop can be proportional to 1/epsilon\n                # This step is not strictly necessary, so cutoff early\n                # to avoid near-infinite loops\n\n                # find smallest 2 values and indices\n                temp = self.c[i] - self._v\n                j1 = np.argmin(temp)\n                u1 = temp[j1]\n                temp[j1] = np.inf\n                j2 = np.argmin(temp)\n                u2 = temp[j2]\n\n                if u1 < u2:\n                    self._v[j1] -= u2 - u1\n                elif self._y[j1] != -1:\n                    j1 = j2\n                k = self._y[j1]\n                if k != -1:\n                    self._x[k] = -1\n                    self._x[i] = j1\n                    self._y[j1] = i\n                    i = k\n                if k == -1 or abs(u1 - u2) < self.epsilon:\n                    break", "category": "Python"}, {"instruction": "def need_to_create_symlink(directory, checksums, filetype, symlink_path):\n    \"\"\"Check if we need to create a symlink for an existing file.\"\"\"\n", "input": "", "output": "    # If we don't have a symlink path, we don't need to create a symlink\n    if symlink_path is None:\n        return False\n\n    pattern = NgdConfig.get_fileending(filetype)\n    filename, _ = get_name_and_checksum(checksums, pattern)\n    full_filename = os.path.join(directory, filename)\n    symlink_name = os.path.join(symlink_path, filename)\n\n    if os.path.islink(symlink_name):\n        existing_link = os.readlink(symlink_name)\n        if full_filename == existing_link:\n            return False\n\n    return True", "category": "Python"}, {"instruction": "def _ssl_PRF(secret, seed, req_len):\n    \"\"\"\n    Provides the implementation of SSLv3 PRF function:\n\n     SSLv3-PRF(secret, seed) =\n        MD5(secret || SHA-1(\"A\" || secret || seed)) ||\n        MD5(secret || SHA-1(\"BB\" || secret || seed)) ||\n        MD5(secret || SHA-1(\"CCC\" || secret || seed)) || ...\n\n    req_len should not be more than  26 x 16 = 416.\n    \"\"\"\n", "input": "", "output": "    if req_len > 416:\n        warning(\"_ssl_PRF() is not expected to provide more than 416 bytes\")\n        return \"\"\n\n    d = [b\"A\", b\"B\", b\"C\", b\"D\", b\"E\", b\"F\", b\"G\", b\"H\", b\"I\", b\"J\", b\"K\", b\"L\",  # noqa: E501\n         b\"M\", b\"N\", b\"O\", b\"P\", b\"Q\", b\"R\", b\"S\", b\"T\", b\"U\", b\"V\", b\"W\", b\"X\",  # noqa: E501\n         b\"Y\", b\"Z\"]\n    res = b\"\"\n    hash_sha1 = _tls_hash_algs[\"SHA\"]()\n    hash_md5 = _tls_hash_algs[\"MD5\"]()\n    rounds = (req_len + hash_md5.hash_len - 1) // hash_md5.hash_len\n\n    for i in range(rounds):\n        label = d[i] * (i + 1)\n        tmp = hash_sha1.digest(label + secret + seed)\n        res += hash_md5.digest(secret + tmp)\n\n    return res[:req_len]", "category": "Python"}, {"instruction": "def assure_image(fnc):\n    \"\"\"\n    Converts a image ID passed as the 'image' parameter to a image object.\n    \"\"\"\n", "input": "", "output": "    @wraps(fnc)\n    def _wrapped(self, img, *args, **kwargs):\n        if not isinstance(img, Image):\n            # Must be the ID\n            img = self._manager.get(img)\n        return fnc(self, img, *args, **kwargs)\n    return _wrapped", "category": "Python"}, {"instruction": "def init_layout(self):\n        \"\"\" Add all child widgets to the view\n        \"\"\"\n", "input": "", "output": "        super(AndroidSurfaceView, self).init_layout()\n\n        # Force layout using the default params\n        if not self.layout_params:\n            self.set_layout({})", "category": "Python"}, {"instruction": "def stream_header(self, f):\n        \"\"\"Stream the block header in the standard way to the file-like object f.\"\"\"\n", "input": "", "output": "        stream_struct(\"L##L\", f, self.version, self.previous_block_hash,\n                      self.merkle_root, self.height)\n        f.write(b'\\0' * 28)\n        stream_struct(\"LL#S\", f, self.timestamp, self.difficulty, self.nonce, self.solution)", "category": "Python"}, {"instruction": "def set_log_file(self, logfile):\n        \"\"\"\n        Set the log file full path including directory path basename and extension.\n\n        :Parameters:\n           #. logFile (string): the full log file path including basename and\n              extension. If this is given, all of logFileBasename and logFileExtension\n              will be discarded. logfile is equivalent to logFileBasename.logFileExtension\n        \"\"\"\n", "input": "", "output": "        assert isinstance(logfile, basestring), \"logfile must be a string\"\n        basename, extension = os.path.splitext(logfile)\n        self.__set_log_file_basename(logFileBasename=basename)\n        self.set_log_file_extension(logFileExtension=extension)", "category": "Python"}, {"instruction": "def eps(self, nodeids=None):\n        \"\"\"\n        Return the EPs with the given *nodeid*, or all EPs.\n\n        Args:\n            nodeids: an iterable of nodeids of EPs to return; if\n                `None`, return all EPs\n        \"\"\"\n", "input": "", "output": "        if nodeids is None: nodeids = self._nodeids\n        _eps = self._eps\n        return [_eps[nodeid] for nodeid in nodeids]", "category": "Python"}, {"instruction": "def config_mode(self, config_command=\"config\", pattern=\">config\"):\n        \"\"\"Enter into configuration mode on remote device.\"\"\"\n", "input": "", "output": "        return super(RadETXBase, self).config_mode(\n            config_command=config_command, pattern=pattern\n        )", "category": "Python"}, {"instruction": "def bank_account_query(self, number, date, account_type, bank_id):\n        \"\"\"Bank account statement request\"\"\"\n", "input": "", "output": "        return self.authenticated_query(\n            self._bareq(number, date, account_type, bank_id)\n        )", "category": "Python"}, {"instruction": "def name(self) -> str:\n        \"\"\"Return the device name.\"\"\"\n", "input": "", "output": "        return next(\n            attr['Value'] for attr in self._device_json.get('Attributes', [])\n            if attr.get('AttributeDisplayName') == 'desc')", "category": "Python"}, {"instruction": "def get_title(self, group=None):\n        \"\"\"Adds number of comments to title.\"\"\"\n", "input": "", "output": "        title = super(CommentsPlugin, self).get_title()\n        if group is not None:\n            count = GroupComments.objects.filter(group=group).count()\n        else:\n            count = None\n        if count:\n            title = u'%s (%d)' % (title, count)\n        return title", "category": "Python"}, {"instruction": "def _encode_caveat_v1(condition, root_key, third_party_pub_key, key):\n    '''Create a JSON-encoded third-party caveat.\n\n    The third_party_pub_key key represents the PublicKey of the third party\n    we're encrypting the caveat for; the key is the public/private key pair of\n    the party that's adding the caveat.\n\n    @param condition string\n    @param root_key bytes\n    @param third_party_pub_key (PublicKey)\n    @param key (PrivateKey)\n    @return a base64 encoded bytes\n    '''\n", "input": "", "output": "    plain_data = json.dumps({\n        'RootKey': base64.b64encode(root_key).decode('ascii'),\n        'Condition': condition\n    })\n    box = nacl.public.Box(key.key, third_party_pub_key.key)\n\n    encrypted = box.encrypt(six.b(plain_data))\n    nonce = encrypted[0:nacl.public.Box.NONCE_SIZE]\n    encrypted = encrypted[nacl.public.Box.NONCE_SIZE:]\n    return base64.b64encode(six.b(json.dumps({\n        'ThirdPartyPublicKey': str(third_party_pub_key),\n        'FirstPartyPublicKey': str(key.public_key),\n        'Nonce': base64.b64encode(nonce).decode('ascii'),\n        'Id': base64.b64encode(encrypted).decode('ascii')\n    })))", "category": "Python"}, {"instruction": "def warn(self, text):\n\t\t\"\"\" Ajout d'un message de log de type WARN \"\"\"\n", "input": "", "output": "\t\tself.logger.warn(\"{}{}\".format(self.message_prefix, text))", "category": "Python"}, {"instruction": "def update(self, data, overwrite=None, overwrite_sections=True, overwrite_options=True):\n        \"\"\"\n        Updates the currently stored configuration with new *data*, given as a dictionary. When\n        *overwrite_sections* is *False*, sections in *data* that are already present in the current\n        config are skipped. When *overwrite_options* is *False*, existing options are not\n        overwritten. When *overwrite* is not *None*, both *overwrite_sections* and\n        *overwrite_options* are set to its value.\n        \"\"\"\n", "input": "", "output": "        if overwrite is not None:\n            overwrite_sections = overwrite\n            overwrite_options = overwrite\n\n        for section, _data in six.iteritems(data):\n            if not self.has_section(section):\n                self.add_section(section)\n            elif not overwrite_sections:\n                continue\n\n            for option, value in six.iteritems(_data):\n                if overwrite_options or not self.has_option(section, option):\n                    self.set(section, option, str(value))", "category": "Python"}, {"instruction": "def source_bash(args, stdin=None):\n    \"\"\"Simply bash-specific wrapper around source-foreign\n\n    Returns a dict to be used as a new environment\"\"\"\n", "input": "", "output": "    args = list(args)\n    new_args = ['bash', '--sourcer=source']\n    new_args.extend(args)\n    return source_foreign(new_args, stdin=stdin)", "category": "Python"}, {"instruction": "def X(self):\n        \"\"\" Creates design matrix of variables to use in GP regression\n        \n        Returns\n        ----------\n        The design matrix\n        \"\"\"\n", "input": "", "output": "        if self.ar == 1:\n            return np.array([self.data_full[(self.max_lag-1):-1]])\n        else:\n            for i in range(0,self.ar):\n                datapoint = self.data_full[(self.max_lag-i-1):-i-1]         \n                if i == 0:\n                    X = datapoint\n                else:\n                    X = np.vstack((X,datapoint))\n        return X", "category": "Python"}, {"instruction": "def autoencoder_autoregressive():\n  \"\"\"Autoregressive autoencoder model.\"\"\"\n", "input": "", "output": "  hparams = autoencoder_basic()\n  hparams.add_hparam(\"autoregressive_forget_base\", False)\n  hparams.add_hparam(\"autoregressive_mode\", \"none\")\n  hparams.add_hparam(\"autoregressive_decode_steps\", 0)\n  hparams.add_hparam(\"autoregressive_eval_pure_autoencoder\", False)\n  hparams.add_hparam(\"autoregressive_gumbel_sample\", False)\n  return hparams", "category": "Python"}, {"instruction": "def unique(self):\n\t\t\"\"\"\n\t\tReturn a Cache which has every element of self, but without\n\t\tduplication.  Preserve order.  Does not hash, so a bit slow.\n\t\t\"\"\"\n", "input": "", "output": "\t\tnew = self.__class__([])\n\t\tfor elem in self:\n\t\t\tif elem not in new:\n\t\t\t\tnew.append(elem)\n\t\treturn new", "category": "Python"}, {"instruction": "def _get_yum_config_value(name):\n    '''\n    Look for a specific config variable and return its value\n    '''\n", "input": "", "output": "    conf = _get_yum_config()\n    if name in conf.keys():\n        return conf.get(name)\n    return None", "category": "Python"}, {"instruction": "def configure_key_provider(graph, key_ids):\n    \"\"\"\n    Configure a key provider.\n\n    During unit tests, use a static key provider (e.g. without AWS calls).\n\n    \"\"\"\n", "input": "", "output": "    if graph.metadata.testing:\n        # use static provider\n        provider = StaticMasterKeyProvider()\n        provider.add_master_keys_from_list(key_ids)\n        return provider\n\n    # use AWS provider\n    return KMSMasterKeyProvider(key_ids=key_ids)", "category": "Python"}, {"instruction": "def _get_output_cwl_keys(fnargs):\n    \"\"\"Retrieve output_cwl_keys from potentially nested input arguments.\n    \"\"\"\n", "input": "", "output": "    for d in utils.flatten(fnargs):\n        if isinstance(d, dict) and d.get(\"output_cwl_keys\"):\n            return d[\"output_cwl_keys\"]\n    raise ValueError(\"Did not find output_cwl_keys in %s\" % (pprint.pformat(fnargs)))", "category": "Python"}, {"instruction": "def _score_macro_average(self, n_classes):\n        \"\"\"\n        Compute the macro average scores for the ROCAUC curves.\n        \"\"\"\n", "input": "", "output": "        # Gather all FPRs\n        all_fpr = np.unique(np.concatenate([self.fpr[i] for i in range(n_classes)]))\n        avg_tpr = np.zeros_like(all_fpr)\n\n        # Compute the averages per class\n        for i in range(n_classes):\n            avg_tpr += interp(all_fpr, self.fpr[i], self.tpr[i])\n\n        # Finalize the average\n        avg_tpr /= n_classes\n\n        # Store the macro averages\n        self.fpr[MACRO] = all_fpr\n        self.tpr[MACRO] = avg_tpr\n        self.roc_auc[MACRO] = auc(self.fpr[MACRO], self.tpr[MACRO])", "category": "Python"}, {"instruction": "def tag_delete(self, tag):\n        \"\"\"\n        Function is public.\n        Push tags.\n        :return:\n        \"\"\"\n", "input": "", "output": "        if tag:\n            if self.__git_tag_delete(tag):\n                return True\n            return False\n\n        if self.__git_tag_delete(self.get_latest_tag()):\n            return True\n        return False", "category": "Python"}, {"instruction": "def get(cls, resource_type):\n        \"\"\"Returns the ResourceType object for `resource_type`. If no existing object was found, a new type will\n        be created in the database and returned\n\n        Args:\n            resource_type (str): Resource type name\n\n        Returns:\n            :obj:`ResourceType`\n        \"\"\"\n", "input": "", "output": "        if isinstance(resource_type, str):\n            obj = getattr(db, cls.__name__).find_one(cls.resource_type == resource_type)\n\n        elif isinstance(resource_type, int):\n            obj = getattr(db, cls.__name__).find_one(cls.resource_type_id == resource_type)\n\n        elif isinstance(resource_type, cls):\n            return resource_type\n\n        else:\n            obj = None\n\n        if not obj:\n            obj = cls()\n            obj.resource_type = resource_type\n            db.session.add(obj)\n            db.session.commit()\n            db.session.refresh(obj)\n\n        return obj", "category": "Python"}, {"instruction": "def gen_te(self):\n        \"\"\"Generate a :class:`TransactionEnvelope\n        <stellar_base.transaction_envelope.TransactionEnvelope>` around the\n        generated Transaction via the list of operations in this instance.\n\n        :return: A transaction envelope ready to send over the network.\n        :rtype: :class:`TransactionEnvelope\n            <stellar_base.transaction_envelope.TransactionEnvelope>`\n\n        \"\"\"\n", "input": "", "output": "        if self.tx is None:\n            self.gen_tx()\n        te = Te(self.tx, network_id=self.network)\n        if self.te:\n            te.signatures = self.te.signatures\n        self.te = te\n        return te", "category": "Python"}, {"instruction": "def start( self ):\n        \"\"\"\n        Starts the thread in its own event loop if the local and global thread\n        options are true, otherwise runs the thread logic in the main event\n        loop.\n        \"\"\"\n", "input": "", "output": "        if ( self.localThreadingEnabled() and self.globalThreadingEnabled() ):\n            super(XThread, self).start()\n        else:\n            self.run()\n            self.finished.emit()", "category": "Python"}, {"instruction": "def escape_chars(text, chars):\n    \"\"\"Helper function to escape uncomfortable characters.\"\"\"\n", "input": "", "output": "    text = str(text)\n    chars = list(set(chars))\n    if '\\\\' in chars:\n        chars.remove('\\\\')\n        chars.insert(0, '\\\\')\n    for ch in chars:\n        text = text.replace(ch, '\\\\' + ch)\n    return text", "category": "Python"}, {"instruction": "def clean(self):\n        \"\"\"Global cleanup.\"\"\"\n", "input": "", "output": "        super(LineFormSet, self).clean()\n\n        if any(self.errors):\n            # Already seen errors, let's skip.\n            return\n\n        self.clean_unique_fields()", "category": "Python"}, {"instruction": "def _create_console_handles(self, project):\n        r\"\"\"\n        Adds all objects in the given project to the console as variables\n        with handle names taken from each object's name.\n        \"\"\"\n", "input": "", "output": "        import __main__\n        for item in project:\n            __main__.__dict__[item.name] = item", "category": "Python"}, {"instruction": "def _check_dna(self, dna):\n        \"\"\" Check that a DNA string only contains characters in ``GENETIC_MATERIAL_OPTIONS``. \"\"\"\n", "input": "", "output": "        valid_chars = set(self.GENETIC_MATERIAL_OPTIONS)\n        assert all(char in valid_chars for char in dna)", "category": "Python"}, {"instruction": "def docs(session):\n    \"\"\"Build the docs.\"\"\"\n", "input": "", "output": "\n    # Build docs against the latest version of Python, because we can.\n    session.interpreter = 'python3.6'\n\n    # Set the virtualenv dirname.\n    session.virtualenv_dirname = 'docs'\n\n    # Install Sphinx and also all of the google-cloud-* packages.\n    session.chdir(os.path.realpath(os.path.dirname(__file__)))\n    session.install('-r', os.path.join('docs', 'requirements.txt'))\n\n    # Build the docs!\n    session.run(\n        'bash', os.path.join('.', 'scripts', 'update_docs.sh'))", "category": "Python"}, {"instruction": "def on_feed_key(self, key_press):\n        \"\"\"Handles the magictyping when a key is pressed\"\"\"\n", "input": "", "output": "        if key_press.key in {Keys.Escape, Keys.ControlC}:\n            echo(carriage_return=True)\n            raise Abort()\n        if key_press.key == Keys.Backspace:\n            if self.current_command_pos > 0:\n                self.current_command_pos -= 1\n            return key_press\n        ret = None\n        if key_press.key != Keys.CPRResponse:\n            if self.current_command_pos < len(self.current_command):\n                current_key = self.current_command_key\n                ret = KeyPress(current_key)\n                increment = min(\n                    [self.speed, len(self.current_command) - self.current_command_pos]\n                )\n                self.current_command_pos += increment\n            else:\n                # Command is finished, wait for Enter\n                if key_press.key != Keys.Enter:\n                    return None\n                self.current_command_index += 1\n                self.current_command_pos = 0\n                ret = key_press\n        return ret", "category": "Python"}, {"instruction": "def _limits_helper(x1, x2, a, b, snap=False):\n    \"\"\"\n    Given x1, x2, a, b, where:\n\n        x1 - x0         x3 - x2\n    a = ------- ,   b = -------\n        x3 - x0         x3 - x0\n\n    determine the points x0 and x3:\n\n    x0         x1                x2       x3\n    |----------|-----------------|--------|\n\n    \"\"\"\n", "input": "", "output": "    if x2 < x1:\n        raise ValueError(\"x2 < x1\")\n    if a + b >= 1:\n        raise ValueError(\"a + b >= 1\")\n    if a < 0:\n        raise ValueError(\"a < 0\")\n    if b < 0:\n        raise ValueError(\"b < 0\")\n    if snap:\n        if x1 >= 0:\n            x1 = 0\n            a = 0\n        elif x2 <= 0:\n            x2 = 0\n            b = 0\n        if x1 == x2 == 0:\n            # garbage in garbage out\n            return 0., 1.\n    elif x1 == x2:\n        # garbage in garbage out\n        return x1 - 1., x1 + 1.\n    if a == 0 and b == 0:\n        return x1, x2\n    elif a == 0:\n        return x1, (x2 - b * x1) / (1 - b)\n    elif b == 0:\n        return (x1 - a * x2) / (1 - a), x2\n    x0 = ((b / a) * x1 + x2 - (x2 - x1) / (1 - a - b)) / (1 + b / a)\n    x3 = (x2 - x1) / (1 - a - b) + x0\n    return x0, x3", "category": "Python"}, {"instruction": "def _sumterm(lexer):\n    \"\"\"Return a sum term expresssion.\"\"\"\n", "input": "", "output": "    xorterm = _xorterm(lexer)\n    sumterm_prime = _sumterm_prime(lexer)\n    if sumterm_prime is None:\n        return xorterm\n    else:\n        return ('or', xorterm, sumterm_prime)", "category": "Python"}, {"instruction": "def half_light_radius_source(self, kwargs_source, center_x=0, center_y=0, deltaPix=None, numPix=None):\n        \"\"\"\n        computes numerically the half-light-radius of the deflector light and the total photon flux\n\n        :param kwargs_source:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if numPix is None:\n            numPix = 1000\n        if deltaPix is None:\n            deltaPix = 0.005\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        x_grid += center_x\n        y_grid += center_y\n        source_light = self.SourceModel.surface_brightness(x_grid, y_grid, kwargs_source)\n        R_h = analysis_util.half_light_radius(source_light, x_grid, y_grid, center_x=center_x, center_y=center_y)\n        return R_h", "category": "Python"}, {"instruction": "def split_denovos(denovo_path, temp_dir):\n    \"\"\" split de novos from an input file into files, one for each gene\n    \"\"\"\n", "input": "", "output": "    \n    # open the de novos, drop the header line, then sort them (which will be by\n    # HGNC symbol, as the first element of each line)\n    with open(denovo_path, \"r\") as handle:\n        lines = handle.readlines()\n        header = lines.pop(0)\n    \n    basename = os.path.basename(denovo_path)\n    \n    # only include genes with 2+ missense SNVs\n    counts = count_missense_per_gene(lines)\n    counts = dict((k, v) for k, v in counts.items() if v > 1 )\n    \n    genes = set([])\n    for line in sorted(lines):\n        gene = line.split(\"\\t\")[0]\n        \n        # open a new output file whenever we hit a different gene\n        if gene not in genes and gene in counts:\n            genes.add(gene)\n            path = os.path.join(temp_dir, \"tmp.{}.txt\".format(len(genes)))\n            output = open(path, \"w\")\n            output.write(header)\n        \n        if gene in counts:\n            output.write(line)\n    \n    return len(genes)", "category": "Python"}, {"instruction": "def submit(args=None):\n    \"\"\" Performs the submit according to arguments and\n    returns an object describing the result.\n    \"\"\"\n", "input": "", "output": "    streamsx._streams._version._mismatch_check('streamsx.topology.context')\n    cmd_args = _parse_args(args)\n    if cmd_args.topology is not None:\n        app = _get_topology_app(cmd_args)\n    elif cmd_args.main_composite is not None:\n        app = _get_spl_app(cmd_args)\n    elif cmd_args.bundle is not None:\n        app = _get_bundle(cmd_args)\n    _job_config_args(cmd_args, app)\n    sr = _submit(cmd_args, app)\n    if 'return_code' not in sr:\n        sr['return_code'] = 1;\n    print(sr)\n    return sr", "category": "Python"}, {"instruction": "def _PrintCheckDependencyStatus(\n      self, dependency, result, status_message, verbose_output=True):\n    \"\"\"Prints the check dependency status.\n\n    Args:\n      dependency (DependencyDefinition): dependency definition.\n      result (bool): True if the Python module is available and conforms to\n            the minimum required version, False otherwise.\n      status_message (str): status message.\n      verbose_output (Optional[bool]): True if output should be verbose.\n    \"\"\"\n", "input": "", "output": "    if not result or dependency.is_optional:\n      if dependency.is_optional:\n        status_indicator = '[OPTIONAL]'\n      else:\n        status_indicator = '[FAILURE]'\n\n      print('{0:s}\\t{1:s}'.format(status_indicator, status_message))\n\n    elif verbose_output:\n      print('[OK]\\t\\t{0:s}'.format(status_message))", "category": "Python"}, {"instruction": "def editors(self):\n        \"\"\"Legacy access to editor role.\n\n        DEPRECATED:  use ``policy[\"roles/editors\"]`` instead.\"\"\"\n", "input": "", "output": "        result = set()\n        for role in self._EDITOR_ROLES:\n            for member in self._bindings.get(role, ()):\n                result.add(member)\n        return frozenset(result)", "category": "Python"}, {"instruction": "def ordered_load(stream, Loader=yaml.Loader, object_pairs_hook=OrderedDict):\n    \"\"\"\n    Loads an ordered dict into a yaml while preserving the order\n\n    :param stream: the name of the stream\n    :param Loader: the yam loader (such as yaml.SafeLoader)\n    :param object_pairs_hook: the ordered dict\n    \"\"\"\n", "input": "", "output": "\n    # noinspection PyClassHasNoInit\n    class OrderedLoader(Loader):\n        ", "category": "Python"}, {"instruction": "def context(self):\n        \"\"\"\n            :return: A dictionary to contextualize :attr:`template`\n            :rtype: dict\n        \"\"\"\n", "input": "", "output": "        return {\n            'code': self.code,\n            'msg': self.msg,\n            'IssueInstant': timezone.now().isoformat(),\n            'ResponseID': utils.gen_saml_id()\n        }", "category": "Python"}, {"instruction": "def send_mail(send_from, send_to, subject, text, server, mime='plain', files=None):\n    \"\"\"\n    Send an email with attachments.\n    :param send_from: from email adress\n    :param send_to: to email adress\n    :param subject: email subject\n    :param text: text of the email in html\n    :param server: SMTP server\n    :param files: files to attach\n    :return: None\n    \"\"\"\n", "input": "", "output": "    if not files:\n        files = []\n\n    assert type(send_to) == list\n    assert type(files) == list\n\n    msg = MIMEMultipart()\n    msg['From'] = send_from\n    msg['To'] = COMMASPACE.join(send_to)\n    msg['Date'] = formatdate(localtime=True)\n    msg['Subject'] = subject\n\n    msg.attach(MIMEText(text, mime))\n\n    for f in files:\n        part = MIMEBase('application', \"octet-stream\")\n        fp = open(f, \"rb\")\n        file_content = fp.read()\n        part.set_payload(file_content)\n        encoders.encode_base64(part)\n        part.add_header('Content-Disposition', 'attachment; filename=\"%s\"' % os.path.basename(f))\n        msg.attach(part)\n\n    smtp = smtplib.SMTP(server)\n    smtp.sendmail(send_from, send_to, msg.as_string())\n    smtp.close()\n    return", "category": "Python"}, {"instruction": "def retrieve_author(id=None, username=None):\n    \"\"\"\n    Retrieve a SpigotAuthor via their id, or username.\n    :param id:\n    :param username:\n    :return:\n    \"\"\"\n", "input": "", "output": "\n    if id is None and username is None:\n        raise SpigotAuthorException(\"Unable to retrieve an Author without an Identifier\")\n\n    if id is None:\n        return SpigotAuthor.from_username(username)\n    else:\n        return SpigotAuthor.from_id(id)", "category": "Python"}, {"instruction": "def source_url(farm, server, id, secret, size):\n    ''' Url for direct jpg use. '''\n", "input": "", "output": "    if size == 'small':\n        img_size = 'n'\n    elif size == 'medium':\n        img_size = 'c'\n    elif size == 'large':\n        img_size = 'b'\n\n    return 'https://farm{}.staticflickr.com/{}/{}_{}_{}.jpg'.format(\n        farm, server, id, secret, img_size)", "category": "Python"}, {"instruction": "def from_headers(self, headers):\n        \"\"\"Generate a SpanContext object using the trace context header.\n\n        :type headers: dict\n        :param headers: HTTP request headers.\n\n        :rtype: :class:`~opencensus.trace.span_context.SpanContext`\n        :returns: SpanContext generated from the trace context header.\n        \"\"\"\n", "input": "", "output": "        if headers is None:\n            return SpanContext()\n        header = headers.get(_TRACE_CONTEXT_HEADER_NAME)\n        if header is None:\n            return SpanContext()\n        header = str(header.encode('utf-8'))\n        return self.from_header(header)", "category": "Python"}, {"instruction": "def plugin(module, *args, **kwargs):\n    \"\"\"\n    Decorator to extend a package to a view.\n    The module can be a class or function. It will copy all the methods to the class\n\n    ie:\n        # Your module.py\n        my_ext(view, **kwargs):\n            class MyExtension(object):\n                def my_view(self):\n                    return {}\n\n            return MyExtension\n\n        # Your view.py\n        @plugin(my_ext)\n        class Index(View):\n            pass\n\n    :param module: object\n    :param args:\n    :param kwargs:\n    :return:\n    \"\"\"\n", "input": "", "output": "    def wrap(f):\n        m = module(f, *args, **kwargs)\n        if inspect.isclass(m):\n            for k, v in m.__dict__.items():\n                if not k.startswith(\"__\"):\n                    setattr(f, k, v)\n        elif inspect.isfunction(m):\n            setattr(f, kls.__name__, m)\n        return f\n    return wrap", "category": "Python"}, {"instruction": "def StripTypeInfo(rendered_data):\n  \"\"\"Strips type information from rendered data. Useful for debugging.\"\"\"\n", "input": "", "output": "\n  if isinstance(rendered_data, (list, tuple)):\n    return [StripTypeInfo(d) for d in rendered_data]\n  elif isinstance(rendered_data, dict):\n    if \"value\" in rendered_data and \"type\" in rendered_data:\n      return StripTypeInfo(rendered_data[\"value\"])\n    else:\n      result = {}\n      for k, v in iteritems(rendered_data):\n        result[k] = StripTypeInfo(v)\n      return result\n  else:\n    return rendered_data", "category": "Python"}, {"instruction": "def url(self, url_str: str) -> None:\n        \"\"\"\n        url \u91cd\u5199\n        \"\"\"\n", "input": "", "output": "        if \"?\" in url_str:\n            url_arr = url_str.split(\"?\")\n            self.parse_url.path = url_arr[0]\n            self.parse_url.querystring = url_arr[1]", "category": "Python"}, {"instruction": "def do_action_to_analyses(analysis_request, transition_id, all_analyses=False):\n    \"\"\"Cascades the transition to the analysis request analyses. If all_analyses\n    is set to True, the transition will be triggered for all analyses of this\n    analysis request, those from the descendant partitions included.\n    \"\"\"\n", "input": "", "output": "    analyses = list()\n    if all_analyses:\n        analyses = analysis_request.getAnalyses(full_objects=True)\n    else:\n        analyses = analysis_request.objectValues(\"Analysis\")\n    for analysis in analyses:\n        do_action_for(analysis, transition_id)", "category": "Python"}, {"instruction": "def get_rate_from_db(currency: str) -> Decimal:\n    \"\"\"\n    Fetch currency conversion rate from the database\n    \"\"\"\n", "input": "", "output": "    from .models import ConversionRate\n    try:\n        rate = ConversionRate.objects.get_rate(currency)\n    except ConversionRate.DoesNotExist:  # noqa\n        raise ValueError('No conversion rate for %s' % (currency, ))\n    return rate.rate", "category": "Python"}, {"instruction": "async def heater_control(self, device_id, fan_status=None,\n                             power_status=None):\n        \"\"\"Set heater temps.\"\"\"\n", "input": "", "output": "        heater = self.heaters.get(device_id)\n        if heater is None:\n            _LOGGER.error(\"No such device\")\n            return\n        if fan_status is None:\n            fan_status = heater.fan_status\n        if power_status is None:\n            power_status = heater.power_status\n        operation = 0 if fan_status == heater.fan_status else 4\n        payload = {\"subDomain\": heater.sub_domain,\n                   \"deviceId\": device_id,\n                   \"testStatus\": 1,\n                   \"operation\": operation,\n                   \"status\": power_status,\n                   \"windStatus\": fan_status,\n                   \"holdTemp\": heater.set_temp,\n                   \"tempType\": 0,\n                   \"powerLevel\": 0}\n        await self.request(\"deviceControl\", payload)", "category": "Python"}, {"instruction": "def mid(self, value):\n        \"\"\"\n        Sets the MID of the message.\n\n        :type value: Integer\n        :param value: the MID\n        :raise AttributeError: if value is not int or cannot be represented on 16 bits.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(value, int) or value > 65536:\n            raise AttributeError\n        self._mid = value", "category": "Python"}, {"instruction": "def find_contour_yaml(config_file=__file__, names=None):\n    \"\"\"\n    Traverse directory trees to find a contour.yaml file\n\n    Begins with the location of this file then checks the\n    working directory if not found\n\n    Args:\n        config_file: location of this file, override for\n        testing\n    Returns:\n        the path of contour.yaml or None if not found\n    \"\"\"\n", "input": "", "output": "    checked = set()\n    contour_yaml = _find_countour_yaml(os.path.dirname(config_file), checked,\n                                       names=names)\n\n    if not contour_yaml:\n        contour_yaml = _find_countour_yaml(os.getcwd(), checked, names=names)\n\n    return contour_yaml", "category": "Python"}, {"instruction": "def _get_all_unmapped_reads(self, fout):\n        '''Writes all unmapped reads to fout'''\n", "input": "", "output": "        sam_reader = pysam.Samfile(self.bam, \"rb\")\n        for read in sam_reader.fetch(until_eof=True):\n            if read.is_unmapped:\n                print(mapping.aligned_read_to_read(read, ignore_quality=not self.fastq_out), file=fout)", "category": "Python"}, {"instruction": "def major_axes(ell):\n        \"\"\"\n        Gets major axes of ellipsoids\n        \"\"\"\n", "input": "", "output": "        _ = ell[:-1,:-1]\n        U,s,V = N.linalg.svd(_)\n        scalar = -(ell.sum()-_.sum())\n        return N.sqrt(s*scalar)*V", "category": "Python"}, {"instruction": "def reset (self):\n        \"\"\"Reset all log statistics to default values.\"\"\"\n", "input": "", "output": "        # number of logged URLs\n        self.number = 0\n        # number of encountered URL errors\n        self.errors = 0\n        # number of URL errors that were printed\n        self.errors_printed = 0\n        # number of URL warnings\n        self.warnings = 0\n        # number of URL warnings that were printed\n        self.warnings_printed = 0\n        # number of internal errors\n        self.internal_errors = 0\n        # link types\n        self.link_types = ContentTypes.copy()\n        # URL length statistics\n        self.max_url_length = 0\n        self.min_url_length = 0\n        self.avg_url_length = 0.0\n        self.avg_number = 0\n        # overall downloaded bytes\n        self.downloaded_bytes = None", "category": "Python"}, {"instruction": "def fetchone(self):\n        \"\"\"Fetch next row\"\"\"\n", "input": "", "output": "        self._check_executed()\n        row = self.read_next()\n        if row is None:\n            return None\n        self.rownumber += 1\n        return row", "category": "Python"}, {"instruction": "def time_bins(header):\n  '''\n  Returns the time-axis lower bin edge values for the spectrogram.\n  '''\n", "input": "", "output": "  return np.arange(header['number_of_half_frames'], dtype=np.float64)*constants.bins_per_half_frame\\\n  *(1.0 - header['over_sampling']) / header['subband_spacing_hz']", "category": "Python"}, {"instruction": "def enable_rm_ha(self, new_rm_host_id, zk_service_name=None):\n    \"\"\"\n    Enable high availability for a YARN ResourceManager.\n\n    @param new_rm_host_id: id of the host where the second ResourceManager\n                           will be added.\n    @param zk_service_name: Name of the ZooKeeper service to use for auto-failover.\n           If YARN service depends on a ZooKeeper service then that ZooKeeper\n           service will be used for auto-failover and in that case this parameter\n           can be omitted.\n    @return: Reference to the submitted command.\n    @since: API v6\n    \"\"\"\n", "input": "", "output": "    args = dict(\n      newRmHostId = new_rm_host_id,\n      zkServiceName = zk_service_name\n    )\n    return self._cmd('enableRmHa', data=args)", "category": "Python"}, {"instruction": "def vertices(self, values):\n        \"\"\"\n        Assign vertex values to the mesh.\n\n        Parameters\n        --------------\n        values : (n, 3) float\n          Points in space\n        \"\"\"\n", "input": "", "output": "        self._data['vertices'] = np.asanyarray(values,\n                                               order='C',\n                                               dtype=np.float64)", "category": "Python"}, {"instruction": "def get_activity_admin_session(self):\n        \"\"\"Gets the OsidSession associated with the activity administration\n        service.\n\n        return: (osid.learning.ActivityAdminSession) - a\n                ActivityAdminSession\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented - supports_activity_admin() is false\n        compliance: optional - This method must be implemented if\n                    supports_activity_admin() is true.\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_activity_admin():\n            raise Unimplemented()\n        try:\n            from . import sessions\n        except ImportError:\n            raise OperationFailed()\n        try:\n            session = sessions.ActivityAdminSession(runtime=self._runtime)\n        except AttributeError:\n            raise OperationFailed()\n        return session", "category": "Python"}, {"instruction": "def clear(self):\n        \"\"\"Clear the set.\"\"\"\n", "input": "", "output": "        self._set.clear()\n        del self._headers[:]\n        if self.on_update is not None:\n            self.on_update(self)", "category": "Python"}, {"instruction": "def _report_part(api, url, upload, part, e_tag):\n    \"\"\"\n    Used by the worker to report the completion of the part upload.\n    :param api: Api instance.\n    :param url: Part url.\n    :param upload: Upload identifier.\n    :param part: Part number.\n    :param e_tag: ETag\n    \"\"\"\n", "input": "", "output": "    part_data = {\n        'part_number': part,\n        'response': {\n            'headers': {\n                'ETag': e_tag\n            }\n        }\n    }\n    try:\n        api.post(\n            url.format(upload_id=upload, part_number=''), data=part_data\n        )\n    except Exception as e:\n        raise SbgError(\n            'Unable to report part number {}. Reason: {}'.format(\n                part, six.text_type(e)\n            )\n        )", "category": "Python"}, {"instruction": "def set_seed(self, seed):\n\n        \"\"\"\n        Override default values for random initial topic assignment,\n        set to \"seed\" instead.\n        seed is 2-d array (number of samples in LDA model x number\n        of tokens in LDA model)\n        \"\"\"\n", "input": "", "output": "\n        assert seed.dtype == np.int and seed.shape == (self.samples, self.N)\n        self.topic_seed = seed", "category": "Python"}, {"instruction": "def require_paragraph(self):\n        \"\"\"Create a new paragraph unless the currently-active container\n        is already a paragraph.\"\"\"\n", "input": "", "output": "        if self._containers and _is_paragraph(self._containers[-1]):\n            return False\n        else:\n            self.start_paragraph()\n            return True", "category": "Python"}, {"instruction": "def install_package(self, name, index=None, force=False, update=False):\n        \"\"\"Install a given package.\n\n        Args:\n            name (str): The package name to install. This can be any valid\n                pip package specification.\n            index (str): The URL for a pypi index to use.\n            force (bool): For the reinstall of packages during updates.\n            update (bool): Update the package if it is out of date.\n        \"\"\"\n", "input": "", "output": "        cmd = 'install'\n        if force:\n\n            cmd = '{0} {1}'.format(cmd, '--force-reinstall')\n\n        if update:\n\n            cmd = '{0} {1}'.format(cmd, '--update')\n\n        if index:\n\n            cmd = '{0} {1}'.format(cmd, '--index-url {0}'.format(index))\n\n        self.pip('{0} {1}'.format(cmd, name))", "category": "Python"}, {"instruction": "def send_PointerEvent(self, x, y, buttonmask=0):\n        \"\"\"Indicates either pointer movement or a pointer button press or\n           release. The pointer is now at (x-position, y-position),\n           and the current state of buttons 1 to 8 are represented by\n           bits 0 to 7 of button-mask respectively, 0 meaning up, 1\n           meaning down (pressed).\n        \"\"\"\n", "input": "", "output": "        self.sendMessage(struct.pack('!BBHH', 5, buttonmask, x, y))", "category": "Python"}, {"instruction": "def extended_analog(self, pin, data):\n        \"\"\"\n        This method will send an extended-data analog write command\n        to the selected pin..\n\n        :param pin: 0 - 127\n\n        :param data: 0 - 0-0x4000 (14 bits)\n\n        :returns: No return value\n        \"\"\"\n", "input": "", "output": "        task = asyncio.ensure_future(self.core.extended_analog(pin, data))\n        self.loop.run_until_complete(task)", "category": "Python"}, {"instruction": "def save_checkpoint(self, prefix, epoch, save_optimizer_states=False):\n        \"\"\"Saves current progress to checkpoint.\n        Use `mx.callback.module_checkpoint` as `epoch_end_callback` to save during training.\n\n        Parameters\n        ----------\n        prefix : str\n            The file prefix to checkpoint to.\n        epoch : int\n            The current epoch number.\n        save_optimizer_states : bool\n            Whether to save optimizer states to continue training.\n        \"\"\"\n", "input": "", "output": "        self._symbol.save('%s-symbol.json'%prefix)\n        param_name = '%s-%04d.params' % (prefix, epoch)\n        self.save_params(param_name)\n        logging.info('Saved checkpoint to \\\"%s\\\"', param_name)\n        if save_optimizer_states:\n            state_name = '%s-%04d.states' % (prefix, epoch)\n            self.save_optimizer_states(state_name)\n            logging.info('Saved optimizer state to \\\"%s\\\"', state_name)", "category": "Python"}, {"instruction": "def numToDigits(num, places):\n    \"\"\"\n    Helper, for converting numbers to textual digits.\n    \"\"\"\n", "input": "", "output": "    s = str(num)\n    if len(s) < places:\n        return (\"0\" * (places - len(s))) + s\n    elif len(s) > places:\n        return s[len(s)-places: ]\n    else:\n        return s", "category": "Python"}, {"instruction": "def invalid_ipa_characters(unicode_string, indices=False):\n    \"\"\"\n    Return the list of Unicode characters\n    in the given Unicode string\n    that are not IPA valid.\n\n    Return ``None`` if ``unicode_string`` is ``None``.\n\n    :param str unicode_string: the Unicode string to be parsed\n    :param bool indices: if ``True``, return a list of pairs (index, invalid character),\n                         instead of a list of str (characters).\n    :rtype: list of str or list of (int, str) \n    \"\"\"\n", "input": "", "output": "    if unicode_string is None:\n        return None\n    if indices:\n        return [(i, unicode_string[i]) for i in range(len(unicode_string)) if unicode_string[i] not in UNICODE_TO_IPA]\n    return set([c for c in unicode_string if c not in UNICODE_TO_IPA])", "category": "Python"}, {"instruction": "def send_security_email(data):\n    \"\"\"Celery task to send security email.\n\n    :param data: Contains the email data.\n    \"\"\"\n", "input": "", "output": "    msg = Message()\n    msg.__dict__.update(data)\n    current_app.extensions['mail'].send(msg)", "category": "Python"}, {"instruction": "def create_log_inform(self, level_name, msg, name, timestamp=None):\n        \"\"\"Create a katcp logging inform message.\n\n        Usually this will be called from inside a DeviceLogger object,\n        but it is also used by the methods in this class when errors\n        need to be reported to the client.\n\n        \"\"\"\n", "input": "", "output": "        if timestamp is None:\n            timestamp = time.time()\n\n        katcp_version = self.PROTOCOL_INFO.major\n        timestamp_msg = ('%.6f' % timestamp\n                         if katcp_version >= SEC_TS_KATCP_MAJOR\n                         else str(int(timestamp*1000)))\n        return Message.inform(\"log\", level_name, timestamp_msg, name, msg)", "category": "Python"}, {"instruction": "def validate_flags():\n  \"\"\"Validates flags are set to acceptable values.\"\"\"\n", "input": "", "output": "  if FLAGS.cloud_mlengine_model_name:\n    assert not FLAGS.server\n    assert not FLAGS.servable_name\n  else:\n    assert FLAGS.server\n    assert FLAGS.servable_name", "category": "Python"}, {"instruction": "def _inject_format_spec(self, value, format_spec):\n        \"\"\"\n        value: '{x}', format_spec: 'f' -> '{x:f}'\n        \"\"\"\n", "input": "", "output": "        t = type(value)\n        return value[:-1] + t(u':') + format_spec + t(u'}')", "category": "Python"}, {"instruction": "def get_dpi(raise_error=True):\n    \"\"\"Get screen DPI from the OS\n\n    Parameters\n    ----------\n    raise_error : bool\n        If True, raise an error if DPI could not be determined.\n\n    Returns\n    -------\n    dpi : float\n        Dots per inch of the primary screen.\n    \"\"\"\n", "input": "", "output": "    # If we are running without an X server (e.g. OSMesa), use a fixed DPI\n    if 'DISPLAY' not in os.environ:\n        return 96.\n\n    from_xdpyinfo = _get_dpi_from(\n        'xdpyinfo', r'(\\d+)x(\\d+) dots per inch',\n        lambda x_dpi, y_dpi: (x_dpi + y_dpi) / 2)\n    if from_xdpyinfo is not None:\n        return from_xdpyinfo\n\n    from_xrandr = _get_dpi_from(\n        'xrandr', r'(\\d+)x(\\d+).*?(\\d+)mm x (\\d+)mm',\n        lambda x_px, y_px, x_mm, y_mm: 25.4 * (x_px / x_mm + y_px / y_mm) / 2)\n    if from_xrandr is not None:\n        return from_xrandr\n    if raise_error:\n        raise RuntimeError('could not determine DPI')\n    else:\n        logger.warning('could not determine DPI')\n    return 96", "category": "Python"}, {"instruction": "def pending(self, start='-', stop='+', count=1000, consumer=None):\n        \"\"\"\n        List pending messages within the consumer group for this stream.\n\n        :param start: start id (or '-' for oldest pending)\n        :param stop: stop id (or '+' for newest pending)\n        :param count: limit number of messages returned\n        :param consumer: restrict message list to the given consumer\n        :returns: A list containing status for each pending message. Each\n            pending message returns [id, consumer, idle time, deliveries].\n        \"\"\"\n", "input": "", "output": "        return self.database.xpending_range(self.key, self.group, start, stop,\n                                            count, consumer)", "category": "Python"}, {"instruction": "def normalise(self, to_currency):\n        \"\"\"Normalise this balance into a single currency\n\n        Args:\n            to_currency (str): Destination currency\n\n        Returns:\n            (Balance): A new balance object containing a single Money value in the specified currency\n        \"\"\"\n", "input": "", "output": "        out = Money(currency=to_currency)\n        for money in self._money_obs:\n            out += converter.convert(money, to_currency)\n        return Balance([out])", "category": "Python"}, {"instruction": "def tcache(parser, token):\n    \"\"\"\n    This will cache the contents of a template fragment for a given amount\n    of time with support tags.\n\n    Usage::\n        {% tcache [expire_time] [fragment_name] [tags='tag1,tag2'] %}\n            .. some expensive processing ..\n        {% endtcache %}\n\n    This tag also supports varying by a list of arguments:\n        {% tcache [expire_time] [fragment_name] [var1] [var2] .. [tags=tags] %}\n            .. some expensive processing ..\n        {% endtcache %}\n\n    Each unique set of arguments will result in a unique cache entry.\n    \"\"\"\n", "input": "", "output": "    nodelist = parser.parse(('endtcache',))\n    parser.delete_first_token()\n    tokens = token.split_contents()\n    if len(tokens) < 3:\n        raise template.TemplateSyntaxError(\"'%r' tag requires at least 2 arguments.\" % tokens[0])\n    tags = None\n    if len(tokens) > 3 and 'tags=' in tokens[-1]:\n        tags = parser.compile_filter(tokens[-1][5:])\n        del tokens[-1]\n    return CacheNode(nodelist,\n        parser.compile_filter(tokens[1]),\n        tokens[2],  # fragment_name can't be a variable.\n        [parser.compile_filter(token) for token in tokens[3:]],\n        tags\n    )", "category": "Python"}, {"instruction": "def list_tubes(self):\n        \"\"\"Return a list of tubes that this beanstalk instance knows about\n\n        :rtype: list of tubes\n        \"\"\"\n", "input": "", "output": "        with self._sock_ctx() as sock:\n            self._send_message('list-tubes', sock)\n            body = self._receive_data_with_prefix(b'OK', sock)\n            tubes = yaml_load(body)\n            return tubes", "category": "Python"}, {"instruction": "def extend(func):\n    \"\"\"same as :func:`~irc3.dec.extend` but for servers\"\"\"\n", "input": "", "output": "    def callback(context, name, ob):\n        obj = context.context\n        if info.scope == 'class':\n            @functools.wraps(func)\n            def f(self, *args, **kwargs):\n                plugin = obj.get_plugin(ob)\n                return getattr(plugin, func.__name__)(*args, **kwargs)\n            setattr(obj, func.__name__, f.__get__(obj, obj.__class__))\n        else:\n            setattr(obj, func.__name__, func.__get__(obj, obj.__class__))\n    info = venusian.attach(func, callback, category='irc3d.extend')\n    return func", "category": "Python"}, {"instruction": "def export(zone, path=None):\n    '''\n    Export the configuration from memory to stable storage.\n\n    zone : string\n        name of zone\n    path : string\n        path of file to export to\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' zonecfg.export epyon\n        salt '*' zonecfg.export epyon /zones/epyon.cfg\n    '''\n", "input": "", "output": "    ret = {'status': True}\n\n    # export zone\n    res = __salt__['cmd.run_all']('zonecfg -z {zone} export{path}'.format(\n        zone=zone,\n        path=' -f {0}'.format(path) if path else '',\n    ))\n    ret['status'] = res['retcode'] == 0\n    ret['message'] = res['stdout'] if ret['status'] else res['stderr']\n    if ret['message'] == '':\n        del ret['message']\n    else:\n        ret['message'] = _clean_message(ret['message'])\n\n    return ret", "category": "Python"}, {"instruction": "def get_fields_and_fragment_names(\n    context: ValidationContext,\n    cached_fields_and_fragment_names: Dict,\n    parent_type: Optional[GraphQLNamedType],\n    selection_set: SelectionSetNode,\n) -> Tuple[NodeAndDefCollection, List[str]]:\n    \"\"\"Get fields and referenced fragment names\n\n    Given a selection set, return the collection of fields (a mapping of response name\n    to field nodes and definitions) as well as a list of fragment names referenced via\n    fragment spreads.\n    \"\"\"\n", "input": "", "output": "    cached = cached_fields_and_fragment_names.get(selection_set)\n    if not cached:\n        node_and_defs: NodeAndDefCollection = {}\n        fragment_names: Dict[str, bool] = {}\n        collect_fields_and_fragment_names(\n            context, parent_type, selection_set, node_and_defs, fragment_names\n        )\n        cached = (node_and_defs, list(fragment_names))\n        cached_fields_and_fragment_names[selection_set] = cached\n    return cached", "category": "Python"}, {"instruction": "def _start_selecting(self, event):\n        \"\"\"Comienza con el proceso de seleccion.\"\"\"\n", "input": "", "output": "        self._selecting = True\n        canvas = self._canvas\n        x = canvas.canvasx(event.x)\n        y = canvas.canvasy(event.y)\n        self._sstart = (x, y)\n        if not self._sobject:\n            self._sobject = canvas.create_rectangle(\n                self._sstart[0], self._sstart[1], x, y,\n                dash=(3,5), outline='#0000ff'\n            )\n        canvas.itemconfigure(self._sobject, state=tk.NORMAL)", "category": "Python"}, {"instruction": "def get_collection(self, id_or_uri, filter=''):\n        \"\"\"\n        Retrieves a collection of resources.\n\n        Use this function when the 'start' and 'count' parameters are not allowed in the GET call.\n        Otherwise, use get_all instead.\n\n        Optional filtering criteria may be specified.\n\n        Args:\n            id_or_uri: Can be either the resource ID or the resource URI.\n            filter (list or str): General filter/query string.\n\n        Returns:\n             Collection of the requested resource.\n        \"\"\"\n", "input": "", "output": "        if filter:\n            filter = self.__make_query_filter(filter)\n            filter = \"?\" + filter[1:]\n\n        uri = \"{uri}{filter}\".format(uri=self.build_uri(id_or_uri), filter=filter)\n        logger.debug('Get resource collection (uri = %s)' % uri)\n        response = self._connection.get(uri)\n        return self.__get_members(response)", "category": "Python"}, {"instruction": "def chain_head(self):\n        \"\"\"\n        Return the head block of the current chain.\n        \"\"\"\n", "input": "", "output": "        (vec_ptr, vec_len, vec_cap) = ffi.prepare_vec_result()\n\n        try:\n            _libexec(\n                'commit_store_get_chain_head',\n                self.pointer,\n                ctypes.byref(vec_ptr),\n                ctypes.byref(vec_len),\n                ctypes.byref(vec_cap))\n        except ValueError:\n            return None\n\n        return self.deserialize_block(\n            ffi.from_rust_vec(vec_ptr, vec_len, vec_cap))", "category": "Python"}, {"instruction": "def path_regex(self):\n        \"\"\"Return the regex for the path to the build folder.\"\"\"\n", "input": "", "output": "        regex = r'releases/%(VERSION)s/%(PLATFORM)s/%(LOCALE)s/'\n        return regex % {'LOCALE': self.locale,\n                        'PLATFORM': self.platform_regex,\n                        'VERSION': self.version}", "category": "Python"}, {"instruction": "def get_prep_value(self, value):\n        \"\"\"\n        Convert the value to a string so it can be stored in the database.\n        \"\"\"\n", "input": "", "output": "        if value == \"\":\n            return None\n        if isinstance(value, (dict, list)):\n            return self.serializer(value)\n        return super(JSONField, self).get_prep_value(value)", "category": "Python"}, {"instruction": "def data_columns_to_dict(columns_string, dict_string, delimiter=None):\n        \"\"\"\n        Turns column names in a single string into a dictionary with the key being the column name and the value\n        being the value in that column for each row\n        :param columns_string: A string of column names\n        :param dict_string: A string of values\n        :param delimiter: The delimiter to use to split the column and values\n        :return: dict\n        \"\"\"\n", "input": "", "output": "        if delimiter:\n            return {k: v for k, v in zip(columns_string.split(delimiter), dict_string.split(delimiter))}\n        else:\n            return {k: v for k, v in zip(columns_string.split(), dict_string.split())}", "category": "Python"}, {"instruction": "def remove_object(self, key):\n        '''remove an object by key from all layers'''\n", "input": "", "output": "        state = self.state\n        for layer in state.layers:\n            state.layers[layer].pop(key, None)\n        state.need_redraw = True", "category": "Python"}, {"instruction": "def _check_for_dictionary_key(self, logical_id, dictionary, keys):\n        \"\"\"\n        Checks a dictionary to make sure it has a specific key. If it does not, an\n        InvalidResourceException is thrown.\n\n        :param string logical_id: logical id of this resource\n        :param dict dictionary: the dictionary to check\n        :param list keys: list of keys that should exist in the dictionary\n        \"\"\"\n", "input": "", "output": "        for key in keys:\n            if key not in dictionary:\n                raise InvalidResourceException(logical_id, 'Resource is missing the required [{}] '\n                                                           'property.'.format(key))", "category": "Python"}, {"instruction": "def get_available_versions(self, project_name):\n        \"\"\" Query PyPI to see if package has any available versions.\n\n        Args:\n            project_name (str): The name the project on PyPI.\n\n        Returns:\n            dict: Where keys are tuples of parsed versions and values are the\n                versions returned by PyPI.\n        \"\"\"\n", "input": "", "output": "        available_versions = self.pypi_client.package_releases(project_name)\n\n        if not available_versions:\n            available_versions = self.pypi_client.package_releases(\n                project_name.capitalize()\n            )\n\n        # ``dict()`` for Python 2.6 syntax.\n        return dict(\n            (self._parse_version(version), version)\n            for version in available_versions\n        )", "category": "Python"}, {"instruction": "def get_html_tag_lang_params(index_page):\n    \"\"\"\n    Parse lang and xml:lang parameters in the ``<html>`` tag.\n\n    See\n      https://www.w3.org/International/questions/qa-html-language-declarations\n    for details.\n\n    Args:\n        index_page (str): HTML content of the page you wisht to analyze.\n\n    Returns:\n        list: List of :class:`.SourceString` objects.\n    \"\"\"\n", "input": "", "output": "    dom = dhtmlparser.parseString(index_page)\n\n    html_tag = dom.find(\"html\")\n\n    if not html_tag:\n        return []\n\n    html_tag = html_tag[0]\n\n    # parse parameters\n    lang = html_tag.params.get(\"lang\")\n    xml_lang = html_tag.params.get(\"xml:lang\")\n\n    if lang and lang == xml_lang:\n        return [SourceString(lang, source=\"<html> tag\")]\n\n    out = []\n\n    if lang:\n        out.append(SourceString(lang, source=\"<html lang=..>\"))\n\n    if xml_lang:\n        out.append(SourceString(xml_lang, source=\"<html xml:lang=..>\"))\n\n    return out", "category": "Python"}, {"instruction": "async def AddUser(self, users):\n        '''\n        users : typing.Sequence[~AddUser]\n        Returns -> typing.Sequence[~AddUserResult]\n        '''\n", "input": "", "output": "        # map input types to rpc msg\n        _params = dict()\n        msg = dict(type='UserManager',\n                   request='AddUser',\n                   version=2,\n                   params=_params)\n        _params['users'] = users\n        reply = await self.rpc(msg)\n        return reply", "category": "Python"}, {"instruction": "def find(self, other):\n        \"\"\"Return an interable of elements that overlap other in the tree.\"\"\"\n", "input": "", "output": "        iset = self._iset\n        l = binsearch_left_start(iset, other[0] - self._maxlen, 0, len(iset))\n        r = binsearch_right_end(iset, other[1], 0, len(iset))\n        iopts = iset[l:r]\n        iiter = (s for s in iopts if s[0] <= other[1] and s[1] >= other[0])\n        for o in iiter: yield o", "category": "Python"}, {"instruction": "def change_host_event_handler(self, host, event_handler_command):\n        \"\"\"Modify host event handler\n        Format of the line that triggers function call::\n\n        CHANGE_HOST_EVENT_HANDLER;<host_name>;<event_handler_command>\n\n        :param host: host to modify event handler\n        :type host: alignak.objects.host.Host\n        :param event_handler_command: event handler command line\n        :type event_handler_command:\n        :return: None\n        \"\"\"\n", "input": "", "output": "        host.modified_attributes |= DICT_MODATTR[\"MODATTR_EVENT_HANDLER_COMMAND\"].value\n        data = {\"commands\": self.commands, \"call\": event_handler_command}\n        host.change_event_handler(data)\n        self.send_an_element(host.get_update_status_brok())", "category": "Python"}, {"instruction": "def approx_equals(self, other, atol):\n        \"\"\"Test if this grid is equal to another grid.\n\n        Parameters\n        ----------\n        other :\n            Object to be tested\n        atol : float\n            Allow deviations up to this number in absolute value\n            per vector entry.\n\n        Returns\n        -------\n        equals : bool\n            ``True`` if ``other`` is a `RectGrid` instance with all\n            coordinate vectors equal (up to the given tolerance), to\n            the ones of this grid, ``False`` otherwise.\n\n        Examples\n        --------\n        >>> g1 = RectGrid([0, 1], [-1, 0, 2])\n        >>> g2 = RectGrid([-0.1, 1.1], [-1, 0.1, 2])\n        >>> g1.approx_equals(g2, atol=0)\n        False\n        >>> g1.approx_equals(g2, atol=0.15)\n        True\n        \"\"\"\n", "input": "", "output": "        if other is self:\n            return True\n\n        return (type(other) is type(self) and\n                self.ndim == other.ndim and\n                self.shape == other.shape and\n                all(np.allclose(vec_s, vec_o, atol=atol, rtol=0.0)\n                    for (vec_s, vec_o) in zip(self.coord_vectors,\n                                              other.coord_vectors)))", "category": "Python"}, {"instruction": "def oem(self, command, timeout_ms=None, info_cb=DEFAULT_MESSAGE_CALLBACK):\n    \"\"\"Executes an OEM command on the device.\n\n    Args:\n      command: The command to execute, such as 'poweroff' or 'bootconfig read'.\n      timeout_ms: Optional timeout in milliseconds to wait for a response.\n      info_cb: See Download. Messages vary based on command.\n    Returns:\n      The final response from the device.\n    \"\"\"\n", "input": "", "output": "    return self._simple_command(\n        'oem %s' % command, timeout_ms=timeout_ms, info_cb=info_cb)", "category": "Python"}, {"instruction": "def __get_password(self, description='',\n                       confirm=False, clear=False, username='glances'):\n        \"\"\"Read a password from the command line.\n\n        - if confirm = True, with confirmation\n        - if clear = True, plain (clear password)\n        \"\"\"\n", "input": "", "output": "        from glances.password import GlancesPassword\n        password = GlancesPassword(username=username)\n        return password.get_password(description, confirm, clear)", "category": "Python"}, {"instruction": "def _generate_storage_broker_lookup():\n    \"\"\"Return dictionary of available storage brokers.\"\"\"\n", "input": "", "output": "    storage_broker_lookup = dict()\n    for entrypoint in iter_entry_points(\"dtool.storage_brokers\"):\n        StorageBroker = entrypoint.load()\n        storage_broker_lookup[StorageBroker.key] = StorageBroker\n    return storage_broker_lookup", "category": "Python"}, {"instruction": "def something(TokenClass):\n    \"\"\"Do not produce empty tokens.\"\"\"\n", "input": "", "output": "    def callback(lexer, match, context):\n        text = match.group()\n        if not text:\n            return\n        yield match.start(), TokenClass, text\n        context.pos = match.end()\n    return callback", "category": "Python"}, {"instruction": "def delete_one(self, *args, **kwargs):\n        \"\"\"\n        Run the pymongo delete_one command against the default database and collection\n        and return the deleted IDs.\n        \"\"\"\n", "input": "", "output": "        result = self.collection.delete_one(*args, **kwargs)\n        return result.raw_result", "category": "Python"}, {"instruction": "def get_mzmlfile_map(self):\n        \"\"\"Returns dict of mzmlfilenames and their db ids\"\"\"\n", "input": "", "output": "        cursor = self.get_cursor()\n        cursor.execute('SELECT mzmlfile_id, mzmlfilename FROM mzmlfiles')\n        return {fn: fnid for fnid, fn in cursor.fetchall()}", "category": "Python"}, {"instruction": "def from_xdr(cls, xdr):\n        \"\"\"Create the appropriate :class:`Operation` subclass from the XDR\n        structure.\n\n        Decode an XDR base64 encoded string and create the appropriate\n        :class:`Operation` object.\n\n        :param str xdr: The XDR object to create an :class:`Operation` (or\n            subclass) instance from.\n\n        \"\"\"\n", "input": "", "output": "        xdr_decode = base64.b64decode(xdr)\n        op = Xdr.StellarXDRUnpacker(xdr_decode)\n        op = op.unpack_Operation()\n        return cls.from_xdr_object(op)", "category": "Python"}, {"instruction": "def _makePositionId(self, reference, coordinate, types=None):\n        \"\"\"\n        Note that positions should have a reference (we will enforce).\n        Only exact positions need a coordinate.\n        :param reference:\n        :param coordinate:\n        :param types:\n        :return:\n        \"\"\"\n", "input": "", "output": "\n        if reference is None:\n            LOG.error(\"Trying to make position with no reference.\")\n            return None\n\n        curie = '_:'\n        reference = re.sub(r'\\w+\\:', '', reference, 1)\n        if re.match(r'^_', reference):\n            # this is in the case if the reference is a bnode\n            reference = re.sub(r'^_', '', reference)\n        curie += reference\n        if coordinate is not None:\n            # just in case it isn't a string already\n            curie = '-'.join((curie, str(coordinate)))\n        if types is not None:\n            tstring = self._getStrandStringFromPositionTypes(types)\n            if tstring is not None:\n                curie = '-'.join((curie, tstring))\n\n        return curie", "category": "Python"}, {"instruction": "def fast_median(a):\n    \"\"\"Fast median operation for masked array using 50th-percentile\n    \"\"\"\n", "input": "", "output": "    a = checkma(a)\n    #return scoreatpercentile(a.compressed(), 50)\n    if a.count() > 0:\n        out = np.percentile(a.compressed(), 50)\n    else:\n        out = np.ma.masked\n    return out", "category": "Python"}, {"instruction": "def is_bst(root):\n    \"\"\"\n    :type root: TreeNode\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "\n    stack = []\n    pre = None\n    \n    while root or stack:\n        while root:\n            stack.append(root)\n            root = root.left\n        root = stack.pop()\n        if pre and root.val <= pre.val:\n            return False\n        pre = root\n        root = root.right\n\n    return True", "category": "Python"}, {"instruction": "def serialize_to_transport(self, encoding='utf-8', xslt_url=None):\n        \"\"\"Serialize to XML ``bytes`` with prolog.\n\n        Args:\n          encoding: str\n            Encoding to use for XML doc bytes\n          xslt_url: str\n            If specified, add a processing instruction to the XML doc that specifies the\n            download location for an XSLT stylesheet.\n\n        Returns:\n          bytes: XML holding a DataONEError based type.\n\n        \"\"\"\n", "input": "", "output": "        assert encoding in ('utf-8', 'UTF-8')\n        dataone_exception_pyxb = self.get_pyxb()\n        return d1_common.xml.serialize_for_transport(\n            dataone_exception_pyxb, xslt_url=xslt_url\n        )", "category": "Python"}, {"instruction": "def json2pattern(s):\n    \"\"\"\n    Convert JSON format to a query pattern.\n\n    Includes even mongo shell notation without quoted key names.\n    \"\"\"\n", "input": "", "output": "    # make valid JSON by wrapping field names in quotes\n    s, _ = re.subn(r'([{,])\\s*([^,{\\s\\'\"]+)\\s*:', ' \\\\1 \"\\\\2\" : ', s)\n    # handle shell values that are not valid JSON\n    s = shell2json(s)\n    # convert to 1 where possible, to get rid of things like new Date(...)\n    s, n = re.subn(r'([:,\\[])\\s*([^{}\\[\\]\"]+?)\\s*([,}\\]])', '\\\\1 1 \\\\3', s)\n    # now convert to dictionary, converting unicode to ascii\n    try:\n        doc = json.loads(s, object_hook=_decode_pattern_dict)\n        return json.dumps(doc, sort_keys=True, separators=(', ', ': '))\n    except ValueError as ex:\n        return None", "category": "Python"}, {"instruction": "def process_request(self, request):\n        \"\"\"\n        Gets the current user from the request and prepares and connects a signal receiver with the user already\n        attached to it.\n        \"\"\"\n", "input": "", "output": "        # Initialize thread local storage\n        threadlocal.auditlog = {\n            'signal_duid': (self.__class__, time.time()),\n            'remote_addr': request.META.get('REMOTE_ADDR'),\n        }\n\n        # In case of proxy, set 'original' address\n        if request.META.get('HTTP_X_FORWARDED_FOR'):\n            threadlocal.auditlog['remote_addr'] = request.META.get('HTTP_X_FORWARDED_FOR').split(',')[0]\n\n        # Connect signal for automatic logging\n        if hasattr(request, 'user') and is_authenticated(request.user):\n            set_actor = curry(self.set_actor, user=request.user, signal_duid=threadlocal.auditlog['signal_duid'])\n            pre_save.connect(set_actor, sender=LogEntry, dispatch_uid=threadlocal.auditlog['signal_duid'], weak=False)", "category": "Python"}, {"instruction": "def start(self):\n        \"\"\"\n        Start listening to changes\n        \"\"\"\n", "input": "", "output": "        self.running = True\n        self.thread = threading.Thread(target=self._main_loop)\n        self.thread.start()", "category": "Python"}, {"instruction": "def main():\n    \"\"\"Ideally we shouldn't lose the first second of events\"\"\"\n", "input": "", "output": "    with Input() as input_generator:\n        def extra_bytes_callback(string):\n            print('got extra bytes', repr(string))\n            print('type:', type(string))\n            input_generator.unget_bytes(string)\n        time.sleep(1)\n        with CursorAwareWindow(extra_bytes_callback=extra_bytes_callback) as window:\n            window.get_cursor_position()\n            for e in input_generator:\n                print(repr(e))", "category": "Python"}, {"instruction": "def _main():\n    \"\"\" CLI interface \"\"\"\n", "input": "", "output": "    import sys\n\n    if len(sys.argv) < 2:\n        _usage('Expected at least one parameter!')\n\n    sc = sys.argv[1]\n    options = sys.argv[2:]\n    if sc == 'a' or sc == 'advertise':\n        if len(options) > 5 or len(options) < 2:\n            _usage()\n\n        stype,port = options[:2]\n        advertisername = options[2] if len(options) > 2 else None\n        sname = options[3] if len(options) > 3 else ''\n        slocation = options[4] if len(options) > 4 else ''\n\n        service = Service(stype, port, sname, slocation)\n        advertiser = Advertiser([service], advertisername)\n        advertiser.run()\n    elif sc == 's' or sc == 'seek':\n        if len(options) > 4:\n            _usage()\n\n        aname = options[0] if len(options) > 0 else ''\n        stype = options[1] if len(options) > 1 else ''\n        sname = options[2] if len(options) > 2 else ''\n\n        se = Seeker(aname, stype, sname, find_callback=_print_result, error_callback=_print_error)\n        se.run()\n    else:\n        _usage('Unknown subcommand \"' + sys.argv[0] + '\"')", "category": "Python"}, {"instruction": "def urlencode(data):\n    \"\"\"A version of `urllib.urlencode` that isn't insane.\n\n    This only cares that `data` is an iterable of iterables. Each sub-iterable\n    must be of overall length 2, i.e. a name/value pair.\n\n    Unicode strings will be encoded to UTF-8. This is what Django expects; see\n    `smart_text` in the Django documentation.\n    \"\"\"\n", "input": "", "output": "    def dec(string):\n        if isinstance(string, bytes):\n            string = string.decode(\"utf-8\")\n        return quote_plus(string)\n\n    return \"&\".join(\n        \"%s=%s\" % (dec(name), dec(value))\n        for name, value in data)", "category": "Python"}, {"instruction": "def _handle_conflict(self, name):\n        \"\"\"\n        Handles requests that triggered a conflict.\n\n        Respond with a 409 \"Conflict\"\n        \"\"\"\n", "input": "", "output": "        err = HTTPConflict('Member \"%s\" already exists!' % name).exception\n        return self.request.get_response(err)", "category": "Python"}, {"instruction": "def save_state(self, fname: str):\n        \"\"\"\n        Saves the current state of iterator to a file, so that iteration can be\n        continued. Note that the data is not saved, i.e. the iterator must be\n        initialized with the same parameters as in the first call.\n\n        :param fname: File name to save the information to.\n        \"\"\"\n", "input": "", "output": "        with open(fname, \"wb\") as fp:\n            pickle.dump(self.batch_indices, fp)\n            pickle.dump(self.curr_batch_index, fp)\n            np.save(fp, [a.asnumpy() for a in self.inverse_data_permutations])\n            np.save(fp, [a.asnumpy() for a in self.data_permutations])", "category": "Python"}, {"instruction": "def delete_user(self, user_id):\n        \"\"\"Delete user specified user.\n\n        :param str user_id: the ID of the user to delete (Required)\n        :returns: void\n        \"\"\"\n", "input": "", "output": "        api = self._get_api(iam.AccountAdminApi)\n        api.delete_user(user_id)\n        return", "category": "Python"}, {"instruction": "def start_tcp_server(self, ip, port, name=None, timeout=None, protocol=None, family='ipv4'):\n        \"\"\"Starts a new TCP server to given `ip` and `port`.\n\n        Server can be given a `name`, default `timeout` and a `protocol`.\n        `family` can be either ipv4 (default) or ipv6. Notice that you have to\n        use `Accept Connection` keyword for server to receive connections.\n\n        Examples:\n        | Start TCP server | 10.10.10.2 | 53 |\n        | Start TCP server | 10.10.10.2 | 53 | Server1 |\n        | Start TCP server | 10.10.10.2 | 53 | name=Server1 | protocol=GTPV2 |\n        | Start TCP server | 10.10.10.2 | 53 | timeout=5 |\n        | Start TCP server | 0:0:0:0:0:0:0:1 | 53 | family=ipv6 |\n        \"\"\"\n", "input": "", "output": "        self._start_server(TCPServer, ip, port, name, timeout, protocol, family)", "category": "Python"}, {"instruction": "def iter_sparts(self):\n        \"\"\"\n        Generate a 4-tuple `(partname, content_type, reltype, blob)` for each\n        of the serialized parts in the package.\n        \"\"\"\n", "input": "", "output": "        for s in self._sparts:\n            yield (s.partname, s.content_type, s.reltype, s.blob)", "category": "Python"}, {"instruction": "def _add_spanning_relation(self, source, target):\n        \"\"\"add a spanning relation to this docgraph\"\"\"\n", "input": "", "output": "        self.add_edge(source, target, layers={self.ns, self.ns+':unit'},\n                      edge_type=EdgeTypes.spanning_relation)", "category": "Python"}, {"instruction": "def popleft(self, block=True, timeout=None):\n        \"\"\"Remove and return an item from the right side of the\n        GeventDeque. If no elements are present, raises an IndexError.\n\n        If optional args *block* is True and *timeout* is ``None``\n        (the default), block if necessary until an item is\n        available. If *timeout* is a positive number, it blocks at\n        most *timeout* seconds and raises the :class:`IndexError`\n        exception if no item was available within that time. Otherwise\n        (*block* is False), return an item if one is immediately\n        available, else raise the :class:`IndexError` exception\n        (*timeout* is ignored in that case).\n        \"\"\"\n", "input": "", "output": "        return self._pop(block, timeout, left=True)", "category": "Python"}, {"instruction": "def _paramf16(ins):\n    \"\"\" Pushes 32bit fixed point param into the stack\n    \"\"\"\n", "input": "", "output": "    output = _f16_oper(ins.quad[1])\n    output.append('push de')\n    output.append('push hl')\n    return output", "category": "Python"}, {"instruction": "def Relay(self, inventory):\n        \"\"\"\n        Relay the inventory to the remote client.\n\n        Args:\n            inventory (neo.Network.Inventory):\n\n        Returns:\n            bool: True if relayed successfully. False otherwise.\n        \"\"\"\n", "input": "", "output": "        if type(inventory) is MinerTransaction:\n            return False\n\n        if inventory.Hash.ToBytes() in self.KnownHashes:\n            return False\n\n        self.KnownHashes.append(inventory.Hash.ToBytes())\n\n        if type(inventory) is Block:\n            pass\n\n        elif type(inventory) is Transaction or issubclass(type(inventory), Transaction):\n            if not self.AddTransaction(inventory):\n                # if we fail to add the transaction for whatever reason, remove it from the known hashes list or we cannot retry the same transaction again\n                try:\n                    self.KnownHashes.remove(inventory.Hash.ToBytes())\n                except ValueError:\n                    # it not found\n                    pass\n                return False\n        else:\n            # consensus\n            pass\n\n        relayed = self.RelayDirectly(inventory)\n        return relayed", "category": "Python"}, {"instruction": "def _init_go2obj(self, **kws):\n        \"\"\"Initialize go2obj in small dag for source gos.\"\"\"\n", "input": "", "output": "        if 'goids' in kws and 'obodag' in kws:\n            self.godag.go_sources = kws['goids']\n            obo = kws['obodag']\n            for goid in self.godag.go_sources:\n                self.godag.go2obj[goid] = obo[goid]\n        elif 'goid2goobj' in kws:\n            goid2goobj = kws['goid2goobj']\n            self.godag.go_sources = goid2goobj.keys()\n            for goid, goobj in goid2goobj.items():\n                self.godag.go2obj[goid] = goobj\n        elif 'goea_results' in kws:\n            goea_results = kws['goea_results']\n            self.godag.go_sources = [rec.GO for rec in goea_results]\n            self.godag.go2obj = {rec.GO:rec.goterm for rec in goea_results}", "category": "Python"}, {"instruction": "def _AtLeaf(self, attr_value):\n    \"\"\"Called when at a leaf value. Should yield a value.\"\"\"\n", "input": "", "output": "    if isinstance(attr_value, collections.Mapping):\n      # If the result is a dict, return each key/value pair as a new dict.\n      for k, v in iteritems(attr_value):\n        yield {k: v}\n    else:\n      yield attr_value", "category": "Python"}, {"instruction": "def send_video(self, url, name, **videoinfo):\n        \"\"\"Send a pre-uploaded video to the room.\n\n        See http://matrix.org/docs/spec/client_server/r0.2.0.html#m-video\n        for videoinfo\n\n        Args:\n            url (str): The mxc url of the video.\n            name (str): The filename of the video.\n            videoinfo (): Extra information about the video.\n        \"\"\"\n", "input": "", "output": "        return self.client.api.send_content(self.room_id, url, name, \"m.video\",\n                                            extra_information=videoinfo)", "category": "Python"}, {"instruction": "def to_dict(self):\n        \"\"\"Return a dictionary containing Atom data.\"\"\"\n", "input": "", "output": "        data = {'aid': self.aid, 'number': self.number, 'element': self.element}\n        for coord in {'x', 'y', 'z'}:\n            if getattr(self, coord) is not None:\n                data[coord] = getattr(self, coord)\n        if self.charge is not 0:\n            data['charge'] = self.charge\n        return data", "category": "Python"}, {"instruction": "def vcf_as_df(fn):\n    \"\"\"\n    Read VCF file into pandas DataFrame.\n\n    Parameters:\n    -----------\n    fn : str\n        Path to VCF file.\n\n    Returns\n    -------\n    df : pandas.DataFrame\n        The VCF file as a data frame. Note that all header information is thrown\n        away.\n\n    \"\"\"\n", "input": "", "output": "    header_lines = 0\n    with open(fn, 'r') as f:\n        line = f.readline().strip()\n        header_lines += 1\n        while line[0] == '#':\n            line = f.readline().strip()\n            header_lines += 1\n    \n    header_lines -= 2\n    df = pd.read_table(fn, skiprows=header_lines, header=0)\n    df.columns = ['CHROM'] + list(df.columns[1:])\n    return df", "category": "Python"}, {"instruction": "def offset_gen(offset, iterable, skip_signal=None):\n  '''A generator that applies an `offset`, skipping `offset` elements from\n  `iterable`. If skip_signal is a callable, it will be called with every\n  skipped element.\n  '''\n", "input": "", "output": "  offset = int(offset)\n  assert offset >= 0, 'negative offset'\n\n  for item in iterable:\n    if offset > 0:\n      offset -= 1\n      if callable(skip_signal):\n        skip_signal(item)\n    else:\n      yield item", "category": "Python"}, {"instruction": "def emit(self, dictValues, key, value):\n        \"\"\"A new value has been found. This couple : key,value is emitted, and store in the HL7 dictionary.\n        \"\"\"\n", "input": "", "output": "        if key and value:\n            dictValues[key] = value", "category": "Python"}, {"instruction": "def to_dict(cls, acl):\n        \"\"\" transform an ACL to a dict \"\"\"\n", "input": "", "output": "        return {\n            \"perms\": acl.perms,\n            \"id\": {\n                \"scheme\": acl.id.scheme,\n                \"id\": acl.id.id\n            }\n        }", "category": "Python"}, {"instruction": "def get_principal_name(graph_object):\n        \"\"\"Attempts to resolve a principal name.\n        :param graph_object: the Azure AD Graph Object\n        :return: The resolved value or an empty string if unsuccessful.\n        \"\"\"\n", "input": "", "output": "        if hasattr(graph_object, 'user_principal_name'):\n            return graph_object.user_principal_name\n        elif hasattr(graph_object, 'service_principal_names'):\n            return graph_object.service_principal_names[0]\n        elif hasattr(graph_object, 'display_name'):\n            return graph_object.display_name\n        return ''", "category": "Python"}, {"instruction": "def encode(self, s):\n    \"\"\"Transform a human-readable string into a sequence of int ids.\n\n    The ids should be in the range [num_reserved_ids, vocab_size). Ids [0,\n    num_reserved_ids) are reserved.\n\n    EOS is not appended.\n\n    Args:\n      s: human-readable string to be converted.\n\n    Returns:\n      ids: list of integers\n    \"\"\"\n", "input": "", "output": "    return [int(w) + self._num_reserved_ids for w in s.split()]", "category": "Python"}, {"instruction": "def nla_ok(nla, remaining):\n    \"\"\"Check if the attribute header and payload can be accessed safely.\n\n    https://github.com/thom311/libnl/blob/libnl3_2_25/lib/attr.c#L148\n\n    Verifies that the header and payload do not exceed the number of bytes left in the attribute stream. This function\n    must be called before access the attribute header or payload when iterating over the attribute stream using\n    nla_next().\n\n    Positional arguments:\n    nla -- attribute of any kind (nlattr class instance).\n    remaining -- number of bytes remaining in attribute stream (c_int).\n\n    Returns:\n    True if the attribute can be accessed safely, False otherwise.\n    \"\"\"\n", "input": "", "output": "    return remaining.value >= nla.SIZEOF and nla.SIZEOF <= nla.nla_len <= remaining.value", "category": "Python"}, {"instruction": "def simulate(self, n):\n        \"\"\"\n        Evolve multiple sites during one tree traversal\n        \"\"\"\n", "input": "", "output": "        self.tree._tree.seed_node.states = self.ancestral_states(n)\n        categories = np.random.randint(self.ncat, size=n).astype(np.intc)\n\n        for node in self.tree.preorder(skip_seed=True):\n            node.states = self.evolve_states(node.parent_node.states, categories, node.pmats)\n            if node.is_leaf():\n                self.sequences[node.taxon.label] = node.states\n        return self.sequences_to_string()", "category": "Python"}, {"instruction": "def update(self, id, data):\n        \"\"\"\n        Update comment :param:`id` with values from :param:`data` and return\n        updated comment.\n        \"\"\"\n", "input": "", "output": "        self.db.execute([\n            'UPDATE comments SET',\n            ','.join(key + '=' + '?' for key in data),\n            'WHERE id=?;'],\n            list(data.values()) + [id])\n\n        return self.get(id)", "category": "Python"}, {"instruction": "def timeout(seconds=10, default_output='default_output'):\n    \"\"\" function wrapper that limits the amount of time it has to run\n    optional args:\n        seconds - how long it has until the function times out\n        default_output - what will be returned instead of an error\n    \"\"\"\n", "input": "", "output": "    def decorator(func):\n        def _handle_timeout(signum, frame):\n            ", "category": "Python"}, {"instruction": "def detect_proc():\n    \"\"\"Detect /proc filesystem style.\n\n    This checks the /proc/{pid} directory for possible formats. Returns one of\n    the followings as str:\n\n    * `stat`: Linux-style, i.e. ``/proc/{pid}/stat``.\n    * `status`: BSD-style, i.e. ``/proc/{pid}/status``.\n    \"\"\"\n", "input": "", "output": "    pid = os.getpid()\n    for name in ('stat', 'status'):\n        if os.path.exists(os.path.join('/proc', str(pid), name)):\n            return name\n    raise ProcFormatError('unsupported proc format')", "category": "Python"}, {"instruction": "def wait_until_online(function, ip, port):\n    \"\"\"\n    Uses Wait_For_State to wait for the server to come online, then runs the given function.\n    \"\"\"\n", "input": "", "output": "    awaiting_service = Wait_For_State(lambda: not is_port_available(ip, port), function)\n    awaiting_service.start()\n    return awaiting_service", "category": "Python"}, {"instruction": "def _field_to_json(field, row_value):\n    \"\"\"Convert a field into JSON-serializable values.\n\n    Args:\n        field ( \\\n            :class:`~google.cloud.bigquery.schema.SchemaField`, \\\n        ):\n            The SchemaField to use for type conversion and field name.\n\n        row_value (Union[ \\\n            Sequence[list], \\\n            any, \\\n        ]):\n            Row data to be inserted. If the SchemaField's mode is\n            REPEATED, assume this is a list. If not, the type\n            is inferred from the SchemaField's field_type.\n\n    Returns:\n        any:\n            A JSON-serializable object.\n    \"\"\"\n", "input": "", "output": "    if row_value is None:\n        return None\n\n    if field.mode == \"REPEATED\":\n        return _repeated_field_to_json(field, row_value)\n\n    if field.field_type == \"RECORD\":\n        return _record_field_to_json(field.fields, row_value)\n\n    return _scalar_field_to_json(field, row_value)", "category": "Python"}, {"instruction": "def cell(self, row_idx, col_idx):\n        \"\"\"\n        Return |_Cell| instance correponding to table cell at *row_idx*,\n        *col_idx* intersection, where (0, 0) is the top, left-most cell.\n        \"\"\"\n", "input": "", "output": "        cell_idx = col_idx + (row_idx * self._column_count)\n        return self._cells[cell_idx]", "category": "Python"}, {"instruction": "def saved_xids(self):\n        \"\"\"Return previously saved xids.\"\"\"\n", "input": "", "output": "        if self._saved_xids is None:\n            self._saved_xids = []\n            if self.debug:\n                fpfn = os.path.join(self.tcex.args.tc_temp_path, 'xids-saved')\n                if os.path.isfile(fpfn) and os.access(fpfn, os.R_OK):\n                    with open(fpfn) as fh:\n                        self._saved_xids = fh.read().splitlines()\n        return self._saved_xids", "category": "Python"}, {"instruction": "def time_to_hhmmssmmm(time_value, decimal_separator=\".\"):\n    \"\"\"\n    Format the given time value into a ``HH:MM:SS.mmm`` string.\n\n    Examples: ::\n\n        12        => 00:00:12.000\n        12.345    => 00:00:12.345\n        12.345432 => 00:00:12.345\n        12.345678 => 00:00:12.346\n        83        => 00:01:23.000\n        83.456    => 00:01:23.456\n        83.456789 => 00:01:23.456\n        3600      => 01:00:00.000\n        3612.345  => 01:00:12.345\n\n    :param float time_value: a time value, in seconds\n    :param string decimal_separator: the decimal separator, default ``.``\n    :rtype: string\n    \"\"\"\n", "input": "", "output": "    if time_value is None:\n        time_value = 0\n    tmp = time_value\n    hours = int(math.floor(tmp / 3600))\n    tmp -= (hours * 3600)\n    minutes = int(math.floor(tmp / 60))\n    tmp -= minutes * 60\n    seconds = int(math.floor(tmp))\n    tmp -= seconds\n    milliseconds = int(math.floor(tmp * 1000))\n    return \"%02d:%02d:%02d%s%03d\" % (\n        hours,\n        minutes,\n        seconds,\n        decimal_separator,\n        milliseconds\n    )", "category": "Python"}, {"instruction": "def rate_limiting(self):\n        \"\"\"\n        First value is requests remaining, second value is request limit.\n\n        :type: (int, int)\n        \"\"\"\n", "input": "", "output": "        remaining, limit = self.__requester.rate_limiting\n        if limit < 0:\n            self.get_rate_limit()\n        return self.__requester.rate_limiting", "category": "Python"}, {"instruction": "def generate_and_merge_schemas(samples):\n    \"\"\"Iterates through the given samples, generating schemas\n    and merging them, returning the resulting merged schema.\n\n    \"\"\"\n", "input": "", "output": "    merged = generate_schema_for_sample(next(iter(samples)))\n\n    for sample in samples:\n        merged = merge_schema(merged, generate_schema_for_sample(sample))\n\n    return merged", "category": "Python"}, {"instruction": "def patched_model():\n    \"\"\"Context Manager that safely patches django.db.Model.__reduce__().\"\"\"\n", "input": "", "output": "\n    patched = ('__reduce__', '__getstate__', '__setstate__')\n    originals = {}\n    for patch in patched:\n        try:\n            originals[patch] = getattr(models.Model, patch)\n        except:\n            pass\n\n    try:\n        # Patch various parts of the model\n        models.Model.__reduce__ = _reduce\n        try:\n            del models.Model.__getstate__\n        except:\n            pass\n        try:\n            del models.Model.__setstate__\n        except:  # pragma: no cover\n            pass\n\n        yield\n\n    finally:\n        # Restore the model\n        for patch in patched:\n            try:\n                setattr(models.Model, patch, originals[patch])\n            except KeyError:\n                try:\n                    delattr(models.Model, patch)\n                except AttributeError:\n                    pass", "category": "Python"}, {"instruction": "def clean_old_jobs():\n    '''\n    Clean out the old jobs from all returners (if you have it)\n    '''\n", "input": "", "output": "    for returner_ in __opts__[CONFIG_KEY]:\n        fstr = '{0}.clean_old_jobs'.format(returner_)\n        if fstr in _mminion().returners:\n            _mminion().returners[fstr]()", "category": "Python"}, {"instruction": "def update_json(self, fields=None):\n        \"\"\"Update the current entity.\n\n        Call :meth:`update_raw`. Check the response status code, decode JSON\n        and return the decoded JSON as a dict.\n\n        :param fields: See :meth:`update`.\n        :return: A dict consisting of the decoded JSON in the server's\n            response.\n        :raises: ``requests.exceptions.HTTPError`` if the response has an HTTP\n            4XX or 5XX status code.\n        :raises: ``ValueError`` If the response JSON can not be decoded.\n\n        \"\"\"\n", "input": "", "output": "        response = self.update_raw(fields)\n        response.raise_for_status()\n        return response.json()", "category": "Python"}, {"instruction": "def do_copy(self, subcmd, opts, *args):\n        \"\"\"Duplicate something in working copy or repository, remembering history.\n\n        usage:\n            copy SRC DST\n        \n        SRC and DST can each be either a working copy (WC) path or URL:\n          WC  -> WC:   copy and schedule for addition (with history)\n          WC  -> URL:  immediately commit a copy of WC to URL\n          URL -> WC:   check out URL into WC, schedule for addition\n          URL -> URL:  complete server-side copy;  used to branch & tag\n\n        ${cmd_option_list}\n        \"\"\"\n", "input": "", "output": "        print \"'svn %s' opts: %s\" % (subcmd, opts)\n        print \"'svn %s' args: %s\" % (subcmd, args)", "category": "Python"}, {"instruction": "def read(self, **keys):\n        \"\"\"\n        Read the data from disk and return as a numpy array\n        \"\"\"\n", "input": "", "output": "\n        if self.is_scalar:\n            data = self.fitshdu.read_column(self.columns, **keys)\n        else:\n            c = keys.get('columns', None)\n            if c is None:\n                keys['columns'] = self.columns\n            data = self.fitshdu.read(**keys)\n\n        return data", "category": "Python"}, {"instruction": "def us2mc(string):\n    \"\"\"Transform an underscore_case string to a mixedCase string\"\"\"\n", "input": "", "output": "    return re.sub(r'_([a-z])', lambda m: (m.group(1).upper()), string)", "category": "Python"}, {"instruction": "def setattr(self, req, ino, attr, to_set, fi):\n        \"\"\"Set file attributes\n\n        Valid replies:\n            reply_attr\n            reply_err\n        \"\"\"\n", "input": "", "output": "        self.reply_err(req, errno.EROFS)", "category": "Python"}, {"instruction": "def do_pp(self, arg):\n        \"\"\"pp expression\n        Pretty-print the value of the expression.\n        \"\"\"\n", "input": "", "output": "        obj = self._getval(arg)\n        try:\n            repr(obj)\n        except Exception:\n            self.message(bdb.safe_repr(obj))\n        else:\n            self.message(pprint.pformat(obj))", "category": "Python"}, {"instruction": "def _get_sorting_key_values(self, array1, array2):\n        \"\"\"return the sorting key values as a series\"\"\"\n", "input": "", "output": "\n        concat_arrays = numpy.concatenate([array1, array2])\n        unique_values = numpy.unique(concat_arrays)\n\n        return numpy.sort(unique_values)", "category": "Python"}, {"instruction": "def build_attachment1():\n    \"\"\"Build attachment mock. Make sure your content is base64 encoded before passing into attachment.content.\n    Another example: https://github.com/sendgrid/sendgrid-python/blob/master/use_cases/attachment.md\"\"\"\n", "input": "", "output": "    attachment = Attachment()\n    attachment.content = (\"TG9yZW0gaXBzdW0gZG9sb3Igc2l0IGFtZXQsIGNvbnNl\"\n                          \"Y3RldHVyIGFkaXBpc2NpbmcgZWxpdC4gQ3JhcyBwdW12\")\n    attachment.type = \"application/pdf\"\n    attachment.filename = \"balance_001.pdf\"\n    attachment.disposition = \"attachment\"\n    attachment.content_id = \"Balance Sheet\"\n    return attachment", "category": "Python"}, {"instruction": "def _invert(self, pos):\n        \"\"\"Flip bit at pos 1<->0.\"\"\"\n", "input": "", "output": "        assert 0 <= pos < self.len\n        self._datastore.invertbit(pos)", "category": "Python"}, {"instruction": "def list_container_objects(self, container, prefix=None, delimiter=None):\n        \"\"\"List container objects\n\n        :param container: container name (Container is equivalent to\n                          Bucket term in Amazon).\n        :param prefix: prefix query\n        :param delimiter: string to delimit the queries on\n        \"\"\"\n", "input": "", "output": "        LOG.debug('list_container_objects() with %s is success.', self.driver)\n        return self.driver.list_container_objects(container, prefix, delimiter)", "category": "Python"}, {"instruction": "def parse_objective_coefficient(entry):\n    \"\"\"Return objective value for reaction entry.\n\n    Detect objectives that are specified using the non-standardized\n    kinetic law parameters which are used by many pre-FBC SBML models. The\n    objective coefficient is returned for the given reaction, or None if\n    undefined.\n\n    Args:\n        entry: :class:`SBMLReactionEntry`.\n    \"\"\"\n", "input": "", "output": "    for parameter in entry.kinetic_law_reaction_parameters:\n        pid, name, value, units = parameter\n        if (pid == 'OBJECTIVE_COEFFICIENT' or\n                name == 'OBJECTIVE_COEFFICIENT'):\n            return value\n\n    return None", "category": "Python"}, {"instruction": "def note(\n        cls,\n        template,\n        default_params={},\n        stack_depth=0,\n        log_context=None,\n        **more_params\n    ):\n        \"\"\"\n        :param template: *string* human readable string with placeholders for parameters\n        :param default_params: *dict* parameters to fill in template\n        :param stack_depth:  *int* how many calls you want popped off the stack to report the *true* caller\n        :param log_context: *dict* extra key:value pairs for your convenience\n        :param more_params: *any more parameters (which will overwrite default_params)\n        :return:\n        \"\"\"\n", "input": "", "output": "        timestamp = datetime.utcnow()\n        if not is_text(template):\n            Log.error(\"Log.note was expecting a unicode template\")\n\n        Log._annotate(\n            LogItem(\n                context=exceptions.NOTE,\n                format=template,\n                template=template,\n                params=dict(default_params, **more_params)\n            ),\n            timestamp,\n            stack_depth+1\n        )", "category": "Python"}, {"instruction": "def engine(self):\n        \"\"\"\n        Return an engine instance, creating it if it doesn't exist.\n\n        Recreate the engine connection if it wasn't originally created\n        by the current process.\n        \"\"\"\n", "input": "", "output": "        pid = os.getpid()\n        conn = SQLAlchemyTarget._engine_dict.get(self.connection_string)\n        if not conn or conn.pid != pid:\n            # create and reset connection\n            engine = sqlalchemy.create_engine(\n                self.connection_string,\n                connect_args=self.connect_args,\n                echo=self.echo\n            )\n            SQLAlchemyTarget._engine_dict[self.connection_string] = self.Connection(engine, pid)\n        return SQLAlchemyTarget._engine_dict[self.connection_string].engine", "category": "Python"}, {"instruction": "def read_one(self, sequence):\n        \"\"\"\n        Reads one item from the Ringbuffer. If the sequence is one beyond the current tail, this call blocks until an\n        item is added. Currently it isn't possible to control how long this call is going to block.\n\n        :param sequence: (long), the sequence of the item to read.\n        :return: (object), the read item.\n        \"\"\"\n", "input": "", "output": "        check_not_negative(sequence, \"sequence can't be smaller than 0\")\n        return self._encode_invoke(ringbuffer_read_one_codec, sequence=sequence)", "category": "Python"}, {"instruction": "def component(iface):\n    '''\n    Marks the decorated class as a component implementing the given ``iface``\n\n    :param iface: the interface to implement\n    :type iface: :func:`interface`\n    '''\n", "input": "", "output": "\n    def decorator(cls):\n        if not cls:\n            return None\n\n        # Run custom verificator if any\n        if hasattr(cls, '__verify__'):\n            if not cls.__verify__():\n                return None\n\n        if not hasattr(iface, 'implementations'):\n            log.error('%s is not an @interface', iface)\n\n        log.debug(\n            'Registering [%s] (implementation of [%s])' % (\n                get_fqdn(cls),\n                get_fqdn(iface)\n            )\n        )\n        iface.implementations.append(cls)\n\n        def get(cls, context):\n            return context.get_component(cls)\n        cls.get = get.__get__(cls)\n\n        return cls\n\n    return decorator", "category": "Python"}, {"instruction": "def artist_undelete(self, artist_id):\n        \"\"\"Lets you undelete artist (Requires login) (UNTESTED) (Only Builder+).\n\n        Parameters:\n            artist_id (int):\n        \"\"\"\n", "input": "", "output": "        return self._get('artists/{0}/undelete.json'.format(artist_id),\n                         method='POST', auth=True)", "category": "Python"}, {"instruction": "def similar_email(anon, obj, field, val):\n    \"\"\"\n    Generate a random email address using the same domain.\n    \"\"\"\n", "input": "", "output": "    return val if 'betterworks.com' in val else '@'.join([anon.faker.user_name(field=field), val.split('@')[-1]])", "category": "Python"}, {"instruction": "def ActivateInstanceDisks(r, instance, ignore_size=False):\n    \"\"\"\n    Activates an instance's disks.\n\n    @type instance: string\n    @param instance: Instance name\n    @type ignore_size: bool\n    @param ignore_size: Whether to ignore recorded size\n    @return: job id\n    \"\"\"\n", "input": "", "output": "\n    return r.request(\"put\", \"/2/instances/%s/activate-disks\" % instance,\n                     query={\"ignore_size\": ignore_size})", "category": "Python"}, {"instruction": "def OnInsertCols(self, event):\n        \"\"\"Inserts the maximum of 1 and the number of selected columns\"\"\"\n", "input": "", "output": "\n        bbox = self.grid.selection.get_bbox()\n\n        if bbox is None or bbox[1][1] is None:\n            # Insert rows at cursor\n            ins_point = self.grid.actions.cursor[1] - 1\n            no_cols = 1\n        else:\n            # Insert at right edge of bounding box\n            ins_point = bbox[0][1] - 1\n            no_cols = self._get_no_rowscols(bbox)[1]\n\n        with undo.group(_(\"Insert columns\")):\n            self.grid.actions.insert_cols(ins_point, no_cols)\n\n        self.grid.GetTable().ResetView()\n\n        # Update the default sized cell sizes\n        self.grid.actions.zoom()\n\n        event.Skip()", "category": "Python"}, {"instruction": "def lcm( *a ):\n  \"\"\"Least common multiple.\n\n  Usage: lcm( [ 3, 4, 5 ] )\n  or:    lcm( 3, 4, 5 )\n  \"\"\"\n", "input": "", "output": "\n  if len( a ) > 1: return reduce( lcm2, a )\n  if hasattr( a[0], \"__iter__\" ): return reduce( lcm2, a[0] )\n  return a[0]", "category": "Python"}, {"instruction": "def http_handler(name, logname, host, url, method=\"GET\"):\n    \"\"\"\n    A Bark logging handler logging output to an HTTP server, using\n    either GET or POST semantics.\n\n    Similar to logging.handlers.HTTPHandler.\n    \"\"\"\n", "input": "", "output": "\n    return wrap_log_handler(logging.handlers.HTTPHandler(\n        host, url, method=method))", "category": "Python"}, {"instruction": "def _DictToListOfStrings(self, data_dict):\n    \"\"\"Converts a dictionary into a list of strings.\n\n    Args:\n      data_dict (dict[str, object]): dictionary to convert.\n\n    Returns:\n      list[str]: list of strings.\n    \"\"\"\n", "input": "", "output": "    ret_list = []\n    for key, value in iter(data_dict.items()):\n      if key in ('body', 'datetime', 'type', 'room', 'rooms', 'id'):\n        continue\n      ret_list.append('{0:s} = {1!s}'.format(key, value))\n\n    return ret_list", "category": "Python"}, {"instruction": "def print_pack(document_loader,  # type: Loader\n               processobj,       # type: CommentedMap\n               uri,              # type: Text\n               metadata          # type: Dict[Text, Any]\n              ):  # type (...) -> Text\n    \"\"\"Return a CWL serialization of the CWL document in JSON.\"\"\"\n", "input": "", "output": "    packed = pack(document_loader, processobj, uri, metadata)\n    if len(packed[\"$graph\"]) > 1:\n        return json_dumps(packed, indent=4)\n    return json_dumps(packed[\"$graph\"][0], indent=4)", "category": "Python"}, {"instruction": "def visit_If(self, node):\n        \"\"\" Handle iterate variable across branches\n\n        >>> import gast as ast\n        >>> from pythran import passmanager, backend\n        >>> node = ast.parse('''\n        ... def foo(a):\n        ...     if a > 1: b = 1\n        ...     else: b = 3''')\n\n        >>> pm = passmanager.PassManager(\"test\")\n        >>> res = pm.gather(RangeValues, node)\n        >>> res['b']\n        Interval(low=1, high=3)\n        \"\"\"\n", "input": "", "output": "        self.visit(node.test)\n        old_range = self.result\n\n        self.result = old_range.copy()\n        for stmt in node.body:\n            self.visit(stmt)\n        body_range = self.result\n\n        self.result = old_range.copy()\n        for stmt in node.orelse:\n            self.visit(stmt)\n        orelse_range = self.result\n\n        self.result = body_range\n        for k, v in orelse_range.items():\n            if k in self.result:\n                self.result[k] = self.result[k].union(v)\n            else:\n                self.result[k] = v", "category": "Python"}, {"instruction": "def set_mode_broodlord_params(\n            self, zerg_count=None,\n            vassal_overload_sos_interval=None, vassal_queue_items_sos=None):\n        \"\"\"This mode is a way for a vassal to ask for reinforcements to the Emperor.\n\n        Reinforcements are new vassals spawned on demand generally bound on the same socket.\n\n        .. warning:: If you are looking for a way to dynamically adapt the number\n            of workers of an instance, check the Cheaper subsystem - adaptive process spawning mode.\n\n            *Broodlord mode is for spawning totally new instances.*\n\n        :param int zerg_count: Maximum number of zergs to spawn.\n\n        :param int vassal_overload_sos_interval: Ask emperor for reinforcement when overloaded.\n            Accepts the number of seconds to wait between asking for a new reinforcements.\n\n        :param int vassal_queue_items_sos: Ask emperor for sos if listen queue (backlog) has more\n            items than the value specified\n\n        \"\"\"\n", "input": "", "output": "        self._set('emperor-broodlord', zerg_count)\n        self._set('vassal-sos', vassal_overload_sos_interval)\n        self._set('vassal-sos-backlog', vassal_queue_items_sos)\n\n        return self._section", "category": "Python"}, {"instruction": "def get_entry(key):\n    \"\"\"\n    Get a configuration entry\n\n    :param key: key name\n    :returns: mixed value\n    :raises KeyError: if key has not been found\n    :raises TypeError: if key is not str\n    \"\"\"\n", "input": "", "output": "    if type(key) != str:\n        raise TypeError(\"key must be str\")\n    if key not in _config:\n        raise KeyError(\"Nonexistent entry '{key}'\".format(key=key))\n    return _config[key]", "category": "Python"}, {"instruction": "def translate_docs(self, ds, **kwargs):\n        \"\"\"\n        Translate a set of solr results\n        \"\"\"\n", "input": "", "output": "        for d in ds:\n            self.map_doc(d, {}, self.invert_subject_object)\n\n        return [self.translate_doc(d, **kwargs) for d in ds]", "category": "Python"}, {"instruction": "def get_portchannel_info_by_intf_input_interface_name(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_portchannel_info_by_intf = ET.Element(\"get_portchannel_info_by_intf\")\n        config = get_portchannel_info_by_intf\n        input = ET.SubElement(get_portchannel_info_by_intf, \"input\")\n        interface_name = ET.SubElement(input, \"interface-name\")\n        interface_name.text = kwargs.pop('interface_name')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def maximum(self, node):\n        \"\"\"\n        find the max node when node regard as a root node   \n        :param node: \n        :return: max node \n        \"\"\"\n", "input": "", "output": "        temp_node = node\n        while temp_node.right is not None:\n            temp_node = temp_node.right\n        return temp_node", "category": "Python"}, {"instruction": "def look_for_hdl_urls(citation_elements):\n    \"\"\"Looks for handle identifiers that have already been identified as urls\n\n       When finding an hdl, creates a new HDL element.\n       @param citation_elements: (list) elements to process\n    \"\"\"\n", "input": "", "output": "    for el in citation_elements:\n        if el['type'] == 'URL':\n            match = re_hdl.match(el['url_string'])\n            if match:\n                el['type'] = 'HDL'\n                el['hdl_id'] = match.group('hdl_id')\n                del el['url_desc']\n                del el['url_string']", "category": "Python"}, {"instruction": "def reconstruct(self, X=None):\n        \"\"\"Reconstruct representation.\"\"\"\n", "input": "", "output": "\n        if X is None:\n            X = self.X\n        return self.D.dot(self.X)", "category": "Python"}, {"instruction": "def _whoami():\n    # type: () -> Tuple[Text,Text]\n    \"\"\"Return the current operating system account as (username, fullname).\"\"\"\n", "input": "", "output": "    username = getuser()\n    try:\n        if onWindows():\n            get_user_name = ctypes.windll.secur32.GetUserNameExW  # type: ignore\n            size = ctypes.pointer(ctypes.c_ulong(0))\n            get_user_name(3, None, size)\n\n            name_buffer = ctypes.create_unicode_buffer(size.contents.value)\n            get_user_name(3, name_buffer, size)\n            fullname = str(name_buffer.value)\n        else:\n            fullname = pwd.getpwuid(os.getuid())[4].split(',')[0]\n    except (KeyError, IndexError):\n        fullname = username\n\n    return (username, fullname)", "category": "Python"}, {"instruction": "def intermediary_to_schema(tables, relationships, output):\n    \"\"\" Transforms and save the intermediary representation to the file chosen. \"\"\"\n", "input": "", "output": "    dot_file = _intermediary_to_dot(tables, relationships)\n    graph = AGraph()\n    graph = graph.from_string(dot_file)\n    extension = output.split('.')[-1]\n    graph.draw(path=output, prog='dot', format=extension)", "category": "Python"}, {"instruction": "def export(string, template=None, **extra):\n    \"\"\"\n    Decorator for registering view functions and adding\n    templates to it.\n    \"\"\"\n", "input": "", "output": "\n    def wrapped(f):\n        endpoint = (f.__module__ + \".\" + f.__name__)[16:]\n        if template is not None:\n            old_f = f\n\n            def f(**kwargs):\n                rv = old_f(**kwargs)\n                if not isinstance(rv, Response):\n                    rv = TemplateResponse(template, **(rv or {}))\n                return rv\n\n            f.__name__ = old_f.__name__\n            f.__doc__ = old_f.__doc__\n        exported_views[endpoint] = (f, string, extra)\n        return f\n\n    return wrapped", "category": "Python"}, {"instruction": "def status(name, sig=None):\n    '''\n    Return the status for a service via dummy, returns a bool\n    whether the service is running.\n\n    .. versionadded:: 2016.11.3\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' service.status <service name>\n    '''\n", "input": "", "output": "\n    proxy_fn = 'dummy.service_status'\n    resp = __proxy__[proxy_fn](name)\n    if resp['comment'] == 'stopped':\n        return False\n    if resp['comment'] == 'running':\n        return True", "category": "Python"}, {"instruction": "def export_theta(ckout, data):\n    \"\"\"Provide updated set of data with export information for TheTA2 input.\n    \"\"\"\n", "input": "", "output": "    cns_file = chromhacks.bed_to_standardonly(ckout[\"cns\"], data, headers=\"chromosome\")\n    cnr_file = chromhacks.bed_to_standardonly(ckout[\"cnr\"], data, headers=\"chromosome\")\n    out_file = \"%s-theta.input\" % utils.splitext_plus(cns_file)[0]\n    if not utils.file_exists(out_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            cmd = [_get_cmd(), \"export\", \"theta\", cns_file, cnr_file, \"-o\", tx_out_file]\n            do.run(_prep_cmd(cmd, tx_out_file), \"Export CNVkit calls as inputs for TheTA2\")\n    ckout[\"theta_input\"] = out_file\n    return ckout", "category": "Python"}, {"instruction": "def regenerate_recovery_code(self, user_id):\n        \"\"\"Removes the current recovery token, generates and returns a new one\n\n        Args:\n            user_id (str):  The user_id of the user identity.\n\n        See: https://auth0.com/docs/api/management/v2#!/Users/post_recovery_code_regeneration\n        \"\"\"\n", "input": "", "output": "        url = self._url('{}/recovery-code-regeneration'.format(user_id))\n        return self.client.post(url)", "category": "Python"}, {"instruction": "def arp():\n    '''\n    Return the arp table from the minion\n\n    .. versionchanged:: 2015.8.0\n        Added support for SunOS\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' network.arp\n    '''\n", "input": "", "output": "    ret = {}\n    out = __salt__['cmd.run']('arp -an')\n    for line in out.splitlines():\n        comps = line.split()\n        if len(comps) < 4:\n            continue\n        if __grains__['kernel'] == 'SunOS':\n            if ':' not in comps[-1]:\n                continue\n            ret[comps[-1]] = comps[1]\n        elif __grains__['kernel'] == 'OpenBSD':\n            if comps[0] == 'Host' or comps[1] == '(incomplete)':\n                continue\n            ret[comps[1]] = comps[0]\n        elif __grains__['kernel'] == 'AIX':\n            if comps[0] in ('bucket', 'There'):\n                continue\n            ret[comps[3]] = comps[1].strip('(').strip(')')\n        else:\n            ret[comps[3]] = comps[1].strip('(').strip(')')\n\n    return ret", "category": "Python"}, {"instruction": "def print_node(node, detailed=False):\n    \"\"\"Pretty prints the given node\"\"\"\n", "input": "", "output": "    nodename = node['name']\n    print(colors.yellow(\"\\n\" + nodename))\n    # Roles\n    if detailed:\n        for role in get_roles_in_node(node):\n            print_role(_get_role(role), detailed=False)\n    else:\n        print('  Roles: {0}'.format(\", \".join(get_roles_in_node(node))))\n    # Recipes\n    if detailed:\n        for recipe in get_recipes_in_node(node):\n            print \"  Recipe:\", recipe\n            print \"    attributes: {0}\".format(node.get(recipe, \"\"))\n    else:\n        print('  Recipes: {0}'.format(\", \".join(get_recipes_in_node(node))))\n    # Node attributes\n    print \"  Node attributes:\"\n    for attribute in node.keys():\n        if attribute == \"run_list\" or attribute == \"name\":\n            continue\n        print \"    {0}: {1}\".format(attribute, node[attribute])", "category": "Python"}, {"instruction": "def get_usage(self, start=None, end=None):\n        \"\"\"Access historical account usage data.\"\"\"\n", "input": "", "output": "        method = 'GET'\n        endpoint = '/rest/v1/users/{}/usage'.format(self.client.sauce_username)\n        data = {}\n        if start:\n            data['start'] = start\n        if end:\n            data['end'] = end\n        if data:\n            endpoint = '?'.join([endpoint, urlencode(data)])\n        return self.client.request(method, endpoint)", "category": "Python"}, {"instruction": "def node_has_namespace(self, node: BaseEntity, namespace: str) -> bool:\n        \"\"\"Check if the node have the given namespace.\n\n        This also should look in the equivalent nodes.\n        \"\"\"\n", "input": "", "output": "        return any(\n            self._node_has_namespace_helper(n, namespace)\n            for n in self.iter_equivalent_nodes(node)\n        )", "category": "Python"}, {"instruction": "def _convert_asset_timestamp_fields(dict_):\n    \"\"\"\n    Takes in a dict of Asset init args and converts dates to pd.Timestamps\n    \"\"\"\n", "input": "", "output": "    for key in _asset_timestamp_fields & viewkeys(dict_):\n        value = pd.Timestamp(dict_[key], tz='UTC')\n        dict_[key] = None if isnull(value) else value\n    return dict_", "category": "Python"}, {"instruction": "def find_package_data(packages):\n    \"\"\"\n    For a list of packages, find the package_data\n\n    This function scans the subdirectories of a package and considers all\n    non-submodule subdirectories as resources, including them in\n    the package_data\n\n    Returns a dictionary suitable for setup(package_data=<result>)\n    \"\"\"\n", "input": "", "output": "    package_data = {}\n    for package in packages:\n        package_data[package] = []\n        for subdir in find_subdirectories(package):\n            if '.'.join((package, subdir)) in packages: # skip submodules\n                logging.debug(\"skipping submodule %s/%s\" % (package, subdir))\n                continue\n            if skip_tests and (subdir == 'tests'): # skip tests\n                logging.debug(\"skipping tests %s/%s\" % (package, subdir))\n                continue\n            package_data[package] += subdir_findall(package_to_path(package), subdir)\n    return package_data", "category": "Python"}, {"instruction": "def _fix_integrity_error(f):\n    \"\"\"Ensure raising of IntegrityError on unique constraint violations.\n\n    In earlier versions of hdbcli it doesn't raise the hdbcli.dbapi.IntegrityError\n    exception for unique constraint violations. To support also older versions\n    of hdbcli this decorator inspects the raised exception and will rewrite the\n    exception based on HANA's error code.\n    \"\"\"\n", "input": "", "output": "\n    @wraps(f)\n    def wrapper(dialect, *args, **kwargs):\n        try:\n            return f(dialect, *args, **kwargs)\n        except dialect.dbapi.Error as exc:\n            if exc.errorcode == 301 and not isinstance(exc, dialect.dbapi.IntegrityError):\n                raise dialect.dbapi.IntegrityError(exc)\n            raise\n    return wrapper", "category": "Python"}, {"instruction": "def subgroup(self, t, i):\n        \"\"\"Handle parenthesis.\"\"\"\n", "input": "", "output": "\n        current = []\n\n        # (?flags)\n        flags = self.get_flags(i)\n        if flags:\n            self.flags(flags[2:-1])\n            return [flags]\n\n        # (?#comment)\n        comments = self.get_comments(i)\n        if comments:\n            return [comments]\n\n        verbose = self.verbose\n        unicode_flag = self.unicode\n\n        # (?flags:pattern)\n        flags = self.get_flags(i, True)\n        if flags:  # pragma: no cover\n            t = flags\n            self.flags(flags[2:-1], scoped=True)\n\n        current = []\n        try:\n            while t != ')':\n                if not current:\n                    current.append(t)\n                else:\n                    current.extend(self.normal(t, i))\n\n                t = next(i)\n        except StopIteration:\n            pass\n\n        # Restore flags after group\n        self.verbose = verbose\n        self.unicode = unicode_flag\n\n        if t == \")\":\n            current.append(t)\n        return current", "category": "Python"}, {"instruction": "def findPrefixes(self, uri, match='eq'):\n        \"\"\"\n        Find all prefixes that has been mapped to a namespace URI.\n        The local mapping is searched, then it walks up the tree until\n        it reaches the top collecting all matches.\n        @param uri: A namespace URI.\n        @type uri: basestring\n        @param match: A matching function L{Element.matcher}.\n        @type match: basestring\n        @return: A list of mapped prefixes.\n        @rtype: [basestring,...]\n        \"\"\"\n", "input": "", "output": "        result = []\n        for item in self.nsprefixes.items():\n            if self.matcher[match](item[1], uri):\n                prefix = item[0]\n                result.append(prefix)\n        for item in self.specialprefixes.items():\n            if self.matcher[match](item[1], uri):\n                prefix = item[0]\n                result.append(prefix)\n        if self.parent is not None:\n            result += self.parent.findPrefixes(uri, match)\n        return result", "category": "Python"}, {"instruction": "def log_stop(self, start):\n        \"\"\"log a summary line on how the request went\"\"\"\n", "input": "", "output": "        if not logger.isEnabledFor(logging.INFO): return\n\n        stop = time.time()\n        get_elapsed = lambda start, stop, multiplier, rnd: round(abs(stop - start) * float(multiplier), rnd)\n        elapsed = get_elapsed(start, stop, 1000.00, 1)\n        total = \"%0.1f ms\" % (elapsed)\n        logger.info(\"RESPONSE {} {} in {}\".format(self.response.code, self.response.status, total))", "category": "Python"}, {"instruction": "def from_text(text):\n    \"\"\"Convert text into a DNS rdata class value.\n    @param text: the text\n    @type text: string\n    @rtype: int\n    @raises dns.rdataclass.UnknownRdataClass: the class is unknown\n    @raises ValueError: the rdata class value is not >= 0 and <= 65535\n    \"\"\"\n", "input": "", "output": "\n    value = _by_text.get(text.upper())\n    if value is None:\n        match = _unknown_class_pattern.match(text)\n        if match == None:\n            raise UnknownRdataclass\n        value = int(match.group(1))\n        if value < 0 or value > 65535:\n            raise ValueError(\"class must be between >= 0 and <= 65535\")\n    return value", "category": "Python"}, {"instruction": "def takes_kwargs(obj) -> bool:\n    \"\"\"\n    Checks whether a provided object takes in any positional arguments.\n    Similar to takes_arg, we do this for both the __init__ function of\n    the class or a function / method\n    Otherwise, we raise an error\n    \"\"\"\n", "input": "", "output": "    if inspect.isclass(obj):\n        signature = inspect.signature(obj.__init__)\n    elif inspect.ismethod(obj) or inspect.isfunction(obj):\n        signature = inspect.signature(obj)\n    else:\n        raise ConfigurationError(f\"object {obj} is not callable\")\n    return bool(any([p.kind == inspect.Parameter.VAR_KEYWORD  # type: ignore\n                     for p in signature.parameters.values()]))", "category": "Python"}, {"instruction": "def compare_variants_label_plot(data):\n    \"\"\" Return HTML for the Compare variants plot\"\"\"\n", "input": "", "output": "    keys = OrderedDict()\n\n    keys['total_called_variants_known'] = {'name': 'Known Variants'}\n    keys['total_called_variants_novel'] = {'name': 'Novel Variants'}\n\n    pconfig = {\n        'id': 'picard_variantCallingMetrics_variant_label',\n        'title': 'Picard: Variants Called',\n        'ylab': 'Counts of Variants',\n     }\n\n    return bargraph.plot(data, cats=keys, pconfig=pconfig)", "category": "Python"}, {"instruction": "def autocommit(data_access):\n  \"\"\"Make statements autocommit.\n\n  :param data_access: a DataAccess instance\n  \"\"\"\n", "input": "", "output": "  if not data_access.autocommit:\n    data_access.commit()\n  old_autocommit = data_access.autocommit\n  data_access.autocommit = True\n  try:\n    yield data_access\n  finally:\n    data_access.autocommit = old_autocommit", "category": "Python"}, {"instruction": "def load_config(self):\n        \"\"\" Load your configuration settings from a file \"\"\"\n", "input": "", "output": "        conf_file = os.path.join(self._conf_dir, \"dql.json\")\n        if not os.path.exists(conf_file):\n            return {}\n        with open(conf_file, \"r\") as ifile:\n            return json.load(ifile)", "category": "Python"}, {"instruction": "def _filter_unscheduled_routers(self, plugin, context, routers):\n        \"\"\"Filter from list of routers the ones that are not scheduled.\n\n           Only for release < pike.\n        \"\"\"\n", "input": "", "output": "        if NEUTRON_VERSION.version[0] <= NEUTRON_NEWTON_VERSION.version[0]:\n            context, plugin = plugin, context\n        unscheduled_routers = []\n        for router in routers:\n            if (router[routertype.TYPE_ATTR] !=\n                    plugin.get_namespace_router_type_id(context)):\n                # ignore non-namespace routers\n                continue\n            l3_agents = plugin.get_l3_agents_hosting_routers(\n                context, [router['id']])\n            if l3_agents:\n                LOG.debug('Router %(router_id)s has already been '\n                          'hosted by L3 agent %(agent_id)s',\n                          {'router_id': router['id'],\n                           'agent_id': l3_agents[0]['id']})\n            else:\n                unscheduled_routers.append(router)\n        return unscheduled_routers", "category": "Python"}, {"instruction": "def log_player_plays_road_builder(self, player, location1, location2):\n        \"\"\"\n        :param player: catan.game.Player\n        :param location1: string, see hexgrid.location()\n        :param location2: string, see hexgrid.location()\n        \"\"\"\n", "input": "", "output": "        self._logln('{0} plays road builder, builds at {1} and {2}'.format(\n            player.color,\n            location1,\n            location2\n        ))", "category": "Python"}, {"instruction": "def find_missing_dependencies(self, requirement):\n        \"\"\"\n        Find missing dependencies of a Python package.\n\n        :param requirement: A :class:`.Requirement` object.\n        :returns: A list of strings with system package names.\n        \"\"\"\n", "input": "", "output": "        known_dependencies = self.find_known_dependencies(requirement)\n        if known_dependencies:\n            installed_packages = self.find_installed_packages()\n            logger.debug(\"Checking for missing dependencies of %s ..\", requirement.name)\n            missing_dependencies = sorted(set(known_dependencies).difference(installed_packages))\n            if missing_dependencies:\n                logger.debug(\"Found %s: %s\",\n                             pluralize(len(missing_dependencies), \"missing dependency\", \"missing dependencies\"),\n                             concatenate(missing_dependencies))\n            else:\n                logger.info(\"All known dependencies are already installed.\")\n            return missing_dependencies", "category": "Python"}, {"instruction": "def unescape_LDAP(ldap_string):\n    # type: (str) -> str\n    # pylint: disable=C0103\n    \"\"\"\n    Unespaces an LDAP string\n\n    :param ldap_string: The string to unescape\n    :return: The unprotected string\n    \"\"\"\n", "input": "", "output": "    if ldap_string is None:\n        return None\n\n    if ESCAPE_CHARACTER not in ldap_string:\n        # No need to loop\n        return ldap_string\n\n    escaped = False\n    result = \"\"\n\n    for character in ldap_string:\n        if not escaped and character == ESCAPE_CHARACTER:\n            # Escape character found\n            escaped = True\n        else:\n            # Copy the character\n            escaped = False\n            result += character\n\n    return result", "category": "Python"}, {"instruction": "def update_campaign_update(self, campaign_id, campaign, **kwargs):  # noqa: E501\n        \"\"\"Modify a campaign  # noqa: E501\n\n        Modify an update campaign.  # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass asynchronous=True\n        >>> thread = api.update_campaign_update(campaign_id, campaign, asynchronous=True)\n        >>> result = thread.get()\n\n        :param asynchronous bool\n        :param str campaign_id: (required)\n        :param UpdateCampaignPutRequest campaign: Update campaign (required)\n        :return: UpdateCampaign\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('asynchronous'):\n            return self.update_campaign_update_with_http_info(campaign_id, campaign, **kwargs)  # noqa: E501\n        else:\n            (data) = self.update_campaign_update_with_http_info(campaign_id, campaign, **kwargs)  # noqa: E501\n            return data", "category": "Python"}, {"instruction": "def last_archive(self):\n        '''\n        Get the last available archive\n        :return:\n        '''\n", "input": "", "output": "        archives = {}\n        for archive in self.archives():\n            archives[int(archive.split('.')[0].split('-')[-1])] = archive\n\n        return archives and archives[max(archives)] or None", "category": "Python"}, {"instruction": "def spherical_variogram_model(m, d):\r\n    \"\"\"Spherical model, m is [psill, range, nugget]\"\"\"\n", "input": "", "output": "    psill = float(m[0])\r\n    range_ = float(m[1])\r\n    nugget = float(m[2])\r\n    return np.piecewise(d, [d <= range_, d > range_],\r\n                        [lambda x: psill * ((3.*x)/(2.*range_) - (x**3.)/(2.*range_**3.)) + nugget, psill + nugget])", "category": "Python"}, {"instruction": "def write_file(writer, filename):\n  \"\"\"Write all of lines from file using the writer.\"\"\"\n", "input": "", "output": "  for line in txt_line_iterator(filename):\n    writer.write(line)\n    writer.write(\"\\n\")", "category": "Python"}, {"instruction": "def following_key(key):\n    \"\"\"\n    Returns the key immediately following the input key - based on the Java implementation found in\n    org.apache.accumulo.core.data.Key, function followingKey(PartialKey part)\n    :param key: the key to be followed\n    :return: a key that immediately follows the input key\n    \"\"\"\n", "input": "", "output": "    if key.timestamp is not None:\n        key.timestamp -= 1\n    elif key.colVisibility is not None:\n        key.colVisibility = following_array(key.colVisibility)\n    elif key.colQualifier is not None:\n        key.colQualifier = following_array(key.colQualifier)\n    elif key.colFamily is not None:\n        key.colFamily = following_array(key.colFamily)\n    elif key.row is not None:\n        key.row = following_array(key.row)\n    return key", "category": "Python"}, {"instruction": "def clear_measurements(self):\n        \"\"\"Remove all measurements from self.measurements. Reset the\n        measurement counter. All ID are invalidated.\n        \"\"\"\n", "input": "", "output": "        keys = list(self.measurements.keys())\n        for key in keys:\n            del(self.measurements[key])\n        self.meas_counter = -1", "category": "Python"}, {"instruction": "def get_wigner_seitz_cell(self) -> List[List[np.ndarray]]:\n        \"\"\"\n        Returns the Wigner-Seitz cell for the given lattice.\n\n        Returns:\n            A list of list of coordinates.\n            Each element in the list is a \"facet\" of the boundary of the\n            Wigner Seitz cell. For instance, a list of four coordinates will\n            represent a square facet.\n        \"\"\"\n", "input": "", "output": "        vec1 = self._matrix[0]\n        vec2 = self._matrix[1]\n        vec3 = self._matrix[2]\n\n        list_k_points = []\n        for i, j, k in itertools.product([-1, 0, 1], [-1, 0, 1], [-1, 0, 1]):\n            list_k_points.append(i * vec1 + j * vec2 + k * vec3)\n        from scipy.spatial import Voronoi\n\n        tess = Voronoi(list_k_points)\n        to_return = []\n        for r in tess.ridge_dict:\n            if r[0] == 13 or r[1] == 13:\n                to_return.append([tess.vertices[i] for i in tess.ridge_dict[r]])\n\n        return to_return", "category": "Python"}, {"instruction": "def get_all_build_configs_by_labels(self, label_selectors):\n        \"\"\"\n        Returns all builds matching a given set of label selectors. It is up to the\n        calling function to filter the results.\n        \"\"\"\n", "input": "", "output": "        labels = ['%s=%s' % (field, value) for field, value in label_selectors]\n        labels = ','.join(labels)\n        url = self._build_url(\"buildconfigs/\", labelSelector=labels)\n        return self._get(url).json()['items']", "category": "Python"}, {"instruction": "def encode(self, file):\n        \"\"\"This function returns a (tag,state) tuple that is calculated for\n        the given file.  the state will be encrypted with `self.key`\n\n        :param file: the file to encode\n        \"\"\"\n", "input": "", "output": "        tag = Tag()\n        tag.sigma = list()\n\n        state = State(Random.new().read(32), Random.new().read(32))\n\n        f = KeyedPRF(state.f_key, self.prime)\n        alpha = KeyedPRF(state.alpha_key, self.prime)\n\n        done = False\n        chunk_id = 0\n\n        while (not done):\n            sigma = f.eval(chunk_id)\n            for j in range(0, self.sectors):\n                buffer = file.read(self.sectorsize)\n\n                if (len(buffer) > 0):\n                    sigma += alpha.eval(j) * number.bytes_to_long(buffer)\n\n                if (len(buffer) != self.sectorsize):\n                    done = True\n                    break\n            sigma %= self.prime\n            tag.sigma.append(sigma)\n            chunk_id += 1\n\n        state.chunks = chunk_id\n        state.encrypt(self.key)\n\n        return (tag, state)", "category": "Python"}, {"instruction": "def scale_to_zero_one(x):\n    \"\"\"Take some 1d data and scale it so that min matches 0 and max 1.\n    \"\"\"\n", "input": "", "output": "    xscaled = x - np.min(x)\n    xscaled /= np.max(xscaled)\n    return xscaled", "category": "Python"}, {"instruction": "def use_comparative_asset_composition_view(self):\n        \"\"\"Pass through to provider AssetCompositionSession.use_comparative_asset_composition_view\"\"\"\n", "input": "", "output": "        self._object_views['asset_composition'] = COMPARATIVE\n        # self._get_provider_session('asset_composition_session') # To make sure the session is tracked\n        for session in self._get_provider_sessions():\n            try:\n                session.use_comparative_asset_composition_view()\n            except AttributeError:\n                pass", "category": "Python"}, {"instruction": "def cublasDspmv(handle, uplo, n, alpha, AP, x, incx, beta, y, incy):\n    \"\"\"\n    Matrix-vector product for real symmetric-packed matrix.\n\n    \"\"\"\n", "input": "", "output": "\n    status = _libcublas.cublasDspmv_v2(handle,\n                                       _CUBLAS_FILL_MODE[uplo], \n                                       n,\n                                       ctypes.byref(ctypes.c_double(alpha)),\n                                       ctypes.byref(ctypes.c_double(AP)),\n                                       int(x),\n                                       incx,\n                                       ctypes.byref(ctypes.c_double(beta)),\n                                       int(y),\n                                       incy)\n    cublasCheckStatus(status)", "category": "Python"}, {"instruction": "def safe_repr(obj, clip=None):\n    \"\"\"\n    Convert object to string representation, yielding the same result a `repr`\n    but catches all exceptions and returns 'N/A' instead of raising the\n    exception. Strings may be truncated by providing `clip`.\n\n    >>> safe_repr(42)\n    '42'\n    >>> safe_repr('Clipped text', clip=8)\n    'Clip..xt'\n    >>> safe_repr([1,2,3,4], clip=8)\n    '[1,2..4]'\n    \"\"\"\n", "input": "", "output": "    try:\n        s = repr(obj)\n        if not clip or len(s) <= clip:\n            return s\n        else:\n            return s[:clip-4]+'..'+s[-2:]\n    except:\n        return 'N/A'", "category": "Python"}, {"instruction": "def tp_graph(dataframe, image_name, dir='./'):\n    \"\"\"Throughput graph\n\n    :param pandas.DataFrame dataframe: dataframe containing all data\n    :param str dir: the output directory\n    :return: None\n    \"\"\"\n", "input": "", "output": "    fig = pygal.TimeLine(x_title='Elapsed Time In Test (secs)',\n                         x_label_rotation=25,\n                         y_title='Transactions Per Second (count)',\n                         human_readable=True,\n                         js=('scripts/pygal-tooltip.min.js',))\n    fig.add('Transactions per second', [(get_local_time(index), row['count'])\n                                        for index, row in dataframe.iterrows()])\n    fig.render_to_file(filename=os.path.join(dir, image_name))", "category": "Python"}, {"instruction": "def _ge_from_le(self, other):\n    \"\"\"Return a >= b.  Computed by @total_ordering from (not a <= b) or (a == b).\"\"\"\n", "input": "", "output": "    op_result = self.__le__(other)\n    if op_result is NotImplemented:\n        return NotImplemented\n    return not op_result or self == other", "category": "Python"}, {"instruction": "def sound_pressure_low(self):\n        \"\"\"\n        A measurement of the measured sound pressure level, as a\n        percent. Uses A-weighting, which focuses on levels up to 55 dB.\n        \"\"\"\n", "input": "", "output": "        self._ensure_mode(self.MODE_DBA)\n        return self.value(0) * self._scale('DBA')", "category": "Python"}, {"instruction": "def dcc_get(self, mask, host, port, filepath, filesize=None):\n        \"\"\"DCC GET a file from mask. filepath must be an absolute path with an\n        existing directory. filesize is the expected file size.\"\"\"\n", "input": "", "output": "        return self.dcc.create(\n            'get', mask, filepath=filepath, filesize=filesize,\n            host=host, port=port).ready", "category": "Python"}, {"instruction": "def make_symmetric(dict):\n    \"\"\"Makes the given dictionary symmetric. Values are assumed to be unique.\"\"\"\n", "input": "", "output": "    for key, value in list(dict.items()):\n        dict[value] = key\n    return dict", "category": "Python"}, {"instruction": "def equal_to_be(self, be_record):\n        # type: (PathTableRecord) -> bool\n        '''\n        A method to compare a little-endian path table record to its\n        big-endian counterpart.  This is used to ensure that the ISO is sane.\n\n        Parameters:\n         be_record - The big-endian object to compare with the little-endian\n                     object.\n        Returns:\n         True if this record is equal to the big-endian record passed in,\n         False otherwise.\n        '''\n", "input": "", "output": "        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('This Path Table Record is not yet initialized')\n\n        if be_record.len_di != self.len_di or \\\n           be_record.xattr_length != self.xattr_length or \\\n           utils.swab_32bit(be_record.extent_location) != self.extent_location or \\\n           utils.swab_16bit(be_record.parent_directory_num) != self.parent_directory_num or \\\n           be_record.directory_identifier != self.directory_identifier:\n            return False\n        return True", "category": "Python"}, {"instruction": "def simple_detect(agent):\n    \"\"\"\n    -> (os, browser) # tuple of strings\n    \"\"\"\n", "input": "", "output": "    result = detect(agent)\n    os_list = []\n    if 'flavor' in result:\n        os_list.append(result['flavor']['name'])\n    if 'dist' in result:\n        os_list.append(result['dist']['name'])\n    if 'os' in result:\n        os_list.append(result['os']['name'])\n\n    os = os_list and \" \".join(os_list) or \"Unknown OS\"\n    os_version = os_list and (result.get('flavor') and result['flavor'].get('version')) or \\\n        (result.get('dist') and result['dist'].get('version')) or (result.get('os') and result['os'].get('version')) or \"\"\n    browser = 'browser' in result and result['browser'].get('name') or 'Unknown Browser'\n    browser_version = 'browser' in result and result['browser'].get('version') or \"\"\n    if browser_version:\n        browser = \" \".join((browser, browser_version))\n    if os_version:\n        os = \" \".join((os, os_version))\n    return os, browser", "category": "Python"}, {"instruction": "def to_struct(self):\n        \"\"\"\n        Initialize properties of the appropriate struct class from this model class.\n        \"\"\"\n", "input": "", "output": "        structobj = self.struct_type()\n        for k in structobj.attributes():\n            self.log.info(\"Setting attribute %s to %r\" % (k, getattr(self, k)))\n            setattr(structobj, k, getattr(self, k))\n        return structobj", "category": "Python"}, {"instruction": "def smooth(x, y, weights):\r\n    '''\r\n    in case the NLF cannot be described by \r\n    a square root function\r\n    commit bounded polynomial interpolation\r\n    '''\n", "input": "", "output": "    # Spline hard to smooth properly, therefore solfed with\r\n    # bounded polynomal interpolation\r\n    # ext=3: no extrapolation, but boundary value\r\n#     return UnivariateSpline(x, y, w=weights,\r\n#                             s=len(y)*weights.max()*100, ext=3)\r\n\r\n#     return np.poly1d(np.polyfit(x,y,w=weights,deg=2))\r\n    p = np.polyfit(x, y, w=weights, deg=2)\r\n    if np.any(np.isnan(p)):\r\n        # couldn't even do polynomial fit\r\n        # as last option: assume constant noise\r\n        my = np.average(y, weights=weights)\r\n        return lambda x: my\r\n    return lambda xint: np.poly1d(p)(np.clip(xint, x[0], x[-1]))", "category": "Python"}, {"instruction": "def rover_lat_accel(VFR_HUD, SERVO_OUTPUT_RAW):\n    '''return lateral acceleration in m/s/s'''\n", "input": "", "output": "    speed = VFR_HUD.groundspeed\n    yaw_rate = rover_yaw_rate(VFR_HUD, SERVO_OUTPUT_RAW)\n    accel = radians(yaw_rate) * speed\n    return accel", "category": "Python"}, {"instruction": "def lines(text):\n    \"\"\"\n    Generator function to yield lines (delimited with ``'\\n'``) stored in\n    ``text``. This is useful when a regular expression should only match on a\n    per line basis in a memory efficient way.\n    \"\"\"\n", "input": "", "output": "    assert text is not None\n    assert '\\r' not in text\n    previous_newline_index = 0\n    newline_index = text.find('\\n')\n    while newline_index != -1:\n        yield text[previous_newline_index:newline_index]\n        previous_newline_index = newline_index + 1\n        newline_index = text.find('\\n', previous_newline_index)\n    last_line = text[previous_newline_index:]\n    if last_line != '':\n        yield last_line", "category": "Python"}, {"instruction": "def _catchall_enabled(app):\n    \"\"\"Check the bottle app for catchall.\"\"\"\n", "input": "", "output": "    while hasattr(app, 'app'):\n        if isinstance(app, bottle.Bottle):\n            break\n        app = app.app\n    if hasattr(app, 'catchall'):\n        return app.catchall\n    else:\n        return bottle.default_app().catchall", "category": "Python"}, {"instruction": "def base_path(self):\n        \"\"\"\n        Calculate the APIs base path\n        \"\"\"\n", "input": "", "output": "        path = UrlPath()\n\n        # Walk up the API to find the base object\n        parent = self.parent\n        while parent:\n            path_prefix = getattr(parent, 'path_prefix', NoPath)\n            path = path_prefix + path\n            parent = getattr(parent, 'parent', None)\n\n        return path", "category": "Python"}, {"instruction": "def replace_u_end_month(month):\n    \"\"\"Find the latest legitimate month.\"\"\"\n", "input": "", "output": "    month = month.lstrip('-')\n    if month == 'uu' or month == '1u':\n        return '12'\n    if month == 'u0':\n        return '10'\n    if month == '0u':\n        return '09'\n    if month[1] in ['1', '2']:\n        # 'u1' or 'u2'\n        return month.replace('u', '1')\n    # Otherwise it should match r'u[3-9]'.\n    return month.replace('u', '0')", "category": "Python"}, {"instruction": "def _process_launch_error(self, data):\n        \"\"\"\n        Processes a received LAUNCH_ERROR message and notifies listeners.\n        \"\"\"\n", "input": "", "output": "        launch_failure = self._parse_launch_error(data)\n        self.launch_failure = launch_failure\n\n        if self.app_to_launch:\n            self.app_to_launch = None\n            self.app_launch_event.set()\n\n        self.logger.debug(\"Launch status: %s\", launch_failure)\n\n        for listener in self._launch_error_listeners:\n            try:\n                listener.new_launch_error(launch_failure)\n            except Exception:  # pylint: disable=broad-except\n                self.logger.exception(\n                    \"Exception thrown when calling launch error listener\")", "category": "Python"}, {"instruction": "def Execute(self, action, *args, **kw):\n        \"\"\"Directly execute an action through an Environment\n        \"\"\"\n", "input": "", "output": "        action = self.Action(action, *args, **kw)\n        result = action([], [], self)\n        if isinstance(result, SCons.Errors.BuildError):\n            errstr = result.errstr\n            if result.filename:\n                errstr = result.filename + ': ' + errstr\n            sys.stderr.write(\"scons: *** %s\\n\" % errstr)\n            return result.status\n        else:\n            return result", "category": "Python"}, {"instruction": "def log_player_ends_turn(self, player):\n        \"\"\"\n        :param player: catan.game.Player\n        \"\"\"\n", "input": "", "output": "        seconds_delta = (datetime.datetime.now() - self._latest_timestamp).total_seconds()\n        self._logln('{0} ends turn after {1}s'.format(player.color, round(seconds_delta)))\n        self._latest_timestamp = datetime.datetime.now()", "category": "Python"}, {"instruction": "def _check_model(obj, models=None):\n    \"\"\"Checks object if it's a peewee model and unique.\"\"\"\n", "input": "", "output": "    return isinstance(obj, type) and issubclass(obj, pw.Model) and hasattr(obj, '_meta')", "category": "Python"}, {"instruction": "def walk(self, topdown=True, followlinks=True):\n        \"\"\"\n        walk: walk the filesystem (just like os.walk).\n        Use like:\n\n        path = URI('/some/dir')\n        for root, dirs, files in path.walk():\n            do_something()\n\n        root will be an URI object.\n        \"\"\"\n", "input": "", "output": "        return self.connection.walk(self, topdown=topdown, followlinks=followlinks)", "category": "Python"}, {"instruction": "def print_devices(self, devices):\n        \"\"\"Print method for devices.\n        \"\"\"\n", "input": "", "output": "        for device in devices:\n            print('ID: {} OS: {} IP: {} State: {} ({}) Tags: {}'\n                  .format(device.id,\n                          device.operating_system.slug,\n                          self.get_public_ip(device.ip_addresses),\n                          device.state,\n                          'spot' if device.spot_instance else 'on-demand',\n                          device.tags))", "category": "Python"}, {"instruction": "def anon():\n    '''Check for candidates to anonymization'''\n", "input": "", "output": "    header(anon.__doc__)\n    filename = 'urls_to_check.csv'\n\n    candidates = Advice.objects(__raw__={\n        '$or': [\n            {'subject': {\n                '$regex': '(Monsieur|Madame|Docteur|Mademoiselle)\\s+[^X\\s\\.]{3}',\n                '$options': 'imx',\n            }},\n            {'content': {\n                '$regex': '(Monsieur|Madame|Docteur|Mademoiselle)\\s+[^X\\s\\.]{3}',\n                '$options': 'imx',\n            }}\n        ]\n    })\n\n    with open(filename, 'wb') as csvfile:\n        writer = csv.writer(csvfile)\n        # Generate header\n        writer.writerow(csv.ANON_HEADER)\n\n        for idx, advice in enumerate(candidates, 1):\n            writer.writerow(csv.to_anon_row(advice))\n            echo('.' if idx % 50 else white(idx), nl=False)\n        echo(white(idx) if idx % 50 else '')\n\n    success('Total: {0} candidates', len(candidates))", "category": "Python"}, {"instruction": "def uniform(self, a: float, b: float, precision: int = 15) -> float:\n        \"\"\"Get a random number in the range [a, b) or [a, b] depending on rounding.\n\n        :param a: Minimum value.\n        :param b: Maximum value.\n        :param precision: Round a number to a given\n            precision in decimal digits, default is 15.\n        \"\"\"\n", "input": "", "output": "        return round(a + (b - a) * self.random(), precision)", "category": "Python"}, {"instruction": "def configure_swagger(graph):\n    \"\"\"\n    Build a singleton endpoint that provides swagger definitions for all operations.\n\n    \"\"\"\n", "input": "", "output": "    ns = Namespace(\n        subject=graph.config.swagger_convention.name,\n        version=graph.config.swagger_convention.version,\n    )\n    convention = SwaggerConvention(graph)\n    convention.configure(ns, discover=tuple())\n    return ns.subject", "category": "Python"}, {"instruction": "def compute_cardinalities(self):\n        \"\"\" This will count the number of distinct values for each dimension in\n        the dataset and add that count to the model so that it can be used as a\n        hint by UI components. \"\"\"\n", "input": "", "output": "        for dimension in self.model.dimensions:\n            result = self.members(dimension.ref, page_size=0)\n            dimension.spec['cardinality'] = result.get('total_member_count')", "category": "Python"}, {"instruction": "def box(side=1):\n    \"\"\"Draw a box\"\"\"\n", "input": "", "output": "    half_side = side / 2\n    _state[\"ctx\"].rectangle(-half_side, -half_side, side, side)\n    _state[\"ctx\"].fill()", "category": "Python"}, {"instruction": "def ssh_sa_ssh_client_mac(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        ssh_sa = ET.SubElement(config, \"ssh-sa\", xmlns=\"urn:brocade.com:mgmt:brocade-sec-services\")\n        ssh = ET.SubElement(ssh_sa, \"ssh\")\n        client = ET.SubElement(ssh, \"client\")\n        mac = ET.SubElement(client, \"mac\")\n        mac.text = kwargs.pop('mac')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)", "category": "Python"}, {"instruction": "def TakeIf(self: Iterable, f):\n    \"\"\"\n    [\n        {\n            'self': [1, 2, 3],\n            'f': lambda e: e%2,\n            'assert': lambda ret: list(ret)  == [1, 3]\n         }\n    ]\n    \"\"\"\n", "input": "", "output": "    if is_to_destruct(f):\n        f = destruct_func(f)\n\n    return (e for e in self if f(e))", "category": "Python"}, {"instruction": "def from_stmt(stmt, engine, **kwargs):\n    \"\"\"\n    Execute a query in form of texture clause, return the result in form of\n\n    :class:`PrettyTable`.\n\n    :type stmt: TextClause\n    :param stmt:\n\n    :type engine: Engine\n    :param engine:\n\n    :rtype: PrettyTable\n    \"\"\"\n", "input": "", "output": "    result_proxy = engine.execute(stmt, **kwargs)\n    return from_db_cursor(result_proxy.cursor)", "category": "Python"}, {"instruction": "def parameters_changed(self):\n        \"\"\"\n        Parameters have now changed\n        \"\"\"\n", "input": "", "output": "        # Get the model matrices from the kernel\n        (F,L,Qc,H,Pinf,dF,dQc,dPinf) = self.kern.sde()\n\n        # Use the Kalman filter to evaluate the likelihood\n        self._log_marginal_likelihood = self.kf_likelihood(F,L,Qc,H,self.sigma2,Pinf,self.X.T,self.Y.T)\n        gradients  = self.compute_gradients()\n        self.sigma2.gradient_full[:] = gradients[-1]\n        self.kern.gradient_full[:] = gradients[:-1]", "category": "Python"}, {"instruction": "def NormalizeScopes(scope_spec):\n    \"\"\"Normalize scope_spec to a set of strings.\"\"\"\n", "input": "", "output": "    if isinstance(scope_spec, six.string_types):\n        return set(scope_spec.split(' '))\n    elif isinstance(scope_spec, collections.Iterable):\n        return set(scope_spec)\n    raise exceptions.TypecheckError(\n        'NormalizeScopes expected string or iterable, found %s' % (\n            type(scope_spec),))", "category": "Python"}, {"instruction": "def is_shape(df, shape):\n    \"\"\"\n    Asserts that the DataFrame is of a known shape.\n\n    Parameters\n    ==========\n\n    df : DataFrame\n    shape : tuple\n      (n_rows, n_columns). Use None or -1 if you don't care\n      about a dimension.\n\n    Returns\n    =======\n    df : DataFrame\n    \"\"\"\n", "input": "", "output": "    try:\n        check = np.all(np.equal(df.shape, shape) | (np.equal(shape, [-1, -1]) |\n                                                    np.equal(shape, [None, None])))\n        assert check\n    except AssertionError as e:\n        msg = (\"Expected shape: {}\\n\"\n               \"\\t\\tActual shape:   {}\".format(shape, df.shape))\n        e.args = (msg,)\n        raise\n    return df", "category": "Python"}, {"instruction": "def _tostring(value):\n        '''Convert value to XML compatible string'''\n", "input": "", "output": "        if value is True:\n            value = 'true'\n        elif value is False:\n            value = 'false'\n        elif value is None:\n            value = ''\n        return unicode(value)", "category": "Python"}, {"instruction": "def getInterpretation(self):\n        \"\"\"\n        Get the value of the previously POSTed Tropo action.\n        \"\"\"\n", "input": "", "output": "        actions = self._actions\n\n        if (type (actions) is list):\n            dict = actions[0]\n        else:\n            dict = actions\n        return dict['interpretation']", "category": "Python"}, {"instruction": "def reload(self):\n        \"\"\"\n        Refreshes the resource with the data from the server.\n        \"\"\"\n", "input": "", "output": "        try:\n            if hasattr(self, 'href'):\n                data = self._api.get(self.href, append_base=False).json()\n                resource = self.__class__(api=self._api, **data)\n            elif hasattr(self, 'id') and hasattr(self, '_URL') and \\\n                    'get' in self._URL:\n                data = self._api.get(\n                    self._URL['get'].format(id=self.id)).json()\n                resource = self.__class__(api=self._api, **data)\n            else:\n                raise SbgError('Resource can not be refreshed!')\n\n            query = {'id': self.id} if hasattr(self, 'id') else {}\n            extra = {'resource': self.__class__.__name__, 'query': query}\n            logger.info('Reloading {} resource.'.format(self), extra=extra)\n\n        except Exception:\n            raise SbgError('Resource can not be refreshed!')\n\n        self._data = resource._data\n        self._dirty = resource._dirty\n        self._old = copy.deepcopy(self._data.data)\n        return self", "category": "Python"}, {"instruction": "def quat_rotate(rotation, vector):\n    \"\"\"\n    Rotate a vector according to a quaternion. Equivalent to the C++ method tf::quatRotate\n    :param rotation: the rotation\n    :param vector: the vector to rotate\n    :return: the rotated vector\n    \"\"\"\n", "input": "", "output": "\n    def quat_mult_point(q, w):\n        return (q[3] * w[0] + q[1] * w[2] - q[2] * w[1],\n                q[3] * w[1] + q[2] * w[0] - q[0] * w[2],\n                q[3] * w[2] + q[0] * w[1] - q[1] * w[0],\n                -q[0] * w[0] - q[1] * w[1] - q[2] * w[2])\n\n    q = quat_mult_point(rotation, vector)\n    q = tf.transformations.quaternion_multiply(\n        q, tf.transformations.quaternion_inverse(rotation))\n    return [q[0], q[1], q[2]]", "category": "Python"}, {"instruction": "def list_roles(self, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"List all the roles that are registered with the plugin.\n\n        Supported methods:\n            LIST: /auth/{mount_point}/roles. Produces: 200 application/json\n\n\n        :param mount_point: The \"path\" the azure auth method was mounted on.\n        :type mount_point: str | unicode\n        :return: The \"data\" key from the JSON response of the request.\n        :rtype: dict\n        \"\"\"\n", "input": "", "output": "        api_path = '/v1/auth/{mount_point}/roles'.format(mount_point=mount_point)\n        response = self._adapter.list(\n            url=api_path\n        )\n        return response.json().get('data')", "category": "Python"}, {"instruction": "def names(self):\n        \"\"\"\n        Get all names of children\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        if not self.children:\n            return set([self.name])\n        ret = set()\n        for child in self.children:\n            for name in child.names:\n                ret.add(name)\n        return ret", "category": "Python"}, {"instruction": "def used_by_slides(self):\n        \"\"\"Tuple of slide objects based on this slide layout.\"\"\"\n", "input": "", "output": "        # ---getting Slides collection requires going around the horn a bit---\n        slides = self.part.package.presentation_part.presentation.slides\n        return tuple(s for s in slides if s.slide_layout == self)", "category": "Python"}, {"instruction": "def _get_lowstate(self):\n        '''\n        Format the incoming data into a lowstate object\n        '''\n", "input": "", "output": "        if not self.request.body:\n            return\n        data = self.deserialize(self.request.body)\n        self.request_payload = copy(data)\n\n        if data and 'arg' in data and not isinstance(data['arg'], list):\n            data['arg'] = [data['arg']]\n\n        if not isinstance(data, list):\n            lowstate = [data]\n        else:\n            lowstate = data\n\n        return lowstate", "category": "Python"}, {"instruction": "def plot_attention(attention_matrix: np.ndarray, source_tokens: List[str], target_tokens: List[str], filename: str):\n    \"\"\"\n    Uses matplotlib for creating a visualization of the attention matrix.\n\n    :param attention_matrix: The attention matrix.\n    :param source_tokens: A list of source tokens.\n    :param target_tokens: A list of target tokens.\n    :param filename: The file to which the attention visualization will be written to.\n    \"\"\"\n", "input": "", "output": "    try:\n        import matplotlib\n    except ImportError:\n        raise RuntimeError(\"Please install matplotlib.\")\n    matplotlib.use(\"Agg\")\n    import matplotlib.pyplot as plt\n    assert attention_matrix.shape[0] == len(target_tokens)\n\n    plt.imshow(attention_matrix.transpose(), interpolation=\"nearest\", cmap=\"Greys\")\n    plt.xlabel(\"target\")\n    plt.ylabel(\"source\")\n    plt.gca().set_xticks([i for i in range(0, len(target_tokens))])\n    plt.gca().set_yticks([i for i in range(0, len(source_tokens))])\n    plt.gca().set_xticklabels(target_tokens, rotation='vertical')\n    plt.gca().set_yticklabels(source_tokens)\n    plt.tight_layout()\n    plt.savefig(filename)\n    logger.info(\"Saved alignment visualization to \" + filename)", "category": "Python"}, {"instruction": "def from_file(cls, filename):\n        \"\"\"Read the configuration parameters from the Yaml file filename.\"\"\"\n", "input": "", "output": "        try:\n            with open(filename, \"r\") as fh:\n                return cls.from_dict(yaml.safe_load(fh))\n        except Exception as exc:\n            print(\"Error while reading TaskManager parameters from %s\\n\" % filename)\n            raise", "category": "Python"}, {"instruction": "def on_change(self, attr, *callbacks):\n        ''' Add a callback on this object to trigger when ``attr`` changes.\n\n        Args:\n            attr (str) : an attribute name on this object\n            callback (callable) : a callback function to register\n\n        Returns:\n            None\n\n        '''\n", "input": "", "output": "        if len(callbacks) == 0:\n            raise ValueError(\"on_change takes an attribute name and one or more callbacks, got only one parameter\")\n\n        _callbacks = self._callbacks.setdefault(attr, [])\n        for callback in callbacks:\n\n            if callback in _callbacks:\n                continue\n\n            _check_callback(callback, ('attr', 'old', 'new'))\n\n            _callbacks.append(callback)", "category": "Python"}, {"instruction": "def update_search_menu(self):\r\n        \"\"\"Update search menu\"\"\"\n", "input": "", "output": "        # Disabling all actions except the last one\r\n        # (which is Find in files) to begin with\r\n        for child in self.search_menu.actions()[:-1]:\r\n            child.setEnabled(False)\r\n\r\n        widget, textedit_properties = self.get_focus_widget_properties()\r\n        if textedit_properties is None: # widget is not an editor/console\r\n            return\r\n\r\n        # !!! Below this line, widget is expected to be a QPlainTextEdit\r\n        #     instance\r\n        console, not_readonly, readwrite_editor = textedit_properties\r\n\r\n        # Find actions only trigger an effect in the Editor\r\n        if not console:\r\n            for action in self.search_menu.actions():\r\n                try:\r\n                    action.setEnabled(True)\r\n                except RuntimeError:\r\n                    pass\r\n\r\n        # Disable the replace action for read-only files\r\n        self.search_menu_actions[3].setEnabled(readwrite_editor)", "category": "Python"}, {"instruction": "def get_ga_query_dict(self):\n        \"\"\"\n        Adds user agent and IP to the default hit parameters\n        \"\"\"\n", "input": "", "output": "        query_dict = super(GARequestErrorReportingMixin, self).get_ga_query_dict()\n        request = self.get_ga_request()\n        if not request:\n            return query_dict\n        user_ip = request.META.get('HTTP_X_FORWARDED_FOR', request.META.get('REMOTE_ADDR', ''))\n        user_ip = user_ip.split(',')[0].strip()\n        user_agent = request.META.get('HTTP_USER_AGENT')\n        user_language = request.META.get('HTTP_ACCEPT_LANGUAGE')\n        if user_ip:\n            query_dict['uip'] = user_ip\n        if user_agent:\n            query_dict['ua'] = user_agent\n        if user_language:\n            query_dict['ul'] = user_language\n        return query_dict", "category": "Python"}, {"instruction": "def find_child(sexpr: Sexpr, *tags: str) -> Optional[Sexpr]:\n    \"\"\"Search for a tag among direct children of the s-expression.\"\"\"\n", "input": "", "output": "    _assert_valid_sexpr(sexpr)\n\n    for child in sexpr[1:]:\n        if _is_sexpr(child) and child[0] in tags:\n            return child\n\n    return None", "category": "Python"}, {"instruction": "def is_module_function(obj, prop):\n    \"\"\"\n    Checking and setting type to MODULE_FUNCTION\n    Args:\n        obj: ModuleType\n        prop: FunctionType\n    Return:\n        Boolean\n    Raise:\n        prop_type_error: When the type of prop is not valid\n        prop_in_obj_error: When prop is not in the obj(module/class)\n        prop_is_func_error: When prop is not a callable stuff\n    \"\"\"\n", "input": "", "output": "    python_version = sys.version_info[0]\n    if python_version == 3:\n        unicode = str\n\n    if prop and (isinstance(prop, str) or isinstance(prop, unicode)): #property\n        if prop in dir(obj):\n            if (\n                    isinstance(getattr(obj, prop), FunctionType)\n                    or isinstance(getattr(obj, prop), BuiltinFunctionType)\n                    or inspect.ismethod(getattr(obj, prop))\n            ):\n            #inspect.ismethod for python2.7\n            #isinstance(...) for python3.x\n                return True\n            else:\n                ErrorHandler.prop_is_func_error(obj, prop)\n        else:\n            ErrorHandler.prop_in_obj_error(obj, prop)\n    elif prop:\n        ErrorHandler.prop_type_error(prop)\n    return False", "category": "Python"}, {"instruction": "def _yield_week_day(self, enumeration=False):\n        \"\"\"A helper function to reduce the number of nested loops.\n        \n        Parameters\n        ----------\n        enumeration\n            Whether or not to wrap the days in enumerate().\n            \n    \n        Yields\n        -------\n        tuple\n            A tuple with (week, day_index, day) or (week, day),\n            depending on 'enumeration' parameter.\n\n        \"\"\"\n", "input": "", "output": "        if enumeration:\n            # Iterate over all weeks\n            for week in range(1, self.duration + 1):\n                # Iterate over all days\n                for day_index, day in enumerate(self.days):\n                    yield (week, day_index, day)\n        else:\n            # Iterate over all weeks\n            for week in range(1, self.duration + 1):\n                # Iterate over all days\n                for day in self.days:\n                    yield (week, day)", "category": "Python"}, {"instruction": "def parse_signature(cls, function):\n        '''Parses the signature of a method and its annotations to swagger.\n\n        Return a dictionary {arg_name: info}.\n        '''\n", "input": "", "output": "        annotations = function.__annotations__.copy()\n        del annotations['return']\n        result = []\n        for param_name, (param_type, param_obj) in annotations.items():\n            sig_param = function.signature.parameters[param_name]\n            param_description = {\n                'paramType': param_type,\n                'name': param_name,\n                'required': sig_param.default is inspect.Parameter.empty}\n            param_description.update(param_obj.describe())\n            result.append(param_description)\n        return result", "category": "Python"}, {"instruction": "def get_project_logs(self, request):\r\n        \"\"\" Get logs from log service.\r\n        Unsuccessful opertaion will cause an LogException.\r\n        \r\n        :type request: GetProjectLogsRequest\r\n        :param request: the GetProjectLogs request parameters class.\r\n        \r\n        :return: GetLogsResponse\r\n        \r\n        :raise: LogException\r\n        \"\"\"\n", "input": "", "output": "        headers = {}\r\n        params = {}\r\n        if request.get_query() is not None:\r\n            params['query'] = request.get_query()\r\n        project = request.get_project()\r\n        resource = \"/logs\"\r\n        (resp, header) = self._send(\"GET\", project, None, resource, params, headers)\r\n        return GetLogsResponse(resp, header)", "category": "Python"}, {"instruction": "def createGroup(self, group, vendorSpecific=None):\n        \"\"\"See Also: createGroupResponse()\n\n        Args:\n          group:\n          vendorSpecific:\n\n        Returns:\n\n        \"\"\"\n", "input": "", "output": "        response = self.createGroupResponse(group, vendorSpecific)\n        return self._read_boolean_response(response)", "category": "Python"}, {"instruction": "def tabActiveMarkupChanged(self, tab):\n\t\t'''\n\t\tPerform all UI state changes that need to be done when the\n\t\tactive markup class of the current tab has changed.\n\t\t'''\n", "input": "", "output": "\t\tif tab == self.currentTab:\n\t\t\tmarkupClass = tab.getActiveMarkupClass()\n\t\t\tdtMarkdown = (markupClass == markups.MarkdownMarkup)\n\t\t\tdtMkdOrReST = dtMarkdown or (markupClass == markups.ReStructuredTextMarkup)\n\t\t\tself.formattingBox.setEnabled(dtMarkdown)\n\t\t\tself.symbolBox.setEnabled(dtMarkdown)\n\t\t\tself.actionUnderline.setEnabled(dtMarkdown)\n\t\t\tself.actionBold.setEnabled(dtMkdOrReST)\n\t\t\tself.actionItalic.setEnabled(dtMkdOrReST)", "category": "Python"}, {"instruction": "def select(message=\"\", title=\"Lackey Input\", options=None, default=None):\n    \"\"\" Creates a dropdown selection dialog with the specified message and options\n\n     `default` must be one of the options.\n\n     Returns the selected value. \"\"\"\n", "input": "", "output": "    if options is None or len(options) == 0:\n        return \"\"\n    if default is None:\n        default = options[0]\n    if default not in options:\n        raise ValueError(\"<<default>> not in options[]\")\n    root = tk.Tk()\n    input_text = tk.StringVar()\n    input_text.set(message)\n    PopupList(root, message, title, options, default, input_text)\n    root.focus_force()\n    root.mainloop()\n    return str(input_text.get())", "category": "Python"}, {"instruction": "def format_results(self, raw_data, options):\n        '''\n        Returns a python structure that later gets serialized.\n        raw_data\n            full list of objects matching the search term\n        options\n            a dictionary of the given options\n        '''\n", "input": "", "output": "        page_data = self.paginate_results(raw_data, options)\n        results = {}\n        meta = options.copy()\n        meta['more'] = _('Show more results')\n        if page_data and page_data.has_next():\n            meta['next_page'] = page_data.next_page_number()\n        if page_data and page_data.has_previous():\n            meta['prev_page'] = page_data.previous_page_number()\n        results['data'] = [self.format_item(item) for item in page_data.object_list]\n        results['meta'] = meta\n        return results", "category": "Python"}, {"instruction": "def get_sof_term(self, C, rup):\n        \"\"\"\n        In the case of the upper mantle events separate coefficients\n        are considered for normal, reverse and strike-slip\n        \"\"\"\n", "input": "", "output": "        if rup.rake <= -45.0 and rup.rake >= -135.0:\n            # Normal faulting\n            return C[\"FN_UM\"]\n        elif rup.rake > 45.0 and rup.rake < 135.0:\n            # Reverse faulting\n            return C[\"FRV_UM\"]\n        else:\n            # No adjustment for strike-slip faulting\n            return 0.0", "category": "Python"}, {"instruction": "def select_from_drop_down_by_text(self, drop_down_locator, option_locator, option_text, params=None):\n        \"\"\"\n        Select option from drop down widget using text.\n\n        :param drop_down_locator: locator tuple (if any, params needs to be in place) or WebElement instance\n        :param option_locator: locator tuple (if any, params needs to be in place)\n        :param option_text: text to base option selection on\n        :param params: Dictionary containing dictionary of params\n        :return: None\n        \"\"\"\n", "input": "", "output": "        # Open/activate drop down\n        self.click(drop_down_locator, params['drop_down'] if params else None)\n\n        # Get options\n        for option in self.get_present_elements(option_locator, params['option'] if params else None):\n            if self.get_text(option) == option_text:\n                self.click(option)\n                break", "category": "Python"}, {"instruction": "def traverse_nodes(self, node_set, depth=0):\n        \"\"\"BFS traversal of nodes that returns name traversal as large string.\n\n        Args:\n            node_set: Set of input nodes to begin traversal.\n            depth: Current traversal depth for child node viewing.\n\n        Returns:\n            type: String containing tabbed traversal view.\n        \"\"\"\n", "input": "", "output": "\n        tab = \"  \"\n        result = list()\n        for n in node_set:\n            repr = (\n                n\n                if self.nodes[n][\"type\"] == \"variable\"\n                else f\"{n}{inspect.signature(self.nodes[n]['lambda_fn'])}\"\n            )\n\n            result.append(f\"{tab * depth}{repr}\")\n            result.extend(\n                self.traverse_nodes(self.successors(n), depth=depth + 1)\n            )\n        return result", "category": "Python"}, {"instruction": "def toDatetime(value, format=None):\n    \"\"\" Returns a datetime object from the specified value.\n\n        Parameters:\n            value (str): value to return as a datetime\n            format (str): Format to pass strftime (optional; if value is a str).\n    \"\"\"\n", "input": "", "output": "    if value and value is not None:\n        if format:\n            value = datetime.strptime(value, format)\n        else:\n            # https://bugs.python.org/issue30684\n            # And platform support for before epoch seems to be flaky.\n            # TODO check for others errors too.\n            if int(value) == 0:\n                value = 86400\n            value = datetime.fromtimestamp(int(value))\n    return value", "category": "Python"}, {"instruction": "def set_write_bit(fn):\n    # type: (str) -> None\n    \"\"\"\n    Set read-write permissions for the current user on the target path.  Fail silently\n    if the path doesn't exist.\n\n    :param str fn: The target filename or path\n    :return: None\n    \"\"\"\n", "input": "", "output": "\n    fn = fs_encode(fn)\n    if not os.path.exists(fn):\n        return\n    file_stat = os.stat(fn).st_mode\n    os.chmod(fn, file_stat | stat.S_IRWXU | stat.S_IRWXG | stat.S_IRWXO)\n    if not os.path.isdir(fn):\n        for path in [fn, os.path.dirname(fn)]:\n            try:\n                os.chflags(path, 0)\n            except AttributeError:\n                pass\n        return None\n    for root, dirs, files in os.walk(fn, topdown=False):\n        for dir_ in [os.path.join(root, d) for d in dirs]:\n            set_write_bit(dir_)\n        for file_ in [os.path.join(root, f) for f in files]:\n            set_write_bit(file_)", "category": "Python"}, {"instruction": "def get_security_group_id(self, name):\n        \"\"\"\n        Take name string, give back security group ID.\n\n        To get around VPC's API being stupid.\n        \"\"\"\n", "input": "", "output": "        # Memoize entire list of groups\n        if not hasattr(self, '_security_groups'):\n            self._security_groups = {}\n            for group in self.get_all_security_groups():\n                self._security_groups[group.name] = group.id\n        return self._security_groups[name]", "category": "Python"}, {"instruction": "def parser(cls, v):\n        \"\"\"Ensure that the upstream parser gets two digits. \"\"\"\n", "input": "", "output": "        return geoid.census.State.parse(str(v).zfill(2))", "category": "Python"}, {"instruction": "def create(cls, path_name=None, name=None, project_id=None,\n               log_modified_at=None, crawlable=True):\n        \"\"\"Initialize an instance and save it to db.\"\"\"\n", "input": "", "output": "        result = cls(path_name, name, project_id, log_modified_at, crawlable)\n\n        db.session.add(result)\n        db.session.commit()\n\n        crawl_result(result, True)\n\n        return result", "category": "Python"}, {"instruction": "def opened(self):\n        \"\"\"Send the connect message to the server.\"\"\"\n", "input": "", "output": "        # give up if there are no more ddp versions to try\n        if self._ddp_version_index == len(DDP_VERSIONS):\n            self.ddpsocket._debug_log('* DDP VERSION MISMATCH')\n            self.emit('version_mismatch', DDP_VERSIONS)\n            return\n\n        # use server recommended version if we support it\n        if self._retry_new_version in DDP_VERSIONS:\n            self._ddp_version_index = [i for i, x in enumerate(DDP_VERSIONS)\n                                       if x == self._retry_new_version][0]\n\n        connect_msg = {\n            \"msg\": \"connect\",\n            \"version\": DDP_VERSIONS[self._ddp_version_index],\n            \"support\": DDP_VERSIONS\n        }\n\n        # if we've already got a session token then reconnect\n        if self._session:\n            connect_msg[\"session\"] = self._session\n\n        self.send(connect_msg)", "category": "Python"}, {"instruction": "def _set_annotation_to_str(annotation_data: Mapping[str, Mapping[str, bool]], key: str) -> str:\n    \"\"\"Return a set annotation string.\"\"\"\n", "input": "", "output": "    value = annotation_data[key]\n\n    if len(value) == 1:\n        return 'SET {} = \"{}\"'.format(key, list(value)[0])\n\n    x = ('\"{}\"'.format(v) for v in sorted(value))\n\n    return 'SET {} = {{{}}}'.format(key, ', '.join(x))", "category": "Python"}, {"instruction": "def get_mixedins_by_name(target):\n        \"\"\"Get a set of couple (name, field) of target mixedin.\"\"\"\n", "input": "", "output": "\n        result = getattr(target, Mixin.__MIXEDIN_KEY__, None)\n\n        if result is None:\n            result = dict()\n            setattr(target, Mixin.__MIXEDIN_KEY__, result)\n\n        return result", "category": "Python"}, {"instruction": "def chunks(self, size=32, alignment=1):\n        \"\"\"Return chunks of the data aligned as given by `alignment`. `size`\n        must be a multiple of `alignment`. Each chunk is returned as a\n        named two-tuple of its address and data.\n\n        \"\"\"\n", "input": "", "output": "\n        if (size % alignment) != 0:\n            raise Error(\n                'size {} is not a multiple of alignment {}'.format(\n                    size,\n                    alignment))\n\n        address = self.address\n        data = self.data\n\n        # First chunk may be shorter than `size` due to alignment.\n        chunk_offset = (address % alignment)\n\n        if chunk_offset != 0:\n            first_chunk_size = (alignment - chunk_offset)\n            yield self._Chunk(address, data[:first_chunk_size])\n            address += (first_chunk_size // self._word_size_bytes)\n            data = data[first_chunk_size:]\n        else:\n            first_chunk_size = 0\n\n        for offset in range(0, len(data), size):\n            yield self._Chunk(address + offset // self._word_size_bytes,\n                              data[offset:offset + size])", "category": "Python"}, {"instruction": "def build_unit_name(dimensions):\n    \"\"\"Build the name of the unit from its dimensions.\"\"\"\n", "input": "", "output": "    name = ''\n\n    for unit in dimensions:\n        if unit['power'] < 0:\n            name += 'per '\n        power = abs(unit['power'])\n        if power == 1:\n            name += unit['base']\n        elif power == 2:\n            name += 'square ' + unit['base']\n        elif power == 3:\n            name += 'cubic ' + unit['base']\n        elif power > 3:\n            name += unit['base'] + ' to the %g' % power\n        name += ' '\n\n    name = name.strip()\n\n    logging.debug(u'\\tUnit inferred name: %s', name)\n\n    return name", "category": "Python"}, {"instruction": "def update_group_product(self, group_id, product_data):\n        \"\"\"\n        \u4fee\u6539\u5206\u7ec4\u5546\u54c1\n\n        :param group_id: \u5546\u54c1\u5206\u7ec4ID\n        :param product_data: \u5206\u7ec4\u5546\u54c1\u4fe1\u606f\n        :return: \u8fd4\u56de\u7684 JSON \u6570\u636e\u5305\n        \"\"\"\n", "input": "", "output": "        product_data['group_id'] = group_id\n        return self._post(\n            'merchant/group/productmod',\n            data=product_data\n        )", "category": "Python"}, {"instruction": "def occ_issues_lookup(issue=None, code=None):\n    '''\n    Lookup occurrence issue definitions and short codes\n\n    :param issue: Full name of issue, e.g, CONTINENT_COUNTRY_MISMATCH\n    :param code: an issue short code, e.g. ccm\n\n    Usage\n    pygbif.occ_issues_lookup(issue = 'CONTINENT_COUNTRY_MISMATCH')\n    pygbif.occ_issues_lookup(issue = 'MULTIMEDIA_DATE_INVALID')\n    pygbif.occ_issues_lookup(issue = 'ZERO_COORDINATE')\n    pygbif.occ_issues_lookup(code = 'cdiv')\n    '''\n", "input": "", "output": "    if code is None:\n        bb = [trymatch(issue, x) for x in gbifissues['issue'] ]\n        tmp = filter(None, bb)\n    else:\n        bb = [trymatch(code, x) for x in gbifissues['code'] ]\n        tmp = filter(None, bb)\n    return tmp", "category": "Python"}, {"instruction": "def send(self, payload, opcode=ABNF.OPCODE_TEXT):\n        \"\"\"\n        Send the data as string.\n\n        payload: Payload must be utf-8 string or unicode,\n                  if the opcode is OPCODE_TEXT.\n                  Otherwise, it must be string(byte array)\n\n        opcode: operation code to send. Please see OPCODE_XXX.\n        \"\"\"\n", "input": "", "output": "\n        frame = ABNF.create_frame(payload, opcode)\n        return self.send_frame(frame)", "category": "Python"}, {"instruction": "def cross(self, vec):\n        \"\"\"Cross product with another vector\"\"\"\n", "input": "", "output": "        if not isinstance(vec, self.__class__):\n            raise TypeError('Cross product operand must be a vector')\n        return Vector3(0, 0, np.asscalar(np.cross(self, vec)))", "category": "Python"}, {"instruction": "def memory_used(self):\n        \"\"\"To know the allocated memory at function termination.\n\n        ..versionadded:: 4.1\n\n        This property might return None if the function is still running.\n\n        This function should help to show memory leaks or ram greedy code.\n        \"\"\"\n", "input": "", "output": "        if self._end_memory:\n            memory_used = self._end_memory - self._start_memory\n            return memory_used\n        else:\n            return None", "category": "Python"}, {"instruction": "def _tmp_html(data):\n    \"\"\"Yields the path of a temporary HTML file containing data.\"\"\"\n", "input": "", "output": "    filepath = ''\n    try:\n        fid, filepath = tempfile.mkstemp(suffix='.html', prefix='folium_')\n        os.write(fid, data.encode('utf8'))\n        os.close(fid)\n        yield filepath\n    finally:\n        if os.path.isfile(filepath):\n            os.remove(filepath)", "category": "Python"}, {"instruction": "def _token_request(self, grant_type, **kwargs):\n        \"\"\"\n        Do the actual call to the token end-point.\n\n        :param grant_type:\n        :param kwargs: See invoking methods.\n        :return:\n        \"\"\"\n", "input": "", "output": "        payload = {\n            'grant_type': grant_type,\n            'client_id': self._client_id,\n            'client_secret': self._client_secret\n        }\n\n        payload.update(**kwargs)\n\n        return self._realm.client.post(self.get_url('token_endpoint'),\n                                       data=payload)", "category": "Python"}, {"instruction": "def datasets_insert(self, dataset_name, friendly_name=None, description=None):\n    \"\"\"Issues a request to create a dataset.\n\n    Args:\n      dataset_name: the name of the dataset to create.\n      friendly_name: (optional) the friendly name for the dataset\n      description: (optional) a description for the dataset\n    Returns:\n      A parsed result object.\n    Raises:\n      Exception if there is an error performing the operation.\n    \"\"\"\n", "input": "", "output": "    url = Api._ENDPOINT + (Api._DATASETS_PATH % (dataset_name.project_id, ''))\n    data = {\n        'kind': 'bigquery#dataset',\n        'datasetReference': {\n            'projectId': dataset_name.project_id,\n            'datasetId': dataset_name.dataset_id\n        },\n    }\n    if friendly_name:\n      data['friendlyName'] = friendly_name\n    if description:\n      data['description'] = description\n    return datalab.utils.Http.request(url, data=data, credentials=self._credentials)", "category": "Python"}, {"instruction": "def _convert_json(obj):\n    '''\n    Converts from the JSON output provided by ovs-vsctl into a usable Python\n    object tree. In particular, sets and maps are converted from lists to\n    actual sets or maps.\n\n    Args:\n        obj: Object that shall be recursively converted.\n\n    Returns:\n        Converted version of object.\n    '''\n", "input": "", "output": "    if isinstance(obj, dict):\n        return {_convert_json(key): _convert_json(val)\n                for (key, val) in six.iteritems(obj)}\n    elif isinstance(obj, list) and len(obj) == 2:\n        first = obj[0]\n        second = obj[1]\n        if first == 'set' and isinstance(second, list):\n            return [_convert_json(elem) for elem in second]\n        elif first == 'map' and isinstance(second, list):\n            for elem in second:\n                if not isinstance(elem, list) or len(elem) != 2:\n                    return obj\n            return {elem[0]: _convert_json(elem[1]) for elem in second}\n        else:\n            return obj\n    elif isinstance(obj, list):\n        return [_convert_json(elem) for elem in obj]\n    else:\n        return obj", "category": "Python"}, {"instruction": "def from_dict(cls, d):\r\n        \"\"\"Extend Field.from_dict, set display/display_international \r\n        attributes.\"\"\"\n", "input": "", "output": "        phone = super(cls, cls).from_dict(d)\r\n        phone.display = d.get('display', u'')\r\n        phone.display_international = d.get('display_international', u'')\r\n        return phone", "category": "Python"}, {"instruction": "def random_filter(objects, reduction_factor, seed=42):\n    \"\"\"\n    Given a list of objects, returns a sublist by extracting randomly\n    some elements. The reduction factor (< 1) tells how small is the extracted\n    list compared to the original list.\n    \"\"\"\n", "input": "", "output": "    assert 0 < reduction_factor <= 1, reduction_factor\n    rnd = random.Random(seed)\n    out = []\n    for obj in objects:\n        if rnd.random() <= reduction_factor:\n            out.append(obj)\n    return out", "category": "Python"}, {"instruction": "def simple_storages(self):\n        \"\"\"This property gets the list of instances for SimpleStorages\n\n        :returns: a list of instances of SimpleStorages\n        \"\"\"\n", "input": "", "output": "        return simple_storage.SimpleStorageCollection(\n            self._conn, utils.get_subresource_path_by(self, 'SimpleStorage'),\n            redfish_version=self.redfish_version)", "category": "Python"}, {"instruction": "def write_ln(self, *text, sep=' '):\n        \"\"\"\n        Write line\n\n        :param text:\n        :param sep:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if self.text and self.text[-1] != '\\n':\n            self.text += '\\n'\n        self.text += markdown.text(*text, sep) + '\\n'\n        return self", "category": "Python"}, {"instruction": "def reminders_info(self, *, reminder: str, **kwargs) -> SlackResponse:\n        \"\"\"Gets information about a reminder.\n\n        Args:\n            reminder (str): The ID of the reminder. e.g. 'Rm12345678'\n        \"\"\"\n", "input": "", "output": "        self._validate_xoxp_token()\n        kwargs.update({\"reminder\": reminder})\n        return self.api_call(\"reminders.info\", http_verb=\"GET\", params=kwargs)", "category": "Python"}, {"instruction": "def listpid(toggle='basic'): # Add method to exclude elements from list\n    '''list pids'''\n", "input": "", "output": "    proc=psutil.process_iter()# evalute if its better to keep one instance of this or generate here?\n    if toggle=='basic':\n        host=gethostname()\n        host2=os.getenv('HOME').split(sep='/'   )[-1]\n        for row in proc:\n            #~ DPRINT([row.ppid(),row.name(),host],'username,row.name,host')\n            if row.username() in host or row.username() in host2:   #new psutil using grabing timeyyy and not alfa for username so host 2 is getting the timeyyy on UBUNTU  \n                yield row.name(), row.ppid()\n    elif toggle=='all':\n        for row in proc:\n            \n            yield row.name(), row.ppid()\n    elif toggle =='windows-basic':\n        for row in proc:\n            try:\n                pname = psutil.Process(row.pid).name()\n                pname = pname[:-4]#removiing .exe from end\n                yield pname, row.pid\n            except:\n                pass", "category": "Python"}, {"instruction": "def _validate_filter(self, keys, filterset_class):\n        \"\"\"\n        Check that all the filter[key] are valid.\n\n        :param keys: list of FilterSet keys\n        :param filterset_class: :py:class:`django_filters.rest_framework.FilterSet`\n        :raises ValidationError: if key not in FilterSet keys or no FilterSet.\n        \"\"\"\n", "input": "", "output": "        for k in keys:\n            if ((not filterset_class) or (k not in filterset_class.base_filters)):\n                raise ValidationError(\"invalid filter[{}]\".format(k))", "category": "Python"}, {"instruction": "def safe_cast_to_index(array: Any) -> pd.Index:\n    \"\"\"Given an array, safely cast it to a pandas.Index.\n\n    If it is already a pandas.Index, return it unchanged.\n\n    Unlike pandas.Index, if the array has dtype=object or dtype=timedelta64,\n    this function will not attempt to do automatic type conversion but will\n    always return an index with dtype=object.\n    \"\"\"\n", "input": "", "output": "    if isinstance(array, pd.Index):\n        index = array\n    elif hasattr(array, 'to_index'):\n        index = array.to_index()\n    else:\n        kwargs = {}\n        if hasattr(array, 'dtype') and array.dtype.kind == 'O':\n            kwargs['dtype'] = object\n        index = pd.Index(np.asarray(array), **kwargs)\n    return _maybe_cast_to_cftimeindex(index)", "category": "Python"}, {"instruction": "def _copy(self):\n        \"\"\"\n        Create a new L{TransitionTable} just like this one using a copy of the\n        underlying transition table.\n\n        @rtype: L{TransitionTable}\n        \"\"\"\n", "input": "", "output": "        table = {}\n        for existingState, existingOutputs in self.table.items():\n            table[existingState] = {}\n            for (existingInput, existingTransition) in existingOutputs.items():\n                table[existingState][existingInput] = existingTransition\n        return TransitionTable(table)", "category": "Python"}, {"instruction": "def calculate_dates(self, dt):\n        \"\"\"\n        Given a dt, find that day's close and period start (close - offset).\n        \"\"\"\n", "input": "", "output": "        period_end = self.cal.open_and_close_for_session(\n            self.cal.minute_to_session_label(dt),\n        )[1]\n\n        # Align the market close time here with the execution time used by the\n        # simulation clock. This ensures that scheduled functions trigger at\n        # the correct times.\n        self._period_end = self.cal.execution_time_from_close(period_end)\n\n        self._period_start = self._period_end - self.offset\n        self._period_close = self._period_end", "category": "Python"}, {"instruction": "def get_smeared_densities(self, sigma):\n        \"\"\"\n        Returns the Dict representation of the densities, {Spin: densities},\n        but with a Gaussian smearing of std dev sigma applied about the fermi\n        level.\n\n        Args:\n            sigma: Std dev of Gaussian smearing function.\n\n        Returns:\n            Dict of Gaussian-smeared densities.\n        \"\"\"\n", "input": "", "output": "        from scipy.ndimage.filters import gaussian_filter1d\n        smeared_dens = {}\n        diff = [self.energies[i + 1] - self.energies[i]\n                for i in range(len(self.energies) - 1)]\n        avgdiff = sum(diff) / len(diff)\n        for spin, dens in self.densities.items():\n            smeared_dens[spin] = gaussian_filter1d(dens, sigma / avgdiff)\n        return smeared_dens", "category": "Python"}, {"instruction": "def _delta(self, graph, cur_state, char):\n        \"\"\"\n        Args:\n            graph (Fst Acceptor): The DFA\n            cur_state (Fst State): The current State\n            char (Char): The input character\n        Returns:\n            (Fst State): The destination state\n        \"\"\"\n", "input": "", "output": "        for arc in cur_state.arcs:\n            if graph.isyms.find(arc.ilabel) == char:\n                return graph[arc.nextstate]\n        return None", "category": "Python"}, {"instruction": "def add_result(self, _type, test, exc_info=None):\n        \"\"\"\n        Adds the given result to the list\n\n        :param _type: type of the state of the test (TestState.failure, TestState.error, ...)\n        :param test: the test\n        :param exc_info: additional execution information\n        \"\"\"\n", "input": "", "output": "        if exc_info is not None:\n            exc_info = FrozenExcInfo(exc_info)\n        test.time_taken = time.time() - self.start_time\n        test._outcome = None\n        self.result_queue.put((_type, test, exc_info))", "category": "Python"}, {"instruction": "def rr_history(self, query, query_type=\"A\"):\n        '''Get the RR (Resource Record) History of the given domain or IP.\n        The default query type is for 'A' records, but the following query types\n        are supported:\n\n        A, NS, MX, TXT, CNAME\n\n        For details, see https://investigate.umbrella.com/docs/api#dnsrr_domain\n        '''\n", "input": "", "output": "        if query_type not in Investigate.SUPPORTED_DNS_TYPES:\n            raise Investigate.UNSUPPORTED_DNS_QUERY\n\n        # if this is an IP address, query the IP\n        if Investigate.IP_PATTERN.match(query):\n            return self._ip_rr_history(query, query_type)\n\n        # otherwise, query the domain\n        return self._domain_rr_history(query, query_type)", "category": "Python"}, {"instruction": "def peak_signal_to_noise_ratio(true, pred):\n  \"\"\"Image quality metric based on maximal signal power vs. power of the noise.\n\n  Args:\n    true: the ground truth image.\n    pred: the predicted image.\n  Returns:\n    peak signal to noise ratio (PSNR)\n  \"\"\"\n", "input": "", "output": "  return 10.0 * tf.log(1.0 / mean_squared_error(true, pred)) / tf.log(10.0)", "category": "Python"}, {"instruction": "def is_end(self, char):\n        \"\"\"\n        Return `True` if this `char` is part of this node's letter set,\n        `False` otherwise.\n        \"\"\"\n", "input": "", "output": "        char = char.lower()\n\n        return bool(cgaddag.gdg_is_end(self.gdg, self.node, char.encode(\"ascii\")))", "category": "Python"}, {"instruction": "def clone(self):\n        ''' Creates a clone of the current query and all settings.  Further\n            updates to the cloned object or the original object will not\n            affect each other\n        '''\n", "input": "", "output": "        qclone = Query(self.type, self.session)\n        qclone.__query = deepcopy(self.__query)\n        qclone._sort = deepcopy(self._sort)\n        qclone._fields = deepcopy(self._fields)\n        qclone._hints = deepcopy(self.hints)\n        qclone._limit = deepcopy(self._limit)\n        qclone._skip = deepcopy(self._skip)\n        qclone._raw_output = deepcopy(self._raw_output)\n        return qclone", "category": "Python"}, {"instruction": "def get_measured_regions(self):\n        \"\"\"\n        Returns:\n            pandas.DataFrame: Output a dataframe with regions and region sizes\n        \"\"\"\n", "input": "", "output": "        mergeon = ['project_id','project_name',\n                'sample_id','sample_name',\n                'frame_id','frame_name',\n                ]\n        temp = self.loc[:,mergeon+['regions']].\\\n            set_index(mergeon)['regions'].apply(json.dumps).\\\n            reset_index().drop_duplicates()\n        temp['regions'] = temp['regions'].apply(json.loads)\n        rows = []\n        for i,r in temp.iterrows():\n            for label in r['regions']:\n                a = list(r.index)\n                b = list(r.values)\n                a = a+['region_label','region_area_pixels']\n                b = b+[label,r['regions'][label]]\n                rows.append(dict(zip(a,b)))\n        rows = pd.DataFrame(rows).drop(columns='regions').\\\n            drop_duplicates()[mergeon+['region_label','region_area_pixels']]\n        #rows = rows.loc[rows['region_area_pixels']>0].copy()\n        return rows", "category": "Python"}, {"instruction": "def computeDelaunayTriangulation(points):\n    \"\"\" Takes a list of point objects (which must have x and y fields).\n        Returns a list of 3-tuples: the indices of the points that form a\n        Delaunay triangle.\n    \"\"\"\n", "input": "", "output": "    siteList = SiteList(points)\n    context  = Context()\n    context.triangulate = True\n    voronoi(siteList,context)\n    return context.triangles", "category": "Python"}, {"instruction": "def _loads(self, string):\n        \"\"\" If :prop:serialized is True, @string will be unserialized\n            using :prop:serializer\n        \"\"\"\n", "input": "", "output": "        if not self.serialized:\n            return self._decode(string)\n        if string is not None:\n            try:\n                return self.serializer.loads(string)\n            except TypeError:\n                #: catches bytes errors with the builtin json library\n                return self.serializer.loads(self._decode(string))\n            except pickle.UnpicklingError as e:\n                #: incr and decr methods create issues when pickle serialized\n                #  It's a terrible idea for a serialized instance\n                #  to be performing incr and decr methods, but I think\n                #  it makes sense to catch the error regardless\n                decoded = self._decode(string)\n                if decoded.isdigit():\n                    return decoded\n                raise pickle.UnpicklingError(e)", "category": "Python"}, {"instruction": "def _old_run_cmd(self, cmd):\n        '''\n        Cleanly execute the command string\n        '''\n", "input": "", "output": "        try:\n            proc = subprocess.Popen(\n                cmd,\n                shell=True,\n                stderr=subprocess.PIPE,\n                stdout=subprocess.PIPE,\n            )\n\n            data = proc.communicate()\n            return data[0], data[1], proc.returncode\n        except Exception:\n            return ('local', 'Unknown Error', None)", "category": "Python"}, {"instruction": "def estimate_complexity(self, x,y,z,n):\n        \"\"\" \n        calculates a rough guess of runtime based on product of parameters \n        \"\"\"\n", "input": "", "output": "        num_calculations = x * y * z * n\n        run_time = num_calculations / 100000  # a 2014 PC does about 100k calcs in a second (guess based on prior logs)\n        return self.show_time_as_short_string(run_time)", "category": "Python"}, {"instruction": "def raw_sql(self, query, results=False):\n        \"\"\"\n        Execute a given query string. Could have unexpected results if the\n        query modifies the behavior of the session in a way unknown to Ibis; be\n        careful.\n\n        Parameters\n        ----------\n        query : string\n          DML or DDL statement\n        results : boolean, default False\n          Pass True if the query as a result set\n\n        Returns\n        -------\n        cur : ImpalaCursor if results=True, None otherwise\n          You must call cur.release() after you are finished using the cursor.\n        \"\"\"\n", "input": "", "output": "        return self._execute(query, results=results)", "category": "Python"}, {"instruction": "def for_kind(kind_map, type_, fallback_key):\n        \"\"\"\n        Create an Options object from any mapping.\n        \"\"\"\n", "input": "", "output": "        if type_ not in kind_map:\n            if fallback_key not in kind_map:\n                raise ConfigException('\"%s\" is not in the config and has no fallback' % type_)\n\n            config = kind_map[fallback_key]\n        else:\n            config = kind_map[type_]\n\n        if isinstance(config, dict):\n            if 'element' not in config:\n                raise ConfigException('\"%s\" does not define an element' % type_)\n\n            opts = Options(type_, **config)\n        else:\n            opts = Options(type_, config)\n\n        return opts", "category": "Python"}, {"instruction": "def _submit_monotonic_count(self, metric_name, val, metric, custom_tags=None, hostname=None):\n        \"\"\"\n        Submit a metric as a monotonic count, additional tags provided will be added to\n        the ones from the label provided via the metrics object.\n\n        `custom_tags` is an array of 'tag:value' that will be added to the\n        metric when sending the monotonic count to Datadog.\n        \"\"\"\n", "input": "", "output": "\n        _tags = self._metric_tags(metric_name, val, metric, custom_tags, hostname)\n        self.monotonic_count('{}.{}'.format(self.NAMESPACE, metric_name), val, _tags, hostname=hostname)", "category": "Python"}, {"instruction": "def sendResponse(self, message, UUID, routing_key):\n        \"\"\"\n        Send `message` to ``self.output_exchange`` with routing key\n        ``self.output_key``, ``self.content_type`` in ``delivery_mode=2``.\n\n        Args:\n            message (str): message which will be sent\n            UUID: unique identification of message\n            routing_key (str): which routing key to use to send message back\n        \"\"\"\n", "input": "", "output": "        self.sendMessage(\n            exchange=self.output_exchange,\n            routing_key=routing_key,\n            message=message,\n            UUID=UUID\n        )", "category": "Python"}, {"instruction": "def fn_getma(fn, bnum=1, return_ds=False):\n    \"\"\"Get masked array from input filename\n\n    Parameters\n    ----------\n    fn : str\n        Input filename string\n    bnum : int, optional\n        Band number\n    \n    Returns\n    -------\n    np.ma.array    \n        Masked array containing raster values\n    \"\"\"\n", "input": "", "output": "    #Add check for filename existence\n    ds = fn_getds(fn)\n    out = ds_getma(ds, bnum=bnum)\n    if return_ds:\n        out = (out, ds)\n    return out", "category": "Python"}, {"instruction": "def setRoute(self, vehID, edgeList):\n        \"\"\"\n        setRoute(string, list) ->  None\n\n        changes the vehicle route to given edges list.\n        The first edge in the list has to be the one that the vehicle is at at the moment.\n\n        example usage:\n        setRoute('1', ['1', '2', '4', '6', '7'])\n\n        this changes route for vehicle id 1 to edges 1-2-4-6-7\n        \"\"\"\n", "input": "", "output": "        if isinstance(edgeList, str):\n            edgeList = [edgeList]\n        self._connection._beginMessage(tc.CMD_SET_VEHICLE_VARIABLE, tc.VAR_ROUTE, vehID,\n                                       1 + 4 + sum(map(len, edgeList)) + 4 * len(edgeList))\n        self._connection._packStringList(edgeList)\n        self._connection._sendExact()", "category": "Python"}, {"instruction": "def _authenticate_plain(credentials, sock_info):\n    \"\"\"Authenticate using SASL PLAIN (RFC 4616)\n    \"\"\"\n", "input": "", "output": "    source = credentials.source\n    username = credentials.username\n    password = credentials.password\n    payload = ('\\x00%s\\x00%s' % (username, password)).encode('utf-8')\n    cmd = SON([('saslStart', 1),\n               ('mechanism', 'PLAIN'),\n               ('payload', Binary(payload)),\n               ('autoAuthorize', 1)])\n    sock_info.command(source, cmd)", "category": "Python"}, {"instruction": "def fft_convolve(data, h, res_g = None,\n                 plan = None, inplace = False,\n                 kernel_is_fft = False,\n                 kernel_is_fftshifted = False):\n    \n    \"\"\" convolves data with kernel h via FFTs\n\n    \n    data should be either a numpy array or a OCLArray (see doc for fft)\n    both data and h should be same shape\n\n    if data/h are OCLArrays, then:\n        - type should be complex64\n        - shape should be equal and power of two\n        - h is assumed to be already fftshifted\n         (otherwise set kernel_is_fftshifted  to true)\n    \n    \"\"\"\n", "input": "", "output": "\n    \n    if isinstance(data,np.ndarray):\n        return _fft_convolve_numpy(data, h,\n                                   plan = plan,\n                                   kernel_is_fft = kernel_is_fft,\n                                   kernel_is_fftshifted = kernel_is_fftshifted)\n    elif isinstance(data,OCLArray):\n        return _fft_convolve_gpu(data,h, res_g = res_g,\n                                 plan = plan, inplace = inplace,\n                                 kernel_is_fft = kernel_is_fft)\n    else:\n        raise TypeError(\"array argument (1) has bad type: %s\"%type(data))", "category": "Python"}, {"instruction": "def samaccountname(self, base_dn, distinguished_name):\n        \"\"\"Retrieve the sAMAccountName for a specific DistinguishedName\n\n        :param str base_dn: The base DN to search within\n        :param list distinguished_name: The base DN to search within\n        :param list attributes: Object attributes to populate, defaults to all\n\n        :return: A populated ADUser object\n        :rtype: ADUser\n        \"\"\"\n", "input": "", "output": "        mappings = self.samaccountnames(base_dn, [distinguished_name])\n\n        try:\n            # Usually we will find a match, but perhaps not always\n            return mappings[distinguished_name]\n        except KeyError:\n            logging.info(\"%s - unable to retrieve object from AD by DistinguishedName\",\n                         distinguished_name)", "category": "Python"}, {"instruction": "def ncdegree(polynomial):\n    \"\"\"Returns the degree of a noncommutative polynomial.\n\n    :param polynomial: Polynomial of noncommutive variables.\n    :type polynomial: :class:`sympy.core.expr.Expr`.\n\n    :returns: int -- the degree of the polynomial.\n    \"\"\"\n", "input": "", "output": "    degree = 0\n    if is_number_type(polynomial):\n        return degree\n    polynomial = polynomial.expand()\n    for monomial in polynomial.as_coefficients_dict():\n        subdegree = 0\n        for variable in monomial.as_coeff_mul()[1]:\n            if isinstance(variable, Pow):\n                subdegree += variable.exp\n            elif not isinstance(variable, Number) and variable != I:\n                subdegree += 1\n        if subdegree > degree:\n            degree = subdegree\n    return degree", "category": "Python"}, {"instruction": "def autoconf(self):\n        \"\"\"Implements Munin Plugin Auto-Configuration Option.\n        \n        @return: True if plugin can be  auto-configured, False otherwise.\n                 \n        \"\"\"\n", "input": "", "output": "        fpminfo = PHPfpmInfo(self._host, self._port, self._user, self._password, \n                             self._monpath, self._ssl)\n        return fpminfo is not None", "category": "Python"}, {"instruction": "def return_as(returntype):\n    \"\"\"Decorator to cast return of function as the given type\n\n    Parameters\n    ----------\n    returntype : `type`\n        the desired return type of the decorated function\n    \"\"\"\n", "input": "", "output": "    def decorator(func):\n        # @wraps(func) <- we can't use this as normal because it doesn't work\n        #                 on python < 3 for instance methods,\n        #                 see workaround below\n        def wrapped(*args, **kwargs):\n            result = func(*args, **kwargs)\n            try:\n                return returntype(result)\n            except (TypeError, ValueError) as exc:\n                exc.args = (\n                    'failed to cast return from {0} as {1}: {2}'.format(\n                        func.__name__, returntype.__name__, str(exc)),\n                )\n                raise\n        try:\n            return wraps(func)(wrapped)\n        except AttributeError:  # python < 3.0.0\n            wrapped.__doc__ == func.__doc__\n            return wrapped\n\n    return decorator", "category": "Python"}, {"instruction": "def lsf_stable(filt):\n  \"\"\"\n  Tests whether the given filter is stable or not by using the Line Spectral\n  Frequencies (LSF) of the given filter. Needs NumPy.\n\n  Parameters\n  ----------\n  filt :\n    A LTI filter as a LinearFilter object.\n\n  Returns\n  -------\n  A boolean that is true only when the LSF values from forward and backward\n  prediction filters alternates. Critical stability (both forward and backward\n  filters has the same LSF value) is seem as an instability, and returns\n  False.\n\n  See Also\n  --------\n  lsf :\n    Gets the Line Spectral Frequencies from a filter. Needs NumPy.\n  parcor_stable :\n    Tests filter stability with partial correlation coefficients (reflection\n    coefficients).\n\n  \"\"\"\n", "input": "", "output": "  lsf_data = lsf(ZFilter(filt.denpoly))\n  return all(a < b for a, b in blocks(lsf_data, size=2, hop=1))", "category": "Python"}, {"instruction": "def extent_selector_help():\n    \"\"\"Help message for extent selector dialog.\n\n    .. versionadded:: 3.2.1\n\n    :returns: A message object containing helpful information.\n    :rtype: messaging.message.Message\n    \"\"\"\n", "input": "", "output": "\n    message = m.Message()\n    message.add(m.Brand())\n    message.add(heading())\n    message.add(content())\n    return message", "category": "Python"}, {"instruction": "def path_hook(cls, *loader_details):\n        \"\"\"A class method which returns a closure to use on sys.path_hook\n        which will return an instance using the specified loaders and the path\n        called on the closure.\n\n        If the path called on the closure is not a directory, ImportError is\n        raised.\n\n        \"\"\"\n", "input": "", "output": "        def path_hook_for_FileFinder2(path):\n            ", "category": "Python"}, {"instruction": "def clear(self, context=None):\n        \"\"\" Delete all data from the graph. \"\"\"\n", "input": "", "output": "        context = URIRef(context).n3() if context is not None else '?g'\n        query = ", "category": "Python"}, {"instruction": "def _spa_python_import(how):\n    \"\"\"Compile spa.py appropriately\"\"\"\n", "input": "", "output": "\n    from pvlib import spa\n\n    # check to see if the spa module was compiled with numba\n    using_numba = spa.USE_NUMBA\n\n    if how == 'numpy' and using_numba:\n        # the spa module was compiled to numba code, so we need to\n        # reload the module without compiling\n        # the PVLIB_USE_NUMBA env variable is used to tell the module\n        # to not compile with numba\n        warnings.warn('Reloading spa to use numpy')\n        os.environ['PVLIB_USE_NUMBA'] = '0'\n        spa = reload(spa)\n        del os.environ['PVLIB_USE_NUMBA']\n    elif how == 'numba' and not using_numba:\n        # The spa module was not compiled to numba code, so set\n        # PVLIB_USE_NUMBA so it does compile to numba on reload.\n        warnings.warn('Reloading spa to use numba')\n        os.environ['PVLIB_USE_NUMBA'] = '1'\n        spa = reload(spa)\n        del os.environ['PVLIB_USE_NUMBA']\n    elif how != 'numba' and how != 'numpy':\n        raise ValueError(\"how must be either 'numba' or 'numpy'\")\n\n    return spa", "category": "Python"}, {"instruction": "def write(self,\n              container: Container,\n              filepath: str,\n              contents: str\n              ) -> str:\n        \"\"\"\n        Reads the contents of a given file belonging to a container.\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"writing to file [%s] inside container [%s]\",\n                     filepath, container.id)\n        filepath = self._resolve_path(container, filepath)\n\n        # write the file contents to a temporary file on the host before\n        # copying that file to the container\n        (_, fn_host) = tempfile.mkstemp(suffix='.bugzoo')\n        try:\n            with open(fn_host, 'w') as fh:\n                fh.write(contents)\n            self.__mgr_ctr.copy_to(container, fn_host, filepath)\n        finally:\n            os.remove(fn_host)\n\n        logger.debug(\"wrote to file [%s] inside container [%s]\",\n                     filepath, container.id)", "category": "Python"}, {"instruction": "def configs(self):\n    \"\"\"Returns a map of run paths to `ProjectorConfig` protos.\"\"\"\n", "input": "", "output": "    run_path_pairs = list(self.run_paths.items())\n    self._append_plugin_asset_directories(run_path_pairs)\n    # If there are no summary event files, the projector should still work,\n    # treating the `logdir` as the model checkpoint directory.\n    if not run_path_pairs:\n      run_path_pairs.append(('.', self.logdir))\n    if (self._run_paths_changed() or\n        _latest_checkpoints_changed(self._configs, run_path_pairs)):\n      self.readers = {}\n      self._configs, self.config_fpaths = self._read_latest_config_files(\n          run_path_pairs)\n      self._augment_configs_with_checkpoint_info()\n    return self._configs", "category": "Python"}, {"instruction": "def MessageSetItemSizer(field_number):\n  \"\"\"Returns a sizer for extensions of MessageSet.\n\n  The message set message looks like this:\n    message MessageSet {\n      repeated group Item = 1 {\n        required int32 type_id = 2;\n        required string message = 3;\n      }\n    }\n  \"\"\"\n", "input": "", "output": "  static_size = (_TagSize(1) * 2 + _TagSize(2) + _VarintSize(field_number) +\n                 _TagSize(3))\n  local_VarintSize = _VarintSize\n\n  def FieldSize(value):\n    l = value.ByteSize()\n    return static_size + local_VarintSize(l) + l\n\n  return FieldSize", "category": "Python"}, {"instruction": "def logical_or(self, other):\n        \"\"\"logical_or(t) = self(t) or other(t).\"\"\"\n", "input": "", "output": "        return self.operation(other, lambda x, y: int(x or y))", "category": "Python"}, {"instruction": "def set_cmap(cmap):\n    '''\n    Set the color map of the current 'color' scale.\n    '''\n", "input": "", "output": "    scale = _context['scales']['color']\n    for k, v in _process_cmap(cmap).items():\n        setattr(scale, k, v)\n    return scale", "category": "Python"}, {"instruction": "def x_runtime(f, *args, **kwargs):\n    \"\"\"X-Runtime Flask Response Decorator.\"\"\"\n", "input": "", "output": "\n    _t0 = now()\n    r = f(*args, **kwargs)\n    _t1 = now()\n    r.headers['X-Runtime'] = '{0}s'.format(Decimal(str(_t1 - _t0)))\n\n    return r", "category": "Python"}, {"instruction": "def filter_filenames(filenames):\n    \"\"\"\n    Skip files with extentions in `FILE_EXCLUDE_EXTENTIONS` and filenames that\n    contain `FILE_SKIP_PATTENRS`.\n    \"\"\"\n", "input": "", "output": "    filenames_cleaned = []\n    for filename in filenames:\n        keep = True\n        for pattern in FILE_EXCLUDE_EXTENTIONS:\n            if filename.endswith(pattern):\n                keep = False\n        for pattern in FILE_SKIP_PATTENRS:   # This will reject exercises...\n            if pattern in filename:\n                keep = False\n        if keep:\n            filenames_cleaned.append(filename)\n    return filenames_cleaned", "category": "Python"}, {"instruction": "def clear(self):\n        \"\"\"Deletes the history\"\"\"\n", "input": "", "output": "        self._points = _np.empty( (self.prealloc,self.dim) )\n        self._slice_for_run_nr = []\n        self.memleft = self.prealloc", "category": "Python"}, {"instruction": "def get_commits_since(check_name, target_tag=None):\n    \"\"\"\n    Get the list of commits from `target_tag` to `HEAD` for the given check\n    \"\"\"\n", "input": "", "output": "    root = get_root()\n    target_path = os.path.join(root, check_name)\n    command = 'git log --pretty=%s {}{}'.format('' if target_tag is None else '{}... '.format(target_tag), target_path)\n\n    with chdir(root):\n        return run_command(command, capture=True).stdout.splitlines()", "category": "Python"}, {"instruction": "def moma2(self, objective, wt_obj):\n        \"\"\"Find the smallest redistribution vector using Euclidean distance.\n\n        Minimizing the redistribution of fluxes using a quadratic objective\n        function. The distance is minimized by minimizing the sum of\n        (wild type - knockout)^2.\n\n        Creates the constraint that the we select the optimal flux vector that\n        is closest to the wildtype. This might still return an arbitrary flux\n        vector the maximizes the objective function.\n\n        Args:\n            objective: Objective reaction for the model.\n            wt_obj: The flux value for your wild type objective reactions.\n                Can either use an expiremental value or on determined by FBA\n                by using :meth:`.get_fba_obj_flux(objective)`.\n        \"\"\"\n", "input": "", "output": "        obj_expr = 0\n        for reaction in self._adjustment_reactions():\n            v_wt = self._v_wt[reaction]\n            v = self._v[reaction]\n            obj_expr += (v_wt - v)**2\n\n        self._prob.set_objective(obj_expr)\n\n        with self.constraints(self._v_wt[objective] >= wt_obj):\n            self._solve(lp.ObjectiveSense.Minimize)", "category": "Python"}, {"instruction": "def appendInnerHTML(self, html):\n        '''\n            appendInnerHTML - Appends nodes from arbitrary HTML as if doing element.innerHTML += 'someHTML' in javascript.\n\n            @param html <str> - Some HTML\n\n            NOTE: If associated with a document ( AdvancedHTMLParser ), the html will use the encoding associated with\n                    that document.\n\n            @return - None. A browser would return innerHTML, but that's somewhat expensive on a high-level node.\n              So just call .innerHTML explicitly if you need that\n        '''\n", "input": "", "output": "\n        # Late-binding to prevent circular import\n        from .Parser import AdvancedHTMLParser\n\n        # Inherit encoding from the associated document, if any.\n        encoding = None\n        if self.ownerDocument:\n            encoding = self.ownerDocument.encoding\n\n        # Generate blocks (text nodes and AdvancedTag's) from HTML\n        blocks = AdvancedHTMLParser.createBlocksFromHTML(html, encoding)\n\n        # Throw them onto this node\n        self.appendBlocks(blocks)", "category": "Python"}, {"instruction": "def check_service_windows(pfeed, *, as_df=False, include_warnings=False):\n    \"\"\"\n    Analog of :func:`check_frequencies` for ``pfeed.service_windows``\n    \"\"\"\n", "input": "", "output": "    table = 'service_windows'\n    problems = []\n\n    # Preliminary checks\n    if pfeed.service_windows is None:\n        problems.append(['error', 'Missing table', table, []])\n    else:\n        f = pfeed.service_windows.copy()\n        problems = check_for_required_columns(problems, table, f)\n    if problems:\n        return gt.format_problems(problems, as_df=as_df)\n\n    if include_warnings:\n        problems = check_for_invalid_columns(problems, table, f)\n\n    # Check service window ID\n    problems = gt.check_column_id(problems, table, f,\n      'service_window_id')\n\n    # Check start_time and end_time\n    for column in ['start_time', 'end_time']:\n        problems = gt.check_column(problems, table, f, column, gt.valid_time)\n\n    # Check weekday columns\n    v = lambda x: x in range(2)\n    for col in ['monday', 'tuesday', 'wednesday', 'thursday', 'friday',\n      'saturday', 'sunday']:\n        problems = gt.check_column(problems, table, f, col, v)\n\n    return gt.format_problems(problems, as_df=as_df)", "category": "Python"}, {"instruction": "def interface_ip(interface):\n    \"\"\"Determine the IP assigned to us by the given network interface.\"\"\"\n", "input": "", "output": "    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    return socket.inet_ntoa(\n        fcntl.ioctl(\n            sock.fileno(), 0x8915, struct.pack('256s', interface[:15])\n        )[20:24]\n    )", "category": "Python"}, {"instruction": "def get_pool(arg = None, opts = None, abort = False):\n    \"\"\" Returns pool to work with\n\n        Returns a pynipap.Pool object representing the pool we are working with.\n    \"\"\"\n", "input": "", "output": "    # yep, global variables are evil\n    global pool\n\n    try:\n        pool = Pool.list({ 'name': arg })[0]\n    except IndexError:\n        if abort:\n            print(\"Pool '%s' not found.\" % str(arg), file=sys.stderr)\n            sys.exit(1)\n        else:\n            pool = None\n\n    return pool", "category": "Python"}, {"instruction": "def _ReadEncodedData(self, read_size):\n    \"\"\"Reads encoded data from the file-like object.\n\n    Args:\n      read_size (int): number of bytes of encoded data to read.\n\n    Returns:\n      int: number of bytes of encoded data read.\n    \"\"\"\n", "input": "", "output": "    encoded_data = self._file_object.read(read_size)\n\n    read_count = len(encoded_data)\n\n    self._encoded_data = b''.join([self._encoded_data, encoded_data])\n\n    self._decoded_data, self._encoded_data = (\n        self._decoder.Decode(self._encoded_data))\n\n    self._decoded_data_size = len(self._decoded_data)\n\n    return read_count", "category": "Python"}, {"instruction": "def save_shortcuts(self):\r\n        \"\"\"Save shortcuts from table model.\"\"\"\n", "input": "", "output": "        self.check_shortcuts()\r\n        for shortcut in self.source_model.shortcuts:\r\n            shortcut.save()", "category": "Python"}, {"instruction": "def get_context_data(self, **kwargs):\n        \"\"\"This add in the context of strain_list_alive (which filters for all alive animals) and cages which filters for the number of current cages.\"\"\"\n", "input": "", "output": "        \n        context = super(StrainList, self).get_context_data(**kwargs)\n        context['strain_list_alive'] = Strain.objects.filter(animal__Alive=True).annotate(alive=Count('animal'))\n        context['cages'] = Animal.objects.filter(Alive=True).values(\"Cage\")        \n        return context", "category": "Python"}, {"instruction": "def Upload(self,directory,filename):\n        \"\"\"Uploads/Updates/Replaces files\"\"\"\n", "input": "", "output": "        if self._isMediaFile(filename):\n            return self._upload_media(directory,filename)\n        elif self._isConfigFile(filename):\n            return self._update_config(directory,filename)\n\n        print \"Not handled!\"\n        return False", "category": "Python"}, {"instruction": "def funTransEdgeX(theta, rho):\n    \"\"\" Fringe matrix in X\n\n    :param theta: fringe angle, in [rad]\n    :param rho: bend radius, in [m]\n    :return: 2x2 numpy array\n    \"\"\"\n", "input": "", "output": "    return np.matrix([[1, 0], [np.tan(theta) / rho, 1]], dtype=np.double)", "category": "Python"}, {"instruction": "def zip_tokenize_namespace(zip_src, namespace, logger=None):\n    \"\"\" Given a namespace, replaces 'namespace__' with %%%NAMESPACE%%% for all\n        files and ___NAMESPACE___ in all filenames in the zip\n    \"\"\"\n", "input": "", "output": "    if not namespace:\n        return zip_src\n\n    namespace_prefix = \"{}__\".format(namespace)\n    lightning_namespace = \"{}:\".format(namespace)\n    zip_dest = zipfile.ZipFile(io.BytesIO(), \"w\", zipfile.ZIP_DEFLATED)\n    for name in zip_src.namelist():\n        content = zip_src.read(name)\n        try:\n            content = content.decode(\"utf-8\")\n        except UnicodeDecodeError:\n            # Probably a binary file; leave it untouched\n            pass\n        else:\n            content = content.replace(namespace_prefix, \"%%%NAMESPACE%%%\")\n            content = content.replace(lightning_namespace, \"%%%NAMESPACE_OR_C%%%\")\n            content = content.encode(\"utf-8\")\n            name = name.replace(namespace_prefix, \"___NAMESPACE___\")\n        zip_dest.writestr(name, content)\n    return zip_dest", "category": "Python"}, {"instruction": "def expand(sequence):\n    \"\"\"\n    expands a tree of sequences into a single, flat sequence, recursively.\n    \"\"\"\n", "input": "", "output": "    expanse = []\n    for point in sequence:\n        if 'sequence' in point:\n            expanse.extend(expand(point['sequence']))\n        else:\n            expanse.append(point)\n    return sequence.__class__(expanse)", "category": "Python"}, {"instruction": "def retrieve(self, request, *args, **kwargs):\n        \"\"\"Retrieve pzone as a preview or applied if no preview is provided.\"\"\"\n", "input": "", "output": "\n        when_param = get_query_params(self.request).get(\"preview\", None)\n        pk = self.kwargs[\"pk\"]\n\n        when = None\n        if when_param:\n            try:\n                when = parse_date(when_param)\n            except ValueError:\n                # invalid format, set back to None\n                when = None\n\n        pzone = None\n        if when:\n            # we have a date, use it\n            pzone = PZone.objects.preview(pk=pk, when=when)\n        else:\n            # we have no date, just get the pzone\n            pzone = PZone.objects.applied(pk=pk)\n\n        # turn content list into json\n        return Response(PZoneSerializer(pzone).data, content_type=\"application/json\")", "category": "Python"}, {"instruction": "def close(self):\n        \"\"\"Tell the process to exit\n        \"\"\"\n", "input": "", "output": "        try:\n            self.p.stdin.write(bytes(\"X\\n\",\"utf-8\"))\n            self.p.stdin.flush()\n        except IOError:\n            self.report(\"could not send exit command\")\n        self.p.wait() # wait until the process is closed\n        try:\n            os.remove(self.tmpfile) # clean up the temporary file\n        except FileNotFoundError:\n            pass", "category": "Python"}, {"instruction": "def handle_raw_output(ctx, data):\n    \"\"\"If a raw output format is set, dump data and exit\"\"\"\n", "input": "", "output": "    if ctx.obj['format'] == 'json':\n        print(json_dump(data))\n        exit(0)\n    if ctx.obj['format'] == 'yaml':\n        print(yaml_dump(data), end='')\n        exit(0)", "category": "Python"}, {"instruction": "def _default_http_header(self, empty_session_only: bool = False) -> Dict[str, str]:\n        \"\"\"Returns default HTTP header we use for requests.\"\"\"\n", "input": "", "output": "        header = {'Accept-Encoding': 'gzip, deflate',\n                  'Accept-Language': 'en-US,en;q=0.8',\n                  'Connection': 'keep-alive',\n                  'Content-Length': '0',\n                  'Host': 'www.instagram.com',\n                  'Origin': 'https://www.instagram.com',\n                  'Referer': 'https://www.instagram.com/',\n                  'User-Agent': self.user_agent,\n                  'X-Instagram-AJAX': '1',\n                  'X-Requested-With': 'XMLHttpRequest'}\n        if empty_session_only:\n            del header['Host']\n            del header['Origin']\n            del header['Referer']\n            del header['X-Instagram-AJAX']\n            del header['X-Requested-With']\n        return header", "category": "Python"}, {"instruction": "def reload_texts(self, texts, ids, vocabulary=None):\n        \"\"\"Calcula los vectores de terminos de textos y los almacena.\n\n        A diferencia de :func:`~TextClassifier.TextClassifier.store_text` esta\n        funcion borra cualquier informacion almacenada y comienza el conteo\n        desde cero. Se usa para redefinir el vocabulario sobre el que se\n        construyen los vectores.\n\n        Args:\n            texts (list): Una lista de N textos a incorporar.\n            ids (list): Una lista de N ids alfanumericos para los textos.\n        \"\"\"\n", "input": "", "output": "        self._check_id_length(ids)\n        self.ids = np.array(sorted(ids))\n        if vocabulary:\n            self.vectorizer.vocabulary = vocabulary\n        sorted_texts = [x for (y, x) in sorted(zip(ids, texts))]\n        self.term_mat = self.vectorizer.fit_transform(sorted_texts)\n        self._update_tfidf()", "category": "Python"}, {"instruction": "def plot_projectors(self, ax=None, fontsize=12, **kwargs):\n        \"\"\"\n        Plot the PAW projectors.\n\n        Args:\n            ax: matplotlib :class:`Axes` or None if a new figure should be created.\n\n        Returns: `matplotlib` figure\n        \"\"\"\n", "input": "", "output": "        ax, fig, plt = get_ax_fig_plt(ax)\n        title = kwargs.pop(\"title\", \"Projectors\")\n        ax.grid(True)\n        ax.set_xlabel('r [Bohr]')\n        ax.set_ylabel(r\"$r\\tilde p\\, [Bohr]^{-\\frac{1}{2}}$\")\n\n        #ax.axvline(x=self.paw_radius, linewidth=2, color='k', linestyle=\"--\")\n        #ax.annotate(\"$r_c$\", xy=(self.paw_radius + 0.1, 0.1))\n\n        for state, rfunc in self.projector_functions.items():\n            ax.plot(rfunc.mesh, rfunc.mesh * rfunc.values, label=\"TPROJ: \" + state)\n\n        ax.legend(loc=\"best\", shadow=True, fontsize=fontsize)\n\n        return fig", "category": "Python"}, {"instruction": "def nodes(self, value):\n        \"\"\"Remap indices to nodes whenever nodes are changed, e.g. in the\n        `macro` module.\n        \"\"\"\n", "input": "", "output": "        # pylint: disable=attribute-defined-outside-init\n        self._nodes = value\n        self._index2node = {node.index: node for node in self._nodes}", "category": "Python"}, {"instruction": "def print_summary(num_actions, failed, duration):\n    \"\"\"\n    Print a small summary of the executed plan.\n\n    Args:\n        num_actions (int): Total size of the executed plan.\n        failed (:obj:`list` of :obj:`actions.Step`): List of failed actions.\n        duration: Time we spent executing the plan.\n    \"\"\"\n", "input": "", "output": "    num_failed = len(failed)\n    print(", "category": "Python"}, {"instruction": "def check_response(response):\n    \"\"\"\n    checks the response if the server returned an error raises an exception.\n    \"\"\"\n", "input": "", "output": "    if response.status_code < 200 or response.status_code > 300:\n        raise ServerError('API requests returned with error: %s'\n                          % response.status_code)\n\n    try:\n        response_text = loads(response.text)\n    except ValueError:\n        raise ServerError('The API did not returned a JSON string.')\n\n    if not response_text:\n        raise EmptyResponse()\n\n    if 'failure' in response_text:\n        if response_text['failure'] == 'Falscher Dateityp':\n            raise UnsupportedFormat('Please look at picflash.org '\n                                    'witch formats are supported')\n        else:\n            raise UnkownError(response_text['failure'])", "category": "Python"}, {"instruction": "def reverse(array):\n    \"\"\"\n    returns a reversed numpy array\n    \"\"\"\n", "input": "", "output": "    l = list(array)\n    l.reverse()\n    return _n.array(l)", "category": "Python"}, {"instruction": "def _to_full_dict(xmltree):\n    '''\n    Returns the full XML dictionary including attributes.\n    '''\n", "input": "", "output": "    xmldict = {}\n\n    for attrName, attrValue in xmltree.attrib.items():\n        xmldict[attrName] = attrValue\n\n    if not xmltree.getchildren():\n        if not xmldict:\n            # If we don't have attributes, we should return the value as a string\n            # ex: <entry>test</entry>\n            return xmltree.text\n        elif xmltree.text:\n            # XML allows for empty sets with attributes, so we need to make sure that capture this.\n            # ex: <entry name=\"test\"/>\n            xmldict[_conv_name(xmltree.tag)] = xmltree.text\n\n    for item in xmltree:\n        name = _conv_name(item.tag)\n\n        if name not in xmldict:\n            xmldict[name] = _to_full_dict(item)\n        else:\n            # If a tag appears more than once in the same place, convert it to\n            # a list. This may require that the caller watch for such a thing\n            # to happen, and behave accordingly.\n            if not isinstance(xmldict[name], list):\n                xmldict[name] = [xmldict[name]]\n\n            xmldict[name].append(_to_full_dict(item))\n\n    return xmldict", "category": "Python"}, {"instruction": "def parse_config(args=sys.argv):\n    \"\"\"Parse the args using the config_file pattern\n\n    Args:\n        args: sys.argv\n\n    Returns:\n        The populated namespace object from parser.parse_args().\n\n    Raises:\n        TBD\n    \"\"\"\n", "input": "", "output": "    parser = argparse.ArgumentParser(\n        description='Read in the config file')\n    parser.add_argument(\n        'config_file',\n        help='Configuration file.',\n        metavar='FILE', type=extant_file)\n    return parser.parse_args(args[1:])", "category": "Python"}, {"instruction": "def daily_occurrences(self, dt=None, event=None):\n        '''\n        Returns a queryset of for instances that have any overlap with a\n        particular day.\n\n        * ``dt`` may be either a datetime.datetime, datetime.date object, or\n          ``None``. If ``None``, default to the current day.\n\n        * ``event`` can be an ``Event`` instance for further filtering.\n        '''\n", "input": "", "output": "        dt = dt or datetime.now()\n        start = datetime(dt.year, dt.month, dt.day)\n        end = start.replace(hour=23, minute=59, second=59)\n        qs = self.filter(\n            models.Q(\n                start_time__gte=start,\n                start_time__lte=end,\n            ) |\n            models.Q(\n                end_time__gte=start,\n                end_time__lte=end,\n            ) |\n            models.Q(\n                start_time__lt=start,\n                end_time__gt=end\n            )\n        )\n\n        return qs.filter(event=event) if event else qs", "category": "Python"}, {"instruction": "def get_icohp_dict_by_bondlengths(self, minbondlength=0.0, maxbondlength=8.0):\n        \"\"\"\n        get a dict of IcohpValues corresponding to certaind bond lengths\n        Args:\n            minbondlength: defines the minimum of the bond lengths of the bonds\n            maxbondlength: defines the maximum of the bond lengths of the bonds\n        Returns:\n             dict of IcohpValues, the keys correspond to the values from the initial list_labels\n        \"\"\"\n", "input": "", "output": "        newicohp_dict = {}\n        for value in self._icohplist.values():\n            if value._length >= minbondlength and value._length <= maxbondlength:\n                newicohp_dict[value._label] = value\n        return newicohp_dict", "category": "Python"}, {"instruction": "def __load_countries(self, db):\n        \"\"\"Load the list of countries\"\"\"\n", "input": "", "output": "\n        try:\n            countries = self.__read_countries_file()\n        except IOError as e:\n            raise LoadError(str(e))\n\n        try:\n            with db.connect() as session:\n                for country in countries:\n                    session.add(country)\n        except Exception as e:\n            raise LoadError(str(e))", "category": "Python"}, {"instruction": "def do_display(self, arg):\n        \"\"\"\n        display expression\n\n        Add expression to the display list; expressions in this list\n        are evaluated at each step, and printed every time its value\n        changes.\n\n        WARNING: since the expressions is evaluated multiple time, pay\n        attention not to put expressions with side-effects in the\n        display list.\n        \"\"\"\n", "input": "", "output": "        try:\n            value = self._getval_or_undefined(arg)\n        except:\n            return\n        self._get_display_list()[arg] = value", "category": "Python"}, {"instruction": "def commajoin_as_strings(iterable):\n    \"\"\" Join the given iterable with ',' \"\"\"\n", "input": "", "output": "    return _(u',').join((six.text_type(i) for i in iterable))", "category": "Python"}, {"instruction": "def index(self, overwrite=False):\n        \"\"\"\n        Assuming that all necessary data for this Genome has been downloaded,\n        generate the GTF database and save efficient representation of\n        FASTA sequence files.\n        \"\"\"\n", "input": "", "output": "        if self.requires_gtf:\n            self.db.connect_or_create(overwrite=overwrite)\n        if self.requires_transcript_fasta:\n            self.transcript_sequences.index(overwrite=overwrite)\n        if self.requires_protein_fasta:\n            self.protein_sequences.index(overwrite=overwrite)", "category": "Python"}]