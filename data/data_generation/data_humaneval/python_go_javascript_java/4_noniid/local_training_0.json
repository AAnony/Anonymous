[{"instruction": "def AddDefaultMergers(self):\n    \"\"\"Adds the default DataSetMergers defined in this module.\"\"\"\n", "input": "", "output": "    self.AddMerger(AgencyMerger(self))\n    self.AddMerger(StopMerger(self))\n    self.AddMerger(RouteMerger(self))\n    self.AddMerger(ServicePeriodMerger(self))\n    self.AddMerger(FareMerger(self))\n    self.AddMerger(ShapeMerger(self))\n    self.AddMerger(TripMerger(self))\n    self.AddMerger(FareRuleMerger(self))"}, {"instruction": "def instance(host=None, port=None):\n        \"\"\"\n        Singleton to return only one instance of Server.\n\n        :returns: instance of Server\n        \"\"\"\n", "input": "", "output": "\n        if not hasattr(WebServer, \"_instance\") or WebServer._instance is None:\n            assert host is not None\n            assert port is not None\n            WebServer._instance = WebServer(host, port)\n        return WebServer._instance"}, {"instruction": "def get_all(cls, names):\n        \"\"\"\n        Return all queues for the given names (for all available priorities)\n        \"\"\"\n", "input": "", "output": "        names = cls._get_iterable_for_names(names)\n\n        queues = []\n        for queue_name in names:\n            queues.extend(cls.collection(name=queue_name).instances())\n\n        return queues"}, {"instruction": "def as_blocks(self, copy=True):\n        \"\"\"\n        Convert the frame to a dict of dtype -> Constructor Types that each has\n        a homogeneous dtype.\n\n        .. deprecated:: 0.21.0\n\n        NOTE: the dtypes of the blocks WILL BE PRESERVED HERE (unlike in\n              as_matrix)\n\n        Parameters\n        ----------\n        copy : boolean, default True\n\n        Returns\n        -------\n        values : a dict of dtype -> Constructor Types\n        \"\"\"\n", "input": "", "output": "        warnings.warn(\"as_blocks is deprecated and will \"\n                      \"be removed in a future version\",\n                      FutureWarning, stacklevel=2)\n        return self._to_dict_of_blocks(copy=copy)"}, {"instruction": "def deserialize(self, mimetypes):  # pylint: disable=arguments-differ\n        \"\"\" Invoke the deserializer\n\n        Upon successful deserialization a dict will be returned\n        containing the following key/vals:\n\n            {\n                'content': <uploaded object>,\n                'content-type': <content-type of content>,\n                'file-ext': <file extension based on content-type>,\n                'file-name': <file name of content>,\n            }\n\n        :param mimetypes:\n            allowed mimetypes of the object in the request\n            payload\n        :return:\n            normalized dict\n        \"\"\"\n", "input": "", "output": "\n        super(Deserializer, self).deserialize()\n\n        parts = self.parse(mimetypes)\n        data = self.normalize(parts)\n\n        return data"}, {"instruction": "def _run_train_step(self, train_set):\n        \"\"\"Run a training step.\n\n        A training step is made by randomly shuffling the training set,\n        divide into batches and run the variable update nodes for each batch.\n        :param train_set: training set\n        :return: self\n        \"\"\"\n", "input": "", "output": "        np.random.shuffle(train_set)\n\n        batches = [_ for _ in utilities.gen_batches(train_set,\n                                                    self.batch_size)]\n        updates = [self.w_upd8, self.bh_upd8, self.bv_upd8]\n\n        for batch in batches:\n            self.tf_session.run(updates,\n                                feed_dict=self._create_feed_dict(batch))"}, {"instruction": "async def quit(self, message=None):\n        \"\"\" Quit network. \"\"\"\n", "input": "", "output": "        if message is None:\n            message = self.DEFAULT_QUIT_MESSAGE\n\n        await self.rawmsg('QUIT', message)\n        await self.disconnect(expected=True)"}, {"instruction": "def get_content_from_url(self, base_url):\n        \"\"\"\n        Sections can have SectionPage and ArticlePage child objects.\n        These have different fields, and thus have to be treated\n        differently.\n        \"\"\"\n", "input": "", "output": "        # assemble url\n        base_url = base_url.rstrip(\"/\")\n        url = base_url + API_PAGES_ENDPOINT + \"?type=\" + self._content_type + \\\n            \"&fields=\" + \",\".join(self._fields) + \\\n            \"&order=latest_revision_created_at\"\n\n        # make request\n        try:\n            response = requests.get(url)\n            self._base_url = base_url\n            self._content = response.json()\n            self._content = self._content[\"items\"]\n            return self._content\n        except requests.exceptions.ConnectionError:\n            return \"No content could be found from {}. \" \\\n                \"Are you sure this is the correct URL?\".format(base_url)\n        except requests.exceptions.RequestException:\n            return \"Content could not be imported at this time. \" \\\n                   \"Please try again later.\""}, {"instruction": "def\tpurge_url(self, host, path):\n\t\t\"\"\"Purge an individual URL.\"\"\"\n", "input": "", "output": "\t\tcontent = self._fetch(path, method=\"PURGE\", headers={ \"Host\": host }) \n\t\treturn FastlyPurge(self, content)"}, {"instruction": "def get_user_info(apikey, username, password):\n    \"\"\"\n    blogger.getUserInfo(api_key, username, password)\n    => user structure\n    \"\"\"\n", "input": "", "output": "    user = authenticate(username, password)\n    site = Site.objects.get_current()\n    return user_structure(user, site)"}, {"instruction": "def _pydevd_log(level, msg, *args):\n    '''\n    Levels are:\n\n    0 most serious warnings/errors (always printed)\n    1 warnings/significant events\n    2 informational trace\n    3 verbose mode\n    '''\n", "input": "", "output": "    if level <= DebugInfoHolder.DEBUG_TRACE_LEVEL:\n        # yes, we can have errors printing if the console of the program has been finished (and we're still trying to print something)\n        try:\n            try:\n                if args:\n                    msg = msg % args\n            except:\n                msg = '%s - %s' % (msg, args)\n            DebugInfoHolder.DEBUG_STREAM.write('%s\\n' % (msg,))\n            DebugInfoHolder.DEBUG_STREAM.flush()\n        except:\n            pass\n        return True"}, {"instruction": "def auth_approle(self, role_id, secret_id=None, mount_point='approle', use_token=True):\n        \"\"\"POST /auth/<mount_point>/login\n\n        :param role_id:\n        :type role_id:\n        :param secret_id:\n        :type secret_id:\n        :param mount_point:\n        :type mount_point:\n        :param use_token:\n        :type use_token:\n        :return:\n        :rtype:\n        \"\"\"\n", "input": "", "output": "        params = {\n            'role_id': role_id\n        }\n        if secret_id is not None:\n            params['secret_id'] = secret_id\n\n        return self.login('/v1/auth/{0}/login'.format(mount_point), json=params, use_token=use_token)"}, {"instruction": "def ReadCronJobs(self, cronjob_ids=None, cursor=None):\n    \"\"\"Reads all cronjobs from the database.\"\"\"\n", "input": "", "output": "    query = (\"SELECT job, UNIX_TIMESTAMP(create_time), enabled, \"\n             \"forced_run_requested, last_run_status, \"\n             \"UNIX_TIMESTAMP(last_run_time), current_run_id, state, \"\n             \"UNIX_TIMESTAMP(leased_until), leased_by \"\n             \"FROM cron_jobs\")\n    if cronjob_ids is None:\n      cursor.execute(query)\n      return [self._CronJobFromRow(row) for row in cursor.fetchall()]\n\n    query += \" WHERE job_id IN (%s)\" % \", \".join([\"%s\"] * len(cronjob_ids))\n    cursor.execute(query, cronjob_ids)\n    res = []\n    for row in cursor.fetchall():\n      res.append(self._CronJobFromRow(row))\n\n    if len(res) != len(cronjob_ids):\n      missing = set(cronjob_ids) - set([c.cron_job_id for c in res])\n      raise db.UnknownCronJobError(\"CronJob(s) with id(s) %s not found.\" %\n                                   missing)\n    return res"}, {"instruction": "def execute_ssh_command(self, client, command):\n        \"\"\"Execute the provided command and log output.\"\"\"\n", "input": "", "output": "        try:\n            out = ipa_utils.execute_ssh_command(client, command)\n        except Exception as error:\n            raise IpaCloudException(\n                'Command: \"{0}\", failed execution: {1}.'.format(\n                    command, error\n                )\n            )\n        else:\n            self._write_to_log(out)"}, {"instruction": "def tracked(self):\n        '''Return an array of job objects that are being tracked'''\n", "input": "", "output": "        results = json.loads(self.client('track'))\n        results['jobs'] = [Job(self, **job) for job in results['jobs']]\n        return results"}, {"instruction": "def num_nanoclusters(ConcAluminum, coag):\n    \"\"\"Return the number of Aluminum nanoclusters.\"\"\"\n", "input": "", "output": "    return (ConcAluminum / (dens_alum_nanocluster(coag).magnitude\n                            * np.pi * coag.Diameter**3))"}, {"instruction": "def get_grade_system_gradebook_assignment_session(self, proxy):\n        \"\"\"Gets the session for assigning grade system to gradebook mappings.\n\n        arg:    proxy (osid.proxy.Proxy): a proxy\n        return: (osid.grading.GradeSystemGradebookSession) - a\n                ``GradeSystemGradebookAssignmentSession``\n        raise:  NullArgument - ``proxy`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  Unimplemented -\n                ``supports_grade_system_gradebook_assignment()`` is\n                ``false``\n        *compliance: optional -- This method must be implemented if\n        ``supports_grade_system_gradebook_assignment()`` is ``true``.*\n\n        \"\"\"\n", "input": "", "output": "        if not self.supports_grade_system_gradebook_assignment():\n            raise errors.Unimplemented()\n        # pylint: disable=no-member\n        return sessions.GradeSystemGradebookAssignmentSession(proxy=proxy, runtime=self._runtime)"}, {"instruction": "def setDocuments(self, documenting_pid, documented_pid):\n        \"\"\"Add a CiTO, the Citation Typing Ontology, triple asserting that\n        ``documenting_pid`` documents ``documented_pid``.\n\n        Adds assertion: ``documenting_pid cito:documents documented_pid``\n\n        Args:\n          documenting_pid: str\n            PID of a Science Object that documents ``documented_pid``.\n\n          documented_pid: str\n            PID of a Science Object that is documented by ``documenting_pid``.\n\n        \"\"\"\n", "input": "", "output": "        self._check_initialized()\n        documenting_id = self.getObjectByPid(documenting_pid)\n        documented_id = self.getObjectByPid(documented_pid)\n        self.add((documenting_id, CITO.documents, documented_id))"}, {"instruction": "def Split(g, *, maxbuffer=10, tuple_len=None):\n    \"\"\"\n    Split a tuple generator into individual generators.\n\n    Parameters\n    ----------\n    g: tohu generator\n        The generator to be split. The items produced by `g` must be tuples.\n    maxbuffer: integer\n        Maximum number of items produced by `g` that will be buffered.\n    \"\"\"\n", "input": "", "output": "    if tuple_len is None:\n        try:\n            tuple_len = g.tuple_len\n        except AttributeError:\n            raise ValueError(\"Argument 'tuple_len' must be given since generator is not of type TupleGenerator.\")\n\n    g_buffered = BufferedTuple(g, maxbuffer=maxbuffer, tuple_len=tuple_len)\n\n    return tuple(NthElementBuffered(g_buffered, i) for i in range(tuple_len))"}, {"instruction": "def mmf(x, alpha, beta, kappa, delta):\n    \"\"\"Morgan-Mercer-Flodin\n    http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm\n\n    Parameters\n    ----------\n    x: int\n    alpha: float\n    beta: float\n    kappa: float\n    delta: float\n\n    Returns\n    -------\n    float\n        alpha - (alpha - beta) / (1. + (kappa * x)**delta)\n    \"\"\"\n", "input": "", "output": "    return alpha - (alpha - beta) / (1. + (kappa * x)**delta)"}, {"instruction": "def get_capacity(self, legacy=None):\n        \"\"\"Get capacity of all facilities.\n\n        :param legacy: Indicate set of server types to include in response\n\n        Validation of `legacy` is left to the packet api to avoid going out of date if any new value is introduced.\n        The currently known values are:\n          - only (current default, will be switched \"soon\")\n          - include\n          - exclude (soon to be default)\n        \"\"\"\n", "input": "", "output": "        params = None\n        if legacy:\n            params = {'legacy': legacy}\n\n        return self.call_api('/capacity', params=params)['capacity']"}, {"instruction": "async def async_init(self):\n        \"\"\"\n        Handle here the asynchronous part of the init.\n        \"\"\"\n", "input": "", "output": "\n        self.pool = await aioredis.create_pool(\n            (self.host, self.port),\n            db=self.db_id,\n            minsize=self.min_pool_size,\n            maxsize=self.max_pool_size,\n            loop=asyncio.get_event_loop(),\n        )"}, {"instruction": "def in_casapy (helper, caltable=None, selectcals={}, plotoptions={},\n               xaxis=None, yaxis=None, figfile=None):\n    \"\"\"This function is run inside the weirdo casapy IPython environment! A\n    strange set of modules is available, and the\n    `pwkit.environments.casa.scripting` system sets up a very particular\n    environment to allow encapsulated scripting.\n\n    \"\"\"\n", "input": "", "output": "    if caltable is None:\n        raise ValueError ('caltable')\n\n    show_gui = (figfile is None)\n    cp = helper.casans.cp\n\n    helper.casans.tp.setgui (show_gui)\n    cp.open (caltable)\n    cp.selectcal (**selectcals)\n    cp.plotoptions (**plotoptions)\n    cp.plot (xaxis, yaxis)\n\n    if show_gui:\n        import pylab as pl\n        pl.show ()\n    else:\n        cp.savefig (figfile)"}, {"instruction": "def find_project_file(start_dir, basename):\n    '''Walk up the directory tree until we find a file of the given name.'''\n", "input": "", "output": "    prefix = os.path.abspath(start_dir)\n    while True:\n        candidate = os.path.join(prefix, basename)\n        if os.path.isfile(candidate):\n            return candidate\n        if os.path.exists(candidate):\n            raise PrintableError(\n                \"Found {}, but it's not a file.\".format(candidate))\n        if os.path.dirname(prefix) == prefix:\n            # We've walked all the way to the top. Bail.\n            raise PrintableError(\"Can't find \" + basename)\n        # Not found at this level. We must go...shallower.\n        prefix = os.path.dirname(prefix)"}, {"instruction": "def get_beautiful_soup(self, source=None):\n        \"\"\" BeautifulSoup is a toolkit for dissecting an HTML document\n            and extracting what you need. It's great for screen-scraping! \"\"\"\n", "input": "", "output": "        from bs4 import BeautifulSoup\n        if not source:\n            self.wait_for_ready_state_complete()\n            source = self.get_page_source()\n        soup = BeautifulSoup(source, \"html.parser\")\n        return soup"}, {"instruction": "def delete_unique_identity(db, uuid):\n    \"\"\"Remove a unique identity from the registry.\n\n    Function that removes from the registry, the unique identity\n    that matches with uuid. Data related to this identity will be\n    also removed.\n\n    It checks first whether the unique identity is already on the registry.\n    When it is found, the unique identity is removed. Otherwise, it will\n    raise a 'NotFoundError' exception.\n\n    :param db: database manager\n    :param uuid: unique identifier assigned to the unique identity set\n        for being removed\n\n    :raises NotFoundError: raised when the unique identity does not exist\n        in the registry.\n    \"\"\"\n", "input": "", "output": "    with db.connect() as session:\n        uidentity = find_unique_identity(session, uuid)\n\n        if not uidentity:\n            raise NotFoundError(entity=uuid)\n\n        delete_unique_identity_db(session, uidentity)"}, {"instruction": "def info_progress(prefix: str, value: float, max_value: float) -> None:\n    \"\"\" Display info progress in percent.\n\n    :param value: the current value\n    :param max_value: the max value\n    :param prefix: the prefix message to print\n\n\n    \"\"\"\n", "input": "", "output": "    if sys.stdout.isatty():\n        percent = float(value) / max_value * 100\n        sys.stdout.write(prefix + \": %.0f%%\\r\" % percent)\n        sys.stdout.flush()"}, {"instruction": "def times(A, b, offset=0):\n    \"\"\"\n    Times the view of A with b in place (!).\n    Returns modified A\n    Broadcasting is allowed, thus b can be scalar.\n\n    if offset is not zero, make sure b is of right shape!\n\n    :param ndarray A: 2 dimensional array\n    :param ndarray-like b: either one dimensional or scalar\n    :param int offset: same as in view.\n    :rtype: view of A, which is adjusted inplace\n    \"\"\"\n", "input": "", "output": "    return _diag_ufunc(A, b, offset, np.multiply)"}, {"instruction": "def _get_appointee(id):\n    \"\"\"\n    Return a restclients.models.hrp.AppointeePerson object\n    \"\"\"\n", "input": "", "output": "    url = \"%s%s.json\" % (URL_PREFIX, id)\n    response = get_resource(url)\n    return process_json(response)"}, {"instruction": "def _fetch_partition_info(self, topic_id, partition_id):\n        \"\"\"Fetch partition info for given topic-partition.\"\"\"\n", "input": "", "output": "        info_path = \"/brokers/topics/{topic_id}/partitions/{p_id}\"\n        try:\n            _, partition_info = self.get(\n                info_path.format(topic_id=topic_id, p_id=partition_id),\n            )\n            return partition_info\n        except NoNodeError:\n            return {}"}, {"instruction": "def voidage_experimental(m, rho, D, H):\n    r'''Calculates voidage of a bed or mesh given an experimental weight and\n    fixed density, diameter, and height, as shown in [1]_. The formula is also\n    self-evident.\n\n    .. math::\n        \\epsilon = 1 - \\frac{\\frac{m_{mesh}}{\\frac{\\pi}{4}d_{column}^2\n        L_{mesh}}}{\\rho_{material}}\n\n    Parameters\n    ----------\n    m : float\n        Mass of mesh or bed particles weighted, [kg]\n    rho : float\n        Density of solid particles or mesh [kg/m^3]\n    D : float\n        Diameter of the cylindrical bed [m]\n    H : float\n        Height of the demister or bed [m]\n\n    Returns\n    -------\n    voidage : float\n        Voidage of bed of the material []\n\n    Notes\n    -----\n    Should be trusted over manufacturer data.\n\n    Examples\n    --------\n    >>> voidage_experimental(m=126, rho=8000, D=1, H=1)\n    0.9799464771704212\n\n    References\n    ----------\n    .. [1] Hels\u00f8r, T., and H. Svendsen. \"Experimental Characterization of\n       Pressure Drop in Dry Demisters at Low and Elevated Pressures.\" Chemical\n       Engineering Research and Design 85, no. 3 (2007): 377-85.\n       doi:10.1205/cherd06048.\n    '''\n", "input": "", "output": "    return 1 - m/(pi/4*D**2*H)/rho"}, {"instruction": "def contains_rigid(self):\n        \"\"\"Returns True if the Compound contains rigid bodies\n\n        If the Compound contains any particle with a rigid_id != None\n        then contains_rigid will return True. If the Compound has no\n        children (i.e. the Compound resides at the bottom of the containment\n        hierarchy) then contains_rigid will return False.\n\n        Returns\n        -------\n        bool\n            True if the Compound contains any particle with a rigid_id != None\n\n        Notes\n        -----\n        The private variable '_check_if_contains_rigid_bodies' is used to help\n        cache the status of 'contains_rigid'. If '_check_if_contains_rigid_bodies'\n        is False, then the rigid body containment of the Compound has not changed,\n        and the particle tree is not traversed, boosting performance.\n\n        \"\"\"\n", "input": "", "output": "        if self._check_if_contains_rigid_bodies:\n            self._check_if_contains_rigid_bodies = False\n            if any(particle.rigid_id is not None for particle in self._particles()):\n                self._contains_rigid = True\n            else:\n                self._contains_rigid = False\n        return self._contains_rigid"}, {"instruction": "def save_json(histogram: Union[HistogramBase, HistogramCollection], path: Optional[str] = None, **kwargs) -> str:\n    \"\"\"Save histogram to JSON format.\n\n    Parameters\n    ----------\n    histogram : Any histogram\n    path : If set, also writes to the path.\n\n    Returns\n    -------\n    json : The JSON representation of the histogram\n    \"\"\"\n", "input": "", "output": "    # TODO: Implement multiple histograms in one file?\n    data = histogram.to_dict()\n\n    data[\"physt_version\"] = CURRENT_VERSION\n    if isinstance(histogram, HistogramBase):\n        data[\"physt_compatible\"] = COMPATIBLE_VERSION\n    elif isinstance(histogram, HistogramCollection):\n        data[\"physt_compatible\"] = COLLECTION_COMPATIBLE_VERSION\n    else:\n        raise TypeError(\"Cannot save unknown type: {0}\".format(type(histogram)))\n\n    text = json.dumps(data, **kwargs)\n    if path:\n        with open(path, \"w\", encoding=\"utf-8\") as f:\n            f.write(text)\n    return text"}, {"instruction": "def loadtxt(fn, **kwargs):\n    \"\"\"Study the text data file fn. Call numpys loadtxt with keyword\n    arguments based on the study.\n\n    Return data returned from numpy `loadtxt <http://docs.scipy.org/doc/numpy/reference/generated/numpy.loadtxt.html#numpy-loadtxt>`.\n\n    kwargs: keyword arguments accepted by numpys loadtxt. Any keyword\n    arguments provided will take precedence over the ones resulting\n    from the study.\n\n    Set the module attribute PP to the instance of PatternPull used.\n\n    \"\"\"\n", "input": "", "output": "    global PP\n    PP = PatternPull(fn)\n    txtargs = PP.loadtxtargs()\n    txtargs.update(kwargs)      # Let kwargs dominate.\n    return np.loadtxt(fn, **txtargs)"}, {"instruction": "def compute_stats(array, stats, weights):\n    \"\"\"\n    :param array:\n        an array of R elements (which can be arrays)\n    :param stats:\n        a sequence of S statistic functions\n    :param weights:\n        a list of R weights\n    :returns:\n        an array of S elements (which can be arrays)\n    \"\"\"\n", "input": "", "output": "    result = numpy.zeros((len(stats),) + array.shape[1:], array.dtype)\n    for i, func in enumerate(stats):\n        result[i] = apply_stat(func, array, weights)\n    return result"}, {"instruction": "def _publish_stats(self, counter_prefix, stats):\n        \"\"\"Given a stats dictionary from _get_stats_from_socket,\n        publish the individual values.\n        \"\"\"\n", "input": "", "output": "        for stat_name, stat_value in flatten_dictionary(\n            stats,\n            prefix=counter_prefix,\n        ):\n            self.publish_gauge(stat_name, stat_value)"}, {"instruction": "def get_config_env() -> Dict[str, Any]:\n    \"\"\"\n    Returns the environment map that will be used for config checking when variables aren't set.\n    \"\"\"\n", "input": "", "output": "    if 'PULUMI_CONFIG' in os.environ:\n        env_config = os.environ['PULUMI_CONFIG']\n        return json.loads(env_config)\n    return dict()"}, {"instruction": "def show_version(a_device):\n    \"\"\"Execute show version command using Netmiko.\"\"\"\n", "input": "", "output": "    remote_conn = ConnectHandler(**a_device)\n    print()\n    print(\"#\" * 80)\n    print(remote_conn.send_command_expect(\"show version\"))\n    print(\"#\" * 80)\n    print()"}, {"instruction": "def get_command(self, name):\n        \"\"\"Get a :class:`.Command` or subclasses from the internal list\n        of commands.\n\n        This could also be used as a way to get aliases.\n\n        The name could be fully qualified (e.g. ``'foo bar'``) will get\n        the subcommand ``bar`` of the group command ``foo``. If a\n        subcommand is not found then ``None`` is returned just as usual.\n\n        Parameters\n        -----------\n        name: :class:`str`\n            The name of the command to get.\n\n        Returns\n        --------\n        :class:`Command` or subclass\n            The command that was requested. If not found, returns ``None``.\n        \"\"\"\n", "input": "", "output": "\n        # fast path, no space in name.\n        if ' ' not in name:\n            return self.all_commands.get(name)\n\n        names = name.split()\n        obj = self.all_commands.get(names[0])\n        if not isinstance(obj, GroupMixin):\n            return obj\n\n        for name in names[1:]:\n            try:\n                obj = obj.all_commands[name]\n            except (AttributeError, KeyError):\n                return None\n\n        return obj"}, {"instruction": "def install_language(language):\n    \"\"\"Install translation service routines into default namespace.\"\"\"\n", "input": "", "output": "    translator = get_translator(default_domain, default_directory,\n        languages=[get_lang(language)], fallback=True)\n    do_unicode = True\n    translator.install(do_unicode)"}, {"instruction": "def _surfdens(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _surfdens\n        PURPOSE:\n           evaluate the surface density for this potential\n        INPUT:\n           R - Galactocentric cylindrical radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           the surface density\n        HISTORY:\n           2018-08-19 - Written - Bovy (UofT)\n        \"\"\"\n", "input": "", "output": "        return 2.*integrate.quad(lambda x: self._dens(R,x,phi=phi,t=t),0,z)[0]"}, {"instruction": "def apply_log(a: tuple, func: Callable[[Any], Tuple[Any, Log]]) -> Tuple[Any, Log]:\n        \"\"\"Apply a function to a value with a log.\n\n        Helper function to apply a function to a value with a log tuple.\n        \"\"\"\n", "input": "", "output": "        value, log = a\n        new, entry = func(value)\n        return new, log + entry"}, {"instruction": "def copyidfobject(self, idfobject):\n        \"\"\"Add an IDF object to the IDF.\n\n        Parameters\n        ----------\n        idfobject : EpBunch object\n            The IDF object to remove. This usually comes from another idf file,\n            or it can be used to copy within this idf file.\n\n        \"\"\"\n", "input": "", "output": "        return addthisbunch(self.idfobjects,\n                     self.model,\n                     self.idd_info,\n                     idfobject, self)"}, {"instruction": "def is_multisig_script(script, blockchain='bitcoin', **blockchain_opts):\n    \"\"\"\n    Is the given script a multisig script?\n    \"\"\"\n", "input": "", "output": "    if blockchain == 'bitcoin':\n        return btc_is_multisig_script(script, **blockchain_opts)\n    else:\n        raise ValueError('Unknown blockchain \"{}\"'.format(blockchain))"}, {"instruction": "def _NTU_from_P_solver(P1, R1, NTU_min, NTU_max, function, **kwargs):\n    '''Private function to solve the P-NTU method backwards, given the\n    function to use, the upper and lower NTU bounds for consideration,\n    and the desired P1 and R1 values.\n    '''\n", "input": "", "output": "    P1_max = _NTU_from_P_objective(NTU_max, R1, 0, function, **kwargs)\n    P1_min = _NTU_from_P_objective(NTU_min, R1, 0, function, **kwargs)\n    if P1 > P1_max:\n        raise ValueError('No solution possible gives such a high P1; maximum P1=%f at NTU1=%f' %(P1_max, NTU_max))\n    if P1 < P1_min:\n        raise ValueError('No solution possible gives such a low P1; minimum P1=%f at NTU1=%f' %(P1_min, NTU_min))\n    # Construct the function as a lambda expression as solvers don't support kwargs\n    to_solve = lambda NTU1: _NTU_from_P_objective(NTU1, R1, P1, function, **kwargs)\n    return ridder(to_solve, NTU_min, NTU_max)"}, {"instruction": "def update_metadata(self, scaling_group, metadata):\n        \"\"\"\n        Adds the given metadata dict to the existing metadata for the scaling\n        group.\n        \"\"\"\n", "input": "", "output": "        if not isinstance(scaling_group, ScalingGroup):\n            scaling_group = self.get(scaling_group)\n        curr_meta = scaling_group.metadata\n        curr_meta.update(metadata)\n        return self.update(scaling_group, metadata=curr_meta)"}, {"instruction": "def _get_line(self) -> str:\n        \"\"\"Returns the current line from the file while incrementing the index.\"\"\"\n", "input": "", "output": "        line = self.in_lines[self.index]\n        self.index += 1\n        return line"}, {"instruction": "def styles(self, dictobj):\n\t\t\"\"\"\n\t\tAdd or update styles\n\t\t\"\"\"\n", "input": "", "output": "\t\tfor k in dictobj:\n\t\t\tself.chart_style[k] = dictobj[k]"}, {"instruction": "def next(self):\n        \"\"\"Returns the next batch of data.\"\"\"\n", "input": "", "output": "        if not self.iter_next():\n            raise StopIteration\n        data = self.getdata()\n        label = self.getlabel()\n        # iter should stop when last batch is not complete\n        if data[0].shape[0] != self.batch_size:\n        # in this case, cache it for next epoch\n            self._cache_data = data\n            self._cache_label = label\n            raise StopIteration\n        return DataBatch(data=data, label=label, \\\n            pad=self.getpad(), index=None)"}, {"instruction": "def get(self, sid):\n        \"\"\"\n        Constructs a TranscriptionContext\n\n        :param sid: The unique string that identifies the resource\n\n        :returns: twilio.rest.api.v2010.account.transcription.TranscriptionContext\n        :rtype: twilio.rest.api.v2010.account.transcription.TranscriptionContext\n        \"\"\"\n", "input": "", "output": "        return TranscriptionContext(self._version, account_sid=self._solution['account_sid'], sid=sid, )"}, {"instruction": "def matrix(self, x=(0, 0), y=(0, 0) , z=(0, 0)):\r\n        \"\"\"\r\n        Copy the ``pyny.Polyhedron`` along a 3D matrix given by the \r\n        three tuples x, y, z:        \r\n\r\n        :param x: Number of copies and distance between them in this\r\n            direction.\r\n        :type x: tuple (len=2)\r\n        :returns: list of ``pyny.Polyhedron``\r\n        \"\"\"\n", "input": "", "output": "        polygon = np.array([[0,0], [0,1], [1,1]])\r\n        space = Space(Place(polygon, polyhedra=self))\r\n        space = space.matrix(x, y, z, inplace=False)\r\n        return [place.polyhedra[0] for place in space]"}, {"instruction": "def setcreated(self, dt=None):\n        \"\"\"\n        Set I{created}.\n        @param dt: The created date & time.\n            Set as datetime.utc() when I{None}.\n        @type dt: L{datetime}\n        \"\"\"\n", "input": "", "output": "        if dt is None:\n            self.created = Token.utc()\n        else:\n            self.created = dt"}, {"instruction": "def linear_insert(self, item, priority):\n    \"\"\"Linear search. Performance is O(n^2).\"\"\"\n", "input": "", "output": "\n    with self.lock:\n        self_data = self.data\n        rotate = self_data.rotate\n        maxlen = self._maxlen\n        length = len(self_data)\n        count = length\n\n        # in practice, this is better than doing a rotate(-1) every\n        # loop and getting self.data[0] each time only because deque\n        # implements a very efficient iterator in C\n        for i in self_data:\n            if priority > i[1]:\n                break\n            count -= 1\n\n        rotate(-count)\n        self_data.appendleft((item, priority))\n        rotate(length-count)\n\n        try:\n            self.items[item] += 1\n        except TypeError:\n            self.items[repr(item)] += 1\n\n        if maxlen is not None and maxlen < len(self_data):\n                self._poplast()"}, {"instruction": "def incr(self, key, delta=1):\n        \"\"\"Increments the specified key value by the specified value.\n       \n        :param str|unicode key:\n    \n        :param int delta:\n\n        :rtype: bool\n        \"\"\"\n", "input": "", "output": "        return uwsgi.cache_inc(key, delta, self.timeout, self.name)"}, {"instruction": "def _loop_thread_main(self):\n        \"\"\"Main background thread running the event loop.\"\"\"\n", "input": "", "output": "\n        asyncio.set_event_loop(self.loop)\n        self._loop_check.inside_loop = True\n\n        try:\n            self._logger.debug(\"Starting loop in background thread\")\n            self.loop.run_forever()\n            self._logger.debug(\"Finished loop in background thread\")\n        except:  # pylint:disable=bare-except;This is a background worker thread.\n            self._logger.exception(\"Exception raised from event loop thread\")\n        finally:\n            self.loop.close()"}, {"instruction": "def delete_minion_cachedir(minion_id, provider, opts, base=None):\n    '''\n    Deletes a minion's entry from the cloud cachedir. It will search through\n    all cachedirs to find the minion's cache file.\n    Needs `update_cachedir` set to True.\n    '''\n", "input": "", "output": "    if isinstance(opts, dict):\n        __opts__.update(opts)\n\n    if __opts__.get('update_cachedir', False) is False:\n        return\n\n    if base is None:\n        base = __opts__['cachedir']\n\n    driver = next(six.iterkeys(__opts__['providers'][provider]))\n    fname = '{0}.p'.format(minion_id)\n    for cachedir in 'requested', 'active':\n        path = os.path.join(base, cachedir, driver, provider, fname)\n        log.debug('path: %s', path)\n        if os.path.exists(path):\n            os.remove(path)"}, {"instruction": "def peers(cls):\n        \"\"\"Return others of the same concrete type.\"\"\"\n", "input": "", "output": "        contentType = ContentType.objects.get_for_model(cls)\n        return cls.objects.filter(content_type=contentType)"}, {"instruction": "def defaultMachine(use_rpm_default=True):\n    \"\"\" Return the canonicalized machine name. \"\"\"\n", "input": "", "output": "\n    if use_rpm_default:\n        try:\n            # This should be the most reliable way to get the default arch\n            rmachine = subprocess.check_output(['rpm', '--eval=%_target_cpu'], shell=False).rstrip()\n            rmachine = SCons.Util.to_str(rmachine)\n        except Exception as e:\n            # Something went wrong, try again by looking up platform.machine()\n            return defaultMachine(False)\n    else:\n        rmachine = platform.machine()\n\n        # Try to lookup the string in the canon table\n        if rmachine in arch_canon:\n            rmachine = arch_canon[rmachine][0]\n\n    return rmachine"}, {"instruction": "def aggregate(self, variable, components=None, append=False):\n        \"\"\"Compute the aggregate of timeseries components or sub-categories\n\n        Parameters\n        ----------\n        variable: str\n            variable for which the aggregate should be computed\n        components: list of str, default None\n            list of variables, defaults to all sub-categories of `variable`\n        append: bool, default False\n            append the aggregate timeseries to `data` and return None,\n            else return aggregate timeseries\n        \"\"\"\n", "input": "", "output": "        # default components to all variables one level below `variable`\n        components = components or self._variable_components(variable)\n\n        if not len(components):\n            msg = 'cannot aggregate variable `{}` because it has no components'\n            logger().info(msg.format(variable))\n\n            return\n\n        rows = self._apply_filters(variable=components)\n        _data = _aggregate(self.data[rows], 'variable')\n\n        if append is True:\n            self.append(_data, variable=variable, inplace=True)\n        else:\n            return _data"}, {"instruction": "def say(self, event):\n        \"\"\"Chat event handler for incoming events\n        :param event: say-event with incoming chat message\n        \"\"\"\n", "input": "", "output": "\n        try:\n            userid = event.user.uuid\n            recipient = self._get_recipient(event)\n            content = self._get_content(event)\n\n            if self.config.name in content:\n                self.log('I think, someone mentioned me:', content)\n\n        except Exception as e:\n            self.log(\"Error: '%s' %s\" % (e, type(e)), exc=True, lvl=error)"}, {"instruction": "def get_nameserver_detail_output_show_nameserver_nameserver_porttype(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_nameserver_detail = ET.Element(\"get_nameserver_detail\")\n        config = get_nameserver_detail\n        output = ET.SubElement(get_nameserver_detail, \"output\")\n        show_nameserver = ET.SubElement(output, \"show-nameserver\")\n        nameserver_portid_key = ET.SubElement(show_nameserver, \"nameserver-portid\")\n        nameserver_portid_key.text = kwargs.pop('nameserver_portid')\n        nameserver_porttype = ET.SubElement(show_nameserver, \"nameserver-porttype\")\n        nameserver_porttype.text = kwargs.pop('nameserver_porttype')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def modify(self, **kwargs):\n        \"\"\"We need to implement the custom exclusive parameter check.\"\"\"\n", "input": "", "output": "        self._check_exclusive_parameters(**kwargs)\n        return super(Rule, self)._modify(**kwargs)"}, {"instruction": "def search(**criteria):\n    \"\"\"\n    Search registered *component* classes matching the given criteria.\n\n    :param criteria: search criteria of the form: ``a='1', b='x'``\n    :return: parts registered with the given criteria\n    :rtype: :class:`set`\n\n    Will return an empty :class:`set` if nothing is found.\n\n    ::\n\n        from cqparts.search import search\n        import cqparts_motors  # example of a 3rd party lib\n\n        # Get all DC motor classes\n        dc_motors = search(type='motor', current_class='dc')\n\n        # For more complex queries:\n        air_cooled = search(cooling='air')\n        non_aircooled_dcmotors = dc_motors - air_cooled\n        # will be all DC motors that aren't air-cooled\n    \"\"\"\n", "input": "", "output": "    # Find all parts that match the given criteria\n    results = copy(class_list)  # start with full list\n    for (category, value) in criteria.items():\n        results &= index[category][value]\n\n    return results"}, {"instruction": "def supervisor(self):\n        \"\"\"Return an authenticated connection for use, open new if required.\n\n        Returns:\n            SupervisorWebService: New or existing session with the Five9\n            Statistics API.\n        \"\"\"\n", "input": "", "output": "        supervisor = self._cached_client('supervisor')\n        if not self._api_supervisor_session:\n            self._api_supervisor_session = self.__create_supervisor_session(\n                supervisor,\n            )\n        return supervisor"}, {"instruction": "def connect(self):\n        \"\"\"\n        Connects to Redis\n        \"\"\"\n", "input": "", "output": "        logger.info(\"Connecting to Redis on {host}:{port}...\".format(\n            host=self.host, port=self.port))\n\n        super(RedisSubscriber, self).connect()\n        logger.info(\"Successfully connected to Redis\")\n\n        # Subscribe to channel\n        self.pubsub = self.client.pubsub()\n        self.pubsub.subscribe(self.channel)\n\n        logger.info(\"Subscribed to [{channel}] Redis channel\".format(\n            channel=self.channel))\n\n        # Start listening\n        t = Thread(target=self.listen)\n        t.setDaemon(True)\n        t.start()"}, {"instruction": "def stringize(\n        self,\n        rnf_profile=RnfProfile(),\n    ):\n        \"\"\"Create RNF representation of this read.\n\n\t\tArgs:\n\t\t\tread_tuple_id_width (int): Maximal expected string length of read tuple ID.\n\t\t\tgenome_id_width (int): Maximal expected string length of genome ID.\n\t\t\tchr_id_width (int): Maximal expected string length of chromosome ID.\n\t\t\tcoor_width (int): Maximal expected string length of a coordinate.\n\t\t\"\"\"\n", "input": "", "output": "\n        sorted_segments = sorted(self.segments,\n         key=lambda x: (\n          x.genome_id * (10 ** 23) +\n          x.chr_id * (10 ** 21) +\n          (x.left + (int(x.left == 0) * x.right - 1)) * (10 ** 11) +\n          x.right * (10 ** 1) +\n          int(x.direction == \"F\")\n         )\n        )\n\n        segments_strings = [x.stringize(rnf_profile) for x in sorted_segments]\n\n        read_tuple_name = \"__\".join(\n            [\n                self.prefix,\n                format(self.read_tuple_id, 'x').zfill(rnf_profile.read_tuple_id_width),\n                \",\".join(segments_strings),\n                self.suffix,\n            ]\n        )\n\n        return read_tuple_name"}, {"instruction": "def shellfilter(value):\n    \"\"\"Replace HTML chars for shell usage.\"\"\"\n", "input": "", "output": "    replacements = {'\\\\': '\\\\\\\\',\n                    '`': '\\\\`',\n                    \"'\": \"\\\\'\",\n                    '\"': '\\\\\"'}\n    for search, repl in replacements.items():\n        value = value.replace(search, repl)\n    return safestring.mark_safe(value)"}, {"instruction": "def download(odir: Path, source_url: str, irng: Sequence[int]):\n    \"\"\"Download star index files.\n    The default range was useful for my cameras.\n    \"\"\"\n", "input": "", "output": "    assert len(irng) == 2, 'specify start, stop indices'\n\n    odir = Path(odir).expanduser()\n    odir.mkdir(parents=True, exist_ok=True)\n\n    ri = int(source_url.split('/')[-2][:2])\n\n    for i in range(*irng):\n        fn = f'index-{ri:2d}{i:02d}.fits'\n        url = f'{source_url}{fn}'\n        ofn = odir / fn\n        if ofn.is_file():  # no clobber\n            print('skipping', ofn)\n            continue\n        print(f'{url} => {ofn}', end='\\r')\n\n        urlretrieve(url, ofn)"}, {"instruction": "def negotiate_safe(self, name, params):\n        \"\"\"\n        `name` and `params` are sent in the HTTP request by the client. Check\n        if the extension name is supported by this extension, and validate the\n        parameters. Returns a dict with accepted parameters, or None if not\n        accepted.\n        \"\"\"\n", "input": "", "output": "        for param in params.iterkeys():\n            if param not in self.defaults:\n                return\n\n        try:\n            return dict(self.negotiate(name, params))\n        except (KeyError, ValueError, AssertionError):\n            pass"}, {"instruction": "def erase(self, **kwargs):\n        \"\"\"Erase the job (remove job artifacts and trace).\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabJobEraseError: If the job could not be erased\n        \"\"\"\n", "input": "", "output": "        path = '%s/%s/erase' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path)"}, {"instruction": "def _get_ds_descriptions_unsorted(\n            cls, data, ignore_keys=['attrs', 'plotter'], nums=None):\n        \"\"\"Recursive method to get all the file names or datasets out of a\n        dictionary `data` created with the :meth`array_info` method\"\"\"\n", "input": "", "output": "        ds_description = {'ds', 'fname', 'num', 'arr', 'store'}\n        if 'ds' in data:\n            # make sure that the data set has a number assigned to it\n            data['ds'].psy.num\n        keys_in_data = ds_description.intersection(data)\n        if keys_in_data:\n            return {key: data[key] for key in keys_in_data}\n        for key in ignore_keys:\n            data.pop(key, None)\n        func = partial(cls._get_ds_descriptions_unsorted,\n                       ignore_keys=ignore_keys, nums=nums)\n        return chain(*map(lambda d: [d] if isinstance(d, dict) else d,\n                          map(func, six.itervalues(data))))"}, {"instruction": "def group_concat(arg, sep=',', where=None):\n    \"\"\"\n    Concatenate values using the indicated separator (comma by default) to\n    produce a string\n\n    Parameters\n    ----------\n    arg : array expression\n    sep : string, default ','\n    where : bool, default None\n\n    Returns\n    -------\n    concatenated : string scalar\n    \"\"\"\n", "input": "", "output": "    return ops.GroupConcat(arg, sep, where).to_expr()"}, {"instruction": "def add(self, *constraints: Tuple[Bool]) -> None:\n        \"\"\"Adds the constraints to this solver.\n\n        :param constraints: constraints to add\n        \"\"\"\n", "input": "", "output": "        raw_constraints = [\n            c.raw for c in cast(Tuple[Bool], constraints)\n        ]  # type: List[z3.BoolRef]\n        self.constraints.extend(raw_constraints)"}, {"instruction": "def from_base(cls, base, repo):\n        \"\"\"\n        Create a :class:`DXF` object which uses the same host, settings and\n        session as an existing :class:`DXFBase` object.\n\n        :param base: Existing :class:`DXFBase` object.\n        :type base: :class:`DXFBase`\n\n        :param repo: Name of the repository to access on the registry. Typically this is of the form ``username/reponame`` but for your own registries you don't actually have to stick to that.\n        :type repo: str\n\n        :returns: :class:`DXF` object which shares configuration and session with ``base`` but which can also be used to operate on the ``repo`` repository.\n        :rtype: :class:`DXF`\n        \"\"\"\n", "input": "", "output": "        # pylint: disable=protected-access\n        r = cls(base._host, repo, base._auth, base._insecure, base._auth_host, base._tlsverify)\n        r._token = base._token\n        r._headers = base._headers\n        r._sessions = [base._sessions[0]]\n        return r"}, {"instruction": "def is_node(objecttype):\n    \"\"\"\n    Check if the given objecttype has Node as an interface\n    \"\"\"\n", "input": "", "output": "    if not isclass(objecttype):\n        return False\n\n    if not issubclass(objecttype, ObjectType):\n        return False\n\n    for i in objecttype._meta.interfaces:\n        if issubclass(i, Node):\n            return True\n\n    return False"}, {"instruction": "def shp2geom(shp_fn):\n    \"\"\"Extract geometries from input shapefile\n    \n    Need to handle multi-part geom: http://osgeo-org.1560.x6.nabble.com/Multipart-to-singlepart-td3746767.html\n    \"\"\"\n", "input": "", "output": "    ds = ogr.Open(shp_fn)\n    lyr = ds.GetLayer()\n    srs = lyr.GetSpatialRef()\n    lyr.ResetReading()\n    geom_list = []\n    for feat in lyr:\n        geom = feat.GetGeometryRef()\n        geom.AssignSpatialReference(srs)\n        #Duplicate the geometry, or segfault\n        #See: http://trac.osgeo.org/gdal/wiki/PythonGotchas\n        #g = ogr.CreateGeometryFromWkt(geom.ExportToWkt())\n        #g.AssignSpatialReference(srs)\n        g = geom_dup(geom)\n        geom_list.append(g)\n    #geom = ogr.ForceToPolygon(' '.join(geom_list))    \n    #Dissolve should convert multipolygon to single polygon \n    #return geom_list[0]\n    ds = None\n    return geom_list"}, {"instruction": "def is_whitelisted(self, addrinfo):\n        \"\"\"\n        Returns if a result of ``socket.getaddrinfo`` is in the socket address\n        whitelist.\n        \"\"\"\n", "input": "", "output": "        # For details about the ``getaddrinfo`` struct, see the Python docs:\n        # http://docs.python.org/library/socket.html#socket.getaddrinfo\n        family, socktype, proto, canonname, sockaddr = addrinfo\n        address, port = sockaddr[:2]\n        return address in self.socket_address_whitelist"}, {"instruction": "def close(self, cancelled=False):\n        \"\"\"\n        Close this temporary pop-up.\n\n        :param cancelled: Whether the pop-up was cancelled (e.g. by pressing Esc).\n        \"\"\"\n", "input": "", "output": "        self._on_close(cancelled)\n        self._scene.remove_effect(self)"}, {"instruction": "def cancelMarketDepth(self, contracts=None):\n        \"\"\"\n        Cancel streaming market data for contract\n        https://www.interactivebrokers.com/en/software/api/apiguide/java/cancelmktdepth.htm\n        \"\"\"\n", "input": "", "output": "        if contracts == None:\n            contracts = list(self.contracts.values())\n        elif not isinstance(contracts, list):\n            contracts = [contracts]\n\n        for contract in contracts:\n            tickerId = self.tickerId(self.contractString(contract))\n            self.ibConn.cancelMktDepth(tickerId=tickerId)"}, {"instruction": "def __interact_writen(self, fd, data):\n        '''This is used by the interact() method.\n        '''\n", "input": "", "output": "\n        while data != b'' and self.isalive():\n            n = os.write(fd, data)\n            data = data[n:]"}, {"instruction": "def json_api_call(req_function):\n    \"\"\" Wrap a view-like function that returns an object that\n        is convertable from json\n    \"\"\"\n", "input": "", "output": "    @wraps(req_function)\n    def newreq(request, *args, **kwargs):\n        outp = req_function(request, *args, **kwargs)\n        if issubclass(outp.__class__, HttpResponse):\n            return outp\n        else:\n            return '%s' % json.dumps(outp, cls=LazyEncoder)\n    return string_to_response(\"application/json\")(newreq)"}, {"instruction": "def cares_about(self, delta):\n        \"\"\"Return True if this observer \"cares about\" (i.e. wants to be\n        called) for a this delta.\n\n        \"\"\"\n", "input": "", "output": "        if (self.entity_id and delta.get_id() and\n                not re.match(self.entity_id, str(delta.get_id()))):\n            return False\n\n        if self.entity_type and self.entity_type != delta.entity:\n            return False\n\n        if self.action and self.action != delta.type:\n            return False\n\n        if self.predicate and not self.predicate(delta):\n            return False\n\n        return True"}, {"instruction": "def _api_delete(path, data, server=None):\n    '''\n    Do a DELETE request to the API\n    '''\n", "input": "", "output": "    server = _get_server(server)\n    response = requests.delete(\n            url=_get_url(server['ssl'], server['url'], server['port'], path),\n            auth=_get_auth(server['user'], server['password']),\n            headers=_get_headers(),\n            params=data,\n            verify=False\n    )\n    return _api_response(response)"}, {"instruction": "def get_raw(self, name=None):\n        '''Shortcut for getting a :class:`~statsd.raw.Raw` instance\n\n        :keyword name: See :func:`~statsd.client.Client.get_client`\n        :type name: str\n        '''\n", "input": "", "output": "        return self.get_client(name=name, class_=statsd.Raw)"}, {"instruction": "def get_instance(self, payload):\n        \"\"\"\n        Build an instance of AllTimeInstance\n\n        :param dict payload: Payload response from the API\n\n        :returns: twilio.rest.api.v2010.account.usage.record.all_time.AllTimeInstance\n        :rtype: twilio.rest.api.v2010.account.usage.record.all_time.AllTimeInstance\n        \"\"\"\n", "input": "", "output": "        return AllTimeInstance(self._version, payload, account_sid=self._solution['account_sid'], )"}, {"instruction": "def print_stream(file, name):\n    \"\"\"Print stream from file to logger.\"\"\"\n", "input": "", "output": "    logger = logging.getLogger('xenon.{}'.format(name))\n    for line in file:\n        logger.info('[{}] {}'.format(name, line.strip()))"}, {"instruction": "def paths_from_iddname(iddname):\n    \"\"\"Get the EnergyPlus install directory and executable path.\n\n    Parameters\n    ----------\n    iddname : str, optional\n        File path to the IDD.\n\n    Returns\n    -------\n    eplus_exe : str\n        Full path to the EnergyPlus executable.\n    eplus_home : str\n        Full path to the EnergyPlus install directory.\n\n    Raises\n    ------\n    AttributeError (TypeError on Windows)\n        If iddname does not have a directory component (e.g. if None).\n    ValueError\n        If eplus_exe is not a file.\n\n    \"\"\"\n", "input": "", "output": "    eplus_home = os.path.abspath(os.path.dirname(iddname))\n    if platform.system() == 'Windows':\n        eplus_exe = os.path.join(eplus_home, 'energyplus.exe')\n    elif platform.system() == \"Linux\":\n        eplus_exe = os.path.join(eplus_home, 'energyplus')\n    else:\n        eplus_exe = os.path.join(eplus_home, 'energyplus')\n    if not os.path.isfile(eplus_exe):\n        raise ValueError\n    return eplus_exe, eplus_home"}, {"instruction": "async def create_scene(self, room_id, name, color_id=0, icon_id=0):\n        \"\"\"Creates am empty scene.\n\n        Scenemembers need to be added after the scene has been created.\n\n        :returns: A json object including scene id.\n        \"\"\"\n", "input": "", "output": "        name = unicode_to_base64(name)\n        _data = {\n            \"scene\": {\n                ATTR_ROOM_ID: room_id,\n                ATTR_NAME: name,\n                ATTR_COLOR_ID: color_id,\n                ATTR_ICON_ID: icon_id,\n            }\n        }\n        _response = await self.request.post(self._base_path, data=_data)\n        return _response"}, {"instruction": "def all_inspections(obj):\n    \"\"\"\n    Generator to iterate all current Jishaku inspections.\n    \"\"\"\n", "input": "", "output": "\n    for name, callback in INSPECTIONS:\n        result = callback(obj)\n        if result:\n            yield name, result"}, {"instruction": "def _make_ntgrid(grid):\n    \"\"\"make a named tuple grid\n\n    [[\"\",  \"a b\", \"b c\", \"c d\"],\n     [\"x y\", 1,     2,     3 ],\n     [\"y z\", 4,     5,     6 ],\n     [\"z z\", 7,     8,     9 ],]\n    will return\n    ntcol(x_y=ntrow(a_b=1, b_c=2, c_d=3),\n          y_z=ntrow(a_b=4, b_c=5, c_d=6),\n          z_z=ntrow(a_b=7, b_c=8, c_d=9))\"\"\"\n", "input": "", "output": "    hnames = [_nospace(n) for n in grid[0][1:]]\n    vnames = [_nospace(row[0]) for row in grid[1:]]\n    vnames_s = \" \".join(vnames)\n    hnames_s = \" \".join(hnames)\n    ntcol = collections.namedtuple('ntcol', vnames_s)\n    ntrow = collections.namedtuple('ntrow', hnames_s)\n    rdict = [dict(list(zip(hnames, row[1:]))) for row in grid[1:]]\n    ntrows = [ntrow(**rdict[i]) for i, name in enumerate(vnames)]\n    ntcols = ntcol(**dict(list(zip(vnames, ntrows))))\n    return ntcols"}, {"instruction": "def get_changeform_initial_data(self, request):\n        \"\"\"\n        Provide initial datas when creating an entry.\n        \"\"\"\n", "input": "", "output": "        get_data = super(EntryAdmin, self).get_changeform_initial_data(request)\n        return get_data or {\n            'sites': [Site.objects.get_current().pk],\n            'authors': [request.user.pk]\n        }"}, {"instruction": "def write(content, filename='cache'):\n\t\"\"\" write data to cache file\n\tparameters:\n\t\tcache_path - path to cache file\n\t\tcontent - a data structure to save into cache file\"\"\"\n", "input": "", "output": "\tcache_path = get_cache_path(filename)\n\twith open(cache_path, 'w') as file:\n\t\tif content is not None:\n\t\t\tjson.dump(content, file, indent=3, sort_keys=True)"}, {"instruction": "def delete_cors(Bucket,\n           region=None, key=None, keyid=None, profile=None):\n    '''\n    Delete the CORS configuration for the given bucket\n\n    Returns {deleted: true} if CORS was deleted and returns\n    {deleted: False} if CORS was not deleted.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt myminion boto_s3_bucket.delete_cors my_bucket\n\n    '''\n", "input": "", "output": "\n    try:\n        conn = _get_conn(region=region, key=key, keyid=keyid, profile=profile)\n        conn.delete_bucket_cors(Bucket=Bucket)\n        return {'deleted': True, 'name': Bucket}\n    except ClientError as e:\n        return {'deleted': False, 'error': __utils__['boto3.get_error'](e)}"}, {"instruction": "def AsIter(arg):\n  \"\"\"Encapsulates an argument in a tuple, if it's not already iterable.\"\"\"\n", "input": "", "output": "  if isinstance(arg, string_types):\n    rslt = [arg]\n  elif isinstance(arg, collections.Iterable):\n    rslt = arg\n  elif not arg:\n    rslt = []\n  else:\n    rslt = [arg]\n  return tuple(rslt)"}, {"instruction": "def short_hash(*buffers):\n    \"\"\"\n    :param buffer: a binary buffer (e.g. serialized blob)\n    :return: the first 8 characters of base64 ASCII rendition SHA-1\n    \"\"\"\n", "input": "", "output": "    hashed = hashlib.sha1()\n    for buffer in buffers:\n        hashed.update(buffer)\n    return to_ascii(hashed.digest())[:8]"}, {"instruction": "def defaultFile(self):\n        \"\"\"Produce a reasonable default.\"\"\"\n", "input": "", "output": "        filename = self.curframe.f_code.co_filename\n        if filename == '<string>' and self.mainpyfile:\n            filename = self.mainpyfile\n        return filename"}, {"instruction": "def flush_stream_threads(process, out_formatter=None,\n                                  err_formatter=terminal.fg.red, size=1):\n    \"\"\"\n    Context manager that creates 2 threads, one for each standard\n    stream (stdout/stderr), updating in realtime the piped data.\n    The formatters are callables that receives manipulates the data,\n    e.g. coloring it before writing to a ``sys`` stream. See\n    ``FlushStreamThread`` for more information.\n    \"\"\"\n", "input": "", "output": "    out = FlushStreamThread(process=process, stream_name=\"stdout\",\n                            formatter=out_formatter, size=size)\n    err = FlushStreamThread(process=process, stream_name=\"stderr\",\n                            formatter=err_formatter, size=size)\n    out.start()\n    err.start()\n    yield out, err\n    out.join()\n    err.join()"}, {"instruction": "def get_tunnel_info_output_tunnel_has_conflicts(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_tunnel_info = ET.Element(\"get_tunnel_info\")\n        config = get_tunnel_info\n        output = ET.SubElement(get_tunnel_info, \"output\")\n        tunnel = ET.SubElement(output, \"tunnel\")\n        has_conflicts = ET.SubElement(tunnel, \"has-conflicts\")\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def rho2sigma(self, rho0, Ra, Rs):\n        \"\"\"\n        converts 3d density into 2d projected density parameter\n        :param rho0:\n        :param Ra:\n        :param Rs:\n        :return:\n        \"\"\"\n", "input": "", "output": "        return np.pi * rho0 * Ra * Rs / (Rs + Ra)"}, {"instruction": "def ints2str(self, int_values):\n    \"\"\"Conversion list[int] => decoded string.\"\"\"\n", "input": "", "output": "    if not self._encoder:\n      raise ValueError(\n          \"Text.ints2str is not available because encoder hasn't been defined.\")\n    return self._encoder.decode(int_values)"}, {"instruction": "def to_unicode(text):\n    \"\"\"\n    Return *text* as a unicode string. All text in Python 3 is unicode, so\n    this just returns *text* unchanged.\n    \"\"\"\n", "input": "", "output": "    if not isinstance(text, str):\n        tmpl = 'expected unicode string, got %s value %s'\n        raise TypeError(tmpl % (type(text), text))\n    return text"}, {"instruction": "def getResourceMapPid(self):\n        \"\"\"Returns:\n\n        str : PID of the Resource Map itself.\n\n        \"\"\"\n", "input": "", "output": "        ore = [\n            o for o in self.subjects(predicate=rdflib.RDF.type, object=ORE.ResourceMap)\n        ][0]\n        pid = [str(o) for o in self.objects(predicate=DCTERMS.identifier, subject=ore)][\n            0\n        ]\n        return pid"}, {"instruction": "def _symbol_bottom_simple(x, model_hparams, vocab_size, name, reuse):\n  \"\"\"Bottom transformation for symbols.\"\"\"\n", "input": "", "output": "  with tf.variable_scope(name, reuse=reuse):\n    # Ensure the inputs are 3-D\n    if len(x.get_shape()) == 4:\n      x = tf.squeeze(x, axis=3)\n    while len(x.get_shape()) < 3:\n      x = tf.expand_dims(x, axis=-1)\n\n    var = get_weights(model_hparams, vocab_size)\n    x = common_layers.dropout_no_scaling(\n        x, 1.0 - model_hparams.symbol_dropout)\n    ret = common_layers.gather(var, x)\n    if model_hparams.multiply_embedding_mode == \"sqrt_depth\":\n      ret *= model_hparams.hidden_size**0.5\n    ret *= tf.expand_dims(\n        common_layers.cast_like(tf.not_equal(x, 0), ret), -1)\n    return ret"}, {"instruction": "def dkim_sign(message, dkim_domain=None, dkim_key=None, dkim_selector=None, dkim_headers=None):\n    \"\"\"Return signed email message if dkim package and settings are available.\"\"\"\n", "input": "", "output": "    try:\n        import dkim\n    except ImportError:\n        pass\n    else:\n        if dkim_domain and dkim_key:\n            sig = dkim.sign(message,\n                            dkim_selector,\n                            dkim_domain,\n                            dkim_key,\n                            include_headers=dkim_headers)\n            message = sig + message\n    return message"}, {"instruction": "def _is_leonardo_module(whatever):\n    '''check if is leonardo module'''\n", "input": "", "output": "\n    # check if is python module\n    if hasattr(whatever, 'default') \\\n            or hasattr(whatever, 'leonardo_module_conf'):\n        return True\n\n    # check if is python object\n    for key in dir(whatever):\n        if 'LEONARDO' in key:\n            return True"}, {"instruction": "def update(self, td):\r\n        \"\"\"Update state of ball\"\"\"\n", "input": "", "output": "        self.sprite.last_position = self.sprite.position\r\n        self.sprite.last_velocity = self.sprite.velocity\r\n        if self.particle_group != None:\r\n            self.update_particle_group(td)"}, {"instruction": "def entityId(self, partial, channel=None):\n        '''Get an entity's full id provided a partial one.\n\n        Raises EntityNotFound if partial cannot be resolved.\n        @param partial The partial id (e.g. mysql, precise/mysql).\n        @param channel Optional channel name.\n        '''\n", "input": "", "output": "        url = '{}/{}/meta/any'.format(self.url, _get_path(partial))\n        data = self._get(_add_channel(url, channel))\n        return data.json()['Id']"}, {"instruction": "def hashes(self):\n        \"\"\"Hashes of all possible permutations of the URL in canonical form\"\"\"\n", "input": "", "output": "        for url_variant in self.url_permutations(self.canonical):\n            url_hash = self.digest(url_variant)\n            yield url_hash"}, {"instruction": "def dedent(content):\n    \"\"\"\n    Remove leading indent from a block of text.\n    Used when generating descriptions from docstrings.\n\n    Note that python's `textwrap.dedent` doesn't quite cut it,\n    as it fails to dedent multiline docstrings that include\n    unindented text on the initial line.\n    \"\"\"\n", "input": "", "output": "    content = force_text(content)\n    whitespace_counts = [len(line) - len(line.lstrip(' '))\n                         for line in content.splitlines()[1:] if line.lstrip()]\n\n    # unindent the content if needed\n    if whitespace_counts:\n        whitespace_pattern = '^' + (' ' * min(whitespace_counts))\n        content = re.sub(re.compile(whitespace_pattern, re.MULTILINE), '', content)\n\n    return content.strip()"}, {"instruction": "def camelcase_to_slash(name):\n    \"\"\" Converts CamelCase to camel/case\n\n    code ripped from http://stackoverflow.com/questions/1175208/does-the-python-standard-library-have-function-to-convert-camelcase-to-camel-cas\n    \"\"\"\n", "input": "", "output": "\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1/\\2', name)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1/\\2', s1).lower()"}, {"instruction": "def logger_has_handlers(logger):\n    \"\"\"\n    Check if given logger has at least 1 handler associated, return a boolean value.\n\n    Since Python 2 doesn't provide Logger.hasHandlers(), we have to perform the lookup by ourself.\n    \"\"\"\n", "input": "", "output": "    if six.PY3:\n        return logger.hasHandlers()\n    else:\n        c = logger\n        rv = False\n        while c:\n            if c.handlers:\n                rv = True\n                break\n            if not c.propagate:\n                break\n            else:\n                c = c.parent\n        return rv"}, {"instruction": "def get_kernel_spec(self, kernel_name):\n        \"\"\"Returns a :class:`KernelSpec` instance for the given kernel_name.\n\n        Raises :exc:`NoSuchKernel` if the given kernel name is not found.\n        \"\"\"\n", "input": "", "output": "        try:\n            return super(EnvironmentKernelSpecManager,\n                         self).get_kernel_spec(kernel_name)\n        except (NoSuchKernel, FileNotFoundError):\n            venv_kernel_name = kernel_name.lower()\n            specs = self.get_all_kernel_specs_for_envs()\n            if venv_kernel_name in specs:\n                return specs[venv_kernel_name]\n            else:\n                raise NoSuchKernel(kernel_name)"}, {"instruction": "def _cb_inform_interface_change(self, msg):\n        \"\"\"Update the sensors and requests available.\"\"\"\n", "input": "", "output": "        self._logger.debug('cb_inform_interface_change(%s)', msg)\n        self._interface_changed.set()"}, {"instruction": "def get_vnetwork_dvpgs_input_vcenter(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_vnetwork_dvpgs = ET.Element(\"get_vnetwork_dvpgs\")\n        config = get_vnetwork_dvpgs\n        input = ET.SubElement(get_vnetwork_dvpgs, \"input\")\n        vcenter = ET.SubElement(input, \"vcenter\")\n        vcenter.text = kwargs.pop('vcenter')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def do_preferrep(self, line):\n        \"\"\"preferrep <member node> [member node ...] Add one or more preferred Member\n        Nodes to replication policy.\"\"\"\n", "input": "", "output": "        mns = self._split_args(line, 1, -1)\n        self._command_processor.get_session().get_replication_policy().add_preferred(\n            mns\n        )\n        self._print_info_if_verbose(\n            \"Set {} as preferred replication target(s)\".format(\", \".join(mns))\n        )"}, {"instruction": "def _add_chrome_proxy_extension(\n        chrome_options, proxy_string, proxy_user, proxy_pass):\n    \"\"\" Implementation of https://stackoverflow.com/a/35293284 for\n        https://stackoverflow.com/questions/12848327/\n        (Run Selenium on a proxy server that requires authentication.) \"\"\"\n", "input": "", "output": "    if not \"\".join(sys.argv) == \"-c\":\n        # Single-threaded\n        proxy_helper.create_proxy_zip(proxy_string, proxy_user, proxy_pass)\n    else:\n        # Pytest multi-threaded test\n        lock = threading.Lock()\n        with lock:\n            time.sleep(random.uniform(0.02, 0.15))\n            if not os.path.exists(PROXY_ZIP_PATH):\n                proxy_helper.create_proxy_zip(\n                    proxy_string, proxy_user, proxy_pass)\n            time.sleep(random.uniform(0.1, 0.2))\n    proxy_zip = PROXY_ZIP_PATH\n    if not os.path.exists(PROXY_ZIP_PATH):\n        # Handle \"Permission denied\" on the default proxy.zip path\n        proxy_zip = PROXY_ZIP_PATH_2\n    chrome_options.add_extension(proxy_zip)\n    return chrome_options"}, {"instruction": "def depsOf_of_mirteFile_module_definition(defs):\n    \"\"\" Returns a function that returns the dependencies of a module\n        definition by its name, where defs is a dictionary of module\n        definitions from a mirteFile \"\"\"\n", "input": "", "output": "    return lambda x: (list(filter(lambda z: z is not None and z in defs,\n                             map(lambda y: y[1].get('type'),\n                                 six.iteritems(defs[x]['settings'])\n                                 if 'settings' in defs[x] else [])))) + \\\n        (list(defs[x]['inherits']) if 'inherits' in defs[x] else [])"}, {"instruction": "def get_cmdclass():\n    \"\"\" DEPRICATE \"\"\"\n", "input": "", "output": "    try:\n        from Cython.Distutils import build_ext\n        cmdclass = {'build_ext': build_ext}\n        return cmdclass\n    except Exception as ex:\n        print(ex)\n        print('WARNING: Cython is not installed. This is only a problem if you are building C extensions')\n        return {}"}, {"instruction": "def input_validate_nonce(nonce, name='nonce', pad = False):\n    \"\"\" Input validation for nonces. \"\"\"\n", "input": "", "output": "    if type(nonce) is not str:\n        raise pyhsm.exception.YHSM_WrongInputType( \\\n            name, str, type(nonce))\n    if len(nonce) > pyhsm.defines.YSM_AEAD_NONCE_SIZE:\n        raise pyhsm.exception.YHSM_InputTooLong(\n            name, pyhsm.defines.YSM_AEAD_NONCE_SIZE, len(nonce))\n    if pad:\n        return nonce.ljust(pyhsm.defines.YSM_AEAD_NONCE_SIZE, chr(0x0))\n    else:\n        return nonce"}, {"instruction": "def config_dict(config):\n    \"\"\"\n    Given a Sphinx config object, return a dictionary of config\n    values.\n    \"\"\"\n", "input": "", "output": "    return dict(\n        (key, getattr(config, key))\n        for key in config.values\n    )"}, {"instruction": "def args(self, args):\n        '''Set additional arguments to be passed to the fitness function\n\n        Args:\n            args (dict): additional arguments\n        '''\n", "input": "", "output": "        self._args = args\n        self._logger.log('debug', 'Args set to {}'.format(args))"}, {"instruction": "def cli(ctx, id_number, new_value):\n    \"\"\"Update a status name\n\nOutput:\n\n    an empty dictionary\n    \"\"\"\n", "input": "", "output": "    return ctx.gi.status.update_status(id_number, new_value)"}, {"instruction": "def modifie_options(self, field_option, value):\n        \"\"\"Set options in modifications.\n        All options will be stored since it should be grouped in the DB.\"\"\"\n", "input": "", "output": "        options = dict(self[\"options\"] or {}, **{field_option: value})\n        self.modifications[\"options\"] = options"}, {"instruction": "def execute_ping(host_list, remote_user, remote_pass,\n                 sudo=False, sudo_user=None, sudo_pass=None):\n    '''\n    Execute ls on some hosts\n    '''\n", "input": "", "output": "    runner = spam.ansirunner.AnsibleRunner()\n    result, failed_hosts = runner.ansible_perform_operation(\n        host_list=host_list,\n        remote_user=remote_user,\n        remote_pass=remote_pass,\n        sudo=sudo,\n        sudo_pass=sudo_pass,\n        sudo_user=sudo_user,\n        module=\"ping\")\n\n    print result, failed_hosts\n    dark_hosts = runner.ansible_get_dark_hosts(result)\n    print \"dark hosts: \", dark_hosts"}, {"instruction": "def getHeader(self):\n        \"\"\"\n        Returns the file header as dict\n\n        Parameters\n        ----------\n        None\n        \"\"\"\n", "input": "", "output": "        return {\"technician\": self.getTechnician(), \"recording_additional\": self.getRecordingAdditional(),\n                \"patientname\": self.getPatientName(), \"patient_additional\": self.getPatientAdditional(),\n                \"patientcode\": self.getPatientCode(), \"equipment\": self.getEquipment(),\n                \"admincode\": self.getAdmincode(), \"gender\": self.getGender(), \"startdate\": self.getStartdatetime(),\n                \"birthdate\": self.getBirthdate()}"}, {"instruction": "def get_ip(request):\n    \"\"\"Return the IP address inside the HTTP_X_FORWARDED_FOR var inside\n    the `request` object.\n\n    The return of this function can be overrided by the\n    `LOCAL_GEOLOCATION_IP` variable in the `conf` module.\n\n    This function will skip local IPs (starting with 10. and equals to\n    127.0.0.1).\n    \"\"\"\n", "input": "", "output": "    if getsetting('LOCAL_GEOLOCATION_IP'):\n        return getsetting('LOCAL_GEOLOCATION_IP')\n\n    forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')\n\n    if not forwarded_for:\n        return UNKNOWN_IP\n\n    for ip in forwarded_for.split(','):\n        ip = ip.strip()\n        if not ip.startswith('10.') and not ip == '127.0.0.1':\n            return ip\n\n    return UNKNOWN_IP"}, {"instruction": "def reset_network(roles, extra_vars=None):\n    \"\"\"Reset the network constraints (latency, bandwidth ...)\n\n    Remove any filter that have been applied to shape the traffic.\n\n    Args:\n        roles (dict): role->hosts mapping as returned by\n            :py:meth:`enoslib.infra.provider.Provider.init`\n        inventory (str): path to the inventory\n    \"\"\"\n", "input": "", "output": "    logger.debug('Reset the constraints')\n\n    if not extra_vars:\n        extra_vars = {}\n\n    tmpdir = os.path.join(os.getcwd(), TMP_DIRNAME)\n\n    _check_tmpdir(tmpdir)\n    utils_playbook = os.path.join(ANSIBLE_DIR, 'utils.yml')\n    options = {'enos_action': 'tc_reset',\n               'tc_output_dir': tmpdir}\n    options.update(extra_vars)\n    run_ansible([utils_playbook], roles=roles, extra_vars=options)"}, {"instruction": "def use_value(self, value):\n        \"\"\"Converts value to field type or use original\"\"\"\n", "input": "", "output": "        if self.check_value(value):\n            return value\n        return self.convert_value(value)"}, {"instruction": "def get_country_info_from_iso3(cls, iso3, use_live=True, exception=None):\n        # type: (str, bool, Optional[ExceptionUpperBound]) -> Optional[Dict[str]]\n        \"\"\"Get country information from ISO3 code\n\n        Args:\n            iso3 (str): ISO3 code for which to get country information\n            use_live (bool): Try to get use latest data from web rather than file in package. Defaults to True.\n            exception (Optional[ExceptionUpperBound]): An exception to raise if country not found. Defaults to None.\n\n        Returns:\n            Optional[Dict[str]]: country information\n        \"\"\"\n", "input": "", "output": "        countriesdata = cls.countriesdata(use_live=use_live)\n        country = countriesdata['countries'].get(iso3.upper())\n        if country is not None:\n            return country\n\n        if exception is not None:\n            raise exception\n        return None"}, {"instruction": "def make_k8s_lando_router(config, obj, queue_name):\n        \"\"\"\n        Makes MessageRouter which can listen to queue_name sending messages to the k8s version of lando.\n        :param config: WorkerConfig/ServerConfig: settings for connecting to the queue\n        :param obj: object: implements lando specific methods\n        :param queue_name: str: name of the queue we will listen on.\n        :return MessageRouter\n        \"\"\"\n", "input": "", "output": "        return MessageRouter(config, obj, queue_name, K8S_LANDO_INCOMING_MESSAGES,\n                             processor_constructor=WorkQueueProcessor)"}, {"instruction": "def resource_schema(raml_resource):\n    \"\"\" Get schema properties of RAML resource :raml_resource:.\n\n    Must be called with RAML resource that defines body schema. First\n    body that defines schema is used. Schema is converted on return using\n    'convert_schema'.\n\n    :param raml_resource: Instance of ramlfications.raml.ResourceNode of\n        POST method.\n    \"\"\"\n", "input": "", "output": "    # NOTE: Must be called with resource that defines body schema\n    log.info('Searching for model schema')\n    if not raml_resource.body:\n        raise ValueError('RAML resource has no body to setup database '\n                         'schema from')\n\n    for body in raml_resource.body:\n        if body.schema:\n            return convert_schema(body.schema, body.mime_type)\n    log.debug('No model schema found.')"}, {"instruction": "def close(self, code=None):\n        '''return a `close` :class:`Frame`.\n        '''\n", "input": "", "output": "        code = code or 1000\n        body = pack('!H', code)\n        body += self._close_codes.get(code, '').encode('utf-8')\n        return self.encode(body, opcode=0x8)"}, {"instruction": "def _get_method(self, rdata):\n        \"\"\"\n        Returns jsonrpc request's method value.\n\n        InvalidRequestError will be raised if it's missing or is wrong type.\n        MethodNotFoundError will be raised if a method with given method name\n        does not exist.\n        \"\"\"\n", "input": "", "output": "        if 'method' in rdata:\n            if not isinstance(rdata['method'], basestring):\n                raise InvalidRequestError\n        else:\n            raise InvalidRequestError\n\n        if rdata['method'] not in self.method_data.keys():\n            raise MethodNotFoundError\n\n        return rdata['method']"}, {"instruction": "def toString(self):\n        \"\"\"\n        Returns the network layers as a string.\n        \"\"\"\n", "input": "", "output": "        output = \"\"\n        for layer in reverse(self.layers):\n            output += layer.toString()\n        return output"}, {"instruction": "def contour(c, subsample=1, size=10, color='g'):\n        \"\"\" Draws a contour on the current plot by scattering points.\n\n        Parameters\n        ----------\n        c : :obj:`autolab_core.Contour`\n            contour to draw\n        subsample : int\n            subsample rate for boundary pixels\n        size : int\n            size of scattered points\n        color : :obj:`str`\n            color of box\n        \"\"\"\n", "input": "", "output": "        if not isinstance(c, Contour):\n            raise ValueError('Input must be of type Contour')\n            \n        for i in range(c.num_pixels)[0::subsample]:\n            plt.scatter(c.boundary_pixels[i,1], c.boundary_pixels[i,0], s=size, c=color)"}, {"instruction": "def get_as_list(self, tag_name):\n        \"\"\"\n        Return the value of a tag, making sure that it's a list.  Absent\n        tags are returned as an empty-list; single tags are returned as a\n        one-element list.\n\n        The returned list is a copy, and modifications do not affect the\n        original object.\n        \"\"\"\n", "input": "", "output": "        val = self.get(tag_name, [])\n        if isinstance(val, list):\n            return val[:]\n        else:\n            return [val]"}, {"instruction": "def _prompt_changer(attr, val):\n    \"\"\"Change the current prompt theme\"\"\"\n", "input": "", "output": "    try:\n        sys.ps1 = conf.color_theme.prompt(conf.prompt)\n    except Exception:\n        pass\n    try:\n        apply_ipython_style(get_ipython())\n    except NameError:\n        pass"}, {"instruction": "def disable_cors(self):\n        \"\"\"\n        Switches CORS off.\n\n        :returns: CORS status in JSON format\n        \"\"\"\n", "input": "", "output": "        return self.update_cors_configuration(\n            enable_cors=False,\n            allow_credentials=False,\n            origins=[],\n            overwrite_origins=True\n        )"}, {"instruction": "def blocked(self):\n        r\"\"\"Returns a :class:`list` of :class:`User`\\s that the user has blocked.\n\n        .. note::\n\n            This only applies to non-bot accounts.\n        \"\"\"\n", "input": "", "output": "        return [r.user for r in self._relationships.values() if r.type is RelationshipType.blocked]"}, {"instruction": "def filter(self, datax, datay):\n        \"\"\"Filter a set of datax and datay according to `self.points`\"\"\"\n", "input": "", "output": "        f = np.ones(datax.shape, dtype=bool)\n        for i, p in enumerate(zip(datax, datay)):\n            f[i] = PolygonFilter.point_in_poly(p, self.points)\n\n        if self.inverted:\n            np.invert(f, f)\n\n        return f"}, {"instruction": "def _iand(self, other):\n\t\t\"\"\"Set multiplicity of each element to the minimum of the two collections.\n\n\t\tif isinstance(other, _basebag):\n\t\t\tThis runs in O(other.num_unique_elements())\n\t\telse:\n\t\t\tThis runs in O(len(other))\n\t\t\"\"\"\n", "input": "", "output": "\t\t# TODO do we have to create a bag from the other first?\n\t\tif not isinstance(other, _basebag):\n\t\t\tother = self._from_iterable(other)\n\t\tfor elem, old_count in set(self.counts()):\n\t\t\tother_count = other.count(elem)\n\t\t\tnew_count = min(other_count, old_count)\n\t\t\tself._set_count(elem, new_count)\n\t\treturn self"}, {"instruction": "def diff(self, n=1):\n    \"\"\"\n    Differentiate (n-th derivative, where the default n is 1).\n    \"\"\"\n", "input": "", "output": "    d = self._data\n    for unused in xrange(n):\n      d = OrderedDict((k - 1, k * v) for k, v in iteritems(d) if k != 0)\n    return Poly(d, zero=self.zero)"}, {"instruction": "def vm_state(vm_=None, **kwargs):\n    '''\n    Return list of all the vms and their state.\n\n    If you pass a VM name in as an argument then it will return info\n    for just the named VM, otherwise it will return all VMs.\n\n    :param vm_: name of the domain\n    :param connection: libvirt connection URI, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param username: username to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n    :param password: password to connect with, overriding defaults\n\n        .. versionadded:: 2019.2.0\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.vm_state <domain>\n    '''\n", "input": "", "output": "    def _info(dom):\n        "}, {"instruction": "def status(self, job_ids):\n        \"\"\"Get the status of a list of jobs identified by their ids.\n\n        Parameters\n        ----------\n        job_ids : list of str\n            Identifiers for the jobs.\n\n        Returns\n        -------\n        list of int\n            Status codes for each requested job.\n        \"\"\"\n", "input": "", "output": "        states = []\n        statuses = self.deployer.get_vm_status([self.resources.get(job_id) for job_id in job_ids])\n        for status in statuses:\n            states.append(translate_table.get(status.state['Name'], \"PENDING\"))\n        return states"}, {"instruction": "def delete_webhook(self, id, **kwargs):  # noqa: E501\n        \"\"\"Delete a specific webhook  # noqa: E501\n\n          # noqa: E501\n        This method makes a synchronous HTTP request by default. To make an\n        asynchronous HTTP request, please pass async_req=True\n        >>> thread = api.delete_webhook(id, async_req=True)\n        >>> result = thread.get()\n\n        :param async_req bool\n        :param str id: (required)\n        :return: ResponseContainerNotificant\n                 If the method is called asynchronously,\n                 returns the request thread.\n        \"\"\"\n", "input": "", "output": "        kwargs['_return_http_data_only'] = True\n        if kwargs.get('async_req'):\n            return self.delete_webhook_with_http_info(id, **kwargs)  # noqa: E501\n        else:\n            (data) = self.delete_webhook_with_http_info(id, **kwargs)  # noqa: E501\n            return data"}, {"instruction": "def _check_pillar_exact_minions(self, expr, delimiter, greedy):\n        '''\n        Return the minions found by looking via pillar\n        '''\n", "input": "", "output": "        return self._check_cache_minions(expr,\n                                         delimiter,\n                                         greedy,\n                                         'pillar',\n                                         exact_match=True)"}, {"instruction": "def prepare(self, ansi='', ensure_trailing_newline=False):\n        \"\"\" Load the contents of 'ansi' into this object \"\"\"\n", "input": "", "output": "\n        body, styles = self.apply_regex(ansi)\n\n        if ensure_trailing_newline and _needs_extra_newline(body):\n            body += '\\n'\n\n        self._attrs = {\n            'dark_bg': self.dark_bg,\n            'line_wrap': self.line_wrap,\n            'font_size': self.font_size,\n            'body': body,\n            'styles': styles,\n        }\n\n        return self._attrs"}, {"instruction": "def Take(self: dict, n):\n    \"\"\"\n    [\n        {\n            'self': [1, 2, 3],\n            'n': 2,\n            'assert': lambda ret: list(ret)  == [1, 2]\n         }\n    ]\n    \"\"\"\n", "input": "", "output": "\n    for i, e in enumerate(self.items()):\n        if i == n:\n            break\n        yield e"}, {"instruction": "def __flush(self, async=True):\n        \"\"\" Flushes messages through current HttpRequest and closes it.\n            It assumes a current requesthandler and requires a lock\n            on self.lock \"\"\"\n", "input": "", "output": "        rh = self.rh\n        messages = list(self.messages)\n        stream_notices = list(self.stream_notices)\n        self.stream_notices = []\n        self.messages = []\n        args = (rh, messages, stream_notices)\n        if async:\n            self.hub.threadPool.execute_named(self.__inner_flush,\n                '%s __inner__flush' % self.hub.l.name, *args)\n        else:\n            self.__inner_flush(*args)\n        self.rh = None\n        self._set_timeout(int(time.time() + self.hub.timeout))"}, {"instruction": "def get_behaviors(brain_or_object):\n    \"\"\"Iterate over all behaviors that are assigned to the object\n\n    :param brain_or_object: A single catalog brain or content object\n    :type brain_or_object: ATContentType/DexterityContentType/CatalogBrain\n    :returns: Behaviors\n    :rtype: list\n    \"\"\"\n", "input": "", "output": "    obj = get_object(brain_or_object)\n    if not is_dexterity_content(obj):\n        fail(400, \"Only Dexterity contents can have assigned behaviors\")\n    assignable = IBehaviorAssignable(obj, None)\n    if not assignable:\n        return {}\n    out = {}\n    for behavior in assignable.enumerateBehaviors():\n        for name, field in getFields(behavior.interface).items():\n            out[name] = field\n    return out"}, {"instruction": "def iscsi_iqn():\n    '''\n    Return iSCSI IQN\n    '''\n", "input": "", "output": "    grains = {}\n    grains['iscsi_iqn'] = False\n    if salt.utils.platform.is_linux():\n        grains['iscsi_iqn'] = _linux_iqn()\n    elif salt.utils.platform.is_windows():\n        grains['iscsi_iqn'] = _windows_iqn()\n    elif salt.utils.platform.is_aix():\n        grains['iscsi_iqn'] = _aix_iqn()\n    return grains"}, {"instruction": "def get_lookup_value(self, value):\n        \"\"\"\n        Override this method to convert displayed values to lookup values\n        \"\"\"\n", "input": "", "output": "        choices = self._field_choices()\n        if choices:\n            if isinstance(value, list):\n                return [c[0] for c in choices if c[1] in value]\n            else:\n                for c in choices:\n                    if c[1] == value:\n                        return c[0]\n        return value"}, {"instruction": "def synchronized(wrapped):\n    \"\"\"The missing @synchronized decorator\n\n    https://git.io/vydTA\"\"\"\n", "input": "", "output": "    _lock = threading.RLock()\n\n    @functools.wraps(wrapped)\n    def _wrapper(*args, **kwargs):\n        with _lock:\n            return wrapped(*args, **kwargs)\n    return _wrapper"}, {"instruction": "def generate(env):\n    \"\"\"Add Builders and construction variables for the OS/2 to an Environment.\"\"\"\n", "input": "", "output": "    cc.generate(env)\n\n    env['CC']         = 'icc'\n    env['CCCOM']      = '$CC $CFLAGS $CCFLAGS $CPPFLAGS $_CPPDEFFLAGS $_CPPINCFLAGS /c $SOURCES /Fo$TARGET'\n    env['CXXCOM']     = '$CXX $CXXFLAGS $CPPFLAGS $_CPPDEFFLAGS $_CPPINCFLAGS /c $SOURCES /Fo$TARGET'\n    env['CPPDEFPREFIX']  = '/D'\n    env['CPPDEFSUFFIX']  = ''\n    env['INCPREFIX']  = '/I'\n    env['INCSUFFIX']  = ''\n    env['CFILESUFFIX'] = '.c'\n    env['CXXFILESUFFIX'] = '.cc'"}, {"instruction": "def get_mapping_variable(variable_name, variables_mapping):\n    \"\"\" get variable from variables_mapping.\n\n    Args:\n        variable_name (str): variable name\n        variables_mapping (dict): variables mapping\n\n    Returns:\n        mapping variable value.\n\n    Raises:\n        exceptions.VariableNotFound: variable is not found.\n\n    \"\"\"\n", "input": "", "output": "    try:\n        return variables_mapping[variable_name]\n    except KeyError:\n        raise exceptions.VariableNotFound(\"{} is not found.\".format(variable_name))"}, {"instruction": "def getargspec(fn):  # type: (Callable) -> inspect.ArgSpec\n    \"\"\"Get the names and default values of a function's arguments.\n\n    Args:\n        fn (function): a function\n\n    Returns:\n        `inspect.ArgSpec`:  A collections.namedtuple with the following attributes:\n\n            * Args:\n                args (list): a list of the argument names (it may contain nested lists).\n                varargs (str): name of the * argument or None.\n                keywords (str): names of the ** argument or None.\n                defaults (tuple): an n-tuple of the default values of the last n arguments.\n    \"\"\"\n", "input": "", "output": "    if six.PY2:\n        return inspect.getargspec(fn)\n    elif six.PY3:\n        full_arg_spec = inspect.getfullargspec(fn)\n        return inspect.ArgSpec(full_arg_spec.args, full_arg_spec.varargs, full_arg_spec.varkw, full_arg_spec.defaults)"}, {"instruction": "def view(self, filename=None, directory=None, cleanup=False):\n        \"\"\"Save the source to file, open the rendered result in a viewer.\n\n        Args:\n            filename: Filename for saving the source (defaults to ``name`` + ``'.gv'``)\n            directory: (Sub)directory for source saving and rendering.\n            cleanup (bool): Delete the source file after rendering.\n        Returns:\n            The (possibly relative) path of the rendered file.\n        Raises:\n            graphviz.ExecutableNotFound: If the Graphviz executable is not found.\n            subprocess.CalledProcessError: If the exit status is non-zero.\n            RuntimeError: If opening the viewer is not supported.\n\n        Short-cut method for calling :meth:`.render` with ``view=True``.\n        \"\"\"\n", "input": "", "output": "        return self.render(filename=filename, directory=directory, view=True,\n                           cleanup=cleanup)"}, {"instruction": "def Crawl(self, seed, seedClient=None, jobClient=None, rounds=1, index=True):\n        \"\"\"\n        Launch a crawl using the given seed\n        :param seed: Type (Seed or SeedList) - used for crawl\n        :param seedClient: if a SeedList is given, the SeedClient to upload, if None a default will be created\n        :param jobClient: the JobClient to be used, if None a default will be created\n        :param rounds: the number of rounds in the crawl\n        :return: a CrawlClient to monitor and control the crawl\n        \"\"\"\n", "input": "", "output": "        if seedClient is None:\n            seedClient = self.Seeds()\n        if jobClient is None:\n            jobClient = self.Jobs()\n\n        if type(seed) != Seed:\n            seed = seedClient.create(jobClient.crawlId + '_seeds', seed)\n        return CrawlClient(self.server, seed, jobClient, rounds, index)"}, {"instruction": "def copy(self):\n        \"\"\"\n        Returns a deep copy of the particle. The particle is not added to any simulation by default.\n        \"\"\"\n", "input": "", "output": "        np = Particle()\n        memmove(byref(np), byref(self), sizeof(self))\n        return np"}, {"instruction": "def get(feature, obj, **kwargs):\n    '''Obtain a feature from a set of morphology objects\n\n    Parameters:\n        feature(string): feature to extract\n        obj: a neuron, population or neurite tree\n        **kwargs: parameters to forward to underlying worker functions\n\n    Returns:\n        features as a 1D or 2D numpy array.\n\n    '''\n", "input": "", "output": "\n    feature = (NEURITEFEATURES[feature] if feature in NEURITEFEATURES\n               else NEURONFEATURES[feature])\n\n    return _np.array(list(feature(obj, **kwargs)))"}, {"instruction": "def WaitUntilDone(self, timeout=None):\n    \"\"\"Wait until the operation is done.\n\n    Args:\n      timeout: timeout in seconds. None means default timeout (1 hour).\n               0 means no timeout (wait forever).\n    Returns:\n      Operation object with refreshed target_file.\n    Raises:\n      PollTimeoutError: if timeout is reached.\n    \"\"\"\n", "input": "", "output": "\n    utils.Poll(\n        generator=self.GetState,\n        condition=lambda s: s != self.__class__.STATE_RUNNING,\n        timeout=timeout)\n    self.target_file = self.target_file.Get()\n    return self"}, {"instruction": "def setButtonText(self, which, text):\n        \"\"\"\n        Sets the display text for the inputed button to the given text.\n\n        :param      which | <XOverlayWizard.WizardButton>\n                    text  | <str>\n        \"\"\"\n", "input": "", "output": "        try:\n            self._buttons[which].setText(text)\n        except KeyError:\n            pass"}, {"instruction": "def _parse(cls, data, key=None):\n        \"\"\"\n        Parse a set of data to extract entity-only data.\n\n        Use classmethod `parse` if available, otherwise use the `endpoint`\n        class variable to extract data from a data blob.\n        \"\"\"\n", "input": "", "output": "        parse = cls.parse if cls.parse is not None else cls.get_endpoint()\n\n        if callable(parse):\n            data = parse(data)\n        elif isinstance(parse, str):\n            data = data[key]\n        else:\n            raise Exception('\"parse\" should be a callable or string got, {0}'\n                            .format(parse))\n        return data"}, {"instruction": "def cancel_order(self, order_id, stock):\n        \"\"\"Cancel An Order\n\n        https://starfighter.readme.io/docs/cancel-an-order\n        \"\"\"\n", "input": "", "output": "        url_fragment = 'venues/{venue}/stocks/{stock}/orders/{order_id}'.format(\n            venue=self.venue,\n            stock=stock,\n            order_id=order_id,\n        )\n        url = urljoin(self.base_url, url_fragment)\n        return self.session.delete(url).json()"}, {"instruction": "def windowing(self, windowing):\n        \"\"\"Sets the windowing of this ChartSettings.\n\n        For the tabular view, whether to use the full time window for the query or the last X minutes  # noqa: E501\n\n        :param windowing: The windowing of this ChartSettings.  # noqa: E501\n        :type: str\n        \"\"\"\n", "input": "", "output": "        allowed_values = [\"full\", \"last\"]  # noqa: E501\n        if windowing not in allowed_values:\n            raise ValueError(\n                \"Invalid value for `windowing` ({0}), must be one of {1}\"  # noqa: E501\n                .format(windowing, allowed_values)\n            )\n\n        self._windowing = windowing"}, {"instruction": "def on_unexpected_error(e):  # pragma: no cover\n    \"\"\"Catch-all error handler\n\n    Unexpected errors will be handled by this function.\n    \"\"\"\n", "input": "", "output": "    sys.stderr.write('Unexpected error: {} ({})\\n'.format(\n        str(e), e.__class__.__name__))\n    sys.stderr.write('See file slam_error.log for additional details.\\n')\n    sys.exit(1)"}, {"instruction": "def make_workspace(measurement, channel=None, name=None, silence=False):\n    \"\"\"\n    Create a workspace containing the model for a measurement\n\n    If `channel` is None then include all channels in the model\n\n    If `silence` is True, then silence HistFactory's output on\n    stdout and stderr.\n    \"\"\"\n", "input": "", "output": "    context = silence_sout_serr if silence else do_nothing\n    with context():\n        hist2workspace = ROOT.RooStats.HistFactory.HistoToWorkspaceFactoryFast(\n            measurement)\n        if channel is not None:\n            workspace = hist2workspace.MakeSingleChannelModel(\n                measurement, channel)\n        else:\n            workspace = hist2workspace.MakeCombinedModel(measurement)\n    workspace = asrootpy(workspace)\n    keepalive(workspace, measurement)\n    if name is not None:\n        workspace.SetName('workspace_{0}'.format(name))\n    return workspace"}, {"instruction": "def delete(self, object_id):\n        \"\"\"\n        Delete an object by its id\n\n        :param object_id: the objects id.\n        :return: the deleted object\n        :raises: :class: NoResultFound when the object could not be found\n        \"\"\"\n", "input": "", "output": "        obj = self.session.query(self.cls).filter_by(id=object_id).one()\n        self.session.delete(obj)\n        return obj"}, {"instruction": "def computeDistortion(self, eEye, fU, fV):\n        \"\"\"\n        Gets the result of the distortion function for the specified eye and input UVs. UVs go from 0,0 in \n        the upper left of that eye's viewport and 1,1 in the lower right of that eye's viewport.\n        Returns true for success. Otherwise, returns false, and distortion coordinates are not suitable.\n        \"\"\"\n", "input": "", "output": "\n        fn = self.function_table.computeDistortion\n        pDistortionCoordinates = DistortionCoordinates_t()\n        result = fn(eEye, fU, fV, byref(pDistortionCoordinates))\n        return result, pDistortionCoordinates"}, {"instruction": "def convert(self, vroot, entry_variables):\n        \"\"\"\n        All functions are replaced with the same `new` function.\n\n        Args:\n            vroot (:obj:`Variable`): NNabla Variable\n            entry_variables (:obj:`Variable`): Entry variable from which the conversion starts.\n        \"\"\"\n", "input": "", "output": "        self.graph_info = GraphInfo(vroot)\n        self.entry_variables = entry_variables\n\n        with nn.parameter_scope(self.name):\n            # Function loop in the forward order\n            for t, func in enumerate(self.graph_info.funcs):\n                if func.name in self.inner_prod_functions:\n                    inner_prod_func = func\n                    o = self._fixed_point_weight_conversion(inner_prod_func)\n                    continue\n                # Identity conversion\n                o = self._identity_conversion(func)\n\n        self.end_variable = o\n\n        if self.call_forward:\n            o.forward(clear_buffer=True)\n        return self.end_variable"}, {"instruction": "def get(self, filepath):\n        \"\"\"\n        Get the contents of the specified file.\n        \"\"\"\n", "input": "", "output": "        exists = self.fs.exists(filepath)\n        if exists:\n            mime = magic.Magic(mime=True)\n            mime_type = mime.from_file(filepath)\n            if mime_type in self.unsupported_types:\n                self.set_status(204)\n                return\n            else:\n                contents = self.fs.read_file(filepath)\n            self.write({'filepath':filepath,'contents': contents})\n        else:\n            raise tornado.web.HTTPError(404)"}, {"instruction": "def get(self, *args, **kwargs):\n        \"\"\"\n        Works just like the default Manager's :func:`get` method, but\n        you can pass an additional keyword argument named ``path`` specifying\n        the full path of the object you want to retrieve, e.g.\n        ``\"path/to/folder/readme.txt\"``.\n        \"\"\"\n", "input": "", "output": "        if 'path' in kwargs:\n            kwargs = self.get_filter_args_with_path(True, **kwargs)\n        return super(FileNodeManager, self).get(\n            *args, **kwargs)"}, {"instruction": "def qos_rcv_queue_multicast_threshold_traffic_class6(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        qos = ET.SubElement(config, \"qos\", xmlns=\"urn:brocade.com:mgmt:brocade-qos\")\n        rcv_queue = ET.SubElement(qos, \"rcv-queue\")\n        multicast = ET.SubElement(rcv_queue, \"multicast\")\n        threshold = ET.SubElement(multicast, \"threshold\")\n        traffic_class6 = ET.SubElement(threshold, \"traffic-class6\")\n        traffic_class6.text = kwargs.pop('traffic_class6')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def _copy(self, other, copy_func):\n        \"\"\"\n        Copies the contents of another ParsableOctetString object to itself\n\n        :param object:\n            Another instance of the same class\n\n        :param copy_func:\n            An reference of copy.copy() or copy.deepcopy() to use when copying\n            lists, dicts and objects\n        \"\"\"\n", "input": "", "output": "\n        super(ParsableOctetString, self)._copy(other, copy_func)\n        self._bytes = other._bytes\n        self._parsed = copy_func(other._parsed)"}, {"instruction": "def _import_lua_dependencies(lua, lua_globals):\n        \"\"\"\n        Imports lua dependencies that are supported by redis lua scripts.\n\n        The current implementation is fragile to the target platform and lua version\n        and may be disabled if these imports are not needed.\n\n        Included:\n            - cjson lib.\n        Pending:\n            - base lib.\n            - table lib.\n            - string lib.\n            - math lib.\n            - debug lib.\n            - cmsgpack lib.\n        \"\"\"\n", "input": "", "output": "        if sys.platform not in ('darwin', 'windows'):\n            import ctypes\n            ctypes.CDLL('liblua5.2.so', mode=ctypes.RTLD_GLOBAL)\n\n        try:\n            lua_globals.cjson = lua.eval('require \"cjson\"')\n        except RuntimeError:\n            raise RuntimeError(\"cjson not installed\")"}, {"instruction": "def _serialize_parameters(parameters):\n        \"\"\"Serialize some parameters to match python native types with formats\n        specified in google api docs like:\n        * True/False -> \"true\"/\"false\",\n        * {\"a\": 1, \"b\":2} -> \"a:1|b:2\"\n\n        :type parameters: dict oif query parameters\n        \"\"\"\n", "input": "", "output": "\n        for key, value in parameters.items():\n            if isinstance(value, bool):\n                parameters[key] = \"true\" if value else \"false\"\n            elif isinstance(value, dict):\n                parameters[key] = \"|\".join(\n                    (\"%s:%s\" % (k, v) for k, v in value.items()))\n            elif isinstance(value, (list, tuple)):\n                parameters[key] = \"|\".join(value)\n        return parameters"}, {"instruction": "def generate_tar_files(directory_list):\n    \"\"\"Public function that reads a list of local directories and generates tar archives from them\"\"\"\n", "input": "", "output": "    \n    tar_file_list = []\n\n    for directory in directory_list:\n        if dir_exists(directory):\n            _generate_tar(directory)                  # create the tar archive\n            tar_file_list.append(directory + '.tar')  # append the tar archive filename to the returned tar_file_list list\n        else:\n            stderr(\"The directory '\" + directory + \"' does not exist and a tar archive could not be created from it.\", exit=1)            \n\n    return tar_file_list"}, {"instruction": "def get_default_query_from_module(module):\n  \"\"\" Given a %%sql module return the default (last) query for the module.\n\n  Args:\n    module: the %%sql module.\n\n  Returns:\n    The default query associated with this module.\n  \"\"\"\n", "input": "", "output": "  if isinstance(module, types.ModuleType):\n    return module.__dict__.get(_SQL_MODULE_LAST, None)\n  return None"}, {"instruction": "def end_range(self):\n      \"\"\"Similar to the junction range but don't need to check for leftmost or rightmost\"\"\"\n", "input": "", "output": "      if len(self._exons) == 0: return None\n      return GenomicRange(self._exons[0].chr,\n             min([x.end for x in self._exons]),\n             max([x.end for x in self._exons]))"}, {"instruction": "def get_gpg_home( appname, config_dir=None ):\n    \"\"\"\n    Get the GPG keyring directory for a particular application.\n    Return the path.\n    \"\"\"\n", "input": "", "output": "    assert is_valid_appname(appname)\n    config_dir = get_config_dir( config_dir )\n    path = os.path.join( config_dir, \"gpgkeys\", appname )\n    return path"}, {"instruction": "def _load_config(self):\n        \"\"\"\n        Loads the YAML configuration file and sets python dictionary and raw contents\n        as instance attrs.\n        \"\"\"\n", "input": "", "output": "        if not os.path.exists(self._path):\n            sys.exit(\"Config path %s does not exist\" % self._path)\n        # create empty config object\n        self._config_dict = {}\n        # read file and marshal yaml\n        with open(self._path, 'r') as f:\n            self._raw = f.read()\n            self._config_dict = yaml.load(self._raw)"}, {"instruction": "def analysis(self):\n        \"\"\"The list of analysis of ``words`` layer elements.\"\"\"\n", "input": "", "output": "        if not self.is_tagged(ANALYSIS):\n            self.tag_analysis()\n        return [word[ANALYSIS] for word in self.words]"}, {"instruction": "def get_bucket_type_props(self, bucket_type):\n        \"\"\"\n        Fetch bucket-type properties\n        \"\"\"\n", "input": "", "output": "        self._check_bucket_types(bucket_type)\n        msg_code = riak.pb.messages.MSG_CODE_GET_BUCKET_TYPE_REQ\n        codec = self._get_codec(msg_code)\n        msg = codec.encode_get_bucket_type_props(bucket_type)\n        resp_code, resp = self._request(msg, codec)\n        return codec.decode_bucket_props(resp.props)"}, {"instruction": "def decodeEntities(self, len, what, end, end2, end3):\n        \"\"\"This function is deprecated, we now always process entities\n          content through xmlStringDecodeEntities  TODO: remove it in\n          next major release.  [67] Reference ::= EntityRef | CharRef\n            [69] PEReference ::= '%' Name ';' \"\"\"\n", "input": "", "output": "        ret = libxml2mod.xmlDecodeEntities(self._o, len, what, end, end2, end3)\n        return ret"}, {"instruction": "def _humanSortKey(s):\n  \"\"\"Sort strings with numbers in a way that makes sense to humans (e.g., 5 < 20)\"\"\"\n", "input": "", "output": "  if isinstance(s, str):\n    return [w.isdigit() and int(w) or w for w in re.split(r'(\\d+)', s)]\n  else:\n    return s"}, {"instruction": "def add_acquisition_source(\n        self,\n        method,\n        submission_number=None,\n        internal_uid=None,\n        email=None,\n        orcid=None,\n        source=None,\n        datetime=None,\n    ):\n        \"\"\"Add acquisition source.\n\n        :type submission_number: integer\n\n        :type email: integer\n\n        :type source: string\n\n        :param method: method of acquisition for the suggested document\n        :type method: string\n\n        :param orcid: orcid of the user that is creating the record\n        :type orcid: string\n\n        :param internal_uid: id of the user that is creating the record\n        :type internal_uid: string\n\n        :param datetime: UTC datetime in ISO 8601 format\n        :type datetime: string\n        \"\"\"\n", "input": "", "output": "        acquisition_source = self._sourced_dict(source)\n\n        acquisition_source['submission_number'] = str(submission_number)\n        for key in ('datetime', 'email', 'method', 'orcid', 'internal_uid'):\n            if locals()[key] is not None:\n                acquisition_source[key] = locals()[key]\n\n        self.obj['acquisition_source'] = acquisition_source"}, {"instruction": "async def enable_events(self) -> asyncio.Task:\n        \"\"\"Connects to the websocket. Returns a listening task.\"\"\"\n", "input": "", "output": "        return await self._connection.ws_connect(\n            on_message=self._ws_on_message, on_error=self._ws_on_error\n        )"}, {"instruction": "def can_be_(self, state):\n    \"\"\"Check if machine can transit to given state.\"\"\"\n", "input": "", "output": "    translator = self._meta['translator']\n    state = translator.translate(state)\n\n    if self._meta['complete']:\n        return True\n\n    if self.actual_state is None:\n        return True\n\n    transitions = self._meta['transitions'][self.actual_state]\n    return state in transitions"}, {"instruction": "def ignore_whitespace_text_nodes(cls, wrapped_node):\n        \"\"\"\n        Find and delete any text nodes containing nothing but whitespace in\n        in the given node and its descendents.\n\n        This is useful for cleaning up excess low-value text nodes in a\n        document DOM after parsing a pretty-printed XML document.\n        \"\"\"\n", "input": "", "output": "        for child in wrapped_node.children:\n            if child.is_text and child.value.strip() == '':\n                child.delete()\n            else:\n                cls.ignore_whitespace_text_nodes(child)"}, {"instruction": "def p_if_statement_2(self, p):\n        \"\"\"if_statement : IF LPAREN expr RPAREN statement ELSE statement\"\"\"\n", "input": "", "output": "        p[0] = self.asttypes.If(\n            predicate=p[3], consequent=p[5], alternative=p[7])\n        p[0].setpos(p)"}, {"instruction": "def parent(self):\n        \"\"\"\n        Select the direct child(ren) from the UI element(s) given by the query expression, see ``QueryCondition`` for\n        more details about the selectors.\n\n        Warnings:\n            Experimental method, may not be available for all drivers.\n\n        Returns:\n            :py:class:`UIObjectProxy <poco.proxy.UIObjectProxy>`: a new UI proxy object representing the direct parent\n            of the first UI element.\n        \"\"\"\n", "input": "", "output": "\n        sub_query = build_query(None)  # as placeholder\n        query = ('^', (self.query, sub_query))\n        obj = UIObjectProxy(self.poco)\n        obj.query = query\n        return obj"}, {"instruction": "def trim(self, video_name, out, start, duration):\n        \"\"\"\n        Trims a clip to be duration starting at start\n        @param video_name : name of the input video\n        @param out : name of the output video\n        @param start : starting position after the trim\n        @param duration : duration of video after start\n        \"\"\"\n", "input": "", "output": "        command = ['ffmpeg', '-ss', start, '-i', video_name, '-c:v', 'huffyuv',\n                   '-y', '-preset', 'veryslow', '-t', duration, out]\n        if self.verbose:\n            print 'Trimming {0} into {1}'.format(video_name, out)\n            print ' '.join(command)\n        call(command)"}, {"instruction": "def newFile(self, *path):\n        \"\"\"\n        Open a new file somewhere in this Store's file area.\n\n        @param path: a sequence of path segments.\n\n        @return: an L{AtomicFile}.\n        \"\"\"\n", "input": "", "output": "        assert len(path) > 0, \"newFile requires a nonzero number of segments\"\n        if self.dbdir is None:\n            if self.filesdir is None:\n                raise RuntimeError(\"This in-memory store has no file directory\")\n            else:\n                tmpbase = self.filesdir\n        else:\n            tmpbase = self.dbdir\n        tmpname = tmpbase.child('temp').child(str(tempCounter.next()) + \".tmp\")\n        return AtomicFile(tmpname.path, self.newFilePath(*path))"}, {"instruction": "def cublasZgerc(handle, m, n, alpha, x, incx, y, incy, A, lda):\n    \"\"\"\n    Rank-1 operation on complex general matrix.\n\n    \"\"\"\n", "input": "", "output": "\n    status = _libcublas.cublasZgerc_v2(handle,\n                                       m, n, ctypes.byref(cuda.cuDoubleComplex(alpha.real,\n                                                                               alpha.imag)),\n                                       int(x), incx, int(y), incy, int(A), lda)\n    cublasCheckStatus(status)"}, {"instruction": "def request_frame(self):\n        \"\"\"Construct initiating frame.\"\"\"\n", "input": "", "output": "        self.session_id = get_new_session_id()\n        return FrameActivateSceneRequest(scene_id=self.scene_id, session_id=self.session_id)"}, {"instruction": "def _send_command_wrapper(self, cmd):\n        \"\"\"\n        Send command to the remote device with a caching feature to avoid sending the same command\n        twice based on the SSH_MAPPER_BASE dict cmd key.\n\n        Parameters\n        ----------\n        cmd : str\n            The command to send to the remote device after checking cache.\n\n        Returns\n        -------\n        response : str\n            The response from the remote device.\n        \"\"\"\n", "input": "", "output": "        cached_results = self._results_cache.get(cmd)\n        if not cached_results:\n            response = self._send_command(cmd)\n            self._results_cache[cmd] = response\n            return response\n        else:\n            return cached_results"}, {"instruction": "def checkout(self, revision_id):\n        \"\"\"\n        :param revision_id: :class:`revision.data.Revision` ID.\n        :type revision_id: str\n        \"\"\"\n", "input": "", "output": "        index = 0\n        found = False\n        for revision in self.revisions:\n            if revision.revision_id == revision_id:\n                self.current_index = index\n                found = True\n\n            index += 1\n\n        if not found:\n            raise RuntimeError(\"\")"}, {"instruction": "def decode_feedback(binary_tuples):\n  \"\"\" Returns a list of tuples in (datetime, token_str) format \n  \n        binary_tuples   the binary-encoded feedback tuples\n  \"\"\"\n", "input": "", "output": "  \n  fmt = '!lh32s'\n  size = struct.calcsize(fmt)\n  with StringIO(binary_tuples) as f:\n    return [(datetime.datetime.fromtimestamp(ts), binascii.hexlify(tok))\n            for ts, toklen, tok in (struct.unpack(fmt, tup) \n                              for tup in iter(lambda: f.read(size), ''))]"}, {"instruction": "def line_break(self):\n        \"\"\"insert as many line breaks as the insert_line_break variable says\n        \"\"\"\n", "input": "", "output": "        for i in range(self.slide.insert_line_break):\n            # needs to be inside text:p\n            if not self._in_tag(ns(\"text\", \"p\")):\n                # we can just add a text:p and no line-break\n                # Create paragraph style first\n                self.add_node(ns(\"text\", \"p\"))\n            self.add_node(ns(\"text\", \"line-break\"))\n            self.pop_node()\n            if self.cur_node.tag == ns(\"text\", \"p\"):\n                return\n\n            if self.cur_node.getparent().tag != ns(\"text\", \"p\"):\n                self.pop_node()\n        self.slide.insert_line_break = 0"}, {"instruction": "def flag_forgotten_entries(session, today=None):\n    \"\"\"Flag any entries from previous days where users forgot to sign\n    out.\n\n    :param session: SQLAlchemy session through which to access the database.\n    :param today: (optional) The current date as a `datetime.date` object. Used for testing.\n    \"\"\"\n", "input": "", "output": "    today = date.today() if today is None else today\n\n    forgotten = (\n        session\n        .query(Entry)\n        .filter(Entry.time_out.is_(None))\n        .filter(Entry.forgot_sign_out.is_(False))\n        .filter(Entry.date < today)\n    )\n\n    for entry in forgotten:\n        e = sign_out(entry, forgot=True)\n        logger.debug('Signing out forgotten entry: {}'.format(e))\n        session.add(e)\n\n    session.commit()"}, {"instruction": "def by_median_home_value(self,\n                             lower=-1,\n                             upper=2 ** 31,\n                             zipcode_type=ZipcodeType.Standard,\n                             sort_by=SimpleZipcode.median_home_value.name,\n                             ascending=False,\n                             returns=DEFAULT_LIMIT):\n        \"\"\"\n        Search zipcode information by median home value.\n        \"\"\"\n", "input": "", "output": "        return self.query(\n            median_home_value_lower=lower,\n            median_home_value_upper=upper,\n            sort_by=sort_by, zipcode_type=zipcode_type,\n            ascending=ascending, returns=returns,\n        )"}, {"instruction": "def bind_sockets(address, port):\n    ''' Bind a socket to a port on an address.\n\n    Args:\n        address (str) :\n            An address to bind a port on, e.g. ``\"localhost\"``\n\n        port (int) :\n            A port number to bind.\n\n            Pass 0 to have the OS automatically choose a free port.\n\n    This function returns a 2-tuple with the new socket as the first element,\n    and the port that was bound as the second. (Useful when passing 0 as a port\n    number to bind any free port.)\n\n    Returns:\n        (socket, port)\n\n    '''\n", "input": "", "output": "    ss = netutil.bind_sockets(port=port or 0, address=address)\n    assert len(ss)\n    ports = {s.getsockname()[1] for s in ss}\n    assert len(ports) == 1, \"Multiple ports assigned??\"\n    actual_port = ports.pop()\n    if port:\n        assert actual_port == port\n    return ss, actual_port"}, {"instruction": "def _log_to_stderr(self, record):\n    \"\"\"Emits the record to stderr.\n\n    This temporarily sets the handler stream to stderr, calls\n    StreamHandler.emit, then reverts the stream back.\n\n    Args:\n      record: logging.LogRecord, the record to log.\n    \"\"\"\n", "input": "", "output": "    # emit() is protected by a lock in logging.Handler, so we don't need to\n    # protect here again.\n    old_stream = self.stream\n    self.stream = sys.stderr\n    try:\n      super(PythonHandler, self).emit(record)\n    finally:\n      self.stream = old_stream"}, {"instruction": "def reset_password(self, user, password):\n        \"\"\"\n        Service method to reset a user's password. The same as :meth:`change_password`\n        except we this method sends a different notification email.\n\n        Sends signal `password_reset`.\n\n        :param user:\n        :param password:\n        :return:\n        \"\"\"\n", "input": "", "output": "        user.password = password\n        self.user_manager.save(user)\n        if app.config.SECURITY_SEND_PASSWORD_RESET_NOTICE_EMAIL:\n            self.send_mail(\n                _('flask_unchained.bundles.security:email_subject.password_reset_notice'),\n                to=user.email,\n                template='security/email/password_reset_notice.html',\n                user=user)\n        password_reset.send(app._get_current_object(), user=user)"}, {"instruction": "def security_iter(nodearr):\n        \"\"\" provide a security data iterator by returning a tuple of (Element, SecurityError) which are mutually exclusive \"\"\"\n", "input": "", "output": "        assert nodearr.name() == 'securityData' and nodearr.isArray()\n        for i in range(nodearr.numValues()):\n            node = nodearr.getValue(i)\n            err = XmlHelper.get_security_error(node)\n            result = (None, err) if err else (node, None)\n            yield result"}, {"instruction": "def abrt(self, s=None, post=None, noraise=False):\n        \"\"\" Prints the abrt banner and raises ``ProgressAbrt`` exception\n\n        When ``noraise`` flag is set to ``True``, then the exception is not\n        raised, and progress is allowed to continue.\n\n        If ``post`` function is supplied it is invoked with no arguments after\n        the close banner is printed, but before exceptions are raised. The\n        ``post`` function takes no arguments.\n        \"\"\"\n", "input": "", "output": "        s = s or self.abrt_msg\n        self.printer(self.color.red(s))\n        if post:\n            post()\n        if noraise:\n            return\n        raise ProgressAbrt()"}, {"instruction": "def Add(self, other):\n    \"\"\"Add other to self pointwise.\n\n    Requires that both self and other are of the same length, and contain\n    identical timestamps. Typically this means that Normalize has been called\n    on both with identical time parameters.\n\n    Args:\n      other: The sequence to add to self.\n\n    Raises:\n      RuntimeError: other does not contain the same timestamps as self.\n    \"\"\"\n", "input": "", "output": "    if len(self.data) != len(other.data):\n      raise RuntimeError(\"Can only add series of identical lengths.\")\n    for i in range(len(self.data)):\n      if self.data[i][1] != other.data[i][1]:\n        raise RuntimeError(\"Timestamp mismatch.\")\n      if self.data[i][0] is None and other.data[i][0] is None:\n        continue\n      self.data[i][0] = (self.data[i][0] or 0) + (other.data[i][0] or 0)"}, {"instruction": "def basemz(df):\n    \"\"\"\n    The mz of the most abundant ion.\n    \"\"\"\n", "input": "", "output": "    # returns the\n    d = np.array(df.columns)[df.values.argmax(axis=1)]\n    return Trace(d, df.index, name='basemz')"}, {"instruction": "def _sendAction(self, action, attrs=None, chan_vars=None):\n        \"\"\"Send action to Asterisk Manager Interface.\n        \n        @param action:    Action name\n        @param attrs:     Tuple of key-value pairs for action attributes.\n        @param chan_vars: Tuple of key-value pairs for channel variables.\n\n        \"\"\"\n", "input": "", "output": "        self._conn.write(\"Action: %s\\r\\n\" % action)\n        if attrs:\n            for (key,val) in attrs:\n                self._conn.write(\"%s: %s\\r\\n\" % (key, val))\n        if chan_vars:\n            for (key,val) in chan_vars:\n                self._conn.write(\"Variable: %s=%s\\r\\n\" % (key, val))\n        self._conn.write(\"\\r\\n\")"}, {"instruction": "def isHereDoc(self, line, column):\n        \"\"\"Check if text at given position is a here document.\n\n        If language is not known, or text is not parsed yet, ``False`` is returned\n        \"\"\"\n", "input": "", "output": "        return self._highlighter is not None and \\\n               self._highlighter.isHereDoc(self.document().findBlockByNumber(line), column)"}, {"instruction": "def start(self, reloading=False):\n        \"\"\"Called when the module is loaded.\n\n        If the load is due to a reload of the module, then the 'reloading'\n        argument will be set to True. By default, this method calls the\n        controller's listen() for each event in the self.event_handlers dict.\n        \"\"\"\n", "input": "", "output": "        for event in self.event_handlers:\n            self.controller.listen(event)"}, {"instruction": "def update(self, friendly_name=values.unset,\n               default_deployment_sid=values.unset):\n        \"\"\"\n        Update the FleetInstance\n\n        :param unicode friendly_name: A human readable description for this Fleet.\n        :param unicode default_deployment_sid: A default Deployment SID.\n\n        :returns: Updated FleetInstance\n        :rtype: twilio.rest.preview.deployed_devices.fleet.FleetInstance\n        \"\"\"\n", "input": "", "output": "        return self._proxy.update(\n            friendly_name=friendly_name,\n            default_deployment_sid=default_deployment_sid,\n        )"}, {"instruction": "def check_value(self, value):\n        \"\"\"Check the validity of a value for the field.\"\"\"\n", "input": "", "output": "        #if self.readonly:\n        #    raise error.Error(\n        #        \"'{field_name}' field is readonly\".format(\n        #            field_name=self.name))\n        if value and self.size:\n            if not is_string(value):\n                raise ValueError(\"Value supplied has to be a string\")\n            if len(value) > self.size:\n                raise ValueError(\n                    \"Lenght of the '{0}' is limited to {1}\".format(\n                        self.name, self.size))\n        if not value and self.required:\n            raise ValueError(\"'{0}' field is required\".format(self.name))\n        return value"}, {"instruction": "def prepare_request(self, **kwargs):\n        \"\"\"\n        Configure all things to make real network request.\n        This method is called before doing real request via\n        transport extension.\n        \"\"\"\n", "input": "", "output": "\n        if self.transport is None:\n            self.setup_transport(self.transport_param)\n        self.reset()\n        self.request_counter = next(REQUEST_COUNTER)\n        if kwargs:\n            self.setup(**kwargs)\n        if self.proxylist.size() and self.config['proxy_auto_change']:\n            self.change_proxy()\n        self.request_method = self.detect_request_method()\n        self.transport.process_config(self)"}, {"instruction": "def untlpydict2dcformatteddict(untl_dict, **kwargs):\n    \"\"\"Convert a UNTL data dictionary to a formatted DC data dictionary.\"\"\"\n", "input": "", "output": "    ark = kwargs.get('ark', None)\n    domain_name = kwargs.get('domain_name', None)\n    scheme = kwargs.get('scheme', 'http')\n    resolve_values = kwargs.get('resolve_values', None)\n    resolve_urls = kwargs.get('resolve_urls', None)\n    verbose_vocabularies = kwargs.get('verbose_vocabularies', None)\n    # Get the UNTL object.\n    untl_py = untldict2py(untl_dict)\n    # Convert it to a DC object.\n    dc_py = untlpy2dcpy(\n        untl_py,\n        ark=ark,\n        domain_name=domain_name,\n        resolve_values=resolve_values,\n        resolve_urls=resolve_urls,\n        verbose_vocabularies=verbose_vocabularies,\n        scheme=scheme\n    )\n    # Return a formatted DC dictionary.\n    return dcpy2formatteddcdict(dc_py)"}, {"instruction": "def _set_next_host_location(self, context):\n        '''\n        A function which sets the next host location on the request, if applicable. \n\n        :param ~azure.storage.models.RetryContext context: \n            The retry context containing the previous host location and the request \n            to evaluate and possibly modify.\n        '''\n", "input": "", "output": "        if len(context.request.host_locations) > 1:\n            # If there's more than one possible location, retry to the alternative\n            if context.location_mode == LocationMode.PRIMARY:\n                context.location_mode = LocationMode.SECONDARY\n            else:\n                context.location_mode = LocationMode.PRIMARY\n\n            context.request.host = context.request.host_locations.get(context.location_mode)"}, {"instruction": "def set_gid(self):\n        \"\"\"Change the group of the running process\"\"\"\n", "input": "", "output": "        if self.group:\n            gid = getgrnam(self.group).gr_gid\n            try:\n                os.setgid(gid)\n            except Exception:\n                message = (\"Unable to switch ownership to {0}:{1}. \" +\n                           \"Did you start the daemon as root?\")\n                print(message.format(self.user, self.group))\n                sys.exit(1)"}, {"instruction": "def sub(self, key):\n        \"\"\"Returns new Vyper instance representing a sub tree of this instance.\n        \"\"\"\n", "input": "", "output": "        subv = Vyper()\n        data = self.get(key)\n        if isinstance(data, dict):\n            subv._config = data\n            return subv\n        else:\n            return None"}, {"instruction": "def from_proto(cls, proto_mesh, scale):\n        \"\"\"\n        TODO: add documentation\n        \"\"\"\n", "input": "", "output": "\n        mesh = cls(**proto_mesh.items())\n        mesh._copy_roche_values()\n        mesh._scale_mesh(scale=scale)\n\n        return mesh"}, {"instruction": "def active():\n    '''\n    List existing device-mapper device details.\n    '''\n", "input": "", "output": "    ret = {}\n    # TODO: This command should be extended to collect more information, such as UUID.\n    devices = __salt__['cmd.run_stdout']('dmsetup ls --target crypt')\n    out_regex = re.compile(r'(?P<devname>\\w+)\\W+\\((?P<major>\\d+), (?P<minor>\\d+)\\)')\n\n    log.debug(devices)\n    for line in devices.split('\\n'):\n        match = out_regex.match(line)\n        if match:\n            dev_info = match.groupdict()\n            ret[dev_info['devname']] = dev_info\n        else:\n            log.warning('dmsetup output does not match expected format')\n\n    return ret"}, {"instruction": "def _split_classes_and_methods(folds):\n    \"\"\"\n    Split out classes and methods into two separate lists.\n\n    Parameters\n    ----------\n    folds : list of :class:`FoldScopeHelper`\n        The result of :func:`_get_fold_levels`.\n\n    Returns\n    -------\n    classes, functions: list of :class:`FoldScopeHelper`\n        Two separate lists of :class:`FoldScopeHelper` objects. The former\n        contains only class definitions while the latter contains only\n        function/method definitions.\n    \"\"\"\n", "input": "", "output": "    classes = []\n    functions = []\n    for fold in folds:\n        if fold.def_type == OED.FUNCTION_TOKEN:\n            functions.append(fold)\n        elif fold.def_type == OED.CLASS_TOKEN:\n            classes.append(fold)\n\n    return classes, functions"}, {"instruction": "def intersection_update(self, *iterables):\n        \"\"\"\n        Update the set, keeping only elements found in it and all *iterables*.\n        \"\"\"\n", "input": "", "output": "        _set = self._set\n        _list = self._list\n        _set.intersection_update(*iterables)\n        _list.clear()\n        _list.update(_set)\n        return self"}, {"instruction": "def i18n_system_locale():\n    \"\"\"\n    Return the system locale\n    :return: the system locale (as a string)\n    \"\"\"\n", "input": "", "output": "    log.debug('i18n_system_locale() called')\n    lc, encoding = locale.getlocale()\n    log.debug('locale.getlocale() = (lc=\"{lc}\", encoding=\"{encoding}).'.format(lc=lc, encoding=encoding))\n    if lc is None:\n        lc, encoding = locale.getdefaultlocale()\n        log.debug('locale.getdefaultlocale() = (lc=\"{lc}\", encoding=\"{encoding}).'.format(lc=lc, encoding=encoding))\n    return lc"}, {"instruction": "def get_items(self):\n        \"\"\"This is out of spec, but required for adaptive assessment parts?\"\"\"\n", "input": "", "output": "        ils = get_item_lookup_session(runtime=self._runtime, proxy=self._proxy)\n        ils.use_federated_bank_view()\n        items = []\n        if self.has_items():\n            for idstr in self._my_map['itemIds']:\n                items.append(ils.get_item(Id(idstr)))\n        return ItemList(items, runtime=self._runtime, proxy=self._proxy)"}, {"instruction": "def flip_labels(obj):\n    \"\"\"\n    Rename fields x to y and y to x\n\n    Parameters\n    ----------\n    obj : dict_like\n        Object with labels to rename\n    \"\"\"\n", "input": "", "output": "    def sub(a, b):\n        "}, {"instruction": "def bind_socket(address, port):\n    \"\"\" Returns a socket bound on (address:port). \"\"\"\n", "input": "", "output": "\n    assert address\n    assert port\n    bindsocket = socket.socket()\n    try:\n        bindsocket.bind((address, port))\n    except socket.error:\n        logger.error(\"Couldn't bind socket on %s:%s\", address, port)\n        return None\n\n    logger.info('Listening on %s:%s', address, port)\n    bindsocket.listen(0)\n    return bindsocket"}, {"instruction": "def wsgi(self, environ, start_response):\n        \"\"\"Implements the mapper's WSGI interface.\"\"\"\n", "input": "", "output": "        request = Request(environ)\n        ctx = Context(request)\n        try:\n            try:\n                response = self(request, ctx)\n                ctx._run_callbacks('finalize', (request, response))\n                response = response.conditional_to(request)\n            except HTTPException as e:\n                response = e.response\n            except Exception:\n                self.handle_error(request, ctx)\n                response = InternalServerError().response\n\n            response.add_callback(lambda: ctx._run_callbacks('close'))\n            return response(environ, start_response)\n        finally:\n            ctx._run_callbacks('teardown', log_errors=True)"}, {"instruction": "def rotation_from_axes(x_axis, y_axis, z_axis):\n        \"\"\"Convert specification of axis in target frame to\n        a rotation matrix from source to target frame.\n\n        Parameters\n        ----------\n        x_axis : :obj:`numpy.ndarray` of float\n            A normalized 3-vector for the target frame's x-axis.\n\n        y_axis : :obj:`numpy.ndarray` of float\n            A normalized 3-vector for the target frame's y-axis.\n\n        z_axis : :obj:`numpy.ndarray` of float\n            A normalized 3-vector for the target frame's z-axis.\n\n        Returns\n        -------\n        :obj:`numpy.ndarray` of float\n            A 3x3 rotation matrix that transforms from a source frame to the\n            given target frame.\n        \"\"\"\n", "input": "", "output": "        return np.hstack((x_axis[:,np.newaxis], y_axis[:,np.newaxis], z_axis[:,np.newaxis]))"}, {"instruction": "def _count_dollars_before_index(self, s, i):\n        \"\"\"Returns the number of '$' characters right in front of s[i].\"\"\"\n", "input": "", "output": "        dollar_count = 0\n        dollar_index = i - 1\n        while dollar_index > 0 and s[dollar_index] == '$':\n            dollar_count += 1\n            dollar_index -= 1\n        return dollar_count"}, {"instruction": "def parse_cgmlst_alleles(cgmlst_fasta):\n    \"\"\"Parse cgMLST alleles from fasta file\n    cgMLST FASTA file must have a header format of \">{marker name}|{allele name}\"\n\n    Args:\n        cgmlst_fasta (str): cgMLST fasta file path\n\n    Returns:\n        dict of list: Marker name to list of allele sequences\n    \"\"\"\n", "input": "", "output": "    out = defaultdict(list)\n    for header, seq in parse_fasta(cgmlst_fasta):\n        if not '|' in header:\n            raise Exception('Unexpected format for cgMLST fasta file header. No \"|\" (pipe) delimiter present! Header=\"{}\"'.format(header))\n        marker_name, allele_name = header.split('|')\n        out[marker_name].append(seq)\n    return out"}, {"instruction": "def migrate_050_to_051(session):\n    \"\"\"Set time_out field of all flagged\n    timesheet entries to Null.\n    \"\"\"\n", "input": "", "output": "    entries_to_update = session.query(Entry).filter(\n            Entry.forgot_sign_out.is_(True)).filter(\n            Entry.time_out.isnot(None))\n\n    for entry in entries_to_update:\n        entry.time_out = None\n        logging.info('Entry updated {}'.format(entry.uuid))\n        logging.debug(entry.uuid)\n        session.add(entry)"}, {"instruction": "def _get_file_content(source):\n    \"\"\"Return a tuple, each value being a line of the source file.\n\n    Remove empty lines and comments (lines starting with a '#').\n\n    \"\"\"\n", "input": "", "output": "    filepath = os.path.join('siglists', source + '.txt')\n\n    lines = []\n    with resource_stream(__name__, filepath) as f:\n        for i, line in enumerate(f):\n            line = line.decode('utf-8', 'strict').strip()\n            if not line or line.startswith('#'):\n                continue\n\n            try:\n                re.compile(line)\n            except Exception as ex:\n                raise BadRegularExpressionLineError(\n                    'Regex error: {} in file {} at line {}'.format(\n                        str(ex),\n                        filepath,\n                        i\n                    )\n                )\n\n            lines.append(line)\n\n    if source in _SPECIAL_EXTENDED_VALUES:\n        lines = lines + _SPECIAL_EXTENDED_VALUES[source]\n\n    return tuple(lines)"}, {"instruction": "def qos_map_cos_mutation_cos1(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        qos = ET.SubElement(config, \"qos\", xmlns=\"urn:brocade.com:mgmt:brocade-qos\")\n        map = ET.SubElement(qos, \"map\")\n        cos_mutation = ET.SubElement(map, \"cos-mutation\")\n        name_key = ET.SubElement(cos_mutation, \"name\")\n        name_key.text = kwargs.pop('name')\n        cos1 = ET.SubElement(cos_mutation, \"cos1\")\n        cos1.text = kwargs.pop('cos1')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def gpib_command(self, session, command_byte):\n        \"\"\"Write GPIB command byte on the bus.\n\n        Corresponds to viGpibCommand function of the VISA library.\n        See: https://linux-gpib.sourceforge.io/doc_html/gpib-protocol.html#REFERENCE-COMMAND-BYTES\n\n        :param command_byte: command byte to send\n        :type command_byte: int, must be [0 255]\n        :return: return value of the library call\n        :rtype: :class:`pyvisa.constants.StatusCode`\n        \"\"\"\n", "input": "", "output": "        try:\n            return self.sessions[session].gpib_command(command_byte)\n\n        except KeyError:\n            return constants.StatusCode.error_invalid_object"}, {"instruction": "def reply(self, connection, reply, orig_req):\n        \"\"\"Send an asynchronous reply to an earlier request.\n\n        Parameters\n        ----------\n        connection : ClientConnection object\n            The client to send the reply to.\n        reply : Message object\n            The reply message to send.\n        orig_req : Message object\n            The request message being replied to. The reply message's\n            id is overridden with the id from orig_req before the\n            reply is sent.\n\n        \"\"\"\n", "input": "", "output": "        if isinstance(connection, ClientRequestConnection):\n            self._logger.warn(\n                'Deprecation warning: do not use self.reply() '\n                'within a reply handler context -- use req.reply(*msg_args)\\n'\n                'or req.reply_with_message(msg) Traceback:\\n %s',\n                \"\".join(traceback.format_stack()))\n            # Get the underlying ClientConnection instance\n            connection = connection.client_connection\n        connection.reply(reply, orig_req)"}, {"instruction": "def untrace_method(cls, method):\n    \"\"\"\n    Untraces given class method.\n\n    :param cls: Class of the method.\n    :type cls: object\n    :param method: Method to untrace.\n    :type method: object\n    :return: Definition success.\n    :rtype: bool\n    \"\"\"\n", "input": "", "output": "\n    if not is_traced(method):\n        return False\n\n    name = get_method_name(method)\n    if is_class_method(method):\n        setattr(cls, name, classmethod(untracer(method)))\n    elif is_static_method(method):\n        setattr(cls, name, staticmethod(untracer(method)))\n    else:\n        setattr(cls, name, untracer(method))\n    return True"}, {"instruction": "def hash160(msg_bytes):\n    '''\n    byte-like -> bytes\n    '''\n", "input": "", "output": "    h = hashlib.new('ripemd160')\n    if 'decred' in riemann.get_current_network_name():\n        h.update(blake256(msg_bytes))\n        return h.digest()\n    h.update(sha256(msg_bytes))\n    return h.digest()"}, {"instruction": "def normalize(dt, tz=UTC):\n    \"\"\"\n    Convert date or datetime to datetime with timezone.\n\n    :param dt: date to normalize\n    :param tz: the normalized date's timezone\n    :return: date as datetime with timezone\n    \"\"\"\n", "input": "", "output": "    if type(dt) is date:\n        dt = dt + relativedelta(hour=0)\n    elif type(dt) is datetime:\n        pass\n    else:\n        raise ValueError(\"unknown type %s\" % type(dt))\n\n    if dt.tzinfo:\n        dt = dt.astimezone(tz)\n    else:\n        dt = dt.replace(tzinfo=tz)\n\n    return dt"}, {"instruction": "def runGetVariant(self, id_):\n        \"\"\"\n        Returns a variant with the given id\n        \"\"\"\n", "input": "", "output": "        compoundId = datamodel.VariantCompoundId.parse(id_)\n        dataset = self.getDataRepository().getDataset(compoundId.dataset_id)\n        variantSet = dataset.getVariantSet(compoundId.variant_set_id)\n        gaVariant = variantSet.getVariant(compoundId)\n        # TODO variant is a special case here, as it's returning a\n        # protocol element rather than a datamodel object. We should\n        # fix this for consistency.\n        jsonString = protocol.toJson(gaVariant)\n        return jsonString"}, {"instruction": "def fingerprint(self):\n    \"\"\"A memoized sha1 hexdigest hashing the contents of this PayloadField\n\n    The fingerprint returns either a string or None.  If the return is None, consumers of the\n    fingerprint may choose to elide this PayloadField from their combined hash computation.\n\n    :API: public\n    \"\"\"\n", "input": "", "output": "    if self._fingerprint_memo is None:\n      self._fingerprint_memo = self._compute_fingerprint()\n    return self._fingerprint_memo"}, {"instruction": "def get_approvals(self, issue_id_or_key, start=0, limit=50):\n        \"\"\"\n        Get all approvals on a request, for a given request ID/Key\n\n        :param issue_id_or_key: str\n        :param start: OPTIONAL: int\n        :param limit: OPTIONAL: int\n        :return:\n        \"\"\"\n", "input": "", "output": "        url = 'rest/servicedeskapi/request/{}/approval'.format(issue_id_or_key)\n        params = {}\n        if start is not None:\n            params['start'] = int(start)\n        if limit is not None:\n            params['limit'] = int(limit)\n\n        return self.get(url, headers=self.experimental_headers, params=params).get('values')"}, {"instruction": "def _get_image_size(self, maxcharno, maxlineno):\n        \"\"\"\n        Get the required image size.\n        \"\"\"\n", "input": "", "output": "        return (self._get_char_x(maxcharno) + self.image_pad,\n                self._get_line_y(maxlineno + 0) + self.image_pad)"}, {"instruction": "def point_eval(M, C, x):\n    \"\"\"\n    Evaluates M(x) and C(x).\n\n    Minimizes computation; evaluating M(x) and C(x) separately would\n    evaluate the off-diagonal covariance term twice, but callling\n    point_eval(M,C,x) would only evaluate it once.\n\n    Also chunks the evaluations if the off-diagonal term.\n    \"\"\"\n", "input": "", "output": "\n    x_ = regularize_array(x)\n\n    M_out = empty(x_.shape[0])\n    V_out = empty(x_.shape[0])\n\n    if isinstance(C, pymc.gp.BasisCovariance):\n        y_size = len(C.basis)\n    elif C.obs_mesh is not None:\n        y_size = C.obs_mesh.shape[0]\n    else:\n        y_size = 1\n\n    n_chunks = ceil(y_size * x_.shape[0] / float(chunksize))\n    bounds = array(linspace(0, x_.shape[0], n_chunks + 1), dtype='int')\n    cmin = bounds[:-1]\n    cmax = bounds[1:]\n    for (cmin, cmax) in zip(bounds[:-1], bounds[1:]):\n        x__ = x_[cmin:cmax]\n        V_out[cmin:cmax], Uo_Cxo = C(x__, regularize=False, return_Uo_Cxo=True)\n        M_out[cmin:cmax] = M(x__, regularize=False, Uo_Cxo=Uo_Cxo)\n\n    if len(x.shape) > 1:\n        targ_shape = x.shape[:-1]\n    else:\n        targ_shape = x.shape\n    return M_out.reshape(targ_shape), V_out.reshape(targ_shape)"}, {"instruction": "def add_gateway_responses(self, gateway_responses):\n        \"\"\"\n        Add Gateway Response definitions to Swagger.\n\n        :param dict gateway_responses: Dictionary of GatewayResponse configuration which gets translated.\n        \"\"\"\n", "input": "", "output": "        self.gateway_responses = self.gateway_responses or {}\n\n        for response_type, response in gateway_responses.items():\n            self.gateway_responses[response_type] = response.generate_swagger()"}, {"instruction": "def current_state(self, *,\n                      chat: typing.Union[str, int, None] = None,\n                      user: typing.Union[str, int, None] = None) -> FSMContext:\n        \"\"\"\n        Get current state for user in chat as context\n\n        .. code-block:: python3\n\n            with dp.current_state(chat=message.chat.id, user=message.user.id) as state:\n                pass\n\n            state = dp.current_state()\n            state.set_state('my_state')\n\n        :param chat:\n        :param user:\n        :return:\n        \"\"\"\n", "input": "", "output": "        if chat is None:\n            chat_obj = types.Chat.get_current()\n            chat = chat_obj.id if chat_obj else None\n        if user is None:\n            user_obj = types.User.get_current()\n            user = user_obj.id if user_obj else None\n\n        return FSMContext(storage=self.storage, chat=chat, user=user)"}, {"instruction": "def loadXMLGenericData(filename):  # not tested\n    \"\"\"Read any type of vtk data object encoded in XML format. Return an ``Actor(vtkActor)`` object.\"\"\"\n", "input": "", "output": "    reader = vtk.vtkXMLGenericDataObjectReader()\n    reader.SetFileName(filename)\n    reader.Update()\n    return Actor(reader.GetOutput())"}, {"instruction": "def current_branch(self):\n        \"\"\"The name of the branch that's currently checked out in the working tree (a string or :data:`None`).\"\"\"\n", "input": "", "output": "        output = self.context.capture('git', 'rev-parse', '--abbrev-ref', 'HEAD', check=False, silent=True)\n        return output if output != 'HEAD' else None"}, {"instruction": "def new_registry(attribute=None):\n    \"\"\"\n    Returns an empty dict and a @register decorator.\n    \"\"\"\n", "input": "", "output": "    registry = {}\n\n    def register(key):\n        def decorator(func):\n            registry[key] = func\n            if attribute:\n                setattr(func, attribute, key)\n            return func\n        return decorator\n\n    return registry, register"}, {"instruction": "def issue_date(self):\n        \"\"\"Date when the DOI was issued (:class:`datetime.datetime.Datetime`).\n        \"\"\"\n", "input": "", "output": "        dates = _pluralize(self._r['dates'], 'date')\n        for date in dates:\n            if date['@dateType'] == 'Issued':\n                return datetime.datetime.strptime(date['#text'], '%Y-%m-%d')"}, {"instruction": "def create_powerflow_problem(timerange, components):\n    \"\"\"\n    Create PyPSA network object and fill with data\n    Parameters\n    ----------\n    timerange: Pandas DatetimeIndex\n        Time range to be analyzed by PF\n    components: dict\n    Returns\n    -------\n    network: PyPSA powerflow problem object\n    \"\"\"\n", "input": "", "output": "\n    # initialize powerflow problem\n    network, snapshots = init_pypsa_network(timerange)\n\n    # add components to network\n    for component in components.keys():\n        network.import_components_from_dataframe(components[component],\n                                                 component)\n\n    return network, snapshots"}, {"instruction": "def _GetStat(self):\n    \"\"\"Retrieves information about the file entry.\n\n    Returns:\n      VFSStat: a stat object.\n    \"\"\"\n", "input": "", "output": "    stat_object = super(BDEFileEntry, self)._GetStat()\n\n    stat_object.size = self._bde_volume.get_size()\n\n    return stat_object"}, {"instruction": "def refresh_datasources(self, refreshAll=True):\n        \"\"\"endpoint that refreshes druid datasources metadata\"\"\"\n", "input": "", "output": "        session = db.session()\n        DruidCluster = ConnectorRegistry.sources['druid'].cluster_class\n        for cluster in session.query(DruidCluster).all():\n            cluster_name = cluster.cluster_name\n            valid_cluster = True\n            try:\n                cluster.refresh_datasources(refreshAll=refreshAll)\n            except Exception as e:\n                valid_cluster = False\n                flash(\n                    \"Error while processing cluster '{}'\\n{}\".format(\n                        cluster_name, utils.error_msg_from_exception(e)),\n                    'danger')\n                logging.exception(e)\n                pass\n            if valid_cluster:\n                cluster.metadata_last_refreshed = datetime.now()\n                flash(\n                    _('Refreshed metadata from cluster [{}]').format(\n                        cluster.cluster_name),\n                    'info')\n        session.commit()\n        return redirect('/druiddatasourcemodelview/list/')"}, {"instruction": "def path_to(self, p):\n        \"\"\"Returns the absolute path to a given relative path.\"\"\"\n", "input": "", "output": "        if os.path.isabs(p):\n            return p\n\n        return os.sep.join([self._original_dir, p])"}, {"instruction": "def delete(uid):\n        '''\n        Delete by uid\n        '''\n", "input": "", "output": "\n        q_u1 = TabPostHist.delete().where(TabPostHist.post_id == uid)\n        q_u1.execute()\n        q_u2 = TabRel.delete().where(TabRel.post_f_id == uid or TabRel.post_t_id == uid)\n        q_u2.execute()\n        q_u3 = TabCollect.delete().where(TabCollect.post_id == uid)\n        q_u3.execute()\n        q_u4 = TabPost2Tag.delete().where(TabPost2Tag.post_id == uid)\n        q_u4.execute()\n        q_u5 = TabUsage.delete().where(TabUsage.post_id == uid)\n        q_u5.execute()\n\n        reply_arr = []\n        for reply in TabUser2Reply.select().where(TabUser2Reply.reply_id == uid):\n            reply_arr.append(reply.reply_id.uid)\n\n        q_u6 = TabUser2Reply.delete().where(TabUser2Reply.reply_id == uid)\n        q_u6.execute()\n\n        for replyid in reply_arr:\n            TabReply.delete().where(TabReply.uid == replyid).execute()\n\n        q_u7 = TabEvaluation.delete().where(TabEvaluation.post_id == uid)\n        q_u7.execute()\n        q_u8 = TabRating.delete().where(TabRating.post_id == uid)\n        q_u8.execute()\n        return MHelper.delete(TabPost, uid)"}, {"instruction": "def median(array):\n    \"\"\"\n    Return the median value of a list of numbers.\n    \"\"\"\n", "input": "", "output": "    n = len(array)\n\n    if n < 1:\n        return 0\n    elif n == 1:\n        return array[0]\n\n    sorted_vals = sorted(array)\n    midpoint = int(n / 2)\n    if n % 2 == 1:\n        return sorted_vals[midpoint]\n    else:\n        return (sorted_vals[midpoint - 1] + sorted_vals[midpoint]) / 2.0"}, {"instruction": "def _get_device_info(self, device_id):\n        \"\"\"Queries the Spark Cloud for detailed information about a device.\"\"\"\n", "input": "", "output": "        params = {'access_token': self.access_token}\n        r = self.spark_api(device_id).GET(params=params, timeout=30)\n        self._check_error(r)\n        return r.json()"}, {"instruction": "def _titan_cn_file(cnr_file, work_dir, data):\n    \"\"\"Convert CNVkit or GATK4 normalized input into TitanCNA ready format.\n    \"\"\"\n", "input": "", "output": "    out_file = os.path.join(work_dir, \"%s.cn\" % (utils.splitext_plus(os.path.basename(cnr_file))[0]))\n    support_cols = {\"cnvkit\": [\"chromosome\", \"start\", \"end\", \"log2\"],\n                    \"gatk-cnv\": [\"CONTIG\", \"START\", \"END\", \"LOG2_COPY_RATIO\"]}\n    cols = support_cols[cnvkit.bin_approach(data)]\n    if not utils.file_uptodate(out_file, cnr_file):\n        with file_transaction(data, out_file) as tx_out_file:\n            iterator = pd.read_table(cnr_file, sep=\"\\t\", iterator=True, header=0, comment=\"@\")\n            with open(tx_out_file, \"w\") as handle:\n                for chunk in iterator:\n                    chunk = chunk[cols]\n                    chunk.columns = [\"chrom\", \"start\", \"end\", \"logR\"]\n                    if cnvkit.bin_approach(data) == \"cnvkit\":\n                        chunk['start'] += 1\n                    chunk.to_csv(handle, mode=\"a\", sep=\"\\t\", index=False)\n    return out_file"}, {"instruction": "def flatten_iterable(iterable):\r\n    \"\"\"flatten iterable, but leaves out strings\r\n\r\n    [[[1, 2, 3], [4, 5]], 6] -> [1, 2, 3, 4, 5, 6]\r\n\r\n    \"\"\"\n", "input": "", "output": "    for item in iterable:\r\n        if isinstance(item, collections.Iterable) and not isinstance(item, basestring):\r\n            for sub in flatten_iterable(item):\r\n                yield sub\r\n        else:\r\n            yield item"}, {"instruction": "def retreive_sigma_mu_data(self):\n        \"\"\"\n        For the general form of the GMPE this retrieves the sigma mu\n        values from the hdf5 file using the \"general\" model, i.e. sigma mu\n        factors that are independent of the choice of region or depth\n        \"\"\"\n", "input": "", "output": "        fle = h5py.File(os.path.join(BASE_PATH,\n                                     \"KothaEtAl2019_SigmaMu_Fixed.hdf5\"), \"r\")\n        self.mags = fle[\"M\"][:]\n        self.dists = fle[\"R\"][:]\n        self.periods = fle[\"T\"][:]\n        self.pga = fle[\"PGA\"][:]\n        self.pgv = fle[\"PGV\"][:]\n        self.s_a = fle[\"SA\"][:]\n        fle.close()"}, {"instruction": "def addAdminResource(self, pluginSubPath: bytes, resource: BasicResource) -> None:\n        \"\"\" Add Site Resource\n\n        Add a cusotom implementation of a served http resource.\n\n        :param pluginSubPath: The resource path where you want to serve this resource.\n        :param resource: The resource to serve.\n        :return: None\n\n        \"\"\"\n", "input": "", "output": "        pluginSubPath = pluginSubPath.strip(b'/')\n        self.__rootAdminResource.putChild(pluginSubPath, resource)"}, {"instruction": "def register_job_definition(self, json_fpath):\n        \"\"\"Register a job definition with AWS Batch, using a JSON\"\"\"\n", "input": "", "output": "        with open(json_fpath) as f:\n            job_def = json.load(f)\n        response = self._client.register_job_definition(**job_def)\n        status_code = response['ResponseMetadata']['HTTPStatusCode']\n        if status_code != 200:\n            msg = 'Register job definition request received status code {0}:\\n{1}'\n            raise Exception(msg.format(status_code, response))\n        return response"}, {"instruction": "def get_qemu_info(path, backing_chain=False, fail_on_error=True):\n    \"\"\"\n    Get info on a given qemu disk\n\n    Args:\n        path(str): Path to the required disk\n        backing_chain(boo): if true, include also info about\n        the image predecessors.\n    Return:\n        object: if backing_chain == True then a list of dicts else a dict\n    \"\"\"\n", "input": "", "output": "\n    cmd = ['qemu-img', 'info', '--output=json', path]\n\n    if backing_chain:\n        cmd.insert(-1, '--backing-chain')\n\n    result = run_command_with_validation(\n        cmd, fail_on_error, msg='Failed to get info for {}'.format(path)\n    )\n\n    return json.loads(result.out)"}, {"instruction": "def check_int(self, param, error_msg):\n        \"\"\"\n        This function check if the parameter is int.\n        If yes, the function returns the parameter,\n        if not, it raises error message.\n        \n        **Args:**\n        \n        * `param` : parameter to check (int or similar)\n\n        * `error_ms` : lowest allowed value (int), or None        \n        \n        **Returns:**\n        \n        * `param` : parameter (int)\n        \"\"\"\n", "input": "", "output": "        if type(param) == int:\n            return int(param)\n        else:\n            raise ValueError(error_msg)"}, {"instruction": "def op_gate_of_type(op: raw_types.Operation,\n                    gate_type: Type[TV]) -> Optional[TV]:\n    \"\"\"Returns the gate of given type, if the op has that gate otherwise None.\n    \"\"\"\n", "input": "", "output": "    if isinstance(op, GateOperation) and isinstance(op.gate, gate_type):\n        return op.gate\n    return None"}, {"instruction": "def consume(self, chars, min=0, max=-1):\n        \"\"\"\n        Consume chars until min/max is satisfied is valid.\n        \"\"\"\n", "input": "", "output": "        return self._src.consume(chars=chars, min=min, max=max)"}, {"instruction": "def log(self, time, message, level=None, attachment=None):\n        \"\"\"Logs a message with attachment.\n\n        The attachment is a dict of:\n            name: name of attachment\n            data: file content\n            mime: content type for attachment\n        \"\"\"\n", "input": "", "output": "        logger.debug(\"log queued\")\n\n        args = {\n            \"time\": time,\n            \"message\": message,\n            \"level\": level,\n            \"attachment\": attachment,\n        }\n        self.queue.put_nowait((\"log\", args))"}, {"instruction": "def reward_bonus(self, assignment_id, amount, reason):\n        \"\"\"Print out bonus info for the assignment\"\"\"\n", "input": "", "output": "        logger.info(\n            'Award ${} for assignment {}, with reason \"{}\"'.format(\n                amount, assignment_id, reason\n            )\n        )"}, {"instruction": "def _setDefaults(configObj={}):\n    \"\"\"set up the default parameters to run drizzle\n       build,single,units,wt_scl,pixfrac,kernel,fillval,\n       rot,scale,xsh,ysh,blotnx,blotny,outnx,outny,data\n\n       Used exclusively for unit-testing, if any are defined.\n\n    \"\"\"\n", "input": "", "output": "\n    paramDict={\"build\":True,\n              \"single\":True,\n              \"stepsize\":10,\n              \"in_units\":\"cps\",\n              \"wt_scl\":1.,\n              \"pixfrac\":1.,\n              \"kernel\":\"square\",\n              \"fillval\":999.,\n              \"maskval\": None,\n              \"rot\":0.,\n              \"scale\":1.,\n              \"xsh\":0.,\n              \"ysh\":0.,\n              \"blotnx\":2048,\n              \"blotny\":2048,\n              \"outnx\":4096,\n              \"outny\":4096,\n              \"data\":None,\n              \"driz_separate\":True,\n              \"driz_combine\":False}\n\n    if(len(configObj) !=0):\n        for key in configObj.keys():\n            paramDict[key]=configObj[key]\n\n    return paramDict"}, {"instruction": "def selection(self):\n        \"\"\"Returns items in selection as a QItemSelection object\"\"\"\n", "input": "", "output": "        sel = QtGui.QItemSelection()\n        for index in self.selectedIndexes():\n            sel.select(index, index)\n        return sel"}, {"instruction": "def nominal_step(x=None):\n    \"\"\"Return nominal step\"\"\"\n", "input": "", "output": "    if x is None:\n        return 1.0\n    return np.log1p(np.abs(x)).clip(min=1.0)"}, {"instruction": "def log_request(self, code='-', size='-'):\n        \"\"\"Logs the current request.\"\"\"\n", "input": "", "output": "        print_size = getattr(thread_local, 'size', -1)\n        if size != '-':\n            size_str = ' (%s)' % size\n        elif print_size >= 0:\n            size_str = self.log_size_string(print_size) + ' '\n        else:\n            size_str = ''\n        if not self.server.suppress_noise or (code != 200 and code != 304):\n            self.log_message(\n                '%s\"%s\" %s', size_str, self.requestline, str(code))\n        if print_size >= 0:\n            thread_local.size = -1"}, {"instruction": "def get_html_output(self):\n        \"\"\" Return line generator. \"\"\"\n", "input": "", "output": "        def html_splitlines(lines):\n            # this cool function was taken from trac.\n            # http://projects.edgewall.com/trac/\n            open_tag_re = re.compile(r'<(\\w+)(\\s.*)?[^/]?>')\n            close_tag_re = re.compile(r'</(\\w+)>')\n            open_tags = []\n            for line in lines:\n                for tag in open_tags:\n                    line = tag.group(0) + line\n                open_tags = []\n                for tag in open_tag_re.finditer(line):\n                    open_tags.append(tag)\n                open_tags.reverse()\n                for ctag in close_tag_re.finditer(line):\n                    for otag in open_tags:\n                        if otag.group(1) == ctag.group(1):\n                            open_tags.remove(otag)\n                            break\n                for tag in open_tags:\n                    line += '</%s>' % tag.group(1)\n                yield line\n\n        if self.error:\n            return escape(self.raw).splitlines()\n        return list(html_splitlines(self.out.getvalue().splitlines()))"}, {"instruction": "def _time_to_datetime(value):\n  \"\"\"Convert a time to a datetime for Cloud Datastore storage.\n\n  Args:\n    value: A datetime.time object.\n\n  Returns:\n    A datetime object with date set to 1970-01-01.\n  \"\"\"\n", "input": "", "output": "  if not isinstance(value, datetime.time):\n    raise TypeError('Cannot convert to datetime expected time value; '\n                    'received %s' % value)\n  return datetime.datetime(1970, 1, 1,\n                           value.hour, value.minute, value.second,\n                           value.microsecond)"}, {"instruction": "def remove_setting(self, section, name, save=False):\n    '''remove a setting from the global config\n    '''\n", "input": "", "output": "    configfile = get_configfile()\n    return _remove_setting(section, name, configfile, save)"}, {"instruction": "def unassign_comment_from_book(self, comment_id, book_id):\n        \"\"\"Removes a ``Comment`` from a ``Book``.\n\n        arg:    comment_id (osid.id.Id): the ``Id`` of the ``Comment``\n        arg:    book_id (osid.id.Id): the ``Id`` of the ``Book``\n        raise:  NotFound - ``comment_id`` or ``book_id`` not found or\n                ``comment_id`` not assigned to ``book_id``\n        raise:  NullArgument - ``comment_id`` or ``book_id`` is ``null``\n        raise:  OperationFailed - unable to complete request\n        raise:  PermissionDenied - authorization failure\n        *compliance: mandatory -- This method must be implemented.*\n\n        \"\"\"\n", "input": "", "output": "        # Implemented from template for\n        # osid.resource.ResourceBinAssignmentSession.unassign_resource_from_bin\n        mgr = self._get_provider_manager('COMMENTING', local=True)\n        lookup_session = mgr.get_book_lookup_session(proxy=self._proxy)\n        lookup_session.get_book(book_id)  # to raise NotFound\n        self._unassign_object_from_catalog(comment_id, book_id)"}, {"instruction": "def load(self, page = None, verbose=False):\n        \"\"\"\n        call to execute the collection loading\n        :param page: integer of the page to load\n        :param verbose: boolean to print to console\n        :returns response\n        :raises the SalesKingException\n        \"\"\"\n", "input": "", "output": "        url = self._build_query_url(page, verbose)\n        response = self._load(url, verbose)\n        response = self._post_load(response, verbose)\n        return response"}, {"instruction": "def branches(self):\n        \"\"\"\n        Returns a data frame of all branches in origin.  The DataFrame will have the columns:\n\n         * repository\n         * local\n         * branch\n\n        :returns: DataFrame\n        \"\"\"\n", "input": "", "output": "\n        df = pd.DataFrame(columns=['repository', 'local', 'branch'])\n\n        if _has_joblib:\n            ds = Parallel(n_jobs=-1, backend='threading', verbose=0)(\n                delayed(_branches_func)\n                (x) for x in self.repos\n            )\n            for d in ds:\n                df = df.append(d)\n        else:\n            for repo in self.repos:\n                try:\n                    df = df.append(_branches_func(repo))\n                except GitCommandError:\n                    print('Warning! Repo: %s couldn\\'t be inspected' % (repo, ))\n\n        df.reset_index()\n\n        return df"}, {"instruction": "def rescale_gradients(self, scale: float):\n        \"\"\"\n        Rescales gradient arrays of executors by scale.\n        \"\"\"\n", "input": "", "output": "        for exe in self.executors:\n            for arr in exe.grad_arrays:\n                if arr is None:\n                    continue\n                arr *= scale"}, {"instruction": "def new(cls, freeform_builder, x, y):\n        \"\"\"Return a new _LineSegment object ending at point *(x, y)*.\n\n        Both *x* and *y* are rounded to the nearest integer before use.\n        \"\"\"\n", "input": "", "output": "        return cls(freeform_builder, int(round(x)), int(round(y)))"}, {"instruction": "async def invites(self):\n        \"\"\"|coro|\n\n        Returns a list of all active instant invites from the guild.\n\n        You must have the :attr:`~Permissions.manage_guild` permission to get\n        this information.\n\n        Raises\n        -------\n        Forbidden\n            You do not have proper permissions to get the information.\n        HTTPException\n            An error occurred while fetching the information.\n\n        Returns\n        -------\n        List[:class:`Invite`]\n            The list of invites that are currently active.\n        \"\"\"\n", "input": "", "output": "\n        data = await self._state.http.invites_from(self.id)\n        result = []\n        for invite in data:\n            channel = self.get_channel(int(invite['channel']['id']))\n            invite['channel'] = channel\n            invite['guild'] = self\n            result.append(Invite(state=self._state, data=invite))\n\n        return result"}, {"instruction": "def format_page(self, page):\n        \"\"\"\n        Banana banana\n        \"\"\"\n", "input": "", "output": "        self.formatting_page_signal(self, page)\n        return self._format_page(page)"}, {"instruction": "def sleep_if_necessary(cls, user, token, endpoint='search', msg=''):\n        \"\"\"Sleep a little if hit github recently to honor rate limit.\n        \"\"\"\n", "input": "", "output": "        my_kw = {'auth': (user, token)} if user else {}\n        info = requests.get('https://api.github.com/rate_limit', **my_kw)\n        info_dict = info.json()\n        remaining = info_dict['resources'][endpoint]['remaining']\n        logging.debug('Search remaining on github is at %s', remaining)\n\n        if remaining <= 5:\n            sleep_time = 120\n        else:\n            sleep_time = 0\n        if sleep_time:\n            logging.warning('Sleep %i since github requests remaining  = %i%s',\n                            sleep_time, remaining, msg)\n            time.sleep(sleep_time)\n            return True\n\n        return False"}, {"instruction": "def buffer_iter(self, block_size=1024):\n        \"\"\"\n        Iterate through chunks of the vertices, and indices buffers seamlessly.\n\n        .. note::\n\n            To see a usage example, look at the :class:`ShapeBuffer` description.\n        \"\"\"\n", "input": "", "output": "        streams = (\n            self.vert_data,\n            self.idx_data,\n        )\n\n        # Chain streams seamlessly\n        for stream in streams:\n            stream.seek(0)\n            while True:\n                chunk = stream.read(block_size)\n                if chunk:\n                    yield chunk\n                else:\n                    break"}, {"instruction": "def remove(self):\n        \"\"\"Remove this file.\"\"\"\n", "input": "", "output": "        if self.exists() or self.islink():\n            self.fs.unlink(self.get_internal_path())\n            return 1\n        return None"}, {"instruction": "def context_changed(self, context):\n        \"\"\" :type context: dict \"\"\"\n", "input": "", "output": "        self._image.set_cmap(context['colormap'])\n        self._image.set_clim(context['min'], context['max'])\n        self._image.set_interpolation(context['interpolation'])\n        self._update_indicators(context)\n\n        self._set_view_limits()\n\n        if self._model.index_direction is not SliceDirection.depth:\n            self._image.axes.set_ylabel(context['samples_unit'])"}, {"instruction": "def handle_not_found(exception, **extra):\n    \"\"\"Custom blueprint exception handler.\"\"\"\n", "input": "", "output": "    assert isinstance(exception, NotFound)\n\n    page = Page.query.filter(db.or_(Page.url == request.path,\n                                    Page.url == request.path + \"/\")).first()\n\n    if page:\n        _add_url_rule(page.url)\n        return render_template(\n            [\n                page.template_name,\n                current_app.config['PAGES_DEFAULT_TEMPLATE']\n            ],\n            page=page\n        )\n    elif 'wrapped' in extra:\n        return extra['wrapped'](exception)\n    else:\n        return exception"}, {"instruction": "def d2Ibr_dV2(Ybr, V, lam):\n    \"\"\" Computes 2nd derivatives of complex branch current w.r.t. voltage.\n    \"\"\"\n", "input": "", "output": "    nb = len(V)\n    diaginvVm = spdiag(div(matrix(1.0, (nb, 1)), abs(V)))\n\n    Haa = spdiag(mul(-(Ybr.T * lam), V))\n    Hva = -1j * Haa * diaginvVm\n    Hav = Hva\n    Hvv = spmatrix([], [], [], (nb, nb))\n\n    return Haa, Hav, Hva, Hvv"}, {"instruction": "def unpack_grad_tuple(gv, gpt):\n    \"\"\"Unpack a previously packed collection of gradient tensors.\n\n  Args:\n    gv: A (grad, var) pair to be unpacked.\n    gpt: A GradPackTuple describing the packing operation that produced gv.\n\n  Returns:\n    A list of (grad, var) pairs corresponding to the values that were\n     originally packed into gv, maybe following subsequent operations like\n     reduction.\n  \"\"\"\n", "input": "", "output": "    elt_widths = [x.num_elements() for x in gpt.shapes]\n    with tf.device(gv[0][0].device):\n        with tf.name_scope(\"unpack\"):\n            splits = tf.split(gv[0], elt_widths)\n            unpacked_gv = []\n            for idx, s in enumerate(splits):\n                unpacked_gv.append((tf.reshape(s, gpt.shapes[idx]),\n                                    gpt.vars[idx]))\n    return unpacked_gv"}, {"instruction": "def load(self, cellpy_file, parent_level=\"CellpyData\"):\n        \"\"\"Loads a cellpy file.\n\n        Args:\n            cellpy_file (path, str): Full path to the cellpy file.\n            parent_level (str, optional): Parent level\n\n        \"\"\"\n", "input": "", "output": "\n        try:\n            self.logger.debug(\"loading cellpy-file (hdf5):\")\n            self.logger.debug(cellpy_file)\n            new_datasets = self._load_hdf5(cellpy_file, parent_level)\n            self.logger.debug(\"cellpy-file loaded\")\n        except AttributeError:\n            new_datasets = []\n            self.logger.warning(\"This cellpy-file version is not supported by\"\n                                \"current reader (try to update cellpy).\")\n\n        if new_datasets:\n            for dataset in new_datasets:\n                self.datasets.append(dataset)\n        else:\n            # raise LoadError\n            self.logger.warning(\"Could not load\")\n            self.logger.warning(str(cellpy_file))\n\n        self.number_of_datasets = len(self.datasets)\n        self.status_datasets = self._validate_datasets()\n        self._invent_a_name(cellpy_file)\n        return self"}, {"instruction": "def close(self):\n        \"\"\"\n        Close the file.\n        \"\"\"\n", "input": "", "output": "        if not self.closed:\n            self.closed = True\n            retval = self.f.close()\n            if self.base_mode != \"r\":\n                self.__size = self.fs.get_path_info(self.name)[\"size\"]\n            return retval"}, {"instruction": "def recherche(self, pattern, entete):\n        \"\"\"Performs a search field by field, using functions defined in formats.\n        Matchs are marked with info[`font`]\n\n        :param pattern: String to look for\n        :param entete: Fields to look into\n        :return: Nothing. The collection is changed in place\n        \"\"\"\n", "input": "", "output": "\n        new_liste = []\n        sub_patterns = pattern.split(\" \")\n        for p in self:\n            d_font = {att: False for att in entete}\n            row_valid = True\n            for sub_pattern in sub_patterns:\n                found = False\n                for att in entete:\n                    fonction_recherche = formats.ASSOCIATION[att][1]\n                    attr_found = bool(fonction_recherche(p[att], sub_pattern))\n                    if attr_found:\n                        found = True\n                        d_font[att] = True\n                if not found:\n                    row_valid = False\n                    break\n            if row_valid:\n                new_liste.append(p)\n                info = dict(self.get_info(Id=p.Id),font=d_font)\n                self.infos[p.Id] = info\n\n        list.__init__(self, new_liste)"}, {"instruction": "def on_event(self, evt, is_final):\n        \"\"\" this is invoked from in response to COM PumpWaitingMessages - different thread \"\"\"\n", "input": "", "output": "        for msg in XmlHelper.message_iter(evt):\n            for node, error in XmlHelper.security_iter(msg.GetElement('securityData')):\n                if error:\n                    self.security_errors.append(error)\n                else:\n                    self.on_security_node(node)\n\n        if is_final and self.response_type == 'frame':\n            index = self.response.pop('security')\n            frame = DataFrame(self.response, columns=self.fields, index=index)\n            frame.index.name = 'security'\n            self.response = frame"}, {"instruction": "def cache(self):\n      \"\"\"Caches the result of loader(filename) to cachename.\"\"\"\n", "input": "", "output": "      msg = 'Saving updates from more recent \"%s\" to \"%s\"'\n      log.info(msg, self.filename, self.cachename)\n      with open(self.cachename, 'wb') as output:\n          cPickle.dump(self._dict, output, -1)"}, {"instruction": "def post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Reassign Storage Adapter Port (requires DPM mode).\"\"\"\n", "input": "", "output": "        assert wait_for_completion is True  # async not supported yet\n        partition_oid = uri_parms[0]\n        partition_uri = '/api/partitions/' + partition_oid\n        hba_oid = uri_parms[1]\n        hba_uri = '/api/partitions/' + partition_oid + '/hbas/' + hba_oid\n        try:\n            hba = hmc.lookup_by_uri(hba_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        partition = hmc.lookup_by_uri(partition_uri)  # assert it exists\n        cpc = partition.manager.parent\n        assert cpc.dpm_enabled\n        check_valid_cpc_status(method, uri, cpc)\n        check_partition_status(method, uri, partition,\n                               invalid_statuses=['starting', 'stopping'])\n        check_required_fields(method, uri, body, ['adapter-port-uri'])\n\n        # Reflect the effect of the operation on the HBA\n        new_port_uri = body['adapter-port-uri']\n        hba.properties['adapter-port-uri'] = new_port_uri"}, {"instruction": "def raw(self, klass, _name=\"default\", **attributes):\n        \"\"\"\n        Get the raw attribute dict for a given named model.\n\n        :param klass: The class\n        :type klass: class\n\n        :param _name: The type\n        :type _name: str\n\n        :param attributes: The instance attributes\n        :type attributes: dict\n\n        :return: dict\n        \"\"\"\n", "input": "", "output": "        raw = self._definitions[klass][_name](self._faker)\n\n        raw.update(attributes)\n\n        return raw"}, {"instruction": "def torrent_from_url(self, url, cache=True, prefetch=False):\n        \"\"\"Create a Torrent object from a given URL.\n\n        If the cache option is set, check to see if we already have a Torrent\n        object representing it. If prefetch is set, automatically query the\n        torrent's info page to fill in the torrent object. (If prefetch is\n        false, then the torrent page will be queried lazily on-demand.)\n\n        \"\"\"\n", "input": "", "output": "        if self._use_cache(cache) and url in self._torrent_cache:\n            return self._torrent_cache[url]\n        torrent = Torrent(url, cache, prefetch)\n        if cache:\n            self._torrent_cache[url] = torrent\n        return torrent"}, {"instruction": "def set_unavailable(self):\n        \"\"\"Sets the agent availability to False.\"\"\"\n", "input": "", "output": "        show = PresenceShow.NONE\n        self.set_presence(PresenceState(available=False, show=show))"}, {"instruction": "def p_field_optional1_3(self, p):\n        \"\"\"\n        field : alias name arguments selection_set\n        \"\"\"\n", "input": "", "output": "        p[0] = Field(name=p[2], alias=p[1], arguments=p[3], selections=p[4])"}, {"instruction": "def mask(args):\n    \"\"\"\n    %prog mask fastafile\n\n    Mask the contaminants. By default, this will compare against UniVec_Core and\n    Ecoli.fasta. Merge the contaminant results, and use `maskFastaFromBed`. Can\n    perform FASTA tidy if requested.\n    \"\"\"\n", "input": "", "output": "    p = OptionParser(mask.__doc__)\n    p.add_option(\"--db\",\n                 help=\"Contaminant db other than Ecoli K12 [default: %default]\")\n    opts, args = p.parse_args(args)\n\n    if len(args) != 1:\n        sys.exit(not p.print_help())\n\n    fastafile, = args\n    assert op.exists(fastafile)\n\n    outfastafile = fastafile.rsplit(\".\", 1)[0] + \".masked.fasta\"\n    vecbedfile = blast([fastafile])\n    ecoliurl = \\\n    \"ftp://ftp.ncbi.nih.gov/genomes/Bacteria/Escherichia_coli_K_12_substr__DH10B_uid58979/NC_010473.fna\"\n    ecolifile = opts.db or download(ecoliurl, filename=\"Ecoli.fasta\")\n    assert op.exists(ecolifile)\n    ecolibedfile = blast([fastafile, \"--db={0}\".format(ecolifile)])\n\n    cmd = \"cat {0} {1}\".format(vecbedfile, ecolibedfile)\n    cmd += \" | mergeBed -nms -d 100 -i stdin\"\n    cmd += \" | maskFastaFromBed -fi {0} -bed stdin -fo {1}\".\\\n            format(fastafile, outfastafile)\n    sh(cmd)\n\n    return tidy([outfastafile])"}, {"instruction": "def cancel_port_forward(self, address, port):\n        \"\"\"\n        Ask the server to cancel a previous port-forwarding request.  No more\n        connections to the given address & port will be forwarded across this\n        ssh connection.\n\n        :param str address: the address to stop forwarding\n        :param int port: the port to stop forwarding\n        \"\"\"\n", "input": "", "output": "        if not self.active:\n            return\n        self._tcp_handler = None\n        self.global_request(\"cancel-tcpip-forward\", (address, port), wait=True)"}, {"instruction": "def chown(self, path, uid, gid):\n        \"\"\"\n        Change the owner (``uid``) and group (``gid``) of a file.  As with\n        Python's `os.chown` function, you must pass both arguments, so if you\n        only want to change one, use `stat` first to retrieve the current\n        owner and group.\n\n        :param str path: path of the file to change the owner and group of\n        :param int uid: new owner's uid\n        :param int gid: new group id\n        \"\"\"\n", "input": "", "output": "        path = self._adjust_cwd(path)\n        self._log(DEBUG, \"chown({!r}, {!r}, {!r})\".format(path, uid, gid))\n        attr = SFTPAttributes()\n        attr.st_uid, attr.st_gid = uid, gid\n        self._request(CMD_SETSTAT, path, attr)"}, {"instruction": "def lookup_family_by_name(name):\n    \"\"\"https://github.com/thom311/libnl/blob/libnl3_2_25/lib/genl/mngt.c#L106.\n\n    Positional arguments:\n    name -- string.\n\n    Returns:\n    genl_ops class instance or None.\n    \"\"\"\n", "input": "", "output": "    for ops in nl_list_for_each_entry(genl_ops(), genl_ops_list, 'o_list'):\n        if ops.o_name == name:\n            return ops\n    return None"}, {"instruction": "def fill_auth_list_from_groups(self, auth_provider, user_groups, auth_list):\n        '''\n        Returns a list of authorisation matchers that a user is eligible for.\n        This list is a combination of the provided personal matchers plus the\n        matchers of any group the user is in.\n        '''\n", "input": "", "output": "        group_names = [item for item in auth_provider if item.endswith('%')]\n        if group_names:\n            for group_name in group_names:\n                if group_name.rstrip(\"%\") in user_groups:\n                    for matcher in auth_provider[group_name]:\n                        auth_list.append(matcher)\n        return auth_list"}, {"instruction": "def update_distant_reference(self, ref):\n        \"\"\"Validate and update the reference in Zotero.\n\n        Existing fields not present will be left unmodified.\n        \"\"\"\n", "input": "", "output": "        self.validate_reference_data(ref[\"data\"])\n        self._zotero_lib.update_item(ref)"}, {"instruction": "def list(self, identity_id, per_page=20, page=1):\n        \"\"\" Get a list of tokens\n\n            :param identity_id: The ID of the identity to retrieve tokens for\n            :param per_page: The number of results per page returned\n            :param page: The page number of the results\n            :return: dict of REST API output with headers attached\n            :rtype: :class:`~datasift.request.DictResponse`\n            :raises: :class:`~datasift.exceptions.DataSiftApiException`,\n                :class:`requests.exceptions.HTTPError`\n        \"\"\"\n", "input": "", "output": "\n        params = {'per_page': per_page, 'page': page}\n\n        return self.request.get(str(identity_id) + '/token', params)"}, {"instruction": "def set_iam_policy(self, policy):\n        \"\"\"Sets the access control policy on an instance resource. Replaces any\n        existing policy.\n\n        For more information about policy, please see documentation of\n        class `google.cloud.bigtable.policy.Policy`\n\n        For example:\n\n        .. literalinclude:: snippets.py\n            :start-after: [START bigtable_set_iam_policy]\n            :end-before: [END bigtable_set_iam_policy]\n\n        :type policy: :class:`google.cloud.bigtable.policy.Policy`\n        :param policy: A new IAM policy to replace the current IAM policy\n                       of this instance\n\n        :rtype: :class:`google.cloud.bigtable.policy.Policy`\n        :returns: The current IAM policy of this instance.\n        \"\"\"\n", "input": "", "output": "        instance_admin_client = self._client.instance_admin_client\n        resp = instance_admin_client.set_iam_policy(\n            resource=self.name, policy=policy.to_pb()\n        )\n        return Policy.from_pb(resp)"}, {"instruction": "def format_rst(self):\n        \"\"\"\n        return table in RST format\n        \"\"\"\n", "input": "", "output": "        res = ''\n        num_cols = len(self.header)\n        col_width = 25\n        for _ in range(num_cols):\n            res += ''.join(['=' for _ in range(col_width - 1)]) + ' ' \n        res += '\\n'\n        for c in self.header:\n            res += c.ljust(col_width) \n        res += '\\n'\n        for _ in range(num_cols):\n            res += ''.join(['=' for _ in range(col_width - 1)]) + ' ' \n        res += '\\n'\n        for row in self.arr:\n            for c in row:\n                res += self.force_to_string(c).ljust(col_width)\n            res += '\\n' \n        for _ in range(num_cols):\n            res += ''.join(['=' for _ in range(col_width - 1)]) + ' ' \n        res += '\\n'\n        return res"}, {"instruction": "def set_install_dir(self, install_dir=None, version=None, verbose=False):\n        \"\"\"\n        Sets the path to the cassandra source directory for use by this node.\n        \"\"\"\n", "input": "", "output": "        if version is None:\n            self.__install_dir = install_dir\n            if install_dir is not None:\n                common.validate_install_dir(install_dir)\n        else:\n            self.__install_dir = self.node_setup(version, verbose=verbose)\n\n        self._cassandra_version = common.get_version_from_build(self.__install_dir, cassandra=True)\n\n        if self.get_base_cassandra_version() >= 4.0:\n            self.network_interfaces['thrift'] = None\n\n        self.import_config_files()\n        self.import_bin_files()\n        self.__conf_updated = False\n        return self"}, {"instruction": "def redef(obj, key, value, **kwargs):\n    '''A static constructor helper function'''\n", "input": "", "output": "    return Redef(obj, key, value=value, **kwargs)"}, {"instruction": "def is_parent_of(page1, page2):\n    \"\"\"\n    Determines whether a given page is the parent of another page\n\n    Example::\n\n        {% if page|is_parent_of:feincms_page %} ... {% endif %}\n    \"\"\"\n", "input": "", "output": "\n    try:\n        return page1.tree_id == page2.tree_id and page1.lft < page2.lft and page1.rght > page2.rght\n    except AttributeError:\n        return False"}, {"instruction": "def render_GET(self, request):\n    \"\"\"Renders a GET request, by showing this nodes stats and children.\"\"\"\n", "input": "", "output": "    fullPath = request.path.split('/')\n    if not fullPath[-1]:\n      fullPath = fullPath[:-1]\n    parts = fullPath[2:]\n    statDict = util.lookup(scales.getStats(), parts)\n\n    if statDict is None:\n      request.setResponseCode(404)\n      return \"Path not found.\"\n\n    if 'query' in request.args:\n      query = request.args['query'][0]\n    else:\n      query = None\n\n    if 'format' in request.args and request.args['format'][0] == 'json':\n      request.headers['content-type'] = 'text/javascript; charset=UTF-8'\n      formats.jsonFormat(request, statDict, query)\n    elif 'format' in request.args and request.args['format'][0] == 'prettyjson':\n      request.headers['content-type'] = 'text/javascript; charset=UTF-8'\n      formats.jsonFormat(request, statDict, query, pretty=True)\n    else:\n      formats.htmlHeader(request, '/' + '/'.join(parts), self.serverName, query)\n      formats.htmlFormat(request, tuple(parts), statDict, query)\n\n    return ''"}, {"instruction": "def parse_example_line(lisp_string: str) -> Dict:\n    \"\"\"\n    Training data in WikitableQuestions comes with examples in the form of lisp strings in the format:\n        (example (id <example-id>)\n                 (utterance <question>)\n                 (context (graph tables.TableKnowledgeGraph <table-filename>))\n                 (targetValue (list (description <answer1>) (description <answer2>) ...)))\n\n    We parse such strings and return the parsed information here.\n    \"\"\"\n", "input": "", "output": "    id_piece, rest = lisp_string.split(') (utterance \"')\n    example_id = id_piece.split('(id ')[1]\n    question, rest = rest.split('\") (context (graph tables.TableKnowledgeGraph ')\n    table_filename, rest = rest.split(')) (targetValue (list')\n    target_value_strings = rest.strip().split(\"(description\")\n    target_values = []\n    for string in target_value_strings:\n        string = string.replace(\")\", \"\").replace('\"', '').strip()\n        if string != \"\":\n            target_values.append(string)\n    return {'id': example_id,\n            'question': question,\n            'table_filename': table_filename,\n            'target_values': target_values}"}, {"instruction": "def backward_delete_char(self, e): # (Rubout)\r\n        u\"\"\"Delete the character behind the cursor. A numeric argument means\r\n        to kill the characters instead of deleting them.\"\"\"\n", "input": "", "output": "        self.l_buffer.backward_delete_char(self.argument_reset)\r\n        self.finalize()"}, {"instruction": "def get_versioned_delete_collector_class():\n    \"\"\"\n    Gets the class to use for deletion collection.\n\n    :return: class\n    \"\"\"\n", "input": "", "output": "    key = 'VERSIONED_DELETE_COLLECTOR'\n    try:\n        cls = _cache[key]\n    except KeyError:\n        collector_class_string = getattr(settings, key)\n        cls = import_from_string(collector_class_string, key)\n        _cache[key] = cls\n    return cls"}, {"instruction": "def trace_integrations(integrations, tracer=None):\n    \"\"\"Enable tracing on the selected integrations.\n    :type integrations: list\n    :param integrations: The integrations to be traced.\n    \"\"\"\n", "input": "", "output": "    integrated = []\n\n    for item in integrations:\n        module_name = 'opencensus.ext.{}.trace'.format(item)\n        try:\n            module = importlib.import_module(module_name)\n            module.trace_integration(tracer=tracer)\n            integrated.append(item)\n        except Exception as e:\n            log.warning('Failed to integrate module: {}'.format(module_name))\n            log.warning('{}'.format(e))\n\n    return integrated"}, {"instruction": "def get_packages_of_type(self, package_types, mask=None):\n        \"\"\"Get packages that match a certain type.\n\n        Each ordering package has a type, so return all packages that match\n        the types we are looking for\n\n        :param list package_types: List of strings representing the package\n                                   type keynames we are interested in.\n        :param string mask: Mask to specify the properties we want to retrieve\n        \"\"\"\n", "input": "", "output": "\n        _filter = {\n            'type': {\n                'keyName': {\n                    'operation': 'in',\n                    'options': [\n                        {'name': 'data',\n                         'value': package_types}\n                    ],\n                },\n            },\n        }\n\n        packages = self.package_svc.getAllObjects(mask=mask, filter=_filter)\n        packages = self.filter_outlet_packages(packages)\n        return packages"}, {"instruction": "def record(self):\n        # type: () -> bytes\n        '''\n        A method to generate the string representing this UDF ICB Tag.\n\n        Parameters:\n         None.\n        Returns:\n         A string representing this UDF ICB Tag.\n        '''\n", "input": "", "output": "        if not self._initialized:\n            raise pycdlibexception.PyCdlibInternalError('UDF ICB Tag not initialized')\n\n        return struct.pack(self.FMT, self.prior_num_direct_entries,\n                           self.strategy_type, self.strategy_param,\n                           self.max_num_entries, 0, self.file_type,\n                           self.parent_icb_log_block_num,\n                           self.parent_icb_part_ref_num, self.flags)"}, {"instruction": "def _bell(self):\n        u'''ring the bell if requested.'''\n", "input": "", "output": "        if self.bell_style == u'none':\n            pass\n        elif self.bell_style == u'visible':\n            raise NotImplementedError(u\"Bellstyle visible is not implemented yet.\")\n        elif self.bell_style == u'audible':\n            self.console.bell()\n        else:\n            raise ReadlineError(u\"Bellstyle %s unknown.\"%self.bell_style)"}, {"instruction": "def _select_row_by_column_value(tree_view, list_store, column, value):\n        \"\"\"Helper method to select a tree view row\n\n        :param Gtk.TreeView tree_view: Tree view who's row is to be selected\n        :param Gtk.ListStore list_store: List store of the tree view\n        :param int column: Column in which the value is searched\n        :param value: Value to search for\n        :returns: Row of list store that has selected\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "        for row_num, iter_elem in enumerate(list_store):\n            if iter_elem[column] == value:\n                tree_view.set_cursor(row_num)\n                return row_num"}, {"instruction": "def get_trunk_interfaces(devId):\n    \"\"\"Function takes devId as input to RESTFULL call to HP IMC platform\n    :param devId: output of get_dev_details\n    :return: list of dictionaries containing of interfaces configured as an 802.1q trunk\n    \"\"\"\n", "input": "", "output": "\n    # checks to see if the imc credentials are already available\n    if auth is None or url is None:\n        set_imc_creds()\n    global r\n    get_trunk_interfaces_url = \"/imcrs/vlan/trunk?devId=\" + str(devId) + \"&start=1&size=5000&total=false\"\n    f_url = url + get_trunk_interfaces_url\n    payload = None\n    # creates the URL using the payload variable as the contents\n    r = requests.get(f_url, auth=auth, headers=headers)\n    # r.status_code\n    if r.status_code == 200:\n        dev_trunk_interfaces = (json.loads(r.text))\n        if len(dev_trunk_interfaces) == 2:\n            return dev_trunk_interfaces['trunkIf']\n        else:\n            dev_trunk_interfaces['trunkIf'] = [\"No trunk inteface\"]\n            return dev_trunk_interfaces['trunkIf']"}, {"instruction": "def _entry_must_exist(df, k1, k2):\n    \"\"\"Evaluate key-subkey existence.\n\n    Checks that the key-subkey combo exists in the\n    configuration options.\n    \"\"\"\n", "input": "", "output": "    count = df[(df['k1'] == k1) &\n               (df['k2'] == k2)].shape[0]\n    if count == 0:\n        raise NotRegisteredError(\n            \"Option {0}.{1} not registered\".format(k1, k2))"}, {"instruction": "async def sources(client: Client, pubkey: str) -> dict:\n    \"\"\"\n    GET transaction sources\n\n    :param client: Client to connect to the api\n    :param pubkey: Public key\n    :return:\n    \"\"\"\n", "input": "", "output": "    return await client.get(MODULE + '/sources/%s' % pubkey, schema=SOURCES_SCHEMA)"}, {"instruction": "def clean(self):\n        \"\"\"\n        Automatically construct the suggestion input and weight by taking all\n        possible permutation of Person's name as ``input`` and taking their\n        popularity as ``weight``.\n        \"\"\"\n", "input": "", "output": "        self.suggest = {\n            'input': [' '.join(p) for p in permutations(self.name.split())],\n            'weight': self.popularity\n        }"}, {"instruction": "def stop_program(self, turn_off_load=True):\n        \"\"\"\n        Stops running programmed test sequence\n        :return: None\n        \"\"\"\n", "input": "", "output": "        self.__set_buffer_start(self.CMD_STOP_PROG)\n        self.__set_checksum()\n        self.__send_buffer()\n        if turn_off_load and self.load_on:\n            self.load_on = False"}, {"instruction": "def init_widget(self):\n        \"\"\" Initialize the underlying widget.\n\n        \"\"\"\n", "input": "", "output": "        super(AndroidTextureView, self).__init__(self)\n        w = self.widget\n        w.setSurfaceTextureListener(w.getId())\n        w.onSurfaceTextureAvailable.connect(self.on_surface_texture_available)\n        w.onSurfaceTextureDestroyed.connect(self.on_surface_texture_destroyed)\n        w.onSurfaceTextureChanged.connect(self.on_surface_texture_changed)\n        w.onSurfaceTextureUpdated.connect(self.on_surface_texture_updated)"}, {"instruction": "def help(ctx, topic, **kw):\n    \"\"\"Show help for any command.\n    \"\"\"\n", "input": "", "output": "    # The help command implementation is taken from\n    # https://www.burgundywall.com/post/having-click-help-subcommand\n    if topic is None:\n        click.echo(ctx.parent.get_help())\n    else:\n        click.echo(main.commands[topic].get_help(ctx))"}, {"instruction": "def update(self, cardconnection, ccevent):\n        '''CardConnectionObserver callback.'''\n", "input": "", "output": "\n        apduline = \"\"\n        if 'connect' == ccevent.type:\n            apduline += 'connecting to ' + cardconnection.getReader()\n\n        elif 'disconnect' == ccevent.type:\n            apduline += 'disconnecting from ' + cardconnection.getReader()\n\n        elif 'command' == ccevent.type:\n            apduline += '> ' + toHexString(ccevent.args[0])\n\n        elif 'response' == ccevent.type:\n            if [] == ccevent.args[0]:\n                apduline += \"< %-2X %-2X\" % tuple(ccevent.args[-2:])\n            else:\n                apduline += \"< \" + toHexString(ccevent.args[0]) + \\\n                            \"%-2X %-2X\" % tuple(ccevent.args[-2:])\n\n        self.apdutextctrl.AppendText(apduline + \"\\n\")"}, {"instruction": "def every_other(x, name=None):\n  \"\"\"Drops every other value from the tensor and returns a 1D tensor.\n\n  This is useful if you are running multiple inputs through a model tower\n  before splitting them and you want to line it up with some other data.\n\n  Args:\n    x: the target tensor.\n    name: the name for this op, defaults to every_other\n  Returns:\n    A tensorflow op.\n  \"\"\"\n", "input": "", "output": "  with tf.name_scope(name, 'every_other', [x]) as scope:\n    x = tf.convert_to_tensor(x, name='x')\n    return tf.reshape(\n        tf.slice(\n            tf.reshape(x, [-1, 2]), [0, 0], [-1, 1]),\n        [-1],\n        name=scope)"}, {"instruction": "def change_speed(body, speed=1):\n    \"\"\"Change the voice speed of the wave body.\"\"\"\n", "input": "", "output": "    if speed == 1:\n        return body\n\n    length = int(len(body) * speed)\n    rv = bytearray(length)\n\n    step = 0\n    for v in body:\n        i = int(step)\n        while i < int(step + speed) and i < length:\n            rv[i] = v\n            i += 1\n        step += speed\n    return rv"}, {"instruction": "def conv(col, fromBase, toBase):\n    \"\"\"\n    Convert a number in a string column from one base to another.\n\n    >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n    >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n    [Row(hex=u'15')]\n    \"\"\"\n", "input": "", "output": "    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.conv(_to_java_column(col), fromBase, toBase))"}, {"instruction": "def _fault_to_exception(f):\n    \"\"\" Converts XML-RPC Fault objects to Pynipap-exceptions.\n\n        TODO: Is this one neccesary? Can be done inline...\n    \"\"\"\n", "input": "", "output": "\n    e = _fault_to_exception_map.get(f.faultCode)\n    if e is None:\n        e = NipapError\n    return e(f.faultString)"}, {"instruction": "def get_features(self, jid):\n        \"\"\"\n        Return the features supported by a service.\n\n        :param jid: Address of the PubSub service to query.\n        :type jid: :class:`aioxmpp.JID`\n        :return: Set of supported features\n        :rtype: set containing :class:`~.pubsub.xso.Feature` enumeration\n                members.\n\n        This simply uses service discovery to obtain the set of features and\n        converts the features to :class:`~.pubsub.xso.Feature` enumeration\n        members. To get the full feature information, resort to using\n        :meth:`.DiscoClient.query_info` directly on `jid`.\n\n        Features returned by the peer which are not valid pubsub features are\n        not returned.\n        \"\"\"\n", "input": "", "output": "\n        response = yield from self._disco.query_info(jid)\n        result = set()\n        for feature in response.features:\n            try:\n                result.add(pubsub_xso.Feature(feature))\n            except ValueError:\n                continue\n        return result"}, {"instruction": "def schedule_and_propagate_host_downtime(self, host, start_time, end_time,\n                                             fixed, trigger_id, duration, author, comment):\n        \"\"\"DOES NOTHING (Should create host downtime and start it?)\n        Format of the line that triggers function call::\n\n        SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME;<host_name>;<start_time>;<end_time>;\n        <fixed>;<trigger_id>;<duration>;<author>;<comment>\n\n        :return: None\n        \"\"\"\n", "input": "", "output": "        logger.warning(\"The external command 'SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME' \"\n                       \"is not currently implemented in Alignak. If you really need it, \"\n                       \"request for its implementation in the project repository: \"\n                       \"https://github.com/Alignak-monitoring/alignak\")\n        self.send_an_element(make_monitoring_log(\n            'warning', 'SCHEDULE_AND_PROPAGATE_HOST_DOWNTIME: this command is not implemented!'))"}, {"instruction": "def request_type(self):\n        \"\"\"Retrieve the type of the request, by fetching it from\n        `xenon.proto.xenon_pb2`.\"\"\"\n", "input": "", "output": "        if self.static and not self.uses_request:\n            return getattr(xenon_pb2, 'Empty')\n\n        if not self.uses_request:\n            return None\n\n        return getattr(xenon_pb2, self.request_name)"}, {"instruction": "def projection_box(self, min_x, min_y, max_x, max_y):\n        \"\"\"Add a bounding box in projected (native) coordinates to the query.\n\n        This adds a request for a spatial bounding box, bounded by (`min_x`, `max_x`) for\n        x direction and (`min_y`, `max_y`) for the y direction. This modifies the query\n        in-place, but returns ``self`` so that multiple queries can be chained together\n        on one line.\n\n        This replaces any existing spatial queries that have been set.\n\n        Parameters\n        ----------\n        min_x : float\n            The left edge of the bounding box\n        min_y : float\n            The bottom edge of the bounding box\n        max_x : float\n            The right edge of the bounding box\n        max_y: float\n            The top edge of the bounding box\n\n        Returns\n        -------\n        self : NCSSQuery\n            Returns self for chaining calls\n\n        \"\"\"\n", "input": "", "output": "        self._set_query(self.spatial_query, minx=min_x, miny=min_y,\n                        maxx=max_x, maxy=max_y)\n        return self"}, {"instruction": "def extract_points(self, pid, points):\n        \"\"\"Extract values at certain points in the grid from a given parameter\n        set. Cells are selected by interpolating the centroids of the cells\n        towards the line using a \"nearest\" scheme.\n\n        Note that data is only returned for the points provided. If you want to\n        extract multiple data points along a line, defined by start and end\n        point, use the **extract_along_line** function.\n\n        Parameters\n        ----------\n        pid: int\n            The parameter id to extract values from\n        points: Nx2 numpy.ndarray\n            (x, y) pairs\n\n        Returns\n        -------\n        values: numpy.ndarray (n x 1)\n            data values for extracted data points\n        \"\"\"\n", "input": "", "output": "        xy = self.grid.get_element_centroids()\n        data = self.parsets[pid]\n\n        iobj = spi.NearestNDInterpolator(xy, data)\n        values = iobj(points)\n        return values"}, {"instruction": "def _save_stdin(self, stdin):\n\t\t\"\"\"\n\t\tCreates a temporary dir (self.temp_dir) and saves the given input\n\t\tstream to a file within that dir. Returns the path to the file. The dir\n\t\tis removed in the __del__ method.\n\t\t\"\"\"\n", "input": "", "output": "\t\tself.temp_dir = TemporaryDirectory()\n\t\tfile_path = os.path.join(self.temp_dir.name, 'dataset')\n\n\t\ttry:\n\t\t\twith open(file_path, 'w') as f:\n\t\t\t\tfor line in stdin:\n\t\t\t\t\tf.write(line)\n\t\texcept TypeError:\n\t\t\tself.temp_dir.cleanup()\n\t\t\traise ValueError('Could not read stdin')\n\n\t\treturn file_path"}, {"instruction": "def bartlett(timeseries, segmentlength, noverlap=None, window=None, plan=None):\n    # pylint: disable=unused-argument\n    \"\"\"Calculate an PSD of this `TimeSeries` using Bartlett's method\n\n    Parameters\n    ----------\n    timeseries : `~gwpy.timeseries.TimeSeries`\n        input `TimeSeries` data.\n\n    segmentlength : `int`\n        number of samples in single average.\n\n    noverlap : `int`\n        number of samples to overlap between segments, defaults to 50%.\n\n    window : `tuple`, `str`, optional\n        window parameters to apply to timeseries prior to FFT\n\n    plan : `REAL8FFTPlan`, optional\n        LAL FFT plan to use when generating average spectrum\n\n    Returns\n    -------\n    spectrum : `~gwpy.frequencyseries.FrequencySeries`\n        average power `FrequencySeries`\n\n    See also\n    --------\n    lal.REAL8AverageSpectrumWelch\n    \"\"\"\n", "input": "", "output": "    return _lal_spectrum(timeseries, segmentlength, noverlap=0,\n                         method='welch', window=window, plan=plan)"}, {"instruction": "async def _wait(self):\n        '''\n        Wait on the other editatoms who are constructing nodes my new nodes refer to\n        '''\n", "input": "", "output": "        for buid in self.otherbldgbuids:\n            nodeevnt = self.allbldgbuids.get(buid)\n            if nodeevnt is None:\n                continue\n            await nodeevnt[1].wait()"}, {"instruction": "def save_config(self, cmd=\"save\", confirm=False, confirm_response=\"\"):\n        \"\"\" Save Config for HuaweiSSH\"\"\"\n", "input": "", "output": "        return super(HuaweiBase, self).save_config(\n            cmd=cmd, confirm=confirm, confirm_response=confirm_response\n        )"}, {"instruction": "def returned(n):\n\t\"\"\"Generate a random walk and return True if the walker has returned to\n\tthe origin after taking `n` steps.\n\t\"\"\"\n", "input": "", "output": "\t## `takei` yield lazily so we can short-circuit and avoid computing the rest of the walk\n\tfor pos in randwalk() >> drop(1) >> takei(xrange(n-1)):\n\t\tif pos == Origin:\n\t\t\treturn True\n\treturn False"}, {"instruction": "def rosmsg(self):\n        \"\"\":obj:`sensor_msgs.Image` : ROS Image\n        \"\"\"\n", "input": "", "output": "        from cv_bridge import CvBridge, CvBridgeError\n        cv_bridge = CvBridge()\n        try:\n            return cv_bridge.cv2_to_imgmsg(self._data, encoding=self._encoding)\n        except CvBridgeError as cv_bridge_exception:\n            logging.error('%s' % (str(cv_bridge_exception)))"}, {"instruction": "def _get_netengine_arguments(self, required=False):\n        \"\"\"\n        returns list of available config params\n        returns list of required config params if required is True\n        for internal use only\n        \"\"\"\n", "input": "", "output": "        # inspect netengine class\n        backend_class = self._get_netengine_backend()\n        argspec = inspect.getargspec(backend_class.__init__)\n        # store args\n        args = argspec.args\n        # remove known arguments\n        for argument_name in ['self', 'host', 'port']:\n            args.remove(argument_name)\n\n        if required:\n            # list of default values\n            default_values = list(argspec.defaults)\n            # always remove last default value, which is port number\n            default_values = default_values[0:-1]\n\n            # remove an amount of arguments equals to number of default values, starting from right\n            args = args[0:len(args)-len(default_values)]\n\n        return args"}, {"instruction": "def get_date_of_author(_id):\n    \"\"\"Pass author id and return the name of its associated date.\"\"\"\n", "input": "", "output": "    _dict = get_date_author()\n    for date, ids in _dict.items():\n        if _id in ids:\n            return date\n    return None"}, {"instruction": "def newNsPropEatName(self, ns, name, value):\n        \"\"\"Create a new property tagged with a namespace and carried\n           by a node. \"\"\"\n", "input": "", "output": "        if ns is None: ns__o = None\n        else: ns__o = ns._o\n        ret = libxml2mod.xmlNewNsPropEatName(self._o, ns__o, name, value)\n        if ret is None:raise treeError('xmlNewNsPropEatName() failed')\n        __tmp = xmlAttr(_obj=ret)\n        return __tmp"}, {"instruction": "def dist(self, x1, x2):\n        \"\"\"Return the distance between ``x1`` and ``x2``.\n\n        Parameters\n        ----------\n        x1, x2 : `LinearSpaceElement`\n            Elements whose distance to compute.\n\n        Returns\n        -------\n        dist : float\n            Distance between ``x1`` and ``x2``.\n        \"\"\"\n", "input": "", "output": "        if x1 not in self:\n            raise LinearSpaceTypeError('`x1` {!r} is not an element of '\n                                       '{!r}'.format(x1, self))\n        if x2 not in self:\n            raise LinearSpaceTypeError('`x2` {!r} is not an element of '\n                                       '{!r}'.format(x2, self))\n        return float(self._dist(x1, x2))"}, {"instruction": "def get_aoi(self, solar_zenith, solar_azimuth):\n        \"\"\"Get the angle of incidence on the system.\n\n        Parameters\n        ----------\n        solar_zenith : float or Series.\n            Solar zenith angle.\n        solar_azimuth : float or Series.\n            Solar azimuth angle.\n\n        Returns\n        -------\n        aoi : Series\n            The angle of incidence\n        \"\"\"\n", "input": "", "output": "\n        aoi = irradiance.aoi(self.surface_tilt, self.surface_azimuth,\n                             solar_zenith, solar_azimuth)\n        return aoi"}, {"instruction": "def compute_distance_matrix(points):\n    \"\"\"\n    Return a matrix of distance (in meters) between every point in a given list\n    of (lat, lon) location tuples.\n    \"\"\"\n", "input": "", "output": "    n = len(points)\n    return [[1000 * great_circle_distance(points[i], points[j])\n             for j in range(n)] for i in range(n)]"}, {"instruction": "def start(self, version=None, **kwargs):#game_version=None, data_version=None, **kwargs):\n    \"\"\"Launch the game process.\"\"\"\n", "input": "", "output": "    if not version:\n        version = self.mostRecentVersion\n    pysc2Version = lib.Version( # convert to pysc2 Version\n        version.version,\n        version.baseVersion,\n        version.dataHash,\n        version.fixedHash)\n    return sc_process.StarcraftProcess(\n                self,\n                exec_path=self.exec_path(version.baseVersion),\n                version=pysc2Version,\n                **kwargs)"}, {"instruction": "def apply_flat(self, config, namespace_separator='_', prefix=''):\n        # type: (Dict[str, Any], str, str) -> None\n        \"\"\"Apply additional configuration from a flattened dictionary\n\n        This will look for dictionary items that match flattened keys from base_config and apply their values on the\n        current configuration object.\n\n        This can be useful for applying configuration from environment variables and flat configuration file formats\n        such as INI files.\n        \"\"\"\n", "input": "", "output": "        self._init_flat_pointers()\n        for key_stack, (container, orig_key) in self._flat_pointers.items():\n            flat_key = '{prefix}{joined_key}'.format(prefix=prefix, joined_key=namespace_separator.join(key_stack))\n            if flat_key in config:\n                container[orig_key] = config[flat_key]"}, {"instruction": "def from_parmed(cls, path, *args, **kwargs):\n        \"\"\"\n        Try to load a file automatically with ParmEd. Not guaranteed to work, but\n        might be useful if it succeeds.\n\n        Arguments\n        ---------\n        path : str\n            Path to file that ParmEd can load\n        \"\"\"\n", "input": "", "output": "        st = parmed.load_file(path, structure=True, *args, **kwargs)\n        box = kwargs.pop('box', getattr(st, 'box', None))\n        velocities = kwargs.pop('velocities', getattr(st, 'velocities', None))\n        positions = kwargs.pop('positions', getattr(st, 'positions', None))\n        return cls(master=st, topology=st.topology, positions=positions, box=box,\n                   velocities=velocities, path=path, **kwargs)"}, {"instruction": "def NoExclusions(self):\n        \"\"\"Determine that there are no exclusion criterion in play\n\n        :return: True if there is no real boundary specification of any kind.\n\n        Simple method allowing parsers to short circuit the determination of\n        missingness, which can be moderately compute intensive.\n        \"\"\"\n", "input": "", "output": "        if len(self.start_bounds) + len(self.target_rs) + len(self.ignored_rs) == 0:\n            return BoundaryCheck.chrom == -1\n        return False"}, {"instruction": "def export_project(self):\n        \"\"\" Processes misc options specific for GCC ARM, and run generator. \"\"\"\n", "input": "", "output": "        output = copy.deepcopy(self.generated_project)\n        self.process_data_for_makefile(self.workspace)\n        self._fix_sublime_paths(self.workspace)\n        self.workspace['linker_options'] =[]\n\n        output['path'], output['files']['makefile'] = self.gen_file_jinja('makefile_gcc.tmpl', self.workspace, 'Makefile', self.workspace['output_dir']['path'])\n\n        self.workspace['buildsys_name'] = 'Make'\n        self.workspace['buildsys_cmd'] = 'make all'\n\n        path, output['files']['sublimetext'] = self.gen_file_jinja(\n            'sublimetext.sublime-project.tmpl', self.workspace, '%s.sublime-project' % self.workspace['name'], self.workspace['output_dir']['path'])\n        generated_projects = output\n        return generated_projects"}, {"instruction": "def initialize_media_descriptor(self) -> None:\n        \"\"\"\n        Returns the media descriptor for the first media descriptor where\n        the file can be found.\n        \"\"\"\n", "input": "", "output": "\n        for md in self.media_descriptors:\n            media_path = self.get_media_path(md)\n            if media_path.is_file():\n                self.media_descriptor = md\n                return\n\n        raise FileNotFoundError(\n            "}, {"instruction": "def compute_empirical(cls, X):\n        \"\"\"Compute empirical distribution.\"\"\"\n", "input": "", "output": "        z_left = []\n        z_right = []\n        L = []\n        R = []\n\n        U, V = cls.split_matrix(X)\n        N = len(U)\n        base = np.linspace(EPSILON, 1.0 - EPSILON, COMPUTE_EMPIRICAL_STEPS)\n        # See https://github.com/DAI-Lab/Copulas/issues/45\n\n        for k in range(COMPUTE_EMPIRICAL_STEPS):\n            left = sum(np.logical_and(U <= base[k], V <= base[k])) / N\n            right = sum(np.logical_and(U >= base[k], V >= base[k])) / N\n\n            if left > 0:\n\n                z_left.append(base[k])\n                L.append(left / base[k] ** 2)\n\n            if right > 0:\n                z_right.append(base[k])\n                R.append(right / (1 - z_right[k]) ** 2)\n\n        return z_left, L, z_right, R"}, {"instruction": "def script_level(self, container):\n        \"\"\"Nesting level of super/subscript.\"\"\"\n", "input": "", "output": "        try:\n            level = self.parent.script_level(container)\n        except AttributeError:\n            level = -1\n        return level + 1 if self.is_script(container) else level"}, {"instruction": "def merge_unscoped_hparams(scopes_and_hparams):\n  \"\"\"Merge multiple HParams into one with scopes.\"\"\"\n", "input": "", "output": "  merged_values = {}\n  for (scope, hparams) in scopes_and_hparams:\n    for key, value in six.iteritems(hparams.values()):\n      scoped_key = \"%s.%s\" % (scope, key)\n      merged_values[scoped_key] = value\n\n  return hparam.HParams(**merged_values)"}, {"instruction": "def difference(self, *args):\n        \"\"\"\n        Take the difference between one array and a number of other arrays.\n        Only the elements present in just the first array will remain.\n        \"\"\"\n", "input": "", "output": "        setobj = set(self.obj)\n        for i, v in enumerate(args):\n            setobj = setobj - set(args[i])\n        return self._wrap(self._clean._toOriginal(setobj))"}, {"instruction": "def minibatch(items, size=8):\n    \"\"\"Iterate over batches of items. `size` may be an iterator,\n    so that batch-size can vary on each step.\n    \"\"\"\n", "input": "", "output": "    if isinstance(size, int):\n        size_ = itertools.repeat(size)\n    else:\n        size_ = size\n    items = iter(items)\n    while True:\n        batch_size = next(size_)\n        batch = list(itertools.islice(items, int(batch_size)))\n        if len(batch) == 0:\n            break\n        yield list(batch)"}, {"instruction": "def filesystem_absent(name, force=False, recursive=False):\n    '''\n    ensure filesystem is absent on the system\n\n    name : string\n        name of filesystem\n    force : boolean\n        try harder to destroy the dataset (zfs destroy -f)\n    recursive : boolean\n        also destroy all the child datasets (zfs destroy -r)\n\n    .. warning::\n\n        If a volume with ``name`` exists, this state will succeed without\n        destroying the volume specified by ``name``. This module is dataset type sensitive.\n\n    '''\n", "input": "", "output": "    if not __utils__['zfs.is_dataset'](name):\n        ret = {'name': name,\n               'changes': {},\n               'result': False,\n               'comment': 'invalid dataset name: {0}'.format(name)}\n    else:\n        ret = _absent(name, 'filesystem', force, recursive)\n    return ret"}, {"instruction": "def deserialize_by_field(value, field):\n    \"\"\"\n    Some types get serialized to JSON, as strings.\n    If we know what they are supposed to be, we can deserialize them\n    \"\"\"\n", "input": "", "output": "    if isinstance(field, forms.DateTimeField):\n        value = parse_datetime(value)\n    elif isinstance(field, forms.DateField):\n        value = parse_date(value)\n    elif isinstance(field, forms.TimeField):\n        value = parse_time(value)\n    return value"}, {"instruction": "def end(self):\n        \"\"\"\n        Ends the response. Useful for quickly ending connection with no data\n        sent\n        \"\"\"\n", "input": "", "output": "        self.send_headers()\n        self.write()\n        self.write_eof()\n        self.has_ended = True"}, {"instruction": "def int_out_of_bounds(self, index, axis=0):\n        \"\"\" examples if index is out of local processing bounds\n\n        function is used to perform examples for index of type integer\n        :param index: global index to examples as type int\n        :param axis: current axis to examples\n        :return: return input or raise error\n        \"\"\"\n", "input": "", "output": "        #if index >= self._global_shape[axis]:\n        if index > self._global_shape[axis]:\n            raise IndexError('index is larger than the upper bound')\n\n        # wrap around index if negative like in python\n        if index < 0:\n            index += self._global_shape[axis]\n            #warnings.warn('warp around may occur')\n\n        # check for invalid wrap around\n        if index < 0:\n            raise IndexError('index is smaller than the lower bound')\n\n        return index"}, {"instruction": "def _error(self, x):\n        \"\"\"Error function.\n        Once self.y_desired has been defined, compute the error\n        of input x using the forward model.\n        \"\"\"\n", "input": "", "output": "        y_pred = self.fmodel.predict_y(x)\n        err_v  = y_pred - self.goal\n        error = sum(e*e for e in err_v)\n        return error"}, {"instruction": "def serialize(self):\n        \"\"\" Get the serialized Dynamo format for the update \"\"\"\n", "input": "", "output": "        if self.action == 'Create':\n            payload = self.extra['index'].schema()\n        else:\n            payload = {\n                'IndexName': self.index_name,\n            }\n            if self.action == 'Update':\n                payload['ProvisionedThroughput'] = \\\n                    self.extra['throughput'].schema()\n        return {\n            self.action: payload\n        }"}, {"instruction": "def retire(self, process_schemas):\n        \"\"\"Retire obsolete processes.\n\n        Remove old process versions without data. Find processes that have been\n        registered but do not exist in the code anymore, then:\n\n        - If they do not have data: remove them\n        - If they have data: flag them not active (``is_active=False``)\n\n        \"\"\"\n", "input": "", "output": "        process_slugs = set(ps['slug'] for ps in process_schemas)\n\n        # Processes that are in DB but not in the code\n        retired_processes = Process.objects.filter(~Q(slug__in=process_slugs))\n\n        # Remove retired processes which do not have data\n        retired_processes.filter(data__exact=None).delete()\n\n        # Remove non-latest processes which do not have data\n        latest_version_processes = Process.objects.order_by('slug', '-version').distinct('slug')\n        Process.objects.filter(data__exact=None).difference(latest_version_processes).delete()\n\n        # Deactivate retired processes which have data\n        retired_processes.update(is_active=False)"}, {"instruction": "def size(self, table=None):\n        \"\"\"\n        Return the size, in bytes, of the profile or *table*.\n\n        If *table* is `None`, this function returns the size of the\n        whole profile (i.e. the sum of the table sizes). Otherwise, it\n        returns the size of *table*.\n\n        Note: if the file is gzipped, it returns the compressed size.\n        \"\"\"\n", "input": "", "output": "        size = 0\n        if table is None:\n            for table in self.relations:\n                size += self.size(table)\n        else:\n            try:\n                fn = _table_filename(os.path.join(self.root, table))\n                size += os.stat(fn).st_size\n            except ItsdbError:\n                pass\n        return size"}, {"instruction": "def links(self, r_server=None, mask=None):\n        \"\"\"\n        Get LINKS information.\n        Optional arguments:\n        * r_server=None - Forward the query to this server.\n        * mask=None - Match mask servers.\n        \"\"\"\n", "input": "", "output": "        with self.lock:\n            if not r_server:\n                self.send('LINKS')\n            elif not mask and r_server:\n                self.send('LINKS %s' % r_server)\n            else:\n                self.send('LINKS %s %s' % (r_server, mask))\n            links = {}\n            while self.readable():\n                msg = self._recv(expected_replies=('364', '365'))\n                segments = msg[2].split()\n                if msg[0] == '364':\n                    server = segments[0]\n                    desc = ' '.join(segments[3:])\n                    links[server] = desc\n                elif msg[0] == '365':\n                    break\n            return links"}, {"instruction": "def string_width(word: str) -> int:\n    \"\"\"\n    :param word:\n    :return: Widths of word\n\n    Usage:\n\n        >>> string_width('abc')\n        3\n        >>> string_width('\uff21b\u3057\u30fc')\n        7\n        >>> string_width('')\n        0\n    \"\"\"\n", "input": "", "output": "    return sum(map(lambda x: 2 if east_asian_width(x) in 'FWA' else 1, word))"}, {"instruction": "def local_minima(img, min_distance = 4):\n    r\"\"\"\n    Returns all local minima from an image.\n    \n    Parameters\n    ----------\n    img : array_like\n        The image.\n    min_distance : integer\n        The minimal distance between the minimas in voxels. If it is less, only the lower minima is returned.\n    \n    Returns\n    -------\n    indices : sequence\n        List of all minima indices.\n    values : sequence\n        List of all minima values.\n    \"\"\"\n", "input": "", "output": "    # @TODO: Write a unittest for this.\n    fits = numpy.asarray(img)\n    minfits = minimum_filter(fits, size=min_distance) # default mode is reflect\n    minima_mask = fits == minfits\n    good_indices = numpy.transpose(minima_mask.nonzero())\n    good_fits = fits[minima_mask]\n    order = good_fits.argsort()\n    return good_indices[order], good_fits[order]"}, {"instruction": "def cmap(self, background_color='#000000', random_state=None):\n        \"\"\"\n        Define a matplotlib colormap consisting of (random) muted\n        colors.\n\n        This is very useful for plotting the segmentation image.\n\n        Parameters\n        ----------\n        background_color : str or `None`, optional\n            A hex string in the \"#rrggbb\" format defining the first\n            color in the colormap.  This color will be used as the\n            background color (label = 0) when plotting the segmentation\n            image.  The default is black ('#000000').\n\n        random_state : int or `~numpy.random.RandomState`, optional\n            The pseudo-random number generator state used for random\n            sampling.  Separate function calls with the same\n            ``random_state`` will generate the same colormap.\n        \"\"\"\n", "input": "", "output": "\n        return self.make_cmap(background_color=background_color,\n                              random_state=random_state)"}, {"instruction": "def _get_content_type_queryset(models_list):\n    \"\"\" Get list of services content types \"\"\"\n", "input": "", "output": "    content_type_ids = {c.id for c in ContentType.objects.get_for_models(*models_list).values()}\n    return ContentType.objects.filter(id__in=content_type_ids)"}, {"instruction": "def glymurrc_fname():\n    \"\"\"Return the path to the configuration file.\n\n    Search order:\n        1) current working directory\n        2) environ var XDG_CONFIG_HOME\n        3) $HOME/.config/glymur/glymurrc\n    \"\"\"\n", "input": "", "output": "\n    # Current directory.\n    fname = os.path.join(os.getcwd(), 'glymurrc')\n    if os.path.exists(fname):\n        return fname\n\n    confdir = get_configdir()\n    if confdir is not None:\n        fname = os.path.join(confdir, 'glymurrc')\n        if os.path.exists(fname):\n            return fname\n\n    # didn't find a configuration file.\n    return None"}, {"instruction": "def clear_context(self, app=None):\n        \"\"\"Clear the component's context.\n\n        Keyword Args:\n            app (flask.Flask, optional): The app to clear this component's\n                context for. If omitted, the value from ``Component.app`` is\n                used.\n        \"\"\"\n", "input": "", "output": "        if (app is None and self._context is _CONTEXT_MISSING\n                and not in_app_context()):\n            raise RuntimeError(\"Attempted to clear component context without\"\n                               \" a bound app context or eager app set! Please\"\n                               \" pass the related app you want to update the\"\n                               \" context for!\")\n\n        if self._context is not _CONTEXT_MISSING:\n            self._context = DEFAULT_DICT\n        else:\n            key = self._get_context_name(app=app)\n            setattr(_CONTEXT_LOCALS, key, DEFAULT_DICT)"}, {"instruction": "def flatten_model(model):\n    \"\"\"Flatten a model to a list of models.\n    This is used to flatten a ``Binder``'ish model down to a list\n    of contained models.\n    \"\"\"\n", "input": "", "output": "    yield model\n    if isinstance(model, (TranslucentBinder, Binder,)):\n        for m in model:\n            # yield from flatten_model(m)\n            for x in flatten_model(m):\n                yield x"}, {"instruction": "def btc_is_p2sh_script( script_hex ):\n    \"\"\"\n    Is the given scriptpubkey a p2sh script?\n    \"\"\"\n", "input": "", "output": "    if script_hex.startswith(\"a914\") and script_hex.endswith(\"87\") and len(script_hex) == 46:\n        return True\n    else:\n        return False"}, {"instruction": "def __get_factory_with_context(self, factory_name):\n        # type: (str) -> Tuple[type, FactoryContext]\n        \"\"\"\n        Retrieves the factory registered with the given and its factory context\n\n        :param factory_name: The name of the factory\n        :return: A (factory, context) tuple\n        :raise TypeError: Unknown factory, or factory not manipulated\n        \"\"\"\n", "input": "", "output": "        factory = self.__factories.get(factory_name)\n        if factory is None:\n            raise TypeError(\"Unknown factory '{0}'\".format(factory_name))\n\n        # Get the factory context\n        factory_context = getattr(\n            factory, constants.IPOPO_FACTORY_CONTEXT, None\n        )\n        if factory_context is None:\n            raise TypeError(\n                \"Factory context missing in '{0}'\".format(factory_name)\n            )\n\n        return factory, factory_context"}, {"instruction": "def kunc_v(p, v0, k0, k0p, order=5, min_strain=0.01):\n    \"\"\"\n    find volume at given pressure using brenth in scipy.optimize\n\n    :param p: pressure in GPa\n    :param v0: unit-cell volume in A^3 at 1 bar\n    :param k0: bulk modulus at reference conditions\n    :param k0p: pressure derivative of bulk modulus at reference conditions\n    :param order: order of Kunc function\n    :param min_strain: defining minimum v/v0 value to search volume for\n    :return: unit-cell volume at high pressure in GPa\n    :note: a wrapper function vectorizing kunc_v_single\n    \"\"\"\n", "input": "", "output": "    if isuncertainties([p, v0, k0, k0p]):\n        f_u = np.vectorize(uct.wrap(kunc_v_single), excluded=[1, 2, 3, 4, 5])\n        return f_u(p, v0, k0, k0p, order=order, min_strain=min_strain)\n    else:\n        f_v = np.vectorize(kunc_v_single, excluded=[1, 2, 3, 4, 5])\n        return f_v(p, v0, k0, k0p, order=order, min_strain=min_strain)"}, {"instruction": "def _eight_byte_real_to_float(value):\n    \"\"\"\n    Convert a number from GDSII 8 byte real format to float.\n\n    Parameters\n    ----------\n    value : string\n        The GDSII binary string representation of the number.\n\n    Returns\n    -------\n    out : float\n        The number represented by ``value``.\n    \"\"\"\n", "input": "", "output": "    short1, short2, long3 = struct.unpack('>HHL', value)\n    exponent = (short1 & 0x7f00) // 256 - 64\n    mantissa = (((short1 & 0x00ff) * 65536 + short2) * 4294967296 +\n                long3) / 72057594037927936.0\n    if short1 & 0x8000:\n        return -mantissa * 16.**exponent\n    return mantissa * 16.**exponent"}, {"instruction": "def range_mac(mac_start, mac_end, step=1):\n    \"\"\"Iterate over mac addresses (given as string).\"\"\"\n", "input": "", "output": "    start = int(EUI(mac_start))\n    end = int(EUI(mac_end))\n    for i_mac in range(start, end, step):\n        mac = EUI(int(EUI(i_mac)) + 1)\n        ip = ['10'] + [str(int(i, 2)) for i in mac.bits().split('-')[-3:]]\n        yield str(mac).replace('-', ':'), '.'.join(ip)"}, {"instruction": "def remove_end_optionals(ir_blocks):\n    \"\"\"Return a list of IR blocks as a copy of the original, with EndOptional blocks removed.\"\"\"\n", "input": "", "output": "    new_ir_blocks = []\n    for block in ir_blocks:\n        if not isinstance(block, EndOptional):\n            new_ir_blocks.append(block)\n    return new_ir_blocks"}, {"instruction": "def handle_attribute_value(self, value):\n        \"\"\"Check attribute. Especially designed for avoiding URLs in the form:\n        javascript:myXSSFunction();\"\"\"\n", "input": "", "output": "        if self.re_js.match(value) or self.re_vb.match(value):\n            return ''\n        return value"}, {"instruction": "def perform_command(self):\n        \"\"\"\n        Perform command and return the appropriate exit code.\n\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "        self.log(u\"This function should be overloaded in derived classes\")\n        self.log([u\"Invoked with %s\", self.actual_arguments])\n        return self.NO_ERROR_EXIT_CODE"}, {"instruction": "def receive_ping(self, ping: Ping):\n        \"\"\" Handle a Ping message by answering with a Pong. \"\"\"\n", "input": "", "output": "\n        self.log_healthcheck.debug(\n            'Ping received',\n            message_id=ping.nonce,\n            message=ping,\n            sender=pex(ping.sender),\n        )\n\n        pong = Pong(nonce=ping.nonce)\n        self.raiden.sign(pong)\n\n        try:\n            self.maybe_send(ping.sender, pong)\n        except (InvalidAddress, UnknownAddress) as e:\n            self.log.debug(\"Couldn't send the `Delivered` message\", e=e)"}, {"instruction": "def query_ssos(self):\n        \"\"\"\n        Use the MPC file that has been built up in processing this work\n        unit to generate another workunit.\n        \"\"\"\n", "input": "", "output": "        self._ssos_queried = True\n        mpc_filename = self.save()\n        return self.builder.build_workunit(mpc_filename)"}, {"instruction": "def mobility(sdat, tstart=None, tend=None):\n    \"\"\"Plates mobility.\n\n    Compute the ratio vsurf / vrms.\n\n    Args:\n        sdat (:class:`~stagpy.stagyydata.StagyyData`): a StagyyData instance.\n        tstart (float): time at which the computation should start. Use the\n            beginning of the time series data if set to None.\n        tend (float): time at which the computation should end. Use the\n            end of the time series data if set to None.\n    Returns:\n        tuple of :class:`numpy.array`: mobility and time arrays.\n    \"\"\"\n", "input": "", "output": "    tseries = sdat.tseries_between(tstart, tend)\n    steps = sdat.steps[tseries.index[0]:tseries.index[-1]]\n    time = []\n    mob = []\n    for step in steps.filter(rprof=True):\n        time.append(step.timeinfo['t'])\n        mob.append(step.rprof.iloc[-1].loc['vrms'] / step.timeinfo['vrms'])\n    return np.array(mob), np.array(time)"}, {"instruction": "def save_dir(key, dir_path, *refs):\n    \"\"\"Convert the given parameters to a special JSON object.\n\n    JSON object is of the form:\n    { key: {\"dir\": dir_path}}, or\n    { key: {\"dir\": dir_path, \"refs\": [refs[0], refs[1], ... ]}}\n\n    \"\"\"\n", "input": "", "output": "    if not os.path.isdir(dir_path):\n        return error(\n            \"Output '{}' set to a missing directory: '{}'.\".format(key, dir_path)\n        )\n\n    result = {key: {\"dir\": dir_path}}\n\n    if refs:\n        missing_refs = [\n            ref for ref in refs if not (os.path.isfile(ref) or os.path.isdir(ref))\n        ]\n        if len(missing_refs) > 0:\n            return error(\n                \"Output '{}' set to missing references: '{}'.\".format(\n                    key, ', '.join(missing_refs)\n                )\n            )\n        result[key][\"refs\"] = refs\n\n    return json.dumps(result)"}, {"instruction": "def get_expanded_schema(self, schema_name):\n        \"\"\"\n        Return a schema file with all $ref properties expanded\n        \"\"\"\n", "input": "", "output": "        if schema_name not in self.expanded_schemas:\n            fn = self.get_schema_file(schema_name)\n            schemas_folder = self.get_schemas_folder()\n            base_uri = self.get_schema_path(schemas_folder)\n\n            with open(fn) as f:\n                jsn_schema = jsonref.load(f, base_uri=base_uri)\n\n                # cache the schema for future use\n                self.expanded_schemas[schema_name] = jsn_schema\n        else:\n            jsn_schema = self.expanded_schemas[schema_name]\n\n        return jsn_schema"}, {"instruction": "def memoize(func):\n    '''\n    Memoize aka cache the return output of a function\n    given a specific set of arguments\n\n    .. versionedited:: 2016.3.4\n\n    Added **kwargs support.\n    '''\n", "input": "", "output": "    cache = {}\n\n    @wraps(func)\n    def _memoize(*args, **kwargs):\n        str_args = []\n        for arg in args:\n            if not isinstance(arg, six.string_types):\n                str_args.append(six.text_type(arg))\n            else:\n                str_args.append(arg)\n\n        args_ = ','.join(list(str_args) + ['{0}={1}'.format(k, kwargs[k]) for k in sorted(kwargs)])\n        if args_ not in cache:\n            cache[args_] = func(*args, **kwargs)\n        return cache[args_]\n\n    return _memoize"}, {"instruction": "def post_login(self, came_from=lurl('/')):\n        \"\"\"\n        Redirect the user to the initially requested page on successful\n        authentication or redirect her back to the login page if login failed.\n\n        \"\"\"\n", "input": "", "output": "        if not request.identity:\n            login_counter = request.environ.get('repoze.who.logins', 0) + 1\n            redirect('/login',\n                params=dict(came_from=came_from, __logins=login_counter))\n        userid = request.identity['repoze.who.userid']\n        flash(_('Welcome back, %s!') % userid)\n        redirect(came_from)"}, {"instruction": "def get_checks_paths(checks_paths=None):\n    \"\"\"\n    Get path to checks.\n\n    :param checks_paths: list of str, directories where the checks are present\n    :return: list of str (absolute path of directory with checks)\n    \"\"\"\n", "input": "", "output": "    p = os.path.join(__file__, os.pardir, os.pardir, os.pardir, \"checks\")\n    p = os.path.abspath(p)\n    # let's utilize the default upstream checks always\n    if checks_paths:\n        p += [os.path.abspath(x) for x in checks_paths]\n    return [p]"}, {"instruction": "def column_reflection_fallback(self):\n        \"\"\"If we can't reflect the table, use a query to at least get column names.\"\"\"\n", "input": "", "output": "        sql = sa.select([sa.text(\"*\")]).select_from(self._table)\n        col_names = self.engine.execute(sql).keys()\n        col_dict = [{'name': col_name} for col_name in col_names]\n        return col_dict"}, {"instruction": "def anonymous_login(self):\n        \"\"\"Login as anonymous user\n\n        :return: logon result, see `CMsgClientLogonResponse.eresult <https://github.com/ValvePython/steam/blob/513c68ca081dc9409df932ad86c66100164380a6/protobufs/steammessages_clientserver.proto#L95-L118>`_\n        :rtype: :class:`.EResult`\n        \"\"\"\n", "input": "", "output": "        self._LOG.debug(\"Attempting Anonymous login\")\n\n        self._pre_login()\n\n        self.username = None\n        self.login_key = None\n\n        message = MsgProto(EMsg.ClientLogon)\n        message.header.steamid = SteamID(type='AnonUser', universe='Public')\n        message.body.protocol_version = 65579\n        self.send(message)\n\n        resp = self.wait_msg(EMsg.ClientLogOnResponse, timeout=30)\n        return EResult(resp.body.eresult) if resp else EResult.Fail"}, {"instruction": "def leaves(self, nodes=None, unique=True):\n        \"\"\"Get the leaves of the tree starting at this root.\n\n        Args:\n            nodes (iterable): limit leaves for these node names\n            unique: only include individual leaf nodes once\n\n        Returns:\n            list of leaf nodes\n\n        \"\"\"\n", "input": "", "output": "        if nodes is None:\n            return super(DependencyTree, self).leaves(unique=unique)\n\n        res = list()\n        for child_id in nodes:\n            for sub_child in self._all_nodes[child_id].leaves(unique=unique):\n                if not unique or sub_child not in res:\n                    res.append(sub_child)\n        return res"}, {"instruction": "def xstep(self):\n        r\"\"\"Minimise Augmented Lagrangian with respect to\n        :math:`\\mathbf{x}`.\n        \"\"\"\n", "input": "", "output": "\n        b = self.AHSf + self.rho*np.sum(\n            np.conj(self.Gf)*sl.rfftn(self.Y-self.U, axes=self.axes),\n            axis=self.Y.ndim-1)\n        self.Xf = b / (self.AHAf + self.rho*self.GHGf)\n        self.X = sl.irfftn(self.Xf, self.axsz, axes=self.axes)\n\n        if self.opt['LinSolveCheck']:\n            ax = (self.AHAf + self.rho*self.GHGf)*self.Xf\n            self.xrrs = sl.rrs(ax, b)\n        else:\n            self.xrrs = None"}, {"instruction": "def kml_network_link(href, name=None, region_coords=None, visible=True):\n    \"\"\"\n    Create the KML <NetworkLink> Tag for a certain Region in the RegionGrid.\n\n    Args:\n        region_coords (RegionCoordinate):\n        href (str): the href attribute of the NetworkLink\n        name (str): KML <name>\n        visible (bool): If true the network link will appear as 'visible'\n            (i.e. checked) in Google Earth.\n\n    Returns:\n        KMLElement: the KML <NetworkLink>\n\n    \"\"\"\n", "input": "", "output": "    nl = KML.NetworkLink()\n    if name is None and region_coords is not None:\n        name = kml_element_name(region_coords, \"NL\")\n    if name is not None:\n        nl.append(KML.name(name))\n    if region_coords is not None:\n        min_lod_pixels = DEFAULT_MIN_LOD_PIXELS * (2 ** region_coords.log_tiles_per_row)\n        nl.append(kml_region(region_coords, min_lod_pixels=min_lod_pixels))\n    if not visible:\n        nl.append(KML.visibility(0))\n\n    nl.append(KML.Link(\n        KML.href(href), KML.viewRefreshMode(\"onRegion\")))\n\n    return nl"}, {"instruction": "def delete_nic(access_token, subscription_id, resource_group, nic_name):\n    '''Delete a network interface.\n\n    Args:\n        access_token (str): A valid Azure authentication token.\n        subscription_id (str): Azure subscription id.\n        resource_group (str): Azure resource group name.\n        nic_name (str): Name of the NIC.\n\n    Returns:\n        HTTP response.\n    '''\n", "input": "", "output": "    endpoint = ''.join([get_rm_endpoint(),\n                        '/subscriptions/', subscription_id,\n                        '/resourceGroups/', resource_group,\n                        '/providers/Microsoft.Network/networkInterfaces/', nic_name,\n                        '?api-version=', NETWORK_API])\n    return do_delete(endpoint, access_token)"}, {"instruction": "def _restart_target(self):\n        \"\"\"\n        Restart our Target.\n        \"\"\"\n", "input": "", "output": "        if self._server:\n            if self._server.returncode is None:\n                self._server.kill()\n                time.sleep(0.2)\n        self._server = subprocess.Popen(\"python session_server.py\", stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n        time.sleep(0.2)"}, {"instruction": "def get_validator_list(self):\n        \"\"\"Return a list of validators specified in the override file\"\"\"\n", "input": "", "output": "        ignore = [\n            'dict',\n        ]\n        vlist = []\n        if not self.override:\n            return vlist\n\n        for k, v in list(self.override['classes'].items()):\n            if 'validator' in v:\n                validator = v['validator']\n                if validator not in ignore and validator not in vlist:\n                    vlist.append(validator)\n\n        for k, v in list(self.override['classes'].items()):\n            for kp, vp in list(v.items()):\n                if 'validator' in vp:\n                    validator = vp['validator']\n                    if validator not in ignore and validator not in vlist:\n                        vlist.append(validator)\n        return sorted(vlist)"}, {"instruction": "def scan(self, folder, sub=None, next_=None):\n        \"\"\" Request immediate rescan of a folder, or a specific path within a\n        folder.\n\n            Args:\n                folder (str): Folder ID.\n                sub (str): Path relative to the folder root. If sub is omitted\n                    the entire folder is scanned for changes, otherwise only\n                    the given path children are scanned.\n                next_ (int): Delays Syncthing's automated rescan interval for\n                    a given amount of seconds.\n\n            Returns:\n                str\n        \"\"\"\n", "input": "", "output": "        if not sub:\n            sub = ''\n        assert isinstance(sub, string_types)\n        assert isinstance(next_, int) or next_ is None\n        return self.post('scan', params={'folder': folder,\n                                         'sub': sub,\n                                         'next': next_})"}, {"instruction": "def distribute(self, func, partitioned_chunks, kwargs):\n        \"\"\"\n        Calculates the features in a sequential fashion by pythons map command\n\n        :param func: the function to send to each worker.\n        :type func: callable\n        :param partitioned_chunks: The list of data chunks - each element is again\n            a list of chunks - and should be processed by one worker.\n        :type partitioned_chunks: iterable\n        :param kwargs: parameters for the map function\n        :type kwargs: dict of string to parameter\n\n        :return: The result of the calculation as a list - each item should be the result of the application of func\n            to a single element.\n        \"\"\"\n", "input": "", "output": "        return map(partial(func, **kwargs), partitioned_chunks)"}, {"instruction": "def set_threadlocal(self, **values):\n        \"\"\"Set current thread's logging context to specified `values`\"\"\"\n", "input": "", "output": "        with self._lock:\n            self._ensure_threadlocal()\n            self._tpayload.context = values"}, {"instruction": "def get_hcurves(self, imtls=None):\n        \"\"\"\n        :param imtls: intensity measure types and levels\n        :returns: an array of (R, N) hazard curves\n        \"\"\"\n", "input": "", "output": "        self.init()\n        if imtls is None:\n            imtls = self.imtls\n        pmaps = [pmap.convert2(imtls, self.sids)\n                 for pmap in self.get_pmaps()]\n        return numpy.array(pmaps)"}, {"instruction": "def release(self):\n        \"\"\"Release the lock\"\"\"\n", "input": "", "output": "        base64_key = _encode(self.key)\n        base64_value = _encode(self._uuid)\n\n        txn = {\n            'compare': [{\n                'key': base64_key,\n                'result': 'EQUAL',\n                'target': 'VALUE',\n                'value': base64_value\n            }],\n            'success': [{\n                'request_delete_range': {\n                    'key': base64_key\n                }\n            }]\n        }\n\n        result = self.client.transaction(txn)\n        if 'succeeded' in result:\n            return result['succeeded']\n        return False"}, {"instruction": "def _get_metric(self, metric, tablename, index_name=None):\n        \"\"\" Fetch a read/write capacity metric \"\"\"\n", "input": "", "output": "        end = time.time()\n        begin = end - 3 * 60  # 3 minute window\n        dimensions = [{\"Name\": \"TableName\", \"Value\": tablename}]\n        if index_name is not None:\n            dimensions.append({\"Name\": \"GlobalSecondaryIndexName\", \"Value\": index_name})\n        period = 60\n        data = self.cloudwatch_connection.get_metric_statistics(\n            Period=period,\n            StartTime=begin,\n            EndTime=end,\n            MetricName=metric,\n            Namespace=\"AWS/DynamoDB\",\n            Statistics=[\"Sum\"],\n            Dimensions=dimensions,\n        )\n        points = data[\"Datapoints\"]\n        if not points:\n            return 0\n        else:\n            points.sort(key=lambda r: r[\"Timestamp\"])\n            return float(points[-1][\"Sum\"]) / period"}, {"instruction": "def ensure_regex_namespace(self, keyword: str, pattern: str) -> Namespace:\n        \"\"\"Get or create a regular expression namespace.\n\n        :param keyword: The keyword of a regular expression namespace\n        :param pattern: The pattern for a regular expression namespace\n        \"\"\"\n", "input": "", "output": "        if pattern is None:\n            raise ValueError('cannot have null pattern')\n\n        namespace = self.get_namespace_by_keyword_pattern(keyword, pattern)\n\n        if namespace is None:\n            log.info('creating regex namespace: %s:%s', keyword, pattern)\n            namespace = Namespace(\n                keyword=keyword,\n                pattern=pattern\n            )\n            self.session.add(namespace)\n            self.session.commit()\n\n        return namespace"}, {"instruction": "def to_task(self):\n        \"\"\"Return a task object representing this message.\"\"\"\n", "input": "", "output": "        from google.appengine.api.taskqueue import Task\n\n        task_args = self.get_task_args().copy()\n\n        payload = None\n        if 'payload' in task_args:\n            payload = task_args.pop('payload')\n\n        kwargs = {\n            'method': METHOD_TYPE,\n            'payload': json.dumps(payload)\n        }\n\n        kwargs.update(task_args)\n\n        return Task(**kwargs)"}, {"instruction": "def send_request(endpoint, **kwargs):\n    \"\"\"Return the response to a query as JSON from the NewsAPI web service.\n\n    The basic API is limited to 100 results which is chosen unless explicitly\n    given as an argument. Beyond that, paging is supported through the \"page\"\n    argument, if needed.\n\n    Parameters\n    ----------\n    endpoint : str\n        Endpoint to query, e.g. \"everything\" or \"top-headlines\"\n\n    kwargs : dict\n        A list of keyword arguments passed as parameters with the query.\n        The basic ones are \"q\" which is the search query, \"from\" is a start\n        date formatted as for instance 2018-06-10 and \"to\" is an end date\n        with the same format.\n\n    Returns\n    -------\n    res_json : dict\n        The response from the web service as a JSON dict.\n    \"\"\"\n", "input": "", "output": "    if api_key is None:\n        logger.error('NewsAPI cannot be used without an API key')\n        return None\n    url = '%s/%s' % (newsapi_url, endpoint)\n    if 'apiKey' not in kwargs:\n        kwargs['apiKey'] = api_key\n    if 'pageSize' not in kwargs:\n        kwargs['pageSize'] = 100\n    res = requests.get(url, params=kwargs)\n    res.raise_for_status()\n    res_json = res.json() \n    return res_json"}, {"instruction": "def update_jsonb(uid, extinfo):\n        '''\n        Update the json.\n        '''\n", "input": "", "output": "        cur_extinfo = MPost.get_by_uid(uid).extinfo\n        for key in extinfo:\n            cur_extinfo[key] = extinfo[key]\n        entry = TabPost.update(\n            extinfo=cur_extinfo,\n        ).where(TabPost.uid == uid)\n        entry.execute()\n        return uid"}, {"instruction": "def handleConnectionState(self, msg):\n        \"\"\":Return: True if IBPy message `msg` indicates the connection is unavailable for any reason, else False.\"\"\"\n", "input": "", "output": "        self.connected = not (msg.typeName == \"error\" and\n                              msg.errorCode in dataTypes[\"DISCONNECT_ERROR_CODES\"])\n\n        if self.connected:\n            self.connection_tracking[\"errors\"] = []\n            self.connection_tracking[\"disconnected\"] = False\n\n            if msg.typeName == dataTypes[\"MSG_CURRENT_TIME\"] and not self.connection_tracking[\"connected\"]:\n                self.log.info(\"[CONNECTION TO IB ESTABLISHED]\")\n                self.connection_tracking[\"connected\"] = True\n                self.ibCallback(caller=\"handleConnectionOpened\", msg=\"<connectionOpened>\")\n        else:\n            self.connection_tracking[\"connected\"] = False\n\n            if not self.connection_tracking[\"disconnected\"]:\n                self.connection_tracking[\"disconnected\"] = True\n                self.log.info(\"[CONNECTION TO IB LOST]\")"}, {"instruction": "def _parse_ports(port_values: dict) -> dict:\n        \"\"\"Parse ports key.\n\n        Args:\n            port_values (dict): ports configuration values\n\n        Returns:\n            dict, Ports specification which contains exposed ports\n\n        \"\"\"\n", "input": "", "output": "        # Initialising empty dictionary\n        endpoints = {}\n\n        for port_element in port_values:\n            target_port = port_element.split(':')\n            for port in target_port:\n                endpoints[int(port)] = int(port)\n\n        # Setting the types\n        endpoint_spec = docker.types.EndpointSpec(ports=endpoints)\n        return endpoint_spec"}, {"instruction": "def digest(dirname, glob=None):\n    \"\"\"Returns the md5 digest of all interesting files (or glob) in `dirname`.\n    \"\"\"\n", "input": "", "output": "    md5 = hashlib.md5()\n    if glob is None:\n        fnames = [fname for _, fname in list_files(Path(dirname))]\n        for fname in sorted(fnames):\n            fname = os.path.join(dirname, fname)\n            md5.update(open(fname, 'rb').read())\n    else:\n        fnames = Path(dirname).glob(glob)\n        for fname in sorted(fnames):\n            md5.update(fname.open('rb').read())\n    return md5.hexdigest()"}, {"instruction": "def getBool(t):\n    \"\"\"If t is of type bool, return it, otherwise raise InvalidTypeError.\n    \"\"\"\n", "input": "", "output": "    b = c_int()\n    if PL_get_long(t, byref(b)):\n        return bool(b.value)\n    else:\n        raise InvalidTypeError(\"bool\")"}, {"instruction": "def _refresh_editor_and_scrollbars(self):\n        \"\"\"\n        Refrehes editor content and scollbars.\n\n        We generate a fake resize event to refresh scroll bar.\n\n        We have the same problem as described here:\n        http://www.qtcentre.org/threads/44803 and we apply the same solution\n        (don't worry, there is no visual effect, the editor does not grow up\n        at all, even with a value = 500)\n        \"\"\"\n", "input": "", "output": "        TextHelper(self.editor).mark_whole_doc_dirty()\n        self.editor.repaint()\n        s = self.editor.size()\n        s.setWidth(s.width() + 1)\n        self.editor.resizeEvent(QResizeEvent(self.editor.size(), s))"}, {"instruction": "def _full_diff(merge_result, key, context_lines=3):\n    \"\"\"Generate a full diff based on a Weave merge result\"\"\"\n", "input": "", "output": "    header_printed = False\n    for group in _split_diff(merge_result, context_lines=context_lines):\n        if not header_printed:\n            header_printed = True\n            yield color.Header('diff a/%s b/%s' % (key, key))\n            yield color.DeletedHeader('--- %s' % key)\n            yield color.AddedHeader('+++ %s' % key)\n\n        for l in _diff_group(group):\n            yield l"}, {"instruction": "def parse_options_header(value):\n    \"\"\"Parse a ``Content-Type`` like header into a tuple with the content\n    type and the options:\n\n    >>> parse_options_header('text/html; charset=utf8')\n    ('text/html', {'charset': 'utf8'})\n\n    This should not be used to parse ``Cache-Control`` like headers that use\n    a slightly different format.  For these headers use the\n    :func:`parse_dict_header` function.\n\n    .. versionadded:: 0.5\n\n    :param value: the header to parse.\n    :return: (str, options)\n    \"\"\"\n", "input": "", "output": "    def _tokenize(string):\n        for match in _option_header_piece_re.finditer(string):\n            key, value = match.groups()\n            key = unquote_header_value(key)\n            if value is not None:\n                value = unquote_header_value(value, key == 'filename')\n            yield key, value\n\n    if not value:\n        return '', {}\n\n    parts = _tokenize(';' + value)\n    name = next(parts)[0]\n    extra = dict(parts)\n    return name, extra"}, {"instruction": "def apply(self, something: 'Reader') -> 'Reader':\n        r\"\"\"(<*>) :: f (a -> b) -> f a -> f b.\n\n        Haskell: f <*> g = \\x -> f x (g x)\n\n        Apply (<*>) is a beefed up map. It takes a Reader that\n        has a function in it and another Reader, and extracts that\n        function from the first Reader and then maps it over the second\n        one (composes the two functions).\n        \"\"\"\n", "input": "", "output": "\n        def _compose(x: Any):\n            f = self.run(x)\n            try:\n                ret = f(something.run(x))\n            except TypeError:\n                ret = partial(f, something.run(x))\n            return ret\n\n        return Reader(_compose)"}, {"instruction": "def kernel_restarted_message(self, msg):\r\n        \"\"\"Show kernel restarted/died messages.\"\"\"\n", "input": "", "output": "        if not self.is_error_shown:\r\n            # If there are kernel creation errors, jupyter_client will\r\n            # try to restart the kernel and qtconsole prints a\r\n            # message about it.\r\n            # So we read the kernel's stderr_file and display its\r\n            # contents in the client instead of the usual message shown\r\n            # by qtconsole.\r\n            try:\r\n                stderr = self._read_stderr()\r\n            except Exception:\r\n                stderr = None\r\n            if stderr:\r\n                self.show_kernel_error('<tt>%s</tt>' % stderr)\r\n        else:\r\n            self.shellwidget._append_html(\"<br>%s<hr><br>\" % msg,\r\n                                          before_prompt=False)"}, {"instruction": "def find_all_commands(management_dir):\n    \"\"\"\n    Find all valid commands in a directory\n    management_dir : directory path\n    return - List of commands\n    \"\"\"\n", "input": "", "output": "    try:\n        #Find all commands in the directory that are not __init__.py and end in .py.  Then, remove the trailing .py\n        return [f[:-3] for f in os.listdir(management_dir) if f.endswith('.py') and not f.startswith(\"__\")]\n    except OSError:\n        #If nothing is found, return empty\n        return []"}, {"instruction": "def draw_random(obj, **kwds):\n    \"\"\"Draw random variates from obj.random method.\n\n    If the object has parents whose value must be updated, use\n    parent_name=trace_generator_function.\n\n    Ex:\n    R = draw_random(theta, beta=pymc.utils.trace_generator(beta.trace))\n    R.next()\n    \"\"\"\n", "input": "", "output": "    while True:\n        for k, v in six.iteritems(kwds):\n            obj.parents[k] = v.next()\n        yield obj.random()"}, {"instruction": "def run(self):\n        \"\"\"\n        1. count the words for each of the :py:meth:`~.InputText.output` targets created by :py:class:`~.InputText`\n        2. write the count into the :py:meth:`~.WordCount.output` target\n        \"\"\"\n", "input": "", "output": "        count = {}\n\n        # NOTE: self.input() actually returns an element for the InputText.output() target\n        for f in self.input():  # The input() method is a wrapper around requires() that returns Target objects\n            for line in f.open('r'):  # Target objects are a file system/format abstraction and this will return a file stream object\n                for word in line.strip().split():\n                    count[word] = count.get(word, 0) + 1\n\n        # output data\n        f = self.output().open('w')\n        for word, count in six.iteritems(count):\n            f.write(\"%s\\t%d\\n\" % (word, count))\n        f.close()"}, {"instruction": "def object_new(self, template=None, **kwargs):\n        \"\"\"Creates a new object from an IPFS template.\n\n        By default this creates and returns a new empty merkledag node, but you\n        may pass an optional template argument to create a preformatted node.\n\n        .. code-block:: python\n\n            >>> c.object_new()\n            {'Hash': 'QmdfTbBqBPQ7VNxZEYEj14VmRuZBkqFbiwReogJgS1zR1n'}\n\n        Parameters\n        ----------\n        template : str\n            Blueprints from which to construct the new object. Possible values:\n\n             * ``\"unixfs-dir\"``\n             * ``None``\n\n        Returns\n        -------\n            dict : Object hash\n        \"\"\"\n", "input": "", "output": "        args = (template,) if template is not None else ()\n        return self._client.request('/object/new', args,\n                                    decoder='json', **kwargs)"}, {"instruction": "def make_transparent(image):\n    \"\"\"Turn all black pixels in an image into transparent ones\"\"\"\n", "input": "", "output": "    data = image.copy().getdata()\n    modified = []\n    for item in data:\n        if _check_pixel(item) is True:\n            modified.append((255, 255, 255, 255))  # White transparent pixel\n            continue\n        modified.append(item)\n    image.putdata(modified)\n    return image"}, {"instruction": "def list_domains():\n    '''\n    Return a list of virtual machine names on the minion\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt '*' virt.list_domains\n    '''\n", "input": "", "output": "    data = __salt__['vmadm.list'](keyed=True)\n    vms = [\"UUID                                  TYPE  RAM      STATE             ALIAS\"]\n    for vm in data:\n        vms.append(\"{vmuuid}{vmtype}{vmram}{vmstate}{vmalias}\".format(\n            vmuuid=vm.ljust(38),\n            vmtype=data[vm]['type'].ljust(6),\n            vmram=data[vm]['ram'].ljust(9),\n            vmstate=data[vm]['state'].ljust(18),\n            vmalias=data[vm]['alias'],\n        ))\n    return vms"}, {"instruction": "def route_table_get(name, resource_group, **kwargs):\n    '''\n    .. versionadded:: 2019.2.0\n\n    Get details about a specific route table.\n\n    :param name: The name of the route table to query.\n\n    :param resource_group: The resource group name assigned to the\n        route table.\n\n    CLI Example:\n\n    .. code-block:: bash\n\n        salt-call azurearm_network.route_table_get test-rt-table testgroup\n\n    '''\n", "input": "", "output": "    expand = kwargs.get('expand')\n\n    netconn = __utils__['azurearm.get_client']('network', **kwargs)\n\n    try:\n        table = netconn.route_tables.get(\n            route_table_name=name,\n            resource_group_name=resource_group,\n            expand=expand\n        )\n        result = table.as_dict()\n    except CloudError as exc:\n        __utils__['azurearm.log_cloud_error']('network', str(exc), **kwargs)\n        result = {'error': str(exc)}\n\n    return result"}, {"instruction": "def get_header(self, header_name, default=None):\n        \"\"\"Retrieve ``header_name`` from this request headers.\n        \"\"\"\n", "input": "", "output": "        return self.headers.get(\n            header_name, self.unredirected_headers.get(header_name, default))"}, {"instruction": "def post(f, *args, **kwargs):\n    \"\"\"Automatically log progress on function exit. Default logging value:\n    info.\n\n    *Logging with values contained in the parameters of the decorated function*\n    Message (args[0]) may be a string to be formatted with parameters passed to\n    the decorated function. Each '{varname}' will be replaced by the value of\n    the parameter of the same name.\n\n    *Keyword parameters*\n    - log :: integer\n      - Specifies a custom level of logging to pass to the active logger.\n      - Default: INFO\n\n    *Exceptions:*\n    - IndexError and ValueError\n      - will be returned if *args contains a string that does not correspond to\n        a parameter name of the decorated function, or if there are more '{}'s\n        than there are *args.\n\n    \"\"\"\n", "input": "", "output": "    kwargs.update({'postfix_only': True})\n    return _stump(f, *args, **kwargs)"}, {"instruction": "def tag_audio(filename, tracklisting):\n    \"\"\"Return True if audio tagged successfully; handle tagging audio.\"\"\"\n", "input": "", "output": "    # TODO: maybe actually glob for files, then try tagging if present?\n    if not(tag_audio_file(filename + '.m4a', tracklisting) or\n           tag_audio_file(filename + '.mp3', tracklisting)):\n        print(\"Cannot find or access any relevant M4A or MP3 audio file.\")\n        print(\"Trying to save a text file instead.\")\n        write_text(filename, tracklisting)\n        return False\n    return True"}, {"instruction": "def walk_dependencies(root, visitor):\n    \"\"\"\n    Call visitor on root and all dependencies reachable from it in breadth\n    first order.\n\n    Args:\n        root (component): component function or class\n        visitor (function): signature is `func(component, parent)`.  The\n            call on root is `visitor(root, None)`.\n    \"\"\"\n", "input": "", "output": "    def visit(parent, visitor):\n        for d in get_dependencies(parent):\n            visitor(d, parent)\n            visit(d, visitor)\n\n    visitor(root, None)\n    visit(root, visitor)"}, {"instruction": "def install_handler(self, app):\n        \"\"\"Install logging handler.\"\"\"\n", "input": "", "output": "        # Configure python logging\n        if app.config['LOGGING_CONSOLE_PYWARNINGS']:\n            self.capture_pywarnings(logging.StreamHandler())\n\n        if app.config['LOGGING_CONSOLE_LEVEL'] is not None:\n            for h in app.logger.handlers:\n                h.setLevel(app.config['LOGGING_CONSOLE_LEVEL'])\n\n        # Add request_id to log record\n        app.logger.addFilter(add_request_id_filter)"}, {"instruction": "def _cache_translation_needs_fallback(instance, language_code, related_name, timeout=cache.default_timeout):\n    \"\"\"\n    Store the fact that a translation doesn't exist, and the fallback should be used.\n    \"\"\"\n", "input": "", "output": "    if not appsettings.PARLER_ENABLE_CACHING or not instance.pk or instance._state.adding:\n        return\n\n    tr_model = instance._parler_meta.get_model_by_related_name(related_name)\n    key = get_translation_cache_key(tr_model, instance.pk, language_code)\n    cache.set(key, {'__FALLBACK__': True}, timeout=timeout)"}, {"instruction": "def switch_to_output(self, value=False, **kwargs):\n        \"\"\"Switch the pin state to a digital output with the provided starting\n        value (True/False for high or low, default is False/low).\n        \"\"\"\n", "input": "", "output": "        self.direction = digitalio.Direction.OUTPUT\n        self.value = value"}, {"instruction": "def error_map_source(self, kwargs_source, x_grid, y_grid, cov_param):\n        \"\"\"\n        variance of the linear source reconstruction in the source plane coordinates,\n        computed by the diagonal elements of the covariance matrix of the source reconstruction as a sum of the errors\n        of the basis set.\n\n        :param kwargs_source: keyword arguments of source model\n        :param x_grid: x-axis of positions to compute error map\n        :param y_grid: y-axis of positions to compute error map\n        :param cov_param: covariance matrix of liner inversion parameters\n        :return: diagonal covariance errors at the positions (x_grid, y_grid)\n        \"\"\"\n", "input": "", "output": "\n        error_map = np.zeros_like(x_grid)\n        basis_functions, n_source = self.SourceModel.functions_split(x_grid, y_grid, kwargs_source)\n        basis_functions = np.array(basis_functions)\n\n        if cov_param is not None:\n            for i in range(len(error_map)):\n                error_map[i] = basis_functions[:, i].T.dot(cov_param[:n_source, :n_source]).dot(basis_functions[:, i])\n        return error_map"}, {"instruction": "def _rapply(d, func, *args, **kwargs):\n    \"\"\"Apply a function to all values in a dictionary or list of dictionaries, recursively.\"\"\"\n", "input": "", "output": "    if isinstance(d, (tuple, list)):\n        return [_rapply(each, func, *args, **kwargs) for each in d]\n    if isinstance(d, dict):\n        return {\n            key: _rapply(value, func, *args, **kwargs) for key, value in iteritems(d)\n        }\n    else:\n        return func(d, *args, **kwargs)"}, {"instruction": "def pbkdf2(seed: str or bytes, dk_len: int) -> bytes:\n    \"\"\"\n    Derive one key from a seed.\n\n    :param seed: the secret pass phrase to generate the keys from.\n    :param dk_len: the length in bytes of every derived key.\n    :return:\n    \"\"\"\n", "input": "", "output": "    key = b''\n    index = 1\n    bytes_seed = str_to_bytes(seed)\n    while len(key) < dk_len:\n        key += Digest.sha256(b''.join([bytes_seed, index.to_bytes(4, 'big', signed=True)]))\n        index += 1\n    return key[:dk_len]"}, {"instruction": "def get(self, item, default=None):\n        \"\"\"\n        Returns the value ``item`` from the host or hosts group variables.\n\n        Arguments:\n            item(``str``): The variable to get\n            default(``any``): Return value if item not found\n        \"\"\"\n", "input": "", "output": "        if hasattr(self, item):\n            return getattr(self, item)\n        try:\n            return self.__getitem__(item)\n\n        except KeyError:\n            return default"}, {"instruction": "def _Rzderiv(self,R,z,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _Rzderiv\n        PURPOSE:\n           evaluate the mixed radial, vertical derivative for this potential\n        INPUT:\n           R - Galactocentric cylindrical radius\n           z - vertical height\n           phi - azimuth\n           t - time\n        OUTPUT:\n           the mixed radial, vertical derivative\n        \"\"\"\n", "input": "", "output": "        if not self.isNonAxi:\n            phi= 0.\n        x,y,z= self._compute_xyz(R,phi,z,t)\n        phixza= self._2ndderiv_xyz(x,y,z,0,2)\n        phiyza= self._2ndderiv_xyz(x,y,z,1,2)\n        ang = self._omegab*t + self._pa\n        c, s = np.cos(ang), np.sin(ang)\n        phixz = c*phixza + s*phiyza\n        phiyz = -s*phixza + c*phiyza\n        return np.cos(phi)*phixz + np.sin(phi)*phiyz"}, {"instruction": "def is_path_like(obj, attr=('name', 'is_file', 'is_dir', 'iterdir')):\n    \"\"\"test if object is pathlib.Path like\"\"\"\n", "input": "", "output": "    for a in attr:\n        if not hasattr(obj, a):\n            return False\n    return True"}, {"instruction": "def from_fqdn(cls, fqdn):\n        \"\"\"Retrieve domain id associated to a FQDN.\"\"\"\n", "input": "", "output": "        result = cls.list({'fqdn': fqdn})\n        if len(result) > 0:\n            return result[0]['id']"}, {"instruction": "def wait_for_compute_global_operation(project_name, operation):\n    \"\"\"Poll for global compute operation until finished.\"\"\"\n", "input": "", "output": "    logger.info(\"wait_for_compute_global_operation: \"\n                \"Waiting for operation {} to finish...\".format(\n                    operation[\"name\"]))\n\n    for _ in range(MAX_POLLS):\n        result = compute.globalOperations().get(\n            project=project_name,\n            operation=operation[\"name\"],\n        ).execute()\n        if \"error\" in result:\n            raise Exception(result[\"error\"])\n\n        if result[\"status\"] == \"DONE\":\n            logger.info(\"wait_for_compute_global_operation: \"\n                        \"Operation done.\")\n            break\n\n        time.sleep(POLL_INTERVAL)\n\n    return result"}, {"instruction": "def _datasets_line(args):\n  \"\"\"Implements the BigQuery datasets magic used to display datasets in a project.\n\n   The supported syntax is:\n\n       %bigquery datasets [-f <filter>] [-p|--project <project_id>]\n\n  Args:\n    args: the arguments following '%bigquery datasets'.\n  Returns:\n    The HTML rendering for the table of datasets.\n  \"\"\"\n", "input": "", "output": "  filter_ = args['filter'] if args['filter'] else '*'\n  return _render_list([str(dataset) for dataset in datalab.bigquery.Datasets(args['project'])\n                       if fnmatch.fnmatch(str(dataset), filter_)])"}, {"instruction": "def process_settings(pelicanobj):\n    \"\"\"Sets user specified settings (see README for more details)\"\"\"\n", "input": "", "output": "\n    # Default settings\n    inline_settings = {}\n    inline_settings['config'] = {'[]':('', 'pelican-inline')}\n\n    # Get the user specified settings\n    try:\n        settings = pelicanobj.settings['MD_INLINE']\n    except:\n        settings = None\n\n    # If settings have been specified, add them to the config\n    if isinstance(settings, dict):\n        inline_settings['config'].update(settings)\n\n    return inline_settings"}, {"instruction": "def _handle_clear(self, load):\n        '''\n        Process a cleartext command\n\n        :param dict load: Cleartext payload\n        :return: The result of passing the load to a function in ClearFuncs corresponding to\n                 the command specified in the load's 'cmd' key.\n        '''\n", "input": "", "output": "        log.trace('Clear payload received with command %s', load['cmd'])\n        cmd = load['cmd']\n        if cmd.startswith('__'):\n            return False\n        if self.opts['master_stats']:\n            start = time.time()\n        ret = getattr(self.clear_funcs, cmd)(load), {'fun': 'send_clear'}\n        if self.opts['master_stats']:\n            stats = salt.utils.event.update_stats(self.stats, start, load)\n            self._post_stats(stats)\n        return ret"}, {"instruction": "def get_tile_gid(self, x, y, layer):\n        \"\"\" Return the tile image GID for this location\n\n        :param x: x coordinate\n        :param y: y coordinate\n        :param layer: layer number\n        :rtype: surface if found, otherwise ValueError\n        \"\"\"\n", "input": "", "output": "        try:\n            assert (x >= 0 and y >= 0 and layer >= 0)\n        except AssertionError:\n            raise ValueError\n\n        try:\n            return self.layers[int(layer)].data[int(y)][int(x)]\n        except (IndexError, ValueError):\n            msg = \"Coords: ({0},{1}) in layer {2} is invalid\"\n            logger.debug(msg, (x, y, layer))\n            raise ValueError"}, {"instruction": "def get_intersectionsbysubsets(df,cols_fracby2vals,cols_subset,col_ids):\n    \"\"\"\n    cols_fracby:\n    cols_subset:\n    \"\"\"\n", "input": "", "output": "    for col_fracby in cols_fracby2vals:\n        val=cols_fracby2vals[col_fracby]\n        ids=df.loc[(df[col_fracby]==val),col_ids].dropna().unique()\n        for col_subset in cols_subset:\n            for subset in dropna(df[col_subset].unique()):\n                ids_subset=df.loc[(df[col_subset]==subset),col_ids].dropna().unique()\n                df.loc[(df[col_subset]==subset),f'P {col_fracby} {col_subset}']=len(set(ids_subset).intersection(ids))/len(ids_subset)\n    return df"}, {"instruction": "def get_argv_for_command(self):\n        \"\"\"\n        Returns stripped arguments that would be passed into the command.\n        \"\"\"\n", "input": "", "output": "        argv = [a for a in self.argv]\n        argv.insert(0, self.prog_name)\n        return argv"}, {"instruction": "def iter(self, name):\n        '''\n        Iterate through values added with add() from each scope frame.\n        '''\n", "input": "", "output": "        for frame in self.frames:\n            vals = frame.get(name)\n            if vals is None:\n                continue\n            for valu in vals:\n                yield valu"}, {"instruction": "def weld_str_get(array, i):\n    \"\"\"Retrieve character at index i.\n\n    Parameters\n    ----------\n    array : numpy.ndarray or WeldObject\n        Input data.\n    i : int\n        Index of character to retrieve. If greater than length of string, returns None.\n\n    Returns\n    -------\n    WeldObject\n        Representation of this computation.\n\n    \"\"\"\n", "input": "", "output": "    obj_id, weld_obj = create_weld_object(array)\n    index_literal = to_weld_literal(i, WeldLong())\n    missing_literal = default_missing_data_literal(WeldVec(WeldChar()))\n    missing_literal_id = get_weld_obj_id(weld_obj, missing_literal)\n\n    weld_template = "}, {"instruction": "def p_class_declaration_statement(p):\n    '''class_declaration_statement : class_entry_type STRING extends_from implements_list LBRACE class_statement_list RBRACE\n                                   | INTERFACE STRING interface_extends_list LBRACE class_statement_list RBRACE'''\n", "input": "", "output": "    if len(p) == 8:\n        p[0] = ast.Class(p[2], p[1], p[3], p[4], p[6], lineno=p.lineno(2))\n    else:\n        p[0] = ast.Interface(p[2], p[3], p[5], lineno=p.lineno(1))"}, {"instruction": "def disassemble_string(self, lpAddress, code):\n        \"\"\"\n        Disassemble instructions from a block of binary code.\n\n        @type  lpAddress: int\n        @param lpAddress: Memory address where the code was read from.\n\n        @type  code: str\n        @param code: Binary code to disassemble.\n\n        @rtype:  list of tuple( long, int, str, str )\n        @return: List of tuples. Each tuple represents an assembly instruction\n            and contains:\n             - Memory address of instruction.\n             - Size of instruction in bytes.\n             - Disassembly line of instruction.\n             - Hexadecimal dump of instruction.\n        \"\"\"\n", "input": "", "output": "        aProcess = self.get_process()\n        return aProcess.disassemble_string(lpAddress, code)"}, {"instruction": "def unit_ball_L_inf(shape, precondition=True):\n  \"\"\"A tensorflow variable tranfomed to be constrained in a L_inf unit ball.\n\n  Note that this code also preconditions the gradient to go in the L_inf\n  direction of steepest descent.\n\n  EXPERIMENTAL: Do not use for adverserial examples if you need to be confident\n  they are strong attacks. We are not yet confident in this code.\n  \"\"\"\n", "input": "", "output": "  x = tf.Variable(tf.zeros(shape))\n  if precondition:\n    return constrain_L_inf_precondition(x)\n  else:\n    return constrain_L_inf(x)"}, {"instruction": "def _load_data():\n    \"\"\"Load the word and character mapping data into a dictionary.\n\n    In the data files, each line is formatted like this:\n        HANZI   PINYIN_READING/PINYIN_READING\n\n    So, lines need to be split by '\\t' and then the Pinyin readings need to be\n    split by '/'.\n\n    \"\"\"\n", "input": "", "output": "    data = {}\n    for name, file_name in (('words', 'hanzi_pinyin_words.tsv'),\n                            ('characters', 'hanzi_pinyin_characters.tsv')):\n        # Split the lines by tabs: [[hanzi, pinyin]...].\n        lines = [line.split('\\t') for line in\n                 dragonmapper.data.load_data_file(file_name)]\n        # Make a dictionary: {hanzi: [pinyin, pinyin]...}.\n        data[name] = {hanzi: pinyin.split('/') for hanzi, pinyin in lines}\n    return data"}, {"instruction": "def edit_config_input_edit_content_url_url(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        edit_config = ET.Element(\"edit_config\")\n        config = edit_config\n        input = ET.SubElement(edit_config, \"input\")\n        edit_content = ET.SubElement(input, \"edit-content\")\n        url = ET.SubElement(edit_content, \"url\")\n        url = ET.SubElement(url, \"url\")\n        url.text = kwargs.pop('url')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def uni_char_code(a: str, b: str, c: str, d: str):\n    \"\"\"Convert unicode characters to integers.\n\n    Converts four hexadecimal chars to the integer that the string represents.\n    For example, uni_char_code('0','0','0','f') will return 15,\n    and uni_char_code('0','0','f','f') returns 255.\n\n    Returns a negative number on error, if a char was invalid.\n\n    This is implemented by noting that char2hex() returns -1 on error,\n    which means the result of ORing the char2hex() will also be negative.\n    \"\"\"\n", "input": "", "output": "    return char2hex(a) << 12 | char2hex(b) << 8 | char2hex(c) << 4 | char2hex(d)"}, {"instruction": "def reduced_chi_squareds(self, p=None):\n        \"\"\"\n        Returns the reduced chi squared for each massaged data set. \n\n        p=None means use the fit results.\n        \"\"\"\n", "input": "", "output": "        if len(self._set_xdata)==0 or len(self._set_ydata)==0: return None\n\n        if p is None: p = self.results[0]\n        r = self.studentized_residuals(p)\n        \n        # In case it's not possible to calculate\n        if r is None: return\n\n        # calculate the number of points\n        N = 0\n        for i in range(len(r)): N += len(r[i])\n\n        # degrees of freedom\n        dof_per_point = self.degrees_of_freedom()/N\n\n        for n in range(len(r)):\n            r[n] = sum(r[n]**2)/(len(r[n])*dof_per_point)\n\n        return r"}, {"instruction": "def _num_required_args(func):\n    \"\"\" Number of args for func\n\n        >>> def foo(a, b, c=None):\n        ... return a + b + c\n\n        >>> _num_required_args(foo)\n        2\n\n        >>> def bar(*args):\n        ... return sum(args)\n\n        >>> print(_num_required_args(bar))\n        None\n\n        borrowed from: https://github.com/pytoolz/toolz\n    \"\"\"\n", "input": "", "output": "    try:\n        spec = inspect.getargspec(func)\n        if spec.varargs:\n            return None\n        num_defaults = len(spec.defaults) if spec.defaults else 0\n        return len(spec.args) - num_defaults\n    except TypeError:\n        return None"}, {"instruction": "def clean(self):\n        \"\"\"\n        Checks for the identification and password.\n\n        If the combination can't be found will raise an invalid sign in error.\n\n        \"\"\"\n", "input": "", "output": "        identification = self.cleaned_data.get('identification')\n        password = self.cleaned_data.get('password')\n\n        if identification and password:\n            self.user_cache = authenticate(identification=identification, \n                                password=password)\n            if self.user_cache is None:\n                raise forms.ValidationError(_(u\"Please enter a correct \"\n                        \"username or email address and password. \"\n                        \"Note that both fields are case-sensitive.\"))\n        return self.cleaned_data"}, {"instruction": "def set_settings(self, releases=None, default_release=None):\n        \"\"\"set path to storage\"\"\"\n", "input": "", "output": "        super(ReplicaSets, self).set_settings(releases, default_release)\n        Servers().set_settings(releases, default_release)"}, {"instruction": "def getExtensions(self, extname='SCI', section=None):\n        \"\"\" Return the list of EXTVER values for extensions with name specified\n        in extname.\n\n        \"\"\"\n", "input": "", "output": "        if section is None:\n            numext = 0\n            section = []\n            for hdu in self._image:\n                if 'extname' in hdu.header and hdu.header['extname'] == extname:\n                    section.append(hdu.header['extver'])\n        else:\n            if not isinstance(section,list):\n                section = [section]\n\n        return section"}, {"instruction": "def _parse_input_node(cls, node):\n        \"\"\"\n        :param node: xml node\n        :return: dict\n        \"\"\"\n", "input": "", "output": "        data = {}\n        child = node.getchildren()\n        if not child and node.get('name'):\n            val = node.text\n        elif child:  # if tag = \"{http://activiti.org/bpmn}script\" then data_typ = 'script'\n            data_typ = child[0].tag.split('}')[1]\n            val = getattr(cls, '_parse_%s' % data_typ)(child[0])\n        data[node.get('name')] = val\n        return data"}, {"instruction": "def set_bpp(self, bpp):\n        \"\"\"\n        Set the bit depth (per band) for the output.\n        A typical \"32-bit RGBA\" image would be 8; a 48-bit image would\n        be 16, etc.\n        \"\"\"\n", "input": "", "output": "        self.bpp = bpp\n        self.maxc = 2 ** self.bpp - 1\n        self._set_dtype()\n        self.calc_cmap()\n        self.recalc(callback=False)"}, {"instruction": "def dump(self, raw=False):\n        ''' Dump all output currently in the arm's output queue. '''\n", "input": "", "output": "        raw_out = self.ser.read(self.ser.in_waiting)\n        if raw:\n            return raw_out\n        return raw_out.decode(OUTPUT_ENCODING)"}, {"instruction": "def chi_squared(source_frequency, target_frequency):\n    \"\"\"Calculate the Chi Squared statistic by comparing ``source_frequency`` with ``target_frequency``.\n\n    Example:\n        >>> chi_squared({'a': 2, 'b': 3}, {'a': 1, 'b': 2})\n        0.1\n\n    Args:\n        source_frequency (dict): Frequency map of the text you are analyzing\n        target_frequency (dict): Frequency map of the target language to compare with\n\n    Returns:\n        Decimal value of the chi-squared statistic\n    \"\"\"\n", "input": "", "output": "    # Ignore any symbols from source that are not in target.\n    # TODO: raise Error if source_len is 0?\n    target_prob = frequency_to_probability(target_frequency)\n    source_len = sum(v for k, v in source_frequency.items() if k in target_frequency)\n\n    result = 0\n    for symbol, prob in target_prob.items():\n        symbol_frequency = source_frequency.get(symbol, 0)  # Frequecy is 0 if it doesnt appear in source\n        result += _calculate_chi_squared(symbol_frequency, prob, source_len)\n\n    return result"}, {"instruction": "def _subperiod_tick(self, current_interval, intervals):\n        \"\"\"Tick each sub-period, copying group_decisions to subperiod_group_decisions.\"\"\"\n", "input": "", "output": "        self.refresh_from_db()\n        for key, value in self.group_decisions.items():\n            self.subperiod_group_decisions[key] = value\n        self.send('group_decisions', self.subperiod_group_decisions)\n        self.save(update_fields=['subperiod_group_decisions'])"}, {"instruction": "def update_metadata_from_rmd_options(name, value, metadata):\n    \"\"\"\n    Update metadata using the _BOOLEAN_OPTIONS_DICTIONARY mapping\n    :param name: option name\n    :param value: option value\n    :param metadata:\n    :return:\n    \"\"\"\n", "input": "", "output": "    for jupyter_option, rmd_option, rev in _BOOLEAN_OPTIONS_DICTIONARY:\n        if name == rmd_option:\n            try:\n                metadata[jupyter_option] = _py_logical_values(value) != rev\n                return True\n            except RLogicalValueError:\n                pass\n    return False"}, {"instruction": "def folderitems(self, full_objects=False, classic=True):\n        \"\"\"Sort by Categories\n        \"\"\"\n", "input": "", "output": "        bsc = getToolByName(self.context, \"bika_setup_catalog\")\n        self.an_cats = bsc(\n            portal_type=\"AnalysisCategory\",\n            sort_on=\"sortable_title\")\n        self.an_cats_order = dict([\n            (b.Title, \"{:04}\".format(a))\n            for a, b in enumerate(self.an_cats)])\n        items = super(AnalysisServicesView, self).folderitems()\n        if self.do_cats:\n            self.categories = map(lambda x: x[0],\n                                  sorted(self.categories, key=lambda x: x[1]))\n        else:\n            self.categories.sort()\n        return items"}, {"instruction": "def _audio_item(self, stream_url=None, offset=0, push_buffer=True, opaque_token=None):\n        \"\"\"Builds an AudioPlayer Directive's audioItem and updates current_stream\"\"\"\n", "input": "", "output": "        audio_item = {'stream': {}}\n        stream = audio_item['stream']\n\n        # existing stream\n        if not stream_url:\n            # stream.update(current_stream.__dict__)\n            stream['url'] = current_stream.url\n            stream['token'] = current_stream.token\n            stream['offsetInMilliseconds'] = current_stream.offsetInMilliseconds\n\n        # new stream\n        else:\n            stream['url'] = stream_url\n            stream['token'] = opaque_token or str(uuid.uuid4())\n            stream['offsetInMilliseconds'] = offset\n\n        if push_buffer:  # prevents enqueued streams from becoming current_stream\n            push_stream(stream_cache, context['System']['user']['userId'], stream)\n        return audio_item"}, {"instruction": "def sub(self, b):\n        \"\"\"\n        Binary operation: sub\n\n        :param b: The other operand\n        :return: self - b\n        \"\"\"\n", "input": "", "output": "        new_bits = max(self.bits, b.bits)\n\n        overflow = self._wrapped_overflow_sub(self, b)\n        if overflow:\n            return StridedInterval.top(self.bits)\n\n        lb = self._modular_sub(self.lower_bound, b.upper_bound, new_bits)\n        ub = self._modular_sub(self.upper_bound, b.lower_bound, new_bits)\n\n        # Is it initialized?\n        uninitialized = self.uninitialized or b.uninitialized\n\n        # Take the GCD of two operands' strides\n        stride = fractions.gcd(self.stride, b.stride)\n\n        return StridedInterval(bits=new_bits, stride=stride, lower_bound=lb, upper_bound=ub,\n                               uninitialized=uninitialized).normalize()"}, {"instruction": "def _container_start_handler(ion_type, length, ctx):\n    \"\"\"Handles container delegation.\"\"\"\n", "input": "", "output": "    _, self = yield\n\n    container_ctx = ctx.derive_container_context(length)\n    if ctx.annotations and ctx.limit != container_ctx.limit:\n        # 'ctx' is the annotation wrapper context. `container_ctx` represents the wrapper's 'value' subfield. Their\n        # limits must match.\n        raise IonException('Incorrect annotation wrapper length.')\n    delegate = _container_handler(ion_type, container_ctx)\n\n    # We start the container, and transition to the new container processor.\n    yield ctx.event_transition(\n        IonEvent, IonEventType.CONTAINER_START, ion_type, value=None, whence=delegate\n    )"}, {"instruction": "def right(self, expand=None):\n        \"\"\" Returns a new Region right of the current region with a width of ``expand`` pixels.\n\n        Does not include the current region. If range is omitted, it reaches to the right border\n        of the screen. The new region has the same height and y-position as the current region.\n        \"\"\"\n", "input": "", "output": "        if expand == None:\n            x = self.x+self.w\n            y = self.y\n            w = self.getScreen().getBounds()[2] - x\n            h = self.h\n        else:\n            x = self.x+self.w\n            y = self.y\n            w = expand\n            h = self.h\n        return Region(x, y, w, h).clipRegionToScreen()"}, {"instruction": "def raw_repr(obj):\n    '''Produce a representation using the default repr() regardless of\n    whether the object provides an implementation of its own.'''\n", "input": "", "output": "    if isproxy(obj):\n        return '<%s with prime_id=%d>' % (obj.__class__.__name__, obj.prime_id)\n    else:\n        return repr(obj)"}, {"instruction": "def _after_handler(self, iid, callback, args):\n        \"\"\"Proxy to called by after() in mainloop\"\"\"\n", "input": "", "output": "        self._after_id = None\n        self.update_state(iid, \"normal\")\n        self.call_callbacks(iid, callback, args)"}, {"instruction": "def from_stored(self, key):\n        \"\"\"\n        Set the current collection as based on a stored one. The key argument\n        is the key off the stored collection.\n        \"\"\"\n", "input": "", "output": "        # only one stored key allowed\n        if self.stored_key:\n            raise ValueError('This collection is already based on a stored one')\n\n        # prepare the collection\n        self.stored_key = key\n        self.intersect(_StoredCollection(self.cls.get_connection(), key))\n        self.sort(by='nosort')  # keep stored order\n\n        # count the number of results to manage empty result (to not behave like\n        # expired key)\n        self._stored_len = self.cls.get_connection().llen(key)\n\n        return self"}, {"instruction": "def is_logged(self, user):\n        \"\"\"Check if a logged user is trying to access the register page.\n           If so, redirect him/her to his/her profile\"\"\"\n", "input": "", "output": "\n        response = None\n        if user.is_authenticated():\n            if not user.needs_update:\n                response = redirect('user_profile', username=user.username)\n\n        return response"}, {"instruction": "def floatify_latlng(input_value):\n    \"\"\"\n    Work around a JSON dict with string, not float, lat/lngs.\n\n    Given anything (list/dict/etc) it will return that thing again, *but* any\n    dict (at any level) that has only 2 elements lat & lng, will be replaced\n    with the lat & lng turned into floats.\n\n    If the API returns the lat/lng as strings, and not numbers, then this\n    function will 'clean them up' to be floats.\n    \"\"\"\n", "input": "", "output": "    if isinstance(input_value, collections.Mapping):\n        if len(input_value) == 2 and sorted(input_value.keys()) == ['lat', 'lng']:\n            # This dict has only 2 keys 'lat' & 'lon'\n            return {'lat': float_if_float(input_value[\"lat\"]), 'lng': float_if_float(input_value[\"lng\"])}\n        else:\n            return dict((key, floatify_latlng(value)) for key, value in input_value.items())\n    elif isinstance(input_value, collections.MutableSequence):\n        return [floatify_latlng(x) for x in input_value]\n    else:\n        return input_value"}, {"instruction": "def to_protobuf(self):\n        \"\"\"Convert the current object to protobuf.\n\n        :rtype: :class:`google.type.latlng_pb2.LatLng`.\n        :returns: The current point as a protobuf.\n        \"\"\"\n", "input": "", "output": "        return latlng_pb2.LatLng(latitude=self.latitude, longitude=self.longitude)"}, {"instruction": "def package(self):\n        \"\"\"Copy Flatbuffers' artifacts to package folder\n        \"\"\"\n", "input": "", "output": "        cmake = self.configure_cmake()\n        cmake.install()\n        self.copy(pattern=\"LICENSE.txt\", dst=\"licenses\")\n        self.copy(pattern=\"FindFlatBuffers.cmake\", dst=os.path.join(\"lib\", \"cmake\", \"flatbuffers\"), src=\"CMake\")\n        self.copy(pattern=\"flathash*\", dst=\"bin\", src=\"bin\")\n        self.copy(pattern=\"flatc*\", dst=\"bin\", src=\"bin\")\n        if self.settings.os == \"Windows\" and self.options.shared:\n            if self.settings.compiler == \"Visual Studio\":\n                shutil.move(os.path.join(self.package_folder, \"lib\", \"%s.dll\" % self.name),\n                            os.path.join(self.package_folder, \"bin\", \"%s.dll\" % self.name))\n            elif self.settings.compiler == \"gcc\":\n                shutil.move(os.path.join(self.package_folder, \"lib\", \"lib%s.dll\" % self.name),\n                            os.path.join(self.package_folder, \"bin\", \"lib%s.dll\" % self.name))"}, {"instruction": "def halt(self):\n        \"\"\"Halts the CPU Core.\n\n        Args:\n          self (JLink): the ``JLink`` instance\n\n        Returns:\n          ``True`` if halted, ``False`` otherwise.\n        \"\"\"\n", "input": "", "output": "        res = int(self._dll.JLINKARM_Halt())\n        if res == 0:\n            time.sleep(1)\n            return True\n        return False"}, {"instruction": "def ValidateLanguageCode(lang, column_name=None, problems=None):\n  \"\"\"\n  Validates a non-required language code value using the pybcp47 module:\n    - if invalid adds InvalidValue error (if problems accumulator is provided)\n    - distinguishes between 'not well-formed' and 'not valid' and adds error\n      reasons accordingly\n    - an empty language code is regarded as valid! Otherwise we might end up\n      with many duplicate errors because of the required field checks.\n    - returns true if the language is valid, false if not well-formed or\n      invalid.\n  \"\"\"\n", "input": "", "output": "  if util.IsEmpty(lang):\n    return True\n  bcp47_obj = parser.ParseLanguage(str(lang.lower()))\n  if not bcp47_obj.wellformed:\n    if problems:\n      problems.InvalidValue(column_name, lang,\n                            'language code \"%s\" is not well-formed' %\n                            lang, type=problems_class.TYPE_ERROR)\n    return False\n  if not bcp47_obj.valid:\n    if problems:\n      problems.InvalidValue(column_name, lang,\n                            'language code \"%s\" is not valid, parses as: %s' %\n                            (lang, bcp47_obj), type=problems_class.TYPE_WARNING)\n    return False\n  return True"}, {"instruction": "def dir_tails(self, rr_id: str) -> str:\n        \"\"\"\n        Return path to the correct directory for the tails file on input revocation registry identifier.\n\n        :param rr_id: revocation registry identifier of interest\n        :return: path to tails dir for input revocation registry identifier\n        \"\"\"\n", "input": "", "output": "\n        return Tails.dir(self._dir_tails, rr_id)"}, {"instruction": "def en_last(self):\n        \"\"\" Report the energies from the last SCF present in the output.\n\n        Returns a |dict| providing the various energy values from the\n        last SCF cycle performed in the output. Keys are those of\n        :attr:`~opan.output.OrcaOutput.p_en`.\n        Any energy value not relevant to the parsed\n        output is assigned as |None|.\n\n        Returns\n        -------\n        last_ens\n            |dict| of |npfloat_|--\n            Energies from the last SCF present in the output.\n\n        \"\"\"\n", "input": "", "output": "\n        # Initialize the return dict\n        last_ens = dict()\n\n        # Iterate and store\n        for (k,l) in self.en.items():\n            last_ens.update({ k : l[-1] if l != [] else None })\n        ##next (k,l)\n\n        # Should be ready to return?\n        return last_ens"}, {"instruction": "def path_helper(self, operations, view, app=None, **kwargs):\n        \"\"\"Path helper that allows passing a Flask view function.\"\"\"\n", "input": "", "output": "        rule = self._rule_for_view(view, app=app)\n        operations.update(yaml_utils.load_operations_from_docstring(view.__doc__))\n        if hasattr(view, 'view_class') and issubclass(view.view_class, MethodView):\n            for method in view.methods:\n                if method in rule.methods:\n                    method_name = method.lower()\n                    method = getattr(view.view_class, method_name)\n                    operations[method_name] = yaml_utils.load_yaml_from_docstring(method.__doc__)\n        return self.flaskpath2openapi(rule.rule)"}, {"instruction": "def prune_empty_node(node, seen):\n    \"\"\"\n    Recursively remove empty branches and return whether this makes the node\n    itself empty.\n\n    The ``seen`` parameter is used to avoid infinite recursion due to cycles\n    (you never know).\n    \"\"\"\n", "input": "", "output": "    if node.methods:\n        return False\n    if id(node) in seen:\n        return True\n    seen = seen | {id(node)}\n    for branch in list(node.branches):\n        if prune_empty_node(branch, seen):\n            node.branches.remove(branch)\n        else:\n            return False\n    return True"}, {"instruction": "def p_do_loop_while(p):\n    \"\"\" statement : do_start program_co label_loop WHILE expr\n                  | do_start label_loop WHILE expr\n                  | DO label_loop WHILE expr\n    \"\"\"\n", "input": "", "output": "    if len(p) == 6:\n        q = make_block(p[2], p[3])\n        r = p[5]\n    else:\n        q = p[2]\n        r = p[4]\n\n    if p[1] == 'DO':\n        gl.LOOPS.append(('DO',))\n\n    p[0] = make_sentence('DO_WHILE', r, q)\n    gl.LOOPS.pop()\n\n    if is_number(r):\n        api.errmsg.warning_condition_is_always(p.lineno(3), bool(r.value))\n    if q is None:\n        api.errmsg.warning_empty_loop(p.lineno(3))"}, {"instruction": "def calc_prob_mom(returns, other_returns):\n    \"\"\"\n    `Probabilistic momentum <http://cssanalytics.wordpress.com/2014/01/28/are-simple-momentum-strategies-too-dumb-introducing-probabilistic-momentum/>`_ (see `momentum investing <https://www.investopedia.com/terms/m/momentum_investing.asp>`_)\n\n    Basically the \"probability or confidence that one asset\n    is going to outperform the other\".\n\n    Source:\n        http://cssanalytics.wordpress.com/2014/01/28/are-simple-momentum-strategies-too-dumb-introducing-probabilistic-momentum/ # NOQA\n    \"\"\"\n", "input": "", "output": "    return t.cdf(returns.calc_information_ratio(other_returns),\n                 len(returns) - 1)"}, {"instruction": "def device(self, idx):\n        \"\"\"Get a specific GPU device\n\n        Args:\n            idx: index of device\n\n        Returns:\n            NvidiaDevice: single GPU device\n        \"\"\"\n", "input": "", "output": "\n        class GpuDevice(Structure):\n            pass\n\n        c_nvmlDevice_t = POINTER(GpuDevice)\n\n        c_index = c_uint(idx)\n        device = c_nvmlDevice_t()\n        _check_return(_NVML.get_function(\n            \"nvmlDeviceGetHandleByIndex_v2\")(c_index, byref(device)))\n        return NvidiaDevice(device)"}, {"instruction": "def SerializeExclusiveData(self, writer):\n        \"\"\"\n        Serialize object.\n\n        Args:\n            writer (neo.IO.BinaryWriter):\n        \"\"\"\n", "input": "", "output": "        writer.WriteVarBytes(self.Script)\n        if self.Version >= 1:\n            writer.WriteFixed8(self.Gas)"}, {"instruction": "def points(self, size=1.0, highlight=None, colorlist=None, opacity=1.0):\n        \"\"\"Display the system as points.\n\n        :param float size: the size of the points.\n\n\n        \"\"\"\n", "input": "", "output": "        if colorlist is None:\n            colorlist = [get_atom_color(t) for t in self.topology['atom_types']]\n        if highlight is not None:\n            if isinstance(highlight, int):\n                colorlist[highlight] = 0xff0000\n            if isinstance(highlight, (list, np.ndarray)):\n                for i in highlight:\n                    colorlist[i] = 0xff0000\n\n        sizes = [size] * len(self.topology['atom_types'])\n\n        points = self.add_representation('points', {'coordinates': self.coordinates.astype('float32'),\n                                                    'colors': colorlist,\n                                                    'sizes': sizes,\n                                                    'opacity': opacity})\n        # Update closure\n        def update(self=self, points=points):\n            self.update_representation(points, {'coordinates': self.coordinates.astype('float32')})\n\n        self.update_callbacks.append(update)\n        self.autozoom(self.coordinates)"}, {"instruction": "def entropy_bits_nrange(\n        minimum: Union[int, float], maximum: Union[int, float]\n) -> float:\n    \"\"\"Calculate the number of entropy bits in a range of numbers.\"\"\"\n", "input": "", "output": "    # Shannon:\n    # d = fabs(maximum - minimum)\n    # ent = -(1/d) * log(1/d, 2) * d\n    # Aprox form: log10(digits) * log2(10)\n    if not isinstance(minimum, (int, float)):\n        raise TypeError('minimum can only be int or float')\n    if not isinstance(maximum, (int, float)):\n        raise TypeError('maximum can only be int or float')\n    if minimum < 0:\n        raise ValueError('minimum should be greater than 0')\n    if maximum < 0:\n        raise ValueError('maximum should be greater than 0')\n\n    dif = fabs(maximum - minimum)\n    if dif == 0:\n        return 0.0\n\n    ent = log10(dif) * 3.321928\n    return ent"}, {"instruction": "def _create_paths(self, basedir, name=None):\n        \"\"\"Create datadir and subdir paths.\"\"\"\n", "input": "", "output": "        if name:\n            datapath = os.path.join(basedir, name)\n        else:\n            datapath = basedir\n\n        dbpath = os.path.join(datapath, 'db')\n        if not os.path.exists(dbpath):\n            os.makedirs(dbpath)\n        if self.args['verbose']:\n            print('creating directory: %s' % dbpath)\n\n        return datapath"}, {"instruction": "def derivativeX(self,*args):\n        '''\n        Returns the derivative of the function with respect to the X dimension.\n        This is the first input whenever n_dims < 4 and the second input otherwise.\n        '''\n", "input": "", "output": "        if self.n_dims >= 4:\n            j = 1\n        else:\n            j = 0\n        if self.i_dim == j:\n            return np.ones_like(*args[0])\n        else:\n            return np.zeros_like(*args[0])"}, {"instruction": "def _create_request_map(cls, input_map):\n        \"\"\"Create request map.\"\"\"\n", "input": "", "output": "        mapped = super(Certificate, cls)._create_request_map(input_map)\n        if mapped.get('service') == CertificateType.developer:\n            mapped['service'] = CertificateType.bootstrap\n        return mapped"}, {"instruction": "def _common_parser():\n    \"\"\"Returns a parser with common command-line options for all the scripts\n    in the fortpy suite.\n    \"\"\"\n", "input": "", "output": "    import argparse\n    parser = argparse.ArgumentParser(add_help=False)\n    parser.add_argument(\"-examples\", action=\"store_true\",\n                        help=\"See detailed help and examples for this script.\")\n    parser.add_argument(\"-verbose\", action=\"store_true\",\n                        help=\"See verbose output as the script runs.\")\n    parser.add_argument('-action', nargs=1, choices=['save','print'], default='print',\n                        help=\"Specify what to do with the output (print or save)\")\n    parser.add_argument(\"-debug\", action=\"store_true\",\n                        help=\"Print verbose calculation information for debugging.\")\n\n    return parser"}, {"instruction": "def ind_nodes(self, graph=None):\n        \"\"\" Returns a list of all nodes in the graph with no dependencies. \"\"\"\n", "input": "", "output": "        if graph is None:\n            graph = self.graph\n\n        dependent_nodes = set(\n            node for dependents in six.itervalues(graph) for node in dependents\n        )\n        return [node for node in graph.keys() if node not in dependent_nodes]"}, {"instruction": "def get_stp_mst_detail_output_msti_port_rx_bpdu_count(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_stp_mst_detail = ET.Element(\"get_stp_mst_detail\")\n        config = get_stp_mst_detail\n        output = ET.SubElement(get_stp_mst_detail, \"output\")\n        msti = ET.SubElement(output, \"msti\")\n        instance_id_key = ET.SubElement(msti, \"instance-id\")\n        instance_id_key.text = kwargs.pop('instance_id')\n        port = ET.SubElement(msti, \"port\")\n        rx_bpdu_count = ET.SubElement(port, \"rx-bpdu-count\")\n        rx_bpdu_count.text = kwargs.pop('rx_bpdu_count')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def get_stp_brief_info_output_spanning_tree_info_spanning_tree_mode_pvstp_pvstp_port_interface_type(self, **kwargs):\n        \"\"\"Auto Generated Code\n        \"\"\"\n", "input": "", "output": "        config = ET.Element(\"config\")\n        get_stp_brief_info = ET.Element(\"get_stp_brief_info\")\n        config = get_stp_brief_info\n        output = ET.SubElement(get_stp_brief_info, \"output\")\n        spanning_tree_info = ET.SubElement(output, \"spanning-tree-info\")\n        spanning_tree_mode = ET.SubElement(spanning_tree_info, \"spanning-tree-mode\")\n        pvstp = ET.SubElement(spanning_tree_mode, \"pvstp\")\n        pvstp = ET.SubElement(pvstp, \"pvstp\")\n        vlan_id_key = ET.SubElement(pvstp, \"vlan-id\")\n        vlan_id_key.text = kwargs.pop('vlan_id')\n        port = ET.SubElement(pvstp, \"port\")\n        interface_type = ET.SubElement(port, \"interface-type\")\n        interface_type.text = kwargs.pop('interface_type')\n\n        callback = kwargs.pop('callback', self._callback)\n        return callback(config)"}, {"instruction": "def consume(self, limit=None):\n        \"\"\"Returns an iterator that waits for one message at a time.\"\"\"\n", "input": "", "output": "        for total_message_count in count():\n            if limit and total_message_count >= limit:\n                raise StopIteration\n\n            if not self.channel.is_open:\n                raise StopIteration\n\n            self.channel.wait()\n            yield True"}, {"instruction": "def callback_oauth2(self, request):\n        \"\"\"\n            Process for oAuth 2\n            :param request: contains the current session\n            :return:\n        \"\"\"\n", "input": "", "output": "        callback_url = self.callback_url(request)\n\n        oauth = OAuth2Session(client_id=self.consumer_key, redirect_uri=callback_url, scope=self.scope)\n        request_token = oauth.fetch_token(self.REQ_TOKEN,\n                                          code=request.GET.get('code', ''),\n                                          authorization_response=callback_url,\n                                          client_secret=self.consumer_secret,\n                                          scope=self.scope,\n                                          verify=False)\n        return request_token.get('access_token')"}, {"instruction": "def _dedent(text, tabsize=8, skip_first_line=False):\n    \"\"\"_dedent(text, tabsize=8, skip_first_line=False) -> dedented text\n\n        \"text\" is the text to dedent.\n        \"tabsize\" is the tab width to use for indent width calculations.\n        \"skip_first_line\" is a boolean indicating if the first line should\n            be skipped for calculating the indent width and for dedenting.\n            This is sometimes useful for docstrings and similar.\n\n    textwrap.dedent(s), but don't expand tabs to spaces\n    \"\"\"\n", "input": "", "output": "    lines = text.splitlines(1)\n    _dedentlines(lines, tabsize=tabsize, skip_first_line=skip_first_line)\n    return ''.join(lines)"}, {"instruction": "def fetch( hash ):\n    \"\"\"\n    Fetches CallDescriptor from the local database given a hash key representing the call. If it doesn't exist returns None.\n\n    :param str hash: The sha1 hexdigest to look the CallDescriptor up by.\n\n    :rtype: CallDescriptor corresponding to the hash passed or None if it wasn't found.\n    \"\"\"\n", "input": "", "output": "    res = select_io( hash )\n\n    if res:\n      p = { 'methodname': '', 'returnval': '', 'args': '', 'stack': '' }\n      for packet in res:\n        hash, stack, methodname, returnval, args, packet_num = packet\n        p['methodname'] = p['methodname'] + methodname\n        p['returnval']  = p['returnval'] + returnval\n        p['args']       = p['args'] + args\n        p['stack']      = p['stack'] + stack\n                             \n      return CallDescriptor( hash = hash,\n                             stack = p['stack'],\n                             method = p['methodname'],\n                             returnval = pickle.loads( str( p['returnval'] ) ),\n                             args = pickle.loads( str( p['args'] ) ) )\n    return None"}, {"instruction": "def get_splits(split_bed, gff_file, stype, key):\n    \"\"\"\n    Use intersectBed to find the fused gene => split genes mappings.\n    \"\"\"\n", "input": "", "output": "    bed_file = get_bed_file(gff_file, stype, key)\n    cmd = \"intersectBed -a {0} -b {1} -wao\".format(split_bed, bed_file)\n    cmd += \" | cut -f4,10\"\n    p = popen(cmd)\n    splits = defaultdict(set)\n    for row in p:\n        a, b = row.split()\n        splits[a].add(b)\n\n    return splits"}, {"instruction": "def id(self, value):\n        \"\"\"Split into server_and_prefix and identifier.\"\"\"\n", "input": "", "output": "        i = value.rfind('/')\n        if (i > 0):\n            self.server_and_prefix = value[:i]\n            self.identifier = value[(i + 1):]\n        elif (i == 0):\n            self.server_and_prefix = ''\n            self.identifier = value[(i + 1):]\n        else:\n            self.server_and_prefix = ''\n            self.identifier = value"}, {"instruction": "def pkg_blacklist(self):\n        \"\"\"Manage blacklist packages\n        \"\"\"\n", "input": "", "output": "        blacklist = BlackList()\n        options = [\n            \"-b\",\n            \"--blacklist\"\n        ]\n        flag = [\n            \"--add\",\n            \"--remove\"\n        ]\n        command = [\"list\"]\n        if (len(self.args) == 2 and self.args[0] in options and\n                self.args[1] == command[0]):\n            blacklist.listed()\n        elif (len(self.args) > 2 and self.args[0] in options and\n                flag[0] in self.args):\n            self.args.remove(flag[0])\n            blacklist.add(self.args[1:])\n        elif (len(self.args) == 3 and self.args[0] in options and\n                \"ALL\" in self.args and flag[1] in self.args):\n            self.args.remove(flag[1])\n            blacklist.remove(blacklist.get_black())\n        elif (len(self.args) > 2 and self.args[0] in options and\n                flag[1] in self.args):\n            self.args.remove(flag[1])\n            blacklist.remove(self.args[1:])\n        else:\n            usage(\"\")"}, {"instruction": "def do_gen(argdict):\n    '''Generate the whole site.'''\n", "input": "", "output": "    site = make_site_obj(argdict)\n    try:\n        st = time.time()\n        site.generate()\n        et = time.time()\n        print \"Generated Site in %f seconds.\"% (et-st)\n    except ValueError as e: # pragma: no cover\n        print \"Cannot generate. You are not within a simplystatic \\\ntree and you didn't specify a directory.\""}, {"instruction": "def _get_current_albedo(self):\n        '''Simple step-function albedo based on ice line at temperature Tf.'''\n", "input": "", "output": "        ice = self.subprocess['iceline'].ice\n        # noice = self.subprocess['iceline'].diagnostics['noice']\n        cold_albedo = self.subprocess['cold_albedo'].albedo\n        warm_albedo = self.subprocess['warm_albedo'].albedo\n        albedo = Field(np.where(ice, cold_albedo, warm_albedo), domain=self.domains['Ts'])\n        return albedo"}, {"instruction": "def parse_obj(obj):\n    \"\"\"\n    >>> parse_obj('bucket/key')\n    ('bucket', 'key')\n    >>> parse_obj('my-bucket/path/to/file.txt')\n    ('my-bucket', 'path/to/file.txt')\n    >>> parse_obj('s3://this_bucket/some/path.txt')\n    ('this_bucket', 'some/path.txt')\n    >>> parse_obj('https://s3.amazonaws.com/bucket/file.txt')\n    ('bucket', 'file.txt')\n    >>> parse_obj('http://the-bucket.s3.amazonaws.com/the/file.txt')\n    ('the-bucket', 'the/file.txt')\n    \"\"\"\n", "input": "", "output": "    obj = obj.lstrip('s3://')\n    if obj.startswith('http'):\n        url = urlparse.urlparse(obj)\n        if url.netloc == 's3.amazonaws.com':\n            path = url.path[1:]  # remove leading slash\n            bucket, key = path.split('/', 1)\n        else:\n            # bucket.s3.amazonaws.com form\n            bucket = url.netloc.split('.', 1)[0]\n            key = url.path[1:]\n    else:\n        bucket, key = obj.split('/', 1)\n    return bucket, key"}, {"instruction": "async def on_raw_422(self, message):\n        \"\"\" MOTD is missing. \"\"\"\n", "input": "", "output": "        await self._registration_completed(message)\n        self.motd = None\n        await self.on_connect()"}, {"instruction": "def compute_rewards(self, scores):\n        \"\"\"Compute the velocity of the best scores\n\n        The velocities are the k distances between the k+1 best scores.\n        \"\"\"\n", "input": "", "output": "        k = self.k\n        m = max(len(scores) - k, 0)\n        best_scores = sorted(scores)[-k - 1:]\n        velocities = np.diff(best_scores)\n        nans = np.full(m, np.nan)\n        return list(velocities) + list(nans)"}, {"instruction": "def xclaim(self, stream, group_name, consumer_name, min_idle_time,\n               id, *ids):\n        \"\"\"Claim a message for a given consumer\"\"\"\n", "input": "", "output": "        fut = self.execute(\n            b'XCLAIM', stream, group_name, consumer_name, min_idle_time,\n            id, *ids\n        )\n        return wait_convert(fut, parse_messages)"}, {"instruction": "def visitIgnoreDirective(self, ctx: jsgParser.IgnoreDirectiveContext):\n        \"\"\" directive: '.IGNORE' name* SEMI \"\"\"\n", "input": "", "output": "        for name in as_tokens(ctx.name()):\n            self._context.directives.append('_CONTEXT.IGNORE.append(\"{}\")'.format(name))"}, {"instruction": "def run(self, *args):\n        \"\"\"Add an identity to the registry.\"\"\"\n", "input": "", "output": "\n        params = self.parser.parse_args(args)\n\n        code = self.add(params.source, params.email, params.name, params.username,\n                        params.uuid, params.matching, params.interactive)\n\n        return code"}, {"instruction": "def to_satoshis(input_quantity, input_type):\n    ''' convert to satoshis, no rounding '''\n", "input": "", "output": "    assert input_type in UNIT_CHOICES, input_type\n\n    # convert to satoshis\n    if input_type in ('btc', 'mbtc', 'bit'):\n        satoshis = float(input_quantity) * float(UNIT_MAPPINGS[input_type]['satoshis_per'])\n    elif input_type == 'satoshi':\n        satoshis = input_quantity\n    else:\n        raise Exception('Invalid Unit Choice: %s' % input_type)\n\n    return int(satoshis)"}, {"instruction": "def _set(self, **kwargs):\n        \"\"\"\n        Sets user-supplied params.\n        \"\"\"\n", "input": "", "output": "        for param, value in kwargs.items():\n            p = getattr(self, param)\n            if value is not None:\n                try:\n                    value = p.typeConverter(value)\n                except TypeError as e:\n                    raise TypeError('Invalid param value given for param \"%s\". %s' % (p.name, e))\n            self._paramMap[p] = value\n        return self"}, {"instruction": "def new_worker(self, name: str):\n        \"\"\"Creates a new Worker and start a new Thread with it. Returns the Worker.\"\"\"\n", "input": "", "output": "        if not self.running:\n            return self.immediate_worker\n        worker = self._new_worker(name)\n        self._start_worker(worker)\n        return worker"}, {"instruction": "def add_slices(self, dashboard_id):\n        \"\"\"Add and save slices to a dashboard\"\"\"\n", "input": "", "output": "        data = json.loads(request.form.get('data'))\n        session = db.session()\n        Slice = models.Slice  # noqa\n        dash = (\n            session.query(models.Dashboard).filter_by(id=dashboard_id).first())\n        check_ownership(dash, raise_if_false=True)\n        new_slices = session.query(Slice).filter(\n            Slice.id.in_(data['slice_ids']))\n        dash.slices += new_slices\n        session.merge(dash)\n        session.commit()\n        session.close()\n        return 'SLICES ADDED'"}, {"instruction": "def get_notifier(provider_name: str, strict: bool = False) -> Provider:\n    \"\"\"\n    Convenience method to return an instantiated :class:`~notifiers.core.Provider` object according to it ``name``\n\n    :param provider_name: The ``name`` of the requested :class:`~notifiers.core.Provider`\n    :param strict: Raises a :class:`ValueError` if the given provider string was not found\n    :return: :class:`Provider` or None\n    :raises ValueError: In case ``strict`` is True and provider not found\n    \"\"\"\n", "input": "", "output": "    if provider_name in _all_providers:\n        log.debug(\"found a match for '%s', returning\", provider_name)\n        return _all_providers[provider_name]()\n    elif strict:\n        raise NoSuchNotifierError(name=provider_name)"}, {"instruction": "def _field_sort_name(cls, name):\n        \"\"\"Get a sort key for a field name that determines the order\n        fields should be written in.\n\n        Fields names are kept unchanged, unless they are instances of\n        :class:`DateItemField`, in which case `year`, `month`, and `day`\n        are replaced by `date0`, `date1`, and `date2`, respectively, to\n        make them appear in that order.\n        \"\"\"\n", "input": "", "output": "        if isinstance(cls.__dict__[name], DateItemField):\n            name = re.sub('year',  'date0', name)\n            name = re.sub('month', 'date1', name)\n            name = re.sub('day',   'date2', name)\n        return name"}, {"instruction": "def _writeConnectivity(self, links, fileObject):\n        \"\"\"\n        Write Connectivity Lines to File Method\n        \"\"\"\n", "input": "", "output": "        for link in links:\n            linkNum = link.linkNumber\n            downLink = link.downstreamLinkID\n            numUpLinks = link.numUpstreamLinks\n            upLinks = ''\n            for upLink in link.upstreamLinks:\n                upLinks = '{}{:>5}'.format(upLinks, str(upLink.upstreamLinkID))\n\n            line = 'CONNECT{:>5}{:>5}{:>5}{}\\n'.format(linkNum, downLink, numUpLinks, upLinks)\n            fileObject.write(line)\n        fileObject.write('\\n')"}, {"instruction": "def go_from(self, vertex):\n        \"\"\"\n        Tell the edge to go out from this vertex.\n\n        Args:\n            vertex (Vertex): vertex to go from.\n        \"\"\"\n", "input": "", "output": "        if self.vertex_out:\n            self.vertex_out.edges_out.remove(self)\n        self.vertex_out = vertex\n        vertex.edges_out.add(self)"}, {"instruction": "def delete_dataset(self, dataset_name):\n        \"\"\" Deletes the dataset having the specified name\n\n        :param dataset_name: the name that the dataset has on the repository\n        :return: None\n        \"\"\"\n", "input": "", "output": "        url = self.address + \"/datasets/\" + dataset_name\n        header = self.__check_authentication()\n        response = requests.delete(url, headers=header)\n        if response.status_code != 200:\n            raise ValueError(\"Code {}: {}\".format(response.status_code, response.json().get(\"error\")))\n        self.logger.debug(\"Dataset {} was deleted from the repository\".format(dataset_name))"}, {"instruction": "def _is_not_considered_falsey(value, ignore_types=()):\n    '''\n    Helper function for filter_falsey to determine if something is not to be\n    considered falsey.\n\n    :param any value: The value to consider\n    :param list ignore_types: The types to ignore when considering the value.\n\n    :return bool\n    '''\n", "input": "", "output": "    return isinstance(value, bool) or type(value) in ignore_types or value"}, {"instruction": "def get_link(self, path, method, callback, view):\n        \"\"\"\n        Return a `coreapi.Link` instance for the given endpoint.\n        \"\"\"\n", "input": "", "output": "        fields = self.get_path_fields(path, method, callback, view)\n        fields += self.get_serializer_fields(path, method, callback, view)\n        fields += self.get_pagination_fields(path, method, callback, view)\n        fields += self.get_filter_fields(path, method, callback, view)\n\n        if fields and any([field.location in ('form', 'body')\n                           for field in fields]):\n            encoding = self.get_encoding(path, method, callback, view)\n        else:\n            encoding = None\n\n        description = self.get_description(path, method, callback, view)\n\n        link = coreapi.Link(\n            url=urlparse.urljoin(self.url, path),\n            action=method.lower(),\n            encoding=encoding,\n            description=description,\n            fields=fields,\n            transform=None,  # Not handled, but here for future reference\n        )\n        link._responses = self.get_responses(path, method, callback, view)\n        link._produces = self.get_produces(path, method, callback, view)\n\n        return link"}, {"instruction": "def freeze_subjects(self):\n        \"\"\"Converts variable data into numpy arrays.\n\n        This is required after all subjects have been added via the\n        add_subject function, since we don't know ahead of time who is\n        participating in the analysis due to various filtering possibilities.\n        \"\"\"\n", "input": "", "output": "        self.phenotype_data = numpy.array(self.phenotype_data)\n        self.covariate_data = numpy.array(self.covariate_data)"}, {"instruction": "def serialize(self, outbuffer):\n\t\t\"\"\"Serialize this OmapiStartupMessage to the given outbuffer.\n\t\t@type outbuffer: OutBuffer\n\t\t\"\"\"\n", "input": "", "output": "\t\toutbuffer.add_net32int(self.protocol_version)\n\t\toutbuffer.add_net32int(self.header_size)"}, {"instruction": "def bic(self) -> str:\n        \"\"\"Generate random ``BIC`` (Bank ID Code).\n\n        :return: BIC.\n\n        :Example:\n            044025575.\n        \"\"\"\n", "input": "", "output": "        country_code = '04'\n        code = '{:02}'.format(self.random.randint(1, 10))\n        bank_number = '{:02}'.format(self.random.randint(0, 99))\n        bank_office = '{:03}'.format(self.random.randint(50, 999))\n        bic = country_code + code + bank_number + bank_office\n        return bic"}, {"instruction": "def _maxlength(X):\n    \"\"\" Returns the maximum length of signal trajectories X \"\"\"\n", "input": "", "output": "    N = 0\n    for x in X:\n        if len(x) > N:\n            N = len(x)\n    return N"}, {"instruction": "def tukey(winlen, alpha):\n    \"\"\"\n    Generate a tukey (tapered cosine) window\n    :param winlen: length of the window, in samples\n    :type winlen: int\n    :param alpha: proportion of the window to be tapered. \n    0 = rectangular window\n    1.0 = hann window\n    :type alpha: float\n    \"\"\"\n", "input": "", "output": "    taper = hann(winlen * alpha)\n    rect = np.ones(winlen - len(taper) + 1)\n    win = fftconvolve(taper, rect)\n    win = win / np.amax(win)\n    return win"}, {"instruction": "def leaveoneout(self):\n        \"\"\"Train & Test using leave one out\"\"\"\n", "input": "", "output": "        traintestfile = self.fileprefix + '.train'\n        options = \"-F \" + self.format + \" \" +  self.timbloptions + \" -t leave_one_out\"\n        if sys.version < '3':\n            self.api = timblapi.TimblAPI(b(options), b\"\")\n        else:\n            self.api = timblapi.TimblAPI(options, \"\")\n        if self.debug:\n            print(\"Enabling debug for timblapi\",file=stderr)\n            self.api.enableDebug()\n        print(\"Calling Timbl API : \" + options,file=stderr)\n        if sys.version < '3':\n            self.api.learn(b(traintestfile))\n            self.api.test(b(traintestfile), b(self.fileprefix + '.out'),b'')\n        else:\n            self.api.learn(u(traintestfile))\n            self.api.test(u(traintestfile), u(self.fileprefix + '.out'),'')\n        return self.api.getAccuracy()"}, {"instruction": "def op_decanonicalize(op_name, canonical_op):\n    \"\"\"\n    Get the current representation of a parsed operation's data, given the canonical representation\n    Meant for backwards-compatibility\n    \"\"\"\n", "input": "", "output": "    global DECANONICALIZE_METHODS\n\n    if op_name not in DECANONICALIZE_METHODS:\n        # no decanonicalization needed\n        return canonical_op\n    else:\n        return DECANONICALIZE_METHODS[op_name](canonical_op)"}, {"instruction": "def get_flat_models_from_models(models: Sequence[Type['main.BaseModel']]) -> Set[Type['main.BaseModel']]:\n    \"\"\"\n    Take a list of ``models`` and generate a set with them and all their sub-models in their trees. I.e. if you pass\n    a list of two models, ``Foo`` and ``Bar``, both subclasses of Pydantic ``BaseModel`` as models, and ``Bar`` has\n    a field of type ``Baz`` (also subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.\n    \"\"\"\n", "input": "", "output": "    flat_models: Set[Type['main.BaseModel']] = set()\n    for model in models:\n        flat_models |= get_flat_models_from_model(model)\n    return flat_models"}, {"instruction": "def toString(self):\n        \"\"\" Returns time as string. \"\"\"\n", "input": "", "output": "        slist = self.toList()\n        string = angle.slistStr(slist)\n        return string if slist[0] == '-' else string[1:]"}, {"instruction": "def get_scraperclasses():\n    \"\"\"Find all comic scraper classes in the plugins directory.\n    The result is cached.\n    @return: list of Scraper classes\n    @rtype: list of Scraper\n    \"\"\"\n", "input": "", "output": "    global _scraperclasses\n    if _scraperclasses is None:\n        out.debug(u\"Loading comic modules...\")\n        modules = loader.get_modules('plugins')\n        plugins = loader.get_plugins(modules, Scraper)\n        _scraperclasses = list(plugins)\n        check_scrapers()\n        out.debug(u\"... %d modules loaded.\" % len(_scraperclasses))\n    return _scraperclasses"}, {"instruction": "def get_polling_override(self):\n        \"\"\"Get the current polling override value in milliseconds. \n\n        See :meth:`set_polling_override` for more information. \n\n        Returns:\n            None on error, otherwise the current override period in milliseconds \n            (0 = disabled). \n        \"\"\"\n", "input": "", "output": "        polling_override = self.get_characteristic_handle_from_uuid(UUID_POLLING_OVERRIDE)\n        if polling_override is None:\n            logger.warn('Failed to find handle for polling override')\n            return None\n        override_ms = self.dongle._read_attribute(self.conn_handle, polling_override, True)\n        return None if override_ms is None else ord(override_ms)"}, {"instruction": "def copy(self):\n        \"\"\"\n        Returns a copy of the datamat.\n        \"\"\"\n", "input": "", "output": "        return self.filter(np.ones(self._num_fix).astype(bool))"}, {"instruction": "def likelihood_table_to_probs(self, lktable):\n        \"\"\"\n        Calculates this formula (1), given the log of the numerator as input\n                     \n                     p_k * f(x_i, a_k)\n        t_k(x_i) = -----------------------\n                    ---K\n                    \\   p_k * f(x_i, a_k)\n                    /__k=1\n        \n        x_i is data point i\n        P_k is cluster k of K\n        t_k is the posterior probability of x_i belonging to P_k\n        p_k is the prior probability of belong to P_k (the proportional size of P_k)\n        f(x, a) is the likelihood of x with parameters a\n        \"\"\"\n", "input": "", "output": "        m = lktable.max(1)  # row max of lktable\n        shifted = lktable-m[:,np.newaxis]  # shift lktable of log-likelihoods to a non-underflowing range\n        expsum = np.exp(shifted).sum(1)  # convert logs to (scaled) normal space, and sum the rows\n        logexpsum = np.log(expsum)+m  # convert back to log space, and undo the scaling\n        return np.exp(lktable - logexpsum[:, np.newaxis])"}, {"instruction": "def _get_error_response(self, exception):\n        \"\"\"\n        Trasform pyston exceptions to Is-core exceptions and raise it\n        \"\"\"\n", "input": "", "output": "        response_exceptions = {\n            MimerDataException: HTTPBadRequestResponseException,\n            NotAllowedException: HTTPForbiddenResponseException,\n            UnsupportedMediaTypeException: HTTPUnsupportedMediaTypeResponseException,\n            Http404: Http404,\n            ResourceNotFoundException: Http404,\n            NotAllowedMethodException: HTTPMethodNotAllowedResponseException,\n            DuplicateEntryException: HTTPDuplicateResponseException,\n            ConflictException: HTTPDuplicateResponseException,\n        }\n        response_exception = response_exceptions.get(type(exception))\n        if response_exception:\n            raise response_exception\n        return super(RESTResourceMixin, self)._get_error_response(exception)"}, {"instruction": "def require_prebuilt_dist(func):\n    \"\"\"Decorator for ToolchainCL methods. If present, the method will\n    automatically make sure a dist has been built before continuing\n    or, if no dists are present or can be obtained, will raise an\n    error.\n    \"\"\"\n", "input": "", "output": "\n    @wraps(func)\n    def wrapper_func(self, args):\n        ctx = self.ctx\n        ctx.set_archs(self._archs)\n        ctx.prepare_build_environment(user_sdk_dir=self.sdk_dir,\n                                      user_ndk_dir=self.ndk_dir,\n                                      user_android_api=self.android_api,\n                                      user_ndk_api=self.ndk_api)\n        dist = self._dist\n        if dist.needs_build:\n            if dist.folder_exists():  # possible if the dist is being replaced\n                dist.delete()\n            info_notify('No dist exists that meets your requirements, '\n                        'so one will be built.')\n            build_dist_from_args(ctx, dist, args)\n        func(self, args)\n    return wrapper_func"}, {"instruction": "def get_complex_and_node_state(self, hosts, services):\n        \"\"\"Get state , handle AND aggregation ::\n\n           * Get the worst state. 2 or max of sons (3 <=> UNKNOWN < CRITICAL <=> 2)\n           * Revert if it's a not node\n\n        :param hosts: host objects\n        :param services: service objects\n        :return: 0, 1 or 2\n        :rtype: int\n        \"\"\"\n", "input": "", "output": "        # First we get the state of all our sons\n        states = [s.get_state(hosts, services) for s in self.sons]\n        # Next we calculate the worst state\n        if 2 in states:\n            worst_state = 2\n        else:\n            worst_state = max(states)\n        # Then we handle eventual not value\n        if self.not_value:\n            return self.get_reverse_state(worst_state)\n        return worst_state"}, {"instruction": "def _prepare_sample_data(self, submission_type):\n    \"\"\"Prepares sample data for the submission.\n\n    Args:\n      submission_type: type of the submission.\n    \"\"\"\n", "input": "", "output": "    # write images\n    images = np.random.randint(0, 256,\n                               size=[BATCH_SIZE, 299, 299, 3], dtype=np.uint8)\n    for i in range(BATCH_SIZE):\n      Image.fromarray(images[i, :, :, :]).save(\n          os.path.join(self._sample_input_dir, IMAGE_NAME_PATTERN.format(i)))\n    # write target class for targeted attacks\n    if submission_type == 'targeted_attack':\n      target_classes = np.random.randint(1, 1001, size=[BATCH_SIZE])\n      target_class_filename = os.path.join(self._sample_input_dir,\n                                           'target_class.csv')\n      with open(target_class_filename, 'w') as f:\n        for i in range(BATCH_SIZE):\n          f.write((IMAGE_NAME_PATTERN + ',{1}\\n').format(i, target_classes[i]))"}, {"instruction": "def _GetFileSystemCacheIdentifier(self, path_spec):\n    \"\"\"Determines the file system cache identifier for the path specification.\n\n    Args:\n      path_spec (PathSpec): path specification.\n\n    Returns:\n      str: identifier of the VFS object.\n    \"\"\"\n", "input": "", "output": "    string_parts = []\n\n    string_parts.append(getattr(path_spec.parent, 'comparable', ''))\n    string_parts.append('type: {0:s}'.format(path_spec.type_indicator))\n\n    return ''.join(string_parts)"}, {"instruction": "def get_credentials():\n    \"\"\"Gets valid user credentials from storage.\n\n    If nothing has been stored, or if the stored credentials are invalid,\n    the OAuth2 flow is completed to obtain the new credentials.\n\n    Returns:\n        Credentials, the obtained credential.\n    \"\"\"\n", "input": "", "output": "    home_dir = os.path.expanduser('~')\n    credential_dir = os.path.join(home_dir, '.credentials')\n    if not os.path.exists(credential_dir):\n        os.makedirs(credential_dir)\n    credential_path = os.path.join(credential_dir,\n                                   'calendar-python-quickstart.json')\n\n    store = Storage(credential_path)\n    credentials = store.get()\n    if not credentials or credentials.invalid:\n        flow = client.flow_from_clientsecrets(CLIENT_SECRET_FILE, SCOPES)\n        flow.user_agent = APPLICATION_NAME\n        if flags:\n            credentials = tools.run_flow(flow, store, flags)\n        else: # Needed only for compatibility with Python 2.6\n            credentials = tools.run(flow, store)\n        print('Storing credentials to ' + credential_path)\n    return credentials"}, {"instruction": "def store_integers(items, allow_zero=True):\n    \"\"\"Store integers from the given list in a storage.\n\n    This is an example function to show autodoc style.\n\n    Return :class:`Storage` instance with integers from the given list.\n\n    Examples::\n\n        >>> storage = store_integers([1, 'foo', 2, 'bar', 0])\n        >>> storage.items\n        [1, 2, 0]\n        >>> storage = store_integers([1, 'foo', 2, 'bar', 0], allow_zero=False)\n        >>> storage.items\n        [1, 2]\n\n    :param items:\n        List of objects of any type, only :class:`int` instances will be\n        stored.\n    :param allow_zero:\n        Boolean -- if ``False``, ``0`` integers will be skipped.\n        Defaults to ``True``.\n\n    \"\"\"\n", "input": "", "output": "    ints = [x for x in items if isinstance(x, int) and (allow_zero or x != 0)]\n    storage = Storage(ints)\n    return storage"}, {"instruction": "def _bubbleP(cls, T):\n        \"\"\"Using ancillary equation return the pressure of bubble point\"\"\"\n", "input": "", "output": "        c = cls._blend[\"bubble\"]\n        Tj = cls._blend[\"Tj\"]\n        Pj = cls._blend[\"Pj\"]\n        Tita = 1-T/Tj\n\n        suma = 0\n        for i, n in zip(c[\"i\"], c[\"n\"]):\n            suma += n*Tita**(i/2.)\n        P = Pj*exp(Tj/T*suma)\n        return P"}, {"instruction": "def get_newest(blocks, layout_blocks):\n    \"\"\"Filter out old layout blocks from list\n\n    Arguments:\n    List:blocks        -- List of block objects\n    List:layout_blocks -- List of layout block indexes\n\n    Returns:\n    List -- Newest layout blocks in list\n    \"\"\"\n", "input": "", "output": "    layout_temp = list(layout_blocks)\n\n    for i in range(0, len(layout_temp)):\n        for k in range(0, len(layout_blocks)):\n            if blocks[layout_temp[i]].ec_hdr.image_seq != blocks[layout_blocks[k]].ec_hdr.image_seq:\n                continue\n\n            if blocks[layout_temp[i]].leb_num != blocks[layout_blocks[k]].leb_num:\n                continue\n\n            if blocks[layout_temp[i]].vid_hdr.sqnum > blocks[layout_blocks[k]].vid_hdr.sqnum:\n                del layout_blocks[k]\n                break\n\n    return layout_blocks"}, {"instruction": "def approx_equals(self, other, atol):\n        \"\"\"Return ``True`` in case of approximate equality.\n\n        Returns\n        -------\n        approx_eq : bool\n            ``True`` if ``other`` is a `RectPartition` instance with\n            ``self.set == other.set`` up to ``atol`` and\n            ``self.grid == other.other`` up to ``atol``, ``False`` otherwise.\n        \"\"\"\n", "input": "", "output": "        if other is self:\n            return True\n        elif not isinstance(other, RectPartition):\n            return False\n        else:\n            return (self.set.approx_equals(other.set, atol=atol) and\n                    self.grid.approx_equals(other.grid, atol=atol))"}, {"instruction": "async def get_sleep_timer_settings(self) -> List[Setting]:\n        \"\"\"Get sleep timer settings.\"\"\"\n", "input": "", "output": "        return [\n            Setting.make(**x)\n            for x in await self.services[\"system\"][\"getSleepTimerSettings\"]({})\n        ]"}, {"instruction": "def build_params(self, params, i):\n        \"\"\"\n        Populates a dictionary with the name/value pairs necessary\n        to identify this Tag in a request.\n        \"\"\"\n", "input": "", "output": "        prefix = 'Tags.member.%d.' % i\n        params[prefix+'ResourceId'] = self.resource_id\n        params[prefix+'ResourceType'] = self.resource_type\n        params[prefix+'Key'] = self.key\n        params[prefix+'Value'] = self.value\n        if self.propagate_at_launch:\n            params[prefix+'PropagateAtLaunch'] = 'true'\n        else:\n            params[prefix+'PropagateAtLaunch'] = 'false'"}, {"instruction": "def set_timezone(self, request, org):\n        \"\"\"Set the current timezone from the org configuration.\"\"\"\n", "input": "", "output": "        if org and org.timezone:\n            timezone.activate(org.timezone)"}, {"instruction": "def send(self, data, **kws):\r\n        \"\"\"Send data to the socket. The socket must be connected to a remote\r\n        socket. Ammount sent may be less than the data provided.\"\"\"\n", "input": "", "output": "        return yield_(Send(self, data, timeout=self._timeout, **kws))"}, {"instruction": "def send(\n        self, record, message, resource=None, labels=None, trace=None, span_id=None\n    ):\n        \"\"\"Overrides transport.send().\n\n        :type record: :class:`logging.LogRecord`\n        :param record: Python log record that the handler was called with.\n\n        :type message: str\n        :param message: The message from the ``LogRecord`` after being\n                        formatted by the associated log formatters.\n\n        :type resource: :class:`~google.cloud.logging.resource.Resource`\n        :param resource: (Optional) Monitored resource of the entry.\n\n        :type labels: dict\n        :param labels: (Optional) Mapping of labels for the entry.\n        \"\"\"\n", "input": "", "output": "        info = {\"message\": message, \"python_logger\": record.name}\n        self.logger.log_struct(\n            info,\n            severity=record.levelname,\n            resource=resource,\n            labels=labels,\n            trace=trace,\n            span_id=span_id,\n        )"}, {"instruction": "def pprint(obj, *args, **kwargs):\n    \"\"\"Pretty-printing function that can pretty-print OrderedDicts\n    like regular dictionaries. Useful for printing the output of\n    :meth:`marshmallow.Schema.dump`.\n    \"\"\"\n", "input": "", "output": "    if isinstance(obj, collections.OrderedDict):\n        print(json.dumps(obj, *args, **kwargs))\n    else:\n        py_pprint(obj, *args, **kwargs)"}, {"instruction": "def get_field_list(fields, schema):\n  \"\"\" Convert a field list spec into a real list of field names.\n\n      For tables, we return only the top-level non-RECORD fields as Google charts\n      can't handle nested data.\n  \"\"\"\n", "input": "", "output": "  # If the fields weren't supplied get them from the schema.\n  if schema:\n    all_fields = [f['name'] for f in schema._bq_schema if f['type'] != 'RECORD']\n\n  if isinstance(fields, list):\n    if schema:\n      # validate fields exist\n      for f in fields:\n        if f not in all_fields:\n          raise Exception('Cannot find field %s in given schema' % f)\n    return fields\n  if isinstance(fields, basestring) and fields != '*':\n    if schema:\n      # validate fields exist\n      for f in fields.split(','):\n        if f not in all_fields:\n          raise Exception('Cannot find field %s in given schema' % f)\n      return fields.split(',')\n  if not schema:\n    return []\n  return all_fields"}, {"instruction": "def _R2deriv(self,R,phi=0.,t=0.):\n        \"\"\"\n        NAME:\n           _R2deriv\n        PURPOSE:\n           evaluate the second radial derivative\n        INPUT:\n           R\n           phi\n           t\n        OUTPUT:\n           d2phi/dR2\n        HISTORY:\n           2016-06-02 - Written - Bovy (UofT)\n        \"\"\"\n", "input": "", "output": "        return self._Pot.R2deriv(R,0.,phi=phi,t=t,use_physical=False)"}, {"instruction": "def setPrefilter(self, edfsignal, prefilter):\n        \"\"\"\n        Sets the prefilter of signal edfsignal (\"HP:0.1Hz\", \"LP:75Hz N:50Hz\", etc.)\n\n        :param edfsignal: int\n        :param prefilter: str\n\n        Notes\n        -----\n        This function is optional for every signal and can be called only after opening a file in writemode and before the first sample write action.\n        \"\"\"\n", "input": "", "output": "        if edfsignal < 0 or edfsignal > self.n_channels:\n            raise ChannelDoesNotExist(edfsignal)\n        self.channels[edfsignal]['prefilter'] = prefilter\n        self.update_header()"}, {"instruction": "def from_bytes(self, string):\n        \"\"\"Deserialize the binder's annotations from a byte string.\"\"\"\n", "input": "", "output": "        msg = srsly.msgpack_loads(gzip.decompress(string))\n        self.attrs = msg[\"attrs\"]\n        self.strings = set(msg[\"strings\"])\n        lengths = numpy.fromstring(msg[\"lengths\"], dtype=\"int32\")\n        flat_spaces = numpy.fromstring(msg[\"spaces\"], dtype=bool)\n        flat_tokens = numpy.fromstring(msg[\"tokens\"], dtype=\"uint64\")\n        shape = (flat_tokens.size // len(self.attrs), len(self.attrs))\n        flat_tokens = flat_tokens.reshape(shape)\n        flat_spaces = flat_spaces.reshape((flat_spaces.size, 1))\n        self.tokens = NumpyOps().unflatten(flat_tokens, lengths)\n        self.spaces = NumpyOps().unflatten(flat_spaces, lengths)\n        for tokens in self.tokens:\n            assert len(tokens.shape) == 2, tokens.shape\n        return self"}, {"instruction": "def chk_goids(goids, msg=None, raise_except=True):\n    \"\"\"check that all GO IDs have the proper format.\"\"\"\n", "input": "", "output": "    for goid in goids:\n        if not goid_is_valid(goid):\n            if raise_except:\n                raise RuntimeError(\"BAD GO({GO}): {MSG}\".format(GO=goid, MSG=msg))\n            else:\n                return goid"}, {"instruction": "def _route(self):\n        ''' Handles server route instantiation. '''\n", "input": "", "output": "        self._app.route('/',\n                        method='GET',\n                        callback=self._get_logger_list)\n        self._app.route('/stats',\n                        method='GET',\n                        callback=self._fetch_handler_stats)\n        self._app.route('/<name>/start',\n                        method='POST',\n                        callback=self._add_logger_by_name)\n        self._app.route('/<name>/stop',\n                        method='DELETE',\n                        callback=self._stop_logger_by_name)\n        self._app.route('/<name>/config',\n                        method='GET',\n                        callback=self._get_logger_conf)\n        self._app.route('/<name>/rotate',\n                        method='POST',\n                        callback=self._rotate_capturer_log)"}, {"instruction": "def make_request(parameters):\n    \"\"\"Submit a getfeature request to DataBC WFS and return features\n    \"\"\"\n", "input": "", "output": "    r = requests.get(bcdata.WFS_URL, params=parameters)\n    return r.json()[\"features\"]"}, {"instruction": "def relative_ref(*parts):\n    \"\"\"\n    Create a reference builder with specified relative location.\n    using getter.relative_ref(\"some\", \"submodel\") to get a value with key\n    \"toto\" will gives reference.Relative(\"some\", \"submodel\", \"toto\")\n    \"\"\"\n", "input": "", "output": "\n    def relative_ref(_value, context, **_params):\n        return reference.Relative(*(parts + (context[\"key\"], )))\n\n    return relative_ref"}, {"instruction": "def indexTupleFromItem(self, treeItem): # TODO: move to BaseTreeItem?\n        \"\"\" Return (first column model index, last column model index) tuple for a configTreeItem\n        \"\"\"\n", "input": "", "output": "        if not treeItem:\n            return (QtCore.QModelIndex(), QtCore.QModelIndex())\n\n        if not treeItem.parentItem: # TODO: only necessary because of childNumber?\n            return (QtCore.QModelIndex(), QtCore.QModelIndex())\n\n        # Is there a bug in Qt in QStandardItemModel::indexFromItem?\n        # It passes the parent in createIndex. TODO: investigate\n\n        row =  treeItem.childNumber()\n        return (self.createIndex(row, 0, treeItem),\n                self.createIndex(row, self.columnCount() - 1, treeItem))"}, {"instruction": "def _find_datastream(self, name):\n        \"\"\"Find and return if a datastream exists, by name.\"\"\"\n", "input": "", "output": "        for stream in self.data_streams: #search to see if this is a new datastream or a known one\n            if stream.name == name:\n                return stream\n        return None"}, {"instruction": "def require_representation(self, req):\n        \"\"\"Require raw representation dictionary from falcon request object.\n\n        This does not perform any field parsing or validation but only uses\n        allowed content-encoding handler to decode content body.\n\n        Note:\n            Currently only JSON is allowed as content type.\n\n        Args:\n            req (falcon.Request): request object\n\n        Returns:\n            dict: raw dictionary of representation supplied in request body\n\n        \"\"\"\n", "input": "", "output": "        try:\n            type_, subtype, _ = parse_mime_type(req.content_type)\n            content_type = '/'.join((type_, subtype))\n        except:\n            raise falcon.HTTPUnsupportedMediaType(\n                description=\"Invalid Content-Type header: {}\".format(\n                    req.content_type\n                )\n            )\n\n        if content_type == 'application/json':\n            body = req.stream.read()\n            return json.loads(body.decode('utf-8'))\n        else:\n            raise falcon.HTTPUnsupportedMediaType(\n                description=\"only JSON supported, got: {}\".format(content_type)\n            )"}, {"instruction": "<b>getSDbl</b><br>\npublic static String getSDbl( double Value, int DecPrec )<br>\nReturns double converted to string.<br>\nIf Value is Double.NaN returns \"NaN\".<br>\nIf DecPrec is < 0 is DecPrec set 0.<br>\n\n@param Value - value\n@param DecPrec - decimal precision\n@return double as string", "input": "Java", "output": "public static String getSDbl( double Value, int DecPrec ) {\n\t\t//\n\t\tString Result = \"\";\n\t\t//\n\t\tif ( Double.isNaN( Value ) ) return \"NaN\";\n\t\t//\n\t\tif ( DecPrec < 0 ) DecPrec = 0;\n\t\t//\n\t\tString DFS = \"###,###,##0\";\n\t\t//\n\t\tif ( DecPrec > 0 ) {\n\t\t\tint idx = 0;\n\t\t\tDFS += \".\";\n\t\t\twhile ( idx < DecPrec ) {\n\t\t\t\tDFS = DFS + \"0\";\n\t\t\t\tidx ++;\n\t\t\t\tif ( idx > 100 ) break;\n\t\t\t}\n\t\t}\n\t\t//\n//\t\tLocale locale  = new Locale(\"en\", \"UK\");\n\t\t//\n\t\tDecimalFormatSymbols DcmFrmSmb = new DecimalFormatSymbols( Locale.getDefault());\n\t\tDcmFrmSmb.setDecimalSeparator('.');\n\t\tDcmFrmSmb.setGroupingSeparator(' ');\n\t\t//\n\t\tDecimalFormat DcmFrm;\n\t\t//\n\t\tDcmFrm = new DecimalFormat( DFS, DcmFrmSmb );\n\t\t//\n\t//\tDcmFrm.setGroupingSize( 3 );\n\t\t//\n\t\tResult = DcmFrm.format( Value );\n\t\t//\n\t\treturn Result;\n\t}"}, {"instruction": "Build metadata resolver aggregate. Loops through metadata resources\nand attempts to resolve the metadata.\n\n@param entityId the entity id", "input": "Java", "output": "@Synchronized\n    @SneakyThrows\n    public void buildMetadataResolverAggregate(final String entityId) {\n        LOGGER.trace(\"Building metadata resolver aggregate\");\n        this.metadataResolver = new ChainingMetadataResolver();\n        val resolvers = new ArrayList<MetadataResolver>();\n        val entries = this.metadataResources.entrySet();\n        entries.forEach(entry -> {\n            val resource = entry.getKey();\n            LOGGER.debug(\"Loading [{}]\", resource.getFilename());\n            resolvers.addAll(loadMetadataFromResource(entry.getValue(), resource, entityId));\n        });\n        this.metadataResolver.setId(ChainingMetadataResolver.class.getCanonicalName());\n        this.metadataResolver.setResolvers(resolvers);\n        LOGGER.debug(\"Collected metadata from [{}] resolvers(s). Initializing aggregate resolver...\", resolvers.size());\n        this.metadataResolver.initialize();\n        LOGGER.info(\"Metadata aggregate initialized successfully.\");\n    }"}, {"instruction": "\u89e3\u538b\u7f29", "input": "Java", "output": "public void unSimplify()\n    {\n        if (sourceNode == null)\n        {\n            sourceNode = new MDAGNode(false);\n            equivalenceClassMDAGNodeHashMap = new HashMap<MDAGNode, MDAGNode>();\n            MDAGNode[] toNodeArray = new MDAGNode[mdagDataArray.length];\n            createMDAGNode(simplifiedSourceNode, -1, toNodeArray, new MDAGNode[mdagDataArray.length]);\n            // \u6784\u5efa\u6ce8\u518c\u8868\n            for (MDAGNode mdagNode : toNodeArray)\n            {\n                equivalenceClassMDAGNodeHashMap.put(mdagNode, mdagNode);\n            }\n            // \u6254\u6389\u5783\u573e\n            simplifiedSourceNode = null;\n        }\n    }"}, {"instruction": "Checks if the constant pool contains a reference to a given field, either for writing or reading.\n\n@param className must be provided JVM-style, such as {@code java/lang/String}", "input": "Java", "output": "public boolean usesField(String className, String fieldName) {\n\t\tint classIndex = findClass(className);\n\t\tif (classIndex == NOT_FOUND) return false;\n\t\tint fieldNameIndex = findUtf8(fieldName);\n\t\tif (fieldNameIndex == NOT_FOUND) return false;\n\t\t\n\t\tfor (int i = 1; i < maxPoolSize; i++) {\n\t\t\tif (types[i] == FIELD && readValue(offsets[i]) == classIndex) {\n\t\t\t\tint nameAndTypeIndex = readValue(offsets[i] + 2);\n\t\t\t\tif (readValue(offsets[nameAndTypeIndex]) == fieldNameIndex) return true;\n\t\t\t}\n\t\t}\n\t\treturn false;\n\t}"}, {"instruction": "Helper method for getDownstreamRelationship.\n\nFor each given build, find the build number range of the given project and put that into the map.", "input": "Java", "output": "private void checkAndRecord(AbstractProject that, TreeMap<Integer, RangeSet> r, Collection<R> builds) {\n        for (R build : builds) {\n            RangeSet rs = build.getDownstreamRelationship(that);\n            if(rs==null || rs.isEmpty())\n                continue;\n\n            int n = build.getNumber();\n\n            RangeSet value = r.get(n);\n            if(value==null)\n                r.put(n,rs);\n            else\n                value.add(rs);\n        }\n    }"}, {"instruction": "Write an unserialized message with a known length, uncompressed.", "input": "Java", "output": "private int writeKnownLengthUncompressed(InputStream message, int messageLength)\n      throws IOException {\n    if (maxOutboundMessageSize >= 0 && messageLength > maxOutboundMessageSize) {\n      throw Status.RESOURCE_EXHAUSTED\n          .withDescription(\n              String.format(\"message too large %d > %d\", messageLength , maxOutboundMessageSize))\n          .asRuntimeException();\n    }\n    ByteBuffer header = ByteBuffer.wrap(headerScratch);\n    header.put(UNCOMPRESSED);\n    header.putInt(messageLength);\n    // Allocate the initial buffer chunk based on frame header + payload length.\n    // Note that the allocator may allocate a buffer larger or smaller than this length\n    if (buffer == null) {\n      buffer = bufferAllocator.allocate(header.position() + messageLength);\n    }\n    writeRaw(headerScratch, 0, header.position());\n    return writeToOutputStream(message, outputStreamAdapter);\n  }"}, {"instruction": "Authenticate ecp request.\n\n@param credential   the credential\n@param authnRequest the authn request\n@return the authentication", "input": "Java", "output": "protected Authentication authenticateEcpRequest(final Credential credential,\n                                                    final Pair<AuthnRequest, MessageContext> authnRequest) {\n        val issuer = SamlIdPUtils.getIssuerFromSamlObject(authnRequest.getKey());\n        LOGGER.debug(\"Located issuer [{}] from request prior to authenticating [{}]\", issuer, credential.getId());\n\n        val service = getSamlProfileHandlerConfigurationContext().getWebApplicationServiceFactory().createService(issuer);\n        LOGGER.debug(\"Executing authentication request for service [{}] on behalf of credential id [{}]\", service, credential.getId());\n        val authenticationResult = getSamlProfileHandlerConfigurationContext()\n            .getAuthenticationSystemSupport().handleAndFinalizeSingleAuthenticationTransaction(service, credential);\n        return authenticationResult.getAuthentication();\n    }"}, {"instruction": "Gets declared method from specified type by mame and parameters types.\n\n@param type           the type\n@param methodName     the name of the method\n@param parameterTypes the parameter array\n@return a {@link Method} object or null if method doesn't exist", "input": "Java", "output": "public static Method getDeclaredMethod(Class<?> type, String methodName, Class<?>... parameterTypes) {\n        Method method = null;\n        try {\n            method = type.getDeclaredMethod(methodName, parameterTypes);\n            if(method.isBridge()){\n                method = MethodProvider.getInstance().unbride(method, type);\n            }\n        } catch (NoSuchMethodException e) {\n            Class<?> superclass = type.getSuperclass();\n            if (superclass != null) {\n                method = getDeclaredMethod(superclass, methodName, parameterTypes);\n            }\n        } catch (ClassNotFoundException e) {\n            Throwables.propagate(e);\n        } catch (IOException e) {\n            Throwables.propagate(e);\n        }\n        return method;\n    }"}, {"instruction": "Encodes {@code value}.\n@param value a {@link JSONObject}, {@link JSONArray}, String, Boolean, Integer,\nLong, Double or null. May not be {@link Double#isNaN() NaNs} or\n{@link Double#isInfinite() infinities}.\n@return this stringer.\n@throws JSONException if processing of json failed", "input": "Java", "output": "public JSONStringer value(Object value) throws JSONException {\n\t\tif (this.stack.isEmpty()) {\n\t\t\tthrow new JSONException(\"Nesting problem\");\n\t\t}\n\n\t\tif (value instanceof JSONArray) {\n\t\t\t((JSONArray) value).writeTo(this);\n\t\t\treturn this;\n\n\t\t}\n\t\telse if (value instanceof JSONObject) {\n\t\t\t((JSONObject) value).writeTo(this);\n\t\t\treturn this;\n\t\t}\n\n\t\tbeforeValue();\n\n\t\tif (value == null || value instanceof Boolean || value == JSONObject.NULL) {\n\t\t\tthis.out.append(value);\n\n\t\t}\n\t\telse if (value instanceof Number) {\n\t\t\tthis.out.append(JSONObject.numberToString((Number) value));\n\n\t\t}\n\t\telse {\n\t\t\tstring(value.toString());\n\t\t}\n\n\t\treturn this;\n\t}"}, {"instruction": "Create registered service public key defined.\n\n@param registeredService the registered service\n@return the public key", "input": "Java", "output": "private static PublicKey createRegisteredServicePublicKey(final RegisteredService registeredService) {\n        if (registeredService.getPublicKey() == null) {\n            LOGGER.debug(\"No public key is defined for service [{}]. No encoding will take place.\", registeredService);\n            return null;\n        }\n        val publicKey = registeredService.getPublicKey().createInstance();\n        if (publicKey == null) {\n            LOGGER.debug(\"No public key instance created for service [{}]. No encoding will take place.\", registeredService);\n            return null;\n        }\n        return publicKey;\n    }"}, {"instruction": "Often an archive contains an extra top-level directory that's unnecessary when extracted on the disk\ninto the expected location. If your installation sources provide that kind of archives, override\nthis method to find the real root location.\n\n<p>\nThe caller will \"pull up\" the discovered real root by throw away the intermediate directory,\nso that the user-configured \"tool home\" directory contains the right files.\n\n<p>\nThe default implementation applies some heuristics to auto-determine if the pull up is necessary.\nThis should work for typical archive files.\n\n@param root\nThe directory that contains the extracted archive. This directory contains nothing but the\nextracted archive. For example, if the user installed\nhttp://archive.apache.org/dist/ant/binaries/jakarta-ant-1.1.zip , this directory would contain\na single directory \"jakarta-ant\".\n\n@return\nReturn the real top directory inside {@code root} that contains the meat. In the above example,\n{@code root.child(\"jakarta-ant\")} should be returned. If there's no directory to pull up,\nreturn null.", "input": "Java", "output": "protected FilePath findPullUpDirectory(FilePath root) throws IOException, InterruptedException {\n        // if the directory just contains one directory and that alone, assume that's the pull up subject\n        // otherwise leave it as is.\n        List<FilePath> children = root.list();\n        if(children.size()!=1)    return null;\n        if(children.get(0).isDirectory())\n            return children.get(0);\n        return null;\n    }"}, {"instruction": "Text coming from an input stream considered as one document\n\n@param is    the input stream to read from\n@param label the label to assign\n@return a dataset with a applyTransformToDestination of weights(relative to impl; could be word counts or tfidf scores)", "input": "Java", "output": "@Override\n    public DataSet vectorize(InputStream is, String label) {\n        try {\n            BufferedReader reader = new BufferedReader(new InputStreamReader(is, \"UTF-8\"));\n            String line = \"\";\n            StringBuilder builder = new StringBuilder();\n            while ((line = reader.readLine()) != null) {\n                builder.append(line);\n            }\n            return vectorize(builder.toString(), label);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }"}, {"instruction": "Determines if the candidate should be accepted into the main space, as determined by its\nfrequency relative to the victim. A small amount of randomness is used to protect against hash\ncollision attacks, where the victim's frequency is artificially raised so that no new entries\nare admitted.\n\n@param candidateKey the key for the entry being proposed for long term retention\n@param victimKey the key for the entry chosen by the eviction policy for replacement\n@return if the candidate should be admitted and the victim ejected", "input": "Java", "output": "@GuardedBy(\"evictionLock\")\n  boolean admit(K candidateKey, K victimKey) {\n    int victimFreq = frequencySketch().frequency(victimKey);\n    int candidateFreq = frequencySketch().frequency(candidateKey);\n    if (candidateFreq > victimFreq) {\n      return true;\n    } else if (candidateFreq <= 5) {\n      // The maximum frequency is 15 and halved to 7 after a reset to age the history. An attack\n      // exploits that a hot candidate is rejected in favor of a hot victim. The threshold of a warm\n      // candidate reduces the number of random acceptances to minimize the impact on the hit rate.\n      return false;\n    }\n    int random = ThreadLocalRandom.current().nextInt();\n    return ((random & 127) == 0);\n  }"}, {"instruction": "Converts a Flink application status enum to a YARN application status enum.\n@param status The Flink application status.\n@return The corresponding YARN application status.", "input": "Java", "output": "private FinalApplicationStatus getYarnStatus(ApplicationStatus status) {\n\t\tif (status == null) {\n\t\t\treturn FinalApplicationStatus.UNDEFINED;\n\t\t}\n\t\telse {\n\t\t\tswitch (status) {\n\t\t\t\tcase SUCCEEDED:\n\t\t\t\t\treturn FinalApplicationStatus.SUCCEEDED;\n\t\t\t\tcase FAILED:\n\t\t\t\t\treturn FinalApplicationStatus.FAILED;\n\t\t\t\tcase CANCELED:\n\t\t\t\t\treturn FinalApplicationStatus.KILLED;\n\t\t\t\tdefault:\n\t\t\t\t\treturn FinalApplicationStatus.UNDEFINED;\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "\u8fd4\u56de\u65e0\u5e8f\u96c6\u5408\u4e2d\u7684\u6700\u5c0f\u503c\u548c\u6700\u5927\u503c\n\n\u5728\u8fd4\u56de\u7684Pair\u4e2d\uff0c\u7b2c\u4e00\u4e2a\u4e3a\u6700\u5c0f\u503c\uff0c\u7b2c\u4e8c\u4e2a\u4e3a\u6700\u5927\u503c", "input": "Java", "output": "public static <T> Pair<T, T> minAndMax(Collection<? extends T> coll, Comparator<? super T> comp) {\n\n\t\tIterator<? extends T> i = coll.iterator();\n\t\tT minCandidate = i.next();\n\t\tT maxCandidate = minCandidate;\n\n\t\twhile (i.hasNext()) {\n\t\t\tT next = i.next();\n\t\t\tif (comp.compare(next, minCandidate) < 0) {\n\t\t\t\tminCandidate = next;\n\t\t\t} else if (comp.compare(next, maxCandidate) > 0) {\n\t\t\t\tmaxCandidate = next;\n\t\t\t}\n\t\t}\n\n\t\treturn Pair.of(minCandidate, maxCandidate);\n\t}"}, {"instruction": "\u901a\u8fc7\u54cd\u5e94\u900f\u4f20\u6570\u636e\n\n@param context  RpcInvokeContext\n@param response \u54cd\u5e94", "input": "Java", "output": "public static void carryWithResponse(RpcInvokeContext context, SofaResponse response) {\n        if (context != null) {\n            Map<String, String> responseBaggage = context.getAllResponseBaggage();\n            if (CommonUtils.isNotEmpty(responseBaggage)) {\n                String prefix = RemotingConstants.RPC_RESPONSE_BAGGAGE + \".\";\n                for (Map.Entry<String, String> entry : responseBaggage.entrySet()) {\n                    response.addResponseProp(prefix + entry.getKey(), entry.getValue());\n                }\n            }\n        }\n    }"}, {"instruction": "Gets the action (first instance to be found) of a specified type that contributed to this build.\n\n@param type\n@return The action or <code>null</code> if no such actions exist.\n@see #getActions(Class)", "input": "Java", "output": "public <T extends Action> T getAction(Class<T> type) {\n        // Shortcut: if the persisted list has one, return it.\n        for (Action a : getActions()) {\n            if (type.isInstance(a)) {\n                return type.cast(a);\n            }\n        }\n        // Otherwise check transient factories.\n        for (TransientActionFactory<?> taf : TransientActionFactory.factoriesFor(getClass(), type)) {\n            for (Action a : createFor(taf)) {\n                if (type.isInstance(a)) {\n                    return type.cast(a);\n                }\n            }\n        }\n        return null;\n    }"}, {"instruction": "Creates a sorted set with all the parameters from the given {@code query}, ordered lexicographically by name and value.\n\n@param queryString the query string\n@return a sorted set with all parameters, or {@code null} if the query string is {@code null} or empty.", "input": "Java", "output": "private static SortedSet<QueryParameter> createSortedParameters(final String queryString) {\n\t\tif (queryString == null || queryString.isEmpty()) {\n\t\t\treturn null;\n\t\t}\n\n\t\tfinal String[] pairs = queryString.split(\"&\");\n\t\tfinal SortedSet<QueryParameter> params = new TreeSet<>();\n\n\t\tfor (final String pair : pairs) {\n\t\t\tif (pair.length() == 0) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tString[] tokens = pair.split(\"=\", 2);\n\t\t\tswitch (tokens.length) {\n\t\t\tcase 1:\n\t\t\t\tif (pair.charAt(0) == '=') {\n\t\t\t\t\tparams.add(new QueryParameter(\"\", tokens[0]));\n\t\t\t\t} else {\n\t\t\t\t\tparams.add(new QueryParameter(tokens[0], \"\"));\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase 2:\n\t\t\t\tparams.add(new QueryParameter(tokens[0], tokens[1]));\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t\treturn params;\n\t}"}, {"instruction": "This method converts existing X509TrustManagers to ClientX509ExtendedTrustManagers.\n\n@param trustManagers\n@param tlsConfig\n@return", "input": "Java", "output": "public static TrustManager[] decorate(TrustManager[] trustManagers, TLSConfig tlsConfig) {\n\t\tif (null!=trustManagers && trustManagers.length>0) {\n\t\t\tTrustManager[] decoratedTrustManagers = new TrustManager[trustManagers.length];\n\t\t\t\n\t\t\tfor (int i=0; i<trustManagers.length; ++i) {\n\t\t\t\tTrustManager trustManager = trustManagers[i];\n\t\t\t\t\n\t\t\t\tif (trustManager instanceof X509TrustManager){\n\t\t\t\t\tdecoratedTrustManagers[i] = new ClientX509ExtendedTrustManager((X509TrustManager)trustManager, tlsConfig);\n\t\t\t\t}else {\n\t\t\t\t\tdecoratedTrustManagers[i] = trustManager;\n\t\t\t\t}\n\t\t\t}\n\t\t\t\n\t\t\treturn decoratedTrustManagers;\n\t\t}\n\t\t\n\t\treturn trustManagers;\n\t}"}, {"instruction": "todo dmgcodevil: it would be better to reuse the code from build() method", "input": "Java", "output": "public HystrixObservableCommand.Setter buildObservableCommandSetter() {\n        HystrixObservableCommand.Setter setter = HystrixObservableCommand.Setter\n                .withGroupKey(HystrixCommandGroupKey.Factory.asKey(groupKey))\n                .andCommandKey(HystrixCommandKey.Factory.asKey(commandKey));\n        try {\n            setter.andCommandPropertiesDefaults(HystrixPropertiesManager.initializeCommandProperties(commandProperties));\n        } catch (IllegalArgumentException e) {\n            throw new HystrixPropertyException(\"Failed to set Command properties. \" + getInfo(), e);\n        }\n        return setter;\n    }"}, {"instruction": "Load some or all of completely persisted Values", "input": "Java", "output": "byte[] loadPersist() {\n    // 00       assert: not written yet\n    // 01       assert: load-after-delete\n    // 10       expected; read\n    // 11       assert: load-after-delete\n    assert isPersisted();\n    try {\n      byte[] res = H2O.getPM().load(backend(), this);\n      assert !isDeleted();        // Race in user-land: load-after-delete\n      return res;\n    } catch( IOException ioe ) { throw Log.throwErr(ioe); }\n  }"}, {"instruction": "Add the given stream element queue entry to the operator's stream element queue. This\noperation blocks until the element has been added.\n\n<p>For that it tries to put the element into the queue and if not successful then it waits on\nthe checkpointing lock. The checkpointing lock is also used by the {@link Emitter} to output\nelements. The emitter is also responsible for notifying this method if the queue has capacity\nleft again, by calling notifyAll on the checkpointing lock.\n\n@param streamElementQueueEntry to add to the operator's queue\n@param <T> Type of the stream element queue entry's result\n@throws InterruptedException if the current thread has been interrupted", "input": "Java", "output": "private <T> void addAsyncBufferEntry(StreamElementQueueEntry<T> streamElementQueueEntry) throws InterruptedException {\n\t\tassert(Thread.holdsLock(checkpointingLock));\n\n\t\tpendingStreamElementQueueEntry = streamElementQueueEntry;\n\n\t\twhile (!queue.tryPut(streamElementQueueEntry)) {\n\t\t\t// we wait for the emitter to notify us if the queue has space left again\n\t\t\tcheckpointingLock.wait();\n\t\t}\n\n\t\tpendingStreamElementQueueEntry = null;\n\t}"}, {"instruction": "------------------------------------------------------------------------", "input": "Java", "output": "private static String getAndCheckOption(Configuration config, ConfigOption<String> primaryOption, ConfigOption<String> fallbackOption) {\n\t\tString value = config.getString(primaryOption, config.getString(fallbackOption));\n\t\tif (value != null) {\n\t\t\treturn value;\n\t\t}\n\t\telse {\n\t\t\tthrow new IllegalConfigurationException(\"The config option \" + primaryOption.key() +\n\t\t\t\t\t\" or \" + fallbackOption.key() + \" is missing.\");\n\t\t}\n\t}"}, {"instruction": "Gets origin weight.\n\n@return the origin weight", "input": "Java", "output": "public int getOriginWeight() {\n        if (originWeight == null) {\n            if (providerInfo == null) {\n                originWeight = RpcConfigs.getIntValue(RpcOptions.PROVIDER_WEIGHT);\n            } else {\n                originWeight = CommonUtils.parseInt(providerInfo.getStaticAttr(ProviderInfoAttrs.ATTR_WEIGHT),\n                    RpcConfigs.getIntValue(RpcOptions.PROVIDER_WEIGHT));\n            }\n        }\n        return originWeight;\n    }"}, {"instruction": "\u68c0\u67e5\u8bc1\u4e66\u94fe\n\n@param rootCerts\n\u6839\u8bc1\u4e66\n@param cert\n\u5f85\u9a8c\u8bc1\u7684\u8bc1\u4e66\n@return", "input": "Java", "output": "public static boolean verifyCertificate(X509Certificate cert) {\n\t\t\n\t\tif ( null == cert) {\n\t\t\tLogUtil.writeErrorLog(\"cert must Not null\");\n\t\t\treturn false;\n\t\t}\n\t\ttry {\n\t\t\tcert.checkValidity();//\u9a8c\u8bc1\u6709\u6548\u671f\n//\t\t\tcert.verify(middleCert.getPublicKey());\n\t\t\tif(!verifyCertificateChain(cert)){\n\t\t\t\treturn false;\n\t\t\t}\n\t\t} catch (Exception e) {\n\t\t\tLogUtil.writeErrorLog(\"verifyCertificate fail\", e);\n\t\t\treturn false;\n\t\t}\n\t\t\n\t\tif(SDKConfig.getConfig().isIfValidateCNName()){\n\t\t\t// \u9a8c\u8bc1\u516c\u94a5\u662f\u5426\u5c5e\u4e8e\u94f6\u8054\n\t\t\tif(!UNIONPAY_CNNAME.equals(CertUtil.getIdentitiesFromCertficate(cert))) {\n\t\t\t\tLogUtil.writeErrorLog(\"cer owner is not CUP:\" + CertUtil.getIdentitiesFromCertficate(cert));\n\t\t\t\treturn false;\n\t\t\t}\n\t\t} else {\n\t\t\t// \u9a8c\u8bc1\u516c\u94a5\u662f\u5426\u5c5e\u4e8e\u94f6\u8054\n\t\t\tif(!UNIONPAY_CNNAME.equals(CertUtil.getIdentitiesFromCertficate(cert)) \n\t\t\t\t\t&& !\"00040000:SIGN\".equals(CertUtil.getIdentitiesFromCertficate(cert))) {\n\t\t\t\tLogUtil.writeErrorLog(\"cer owner is not CUP:\" + CertUtil.getIdentitiesFromCertficate(cert));\n\t\t\t\treturn false;\n\t\t\t}\n\t\t}\n\t\treturn true;\t\t\n\t}"}, {"instruction": "Sync part of an open file to the file system.\n\n@param fd      The file descriptor of the source file.\n@param offset  The offset within the file.\n@param nbytes  The number of bytes to be synced.\n@param flags   Signal how to synchronize", "input": "Java", "output": "private static void trySyncFileRange(int fd, long offset, long nbytes, int flags)\n  {\n    if (!initialized || !syncFileRangePossible || fd < 0) {\n      return;\n    }\n    try {\n      int ret_code = sync_file_range(fd, offset, nbytes, flags);\n      if (ret_code != 0) {\n        log.warn(\"failed on syncing fd [%d], offset [%d], bytes [%d], ret_code [%d], errno [%d]\",\n            fd, offset, nbytes, ret_code, Native.getLastError());\n        return;\n      }\n    }\n    catch (UnsupportedOperationException uoe) {\n      log.warn(uoe, \"sync_file_range is not supported\");\n      syncFileRangePossible = false;\n    }\n    catch (UnsatisfiedLinkError nle) {\n      log.warn(nle, \"sync_file_range failed on fd [%d], offset [%d], bytes [%d]\", fd, offset, nbytes);\n      syncFileRangePossible = false;\n    }\n    catch (Exception e) {\n      log.warn(e, \"Unknown exception: sync_file_range failed on fd [%d], offset [%d], bytes [%d]\",\n          fd, offset, nbytes);\n      syncFileRangePossible = false;\n    }\n  }"}, {"instruction": "Executes one iteration of the MetadataCleaner. This ensures that there cannot be more than one concurrent executions of\nsuch an iteration (whether it's from this direct call or from the regular MetadataCleaner invocation). If concurrent\ninvocations are made, then subsequent calls will be tied to the execution of the first, and will all complete at\nthe same time (even though there's only one executing).\n\n@return A CompletableFuture that, when completed, indicates that the operation completed (successfully or not).", "input": "Java", "output": "CompletableFuture<Void> runOnce() {\n        CompletableFuture<Void> result;\n        synchronized (this.singleRunLock) {\n            if (this.currentIteration != null) {\n                // Some other iteration is in progress. Piggyback on that one and return when it is done.\n                return this.currentIteration;\n            } else {\n                // No other iteration is running.\n                this.currentIteration = new CompletableFuture<>();\n                this.currentIteration.whenComplete((r, ex) -> {\n                    // Unregister the current iteration when done.\n                    synchronized (this.singleRunLock) {\n                        this.currentIteration = null;\n                    }\n                });\n                result = this.currentIteration;\n            }\n        }\n\n        Futures.completeAfter(this::runOnceInternal, result);\n        return result;\n    }"}, {"instruction": "-----------------------------------------------------------------------", "input": "Java", "output": "private static StringBuilder simpleQuote(final StringBuilder sb, final String value) {\n\t\tfor (int i = 0; i < value.length(); ++i) {\n\t\t\tfinal char c = value.charAt(i);\n\t\t\tswitch (c) {\n\t\t\t\tcase '\\\\':\n\t\t\t\tcase '^':\n\t\t\t\tcase '$':\n\t\t\t\tcase '.':\n\t\t\t\tcase '|':\n\t\t\t\tcase '?':\n\t\t\t\tcase '*':\n\t\t\t\tcase '+':\n\t\t\t\tcase '(':\n\t\t\t\tcase ')':\n\t\t\t\tcase '[':\n\t\t\t\tcase '{':\n\t\t\t\t\tsb.append('\\\\');\n\t\t\t\tdefault:\n\t\t\t\t\tsb.append(c);\n\t\t\t}\n\t\t}\n\t\treturn sb;\n\t}"}, {"instruction": "Registers a KvState instance for the given key group index.\n\n@param keyGroupRange  Key group range to register\n@param kvStateId      ID of the KvState instance at the key group index.\n@param kvStateAddress Server address of the KvState instance at the key group index.\n@throws IndexOutOfBoundsException If key group range start < 0 or key group range end >= Number of key groups", "input": "Java", "output": "public void registerKvState(KeyGroupRange keyGroupRange, KvStateID kvStateId, InetSocketAddress kvStateAddress) {\n\n\t\tif (keyGroupRange.getStartKeyGroup() < 0 || keyGroupRange.getEndKeyGroup() >= numKeyGroups) {\n\t\t\tthrow new IndexOutOfBoundsException(\"Key group index\");\n\t\t}\n\n\t\tfor (int kgIdx = keyGroupRange.getStartKeyGroup(); kgIdx <= keyGroupRange.getEndKeyGroup(); ++kgIdx) {\n\n\t\t\tif (kvStateIds[kgIdx] == null && kvStateAddresses[kgIdx] == null) {\n\t\t\t\tnumRegisteredKeyGroups++;\n\t\t\t}\n\n\t\t\tkvStateIds[kgIdx] = kvStateId;\n\t\t\tkvStateAddresses[kgIdx] = kvStateAddress;\n\t\t}\n\t}"}, {"instruction": "Returns all blocking result partitions whose receivers can be scheduled/updated.", "input": "Java", "output": "List<IntermediateResultPartition> finishAllBlockingPartitions() {\n\t\tList<IntermediateResultPartition> finishedBlockingPartitions = null;\n\n\t\tfor (IntermediateResultPartition partition : resultPartitions.values()) {\n\t\t\tif (partition.getResultType().isBlocking() && partition.markFinished()) {\n\t\t\t\tif (finishedBlockingPartitions == null) {\n\t\t\t\t\tfinishedBlockingPartitions = new LinkedList<IntermediateResultPartition>();\n\t\t\t\t}\n\n\t\t\t\tfinishedBlockingPartitions.add(partition);\n\t\t\t}\n\t\t}\n\n\t\tif (finishedBlockingPartitions == null) {\n\t\t\treturn Collections.emptyList();\n\t\t}\n\t\telse {\n\t\t\treturn finishedBlockingPartitions;\n\t\t}\n\t}"}, {"instruction": "------------------------------------ File Input Format -----------------------------------------", "input": "Java", "output": "public <X> DataSource<X> readFile(FileInputFormat<X> inputFormat, String filePath) {\n\t\tif (inputFormat == null) {\n\t\t\tthrow new IllegalArgumentException(\"InputFormat must not be null.\");\n\t\t}\n\t\tif (filePath == null) {\n\t\t\tthrow new IllegalArgumentException(\"The file path must not be null.\");\n\t\t}\n\n\t\tinputFormat.setFilePath(new Path(filePath));\n\t\ttry {\n\t\t\treturn createInput(inputFormat, TypeExtractor.getInputFormatTypes(inputFormat));\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new InvalidProgramException(\"The type returned by the input format could not be automatically determined. \" +\n\t\t\t\t\t\"Please specify the TypeInformation of the produced type explicitly by using the \" +\n\t\t\t\t\t\"'createInput(InputFormat, TypeInformation)' method instead.\");\n\t\t}\n\t}"}, {"instruction": "Leave the information about login failure.\n\n<p>\nOtherwise it seems like Acegi doesn't really leave the detail of the failure anywhere.", "input": "Java", "output": "@Override\n    protected void onUnsuccessfulAuthentication(HttpServletRequest request, HttpServletResponse response, AuthenticationException failed) throws IOException {\n        super.onUnsuccessfulAuthentication(request, response, failed);\n        LOGGER.log(Level.FINE, \"Login attempt failed\", failed);\n        Authentication auth = failed.getAuthentication();\n        if (auth != null) {\n            SecurityListener.fireFailedToLogIn(auth.getName());\n        }\n    }"}, {"instruction": "Configure ldap authentication provider.\n\n@param auth the auth\n@param ldap the ldap", "input": "Java", "output": "protected void configureLdapAuthenticationProvider(final AuthenticationManagerBuilder auth, final MonitorProperties.Endpoints.LdapSecurity ldap) {\n        if (isLdapAuthorizationActive()) {\n            val p = new MonitorEndpointLdapAuthenticationProvider(ldap, securityProperties);\n            auth.authenticationProvider(p);\n        } else {\n            LOGGER.trace(\"LDAP authorization is undefined, given no LDAP url, base-dn, search filter or role/group filter is configured\");\n        }\n    }"}, {"instruction": "For 'speed' reasons, Eclipse works a lot with char arrays. I have my doubts this was a fruitful exercise,\nbut we need to deal with it. This turns [[java][lang][String]] into \"java.lang.String\".", "input": "Java", "output": "public static String toQualifiedName(char[][] typeName) {\n\t\tint len = typeName.length - 1; // number of dots\n\t\tif (len == 0) return new String(typeName[0]);\n\t\t\n\t\tfor (char[] c : typeName) len += c.length;\n\t\tchar[] ret = new char[len];\n\t\tchar[] part = typeName[0];\n\t\tSystem.arraycopy(part, 0, ret, 0, part.length);\n\t\tint pos = part.length;\n\t\tfor (int i = 1; i < typeName.length; i++) {\n\t\t\tret[pos++] = '.';\n\t\t\tpart = typeName[i];\n\t\t\tSystem.arraycopy(part, 0, ret, pos, part.length);\n\t\t\tpos += part.length;\n\t\t}\n\t\treturn new String(ret);\n\t}"}, {"instruction": "If this specified class represents a primitive type (int, float, etc.)\nthen it is translated into its wrapper type (Integer, Float, etc.). If\nthe passed class is not a primitive then it is just returned.\n\n@param primitive class\n@return class", "input": "Java", "output": "private static Class<?> translateFromPrimitive(Class<?> primitive) {\n        if (!primitive.isPrimitive()) {\n            return primitive;\n        }\n\n        if (Boolean.TYPE.equals(primitive)) {\n            return Boolean.class;\n        }\n        if (Character.TYPE.equals(primitive)) {\n            return Character.class;\n        }\n        if (Byte.TYPE.equals(primitive)) {\n            return Byte.class;\n        }\n        if (Short.TYPE.equals(primitive)) {\n            return Short.class;\n        }\n        if (Integer.TYPE.equals(primitive)) {\n            return Integer.class;\n        }\n        if (Long.TYPE.equals(primitive)) {\n            return Long.class;\n        }\n        if (Float.TYPE.equals(primitive)) {\n            return Float.class;\n        }\n        if (Double.TYPE.equals(primitive)) {\n            return Double.class;\n        }\n\n        throw new RuntimeException(\"Error translating type:\" + primitive);\n    }"}, {"instruction": "Copies the content of this string to a byte array.\n\n@param srcIdx the starting offset of characters to copy.\n@param dst the destination byte array.\n@param dstIdx the starting offset in the destination byte array.\n@param length the number of characters to copy.", "input": "Java", "output": "public void copy(int srcIdx, byte[] dst, int dstIdx, int length) {\n        if (isOutOfBounds(srcIdx, length, length())) {\n            throw new IndexOutOfBoundsException(\"expected: \" + \"0 <= srcIdx(\" + srcIdx + \") <= srcIdx + length(\"\n                            + length + \") <= srcLen(\" + length() + ')');\n        }\n\n        System.arraycopy(value, srcIdx + offset, checkNotNull(dst, \"dst\"), dstIdx, length);\n    }"}, {"instruction": "Register an instance.\n\n@param registration registration info\n@param builder      UriComponentsBuilder\n@return The registered instance id;", "input": "Java", "output": "@PostMapping(path = \"/instances\", consumes = MediaType.APPLICATION_JSON_VALUE)\n    public Mono<ResponseEntity<Map<String, InstanceId>>> register(@RequestBody Registration registration,\n                                                                  UriComponentsBuilder builder) {\n        Registration withSource = Registration.copyOf(registration).source(\"http-api\").build();\n        LOGGER.debug(\"Register instance {}\", withSource);\n        return registry.register(withSource).map(id -> {\n            URI location = builder.replacePath(\"/instances/{id}\").buildAndExpand(id).toUri();\n            return ResponseEntity.created(location).body(Collections.singletonMap(\"id\", id));\n        });\n    }"}, {"instruction": "fill the result set with the remaining flow capacity .\n\n@param stats reference to the result container which contains all the results, this specific\nmethod will only work on the property \"remainingFlowCapacity\".", "input": "Java", "output": "protected void fillRemainingFlowCapacityAndLastDispatchedTime(final ExecutorInfo stats) {\n\n    final AzkabanExecutorServer server = AzkabanExecutorServer.getApp();\n    if (server != null) {\n      final FlowRunnerManager runnerMgr = AzkabanExecutorServer.getApp().getFlowRunnerManager();\n      final int assignedFlows = runnerMgr.getNumRunningFlows() + runnerMgr.getNumQueuedFlows();\n      stats.setRemainingFlowCapacity(runnerMgr.getMaxNumRunningFlows() - assignedFlows);\n      stats.setNumberOfAssignedFlows(assignedFlows);\n      stats.setLastDispatchedTime(runnerMgr.getLastFlowSubmittedTime());\n    } else {\n      logger.error(\"failed to get data for remaining flow capacity or LastDispatchedTime\"\n          + \" as the AzkabanExecutorServer has yet been initialized.\");\n    }\n  }"}, {"instruction": "\u8bc6\u522bJDBC\u9a71\u52a8\u540d\n\n@param ds \u6570\u636e\u6e90\n@return \u9a71\u52a8", "input": "Java", "output": "public static String identifyDriver(DataSource ds) {\r\n\t\tif(ds instanceof DataSourceWrapper) {\r\n\t\t\tfinal String driver = ((DataSourceWrapper)ds).getDriver();\r\n\t\t\tif(StrUtil.isNotBlank(driver)) {\r\n\t\t\t\treturn driver;\r\n\t\t\t}\r\n\t\t}\r\n\t\t\r\n\t\tConnection conn = null;\r\n\t\tString driver = null;\r\n\t\ttry {\r\n\t\t\ttry {\r\n\t\t\t\tconn = ds.getConnection();\r\n\t\t\t} catch (SQLException e) {\r\n\t\t\t\tthrow new DbRuntimeException(\"Get Connection error !\", e);\r\n\t\t\t} catch (NullPointerException e) {\r\n\t\t\t\tthrow new DbRuntimeException(\"Unexpected NullPointException, maybe [jdbcUrl] or [url] is empty!\", e);\r\n\t\t\t}\r\n\t\t\tdriver = identifyDriver(conn);\r\n\t\t} finally {\r\n\t\t\tDbUtil.close(conn);\r\n\t\t}\r\n\r\n\t\treturn driver;\r\n\t}"}, {"instruction": "{@inheritDoc}", "input": "Java", "output": "@Override\n    public double f1Score(INDArray examples, INDArray labels) {\n        if (examples.rank() == 3)\n            examples = TimeSeriesUtils.reshape3dTo2d(examples, LayerWorkspaceMgr.noWorkspaces(), ArrayType.ACTIVATIONS);\n        if (labels.rank() == 3)\n            labels = TimeSeriesUtils.reshape3dTo2d(labels, LayerWorkspaceMgr.noWorkspaces(), ArrayType.ACTIVATIONS);\n        return super.f1Score(examples, labels);\n    }"}, {"instruction": "arg.toFloat", "input": "Java", "output": "public static java.lang.Float toFloat(Object arg) throws NoSuchMethodException {\n        if (arg instanceof java.lang.Integer) return boxToFloat((float)unboxToInt(arg));\n        if (arg instanceof java.lang.Long) return boxToFloat((float)unboxToLong(arg));\n        if (arg instanceof java.lang.Float) return (java.lang.Float)arg;\n        if (arg instanceof java.lang.Double) return boxToFloat((float)unboxToDouble(arg));\n        if (arg instanceof java.lang.Character) return boxToFloat((float)unboxToChar(arg));\n        if (arg instanceof java.lang.Byte) return boxToFloat((float)unboxToByte(arg));\n        if (arg instanceof java.lang.Short) return boxToFloat((float)unboxToShort(arg));\n        throw new NoSuchMethodException();\n    }"}, {"instruction": "Gets the view properties configured for this view.\n@since 1.406", "input": "Java", "output": "public DescribableList<ViewProperty,ViewPropertyDescriptor> getProperties() {\n        // readResolve was the best place to do this, but for compatibility reasons,\n        // this class can no longer have readResolve() (the mechanism itself isn't suitable for class hierarchy)\n        // see JENKINS-9431\n        //\n        // until we have that, putting this logic here.\n        synchronized (PropertyList.class) {\n            if (properties == null) {\n                properties = new PropertyList(this);\n            } else {\n                properties.setOwner(this);\n            }\n            return properties;\n        }\n    }"}, {"instruction": "Return a view macro that may or may not be defined in a certain schema. If it's not defined, returns null.\n\n@param schemaPlus   schema\n@param functionName function name\n\n@return view, or null", "input": "Java", "output": "@Nullable\n  private static TableMacro getView(final SchemaPlus schemaPlus, final String functionName)\n  {\n    // Look for a zero-arg function that is also a TableMacro. The returned value\n    // is never null so we don't need to check for that.\n    final Collection<org.apache.calcite.schema.Function> functions =\n        schemaPlus.getFunctions(functionName);\n\n    for (org.apache.calcite.schema.Function function : functions) {\n      if (function.getParameters().isEmpty() && function instanceof TableMacro) {\n        return (TableMacro) function;\n      }\n    }\n\n    return null;\n  }"}, {"instruction": "\u8fde\u63a5\u8f6c\u5b57\u7b26\u4e32\n\n@param local1  \u672c\u5730\u5730\u5740\n@param remote1 \u8fdc\u7a0b\u5730\u5740\n@return \u5730\u5740\u4fe1\u606f\u5b57\u7b26\u4e32", "input": "Java", "output": "public static String channelToString(SocketAddress local1, SocketAddress remote1) {\n        try {\n            InetSocketAddress local = (InetSocketAddress) local1;\n            InetSocketAddress remote = (InetSocketAddress) remote1;\n            return toAddressString(local) + \" -> \" + toAddressString(remote);\n        } catch (Exception e) {\n            return local1 + \"->\" + remote1;\n        }\n    }"}, {"instruction": "This method uses locks because it can be used during indexing,\nand Druid can call aggregate() and get() concurrently\nhttps://github.com/apache/incubator-druid/pull/3956", "input": "Java", "output": "@Override\n  public void aggregate(final ByteBuffer buf, final int position)\n  {\n    final ArrayOfDoublesSketch update = selector.getObject();\n    if (update == null) {\n      return;\n    }\n    // Wrapping memory and ArrayOfDoublesUnion is inexpensive compared to union operations.\n    // Maintaining a cache of wrapped objects per buffer position like in Theta sketch aggregator\n    // might might be considered, but it would increase complexity including relocate() support.\n    final WritableMemory mem = WritableMemory.wrap(buf, ByteOrder.LITTLE_ENDIAN);\n    final WritableMemory region = mem.writableRegion(position, maxIntermediateSize);\n    final Lock lock = stripedLock.getAt(ArrayOfDoublesSketchBuildBufferAggregator.lockIndex(position)).writeLock();\n    lock.lock();\n    try {\n      final ArrayOfDoublesUnion union = ArrayOfDoublesSketches.wrapUnion(region);\n      union.update(update);\n    }\n    finally {\n      lock.unlock();\n    }\n  }"}, {"instruction": "[VARIABLE \"bucket_\"]", "input": "Java", "output": "public Page<Bucket> listBucketsWithSizeAndPrefix(String prefix) {\n    // [START listBucketsWithSizeAndPrefix]\n    // Include a prefix of bucket-name to reduce search space.\n    // For more information read https://cloud.google.com/storage/docs/json_api/v1/buckets/list\n    Page<Bucket> buckets =\n        storage.list(BucketListOption.pageSize(100), BucketListOption.prefix(prefix));\n    for (Bucket bucket : buckets.iterateAll()) {\n      // do something with the bucket\n    }\n    // [END listBucketsWithSizeAndPrefix]\n    return buckets;\n  }"}, {"instruction": "Calculate the inner product value between vectors.", "input": "Java", "output": "static double inner_product(SparseVector vec1, SparseVector vec2)\n    {\n        Iterator<Map.Entry<Integer, Double>> it;\n        SparseVector other;\n        if (vec1.size() < vec2.size())\n        {\n            it = vec1.entrySet().iterator();\n            other = vec2;\n        }\n        else\n        {\n            it = vec2.entrySet().iterator();\n            other = vec1;\n        }\n        double prod = 0;\n        while (it.hasNext())\n        {\n            Map.Entry<Integer, Double> entry = it.next();\n            prod += entry.getValue() * other.get(entry.getKey());\n        }\n        return prod;\n    }"}, {"instruction": "Gets the most recent health check results for each IP for the instance that is referenced by\nthe given target pool.\n\n<p>Sample code:\n\n<pre><code>\ntry (TargetPoolClient targetPoolClient = TargetPoolClient.create()) {\nProjectRegionTargetPoolName targetPool = ProjectRegionTargetPoolName.of(\"[PROJECT]\", \"[REGION]\", \"[TARGET_POOL]\");\nInstanceReference instanceReferenceResource = InstanceReference.newBuilder().build();\nTargetPoolInstanceHealth response = targetPoolClient.getHealthTargetPool(targetPool, instanceReferenceResource);\n}\n</code></pre>\n\n@param targetPool Name of the TargetPool resource to which the queried instance belongs.\n@param instanceReferenceResource\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "@BetaApi\n  public final TargetPoolInstanceHealth getHealthTargetPool(\n      ProjectRegionTargetPoolName targetPool, InstanceReference instanceReferenceResource) {\n\n    GetHealthTargetPoolHttpRequest request =\n        GetHealthTargetPoolHttpRequest.newBuilder()\n            .setTargetPool(targetPool == null ? null : targetPool.toString())\n            .setInstanceReferenceResource(instanceReferenceResource)\n            .build();\n    return getHealthTargetPool(request);\n  }"}, {"instruction": "Gets a set of locality groups that should be added to the index table (not the metrics table).\n\n@param table Table for the locality groups, see AccumuloClient#getTable\n@return Mapping of locality group to column families in the locality group, 1:1 mapping in\nthis case", "input": "Java", "output": "public static Map<String, Set<Text>> getLocalityGroups(AccumuloTable table)\n    {\n        Map<String, Set<Text>> groups = new HashMap<>();\n        // For each indexed column\n        for (AccumuloColumnHandle columnHandle : table.getColumns().stream().filter(AccumuloColumnHandle::isIndexed).collect(Collectors.toList())) {\n            // Create a Text version of the index column family\n            Text indexColumnFamily = new Text(getIndexColumnFamily(columnHandle.getFamily().get().getBytes(UTF_8), columnHandle.getQualifier().get().getBytes(UTF_8)).array());\n\n            // Add this to the locality groups,\n            // it is a 1:1 mapping of locality group to column families\n            groups.put(indexColumnFamily.toString(), ImmutableSet.of(indexColumnFamily));\n        }\n        return groups;\n    }"}, {"instruction": "Called by {@link Executor} to kill excessive executors from this computer.", "input": "Java", "output": "protected void removeExecutor(final Executor e) {\n        final Runnable task = new Runnable() {\n            @Override\n            public void run() {\n                synchronized (Computer.this) {\n                    executors.remove(e);\n                    addNewExecutorIfNecessary();\n                    if (!isAlive()) {\n                        AbstractCIBase ciBase = Jenkins.getInstanceOrNull();\n                        if (ciBase != null) { // TODO confirm safe to assume non-null and use getInstance()\n                            ciBase.removeComputer(Computer.this);\n                        }\n                    }\n                }\n            }\n        };\n        if (!Queue.tryWithLock(task)) {\n            // JENKINS-28840 if we couldn't get the lock push the operation to a separate thread to avoid deadlocks\n            threadPoolForRemoting.submit(Queue.wrapWithLock(task));\n        }\n    }"}, {"instruction": "bits\u8f6cbytes\n\n@param bits \u4e8c\u8fdb\u5236\n@return bytes", "input": "Java", "output": "public static byte[] bits2Bytes(String bits) {\n        int lenMod  = bits.length() % 8;\n        int byteLen = bits.length() / 8;\n        // \u4e0d\u662f8\u7684\u500d\u6570\u524d\u9762\u88650\n        if (lenMod != 0) {\n            for (int i = lenMod; i < 8; i++) {\n                bits = \"0\" + bits;\n            }\n            byteLen++;\n        }\n        byte[] bytes = new byte[byteLen];\n        for (int i = 0; i < byteLen; ++i) {\n            for (int j = 0; j < 8; ++j) {\n                bytes[i] <<= 1;\n                bytes[i] |= bits.charAt(i * 8 + j) - '0';\n            }\n        }\n        return bytes;\n    }"}, {"instruction": "Renew disabled data source names.\n\n@param disabledStateChangedEvent disabled state changed event", "input": "Java", "output": "@Subscribe\n    public synchronized void renew(final DisabledStateChangedEvent disabledStateChangedEvent) {\n        OrchestrationShardingSchema shardingSchema = disabledStateChangedEvent.getShardingSchema();\n        if (getName().equals(shardingSchema.getSchemaName())) {\n            for (MasterSlaveRule each : shardingRule.getMasterSlaveRules()) {\n                ((OrchestrationMasterSlaveRule) each).updateDisabledDataSourceNames(shardingSchema.getDataSourceName(), disabledStateChangedEvent.isDisabled());\n            }\n        }\n    }"}, {"instruction": "\u83b7\u5f97 Sofa Runtime \u7684\u65e5\u5fd7\u5bf9\u8c61\uff0c\u6253\u5370\u51fa\u83b7\u5f97\u914d\u7f6e\u4e2d\u5fc3\u5730\u5740", "input": "Java", "output": "private void printUserData(String dataId, UserData userData) {\n        StringBuilder sb = new StringBuilder();\n        int count = 0;\n        if (userData != null && userData.getZoneData() != null) {\n            Map<String, List<String>> oneUserData = userData.getZoneData();\n            for (Map.Entry<String, List<String>> entry : oneUserData.entrySet()) {\n                sb.append(\"  --- \").append(entry.getKey()).append(\"\\n\");\n                for (String provider : entry.getValue()) {\n                    sb.append(\"   >>> \").append((String) provider).append(\"\\n\");\n                    ++count;\n                }\n            }\n        }\n        if (LOGGER.isInfoEnabled()) {\n            LOGGER.info(LogCodes.getLog(LogCodes.INFO_ROUTE_REGISTRY_URLS_HANDLE,\n                dataId, count, sb.toString()));\n        }\n    }"}, {"instruction": "Creates a new pending checkpoint tracker.\n\n@param checkpointId ID of the checkpoint.\n@param triggerTimestamp Trigger timestamp of the checkpoint.\n@param props The checkpoint properties.\n@return Tracker for statistics gathering.", "input": "Java", "output": "PendingCheckpointStats reportPendingCheckpoint(\n\t\t\tlong checkpointId,\n\t\t\tlong triggerTimestamp,\n\t\t\tCheckpointProperties props) {\n\n\t\tConcurrentHashMap<JobVertexID, TaskStateStats> taskStateStats = createEmptyTaskStateStatsMap();\n\n\t\tPendingCheckpointStats pending = new PendingCheckpointStats(\n\t\t\t\tcheckpointId,\n\t\t\t\ttriggerTimestamp,\n\t\t\t\tprops,\n\t\t\t\ttotalSubtaskCount,\n\t\t\t\ttaskStateStats,\n\t\t\t\tnew PendingCheckpointStatsCallback());\n\n\t\tstatsReadWriteLock.lock();\n\t\ttry {\n\t\t\tcounts.incrementInProgressCheckpoints();\n\t\t\thistory.addInProgressCheckpoint(pending);\n\n\t\t\tdirty = true;\n\t\t} finally {\n\t\t\tstatsReadWriteLock.unlock();\n\t\t}\n\n\t\treturn pending;\n\t}"}, {"instruction": "[VARIABLE \"my_second_key_name\"]", "input": "Java", "output": "public List<Entity> getMultiple(String firstKeyName, String secondKeyName) {\n    Datastore datastore = transaction.getDatastore();\n    // TODO change so that it's not necessary to hold the entities in a list for integration testing\n    // [START getMultiple]\n    KeyFactory keyFactory = datastore.newKeyFactory().setKind(\"MyKind\");\n    Key firstKey = keyFactory.newKey(firstKeyName);\n    Key secondKey = keyFactory.newKey(secondKeyName);\n    Iterator<Entity> entitiesIterator = transaction.get(firstKey, secondKey);\n    List<Entity> entities = Lists.newArrayList();\n    while (entitiesIterator.hasNext()) {\n      Entity entity = entitiesIterator.next();\n      // do something with the entity\n      entities.add(entity);\n    }\n    transaction.commit();\n    // [END getMultiple]\n    return entities;\n  }"}, {"instruction": "Create XA connection from normal connection.\n\n@param databaseType database type\n@param connection normal connection\n@param xaDataSource XA data source\n@return XA connection", "input": "Java", "output": "public static XAConnection createXAConnection(final DatabaseType databaseType, final XADataSource xaDataSource, final Connection connection) {\n        switch (databaseType) {\n            case MySQL:\n                return new MySQLXAConnectionWrapper().wrap(xaDataSource, connection);\n            case PostgreSQL:\n                return new PostgreSQLXAConnectionWrapper().wrap(xaDataSource, connection);\n            case H2:\n                return new H2XAConnectionWrapper().wrap(xaDataSource, connection);\n            default:\n                throw new UnsupportedOperationException(String.format(\"Cannot support database type: `%s`\", databaseType));\n        }\n    }"}, {"instruction": "{@inheritDoc}", "input": "Java", "output": "@Override\r\n  public int size()\r\n  {\r\n    if (size < 0) {\r\n      size = 0;\r\n      for (int i = 0; i <= lastWordIndex; i++) {\r\n        int w = words[i];\r\n        if (isLiteral(w)) {\r\n          size += getLiteralBitCount(w);\r\n        } else {\r\n          if (isZeroSequence(w)) {\r\n            if (!isSequenceWithNoBits(w)) {\r\n              size++;\r\n            }\r\n          } else {\r\n            size += maxLiteralLengthMultiplication(getSequenceCount(w) + 1);\r\n            if (!isSequenceWithNoBits(w)) {\r\n              size--;\r\n            }\r\n          }\r\n        }\r\n      }\r\n    }\r\n    return size;\r\n  }"}, {"instruction": "This will encode the random metadata fields, and repeatedly lookup the default other headers.", "input": "Java", "output": "@Benchmark\n  @BenchmarkMode(Mode.SampleTime)\n  @OutputTimeUnit(TimeUnit.NANOSECONDS)\n  public ByteBuf encodeClientHeaders() throws Exception {\n    scratchBuffer.clear();\n    Http2Headers headers =\n        Utils.convertClientHeaders(metadata, scheme, defaultPath, authority, Utils.HTTP_METHOD,\n            userAgent);\n    headersEncoder.encodeHeaders(1, headers, scratchBuffer);\n    return scratchBuffer;\n  }"}, {"instruction": "Return big decimal from buffer.\n\n@see mysql-5.1.60/strings/decimal.c - bin2decimal()", "input": "Java", "output": "public final BigDecimal getDecimal(final int pos, final int precision, final int scale) {\r\n        final int intg = precision - scale;\r\n        final int frac = scale;\r\n        final int intg0 = intg / DIG_PER_INT32;\r\n        final int frac0 = frac / DIG_PER_INT32;\r\n        final int intg0x = intg - intg0 * DIG_PER_INT32;\r\n        final int frac0x = frac - frac0 * DIG_PER_INT32;\r\n\r\n        final int binSize = intg0 * SIZE_OF_INT32 + dig2bytes[intg0x] + frac0 * SIZE_OF_INT32 + dig2bytes[frac0x];\r\n        if (pos + binSize > limit || pos < 0) {\r\n            throw new IllegalArgumentException(\"limit excceed: \" + (pos < 0 ? pos : (pos + binSize)));\r\n        }\r\n        return getDecimal0(origin + pos, intg, frac, // NL\r\n            intg0,\r\n            frac0,\r\n            intg0x,\r\n            frac0x);\r\n    }"}, {"instruction": "This method tries to isolate class loading during a Function call\n\n@param clazzName    The Class which has a static method called `runTask`\n@param input        The input for `runTask`, must have `input.getClass()` be the class of the input for runTask\n@param loader       The loader to use as the context class loader during invocation\n@param <InputType>  The input type of the method.\n@param <OutputType> The output type of the method. The result of runTask must be castable to this type.\n\n@return The result of the method invocation", "input": "Java", "output": "public static <InputType, OutputType> OutputType invokeForeignLoader(\n      final String clazzName,\n      final InputType input,\n      final ClassLoader loader\n  )\n  {\n    log.debug(\"Launching [%s] on class loader [%s] with input class [%s]\", clazzName, loader, input.getClass());\n    final ClassLoader oldLoader = Thread.currentThread().getContextClassLoader();\n    try {\n      Thread.currentThread().setContextClassLoader(loader);\n      final Class<?> clazz = loader.loadClass(clazzName);\n      final Method method = clazz.getMethod(\"runTask\", input.getClass());\n      return (OutputType) method.invoke(null, input);\n    }\n    catch (IllegalAccessException | InvocationTargetException | ClassNotFoundException | NoSuchMethodException e) {\n      throw new RuntimeException(e);\n    }\n    finally {\n      Thread.currentThread().setContextClassLoader(oldLoader);\n    }\n  }"}, {"instruction": "Sends the registration information of {@link InstanceInfo} receiving by\nthis node to the peer node represented by this class.\n\n@param info\nthe instance information {@link InstanceInfo} of any instance\nthat is send to this instance.\n@throws Exception", "input": "Java", "output": "public void register(final InstanceInfo info) throws Exception {\n        long expiryTime = System.currentTimeMillis() + getLeaseRenewalOf(info);\n        batchingDispatcher.process(\n                taskId(\"register\", info),\n                new InstanceReplicationTask(targetHost, Action.Register, info, null, true) {\n                    public EurekaHttpResponse<Void> execute() {\n                        return replicationClient.register(info);\n                    }\n                },\n                expiryTime\n        );\n    }"}, {"instruction": "return current date time by specified hour:minute\n\n@param plan format: hh:mm", "input": "Java", "output": "public static Date getCurrentDateByPlan(String plan, String pattern) {\n\t\ttry {\n\t\t\tFastDateFormat format = FastDateFormat.getInstance(pattern);\n\t\t\tDate end = format.parse(plan);\n\t\t\tCalendar today = Calendar.getInstance();\n\t\t\tend = DateUtils.setYears(end, (today.get(Calendar.YEAR)));\n\t\t\tend = DateUtils.setMonths(end, today.get(Calendar.MONTH));\n\t\t\tend = DateUtils.setDays(end, today.get(Calendar.DAY_OF_MONTH));\n\t\t\treturn end;\n\t\t} catch (Exception e) {\n\t\t\tthrow ExceptionUtil.unchecked(e);\n\t\t}\n\t}"}, {"instruction": "Load a computation graph from a InputStream\n@param is the inputstream to get the computation graph from\n@return the loaded computation graph\n\n@throws IOException", "input": "Java", "output": "public static ComputationGraph restoreComputationGraph(@NonNull InputStream is, boolean loadUpdater)\n            throws IOException {\n        checkInputStream(is);\n\n        File tmpFile = null;\n        try{\n            tmpFile = tempFileFromStream(is);\n            return restoreComputationGraph(tmpFile, loadUpdater);\n        } finally {\n            if(tmpFile != null){\n                tmpFile.delete();\n            }\n        }\n    }"}, {"instruction": "TODO: almost identical to ModelsHandler; refactor", "input": "Java", "output": "public static ModelMetrics getFromDKV(Key key) {\n    if (null == key)\n      throw new IllegalArgumentException(\"Got null key.\");\n\n    Value v = DKV.get(key);\n    if (null == v)\n      throw new IllegalArgumentException(\"Did not find key: \" + key.toString());\n\n    Iced ice = v.get();\n    if (! (ice instanceof ModelMetrics))\n      throw new IllegalArgumentException(\"Expected a Model for key: \" + key.toString() + \"; got a: \" + ice.getClass());\n\n    return (ModelMetrics)ice;\n  }"}, {"instruction": "Creates the high-availability services for the TaskManagers participating in\na Flink YARN application.\n\n@param flinkConfig  The Flink configuration.\n@param hadoopConfig The Hadoop configuration for the YARN cluster.\n\n@return The created high-availability services.\n\n@throws IOException Thrown, if the high-availability services could not be initialized.", "input": "Java", "output": "public static YarnHighAvailabilityServices forYarnTaskManager(\n\t\t\tConfiguration flinkConfig,\n\t\t\torg.apache.hadoop.conf.Configuration hadoopConfig) throws IOException {\n\n\t\tcheckNotNull(flinkConfig, \"flinkConfig\");\n\t\tcheckNotNull(hadoopConfig, \"hadoopConfig\");\n\n\t\tfinal HighAvailabilityMode mode = HighAvailabilityMode.fromConfig(flinkConfig);\n\t\tswitch (mode) {\n\t\t\tcase NONE:\n\t\t\t\treturn new YarnPreConfiguredMasterNonHaServices(\n\t\t\t\t\tflinkConfig,\n\t\t\t\t\thadoopConfig,\n\t\t\t\t\tHighAvailabilityServicesUtils.AddressResolution.TRY_ADDRESS_RESOLUTION);\n\n\t\t\tcase ZOOKEEPER:\n\t\t\t\tthrow  new UnsupportedOperationException(\"to be implemented\");\n\n\t\t\tdefault:\n\t\t\t\tthrow new IllegalConfigurationException(\"Unrecognized high availability mode: \" + mode);\n\t\t}\n\t}"}, {"instruction": "\u622a\u53d6\u96c6\u5408\u7684\u90e8\u5206\n\n@param <T> \u96c6\u5408\u5143\u7d20\u7c7b\u578b\n@param list \u88ab\u622a\u53d6\u7684\u6570\u7ec4\n@param start \u5f00\u59cb\u4f4d\u7f6e\uff08\u5305\u542b\uff09\n@param end \u7ed3\u675f\u4f4d\u7f6e\uff08\u4e0d\u5305\u542b\uff09\n@param step \u6b65\u8fdb\n@return \u622a\u53d6\u540e\u7684\u6570\u7ec4\uff0c\u5f53\u5f00\u59cb\u4f4d\u7f6e\u8d85\u8fc7\u6700\u5927\u65f6\uff0c\u8fd4\u56de\u7a7a\u7684List\n@since 4.0.6", "input": "Java", "output": "public static <T> List<T> sub(List<T> list, int start, int end, int step) {\r\n\t\tif (list == null || list.isEmpty()) {\r\n\t\t\treturn null;\r\n\t\t}\r\n\r\n\t\tfinal int size = list.size();\r\n\t\tif (start < 0) {\r\n\t\t\tstart += size;\r\n\t\t}\r\n\t\tif (end < 0) {\r\n\t\t\tend += size;\r\n\t\t}\r\n\t\tif (start == size) {\r\n\t\t\treturn new ArrayList<>(0);\r\n\t\t}\r\n\t\tif (start > end) {\r\n\t\t\tint tmp = start;\r\n\t\t\tstart = end;\r\n\t\t\tend = tmp;\r\n\t\t}\r\n\t\tif (end > size) {\r\n\t\t\tif (start >= size) {\r\n\t\t\t\treturn new ArrayList<>(0);\r\n\t\t\t}\r\n\t\t\tend = size;\r\n\t\t}\r\n\r\n\t\tif (step <= 1) {\r\n\t\t\treturn list.subList(start, end);\r\n\t\t}\r\n\r\n\t\tfinal List<T> result = new ArrayList<>();\r\n\t\tfor (int i = start; i < end; i += step) {\r\n\t\t\tresult.add(list.get(i));\r\n\t\t}\r\n\t\treturn result;\r\n\t}"}, {"instruction": "Create a DataBuffer for indices of given arrays of indices.\n@param indices\n@param shape\n@return", "input": "Java", "output": "protected static DataBuffer createIndiceBuffer(long[][] indices, long[] shape){\n        checkNotNull(indices);\n        checkNotNull(shape);\n        if(indices.length == 0){\n            return Nd4j.getDataBufferFactory().createLong(shape.length);\n        }\n\n        if (indices.length == shape.length) {\n            return Nd4j.createBuffer(ArrayUtil.flattenF(indices));\n        }\n\n        return Nd4j.createBuffer(ArrayUtil.flatten(indices));\n    }"}, {"instruction": "\u521b\u5efa\u5f15\u64ce\n\n@param config \u6a21\u677f\u914d\u7f6e\n@return {@link org.rythmengine.RythmEngine}", "input": "Java", "output": "private static org.rythmengine.RythmEngine createEngine(TemplateConfig config) {\r\n\t\tif (null == config) {\r\n\t\t\tconfig = new TemplateConfig();\r\n\t\t}\r\n\t\t\r\n\t\tfinal Properties props = new Properties();\r\n\t\tfinal String path = config.getPath();\r\n\t\tif (null != path) {\r\n\t\t\tprops.put(\"home.template\", path);\r\n\t\t}\r\n\r\n\t\tfinal org.rythmengine.RythmEngine engine = new org.rythmengine.RythmEngine(props);\r\n\t\treturn engine;\r\n\t}"}, {"instruction": "Used during {@link Jenkins#refreshExtensions()} to add new components into existing {@link ExtensionList}s.\nDo not call from anywhere else.", "input": "Java", "output": "public void refresh(ExtensionComponentSet delta) {\n        boolean fireOnChangeListeners = false;\n        synchronized (getLoadLock()) {\n            if (extensions==null)\n                return;     // not yet loaded. when we load it, we'll load everything visible by then, so no work needed\n\n            Collection<ExtensionComponent<T>> found = load(delta);\n            if (!found.isEmpty()) {\n                List<ExtensionComponent<T>> l = Lists.newArrayList(extensions);\n                l.addAll(found);\n                extensions = sort(l);\n                fireOnChangeListeners = true;\n            }\n        }\n        if (fireOnChangeListeners) {\n            fireOnChangeListeners();\n        }\n    }"}, {"instruction": "\u7ed3\u679c\u7684\u6761\u76ee\u6570\n@param conn \u6570\u636e\u5e93\u8fde\u63a5\u5bf9\u8c61\n@param where \u67e5\u8be2\u6761\u4ef6\n@return \u590d\u5408\u6761\u4ef6\u7684\u7ed3\u679c\u6570\n@throws SQLException SQL\u6267\u884c\u5f02\u5e38", "input": "Java", "output": "public int count(Connection conn, Entity where) throws SQLException {\r\n\t\tcheckConn(conn);\r\n\t\t\r\n\t\tfinal Query query = new Query(SqlUtil.buildConditions(where), where.getTableName());\r\n\t\tPreparedStatement ps = null;\r\n\t\ttry {\r\n\t\t\tps = dialect.psForCount(conn, query);\r\n\t\t\treturn SqlExecutor.query(ps, new NumberHandler()).intValue();\r\n\t\t} catch (SQLException e) {\r\n\t\t\tthrow e;\r\n\t\t} finally {\r\n\t\t\tDbUtil.close(ps);\r\n\t\t}\r\n\t}"}, {"instruction": "Loads the persisted version of each process definition and set values on the in-memory\nversion to be consistent.", "input": "Java", "output": "protected void makeProcessDefinitionsConsistentWithPersistedVersions(ParsedDeployment parsedDeployment) {\n        for (ProcessDefinitionEntity processDefinition : parsedDeployment.getAllProcessDefinitions()) {\n            ProcessDefinitionEntity persistedProcessDefinition =\n                    bpmnDeploymentHelper.getPersistedInstanceOfProcessDefinition(processDefinition);\n\n            if (persistedProcessDefinition != null) {\n                processDefinition.setId(persistedProcessDefinition.getId());\n                processDefinition.setVersion(persistedProcessDefinition.getVersion());\n                processDefinition.setSuspensionState(persistedProcessDefinition.getSuspensionState());\n            }\n        }\n    }"}, {"instruction": "\u68c0\u67e5\u76ee\u6807\u7c7b\u662f\u5426\u53ef\u4ee5\u4ece\u539f\u7c7b\u8f6c\u5316<br>\n\u8f6c\u5316\u5305\u62ec\uff1a<br>\n1\u3001\u539f\u7c7b\u662f\u5bf9\u8c61\uff0c\u76ee\u6807\u7c7b\u578b\u662f\u539f\u7c7b\u578b\u5b9e\u73b0\u7684\u63a5\u53e3<br>\n2\u3001\u76ee\u6807\u7c7b\u578b\u662f\u539f\u7c7b\u578b\u7684\u7236\u7c7b<br>\n3\u3001\u4e24\u8005\u662f\u539f\u59cb\u7c7b\u578b\u6216\u8005\u5305\u88c5\u7c7b\u578b\uff08\u76f8\u4e92\u8f6c\u6362\uff09\n\n@param targetType \u76ee\u6807\u7c7b\u578b\n@param sourceType \u539f\u7c7b\u578b\n@return \u662f\u5426\u53ef\u8f6c\u5316", "input": "Java", "output": "public static boolean isAssignable(Class<?> targetType, Class<?> sourceType) {\r\n\t\tif (null == targetType || null == sourceType) {\r\n\t\t\treturn false;\r\n\t\t}\r\n\r\n\t\t// \u5bf9\u8c61\u7c7b\u578b\r\n\t\tif (targetType.isAssignableFrom(sourceType)) {\r\n\t\t\treturn true;\r\n\t\t}\r\n\r\n\t\t// \u57fa\u672c\u7c7b\u578b\r\n\t\tif (targetType.isPrimitive()) {\r\n\t\t\t// \u539f\u59cb\u7c7b\u578b\r\n\t\t\tClass<?> resolvedPrimitive = BasicType.wrapperPrimitiveMap.get(sourceType);\r\n\t\t\tif (resolvedPrimitive != null && targetType.equals(resolvedPrimitive)) {\r\n\t\t\t\treturn true;\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\t// \u5305\u88c5\u7c7b\u578b\r\n\t\t\tClass<?> resolvedWrapper = BasicType.primitiveWrapperMap.get(sourceType);\r\n\t\t\tif (resolvedWrapper != null && targetType.isAssignableFrom(resolvedWrapper)) {\r\n\t\t\t\treturn true;\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn false;\r\n\t}"}, {"instruction": "\u4ece\u6587\u4ef6\u4e2d\u8bfb\u53d6\u6bcf\u4e00\u884c\u6570\u636e\n\n@param <T> \u96c6\u5408\u7c7b\u578b\n@param collection \u96c6\u5408\n@return \u6587\u4ef6\u4e2d\u7684\u6bcf\u884c\u5185\u5bb9\u7684\u96c6\u5408\n@throws IORuntimeException IO\u5f02\u5e38", "input": "Java", "output": "public <T extends Collection<String>> T readLines(T collection) throws IORuntimeException {\r\n\t\tBufferedReader reader = null;\r\n\t\ttry {\r\n\t\t\treader = FileUtil.getReader(file, charset);\r\n\t\t\tString line;\r\n\t\t\twhile (true) {\r\n\t\t\t\tline = reader.readLine();\r\n\t\t\t\tif (line == null) {\r\n\t\t\t\t\tbreak;\r\n\t\t\t\t}\r\n\t\t\t\tcollection.add(line);\r\n\t\t\t}\r\n\t\t\treturn collection;\r\n\t\t} catch (IOException e) {\r\n\t\t\tthrow new IORuntimeException(e);\r\n\t\t} finally {\r\n\t\t\tIoUtil.close(reader);\r\n\t\t}\r\n\t}"}, {"instruction": "Invokes a nullary static factory method using reflection to stay backwards-compatible with older JDKs.\n\n@param fqcn The fully qualified class name of the type to be produced.\n@param methodName The name of the factory method.\n@return the object produced.", "input": "Java", "output": "private static Object invokeNullaryFactoryMethod(final String fqcn, final String methodName) {\n        try {\n            final Class<?> type = Class.forName(fqcn);\n            final Method method = type.getMethod(methodName);\n\n            return method.invoke(null);\n            // any exception is really unexpected since the type name has\n            // already been verified\n        } catch (final Exception e) {\n            throw new InstantiationException(\n                    String.format(\"Could not create %s#%s(): %s\", fqcn, methodName, e), e);\n        }\n    }"}, {"instruction": "Updates the task execution state for a given task.\n\n@param taskExecutionState New task execution state for a given task\n@return Acknowledge the task execution state update", "input": "Java", "output": "@Override\n\tpublic CompletableFuture<Acknowledge> updateTaskExecutionState(\n\t\t\tfinal TaskExecutionState taskExecutionState) {\n\t\tcheckNotNull(taskExecutionState, \"taskExecutionState\");\n\n\t\tif (executionGraph.updateState(taskExecutionState)) {\n\t\t\treturn CompletableFuture.completedFuture(Acknowledge.get());\n\t\t} else {\n\t\t\treturn FutureUtils.completedExceptionally(\n\t\t\t\tnew ExecutionGraphException(\"The execution attempt \" +\n\t\t\t\t\ttaskExecutionState.getID() + \" was not found.\"));\n\t\t}\n\t}"}, {"instruction": "Eagerly reads {@code byteCount} bytes from the source before launching a background task to\nprocess the data.  This avoids corrupting the stream.", "input": "Java", "output": "void pushDataLater(final int streamId, final BufferedSource source, final int byteCount,\n      final boolean inFinished) throws IOException {\n    final Buffer buffer = new Buffer();\n    source.require(byteCount); // Eagerly read the frame before firing client thread.\n    source.read(buffer, byteCount);\n    if (buffer.size() != byteCount) throw new IOException(buffer.size() + \" != \" + byteCount);\n    pushExecutorExecute(new NamedRunnable(\"OkHttp %s Push Data[%s]\", connectionName, streamId) {\n      @Override public void execute() {\n        try {\n          boolean cancel = pushObserver.onData(streamId, buffer, byteCount, inFinished);\n          if (cancel) writer.rstStream(streamId, ErrorCode.CANCEL);\n          if (cancel || inFinished) {\n            synchronized (Http2Connection.this) {\n              currentPushRequests.remove(streamId);\n            }\n          }\n        } catch (IOException ignored) {\n        }\n      }\n    });\n  }"}, {"instruction": "Serialize array data linearly.", "input": "Java", "output": "public void write(DataOutput out) throws IOException {\n        if (array == null) {\n            out.write(NDARRAY_SER_VERSION_HEADER_NULL);\n            return;\n        }\n\n        INDArray toWrite;\n        if (array.isView()) {\n            toWrite = array.dup();\n        } else {\n            toWrite = array;\n        }\n\n        //Write version header: this allows us to maintain backward compatibility in the future,\n        // with features such as compression, sparse arrays or changes on the DataVec side\n        out.write(NDARRAY_SER_VERSION_HEADER);\n        Nd4j.write(toWrite, new DataOutputStream(new DataOutputWrapperStream(out)));\n    }"}, {"instruction": "Create collection.\n\n@param mongoTemplate  the mongo template\n@param collectionName the collection name\n@param dropCollection the drop collection", "input": "Java", "output": "public void createCollection(final MongoOperations mongoTemplate, final String collectionName, final boolean dropCollection) {\n        if (dropCollection) {\n            LOGGER.trace(\"Dropping database collection: [{}]\", collectionName);\n            mongoTemplate.dropCollection(collectionName);\n        }\n\n        if (!mongoTemplate.collectionExists(collectionName)) {\n            LOGGER.trace(\"Creating database collection: [{}]\", collectionName);\n            mongoTemplate.createCollection(collectionName);\n        }\n    }"}, {"instruction": "---------------------------------------------------------------------", "input": "Java", "output": "public SlotReport createSlotReport(ResourceID resourceId) {\n\t\tfinal int numberSlots = taskSlots.size();\n\n\t\tList<SlotStatus> slotStatuses = Arrays.asList(new SlotStatus[numberSlots]);\n\n\t\tfor (int i = 0; i < numberSlots; i++) {\n\t\t\tTaskSlot taskSlot = taskSlots.get(i);\n\t\t\tSlotID slotId = new SlotID(resourceId, taskSlot.getIndex());\n\n\t\t\tSlotStatus slotStatus = new SlotStatus(\n\t\t\t\tslotId,\n\t\t\t\ttaskSlot.getResourceProfile(),\n\t\t\t\ttaskSlot.getJobId(),\n\t\t\t\ttaskSlot.getAllocationId());\n\n\t\t\tslotStatuses.set(i, slotStatus);\n\t\t}\n\n\t\tfinal SlotReport slotReport = new SlotReport(slotStatuses);\n\n\t\treturn slotReport;\n\t}"}, {"instruction": "Handle registered service loaded event.\n\n@param event the event", "input": "Java", "output": "@EventListener\n    public void handleRegisteredServicesLoadedEvent(final CasRegisteredServicesLoadedEvent event) {\n        event.getServices()\n            .stream()\n            .filter(OidcRegisteredService.class::isInstance)\n            .forEach(s -> {\n                LOGGER.trace(\"Attempting to reconcile scopes and attributes for service [{}] of type [{}]\",\n                    s.getServiceId(), s.getClass().getSimpleName());\n                this.scopeToAttributesFilter.reconcile(s);\n            });\n    }"}, {"instruction": "\u904d\u5386\u67d0\u4e2a\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\u6216\u76ee\u5f55\uff0c\u4e0d\u4f1a\u9012\u5f52\u904d\u5386\n\n@param path \u904d\u5386\u67d0\u4e2a\u76ee\u5f55\u4e0b\u6240\u6709\u6587\u4ef6\u6216\u76ee\u5f55\n@param filter \u6587\u4ef6\u6216\u76ee\u5f55\u8fc7\u6ee4\u5668\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8fc7\u6ee4\u5668\u8fd4\u56de\u81ea\u5df1\u9700\u8981\u7684\u6587\u4ef6\u6216\u76ee\u5f55\u540d\u5217\u8868\n@return \u76ee\u5f55\u6216\u6587\u4ef6\u540d\u5217\u8868\n@since 4.0.5", "input": "Java", "output": "public List<String> ls(String path, final Filter<LsEntry> filter) {\r\n\t\tfinal List<String> fileNames = new ArrayList<>();\r\n\t\ttry {\r\n\t\t\tchannel.ls(path, new LsEntrySelector() {\r\n\t\t\t\t@Override\r\n\t\t\t\tpublic int select(LsEntry entry) {\r\n\t\t\t\t\tString fileName = entry.getFilename();\r\n\t\t\t\t\tif (false == StrUtil.equals(\".\", fileName) && false == StrUtil.equals(\"..\", fileName)) {\r\n\t\t\t\t\t\tif (null == filter || filter.accept(entry)) {\r\n\t\t\t\t\t\t\tfileNames.add(entry.getFilename());\r\n\t\t\t\t\t\t}\r\n\t\t\t\t\t}\r\n\t\t\t\t\treturn CONTINUE;\r\n\t\t\t\t}\r\n\t\t\t});\r\n\t\t} catch (SftpException e) {\r\n\t\t\tthrow new JschRuntimeException(e);\r\n\t\t}\r\n\t\treturn fileNames;\r\n\t}"}, {"instruction": "DES\u52a0\u5bc6\u6a21\u677f\n\n@param data           \u6570\u636e\n@param key            \u79d8\u94a5\n@param algorithm      \u52a0\u5bc6\u7b97\u6cd5\n@param transformation \u8f6c\u53d8\n@param isEncrypt      {@code true}: \u52a0\u5bc6 {@code false}: \u89e3\u5bc6\n@return \u5bc6\u6587\u6216\u8005\u660e\u6587\uff0c\u9002\u7528\u4e8eDES\uff0c3DES\uff0cAES", "input": "Java", "output": "public static byte[] desTemplate(byte[] data, byte[] key, String algorithm, String transformation, boolean isEncrypt) {\n        if (data == null || data.length == 0 || key == null || key.length == 0) return null;\n        try {\n            SecretKeySpec keySpec = new SecretKeySpec(key, algorithm);\n            Cipher        cipher  = Cipher.getInstance(transformation);\n            SecureRandom  random  = new SecureRandom();\n            cipher.init(isEncrypt ? Cipher.ENCRYPT_MODE : Cipher.DECRYPT_MODE, keySpec, random);\n            return cipher.doFinal(data);\n        } catch (Throwable e) {\n            e.printStackTrace();\n            return null;\n        }\n    }"}, {"instruction": "Convenience method to perform a reverse DNS lookup. Threads the request\nthrough a custom Runnable class in order to prevent inordinately long\nuser waits while performing reverse lookup.\n\n@param remoteIp the remote ip\n@return the remote host name", "input": "Java", "output": "protected String getRemoteHostName(final String remoteIp) {\n        val revDNS = new ReverseDNSRunnable(remoteIp);\n        val t = new Thread(revDNS);\n        t.start();\n        try {\n            t.join(this.timeout);\n        } catch (final InterruptedException e) {\n            LOGGER.debug(\"Threaded lookup failed.  Defaulting to IP [{}].\", remoteIp, e);\n        }\n        val remoteHostName = revDNS.getHostName();\n        LOGGER.debug(\"Found remote host name [{}].\", remoteHostName);\n        return StringUtils.isNotBlank(remoteHostName) ? remoteHostName : remoteIp;\n    }"}, {"instruction": "Scan {@code uri} for a session ID. This is identified by scanning for \"{code /session/}\" and\nthen extracting the next fragment of the URL. This means that both \"{@code /session/foo}\" and\n\"{@code /wd/hub/session/foo/bar}\" would both identify the session id as being \"foo\".", "input": "Java", "output": "public static Optional<String> getSessionId(String uri) {\n    int sessionIndex = uri.indexOf(\"/session/\");\n    if (sessionIndex != -1) {\n      sessionIndex += \"/session/\".length();\n      int nextSlash = uri.indexOf(\"/\", sessionIndex);\n      if (nextSlash != -1) {\n        return Optional.of(uri.substring(sessionIndex, nextSlash));\n      }\n      return Optional.of(uri.substring(sessionIndex));\n    }\n    return Optional.empty();\n  }"}, {"instruction": "This method creates compressed INDArray from Java float array, skipping usual INDArray instantiation routines\n\n@param data\n@param shape\n@param order\n@return", "input": "Java", "output": "@Override\n    public INDArray compress(float[] data, int[] shape, char order) {\n        FloatPointer pointer = new FloatPointer(data);\n\n        DataBuffer shapeInfo = Nd4j.getShapeInfoProvider().createShapeInformation(ArrayUtil.toLongArray(shape), order, DataType.FLOAT).getFirst();\n        DataBuffer buffer = compressPointer(DataTypeEx.FLOAT, pointer, data.length, 4);\n\n        return Nd4j.createArrayFromShapeBuffer(buffer, shapeInfo);\n    }"}, {"instruction": "Formats the string as {@link #format(String, Object...)}, but instead of failing on illegal format, returns the\nconcatenated format string and format arguments. Should be used for unimportant formatting like logging,\nexception messages, typically not directly.", "input": "Java", "output": "public static String nonStrictFormat(String message, Object... formatArgs)\n  {\n    if (formatArgs == null || formatArgs.length == 0) {\n      return message;\n    }\n    try {\n      return String.format(Locale.ENGLISH, message, formatArgs);\n    }\n    catch (IllegalFormatException e) {\n      StringBuilder bob = new StringBuilder(message);\n      for (Object formatArg : formatArgs) {\n        bob.append(\"; \").append(formatArg);\n      }\n      return bob.toString();\n    }\n  }"}, {"instruction": "Checks and installs all the add-ons whose installation status is {@code NOT_INSTALLED} that have (now) all required\ndependencies fulfilled.\n<p>\nShould be called after an installation of an add-on.\n\n@see #addAddOnImpl(AddOn)\n@see AddOn.InstallationStatus#NOT_INSTALLED\n@since 2.4.0", "input": "Java", "output": "private void checkAndInstallAddOnsNotInstalled() {\r\n        List<AddOn> runnableAddOns = new ArrayList<>();\r\n        for (AddOn addOn : aoc.getAddOns()) {\r\n            if (AddOn.InstallationStatus.NOT_INSTALLED == addOn.getInstallationStatus() && addOnLoaders.get(addOn.getId()) == null) {\r\n                AddOnRunRequirements reqs = addOn.calculateRunRequirements(aoc.getInstalledAddOns());\r\n                if (reqs.isRunnable()) {\r\n                    runnableAddOns.add(addOn);\r\n                }\r\n            }\r\n        }\r\n\r\n        for (AddOn addOn : runnableAddOns) {\r\n            addAddOnImpl(addOn);\r\n        }\r\n    }"}, {"instruction": "Apply L1 and L2 regularization, if necessary. Note that L1/L2 may differ for different layers in the same block\n\n@param layer        The layer to apply L1/L2 to\n@param paramName    Parameter name in the given layer\n@param gradientView Gradient view array for the layer + param\n@param paramsView   Parameter view array for the layer + param", "input": "Java", "output": "protected void applyRegularization(Regularization.ApplyStep step, Trainable layer, String paramName, INDArray gradientView, INDArray paramsView, int iter, int epoch, double lr) {\n        //TODO: do this for multiple contiguous params/layers (fewer, larger ops)\n\n        List<Regularization> l = layer.getConfig().getRegularizationByParam(paramName);\n        if(l != null && !l.isEmpty()){\n            for(Regularization r : l){\n                if(r.applyStep() == step){\n                    r.apply(paramsView, gradientView, lr, iter, epoch);\n                }\n            }\n        }\n    }"}, {"instruction": "\u8fc7\u6ee4<br>\n\u8fc7\u6ee4\u8fc7\u7a0b\u901a\u8fc7\u4f20\u5165\u7684Filter\u5b9e\u73b0\u6765\u8fc7\u6ee4\u8fd4\u56de\u9700\u8981\u7684\u5143\u7d20\u5185\u5bb9\uff0c\u8fd9\u4e2aFilter\u5b9e\u73b0\u53ef\u4ee5\u5b9e\u73b0\u4ee5\u4e0b\u529f\u80fd\uff1a\n\n<pre>\n1\u3001\u8fc7\u6ee4\u51fa\u9700\u8981\u7684\u5bf9\u8c61\uff0c{@link Filter#accept(Object)}\u65b9\u6cd5\u8fd4\u56detrue\u7684\u5bf9\u8c61\u5c06\u88ab\u52a0\u5165\u7ed3\u679c\u96c6\u5408\u4e2d\n</pre>\n\n@param <T> \u96c6\u5408\u5143\u7d20\u7c7b\u578b\n@param collection \u96c6\u5408\n@param filter \u8fc7\u6ee4\u5668\n@return \u8fc7\u6ee4\u540e\u7684\u6570\u7ec4\n@since 3.1.0", "input": "Java", "output": "public static <T> Collection<T> filter(Collection<T> collection, Filter<T> filter) {\r\n\t\tif (null == collection || null == filter) {\r\n\t\t\treturn collection;\r\n\t\t}\r\n\r\n\t\tCollection<T> collection2 = ObjectUtil.clone(collection);\r\n\t\ttry {\r\n\t\t\tcollection2.clear();\r\n\t\t} catch (UnsupportedOperationException e) {\r\n\t\t\t// \u514b\u9686\u540e\u7684\u5bf9\u8c61\u4e0d\u652f\u6301\u6e05\u7a7a\uff0c\u8bf4\u660e\u4e3a\u4e0d\u53ef\u53d8\u96c6\u5408\u5bf9\u8c61\uff0c\u4f7f\u7528\u9ed8\u8ba4\u7684ArrayList\u4fdd\u5b58\u7ed3\u679c\r\n\t\t\tcollection2 = new ArrayList<>();\r\n\t\t}\r\n\r\n\t\tfor (T t : collection) {\r\n\t\t\tif (filter.accept(t)) {\r\n\t\t\t\tcollection2.add(t);\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn collection2;\r\n\t}"}, {"instruction": "Add an ndarray to the storage\n\n@param array the array to add", "input": "Java", "output": "@Override\n    public void addUpdate(NDArrayMessage array) {\n        UnsafeBuffer directBuffer = (UnsafeBuffer) NDArrayMessage.toBuffer(array);\n        byte[] data = directBuffer.byteArray();\n        if (data == null) {\n            data = new byte[directBuffer.capacity()];\n            directBuffer.getBytes(0, data, 0, data.length);\n        }\n        byte[] key = ByteBuffer.allocate(4).putInt(size).array();\n        try {\n            db.put(key, data);\n        } catch (RocksDBException e) {\n            throw new RuntimeException(e);\n        }\n\n        size++;\n\n    }"}, {"instruction": "Return the sub task's serialized job information.\n\n@return serialized job information (may be <tt>null</tt> before a call to {@link\n#loadBigData(PermanentBlobService)}).", "input": "Java", "output": "@Nullable\n\tpublic SerializedValue<JobInformation> getSerializedJobInformation() {\n\t\tif (serializedJobInformation instanceof NonOffloaded) {\n\t\t\tNonOffloaded<JobInformation> jobInformation =\n\t\t\t\t(NonOffloaded<JobInformation>) serializedJobInformation;\n\t\t\treturn jobInformation.serializedValue;\n\t\t} else {\n\t\t\tthrow new IllegalStateException(\n\t\t\t\t\"Trying to work with offloaded serialized job information.\");\n\t\t}\n\t}"}, {"instruction": "Prepare a command line for execution from a Windows batch script.\n\nThe method quotes all arguments so that spaces are handled as expected. Quotes within arguments\nare \"double quoted\" (which is batch for escaping a quote). This page has more details about\nquoting and other batch script fun stuff: http://ss64.com/nt/syntax-esc.html", "input": "Java", "output": "private static String prepareWindowsCommand(List<String> cmd, Map<String, String> childEnv) {\n    StringBuilder cmdline = new StringBuilder();\n    for (Map.Entry<String, String> e : childEnv.entrySet()) {\n      cmdline.append(String.format(\"set %s=%s\", e.getKey(), e.getValue()));\n      cmdline.append(\" && \");\n    }\n    for (String arg : cmd) {\n      cmdline.append(quoteForBatchScript(arg));\n      cmdline.append(\" \");\n    }\n    return cmdline.toString();\n  }"}, {"instruction": "\u6839\u636e\u4e3b\u952e\u8fdb\u884c\u67e5\u8be2\n\n@param ms", "input": "Java", "output": "public String selectByPrimaryKey(MappedStatement ms) {\n        final Class<?> entityClass = getEntityClass(ms);\n        //\u5c06\u8fd4\u56de\u503c\u4fee\u6539\u4e3a\u5b9e\u4f53\u7c7b\u578b\n        setResultType(ms, entityClass);\n        StringBuilder sql = new StringBuilder();\n        sql.append(SqlHelper.selectAllColumns(entityClass));\n        sql.append(SqlHelper.fromTable(entityClass, tableName(entityClass)));\n        sql.append(SqlHelper.wherePKColumns(entityClass));\n        return sql.toString();\n    }"}, {"instruction": "Removes the given panels of given panel type from the workbench panel.\n\n@param panels the panels to remove from the workbench panel\n@param panelType the type of the panels\n@throws IllegalArgumentException if any of the parameters is {@code null}.\n@since 2.5.0\n@see #addPanels(List, PanelType)\n@see #removePanel(AbstractPanel, PanelType)", "input": "Java", "output": "public void removePanels(List<AbstractPanel> panels, PanelType panelType) {\r\n\t\tvalidateNotNull(panels, \"panels\");\r\n\t\tvalidateNotNull(panelType, \"panelType\");\r\n\r\n\t\tremovePanels(getTabbedFull(), panels);\r\n\r\n\t\tswitch (panelType) {\r\n\t\tcase SELECT:\r\n\t\t\tremovePanels(getTabbedSelect(), panels);\r\n\t\t\tbreak;\r\n\t\tcase STATUS:\r\n\t\t\tremovePanels(getTabbedStatus(), panels);\r\n\t\t\tbreak;\r\n\t\tcase WORK:\r\n\t\t\tremovePanels(getTabbedWork(), panels);\r\n\t\t\tbreak;\r\n\t\tdefault:\r\n\t\t\tbreak;\r\n\t\t}\r\n\t}"}, {"instruction": "[VARIABLE my_singer_id]", "input": "Java", "output": "public Timestamp singleUseReadOnlyTransaction(long singerId) {\n    // [START singleUseReadOnlyTransaction]\n    String column = \"FirstName\";\n    ReadOnlyTransaction txn = dbClient.singleUseReadOnlyTransaction();\n    Struct row = txn.readRow(\"Singers\", Key.of(singerId), Collections.singleton(column));\n    row.getString(column);\n    Timestamp timestamp = txn.getReadTimestamp();\n    // [END singleUseReadOnlyTransaction]\n    return timestamp;\n  }"}, {"instruction": "Invokes the given {@code script}, synchronously, as a {@link ProxyScript}, handling any {@code Exception} thrown during\nthe invocation.\n<p>\nThe context class loader of caller thread is replaced with the class loader {@code AddOnLoader} to allow the script to\naccess classes of add-ons.\n\n@param script the script to invoke.\n@param msg the HTTP message being proxied.\n@param request {@code true} if processing the request, {@code false} otherwise.\n@return {@code true} if the request should be forward to the server, {@code false} otherwise.\n@since 2.2.0\n@see #getInterface(ScriptWrapper, Class)", "input": "Java", "output": "public boolean invokeProxyScript(ScriptWrapper script, HttpMessage msg, boolean request) {\r\n\t\tvalidateScriptType(script, TYPE_PROXY);\r\n\r\n\t\tWriter writer = getWriters(script);\r\n\t\ttry {\r\n\t\t\t// Dont need to check if enabled as it can only be invoked manually\r\n\t\t\tProxyScript s = this.getInterface(script, ProxyScript.class);\r\n\t\t\t\r\n\t\t\tif (s != null) {\r\n\t\t\t\tif (request) {\r\n\t\t\t\t\treturn s.proxyRequest(msg);\r\n\t\t\t\t} else {\r\n\t\t\t\t\treturn s.proxyResponse(msg);\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t} else {\r\n\t\t\t\thandleUnspecifiedScriptError(script, writer, Constant.messages.getString(\"script.interface.proxy.error\"));\r\n\t\t\t}\r\n\t\t\r\n\t\t} catch (Exception e) {\r\n\t\t\thandleScriptException(script, writer, e);\r\n\t\t}\r\n    \t// Return true so that the request is submitted - if we returned false all proxying would fail on script errors\r\n    \treturn true;\r\n\t}"}, {"instruction": "Scan for entities with the specified annotations.\n@param annotationTypes the annotation types used on the entities\n@return a set of entity classes\n@throws ClassNotFoundException if an entity class cannot be loaded", "input": "Java", "output": "@SafeVarargs\n\tpublic final Set<Class<?>> scan(Class<? extends Annotation>... annotationTypes)\n\t\t\tthrows ClassNotFoundException {\n\t\tList<String> packages = getPackages();\n\t\tif (packages.isEmpty()) {\n\t\t\treturn Collections.emptySet();\n\t\t}\n\t\tClassPathScanningCandidateComponentProvider scanner = new ClassPathScanningCandidateComponentProvider(\n\t\t\t\tfalse);\n\t\tscanner.setEnvironment(this.context.getEnvironment());\n\t\tscanner.setResourceLoader(this.context);\n\t\tfor (Class<? extends Annotation> annotationType : annotationTypes) {\n\t\t\tscanner.addIncludeFilter(new AnnotationTypeFilter(annotationType));\n\t\t}\n\t\tSet<Class<?>> entitySet = new HashSet<>();\n\t\tfor (String basePackage : packages) {\n\t\t\tif (StringUtils.hasText(basePackage)) {\n\t\t\t\tfor (BeanDefinition candidate : scanner\n\t\t\t\t\t\t.findCandidateComponents(basePackage)) {\n\t\t\t\t\tentitySet.add(ClassUtils.forName(candidate.getBeanClassName(),\n\t\t\t\t\t\t\tthis.context.getClassLoader()));\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn entitySet;\n\t}"}, {"instruction": "Create the dimensions for the flatbuffer builder\n@param bufferBuilder the buffer builder to use\n@param arr the input array\n@return", "input": "Java", "output": "public static int createDims(FlatBufferBuilder bufferBuilder,INDArray arr) {\n        int[] tensorDimOffsets = new int[arr.rank()];\n        int[] nameOffset = new int[arr.rank()];\n        for(int i = 0; i < tensorDimOffsets.length; i++) {\n            nameOffset[i] = bufferBuilder.createString(\"\");\n            tensorDimOffsets[i] = TensorDim.createTensorDim(bufferBuilder,arr.size(i),nameOffset[i]);\n        }\n\n        return Tensor.createShapeVector(bufferBuilder,tensorDimOffsets);\n    }"}, {"instruction": "Resolve principal.\n\n@param handler    the handler name\n@param resolver   the resolver\n@param credential the credential\n@param principal  the current authenticated principal from a handler, if any.\n@return the principal", "input": "Java", "output": "protected Principal resolvePrincipal(final AuthenticationHandler handler, final PrincipalResolver resolver,\n                                         final Credential credential, final Principal principal) {\n        if (resolver.supports(credential)) {\n            try {\n                val p = resolver.resolve(credential, Optional.ofNullable(principal), Optional.ofNullable(handler));\n                LOGGER.debug(\"[{}] resolved [{}] from [{}]\", resolver, p, credential);\n                return p;\n            } catch (final Exception e) {\n                LOGGER.error(\"[{}] failed to resolve principal from [{}]\", resolver, credential, e);\n            }\n        } else {\n            LOGGER.warn(\n                \"[{}] is configured to use [{}] but it does not support [{}], which suggests a configuration problem.\",\n                handler.getName(), resolver, credential);\n        }\n        return null;\n    }"}, {"instruction": "\u552f\u4e00\u6807\u8bc6UniqueName\u7684\u4ea7\u751f\u65b9\u6cd5\uff0c\u4e3b\u8981\u7528\u4e8e\u5185\u90e8\u627e\u63a5\u53e3\u7b49\uff0c\u683c\u5f0f\u4e3ainterface:version[:uniqueId]\n\n@param interfaceConfig \u670d\u52a1\u63d0\u4f9b\u8005\u6216\u8005\u670d\u52a1\u6d88\u8d39\u8005\u914d\u7f6e\n@return \u914d\u7f6e\u552f\u4e00\u540d\u5b57", "input": "Java", "output": "public static String getUniqueName(AbstractInterfaceConfig interfaceConfig) {\n        // \u52a0\u4e0a 1.0 \u662f\u4e3a\u4e86\u517c\u5bb9\u4e4b\u524d\u7684\u7248\u672c\n        String version = interfaceConfig.getVersion();\n        String uniqueId = interfaceConfig.getUniqueId();\n        return interfaceConfig.getInterfaceId()\n            + (StringUtils.isEmpty(version) ? \":1.0\" : \":\" + version)\n            + (StringUtils.isEmpty(uniqueId) ? \"\" : \":\" + uniqueId);\n    }"}, {"instruction": "Must be called from syncContext", "input": "Java", "output": "private void shutdownNameResolverAndLoadBalancer(boolean channelIsActive) {\n    syncContext.throwIfNotInThisSynchronizationContext();\n    if (channelIsActive) {\n      checkState(nameResolverStarted, \"nameResolver is not started\");\n      checkState(lbHelper != null, \"lbHelper is null\");\n    }\n    if (nameResolver != null) {\n      cancelNameResolverBackoff();\n      nameResolver.shutdown();\n      nameResolverStarted = false;\n      if (channelIsActive) {\n        nameResolver = getNameResolver(target, nameResolverFactory, nameResolverHelper);\n      } else {\n        nameResolver = null;\n      }\n    }\n    if (lbHelper != null) {\n      lbHelper.lb.shutdown();\n      lbHelper = null;\n    }\n    subchannelPicker = null;\n  }"}, {"instruction": "Retrieve instance of {@link HystrixConcurrencyStrategy} to use based on order of precedence as defined in {@link HystrixPlugins} class header.\n<p>\nOverride default by using {@link #registerConcurrencyStrategy(HystrixConcurrencyStrategy)} or setting property (via Archaius): <code>hystrix.plugin.HystrixConcurrencyStrategy.implementation</code> with the\nfull classname to load.\n\n@return {@link HystrixConcurrencyStrategy} implementation to use", "input": "Java", "output": "public HystrixConcurrencyStrategy getConcurrencyStrategy() {\n        if (concurrencyStrategy.get() == null) {\n            // check for an implementation from Archaius first\n            Object impl = getPluginImplementation(HystrixConcurrencyStrategy.class);\n            if (impl == null) {\n                // nothing set via Archaius so initialize with default\n                concurrencyStrategy.compareAndSet(null, HystrixConcurrencyStrategyDefault.getInstance());\n                // we don't return from here but call get() again in case of thread-race so the winner will always get returned\n            } else {\n                // we received an implementation from Archaius so use it\n                concurrencyStrategy.compareAndSet(null, (HystrixConcurrencyStrategy) impl);\n            }\n        }\n        return concurrencyStrategy.get();\n    }"}, {"instruction": "\u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\n\n@param file \u6587\u4ef6\n@return \u6587\u4ef6\u5185\u5bb9\n@throws IOException \u53d1\u9001IO\u5f02\u5e38", "input": "Java", "output": "public static String file2String(File file) throws IOException {\n        if (file == null || !file.exists() || !file.isFile() || !file.canRead()) {\n            return null;\n        }\n        FileReader reader = null;\n        StringWriter writer = null;\n        try {\n            reader = new FileReader(file);\n            writer = new StringWriter();\n            char[] cbuf = new char[1024];\n            int len = 0;\n            while ((len = reader.read(cbuf)) != -1) {\n                writer.write(cbuf, 0, len);\n            }\n            return writer.toString();\n        } finally {\n            IOUtils.closeQuietly(reader);\n            IOUtils.closeQuietly(writer);\n        }\n    }"}, {"instruction": "\u5408\u5e76pipeline\u53c2\u6570\u8bbe\u7f6e", "input": "Java", "output": "public void merge(PipelineParameter pipelineParameter) {\n        try {\n            Field[] fields = this.getClass().getDeclaredFields();\n            for (int i = 0; i < fields.length; i++) {\n                Field field = fields[i];\n                // Skip static and final fields.\n                if (Modifier.isStatic(field.getModifiers()) || Modifier.isFinal(field.getModifiers())) {\n                    continue;\n                }\n\n                ReflectionUtils.makeAccessible(field);\n                Object srcValue = field.get(pipelineParameter);\n                if (srcValue != null) { // \u5ffd\u7565null\u503c\n                    field.set(this, srcValue);\n                }\n            }\n        } catch (Exception e) {\n            // ignore\n        }\n    }"}, {"instruction": "\u4ece\u767b\u8bb0\u7c3f\u4e2d\u79fb\u9664\u8def\u5f84\u5bf9\u5e94\u7684\u72b6\u6001\u4eec<br>\nRemoves from equivalenceClassMDAGNodeHashmap the entries of all the nodes in a _transition path.\n\n@param str a String corresponding to a _transition path from sourceNode", "input": "Java", "output": "private void removeTransitionPathRegisterEntries(String str)\n    {\n        MDAGNode currentNode = sourceNode;\n\n        int charCount = str.length();\n\n        for (int i = 0; i < charCount; i++)\n        {\n            currentNode = currentNode.transition(str.charAt(i));\n            if (equivalenceClassMDAGNodeHashMap.get(currentNode) == currentNode)\n                equivalenceClassMDAGNodeHashMap.remove(currentNode);\n\n            //The hashCode of an MDAGNode is cached the first time a hash is performed without a cache value present.\n            //Since we just hashed currentNode, we must clear this regardless of its presence in equivalenceClassMDAGNodeHashMap\n            //since we're not actually declaring equivalence class representatives here.\n            if (currentNode != null) currentNode.clearStoredHashCode();\n        }\n    }"}, {"instruction": "Create a composition with {@link LottieCompositionFactory}\n\n@return True if the composition is different from the previously set composition, false otherwise.", "input": "Java", "output": "public boolean setComposition(LottieComposition composition) {\n    if (this.composition == composition) {\n      return false;\n    }\n\n    isDirty = false;\n    clearComposition();\n    this.composition = composition;\n    buildCompositionLayer();\n    animator.setComposition(composition);\n    setProgress(animator.getAnimatedFraction());\n    setScale(scale);\n    updateBounds();\n\n    // We copy the tasks to a new ArrayList so that if this method is called from multiple threads,\n    // then there won't be two iterators iterating and removing at the same time.\n    Iterator<LazyCompositionTask> it = new ArrayList<>(lazyCompositionTasks).iterator();\n    while (it.hasNext()) {\n      LazyCompositionTask t = it.next();\n      t.run(composition);\n      it.remove();\n    }\n    lazyCompositionTasks.clear();\n\n    composition.setPerformanceTrackingEnabled(performanceTrackingEnabled);\n\n    return true;\n  }"}, {"instruction": "Slow path in case a line of bytes cannot be read in one #fill() operation. This is still faster\nthan creating the StrinbBuilder, String, then encoding as byte[] in Protocol, then decoding\nback into a String.", "input": "Java", "output": "private byte[] readLineBytesSlowly() {\n    ByteArrayOutputStream bout = null;\n    while (true) {\n      ensureFill();\n\n      byte b = buf[count++];\n      if (b == '\\r') {\n        ensureFill(); // Must be one more byte\n\n        byte c = buf[count++];\n        if (c == '\\n') {\n          break;\n        }\n\n        if (bout == null) {\n          bout = new ByteArrayOutputStream(16);\n        }\n\n        bout.write(b);\n        bout.write(c);\n      } else {\n        if (bout == null) {\n          bout = new ByteArrayOutputStream(16);\n        }\n\n        bout.write(b);\n      }\n    }\n\n    return bout == null ? new byte[0] : bout.toByteArray();\n  }"}, {"instruction": "\u4ece\u5b57\u8282\u6570\u7ec4\u52a0\u8f7d\uff08\u53d1\u73b0\u5728MacOS\u4e0a\uff0c\u6b64\u65b9\u6cd5\u6bd4ByteArray\u66f4\u5feb\uff09\n@param bytes\n@param offset\n@param value\n@return", "input": "Java", "output": "public boolean load(byte[] bytes, int offset, V[] value)\n    {\n        if (bytes == null) return false;\n        size = ByteUtil.bytesHighFirstToInt(bytes, offset);\n        offset += 4;\n        base = new int[size + 65535];   // \u591a\u7559\u4e00\u4e9b\uff0c\u9632\u6b62\u8d8a\u754c\n        check = new int[size + 65535];\n        for (int i = 0; i < size; i++)\n        {\n            base[i] = ByteUtil.bytesHighFirstToInt(bytes, offset);\n            offset += 4;\n            check[i] = ByteUtil.bytesHighFirstToInt(bytes, offset);\n            offset += 4;\n        }\n        v = value;\n        return true;\n    }"}, {"instruction": "\u79fb\u52a8\u6587\u4ef6\u6216\u8005\u76ee\u5f55\n\n@param src \u6e90\u6587\u4ef6\u6216\u8005\u76ee\u5f55\n@param dest \u76ee\u6807\u6587\u4ef6\u6216\u8005\u76ee\u5f55\n@param isOverride \u662f\u5426\u8986\u76d6\u76ee\u6807\uff0c\u53ea\u6709\u76ee\u6807\u4e3a\u6587\u4ef6\u624d\u8986\u76d6\n@throws IORuntimeException IO\u5f02\u5e38", "input": "Java", "output": "public static void move(File src, File dest, boolean isOverride) throws IORuntimeException {\r\n\t\t// check\r\n\t\tif (false == src.exists()) {\r\n\t\t\tthrow new IORuntimeException(\"File not found: \" + src);\r\n\t\t}\r\n\r\n\t\t// \u6765\u6e90\u4e3a\u6587\u4ef6\u5939\uff0c\u76ee\u6807\u4e3a\u6587\u4ef6\r\n\t\tif (src.isDirectory() && dest.isFile()) {\r\n\t\t\tthrow new IORuntimeException(StrUtil.format(\"Can not move directory [{}] to file [{}]\", src, dest));\r\n\t\t}\r\n\r\n\t\tif (isOverride && dest.isFile()) {// \u53ea\u6709\u76ee\u6807\u4e3a\u6587\u4ef6\u7684\u60c5\u51b5\u4e0b\u8986\u76d6\u4e4b\r\n\t\t\tdest.delete();\r\n\t\t}\r\n\r\n\t\t// \u6765\u6e90\u4e3a\u6587\u4ef6\uff0c\u76ee\u6807\u4e3a\u6587\u4ef6\u5939\r\n\t\tif (src.isFile() && dest.isDirectory()) {\r\n\t\t\tdest = new File(dest, src.getName());\r\n\t\t}\r\n\r\n\t\tif (false == src.renameTo(dest)) {\r\n\t\t\t// \u5728\u6587\u4ef6\u7cfb\u7edf\u4e0d\u540c\u7684\u60c5\u51b5\u4e0b\uff0crenameTo\u4f1a\u5931\u8d25\uff0c\u6b64\u65f6\u4f7f\u7528copy\uff0c\u7136\u540e\u5220\u9664\u539f\u6587\u4ef6\r\n\t\t\ttry {\r\n\t\t\t\tcopy(src, dest, isOverride);\r\n\t\t\t} catch (Exception e) {\r\n\t\t\t\tthrow new IORuntimeException(StrUtil.format(\"Move [{}] to [{}] failed!\", src, dest), e);\r\n\t\t\t}\r\n\t\t\t// \u590d\u5236\u540e\u5220\u9664\u6e90\r\n\t\t\tdel(src);\r\n\t\t}\r\n\t}"}, {"instruction": "Produce ticket.\n\n@param <T>                 the type parameter\n@param proxyGrantingTicket the proxy granting ticket\n@param service             the service\n@param ticketId            the ticket id\n@param clazz               the clazz\n@return the ticket", "input": "Java", "output": "protected <T extends Ticket> T produceTicket(final ProxyGrantingTicket proxyGrantingTicket,\n                                                 final Service service, final String ticketId,\n                                                 final Class<T> clazz) {\n        val expirationPolicyToUse = determineExpirationPolicyForService(service);\n        val result = proxyGrantingTicket.grantProxyTicket(\n            ticketId,\n            service,\n            expirationPolicyToUse,\n            this.onlyTrackMostRecentSession);\n\n        if (!clazz.isAssignableFrom(result.getClass())) {\n            throw new ClassCastException(\"Result [\" + result\n                + \" is of type \" + result.getClass()\n                + \" when we were expecting \" + clazz);\n        }\n        return (T) result;\n    }"}, {"instruction": "ZAP: Added type arguments.", "input": "Java", "output": "protected Hashtable<String, String> parseParameter(String param){\r\n        // ZAP: Added type arguments.\r\n        Hashtable<String, String> table = new Hashtable<>();\r\n        \r\n        try{\t  \r\n            matcher2 = pSeparator.matcher(param);\r\n            while (matcher2.find()){\r\n                // start of a request\r\n                table.put(matcher2.group(1), matcher2.group(2));\r\n                \r\n            }\r\n        } catch(Exception e){\r\n        \tlogger.error(e.getMessage(), e);\r\n        }\r\n        return table;\r\n        \r\n    }"}, {"instruction": "(and return null).", "input": "Java", "output": "public static Long attemptUUIDParseHigh(BufferedString str) {\n    final byte[] buf = str.getBuffer();\n    int i=str.getOffset();\n    if ( i== -1 ) return markBad(str);\n    long hi=0;\n    if( buf[i++]!='-' ) return markBad(str);\n    hi = get2(hi,buf,(i+=2)-2);\n    hi = get2(hi,buf,(i+=2)-2);\n    if( buf[i++]!='-' ) return markBad(str);\n    hi = get2(hi,buf,(i+=2)-2);\n    hi = get2(hi,buf,(i+=2)-2);\n    hi = get2(hi,buf,(i+=2)-2);\n    hi = get2(hi,buf,(i+=2)-2);\n    hi = get2(hi,buf,(i+=2)-2);\n    return attemptUUIDParseEnd(str, hi, buf, i);\n  }"}, {"instruction": "This method checks for something somewhere\n\n@param operands", "input": "Java", "output": "public PenaltyCause[] processOperands(INDArray... operands) {\n        if (operands == null)\n            return new PenaltyCause[] {NONE};\n\n        List<PenaltyCause> causes = new ArrayList<>();\n        for (int e = 0; e < operands.length - 1; e++) {\n            if (operands[e] == null && operands[e + 1] == null)\n                continue;\n\n            PenaltyCause lc[] = processOperands(operands[e], operands[e + 1]);\n\n            for (PenaltyCause cause : lc) {\n                if (cause != NONE && !causes.contains(cause))\n                    causes.add(cause);\n            }\n        }\n        if (causes.isEmpty())\n            causes.add(NONE);\n\n        return causes.toArray(new PenaltyCause[0]);\n    }"}, {"instruction": "This method initializes this", "input": "Java", "output": "private  void initialize() {\r\n\t\thistoryListFiltersButtonGroup = new DeselectableButtonGroup();\r\n\r\n\t\tthis.setLayout(new BorderLayout());\r\n\t    if (Model.getSingleton().getOptionsParam().getViewParam().getWmUiHandlingOption() == 0) {\r\n\t    \tthis.setSize(600, 200);\r\n\t    }\r\n\t\tthis.add(getHistoryPanel(), java.awt.BorderLayout.CENTER);\r\n\t\t\r\n\t\tthis.setDefaultAccelerator(view.getMenuShortcutKeyStroke(KeyEvent.VK_H, KeyEvent.SHIFT_DOWN_MASK, false));\r\n\t\tthis.setMnemonic(Constant.messages.getChar(\"history.panel.mnemonic\"));\r\n\t\t\r\n\t}"}, {"instruction": "\u83b7\u53d6XLSX\u5de5\u4f5c\u7c3f\u6307\u5b9asheet\u4e2d\u56fe\u7247\u5217\u8868\n\n@param workbook \u5de5\u4f5c\u7c3f{@link Workbook}\n@param sheetIndex sheet\u7684\u7d22\u5f15\n@return \u56fe\u7247\u6620\u5c04\uff0c\u952e\u683c\u5f0f\uff1a\u884c_\u5217\uff0c\u503c\uff1a{@link PictureData}", "input": "Java", "output": "private static Map<String, PictureData> getPicMapXlsx(XSSFWorkbook workbook, int sheetIndex) {\r\n\t\tfinal Map<String, PictureData> sheetIndexPicMap = new HashMap<String, PictureData>();\r\n\t\tfinal XSSFSheet sheet = workbook.getSheetAt(sheetIndex);\r\n\t\tXSSFDrawing drawing;\r\n\t\tfor (POIXMLDocumentPart dr : sheet.getRelations()) {\r\n\t\t\tif (dr instanceof XSSFDrawing) {\r\n\t\t\t\tdrawing = (XSSFDrawing) dr;\r\n\t\t\t\tfinal List<XSSFShape> shapes = drawing.getShapes();\r\n\t\t\t\tXSSFPicture pic;\r\n\t\t\t\tCTMarker ctMarker;\r\n\t\t\t\tfor (XSSFShape shape : shapes) {\r\n\t\t\t\t\tpic = (XSSFPicture) shape;\r\n\t\t\t\t\tctMarker = pic.getPreferredSize().getFrom();\r\n\t\t\t\t\tsheetIndexPicMap.put(StrUtil.format(\"{}_{}\", ctMarker.getRow(), ctMarker.getCol()), pic.getPictureData());\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn sheetIndexPicMap;\r\n\t}"}, {"instruction": "Build credential for metadata signature validation basic credential.\n\n@param resource the resource\n@return the basic credential\n@throws Exception the exception", "input": "Java", "output": "public static BasicCredential buildCredentialForMetadataSignatureValidation(final Resource resource) throws Exception {\n        try {\n            val x509FactoryBean = new BasicX509CredentialFactoryBean();\n            x509FactoryBean.setCertificateResource(resource);\n            x509FactoryBean.afterPropertiesSet();\n            return x509FactoryBean.getObject();\n        } catch (final Exception e) {\n            LOGGER.trace(e.getMessage(), e);\n\n            LOGGER.debug(\"Credential cannot be extracted from [{}] via X.509. Treating it as a public key to locate credential...\", resource);\n            val credentialFactoryBean = new BasicResourceCredentialFactoryBean();\n            credentialFactoryBean.setPublicKeyInfo(resource);\n            credentialFactoryBean.afterPropertiesSet();\n            return credentialFactoryBean.getObject();\n        }\n    }"}, {"instruction": "Allocate and return a  new array\nbased on the vertex id and weight initialization.\n@return the allocated array", "input": "Java", "output": "public INDArray storeAndAllocateNewArray() {\n        Preconditions.checkState(variableType == VariableType.VARIABLE, \"Unable to allocate and store array for variable of type %s: only\" +\n                \" VARIABLE type variables can be initialized using this method\", variableType);\n\n        if(!sameDiff.arrayAlreadyExistsForVarName(varName)){\n            long[] shape = getShape();\n            INDArray arr = getWeightInitScheme().create(dataType(), shape);\n            sameDiff.associateArrayWithVariable(arr, this);\n            if(log.isTraceEnabled()){\n                log.trace(\"Generated and stored new array for variable \\\"{}\\\": shape {}\", getVarName(), Arrays.toString(arr.shape()));\n            }\n            return arr;\n        }\n\n        //Variable type SDVariables: shape should never change (i.e., these are params in the net!)\n        INDArray ret = getArr();\n        return ret;\n    }"}, {"instruction": "Creates a CompletableFuture that will do nothing and complete after a specified delay, without using a thread during\nthe delay.\n\n@param delay           The duration of the delay (how much to wait until completing the Future).\n@param executorService An ExecutorService that will be used to complete the Future on.\n@return A CompletableFuture that will complete after the specified delay.", "input": "Java", "output": "public static CompletableFuture<Void> delayedFuture(Duration delay, ScheduledExecutorService executorService) {\n        CompletableFuture<Void> result = new CompletableFuture<>();\n        if (delay.toMillis() == 0) {\n            // Zero delay; no need to bother with scheduling a task in the future.\n            result.complete(null);\n        } else {\n            ScheduledFuture<Boolean> sf = executorService.schedule(() -> result.complete(null), delay.toMillis(), TimeUnit.MILLISECONDS);\n            result.whenComplete((r, ex) -> sf.cancel(true));\n        }\n\n        return result;\n    }"}, {"instruction": "Returns all unsaved resources of the given {@code addOns} and {@code extensions} wrapped in {@code <li>} elements or an\nempty {@code String} if there are no unsaved resources.\n\n@param addOns the add-ons that will be queried for unsaved resources\n@param extensions the extensions that will be queried for unsaved resources\n@return a {@code String} containing all unsaved resources or empty {@code String} if none\n@since 2.4.0\n@see Extension#getUnsavedResources()", "input": "Java", "output": "private static String getExtensionsUnsavedResources(Collection<AddOn> addOns, Set<Extension> extensions) {\n        List<String> unsavedResources = new ArrayList<>();\n        for (AddOn addOn : addOns) {\n            for (Extension extension : addOn.getLoadedExtensions()) {\n                if (!extension.isEnabled()) {\n                    continue;\n                }\n\n                List<String> resources = extension.getUnsavedResources();\n                if (resources != null) {\n                    unsavedResources.addAll(resources);\n                }\n            }\n        }\n        for (Extension extension : extensions) {\n            if (!extension.isEnabled()) {\n                continue;\n            }\n\n            List<String> resources = extension.getUnsavedResources();\n            if (resources != null) {\n                unsavedResources.addAll(resources);\n            }\n        }\n        return wrapEntriesInLiTags(unsavedResources);\n    }"}, {"instruction": "For the garbage collector in Java, it's better to keep new objects short-living, but once they are old enough\n(i. e. promoted to old generation), try to keep them alive. In {@link #poll()}, we fetch and deserialize all\nexisting segments each time, and then replace them in {@link #dataSources}. This method allows to use already\nexisting (old) segments when possible, effectively interning them a-la {@link String#intern} or {@link\ncom.google.common.collect.Interner}, aiming to make the majority of {@link DataSegment} objects garbage soon after\nthey are deserialized and to die in young generation. It allows to avoid fragmentation of the old generation and\nfull GCs.", "input": "Java", "output": "private DataSegment replaceWithExistingSegmentIfPresent(DataSegment segment)\n  {\n    DruidDataSource dataSource = Optional.ofNullable(dataSources).map(m -> m.get(segment.getDataSource())).orElse(null);\n    if (dataSource == null) {\n      return segment;\n    }\n    DataSegment alreadyExistingSegment = dataSource.getSegment(segment.getId());\n    return alreadyExistingSegment != null ? alreadyExistingSegment : segment;\n  }"}, {"instruction": "\u8ba1\u7b97Hash\u503c\n@param str \u88ab\u8ba1\u7b97Hash\u7684\u5b57\u7b26\u4e32\n@param k Hash\u7b97\u6cd5\u5e8f\u53f7\n@return Hash\u503c", "input": "Java", "output": "public static int hash(String str, int k) {\r\n\t\tswitch (k) {\r\n\t\t\tcase 0:\r\n\t\t\t\treturn HashUtil.rsHash(str);\r\n\t\t\tcase 1:\r\n\t\t\t\treturn HashUtil.jsHash(str);\r\n\t\t\tcase 2:\r\n\t\t\t\treturn HashUtil.elfHash(str);\r\n\t\t\tcase 3:\r\n\t\t\t\treturn HashUtil.bkdrHash(str);\r\n\t\t\tcase 4:\r\n\t\t\t\treturn HashUtil.apHash(str);\r\n\t\t\tcase 5:\r\n\t\t\t\treturn HashUtil.djbHash(str);\r\n\t\t\tcase 6:\r\n\t\t\t\treturn HashUtil.sdbmHash(str);\r\n\t\t\tcase 7:\r\n\t\t\t\treturn HashUtil.pjwHash(str);\r\n\t\t\tdefault:\r\n\t\t\t\treturn 0;\r\n\t\t}\r\n\t}"}, {"instruction": "Iterates over all active HTTP/2 streams.\n\n<p>This method must not be called outside of the event loop.", "input": "Java", "output": "final void forEachActiveStream(final Http2FrameStreamVisitor streamVisitor) throws Http2Exception {\n        assert ctx.executor().inEventLoop();\n\n        connection().forEachActiveStream(new Http2StreamVisitor() {\n            @Override\n            public boolean visit(Http2Stream stream) {\n                try {\n                    return streamVisitor.visit((Http2FrameStream) stream.getProperty(streamKey));\n                } catch (Throwable cause) {\n                    onError(ctx, false, cause);\n                    return false;\n                }\n            }\n        });\n    }"}, {"instruction": "Turn the arguments into a list.\n\n@param args the args\n@return the string[]", "input": "Java", "output": "private static String[] toResources(final Object[] args) {\n        val object = args[0];\n        if (object instanceof AuthenticationTransaction) {\n            val transaction = AuthenticationTransaction.class.cast(object);\n            return new String[]{SUPPLIED_CREDENTIALS + transaction.getCredentials()};\n        }\n        return new String[]{SUPPLIED_CREDENTIALS + CollectionUtils.wrap(object)};\n    }"}, {"instruction": "Retrieve authn request authn request.\n\n@param request the request\n@return the authn request\n@throws Exception the exception", "input": "Java", "output": "protected AuthnRequest retrieveSamlAuthenticationRequestFromHttpRequest(final HttpServletRequest request) throws Exception {\n        LOGGER.debug(\"Retrieving authentication request from scope\");\n        val requestValue = request.getParameter(SamlProtocolConstants.PARAMETER_SAML_REQUEST);\n        if (StringUtils.isBlank(requestValue)) {\n            throw new IllegalArgumentException(\"SAML request could not be determined from the authentication request\");\n        }\n        val encodedRequest = EncodingUtils.decodeBase64(requestValue.getBytes(StandardCharsets.UTF_8));\n        return (AuthnRequest) XMLObjectSupport.unmarshallFromInputStream(samlProfileHandlerConfigurationContext.getOpenSamlConfigBean().getParserPool(),\n            new ByteArrayInputStream(encodedRequest));\n    }"}, {"instruction": "Gets the JDK installation of the given name, or returns null.", "input": "Java", "output": "public JDK getJDK(String name) {\n        if(name==null) {\n            // if only one JDK is configured, \"default JDK\" should mean that JDK.\n            List<JDK> jdks = getJDKs();\n            if(jdks.size()==1)  return jdks.get(0);\n            return null;\n        }\n        for (JDK j : getJDKs()) {\n            if(j.getName().equals(name))\n                return j;\n        }\n        return null;\n    }"}, {"instruction": "Sets the negotiation type for the HTTP/2 connection.\n\n<p>If TLS is enabled a default {@link SSLSocketFactory} is created using the best\n{@link java.security.Provider} available and is NOT based on\n{@link SSLSocketFactory#getDefault}. To more precisely control the TLS configuration call\n{@link #sslSocketFactory} to override the socket factory used.\n\n<p>Default: <code>TLS</code>\n\n@deprecated use {@link #usePlaintext()} or {@link #useTransportSecurity()} instead.", "input": "Java", "output": "@Deprecated\n  public final OkHttpChannelBuilder negotiationType(io.grpc.okhttp.NegotiationType type) {\n    Preconditions.checkNotNull(type, \"type\");\n    switch (type) {\n      case TLS:\n        negotiationType = NegotiationType.TLS;\n        break;\n      case PLAINTEXT:\n        negotiationType = NegotiationType.PLAINTEXT;\n        break;\n      default:\n        throw new AssertionError(\"Unknown negotiation type: \" + type);\n    }\n    return this;\n  }"}, {"instruction": "Creates a subscription to a given topic. See the &lt;a\nhref=\"https://cloud.google.com/pubsub/docs/admin#resource_names\"&gt; resource name\nrules&lt;/a&gt;. If the subscription already exists, returns `ALREADY_EXISTS`. If the\ncorresponding topic doesn't exist, returns `NOT_FOUND`.\n\n<p>If the name is not provided in the request, the server will assign a random name for this\nsubscription on the same project as the topic, conforming to the [resource name\nformat](https://cloud.google.com/pubsub/docs/admin#resource_names). The generated name is\npopulated in the returned Subscription object. Note that for REST API requests, you must\nspecify a name in the request.\n\n<p>Sample code:\n\n<pre><code>\ntry (SubscriptionAdminClient subscriptionAdminClient = SubscriptionAdminClient.create()) {\nProjectSubscriptionName name = ProjectSubscriptionName.of(\"[PROJECT]\", \"[SUBSCRIPTION]\");\nProjectTopicName topic = ProjectTopicName.of(\"[PROJECT]\", \"[TOPIC]\");\nPushConfig pushConfig = PushConfig.newBuilder().build();\nint ackDeadlineSeconds = 0;\nSubscription response = subscriptionAdminClient.createSubscription(name.toString(), topic.toString(), pushConfig, ackDeadlineSeconds);\n}\n</code></pre>\n\n@param name The name of the subscription. It must have the format\n`\"projects/{project}/subscriptions/{subscription}\"`. `{subscription}` must start with a\nletter, and contain only letters (`[A-Za-z]`), numbers (`[0-9]`), dashes (`-`), underscores\n(`_`), periods (`.`), tildes (`~`), plus (`+`) or percent signs (`%`). It must be between 3\nand 255 characters in length, and it must not start with `\"goog\"`\n@param topic The name of the topic from which this subscription is receiving messages. Format\nis `projects/{project}/topics/{topic}`. The value of this field will be `_deleted-topic_`\nif the topic has been deleted.\n@param pushConfig If push delivery is used with this subscription, this field is used to\nconfigure it. An empty `pushConfig` signifies that the subscriber will pull and ack\nmessages using API methods.\n@param ackDeadlineSeconds The approximate amount of time (on a best-effort basis) Pub/Sub waits\nfor the subscriber to acknowledge receipt before resending the message. In the interval\nafter the message is delivered and before it is acknowledged, it is considered to be\n&lt;i&gt;outstanding&lt;/i&gt;. During that time period, the message will not be\nredelivered (on a best-effort basis).\n<p>For pull subscriptions, this value is used as the initial value for the ack deadline. To\noverride this value for a given message, call `ModifyAckDeadline` with the corresponding\n`ack_id` if using non-streaming pull or send the `ack_id` in a\n`StreamingModifyAckDeadlineRequest` if using streaming pull. The minimum custom deadline\nyou can specify is 10 seconds. The maximum custom deadline you can specify is 600 seconds\n(10 minutes). If this parameter is 0, a default value of 10 seconds is used.\n<p>For push delivery, this value is also used to set the request timeout for the call to\nthe push endpoint.\n<p>If the subscriber never acknowledges the message, the Pub/Sub system will eventually\nredeliver the message.\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "public final Subscription createSubscription(\n      String name, String topic, PushConfig pushConfig, int ackDeadlineSeconds) {\n\n    Subscription request =\n        Subscription.newBuilder()\n            .setName(name)\n            .setTopic(topic)\n            .setPushConfig(pushConfig)\n            .setAckDeadlineSeconds(ackDeadlineSeconds)\n            .build();\n    return createSubscription(request);\n  }"}, {"instruction": "Checks the current resource less than or equal with the other resource by comparing\nall the fields in the resource.\n\n@param other The resource to compare\n@return True if current resource is less than or equal with the other resource, otherwise return false.", "input": "Java", "output": "public boolean lessThanOrEqual(@Nonnull ResourceSpec other) {\n\t\tint cmp1 = Double.compare(this.cpuCores, other.cpuCores);\n\t\tint cmp2 = Integer.compare(this.heapMemoryInMB, other.heapMemoryInMB);\n\t\tint cmp3 = Integer.compare(this.directMemoryInMB, other.directMemoryInMB);\n\t\tint cmp4 = Integer.compare(this.nativeMemoryInMB, other.nativeMemoryInMB);\n\t\tint cmp5 = Integer.compare(this.stateSizeInMB, other.stateSizeInMB);\n\t\tif (cmp1 <= 0 && cmp2 <= 0 && cmp3 <= 0 && cmp4 <= 0 && cmp5 <= 0) {\n\t\t\tfor (Resource resource : extendedResources.values()) {\n\t\t\t\tif (!other.extendedResources.containsKey(resource.getName()) ||\n\t\t\t\t\tother.extendedResources.get(resource.getName()).getResourceAggregateType() != resource.getResourceAggregateType() ||\n\t\t\t\t\t\tother.extendedResources.get(resource.getName()).getValue() < resource.getValue()) {\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn true;\n\t\t}\n\t\treturn false;\n\t}"}, {"instruction": "Deflate the given string via a {@link java.util.zip.Deflater}.\n\n@param data the data\n@return base64 encoded string", "input": "Java", "output": "public static String deflate(final String data) {\n        val deflater = new Deflater();\n        deflater.setInput(data.getBytes(StandardCharsets.UTF_8));\n        deflater.finish();\n        val buffer = new byte[data.length()];\n        val resultSize = deflater.deflate(buffer);\n        val output = new byte[resultSize];\n        System.arraycopy(buffer, 0, output, 0, resultSize);\n        return EncodingUtils.encodeBase64(output);\n    }"}, {"instruction": "{@inheritDoc}.\n\nNote that the look up is case-insensitive.", "input": "Java", "output": "@Override public TopLevelItem getItem(String name) throws AccessDeniedException {\n        if (name==null)    return null;\n        TopLevelItem item = items.get(name);\n        if (item==null)\n            return null;\n        if (!item.hasPermission(Item.READ)) {\n            if (item.hasPermission(Item.DISCOVER)) {\n                throw new AccessDeniedException(\"Please login to access job \" + name);\n            }\n            return null;\n        }\n        return item;\n    }"}, {"instruction": "\u8fd4\u56de\u5305\u542b\u5b57\u4e32\u7684key<br>\nRetrieves all the Strings in the MDAG that contain a given String.\n\n@param str a String that is contained in all the desired Strings\n@return a HashSet containing all the Strings present in the MDAG that begin with {@code prefixString}", "input": "Java", "output": "public HashSet<String> getStringsWithSubstring(String str)\n    {\n        HashSet<String> strHashSet = new HashSet<String>();\n\n        if (sourceNode != null)      //if the MDAG hasn't been simplified\n            getStrings(strHashSet, SearchCondition.SUBSTRING_SEARCH_CONDITION, str, \"\", sourceNode.getOutgoingTransitions());\n        else\n            getStrings(strHashSet, SearchCondition.SUBSTRING_SEARCH_CONDITION, str, \"\", simplifiedSourceNode);\n\n        return strHashSet;\n    }"}, {"instruction": "This method initializes chkProxyChainAuth\n\n@return javax.swing.JCheckBox", "input": "Java", "output": "private JCheckBox getChkProxyChainAuth() {\r\n\t\tif (chkProxyChainAuth == null) {\r\n\t\t\tchkProxyChainAuth = new JCheckBox();\r\n\t\t\tchkProxyChainAuth.setText(Constant.messages.getString(\"conn.options.proxy.auth.required\"));\r\n\t\t\tchkProxyChainAuth.addActionListener(new java.awt.event.ActionListener() { \r\n\r\n\t\t\t\t@Override\r\n\t\t\t\tpublic void actionPerformed(java.awt.event.ActionEvent e) {    \r\n\r\n\t\t\t\t\tsetProxyChainAuthEnabled(chkProxyChainAuth.isSelected());\r\n\t\t\t\t}\r\n\t\t\t});\r\n\r\n\t\t}\r\n\t\treturn chkProxyChainAuth;\r\n\t}"}, {"instruction": "Strategy method used to create the {@link ApplicationContext}. By default this\nmethod will respect any explicitly set application context or application context\nclass before falling back to a suitable default.\n@return the application context (not yet refreshed)\n@see #setApplicationContextClass(Class)", "input": "Java", "output": "protected ConfigurableApplicationContext createApplicationContext() {\n\t\tClass<?> contextClass = this.applicationContextClass;\n\t\tif (contextClass == null) {\n\t\t\ttry {\n\t\t\t\tswitch (this.webApplicationType) {\n\t\t\t\tcase SERVLET:\n\t\t\t\t\tcontextClass = Class.forName(DEFAULT_SERVLET_WEB_CONTEXT_CLASS);\n\t\t\t\t\tbreak;\n\t\t\t\tcase REACTIVE:\n\t\t\t\t\tcontextClass = Class.forName(DEFAULT_REACTIVE_WEB_CONTEXT_CLASS);\n\t\t\t\t\tbreak;\n\t\t\t\tdefault:\n\t\t\t\t\tcontextClass = Class.forName(DEFAULT_CONTEXT_CLASS);\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (ClassNotFoundException ex) {\n\t\t\t\tthrow new IllegalStateException(\n\t\t\t\t\t\t\"Unable create a default ApplicationContext, \"\n\t\t\t\t\t\t\t\t+ \"please specify an ApplicationContextClass\",\n\t\t\t\t\t\tex);\n\t\t\t}\n\t\t}\n\t\treturn (ConfigurableApplicationContext) BeanUtils.instantiateClass(contextClass);\n\t}"}, {"instruction": "This method returns intersection point of shape border and line.\n@param shape\n@param line\n@return Point", "input": "Java", "output": "private static Point getIntersection(Shape shape,\n                                         Line2D.Double line) {\n        if (shape instanceof Ellipse2D) {\n            return getEllipseIntersection(shape,\n                                          line);\n        } else if (shape instanceof Rectangle2D || shape instanceof Path2D) {\n            return getShapeIntersection(shape,\n                                        line);\n        } else {\n            // something strange\n            return null;\n        }\n    }"}, {"instruction": "MD5\u52a0\u5bc6\u6587\u4ef6\n\n@param file \u6587\u4ef6\n@return \u6587\u4ef6\u7684MD5\u6821\u9a8c\u7801", "input": "Java", "output": "public static byte[] md5FileToByte(File file) {\n        if (file == null) return null;\n        FileInputStream   fis               = null;\n        DigestInputStream digestInputStream = null;\n        try {\n            fis = new FileInputStream(file);\n            MessageDigest md = MessageDigest.getInstance(\"MD5\");\n            digestInputStream = new DigestInputStream(fis, md);\n            byte[] buffer = new byte[256 * 1024];\n            while (digestInputStream.read(buffer) > 0) ;\n            md = digestInputStream.getMessageDigest();\n            return md.digest();\n        } catch (NoSuchAlgorithmException | IOException e) {\n            e.printStackTrace();\n            return null;\n        } finally {\n            IOKit.closeQuietly(fis);\n            IOKit.closeQuietly(digestInputStream);\n        }\n    }"}, {"instruction": "Reset the TableColumnManager to only manage the TableColumns that are\ncurrently visible in the table.\n\nGenerally this method should only be invoked by the TableColumnManager\nwhen the TableModel of the table is changed.", "input": "Java", "output": "public void reset() {\r\n\t\ttable.getColumnModel().removeColumnModelListener(this);\r\n\t\tcolumnModel = table.getColumnModel();\r\n\t\tcolumnModel.addColumnModelListener(this);\r\n\r\n\t\t// Keep a duplicate TableColumns for managing hidden TableColumns\r\n\r\n\t\tint count = columnModel.getColumnCount();\r\n\t\tallColumns = new ArrayList<>(count);\r\n\r\n\t\tfor (int i = 0; i < count; i++) {\r\n\t\t\tallColumns.add(columnModel.getColumn(i));\r\n\t\t}\r\n\t}"}, {"instruction": "Extracts Zipkin metrics to provide backward compatibility", "input": "Java", "output": "@Get(\"/metrics\")\n  @ProducesJson\n  public ObjectNode fetchMetricsFromMicrometer() {\n    ObjectNode metricsJson = factory.objectNode();\n    // Get the Zipkin Custom meters for constructing the Metrics endpoint\n    for (Meter meter : meterRegistry.getMeters()) {\n      String name = meter.getId().getName();\n      if (!name.startsWith(\"zipkin_collector\")) continue;\n      String transport = meter.getId().getTag(\"transport\");\n      if (transport == null) continue;\n      switch (meter.getId().getType()) {\n        case COUNTER:\n          metricsJson.put(\"counter.\" + name + \".\" + transport,\n            ((Counter) meter).count());\n          continue;\n        case GAUGE:\n          metricsJson.put(\"gauge.\" + name + \".\" + transport,\n            ((Gauge) meter).value());\n      }\n    }\n    return metricsJson;\n  }"}, {"instruction": "Accepts submission from the configuration page.", "input": "Java", "output": "@RequirePOST\n    public synchronized void doConfigSubmit( StaplerRequest req, StaplerResponse rsp ) throws IOException, ServletException, FormException {\n        BulkChange bc = new BulkChange(this);\n        try {\n            checkPermission(ADMINISTER);\n\n            JSONObject json = req.getSubmittedForm();\n\n            systemMessage = Util.nullify(req.getParameter(\"system_message\"));\n\n            boolean result = true;\n            for (Descriptor<?> d : Functions.getSortedDescriptorsForGlobalConfigUnclassified())\n                result &= configureDescriptor(req,json,d);\n            \n            save();\n            updateComputerList();\n            if(result)\n                FormApply.success(req.getContextPath()+'/').generateResponse(req, rsp, null);\n            else\n                FormApply.success(\"configure\").generateResponse(req, rsp, null);    // back to config\n        } finally {\n            bc.commit();\n        }\n    }"}, {"instruction": "\u8bbe\u7f6e\u6620\u5c04\u8868\n@param deprelTranslatorPath \u6620\u5c04\u8868\u8def\u5f84\n@return", "input": "Java", "output": "public IDependencyParser setDeprelTranslater(String deprelTranslatorPath)\n    {\n        deprelTranslater = GlobalObjectPool.get(deprelTranslatorPath);\n        if (deprelTranslater != null) return this;\n\n        IOUtil.LineIterator iterator = new IOUtil.LineIterator(deprelTranslatorPath);\n        deprelTranslater = new TreeMap<String, String>();\n        while (iterator.hasNext())\n        {\n            String[] args = iterator.next().split(\"\\\\s\");\n            deprelTranslater.put(args[0], args[1]);\n        }\n        if (deprelTranslater.size() == 0)\n        {\n            deprelTranslater = null;\n        }\n        GlobalObjectPool.put(deprelTranslatorPath, deprelTranslater);\n\n        return this;\n    }"}, {"instruction": "truncate\u64cd\u4f5c\n\n@param config", "input": "Java", "output": "private void truncate(BatchExecutor batchExecutor, MappingConfig config) throws SQLException {\n        DbMapping dbMapping = config.getDbMapping();\n        StringBuilder sql = new StringBuilder();\n        sql.append(\"TRUNCATE TABLE \").append(SyncUtil.getDbTableName(dbMapping));\n        batchExecutor.execute(sql.toString(), new ArrayList<>());\n        if (logger.isTraceEnabled()) {\n            logger.trace(\"Truncate target table, sql: {}\", sql);\n        }\n    }"}, {"instruction": "Generates a function call with null handling, automatic binding of session parameter, etc.", "input": "Java", "output": "public BytecodeNode generateCall(\n            String name,\n            ScalarFunctionImplementation function,\n            List<BytecodeNode> arguments,\n            Optional<OutputBlockVariableAndType> outputBlockVariableAndType)\n    {\n        Optional<BytecodeNode> instance = Optional.empty();\n        if (function.getInstanceFactory().isPresent()) {\n            FieldDefinition field = cachedInstanceBinder.getCachedInstance(function.getInstanceFactory().get());\n            instance = Optional.of(scope.getThis().getField(field));\n        }\n        return generateInvocation(scope, name, function, instance, arguments, callSiteBinder, outputBlockVariableAndType);\n    }"}, {"instruction": "This method will return a borrowed object to the bag.  Objects\nthat are borrowed from the bag but never \"requited\" will result\nin a memory leak.\n\n@param bagEntry the value to return to the bag\n@throws NullPointerException if value is null\n@throws IllegalStateException if the bagEntry was not borrowed from the bag", "input": "Java", "output": "public void requite(final T bagEntry)\n   {\n      bagEntry.setState(STATE_NOT_IN_USE);\n\n      for (int i = 0; waiters.get() > 0; i++) {\n         if (bagEntry.getState() != STATE_NOT_IN_USE || handoffQueue.offer(bagEntry)) {\n            return;\n         }\n         else if ((i & 0xff) == 0xff) {\n            parkNanos(MICROSECONDS.toNanos(10));\n         }\n         else {\n            yield();\n         }\n      }\n\n      final List<Object> threadLocalList = threadList.get();\n      if (threadLocalList.size() < 50) {\n         threadLocalList.add(weakThreadLocals ? new WeakReference<>(bagEntry) : bagEntry);\n      }\n   }"}, {"instruction": "region AbstractService Implementation", "input": "Java", "output": "@Override\n    protected void doStart() {\n        log.info(\"{}: Starting.\", this.traceObjectId);\n\n        Services.startAsync(this.durableLog, this.executor)\n                .thenComposeAsync(v -> startWhenDurableLogOnline(), this.executor)\n                .whenComplete((v, ex) -> {\n                    if (ex == null) {\n                        // We are started and ready to accept requests when DurableLog starts. All other (secondary) services\n                        // are not required for accepting new operations and can still start in the background.\n                        notifyStarted();\n                    } else {\n                        doStop(ex);\n                    }\n                });\n    }"}, {"instruction": "Generates the REST API documentation.\n\n@param args args[0] contains the directory into which the generated files are placed\n@throws IOException if any file operation failed", "input": "Java", "output": "public static void main(String[] args) throws IOException {\n\t\tString outputDirectory = args[0];\n\n\t\tfor (final RestAPIVersion apiVersion : RestAPIVersion.values()) {\n\t\t\tif (apiVersion == RestAPIVersion.V0) {\n\t\t\t\t// this version exists only for testing purposes\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tcreateHtmlFile(\n\t\t\t\tnew DocumentingDispatcherRestEndpoint(),\n\t\t\t\tapiVersion,\n\t\t\t\tPaths.get(outputDirectory, \"rest_\" + apiVersion.getURLVersionPrefix() + \"_dispatcher.html\"));\n\t\t}\n\t}"}, {"instruction": "necessary different) mode, and then a code.", "input": "Java", "output": "State latchAndAppend(int mode, int value) {\n    //assert binaryShiftByteCount == 0;\n    int bitCount = this.bitCount;\n    Token token = this.token;\n    if (mode != this.mode) {\n      int latch = HighLevelEncoder.LATCH_TABLE[this.mode][mode];\n      token = token.add(latch & 0xFFFF, latch >> 16);\n      bitCount += latch >> 16;\n    }\n    int latchModeBitCount = mode == HighLevelEncoder.MODE_DIGIT ? 4 : 5;\n    token = token.add(value, latchModeBitCount);\n    return new State(token, mode, 0, bitCount + latchModeBitCount);\n  }"}, {"instruction": "TODO: return VecDomainV4", "input": "Java", "output": "@SuppressWarnings(\"unused\") // called through reflection by RequestServer\n  public FramesV3 columnDomain(int version, FramesV3 s) {\n    Frame frame = getFromDKV(\"key\", s.frame_id.key());\n    Vec vec = frame.vec(s.column);\n    if (vec == null)\n      throw new H2OColumnNotFoundArgumentException(\"column\", s.frame_id.toString(), s.column);\n    s.domain = new String[1][];\n    s.domain[0] = vec.domain();\n    return s;\n  }"}, {"instruction": "Due to the return type change in {@link Executable} in 1.377, the caller needs a special precaution now.\n@param e Executable\n@return Discovered subtask", "input": "Java", "output": "public static @Nonnull SubTask getParentOf(@Nonnull Executable e) \n            throws Error, RuntimeException {\n        try {\n            return e.getParent();\n        } catch (AbstractMethodError ignored) { // will fallback to a private implementation\n            try {\n                Method m = e.getClass().getMethod(\"getParent\");\n                m.setAccessible(true);\n                return (SubTask) m.invoke(e);\n            } catch (IllegalAccessException x) {\n                throw (Error)new IllegalAccessError().initCause(x);\n            } catch (NoSuchMethodException x) {\n                throw (Error)new NoSuchMethodError().initCause(x);\n            } catch (InvocationTargetException x) {\n                Throwable y = x.getTargetException();\n                if (y instanceof Error)     throw (Error)y;\n                if (y instanceof RuntimeException)     throw (RuntimeException)y;\n                throw new Error(x);\n            }\n        }\n    }"}, {"instruction": "Open a file for reading OR writing. The {@link FileChannel} that is returned will only allow\nreads or writes depending on the {@link OpenOption}s that are specified. If any of the\nfollowing have been specified, the {@link FileChannel} will be write-only: {@link\nStandardOpenOption#CREATE}\n\n<ul>\n<li>{@link StandardOpenOption#CREATE}\n<li>{@link StandardOpenOption#CREATE_NEW}\n<li>{@link StandardOpenOption#WRITE}\n<li>{@link StandardOpenOption#TRUNCATE_EXISTING}\n</ul>\n\nIn all other cases the {@link FileChannel} will be read-only.\n\n@param path The path to the file to open or create\n@param options The options specifying how the file should be opened, and whether the {@link\nFileChannel} should be read-only or write-only.\n@param attrs (not supported, the values will be ignored)\n@throws IOException", "input": "Java", "output": "@Override\n  public FileChannel newFileChannel(\n      Path path, Set<? extends OpenOption> options, FileAttribute<?>... attrs) throws IOException {\n    checkNotNull(path);\n    initStorage();\n    CloudStorageUtil.checkNotNullArray(attrs);\n    if (options.contains(StandardOpenOption.CREATE_NEW)) {\n      Files.createFile(path, attrs);\n    } else if (options.contains(StandardOpenOption.CREATE) && !Files.exists(path)) {\n      Files.createFile(path, attrs);\n    }\n    if (options.contains(StandardOpenOption.WRITE)\n        || options.contains(StandardOpenOption.CREATE)\n        || options.contains(StandardOpenOption.CREATE_NEW)\n        || options.contains(StandardOpenOption.TRUNCATE_EXISTING)) {\n      return new CloudStorageWriteFileChannel(newWriteChannel(path, options));\n    } else {\n      return new CloudStorageReadFileChannel(newReadChannel(path, options));\n    }\n  }"}, {"instruction": "\u83b7\u53d6\u4e00\u4e0bbinlog format\u683c\u5f0f", "input": "Java", "output": "private void loadBinlogFormat() {\n        ResultSetPacket rs = null;\n        try {\n            rs = query(\"show variables like 'binlog_format'\");\n        } catch (IOException e) {\n            throw new CanalParseException(e);\n        }\n\n        List<String> columnValues = rs.getFieldValues();\n        if (columnValues == null || columnValues.size() != 2) {\n            logger.warn(\"unexpected binlog format query result, this may cause unexpected result, so throw exception to request network to io shutdown.\");\n            throw new IllegalStateException(\"unexpected binlog format query result:\" + rs.getFieldValues());\n        }\n\n        binlogFormat = BinlogFormat.valuesOf(columnValues.get(1));\n        if (binlogFormat == null) {\n            throw new IllegalStateException(\"unexpected binlog format query result:\" + rs.getFieldValues());\n        }\n    }"}, {"instruction": "Unregisters the KvState instance identified by the given KvStateID.\n\n@param jobId     JobId the KvState instance belongs to\n@param kvStateId KvStateID to identify the KvState instance\n@param keyGroupRange    Key group range the KvState instance belongs to", "input": "Java", "output": "public void unregisterKvState(\n\t\t\tJobID jobId,\n\t\t\tJobVertexID jobVertexId,\n\t\t\tKeyGroupRange keyGroupRange,\n\t\t\tString registrationName,\n\t\t\tKvStateID kvStateId) {\n\n\t\tKvStateEntry<?, ?, ?> entry = registeredKvStates.remove(kvStateId);\n\t\tif (entry != null) {\n\t\t\tentry.clear();\n\n\t\t\tfinal KvStateRegistryListener listener = getKvStateRegistryListener(jobId);\n\t\t\tif (listener != null) {\n\t\t\t\tlistener.notifyKvStateUnregistered(\n\t\t\t\t\t\tjobId,\n\t\t\t\t\t\tjobVertexId,\n\t\t\t\t\t\tkeyGroupRange,\n\t\t\t\t\t\tregistrationName);\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "Takes and rotates the it 90 degrees", "input": "Java", "output": "private static byte[][] rotateArray(byte[][] bitarray) {\n    byte[][] temp = new byte[bitarray[0].length][bitarray.length];\n    for (int ii = 0; ii < bitarray.length; ii++) {\n      // This makes the direction consistent on screen when rotating the\n      // screen;\n      int inverseii = bitarray.length - ii - 1;\n      for (int jj = 0; jj < bitarray[0].length; jj++) {\n        temp[jj][inverseii] = bitarray[ii][jj];\n      }\n    }\n    return temp;\n  }"}, {"instruction": "Opens the interactive CLI shell.", "input": "Java", "output": "public void open() {\n\t\tisRunning = true;\n\n\t\t// print welcome\n\t\tterminal.writer().append(CliStrings.MESSAGE_WELCOME);\n\n\t\t// begin reading loop\n\t\twhile (isRunning) {\n\t\t\t// make some space to previous command\n\t\t\tterminal.writer().append(\"\\n\");\n\t\t\tterminal.flush();\n\n\t\t\tfinal String line;\n\t\t\ttry {\n\t\t\t\tline = lineReader.readLine(prompt, null, (MaskingCallback) null, null);\n\t\t\t} catch (UserInterruptException e) {\n\t\t\t\t// user cancelled line with Ctrl+C\n\t\t\t\tcontinue;\n\t\t\t} catch (EndOfFileException | IOError e) {\n\t\t\t\t// user cancelled application with Ctrl+D or kill\n\t\t\t\tbreak;\n\t\t\t} catch (Throwable t) {\n\t\t\t\tthrow new SqlClientException(\"Could not read from command line.\", t);\n\t\t\t}\n\t\t\tif (line == null) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tfinal Optional<SqlCommandCall> cmdCall = parseCommand(line);\n\t\t\tcmdCall.ifPresent(this::callCommand);\n\t\t}\n\t}"}, {"instruction": "Resolve the JVM arguments to use.\n@return a {@link RunArguments} defining the JVM arguments", "input": "Java", "output": "protected RunArguments resolveJvmArguments() {\n\t\tStringBuilder stringBuilder = new StringBuilder();\n\t\tif (this.systemPropertyVariables != null) {\n\t\t\tstringBuilder.append(this.systemPropertyVariables.entrySet().stream()\n\t\t\t\t\t.map((e) -> SystemPropertyFormatter.format(e.getKey(), e.getValue()))\n\t\t\t\t\t.collect(Collectors.joining(\" \")));\n\t\t}\n\t\tif (this.jvmArguments != null) {\n\t\t\tstringBuilder.append(\" \").append(this.jvmArguments);\n\t\t}\n\t\treturn new RunArguments(stringBuilder.toString());\n\t}"}, {"instruction": "@param req HTTP request\n\n@return serialized user map", "input": "Java", "output": "@GET\n  @Path(\"/db/{authenticatorName}/cachedSerializedUserMap\")\n  @Produces(SmileMediaTypes.APPLICATION_JACKSON_SMILE)\n  @Consumes(MediaType.APPLICATION_JSON)\n  @ResourceFilters(BasicSecurityResourceFilter.class)\n  public Response getCachedSerializedUserMap(\n      @Context HttpServletRequest req,\n      @PathParam(\"authenticatorName\") final String authenticatorName\n  )\n  {\n    return handler.getCachedSerializedUserMap(authenticatorName);\n  }"}, {"instruction": "\u4ececontent\u4e2d\u5339\u914d\u51fa\u591a\u4e2a\u503c\u5e76\u6839\u636etemplate\u751f\u6210\u65b0\u7684\u5b57\u7b26\u4e32<br>\n\u5339\u914d\u7ed3\u675f\u540e\u4f1a\u5220\u9664\u5339\u914d\u5185\u5bb9\u4e4b\u524d\u7684\u5185\u5bb9\uff08\u5305\u62ec\u5339\u914d\u5185\u5bb9\uff09<br>\n\u4f8b\u5982\uff1a<br>\ncontent 2013\u5e745\u6708 pattern (.*?)\u5e74(.*?)\u6708 template\uff1a $1-$2 return 2013-5\n\n@param pattern \u5339\u914d\u6b63\u5219\n@param contentHolder \u88ab\u5339\u914d\u7684\u5185\u5bb9\u7684Holder\uff0cvalue\u4e3a\u5185\u5bb9\u6b63\u6587\uff0c\u7ecf\u8fc7\u8fd9\u4e2a\u65b9\u6cd5\u7684\u539f\u6587\u5c06\u88ab\u53bb\u6389\u5339\u914d\u4e4b\u524d\u7684\u5185\u5bb9\n@param template \u751f\u6210\u5185\u5bb9\u6a21\u677f\uff0c\u53d8\u91cf $1 \u8868\u793agroup1\u7684\u5185\u5bb9\uff0c\u4ee5\u6b64\u7c7b\u63a8\n@return \u65b0\u5b57\u7b26\u4e32", "input": "Java", "output": "public static String extractMultiAndDelPre(Pattern pattern, Holder<CharSequence> contentHolder, String template) {\r\n\t\tif (null == contentHolder || null == pattern || null == template) {\r\n\t\t\treturn null;\r\n\t\t}\r\n\r\n\t\tHashSet<String> varNums = findAll(PatternPool.GROUP_VAR, template, 1, new HashSet<String>());\r\n\r\n\t\tfinal CharSequence content = contentHolder.get();\r\n\t\tMatcher matcher = pattern.matcher(content);\r\n\t\tif (matcher.find()) {\r\n\t\t\tfor (String var : varNums) {\r\n\t\t\t\tint group = Integer.parseInt(var);\r\n\t\t\t\ttemplate = template.replace(\"$\" + var, matcher.group(group));\r\n\t\t\t}\r\n\t\t\tcontentHolder.set(StrUtil.sub(content, matcher.end(), content.length()));\r\n\t\t\treturn template;\r\n\t\t}\r\n\t\treturn null;\r\n\t}"}, {"instruction": "Helper to convert retention policy from RPC call to internal representation.\n\n@param policy The retention policy from RPC interface.\n@return New instance of RetentionPolicy.", "input": "Java", "output": "public static final RetentionPolicy encode(final Controller.RetentionPolicy policy) {\n        // Using default enum type of UNKNOWN(0) to detect if retention policy has been set or not.\n        // This is required since proto3 does not have any other way to detect if a field has been set or not.\n        if (policy != null && policy.getRetentionType() != Controller.RetentionPolicy.RetentionPolicyType.UNKNOWN) {\n            return RetentionPolicy.builder()\n                    .retentionType(RetentionPolicy.RetentionType.valueOf(policy.getRetentionType().name()))\n                    .retentionParam(policy.getRetentionParam())\n                    .build();\n        } else {\n            return null;\n        }\n    }"}, {"instruction": "Register spider for monitor.\n\n@param spiders spiders\n@return this\n@throws JMException JMException", "input": "Java", "output": "public synchronized SpiderMonitor register(Spider... spiders) throws JMException {\n        for (Spider spider : spiders) {\n            MonitorSpiderListener monitorSpiderListener = new MonitorSpiderListener();\n            if (spider.getSpiderListeners() == null) {\n                List<SpiderListener> spiderListeners = new ArrayList<SpiderListener>();\n                spiderListeners.add(monitorSpiderListener);\n                spider.setSpiderListeners(spiderListeners);\n            } else {\n                spider.getSpiderListeners().add(monitorSpiderListener);\n            }\n            SpiderStatusMXBean spiderStatusMBean = getSpiderStatusMBean(spider, monitorSpiderListener);\n            registerMBean(spiderStatusMBean);\n            spiderStatuses.add(spiderStatusMBean);\n        }\n        return this;\n    }"}, {"instruction": "Wraps a callable chain in a user presentable callable that will inject the default call context\nand trace the call.", "input": "Java", "output": "private <RequestT, ResponseT> UnaryCallable<RequestT, ResponseT> createUserFacingUnaryCallable(\n      String methodName, UnaryCallable<RequestT, ResponseT> inner) {\n\n    UnaryCallable<RequestT, ResponseT> traced =\n        new TracedUnaryCallable<>(\n            inner,\n            clientContext.getTracerFactory(),\n            SpanName.of(TRACING_OUTER_CLIENT_NAME, methodName));\n\n    return traced.withDefaultCallContext(clientContext.getDefaultCallContext());\n  }"}, {"instruction": "Add converters useful for most Spring Boot applications.\n@param registry the registry of converters to add to (must also be castable to\nConversionService, e.g. being a {@link ConfigurableConversionService})\n@throws ClassCastException if the given ConverterRegistry could not be cast to a\nConversionService", "input": "Java", "output": "public static void addApplicationConverters(ConverterRegistry registry) {\n\t\taddDelimitedStringConverters(registry);\n\t\tregistry.addConverter(new StringToDurationConverter());\n\t\tregistry.addConverter(new DurationToStringConverter());\n\t\tregistry.addConverter(new NumberToDurationConverter());\n\t\tregistry.addConverter(new DurationToNumberConverter());\n\t\tregistry.addConverter(new StringToDataSizeConverter());\n\t\tregistry.addConverter(new NumberToDataSizeConverter());\n\t\tregistry.addConverterFactory(new StringToEnumIgnoringCaseConverterFactory());\n\t}"}, {"instruction": "Creates a data stream from the given non-empty collection. The type of the data stream is that of the\nelements in the collection.\n\n<p>The framework will try and determine the exact type from the collection elements. In case of generic\nelements, it may be necessary to manually supply the type information via\n{@link #fromCollection(java.util.Collection, org.apache.flink.api.common.typeinfo.TypeInformation)}.\n\n<p>Note that this operation will result in a non-parallel data stream source, i.e. a data stream source with\nparallelism one.\n\n@param data\nThe collection of elements to create the data stream from.\n@param <OUT>\nThe generic type of the returned data stream.\n@return\nThe data stream representing the given collection", "input": "Java", "output": "public <OUT> DataStreamSource<OUT> fromCollection(Collection<OUT> data) {\n\t\tPreconditions.checkNotNull(data, \"Collection must not be null\");\n\t\tif (data.isEmpty()) {\n\t\t\tthrow new IllegalArgumentException(\"Collection must not be empty\");\n\t\t}\n\n\t\tOUT first = data.iterator().next();\n\t\tif (first == null) {\n\t\t\tthrow new IllegalArgumentException(\"Collection must not contain null elements\");\n\t\t}\n\n\t\tTypeInformation<OUT> typeInfo;\n\t\ttry {\n\t\t\ttypeInfo = TypeExtractor.getForObject(first);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + first.getClass()\n\t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n\t\t}\n\t\treturn fromCollection(data, typeInfo);\n\t}"}, {"instruction": "Make a prediction on a new data point using a Binomial model.\n\n@param data A new data point.\n@return The prediction.\n@throws PredictException", "input": "Java", "output": "public AnomalyDetectionPrediction predictAnomalyDetection(RowData data) throws PredictException {\n    double[] preds = preamble(ModelCategory.AnomalyDetection, data, 0.0);\n\n    AnomalyDetectionPrediction p = new AnomalyDetectionPrediction();\n    p.normalizedScore = preds[0];\n    p.score = preds[1];\n    if (enableLeafAssignment) { // only get leaf node assignment if enabled\n      SharedTreeMojoModel.LeafNodeAssignments assignments = leafNodeAssignmentExtended(data);\n      p.leafNodeAssignments = assignments._paths;\n      p.leafNodeAssignmentIds = assignments._nodeIds;\n    }\n    if (enableStagedProbabilities) {\n        double[] rawData = nanArray(m.nfeatures());\n        rawData = fillRawData(data, rawData);\n        p.stageProbabilities = ((SharedTreeMojoModel) m).scoreStagedPredictions(rawData, preds.length);\n    }\n    return p;\n  }"}, {"instruction": "This method is not intended to be used in the client.\nThe client makes a request to the server to get the {@link ExternalStorageLocation}", "input": "Java", "output": "@Override\n    public ExternalStorageLocation getLocation(Operation operation, PayloadType payloadType, String path) {\n        String uri;\n        switch (payloadType) {\n            case WORKFLOW_INPUT:\n            case WORKFLOW_OUTPUT:\n                uri = \"workflow\";\n                break;\n            case TASK_INPUT:\n            case TASK_OUTPUT:\n                uri = \"tasks\";\n                break;\n            default:\n                throw new ConductorClientException(String.format(\"Invalid payload type: %s for operation: %s\", payloadType.toString(), operation.toString()));\n        }\n        return clientBase.getForEntity(String.format(\"%s/externalstoragelocation\", uri), new Object[]{\"path\", path, \"operation\", operation.toString(), \"payloadType\", payloadType.toString()}, ExternalStorageLocation.class);\n    }"}, {"instruction": "\u5206\u8bcd\n@param text \u6587\u672c\n@return \u5206\u8bcd\u7ed3\u679c", "input": "Java", "output": "public static List<Term> segment(String text)\n    {\n        List<Term> termList = new LinkedList<Term>();\n        Matcher matcher = WEB_URL.matcher(text);\n        int begin = 0;\n        int end;\n        while (matcher.find())\n        {\n            end = matcher.start();\n            termList.addAll(SEGMENT.seg(text.substring(begin, end)));\n            termList.add(new Term(matcher.group(), Nature.xu));\n            begin = matcher.end();\n        }\n        if (begin < text.length()) termList.addAll(SEGMENT.seg(text.substring(begin)));\n\n        return termList;\n    }"}, {"instruction": "Resolves a single generic type argument for the given field.\n\n@param field The field\n@return The type argument or {@link Optional#empty()}", "input": "Java", "output": "public static Optional<Class> resolveGenericTypeArgument(Field field) {\n        Type genericType = field != null ? field.getGenericType() : null;\n        if (genericType instanceof ParameterizedType) {\n            Type[] typeArguments = ((ParameterizedType) genericType).getActualTypeArguments();\n            if (typeArguments.length > 0) {\n                Type typeArg = typeArguments[0];\n                return resolveParameterizedTypeArgument(typeArg);\n            }\n        }\n        return Optional.empty();\n    }"}, {"instruction": "/* (non-Javadoc)\n@see org.parosproxy.paros.db.paros.TableContext#getDataForContext(int)", "input": "Java", "output": "@Override\r\n\tpublic synchronized List<RecordContext> getDataForContext (int contextId) throws DatabaseException {\r\n    \ttry {\r\n\t\t\tList<RecordContext> result = new ArrayList<>();\r\n\t\t\tpsGetAllDataForContext.setInt(1, contextId);\r\n\t\t\ttry (ResultSet rs = psGetAllDataForContext.executeQuery()) {\r\n\t\t\t\twhile (rs.next()) {\r\n\t\t\t\t\tresult.add(new RecordContext(rs.getLong(DATAID), rs.getInt(CONTEXTID), rs.getInt(TYPE), rs.getString(DATA)));\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\t\r\n\t\t\treturn result;\r\n\t\t} catch (SQLException e) {\r\n\t\t\tthrow new DatabaseException(e);\r\n\t\t}\r\n    }"}, {"instruction": "Returns a future holding the serialized request result.\n\n@param jobId                     JobID of the job the queryable state\nbelongs to\n@param queryableStateName        Name under which the state is queryable\n@param keyHashCode               Integer hash code of the key (result of\na call to {@link Object#hashCode()}\n@param serializedKeyAndNamespace Serialized key and namespace to query\nKvState instance with\n@return Future holding the serialized result", "input": "Java", "output": "private CompletableFuture<KvStateResponse> getKvState(\n\t\t\tfinal JobID jobId,\n\t\t\tfinal String queryableStateName,\n\t\t\tfinal int keyHashCode,\n\t\t\tfinal byte[] serializedKeyAndNamespace) {\n\t\tLOG.debug(\"Sending State Request to {}.\", remoteAddress);\n\t\ttry {\n\t\t\tKvStateRequest request = new KvStateRequest(jobId, queryableStateName, keyHashCode, serializedKeyAndNamespace);\n\t\t\treturn client.sendRequest(remoteAddress, request);\n\t\t} catch (Exception e) {\n\t\t\tLOG.error(\"Unable to send KVStateRequest: \", e);\n\t\t\treturn FutureUtils.getFailedFuture(e);\n\t\t}\n\t}"}, {"instruction": "\u63d2\u5165\u6570\u636e<br>\n\u6b64\u65b9\u6cd5\u4e0d\u4f1a\u5173\u95edConnection\n@param conn \u6570\u636e\u5e93\u8fde\u63a5\n@param record \u8bb0\u5f55\n@return \u4e3b\u952e\u5217\u8868\n@throws SQLException SQL\u6267\u884c\u5f02\u5e38", "input": "Java", "output": "public List<Object> insertForGeneratedKeys(Connection conn, Entity record) throws SQLException {\r\n\t\tcheckConn(conn);\r\n\t\tif(CollectionUtil.isEmpty(record)){\r\n\t\t\tthrow new SQLException(\"Empty entity provided!\");\r\n\t\t}\r\n\t\t\r\n\t\tPreparedStatement ps = null;\r\n\t\ttry {\r\n\t\t\tps = dialect.psForInsert(conn, record);\r\n\t\t\tps.executeUpdate();\r\n\t\t\treturn StatementUtil.getGeneratedKeys(ps);\r\n\t\t} catch (SQLException e) {\r\n\t\t\tthrow e;\r\n\t\t} finally {\r\n\t\t\tDbUtil.close(ps);\r\n\t\t}\r\n\t}"}, {"instruction": "Get activation enum value from Keras layer configuration.\n\n@param layerConfig dictionary containing Keras layer configuration\n@return DL4J activation enum value\n@throws InvalidKerasConfigurationException     Invalid Keras config\n@throws UnsupportedKerasConfigurationException Unsupported Keras config", "input": "Java", "output": "public static Activation getActivationFromConfig(Map<String, Object> layerConfig, KerasLayerConfiguration conf)\n            throws InvalidKerasConfigurationException, UnsupportedKerasConfigurationException {\n        Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\n        if (!innerConfig.containsKey(conf.getLAYER_FIELD_ACTIVATION()))\n            throw new InvalidKerasConfigurationException(\"Keras layer is missing \"\n                    + conf.getLAYER_FIELD_ACTIVATION() + \" field\");\n        return mapToActivation((String) innerConfig.get(conf.getLAYER_FIELD_ACTIVATION()), conf);\n    }"}, {"instruction": "does not check validity of format and returns over-estimated result for invalid string (see UT)", "input": "Java", "output": "public static int estimatedBinaryLengthAsUTF8(String value)\n  {\n    int length = 0;\n    for (int i = 0; i < value.length(); i++) {\n      char var10 = value.charAt(i);\n      if (var10 < 0x80) {\n        length += 1;\n      } else if (var10 < 0x800) {\n        length += 2;\n      } else if (Character.isSurrogate(var10)) {\n        length += 4;\n        i++;\n      } else {\n        length += 3;\n      }\n    }\n    return length;\n  }"}, {"instruction": "\u7528\u4e8eChannel\u7684\u914d\u7f6e\u5f3a\u5236\u63a8\u9001\n\n@param channelId\n@param status\n@throws WebxException", "input": "Java", "output": "public void doNotify(@Param(\"pageIndex\") int pageIndex, @Param(\"searchKey\") String searchKey,\n                         @Param(\"channelId\") Long channelId, @Param(\"status\") String status, Navigator nav)\n                                                                                                           throws WebxException {\n\n        channelService.notifyChannel(channelId);\n        nav.redirectToLocation(\"channelList.htm?pageIndex=\" + pageIndex + \"&searchKey=\" + urlEncode(searchKey));\n    }"}, {"instruction": "since Erlang discerns between exit and exit/2.", "input": "Java", "output": "private void exit(final int arity, final OtpErlangPid to,\n            final OtpErlangObject reason) {\n        try {\n            final String node = to.node();\n            if (node.equals(home.node())) {\n                home.deliver(new OtpMsg(OtpMsg.exitTag, self, to, reason));\n            } else {\n                final OtpCookedConnection conn = home.getConnection(node);\n                if (conn == null) {\n                    return;\n                }\n                switch (arity) {\n                case 1:\n                    conn.exit(self, to, reason);\n                    break;\n\n                case 2:\n                    conn.exit2(self, to, reason);\n                    break;\n                }\n            }\n        } catch (final Exception e) {\n        }\n    }"}, {"instruction": "Creates a SSLEngineFactory to be used by internal communication server endpoints.", "input": "Java", "output": "public static SSLHandlerFactory createInternalServerSSLEngineFactory(final Configuration config) throws Exception {\n\t\tSSLContext sslContext = createInternalSSLContext(config);\n\t\tif (sslContext == null) {\n\t\t\tthrow new IllegalConfigurationException(\"SSL is not enabled for internal communication.\");\n\t\t}\n\n\t\treturn new SSLHandlerFactory(\n\t\t\t\tsslContext,\n\t\t\t\tgetEnabledProtocols(config),\n\t\t\t\tgetEnabledCipherSuites(config),\n\t\t\t\tfalse,\n\t\t\t\ttrue,\n\t\t\t\tconfig.getInteger(SecurityOptions.SSL_INTERNAL_HANDSHAKE_TIMEOUT),\n\t\t\t\tconfig.getInteger(SecurityOptions.SSL_INTERNAL_CLOSE_NOTIFY_FLUSH_TIMEOUT));\n\t}"}, {"instruction": "\u62f7\u8d1d\u6d41\n\n@param in \u8f93\u5165\u6d41\n@param out \u8f93\u51fa\u6d41\n@param bufferSize \u7f13\u5b58\u5927\u5c0f\n@param streamProgress \u8fdb\u5ea6\u6761\n@return \u4f20\u8f93\u7684byte\u6570\n@throws IORuntimeException IO\u5f02\u5e38", "input": "Java", "output": "public static long copy(InputStream in, OutputStream out, int bufferSize, StreamProgress streamProgress) throws IORuntimeException {\r\n\t\tAssert.notNull(in, \"InputStream is null !\");\r\n\t\tAssert.notNull(out, \"OutputStream is null !\");\r\n\t\tif (bufferSize <= 0) {\r\n\t\t\tbufferSize = DEFAULT_BUFFER_SIZE;\r\n\t\t}\r\n\r\n\t\tbyte[] buffer = new byte[bufferSize];\r\n\t\tif (null != streamProgress) {\r\n\t\t\tstreamProgress.start();\r\n\t\t}\r\n\t\tlong size = 0;\r\n\t\ttry {\r\n\t\t\tfor (int readSize = -1; (readSize = in.read(buffer)) != EOF;) {\r\n\t\t\t\tout.write(buffer, 0, readSize);\r\n\t\t\t\tsize += readSize;\r\n\t\t\t\tout.flush();\r\n\t\t\t\tif (null != streamProgress) {\r\n\t\t\t\t\tstreamProgress.progress(size);\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t} catch (IOException e) {\r\n\t\t\tthrow new IORuntimeException(e);\r\n\t\t}\r\n\t\tif (null != streamProgress) {\r\n\t\t\tstreamProgress.finish();\r\n\t\t}\r\n\t\treturn size;\r\n\t}"}, {"instruction": "Create a {@link LinkedHashMap} from an array of values.\n\n@param values The values\n@return The created map", "input": "Java", "output": "@UsedByGeneratedCode\n    public static Map mapOf(Object... values) {\n        int len = values.length;\n        if (len % 2 != 0) {\n            throw new IllegalArgumentException(\"Number of arguments should be an even number representing the keys and values\");\n        }\n\n        Map answer = new LinkedHashMap(len / 2);\n        int i = 0;\n        while (i < values.length - 1) {\n            answer.put(values[i++], values[i++]);\n        }\n        return answer;\n    }"}, {"instruction": "Attaches an existing Disk resource to an instance. You must first create the disk before you\ncan attach it. It is not possible to create and attach a disk at the same time. For more\ninformation, read Adding a persistent disk to your instance.\n\n<p>Sample code:\n\n<pre><code>\ntry (InstanceClient instanceClient = InstanceClient.create()) {\nProjectZoneInstanceName instance = ProjectZoneInstanceName.of(\"[PROJECT]\", \"[ZONE]\", \"[INSTANCE]\");\nBoolean forceAttach = false;\nAttachedDisk attachedDiskResource = AttachedDisk.newBuilder().build();\nOperation response = instanceClient.attachDiskInstance(instance, forceAttach, attachedDiskResource);\n}\n</code></pre>\n\n@param instance The instance name for this request.\n@param forceAttach Whether to force attach the disk even if it's currently attached to another\ninstance.\n@param attachedDiskResource An instance-attached disk resource.\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "@BetaApi\n  public final Operation attachDiskInstance(\n      ProjectZoneInstanceName instance, Boolean forceAttach, AttachedDisk attachedDiskResource) {\n\n    AttachDiskInstanceHttpRequest request =\n        AttachDiskInstanceHttpRequest.newBuilder()\n            .setInstance(instance == null ? null : instance.toString())\n            .setForceAttach(forceAttach)\n            .setAttachedDiskResource(attachedDiskResource)\n            .build();\n    return attachDiskInstance(request);\n  }"}, {"instruction": "Exclusive buffer is recycled to this input channel directly and it may trigger return extra\nfloating buffer and notify increased credit to the producer.\n\n@param segment The exclusive segment of this channel.", "input": "Java", "output": "@Override\n\tpublic void recycle(MemorySegment segment) {\n\t\tint numAddedBuffers;\n\n\t\tsynchronized (bufferQueue) {\n\t\t\t// Similar to notifyBufferAvailable(), make sure that we never add a buffer\n\t\t\t// after releaseAllResources() released all buffers (see below for details).\n\t\t\tif (isReleased.get()) {\n\t\t\t\ttry {\n\t\t\t\t\tinputGate.returnExclusiveSegments(Collections.singletonList(segment));\n\t\t\t\t\treturn;\n\t\t\t\t} catch (Throwable t) {\n\t\t\t\t\tExceptionUtils.rethrow(t);\n\t\t\t\t}\n\t\t\t}\n\t\t\tnumAddedBuffers = bufferQueue.addExclusiveBuffer(new NetworkBuffer(segment, this), numRequiredBuffers);\n\t\t}\n\n\t\tif (numAddedBuffers > 0 && unannouncedCredit.getAndAdd(numAddedBuffers) == 0) {\n\t\t\tnotifyCreditAvailable();\n\t\t}\n\t}"}, {"instruction": "Get object from the storage, by key\n\n@param key", "input": "Java", "output": "@Override\n    public INDArray get(T key) {\n        try {\n            if (emulateIsAbsent)\n                lock.readLock().lock();\n\n            if (containsKey(key)) {\n                INDArray result = compressedEntries.get(key);\n\n                // TODO: we don't save decompressed entries here, but something like LRU might be good idea\n                return compressor.decompress(result);\n            } else {\n                return null;\n            }\n        } finally {\n            if (emulateIsAbsent)\n                lock.readLock().unlock();\n        }\n    }"}, {"instruction": "Only for debugging.\nPrints local timeline to stdout.\n\nTo be used in case of an error when global timeline can not be relied upon as we might not be able to talk to other nodes.", "input": "Java", "output": "static void printMyTimeLine(){\n    long [] s = TimeLine.snapshot();\n    System.err.println(\"===================================<TIMELINE>==============================================\");\n    for(int i = 0; i < TimeLine.length(); ++i) {\n      long lo = TimeLine.l0(s, i),hi = TimeLine.l8(s, i);\n      int port = (int)((lo >> 8) & 0xFFFF);\n      String op = TimeLine.send_recv(s,i) == 0?\"SEND\":\"RECV\";\n      if(!TimeLine.isEmpty(s, i) && (lo & 0xFF) == UDP.udp.exec.ordinal())\n        System.err.println(TimeLine.ms(s, i) + \": \" + op + \" \" + (((TimeLine.ns(s, i) & 4) != 0)?\"TCP\":\"UDP\")  +  TimeLine.inet(s, i) + \":\" + port + \" | \" + UDP.printx16(lo, hi));\n    }\n    System.err.println(\"===========================================================================================\");\n  }"}, {"instruction": "Gets registered service and facade.\n\n@param request the request\n@return the registered service and facade", "input": "Java", "output": "protected Pair<SamlRegisteredService, SamlRegisteredServiceServiceProviderMetadataFacade> getRegisteredServiceAndFacade(final AuthnRequest request) {\n        val issuer = SamlIdPUtils.getIssuerFromSamlObject(request);\n        LOGGER.debug(\"Located issuer [{}] from authentication context\", issuer);\n\n        val registeredService = verifySamlRegisteredService(issuer);\n\n        LOGGER.debug(\"Located SAML metadata for [{}]\", registeredService.getServiceId());\n        val adaptor = getSamlMetadataFacadeFor(registeredService, request);\n\n        if (adaptor.isEmpty()) {\n            throw new UnauthorizedServiceException(UnauthorizedServiceException.CODE_UNAUTHZ_SERVICE,\n                \"Cannot find metadata linked to \" + issuer);\n        }\n        val facade = adaptor.get();\n        return Pair.of(registeredService, facade);\n    }"}, {"instruction": "Gets a command line string, which can be passed to agent start command.\n\n@param computer Computer, for which the arguments need to be constructed.\n@return Command line arguments.\nIt may be empty if the working directory is disabled or\nif the Computer type is not {@link SlaveComputer}.", "input": "Java", "output": "@Nonnull\n    @Restricted(NoExternalUse.class)\n    public String toCommandLineString(@Nonnull SlaveComputer computer) {\n        if(disabled) {\n            return \"\";\n        }\n        \n        StringBuilder bldr = new StringBuilder();\n        bldr.append(\"-workDir \\\"\");\n        if (workDirPath == null) {\n            Slave node = computer.getNode();\n            if (node == null) {\n                // It is not possible to launch this node anyway.\n                return \"\";\n            }\n            bldr.append(node.getRemoteFS());\n        } else {\n            bldr.append(workDirPath);\n        }\n        bldr.append(\"\\\"\");\n        \n        if (!DEFAULT_INTERNAL_DIR.equals(internalDir)) {\n            bldr.append(\" -internalDir \\\"\");\n            bldr.append(internalDir);\n            bldr.append(\"\\\"\");\n        }\n        \n        if (failIfWorkDirIsMissing) {\n            bldr.append(\" -failIfWorkDirIsMissing\"); \n        }\n                \n        return bldr.toString();\n    }"}, {"instruction": "Create/load a HikariConfig from Hibernate properties.\n\n@param props a map of Hibernate properties\n@return a HikariConfig", "input": "Java", "output": "@SuppressWarnings(\"rawtypes\")\n   public static HikariConfig loadConfiguration(Map props)\n   {\n      Properties hikariProps = new Properties();\n      copyProperty(AvailableSettings.ISOLATION, props, \"transactionIsolation\", hikariProps);\n      copyProperty(AvailableSettings.AUTOCOMMIT, props, \"autoCommit\", hikariProps);\n      copyProperty(AvailableSettings.DRIVER, props, \"driverClassName\", hikariProps);\n      copyProperty(AvailableSettings.URL, props, \"jdbcUrl\", hikariProps);\n      copyProperty(AvailableSettings.USER, props, \"username\", hikariProps);\n      copyProperty(AvailableSettings.PASS, props, \"password\", hikariProps);\n\n      for (Object keyo : props.keySet()) {\n         String key = (String) keyo;\n         if (key.startsWith(CONFIG_PREFIX)) {\n            hikariProps.setProperty(key.substring(CONFIG_PREFIX.length()), (String) props.get(key));\n         }\n      }\n\n      return new HikariConfig(hikariProps);\n   }"}, {"instruction": "Start the slot pool to accept RPC calls.\n\n@param jobMasterId The necessary leader id for running the job.\n@param newJobManagerAddress for the slot requests which are sent to the resource manager\n@param componentMainThreadExecutor The main thread executor for the job master's main thread.", "input": "Java", "output": "public void start(\n\t\t@Nonnull JobMasterId jobMasterId,\n\t\t@Nonnull String newJobManagerAddress,\n\t\t@Nonnull ComponentMainThreadExecutor componentMainThreadExecutor) throws Exception {\n\n\t\tthis.jobMasterId = jobMasterId;\n\t\tthis.jobManagerAddress = newJobManagerAddress;\n\t\tthis.componentMainThreadExecutor = componentMainThreadExecutor;\n\n\t\tscheduleRunAsync(this::checkIdleSlot, idleSlotTimeout);\n\n\t\tif (log.isDebugEnabled()) {\n\t\t\tscheduleRunAsync(this::scheduledLogStatus, STATUS_LOG_INTERVAL_MS, TimeUnit.MILLISECONDS);\n\t\t}\n\t}"}, {"instruction": "Static helper that can be used to exit a {@link SpringApplication} and obtain a\ncode indicating success (0) or otherwise. Does not throw exceptions but should\nprint stack traces of any encountered. Applies the specified\n{@link ExitCodeGenerator} in addition to any Spring beans that implement\n{@link ExitCodeGenerator}. In the case of multiple exit codes the highest value\nwill be used (or if all values are negative, the lowest value will be used)\n@param context the context to close if possible\n@param exitCodeGenerators exist code generators\n@return the outcome (0 if successful)", "input": "Java", "output": "public static int exit(ApplicationContext context,\n\t\t\tExitCodeGenerator... exitCodeGenerators) {\n\t\tAssert.notNull(context, \"Context must not be null\");\n\t\tint exitCode = 0;\n\t\ttry {\n\t\t\ttry {\n\t\t\t\tExitCodeGenerators generators = new ExitCodeGenerators();\n\t\t\t\tCollection<ExitCodeGenerator> beans = context\n\t\t\t\t\t\t.getBeansOfType(ExitCodeGenerator.class).values();\n\t\t\t\tgenerators.addAll(exitCodeGenerators);\n\t\t\t\tgenerators.addAll(beans);\n\t\t\t\texitCode = generators.getExitCode();\n\t\t\t\tif (exitCode != 0) {\n\t\t\t\t\tcontext.publishEvent(new ExitCodeEvent(context, exitCode));\n\t\t\t\t}\n\t\t\t}\n\t\t\tfinally {\n\t\t\t\tclose(context);\n\t\t\t}\n\t\t}\n\t\tcatch (Exception ex) {\n\t\t\tex.printStackTrace();\n\t\t\texitCode = (exitCode != 0) ? exitCode : 1;\n\t\t}\n\t\treturn exitCode;\n\t}"}, {"instruction": "Determine consent event string.\n\n@param requestContext the request context\n@return the string", "input": "Java", "output": "protected String determineConsentEvent(final RequestContext requestContext) {\n        val webService = WebUtils.getService(requestContext);\n        val service = this.authenticationRequestServiceSelectionStrategies.resolveService(webService);\n        if (service == null) {\n            return null;\n        }\n\n        val registeredService = getRegisteredServiceForConsent(requestContext, service);\n\n        val authentication = WebUtils.getAuthentication(requestContext);\n        if (authentication == null) {\n            return null;\n        }\n\n        return isConsentRequired(service, registeredService, authentication, requestContext);\n    }"}, {"instruction": "<code>\n.google.cloud.datalabeling.v1beta1.LabelTextClassificationOperationMetadata text_classification_details = 9;\n</code>", "input": "Java", "output": "public com.google.cloud.datalabeling.v1beta1.LabelTextClassificationOperationMetadataOrBuilder\n      getTextClassificationDetailsOrBuilder() {\n    if (detailsCase_ == 9) {\n      return (com.google.cloud.datalabeling.v1beta1.LabelTextClassificationOperationMetadata)\n          details_;\n    }\n    return com.google.cloud.datalabeling.v1beta1.LabelTextClassificationOperationMetadata\n        .getDefaultInstance();\n  }"}, {"instruction": "Execute add operation boolean.\n\n@param connectionFactory the connection factory\n@param entry             the entry\n@return true/false", "input": "Java", "output": "public static boolean executeAddOperation(final ConnectionFactory connectionFactory, final LdapEntry entry) {\n        try (val connection = createConnection(connectionFactory)) {\n            val operation = new AddOperation(connection);\n            operation.execute(new AddRequest(entry.getDn(), entry.getAttributes()));\n            return true;\n        } catch (final LdapException e) {\n            LOGGER.error(e.getMessage(), e);\n        }\n        return false;\n    }"}, {"instruction": "Set the list of search domains of the resolver.\n\n@param searchDomains the search domains\n@return {@code this}", "input": "Java", "output": "public DnsNameResolverBuilder searchDomains(Iterable<String> searchDomains) {\n        checkNotNull(searchDomains, \"searchDomains\");\n\n        final List<String> list = new ArrayList<String>(4);\n\n        for (String f : searchDomains) {\n            if (f == null) {\n                break;\n            }\n\n            // Avoid duplicate entries.\n            if (list.contains(f)) {\n                continue;\n            }\n\n            list.add(f);\n        }\n\n        this.searchDomains = list.toArray(new String[0]);\n        return this;\n    }"}, {"instruction": "Build saml assertion assertion.\n\n@param authnRequest   the authn request\n@param request        the request\n@param response       the response\n@param casAssertion   the cas assertion\n@param service        the service\n@param adaptor        the adaptor\n@param binding        the binding\n@param messageContext the message context\n@return the assertion", "input": "Java", "output": "protected Assertion buildSamlAssertion(final RequestAbstractType authnRequest,\n                                           final HttpServletRequest request,\n                                           final HttpServletResponse response,\n                                           final Object casAssertion,\n                                           final SamlRegisteredService service,\n                                           final SamlRegisteredServiceServiceProviderMetadataFacade adaptor,\n                                           final String binding,\n                                           final MessageContext messageContext) {\n        return samlResponseBuilderConfigurationContext.getSamlProfileSamlAssertionBuilder()\n            .build(authnRequest, request, response, casAssertion, service, adaptor, binding, messageContext);\n    }"}, {"instruction": "This method is for compatibilty so that newer version of KafkaIndexTaskIOConfig can be read by\nold version of Druid. Note that this method returns end sequence numbers instead of start. This is because\n{@link SeekableStreamStartSequenceNumbers} didn't exist before.", "input": "Java", "output": "@JsonProperty\n  @Deprecated\n  public SeekableStreamEndSequenceNumbers<Integer, Long> getStartPartitions()\n  {\n    // Converting to start sequence numbers. This is allowed for Kafka because the start offset is always inclusive.\n    final SeekableStreamStartSequenceNumbers<Integer, Long> startSequenceNumbers = getStartSequenceNumbers();\n    return new SeekableStreamEndSequenceNumbers<>(\n        startSequenceNumbers.getStream(),\n        startSequenceNumbers.getPartitionSequenceNumberMap()\n    );\n  }"}, {"instruction": "Transform an object\nin to another object\n\n@param input the record to transform\n@return the transformed writable", "input": "Java", "output": "@Override\n    public Object map(Object input) {\n        List<Long> list = (List<Long>) input;\n        switch (mathOp) {\n            case Add:\n                long sum = 0;\n                for (Long w : list)\n                    sum += w;\n                return new LongWritable(sum);\n            case Subtract:\n                return list.get(0) - list.get(1);\n            case Multiply:\n                long product = 1;\n                for (Long w : list)\n                    product *= w;\n                return product;\n            case Divide:\n                return list.get(0) / list.get(1);\n            case Modulus:\n                return list.get(0) % list.get(1);\n            case ReverseSubtract:\n            case ReverseDivide:\n            case ScalarMin:\n            case ScalarMax:\n            default:\n                throw new RuntimeException(\"Invalid mathOp: \" + mathOp); //Should never happen\n        }\n    }"}, {"instruction": "/*\nWhen the connection fails - send exit to all local pids with links\nthrough this connection", "input": "Java", "output": "synchronized void breakLinks() {\n        if (links != null) {\n            final Link[] l = links.clearLinks();\n\n            if (l != null) {\n                final int len = l.length;\n\n                for (int i = 0; i < len; i++) {\n                    // send exit \"from\" remote pids to local ones\n                    self.deliver(new OtpMsg(OtpMsg.exitTag, l[i].remote(), l[i]\n                            .local(), new OtpErlangAtom(\"noconnection\")));\n                }\n            }\n        }\n    }"}, {"instruction": "\u65b9\u6cd5\u662f\u5426\u4e3aGetter\u65b9\u6cd5<br>\n\u5339\u914d\u89c4\u5219\u5982\u4e0b\uff08\u5ffd\u7565\u5927\u5c0f\u5199\uff09\uff1a\n\n<pre>\n\u5b57\u6bb5\u540d    -\u300b \u65b9\u6cd5\u540d\nisName  -\u300b isName\nisName  -\u300b isIsName\nisName  -\u300b getIsName\nname     -\u300b isName\nname     -\u300b getName\n</pre>\n\n@param methodName \u65b9\u6cd5\u540d\n@param fieldName \u5b57\u6bb5\u540d\n@param isBooeanField \u662f\u5426\u4e3aBoolean\u7c7b\u578b\u5b57\u6bb5\n@return \u662f\u5426\u5339\u914d", "input": "Java", "output": "private boolean isMatchGetter(String methodName, String fieldName, boolean isBooeanField) {\r\n\t\t// \u5168\u90e8\u8f6c\u4e3a\u5c0f\u5199\uff0c\u5ffd\u7565\u5927\u5c0f\u5199\u6bd4\u8f83\r\n\t\tmethodName = methodName.toLowerCase();\r\n\t\tfieldName = fieldName.toLowerCase();\r\n\r\n\t\tif (false == methodName.startsWith(\"get\") && false == methodName.startsWith(\"is\")) {\r\n\t\t\t// \u975e\u6807\u51c6Getter\u65b9\u6cd5\r\n\t\t\treturn false;\r\n\t\t}\r\n\t\tif(\"getclass\".equals(methodName)) {\r\n\t\t\t//\u8df3\u8fc7getClass\u65b9\u6cd5\r\n\t\t\treturn false;\r\n\t\t}\r\n\r\n\t\t// \u9488\u5bf9Boolean\u7c7b\u578b\u7279\u6b8a\u68c0\u67e5\r\n\t\tif (isBooeanField) {\r\n\t\t\tif (fieldName.startsWith(\"is\")) {\r\n\t\t\t\t// \u5b57\u6bb5\u5df2\u7ecf\u662fis\u5f00\u5934\r\n\t\t\t\tif (methodName.equals(fieldName) // isName -\u300b isName\r\n\t\t\t\t\t\t|| methodName.equals(\"get\" + fieldName)// isName -\u300b getIsName\r\n\t\t\t\t\t\t|| methodName.equals(\"is\" + fieldName)// isName -\u300b isIsName\r\n\t\t\t\t) {\r\n\t\t\t\t\treturn true;\r\n\t\t\t\t}\r\n\t\t\t} else if (methodName.equals(\"is\" + fieldName)) {\r\n\t\t\t\t// \u5b57\u6bb5\u975eis\u5f00\u5934\uff0c name -\u300b isName\r\n\t\t\t\treturn true;\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\t// \u5305\u62ecboolean\u7684\u4efb\u4f55\u7c7b\u578b\u53ea\u6709\u4e00\u79cd\u5339\u914d\u60c5\u51b5\uff1aname -\u300b getName\r\n\t\treturn methodName.equals(\"get\" + fieldName);\r\n\t}"}, {"instruction": "Try to instantiate the given class.\n\n@param name        The class name\n@param classLoader The class loader to use\n@return The instantiated instance or {@link Optional#empty()}", "input": "Java", "output": "public static Optional<?> tryInstantiate(String name, ClassLoader classLoader) {\n        try {\n            return ClassUtils.forName(name, classLoader)\n                .flatMap(InstantiationUtils::tryInstantiate);\n        } catch (Throwable e) {\n            Logger log = LoggerFactory.getLogger(InstantiationUtils.class);\n            if (log.isDebugEnabled()) {\n                log.debug(\"Tried, but could not instantiate type: \" + name, e);\n            }\n            return Optional.empty();\n        }\n    }"}, {"instruction": "Does any of the plugin has updates?", "input": "Java", "output": "@Exported\n    public boolean hasUpdates() {\n        Data data = getData();\n        if(data==null)      return false;\n        \n        for (PluginWrapper pw : Jenkins.getInstance().getPluginManager().getPlugins()) {\n            if(!pw.isBundled() && pw.getUpdateInfo()!=null)\n                // do not advertize updates to bundled plugins, since we generally want users to get them\n                // as a part of jenkins.war updates. This also avoids unnecessary pinning of plugins. \n                return true;\n        }\n        return false;\n    }"}, {"instruction": "Sets the new policy.\n\n@param policy\nthe new policy\n\n@throws NullPointerException\nif policy is {@code null}\n\n@see UndoManagerPolicy", "input": "Java", "output": "public final void setUndoManagerPolicy(UndoManagerPolicy policy) throws NullPointerException {\n\t\tif (policy == null) {\n\t\t\tthrow new NullPointerException(\"The policy must not be null.\");\n\t\t}\n\t\t\n\t\tif (this.policy == policy) {\n\t\t\treturn;\n\t\t}\n\n\t\tfinal UndoManagerPolicy oldPolicy = this.policy;\n\t\tthis.policy = policy;\n\t\t\n\t\tif (oldPolicy == UndoManagerPolicy.DEFAULT) {\n\t\t\tthis.textComponent.removePropertyChangeListener(\"editable\", this);\n\t\t\tthis.textComponent.removePropertyChangeListener(\"enabled\", this);\n\t\t}\n\n\t\tif (this.policy == UndoManagerPolicy.DEFAULT) {\n\t\t\tthis.textComponent.addPropertyChangeListener(\"editable\", this);\n\t\t\tthis.textComponent.addPropertyChangeListener(\"enabled\", this);\n\t\t}\n\n\t\thandleUndoManagerPolicy();\n\t}"}, {"instruction": "Gets the serializer that recognizes the previous serialization schema of the state.\nThis is the serializer that should be used for restoring the state, i.e. when the state\nis still in the previous serialization schema.\n\n<p>This method only returns a serializer if this provider has the previous serializer's\nsnapshot. Otherwise, trying to access the previous schema serializer will fail\nwith an exception.\n\n@return a serializer that reads and writes in the previous schema of the state.", "input": "Java", "output": "@Nonnull\n\tpublic final TypeSerializer<T> previousSchemaSerializer() {\n\t\tif (cachedRestoredSerializer != null) {\n\t\t\treturn cachedRestoredSerializer;\n\t\t}\n\n\t\tif (previousSerializerSnapshot == null) {\n\t\t\tthrow new UnsupportedOperationException(\n\t\t\t\t\"This provider does not contain the state's previous serializer's snapshot. Cannot provider a serializer for previous schema.\");\n\t\t}\n\n\t\tthis.cachedRestoredSerializer = previousSerializerSnapshot.restoreSerializer();\n\t\treturn cachedRestoredSerializer;\n\t}"}, {"instruction": "Compute the ideal deadline, set subsequent modacks to this deadline, and return it.", "input": "Java", "output": "@InternalApi\n  int computeDeadlineSeconds() {\n    int sec = ackLatencyDistribution.getPercentile(PERCENTILE_FOR_ACK_DEADLINE_UPDATES);\n\n    // Use Ints.constrainToRange when we get guava 21.\n    if (sec < Subscriber.MIN_ACK_DEADLINE_SECONDS) {\n      sec = Subscriber.MIN_ACK_DEADLINE_SECONDS;\n    } else if (sec > Subscriber.MAX_ACK_DEADLINE_SECONDS) {\n      sec = Subscriber.MAX_ACK_DEADLINE_SECONDS;\n    }\n    return sec;\n  }"}, {"instruction": "Converts a 128 bit array into a UUID.\nCopied from UUID's private constructor.", "input": "Java", "output": "@VisibleForTesting\n    static UUID bytesToUUID(byte[] data) {\n        long msb = 0;\n        long lsb = 0;\n        assert data.length == 16 : \"data must be 16 bytes in length\";\n        for (int i = 0; i < 8; i++) {\n            msb = (msb << 8) | (data[i] & 0xff);\n        }\n        for (int i = 8; i < 16; i++) {\n            lsb = (lsb << 8) | (data[i] & 0xff);\n        }\n        return new UUID(msb, lsb);\n    }"}, {"instruction": "\u52a0\u5bc6\n\n@param data \u6570\u636e\n@param cipherKey \u5bc6\u94a5\n@return \u5bc6\u6587", "input": "Java", "output": "public static String encrypt(CharSequence data, CharSequence cipherKey) {\r\n\t\tfinal int dataLen = data.length();\r\n\t\tfinal int cipherKeyLen = cipherKey.length();\r\n\r\n\t\tfinal char[] cipherArray = new char[dataLen];\r\n\t\tfor (int i = 0; i < dataLen / cipherKeyLen + 1; i++) {\r\n\t\t\tfor (int t = 0; t < cipherKeyLen; t++) {\r\n\t\t\t\tif (t + i * cipherKeyLen < dataLen) {\r\n\t\t\t\t\tfinal char dataChar = data.charAt(t + i * cipherKeyLen);\r\n\t\t\t\t\tfinal char cipherKeyChar = cipherKey.charAt(t);\r\n\t\t\t\t\tcipherArray[t + i * cipherKeyLen] = (char) ((dataChar + cipherKeyChar - 64) % 95 + 32);\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\treturn String.valueOf(cipherArray);\r\n\t}"}, {"instruction": "\u6839\u636e\u5bc6\u94a5\u7c7b\u578b\u83b7\u5f97\u76f8\u5e94\u5bc6\u94a5\n\n@param type \u7c7b\u578b {@link KeyType}\n@return {@link Key}", "input": "Java", "output": "protected Key getKeyByType(KeyType type) {\r\n\t\tswitch (type) {\r\n\t\tcase PrivateKey:\r\n\t\t\tif (null == this.privateKey) {\r\n\t\t\t\tthrow new NullPointerException(\"Private key must not null when use it !\");\r\n\t\t\t}\r\n\t\t\treturn this.privateKey;\r\n\t\tcase PublicKey:\r\n\t\t\tif (null == this.publicKey) {\r\n\t\t\t\tthrow new NullPointerException(\"Public key must not null when use it !\");\r\n\t\t\t}\r\n\t\t\treturn this.publicKey;\r\n\t\t}\r\n\t\tthrow new CryptoException(\"Uknown key type: \" + type);\r\n\t}"}, {"instruction": "only for Jelly", "input": "Java", "output": "@Restricted(NoExternalUse.class)\n    public Collection<TokenInfoAndStats> getTokenList() {\n        return tokenStore.getTokenListSortedByName()\n                .stream()\n                .map(token -> {\n                    ApiTokenStats.SingleTokenStats stats = tokenStats.findTokenStatsById(token.getUuid());\n                    return new TokenInfoAndStats(token, stats);\n                })\n                .collect(Collectors.toList());\n    }"}, {"instruction": "GRPC_PROXY_EXP is deprecated but let's maintain compatibility for now.", "input": "Java", "output": "private static InetSocketAddress overrideProxy(String proxyHostPort) {\n    if (proxyHostPort == null) {\n      return null;\n    }\n\n    String[] parts = proxyHostPort.split(\":\", 2);\n    int port = 80;\n    if (parts.length > 1) {\n      port = Integer.parseInt(parts[1]);\n    }\n    log.warning(\n        \"Detected GRPC_PROXY_EXP and will honor it, but this feature will \"\n            + \"be removed in a future release. Use the JVM flags \"\n            + \"\\\"-Dhttps.proxyHost=HOST -Dhttps.proxyPort=PORT\\\" to set the https proxy for \"\n            + \"this JVM.\");\n    return new InetSocketAddress(parts[0], port);\n  }"}, {"instruction": "Start a new workflow.  Returns the ID of the workflow instance that can be later used for tracking.\n\n@param name          Name of the workflow you want to start.\n@param version       Version of the workflow you want to start.\n@param correlationId CorrelationID of the workflow you want to start.\n@param input         Input to the workflow you want to start.\n@return the id of the workflow instance that can be use for tracking.", "input": "Java", "output": "@Service\n    public String startWorkflow(String name, Integer version, String correlationId, Map<String, Object> input) {\n        WorkflowDef workflowDef = metadataService.getWorkflowDef(name, version);\n        if (workflowDef == null) {\n            throw new ApplicationException(ApplicationException.Code.NOT_FOUND, String.format(\"No such workflow found by name: %s, version: %d\", name, version));\n        }\n        return workflowExecutor.startWorkflow(workflowDef.getName(), workflowDef.getVersion(), correlationId, input, null);\n    }"}, {"instruction": "Deletes an access config from an instance's network interface.\n\n<p>Sample code:\n\n<pre><code>\ntry (InstanceClient instanceClient = InstanceClient.create()) {\nProjectZoneInstanceName instance = ProjectZoneInstanceName.of(\"[PROJECT]\", \"[ZONE]\", \"[INSTANCE]\");\nString networkInterface = \"\";\nString accessConfig = \"\";\nOperation response = instanceClient.deleteAccessConfigInstance(instance, networkInterface, accessConfig);\n}\n</code></pre>\n\n@param instance The instance name for this request.\n@param networkInterface The name of the network interface.\n@param accessConfig The name of the access config to delete.\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "@BetaApi\n  public final Operation deleteAccessConfigInstance(\n      ProjectZoneInstanceName instance, String networkInterface, String accessConfig) {\n\n    DeleteAccessConfigInstanceHttpRequest request =\n        DeleteAccessConfigInstanceHttpRequest.newBuilder()\n            .setInstance(instance == null ? null : instance.toString())\n            .setNetworkInterface(networkInterface)\n            .setAccessConfig(accessConfig)\n            .build();\n    return deleteAccessConfigInstance(request);\n  }"}, {"instruction": "Produce ticket.\n\n@param <T>                  the type parameter\n@param ticketGrantingTicket the ticket granting ticket\n@param service              the service\n@param credentialProvided   the credential provided\n@param ticketId             the ticket id\n@param clazz                the clazz\n@return the ticket", "input": "Java", "output": "protected <T extends Ticket> T produceTicket(final TicketGrantingTicket ticketGrantingTicket,\n                                                 final Service service,\n                                                 final boolean credentialProvided,\n                                                 final String ticketId,\n                                                 final Class<T> clazz) {\n        val expirationPolicyToUse = determineExpirationPolicyForService(service);\n\n        val result = ticketGrantingTicket.grantServiceTicket(\n            ticketId,\n            service,\n            expirationPolicyToUse,\n            credentialProvided,\n            trackMostRecentSession);\n\n        if (!clazz.isAssignableFrom(result.getClass())) {\n            throw new ClassCastException(\"Result [\" + result\n                + \" is of type \" + result.getClass()\n                + \" when we were expecting \" + clazz);\n        }\n        return (T) result;\n    }"}, {"instruction": "Populates the task input from external payload storage if the external storage path is specified.\n\n@param task the task for which the input is to be populated.", "input": "Java", "output": "private void populateTaskInput(Task task) {\n        if (StringUtils.isNotBlank(task.getExternalInputPayloadStoragePath())) {\n            WorkflowTaskMetrics.incrementExternalPayloadUsedCount(task.getTaskDefName(), ExternalPayloadStorage.Operation.READ.name(), ExternalPayloadStorage.PayloadType.TASK_INPUT.name());\n            task.setInputData(downloadFromExternalStorage(ExternalPayloadStorage.PayloadType.TASK_INPUT, task.getExternalInputPayloadStoragePath()));\n            task.setExternalInputPayloadStoragePath(null);\n        }\n    }"}, {"instruction": "Gets an optional enum param, returning {@code null} if the parameter was not found.\n\n@param <E> the type of the enum that will be returned\n@param params the params\n@param paramName the param name\n@param enumType the type of the enum\n@return the enum, or {@code null}\n@throws ApiException if the param value does not match any of the possible enum values", "input": "Java", "output": "public static <E extends Enum<E>> E getOptionalEnumParam(JSONObject params, String paramName,\n\t\t\tClass<E> enumType) throws ApiException {\n\t\tString enumValS = params.optString(paramName, null);\n\t\tE enumVal = null;\n\t\tif (enumValS != null && !enumValS.isEmpty()) {\n\t\t\ttry {\n\t\t\t\tenumVal = Enum.valueOf(enumType, enumValS);\n\t\t\t} catch (Exception ex) {\n\t\t\t\tthrow new ApiException(ApiException.Type.ILLEGAL_PARAMETER, paramName + \": \"\n\t\t\t\t\t\t+ ex.getLocalizedMessage());\n\t\t\t}\n\t\t}\n\t\treturn enumVal;\n\t}"}, {"instruction": "This method will either generate a new random sessionId or will retrieve the value stored\nin the \"io\" cookie.  Failures to parse will cause a logging warning to be generated and a\nrandom uuid to be generated instead (same as not passing a cookie in the first place).", "input": "Java", "output": "private UUID generateOrGetSessionIdFromRequest(HttpHeaders headers) {\n        List<String> values = headers.getAll(\"io\");\n        if (values.size() == 1) {\n            try {\n                return UUID.fromString(values.get(0));\n            } catch (IllegalArgumentException iaex) {\n                log.warn(\"Malformed UUID received for session! io=\" + values.get(0));\n            }\n        }\n\n        for (String cookieHeader : headers.getAll(HttpHeaderNames.COOKIE)) {\n            Set<Cookie> cookies = ServerCookieDecoder.LAX.decode(cookieHeader);\n\n            for (Cookie cookie : cookies) {\n                if (cookie.name().equals(\"io\")) {\n                    try {\n                        return UUID.fromString(cookie.value());\n                    } catch (IllegalArgumentException iaex) {\n                        log.warn(\"Malformed UUID received for session! io=\" + cookie.value());\n                    }\n                }\n            }\n        }\n\n        return UUID.randomUUID();\n    }"}, {"instruction": "\u5c06setting\u4e2d\u7684\u952e\u503c\u5173\u7cfb\u6620\u5c04\u5230\u5bf9\u8c61\u4e2d\uff0c\u539f\u7406\u662f\u8c03\u7528\u5bf9\u8c61\u5bf9\u5e94\u7684set\u65b9\u6cd5<br>\n\u53ea\u652f\u6301\u57fa\u672c\u7c7b\u578b\u7684\u8f6c\u6362\n\n@param group \u5206\u7ec4\n@param bean Bean\u5bf9\u8c61\n@return Bean", "input": "Java", "output": "public Object toBean(final String group, Object bean) {\r\n\t\treturn BeanUtil.fillBean(bean, new ValueProvider<String>(){\r\n\r\n\t\t\t@Override\r\n\t\t\tpublic Object value(String key, Type valueType) {\r\n\t\t\t\tfinal String value = getByGroup(key, group);\r\n//\t\t\t\tif (null != value) {\r\n//\t\t\t\t\tlog.debug(\"Parse setting to object field [{}={}]\", key, value);\r\n//\t\t\t\t}\r\n\t\t\t\treturn value;\r\n\t\t\t}\r\n\r\n\t\t\t@Override\r\n\t\t\tpublic boolean containsKey(String key) {\r\n\t\t\t\treturn null != getByGroup(key, group);\r\n\t\t\t}\r\n\t\t}, CopyOptions.create());\r\n\t}"}, {"instruction": "Translates the properties as much as possible, and truncates at the first non-translatable property", "input": "Java", "output": "public static <X, Y> List<LocalProperty<Y>> translate(List<? extends LocalProperty<X>> properties, Function<X, Optional<Y>> translator)\n    {\n        properties = normalizeAndPrune(properties);\n\n        ImmutableList.Builder<LocalProperty<Y>> builder = ImmutableList.builder();\n        for (LocalProperty<X> property : properties) {\n            Optional<LocalProperty<Y>> translated = property.translate(translator);\n            if (translated.isPresent()) {\n                builder.add(translated.get());\n            }\n            else if (!(property instanceof ConstantProperty)) {\n                break; // Only break if we fail to translate non-constants\n            }\n        }\n\n        return builder.build();\n    }"}, {"instruction": "/*\nReturns a unique name for the given script name", "input": "Java", "output": "private String getUniqueScriptName(String name, String ext) {\r\n\t\tif (this.getScriptImpl(name) == null) {\r\n\t\t\t// Its unique\r\n\t\t\treturn name;\r\n\t\t}\r\n\t\t// Its not unique, add a suitable index...\r\n\t\tString stub = name.substring(0, name.length() - ext.length() - 1);\r\n\t\tint index = 1;\r\n\t\tdo {\r\n\t\t\tindex++;\r\n\t\t\tname = stub + \"(\" + index + \").\" + ext;\r\n\t\t}\r\n\t\twhile (this.getScriptImpl(name) != null);\r\n\t\t\r\n\t\treturn name;\r\n\t}"}, {"instruction": "Make a TypeVariableName for the given TypeMirror. This form is used internally to avoid\ninfinite recursion in cases like {@code Enum<E extends Enum<E>>}. When we encounter such a\nthing, we will make a TypeVariableName without bounds and add that to the {@code typeVariables}\nmap before looking up the bounds. Then if we encounter this TypeVariable again while\nconstructing the bounds, we can just return it from the map. And, the code that put the entry\nin {@code variables} will make sure that the bounds are filled in before returning.", "input": "Java", "output": "static TypeVariableName get(\n      TypeVariable mirror, Map<TypeParameterElement, TypeVariableName> typeVariables) {\n    TypeParameterElement element = (TypeParameterElement) mirror.asElement();\n    TypeVariableName typeVariableName = typeVariables.get(element);\n    if (typeVariableName == null) {\n      // Since the bounds field is public, we need to make it an unmodifiableList. But we control\n      // the List that that wraps, which means we can change it before returning.\n      List<TypeName> bounds = new ArrayList<>();\n      List<TypeName> visibleBounds = Collections.unmodifiableList(bounds);\n      typeVariableName = new TypeVariableName(element.getSimpleName().toString(), visibleBounds);\n      typeVariables.put(element, typeVariableName);\n      for (TypeMirror typeMirror : element.getBounds()) {\n        bounds.add(TypeName.get(typeMirror, typeVariables));\n      }\n      bounds.remove(OBJECT);\n    }\n    return typeVariableName;\n  }"}, {"instruction": "set bit from segments.\n\n@param segments target segments.\n@param baseOffset bits base offset.\n@param index bit index from base offset.", "input": "Java", "output": "public static void bitSet(MemorySegment[] segments, int baseOffset, int index) {\n\t\tif (segments.length == 1) {\n\t\t\tint offset = baseOffset + ((index & BIT_BYTE_POSITION_MASK) >>> 3);\n\t\t\tMemorySegment segment = segments[0];\n\t\t\tbyte current = segment.get(offset);\n\t\t\tcurrent |= (1 << (index & BIT_BYTE_INDEX_MASK));\n\t\t\tsegment.put(offset, current);\n\t\t} else {\n\t\t\tbitSetMultiSegments(segments, baseOffset, index);\n\t\t}\n\t}"}, {"instruction": "Removes the field at the given position.\n<p>\nThis method should be used carefully. Be aware that as the field is actually removed from the record, the\ntotal number of fields is modified, and all fields to the right of the field removed shift one position to\nthe left.\n\n@param fieldNum The position of the field to be removed, starting at zero.\n@throws IndexOutOfBoundsException Thrown, when the position is not between 0 (inclusive) and the\nnumber of fields (exclusive).", "input": "Java", "output": "public void removeField(int fieldNum)\n\t{\n\t\t// range check\n\t\tif (fieldNum < 0 || fieldNum >= this.numFields) {\n\t\t\tthrow new IndexOutOfBoundsException();\n\t\t}\n\t\tint lastIndex = this.numFields - 1;\t\t\n\n\t\tif (fieldNum < lastIndex) {\n\t\t\tint len = lastIndex - fieldNum;\n\t\t\tSystem.arraycopy(this.offsets, fieldNum + 1, this.offsets, fieldNum, len);\n\t\t\tSystem.arraycopy(this.lengths, fieldNum + 1, this.lengths, fieldNum, len);\n\t\t\tSystem.arraycopy(this.readFields, fieldNum + 1, this.readFields, fieldNum, len);\n\t\t\tSystem.arraycopy(this.writeFields, fieldNum + 1, this.writeFields, fieldNum, len);\n\t\t\tmarkModified(fieldNum);\n\t\t}\n\t\tthis.offsets[lastIndex] = NULL_INDICATOR_OFFSET;\n\t\tthis.lengths[lastIndex] = 0;\n\t\tthis.writeFields[lastIndex] = null;\n\n\t\tsetNumFields(lastIndex);\n\t}"}, {"instruction": "\u5c06\u952e\u503c\u5bf9\u52a0\u5165\u5230\u5bf9\u5e94\u5206\u7ec4\u4e2d\n\n@param group \u5206\u7ec4\n@param key \u952e\n@param value \u503c\n@return \u6b64key\u4e4b\u524d\u5b58\u5728\u7684\u503c\uff0c\u5982\u679c\u6ca1\u6709\u8fd4\u56denull", "input": "Java", "output": "public String put(String group, String key, String value) {\r\n\t\tgroup = StrUtil.nullToEmpty(group).trim();\r\n\t\twriteLock.lock();\r\n\t\ttry {\r\n\t\t\tLinkedHashMap<String, String> valueMap = this.get(group);\r\n\t\t\tif (null == valueMap) {\r\n\t\t\t\tvalueMap = new LinkedHashMap<>();\r\n\t\t\t\tthis.put(group, valueMap);\r\n\t\t\t}\r\n\t\t\tthis.size = -1;\r\n\t\t\treturn valueMap.put(key, value);\r\n\t\t} finally {\r\n\t\t\twriteLock.unlock();\r\n\t\t}\r\n\t}"}, {"instruction": "Configure Azkaban metrics tracking for a new flowRunner instance", "input": "Java", "output": "private void configureFlowLevelMetrics(final FlowRunner flowRunner) {\n    logger.info(\"Configuring Azkaban metrics tracking for flow runner object\");\n\n    if (MetricReportManager.isAvailable()) {\n      final MetricReportManager metricManager = MetricReportManager.getInstance();\n      // Adding NumFailedFlow Metric listener\n      flowRunner.addListener((NumFailedFlowMetric) metricManager\n          .getMetricFromName(NumFailedFlowMetric.NUM_FAILED_FLOW_METRIC_NAME));\n    }\n\n  }"}, {"instruction": "\u5c1d\u8bd5\u904d\u5386\u5e76\u8c03\u7528\u6b64\u7c7b\u7684\u6240\u6709\u6784\u9020\u65b9\u6cd5\uff0c\u76f4\u5230\u6784\u9020\u6210\u529f\u5e76\u8fd4\u56de\n\n@param <T> \u5bf9\u8c61\u7c7b\u578b\n@param beanClass \u88ab\u6784\u9020\u7684\u7c7b\n@return \u6784\u9020\u540e\u7684\u5bf9\u8c61", "input": "Java", "output": "public static <T> T newInstanceIfPossible(Class<T> beanClass) {\r\n\t\tAssert.notNull(beanClass);\r\n\t\ttry {\r\n\t\t\treturn newInstance(beanClass);\r\n\t\t} catch (Exception e) {\r\n\t\t\t// ignore\r\n\t\t\t// \u9ed8\u8ba4\u6784\u9020\u4e0d\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u67e5\u627e\u5176\u5b83\u6784\u9020\r\n\t\t}\r\n\t\t\r\n\t\tfinal Constructor<T>[] constructors = getConstructors(beanClass);\r\n\t\tClass<?>[] parameterTypes;\r\n\t\tfor (Constructor<T> constructor : constructors) {\r\n\t\t\tparameterTypes = constructor.getParameterTypes();\r\n\t\t\tif (0 == parameterTypes.length) {\r\n\t\t\t\tcontinue;\r\n\t\t\t}\r\n\t\t\tconstructor.setAccessible(true);\r\n\t\t\ttry {\r\n\t\t\t\treturn constructor.newInstance(ClassUtil.getDefaultValues(parameterTypes));\r\n\t\t\t} catch (Exception e) {\r\n\t\t\t\t// \u6784\u9020\u51fa\u9519\u65f6\u7ee7\u7eed\u5c1d\u8bd5\u4e0b\u4e00\u79cd\u6784\u9020\u65b9\u5f0f\r\n\t\t\t\tcontinue;\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn null;\r\n\t}"}, {"instruction": "This method defines TokenizerFactory instance to be using during model building\n\n@param tokenizerFactory TokenizerFactory instance", "input": "Java", "output": "public void setTokenizerFactory(@NonNull TokenizerFactory tokenizerFactory) {\n        this.tokenizerFactory = tokenizerFactory;\n\n        if (sentenceIter != null) {\n            SentenceTransformer transformer = new SentenceTransformer.Builder().iterator(sentenceIter)\n                            .tokenizerFactory(this.tokenizerFactory).build();\n            this.iterator = new AbstractSequenceIterator.Builder<>(transformer).build();\n        }\n    }"}, {"instruction": "Checks whether the given directory exists and is writable. If it doesn't exist, this method\nwill attempt to create it.\n\n@param uploadDir directory to check\n@param log logger used for logging output\n@throws IOException if the directory does not exist and cannot be created, or if the\ndirectory isn't writable", "input": "Java", "output": "private static synchronized void checkAndCreateUploadDir(final Path uploadDir, final Logger log) throws IOException {\n\t\tif (Files.exists(uploadDir) && Files.isWritable(uploadDir)) {\n\t\t\tlog.info(\"Using directory {} for file uploads.\", uploadDir);\n\t\t} else if (Files.isWritable(Files.createDirectories(uploadDir))) {\n\t\t\tlog.info(\"Created directory {} for file uploads.\", uploadDir);\n\t\t} else {\n\t\t\tlog.warn(\"Upload directory {} cannot be created or is not writable.\", uploadDir);\n\t\t\tthrow new IOException(\n\t\t\t\tString.format(\"Upload directory %s cannot be created or is not writable.\",\n\t\t\t\t\tuploadDir));\n\t\t}\n\t}"}, {"instruction": "\u6267\u884cShell\u547d\u4ee4\n\n@param session Session\u4f1a\u8bdd\n@param cmd \u547d\u4ee4\n@param charset \u53d1\u9001\u548c\u8bfb\u53d6\u5185\u5bb9\u7684\u7f16\u7801\n@param errStream \u9519\u8bef\u4fe1\u606f\u8f93\u51fa\u5230\u7684\u4f4d\u7f6e\n@return {@link ChannelExec}\n@since 4.3.1", "input": "Java", "output": "public static String exec(Session session, String cmd, Charset charset, OutputStream errStream) {\r\n\t\tif (null == charset) {\r\n\t\t\tcharset = CharsetUtil.CHARSET_UTF_8;\r\n\t\t}\r\n\t\tChannelExec channel = (ChannelExec) openChannel(session, ChannelType.EXEC);\r\n\t\tchannel.setCommand(StrUtil.bytes(cmd, charset));\r\n\t\tchannel.setInputStream(null);\r\n\t\tchannel.setErrStream(errStream);\r\n\t\tInputStream in = null;\r\n\t\ttry {\r\n\t\t\tin = channel.getInputStream();\r\n\t\t\treturn IoUtil.read(in, CharsetUtil.CHARSET_UTF_8);\r\n\t\t} catch (IOException e) {\r\n\t\t\tthrow new IORuntimeException(e);\r\n\t\t} finally {\r\n\t\t\tIoUtil.close(in);\r\n\t\t\tclose(channel);\r\n\t\t}\r\n\t}"}, {"instruction": "get the last reference invoke information\n\n@param clear true: framework will clear the ThreadLocal when return\n@return RPC Reference Context, it can be null", "input": "Java", "output": "public static RpcReferenceContext lastReferenceContext(boolean clear) {\n        try {\n            RpcInvokeContext invokeCtx = RpcInvokeContext.getContext();\n            RpcReferenceContext referenceCtx = (RpcReferenceContext) invokeCtx\n                .get(RemotingConstants.INVOKE_CTX_RPC_REF_CTX);\n            if (referenceCtx != null) {\n                String resultCode = (String) invokeCtx.get(RemotingConstants.INVOKE_CTX_RPC_RESULT_CODE);\n                if (resultCode != null) {\n                    referenceCtx.setResultCode(ResultCodeEnum.getResultCode(resultCode));\n                }\n            }\n            return referenceCtx;\n        } finally {\n            if (clear) {\n                clearReferenceContext();\n            }\n        }\n    }"}, {"instruction": "Initialize the ModelBuilder, validating all arguments and preparing the\ntraining frame.  This call is expected to be overridden in the subclasses\nand each subclass will start with \"super.init();\".  This call is made\nby the front-end whenever the GUI is clicked, and needs to be fast;\nheavy-weight prep needs to wait for the trainModel() call.\n\nValidate the probs.", "input": "Java", "output": "@Override public void init(boolean expensive) {\n    super.init(expensive);\n    for( double p : _parms._probs )\n      if( p < 0.0 || p > 1.0 )\n        error(\"_probs\",\"Probabilities must be between 0 and 1\");\n    _ncols = train().numCols()-numSpecialCols(); //offset/weights/nfold - should only ever be weights\n    if ( numSpecialCols() == 1 && _weights == null)\n      throw new IllegalArgumentException(\"The only special Vec that is supported for Quantiles is observation weights.\");\n    if ( numSpecialCols() >1 ) throw new IllegalArgumentException(\"Cannot handle more than 1 special vec (weights)\");\n  }"}, {"instruction": "{@link #compute} can be called by multiple threads, so this function should be thread-safe to avoid extra\nscript compilation.", "input": "Java", "output": "@EnsuresNonNull(\"fn\")\n  private Function getCompiledScript()\n  {\n    // JavaScript configuration should be checked when it's actually used because someone might still want Druid\n    // nodes to be able to deserialize JavaScript-based objects even though JavaScript is disabled.\n    Preconditions.checkState(config.isEnabled(), \"JavaScript is disabled\");\n\n    Function syncedFn = fn;\n    if (syncedFn == null) {\n      synchronized (config) {\n        syncedFn = fn;\n        if (syncedFn == null) {\n          syncedFn = compile(function);\n          fn = syncedFn;\n        }\n      }\n    }\n    return syncedFn;\n  }"}, {"instruction": "Creates a state info from a new meta info to use with a k/v state.\n\n<p>Creates the column family for the state.\nSets TTL compaction filter if {@code ttlCompactFiltersManager} is not {@code null}.", "input": "Java", "output": "public static RocksDBKeyedStateBackend.RocksDbKvStateInfo createStateInfo(\n\t\tRegisteredStateMetaInfoBase metaInfoBase,\n\t\tRocksDB db,\n\t\tFunction<String, ColumnFamilyOptions> columnFamilyOptionsFactory,\n\t\t@Nullable RocksDbTtlCompactFiltersManager ttlCompactFiltersManager) {\n\n\t\tColumnFamilyDescriptor columnFamilyDescriptor = createColumnFamilyDescriptor(\n\t\t\tmetaInfoBase, columnFamilyOptionsFactory, ttlCompactFiltersManager);\n\t\treturn new RocksDBKeyedStateBackend.RocksDbKvStateInfo(createColumnFamily(columnFamilyDescriptor, db), metaInfoBase);\n\t}"}, {"instruction": "Example of how to enable default event-based hold for a bucket", "input": "Java", "output": "public Bucket enableDefaultEventBasedHold(String bucketName) throws StorageException {\n    // [START storage_enable_default_event_based_hold]\n    // Instantiate a Google Cloud Storage client\n    Storage storage = StorageOptions.getDefaultInstance().getService();\n\n    // The name of a bucket, e.g. \"my-bucket\"\n    // String bucketName = \"my-bucket\";\n\n    Bucket bucket =\n        storage.update(BucketInfo.newBuilder(bucketName).setDefaultEventBasedHold(true).build());\n\n    System.out.println(\"Default event-based hold was enabled for \" + bucketName);\n    // [END storage_enable_default_event_based_hold]\n    return bucket;\n  }"}, {"instruction": "Create a node.\n\n@param path\n@param data\n@param mode\n@return create node's path\n@throws ZkInterruptedException if operation was interrupted, or a required reconnection got interrupted\n@throws IllegalArgumentException if called from anything except the ZooKeeper event thread\n@throws ZkException if any ZooKeeper exception occurred\n@throws RuntimeException if any other exception occurs", "input": "Java", "output": "public String create(final String path, Object data, final CreateMode mode) throws ZkInterruptedException,\n                                                                               IllegalArgumentException, ZkException,\n                                                                               RuntimeException {\n        if (path == null) {\n            throw new NullPointerException(\"path must not be null.\");\n        }\n        final byte[] bytes = data == null ? null : serialize(data);\n\n        return retryUntilConnected(new Callable<String>() {\n\n            @Override\n            public String call() throws Exception {\n                return _connection.create(path, bytes, mode);\n            }\n        });\n    }"}, {"instruction": "\u540c\u65f6\u8fd4\u56de\u65e0\u5e8f\u96c6\u5408\u4e2d\u7684\u6700\u5c0f\u503c\u548c\u6700\u5927\u503c\uff0c\u4f7f\u7528\u5143\u7d20\u9ed8\u8ba4\u6392\u5e8f\n\n\u5728\u8fd4\u56de\u7684Pair\u4e2d\uff0c\u7b2c\u4e00\u4e2a\u4e3a\u6700\u5c0f\u503c\uff0c\u7b2c\u4e8c\u4e2a\u4e3a\u6700\u5927\u503c", "input": "Java", "output": "public static <T extends Object & Comparable<? super T>> Pair<T, T> minAndMax(Collection<? extends T> coll) {\n\t\tIterator<? extends T> i = coll.iterator();\n\t\tT minCandidate = i.next();\n\t\tT maxCandidate = minCandidate;\n\n\t\twhile (i.hasNext()) {\n\t\t\tT next = i.next();\n\t\t\tif (next.compareTo(minCandidate) < 0) {\n\t\t\t\tminCandidate = next;\n\t\t\t} else if (next.compareTo(maxCandidate) > 0) {\n\t\t\t\tmaxCandidate = next;\n\t\t\t}\n\t\t}\n\t\treturn Pair.of(minCandidate, maxCandidate);\n\t}"}, {"instruction": "In addition to the expiration requested by the super class, we also check the expiration is not too far in the future.\nEspecially to detect maliciously crafted cookie.", "input": "Java", "output": "@Override\n    protected boolean isTokenExpired(long tokenExpiryTimeMs) {\n        long nowMs = System.currentTimeMillis();\n        long maxExpirationMs = TimeUnit.SECONDS.toMillis(tokenValiditySeconds) + nowMs;\n        if(!SKIP_TOO_FAR_EXPIRATION_DATE_CHECK && tokenExpiryTimeMs > maxExpirationMs){\n            // attempt to use a cookie that has more than the maximum allowed expiration duration\n            // was either created before a change of configuration or maliciously crafted\n            long diffMs = tokenExpiryTimeMs - maxExpirationMs;\n            LOGGER.log(Level.WARNING, \"Attempt to use a cookie with an expiration duration larger than the one configured (delta of: {0} ms)\", diffMs);\n            return true;\n        }\n        // Check it has not expired\n        if (tokenExpiryTimeMs < nowMs) {\n            return true;\n        }\n        return false;\n    }"}, {"instruction": "Creates a new big-endian buffer whose content is a subregion of\nthe specified {@code string} encoded in the specified {@code charset}.\nThe new buffer's {@code readerIndex} and {@code writerIndex} are\n{@code 0} and the length of the encoded string respectively.", "input": "Java", "output": "public static ByteBuf copiedBuffer(\n            CharSequence string, int offset, int length, Charset charset) {\n        if (string == null) {\n            throw new NullPointerException(\"string\");\n        }\n        if (length == 0) {\n            return EMPTY_BUFFER;\n        }\n\n        if (string instanceof CharBuffer) {\n            CharBuffer buf = (CharBuffer) string;\n            if (buf.hasArray()) {\n                return copiedBuffer(\n                        buf.array(),\n                        buf.arrayOffset() + buf.position() + offset,\n                        length, charset);\n            }\n\n            buf = buf.slice();\n            buf.limit(length);\n            buf.position(offset);\n            return copiedBuffer(buf, charset);\n        }\n\n        return copiedBuffer(CharBuffer.wrap(string, offset, offset + length), charset);\n    }"}, {"instruction": "\u83b7\u53d6\u6c49\u5b57\u5bf9\u5e94\u7684ascii\u7801\n@param chs \u6c49\u5b57\n@return ascii\u7801", "input": "Java", "output": "private static int getChsAscii(String chs) {\r\n\t\tint asc = 0;\r\n\t\tbyte[] bytes = chs.getBytes(CharsetUtil.CHARSET_GBK);\r\n\t\tswitch (bytes.length) {\r\n\t\tcase 1:\r\n\t\t\t// \u82f1\u6587\u5b57\u7b26\r\n\t\t\tasc = bytes[0];\r\n\t\t\tbreak;\r\n\t\tcase 2:\r\n\t\t\t// \u4e2d\u6587\u5b57\u7b26\r\n\t\t\tint hightByte = 256 + bytes[0];\r\n\t\t\tint lowByte = 256 + bytes[1];\r\n\t\t\tasc = (256 * hightByte + lowByte) - 256 * 256;\r\n\t\t\tbreak;\r\n\t\tdefault:\r\n\t\t\tthrow new UtilException(\"Illegal resource string\");\r\n\t\t}\r\n\t\treturn asc;\r\n\t}"}, {"instruction": "Parse string as key-value string and return the value matches key name.\nexample:\nkeyvalue('k1=v1;k2=v2', ';', '=', 'k2') = 'v2'\nkeyvalue('k1:v1,k2:v2', ',', ':', 'k3') = NULL\n\n@param str     target string.\n@param pairSeparator  separator between key-value tuple.\n@param kvSeparator  separator between key and value.\n@param keyName name of the key whose value you want return.\n@return target value.", "input": "Java", "output": "public static BinaryString keyValue(\n\t\tBinaryString str, BinaryString pairSeparator, BinaryString kvSeparator, BinaryString keyName) {\n\t\tif (str == null || str.getSizeInBytes() == 0) {\n\t\t\treturn null;\n\t\t}\n\t\tif (pairSeparator != null && pairSeparator.getSizeInBytes() == 1 &&\n\t\t\tkvSeparator != null && kvSeparator.getSizeInBytes() == 1) {\n\t\t\treturn str.keyValue(pairSeparator.getByte(0), kvSeparator.getByte(0), keyName);\n\t\t} else {\n\t\t\treturn BinaryString.fromString(\n\t\t\t\tkeyValue(\n\t\t\t\t\tBinaryString.safeToString(str),\n\t\t\t\t\tBinaryString.safeToString(pairSeparator),\n\t\t\t\t\tBinaryString.safeToString(kvSeparator),\n\t\t\t\t\tBinaryString.safeToString(keyName)));\n\t\t}\n\t}"}, {"instruction": "\u5b9e\u4f8b\u5316\u5bf9\u8c61\n\n@param <T> \u5bf9\u8c61\u7c7b\u578b\n@param clazz \u7c7b\n@param params \u6784\u9020\u51fd\u6570\u53c2\u6570\n@return \u5bf9\u8c61\n@throws UtilException \u5305\u88c5\u5404\u7c7b\u5f02\u5e38", "input": "Java", "output": "public static <T> T newInstance(Class<T> clazz, Object... params) throws UtilException {\r\n\t\tif (ArrayUtil.isEmpty(params)) {\r\n\t\t\tfinal Constructor<T> constructor = getConstructor(clazz);\r\n\t\t\ttry {\r\n\t\t\t\treturn constructor.newInstance();\r\n\t\t\t} catch (Exception e) {\r\n\t\t\t\tthrow new UtilException(e, \"Instance class [{}] error!\", clazz);\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tfinal Class<?>[] paramTypes = ClassUtil.getClasses(params);\r\n\t\tfinal Constructor<T> constructor = getConstructor(clazz, paramTypes);\r\n\t\tif (null == constructor) {\r\n\t\t\tthrow new UtilException(\"No Constructor matched for parameter types: [{}]\", new Object[] { paramTypes });\r\n\t\t}\r\n\t\ttry {\r\n\t\t\treturn constructor.newInstance(params);\r\n\t\t} catch (Exception e) {\r\n\t\t\tthrow new UtilException(e, \"Instance class [{}] error!\", clazz);\r\n\t\t}\r\n\t}"}, {"instruction": "Creates a Graph from CSV input with vertex values and edge values.\nThe vertex values are specified through a vertices input file or a user-defined map function.\n\n@param vertexKey the type of the vertex IDs\n@param vertexValue the type of the vertex values\n@param edgeValue the type of the edge values\n@return a Graph with vertex and edge values.", "input": "Java", "output": "@SuppressWarnings(\"unchecked\")\n\tpublic <K, VV, EV> Graph<K, VV, EV> types(Class<K> vertexKey, Class<VV> vertexValue,\n\t\t\tClass<EV> edgeValue) {\n\n\t\tif (edgeReader == null) {\n\t\t\tthrow new RuntimeException(\"The edge input file cannot be null!\");\n\t\t}\n\n\t\tDataSet<Tuple3<K, K, EV>> edges = edgeReader.types(vertexKey, vertexKey, edgeValue);\n\n\t\t// the vertex value can be provided by an input file or a user-defined mapper\n\t\tif (vertexReader != null) {\n\t\t\tDataSet<Tuple2<K, VV>> vertices = vertexReader\n\t\t\t\t.types(vertexKey, vertexValue)\n\t\t\t\t\t.name(GraphCsvReader.class.getName());\n\n\t\t\treturn Graph.fromTupleDataSet(vertices, edges, executionContext);\n\t\t}\n\t\telse if (mapper != null) {\n\t\t\treturn Graph.fromTupleDataSet(edges, (MapFunction<K, VV>) mapper, executionContext);\n\t\t}\n\t\telse {\n\t\t\tthrow new RuntimeException(\"Vertex values have to be specified through a vertices input file\"\n\t\t\t\t\t+ \"or a user-defined map function.\");\n\t\t}\n\t}"}, {"instruction": "Tries to repair the lattice by creating and adding an additional Viterbi node to the RIGHT of the newly\ninserted user dictionary entry by using the substring of the node in the lattice that overlaps the least\n@param lattice\n@param nodeEndIndex", "input": "Java", "output": "private void repairBrokenLatticeAfter(ViterbiLattice lattice, int nodeEndIndex) {\n        ViterbiNode[][] nodeEndIndices = lattice.getEndIndexArr();\n\n        for (int endIndex = nodeEndIndex + 1; endIndex < nodeEndIndices.length; endIndex++) {\n            if (nodeEndIndices[endIndex] != null) {\n                ViterbiNode glueBase = findGlueNodeCandidate(nodeEndIndex, nodeEndIndices[endIndex], endIndex);\n                if (glueBase != null) {\n                    int delta = endIndex - nodeEndIndex;\n                    String glueBaseSurface = glueBase.getSurface();\n                    String surface = glueBaseSurface.substring(glueBaseSurface.length() - delta);\n                    ViterbiNode glueNode = createGlueNode(nodeEndIndex, glueBase, surface);\n                    lattice.addNode(glueNode, nodeEndIndex, nodeEndIndex + glueNode.getSurface().length());\n                    return;\n                }\n            }\n        }\n    }"}, {"instruction": "Convert a DataSet to the equivalent MultiDataSet", "input": "Java", "output": "public static MultiDataSet toMultiDataSet(DataSet dataSet) {\n        INDArray f = dataSet.getFeatures();\n        INDArray l = dataSet.getLabels();\n        INDArray fMask = dataSet.getFeaturesMaskArray();\n        INDArray lMask = dataSet.getLabelsMaskArray();\n\n        INDArray[] fNew = f == null ? null : new INDArray[] {f};\n        INDArray[] lNew = l == null ? null : new INDArray[] {l};\n        INDArray[] fMaskNew = (fMask != null ? new INDArray[] {fMask} : null);\n        INDArray[] lMaskNew = (lMask != null ? new INDArray[] {lMask} : null);\n\n        return new org.nd4j.linalg.dataset.MultiDataSet(fNew, lNew, fMaskNew, lMaskNew);\n    }"}, {"instruction": "Generic method to create an input data stream with {@link org.apache.flink.api.common.io.InputFormat}.\n\n<p>The data stream is typed to the given TypeInformation. This method is intended for input formats\nwhere the return type cannot be determined by reflection analysis, and that do not implement the\n{@link org.apache.flink.api.java.typeutils.ResultTypeQueryable} interface.\n\n<p><b>NOTES ON CHECKPOINTING: </b> In the case of a {@link FileInputFormat}, the source\n(which executes the {@link ContinuousFileMonitoringFunction}) monitors the path, creates the\n{@link org.apache.flink.core.fs.FileInputSplit FileInputSplits} to be processed, forwards\nthem to the downstream {@link ContinuousFileReaderOperator} to read the actual data, and exits,\nwithout waiting for the readers to finish reading. This implies that no more checkpoint\nbarriers are going to be forwarded after the source exits, thus having no checkpoints.\n\n@param inputFormat\nThe input format used to create the data stream\n@param typeInfo\nThe information about the type of the output type\n@param <OUT>\nThe type of the returned data stream\n@return The data stream that represents the data created by the input format", "input": "Java", "output": "@PublicEvolving\n\tpublic <OUT> DataStreamSource<OUT> createInput(InputFormat<OUT, ?> inputFormat, TypeInformation<OUT> typeInfo) {\n\t\tDataStreamSource<OUT> source;\n\n\t\tif (inputFormat instanceof FileInputFormat) {\n\t\t\t@SuppressWarnings(\"unchecked\")\n\t\t\tFileInputFormat<OUT> format = (FileInputFormat<OUT>) inputFormat;\n\n\t\t\tsource = createFileInput(format, typeInfo, \"Custom File source\",\n\t\t\t\t\tFileProcessingMode.PROCESS_ONCE, -1);\n\t\t} else {\n\t\t\tsource = createInput(inputFormat, typeInfo, \"Custom Source\");\n\t\t}\n\t\treturn source;\n\t}"}, {"instruction": "Gets the interface {@code class1} from the given {@code script}. Might return {@code null} if the {@code script} does not\nimplement the interface.\n<p>\nFirst tries to get the interface directly from the {@code script} by calling the method\n{@code ScriptWrapper.getInterface(Class)}, if it returns {@code null} the interface will be extracted from the script\nafter invoking it, using the method {@code Invocable.getInterface(Class)}.\n<p>\nThe context class loader of caller thread is replaced with the class loader {@code AddOnLoader} to allow the script to\naccess classes of add-ons. If this behaviour is not desired call the method {@code getInterfaceWithOutAddOnLoader(}\ninstead.\n\n@param script the script that will be invoked\n@param class1 the interface that will be obtained from the script\n@return the interface implemented by the script, or {@code null} if the {@code script} does not implement the interface.\n@throws ScriptException if the engine of the given {@code script} was not found.\n@throws IOException if an error occurred while obtaining the interface directly from the script (\n{@code ScriptWrapper.getInterface(Class)})\n@see #getInterfaceWithOutAddOnLoader(ScriptWrapper, Class)\n@see ScriptWrapper#getInterface(Class)\n@see Invocable#getInterface(Class)", "input": "Java", "output": "public <T> T getInterface(ScriptWrapper script, Class<T> class1) throws ScriptException, IOException {\r\n\t\r\n\t\tClassLoader previousContextClassLoader = Thread.currentThread().getContextClassLoader();\r\n\t\tThread.currentThread().setContextClassLoader(ExtensionFactory.getAddOnLoader());\r\n\t\ttry {\r\n\t\t\tT iface = script.getInterface(class1);\r\n\t\t\t\r\n\t\t\tif (iface != null) {\r\n\t\t\t\t// the script wrapper has overriden the usual scripting mechanism\r\n\t\t\t\treturn iface;\r\n\t\t\t}\r\n\t\t} finally {\r\n\t\t\tThread.currentThread().setContextClassLoader(previousContextClassLoader);\r\n\t\t}\r\n\t\t\r\n\t\tif (script.isRunableStandalone()) {\r\n\t\t\treturn null;\r\n\t\t}\r\n\r\n\t\tInvocable invocable = invokeScript(script);\r\n\t\tif (invocable != null) {\r\n\t\t\treturn invocable.getInterface(class1);\r\n\t\t}\r\n\t\treturn null;\r\n\r\n\t}"}, {"instruction": "and patches .resolve() on LocalDeclaration itself to just-in-time replace the 'val' vartype with the right one.", "input": "Java", "output": "public static TypeBinding skipResolveInitializerIfAlreadyCalled(Expression expr, BlockScope scope) {\n\t\tif (expr.resolvedType != null) return expr.resolvedType;\n\t\ttry {\n\t\t\treturn expr.resolveType(scope);\n\t\t} catch (NullPointerException e) {\n\t\t\treturn null;\n\t\t} catch (ArrayIndexOutOfBoundsException e) {\n\t\t\t// This will occur internally due to for example 'val x = mth(\"X\");', where mth takes 2 arguments.\n\t\t\treturn null;\n\t\t}\n\t}"}, {"instruction": "Encodes the specified cookies into a single Cookie header value.\n\n@param cookies\nsome cookies\n@return a Rfc6265 style Cookie header value, null if no cookies are passed.", "input": "Java", "output": "public String encode(Cookie... cookies) {\n        if (checkNotNull(cookies, \"cookies\").length == 0) {\n            return null;\n        }\n\n        StringBuilder buf = stringBuilder();\n        if (strict) {\n            if (cookies.length == 1) {\n                encode(buf, cookies[0]);\n            } else {\n                Cookie[] cookiesSorted = Arrays.copyOf(cookies, cookies.length);\n                Arrays.sort(cookiesSorted, COOKIE_COMPARATOR);\n                for (Cookie c : cookiesSorted) {\n                    encode(buf, c);\n                }\n            }\n        } else {\n            for (Cookie c : cookies) {\n                encode(buf, c);\n            }\n        }\n        return stripTrailingSeparatorOrNull(buf);\n    }"}, {"instruction": "Applies Givens rotation to sparse vectors one of which is in compressed form.\n\n@param N The number of elements in vectors X and Y\n@param X a sparse vector\n@param Y a full-storage vector\n@param c a scalar\n@param s a scalar", "input": "Java", "output": "@Override\n    public void rot(long N, INDArray X, INDArray Y, double c, double s) {\n\n\n        if (X instanceof BaseSparseNDArray) {\n            BaseSparseNDArray sparseX = (BaseSparseNDArray) X;\n\n            switch (X.data().dataType()) {\n                case DOUBLE:\n                    droti(N, X, sparseX.getVectorCoordinates(), Y, c, s);\n                    break;\n                case FLOAT:\n                    sroti(N, X, sparseX.getVectorCoordinates(), Y, c, s);\n                    break;\n                case HALF:\n                    hroti(N, X, sparseX.getVectorCoordinates(), Y, c, s);\n                    break;\n                default:\n                    throw new UnsupportedOperationException();\n            }\n        } else {\n            throw new UnsupportedOperationException();\n        }\n    }"}, {"instruction": "This method allows to remove graph from the GraphServer instance\n@param graphId", "input": "Java", "output": "public void dropGraph(long graphId) {\n        val builder = new FlatBufferBuilder(128);\n\n        val off = FlatDropRequest.createFlatDropRequest(builder, graphId);\n        builder.finish(off);\n\n        val req = FlatDropRequest.getRootAsFlatDropRequest(builder.dataBuffer());\n\n        val v = blockingStub.forgetGraph(req);\n        if (v.status() != 0)\n            throw new ND4JIllegalStateException(\"registerGraph() gRPC call failed\");\n    }"}, {"instruction": "Get the output size of a deconvolution operation for given input data. In deconvolution, we compute the inverse\nof the shape computation of a convolution.\n\n@param inputData       Input data\n@param kernel          Kernel size (height/width)\n@param strides         Strides (height/width)\n@param padding         Padding (height/width)\n@param convolutionMode Convolution mode (Same, Strict, Truncate)\n@param dilation        Kernel dilation (height/width)\n@return Output size: int[2] with output height/width", "input": "Java", "output": "public static int[] getDeconvolutionOutputSize(INDArray inputData, int[] kernel, int[] strides, int[] padding,\n                                                   ConvolutionMode convolutionMode, int[] dilation) {\n\n        // FIXME: int cast\n        int hIn = (int) inputData.size(2);\n        int wIn = (int) inputData.size(3);\n        int[] eKernel = effectiveKernelSize(kernel, dilation);\n\n        if (convolutionMode == ConvolutionMode.Same) {\n            int hOut = strides[0] * hIn;\n            int wOut = strides[1] * wIn;\n            return new int[]{hOut, wOut};\n        }\n\n        int hOut = strides[0] * (hIn - 1) + eKernel[0] - 2 * padding[0];\n        int wOut = strides[1] * (wIn - 1) + eKernel[1] - 2 * padding[1];\n\n        return new int[]{hOut, wOut};\n    }"}, {"instruction": "Creates a new {@link NameResolver}. Override this method to create an alternative {@link NameResolver}\nimplementation or override the default configuration.", "input": "Java", "output": "protected NameResolver<InetAddress> newNameResolver(EventLoop eventLoop,\n                                                        ChannelFactory<? extends DatagramChannel> channelFactory,\n                                                        DnsServerAddressStreamProvider nameServerProvider)\n            throws Exception {\n        // once again, channelFactory and nameServerProvider are most probably set in builder already,\n        // but I do reassign them again to avoid corner cases with override methods\n        return dnsResolverBuilder.eventLoop(eventLoop)\n                .channelFactory(channelFactory)\n                .nameServerProvider(nameServerProvider)\n                .build();\n    }"}, {"instruction": "\u6bd4\u8f83\u4e24\u4e2a\u6587\u4ef6\u5185\u5bb9\u662f\u5426\u76f8\u540c<br>\n\u9996\u5148\u6bd4\u8f83\u957f\u5ea6\uff0c\u957f\u5ea6\u4e00\u81f4\u518d\u6bd4\u8f83\u5185\u5bb9\uff0c\u6bd4\u8f83\u5185\u5bb9\u91c7\u7528\u6309\u884c\u8bfb\u53d6\uff0c\u6bcf\u884c\u6bd4\u8f83<br>\n\u6b64\u65b9\u6cd5\u6765\u81eaApache Commons io\n\n@param file1 \u6587\u4ef61\n@param file2 \u6587\u4ef62\n@param charset \u7f16\u7801\uff0cnull\u8868\u793a\u4f7f\u7528\u5e73\u53f0\u9ed8\u8ba4\u7f16\u7801 \u4e24\u4e2a\u6587\u4ef6\u5185\u5bb9\u4e00\u81f4\u8fd4\u56detrue\uff0c\u5426\u5219false\n@throws IORuntimeException IO\u5f02\u5e38\n@since 4.0.6", "input": "Java", "output": "public static boolean contentEqualsIgnoreEOL(File file1, File file2, Charset charset) throws IORuntimeException {\r\n\t\tboolean file1Exists = file1.exists();\r\n\t\tif (file1Exists != file2.exists()) {\r\n\t\t\treturn false;\r\n\t\t}\r\n\r\n\t\tif (!file1Exists) {\r\n\t\t\t// \u4e24\u4e2a\u6587\u4ef6\u90fd\u4e0d\u5b58\u5728\uff0c\u8fd4\u56detrue\r\n\t\t\treturn true;\r\n\t\t}\r\n\r\n\t\tif (file1.isDirectory() || file2.isDirectory()) {\r\n\t\t\t// \u4e0d\u6bd4\u8f83\u76ee\u5f55\r\n\t\t\tthrow new IORuntimeException(\"Can't compare directories, only files\");\r\n\t\t}\r\n\r\n\t\tif (equals(file1, file2)) {\r\n\t\t\t// \u540c\u4e00\u4e2a\u6587\u4ef6\r\n\t\t\treturn true;\r\n\t\t}\r\n\r\n\t\tReader input1 = null;\r\n\t\tReader input2 = null;\r\n\t\ttry {\r\n\t\t\tinput1 = getReader(file1, charset);\r\n\t\t\tinput2 = getReader(file2, charset);\r\n\t\t\treturn IoUtil.contentEqualsIgnoreEOL(input1, input2);\r\n\t\t} finally {\r\n\t\t\tIoUtil.close(input1);\r\n\t\t\tIoUtil.close(input2);\r\n\t\t}\r\n\t}"}, {"instruction": "Get layer output type.\n\n@param  inputType    Array of InputTypes\n@return              output type as InputType\n@throws InvalidKerasConfigurationException Invalid Keras config", "input": "Java", "output": "@Override\n    public InputType getOutputType(InputType... inputType) throws InvalidKerasConfigurationException {\n        if (inputType.length > 1)\n            throw new InvalidKerasConfigurationException(\n                            \"Keras LRN layer accepts only one input (received \" + inputType.length + \")\");\n        return this.getLocalResponseNormalization().getOutputType(-1, inputType[0]);\n    }"}, {"instruction": "Get metric snapshots for a metric and date specification", "input": "Java", "output": "private void handleGetMetricHistory(final int executorId, final HttpServletRequest req,\n      final HashMap<String, Object> ret, final User user) throws IOException,\n      ServletException {\n    try {\n      final Map<String, Object> result =\n          this.execManagerAdapter.callExecutorStats(executorId,\n              ConnectorParams.STATS_GET_METRICHISTORY, getAllParams(req));\n      if (result.containsKey(ConnectorParams.RESPONSE_ERROR)) {\n        ret.put(ConnectorParams.RESPONSE_ERROR,\n            result.get(ConnectorParams.RESPONSE_ERROR).toString());\n      } else {\n        ret.put(\"data\", result.get(\"data\"));\n      }\n    } catch (final ExecutorManagerException ex) {\n      logger.error(ex.getMessage(), ex);\n      ret.put(\"error\", \"Failed to fetch metric history\");\n    }\n  }"}, {"instruction": "Gets the XML representation of the given API {@code response}.\n<p>\nAn XML element named with name of the endpoint and with child elements as given by\n{@link ApiResponse#toXML(Document, Element)}.\n\n@param endpointName the name of the API endpoint, must not be {@code null}.\n@param response the API response, must not be {@code null}.\n@return the XML representation of the given response.\n@throws ApiException if an error occurred while converting the response.", "input": "Java", "output": "static String responseToXml(String endpointName, ApiResponse response) throws ApiException {\n\t\ttry {\n\t\t\tDocumentBuilderFactory docFactory = DocumentBuilderFactory.newInstance();\n\t\t\tDocumentBuilder docBuilder = docFactory.newDocumentBuilder();\n\t\n\t\t\tDocument doc = docBuilder.newDocument();\n\t\t\tElement rootElement = doc.createElement(endpointName);\n\t\t\tdoc.appendChild(rootElement);\n\t\t\tresponse.toXML(doc, rootElement);\n\t\t\t\n\t\t\tTransformerFactory transformerFactory = TransformerFactory.newInstance();\n\t\t\tTransformer transformer = transformerFactory.newTransformer();\n\t\t\tDOMSource source = new DOMSource(doc);\n\t\t\t\n\t\t\tStringWriter sw = new StringWriter();\n\t\t\tStreamResult result =  new StreamResult(sw);\n\t\t\ttransformer.transform(source, result);\n\t\t\t\n\t\t\treturn sw.toString();\n\n\t\t} catch (Exception e) {\n\t\t\tlogger.error(\"Failed to convert API response to XML: \" + e.getMessage(), e);\n\t\t\tthrow new ApiException(ApiException.Type.INTERNAL_ERROR, e);\n\t\t}\n\t}"}, {"instruction": "Track trusted multifactor authentication attribute.\n\n@param authn         the authn\n@param attributeName the attribute name", "input": "Java", "output": "public static void trackTrustedMultifactorAuthenticationAttribute(\n        final Authentication authn,\n        final String attributeName) {\n\n        val newAuthn = DefaultAuthenticationBuilder.newInstance(authn)\n            .addAttribute(attributeName, Boolean.TRUE)\n            .build();\n        LOGGER.debug(\"Updated authentication session to remember trusted multifactor record via [{}]\", attributeName);\n        authn.update(newAuthn);\n    }"}, {"instruction": "Swap data source to database access configuration.\n\n@param dataSource data source\n@return database access configuration", "input": "Java", "output": "public DatabaseAccessConfiguration swap(final DataSource dataSource) {\n        DataSourcePropertyProvider provider = DataSourcePropertyProviderLoader.getProvider(dataSource);\n        try {\n            String url = (String) findGetterMethod(dataSource, provider.getURLPropertyName()).invoke(dataSource);\n            String username = (String) findGetterMethod(dataSource, provider.getUsernamePropertyName()).invoke(dataSource);\n            String password = (String) findGetterMethod(dataSource, provider.getPasswordPropertyName()).invoke(dataSource);\n            return new DatabaseAccessConfiguration(url, username, password);\n        } catch (final ReflectiveOperationException ex) {\n            throw new ShardingException(\"Cannot swap data source type: `%s`, please provide an implementation from SPI `%s`\", \n                    dataSource.getClass().getName(), DataSourcePropertyProvider.class.getName());\n        }\n    }"}, {"instruction": "Read a little endian integer from the stream.\n\n@param n\nthe number of bytes to read\n\n@return the bytes read, converted from little endian to an integer.\n\n@exception OtpErlangDecodeException\nif the next byte cannot be read.", "input": "Java", "output": "public long readLE(final int n) throws OtpErlangDecodeException {\n        final byte[] b = new byte[n];\n        try {\n            super.read(b);\n        } catch (final IOException e) {\n            throw new OtpErlangDecodeException(\"Cannot read from input stream\");\n        }\n        long v = 0;\n        int i = n;\n        while (i-- > 0) {\n            v = v << 8 | (long) b[i] & 0xff;\n        }\n        return v;\n    }"}, {"instruction": "Delete TGT's service tickets.\n\n@param ticket the ticket\n@return the count of tickets that were removed including child tickets and zero if the ticket was not deleted", "input": "Java", "output": "protected int deleteChildren(final TicketGrantingTicket ticket) {\n        val count = new AtomicInteger(0);\n        val services = ticket.getServices();\n        if (services != null && !services.isEmpty()) {\n            services.keySet().forEach(ticketId -> {\n                if (deleteSingleTicket(ticketId)) {\n                    LOGGER.debug(\"Removed ticket [{}]\", ticketId);\n                    count.incrementAndGet();\n                } else {\n                    LOGGER.debug(\"Unable to remove ticket [{}]\", ticketId);\n                }\n            });\n        }\n        return count.intValue();\n    }"}, {"instruction": "Returns the boolean value to which the specified key is mapped,\nor defaultValue if there is no mapping for the key. The key match is case-insensitive.", "input": "Java", "output": "public boolean getBoolean(String key, boolean defaultValue) {\n    String value = get(key);\n    // We can't use `Boolean.parseBoolean` here, as it returns false for invalid strings.\n    if (value == null) {\n      return defaultValue;\n    } else if (value.equalsIgnoreCase(\"true\")) {\n      return true;\n    } else if (value.equalsIgnoreCase(\"false\")) {\n      return false;\n    } else {\n      throw new IllegalArgumentException(value + \" is not a boolean string.\");\n    }\n  }"}, {"instruction": "Build output frame from the multi-column results", "input": "Java", "output": "public static Frame buildOutput(int[] gbCols, int noutCols, Frame fr, String[] fcnames, int ngrps, MRTask mrfill) {\n\n    // Build the output!\n    // the names of columns\n    final int nCols = gbCols.length + noutCols;\n    String[] names = new String[nCols];\n    String[][] domains = new String[nCols][];\n    byte[] types = new byte[nCols];\n    for (int i = 0; i < gbCols.length; i++) {\n      names[i] = fr.name(gbCols[i]);\n      domains[i] = fr.domains()[gbCols[i]];\n      types[i] = fr.vec(names[i]).get_type();\n    }\n    for (int i = 0; i < fcnames.length; i++) {\n      names[i + gbCols.length] = fcnames[i];\n      types[i + gbCols.length] = Vec.T_NUM;\n    }\n    Vec v = Vec.makeZero(ngrps); // dummy layout vec\n    // Convert the output arrays into a Frame, also doing the post-pass work\n    Frame f =  mrfill.doAll(types, new Frame(v)).outputFrame(names, domains);\n    v.remove();\n    return f;\n  }"}, {"instruction": "Export the scores in delimited (one per line) UTF-8 format with the specified delimiter\n\n@param outputStream Stream to write to\n@param delimiter    Delimiter to use", "input": "Java", "output": "public void exportScores(OutputStream outputStream, String delimiter) throws IOException {\n        StringBuilder sb = new StringBuilder();\n        sb.append(\"Iteration\").append(delimiter).append(\"Score\");\n        for (Pair<Integer, Double> p : scoreVsIter) {\n            sb.append(\"\\n\").append(p.getFirst()).append(delimiter).append(p.getSecond());\n        }\n        outputStream.write(sb.toString().getBytes(\"UTF-8\"));\n    }"}, {"instruction": "Gets a list of {@link AccumuloColumnConstraint} based on the given constraint ID, excluding the row ID column\n\n@param rowIdName Presto column name mapping to the Accumulo row ID\n@param constraint Set of query constraints\n@return List of all column constraints", "input": "Java", "output": "private static List<AccumuloColumnConstraint> getColumnConstraints(String rowIdName, TupleDomain<ColumnHandle> constraint)\n    {\n        ImmutableList.Builder<AccumuloColumnConstraint> constraintBuilder = ImmutableList.builder();\n        for (ColumnDomain<ColumnHandle> columnDomain : constraint.getColumnDomains().get()) {\n            AccumuloColumnHandle columnHandle = (AccumuloColumnHandle) columnDomain.getColumn();\n\n            if (!columnHandle.getName().equals(rowIdName)) {\n                // Family and qualifier will exist for non-row ID columns\n                constraintBuilder.add(new AccumuloColumnConstraint(\n                        columnHandle.getName(),\n                        columnHandle.getFamily().get(),\n                        columnHandle.getQualifier().get(),\n                        Optional.of(columnDomain.getDomain()),\n                        columnHandle.isIndexed()));\n            }\n        }\n\n        return constraintBuilder.build();\n    }"}, {"instruction": "\u5206\u9875\u67e5\u8be2<br>\n\u6b64\u65b9\u6cd5\u4e0d\u4f1a\u5173\u95edConnection\n\n@param <T> \u7ed3\u679c\u5bf9\u8c61\u7c7b\u578b\n@param conn \u6570\u636e\u5e93\u8fde\u63a5\u5bf9\u8c61\n@param fields \u8fd4\u56de\u7684\u5b57\u6bb5\u5217\u8868\uff0cnull\u5219\u8fd4\u56de\u6240\u6709\u5b57\u6bb5\n@param where \u6761\u4ef6\u5b9e\u4f53\u7c7b\uff08\u5305\u542b\u8868\u540d\uff09\n@param page \u5206\u9875\u5bf9\u8c61\n@param rsh \u7ed3\u679c\u96c6\u5904\u7406\u5bf9\u8c61\n@return \u7ed3\u679c\u5bf9\u8c61\n@throws SQLException SQL\u6267\u884c\u5f02\u5e38", "input": "Java", "output": "public <T> T page(Connection conn, Collection<String> fields, Entity where, Page page, RsHandler<T> rsh) throws SQLException {\r\n\t\tcheckConn(conn);\r\n\t\tif(null == page){\r\n\t\t\treturn this.find(conn, fields, where, rsh);\r\n\t\t}\r\n\t\t\r\n\t\tfinal Query query = new Query(SqlUtil.buildConditions(where), where.getTableName());\r\n\t\tquery.setFields(fields);\r\n\t\tquery.setPage(page);\r\n\t\treturn SqlExecutor.queryAndClosePs(dialect.psForPage(conn, query), rsh);\r\n\t}"}, {"instruction": "Gets loader.\n\n@param resource the resource\n@param name     the name\n@return the loader", "input": "Java", "output": "public BaseConfigurationPropertiesLoader getLoader(final Resource resource,\n                                                       final String name) {\n        val filename = StringUtils.defaultString(resource.getFilename()).toLowerCase();\n\n        if (filename.endsWith(\".properties\")) {\n            return new SimpleConfigurationPropertiesLoader(this.configurationCipherExecutor, name, resource);\n        }\n        if (filename.endsWith(\".groovy\")) {\n            return new GroovyConfigurationPropertiesLoader(this.configurationCipherExecutor, name,\n                getApplicationProfiles(environment), resource);\n        }\n        if (filename.endsWith(\".yaml\") || filename.endsWith(\".yml\")) {\n            return new YamlConfigurationPropertiesLoader(this.configurationCipherExecutor, name, resource);\n        }\n        throw new IllegalArgumentException(\"Unable to determine configuration loader for \" + resource);\n    }"}, {"instruction": "This shuts down the flow runner. The call is blocking and awaits execution of all jobs.", "input": "Java", "output": "public void shutdown() {\n    logger.warn(\"Shutting down FlowRunnerManager...\");\n    if (this.azkabanProps.getBoolean(ConfigurationKeys.AZKABAN_POLL_MODEL, false)) {\n      this.pollingService.shutdown();\n    }\n    this.executorService.shutdown();\n    boolean result = false;\n    while (!result) {\n      logger.info(\"Awaiting Shutdown. # of executing flows: \" + getNumRunningFlows());\n      try {\n        result = this.executorService.awaitTermination(1, TimeUnit.MINUTES);\n      } catch (final InterruptedException e) {\n        logger.error(e);\n      }\n    }\n    logger.warn(\"Shutdown FlowRunnerManager complete.\");\n  }"}, {"instruction": "\u8bfb\u53d6\u7c7b\u76f8\u5bf9\u8def\u5f84\u5185\u5bb9\n\n@param file \u6587\u4ef6\n@return \u6587\u4ef6\u5185\u5bb9\uff08\u6309\u884c\uff09\n@throws IOException \u53d1\u9001IO\u5f02\u5e38", "input": "Java", "output": "public static List<String> readLines(File file) throws IOException {\n        List<String> lines = new ArrayList<String>();\n        InputStreamReader reader = null;\n        BufferedReader bufferedReader = null;\n        try {\n            reader = new FileReader(file);\n            bufferedReader = new BufferedReader(reader);\n            String lineText = null;\n            while ((lineText = bufferedReader.readLine()) != null) {\n                lines.add(lineText);\n            }\n            return lines;\n        } finally {\n            if (bufferedReader != null) {\n                bufferedReader.close();\n            }\n            if (reader != null) {\n                reader.close();\n            }\n        }\n    }"}, {"instruction": "\u67e5\u627e\u7c7b\u4e2d\u7684\u6307\u5b9a\u53c2\u6570\u7684\u6784\u9020\u65b9\u6cd5\uff0c\u5982\u679c\u627e\u5230\u6784\u9020\u65b9\u6cd5\uff0c\u4f1a\u81ea\u52a8\u8bbe\u7f6e\u53ef\u8bbf\u95ee\u4e3atrue\n\n@param <T> \u5bf9\u8c61\u7c7b\u578b\n@param clazz \u7c7b\n@param parameterTypes \u53c2\u6570\u7c7b\u578b\uff0c\u53ea\u8981\u4efb\u4f55\u4e00\u4e2a\u53c2\u6570\u662f\u6307\u5b9a\u53c2\u6570\u7684\u7236\u7c7b\u6216\u63a5\u53e3\u6216\u76f8\u7b49\u5373\u53ef\uff0c\u6b64\u53c2\u6570\u53ef\u4ee5\u4e0d\u4f20\n@return \u6784\u9020\u65b9\u6cd5\uff0c\u5982\u679c\u672a\u627e\u5230\u8fd4\u56denull", "input": "Java", "output": "@SuppressWarnings(\"unchecked\")\r\n\tpublic static <T> Constructor<T> getConstructor(Class<T> clazz, Class<?>... parameterTypes) {\r\n\t\tif (null == clazz) {\r\n\t\t\treturn null;\r\n\t\t}\r\n\r\n\t\tfinal Constructor<?>[] constructors = getConstructors(clazz);\r\n\t\tClass<?>[] pts;\r\n\t\tfor (Constructor<?> constructor : constructors) {\r\n\t\t\tpts = constructor.getParameterTypes();\r\n\t\t\tif (ClassUtil.isAllAssignableFrom(pts, parameterTypes)) {\r\n\t\t\t\t// \u6784\u9020\u53ef\u8bbf\u95ee\r\n\t\t\t\tconstructor.setAccessible(true);\r\n\t\t\t\treturn (Constructor<T>) constructor;\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn null;\r\n\t}"}, {"instruction": "This method initializes this panel.", "input": "Java", "output": "private void initialize() {\r\n\t\tthis.setName(Constant.messages.getString(\"httpsessions.options.title\"));\r\n\t\tthis.setLayout(new GridBagLayout());\r\n\r\n\t\tGridBagConstraints gbc = new GridBagConstraints();\r\n\t\tgbc.gridx = 0;\r\n\t\tgbc.weightx = 1.0;\r\n\t\tgbc.anchor = GridBagConstraints.LINE_START;\r\n\t\tgbc.fill = GridBagConstraints.BOTH;\r\n\t\t\r\n\t\tJLabel tokenNamesLabel = new JLabel();\r\n\t\ttokenNamesLabel.setText(Constant.messages.getString(\"httpsessions.options.label.tokens\"));\r\n\r\n\t\tthis.add(tokenNamesLabel, gbc);\r\n\r\n\t\ttokensOptionsPanel = new HttpSessionTokensMultipleOptionsPanel(getDefaultTokensModel());\r\n\t\t\r\n\t\tgbc.weighty = 1.0;\r\n\t\tthis.add(tokensOptionsPanel, gbc);\r\n\t\t\r\n\t\tgbc.weighty = 0.0;\r\n\t\tgbc.insets = new Insets(10, 2, 2, 2);\r\n\t\tthis.add(getChkProxyOnly(), gbc);\r\n\t}"}, {"instruction": "Acquire the lock object.\n\n@param lock the lock\n@return true, if successful", "input": "Java", "output": "public boolean acquire(final Lock lock) {\n        lock.setUniqueId(this.uniqueId);\n        if (this.lockTimeout > 0) {\n            lock.setExpirationDate(ZonedDateTime.now(ZoneOffset.UTC).plusSeconds(this.lockTimeout));\n        } else {\n            lock.setExpirationDate(null);\n        }\n        var success = false;\n        try {\n            if (lock.getApplicationId() != null) {\n                this.entityManager.merge(lock);\n            } else {\n                lock.setApplicationId(this.applicationId);\n                this.entityManager.persist(lock);\n            }\n            success = true;\n        } catch (final Exception e) {\n            success = false;\n            if (LOGGER.isDebugEnabled()) {\n                LOGGER.debug(\"[{}] could not obtain [{}] lock.\", this.uniqueId, this.applicationId, e);\n            } else {\n                LOGGER.info(\"[{}] could not obtain [{}] lock.\", this.uniqueId, this.applicationId);\n            }\n        }\n        return success;\n    }"}, {"instruction": "\u5bfb\u627e\u91cd\u53e0\n@param interval \u4e00\u4e2a\u533a\u95f4\uff0c\u4e0e\u8be5\u533a\u95f4\u91cd\u53e0\n@param direction \u65b9\u5411\uff0c\u8868\u660e\u91cd\u53e0\u533a\u95f4\u5728interval\u7684\u5de6\u8fb9\u8fd8\u662f\u53f3\u8fb9\n@return", "input": "Java", "output": "protected List<Intervalable> checkForOverlaps(Intervalable interval, Direction direction)\n    {\n\n        List<Intervalable> overlaps = new ArrayList<Intervalable>();\n        for (Intervalable currentInterval : this.intervals)\n        {\n            switch (direction)\n            {\n                case LEFT:\n                    if (currentInterval.getStart() <= interval.getEnd())\n                    {\n                        overlaps.add(currentInterval);\n                    }\n                    break;\n                case RIGHT:\n                    if (currentInterval.getEnd() >= interval.getStart())\n                    {\n                        overlaps.add(currentInterval);\n                    }\n                    break;\n            }\n        }\n        return overlaps;\n    }"}, {"instruction": "Make a prediction on a new data point using a Dimension Reduction model (PCA, GLRM)\n@param data A new data point.\n@return The prediction.\n@throws PredictException", "input": "Java", "output": "public DimReductionModelPrediction predictDimReduction(RowData data) throws PredictException {\n    double[] preds = preamble(ModelCategory.DimReduction, data);  // preds contains the x factor\n\n    DimReductionModelPrediction p = new DimReductionModelPrediction();\n    p.dimensions = preds;\n    if (m instanceof GlrmMojoModel && ((GlrmMojoModel) m)._archetypes_raw != null && this.enableGLRMReconstruct)  // only for verion 1.10 or higher\n      p.reconstructed = ((GlrmMojoModel) m).impute_data(preds, new double[m.nfeatures()], ((GlrmMojoModel) m)._nnums,\n              ((GlrmMojoModel) m)._ncats, ((GlrmMojoModel) m)._permutation, ((GlrmMojoModel) m)._reverse_transform,\n              ((GlrmMojoModel) m)._normMul, ((GlrmMojoModel) m)._normSub, ((GlrmMojoModel) m)._losses,\n              ((GlrmMojoModel) m)._transposed, ((GlrmMojoModel) m)._archetypes_raw, ((GlrmMojoModel) m)._catOffsets,\n              ((GlrmMojoModel) m)._numLevels);\n    return p;\n  }"}, {"instruction": "Creates a ZFS file system to migrate the data to.\n\n<p>\nThis has to be done while we still have an interactive access with the user, since it involves the password.\n\n<p>\nAn exception will be thrown if the operation fails. A normal completion means a success.\n\n@return\nThe ZFS dataset name to migrate the data to.", "input": "Java", "output": "private String createZfsFileSystem(final TaskListener listener, String rootUsername, String rootPassword) throws IOException, InterruptedException, ZFSException {\n        // capture the UID that Hudson runs under\n        // so that we can allow this user to do everything on this new partition\n        final int uid = LIBC.geteuid();\n        final int gid = LIBC.getegid();\n        passwd pwd = LIBC.getpwuid(uid);\n        if(pwd==null)\n            throw new IOException(\"Failed to obtain the current user information for \"+uid);\n        final String userName = pwd.pw_name;\n\n        final File home = Jenkins.getInstance().getRootDir();\n\n        // this is the actual creation of the file system.\n        // return true indicating a success\n        return SU.execute(listener, rootUsername, rootPassword, new Create(listener, home, uid, gid, userName));\n    }"}, {"instruction": "Keep the algorithm consistent with Calcite DateTimeUtils.julianDateFloor, but here\nwe take time zone into account.", "input": "Java", "output": "public static long timestampCeil(TimeUnitRange range, long ts, TimeZone tz) {\n\t\t// assume that we are at UTC timezone, just for algorithm performance\n\t\tlong offset = tz.getOffset(ts);\n\t\tlong utcTs = ts + offset;\n\n\t\tswitch (range) {\n\t\t\tcase HOUR:\n\t\t\t\treturn ceil(utcTs, MILLIS_PER_HOUR) - offset;\n\t\t\tcase DAY:\n\t\t\t\treturn ceil(utcTs, MILLIS_PER_DAY) - offset;\n\t\t\tcase MONTH:\n\t\t\tcase YEAR:\n\t\t\tcase QUARTER:\n\t\t\t\tint days = (int) (utcTs / MILLIS_PER_DAY + EPOCH_JULIAN);\n\t\t\t\treturn julianDateFloor(range, days, false) * MILLIS_PER_DAY - offset;\n\t\t\tdefault:\n\t\t\t\t// for MINUTE and SECONDS etc...,\n\t\t\t\t// it is more effective to use arithmetic Method\n\t\t\t\tthrow new AssertionError(range);\n\t\t}\n\t}"}, {"instruction": "cancel given delegation token", "input": "Java", "output": "public void cancelDelegationToken(String delegationToken) throws HiveSQLException {\n    if (saslServer == null) {\n      throw new HiveSQLException(\n          \"Delegation token only supported over kerberos authentication\", \"08S01\");\n    }\n    try {\n      saslServer.cancelDelegationToken(delegationToken);\n    } catch (IOException e) {\n      throw new HiveSQLException(\n          \"Error canceling delegation token \" + delegationToken, \"08S01\", e);\n    }\n  }"}, {"instruction": "Returns an unmodifiable snapshot map ordered by the provided iterator. Beware that obtaining\nthe mappings is <em>NOT</em> a constant-time operation.\n\n@param iteratorSupplier the iterator\n@param limit the maximum number of entries\n@param transformer a function that unwraps the value\n@return an unmodifiable snapshot in the iterator's order", "input": "Java", "output": "Map<K, V> fixedSnapshot(Supplier<Iterator<Node<K, V>>> iteratorSupplier,\n      int limit, Function<V, V> transformer) {\n    requireArgument(limit >= 0);\n    evictionLock.lock();\n    try {\n      maintenance(/* ignored */ null);\n\n      int initialCapacity = Math.min(limit, size());\n      Iterator<Node<K, V>> iterator = iteratorSupplier.get();\n      Map<K, V> map = new LinkedHashMap<>(initialCapacity);\n      while ((map.size() < limit) && iterator.hasNext()) {\n        Node<K, V> node = iterator.next();\n        K key = node.getKey();\n        V value = transformer.apply(node.getValue());\n        if ((key != null) && (value != null) && node.isAlive()) {\n          map.put(key, value);\n        }\n      }\n      return Collections.unmodifiableMap(map);\n    } finally {\n      evictionLock.unlock();\n    }\n  }"}, {"instruction": "Validate.\n\n@param profile the profile", "input": "Java", "output": "@JsonIgnore\n    public void validate(final CommonProfile profile) {\n        if (StringUtils.isBlank(getClientId())) {\n            throw new InvalidResourceSetException(HttpStatus.BAD_REQUEST.value(), \"Authentication request does contain a client id\");\n        }\n\n        if (getScopes().isEmpty()) {\n            throw new InvalidResourceSetException(HttpStatus.BAD_REQUEST.value(), \"Resource set registration is missing scopes\");\n        }\n\n        if (!getOwner().equals(profile.getId())) {\n            throw new InvalidResourceSetException(HttpStatus.FORBIDDEN.value(), \"Resource-set owner does not match the authenticated profile\");\n        }\n    }"}, {"instruction": "The Jaeger Tracer builder bean.\n\n@param configuration The configuration\n@return The builder", "input": "Java", "output": "@Singleton\n    @Primary\n    @Requires(classes = JaegerTracer.Builder.class)\n    JaegerTracer.Builder jaegerTracerBuilder(Configuration configuration) {\n        JaegerTracer.Builder tracerBuilder = resolveBuilder(configuration);\n        if (this.configuration.isExpandExceptionLogs()) {\n            tracerBuilder.withExpandExceptionLogs();\n        }\n        if (this.configuration.isZipkinSharedRpcSpan()) {\n            tracerBuilder.withZipkinSharedRpcSpan();\n        }\n        if (reporter != null) {\n            tracerBuilder.withReporter(reporter);\n        }\n        if (sampler != null) {\n            tracerBuilder.withSampler(sampler);\n        }\n        return tracerBuilder;\n    }"}, {"instruction": "Returns the next available {@link JsonElement} on the reader. Null if none available.\n\n@return the next available {@link JsonElement} on the reader. Null if none available.\n@throws JsonParseException if the incoming stream is malformed JSON.\n@since 1.4", "input": "Java", "output": "public JsonElement next() throws JsonParseException {\n    if (!hasNext()) {\n      throw new NoSuchElementException();\n    }\n    \n    try {\n      return Streams.parse(parser);\n    } catch (StackOverflowError e) {\n      throw new JsonParseException(\"Failed parsing JSON source to Json\", e);\n    } catch (OutOfMemoryError e) {\n      throw new JsonParseException(\"Failed parsing JSON source to Json\", e);\n    } catch (JsonParseException e) {\n      throw e.getCause() instanceof EOFException ? new NoSuchElementException() : e;\n    }\n  }"}, {"instruction": "gets quoted attribute value: QUOTATION *( quotechar / pair ) QUOTATION", "input": "Java", "output": "private String quotedAV() {\n    pos++;\n    beg = pos;\n    end = beg;\n    while (true) {\n\n      if (pos == length) {\n        throw new IllegalStateException(\"Unexpected end of DN: \" + dn);\n      }\n\n      if (chars[pos] == '\"') {\n        // enclosing quotation was found\n        pos++;\n        break;\n      } else if (chars[pos] == '\\\\') {\n        chars[end] = getEscaped();\n      } else {\n        // shift char: required for string with escaped chars\n        chars[end] = chars[pos];\n      }\n      pos++;\n      end++;\n    }\n\n    // skip trailing space chars before comma or semicolon.\n    // (compatibility with RFC 1779)\n    for (; pos < length && chars[pos] == ' '; pos++) {\n    }\n\n    return new String(chars, beg, end - beg);\n  }"}, {"instruction": "Enough attributes available to process? Check collection sizes and determine\nif we have enough data to move on.\n\n@param principal           the principal\n@param principalAttributes the principal attributes\n@return true /false", "input": "Java", "output": "protected boolean enoughAttributesAvailableToProcess(final String principal, final Map<String, Object> principalAttributes) {\n        if (!enoughRequiredAttributesAvailableToProcess(principalAttributes, this.requiredAttributes)) {\n            return false;\n        }\n        if (principalAttributes.size() < this.rejectedAttributes.size()) {\n            LOGGER.debug(\"The size of the principal attributes that are [{}] does not match defined rejected attributes, \"\n                + \"which means the principal is not carrying enough data to grant authorization\", principalAttributes);\n            return false;\n        }\n        return true;\n    }"}, {"instruction": "icon ( or generic html if icon not available )", "input": "Java", "output": "private String getSingleSlotHtml(TestSlot s, String icon) {\n    StringBuilder builder = new StringBuilder();\n    TestSession session = s.getSession();\n    if (icon != null) {\n      builder.append(\"<img \");\n      builder.append(\"src='\").append(icon).append(\"' width='16' height='16'\");\n    } else {\n      builder.append(\"<a href='#' \");\n    }\n\n    if (session != null) {\n      builder.append(\" class='busy' \");\n      builder.append(\" title='\").append(session.get(\"lastCommand\")).append(\"' \");\n    } else {\n      builder.append(\" title='\").append(s.getCapabilities()).append(\"'\");\n    }\n\n    if (icon != null) {\n      builder.append(\" />\\n\");\n    } else {\n      builder.append(\">\");\n      builder.append(s.getCapabilities().get(CapabilityType.BROWSER_NAME));\n      builder.append(\"</a>\");\n    }\n    return builder.toString();\n  }"}, {"instruction": "\u5b57\u8282\u6570\u8f6c\u5408\u9002\u5185\u5b58\u5927\u5c0f\n<p>\u4fdd\u75593\u4f4d\u5c0f\u6570</p>\n\n@param byteNum \u5b57\u8282\u6570\n@return \u5408\u9002\u5185\u5b58\u5927\u5c0f", "input": "Java", "output": "public static String byte2FitMemoryString(final long byteNum) {\n        if (byteNum < 0) {\n            return \"shouldn't be less than zero!\";\n        } else if (byteNum < MemoryConst.KB) {\n            return String.format(\"%d B\", byteNum);\n        } else if (byteNum < MemoryConst.MB) {\n            return String.format(\"%d KB\", byteNum / MemoryConst.KB);\n        } else if (byteNum < MemoryConst.GB) {\n            return String.format(\"%d MB\", byteNum / MemoryConst.MB);\n        } else {\n            return String.format(\"%d GB\", byteNum / MemoryConst.GB);\n        }\n    }"}, {"instruction": "\u83b7\u53d6\u6307\u5b9a\u7c7b\u578b\u5206\u7684\u9ed8\u8ba4\u503c<br>\n\u9ed8\u8ba4\u503c\u89c4\u5219\u4e3a\uff1a\n\n<pre>\n1\u3001\u5982\u679c\u4e3a\u539f\u59cb\u7c7b\u578b\uff0c\u8fd4\u56de0\n2\u3001\u975e\u539f\u59cb\u7c7b\u578b\u8fd4\u56de{@code null}\n</pre>\n\n@param clazz \u7c7b\n@return \u9ed8\u8ba4\u503c\n@since 3.0.8", "input": "Java", "output": "public static Object getDefaultValue(Class<?> clazz) {\r\n\t\tif (clazz.isPrimitive()) {\r\n\t\t\tif (long.class == clazz) {\r\n\t\t\t\treturn 0L;\r\n\t\t\t} else if (int.class == clazz) {\r\n\t\t\t\treturn 0;\r\n\t\t\t} else if (short.class == clazz) {\r\n\t\t\t\treturn (short) 0;\r\n\t\t\t} else if (char.class == clazz) {\r\n\t\t\t\treturn (char) 0;\r\n\t\t\t} else if (byte.class == clazz) {\r\n\t\t\t\treturn (byte) 0;\r\n\t\t\t} else if (double.class == clazz) {\r\n\t\t\t\treturn 0D;\r\n\t\t\t} else if (float.class == clazz) {\r\n\t\t\t\treturn 0f;\r\n\t\t\t} else if (boolean.class == clazz) {\r\n\t\t\t\treturn false;\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\treturn null;\r\n\t}"}, {"instruction": "\u8c03\u7528\u8f6c\u53d1\uff0c\u5982\u679c\u9700\u8981\u8f6c\u53d1\u8fd4\u56de\u8f6c\u53d1\u7ed3\u679c\uff0c\u5426\u5219\u8fd4\u56de<code>null</code>\n\n@return {@link HttpResponse}\uff0c\u65e0\u8f6c\u53d1\u8fd4\u56de <code>null</code>", "input": "Java", "output": "private HttpResponse sendRedirectIfPosible() {\r\n\t\tif (this.maxRedirectCount < 1) {\r\n\t\t\t// \u4e0d\u91cd\u5b9a\u5411\r\n\t\t\treturn null;\r\n\t\t}\r\n\r\n\t\t// \u624b\u52a8\u5b9e\u73b0\u91cd\u5b9a\u5411\r\n\t\tif (this.httpConnection.getHttpURLConnection().getInstanceFollowRedirects()) {\r\n\t\t\tint responseCode;\r\n\t\t\ttry {\r\n\t\t\t\tresponseCode = httpConnection.responseCode();\r\n\t\t\t} catch (IOException e) {\r\n\t\t\t\tthrow new HttpException(e);\r\n\t\t\t}\r\n\t\t\tif (responseCode != HttpURLConnection.HTTP_OK) {\r\n\t\t\t\tif (responseCode == HttpURLConnection.HTTP_MOVED_TEMP || responseCode == HttpURLConnection.HTTP_MOVED_PERM || responseCode == HttpURLConnection.HTTP_SEE_OTHER) {\r\n\t\t\t\t\tthis.url = httpConnection.header(Header.LOCATION);\r\n\t\t\t\t\tif (redirectCount < this.maxRedirectCount) {\r\n\t\t\t\t\t\tredirectCount++;\r\n\t\t\t\t\t\treturn execute();\r\n\t\t\t\t\t} else {\r\n\t\t\t\t\t\tStaticLog.warn(\"URL [{}] redirect count more than two !\", this.url);\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn null;\r\n\t}"}, {"instruction": "Return true if blocking is unnecessary.\nAlas, used in TWO places and the blocking API forces them to share here.", "input": "Java", "output": "@Override public boolean isReleasable() {\n    int r = _rwlock.get();\n    if( _key.home() ) {         // Called from lock_and_invalidate\n      // Home-key blocking: wait for active-GET count to fall to zero, or blocking on deleted object\n      return r <= 0;\n    } else {                    // Called from start_put\n      // Remote-key blocking: wait for active-PUT lock to hit -1\n      assert r == 2 || r == -1; // Either waiting (2) or done (-1) but not started(1)\n      return r == -1;           // done!\n    }\n  }"}, {"instruction": "init fxml when loaded.", "input": "Java", "output": "@PostConstruct\n    public void init() {\n        Timeline task = new Timeline(\n            new KeyFrame(\n                Duration.ZERO,\n                new KeyValue(progress1.progressProperty(), 0),\n                new KeyValue(progress2.progressProperty(), 0),\n                new KeyValue(progress2.secondaryProgressProperty(), 0.5)),\n            new KeyFrame(\n                Duration.seconds(1),\n                new KeyValue(progress2.secondaryProgressProperty(), 1)),\n            new KeyFrame(\n                Duration.seconds(2),\n                new KeyValue(progress1.progressProperty(), 1),\n                new KeyValue(progress2.progressProperty(), 1)));\n        task.setCycleCount(Timeline.INDEFINITE);\n        task.play();\n    }"}, {"instruction": "Get the int value of a transaction isolation level by name.\n\n@param transactionIsolationName the name of the transaction isolation level\n@return the int value of the isolation level or -1", "input": "Java", "output": "public static int getTransactionIsolation(final String transactionIsolationName) {\n      if (transactionIsolationName != null) {\n         try {\n            // use the english locale to avoid the infamous turkish locale bug\n            final String upperCaseIsolationLevelName = transactionIsolationName.toUpperCase(Locale.ENGLISH);\n            return IsolationLevel.valueOf(upperCaseIsolationLevelName).getLevelId();\n         } catch (Exception e) {\n            throw new IllegalArgumentException(\"Invalid transaction isolation value: \" + transactionIsolationName);\n         }\n      }\n\n      return -1;\n   }"}, {"instruction": "Reads until {@code in} is exhausted or the deadline has been reached. This is careful to not\nextend the deadline if one exists already.", "input": "Java", "output": "public static boolean skipAll(Source source, int duration, TimeUnit timeUnit) throws IOException {\n    long now = System.nanoTime();\n    long originalDuration = source.timeout().hasDeadline()\n        ? source.timeout().deadlineNanoTime() - now\n        : Long.MAX_VALUE;\n    source.timeout().deadlineNanoTime(now + Math.min(originalDuration, timeUnit.toNanos(duration)));\n    try {\n      Buffer skipBuffer = new Buffer();\n      while (source.read(skipBuffer, 8192) != -1) {\n        skipBuffer.clear();\n      }\n      return true; // Success! The source has been exhausted.\n    } catch (InterruptedIOException e) {\n      return false; // We ran out of time before exhausting the source.\n    } finally {\n      if (originalDuration == Long.MAX_VALUE) {\n        source.timeout().clearDeadline();\n      } else {\n        source.timeout().deadlineNanoTime(now + originalDuration);\n      }\n    }\n  }"}, {"instruction": "{@inheritDoc}.\n<p>\nChecks whether the IP should even be paid attention to,\nthen does a reverse DNS lookup, and if it matches the supplied pattern, performs SPNEGO\nelse skips the process.\n\n@param remoteIp The remote ip address to validate", "input": "Java", "output": "@Override\n    protected boolean shouldDoSpnego(final String remoteIp) {\n        val ipCheck = ipPatternCanBeChecked(remoteIp);\n        if (ipCheck && !ipPatternMatches(remoteIp)) {\n            return false;\n        }\n        val hostName = getRemoteHostName(remoteIp);\n        LOGGER.debug(\"Retrieved host name for the remote ip is [{}]\", hostName);\n        return this.hostNamePatternString.matcher(hostName).find();\n    }"}, {"instruction": "This method initializes this", "input": "Java", "output": "private void initialize() {\r\n        this.setName(Constant.messages.getString(\"pscan.options.name\"));\r\n        this.setLayout(new GridBagLayout());\r\n\r\n        GridBagConstraints gbc = new GridBagConstraints();\r\n        gbc.gridx = 0;\r\n        gbc.weightx = 1.0;\r\n        gbc.anchor = GridBagConstraints.LINE_START;\r\n        gbc.fill = GridBagConstraints.BOTH;\r\n        \r\n        this.add(new JLabel(Constant.messages.getString(\"pscan.options.header\")), gbc);\r\n\r\n        scannersOptionsPanel = new ScannersMultipleOptionsPanel(getTableModel());\r\n        \r\n        gbc.weighty = 1.0;\r\n        this.add(scannersOptionsPanel, gbc);\r\n        \r\n        //gbc.weighty = 0.0;\r\n\t}"}, {"instruction": "A part of the deviance portion of the saddle point approximation.\n<p>\nReferences:\n<ol>\n<li>Catherine Loader (2000). \"Fast and Accurate Computation of Binomial\nProbabilities.\". <a target=\"_blank\"\nhref=\"http://www.herine.net/stat/papers/dbinom.pdf\">\nhttp://www.herine.net/stat/papers/dbinom.pdf</a></li>\n</ol>\n</p>\n\n@param x  the x value.\n@param mu the average.\n@return a part of the deviance.", "input": "Java", "output": "public static double getDeviancePart(double x, double mu) {\n        double ret;\n        if (FastMath.abs(x - mu) < 0.1 * (x + mu)) {\n            double d = x - mu;\n            double v = d / (x + mu);\n            double s1 = v * d;\n            double s = Double.NaN;\n            double ej = 2.0 * x * v;\n            v = v * v;\n            int j = 1;\n            while (s1 != s) {\n                s = s1;\n                ej *= v;\n                s1 = s + ej / ((j * 2) + 1);\n                ++j;\n            }\n            ret = s1;\n        } else {\n            ret = x * FastMath.log(x / mu) + mu - x;\n        }\n        return ret;\n    }"}, {"instruction": "\u589e\u52a0\u4e00\u4e2a\u6bb5\u843d\n\n@param align \u6bb5\u843d\u5bf9\u9f50\u65b9\u5f0f{@link ParagraphAlignment}\n@param font \u5b57\u4f53\u4fe1\u606f{@link Font}\n@param texts \u6bb5\u843d\u4e2d\u7684\u6587\u672c\uff0c\u652f\u6301\u591a\u4e2a\u6587\u672c\u4f5c\u4e3a\u4e00\u4e2a\u6bb5\u843d\n@return this", "input": "Java", "output": "public Word07Writer addText(ParagraphAlignment align, Font font, String... texts) {\r\n\t\tfinal XWPFParagraph p = this.doc.createParagraph();\r\n\t\tif (null != align) {\r\n\t\t\tp.setAlignment(align);\r\n\t\t}\r\n\t\tif (ArrayUtil.isNotEmpty(texts)) {\r\n\t\t\tXWPFRun run;\r\n\t\t\tfor (String text : texts) {\r\n\t\t\t\trun = p.createRun();\r\n\t\t\t\trun.setText(text);\r\n\t\t\t\tif (null != font) {\r\n\t\t\t\t\trun.setFontFamily(font.getFamily());\r\n\t\t\t\t\trun.setFontSize(font.getSize());\r\n\t\t\t\t\trun.setBold(font.isBold());\r\n\t\t\t\t\trun.setItalic(font.isItalic());\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn this;\r\n\t}"}, {"instruction": "Private helper that fetches the Instances for each application.\n@param serviceId of the service that the instance list should be returned for\n@return List of instances for a given service id\n@throws Exception - retrieving and marshalling service instances may result in an\nException", "input": "Java", "output": "@Override\n\tprotected List<Instance> getInstancesForApp(String serviceId) throws Exception {\n\t\tList<Instance> instances = new ArrayList<>();\n\t\tlog.info(\"Fetching instances for app: \" + serviceId);\n\t\tApplication app = eurekaClient.getApplication(serviceId);\n\t\tif (app == null) {\n\t\t\tlog.warn(\"Eureka returned null for app: \" + serviceId);\n\t\t\treturn instances;\n\t\t}\n\t\ttry {\n\t\t\tList<InstanceInfo> instancesForApp = app.getInstances();\n\t\t\tif (instancesForApp != null) {\n\t\t\t\tlog.info(\"Received instance list for app: \" + serviceId + \", size=\"\n\t\t\t\t\t\t+ instancesForApp.size());\n\t\t\t\tfor (InstanceInfo iInfo : instancesForApp) {\n\t\t\t\t\tInstance instance = marshall(iInfo);\n\t\t\t\t\tif (instance != null) {\n\t\t\t\t\t\tinstances.add(instance);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tlog.warn(\"Failed to retrieve instances from Eureka\", e);\n\t\t}\n\t\treturn instances;\n\t}"}, {"instruction": "Subscribe provider list from direct url\n\n@param directUrl direct url of consume config\n@return Provider group list", "input": "Java", "output": "protected List<ProviderGroup> subscribeFromDirectUrl(String directUrl) {\n        List<ProviderGroup> result = new ArrayList<ProviderGroup>();\n        List<ProviderInfo> tmpProviderInfoList = new ArrayList<ProviderInfo>();\n        String[] providerStrs = StringUtils.splitWithCommaOrSemicolon(directUrl);\n        for (String providerStr : providerStrs) {\n            ProviderInfo providerInfo = convertToProviderInfo(providerStr);\n            if (providerInfo.getStaticAttr(ProviderInfoAttrs.ATTR_SOURCE) == null) {\n                providerInfo.setStaticAttr(ProviderInfoAttrs.ATTR_SOURCE, \"direct\");\n            }\n            tmpProviderInfoList.add(providerInfo);\n        }\n\n        result.add(new ProviderGroup(RpcConstants.ADDRESS_DIRECT_GROUP, tmpProviderInfoList));\n        return result;\n    }"}, {"instruction": "Takes the serialized accumulator results and tries to deserialize them using the provided\nclass loader.\n@param serializedAccumulators The serialized accumulator results.\n@param loader The class loader to use.\n@return The deserialized accumulator results.\n@throws IOException\n@throws ClassNotFoundException", "input": "Java", "output": "public static Map<String, OptionalFailure<Object>> deserializeAccumulators(\n\t\t\tMap<String, SerializedValue<OptionalFailure<Object>>> serializedAccumulators,\n\t\t\tClassLoader loader) throws IOException, ClassNotFoundException {\n\n\t\tif (serializedAccumulators == null || serializedAccumulators.isEmpty()) {\n\t\t\treturn Collections.emptyMap();\n\t\t}\n\n\t\tMap<String, OptionalFailure<Object>> accumulators = new HashMap<>(serializedAccumulators.size());\n\n\t\tfor (Map.Entry<String, SerializedValue<OptionalFailure<Object>>> entry : serializedAccumulators.entrySet()) {\n\n\t\t\tOptionalFailure<Object> value = null;\n\t\t\tif (entry.getValue() != null) {\n\t\t\t\tvalue = entry.getValue().deserializeValue(loader);\n\t\t\t}\n\n\t\t\taccumulators.put(entry.getKey(), value);\n\t\t}\n\n\t\treturn accumulators;\n\t}"}, {"instruction": "Is this computer reachable via the given address?\n\n@param ia      The address to check.\n@param timeout Timeout in seconds.", "input": "Java", "output": "public static boolean checkIsReachable(InetAddress ia, int timeout) throws IOException {\n        for (ComputerPinger pinger : ComputerPinger.all()) {\n            try {\n                if (pinger.isReachable(ia, timeout)) {\n                    return true;\n                }\n            } catch (IOException e) {\n                LOGGER.fine(\"Error checking reachability with \" + pinger + \": \" + e.getMessage());\n            }\n        }\n\n        return false;\n    }"}, {"instruction": "Create a proxy for the specified {@link Connection} instance.\n@param poolEntry the PoolEntry holding pool state\n@param connection the raw database Connection\n@param openStatements a reusable list to track open Statement instances\n@param leakTask the ProxyLeakTask for this connection\n@param now the current timestamp\n@param isReadOnly the default readOnly state of the connection\n@param isAutoCommit the default autoCommit state of the connection\n@return a proxy that wraps the specified {@link Connection}", "input": "Java", "output": "static ProxyConnection getProxyConnection(final PoolEntry poolEntry, final Connection connection, final FastList<Statement> openStatements, final ProxyLeakTask leakTask, final long now, final boolean isReadOnly, final boolean isAutoCommit)\n   {\n      // Body is replaced (injected) by JavassistProxyFactory\n      throw new IllegalStateException(\"You need to run the CLI build and you need target/classes in your classpath to run.\");\n   }"}, {"instruction": "Get the cacheKeys of all the ASG to which query AWS for.\n\n<p>\nThe names are obtained from the {@link com.netflix.eureka.registry.InstanceRegistry} which is then\nused for querying the AWS.\n</p>\n\n@return the set of ASG cacheKeys (asgName + accountId).", "input": "Java", "output": "private Set<CacheKey> getCacheKeys() {\n        Set<CacheKey> cacheKeys = new HashSet<CacheKey>();\n        Applications apps = registry.getApplicationsFromLocalRegionOnly();\n        for (Application app : apps.getRegisteredApplications()) {\n            for (InstanceInfo instanceInfo : app.getInstances()) {\n                String localAccountId = getAccountId(instanceInfo, accountId);\n                String asgName = instanceInfo.getASGName();\n                if (asgName != null) {\n                    CacheKey key = new CacheKey(localAccountId, asgName);\n                    cacheKeys.add(key);\n                }\n            }\n        }\n\n        return cacheKeys;\n    }"}, {"instruction": "This is more advanced and does not make use of the stub.  You should not normally need to do\nthis, but here is how you would.", "input": "Java", "output": "void advancedAsyncCall() {\n    ClientCall<HelloRequest, HelloReply> call =\n        channel.newCall(GreeterGrpc.getSayHelloMethod(), CallOptions.DEFAULT);\n\n    final CountDownLatch latch = new CountDownLatch(1);\n\n    call.start(new ClientCall.Listener<HelloReply>() {\n\n      @Override\n      public void onClose(Status status, Metadata trailers) {\n        Verify.verify(status.getCode() == Status.Code.INTERNAL);\n        Verify.verify(status.getDescription().contains(\"Narwhal\"));\n        // Cause is not transmitted over the wire.\n        latch.countDown();\n      }\n    }, new Metadata());\n\n    call.sendMessage(HelloRequest.newBuilder().setName(\"Marge\").build());\n    call.halfClose();\n\n    if (!Uninterruptibles.awaitUninterruptibly(latch, 1, TimeUnit.SECONDS)) {\n      throw new RuntimeException(\"timeout!\");\n    }\n  }"}, {"instruction": "finds any stages for a dashboard that aren't mapped.\n@param dashboard\n@return a list of deploy PipelineStages that are not mapped", "input": "Java", "output": "private List<PipelineStage> findUnmappedStages(Dashboard dashboard,List<PipelineStage> pipelineStageList){\n        List<PipelineStage> unmappedStages = new ArrayList<>();\n\n        Map<PipelineStage, String> stageToEnvironmentNameMap = PipelineUtils.getStageToEnvironmentNameMap(dashboard);\n\n        for (PipelineStage systemStage : pipelineStageList) {\n            if (PipelineStageType.DEPLOY.equals(systemStage.getType())) {\n                String mappedName = stageToEnvironmentNameMap.get(systemStage);\n                if (mappedName == null || mappedName.isEmpty()) {\n                    unmappedStages.add(systemStage);\n                }\n            }\n        }\n\n        return unmappedStages;\n    }"}, {"instruction": "build consul service from url\n\n@param url a URL object\n@return ConsulService consul service", "input": "Java", "output": "public static ConsulService buildService(URL url) {\n        ConsulService service = new ConsulService();\n        service.setAddress(url.getHost());\n        service.setId(ConsulUtils.convertConsulSerivceId(url));\n        service.setName(url.getPath());\n        service.setPort(url.getPort());\n        List<String> tags = new ArrayList<String>();\n        String env = url.getParameter(Constants.TAG_ENVIRONMENT);\n        if(env != null) tags.add(env);\n        service.setTags(tags);\n\n        return service;\n    }"}, {"instruction": "Value for absent column, i. e. {@link NilColumnValueSelector}, should be equivalent to [null] during index merging.\n\nDuring index merging, if one of the merged indexes has absent columns, {@link StringDimensionMergerV9} ensures\nthat null value is present, and it has index = 0 after sorting, because sorting puts null first. See {@link\nStringDimensionMergerV9#hasNull} and the place where it is assigned.", "input": "Java", "output": "private static IndexedInts getRow(ColumnValueSelector s)\n  {\n    if (s instanceof DimensionSelector) {\n      return ((DimensionSelector) s).getRow();\n    } else if (s instanceof NilColumnValueSelector) {\n      return ZeroIndexedInts.instance();\n    } else {\n      throw new ISE(\n          \"ColumnValueSelector[%s], only DimensionSelector or NilColumnValueSelector is supported\",\n          s.getClass()\n      );\n    }\n  }"}, {"instruction": "Create a new event handler group that combines the consumers in this group with <code>otherHandlerGroup</code>.\n\n@param otherHandlerGroup the event handler group to combine.\n@return a new EventHandlerGroup combining the existing and new consumers into a single dependency group.", "input": "Java", "output": "public EventHandlerGroup<T> and(final EventHandlerGroup<T> otherHandlerGroup)\n    {\n        final Sequence[] combinedSequences = new Sequence[this.sequences.length + otherHandlerGroup.sequences.length];\n        System.arraycopy(this.sequences, 0, combinedSequences, 0, this.sequences.length);\n        System.arraycopy(\n            otherHandlerGroup.sequences, 0,\n            combinedSequences, this.sequences.length, otherHandlerGroup.sequences.length);\n        return new EventHandlerGroup<>(disruptor, consumerRepository, combinedSequences);\n    }"}, {"instruction": "This should only be called as last operation from a method as this may adjust the underlying\narray of components and so affect the index etc.", "input": "Java", "output": "private void consolidateIfNeeded() {\n        // Consolidate if the number of components will exceed the allowed maximum by the current\n        // operation.\n        int size = componentCount;\n        if (size > maxNumComponents) {\n            final int capacity = components[size - 1].endOffset;\n\n            ByteBuf consolidated = allocBuffer(capacity);\n            lastAccessed = null;\n\n            // We're not using foreach to avoid creating an iterator.\n            for (int i = 0; i < size; i ++) {\n                components[i].transferTo(consolidated);\n            }\n\n            components[0] = new Component(consolidated, 0, 0, capacity, consolidated);\n            removeCompRange(1, size);\n        }\n    }"}, {"instruction": "Whether there are anymore records\n\n@return", "input": "Java", "output": "@Override\n    public boolean hasNext() {\n        if(next != null){\n            return true;\n        }\n        if(!recordReader.hasNext()){\n            return false;\n        }\n\n        //Prefetch, until we find one that isn't filtered out - or we run out of data\n        while(next == null && recordReader.hasNext()){\n            Record r = recordReader.nextRecord();\n            List<Writable> temp = transformProcess.execute(r.getRecord());\n            if(temp == null){\n                continue;\n            }\n            next = new org.datavec.api.records.impl.Record(temp, r.getMetaData());\n        }\n\n        return next != null;\n    }"}, {"instruction": "Init method validates configuration defined using", "input": "Java", "output": "protected void init() {\n        if (storage.size() != vocabCache.numWords())\n            throw new RuntimeException(\"Number of words in Vocab isn't matching number of stored Vectors. vocab: [\"\n                            + vocabCache.numWords() + \"]; storage: [\" + storage.size() + \"]\");\n\n        // initializing device cache\n        for (int i = 0; i < Nd4j.getAffinityManager().getNumberOfDevices(); i++) {\n            cacheWrtDevice.add(new ConcurrentHashMap<Integer, INDArray>());\n        }\n    }"}, {"instruction": "Get all the bytes of a given chunk.\nUseful for previewing sections of files.\n\n@param chkIdx index of desired chunk\n@return array of initial bytes", "input": "Java", "output": "public byte[] getPreviewChunkBytes(int chkIdx) {\n    if (chkIdx >= nChunks())\n      throw new H2OIllegalArgumentException(\"Asked for chunk index beyond the number of chunks.\");\n    if (chkIdx == 0)\n      return chunkForChunkIdx(chkIdx)._mem;\n    else { //must eat partial lines\n      // FIXME: a hack to consume partial lines since each preview chunk is seen as cidx=0\n      byte[] mem = chunkForChunkIdx(chkIdx)._mem;\n      int i = 0, j = mem.length-1;\n      while (i < mem.length && mem[i] != CHAR_CR && mem[i] != CHAR_LF) i++;\n      while (j > i && mem[j] != CHAR_CR && mem[j] != CHAR_LF) j--;\n      if (j-i > 1) return Arrays.copyOfRange(mem,i,j);\n      else return null;\n    }\n  }"}, {"instruction": "Reads the given file line-by-line and creates a data stream that contains a string with the\ncontents of each such line. The {@link java.nio.charset.Charset} with the given name will be\nused to read the files.\n\n<p><b>NOTES ON CHECKPOINTING: </b> The source monitors the path, creates the\n{@link org.apache.flink.core.fs.FileInputSplit FileInputSplits} to be processed,\nforwards them to the downstream {@link ContinuousFileReaderOperator readers} to read the actual data,\nand exits, without waiting for the readers to finish reading. This implies that no more checkpoint\nbarriers are going to be forwarded after the source exits, thus having no checkpoints after that point.\n\n@param filePath\nThe path of the file, as a URI (e.g., \"file:///some/local/file\" or \"hdfs://host:port/file/path\")\n@param charsetName\nThe name of the character set used to read the file\n@return The data stream that represents the data read from the given file as text lines", "input": "Java", "output": "public DataStreamSource<String> readTextFile(String filePath, String charsetName) {\n\t\tPreconditions.checkArgument(!StringUtils.isNullOrWhitespaceOnly(filePath), \"The file path must not be null or blank.\");\n\n\t\tTextInputFormat format = new TextInputFormat(new Path(filePath));\n\t\tformat.setFilesFilter(FilePathFilter.createDefaultFilter());\n\t\tTypeInformation<String> typeInfo = BasicTypeInfo.STRING_TYPE_INFO;\n\t\tformat.setCharsetName(charsetName);\n\n\t\treturn readFile(format, filePath, FileProcessingMode.PROCESS_ONCE, -1, typeInfo);\n\t}"}, {"instruction": "Implentation for Iterable interface.\nPlease note: each call for iterator() resets underlying SentenceIterator to the beginning;\n\n@return", "input": "Java", "output": "@Override\n    public Iterator<String> iterator() {\n        this.reset();\n        Iterator<String> ret = new Iterator<String>() {\n            @Override\n            public boolean hasNext() {\n                return BasicLineIterator.this.hasNext();\n            }\n\n            @Override\n            public String next() {\n                return BasicLineIterator.this.nextSentence();\n            }\n\n            @Override\n            public void remove() {\n                throw new UnsupportedOperationException();\n            }\n        };\n\n        return ret;\n    }"}, {"instruction": "Return the appropriate {@link AnnotationAttributes} from the\n{@link AnnotationMetadata}. By default this method will return attributes for\n{@link #getAnnotationClass()}.\n@param metadata the annotation metadata\n@return annotation attributes", "input": "Java", "output": "protected AnnotationAttributes getAttributes(AnnotationMetadata metadata) {\n\t\tString name = getAnnotationClass().getName();\n\t\tAnnotationAttributes attributes = AnnotationAttributes\n\t\t\t\t.fromMap(metadata.getAnnotationAttributes(name, true));\n\t\tAssert.notNull(attributes,\n\t\t\t\t() -> \"No auto-configuration attributes found. Is \"\n\t\t\t\t\t\t+ metadata.getClassName() + \" annotated with \"\n\t\t\t\t\t\t+ ClassUtils.getShortName(name) + \"?\");\n\t\treturn attributes;\n\t}"}, {"instruction": "add additional config entries from the Flink config to the Hadoop config", "input": "Java", "output": "private org.apache.hadoop.conf.Configuration loadHadoopConfigFromFlink() {\n\t\torg.apache.hadoop.conf.Configuration hadoopConfig = new org.apache.hadoop.conf.Configuration();\n\t\tfor (String key : flinkConfig.keySet()) {\n\t\t\tfor (String prefix : flinkConfigPrefixes) {\n\t\t\t\tif (key.startsWith(prefix)) {\n\t\t\t\t\tString newKey = hadoopConfigPrefix + key.substring(prefix.length());\n\t\t\t\t\tString newValue = fixHadoopConfig(key, flinkConfig.getString(key, null));\n\t\t\t\t\thadoopConfig.set(newKey, newValue);\n\n\t\t\t\t\tLOG.debug(\"Adding Flink config entry for {} as {} to Hadoop config\", key, newKey);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn hadoopConfig;\n\t}"}, {"instruction": "ZAP: New method checking for connection upgrade.\n\n@return True if this connection should be upgraded to WebSockets.", "input": "Java", "output": "public boolean isWebSocketUpgrade() {\r\n\t\tif (!getResponseHeader().isEmpty()) {\r\n\t\t\tString connectionHeader = getResponseHeader().getHeader(\"connection\");\r\n\t\t\tString upgradeHeader = getResponseHeader().getHeader(\"upgrade\");\r\n\t\t\t\r\n\t\t\tif (connectionHeader != null && connectionHeader.equalsIgnoreCase(\"upgrade\")) {\r\n\t\t\t\tif (upgradeHeader != null && upgradeHeader.equalsIgnoreCase(\"websocket\")) {\r\n\t\t\t\t\treturn true;\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\t\t\r\n\t\treturn false;\r\n\t}"}, {"instruction": "Return the absolute temp dir for given web server.\n@param prefix server name\n@return the temp dir for given server.", "input": "Java", "output": "protected final File createTempDir(String prefix) {\n\t\ttry {\n\t\t\tFile tempDir = File.createTempFile(prefix + \".\", \".\" + getPort());\n\t\t\ttempDir.delete();\n\t\t\ttempDir.mkdir();\n\t\t\ttempDir.deleteOnExit();\n\t\t\treturn tempDir;\n\t\t}\n\t\tcatch (IOException ex) {\n\t\t\tthrow new WebServerException(\n\t\t\t\t\t\"Unable to create tempDir. java.io.tmpdir is set to \"\n\t\t\t\t\t\t\t+ System.getProperty(\"java.io.tmpdir\"),\n\t\t\t\t\tex);\n\t\t}\n\t}"}, {"instruction": "Poll for a task of a certain type.\n\n@param taskType Task name\n@param workerId Id of the workflow\n@param domain   Domain of the workflow\n@return polled {@link Task}", "input": "Java", "output": "@Service\n    public Task poll(String taskType, String workerId, String domain) {\n        LOGGER.debug(\"Task being polled: /tasks/poll/{}?{}&{}\", taskType, workerId, domain);\n        Task task = executionService.getLastPollTask(taskType, workerId, domain);\n        if (task != null) {\n            LOGGER.debug(\"The Task {} being returned for /tasks/poll/{}?{}&{}\", task, taskType, workerId, domain);\n        }\n        Monitors.recordTaskPollCount(taskType, domain, 1);\n        return task;\n    }"}, {"instruction": "\u52a0\u5bc6\u6216\u89e3\u5bc6\u6307\u5b9a\u503c\uff0c\u8c03\u7528\u6b64\u65b9\u6cd5\u524d\u9700\u521d\u59cb\u5316\u5bc6\u94a5\n\n@param msg \u8981\u52a0\u5bc6\u6216\u89e3\u5bc6\u7684\u6d88\u606f\n@return \u52a0\u5bc6\u6216\u89e3\u5bc6\u540e\u7684\u503c", "input": "Java", "output": "public byte[] crypt(final byte[] msg) {\r\n\t\tfinal ReadLock readLock = this.lock.readLock();\r\n\t\treadLock.lock();\r\n\t\tbyte[] code;\r\n\t\ttry {\r\n\t\t\tfinal int[] sbox = this.sbox.clone();\r\n\t\t\tcode = new byte[msg.length];\r\n\t\t\tint i = 0;\r\n\t\t\tint j = 0;\r\n\t\t\tfor (int n = 0; n < msg.length; n++) {\r\n\t\t\t\ti = (i + 1) % SBOX_LENGTH;\r\n\t\t\t\tj = (j + sbox[i]) % SBOX_LENGTH;\r\n\t\t\t\tswap(i, j, sbox);\r\n\t\t\t\tint rand = sbox[(sbox[i] + sbox[j]) % SBOX_LENGTH];\r\n\t\t\t\tcode[n] = (byte) (rand ^ msg[n]);\r\n\t\t\t}\r\n\t\t} finally {\r\n\t\t\treadLock.unlock();\r\n\t\t}\r\n\t\treturn code;\r\n\t}"}, {"instruction": "Parse the from clause\nzhongshu-comment \u53ea\u89e3\u6790\u4e86\u4e00\u822c\u67e5\u8be2\u548cjoin\u67e5\u8be2\uff0c\u6ca1\u6709\u89e3\u6790\u5b50\u67e5\u8be2\n@param from the from clause.\n@return list of From objects represents all the sources.", "input": "Java", "output": "private List<From> findFrom(SQLTableSource from) {\n        //zhongshu-comment class1.isAssignableFrom(class2) class2\u662f\u4e0d\u662fclass1\u7684\u5b50\u7c7b\u6216\u8005\u5b50\u63a5\u53e3\n        //\u6539\u6210\u7528instanceof \u5e94\u8be5\u4e5f\u884c\u5427\uff1afrom instanceof SQLExprTableSource\n        boolean isSqlExprTable = from.getClass().isAssignableFrom(SQLExprTableSource.class);\n\n        if (isSqlExprTable) {\n            SQLExprTableSource fromExpr = (SQLExprTableSource) from;\n            String[] split = fromExpr.getExpr().toString().split(\",\");\n\n            ArrayList<From> fromList = new ArrayList<>();\n            for (String source : split) {\n                fromList.add(new From(source.trim(), fromExpr.getAlias()));\n            }\n            return fromList;\n        }\n\n        SQLJoinTableSource joinTableSource = ((SQLJoinTableSource) from);\n        List<From> fromList = new ArrayList<>();\n        fromList.addAll(findFrom(joinTableSource.getLeft()));\n        fromList.addAll(findFrom(joinTableSource.getRight()));\n        return fromList;\n    }"}, {"instruction": "Attach a list of network endpoints to the specified network endpoint group.\n\n<p>Sample code:\n\n<pre><code>\ntry (NetworkEndpointGroupClient networkEndpointGroupClient = NetworkEndpointGroupClient.create()) {\nProjectZoneNetworkEndpointGroupName networkEndpointGroup = ProjectZoneNetworkEndpointGroupName.of(\"[PROJECT]\", \"[ZONE]\", \"[NETWORK_ENDPOINT_GROUP]\");\nNetworkEndpointGroupsAttachEndpointsRequest networkEndpointGroupsAttachEndpointsRequestResource = NetworkEndpointGroupsAttachEndpointsRequest.newBuilder().build();\nOperation response = networkEndpointGroupClient.attachNetworkEndpointsNetworkEndpointGroup(networkEndpointGroup, networkEndpointGroupsAttachEndpointsRequestResource);\n}\n</code></pre>\n\n@param networkEndpointGroup The name of the network endpoint group where you are attaching\nnetwork endpoints to. It should comply with RFC1035.\n@param networkEndpointGroupsAttachEndpointsRequestResource\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "@BetaApi\n  public final Operation attachNetworkEndpointsNetworkEndpointGroup(\n      ProjectZoneNetworkEndpointGroupName networkEndpointGroup,\n      NetworkEndpointGroupsAttachEndpointsRequest\n          networkEndpointGroupsAttachEndpointsRequestResource) {\n\n    AttachNetworkEndpointsNetworkEndpointGroupHttpRequest request =\n        AttachNetworkEndpointsNetworkEndpointGroupHttpRequest.newBuilder()\n            .setNetworkEndpointGroup(\n                networkEndpointGroup == null ? null : networkEndpointGroup.toString())\n            .setNetworkEndpointGroupsAttachEndpointsRequestResource(\n                networkEndpointGroupsAttachEndpointsRequestResource)\n            .build();\n    return attachNetworkEndpointsNetworkEndpointGroup(request);\n  }"}, {"instruction": "Given all of the thread pool configuration, what is the actual maximumSize applied to the thread pool\nvia {@link ThreadPoolExecutor#setMaximumPoolSize(int)}\n\nCases:\n1) allowMaximumSizeToDivergeFromCoreSize == false: maximumSize is set to coreSize\n2) allowMaximumSizeToDivergeFromCoreSize == true, maximumSize >= coreSize: thread pool has different core/max sizes, so return the configured max\n3) allowMaximumSizeToDivergeFromCoreSize == true, maximumSize < coreSize: threadpool incorrectly configured, use coreSize for max size\n@return actually configured maximum size of threadpool", "input": "Java", "output": "public Integer actualMaximumSize() {\n        final int coreSize = coreSize().get();\n        final int maximumSize = maximumSize().get();\n        if (getAllowMaximumSizeToDivergeFromCoreSize().get()) {\n            if (coreSize > maximumSize) {\n                return coreSize;\n            } else {\n                return maximumSize;\n            }\n        } else {\n            return coreSize;\n        }\n    }"}, {"instruction": "Removes the token with the given {@code name}.\n<p>\nThe call to this method has no effect if the given {@code name} is null or empty, or a token with the given {@code name}\ndoes not exist.\n\n@param name the name of the token that will be removed", "input": "Java", "output": "public void removeToken(String name) {\r\n        if (name == null || name.isEmpty()) {\r\n            return;\r\n        }\r\n\r\n        for (Iterator<AntiCsrfParamToken> it = tokens.iterator(); it.hasNext();) {\r\n            AntiCsrfParamToken token = it.next();\r\n            if (name.equals(token.getName())) {\r\n                it.remove();\r\n                if (token.isEnabled()) {\r\n                    this.enabledTokensNames.remove(name);\r\n                }\r\n                break;\r\n            }\r\n        }\r\n    }"}, {"instruction": "Utility method for determining the appropriate preprocessor for CNN layers, such as {@link ConvolutionLayer} and\n{@link SubsamplingLayer}\n\n@param inputType     Input type to get the preprocessor for\n@return              Null if no preprocessor is required; otherwise the appropriate preprocessor for the given input type", "input": "Java", "output": "public static InputPreProcessor getPreProcessorForInputTypeCnn3DLayers(InputType inputType, String layerName) {\n        switch (inputType.getType()) {\n            case FF:\n                log.info(\"Automatic addition of FF -> CNN3D preprocessors: not yet implemented (layer name: \\\"\"\n                                + layerName + \"\\\")\");\n                return null;\n            case RNN:\n                log.warn(\"Automatic addition of RNN -> CNN3D preprocessors: not yet implemented (layer name: \\\"\"\n                                + layerName + \"\\\")\");\n                return null;\n            // TODO: handle CNN to CNN3D\n            case CNN3D:\n                return null;\n            default:\n                throw new RuntimeException(\"Unknown input type: \" + inputType);\n        }\n    }"}, {"instruction": "This code is more complicated than you would think because we might require multiple\ntransferTo invocations in order to transfer a single MessageWithHeader to avoid busy waiting.\n\nThe contract is that the caller will ensure position is properly set to the total number\nof bytes transferred so far (i.e. value returned by transferred()).", "input": "Java", "output": "@Override\n  public long transferTo(final WritableByteChannel target, final long position) throws IOException {\n    Preconditions.checkArgument(position == totalBytesTransferred, \"Invalid position.\");\n    // Bytes written for header in this call.\n    long writtenHeader = 0;\n    if (header.readableBytes() > 0) {\n      writtenHeader = copyByteBuf(header, target);\n      totalBytesTransferred += writtenHeader;\n      if (header.readableBytes() > 0) {\n        return writtenHeader;\n      }\n    }\n\n    // Bytes written for body in this call.\n    long writtenBody = 0;\n    if (body instanceof FileRegion) {\n      writtenBody = ((FileRegion) body).transferTo(target, totalBytesTransferred - headerLength);\n    } else if (body instanceof ByteBuf) {\n      writtenBody = copyByteBuf((ByteBuf) body, target);\n    }\n    totalBytesTransferred += writtenBody;\n\n    return writtenHeader + writtenBody;\n  }"}, {"instruction": "Convert the \"messy\" min/max values on a dataset to something clean. For example, 0.895732 becomes 1.0\n\n@param max   Maximum data point value\n@param min   Minimum data point value\n@param nTick Number of tick marks desired on chart (good setting: 5)\n@return double[] of length 2 - with new minimum and maximum", "input": "Java", "output": "public static double[] graphNiceRange(double max, double min, int nTick){\n        if(max == min || !Double.isFinite(max)){\n            if(max == 0.0 || !Double.isFinite(max)){\n                return new double[]{0.0, 1.0};\n            }\n\n            return graphNiceRange(1.5 * max, 0.5 * max, nTick);\n        }\n\n        double range = niceNum(max-min, false);\n        double d = niceNum(range / (nTick-1), true );\n        double graphMin = Math.floor(min/d)*d;\n        double graphMax = Math.ceil(max/d)*d;\n\n\n        return new double[]{graphMin, graphMax};\n    }"}, {"instruction": "Returns type variable equivalent to {@code element}.", "input": "Java", "output": "public static TypeVariableName get(TypeParameterElement element) {\n    String name = element.getSimpleName().toString();\n    List<? extends TypeMirror> boundsMirrors = element.getBounds();\n\n    List<TypeName> boundsTypeNames = new ArrayList<>();\n    for (TypeMirror typeMirror : boundsMirrors) {\n      boundsTypeNames.add(TypeName.get(typeMirror));\n    }\n\n    return TypeVariableName.of(name, boundsTypeNames);\n  }"}, {"instruction": "Configures which fields of the CSV file should be included and which should be skipped. The\npositions in the string (read from position 0 to its length) define whether the field at\nthe corresponding position in the CSV schema should be included.\nparser will look at the first {@code n} fields, where {@code n} is the length of the mask string\nThe parser will skip over all fields where the character at the corresponding position\nin the string is {@code '0'}, {@code 'F'}, or {@code 'f'} (representing the value\n{@code false}). The result contains the fields where the corresponding position in\nthe boolean array is {@code '1'}, {@code 'T'}, or {@code 't'} (representing the value {@code true}).\n\n@param mask The string mask defining which fields to include and which to skip.\n@return The CSV reader instance itself, to allow for fluent function chaining.", "input": "Java", "output": "public CsvReader includeFields(String mask) {\n\t\tboolean[] includedMask = new boolean[mask.length()];\n\n\t\tfor (int i = 0; i < mask.length(); i++) {\n\t\t\tchar c = mask.charAt(i);\n\t\t\tif (c == '1' || c == 'T' || c == 't') {\n\t\t\t\tincludedMask[i] = true;\n\t\t\t} else if (c != '0' && c != 'F' && c != 'f') {\n\t\t\t\tthrow new IllegalArgumentException(\"Mask string may contain only '0' and '1'.\");\n\t\t\t}\n\t\t}\n\n\t\treturn includeFields(includedMask);\n\t}"}, {"instruction": "Construct document from xml string.\n\n@param xmlString the xml string\n@return the document", "input": "Java", "output": "public static Document constructDocumentFromXml(final String xmlString) {\n        try {\n            val builder = new SAXBuilder();\n            builder.setFeature(\"http://xml.org/sax/features/external-general-entities\", false);\n            builder.setFeature(\"http://apache.org/xml/features/disallow-doctype-decl\", true);\n            return builder.build(new ByteArrayInputStream(xmlString.getBytes(Charset.defaultCharset())));\n        } catch (final Exception e) {\n            LOGGER.error(e.getMessage(), e);\n            return null;\n        }\n    }"}, {"instruction": "This method adds specified SequenceElement to vocabulary\n\n@param element the word to add", "input": "Java", "output": "@Override\n    public boolean addToken(T element) {\n        boolean ret = false;\n        T oldElement = vocabulary.putIfAbsent(element.getStorageId(), element);\n        if (oldElement == null) {\n            //putIfAbsent added our element\n            if (element.getLabel() != null) {\n                extendedVocabulary.put(element.getLabel(), element);\n            }\n            oldElement = element;\n            ret = true;\n        } else {\n            oldElement.incrementSequencesCount(element.getSequencesCount());\n            oldElement.increaseElementFrequency((int) element.getElementFrequency());\n        }\n        totalWordCount.addAndGet((long) oldElement.getElementFrequency());\n        return ret;\n    }"}, {"instruction": "Prints out the command line to the listener with some portions masked to prevent sensitive information from being\nrecorded on the listener.\n\n@param cmd     The commands\n@param mask    An array of booleans which control whether a cmd element should be masked (<code>true</code>) or\nremain unmasked (<code>false</code>).\n@param workDir The work dir.", "input": "Java", "output": "protected final void maskedPrintCommandLine(@Nonnull List<String> cmd, @CheckForNull boolean[] mask, @CheckForNull FilePath workDir) {\n        if(mask==null) {\n            printCommandLine(cmd.toArray(new String[0]),workDir);\n            return;\n        }\n        \n        assert mask.length == cmd.size();\n        final String[] masked = new String[cmd.size()];\n        for (int i = 0; i < cmd.size(); i++) {\n            if (mask[i]) {\n                masked[i] = \"********\";\n            } else {\n                masked[i] = cmd.get(i);\n            }\n        }\n        printCommandLine(masked, workDir);\n    }"}, {"instruction": "See https://api.slack.com/methods/rtm.start.", "input": "Java", "output": "public RtmStartResponse rtmStart(String accessToken) throws IOException {\n    HttpUrl url = baseUrl.newBuilder(\"rtm.start\")\n        .addQueryParameter(\"token\", accessToken)\n        .build();\n    Request request = new Request.Builder()\n        .url(url)\n        .build();\n    Call call = httpClient.newCall(request);\n    try (Response response = call.execute()) {\n      JsonAdapter<RtmStartResponse> jsonAdapter = moshi.adapter(RtmStartResponse.class);\n      return jsonAdapter.fromJson(response.body().source());\n    }\n  }"}, {"instruction": "Create a new SAML object.\n\n@param <T>        the generic type\n@param objectType the object type\n@return the t", "input": "Java", "output": "@SneakyThrows\n    public <T extends SAMLObject> T newSamlObject(final Class<T> objectType) {\n        val qName = getSamlObjectQName(objectType);\n        val builder = (SAMLObjectBuilder<T>)\n            XMLObjectProviderRegistrySupport.getBuilderFactory().getBuilder(qName);\n        if (builder == null) {\n            throw new IllegalStateException(\"No SAML object builder is registered for class \" + objectType.getName());\n        }\n        return objectType.cast(builder.buildObject(qName));\n    }"}, {"instruction": "\u5e8f\u5217\u5316\u540e\u62f7\u8d1d\u6d41\u7684\u65b9\u5f0f\u514b\u9686<br>\n\u5bf9\u8c61\u5fc5\u987b\u5b9e\u73b0Serializable\u63a5\u53e3\n\n@param <T> \u5bf9\u8c61\u7c7b\u578b\n@param obj \u88ab\u514b\u9686\u5bf9\u8c61\n@return \u514b\u9686\u540e\u7684\u5bf9\u8c61\n@throws UtilException IO\u5f02\u5e38\u548cClassNotFoundException\u5c01\u88c5", "input": "Java", "output": "@SuppressWarnings(\"unchecked\")\r\n\tpublic static <T> T cloneByStream(T obj) {\r\n\t\tif (null == obj || false == (obj instanceof Serializable)) {\r\n\t\t\treturn null;\r\n\t\t}\r\n\t\tfinal FastByteArrayOutputStream byteOut = new FastByteArrayOutputStream();\r\n\t\tObjectOutputStream out = null;\r\n\t\ttry {\r\n\t\t\tout = new ObjectOutputStream(byteOut);\r\n\t\t\tout.writeObject(obj);\r\n\t\t\tout.flush();\r\n\t\t\tfinal ObjectInputStream in = new ObjectInputStream(new ByteArrayInputStream(byteOut.toByteArray()));\r\n\t\t\treturn (T) in.readObject();\r\n\t\t} catch (Exception e) {\r\n\t\t\tthrow new UtilException(e);\r\n\t\t} finally {\r\n\t\t\tIoUtil.close(out);\r\n\t\t}\r\n\t}"}, {"instruction": "\u8ba1\u7b97\u4e24\u4e2a\u65e5\u671f\u76f8\u5dee\u6708\u6570<br>\n\u5728\u975e\u91cd\u7f6e\u60c5\u51b5\u4e0b\uff0c\u5982\u679c\u8d77\u59cb\u65e5\u671f\u7684\u5929\u5c0f\u4e8e\u7ed3\u675f\u65e5\u671f\u7684\u5929\uff0c\u6708\u6570\u8981\u5c11\u7b971\uff08\u4e0d\u8db31\u4e2a\u6708\uff09\n\n@param isReset \u662f\u5426\u91cd\u7f6e\u65f6\u95f4\u4e3a\u8d77\u59cb\u65f6\u95f4\uff08\u91cd\u7f6e\u5929\u65f6\u5206\u79d2\uff09\n@return \u76f8\u5dee\u6708\u6570\n@since 3.0.8", "input": "Java", "output": "public long betweenMonth(boolean isReset) {\r\n\t\tfinal Calendar beginCal = DateUtil.calendar(begin);\r\n\t\tfinal Calendar endCal = DateUtil.calendar(end);\r\n\r\n\t\tfinal int betweenYear = endCal.get(Calendar.YEAR) - beginCal.get(Calendar.YEAR);\r\n\t\tfinal int betweenMonthOfYear = endCal.get(Calendar.MONTH) - beginCal.get(Calendar.MONTH);\r\n\r\n\t\tint result = betweenYear * 12 + betweenMonthOfYear;\r\n\t\tif (false == isReset) {\r\n\t\t\tendCal.set(Calendar.YEAR, beginCal.get(Calendar.YEAR));\r\n\t\t\tendCal.set(Calendar.MONTH, beginCal.get(Calendar.MONTH));\r\n\t\t\tlong between = endCal.getTimeInMillis() - beginCal.getTimeInMillis();\r\n\t\t\tif (between < 0) {\r\n\t\t\t\treturn result - 1;\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn result;\r\n\t}"}, {"instruction": "true if {@link AbstractBuild#hasParticipant} or {@link hudson.model.Cause.UserIdCause}", "input": "Java", "output": "private boolean relatedTo(@Nonnull AbstractBuild<?, ?> b) {\n        if (b.hasParticipant(this)) {\n            return true;\n        }\n        for (Cause cause : b.getCauses()) {\n            if (cause instanceof Cause.UserIdCause) {\n                String userId = ((Cause.UserIdCause) cause).getUserId();\n                if (userId != null && idStrategy().equals(userId, getId())) {\n                    return true;\n                }\n            }\n        }\n        return false;\n    }"}, {"instruction": "Checks whether this dewey number is compatible to the other dewey number.\n\n<p>True iff this contains other as a prefix or iff they differ only in the last digit whereas\nthe last digit of this is greater than the last digit of other.\n\n@param other The other dewey number to check compatibility against\n@return Whether this dewey number is compatible to the other dewey number", "input": "Java", "output": "public boolean isCompatibleWith(DeweyNumber other) {\n\t\tif (length() > other.length()) {\n\t\t\t// prefix case\n\t\t\tfor (int i = 0; i < other.length(); i++) {\n\t\t\t\tif (other.deweyNumber[i] != deweyNumber[i]) {\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\treturn true;\n\t\t} else if (length() == other.length()) {\n\t\t\t// check init digits for equality\n\t\t\tint lastIndex = length() - 1;\n\t\t\tfor (int i = 0; i < lastIndex; i++) {\n\t\t\t\tif (other.deweyNumber[i] != deweyNumber[i]) {\n\t\t\t\t\treturn false;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// check that the last digit is greater or equal\n\t\t\treturn deweyNumber[lastIndex] >= other.deweyNumber[lastIndex];\n\t\t} else {\n\t\t\treturn false;\n\t\t}\n\t}"}, {"instruction": "This function checks the query for dimensions which can be optimized by applying the dimension extraction\nas the final step of the query instead of on every event.\n\n@param query The query to check for optimizations\n\n@return A collection of DimensionsSpec which can be extracted at the last second upon query completion.", "input": "Java", "output": "public static Collection<DimensionSpec> extractionsToRewrite(GroupByQuery query)\n  {\n    return Collections2.filter(\n        query.getDimensions(), new Predicate<DimensionSpec>()\n        {\n          @Override\n          public boolean apply(DimensionSpec input)\n          {\n            return input.getExtractionFn() != null\n                   && ExtractionFn.ExtractionType.ONE_TO_ONE.equals(\n                input.getExtractionFn().getExtractionType()\n            );\n          }\n        }\n    );\n  }"}, {"instruction": "Creates a new data stream that contains the given elements. The elements must all be of the\nsame type, for example, all of the {@link String} or {@link Integer}.\n\n<p>The framework will try and determine the exact type from the elements. In case of generic\nelements, it may be necessary to manually supply the type information via\n{@link #fromCollection(java.util.Collection, org.apache.flink.api.common.typeinfo.TypeInformation)}.\n\n<p>Note that this operation will result in a non-parallel data stream source, i.e. a data\nstream source with a degree of parallelism one.\n\n@param data\nThe array of elements to create the data stream from.\n@param <OUT>\nThe type of the returned data stream\n@return The data stream representing the given array of elements", "input": "Java", "output": "@SafeVarargs\n\tpublic final <OUT> DataStreamSource<OUT> fromElements(OUT... data) {\n\t\tif (data.length == 0) {\n\t\t\tthrow new IllegalArgumentException(\"fromElements needs at least one element as argument\");\n\t\t}\n\n\t\tTypeInformation<OUT> typeInfo;\n\t\ttry {\n\t\t\ttypeInfo = TypeExtractor.getForObject(data[0]);\n\t\t}\n\t\tcatch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Could not create TypeInformation for type \" + data[0].getClass().getName()\n\t\t\t\t\t+ \"; please specify the TypeInformation manually via \"\n\t\t\t\t\t+ \"StreamExecutionEnvironment#fromElements(Collection, TypeInformation)\", e);\n\t\t}\n\t\treturn fromCollection(Arrays.asList(data), typeInfo);\n\t}"}, {"instruction": "Encode a Uri Chunk, ensuring that all reserved characters are also encoded.\n\n@param value to encode.\n@param type identifying which uri fragment rules to apply.\n@param charset to use.\n@return an encoded uri chunk.", "input": "Java", "output": "private static String encodeChunk(String value, FragmentType type, Charset charset) {\n    byte[] data = value.getBytes(charset);\n    ByteArrayOutputStream encoded = new ByteArrayOutputStream();\n\n    for (byte b : data) {\n      if (type.isAllowed(b)) {\n        encoded.write(b);\n      } else {\n        /* percent encode the byte */\n        pctEncode(b, encoded);\n      }\n    }\n    return new String(encoded.toByteArray());\n  }"}, {"instruction": "TODO duplicated elsewhere", "input": "Java", "output": "private static void assertNoIncompatibleAnnotations(Class<? extends Annotation> annotation,\n                                                        Field field,\n                                                        Class<? extends Annotation>... undesiredAnnotations) {\n        for (Class<? extends Annotation> u : undesiredAnnotations) {\n            if (field.isAnnotationPresent(u)) {\n                throw unsupportedCombinationOfAnnotations(annotation.getSimpleName(),\n                                                          u.getSimpleName());\n            }\n        }\n    }"}, {"instruction": "Deletes this recorder, then go back to the parent.", "input": "Java", "output": "@RequirePOST\n    public synchronized void doDoDelete(StaplerResponse rsp) throws IOException, ServletException {\n        getConfigFile().delete();\n        getParent().logRecorders.remove(name);\n        // Disable logging for all our targets,\n        // then reenable all other loggers in case any also log the same targets\n        for (Target t : targets)\n            t.disable();\n        for (LogRecorder log : getParent().logRecorders.values())\n            for (Target t : log.targets)\n                t.enable();\n        rsp.sendRedirect2(\"..\");\n    }"}, {"instruction": "Helper method which sets up an iteration with the given vertex value.\n\n@param iteration", "input": "Java", "output": "private void setUpIteration(DeltaIteration<?, ?> iteration) {\n\n\t\t// set up the iteration operator\n\t\tif (this.configuration != null) {\n\n\t\t\titeration.name(this.configuration.getName(\"Vertex-centric iteration (\" + computeFunction + \")\"));\n\t\t\titeration.parallelism(this.configuration.getParallelism());\n\t\t\titeration.setSolutionSetUnManaged(this.configuration.isSolutionSetUnmanagedMemory());\n\n\t\t\t// register all aggregators\n\t\t\tfor (Map.Entry<String, Aggregator<?>> entry : this.configuration.getAggregators().entrySet()) {\n\t\t\t\titeration.registerAggregator(entry.getKey(), entry.getValue());\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t// no configuration provided; set default name\n\t\t\titeration.name(\"Vertex-centric iteration (\" + computeFunction + \")\");\n\t\t}\n\t}"}, {"instruction": "Print the job properties except property key contains \"pass\" and \"word\".", "input": "Java", "output": "@VisibleForTesting\n  Map<String, String> printableJobProperties(Props jobProps) {\n    Predicate<String> keyPredicate = new Predicate<String>() {\n\n      @Override\n      public boolean apply(String key) {\n        if (StringUtils.isEmpty(key)) {\n          return true;\n        }\n        key = key.toLowerCase();\n        return !(key.contains(\"pass\") && key.contains(\"word\"));\n      }\n\n    };\n    return Maps.filterKeys(jobProps.getFlattened(), keyPredicate);\n  }"}, {"instruction": "Convert a string time series to\nthe proper writable set based on the schema.\nNote that this does not use arrow.\nThis just uses normal writable objects.\n\n@param stringInput the string input\n@param schema the schema to use\n@return the converted records", "input": "Java", "output": "public static List<List<String>> convertWritableInputToString(List<List<Writable>> stringInput,Schema schema) {\n        List<List<String>> ret = new ArrayList<>();\n        List<List<String>> timeStepAdd = new ArrayList<>();\n        for(int j = 0; j < stringInput.size(); j++) {\n            List<Writable> record = stringInput.get(j);\n            List<String> recordAdd = new ArrayList<>();\n            for(int k = 0; k < record.size(); k++) {\n                recordAdd.add(record.get(k).toString());\n            }\n\n            timeStepAdd.add(recordAdd);\n        }\n\n\n        return ret;\n    }"}, {"instruction": "Removes the mapping from the cache without store-by-value copying nor waiting for synchronous\nlisteners to complete.\n\n@param key key whose mapping is to be removed from the cache\n@return the old value", "input": "Java", "output": "private V removeNoCopyOrAwait(K key) {\n    @SuppressWarnings(\"unchecked\")\n    V[] removed = (V[]) new Object[1];\n    cache.asMap().computeIfPresent(key, (k, expirable) -> {\n      if (!expirable.isEternal() && expirable.hasExpired(currentTimeMillis())) {\n        dispatcher.publishExpired(this, key, expirable.get());\n        statistics.recordEvictions(1L);\n        return null;\n      }\n\n      dispatcher.publishRemoved(this, key, expirable.get());\n      removed[0] = expirable.get();\n      return null;\n    });\n    return removed[0];\n  }"}, {"instruction": "occur while saving.", "input": "Java", "output": "@GuardedBy(\"tasks\")\n  private void saveRunningTasks()\n  {\n    final File restoreFile = getRestoreFile();\n    final List<String> theTasks = new ArrayList<>();\n    for (ForkingTaskRunnerWorkItem forkingTaskRunnerWorkItem : tasks.values()) {\n      theTasks.add(forkingTaskRunnerWorkItem.getTaskId());\n    }\n\n    try {\n      Files.createParentDirs(restoreFile);\n      jsonMapper.writeValue(restoreFile, new TaskRestoreInfo(theTasks));\n    }\n    catch (Exception e) {\n      log.warn(e, \"Failed to save tasks to restore file[%s]. Skipping this save.\", restoreFile);\n    }\n  }"}, {"instruction": "Returns alternative names (non-deprecated keys or previously-set deprecated keys)\nfor a given non-deprecated key.\nIf the given key is deprecated, return null.\n\n@param name property name.\n@return alternative names.", "input": "Java", "output": "private String[] getAlternativeNames(String name) {\n\t\tString altNames[] = null;\n\t\tDeprecatedKeyInfo keyInfo = null;\n\t\tDeprecationContext cur = deprecationContext.get();\n\t\tString depKey = cur.getReverseDeprecatedKeyMap().get(name);\n\t\tif(depKey != null) {\n\t\t\tkeyInfo = cur.getDeprecatedKeyMap().get(depKey);\n\t\t\tif(keyInfo.newKeys.length > 0) {\n\t\t\t\tif(getProps().containsKey(depKey)) {\n\t\t\t\t\t//if deprecated key is previously set explicitly\n\t\t\t\t\tList<String> list = new ArrayList<String>();\n\t\t\t\t\tlist.addAll(Arrays.asList(keyInfo.newKeys));\n\t\t\t\t\tlist.add(depKey);\n\t\t\t\t\taltNames = list.toArray(new String[list.size()]);\n\t\t\t\t}\n\t\t\t\telse {\n\t\t\t\t\taltNames = keyInfo.newKeys;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\treturn altNames;\n\t}"}, {"instruction": "\u6253\u5f00SSH\u4f1a\u8bdd\uff0c\u5e76\u7ed1\u5b9a\u8fdc\u7a0b\u7aef\u53e3\u5230\u672c\u5730\u7684\u4e00\u4e2a\u968f\u673a\u7aef\u53e3\n\n@param sshConn SSH\u8fde\u63a5\u4fe1\u606f\u5bf9\u8c61\n@param remoteHost \u8fdc\u7a0b\u4e3b\u673a\n@param remotePort \u8fdc\u7a0b\u7aef\u53e3\n@return \u6620\u5c04\u540e\u7684\u672c\u5730\u7aef\u53e3\n@throws JschRuntimeException \u8fde\u63a5\u5f02\u5e38", "input": "Java", "output": "public static int openAndBindPortToLocal(Connector sshConn, String remoteHost, int remotePort) throws JschRuntimeException {\r\n\t\tfinal Session session = openSession(sshConn.getHost(), sshConn.getPort(), sshConn.getUser(), sshConn.getPassword());\r\n\t\tif (session == null) {\r\n\t\t\tthrow new JschRuntimeException(\"Error to create SSH Session\uff01\");\r\n\t\t}\r\n\t\tfinal int localPort = generateLocalPort();\r\n\t\tbindPort(session, remoteHost, remotePort, localPort);\r\n\t\treturn localPort;\r\n\t}"}, {"instruction": "check uniqueness of dependency type and params", "input": "Java", "output": "private void validateDepDefinitionUniqueness(final List<FlowTriggerDependency> dependencies) {\n    final Set<String> seen = new HashSet<>();\n    for (final FlowTriggerDependency dep : dependencies) {\n      final Map<String, String> props = dep.getProps();\n      // set.add() returns false when there exists duplicate\n      Preconditions.checkArgument(seen.add(dep.getType() + \":\" + props.toString()), String.format\n          (\"duplicate dependency config %s found, dependency config should be unique\",\n              dep.getName()));\n    }\n  }"}, {"instruction": "Increases the current backoff and returns whether the operation was successful.\n\n@return <code>true</code>, iff the operation was successful. Otherwise, <code>false</code>.", "input": "Java", "output": "protected boolean increaseBackoff() {\n\t\t// Backoff is disabled\n\t\tif (currentBackoff < 0) {\n\t\t\treturn false;\n\t\t}\n\n\t\t// This is the first time backing off\n\t\tif (currentBackoff == 0) {\n\t\t\tcurrentBackoff = initialBackoff;\n\n\t\t\treturn true;\n\t\t}\n\n\t\t// Continue backing off\n\t\telse if (currentBackoff < maxBackoff) {\n\t\t\tcurrentBackoff = Math.min(currentBackoff * 2, maxBackoff);\n\n\t\t\treturn true;\n\t\t}\n\n\t\t// Reached maximum backoff\n\t\treturn false;\n\t}"}, {"instruction": "Update the given picker to the helper if it's different from the current one.", "input": "Java", "output": "private void maybeUpdatePicker(ConnectivityState state, RoundRobinPicker picker) {\n    // Discard the new picker if we are sure it won't make any difference, in order to save\n    // re-processing pending streams, and avoid unnecessary resetting of the pointer in\n    // RoundRobinPicker.\n    if (picker.dropList.equals(currentPicker.dropList)\n        && picker.pickList.equals(currentPicker.pickList)) {\n      return;\n    }\n    currentPicker = picker;\n    logger.log(\n        ChannelLogLevel.INFO, \"{0}: picks={1}, drops={2}\", state, picker.pickList, picker.dropList);\n    helper.updateBalancingState(state, picker);\n  }"}, {"instruction": "return the privateIp address of the given InstanceInfo record. The record could be for the local server\nor a remote server.\n\n@param instanceInfo\n@return the private Ip (also known as localIpv4 in ec2)", "input": "Java", "output": "public static String getPrivateIp(InstanceInfo instanceInfo) {\n        String defaultPrivateIp = null;\n        if (instanceInfo.getDataCenterInfo() instanceof AmazonInfo) {\n            defaultPrivateIp = ((AmazonInfo) instanceInfo.getDataCenterInfo()).get(AmazonInfo.MetaDataKey.localIpv4);\n        }\n\n        if (isNullOrEmpty(defaultPrivateIp)) {\n            // no other information, best effort\n            defaultPrivateIp = instanceInfo.getIPAddr();\n        }\n\n        return defaultPrivateIp;\n    }"}, {"instruction": "\u9a8c\u8bc110\u4f4d\u8eab\u4efd\u7f16\u7801\u662f\u5426\u5408\u6cd5\n\n@param idCard \u8eab\u4efd\u7f16\u7801\n@return \u8eab\u4efd\u8bc1\u4fe1\u606f\u6570\u7ec4\n<p>\n[0] - \u53f0\u6e7e\u3001\u6fb3\u95e8\u3001\u9999\u6e2f [1] - \u6027\u522b(\u7537M,\u5973F,\u672a\u77e5N) [2] - \u662f\u5426\u5408\u6cd5(\u5408\u6cd5true,\u4e0d\u5408\u6cd5false) \u82e5\u4e0d\u662f\u8eab\u4efd\u8bc1\u4ef6\u53f7\u7801\u5219\u8fd4\u56denull\n</p>", "input": "Java", "output": "public static String[] isValidCard10(String idCard) {\r\n\t\tif(StrUtil.isBlank(idCard)) {\r\n\t\t\treturn null;\r\n\t\t}\r\n\t\tString[] info = new String[3];\r\n\t\tString card = idCard.replaceAll(\"[\\\\(|\\\\)]\", \"\");\r\n\t\tif (card.length() != 8 && card.length() != 9 && idCard.length() != 10) {\r\n\t\t\treturn null;\r\n\t\t}\r\n\t\tif (idCard.matches(\"^[a-zA-Z][0-9]{9}$\")) { // \u53f0\u6e7e\r\n\t\t\tinfo[0] = \"\u53f0\u6e7e\";\r\n\t\t\tString char2 = idCard.substring(1, 2);\r\n\t\t\tif (char2.equals(\"1\")) {\r\n\t\t\t\tinfo[1] = \"M\";\r\n\t\t\t} else if (char2.equals(\"2\")) {\r\n\t\t\t\tinfo[1] = \"F\";\r\n\t\t\t} else {\r\n\t\t\t\tinfo[1] = \"N\";\r\n\t\t\t\tinfo[2] = \"false\";\r\n\t\t\t\treturn info;\r\n\t\t\t}\r\n\t\t\tinfo[2] = isValidTWCard(idCard) ? \"true\" : \"false\";\r\n\t\t} else if (idCard.matches(\"^[1|5|7][0-9]{6}\\\\(?[0-9A-Z]\\\\)?$\")) { // \u6fb3\u95e8\r\n\t\t\tinfo[0] = \"\u6fb3\u95e8\";\r\n\t\t\tinfo[1] = \"N\";\r\n\t\t} else if (idCard.matches(\"^[A-Z]{1,2}[0-9]{6}\\\\(?[0-9A]\\\\)?$\")) { // \u9999\u6e2f\r\n\t\t\tinfo[0] = \"\u9999\u6e2f\";\r\n\t\t\tinfo[1] = \"N\";\r\n\t\t\tinfo[2] = isValidHKCard(idCard) ? \"true\" : \"false\";\r\n\t\t} else {\r\n\t\t\treturn null;\r\n\t\t}\r\n\t\treturn info;\r\n\t}"}, {"instruction": "Searches defined database where the URL prefix matches one of the prefixes defined in a {@link JdbcDatabase}.\nThe prefix is determined by:\n<p>\njdbc:<prefix>:...\n\n@param jdbcUrl The connection URL\n@return An optional {@link JdbcDatabase}", "input": "Java", "output": "@SuppressWarnings(\"MagicNumber\")\n    public static Optional<JdbcDatabase> findDatabase(String jdbcUrl) {\n        if (StringUtils.isNotEmpty(jdbcUrl)) {\n            if (!jdbcUrl.startsWith(\"jdbc\")) {\n                throw new IllegalArgumentException(\"JDBC URLs must start with 'jdbc'\");\n            }\n            String partialUrl = jdbcUrl.substring(5);\n            String prefix = partialUrl.substring(0, partialUrl.indexOf(':')).toLowerCase();\n\n            return databases.stream().filter(db -> db.containsPrefix(prefix)).findFirst();\n        }\n        return Optional.empty();\n    }"}, {"instruction": "ZapTocMerger#mergeNodes(TreeNode, TreeNode) instead of UniteAppendMerge#mergeNodes(TreeNode, TreeNode).", "input": "Java", "output": "@Override\n    public TreeNode processMerge(TreeNode node) {\n\n        DefaultMutableTreeNode masterNode = (DefaultMutableTreeNode) node;\n\n        // if master and slave are the same object return the\n        // masterNode\n        if (masterNode.equals(slaveTopNode)) {\n            return masterNode;\n        }\n\n        // If there are not children in slaveTopNode return the\n        // masterNode\n        if (slaveTopNode.getChildCount() == 0) {\n            return masterNode;\n        }\n\n        mergeNodes(masterNode, slaveTopNode);\n        return masterNode;\n    }"}, {"instruction": "Once-per-node shared init", "input": "Java", "output": "@Override public void setupLocal( ) {\n    // Init all the internal tree fields after shipping over the wire\n    _tree.init_tree();\n    // Allocate local shared memory histograms\n    for( int l=_leaf; l<_tree._len; l++ ) {\n      DTree.UndecidedNode udn = _tree.undecided(l);\n      DHistogram hs[] = _hcs[l-_leaf];\n      int sCols[] = udn._scoreCols;\n      if( sCols != null ) { // Sub-selecting just some columns?\n        for( int col : sCols ) // For tracked cols\n          hs[col].init();\n      } else {                 // Else all columns\n        for( int j=0; j<_ncols; j++) // For all columns\n          if( hs[j] != null )        // Tracking this column?\n            hs[j].init();\n      }\n    }\n  }"}, {"instruction": "Encodes URI string.\n\nThis is a two mapping, one from original characters to octets, and\nsubsequently a second from octets to URI characters:\n<p><blockquote><pre>\noriginal character sequence-&gt;octet sequence-&gt;URI character sequence\n</pre></blockquote><p>\n\nAn escaped octet is encoded as a character triplet, consisting of the\npercent character \"%\" followed by the two hexadecimal digits\nrepresenting the octet code. For example, \"%20\" is the escaped\nencoding for the US-ASCII space character.\n<p>\nConversion from the local filesystem character set to UTF-8 will\nnormally involve a two step process. First convert the local character\nset to the UCS; then convert the UCS to UTF-8.\nThe first step in the process can be performed by maintaining a mapping\ntable that includes the local character set code and the corresponding\nUCS code.\nThe next step is to convert the UCS character code to the UTF-8 encoding.\n<p>\nMapping between vendor codepages can be done in a very similar manner\nas described above.\n<p>\nThe only time escape encodings can allowedly be made is when a URI is\nbeing created from its component parts.  The escape and validate methods\nare internally performed within this method.\n\n@param original the original character sequence\n@param allowed those characters that are allowed within a component\n@param charset the protocol charset\n@return URI character sequence\n@throws URIException null component or unsupported character encoding", "input": "Java", "output": "protected static char[] encode(String original, BitSet allowed,\n            String charset) throws URIException {\n        if (original == null) {\n            throw new IllegalArgumentException(\"Original string may not be null\");\n        }\n        if (allowed == null) {\n            throw new IllegalArgumentException(\"Allowed bitset may not be null\");\n        }\n        byte[] rawdata = URLCodec.encodeUrl(allowed, EncodingUtil.getBytes(original, charset));\n        return EncodingUtil.getAsciiString(rawdata).toCharArray();\n    }"}, {"instruction": "Indicates progress by number.\n\n@param number The number", "input": "Java", "output": "@Override\n    public void indicateProgress(int number) {\n        verifySystemOut();\n        progressIndicatorActive = true;\n        String currMsg = lastMessage;\n        try {\n            if (isAnsiEnabled()) {\n                updateStatus(currMsg + ' ' + number);\n            } else {\n                out.print(\"..\");\n                out.print(number);\n            }\n        } finally {\n            lastMessage = currMsg;\n        }\n    }"}, {"instruction": "Returns value of network address cache ttl property if not Android environment. For android,\nDnsNameResolver does not cache the dns lookup result.", "input": "Java", "output": "private static long getNetworkAddressCacheTtlNanos(boolean isAndroid) {\n    if (isAndroid) {\n      // on Android, ignore dns cache.\n      return 0;\n    }\n\n    String cacheTtlPropertyValue = System.getProperty(NETWORKADDRESS_CACHE_TTL_PROPERTY);\n    long cacheTtl = DEFAULT_NETWORK_CACHE_TTL_SECONDS;\n    if (cacheTtlPropertyValue != null) {\n      try {\n        cacheTtl = Long.parseLong(cacheTtlPropertyValue);\n      } catch (NumberFormatException e) {\n        logger.log(\n            Level.WARNING,\n            \"Property({0}) valid is not valid number format({1}), fall back to default({2})\",\n            new Object[] {NETWORKADDRESS_CACHE_TTL_PROPERTY, cacheTtlPropertyValue, cacheTtl});\n      }\n    }\n    return cacheTtl > 0 ? TimeUnit.SECONDS.toNanos(cacheTtl) : cacheTtl;\n  }"}, {"instruction": "Gets a list of unprotected root actions.\nThese URL prefixes should be exempted from access control checks by container-managed security.\nIdeally would be synchronized with {@link #getTarget}.\n@return a list of {@linkplain Action#getUrlName URL names}\n@since 1.495", "input": "Java", "output": "public Collection<String> getUnprotectedRootActions() {\n        Set<String> names = new TreeSet<>();\n        names.add(\"jnlpJars\"); // TODO cleaner to refactor doJnlpJars into a URA (see also JENKINS-44100)\n        // TODO consider caching (expiring cache when actions changes)\n        for (Action a : getActions()) {\n            if (a instanceof UnprotectedRootAction) {\n                String url = a.getUrlName();\n                if (url == null) continue;\n                names.add(url);\n            }\n        }\n        return names;\n    }"}, {"instruction": "Build the ticket properties.\n\n@param webContext the web context\n@return the ticket properties", "input": "Java", "output": "protected Map<String, Serializable> buildTicketProperties(final J2EContext webContext) {\n        val properties = new HashMap<String, Serializable>();\n\n        val themeParamName = casProperties.getTheme().getParamName();\n        val localParamName = casProperties.getLocale().getParamName();\n\n        properties.put(themeParamName, StringUtils.defaultString(webContext.getRequestParameter(themeParamName)));\n        properties.put(localParamName, StringUtils.defaultString(webContext.getRequestParameter(localParamName)));\n        properties.put(CasProtocolConstants.PARAMETER_METHOD,\n            StringUtils.defaultString(webContext.getRequestParameter(CasProtocolConstants.PARAMETER_METHOD)));\n\n        return properties;\n    }"}, {"instruction": "WAP\u652f\u4ed8\n\n@param response\n{HttpServletResponse}\n@param model\n{AlipayTradeWapPayModel}\n@param returnUrl\n\u5f02\u6b65\u901a\u77e5URL\n@param notifyUrl\n\u540c\u6b65\u901a\u77e5URL\n@return {String}\n@throws {AlipayApiException}\n@throws {IOException}", "input": "Java", "output": "public static String wapPayStr(HttpServletResponse response, AlipayTradeWapPayModel model, String returnUrl,\n\t\t\tString notifyUrl) throws AlipayApiException, IOException {\n\t\tAlipayTradeWapPayRequest alipayRequest = new AlipayTradeWapPayRequest();// \u521b\u5efaAPI\u5bf9\u5e94\u7684request\n\t\talipayRequest.setReturnUrl(returnUrl);\n\t\talipayRequest.setNotifyUrl(notifyUrl);// \u5728\u516c\u5171\u53c2\u6570\u4e2d\u8bbe\u7f6e\u56de\u8df3\u548c\u901a\u77e5\u5730\u5740\n\t\talipayRequest.setBizModel(model);// \u586b\u5145\u4e1a\u52a1\u53c2\u6570\n\t\treturn AliPayApiConfigKit.getAliPayApiConfig().getAlipayClient().pageExecute(alipayRequest).getBody(); // \u8c03\u7528SDK\u751f\u6210\u8868\u5355\n\t}"}, {"instruction": "Evaluation for multiple-output networks.<br>\nSee {@link #evaluate(MultiDataSetIterator, Map, Map)}", "input": "Java", "output": "public void evaluate(DataSetIterator iterator, Map<String,IEvaluation> variableEvals){\n        Map<String,Integer> map = new HashMap<>();\n        Map<String,List<IEvaluation>> variableEvalsList = new HashMap<>();\n        for(String s : variableEvals.keySet()){\n            map.put(s, 0);  //Only 1 possible output here with DataSetIterator\n            variableEvalsList.put(s, Collections.singletonList(variableEvals.get(s)));\n        }\n        evaluate(new MultiDataSetIteratorAdapter(iterator), variableEvalsList, map);\n    }"}, {"instruction": "\u5220\u9664\u64cd\u4f5c\n\n@param config\n@param dml", "input": "Java", "output": "private void delete(BatchExecutor batchExecutor, MappingConfig config, SingleDml dml) throws SQLException {\n        Map<String, Object> data = dml.getData();\n        if (data == null || data.isEmpty()) {\n            return;\n        }\n\n        DbMapping dbMapping = config.getDbMapping();\n\n        Map<String, Integer> ctype = getTargetColumnType(batchExecutor.getConn(), config);\n\n        StringBuilder sql = new StringBuilder();\n        sql.append(\"DELETE FROM \").append(SyncUtil.getDbTableName(dbMapping)).append(\" WHERE \");\n\n        List<Map<String, ?>> values = new ArrayList<>();\n        // \u62fc\u63a5\u4e3b\u952e\n        appendCondition(dbMapping, sql, ctype, values, data);\n        batchExecutor.execute(sql.toString(), values);\n        if (logger.isTraceEnabled()) {\n            logger.trace(\"Delete from target table, sql: {}\", sql);\n        }\n    }"}, {"instruction": "ZAP: Added to take into account the package name", "input": "Java", "output": "private List<ClassNameWrapper> getJarClassNames(ClassLoader cl, File file, String packageName) {\r\n    \tList<ClassNameWrapper> classNames = new ArrayList<> ();\r\n        ZipEntry entry = null;\r\n        String className = \"\";\r\n        try (JarFile jarFile = new JarFile(file)) {\r\n            Enumeration<JarEntry> entries = jarFile.entries();\r\n            while (entries.hasMoreElements()) {\r\n                entry = entries.nextElement();\r\n                if (entry.isDirectory() || !entry.getName().endsWith(\".class\")) {\r\n                    continue;\r\n                }\r\n                className = entry.toString().replaceAll(\"\\\\.class$\",\"\").replaceAll(\"/\",\".\");\r\n                if (className.indexOf(packageName) >= 0) {\r\n                    classNames.add(new ClassNameWrapper(cl, className));\r\n                }\r\n            }\r\n        } catch (Exception e) {\r\n        \tlogger.error(\"Failed to open file: \" + file.getAbsolutePath(), e);\r\n        }\r\n        return classNames;\r\n    }"}, {"instruction": "This method tracks op calls\n\n@param op", "input": "Java", "output": "public void processOpCall(CustomOp op) {\n        // total number of invocations\n        invocationsCount.incrementAndGet();\n\n        // number of invocations for this specific op\n        opCounter.incrementCount(op.opName());\n\n        // number of invocations for specific class\n        String opClass = getOpClass(op);\n        classCounter.incrementCount(opClass);\n\n\n        lastZ = 0;\n        prevOpMatching = opClass;\n        prevOpMatchingDetailed = opClass + \" \" + op.opName();\n        prevOpMatchingInverted = opClass + \" \" + op.opName();\n\n        updatePairs(op.opName(), opClass);\n\n        // TODO: to be implemented\n        //for (OpProfilerListener listener : listeners) {\n        //  listener.invoke(op);\n        //}\n    }"}, {"instruction": "Converts a parameters to type arguments.\n@param parameters The parameters\n@return The type arguments", "input": "Java", "output": "@NotNull\n    protected Map<String, Object> toParameterTypes(ParameterElement... parameters) {\n        final LinkedHashMap<String, Object> map = new LinkedHashMap<>(parameters.length);\n        for (ParameterElement ce : parameters) {\n            final ClassElement type = ce.getType();\n            if (type == null) {\n                continue;\n            }\n            final Type typeReference = getTypeForElement(type);\n            map.put(ce.getName(), typeReference);\n        }\n\n        return map;\n    }"}, {"instruction": "/* (non-Javadoc)\n@see org.apache.hive.service.cli.operation.Operation#getNextRowSet(org.apache.hive.service.cli.FetchOrientation, long)", "input": "Java", "output": "@Override\n  public RowSet getNextRowSet(FetchOrientation orientation, long maxRows) throws HiveSQLException {\n    validateDefaultFetchOrientation(orientation);\n    if (orientation.equals(FetchOrientation.FETCH_FIRST)) {\n      resetResultReader();\n    }\n    List<String> rows = readResults((int) maxRows);\n    RowSet rowSet = RowSetFactory.create(resultSchema, getProtocolVersion());\n\n    for (String row : rows) {\n      rowSet.addRow(new String[] {row});\n    }\n    return rowSet;\n  }"}, {"instruction": "Writes the JSON for {@code jsonElement} to {@code writer}.\n@throws JsonIOException if there was a problem writing to the writer", "input": "Java", "output": "public void toJson(JsonElement jsonElement, JsonWriter writer) throws JsonIOException {\n    boolean oldLenient = writer.isLenient();\n    writer.setLenient(true);\n    boolean oldHtmlSafe = writer.isHtmlSafe();\n    writer.setHtmlSafe(htmlSafe);\n    boolean oldSerializeNulls = writer.getSerializeNulls();\n    writer.setSerializeNulls(serializeNulls);\n    try {\n      Streams.write(jsonElement, writer);\n    } catch (IOException e) {\n      throw new JsonIOException(e);\n    } catch (AssertionError e) {\n      AssertionError error = new AssertionError(\"AssertionError (GSON \" + GsonBuildConfig.VERSION + \"): \" + e.getMessage());\n      error.initCause(e);\n      throw error;\n    } finally {\n      writer.setLenient(oldLenient);\n      writer.setHtmlSafe(oldHtmlSafe);\n      writer.setSerializeNulls(oldSerializeNulls);\n    }\n  }"}, {"instruction": "Remove all values for the given key without returning them. This is a minor performance\noptimization if you do not need the previous values.", "input": "Java", "output": "@ExperimentalApi(\"https://github.com/grpc/grpc-java/issues/4691\")\n  public <T> void discardAll(Key<T> key) {\n    if (isEmpty()) {\n      return;\n    }\n    int writeIdx = 0;\n    int readIdx = 0;\n    for (; readIdx < size; readIdx++) {\n      if (bytesEqual(key.asciiName(), name(readIdx))) {\n        continue;\n      }\n      name(writeIdx, name(readIdx));\n      value(writeIdx, value(readIdx));\n      writeIdx++;\n    }\n    int newSize = writeIdx;\n    // Multiply by two since namesAndValues is interleaved.\n    Arrays.fill(namesAndValues, writeIdx * 2, len(), null);\n    size = newSize;\n  }"}, {"instruction": "Returns whether the supplied cache loader has bulk load functionality.", "input": "Java", "output": "private static boolean canBulkLoad(AsyncCacheLoader<?, ?> loader) {\n    try {\n      Class<?> defaultLoaderClass = AsyncCacheLoader.class;\n      if (loader instanceof CacheLoader<?, ?>) {\n        defaultLoaderClass = CacheLoader.class;\n\n        Method classLoadAll = loader.getClass().getMethod(\"loadAll\", Iterable.class);\n        Method defaultLoadAll = CacheLoader.class.getMethod(\"loadAll\", Iterable.class);\n        if (!classLoadAll.equals(defaultLoadAll)) {\n          return true;\n        }\n      }\n\n      Method classAsyncLoadAll = loader.getClass().getMethod(\n          \"asyncLoadAll\", Iterable.class, Executor.class);\n      Method defaultAsyncLoadAll = defaultLoaderClass.getMethod(\n          \"asyncLoadAll\", Iterable.class, Executor.class);\n      return !classAsyncLoadAll.equals(defaultAsyncLoadAll);\n    } catch (NoSuchMethodException | SecurityException e) {\n      logger.log(Level.WARNING, \"Cannot determine if CacheLoader can bulk load\", e);\n      return false;\n    }\n  }"}, {"instruction": "\u751f\u6210\u4e0d\u91cd\u590d\u968f\u673a\u6570 \u6839\u636e\u7ed9\u5b9a\u7684\u6700\u5c0f\u6570\u5b57\u548c\u6700\u5927\u6570\u5b57\uff0c\u4ee5\u53ca\u968f\u673a\u6570\u7684\u4e2a\u6570\uff0c\u4ea7\u751f\u6307\u5b9a\u7684\u4e0d\u91cd\u590d\u7684\u6570\u7ec4\n\n@param begin \u6700\u5c0f\u6570\u5b57\uff08\u5305\u542b\u8be5\u6570\uff09\n@param end \u6700\u5927\u6570\u5b57\uff08\u4e0d\u5305\u542b\u8be5\u6570\uff09\n@param size \u6307\u5b9a\u4ea7\u751f\u968f\u673a\u6570\u7684\u4e2a\u6570\n@return \u968f\u673aint\u6570\u7ec4", "input": "Java", "output": "public static int[] generateRandomNumber(int begin, int end, int size) {\r\n\t\tif (begin > end) {\r\n\t\t\tint temp = begin;\r\n\t\t\tbegin = end;\r\n\t\t\tend = temp;\r\n\t\t}\r\n\t\t// \u52a0\u5165\u903b\u8f91\u5224\u65ad\uff0c\u786e\u4fddbegin<end\u5e76\u4e14size\u4e0d\u80fd\u5927\u4e8e\u8be5\u8868\u793a\u8303\u56f4\r\n\t\tif ((end - begin) < size) {\r\n\t\t\tthrow new UtilException(\"Size is larger than range between begin and end!\");\r\n\t\t}\r\n\t\t// \u79cd\u5b50\u4f60\u53ef\u4ee5\u968f\u610f\u751f\u6210\uff0c\u4f46\u4e0d\u80fd\u91cd\u590d\r\n\t\tint[] seed = new int[end - begin];\r\n\r\n\t\tfor (int i = begin; i < end; i++) {\r\n\t\t\tseed[i - begin] = i;\r\n\t\t}\r\n\t\tint[] ranArr = new int[size];\r\n\t\tRandom ran = new Random();\r\n\t\t// \u6570\u91cf\u4f60\u53ef\u4ee5\u81ea\u5df1\u5b9a\u4e49\u3002\r\n\t\tfor (int i = 0; i < size; i++) {\r\n\t\t\t// \u5f97\u5230\u4e00\u4e2a\u4f4d\u7f6e\r\n\t\t\tint j = ran.nextInt(seed.length - i);\r\n\t\t\t// \u5f97\u5230\u90a3\u4e2a\u4f4d\u7f6e\u7684\u6570\u503c\r\n\t\t\tranArr[i] = seed[j];\r\n\t\t\t// \u5c06\u6700\u540e\u4e00\u4e2a\u672a\u7528\u7684\u6570\u5b57\u653e\u5230\u8fd9\u91cc\r\n\t\t\tseed[j] = seed[seed.length - 1 - i];\r\n\t\t}\r\n\t\treturn ranArr;\r\n\t}"}, {"instruction": "\u521b\u5efa\u7528\u4e8e\u67e5\u8be2\u8bed\u53e5\u7684{@link URLEncoder}<br>\n\u7f16\u7801\u5668\u9488\u5bf9URI\u8def\u5f84\u7f16\u7801\uff0c\u5b9a\u4e49\u5982\u4e0b\uff1a\n\n<pre>\n0x20 ' ' =\u300b '+'\n0x2A, 0x2D, 0x2E, 0x30 to 0x39, 0x41 to 0x5A, 0x5F, 0x61 to 0x7A as-is\n'*', '-', '.', '0' to '9', 'A' to 'Z', '_', 'a' to 'z' Also '=' and '&' \u4e0d\u7f16\u7801\n\u5176\u5b83\u7f16\u7801\u4e3a %nn \u5f62\u5f0f\n</pre>\n\n\u8be6\u7ec6\u89c1\uff1ahttps://www.w3.org/TR/html5/forms.html#application/x-www-form-urlencoded-encoding-algorithm\n\n@return {@link URLEncoder}", "input": "Java", "output": "public static URLEncoder createQuery() {\r\n\t\tfinal URLEncoder encoder = new URLEncoder();\r\n\t\t// Special encoding for space\r\n\t\tencoder.setEncodeSpaceAsPlus(true);\r\n\t\t// Alpha and digit are safe by default\r\n\t\t// Add the other permitted characters\r\n\t\tencoder.addSafeCharacter('*');\r\n\t\tencoder.addSafeCharacter('-');\r\n\t\tencoder.addSafeCharacter('.');\r\n\t\tencoder.addSafeCharacter('_');\r\n\t\tencoder.addSafeCharacter('=');\r\n\t\tencoder.addSafeCharacter('&');\r\n\r\n\t\treturn encoder;\r\n\t}"}, {"instruction": "Resumes ZooKeeper (if it had previously been suspended).\n\n@throws Exception If an exception got thrown.", "input": "Java", "output": "public void resumeZooKeeper() throws Exception {\n        val zk = new ZooKeeperServiceRunner(this.zkPort, this.secureZK, this.tLSKeyStore, this.tLSKeyStorePasswordPath, this.tlsTrustStore);\n        if (this.zkServer.compareAndSet(null, zk)) {\n            // Initialize ZK runner (since nobody else did it for us).\n            zk.initialize();\n            log.info(\"ZooKeeper initialized.\");\n        } else {\n            zk.close();\n        }\n\n        // Start or resume ZK.\n        this.zkServer.get().start();\n        log.info(\"ZooKeeper resumed.\");\n    }"}, {"instruction": "Used to reset a reset a reader group to a checkpoint. This should be removed in time.\n@deprecated Use {@link ReaderGroup#resetReaderGroup(ReaderGroupConfig)} to reset readers to a given Checkpoint.", "input": "Java", "output": "@Override\n    @Deprecated\n    public void resetReadersToCheckpoint(Checkpoint checkpoint) {\n        synchronizer.updateState((state, updates) -> {\n            ReaderGroupConfig config = state.getConfig();\n            Map<Segment, Long> positions = new HashMap<>();\n            for (StreamCut cut : checkpoint.asImpl().getPositions().values()) {\n                positions.putAll(cut.asImpl().getPositions());\n            }\n            updates.add(new ReaderGroupStateInit(config, positions, getEndSegmentsForStreams(config)));\n        });\n    }"}, {"instruction": "Returns the HBase identifiers of all registered column qualifiers for a specific column family.\n\n@param family The name of the column family for which the column qualifier identifiers are returned.\n@return The HBase identifiers of all registered column qualifiers for a specific column family.", "input": "Java", "output": "byte[][] getQualifierKeys(String family) {\n\t\tMap<String, TypeInformation<?>> qualifierMap = familyMap.get(family);\n\n\t\tif (qualifierMap == null) {\n\t\t\tthrow new IllegalArgumentException(\"Family \" + family + \" does not exist in schema.\");\n\t\t}\n\t\tCharset c = Charset.forName(charset);\n\n\t\tbyte[][] qualifierKeys = new byte[qualifierMap.size()][];\n\t\tint i = 0;\n\t\tfor (String name : qualifierMap.keySet()) {\n\t\t\tqualifierKeys[i++] = name.getBytes(c);\n\t\t}\n\t\treturn qualifierKeys;\n\t}"}, {"instruction": "Sets the access control policy on the specified resource. Replaces any existing policy.\n\n<p>Sample code:\n\n<pre><code>\ntry (InstanceTemplateClient instanceTemplateClient = InstanceTemplateClient.create()) {\nProjectGlobalInstanceTemplateResourceName resource = ProjectGlobalInstanceTemplateResourceName.of(\"[PROJECT]\", \"[RESOURCE]\");\nGlobalSetPolicyRequest globalSetPolicyRequestResource = GlobalSetPolicyRequest.newBuilder().build();\nPolicy response = instanceTemplateClient.setIamPolicyInstanceTemplate(resource, globalSetPolicyRequestResource);\n}\n</code></pre>\n\n@param resource Name or id of the resource for this request.\n@param globalSetPolicyRequestResource\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "@BetaApi\n  public final Policy setIamPolicyInstanceTemplate(\n      ProjectGlobalInstanceTemplateResourceName resource,\n      GlobalSetPolicyRequest globalSetPolicyRequestResource) {\n\n    SetIamPolicyInstanceTemplateHttpRequest request =\n        SetIamPolicyInstanceTemplateHttpRequest.newBuilder()\n            .setResource(resource == null ? null : resource.toString())\n            .setGlobalSetPolicyRequestResource(globalSetPolicyRequestResource)\n            .build();\n    return setIamPolicyInstanceTemplate(request);\n  }"}, {"instruction": "/* (non-Javadoc)\n@see org.parosproxy.paros.db.paros.TableContext#insert(int, int, java.lang.String)", "input": "Java", "output": "@Override\r\n\tpublic synchronized RecordContext insert(int contextId, int type, String url) throws DatabaseException {\r\n        try {\r\n\t\t\tpsInsert.setInt(1, contextId);\r\n\t\t\tpsInsert.setInt(2, type);\r\n\t\t\tpsInsert.setString(3, url);\r\n\t\t\tpsInsert.executeUpdate();\r\n\t\t\t\r\n\t\t\tlong id;\r\n\t\t\ttry (ResultSet rs = psGetIdLastInsert.executeQuery()) {\r\n\t\t\t\trs.next();\r\n\t\t\t\tid = rs.getLong(1);\r\n\t\t\t}\r\n\t\t\treturn read(id);\r\n\t\t} catch (SQLException e) {\r\n\t\t\tthrow new DatabaseException(e);\r\n\t\t}\r\n\t\t\r\n    }"}, {"instruction": "Create a metric registry configuration object from the given {@link Configuration}.\n\n@param configuration to generate the metric registry configuration from\n@return Metric registry configuration generated from the configuration", "input": "Java", "output": "public static MetricRegistryConfiguration fromConfiguration(Configuration configuration) {\n\t\tScopeFormats scopeFormats;\n\t\ttry {\n\t\t\tscopeFormats = ScopeFormats.fromConfig(configuration);\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Failed to parse scope format, using default scope formats\", e);\n\t\t\tscopeFormats = ScopeFormats.fromConfig(new Configuration());\n\t\t}\n\n\t\tchar delim;\n\t\ttry {\n\t\t\tdelim = configuration.getString(MetricOptions.SCOPE_DELIMITER).charAt(0);\n\t\t} catch (Exception e) {\n\t\t\tLOG.warn(\"Failed to parse delimiter, using default delimiter.\", e);\n\t\t\tdelim = '.';\n\t\t}\n\n\t\tfinal long maximumFrameSize = AkkaRpcServiceUtils.extractMaximumFramesize(configuration);\n\n\t\t// padding to account for serialization overhead\n\t\tfinal long messageSizeLimitPadding = 256;\n\n\t\treturn new MetricRegistryConfiguration(scopeFormats, delim, maximumFrameSize - messageSizeLimitPadding);\n\t}"}, {"instruction": "Notify all listeners of the priority tree change events (in ascending order)\n@param events The events (top down order) which have changed", "input": "Java", "output": "void notifyParentChanged(List<ParentChangedEvent> events) {\n        for (int i = 0; i < events.size(); ++i) {\n            ParentChangedEvent event = events.get(i);\n            stateOnlyRemovalQueue.priorityChanged(event.state);\n            if (event.state.parent != null && event.state.activeCountForTree != 0) {\n                event.state.parent.offerAndInitializePseudoTime(event.state);\n                event.state.parent.activeCountChangeForTree(event.state.activeCountForTree);\n            }\n        }\n    }"}, {"instruction": "\u901a\u8fc7certId\u83b7\u53d6\u9a8c\u7b7e\u8bc1\u4e66Map\u4e2d\u5bf9\u5e94\u8bc1\u4e66PublicKey\n\n@param certId \u8bc1\u4e66\u7269\u7406\u5e8f\u53f7\n@return \u901a\u8fc7\u8bc1\u4e66\u7f16\u53f7\u83b7\u53d6\u5230\u7684\u516c\u94a5", "input": "Java", "output": "public static PublicKey getValidatePublicKey(String certId) {\n\t\tX509Certificate cf = null;\n\t\tif (certMap.containsKey(certId)) {\n\t\t\t// \u5b58\u5728certId\u5bf9\u5e94\u7684\u8bc1\u4e66\u5bf9\u8c61\n\t\t\tcf = certMap.get(certId);\n\t\t\treturn cf.getPublicKey();\n\t\t} else {\n\t\t\t// \u4e0d\u5b58\u5728\u5219\u91cd\u65b0Load\u8bc1\u4e66\u6587\u4ef6\u76ee\u5f55\n\t\t\tinitValidateCertFromDir();\n\t\t\tif (certMap.containsKey(certId)) {\n\t\t\t\t// \u5b58\u5728certId\u5bf9\u5e94\u7684\u8bc1\u4e66\u5bf9\u8c61\n\t\t\t\tcf = certMap.get(certId);\n\t\t\t\treturn cf.getPublicKey();\n\t\t\t} else {\n\t\t\t\tLogUtil.writeErrorLog(\"\u7f3a\u5c11certId=[\" + certId + \"]\u5bf9\u5e94\u7684\u9a8c\u7b7e\u8bc1\u4e66.\");\n\t\t\t\treturn null;\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "------------------------------------------------------------------------", "input": "Java", "output": "@Override\n\tpublic MutableObjectIterator<E> getIterator() throws InterruptedException {\n\t\tsynchronized (this.iteratorLock) {\n\t\t\t// wait while both the iterator and the exception are not set\n\t\t\twhile (this.iterator == null && this.iteratorException == null) {\n\t\t\t\tthis.iteratorLock.wait();\n\t\t\t}\n\t\t\t\n\t\t\tif (this.iteratorException != null) {\n\t\t\t\tthrow new RuntimeException(\"Error obtaining the sorted input: \" + this.iteratorException.getMessage(),\n\t\t\t\t\tthis.iteratorException);\n\t\t\t}\n\t\t\telse {\n\t\t\t\treturn this.iterator;\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "Attempt to cancel job", "input": "Java", "output": "@Override public final boolean cancel( boolean mayInterruptIfRunning ) {\n    boolean did = false;\n    synchronized(this) {        // Install the answer under lock\n      if( !isCancelled() ) {\n        did = true;             // Did cancel (was not cancelled already)\n        _target.taskRemove(_tasknum);\n        _target = null;         // Flag as canceled\n//        UDPTimeOutThread.PENDING.remove(this);\n      }\n      notifyAll();              // notify in any case\n    }\n    return did;\n  }"}, {"instruction": "Copy the contents of the given Reader to the given Writer.\nCloses both when done.\n\n@param in  the Reader to copy from\n@param out the Writer to copy to\n@return the number of characters copied\n@throws IOException in case of I/O errors", "input": "Java", "output": "public static int copy(Reader in, Writer out) throws IOException {\n        assert in != null : \"No input Reader specified\";\n        assert out != null : \"No output Writer specified\";\n\n        try {\n            int byteCount = 0;\n            char[] buffer = new char[BUFFER_SIZE];\n            int bytesRead = -1;\n            while ((bytesRead = in.read(buffer)) != -1) {\n                out.write(buffer, 0, bytesRead);\n                byteCount += bytesRead;\n            }\n            out.flush();\n            return byteCount;\n        } finally {\n            try {\n                in.close();\n            } catch (IOException ex) {\n            }\n            try {\n                out.close();\n            } catch (IOException ex) {\n            }\n        }\n    }"}, {"instruction": "Initialize Settings for child components in build widget\n1. Build Status criteria settings\n2. Build Duration criteria settings\n\n@param buildScoreSettings", "input": "Java", "output": "private void initBuildScoreChildrenSettings(BuildScoreSettings buildScoreSettings) {\n    ScoreComponentSettings buildStatusSettings = Utils.getInstanceIfNull(\n      buildScoreSettings.getStatus(),\n      ScoreComponentSettings.class\n    );\n    buildStatusSettings.setCriteria(\n      Utils.mergeCriteria(buildScoreSettings.getCriteria(), buildStatusSettings.getCriteria())\n    );\n    buildScoreSettings.setStatus(buildStatusSettings);\n\n    BuildScoreSettings.BuildDurationScoreSettings buildDurationSettings = Utils.getInstanceIfNull(\n      buildScoreSettings.getDuration(),\n      BuildScoreSettings.BuildDurationScoreSettings.class\n    );\n\n    buildDurationSettings.setCriteria(\n      Utils.mergeCriteria(buildScoreSettings.getCriteria(), buildDurationSettings.getCriteria())\n    );\n    buildScoreSettings.setDuration(buildDurationSettings);\n  }"}, {"instruction": "\u521d\u59cb\u5316<br>\n\u8bbe\u5b9a\u6587\u4ef6\u4e2d\u7684host\u548c\u7aef\u53e3\u6709\u4e09\u79cd\u5f62\u5f0f\uff1a\n\n<pre>\nhost = host:port\n</pre>\n\n<pre>\nhost = host\nport = port\n</pre>\n\n<pre>\nhost = host\n</pre>", "input": "Java", "output": "synchronized public void initSingle() {\r\n\t\tif (setting == null) {\r\n\t\t\ttry {\r\n\t\t\t\tsetting = new Setting(MONGO_CONFIG_PATH, true);\r\n\t\t\t} catch (Exception e) {\r\n\t\t\t\t// \u5728single\u6a21\u5f0f\u4e0b\uff0c\u53ef\u4ee5\u6ca1\u6709\u914d\u7f6e\u6587\u4ef6\u3002\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tString group = StrUtil.EMPTY;\r\n\t\tif (null == this.serverAddress) {\r\n\t\t\t//\u5b58\u5728\u552f\u4e00\u5206\u7ec4\r\n\t\t\tif (groups != null && groups.length == 1) {\r\n\t\t\t\tgroup = groups[0];\r\n\t\t\t}\r\n\t\t\tserverAddress = createServerAddress(group);\r\n\t\t}\r\n\r\n\t\tfinal MongoCredential credentail = createCredentail(group);\r\n\t\ttry {\r\n\t\t\tif (null == credentail) {\r\n\t\t\t\tmongo = new MongoClient(serverAddress, buildMongoClientOptions(group));\r\n\t\t\t} else {\r\n\t\t\t\tmongo = new MongoClient(serverAddress, credentail, buildMongoClientOptions(group));\r\n\t\t\t}\r\n\t\t} catch (Exception e) {\r\n\t\t\tthrow new DbRuntimeException(StrUtil.format(\"Init MongoDB pool with connection to [{}] error!\", serverAddress), e);\r\n\t\t}\r\n\r\n\t\tlog.info(\"Init MongoDB pool with connection to [{}]\", serverAddress);\r\n\t}"}, {"instruction": "Store object into storage\n\n@param key\n@param object", "input": "Java", "output": "@Override\n    public void store(T key, INDArray object) {\n        INDArray toStore;\n        if (useInplaceCompression) {\n            compressor.compressi(object);\n            toStore = object;\n        } else {\n            toStore = compressor.compress(object);\n        }\n\n        if (emulateIsAbsent)\n            lock.writeLock().lock();\n\n        compressedEntries.put(key, toStore);\n\n        if (emulateIsAbsent)\n            lock.writeLock().unlock();\n    }"}, {"instruction": "Returns the default project ID, or {@code null} if no default project ID could be found. This\nmethod returns the first available project ID among the following sources:\n\n<ol>\n<li>The project ID specified by the GOOGLE_CLOUD_PROJECT environment variable\n<li>The App Engine project ID\n<li>The project ID specified in the JSON credentials file pointed by the {@code\nGOOGLE_APPLICATION_CREDENTIALS} environment variable\n<li>The Google Cloud SDK project ID\n<li>The Compute Engine project ID\n</ol>", "input": "Java", "output": "public static String getDefaultProjectId() {\n    String projectId = System.getProperty(PROJECT_ENV_NAME, System.getenv(PROJECT_ENV_NAME));\n    if (projectId == null) {\n      projectId =\n          System.getProperty(LEGACY_PROJECT_ENV_NAME, System.getenv(LEGACY_PROJECT_ENV_NAME));\n    }\n    if (projectId == null) {\n      projectId = getAppEngineProjectId();\n    }\n    if (projectId == null) {\n      projectId = getServiceAccountProjectId();\n    }\n    return projectId != null ? projectId : getGoogleCloudProjectId();\n  }"}, {"instruction": "\u83b7\u53d6\u96c6\u5408\u4e2d\u6307\u5b9a\u591a\u4e2a\u4e0b\u6807\u7684\u5143\u7d20\u503c\uff0c\u4e0b\u6807\u53ef\u4ee5\u4e3a\u8d1f\u6570\uff0c\u4f8b\u5982-1\u8868\u793a\u6700\u540e\u4e00\u4e2a\u5143\u7d20\n\n@param <T> \u5143\u7d20\u7c7b\u578b\n@param collection \u96c6\u5408\n@param indexes \u4e0b\u6807\uff0c\u652f\u6301\u8d1f\u6570\n@return \u5143\u7d20\u503c\u5217\u8868\n@since 4.0.6", "input": "Java", "output": "@SuppressWarnings(\"unchecked\")\r\n\tpublic static <T> List<T> getAny(Collection<T> collection, int... indexes) {\r\n\t\tfinal int size = collection.size();\r\n\t\tfinal ArrayList<T> result = new ArrayList<>();\r\n\t\tif (collection instanceof List) {\r\n\t\t\tfinal List<T> list = ((List<T>) collection);\r\n\t\t\tfor (int index : indexes) {\r\n\t\t\t\tif (index < 0) {\r\n\t\t\t\t\tindex += size;\r\n\t\t\t\t}\r\n\t\t\t\tresult.add(list.get(index));\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tObject[] array = ((Collection<T>) collection).toArray();\r\n\t\t\tfor (int index : indexes) {\r\n\t\t\t\tif (index < 0) {\r\n\t\t\t\t\tindex += size;\r\n\t\t\t\t}\r\n\t\t\t\tresult.add((T) array[index]);\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn result;\r\n\t}"}, {"instruction": "Uploads and registers a single resource and adds it to <tt>localResources</tt>.\n\n@param key\nthe key to add the resource under\n@param fs\nthe remote file system to upload to\n@param appId\napplication ID\n@param localSrcPath\nlocal path to the file\n@param localResources\nmap of resources\n\n@return the remote path to the uploaded resource", "input": "Java", "output": "private static Path setupSingleLocalResource(\n\t\t\tString key,\n\t\t\tFileSystem fs,\n\t\t\tApplicationId appId,\n\t\t\tPath localSrcPath,\n\t\t\tMap<String, LocalResource> localResources,\n\t\t\tPath targetHomeDir,\n\t\t\tString relativeTargetPath) throws IOException, URISyntaxException {\n\n\t\tTuple2<Path, LocalResource> resource = Utils.setupLocalResource(\n\t\t\tfs,\n\t\t\tappId.toString(),\n\t\t\tlocalSrcPath,\n\t\t\ttargetHomeDir,\n\t\t\trelativeTargetPath);\n\n\t\tlocalResources.put(key, resource.f1);\n\n\t\treturn resource.f0;\n\t}"}, {"instruction": "VJ: \u52a0\u4e0asymbolType instanceof JavaType\u7684\u5224\u65ad\uff0c\u9632\u6b62\u6570\u7ec4\u8f6c\u6362\u51fa\u9519", "input": "Java", "output": "private static boolean isConstantType(Type symbolType) {\n\t\treturn symbolType.isPrimitive() || symbolType.is(\"java.lang.String\") ||  symbolType.is(\"java.lang.Byte\") ||\n\t\t\t\tsymbolType.is(\"java.lang.Character\") ||\n\t\t\t\tsymbolType.is(\"java.lang.Short\") ||\n\t\t\t\tsymbolType.is(\"java.lang.Integer\") ||\n\t\t\t\tsymbolType.is(\"java.lang.Long\") ||\n\t\t\t\tsymbolType.is(\"java.lang.Float\") ||\n\t\t\t\tsymbolType.is(\"java.lang.Double\") ||\n\t\t\t\tsymbolType.is(\"java.lang.Boolean\");\n\t}"}, {"instruction": "Save a {@code JavaRDD<List<List<Writable>>>} to a Hadoop {@link org.apache.hadoop.io.SequenceFile}. Each record\nis given a unique (but noncontiguous) {@link LongWritable} key, and values are stored as {@link SequenceRecordWritable} instances.\n<p>\nUse {@link #restoreSequenceFileSequences(String, JavaSparkContext)} to restore values saved with this method.\n\n@param path           Path to save the sequence file\n@param rdd            RDD to save\n@param maxOutputFiles Nullable. If non-null: first coalesce the RDD to the specified size (number of partitions)\nto limit the maximum number of output sequence files\n@see #saveSequenceFile(String, JavaRDD)\n@see #saveMapFileSequences(String, JavaRDD)", "input": "Java", "output": "public static void saveSequenceFileSequences(String path, JavaRDD<List<List<Writable>>> rdd,\n                     Integer maxOutputFiles) {\n        path = FilenameUtils.normalize(path, true);\n        if (maxOutputFiles != null) {\n            rdd = rdd.coalesce(maxOutputFiles);\n        }\n        JavaPairRDD<List<List<Writable>>, Long> dataIndexPairs = rdd.zipWithUniqueId(); //Note: Long values are unique + NOT contiguous; more efficient than zipWithIndex\n        JavaPairRDD<LongWritable, SequenceRecordWritable> keyedByIndex =\n                        dataIndexPairs.mapToPair(new SequenceRecordSavePrepPairFunction());\n\n        keyedByIndex.saveAsNewAPIHadoopFile(path, LongWritable.class, SequenceRecordWritable.class,\n                        SequenceFileOutputFormat.class);\n    }"}, {"instruction": "\u6784\u9020Cluster\u5bf9\u8c61\n\n@param consumerBootstrap \u5ba2\u6237\u7aef\u914d\u7f6e\n@return Cluster\u5bf9\u8c61", "input": "Java", "output": "public static Cluster getCluster(ConsumerBootstrap consumerBootstrap) {\n        try {\n            ConsumerConfig consumerConfig = consumerBootstrap.getConsumerConfig();\n            ExtensionClass<Cluster> ext = ExtensionLoaderFactory.getExtensionLoader(Cluster.class)\n                .getExtensionClass(consumerConfig.getCluster());\n            if (ext == null) {\n                throw ExceptionUtils.buildRuntime(\"consumer.cluster\",\n                    consumerConfig.getCluster(), \"Unsupported cluster of client!\");\n            }\n            return ext.getExtInstance(new Class[] { ConsumerBootstrap.class },\n                new Object[] { consumerBootstrap });\n        } catch (SofaRpcRuntimeException e) {\n            throw e;\n        } catch (Throwable e) {\n            throw new SofaRpcRuntimeException(e.getMessage(), e);\n        }\n    }"}, {"instruction": "Get the bean-style property names for the specified object.\n\n@param targetClass the target object\n@return a set of property names", "input": "Java", "output": "public static Set<String> getPropertyNames(final Class<?> targetClass)\n   {\n      HashSet<String> set = new HashSet<>();\n      Matcher matcher = GETTER_PATTERN.matcher(\"\");\n      for (Method method : targetClass.getMethods()) {\n         String name = method.getName();\n         if (method.getParameterTypes().length == 0 && matcher.reset(name).matches()) {\n            name = name.replaceFirst(\"(get|is)\", \"\");\n            try {\n               if (targetClass.getMethod(\"set\" + name, method.getReturnType()) != null) {\n                  name = Character.toLowerCase(name.charAt(0)) + name.substring(1);\n                  set.add(name);\n               }\n            }\n            catch (Exception e) {\n               // fall thru (continue)\n            }\n         }\n      }\n\n      return set;\n   }"}, {"instruction": "Generate response for access token model and view.\n\n@param request  the request\n@param response the response\n@param result   the result\n@return the model and view", "input": "Java", "output": "protected ModelAndView generateResponseForAccessToken(final HttpServletRequest request,\n                                                          final HttpServletResponse response,\n                                                          final OAuth20AccessTokenResponseResult result) {\n        val model = getAccessTokenResponseModel(request, response, result);\n        return new ModelAndView(new MappingJackson2JsonView(MAPPER), model);\n    }"}, {"instruction": "Log {@link CasServiceTicketValidatedEvent} at debug level.\n\n@param e the event", "input": "Java", "output": "@EventListener\n    public void logServiceTicketValidatedEvent(final CasServiceTicketValidatedEvent e) {\n        val principal = e.getServiceTicket().getTicketGrantingTicket().getAuthentication().getPrincipal();\n        LOGGER.debug(VALIDATED_ST_MSG,\n                e.getServiceTicket().getCreationTime(),\n                e.getServiceTicket().getId(),\n                e.getServiceTicket().getService().getId(),\n                principal.getId(),\n                principal.getAttributes());\n    }"}, {"instruction": "\u589e\u52a0\u4e00\u4e2a\u8bcd\u5178\n\n@param key\n@param path\n@param value", "input": "Java", "output": "public static void putLibrary(String key, String path, Object value) {\n        if (key.startsWith(DicLibrary.DEFAULT)) {\n            DicLibrary.put(key, path, (Forest) value);\n        } else if (key.startsWith(StopLibrary.DEFAULT)) {\n            StopLibrary.put(key, path, (StopRecognition) value);\n        } else if (key.startsWith(SynonymsLibrary.DEFAULT)) {\n            SynonymsLibrary.put(key, path, (SmartForest) value);\n        } else if (key.startsWith(AmbiguityLibrary.DEFAULT)) {\n            AmbiguityLibrary.put(key, path, (Forest) value);\n        } else if (key.startsWith(CrfLibrary.DEFAULT)) {\n            CrfLibrary.put(key, path, (SplitWord) value);\n        } else {\n            throw new LibraryException(key + \" type err must start with dic,stop,ambiguity,synonyms\");\n        }\n        ENV.put(key, path);\n    }"}, {"instruction": "Load a custom class, which is provided by a configuration CUSTOM_JMX_ATTRIBUTE_PROCESSOR_PROPERTY.\n\nThis method will try to instantiate an instance of this custom class and with given properties\nas the argument in the constructor.\n\nBasically the custom class must have a constructor that takes an argument with type\nProperties.", "input": "Java", "output": "private void loadCustomJMXAttributeProcessor(final Props props) {\n    final String jmxAttributeEmitter =\n        props.get(CUSTOM_JMX_ATTRIBUTE_PROCESSOR_PROPERTY);\n    if (jmxAttributeEmitter != null) {\n      try {\n        logger.info(\"jmxAttributeEmitter: \" + jmxAttributeEmitter);\n        final Constructor<Props>[] constructors =\n            (Constructor<Props>[]) Class.forName(jmxAttributeEmitter).getConstructors();\n\n        constructors[0].newInstance(props.toProperties());\n      } catch (final Exception e) {\n        logger.error(\"Encountered error while loading and instantiating \"\n            + jmxAttributeEmitter, e);\n        throw new IllegalStateException(\n            \"Encountered error while loading and instantiating \"\n                + jmxAttributeEmitter, e);\n      }\n    } else {\n      logger.info(\"No value for property: \"\n          + CUSTOM_JMX_ATTRIBUTE_PROCESSOR_PROPERTY + \" was found\");\n    }\n  }"}, {"instruction": "Returns the index within this string of the first occurrence of the\nspecified substring. If it is not a substring, return -1.\n\n@param content the content where we've to search into\n@return the index of the occurrence or -1 if no occurrence has been found", "input": "Java", "output": "public int findInContent(String content) {\r\n        int n = content.length();\r\n        int m = pattern.length();\r\n        int skip;\r\n        char val;\r\n\r\n        for (int i = 0; i <= n - m; i = i + skip) {\r\n            skip = 0;\r\n            for (int j = m - 1; j >= 0; j--) {\r\n                if (pattern.charAt(j) != content.charAt(i + j)) {\r\n                    val = content.charAt(i + j);\r\n\r\n                    skip = (occurrence.get(val) != null) ?\r\n                            Math.max(1, j - occurrence.get(val)) :\r\n                            j + 1;\r\n\r\n                    break;\r\n                }\r\n            }\r\n\r\n            if (skip == 0) {\r\n                return i;\r\n            }\r\n        }\r\n        \r\n        return -1;\r\n    }"}, {"instruction": "Find first by uid, otp pair.\n\n@param uid uid to search\n@param otp otp to search\n@return token for uid, otp pair", "input": "Java", "output": "@View(name = \"by_uid_otp\", map = \"function(doc) { if(doc.token && doc.userId) { emit([doc.userId, doc.token], doc) } }\")\n    public CouchDbGoogleAuthenticatorToken findOneByUidForOtp(final String uid, final Integer otp) {\n        val view = createQuery(\"by_uid_otp\").key(ComplexKey.of(uid, otp)).limit(1);\n        return db.queryView(view, CouchDbGoogleAuthenticatorToken.class).stream().findFirst().orElse(null);\n    }"}, {"instruction": "Generates a new secure-random secret key of a length suitable for creating and verifying HMAC signatures\naccording to the specified {@code SignatureAlgorithm} using the specified SecureRandom number generator.  This\nimplementation returns secure-random key sizes as follows:\n\n<table> <caption>Key Sizes</caption> <thead> <tr> <th>Signature Algorithm</th> <th>Generated Key Size</th> </tr> </thead> <tbody> <tr>\n<td>HS256</td> <td>256 bits (32 bytes)</td> </tr> <tr> <td>HS384</td> <td>384 bits (48 bytes)</td> </tr> <tr>\n<td>HS512</td> <td>512 bits (64 bytes)</td> </tr> </tbody> </table>\n\n@param alg    the signature algorithm that will be used with the generated key\n@param random the secure random number generator used during key generation\n@return a new secure-random secret key of a length suitable for creating and verifying HMAC signatures according\nto the specified {@code SignatureAlgorithm} using the specified SecureRandom number generator.\n@see #generateKey()\n@see #generateKey(SignatureAlgorithm)\n@since 0.5\n@deprecated since 0.10.0 - use {@link #generateKey(SignatureAlgorithm)} instead.", "input": "Java", "output": "@Deprecated\n    public static SecretKey generateKey(SignatureAlgorithm alg, SecureRandom random) {\n\n        Assert.isTrue(alg.isHmac(), \"SignatureAlgorithm argument must represent an HMAC algorithm.\");\n\n        KeyGenerator gen;\n\n        try {\n            gen = KeyGenerator.getInstance(alg.getJcaName());\n        } catch (NoSuchAlgorithmException e) {\n            throw new IllegalStateException(\"The \" + alg.getJcaName() + \" algorithm is not available.  \" +\n                \"This should never happen on JDK 7 or later - please report this to the JJWT developers.\", e);\n        }\n\n        return gen.generateKey();\n    }"}, {"instruction": "Tries to write&flush unannounced credits for the next input channel in queue.\n\n<p>This method may be called by the first input channel enqueuing, or the complete\nfuture's callback in previous input channel, or the channel writability changed event.", "input": "Java", "output": "private void writeAndFlushNextMessageIfPossible(Channel channel) {\n\t\tif (channelError.get() != null || !channel.isWritable()) {\n\t\t\treturn;\n\t\t}\n\n\t\twhile (true) {\n\t\t\tRemoteInputChannel inputChannel = inputChannelsWithCredit.poll();\n\n\t\t\t// The input channel may be null because of the write callbacks\n\t\t\t// that are executed after each write.\n\t\t\tif (inputChannel == null) {\n\t\t\t\treturn;\n\t\t\t}\n\n\t\t\t//It is no need to notify credit for the released channel.\n\t\t\tif (!inputChannel.isReleased()) {\n\t\t\t\tAddCredit msg = new AddCredit(\n\t\t\t\t\tinputChannel.getPartitionId(),\n\t\t\t\t\tinputChannel.getAndResetUnannouncedCredit(),\n\t\t\t\t\tinputChannel.getInputChannelId());\n\n\t\t\t\t// Write and flush and wait until this is done before\n\t\t\t\t// trying to continue with the next input channel.\n\t\t\t\tchannel.writeAndFlush(msg).addListener(writeListener);\n\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "init fxml when loaded.", "input": "Java", "output": "@PostConstruct\n    public void init() {\n        validatedText.focusedProperty().addListener((o, oldVal, newVal) -> {\n            if (!newVal) {\n                validatedText.validate();\n            }\n        });\n        validatedPassowrd.focusedProperty().addListener((o, oldVal, newVal) -> {\n            if (!newVal) {\n                validatedPassowrd.validate();\n            }\n        });\n        jfxTextArea.focusedProperty().addListener((o, oldVal, newVal) -> {\n            if (!newVal) {\n                jfxTextArea.validate();\n            }\n        });\n    }"}, {"instruction": "Tries to load the record #N.\n\n@return null if the data failed to load.", "input": "Java", "output": "private R load(int n, Index editInPlace) {\n        assert Thread.holdsLock(this);\n        assert dir != null;\n        R v = load(new File(dir, String.valueOf(n)), editInPlace);\n        if (v==null && editInPlace!=null) {\n            // remember the failure.\n            // if editInPlace==null, we can create a new copy for this, but not sure if it's worth doing,\n            // TODO should we also update numberOnDisk?\n            editInPlace.byNumber.put(n, null);\n        }\n        return v;\n    }"}, {"instruction": "Create a RPC service for the task manager.\n\n@param configuration The configuration for the TaskManager.\n@param haServices to use for the task manager hostname retrieval", "input": "Java", "output": "public static RpcService createRpcService(\n\t\t\tfinal Configuration configuration,\n\t\t\tfinal HighAvailabilityServices haServices) throws Exception {\n\n\t\tcheckNotNull(configuration);\n\t\tcheckNotNull(haServices);\n\n\t\tfinal String taskManagerAddress = determineTaskManagerBindAddress(configuration, haServices);\n\t\tfinal String portRangeDefinition = configuration.getString(TaskManagerOptions.RPC_PORT);\n\n\t\treturn AkkaRpcServiceUtils.createRpcService(taskManagerAddress, portRangeDefinition, configuration);\n\t}"}, {"instruction": "Fit a MultiDataSet (only compute based on the statistics from this {@link MultiDataSet})\n\n@param dataSet the dataset to compute on", "input": "Java", "output": "public void fit(@NonNull MultiDataSet dataSet) {\n        List<S.Builder> featureNormBuilders = new ArrayList<>();\n        List<S.Builder> labelNormBuilders = new ArrayList<>();\n\n        fitPartial(dataSet, featureNormBuilders, labelNormBuilders);\n\n        featureStats = buildList(featureNormBuilders);\n        if (isFitLabel()) {\n            labelStats = buildList(labelNormBuilders);\n        }\n    }"}, {"instruction": "For classpath reasons, we'll put each link file in a separate directory.\nThis must be called only after the job id has been inserted by the job.\n\n@param props The Azkaban properties", "input": "Java", "output": "public static String getDirName(Props props) {\n    String dirSuffix = props.get(CommonJobProperties.NESTED_FLOW_PATH);\n\n    if ((dirSuffix == null) || (dirSuffix.length() == 0)) {\n      dirSuffix = props.get(CommonJobProperties.JOB_ID);\n      if ((dirSuffix == null) || (dirSuffix.length() == 0)) {\n        throw new RuntimeException(\"azkaban.flow.nested.path and azkaban.job.id were not set\");\n      }\n    }\n\n    return \"_resources_\" + dirSuffix.replace(':', '_');\n  }"}, {"instruction": "Encodes 64 or 128 bits from the input into a hex trace ID.\n\n@param high Upper 64bits of the trace ID. Zero means the trace ID is 64-bit.\n@param low Lower 64bits of the trace ID.\n@throws IllegalArgumentException if both values are zero", "input": "Java", "output": "public Builder traceId(long high, long low) {\n      if (high == 0L && low == 0L) throw new IllegalArgumentException(\"empty trace ID\");\n      char[] result = new char[high != 0L ? 32 : 16];\n      int pos = 0;\n      if (high != 0L) {\n        writeHexLong(result, pos, high);\n        pos += 16;\n      }\n      writeHexLong(result, pos, low);\n      this.traceId = new String(result);\n      return this;\n    }"}, {"instruction": "Writes the given {@link AccessExecutionGraph} to the {@link FileSystem} pointed to by\n{@link JobManagerOptions#ARCHIVE_DIR}.\n\n@param rootPath directory to which the archive should be written to\n@param jobId  job id\n@param jsonToArchive collection of json-path pairs to that should be archived\n@return path to where the archive was written, or null if no archive was created\n@throws IOException", "input": "Java", "output": "public static Path archiveJob(Path rootPath, JobID jobId, Collection<ArchivedJson> jsonToArchive) throws IOException {\n\t\ttry {\n\t\t\tFileSystem fs = rootPath.getFileSystem();\n\t\t\tPath path = new Path(rootPath, jobId.toString());\n\t\t\tOutputStream out = fs.create(path, FileSystem.WriteMode.NO_OVERWRITE);\n\n\t\t\ttry (JsonGenerator gen = jacksonFactory.createGenerator(out, JsonEncoding.UTF8)) {\n\t\t\t\tgen.writeStartObject();\n\t\t\t\tgen.writeArrayFieldStart(ARCHIVE);\n\t\t\t\tfor (ArchivedJson archive : jsonToArchive) {\n\t\t\t\t\tgen.writeStartObject();\n\t\t\t\t\tgen.writeStringField(PATH, archive.getPath());\n\t\t\t\t\tgen.writeStringField(JSON, archive.getJson());\n\t\t\t\t\tgen.writeEndObject();\n\t\t\t\t}\n\t\t\t\tgen.writeEndArray();\n\t\t\t\tgen.writeEndObject();\n\t\t\t} catch (Exception e) {\n\t\t\t\tfs.delete(path, false);\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t\tLOG.info(\"Job {} has been archived at {}.\", jobId, path);\n\t\t\treturn path;\n\t\t} catch (IOException e) {\n\t\t\tLOG.error(\"Failed to archive job.\", e);\n\t\t\tthrow e;\n\t\t}\n\t}"}, {"instruction": "Create merge engine instance.\n\n@param databaseType database type\n@param shardingRule sharding rule\n@param routeResult SQL route result\n@param shardingTableMetaData sharding table meta Data\n@param queryResults query results\n@return merge engine instance\n@throws SQLException SQL exception", "input": "Java", "output": "public static MergeEngine newInstance(final DatabaseType databaseType, final ShardingRule shardingRule,\n                                          final SQLRouteResult routeResult, final ShardingTableMetaData shardingTableMetaData, final List<QueryResult> queryResults) throws SQLException {\n        if (routeResult.getSqlStatement() instanceof SelectStatement) {\n            return new DQLMergeEngine(databaseType, routeResult, queryResults);\n        } \n        if (routeResult.getSqlStatement() instanceof DALStatement) {\n            return new DALMergeEngine(shardingRule, queryResults, (DALStatement) routeResult.getSqlStatement(), shardingTableMetaData);\n        }\n        throw new UnsupportedOperationException(String.format(\"Cannot support type '%s'\", routeResult.getSqlStatement().getType()));\n    }"}, {"instruction": "\u83b7\u53d6{@link KeyGenerator}\n\n@param algorithm \u5bf9\u79f0\u52a0\u5bc6\u7b97\u6cd5\n@return {@link KeyGenerator}\n@since 4.5.2", "input": "Java", "output": "public static KeyGenerator getKeyGenerator(String algorithm) {\r\n\t\tfinal Provider provider = GlobalBouncyCastleProvider.INSTANCE.getProvider();\r\n\r\n\t\tKeyGenerator generator;\r\n\t\ttry {\r\n\t\t\tgenerator = (null == provider) //\r\n\t\t\t\t\t? KeyGenerator.getInstance(getMainAlgorithm(algorithm)) //\r\n\t\t\t\t\t: KeyGenerator.getInstance(getMainAlgorithm(algorithm), provider);\r\n\t\t} catch (NoSuchAlgorithmException e) {\r\n\t\t\tthrow new CryptoException(e);\r\n\t\t}\r\n\t\treturn generator;\r\n\t}"}, {"instruction": "True if there is no item in Jenkins that has this name\n@param name The name to test\n@param currentJobName The name of the job that the user is configuring", "input": "Java", "output": "boolean isNameUnique(String name, String currentJobName) {\n        Item item = getItem(name);\n\n        if(null==item) {\n            // the candidate name didn't return any items so the name is unique\n            return true;\n        }\n        else if(item.getName().equals(currentJobName)) {\n            // the candidate name returned an item, but the item is the item\n            // that the user is configuring so this is ok\n            return true;\n        }\n        else {\n            // the candidate name returned an item, so it is not unique\n            return false;\n        }\n    }"}, {"instruction": "Set the state of the RNN layer, for use in {@link #rnnTimeStep(INDArray...)}\n\n@param layerName The name of the layer.\n@param state     The state to set the specified layer to", "input": "Java", "output": "public void rnnSetPreviousState(String layerName, Map<String, INDArray> state) {\n        Layer l = verticesMap.get(layerName).getLayer();\n        if(l instanceof org.deeplearning4j.nn.layers.wrapper.BaseWrapperLayer){\n            l = ((org.deeplearning4j.nn.layers.wrapper.BaseWrapperLayer)l).getUnderlying();\n        }\n        if (l == null || !(l instanceof RecurrentLayer)) {\n            throw new UnsupportedOperationException(\n                    \"Layer \\\"\" + layerName + \"\\\" is not a recurrent layer. Cannot set state\");\n        }\n        ((RecurrentLayer) l).rnnSetPreviousState(state);\n    }"}, {"instruction": "Resolves the default request factory.\n\n@return The default request factory.", "input": "Java", "output": "static HttpResponseFactory resolveDefaultResponseFactory() {\n        Optional<ServiceDefinition<HttpResponseFactory>> definition = SoftServiceLoader.load(HttpResponseFactory.class)\n                .firstOr(\"io.micronaut.http.server.netty.NettyHttpResponseFactory\", HttpResponseFactory.class.getClassLoader());\n\n        if (definition.isPresent()) {\n            ServiceDefinition<HttpResponseFactory> sd = definition.get();\n            try {\n                return sd.load();\n            } catch (Throwable e) {\n                LOG.warn(\"Unable to load default response factory for definition [\" + definition + \"]: \" + e.getMessage(), e);\n            }\n        }\n        return new SimpleHttpResponseFactory();\n    }"}, {"instruction": "Method that assigns a unique {@link Long} value to all elements in the input data set as described below.\n<ul>\n<li> a map function is applied to the input data set\n<li> each map task holds a counter c which is increased for each record\n<li> c is shifted by n bits where n = log2(number of parallel tasks)\n<li> to create a unique ID among all tasks, the task id is added to the counter\n<li> for each record, the resulting counter is collected\n</ul>\n\n@param input the input data set\n@return a data set of tuple 2 consisting of ids and initial values.", "input": "Java", "output": "public static <T> DataSet<Tuple2<Long, T>> zipWithUniqueId (DataSet <T> input) {\n\n\t\treturn input.mapPartition(new RichMapPartitionFunction<T, Tuple2<Long, T>>() {\n\n\t\t\tlong maxBitSize = getBitSize(Long.MAX_VALUE);\n\t\t\tlong shifter = 0;\n\t\t\tlong start = 0;\n\t\t\tlong taskId = 0;\n\t\t\tlong label = 0;\n\n\t\t\t@Override\n\t\t\tpublic void open(Configuration parameters) throws Exception {\n\t\t\t\tsuper.open(parameters);\n\t\t\t\tshifter = getBitSize(getRuntimeContext().getNumberOfParallelSubtasks() - 1);\n\t\t\t\ttaskId = getRuntimeContext().getIndexOfThisSubtask();\n\t\t\t}\n\n\t\t\t@Override\n\t\t\tpublic void mapPartition(Iterable<T> values, Collector<Tuple2<Long, T>> out) throws Exception {\n\t\t\t\tfor (T value : values) {\n\t\t\t\t\tlabel = (start << shifter) + taskId;\n\n\t\t\t\t\tif (getBitSize(start) + shifter < maxBitSize) {\n\t\t\t\t\t\tout.collect(new Tuple2<>(label, value));\n\t\t\t\t\t\tstart++;\n\t\t\t\t\t} else {\n\t\t\t\t\t\tthrow new Exception(\"Exceeded Long value range while generating labels\");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t});\n\t}"}, {"instruction": "Sets the maintenance policy for a cluster.\n\n<p>Sample code:\n\n<pre><code>\ntry (ClusterManagerClient clusterManagerClient = ClusterManagerClient.create()) {\nString projectId = \"\";\nString zone = \"\";\nString clusterId = \"\";\nMaintenancePolicy maintenancePolicy = MaintenancePolicy.newBuilder().build();\nOperation response = clusterManagerClient.setMaintenancePolicy(projectId, zone, clusterId, maintenancePolicy);\n}\n</code></pre>\n\n@param projectId The Google Developers Console [project ID or project\nnumber](https://support.google.com/cloud/answer/6158840).\n@param zone The name of the Google Compute Engine [zone](/compute/docs/zones#available) in\nwhich the cluster resides.\n@param clusterId The name of the cluster to update.\n@param maintenancePolicy The maintenance policy to be set for the cluster. An empty field\nclears the existing maintenance policy.\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "public final Operation setMaintenancePolicy(\n      String projectId, String zone, String clusterId, MaintenancePolicy maintenancePolicy) {\n\n    SetMaintenancePolicyRequest request =\n        SetMaintenancePolicyRequest.newBuilder()\n            .setProjectId(projectId)\n            .setZone(zone)\n            .setClusterId(clusterId)\n            .setMaintenancePolicy(maintenancePolicy)\n            .build();\n    return setMaintenancePolicy(request);\n  }"}, {"instruction": "This method updates VocabCache and all it's elements with Huffman indexes\nPlease note: it should be the same VocabCache as was used for Huffman tree initialization\n\n@param cache VocabCache to be updated.", "input": "Java", "output": "public void applyIndexes(VocabCache<? extends SequenceElement> cache) {\n        if (!buildTrigger)\n            build();\n\n        for (int a = 0; a < words.size(); a++) {\n            if (words.get(a).getLabel() != null) {\n                cache.addWordToIndex(a, words.get(a).getLabel());\n            } else {\n                cache.addWordToIndex(a, words.get(a).getStorageId());\n            }\n\n            words.get(a).setIndex(a);\n        }\n    }"}, {"instruction": "Register an application with its secret.\nExecutors need to first authenticate themselves with the same secret before\nfetching shuffle files written by other executors in this application.", "input": "Java", "output": "public void registerApp(String appId, String shuffleSecret) {\n    // Always put the new secret information to make sure it's the most up to date.\n    // Otherwise we have to specifically look at the application attempt in addition\n    // to the applicationId since the secrets change between application attempts on yarn.\n    shuffleSecretMap.put(appId, shuffleSecret);\n    logger.info(\"Registered shuffle secret for application {}\", appId);\n  }"}, {"instruction": "Gets response mode type.\n\n@param context the context\n@return the response type", "input": "Java", "output": "public static OAuth20ResponseModeTypes getResponseModeType(final J2EContext context) {\n        val responseType = context.getRequestParameter(OAuth20Constants.RESPONSE_MODE);\n        val type = Arrays.stream(OAuth20ResponseModeTypes.values())\n            .filter(t -> t.getType().equalsIgnoreCase(responseType))\n            .findFirst()\n            .orElse(OAuth20ResponseModeTypes.NONE);\n        LOGGER.debug(\"OAuth response type is [{}]\", type);\n        return type;\n    }"}, {"instruction": "Overwrites the document referred to by this DocumentReference. If the document doesn't exist\nyet, it will be created. If you pass {@link SetOptions}, the provided data can be merged into\nan existing document.\n\n@param documentReference The DocumentReference to overwrite.\n@param pojo The POJO that will be used to populate the document contents.\n@param options An object to configure the set behavior.\n@return The instance for chaining.", "input": "Java", "output": "@Nonnull\n  public T set(\n      @Nonnull DocumentReference documentReference,\n      @Nonnull Object pojo,\n      @Nonnull SetOptions options) {\n    Object data = CustomClassMapper.convertToPlainJavaTypes(pojo);\n    if (!(data instanceof Map)) {\n      throw new IllegalArgumentException(\"Can't set a document's data to an array or primitive\");\n    }\n    return performSet(documentReference, (Map<String, Object>) data, options);\n  }"}, {"instruction": "\u8ba1\u7b97\u5e73\u5747\u5f02\u5e38\u7387\uff0c\u5982\u679c\u8c03\u7528\u6b21\u6570\u5c0f\u4e8eleastWindowCount\u5219\u4e0d\u53c2\u4e0e\u8ba1\u7b97\u3002 \u5982\u679c\u6240\u6709\u8c03\u7528\u6b21\u6570\u5747\u4e3a0\u5219\u8fd4\u56de-1\n\n@param invocationStats List<InvocationStat>\n@param leastWindowCount leastWindowCount\n@return The average exception rate of all invocation statics", "input": "Java", "output": "private double calculateAverageExceptionRate(List<InvocationStat> invocationStats, long leastWindowCount) {\n        long sumException = 0;\n        long sumCall = 0;\n        for (InvocationStat invocationStat : invocationStats) {\n\n            long invocationLeastWindowCount = getInvocationLeastWindowCount(invocationStat,\n                ProviderInfoWeightManager.getWeight(invocationStat.getDimension().getProviderInfo()),\n                leastWindowCount);\n\n            if (invocationLeastWindowCount != -1\n                && invocationStat.getInvokeCount() >= invocationLeastWindowCount) {\n                sumException += invocationStat.getExceptionCount();\n                sumCall += invocationStat.getInvokeCount();\n            }\n        }\n        if (sumCall == 0) {\n            return -1;\n        }\n        return CalculateUtils.divide(sumException, sumCall);\n    }"}, {"instruction": "Returns the URL of the index page jelly script.", "input": "Java", "output": "public URL getIndexPage() {\n        // In the current impl dependencies are checked first, so the plugin itself\n        // will add the last entry in the getResources result.\n        URL idx = null;\n        try {\n            Enumeration<URL> en = classLoader.getResources(\"index.jelly\");\n            while (en.hasMoreElements())\n                idx = en.nextElement();\n        } catch (IOException ignore) { }\n        // In case plugin has dependencies but is missing its own index.jelly,\n        // check that result has this plugin's artifactId in it:\n        return idx != null && idx.toString().contains(shortName) ? idx : null;\n    }"}, {"instruction": "Remove a value from the bag.  This method should only be called\nwith objects obtained by <code>borrow(long, TimeUnit)</code> or <code>reserve(T)</code>\n\n@param bagEntry the value to remove\n@return true if the entry was removed, false otherwise\n@throws IllegalStateException if an attempt is made to remove an object\nfrom the bag that was not borrowed or reserved first", "input": "Java", "output": "public boolean remove(final T bagEntry)\n   {\n      if (!bagEntry.compareAndSet(STATE_IN_USE, STATE_REMOVED) && !bagEntry.compareAndSet(STATE_RESERVED, STATE_REMOVED) && !closed) {\n         LOGGER.warn(\"Attempt to remove an object from the bag that was not borrowed or reserved: {}\", bagEntry);\n         return false;\n      }\n\n      final boolean removed = sharedList.remove(bagEntry);\n      if (!removed && !closed) {\n         LOGGER.warn(\"Attempt to remove an object from the bag that does not exist: {}\", bagEntry);\n      }\n\n      return removed;\n   }"}, {"instruction": "Reads a number of items starting with the first one that has a Sequence Number higher than the given one.\n\n@param afterSequenceNumber The sequence to search from.\n@param count               The maximum number of items to read.\n@return An Iterator with the resulting items. If no results are available for the given parameters, an empty iterator is returned.", "input": "Java", "output": "public Iterator<T> read(long afterSequenceNumber, int count) {\n        ListNode<T> firstNode;\n        synchronized (this.lock) {\n            firstNode = this.head;\n        }\n\n        // Find the first node that has a Sequence Number after the given one, but make sure we release and reacquire\n        // the lock with every iteration. This will prevent long-list scans from blocking adds.\n        while (firstNode != null && firstNode.item.getSequenceNumber() <= afterSequenceNumber) {\n            synchronized (this.lock) {\n                firstNode = firstNode.next;\n            }\n        }\n\n        return new NodeIterator<>(firstNode, count, this.lock);\n    }"}, {"instruction": "Get all dashboards filtered by title and Pageable ( default page size = 10)\n\n@param title, pageable\n@return Page<Dashboard>", "input": "Java", "output": "@Override\n    public Page<Dashboard> getDashboardByTitleWithFilter(String title, String type, Pageable pageable) {\n        Page<Dashboard> dashboardItems = null;\n        if ((type != null) && (!type.isEmpty()) && (!UNDEFINED.equalsIgnoreCase(type))) {\n            dashboardItems = dashboardRepository.findAllByTypeContainingIgnoreCaseAndTitleContainingIgnoreCase(type, title, pageable);\n        } else {\n            dashboardItems = dashboardRepository.findAllByTitleContainingIgnoreCase(title, pageable);\n        }\n\n        return dashboardItems;\n    }"}, {"instruction": "Get zero masking flag\n\n@param layerConfig dictionary containing Keras layer configuration\n@return if masking zeros or not\n@throws InvalidKerasConfigurationException Invalid Keras configuration", "input": "Java", "output": "public static boolean getZeroMaskingFromConfig(Map<String, Object> layerConfig,\n                                                   KerasLayerConfiguration conf)\n            throws InvalidKerasConfigurationException {\n        Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\n        boolean hasZeroMasking = true;\n        if (innerConfig.containsKey(conf.getLAYER_FIELD_MASK_ZERO())) {\n            hasZeroMasking = (boolean) innerConfig.get(conf.getLAYER_FIELD_MASK_ZERO());\n        }\n        return hasZeroMasking;\n    }"}, {"instruction": "Can ping.\n\n@return true/false", "input": "Java", "output": "public boolean canPing() {\n        val uidPsw = getClass().getSimpleName();\n        for (val server : this.servers) {\n            LOGGER.debug(\"Attempting to ping RADIUS server [{}] via simulating an authentication request. If the server responds \"\n                + \"successfully, mock authentication will fail correctly.\", server);\n            try {\n                server.authenticate(uidPsw, uidPsw);\n            } catch (final TimeoutException | SocketTimeoutException e) {\n                LOGGER.debug(\"Server [{}] is not available\", server);\n                continue;\n            } catch (final Exception e) {\n                LOGGER.debug(\"Pinging RADIUS server was successful. Response [{}]\", e.getMessage());\n            }\n            return true;\n        }\n        return false;\n    }"}, {"instruction": "Build an object out of a given class and a map for field names to values.\n@param clazz The class to be created.\n@param params A map of the parameters.\n@return An instantiated object.\n@throws Exception when constructor fails.", "input": "Java", "output": "public static Object constructByNamedParams(Class clazz, Map params) throws Exception {\n        Object obj = clazz.getConstructor().newInstance();\n\n        Method[] allMethods = clazz.getMethods();\n        for(Method method : allMethods) {\n            if(method.getName().startsWith(\"set\")) {\n                Object [] o = new Object [1];\n                String propertyName = Introspector.decapitalize(method.getName().substring(3));\n                if (params.containsKey(propertyName)) {\n                    o[0] = params.get(propertyName);\n                    method.invoke(obj, o);\n                }\n            }\n        }\n        return obj;\n    }"}, {"instruction": "Process the queue of commands and dispatch them to the stream. This method is only\ncalled in the event loop", "input": "Java", "output": "private void flush() {\n    try {\n      QueuedCommand cmd;\n      int i = 0;\n      boolean flushedOnce = false;\n      while ((cmd = queue.poll()) != null) {\n        cmd.run(channel);\n        if (++i == DEQUE_CHUNK_SIZE) {\n          i = 0;\n          // Flush each chunk so we are releasing buffers periodically. In theory this loop\n          // might never end as new events are continuously added to the queue, if we never\n          // flushed in that case we would be guaranteed to OOM.\n          channel.flush();\n          flushedOnce = true;\n        }\n      }\n      // Must flush at least once, even if there were no writes.\n      if (i != 0 || !flushedOnce) {\n        channel.flush();\n      }\n    } finally {\n      // Mark the write as done, if the queue is non-empty after marking trigger a new write.\n      scheduled.set(false);\n      if (!queue.isEmpty()) {\n        scheduleFlush();\n      }\n    }\n  }"}, {"instruction": "Hashes a filename into the corresponding local directory, in a manner consistent with\nSpark's DiskBlockManager.getFile().", "input": "Java", "output": "@VisibleForTesting\n  static File getFile(String[] localDirs, int subDirsPerLocalDir, String filename) {\n    int hash = JavaUtils.nonNegativeHash(filename);\n    String localDir = localDirs[hash % localDirs.length];\n    int subDirId = (hash / localDirs.length) % subDirsPerLocalDir;\n    return new File(createNormalizedInternedPathname(\n        localDir, String.format(\"%02x\", subDirId), filename));\n  }"}, {"instruction": "Returns the next items from the queue. If the queue is empty, it blocks the call until at least one item is added.\n\n@param maxCount The maximum number of items to return. This argument will be ignored if the queue is currently empty,\nbut in that case the result will always be completed with exactly one element.\n@return A CompletableFuture that, when completed, will contain the requested result. If the queue is not currently\nempty, this Future will already be completed, otherwise it will be completed the next time the add() method is called.\nIf the queue is closed and this Future is not yet completed, it will be cancelled.\n@throws ObjectClosedException If the Queue is closed.\n@throws IllegalStateException If another call to take() is in progress.", "input": "Java", "output": "public CompletableFuture<Queue<T>> take(int maxCount) {\n        synchronized (this.contents) {\n            Exceptions.checkNotClosed(this.closed, this);\n            Preconditions.checkState(this.pendingTake == null, \"Cannot have more than one concurrent pending take() request.\");\n            Queue<T> result = fetch(maxCount);\n            if (result.size() > 0) {\n                return CompletableFuture.completedFuture(result);\n            } else {\n                this.pendingTake = new CompletableFuture<>();\n                return this.pendingTake;\n            }\n        }\n    }"}, {"instruction": "\u5339\u914d\u6587\u672c\n\n@param text      \u6587\u672c\n@param processor \u5904\u7406\u5668", "input": "Java", "output": "public void parseText(String text, AhoCorasickDoubleArrayTrie.IHit<V> processor)\n    {\n        int length = text.length();\n        int begin = 0;\n        BaseNode<V> state = this;\n\n        for (int i = begin; i < length; ++i)\n        {\n            state = state.transition(text.charAt(i));\n            if (state != null)\n            {\n                V value = state.getValue();\n                if (value != null)\n                {\n                    processor.hit(begin, i + 1, value);\n                }\n            }\n            else\n            {\n                i = begin;\n                ++begin;\n                state = this;\n            }\n        }\n    }"}, {"instruction": "\u9012\u5f52\u538b\u7f29\u6587\u4ef6\u5939<br>\nsrcRootDir\u51b3\u5b9a\u4e86\u8def\u5f84\u622a\u53d6\u7684\u4f4d\u7f6e\uff0c\u4f8b\u5982\uff1a<br>\nfile\u7684\u8def\u5f84\u4e3ad:/a/b/c/d.txt\uff0csrcRootDir\u4e3ad:/a/b\uff0c\u5219\u538b\u7f29\u540e\u7684\u6587\u4ef6\u4e0e\u76ee\u5f55\u4e3a\u7ed3\u6784\u4e3ac/d.txt\n\n@param out \u538b\u7f29\u6587\u4ef6\u5b58\u50a8\u5bf9\u8c61\n@param srcRootDir \u88ab\u538b\u7f29\u7684\u6587\u4ef6\u5939\u6839\u76ee\u5f55\n@param file \u5f53\u524d\u9012\u5f52\u538b\u7f29\u7684\u6587\u4ef6\u6216\u76ee\u5f55\u5bf9\u8c61\n@throws UtilException IO\u5f02\u5e38", "input": "Java", "output": "private static void zip(File file, String srcRootDir, ZipOutputStream out) throws UtilException {\r\n\t\tif (file == null) {\r\n\t\t\treturn;\r\n\t\t}\r\n\r\n\t\tfinal String subPath = FileUtil.subPath(srcRootDir, file); // \u83b7\u53d6\u6587\u4ef6\u76f8\u5bf9\u4e8e\u538b\u7f29\u6587\u4ef6\u5939\u6839\u76ee\u5f55\u7684\u5b50\u8def\u5f84\r\n\t\tif (file.isDirectory()) {// \u5982\u679c\u662f\u76ee\u5f55\uff0c\u5219\u538b\u7f29\u538b\u7f29\u76ee\u5f55\u4e2d\u7684\u6587\u4ef6\u6216\u5b50\u76ee\u5f55\r\n\t\t\tfinal File[] files = file.listFiles();\r\n\t\t\tif (ArrayUtil.isEmpty(files) && StrUtil.isNotEmpty(subPath)) {\r\n\t\t\t\t// \u52a0\u5165\u76ee\u5f55\uff0c\u53ea\u6709\u7a7a\u76ee\u5f55\u65f6\u624d\u52a0\u5165\u76ee\u5f55\uff0c\u975e\u7a7a\u65f6\u4f1a\u5728\u521b\u5efa\u6587\u4ef6\u65f6\u81ea\u52a8\u6dfb\u52a0\u7236\u7ea7\u76ee\u5f55\r\n\t\t\t\taddDir(subPath, out);\r\n\t\t\t}\r\n\t\t\t// \u538b\u7f29\u76ee\u5f55\u4e0b\u7684\u5b50\u6587\u4ef6\u6216\u76ee\u5f55\r\n\t\t\tfor (File childFile : files) {\r\n\t\t\t\tzip(childFile, srcRootDir, out);\r\n\t\t\t}\r\n\t\t} else {// \u5982\u679c\u662f\u6587\u4ef6\u6216\u5176\u5b83\u7b26\u53f7\uff0c\u5219\u76f4\u63a5\u538b\u7f29\u8be5\u6587\u4ef6\r\n\t\t\taddFile(file, subPath, out);\r\n\t\t}\r\n\t}"}, {"instruction": "This method initializes popupMenuFind\n\n@return org.parosproxy.paros.extension.ExtensionPopupMenu", "input": "Java", "output": "private PopupFindMenu getPopupMenuFind() {\r\n        if (popupFindMenu== null) {\r\n            popupFindMenu = new PopupFindMenu();\r\n            popupFindMenu.setText(Constant.messages.getString(\"edit.find.popup\"));\t// ZAP: i18n\r\n            popupFindMenu.addActionListener(new java.awt.event.ActionListener() {\r\n                @Override\r\n                public void actionPerformed(java.awt.event.ActionEvent e) {\r\n                    JTextComponent component = popupFindMenu.getLastInvoker();\r\n                    Window window = getWindowAncestor(component);\r\n                    if (window != null) {\r\n                        showFindDialog(window, component);\r\n                    }\r\n                }\r\n            });\r\n        }\r\n        return popupFindMenu;\r\n    }"}, {"instruction": "\u5c06\u5c5e\u6027\u7684\u8bcd\u6027\u9501\u5b9a\u4e3anature\n\n@param nature \u8bcd\u6027\n@return \u5982\u679c\u9501\u5b9a\u8bcd\u6027\u5728\u8bcd\u6027\u5217\u8868\u4e2d\uff0c\u8fd4\u56de\u771f\uff0c\u5426\u5219\u8fd4\u56de\u5047", "input": "Java", "output": "public boolean confirmNature(Nature nature)\n    {\n        if (attribute.nature.length == 1 && attribute.nature[0] == nature)\n        {\n            return true;\n        }\n        boolean result = true;\n        int frequency = attribute.getNatureFrequency(nature);\n        if (frequency == 0)\n        {\n            frequency = 1000;\n            result = false;\n        }\n        attribute = new CoreDictionary.Attribute(nature, frequency);\n        return result;\n    }"}, {"instruction": "Convenience function equivalent to calling\n{@link #createColumnSelectorPluses(ColumnSelectorStrategyFactory, List, ColumnSelectorFactory)} with a singleton\nlist of dimensionSpecs and then retrieving the only element in the returned array.\n\n@param <ColumnSelectorStrategyClass> The strategy type created by the provided strategy factory.\n@param strategyFactory               A factory provided by query engines that generates type-handling strategies\n@param dimensionSpec                 column to generate a ColumnSelectorPlus object for\n@param cursor                        Used to create value selectors for columns.\n\n@return A ColumnSelectorPlus object", "input": "Java", "output": "public static <ColumnSelectorStrategyClass extends ColumnSelectorStrategy> ColumnSelectorPlus<ColumnSelectorStrategyClass> createColumnSelectorPlus(\n      ColumnSelectorStrategyFactory<ColumnSelectorStrategyClass> strategyFactory,\n      DimensionSpec dimensionSpec,\n      ColumnSelectorFactory cursor\n  )\n  {\n    return createColumnSelectorPluses(strategyFactory, ImmutableList.of(dimensionSpec), cursor)[0];\n  }"}, {"instruction": "To use this method, {@link #objectMapper} should be a smileMapper.", "input": "Java", "output": "protected FullResponseHolder submitSmileRequest(\n      String taskId,\n      HttpMethod method,\n      String encodedPathSuffix,\n      @Nullable String encodedQueryString,\n      byte[] content,\n      boolean retry\n  ) throws IOException, ChannelException, NoTaskLocationException\n  {\n    return submitRequest(\n        taskId,\n        SmileMediaTypes.APPLICATION_JACKSON_SMILE,\n        method,\n        encodedPathSuffix,\n        encodedQueryString,\n        content,\n        retry\n    );\n  }"}, {"instruction": "Method used to setup the toolbar elements. Should not usually be overriden. Instead, use the\n{@link #addToolBarElements(JToolBar, short, int)} method to add elements at various points.\n@param toolbar the tool bar of the status panel", "input": "Java", "output": "protected void setupToolbarElements(JToolBar toolbar) {\n\t\tint x = 0;\n\t\tInsets insets = new Insets(0, 4, 0, 2);\n\n\t\tx = this.addToolBarElements(toolbar, TOOLBAR_LOCATION_START, x);\n\n\t\ttoolbar.add(new JLabel(Constant.messages.getString(panelPrefix + \".toolbar.context.label\")),\n\t\t\t\tLayoutHelper.getGBC(x++, 0, 1, 0, insets));\n\t\ttoolbar.add(getContextSelectComboBox(), LayoutHelper.getGBC(x++, 0, 1, 0, insets));\n\n\t\tx = this.addToolBarElements(toolbar, TOOLBAR_LOCATION_AFTER_CONTEXTS_SELECT, x);\n\n\t\ttoolbar.add(new JLabel(), LayoutHelper.getGBC(x++, 0, 1, 1.0)); // Spacer\n\t\tif (hasOptions()) {\n\t\t\ttoolbar.add(getOptionsButton(), LayoutHelper.getGBC(x++, 0, 1, 0, insets));\n\t\t}\n\n\t\tthis.addToolBarElements(toolbar, TOOLBAR_LOCATION_END, x);\n\t}"}, {"instruction": "Creates a projection list that removes given columns.\n\n<p><b>NOTE:</b> Resulting expression are still unresolved.\n\n@param inputFields names of current columns\n@param dropExpressions columns to remove\n@return projection expressions", "input": "Java", "output": "public static List<Expression> dropFields(List<String> inputFields, List<Expression> dropExpressions) {\n\t\tSet<String> columnsToDrop = dropExpressions.stream()\n\t\t\t.map(expr -> expr.accept(dropColumnsExtractor))\n\t\t\t.collect(Collectors.toSet());\n\n\t\tcolumnsToDrop.forEach(c -> {\n\t\t\tif (!inputFields.contains(c)) {\n\t\t\t\tthrow new ValidationException(format(\"Field %s does not exist in source table\", c));\n\t\t\t}\n\t\t});\n\n\t\treturn inputFields.stream()\n\t\t\t.filter(oldName -> !columnsToDrop.contains(oldName))\n\t\t\t.map(UnresolvedReferenceExpression::new)\n\t\t\t.collect(Collectors.toList());\n\t}"}, {"instruction": "Shortlist variables which match a given regex. Returns empty empty list, if no\neligible variable is found", "input": "Java", "output": "public static List<Variable> getVariablesByRegex(\n      final Collection<Variable> variables, final String regex) {\n    final List<Variable> shortlistedVariables = new ArrayList<>();\n    if (variables != null && regex != null) {\n      for (final Variable var : variables) {\n        if (var.getTitle().matches(regex)) {\n          shortlistedVariables.add(var);\n        }\n      }\n    }\n    return shortlistedVariables;\n  }"}, {"instruction": "Translates to an \"expression\" type leaf filter. Used as a fallback if we can't use a simple leaf filter.", "input": "Java", "output": "@Nullable\n  private static DimFilter toExpressionLeafFilter(\n      final PlannerContext plannerContext,\n      final RowSignature rowSignature,\n      final RexNode rexNode\n  )\n  {\n    final DruidExpression druidExpression = toDruidExpression(plannerContext, rowSignature, rexNode);\n    return druidExpression != null\n           ? new ExpressionDimFilter(druidExpression.getExpression(), plannerContext.getExprMacroTable())\n           : null;\n  }"}, {"instruction": "Create an SDVariable with a fixed/constant value<br>\nConstants are not modified by training/backprop. See {@link VariableType} for more details.\n@param name  Name of the constant SDVariable\n@param constant Value for the constant SDVariable\n@return The created variable", "input": "Java", "output": "public SDVariable constant(String name, @NonNull INDArray constant){\n        Preconditions.checkState(!variables.containsKey(name), \"Variable with name \\\"%s\\\" already exists\", name);\n        if (name == null || name.length() < 1)\n            name = getNewVarName();\n        SDVariable v = new SDVariable(name, VariableType.CONSTANT, this, constant.shape(), constant.dataType(), null);\n        variables.put(name, Variable.builder().name(name).variable(v).build());\n        constantArrays.put(name, new DeviceLocalNDArray(constant));\n        return v;\n    }"}, {"instruction": "\u6784\u5efa\u961f\u5217\n\n@param size       \u961f\u5217\u5927\u5c0f\n@param isPriority \u662f\u5426\u4f18\u5148\u7ea7\u961f\u5217\n@return \u961f\u5217", "input": "Java", "output": "public static BlockingQueue<Runnable> buildQueue(int size, boolean isPriority) {\n        BlockingQueue<Runnable> queue;\n        if (size == 0) { // \u9ed8\u8ba4\u65e0\u961f\u5217\n            queue = new SynchronousQueue<Runnable>();\n        } else { // \u6709\u9650\u961f\u5217\u6216\u65e0\u9650\u961f\u5217\n            if (isPriority) {\n                queue = size < 0 ? new PriorityBlockingQueue<Runnable>()\n                    : new PriorityBlockingQueue<Runnable>(size);\n            } else {\n                queue = size < 0 ? new LinkedBlockingQueue<Runnable>()\n                    : new LinkedBlockingQueue<Runnable>(size);\n            }\n        }\n        return queue;\n    }"}, {"instruction": "Adds a reference to a section of the http data. Should only\nbe called after data has been added to the underlying http data.\n\n@param onError A consumer to call if an IOException occurs\n@return The newly added component, or null if an error occurred", "input": "Java", "output": "Component addComponent(Consumer<IOException> onError) {\n        Component component;\n        try {\n            long readable = readableBytes(data);\n            long offset = position.getAndUpdate(p -> readable);\n            int length = new Long(readable - offset).intValue();\n            component = new Component(length, offset);\n            components.add(component);\n        } catch (IOException e) {\n            onError.accept(e);\n            return null;\n        }\n\n        if (!data.isInMemory()) {\n            fileAccess.getAndUpdate(channel -> {\n                if (channel == null) {\n                    try {\n                        return new RandomAccessFile(data.getFile(), \"r\");\n                    } catch (IOException e) {\n                        onError.accept(e);\n                    }\n                }\n                return channel;\n            });\n        }\n\n        return component;\n    }"}, {"instruction": "Called from RequestVariable.shutdown() to unschedule the task.", "input": "Java", "output": "public void shutdown() {\n        RequestBatch<BatchReturnType, ResponseType, RequestArgumentType> currentBatch = batch.getAndSet(null);\n        if (currentBatch != null) {\n            currentBatch.shutdown();\n        }\n\n        if (timerListenerReference.get() != null) {\n            // if the timer was started we'll clear it so it stops ticking\n            timerListenerReference.get().clear();\n        }\n    }"}, {"instruction": "\u63d0\u4f9b\u7cbe\u786e\u7684\u51cf\u6cd5\u8fd0\u7b97<br>\n\u5982\u679c\u4f20\u5165\u591a\u4e2a\u503c\u4e3anull\u6216\u8005\u7a7a\uff0c\u5219\u8fd4\u56de0\n\n@param values \u591a\u4e2a\u88ab\u51cf\u503c\n@return \u5dee\n@since 4.0.0", "input": "Java", "output": "public static BigDecimal sub(Number... values) {\r\n\t\tif (ArrayUtil.isEmpty(values)) {\r\n\t\t\treturn BigDecimal.ZERO;\r\n\t\t}\r\n\r\n\t\tNumber value = values[0];\r\n\t\tBigDecimal result = new BigDecimal(null == value ? \"0\" : value.toString());\r\n\t\tfor (int i = 1; i < values.length; i++) {\r\n\t\t\tvalue = values[i];\r\n\t\t\tif (null != value) {\r\n\t\t\t\tresult = result.subtract(new BigDecimal(value.toString()));\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn result;\r\n\t}"}, {"instruction": "[VARIABLE \"my_dataset\"]", "input": "Java", "output": "public Sink updateSink(String sinkName, String datasetName) {\n    // [START logging_update_sink]\n    SinkInfo sinkInfo =\n        SinkInfo.newBuilder(sinkName, DatasetDestination.of(datasetName))\n            .setVersionFormat(SinkInfo.VersionFormat.V2)\n            .setFilter(\"severity>=ERROR\")\n            .build();\n    Sink sink = logging.update(sinkInfo);\n    // [END logging_update_sink]\n    return sink;\n  }"}, {"instruction": "Writes the request headers to the given {@link HttpConnection connection}.\n\n<p>\nThis implementation invokes {@link #addRequestHeaders(HttpState,HttpConnection)},\nand then writes each header to the request stream.\n</p>\n\n<p>\nSubclasses may want to override this method to to customize the\nprocessing.\n</p>\n\n@param state the {@link HttpState state} information associated with this method\n@param conn the {@link HttpConnection connection} used to execute\nthis HTTP method\n\n@throws IOException if an I/O (transport) error occurs. Some transport exceptions\ncan be recovered from.\n@throws HttpException  if a protocol exception occurs. Usually protocol exceptions\ncannot be recovered from.\n\n@see #addRequestHeaders\n@see #getRequestHeaders", "input": "Java", "output": "protected void writeRequestHeaders(HttpState state, HttpConnection conn)\n    throws IOException, HttpException {\n        LOG.trace(\"enter HttpMethodBase.writeRequestHeaders(HttpState,\"\n            + \"HttpConnection)\");\n        addRequestHeaders(state, conn);\n\n        String charset = getParams().getHttpElementCharset();\n        \n        Header[] headers = getRequestHeaders();\n        for (int i = 0; i < headers.length; i++) {\n            String s = headers[i].toExternalForm();\n            if (Wire.HEADER_WIRE.enabled()) {\n                Wire.HEADER_WIRE.output(s);\n            }\n            conn.print(s, charset);\n        }\n    }"}, {"instruction": "Execute the specified TransformProcess with the given <i>sequence</i> input data<br>\nNote: this method can only be used if the TransformProcess starts with sequence data, and also returns sequence data\n\n@param inputSequence    Input sequence data to process\n@param transformProcess TransformProcess to execute\n@return Processed (non-sequential) data", "input": "Java", "output": "public static List<List<List<Writable>>> executeSequenceToSequence(List<List<List<Writable>>> inputSequence,\n                                                                       TransformProcess transformProcess) {\n        if (!(transformProcess.getFinalSchema() instanceof SequenceSchema)) {\n            List<List<List<Writable>>> ret = new ArrayList<>(inputSequence.size());\n            for(List<List<Writable>> timeStep : inputSequence) {\n                ret.add(execute(timeStep,null, transformProcess).getFirst());\n            }\n\n            return ret;\n        }\n\n        return execute(null, inputSequence, transformProcess).getSecond();\n    }"}, {"instruction": "Convert a orc struct field of the \"root\" {@link OrcStruct} that represents the \"row\". This method has a cache of\nfield names to field index that is ONLY valid for this {@link OrcStruct}, and should not be used for\nnested {@link OrcStruct} fields of the row. Looks up field index by field name, and delegates to\n{@link OrcStructConverter#convertField(OrcStruct, int)}.", "input": "Java", "output": "@Nullable\n  Object convertRootField(OrcStruct struct, String fieldName)\n  {\n    // this cache is only valid for the root level, to skip the indexOf on fieldNames to get the fieldIndex.\n    TypeDescription schema = struct.getSchema();\n    final List<String> fields = schema.getFieldNames();\n    if (fieldIndexCache == null) {\n      fieldIndexCache = new Object2IntOpenHashMap<>(fields.size());\n      for (int i = 0; i < fields.size(); i++) {\n        fieldIndexCache.put(fields.get(i), i);\n      }\n    }\n    WritableComparable wc = struct.getFieldValue(fieldName);\n\n    int fieldIndex = fieldIndexCache.getOrDefault(fieldName, -1);\n\n    return convertField(struct, fieldIndex);\n  }"}, {"instruction": "\u63d0\u4ea4\u6279\u6b21", "input": "Java", "output": "public void commit() {\n        if (getBulk().numberOfActions() > 0) {\n            BulkResponse response = getBulk().execute().actionGet();\n            if (response.hasFailures()) {\n                for (BulkItemResponse itemResponse : response.getItems()) {\n                    if (!itemResponse.isFailed()) {\n                        continue;\n                    }\n\n                    if (itemResponse.getFailure().getStatus() == RestStatus.NOT_FOUND) {\n                        logger.error(itemResponse.getFailureMessage());\n                    } else {\n                        throw new RuntimeException(\"ES sync commit error\" + itemResponse.getFailureMessage());\n                    }\n                }\n            }\n        }\n    }"}, {"instruction": "\u5224\u65ad\u6307\u5b9a\u7684\u5355\u5143\u683c\u662f\u5426\u662f\u5408\u5e76\u5355\u5143\u683c\n\n@param sheet {@link Sheet}\n@param row \u884c\u53f7\n@param column \u5217\u53f7\n@return \u662f\u5426\u662f\u5408\u5e76\u5355\u5143\u683c", "input": "Java", "output": "public static boolean isMergedRegion(Sheet sheet, int row, int column) {\r\n\t\tfinal int sheetMergeCount = sheet.getNumMergedRegions();\r\n\t\tCellRangeAddress ca;\r\n\t\tfor (int i = 0; i < sheetMergeCount; i++) {\r\n\t\t\tca = sheet.getMergedRegion(i);\r\n\t\t\tif (row >= ca.getFirstRow() && row <= ca.getLastRow() && column >= ca.getFirstColumn() && column <= ca.getLastColumn()) {\r\n\t\t\t\treturn true;\r\n\t\t\t}\r\n\t\t}\r\n\t\treturn false;\r\n\t}"}, {"instruction": "Called when a checkpoint barrier arrives. It closes any open streams to the backend\nand marks them as pending for committing to the external, third-party storage system.\n\n@param checkpointId the id of the latest received checkpoint.\n@throws IOException in case something went wrong when handling the stream to the backend.", "input": "Java", "output": "private void saveHandleInState(final long checkpointId, final long timestamp) throws Exception {\n\n\t\t//only add handle if a new OperatorState was created since the last snapshot\n\t\tif (out != null) {\n\t\t\tint subtaskIdx = getRuntimeContext().getIndexOfThisSubtask();\n\t\t\tStreamStateHandle handle = out.closeAndGetHandle();\n\n\t\t\tPendingCheckpoint pendingCheckpoint = new PendingCheckpoint(\n\t\t\t\tcheckpointId, subtaskIdx, timestamp, handle);\n\n\t\t\tif (pendingCheckpoints.contains(pendingCheckpoint)) {\n\t\t\t\t//we already have a checkpoint stored for that ID that may have been partially written,\n\t\t\t\t//so we discard this \"alternate version\" and use the stored checkpoint\n\t\t\t\thandle.discardState();\n\t\t\t} else {\n\t\t\t\tpendingCheckpoints.add(pendingCheckpoint);\n\t\t\t}\n\t\t\tout = null;\n\t\t}\n\t}"}, {"instruction": "sm3\u8ba1\u7b97\u540e\u8fdb\u884c16\u8fdb\u5236\u8f6c\u6362\n\n@param data\n\u5f85\u8ba1\u7b97\u7684\u6570\u636e\n@param encoding\n\u7f16\u7801\n@return \u8ba1\u7b97\u7ed3\u679c", "input": "Java", "output": "public static String sm3X16Str(String data, String encoding) {\n\t\tbyte[] bytes = sm3(data, encoding);\n\t\tStringBuilder sm3StrBuff = new StringBuilder();\n\t\tfor (int i = 0; i < bytes.length; i++) {\n\t\t\tif (Integer.toHexString(0xFF & bytes[i]).length() == 1) {\n\t\t\t\tsm3StrBuff.append(\"0\").append(\n\t\t\t\t\t\tInteger.toHexString(0xFF & bytes[i]));\n\t\t\t} else {\n\t\t\t\tsm3StrBuff.append(Integer.toHexString(0xFF & bytes[i]));\n\t\t\t}\n\t\t}\n\t\treturn sm3StrBuff.toString();\n\t}"}, {"instruction": "Find all resources that match the given location pattern via the\nAnt-style PathMatcher. Supports resources in jar files and zip files\nand in the file system.\n\n@param locationPattern the location pattern to match\n@return the result as Resource array\n@throws IOException in case of I/O errors\n@see #doFindPathMatchingJarResources\n@see #doFindPathMatchingFileResources", "input": "Java", "output": "@SuppressWarnings(\"MagicNumber\")\n    protected Resource[] findPathMatchingResources(String locationPattern) throws IOException {\n        String rootDirPath = determineRootDir(locationPattern);\n        String subPattern = locationPattern.substring(rootDirPath.length());\n        Resource[] rootDirResources = getResources(rootDirPath);\n        Set<Resource> result = new LinkedHashSet<Resource>(16);\n        for (Resource rootDirResource : rootDirResources) {\n            rootDirResource = resolveRootDirResource(rootDirResource);\n            if (isJarResource(rootDirResource)) {\n                result.addAll(doFindPathMatchingJarResources(rootDirResource, subPattern));\n            } else {\n                result.addAll(doFindPathMatchingFileResources(rootDirResource, subPattern));\n            }\n        }\n        return result.toArray(new Resource[0]);\n    }"}, {"instruction": "Sets the loaded active scan rules of the add-on, allowing to set the status of the active scan rules appropriately and to\nkeep track of the active scan rules loaded so that they can be removed during uninstallation.\n<p>\n<strong>Note:</strong> Helper method to be used (only) by/during (un)installation process and loading of the add-on.\nShould be called when installing/loading the add-on, by setting the loaded active scan rules, and when uninstalling by\nsetting an empty list. The method {@code setLoadedAscanrulesSet(boolean)} should also be called.\n\n@param ascanrules the active scan rules loaded, might be empty if none were actually loaded\n@throws IllegalArgumentException if {@code ascanrules} is {@code null}.\n@since 2.4.3\n@see #setLoadedAscanrulesSet(boolean)\n@see AbstractPlugin#setStatus(Status)", "input": "Java", "output": "void setLoadedAscanrules(List<AbstractPlugin> ascanrules) {\r\n\t\tif (ascanrules == null) {\r\n\t\t\tthrow new IllegalArgumentException(\"Parameter ascanrules must not be null.\");\r\n\t\t}\r\n\r\n\t\tif (ascanrules.isEmpty()) {\r\n\t\t\tloadedAscanrules = Collections.emptyList();\r\n\t\t\treturn;\r\n\t\t}\r\n\r\n\t\tfor (AbstractPlugin ascanrule : ascanrules) {\r\n\t\t\tascanrule.setStatus(getStatus());\r\n\t\t}\r\n\t\tloadedAscanrules = Collections.unmodifiableList(new ArrayList<>(ascanrules));\r\n\t}"}, {"instruction": "Creates a new ScheduledExecutorService that will use daemon threads with appropriate names the threads.\n@param size The number of threads in the threadpool\n@param poolName The name of the pool (this will be printed in logs)\n@return A new executor service.", "input": "Java", "output": "public static ScheduledExecutorService newScheduledThreadPool(int size, String poolName) {\n        // Caller runs only occurs after shutdown, as queue size is unbounded.\n        ScheduledThreadPoolExecutor result = new ScheduledThreadPoolExecutor(size, getThreadFactory(poolName), new CallerRuns());\n\n        // Do not execute any periodic tasks after shutdown.\n        result.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n\n        // Do not execute any delayed tasks after shutdown.\n        result.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n\n        // Remove tasks from the executor once they are done executing. By default, even when canceled, these tasks are\n        // not removed; if this setting is not enabled we could end up with leaked (and obsolete) tasks.\n        result.setRemoveOnCancelPolicy(true);\n        return result;\n    }"}, {"instruction": "Bind indexed elements to the supplied collection.\n@param name the name of the property to bind\n@param target the target bindable\n@param elementBinder the binder to use for elements\n@param aggregateType the aggregate type, may be a collection or an array\n@param elementType the element type\n@param result the destination for results", "input": "Java", "output": "protected final void bindIndexed(ConfigurationPropertyName name, Bindable<?> target,\n\t\t\tAggregateElementBinder elementBinder, ResolvableType aggregateType,\n\t\t\tResolvableType elementType, IndexedCollectionSupplier result) {\n\t\tfor (ConfigurationPropertySource source : getContext().getSources()) {\n\t\t\tbindIndexed(source, name, target, elementBinder, result, aggregateType,\n\t\t\t\t\telementType);\n\t\t\tif (result.wasSupplied() && result.get() != null) {\n\t\t\t\treturn;\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "Turns a String text into a sequence of tokens.\n\n@param text                 input text\n@param filters              characters to filter\n@param lower                whether to lowercase input or not\n@param split                by which string to split words (usually single space)\n@return Sequence of tokens as String array", "input": "Java", "output": "public static String[] textToWordSequence(String text, String filters, boolean lower, String split) {\n        if (lower)\n            text = text.toLowerCase();\n\n        for (String filter: filters.split(\"\")) {\n            text = text.replace(filter, split);\n        }\n        String[] sequences = text.split(split);\n        List<String> seqList = new ArrayList(Arrays.asList(sequences));\n        seqList.removeAll(Arrays.asList(\"\", null));\n\n        return seqList.toArray(new String[seqList.size()]);\n    }"}, {"instruction": "Calculate the output shape for this op\n\n@return", "input": "Java", "output": "public List<LongShapeDescriptor> calculateOutputShape() {\n        if(x == null || y == null)\n            return Collections.emptyList();\n\n        long[] shapeX = x.shape();\n        long[] shapeY = y.shape();\n\n        return Collections.singletonList(LongShapeDescriptor.fromShape(Shape.broadcastOutputShape(shapeX, shapeY),\n                Shape.pickPairwiseDataType(x.dataType(), y.dataType())));\n    }"}, {"instruction": "Judge logic tables is all belong to binding encryptors.\n\n@param logicTableNames logic table names\n@return logic tables is all belong to binding encryptors or not", "input": "Java", "output": "public boolean isAllBindingTables(final Collection<String> logicTableNames) {\n        if (logicTableNames.isEmpty()) {\n            return false;\n        }\n        Optional<BindingTableRule> bindingTableRule = findBindingTableRule(logicTableNames);\n        if (!bindingTableRule.isPresent()) {\n            return false;\n        }\n        Collection<String> result = new TreeSet<>(String.CASE_INSENSITIVE_ORDER);\n        result.addAll(bindingTableRule.get().getAllLogicTables());\n        return !result.isEmpty() && result.containsAll(logicTableNames);\n    }"}, {"instruction": "\u8ba1\u7b97\u6587\u4ef6\u6821\u9a8c\u7801\n\n@param file \u6587\u4ef6\uff0c\u4e0d\u80fd\u4e3a\u76ee\u5f55\n@param checksum {@link Checksum}\n@return Checksum\n@throws IORuntimeException IO\u5f02\u5e38\n@since 4.0.6", "input": "Java", "output": "public static Checksum checksum(File file, Checksum checksum) throws IORuntimeException {\r\n\t\tAssert.notNull(file, \"File is null !\");\r\n\t\tif (file.isDirectory()) {\r\n\t\t\tthrow new IllegalArgumentException(\"Checksums can't be computed on directories\");\r\n\t\t}\r\n\t\ttry {\r\n\t\t\treturn IoUtil.checksum(new FileInputStream(file), checksum);\r\n\t\t} catch (FileNotFoundException e) {\r\n\t\t\tthrow new IORuntimeException(e);\r\n\t\t}\r\n\t}"}, {"instruction": "Set the {@link RestTemplateCustomizer RestTemplateCustomizers} that should be\napplied to the {@link RestTemplate}. Customizers are applied in the order that they\nwere added after builder configuration has been applied. Setting this value will\nreplace any previously configured customizers.\n@param restTemplateCustomizers the customizers to set\n@return a new builder instance\n@see #additionalCustomizers(RestTemplateCustomizer...)", "input": "Java", "output": "public RestTemplateBuilder customizers(\n\t\t\tCollection<? extends RestTemplateCustomizer> restTemplateCustomizers) {\n\t\tAssert.notNull(restTemplateCustomizers,\n\t\t\t\t\"RestTemplateCustomizers must not be null\");\n\t\treturn new RestTemplateBuilder(this.detectRequestFactory, this.rootUri,\n\t\t\t\tthis.messageConverters, this.requestFactorySupplier,\n\t\t\t\tthis.uriTemplateHandler, this.errorHandler, this.basicAuthentication,\n\t\t\t\tCollections.unmodifiableSet(new LinkedHashSet<RestTemplateCustomizer>(\n\t\t\t\t\t\trestTemplateCustomizers)),\n\t\t\t\tthis.requestFactoryCustomizer, this.interceptors);\n\t}"}, {"instruction": "***********************************************************************", "input": "Java", "output": "private Class<?> attemptFromContextLoader(final String driverClassName) {\n      final ClassLoader threadContextClassLoader = Thread.currentThread().getContextClassLoader();\n      if (threadContextClassLoader != null) {\n         try {\n            final Class<?> driverClass = threadContextClassLoader.loadClass(driverClassName);\n            LOGGER.debug(\"Driver class {} found in Thread context class loader {}\", driverClassName, threadContextClassLoader);\n            return driverClass;\n         } catch (ClassNotFoundException e) {\n            LOGGER.debug(\"Driver class {} not found in Thread context class loader {}, trying classloader {}\",\n               driverClassName, threadContextClassLoader, this.getClass().getClassLoader());\n         }\n      }\n\n      return null;\n   }"}, {"instruction": "Converts a map of class elements to type arguments.\n@param parameters The parametesr\n@return The type arguments", "input": "Java", "output": "@NotNull\n    protected  Map<String, Map<String, Object>>  toTypeArguments(ParameterElement... parameters) {\n        final LinkedHashMap<String, Map<String, Object>>  map = new LinkedHashMap<>(parameters.length);\n        for (ParameterElement ce : parameters) {\n            final ClassElement type = ce.getType();\n            if (type == null) {\n                continue;\n            }\n            final Map<String, ClassElement> subArgs = type.getTypeArguments();\n            if (CollectionUtils.isNotEmpty(subArgs)) {\n                map.put(ce.getName(), toTypeArguments(subArgs));\n            }\n        }\n        return map;\n    }"}, {"instruction": "Adds health check URLs to a target pool.\n\n<p>Sample code:\n\n<pre><code>\ntry (TargetPoolClient targetPoolClient = TargetPoolClient.create()) {\nProjectRegionTargetPoolName targetPool = ProjectRegionTargetPoolName.of(\"[PROJECT]\", \"[REGION]\", \"[TARGET_POOL]\");\nTargetPoolsAddHealthCheckRequest targetPoolsAddHealthCheckRequestResource = TargetPoolsAddHealthCheckRequest.newBuilder().build();\nOperation response = targetPoolClient.addHealthCheckTargetPool(targetPool.toString(), targetPoolsAddHealthCheckRequestResource);\n}\n</code></pre>\n\n@param targetPool Name of the target pool to add a health check to.\n@param targetPoolsAddHealthCheckRequestResource\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "@BetaApi\n  public final Operation addHealthCheckTargetPool(\n      String targetPool,\n      TargetPoolsAddHealthCheckRequest targetPoolsAddHealthCheckRequestResource) {\n\n    AddHealthCheckTargetPoolHttpRequest request =\n        AddHealthCheckTargetPoolHttpRequest.newBuilder()\n            .setTargetPool(targetPool)\n            .setTargetPoolsAddHealthCheckRequestResource(targetPoolsAddHealthCheckRequestResource)\n            .build();\n    return addHealthCheckTargetPool(request);\n  }"}, {"instruction": "Generic method to plan a {@link Runnable}.", "input": "Java", "output": "@Override\n    public void planOperation(Runnable operation) {\n        operations.add(operation);\n\n        if (operation instanceof AbstractOperation) {\n            ExecutionEntity execution = ((AbstractOperation) operation).getExecution();\n            if (execution != null) {\n                commandContext.addInvolvedExecution(execution);\n            }\n        }\n\n        logger.debug(\"Operation {} added to agenda\", operation.getClass());\n    }"}, {"instruction": "Normalize by zero mean unit variance\n\n@param frame the data to normalize\n@return a zero mean unit variance centered\nrdd", "input": "Java", "output": "public static DataRowsFacade zeromeanUnitVariance(DataRowsFacade frame, List<String> skipColumns) {\n        List<String> columnsList = DataFrames.toList(frame.get().columns());\n        columnsList.removeAll(skipColumns);\n        String[] columnNames = DataFrames.toArray(columnsList);\n        //first row is std second row is mean, each column in a row is for a particular column\n        List<Row> stdDevMean = stdDevMeanColumns(frame, columnNames);\n        for (int i = 0; i < columnNames.length; i++) {\n            String columnName = columnNames[i];\n            double std = ((Number) stdDevMean.get(0).get(i)).doubleValue();\n            double mean = ((Number) stdDevMean.get(1).get(i)).doubleValue();\n            if (std == 0.0)\n                std = 1; //All same value -> (x-x)/1 = 0\n\n            frame = dataRows(frame.get().withColumn(columnName, frame.get().col(columnName).minus(mean).divide(std)));\n        }\n\n\n\n        return frame;\n    }"}, {"instruction": "Set a header name and value. If the name is not found, it will be added.\nIf the value is null, the header will be removed.\n\n@param name\n@param value", "input": "Java", "output": "public void setHeader(String name, String value) {\r\n//\t\tint pos = 0;\r\n//\t\tint crlfpos = 0;\r\n        Pattern pattern = null;\r\n\r\n        if (getHeaders(name) == null && value != null) {\r\n            // header value not found, append to end\r\n            addHeader(name, value);\r\n        } else {\r\n            pattern = getHeaderRegex(name);\r\n            Matcher matcher = pattern.matcher(mMsgHeader);\r\n            if (value == null) {\r\n                // delete header\r\n                mMsgHeader = matcher.replaceAll(\"\");\r\n            } else {\r\n                // replace header\r\n                String newString = name + \": \" + value + mLineDelimiter;\r\n                mMsgHeader = matcher.replaceAll(Matcher.quoteReplacement(newString));\r\n            }\r\n\r\n            // set into hashtable\r\n            replaceInternalHeaderFields(name, value);\r\n        }\r\n    }"}, {"instruction": "calc the gradient based on the n-step rewards", "input": "Java", "output": "public Gradient[] calcGradient(IDQN current, Stack<MiniTrans<Integer>> rewards) {\n\n        MiniTrans<Integer> minTrans = rewards.pop();\n\n        int size = rewards.size();\n\n        int[] shape = getHistoryProcessor() == null ? mdp.getObservationSpace().getShape()\n                        : getHistoryProcessor().getConf().getShape();\n        int[] nshape = Learning.makeShape(size, shape);\n        INDArray input = Nd4j.create(nshape);\n        INDArray targets = Nd4j.create(size, mdp.getActionSpace().getSize());\n\n        double r = minTrans.getReward();\n        for (int i = size - 1; i >= 0; i--) {\n            minTrans = rewards.pop();\n\n            r = minTrans.getReward() + conf.getGamma() * r;\n            input.putRow(i, minTrans.getObs());\n            INDArray row = minTrans.getOutput()[0];\n            row = row.putScalar(minTrans.getAction(), r);\n            targets.putRow(i, row);\n        }\n\n        return current.gradient(input, targets);\n    }"}, {"instruction": "Convert a string representing a decimal value to a long.\n\nIf the decimal value is not an exact integral value (e.g. 42.0), or if the decimal value\nis too large to be contained within a long, this function returns null.\n\n@param decimalStr string representing a decimal value\n\n@return long equivalent of decimalStr, returns null for non-integral decimals and integral decimal values outside\nof the values representable by longs", "input": "Java", "output": "@Nullable\n  public static Long getExactLongFromDecimalString(String decimalStr)\n  {\n    final Long val = GuavaUtils.tryParseLong(decimalStr);\n    if (val != null) {\n      return val;\n    }\n\n    BigDecimal convertedBD;\n    try {\n      convertedBD = new BigDecimal(decimalStr);\n    }\n    catch (NumberFormatException nfe) {\n      return null;\n    }\n\n    try {\n      return convertedBD.longValueExact();\n    }\n    catch (ArithmeticException ae) {\n      // indicates there was a non-integral part, or the BigDecimal was too big for a long\n      return null;\n    }\n  }"}, {"instruction": "Reads an attribute in {@link #b b}.\n\n@param attrs   prototypes of the attributes that must be parsed during the\nvisit of the class. Any attribute whose type is not equal to\nthe type of one the prototypes is ignored (i.e. an empty\n{@link Attribute} instance is returned).\n@param type    the type of the attribute.\n@param off     index of the first byte of the attribute's content in\n{@link #b b}. The 6 attribute header bytes, containing the\ntype and the length of the attribute, are not taken into\naccount here (they have already been read).\n@param len     the length of the attribute's content.\n@param buf     buffer to be used to call {@link #readUTF8 readUTF8},\n{@link #readClass(int, char[]) readClass} or {@link #readConst\nreadConst}.\n@param codeOff index of the first byte of code's attribute content in\n{@link #b b}, or -1 if the attribute to be read is not a code\nattribute. The 6 attribute header bytes, containing the type\nand the length of the attribute, are not taken into account\nhere.\n@param labels  the labels of the method's code, or <tt>null</tt> if the\nattribute to be read is not a code attribute.\n@return the attribute that has been read, or <tt>null</tt> to skip this\nattribute.", "input": "Java", "output": "private Attribute readAttribute(final Attribute[] attrs, final String type,\n                                    final int off, final int len, final char[] buf, final int codeOff,\n                                    final Label[] labels) {\n        for (int i = 0; i < attrs.length; ++i) {\n            if (attrs[i].type.equals(type)) {\n                return attrs[i].read(this, off, len, buf, codeOff, labels);\n            }\n        }\n        return new Attribute(type).read(this, off, len, null, -1, null);\n    }"}, {"instruction": "TODO: make protected on a minor release", "input": "Java", "output": "byte[] writeAndClose(byte[] payload, StreamWrapper wrapper) throws IOException {\n        ByteArrayOutputStream outputStream = new ByteArrayOutputStream(512);\n        OutputStream compressionStream = wrapper.wrap(outputStream);\n        try {\n            compressionStream.write(payload);\n            compressionStream.flush();\n        } finally {\n            Objects.nullSafeClose(compressionStream);\n        }\n        return outputStream.toByteArray();\n    }"}, {"instruction": "\u83b7\u5f97\u7ed3\u679c\u96c6\u7684\u6240\u6709\u5217\u540d\n\n@param rs \u7ed3\u679c\u96c6\n@return \u5217\u540d\u6570\u7ec4\n@throws DbRuntimeException SQL\u6267\u884c\u5f02\u5e38", "input": "Java", "output": "public static String[] getColumnNames(ResultSet rs) throws DbRuntimeException {\r\n\t\ttry {\r\n\t\t\tResultSetMetaData rsmd = rs.getMetaData();\r\n\t\t\tint columnCount = rsmd.getColumnCount();\r\n\t\t\tString[] labelNames = new String[columnCount];\r\n\t\t\tfor (int i = 0; i < labelNames.length; i++) {\r\n\t\t\t\tlabelNames[i] = rsmd.getColumnLabel(i + 1);\r\n\t\t\t}\r\n\t\t\treturn labelNames;\r\n\t\t} catch (Exception e) {\r\n\t\t\tthrow new DbRuntimeException(\"Get colunms error!\", e);\r\n\t\t}\r\n\t}"}, {"instruction": "Update references to a renamed job in the fingerprint", "input": "Java", "output": "public synchronized void rename(String oldName, String newName) throws IOException {\n        boolean touched = false;\n        if (original != null) {\n            if (original.getName().equals(oldName)) {\n                original.setName(newName);\n                touched = true;\n            }\n        }\n        \n        if (usages != null) {\n            RangeSet r = usages.get(oldName);\n            if (r != null) {\n                usages.put(newName, r);\n                usages.remove(oldName);\n                touched = true;\n            }\n        }\n        \n        if (touched) {\n            save();\n        }\n    }"}, {"instruction": "------------------------------------------------------------------------", "input": "Java", "output": "@Override\n\tpublic Long getValue() {\n\t\tfinal JobStatus status = eg.getState();\n\n\t\tif (status == JobStatus.RUNNING) {\n\t\t\t// running right now - report the uptime\n\t\t\tfinal long runningTimestamp = eg.getStatusTimestamp(JobStatus.RUNNING);\n\t\t\t// we use 'Math.max' here to avoid negative timestamps when clocks change\n\t\t\treturn Math.max(System.currentTimeMillis() - runningTimestamp, 0);\n\t\t}\n\t\telse if (status.isTerminalState()) {\n\t\t\t// not running any more -> finished or not on leader\n\t\t\treturn NO_LONGER_RUNNING;\n\t\t}\n\t\telse {\n\t\t\t// not yet running or not up at the moment\n\t\t\treturn 0L;\n\t\t}\n\t}"}, {"instruction": "\u6784\u5efa\u7533\u8bf7\u6263\u6b3e\u7684Map\n\n@return \u7533\u8bf7\u6263\u6b3e\u7684Map", "input": "Java", "output": "public Map<String, String> pappayapplyBuild() {\n\t\tMap<String, String> map = new HashMap<String, String>();\n\t\tmap.put(\"appid\", getAppId());\n\t\tmap.put(\"mch_id\", getMchId());\n\t\tmap.put(\"nonce_str\", getNonceStr());\n\t\tmap.put(\"body\", getBody());\n\t\tmap.put(\"attach\", getAttach());\n\t\tmap.put(\"out_trade_no\", getOutTradeNo());\n\t\tmap.put(\"total_fee\", getTotalFee());\n\t\tmap.put(\"spbill_create_ip\", getSpbillCreateIp());\n\t\tmap.put(\"notify_url\", getNotifyUrl());\n\t\tmap.put(\"trade_type\", getTradeType().name());\n\t\tmap.put(\"contract_id\", getContractId());\n\t\tmap.put(\"sign\", PaymentKit.createSign(map, getPaternerKey()));\n\t\treturn map;\n\t}"}, {"instruction": "Returns either the default {@link AvroUtils} which throw an exception in cases where Avro\nwould be needed or loads the specific utils for Avro from flink-avro.", "input": "Java", "output": "public static AvroUtils getAvroUtils() {\n\t\t// try and load the special AvroUtils from the flink-avro package\n\t\ttry {\n\t\t\tClass<?> clazz = Class.forName(AVRO_KRYO_UTILS, false, Thread.currentThread().getContextClassLoader());\n\t\t\treturn clazz.asSubclass(AvroUtils.class).getConstructor().newInstance();\n\t\t} catch (ClassNotFoundException e) {\n\t\t\t// cannot find the utils, return the default implementation\n\t\t\treturn new DefaultAvroUtils();\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(\"Could not instantiate \" + AVRO_KRYO_UTILS + \".\", e);\n\t\t}\n\t}"}, {"instruction": "\u63d2\u5165\u4e00\u4e2a\u8bcd\n\n@param key\n@param value", "input": "Java", "output": "public void put(String key, V value)\n    {\n        if (key.length() == 0) return;  // \u5b89\u5168\u8d77\u89c1\n        BaseNode branch = this;\n        char[] chars = key.toCharArray();\n        for (int i = 0; i < chars.length - 1; ++i)\n        {\n            // \u9664\u4e86\u6700\u540e\u4e00\u4e2a\u5b57\u5916\uff0c\u90fd\u662f\u7ee7\u7eed\n            branch.addChild(new Node(chars[i], Status.NOT_WORD_1, null));\n            branch = branch.getChild(chars[i]);\n        }\n        // \u6700\u540e\u4e00\u4e2a\u5b57\u52a0\u5165\u65f6\u5c5e\u6027\u4e3aend\n        if (branch.addChild(new Node<V>(chars[chars.length - 1], Status.WORD_END_3, value)))\n        {\n            ++size; // \u7ef4\u62a4size\n        }\n    }"}, {"instruction": "Constructs an ApproximateHistogram object from the given byte-buffer representation\n\n@param buf ByteBuffer to construct an ApproximateHistogram from\n\n@return ApproximateHistogram constructed from the given ByteBuffer", "input": "Java", "output": "public static ApproximateHistogram fromBytes(ByteBuffer buf)\n  {\n    // negative size indicates compact representation\n    // this works regardless of whether we use int or short for the size since the leftmost bit is the sign bit\n    if (buf.getShort(buf.position()) < 0) {\n      return fromBytesCompact(buf);\n    } else {\n      // ignore size, determine if sparse or dense based on sign of binCount\n      if (buf.getInt(buf.position() + Integer.BYTES) < 0) {\n        return fromBytesSparse(buf);\n      } else {\n        return fromBytesDense(buf);\n      }\n    }\n  }"}, {"instruction": "Locates the index for the given key. This method probes using double hashing.\n\n@param key the key for an entry in the map.\n@return the index where the key was found, or {@code -1} if no entry is found for that key.", "input": "Java", "output": "private int indexOf(int key) {\n\t\tint startIndex = hashIndex(key);\n\t\tint index = startIndex;\n\n\t\tfor (;;) {\n\t\t\tif (values[index] == null) {\n\t\t\t\t// It's available, so no chance that this value exists anywhere in the map.\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t\tif (key == keys[index]) {\n\t\t\t\treturn index;\n\t\t\t}\n\n\t\t\t// Conflict, keep probing ...\n\t\t\tif ((index = probeNext(index)) == startIndex) {\n\t\t\t\treturn -1;\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "Is usage policy accepted by user?\nLooks into the attributes collected by the principal to find {@link #aupAttributeName}.\nIf the attribute contains {@code true}, then the policy is determined as accepted.\n\n@param principal the principal\n@return true if accepted, false otherwise.", "input": "Java", "output": "protected boolean isUsagePolicyAcceptedBy(final Principal principal) {\n        val attributes = principal.getAttributes();\n        LOGGER.debug(\"Principal attributes found for [{}] are [{}]\", principal.getId(), attributes);\n\n        if (attributes != null && attributes.containsKey(this.aupAttributeName)) {\n            val value = CollectionUtils.toCollection(attributes.get(this.aupAttributeName));\n            LOGGER.debug(\"Evaluating attribute value [{}] found for [{}]\", value, this.aupAttributeName);\n            return value.stream().anyMatch(v -> v.toString().equalsIgnoreCase(Boolean.TRUE.toString()));\n        }\n        return false;\n    }"}, {"instruction": "<pre>\nUses YarnClient to kill the job on HDFS.\nUsing JobClient only works partially:\nIf yarn container has started but spark job haven't, it will kill\nIf spark job has started, the cancel will hang until the spark job is complete\nIf the spark job is complete, it will return immediately, with a job not found on job tracker\n</pre>", "input": "Java", "output": "public static void killJobOnCluster(String applicationId, Logger log) throws YarnException,\n      IOException {\n\n    YarnConfiguration yarnConf = new YarnConfiguration();\n    YarnClient yarnClient = YarnClient.createYarnClient();\n    yarnClient.init(yarnConf);\n    yarnClient.start();\n\n    String[] split = applicationId.split(\"_\");\n    ApplicationId aid = ApplicationId.newInstance(Long.parseLong(split[1]),\n        Integer.parseInt(split[2]));\n\n    log.info(\"start klling application: \" + aid);\n    yarnClient.killApplication(aid);\n    log.info(\"successfully killed application: \" + aid);\n  }"}, {"instruction": "Create alias parser instance.\n\n@param lexerEngine lexical analysis engine.\n@return alias parser instance", "input": "Java", "output": "public static AliasExpressionParser createAliasExpressionParser(final LexerEngine lexerEngine) {\n        switch (lexerEngine.getDatabaseType()) {\n            case H2:\n                return new MySQLAliasExpressionParser(lexerEngine);\n            case MySQL:\n                return new MySQLAliasExpressionParser(lexerEngine);\n            case Oracle:\n                return new OracleAliasExpressionParser(lexerEngine);\n            case SQLServer:\n                return new SQLServerAliasExpressionParser(lexerEngine);\n            case PostgreSQL:\n                return new PostgreSQLAliasExpressionParser(lexerEngine);\n            default:\n                throw new UnsupportedOperationException(String.format(\"Cannot support database type: %s\", lexerEngine.getDatabaseType()));\n        }\n    }"}, {"instruction": "Fit a MultiDataSet (only compute based on the statistics from this dataset)\n\n@param dataSet the dataset to compute on", "input": "Java", "output": "@Override\n    public void fit(@NonNull MultiDataSet dataSet) {\n        Map<Integer, NormalizerStats.Builder> inputStatsBuilders = new HashMap<>();\n        Map<Integer, NormalizerStats.Builder> outputStatsBuilders = new HashMap<>();\n\n        fitPartial(dataSet, inputStatsBuilders, outputStatsBuilders);\n\n        inputStats = buildAllStats(inputStatsBuilders);\n        outputStats = buildAllStats(outputStatsBuilders);\n    }"}, {"instruction": "Invalidates the cache.\n\n@param name The name of the cache to invalidate\n@return A maybe that emits a boolean if the operation was successful", "input": "Java", "output": "@Delete\n    public Maybe<Boolean> invalidateCache(@NotBlank @Selector String name) {\n        try {\n            final SyncCache<Object> cache = cacheManager.getCache(name);\n            return Maybe.create(emitter -> cache.async().invalidateAll().whenComplete((aBoolean, throwable) -> {\n                if (throwable != null) {\n                    emitter.onError(throwable);\n                } else {\n                    emitter.onSuccess(aBoolean);\n                    emitter.onComplete();\n                }\n            }));\n        } catch (ConfigurationException e) {\n            // no cache\n            return Maybe.empty();\n        }\n    }"}, {"instruction": "Calculate percentage of successful builds\nAny build with status InProgress, Aborted is excluded from calculation\nBuilds with status as Success, Unstable is included as success build\n\n@param builds iterable build\n@return percentage of build success", "input": "Java", "output": "private Double fetchBuildSuccessRatio(Iterable<Build> builds) {\n    int totalBuilds = 0, totalSuccess = 0;\n    for (Build build : builds) {\n      if (Constants.IGNORE_STATUS.contains(build.getBuildStatus())) {\n        continue;\n      }\n\n      totalBuilds++;\n      if (Constants.SUCCESS_STATUS.contains(build.getBuildStatus())) {\n        totalSuccess++;\n      }\n    }\n    if (totalBuilds == 0) {\n      return 0.0d;\n    }\n    return ((totalSuccess * 100) / (double) totalBuilds);\n  }"}, {"instruction": "\u83b7\u53d6\u5171\u73b0\u9891\u6b21\n\n@param a \u7b2c\u4e00\u4e2a\u8bcd\n@param b \u7b2c\u4e8c\u4e2a\u8bcd\n@return \u7b2c\u4e00\u4e2a\u8bcd@\u7b2c\u4e8c\u4e2a\u8bcd\u51fa\u73b0\u7684\u9891\u6b21", "input": "Java", "output": "public static int getBiFrequency(String a, String b)\n    {\n        int idA = CoreDictionary.trie.exactMatchSearch(a);\n        if (idA == -1)\n        {\n            return 0;\n        }\n        int idB = CoreDictionary.trie.exactMatchSearch(b);\n        if (idB == -1)\n        {\n            return 0;\n        }\n        int index = binarySearch(pair, start[idA], start[idA + 1] - start[idA], idB);\n        if (index < 0) return 0;\n        index <<= 1;\n        return pair[index + 1];\n    }"}, {"instruction": "Takes an image and executes a pipeline of combined transforms.\n\n@param image to transform, null == end of stream\n@param random object to use (or null for deterministic)\n@return transformed image", "input": "Java", "output": "@Override\n    protected ImageWritable doTransform(ImageWritable image, Random random) {\n        if (shuffle) {\n            Collections.shuffle(imageTransforms);\n        }\n\n        currentTransforms.clear();\n\n        // execute each item in the pipeline\n        for (Pair<ImageTransform, Double> tuple : imageTransforms) {\n            if (tuple.getSecond() == 1.0 || rng.nextDouble() < tuple.getSecond()) { // probability of execution\n                currentTransforms.add(tuple.getFirst());\n                image = random != null ? tuple.getFirst().transform(image, random)\n                        : tuple.getFirst().transform(image);\n            }\n        }\n\n        return image;\n    }"}, {"instruction": "Get the value of the <code>name</code> property as a <code>Class</code>\nimplementing the interface specified by <code>xface</code>.\n\nIf no such property is specified, then <code>defaultValue</code> is\nreturned.\n\nAn exception is thrown if the returned class does not implement the named\ninterface.\n\n@param name the class name.\n@param defaultValue default value.\n@param xface the interface implemented by the named class.\n@return property value as a <code>Class</code>,\nor <code>defaultValue</code>.", "input": "Java", "output": "public <U> Class<? extends U> getClass(String name,\n\t                                       Class<? extends U> defaultValue,\n\t                                       Class<U> xface) {\n\t\ttry {\n\t\t\tClass<?> theClass = getClass(name, defaultValue);\n\t\t\tif (theClass != null && !xface.isAssignableFrom(theClass))\n\t\t\t\tthrow new RuntimeException(theClass+\" not \"+xface.getName());\n\t\t\telse if (theClass != null)\n\t\t\t\treturn theClass.asSubclass(xface);\n\t\t\telse\n\t\t\t\treturn null;\n\t\t} catch (Exception e) {\n\t\t\tthrow new RuntimeException(e);\n\t\t}\n\t}"}, {"instruction": "<code>\n.google.cloud.datalabeling.v1beta1.LabelImageOrientedBoundingBoxOperationMetadata image_oriented_bounding_box_details = 14;\n</code>", "input": "Java", "output": "public com.google.cloud.datalabeling.v1beta1.LabelImageOrientedBoundingBoxOperationMetadata\n      getImageOrientedBoundingBoxDetails() {\n    if (detailsCase_ == 14) {\n      return (com.google.cloud.datalabeling.v1beta1.LabelImageOrientedBoundingBoxOperationMetadata)\n          details_;\n    }\n    return com.google.cloud.datalabeling.v1beta1.LabelImageOrientedBoundingBoxOperationMetadata\n        .getDefaultInstance();\n  }"}, {"instruction": "Creates a {@code JComboBox} with scanners' statuses, \"all\", release, beta and alpha.\n\n@return a {@code JComboBox} with scanners' statuses", "input": "Java", "output": "private JComboBox<String> createStatusComboBox() {\r\n        JComboBox<String> comboBox = new JComboBox<>();\r\n        comboBox.addItem(Constant.messages.getString(\"ascan.policy.table.quality.all\"));\r\n        View view = View.getSingleton();\r\n        comboBox.addItem(view.getStatusUI(AddOn.Status.release).toString());\r\n        comboBox.addItem(view.getStatusUI(AddOn.Status.beta).toString());\r\n        comboBox.addItem(view.getStatusUI(AddOn.Status.alpha).toString());\r\n        return comboBox;\r\n    }"}, {"instruction": "This method places incoming VoidAggregation into clipboard, for further tracking\n\n@param aggregation\n@return TRUE, if given VoidAggregation was the last chunk, FALSE otherwise", "input": "Java", "output": "public boolean pin(@NonNull VoidAggregation aggregation) {\n        RequestDescriptor descriptor =\n                        RequestDescriptor.createDescriptor(aggregation.getOriginatorId(), aggregation.getTaskId());\n        VoidAggregation existing = clipboard.get(descriptor);\n        if (existing == null) {\n            existing = aggregation;\n            trackingCounter.incrementAndGet();\n            clipboard.put(descriptor, aggregation);\n        }\n\n        existing.accumulateAggregation(aggregation);\n\n        //if (counter.incrementAndGet() % 10000 == 0)\n        //    log.info(\"Clipboard stats: Totals: {}; Completed: {};\", clipboard.size(), completedQueue.size());\n\n        int missing = existing.getMissingChunks();\n        if (missing == 0) {\n            //  completedQueue.add(existing);\n            completedCounter.incrementAndGet();\n            return true;\n        } else\n            return false;\n    }"}, {"instruction": "Determines whether the given ByteArraySegment represents an Index Page\n\n@param pageContents The ByteArraySegment to check.\n@return True if Index Page, False if Leaf page.\n@throws IllegalDataFormatException If the given contents is not a valid BTreePage format.", "input": "Java", "output": "static boolean isIndexPage(@NonNull ByteArraySegment pageContents) {\n        // Check ID match.\n        int headerId = BitConverter.readInt(pageContents, ID_OFFSET);\n        int footerId = BitConverter.readInt(pageContents, pageContents.getLength() - FOOTER_LENGTH);\n        if (headerId != footerId) {\n            throw new IllegalDataFormatException(\"Invalid Page Format (id mismatch). HeaderId=%s, FooterId=%s.\", headerId, footerId);\n        }\n\n        int flags = pageContents.get(FLAGS_OFFSET);\n        return (flags & FLAG_INDEX_PAGE) == FLAG_INDEX_PAGE;\n    }"}, {"instruction": "This method recreates and registers all {@link ColumnFamilyDescriptor} from Flink's state meta data snapshot.", "input": "Java", "output": "private List<ColumnFamilyDescriptor> createAndRegisterColumnFamilyDescriptors(\n\t\tList<StateMetaInfoSnapshot> stateMetaInfoSnapshots,\n\t\tboolean registerTtlCompactFilter) {\n\n\t\tList<ColumnFamilyDescriptor> columnFamilyDescriptors =\n\t\t\tnew ArrayList<>(stateMetaInfoSnapshots.size());\n\n\t\tfor (StateMetaInfoSnapshot stateMetaInfoSnapshot : stateMetaInfoSnapshots) {\n\t\t\tRegisteredStateMetaInfoBase metaInfoBase =\n\t\t\t\tRegisteredStateMetaInfoBase.fromMetaInfoSnapshot(stateMetaInfoSnapshot);\n\t\t\tColumnFamilyDescriptor columnFamilyDescriptor = RocksDBOperationUtils.createColumnFamilyDescriptor(\n\t\t\t\tmetaInfoBase, columnFamilyOptionsFactory, registerTtlCompactFilter ? ttlCompactFiltersManager : null);\n\t\t\tcolumnFamilyDescriptors.add(columnFamilyDescriptor);\n\t\t}\n\t\treturn columnFamilyDescriptors;\n\t}"}, {"instruction": "Convert a raw record via\nthe {@link TransformProcess}\nto a base 64ed ndarray\n@param batch the record to convert\n@return teh base 64ed ndarray\n@throws IOException", "input": "Java", "output": "public Base64NDArrayBody toArray(BatchCSVRecord batch) throws IOException {\n        List<List<Writable>> converted =  execute(toArrowWritables(toArrowColumnsString(\n                bufferAllocator,transformProcess.getInitialSchema(),\n                batch.getRecordsAsString()),\n                transformProcess.getInitialSchema()),transformProcess);\n\n        ArrowWritableRecordBatch arrowRecordBatch = (ArrowWritableRecordBatch) converted;\n        INDArray convert = ArrowConverter.toArray(arrowRecordBatch);\n        return new Base64NDArrayBody(Nd4jBase64.base64String(convert));\n    }"}, {"instruction": "[VARIABLE 42]", "input": "Java", "output": "public boolean deleteBlobFromStringsWithGeneration(\n      String bucketName, String blobName, long blobGeneration) {\n    // [START deleteBlobFromStringsWithGeneration]\n    boolean deleted =\n        storage.delete(bucketName, blobName, BlobSourceOption.generationMatch(blobGeneration));\n    if (deleted) {\n      // the blob was deleted\n    } else {\n      // the blob was not found\n    }\n    // [END deleteBlobFromStringsWithGeneration]\n    return deleted;\n  }"}, {"instruction": "Set the {@link TaskExecutorCustomizer TaskExecutorCustomizers} that should be\napplied to the {@link ThreadPoolTaskExecutor}. Customizers are applied in the order\nthat they were added after builder configuration has been applied. Setting this\nvalue will replace any previously configured customizers.\n@param customizers the customizers to set\n@return a new builder instance\n@see #additionalCustomizers(TaskExecutorCustomizer...)", "input": "Java", "output": "public TaskExecutorBuilder customizers(Iterable<TaskExecutorCustomizer> customizers) {\n\t\tAssert.notNull(customizers, \"Customizers must not be null\");\n\t\treturn new TaskExecutorBuilder(this.queueCapacity, this.corePoolSize,\n\t\t\t\tthis.maxPoolSize, this.allowCoreThreadTimeOut, this.keepAlive,\n\t\t\t\tthis.awaitTermination, this.awaitTerminationPeriod, this.threadNamePrefix,\n\t\t\t\tthis.taskDecorator, append(null, customizers));\n\t}"}, {"instruction": "Sort the given range of items using quick sort. {@inheritDoc} If the recursion depth falls below\n{@link #getMaxDepth}, then switch to {@link HeapSort}.", "input": "Java", "output": "public void sort(final IndexedSortable s, int p, int r) {\n\t\tint recordsPerSegment = s.recordsPerSegment();\n\t\tint recordSize = s.recordSize();\n\t\tint maxOffset = recordSize * (recordsPerSegment - 1);\n\n\t\tint pN = p / recordsPerSegment;\n\t\tint pO = (p % recordsPerSegment) * recordSize;\n\n\t\tint rN = r / recordsPerSegment;\n\t\tint rO = (r % recordsPerSegment) * recordSize;\n\n\t\tsortInternal(s, recordsPerSegment, recordSize, maxOffset, p, pN, pO, r, rN, rO, getMaxDepth(r - p));\n\t}"}, {"instruction": "Generates a new Key Hash that is immediately after the given one. We define Key Hash H2 to be immediately after\nKey Hash h1 if there doesn't exist Key Hash H3 such that H1&lt;H3&lt;H2. The ordering is performed using {@link UUID#compareTo}.\n\n@return The successor Key Hash, or null if no more successors are available (if {@link IteratorState#isEnd} returns true).", "input": "Java", "output": "static UUID getNextHash(UUID hash) {\n        if (hash == null) {\n            // No hash given. By definition, the first hash is the \"next\" one\".\n            hash = MIN_HASH;\n        } else if (hash.compareTo(MAX_HASH) >= 0) {\n            // Given hash already equals or exceeds the max value. There is no successor.\n            return null;\n        }\n\n        long msb = hash.getMostSignificantBits();\n        long lsb = hash.getLeastSignificantBits();\n        if (lsb == Long.MAX_VALUE) {\n            msb++; // This won't overflow since we've checked that state is not end (i.e., id != MAX).\n            lsb = Long.MIN_VALUE;\n        } else {\n            lsb++;\n        }\n\n        return new UUID(msb, lsb);\n    }"}, {"instruction": "Optimize and condition.\n\n@return and condition", "input": "Java", "output": "public AndCondition optimize() {\n        AndCondition result = new AndCondition();\n        for (Condition each : conditions) {\n            if (Condition.class.equals(each.getClass())) {\n                result.getConditions().add(each);\n            }\n        }\n        if (result.getConditions().isEmpty()) {\n            result.getConditions().add(new NullCondition());\n        }\n        return result;\n    }"}, {"instruction": "Generate the output for all examples/batches in the input iterator, and concatenate them into a single array\nper network output\n\n@param iterator Data to pass through the network\n@return output for all examples in the iterator", "input": "Java", "output": "public INDArray[] output(MultiDataSetIterator iterator){\n        List<INDArray[]> outputs = new ArrayList<>();\n        while(iterator.hasNext()){\n            MultiDataSet next = iterator.next();\n            INDArray[] out = output(false, next.getFeatures(), next.getFeaturesMaskArrays(), next.getLabelsMaskArrays());\n            outputs.add(out);\n        }\n        INDArray[][] arr = outputs.toArray(new INDArray[outputs.size()][0]);\n        return DataSetUtil.mergeFeatures(arr, null).getFirst();\n    }"}, {"instruction": "Converts the given objects into a map of interned strings. See {@link String#intern()}.\n\n@param values The objects\n@return An unmodifiable set of strings\n@see CollectionUtils#mapOf(Object...)", "input": "Java", "output": "@SuppressWarnings(\"unused\")\n    public static Map<String, Object> internMapOf(Object... values) {\n        if (values == null) {\n            return Collections.emptyMap();\n        }\n        int len = values.length;\n        if (len % 2 != 0) {\n            throw new IllegalArgumentException(\"Number of arguments should be an even number representing the keys and values\");\n        }\n\n        Map<String, Object> answer = new HashMap<>((int) (len / 2 / 0.75));\n        int i = 0;\n        while (i < values.length - 1) {\n            String key = values[i++].toString().intern();\n            Object val = values[i++];\n            answer.put(key, val);\n        }\n        return answer;\n    }"}, {"instruction": "Process a resource, searching for links (uris) to other resources.\n\n@param message the HTTP Message", "input": "Java", "output": "private void processResource(HttpMessage message) {\n\t\tList<SpiderParser> parsers = parent.getController().getParsers();\n\n\t\t// Prepare the Jericho source\n\t\tSource source = new Source(message.getResponseBody().toString());\n\t\t\n\t\t// Get the full path of the file\n\t\tString path = null;\n\t\ttry {\n\t\t\tpath = message.getRequestHeader().getURI().getPath();\n\t\t} catch (URIException e) {\n\t\t} finally {\n\t\t\t// Handle null paths.\n\t\t\tif (path == null)\n\t\t\t\tpath = \"\";\n\t\t}\n\t\t\n\t\t// Parse the resource\n\t\tboolean alreadyConsumed = false;\n\t\tfor (SpiderParser parser : parsers) {\t\t\t\n\t\t\tif (parser.canParseResource(message, path, alreadyConsumed)) {\n\t\t\t\tif (log.isDebugEnabled()) log.debug(\"Parser \"+ parser +\" can parse resource '\"+ path + \"'\");\n\t\t\t\tif (parser.parseResource(message, source, depth))\n\t\t\t\t\talreadyConsumed = true;\n\t\t\t} else {\n\t\t\t\tif (log.isDebugEnabled()) log.debug(\"Parser \"+ parser +\" cannot parse resource '\"+ path + \"'\");\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "Normalize the URL for use in the signature. The OAuth spec says the URL protocol and host are to be lower-case,\nand the query and fragments are to be stripped.\n\n@param url The URL.\n@return The URL normalized for use in the signature.", "input": "Java", "output": "protected String normalizeUrl(String url) {\n    try {\n      URL requestURL = new URL(url);\n      StringBuilder normalized = new StringBuilder(requestURL.getProtocol().toLowerCase()).append(\"://\").append(requestURL.getHost().toLowerCase());\n      if ((requestURL.getPort() >= 0) && (requestURL.getPort() != requestURL.getDefaultPort())) {\n        normalized.append(\":\").append(requestURL.getPort());\n      }\n      normalized.append(requestURL.getPath());\n      return normalized.toString();\n    }\n    catch (MalformedURLException e) {\n      throw new IllegalStateException(\"Illegal URL for calculating the OAuth signature.\", e);\n    }\n  }"}, {"instruction": "Updates all the process definition entities to have the correct diagram resource name.  Must\nbe called after createAndPersistNewDiagramsAsNeeded to ensure that any newly-created diagrams\nalready have their resources attached to the deployment.", "input": "Java", "output": "protected void setProcessDefinitionDiagramNames(ParsedDeployment parsedDeployment) {\n        Map<String, ResourceEntity> resources = parsedDeployment.getDeployment().getResources();\n\n        for (ProcessDefinitionEntity processDefinition : parsedDeployment.getAllProcessDefinitions()) {\n            String diagramResourceName = ResourceNameUtil.getProcessDiagramResourceNameFromDeployment(processDefinition,\n                                                                                                      resources);\n            processDefinition.setDiagramResourceName(diagramResourceName);\n        }\n    }"}, {"instruction": "Set the trainingListeners for the ComputationGraph (and all layers in the network)", "input": "Java", "output": "public void setListeners(TrainingListener... listeners) {\n        List<TrainingListener> list = new ArrayList<>();\n        //Check: user might have done setListeners(null) thinking this would clear the current listeners.\n        //This results in an TrainingListener[1] with a single null value -> results in a NPE later\n        if (listeners != null && listeners.length > 0) {\n            for (TrainingListener i : listeners) {\n                if (i != null)\n                    list.add(i);\n            }\n        }\n        setListeners(list);\n    }"}, {"instruction": "This method must be the only place to call {@link #recoverAndCommit(Object)} to ensure that\nthe configuration parameters {@link #transactionTimeout} and\n{@link #ignoreFailuresAfterTransactionTimeout} are respected.", "input": "Java", "output": "private void recoverAndCommitInternal(TransactionHolder<TXN> transactionHolder) {\n\t\ttry {\n\t\t\tlogWarningIfTimeoutAlmostReached(transactionHolder);\n\t\t\trecoverAndCommit(transactionHolder.handle);\n\t\t} catch (final Exception e) {\n\t\t\tfinal long elapsedTime = clock.millis() - transactionHolder.transactionStartTime;\n\t\t\tif (ignoreFailuresAfterTransactionTimeout && elapsedTime > transactionTimeout) {\n\t\t\t\tLOG.error(\"Error while committing transaction {}. \" +\n\t\t\t\t\t\t\"Transaction has been open for longer than the transaction timeout ({}).\" +\n\t\t\t\t\t\t\"Commit will not be attempted again. Data loss might have occurred.\",\n\t\t\t\t\ttransactionHolder.handle, transactionTimeout, e);\n\t\t\t} else {\n\t\t\t\tthrow e;\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "\u5904\u7406\u5b57\u6bb5\u6620\u5c04", "input": "Java", "output": "private List<EventColumn> translateColumns(EventData data, List<EventColumn> columns, DataMediaPair dataMediaPair,\n                                               Multimap<String, String> translateColumnNames,\n                                               TableInfoHolder tableHolder) {\n        List<EventColumn> tcolumns = new ArrayList<EventColumn>();\n        for (EventColumn scolumn : columns) {\n            EventColumn tcolumn = translateColumn(data, scolumn, tableHolder, dataMediaPair, translateColumnNames);\n            if (tcolumn != null) {\n                tcolumns.add(tcolumn);\n            }\n        }\n        return tcolumns;\n    }"}, {"instruction": "\u83b7\u5f97 Sofa Runtime \u7684\u65e5\u5fd7\u5bf9\u8c61\uff0c\u6253\u5370\u51fa\u83b7\u5f97Config Data \u4fe1\u606f", "input": "Java", "output": "private void printConfigData(String dataId, ConfigData configData) {\n\n        StringBuilder sb = new StringBuilder();\n        int count = 0;\n\n        if (configData != null && StringUtils.isNotBlank(configData.getData())) {\n            final String[] split = StringUtils.split(configData.getData(), CONFIG_SEPARATOR);\n            List<String> dataList = Arrays.asList(split);\n            for (String provider : dataList) {\n                sb.append(\"  >>> \").append(provider).append(\"\\n\");\n                count++;\n            }\n        }\n\n        if (LOGGER.isInfoEnabled()) {\n            LOGGER.info(LogCodes.getLiteLog(\n                \"Receive RPC config info: service[{0}]\\n  usable config info[{1}]\\n{2}\",\n                dataId, count, sb.toString()));\n        }\n    }"}, {"instruction": "Throws a DException if the remote throws, wrapping the original exception.", "input": "Java", "output": "@Override public V get() {\n    // check priorities - FJ task can only block on a task with higher priority!\n    Thread cThr = Thread.currentThread();\n    int priority = (cThr instanceof FJWThr) ? ((FJWThr)cThr)._priority : -1;\n    assert _dt.priority() > priority || (_dt.priority() == priority && _dt instanceof MRTask)\n      : \"*** Attempting to block on task (\" + _dt.getClass() + \") with equal or lower priority. Can lead to deadlock! \" + _dt.priority() + \" <=  \" + priority;\n    if( _done ) return result(); // Fast-path shortcut, or throw if exception\n    // Use FJP ManagedBlock for this blocking-wait - so the FJP can spawn\n    // another thread if needed.\n    try { ForkJoinPool.managedBlock(this); } catch( InterruptedException ignore ) { }\n    if( _done ) return result(); // Fast-path shortcut or throw if exception\n    assert isCancelled();\n    return null;\n  }"}, {"instruction": "Returns true if a given context should be invoked.", "input": "Java", "output": "public boolean shouldInvoke(C context) {\n    cleanupExpiredSuppressions();\n\n    if (cache.containsKey(context)) return false;\n\n    Suppression<C> suppression = new Suppression<>(ticker, context, ticker.read() + ttlNanos);\n\n    if (cache.putIfAbsent(context, suppression) != null) return false; // lost race\n\n    suppressions.offer(suppression);\n\n    // If we added an entry, it could make us go over the max size.\n    if (suppressions.size() > cardinality) removeOneSuppression();\n\n    return true;\n  }"}, {"instruction": "Finds an available TCP port.\n\n@param minPortRange The minimum port range\n@param maxPortRange The maximum port range\n@return The available port", "input": "Java", "output": "public static int findAvailableTcpPort(int minPortRange, int maxPortRange) {\n        ArgumentUtils.check(() -> minPortRange > MIN_PORT_RANGE)\n            .orElseFail(\"Port minimum value must be greater than \" + MIN_PORT_RANGE);\n        ArgumentUtils.check(() -> maxPortRange >= minPortRange)\n            .orElseFail(\"Max port range must be greater than minimum port range\");\n        ArgumentUtils.check(() -> maxPortRange <= MAX_PORT_RANGE)\n            .orElseFail(\"Port maximum value must be less than \" + MAX_PORT_RANGE);\n\n        int currentPort = nextPort(minPortRange, maxPortRange);\n        while (!isTcpPortAvailable(currentPort)) {\n            currentPort = nextPort(minPortRange, maxPortRange);\n        }\n        return currentPort;\n    }"}, {"instruction": "Refresh the locally held version of {@link com.netflix.appinfo.AmazonInfo}", "input": "Java", "output": "public synchronized void refresh() {\n        try {\n            AmazonInfo newInfo = getNewAmazonInfo();\n\n            if (shouldUpdate(newInfo, info)) {\n                // the datacenter info has changed, re-sync it\n                logger.info(\"The AmazonInfo changed from : {} => {}\", info, newInfo);\n                this.info = newInfo;\n            }\n        } catch (Throwable t) {\n            logger.error(\"Cannot refresh the Amazon Info \", t);\n        }\n    }"}, {"instruction": "------------------------------------------------------------------------", "input": "Java", "output": "@Override\n\tpublic JobExecutionResult execute(String jobName) throws Exception {\n\t\tPlanExecutor executor = getExecutor();\n\n\t\tPlan p = createProgramPlan(jobName);\n\n\t\t// Session management is disabled, revert this commit to enable\n\t\t//p.setJobId(jobID);\n\t\t//p.setSessionTimeout(sessionTimeout);\n\n\t\tJobExecutionResult result = executor.executePlan(p);\n\n\t\tthis.lastJobExecutionResult = result;\n\t\treturn result;\n\t}"}, {"instruction": "Collects a record and emits it to all writers.", "input": "Java", "output": "@Override\n\tpublic void collect(T record)  {\n\t\tif (record != null) {\n\t\t\tthis.delegate.setInstance(record);\n\t\t\ttry {\n\t\t\t\tfor (RecordWriter<SerializationDelegate<T>> writer : writers) {\n\t\t\t\t\twriter.emit(this.delegate);\n\t\t\t\t}\n\t\t\t}\n\t\t\tcatch (IOException e) {\n\t\t\t\tthrow new RuntimeException(\"Emitting the record caused an I/O exception: \" + e.getMessage(), e);\n\t\t\t}\n\t\t\tcatch (InterruptedException e) {\n\t\t\t\tthrow new RuntimeException(\"Emitting the record was interrupted: \" + e.getMessage(), e);\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\tthrow new NullPointerException(\"The system does not support records that are null.\"\n\t\t\t\t\t\t\t\t+ \"Null values are only supported as fields inside other objects.\");\n\t\t}\n\t}"}, {"instruction": "\u5220\u9664\u5df2\u7ecf\u88ab\u5e9f\u5f03\u7684processId", "input": "Java", "output": "private synchronized void compareProgress(List<Long> processIds) {\n        if (CollectionUtils.isEmpty(processIds) == false) {\n            Long minProcessId = processIds.get(0);\n            // \u5bf9\u6bd4\u4e00\u4e0bprogress\u4e2d\u7684\u8bb0\u5f55\uff0c\u5982\u679c\u5c0f\u4e8e\u5f53\u524d\u6700\u5c0f\u7684processId\uff0c\u76f4\u63a5\u5220\u9664\u5185\u5b58\u4e2d\u7684\u8bb0\u5f55\n            // \u56e0\u4e3a\u53d1\u751f\u8de8\u673a\u5668\u8c03\u7528\u6216\u8005\u51fa\u73b0restart\u6307\u4ee4\uff0c\u5bf9\u5e94\u7684process\u8bb0\u5f55\u4e0d\u4f1a\u88ab\u5220\u9664\n            for (Long processId : progress.keySet()) {\n                if (processId < minProcessId) {\n                    progress.remove(processId);\n                }\n            }\n        }\n    }"}, {"instruction": "Rewrite assignments so that outputs are in terms of the input symbols.\nThis operation only reliably applies to aggregation steps that take partial inputs (e.g. INTERMEDIATE and split FINALs),\nwhich are guaranteed to have exactly one input and one output.\n<p>\nExample:\n'a' := sum('b') => 'b' := sum('b')", "input": "Java", "output": "private static Map<Symbol, Aggregation> inputsAsOutputs(Map<Symbol, Aggregation> assignments)\n    {\n        ImmutableMap.Builder<Symbol, Aggregation> builder = ImmutableMap.builder();\n        for (Map.Entry<Symbol, Aggregation> entry : assignments.entrySet()) {\n            // Should only have one input symbol\n            Symbol input = getOnlyElement(SymbolsExtractor.extractAll(entry.getValue().getCall()));\n            builder.put(input, entry.getValue());\n        }\n        return builder.build();\n    }"}, {"instruction": "Root method that branches for different implementations of {@link KeyedStateHandle}.", "input": "Java", "output": "@Override\n\tpublic RocksDBRestoreResult restore() throws Exception {\n\n\t\tif (restoreStateHandles == null || restoreStateHandles.isEmpty()) {\n\t\t\treturn null;\n\t\t}\n\n\t\tfinal KeyedStateHandle theFirstStateHandle = restoreStateHandles.iterator().next();\n\n\t\tboolean isRescaling = (restoreStateHandles.size() > 1 ||\n\t\t\t!Objects.equals(theFirstStateHandle.getKeyGroupRange(), keyGroupRange));\n\n\t\tif (isRescaling) {\n\t\t\trestoreWithRescaling(restoreStateHandles);\n\t\t} else {\n\t\t\trestoreWithoutRescaling(theFirstStateHandle);\n\t\t}\n\t\treturn new RocksDBRestoreResult(this.db, defaultColumnFamilyHandle,\n\t\t\tnativeMetricMonitor, lastCompletedCheckpointId, backendUID, restoredSstFiles);\n\t}"}, {"instruction": "------ tables ------", "input": "Java", "output": "@Override\n\tpublic void createTable(ObjectPath tablePath, CatalogBaseTable table, boolean ignoreIfExists)\n\t\tthrows TableAlreadyExistException, DatabaseNotExistException {\n\t\tcheckNotNull(tablePath);\n\t\tcheckNotNull(table);\n\n\t\tif (!databaseExists(tablePath.getDatabaseName())) {\n\t\t\tthrow new DatabaseNotExistException(catalogName, tablePath.getDatabaseName());\n\t\t}\n\n\t\tif (tableExists(tablePath)) {\n\t\t\tif (!ignoreIfExists) {\n\t\t\t\tthrow new TableAlreadyExistException(catalogName, tablePath);\n\t\t\t}\n\t\t} else {\n\t\t\ttables.put(tablePath, table.copy());\n\n\t\t\tif ((table instanceof CatalogTable) && ((CatalogTable) table).isPartitioned()) {\n\t\t\t\tpartitions.put(tablePath, new LinkedHashMap<>());\n\t\t\t}\n\t\t}\n\t}"}, {"instruction": "\u8bbe\u7f6e\u8fd4\u56de\u503c\u7c7b\u578b - \u4e3a\u4e86\u8ba9typeHandler\u5728select\u65f6\u6709\u6548\uff0c\u6539\u4e3a\u8bbe\u7f6eresultMap\n\n@param ms\n@param entityClass", "input": "Java", "output": "protected void setResultType(MappedStatement ms, Class<?> entityClass) {\n        EntityTable entityTable = EntityHelper.getEntityTable(entityClass);\n        List<ResultMap> resultMaps = new ArrayList<ResultMap>();\n        resultMaps.add(entityTable.getResultMap(ms.getConfiguration()));\n        MetaObject metaObject = MetaObjectUtil.forObject(ms);\n        metaObject.setValue(\"resultMaps\", Collections.unmodifiableList(resultMaps));\n    }"}, {"instruction": "This method returns 2D array, where each row represents corresponding label\n\n@param labels\n@return", "input": "Java", "output": "@Override\n    public INDArray getWordVectors(@NonNull Collection<String> labels) {\n        int indexes[] = new int[labels.size()];\n        int cnt = 0;\n        boolean useIndexUnknown = useUnknown && vocab.containsWord(getUNK());\n\n        for (String label : labels) {\n            if (vocab.containsWord(label)) {\n                indexes[cnt] = vocab.indexOf(label);\n            } else\n                indexes[cnt] = useIndexUnknown ? vocab.indexOf(getUNK()) : -1;\n            cnt++;\n        }\n\n        while (ArrayUtils.contains(indexes, -1)) {\n            indexes = ArrayUtils.removeElement(indexes, -1);\n        }\n        if (indexes.length == 0) {\n                return Nd4j.empty(((InMemoryLookupTable)lookupTable).getSyn0().dataType());\n        }\n\n        INDArray result = Nd4j.pullRows(lookupTable.getWeights(), 1, indexes);\n        return result;\n    }"}, {"instruction": "Initialize the logging system according to preferences expressed through the\n{@link Environment} and the classpath.\n@param environment the environment\n@param classLoader the classloader", "input": "Java", "output": "protected void initialize(ConfigurableEnvironment environment,\n\t\t\tClassLoader classLoader) {\n\t\tnew LoggingSystemProperties(environment).apply();\n\t\tLogFile logFile = LogFile.get(environment);\n\t\tif (logFile != null) {\n\t\t\tlogFile.applyToSystemProperties();\n\t\t}\n\t\tinitializeEarlyLoggingLevel(environment);\n\t\tinitializeSystem(environment, this.loggingSystem, logFile);\n\t\tinitializeFinalLoggingLevels(environment, this.loggingSystem);\n\t\tregisterShutdownHookIfNecessary(environment, this.loggingSystem);\n\t}"}, {"instruction": "\u8f6c\u6362\u4e3a\u5206\u9875\u8bed\u53e5\n\n@param sql\n@param offset\n@param limit\n@return", "input": "Java", "output": "public String convertToPageSql(String sql, Integer offset, Integer limit) {\n        //\u89e3\u6790SQL\n        Statement stmt;\n        try {\n            stmt = CCJSqlParserUtil.parse(sql);\n        } catch (Throwable e) {\n            throw new PageException(\"\u4e0d\u652f\u6301\u8be5SQL\u8f6c\u6362\u4e3a\u5206\u9875\u67e5\u8be2!\", e);\n        }\n        if (!(stmt instanceof Select)) {\n            throw new PageException(\"\u5206\u9875\u8bed\u53e5\u5fc5\u987b\u662fSelect\u67e5\u8be2!\");\n        }\n        //\u83b7\u53d6\u5206\u9875\u67e5\u8be2\u7684select\n        Select pageSelect = getPageSelect((Select) stmt);\n        String pageSql = pageSelect.toString();\n        //\u7f13\u5b58\u79fb\u5230\u5916\u9762\u4e86\uff0c\u6240\u4ee5\u4e0d\u66ff\u6362\u53c2\u6570\n        if (offset != null) {\n            pageSql = pageSql.replace(START_ROW, String.valueOf(offset));\n        }\n        if (limit != null) {\n            pageSql = pageSql.replace(PAGE_SIZE, String.valueOf(limit));\n        }\n        return pageSql;\n    }"}, {"instruction": "Delete registered service.\n\n@param id the id\n@return the registered service", "input": "Java", "output": "@DeleteOperation(produces = {ActuatorMediaType.V2_JSON, \"application/vnd.cas.services+yaml\", MediaType.APPLICATION_JSON_VALUE})\n    public RegisteredService deleteService(@Selector final String id) {\n        if (NumberUtils.isDigits(id)) {\n            val svc = this.servicesManager.findServiceBy(Long.parseLong(id));\n            if (svc != null) {\n                return this.servicesManager.delete(svc);\n            }\n        } else {\n            val svc = this.servicesManager.findServiceBy(id);\n            if (svc != null) {\n                return this.servicesManager.delete(svc);\n            }\n        }\n        LOGGER.warn(\"Could not locate service definition by id [{}]\", id);\n        return null;\n    }"}, {"instruction": "Find a resource for class name.\n\n@param className The class name\n@return The resource if found, {@code null} otherwise", "input": "Java", "output": "public Resource findResourceForClassName(String className) {\n\n        if (className.contains(CLOSURE_MARKER)) {\n            className = className.substring(0, className.indexOf(CLOSURE_MARKER));\n        }\n        Resource resource = classNameToResourceCache.get(className);\n        if (resource == null) {\n            String classNameWithPathSeparator = className.replace(\".\", FILE_SEPARATOR);\n            for (String pathPattern : getSearchPatternForExtension(classNameWithPathSeparator, \".groovy\", \".java\", \".kt\")) {\n                resource = resolveExceptionSafe(pathPattern);\n                if (resource != null && resource.exists()) {\n                    classNameToResourceCache.put(className, resource);\n                    break;\n                }\n            }\n        }\n        return resource != null && resource.exists() ? resource : null;\n    }"}, {"instruction": "Overrides unsupported type conversions/mappings specified by the user.\n@param vec byte vec holding bin\\ary parquet data\n@param requestedTypes user-specified target types\n@return corrected types", "input": "Java", "output": "public static byte[] correctTypeConversions(ByteVec vec, byte[] requestedTypes) {\n    byte[] metadataBytes = VecParquetReader.readFooterAsBytes(vec);\n    ParquetMetadata metadata = VecParquetReader.readFooter(metadataBytes, ParquetMetadataConverter.NO_FILTER);\n    byte[] roughTypes = roughGuessTypes(metadata.getFileMetaData().getSchema());\n    return correctTypeConversions(roughTypes, requestedTypes);\n  }"}, {"instruction": "This method is used in DL4J LSTM implementation\n@param input\n@return", "input": "Java", "output": "public static INDArray toMmulCompatible(INDArray input) {\n        if (input.rank() != 2)\n            throw new IllegalArgumentException(\"Input must be rank 2 (matrix)\");\n        //Same conditions as GemmParams.copyIfNecessary()\n        boolean doCopy = false;\n        if (input.ordering() == 'c' && (input.stride(0) != input.size(1) || input.stride(1) != 1))\n            doCopy = true;\n        else if (input.ordering() == 'f' && (input.stride(0) != 1 || input.stride(1) != input.size(0)))\n            doCopy = true;\n\n        if (doCopy)\n            return Shape.toOffsetZeroCopyAnyOrder(input);\n        else\n            return input;\n    }"}, {"instruction": "Return an array of the underlying storage from {@code buf} into a byte array.\nThe copy will start at {@code start} and copy {@code length} bytes.\nIf {@code copy} is true a copy will be made of the memory.\nIf {@code copy} is false the underlying storage will be shared, if possible.", "input": "Java", "output": "public static byte[] getBytes(ByteBuf buf, int start, int length, boolean copy) {\n        int capacity = buf.capacity();\n        if (isOutOfBounds(start, length, capacity)) {\n            throw new IndexOutOfBoundsException(\"expected: \" + \"0 <= start(\" + start + \") <= start + length(\" + length\n                    + \") <= \" + \"buf.capacity(\" + capacity + ')');\n        }\n\n        if (buf.hasArray()) {\n            if (copy || start != 0 || length != capacity) {\n                int baseOffset = buf.arrayOffset() + start;\n                return Arrays.copyOfRange(buf.array(), baseOffset, baseOffset + length);\n            } else {\n                return buf.array();\n            }\n        }\n\n        byte[] v = PlatformDependent.allocateUninitializedArray(length);\n        buf.getBytes(start, v);\n        return v;\n    }"}, {"instruction": "[TARGET listMetricsAsync(ListOption...)]", "input": "Java", "output": "public Page<Metric> listMetricsAsync() throws ExecutionException, InterruptedException {\n    // [START listMetricsAsync]\n    Future<AsyncPage<Metric>> future = logging.listMetricsAsync(ListOption.pageSize(100));\n    // ...\n    AsyncPage<Metric> metrics = future.get();\n    for (Metric metric : metrics.iterateAll()) {\n      // do something with the metric\n    }\n    // [END listMetricsAsync]\n    return metrics;\n  }"}, {"instruction": "Creates a subscription to a given topic. See the &lt;a\nhref=\"https://cloud.google.com/pubsub/docs/admin#resource_names\"&gt; resource name\nrules&lt;/a&gt;. If the subscription already exists, returns `ALREADY_EXISTS`. If the\ncorresponding topic doesn't exist, returns `NOT_FOUND`.\n\n<p>If the name is not provided in the request, the server will assign a random name for this\nsubscription on the same project as the topic, conforming to the [resource name\nformat](https://cloud.google.com/pubsub/docs/admin#resource_names). The generated name is\npopulated in the returned Subscription object. Note that for REST API requests, you must\nspecify a name in the request.\n\n<p>Sample code:\n\n<pre><code>\ntry (SubscriptionAdminClient subscriptionAdminClient = SubscriptionAdminClient.create()) {\nProjectSubscriptionName name = ProjectSubscriptionName.of(\"[PROJECT]\", \"[SUBSCRIPTION]\");\nProjectTopicName topic = ProjectTopicName.of(\"[PROJECT]\", \"[TOPIC]\");\nPushConfig pushConfig = PushConfig.newBuilder().build();\nint ackDeadlineSeconds = 0;\nSubscription response = subscriptionAdminClient.createSubscription(name, topic, pushConfig, ackDeadlineSeconds);\n}\n</code></pre>\n\n@param name The name of the subscription. It must have the format\n`\"projects/{project}/subscriptions/{subscription}\"`. `{subscription}` must start with a\nletter, and contain only letters (`[A-Za-z]`), numbers (`[0-9]`), dashes (`-`), underscores\n(`_`), periods (`.`), tildes (`~`), plus (`+`) or percent signs (`%`). It must be between 3\nand 255 characters in length, and it must not start with `\"goog\"`\n@param topic The name of the topic from which this subscription is receiving messages. Format\nis `projects/{project}/topics/{topic}`. The value of this field will be `_deleted-topic_`\nif the topic has been deleted.\n@param pushConfig If push delivery is used with this subscription, this field is used to\nconfigure it. An empty `pushConfig` signifies that the subscriber will pull and ack\nmessages using API methods.\n@param ackDeadlineSeconds The approximate amount of time (on a best-effort basis) Pub/Sub waits\nfor the subscriber to acknowledge receipt before resending the message. In the interval\nafter the message is delivered and before it is acknowledged, it is considered to be\n&lt;i&gt;outstanding&lt;/i&gt;. During that time period, the message will not be\nredelivered (on a best-effort basis).\n<p>For pull subscriptions, this value is used as the initial value for the ack deadline. To\noverride this value for a given message, call `ModifyAckDeadline` with the corresponding\n`ack_id` if using non-streaming pull or send the `ack_id` in a\n`StreamingModifyAckDeadlineRequest` if using streaming pull. The minimum custom deadline\nyou can specify is 10 seconds. The maximum custom deadline you can specify is 600 seconds\n(10 minutes). If this parameter is 0, a default value of 10 seconds is used.\n<p>For push delivery, this value is also used to set the request timeout for the call to\nthe push endpoint.\n<p>If the subscriber never acknowledges the message, the Pub/Sub system will eventually\nredeliver the message.\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "public final Subscription createSubscription(\n      ProjectSubscriptionName name,\n      ProjectTopicName topic,\n      PushConfig pushConfig,\n      int ackDeadlineSeconds) {\n\n    Subscription request =\n        Subscription.newBuilder()\n            .setName(name == null ? null : name.toString())\n            .setTopic(topic == null ? null : topic.toString())\n            .setPushConfig(pushConfig)\n            .setAckDeadlineSeconds(ackDeadlineSeconds)\n            .build();\n    return createSubscription(request);\n  }"}, {"instruction": "Parse and return a user defined function of the form \"{arg1 arg2 . (expr)}\"", "input": "Java", "output": "private AstFunction parseFunctionDefinition() {\n    eatChar('{');\n\n    // Parse the list of ids\n    ArrayList<String> ids = new ArrayList<>();\n    ids.add(\"\");  // 1-based ID list\n    while (skipWS() != '.') {\n      String id = token();\n      if (!Character.isJavaIdentifierStart(id.charAt(0)))\n        throw new IllegalASTException(\"variable must be a valid Java identifier: \" + id);\n      for (char c : id.toCharArray())\n        if (!Character.isJavaIdentifierPart(c))\n          throw new IllegalASTException(\"variable must be a valid Java identifier: \" + id);\n      ids.add(id);\n    }\n\n    // Single dot separates the list of ids from the body of the function\n    eatChar('.');\n\n    // Parse the body\n    AstRoot body = parseNext();\n    if (skipWS() != '}')\n      throw new IllegalASTException(\"Expected the end of the function, but found '\" + peek(0) + \"'\");\n    eatChar('}');\n\n    return new AstFunction(ids, body);\n  }"}, {"instruction": "Replicates all ASG status changes to peer eureka nodes except for\nreplication traffic to this node.", "input": "Java", "output": "private void replicateASGInfoToReplicaNodes(final String asgName,\n                                                final ASGStatus newStatus, final PeerEurekaNode node) {\n        CurrentRequestVersion.set(Version.V2);\n        try {\n            node.statusUpdate(asgName, newStatus);\n        } catch (Throwable e) {\n            logger.error(\"Cannot replicate ASG status information to {}\", node.getServiceUrl(), e);\n        }\n    }"}, {"instruction": "Blocking server-streaming example. Calls listFeatures with a rectangle of interest. Prints each\nresponse feature as it arrives.", "input": "Java", "output": "public void listFeatures(int lowLat, int lowLon, int hiLat, int hiLon) {\n    info(\"*** ListFeatures: lowLat={0} lowLon={1} hiLat={2} hiLon={3}\", lowLat, lowLon, hiLat,\n        hiLon);\n\n    Rectangle request =\n        Rectangle.newBuilder()\n            .setLo(Point.newBuilder().setLatitude(lowLat).setLongitude(lowLon).build())\n            .setHi(Point.newBuilder().setLatitude(hiLat).setLongitude(hiLon).build()).build();\n    Iterator<Feature> features;\n    try {\n      features = blockingStub.listFeatures(request);\n      for (int i = 1; features.hasNext(); i++) {\n        Feature feature = features.next();\n        info(\"Result #\" + i + \": {0}\", feature);\n        if (testHelper != null) {\n          testHelper.onMessage(feature);\n        }\n      }\n    } catch (StatusRuntimeException e) {\n      warning(\"RPC failed: {0}\", e.getStatus());\n      if (testHelper != null) {\n        testHelper.onRpcError(e);\n      }\n    }\n  }"}, {"instruction": "Sets the required and optional fields that should be shown in the panel.\n<p>\nAny fields previously set are removed.\n\n@param requiredFields the required fields.\n@param optionalFields the optional fields.\n@throws IllegalArgumentException if the any of the arguments is {@code null}.\n@since 2.7.0\n@see #setFields(String[])", "input": "Java", "output": "public void setFields(String[] requiredFields, String[] optionalFields) {\n\t\tif (requiredFields == null) {\n\t\t\tthrow new IllegalArgumentException(\"Parameter requiredFields must not be null.\");\n\t\t}\n\n\t\tif (optionalFields == null) {\n\t\t\tthrow new IllegalArgumentException(\"Parameter optionalFields must not be null.\");\n\t\t}\n\n\t\tthis.requiredFields = requiredFields;\n\t\tthis.optionalFields = optionalFields;\n\n\t\tthis.textFields = new HashMap<>(requiredFields.length + optionalFields.length);\n\n\t\tremoveAll();\n\n\t\tint fieldIndex = 0;\n\t\tfor (String fieldName : requiredFields) {\n\t\t\taddRequiredField(fieldName, fieldIndex);\n\t\t\tfieldIndex++;\n\t\t}\n\n\t\tfor (String fieldName : optionalFields) {\n\t\t\taddField(fieldName, fieldIndex);\n\t\t\tfieldIndex++;\n\t\t}\n\t\tadd(Box.createVerticalGlue(), LayoutHelper.getGBC(0, fieldIndex, 2, 0.0d, 1.0d));\n\n\t\tvalidate();\n\t}"}, {"instruction": "Parse the given request, resolving its multipart elements.\n\n@param request the request to parse.\n\n@return the parsing result.\n\n@throws MultipartException if multipart resolution failed.", "input": "Java", "output": "private MultipartParsingResult parseRequest(HttpRequest request) throws MultipartException {\n        String encoding = determineEncoding(request);\n        FileUpload fileUpload = prepareFileUpload(encoding);\n        try {\n            RequestBody body = request.getBody();\n            Assert.notNull(body, \"The body cannot be null.\");\n            List<FileItem> fileItems = fileUpload.parseRequest(new BodyContext(body));\n            return parseFileItems(fileItems, encoding);\n        } catch (FileUploadBase.SizeLimitExceededException ex) {\n            throw new MaxUploadSizeExceededException(fileUpload.getSizeMax(), ex);\n        } catch (FileUploadBase.FileSizeLimitExceededException ex) {\n            throw new MaxUploadSizeExceededException(fileUpload.getFileSizeMax(), ex);\n        } catch (FileUploadException ex) {\n            throw new MultipartException(\"Failed to parse multipart servlet request.\", ex);\n        }\n    }"}, {"instruction": "Sets the deprecation status of an image.\n\n<p>If an empty request body is given, clears the deprecation status instead.\n\n<p>Sample code:\n\n<pre><code>\ntry (ImageClient imageClient = ImageClient.create()) {\nProjectGlobalImageName image = ProjectGlobalImageName.of(\"[PROJECT]\", \"[IMAGE]\");\nDeprecationStatus deprecationStatusResource = DeprecationStatus.newBuilder().build();\nOperation response = imageClient.deprecateImage(image, deprecationStatusResource);\n}\n</code></pre>\n\n@param image Image name.\n@param deprecationStatusResource Deprecation status for a public resource.\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "@BetaApi\n  public final Operation deprecateImage(\n      ProjectGlobalImageName image, DeprecationStatus deprecationStatusResource) {\n\n    DeprecateImageHttpRequest request =\n        DeprecateImageHttpRequest.newBuilder()\n            .setImage(image == null ? null : image.toString())\n            .setDeprecationStatusResource(deprecationStatusResource)\n            .build();\n    return deprecateImage(request);\n  }"}, {"instruction": "Inserts the specified element at the tail of this queue if it is\npossible to do so immediately or if capacity limit is exited\nthe oldest element (the head) will be evicted, and then the new element added at the tail.\nThis method is generally preferable to method {@link #add},\nwhich can fail to insert an element only by throwing an exception.\n\n@throws NullPointerException if the specified element is null", "input": "Java", "output": "@Override\n    public boolean offer(final E e) {\n        requireNonNull(e, ILLEGAL_ELEMENT);\n\n        Supplier<Boolean> offerElement = () -> {\n            if (size == 0) {\n                ringBuffer[tailIndex] = e;\n                modificationsCount++;\n                size++;\n            } else if (size == maxSize) {\n                headIndex = nextIndex(headIndex);\n                tailIndex = nextIndex(tailIndex);\n                ringBuffer[tailIndex] = e;\n                modificationsCount++;\n            } else {\n                tailIndex = nextIndex(tailIndex);\n                ringBuffer[tailIndex] = e;\n                size++;\n                modificationsCount++;\n            }\n            return true;\n        };\n        return writeConcurrently(offerElement);\n    }"}, {"instruction": "Execute the specified TransformProcess with the given <i>sequence</i> input data<br>\nNote: this method can only be used if the TransformProcess starts with sequence data, but returns <i>non-sequential</i>\ndata (after reducing or converting sequential data to individual examples)\n\n@param inputSequence    Input sequence data to process\n@param transformProcess TransformProcess to execute\n@return Processed (non-sequential) data", "input": "Java", "output": "public static List<List<Writable>> executeSequenceToSeparate(List<List<List<Writable>>> inputSequence,\n                                                                 TransformProcess transformProcess) {\n        if (transformProcess.getFinalSchema() instanceof SequenceSchema) {\n            throw new IllegalStateException(\"Cannot return sequence data with this method\");\n        }\n\n        return execute(null, inputSequence, transformProcess).getFirst();\n    }"}, {"instruction": "Return 16-bit unsigned int from buffer. (little-endian)\n\n@see mysql-5.1.60/include/my_global.h - uint2korr", "input": "Java", "output": "public final int getUint16(final int pos) {\r\n        final int position = origin + pos;\r\n\r\n        if (pos + 1 >= limit || pos < 0) throw new IllegalArgumentException(\"limit excceed: \"\r\n                                                                            + (pos < 0 ? pos : (pos + 1)));\r\n\r\n        byte[] buf = buffer;\r\n        return (0xff & buf[position]) | ((0xff & buf[position + 1]) << 8);\r\n    }"}, {"instruction": "Returns the Value Proto at 'fieldPath'. Returns null if the field was not found.", "input": "Java", "output": "@Nullable\n  Value extractField(@Nonnull FieldPath fieldPath) {\n    Value value = null;\n\n    if (fields != null) {\n      Iterator<String> components = fieldPath.getSegments().iterator();\n      value = fields.get(components.next());\n\n      while (value != null && components.hasNext()) {\n        if (value.getValueTypeCase() != Value.ValueTypeCase.MAP_VALUE) {\n          return null;\n        }\n        value = value.getMapValue().getFieldsOrDefault(components.next(), null);\n      }\n    }\n\n    return value;\n  }"}, {"instruction": "\u83b7\u5f97\u5b57\u7b26\u4e32\u503c<br>\n\u652f\u6301Clob\u3001Blob\u3001RowId\n\n@param field \u5b57\u6bb5\u540d\n@param charset \u7f16\u7801\n@return \u5b57\u6bb5\u5bf9\u5e94\u503c\n@since 3.0.6", "input": "Java", "output": "public String getStr(String field, Charset charset) {\r\n\t\tfinal Object obj = get(field);\r\n\t\tif (obj instanceof Clob) {\r\n\t\t\treturn SqlUtil.clobToStr((Clob) obj);\r\n\t\t} else if (obj instanceof Blob) {\r\n\t\t\treturn SqlUtil.blobToStr((Blob) obj, charset);\r\n\t\t} else if (obj instanceof RowId) {\r\n\t\t\tfinal RowId rowId = (RowId) obj;\r\n\t\t\treturn StrUtil.str(rowId.getBytes(), charset);\r\n\t\t}\r\n\t\treturn super.getStr(field);\r\n\t}"}, {"instruction": "Processing of queries with BitmapOffsets, whose Bitmaps has different factorized fullness (bucket), reported from\nthis method, uses different copies of the same code, so JIT compiler analyzes and compiles the code for different\nfactorized fullness separately. The goal is to capture frequency of abstraction usage in compressed bitmap\nalgorithms, i. e.\n- \"Zero sequence\" vs. \"Literal\" vs. \"One sequence\" in {@link org.apache.druid.extendedset.intset.ImmutableConciseSet}\n- {@link org.roaringbitmap.ArrayContainer} vs {@link org.roaringbitmap.BitmapContainer} in Roaring\nand then https://shipilev.net/blog/2015/black-magic-method-dispatch/ comes into play. The secondary goal is to\ncapture HotSpot's thresholds, which it uses to compile conditional blocks differently inside bitmap impls. See\nhttps://bugs.openjdk.java.net/browse/JDK-6743900. The default BlockLayoutMinDiamondPercentage=20, i. e. if\nprobability of taking some branch is less than 20%, it is moved out of the hot path (to save some icache?).\n\nOn the other hand, we don't want to factor fullness into too small pieces, because\n- too little queries may fall into those small buckets, and they are not compiled with Hotspot's C2 compiler\n- if there are a lot of queries for each small factorized fullness and their copies of the code is compiled by\nC2, this pollutes code cache and takes time to perform too many compilations, while some of them likely produce\nidentical code.\n\nIdeally there should be as much buckets as possible as long as Hotspot's C2 output for each bucket is different.", "input": "Java", "output": "private static String factorizeFullness(long bitmapCardinality, long numRows)\n  {\n    if (bitmapCardinality == 0) {\n      return \"0\";\n    } else if (bitmapCardinality == numRows) {\n      return \"1\";\n    } else {\n      double fullness = bitmapCardinality / (double) numRows;\n      int index = Arrays.binarySearch(BITMAP_FULLNESS_FACTORIZATION_STOPS, fullness);\n      if (index < 0) {\n        index = ~index;\n      }\n      return FACTORIZED_FULLNESS[index];\n    }\n  }"}, {"instruction": "tag::configuration[]", "input": "Java", "output": "@Bean\n\tpublic TomcatServletWebServerFactory servletWebServerFactory() {\n\t\treturn new TomcatServletWebServerFactory() {\n\n\t\t\t@Override\n\t\t\tprotected void prepareContext(Host host,\n\t\t\t\t\tServletContextInitializer[] initializers) {\n\t\t\t\tsuper.prepareContext(host, initializers);\n\t\t\t\tStandardContext child = new StandardContext();\n\t\t\t\tchild.addLifecycleListener(new Tomcat.FixContextListener());\n\t\t\t\tchild.setPath(\"/cloudfoundryapplication\");\n\t\t\t\tServletContainerInitializer initializer = getServletContextInitializer(\n\t\t\t\t\t\tgetContextPath());\n\t\t\t\tchild.addServletContainerInitializer(initializer, Collections.emptySet());\n\t\t\t\tchild.setCrossContext(true);\n\t\t\t\thost.addChild(child);\n\t\t\t}\n\n\t\t};\n\t}"}, {"instruction": "Get current thread context ClassLoader\n\n@return return ClassLoader", "input": "Java", "output": "public static ClassLoader getDefault() {\n        ClassLoader loader = null;\n        try {\n            loader = Thread.currentThread().getContextClassLoader();\n        } catch (Exception ignored) {\n        }\n        if (loader == null) {\n            loader = Environment.class.getClassLoader();\n            if (loader == null) {\n                try {\n                    // getClassLoader() returning null indicates the bootstrap ClassLoader\n                    loader = ClassLoader.getSystemClassLoader();\n                } catch (Exception e) {\n                    // Cannot access system ClassLoader - oh well, maybe the caller can live with null...\n                }\n            }\n        }\n        return loader;\n    }"}, {"instruction": "Generate an alert when a security issue (risk/info) is found. Custom\nalert name, description and solution will be used.\n\n@param risk the risk of the new alert\n@param confidence the confidence of the new alert\n@param name the name of the new alert\n@param description the description of the new alert\n@param uri the affected URI\n@param param the name/ID of the affected parameter\n@param attack the attack that shows the issue\n@param otherInfo other information about the issue\n@param solution the solution for the issue\n@param evidence the evidence (in the response) that shows the issue\n@param msg the message that shows the issue", "input": "Java", "output": "protected void bingo(int risk, int confidence, String name, String description, String uri,\r\n            String param, String attack, String otherInfo, String solution,\r\n            String evidence, HttpMessage msg) {\r\n        \r\n        log.debug(\"New alert pluginid=\" + +this.getId() + \" \" + name + \" uri=\" + uri);\r\n        Alert alert = new Alert(this.getId(), risk, confidence, name);\r\n        if (uri == null || uri.equals(\"\")) {\r\n            uri = msg.getRequestHeader().getURI().toString();\r\n        }\r\n        \r\n        if (param == null) {\r\n            param = \"\";\r\n        }\r\n        \r\n        alert.setDetail(description, uri, param, attack, otherInfo, solution, this.getReference(),\r\n                evidence, this.getCweId(), this.getWascId(), msg);\r\n        \r\n        parent.alertFound(alert);\r\n    }"}, {"instruction": "Generates the files for the documents relation. The entries apply the\nfollowing format: <br />\n<code>URL | Content</code>\n\n@param noDocs\nNumber of entries for the documents relation\n@param filterKeyWords\nA list of keywords that should be contained\n@param words\nA list of words to fill the entries\n@param path\nOutput path for the documents relation", "input": "Java", "output": "private static void genDocs(int noDocs, String[] filterKeyWords, String[] words, String path) {\n\n\t\tRandom rand = new Random(Calendar.getInstance().getTimeInMillis());\n\n\t\ttry (BufferedWriter fw = new BufferedWriter(new FileWriter(path))) {\n\t\t\tfor (int i = 0; i < noDocs; i++) {\n\n\t\t\t\tint wordsInDoc = rand.nextInt(40) + 10;\n\t\t\t\t// URL\n\t\t\t\tStringBuilder doc = new StringBuilder(\"url_\" + i + \"|\");\n\t\t\t\tfor (int j = 0; j < wordsInDoc; j++) {\n\t\t\t\t\tif (rand.nextDouble() > 0.9) {\n\t\t\t\t\t\t// Approx. every 10th word is a keyword\n\t\t\t\t\t\tdoc.append(filterKeyWords[rand.nextInt(filterKeyWords.length)] + \" \");\n\t\t\t\t\t} else {\n\t\t\t\t\t\t// Fills up the docs file(s) with random words\n\t\t\t\t\t\tdoc.append(words[rand.nextInt(words.length)] + \" \");\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tdoc.append(\"|\\n\");\n\n\t\t\t\tfw.write(doc.toString());\n\t\t\t}\n\t\t} catch (IOException e) {\n\t\t\te.printStackTrace();\n\t\t}\n\t}"}, {"instruction": "Creates a new config option, using this option's key and default value, and\nadding the given deprecated keys.\n\n<p>When obtaining a value from the configuration via {@link Configuration#getValue(ConfigOption)},\nthe deprecated keys will be checked in the order provided to this method. The first key for which\na value is found will be used - that value will be returned.\n\n@param deprecatedKeys The deprecated keys, in the order in which they should be checked.\n@return A new config options, with the given deprecated keys.", "input": "Java", "output": "public ConfigOption<T> withDeprecatedKeys(String... deprecatedKeys) {\n\t\tfinal Stream<FallbackKey> newDeprecatedKeys = Arrays.stream(deprecatedKeys).map(FallbackKey::createDeprecatedKey);\n\t\tfinal Stream<FallbackKey> currentAlternativeKeys = Arrays.stream(this.fallbackKeys);\n\n\t\t// put deprecated keys last so that they are de-prioritized\n\t\tfinal FallbackKey[] mergedAlternativeKeys = Stream.concat(currentAlternativeKeys, newDeprecatedKeys)\n\t\t\t.toArray(FallbackKey[]::new);\n\t\treturn new ConfigOption<>(key, description, defaultValue, mergedAlternativeKeys);\n\t}"}, {"instruction": "Get BatchNormalization epsilon parameter from Keras layer configuration.\n\n@param layerConfig dictionary containing Keras layer configuration\n@return epsilon\n@throws InvalidKerasConfigurationException Invalid Keras config", "input": "Java", "output": "private double getEpsFromConfig(Map<String, Object> layerConfig) throws InvalidKerasConfigurationException {\n        Map<String, Object> innerConfig = KerasLayerUtils.getInnerLayerConfigFromConfig(layerConfig, conf);\n        if (!innerConfig.containsKey(LAYER_FIELD_EPSILON))\n            throw new InvalidKerasConfigurationException(\n                    \"Keras BatchNorm layer config missing \" + LAYER_FIELD_EPSILON + \" field\");\n        return (double) innerConfig.get(LAYER_FIELD_EPSILON);\n    }"}, {"instruction": "Handles entropy injection across regular and entropy-aware file systems.\n\n<p>If the given file system is entropy-aware (a implements {@link EntropyInjectingFileSystem}),\nthen this method replaces the entropy marker in the path with random characters.\nThe entropy marker is defined by {@link EntropyInjectingFileSystem#getEntropyInjectionKey()}.\n\n<p>If the given file system does not implement {@code EntropyInjectingFileSystem},\nthen this method delegates to {@link FileSystem#create(Path, WriteMode)} and\nreturns the same path in the resulting {@code OutputStreamAndPath}.", "input": "Java", "output": "public static OutputStreamAndPath createEntropyAware(\n\t\t\tFileSystem fs,\n\t\t\tPath path,\n\t\t\tWriteMode writeMode) throws IOException {\n\n\t\t// check and possibly inject entropy into the path\n\t\tfinal EntropyInjectingFileSystem efs = getEntropyFs(fs);\n\t\tfinal Path processedPath = efs == null ? path : resolveEntropy(path, efs, true);\n\n\t\t// create the stream on the original file system to let the safety net\n\t\t// take its effect\n\t\tfinal FSDataOutputStream out = fs.create(processedPath, writeMode);\n\t\treturn new OutputStreamAndPath(out, processedPath);\n\t}"}, {"instruction": "\u5c06\u9a7c\u5cf0\u98ce\u683c\u66ff\u6362\u4e3a\u4e0b\u5212\u7ebf\u98ce\u683c", "input": "Java", "output": "public static String camelhumpToUnderline(String str) {\n        final int size;\n        final char[] chars;\n        final StringBuilder sb = new StringBuilder(\n                (size = (chars = str.toCharArray()).length) * 3 / 2 + 1);\n        char c;\n        for (int i = 0; i < size; i++) {\n            c = chars[i];\n            if (isUppercaseAlpha(c)) {\n                sb.append('_').append(toLowerAscii(c));\n            } else {\n                sb.append(c);\n            }\n        }\n        return sb.charAt(0) == '_' ? sb.substring(1) : sb.toString();\n    }"}, {"instruction": "Check if a {@link ByteBuffer} contains a file identifier.\n\n@param bb A {@code ByteBuffer} to check if it contains the identifier\n`ident`.\n@param ident A `String` identifier of the FlatBuffer file.\n@return True if the buffer contains the file identifier", "input": "Java", "output": "protected static boolean __has_identifier(ByteBuffer bb, String ident) {\n    if (ident.length() != FILE_IDENTIFIER_LENGTH)\n        throw new AssertionError(\"FlatBuffers: file identifier must be length \" +\n                                 FILE_IDENTIFIER_LENGTH);\n    for (int i = 0; i < FILE_IDENTIFIER_LENGTH; i++) {\n      if (ident.charAt(i) != (char)bb.get(bb.position() + SIZEOF_INT + i)) return false;\n    }\n    return true;\n  }"}, {"instruction": "Sends the response headers to the client.", "input": "Java", "output": "private void sendResponseHeaders(ChannelHandlerContext ctx, SendResponseHeadersCommand cmd,\n      ChannelPromise promise) throws Http2Exception {\n    // TODO(carl-mastrangelo): remove this check once https://github.com/netty/netty/issues/6296 is\n    // fixed.\n    int streamId = cmd.stream().id();\n    Http2Stream stream = connection().stream(streamId);\n    if (stream == null) {\n      resetStream(ctx, streamId, Http2Error.CANCEL.code(), promise);\n      return;\n    }\n    if (cmd.endOfStream()) {\n      closeStreamWhenDone(promise, streamId);\n    }\n    encoder().writeHeaders(ctx, streamId, cmd.headers(), 0, cmd.endOfStream(), promise);\n  }"}, {"instruction": "Download the resource at getURL() to the local resource directory, and return the local copy as a File\n\n@return File of the local resource", "input": "Java", "output": "protected File getResourceFile(){\n\n        URL url = getURL();\n        String urlString = url.toString();\n        String filename = urlString.substring(urlString.lastIndexOf('/')+1);\n        File resourceDir = DL4JResources.getDirectory(ResourceType.RESOURCE, resourceName());\n        File localFile = new File(resourceDir, filename);\n\n        String expMD5 = resourceMD5();\n        if(localFile.exists()){\n            try{\n                if(Downloader.checkMD5OfFile(expMD5, localFile)){\n                    return localFile;\n                }\n            } catch (IOException e){\n                //Ignore\n            }\n            //MD5 failed\n            localFile.delete();\n        }\n\n        //Download\n        try {\n            Downloader.download(resourceName(), url, localFile, expMD5, 3);\n        } catch (IOException e){\n            throw new RuntimeException(\"Error downloading labels\",e);\n        }\n\n        return localFile;\n    }"}, {"instruction": "Set current key context of this window set.\n\n<p>Notes: {@code initializeCache(Object)} must be called before\n{@link #addWindow(Window, MergeFunction)} and {@link #retireWindow(Window)}\n\n@param key the current access key", "input": "Java", "output": "public void initializeCache(Object key) throws Exception {\n\t\tthis.sortedWindows = cachedSortedWindows.get(key);\n\t\tif (sortedWindows == null) {\n\t\t\tthis.sortedWindows = new TreeSet<>();\n\t\t\tIterator<Map.Entry<W, W>> keyValues = mapping.iterator();\n\t\t\tif (keyValues != null) {\n\t\t\t\twhile (keyValues.hasNext()) {\n\t\t\t\t\tMap.Entry<W, W> keyValue = keyValues.next();\n\t\t\t\t\tthis.sortedWindows.add(keyValue.getKey());\n\t\t\t\t}\n\t\t\t}\n\t\t\tcachedSortedWindows.put(key, sortedWindows);\n\t\t}\n\t}"}, {"instruction": "Creates a {@code MAIL} request.", "input": "Java", "output": "public static SmtpRequest mail(CharSequence sender, CharSequence... mailParameters) {\n        if (mailParameters == null || mailParameters.length == 0) {\n            return new DefaultSmtpRequest(SmtpCommand.MAIL,\n                                          sender != null ? \"FROM:<\" + sender + '>' : FROM_NULL_SENDER);\n        } else {\n            List<CharSequence> params = new ArrayList<CharSequence>(mailParameters.length + 1);\n            params.add(sender != null? \"FROM:<\" + sender + '>' : FROM_NULL_SENDER);\n            for (CharSequence param : mailParameters) {\n                params.add(param);\n            }\n            return new DefaultSmtpRequest(SmtpCommand.MAIL, params);\n        }\n    }"}, {"instruction": "Sets the number of fields in the record. If the new number of fields is longer than the current number of\nfields, then null fields are appended. If the new number of fields is smaller than the current number of\nfields, then the last fields are truncated.\n\n@param numFields The new number of fields.", "input": "Java", "output": "public void setNumFields(final int numFields) {\n\t\tfinal int oldNumFields = this.numFields;\n\t\t// check whether we increase or decrease the fields \n\t\tif (numFields > oldNumFields) {\n\t\t\tmakeSpace(numFields);\n\t\t\tfor (int i = oldNumFields; i < numFields; i++) {\n\t\t\t\tthis.offsets[i] = NULL_INDICATOR_OFFSET;\n\t\t\t}\n\t\t\tmarkModified(oldNumFields);\n\t\t}\n\t\telse {\n\t\t\t// decrease the number of fields\n\t\t\t// we do not remove the values from the cache, as the objects (if they are there) will most likely\n\t\t\t// be reused when the record is re-filled\n\t\t\tmarkModified(numFields);\n\t\t}\n\t\tthis.numFields = numFields;\n\t}"}, {"instruction": "Enables the usage export feature and sets the usage export bucket where reports are stored. If\nyou provide an empty request body using this method, the usage export feature will be disabled.\n\n<p>Sample code:\n\n<pre><code>\ntry (ProjectClient projectClient = ProjectClient.create()) {\nProjectName project = ProjectName.of(\"[PROJECT]\");\nUsageExportLocation usageExportLocationResource = UsageExportLocation.newBuilder().build();\nOperation response = projectClient.setUsageExportBucketProject(project.toString(), usageExportLocationResource);\n}\n</code></pre>\n\n@param project Project ID for this request.\n@param usageExportLocationResource The location in Cloud Storage and naming method of the daily\nusage report. Contains bucket_name and report_name prefix.\n@throws com.google.api.gax.rpc.ApiException if the remote call fails", "input": "Java", "output": "@BetaApi\n  public final Operation setUsageExportBucketProject(\n      String project, UsageExportLocation usageExportLocationResource) {\n\n    SetUsageExportBucketProjectHttpRequest request =\n        SetUsageExportBucketProjectHttpRequest.newBuilder()\n            .setProject(project)\n            .setUsageExportLocationResource(usageExportLocationResource)\n            .build();\n    return setUsageExportBucketProject(request);\n  }"}, {"instruction": "Assign a SDVariable to represent the gradient of the SDVariable with the specified name\n\n@param variableName the variable name to assign the gradient variable for\n@param variable     the gradient variable", "input": "Java", "output": "public void setGradientForVariableName(String variableName, SDVariable variable) {\n        Preconditions.checkState(variables.containsKey(variableName), \"No variable exists with name \\\"%s\\\"\", variableName);\n        if (variable == null) {\n            throw new ND4JIllegalStateException(\"Unable to set null gradient for variable name \" + variableName);\n        }\n        variables.get(variableName).setGradient(variable);\n    }"}, {"instruction": "Locate the vertices and the codewords area of a black blob using the Start\nand Stop patterns as locators.\n\n@param matrix the scanned barcode image.\n@return an array containing the vertices:\nvertices[0] x, y top left barcode\nvertices[1] x, y bottom left barcode\nvertices[2] x, y top right barcode\nvertices[3] x, y bottom right barcode\nvertices[4] x, y top left codeword area\nvertices[5] x, y bottom left codeword area\nvertices[6] x, y top right codeword area\nvertices[7] x, y bottom right codeword area", "input": "Java", "output": "private static ResultPoint[] findVertices(BitMatrix matrix, int startRow, int startColumn) {\n    int height = matrix.getHeight();\n    int width = matrix.getWidth();\n\n    ResultPoint[] result = new ResultPoint[8];\n    copyToResult(result, findRowsWithPattern(matrix, height, width, startRow, startColumn, START_PATTERN),\n        INDEXES_START_PATTERN);\n\n    if (result[4] != null) {\n      startColumn = (int) result[4].getX();\n      startRow = (int) result[4].getY();\n    }\n    copyToResult(result, findRowsWithPattern(matrix, height, width, startRow, startColumn, STOP_PATTERN),\n        INDEXES_STOP_PATTERN);\n    return result;\n  }"}]